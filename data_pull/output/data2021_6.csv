,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1415335214944071685,20916144,Cyrus Samii,"['A new paper with @DanaBurde @YeWang1576 &amp; Joel Middleton uses principal stratum methods to understand why a schooling intervention in Afghanistan that yielded exceptional initial effects yielded substantially smaller effects in a replication years later: <LINK>', 'Substantively, it offers evidence of ""negative selection on gains"" in education, similar to early ed studies in the US (as cited). Methodologically, it illustrates how to unpack and compare effects in situations where contextual conditions affect treatment use patterns.']",https://arxiv.org/abs/2106.15076,"This paper uses a ""principal strata"" approach to decompose treatment effects and interpret why a schooling intervention that yielded exceptional initial effects yielded substantially smaller effects in a replication years later. The specific application is a set of 2008 and 2015 replications of an intervention aiming to increase primary education for girls in rural Afghanistan. The intervention offers a new schooling option, and as such, its effects depend on how individuals use alternatives that already exist. The principal strata approach accounts variation in use patterns when comparing effects across the replications. Our findings show that even though the share of girls for whom the intervention would be valuable dropped considerably in 2015 as compared to 2008, the intervention was even more efficaciousness for those who continued to benefit from it. ","How to Account for Alternatives When Comparing Effects: Revisiting
  'Bringing Education to Afghan Girls'"
1,1413626275655159813,744256054465302532,Anurag Kumar,"['New Paper Out! We try to convert some state-of-the-art methods for speaker separation into online real time systems while trying to retain performance. We do this for both monoaural and binaural speech.  Paper <LINK>.  \nSpeech samples üéß - <LINK>', '#speech #speechseparation #speakerseparation']",https://arxiv.org/abs/2106.13493,"Deep neural networks have recently shown great success in the task of blind source separation, both under monaural and binaural settings. Although these methods were shown to produce high-quality separations, they were mainly applied under offline settings, in which the model has access to the full input signal while separating the signal. In this study, we convert a non-causal state-of-the-art separation model into a causal and real-time model and evaluate its performance under both online and offline settings. We compare the performance of the proposed model to several baseline methods under anechoic, noisy, and noisy-reverberant recording conditions while exploring both monaural and binaural inputs and outputs. Our findings shed light on the relative difference between causal and non-causal models when performing separation. Our stateful implementation for online separation leads to a minor drop in performance compared to the offline model; 0.8dB for monaural inputs and 0.3dB for binaural inputs while reaching a real-time factor of 0.65. Samples can be found under the following link: this https URL ",Online Self-Attentive Gated RNNs for Real-Time Speaker Separation
2,1413432101257367558,636899073,Dr Ella Peltonen,"['A new paper by @wiebketous, @aaronyiding, me, et al. shared in Arxiv: ""Design Considerations for Data Daemons: Co-creating Design Futures to Explore Ethical Personal Data Management"". Comments are welcome! #AIEthics #AI #Ethics #EthicalAI #data <LINK>']",https://arxiv.org/abs/2106.14975,"Mobile applications and online service providers track our virtual and physical behaviour more actively and with a broader scope than ever before. This has given rise to growing concerns about ethical personal data management. Even though regulation and awareness around data ethics are increasing, end-users are seldom engaged when defining and designing what a future with ethical personal data management should look like. We explore a participatory process that uses design futures, the Future workshop method and design fictions to envision ethical personal data management with end-users and designers. To engage participants effectively, we needed to bridge their differential expertise and make the abstract concepts of data and ethics tangible. By concretely presenting personal data management and control as fictitious entities called Data Daemons, we created a shared understanding of these abstract concepts, and empowered non-expert end-users and designers to become actively engaged in the design process. ","Design Considerations for Data Daemons: Co-creating Design Futures to
  Explore Ethical Personal Data Management"
3,1413168126309535746,2485541372,Becca Roelofs,"['1/ New paper: The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning <LINK>\n\nModels pre-trained on larger datasets exhibit effective robustness (ER) during fine-tuning, but ER vanishes at convergence.\n\nWith @AJAndreassen,@yasamanbb, @bneyshabur. <LINK>', '2/ On several robustness benchmarks,  there exists a clear linear relationship between a model‚Äôs final performance on ID and OOD data.\n\nModels that lie off the linear trend (effectively robust models) are historically extremely rare.', '3/ We identify an entire class of effectively robust models  (pre-trained image classification models in the middle of fine tuning), and we find that models that display ER are able to correctly classify 10% of the examples that a wide variety of models with no ER get incorrect. https://t.co/2v5LuWzuSh', '4/ We also found that training on difficult examples boosted ER on ImageNetV2, but reduced the ID accuracy on ImageNet. Understanding how, why, and when difficult examples contribute to robustness is a super interesting research direction!! https://t.co/5tPGTILTCf']",https://arxiv.org/abs/2106.15831,"Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit ""effective robustness"" and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models. ",The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning
4,1413116956555812870,735418346896818176,Dr. Mona Sloane,"['üì¢ New paper out w/ @ruchowdh &amp; @MannyMoss: ""A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and the Quest for Auditability"" <LINK> -- &gt; we propose a matrix for interdisciplinary #audits and socio-technical assessments of #AI used in #hiring üîé']",https://arxiv.org/abs/2106.12403,"In this paper, we suggest a systematic approach for developing socio-technical assessment for hiring ADS. We suggest using a matrix to expose underlying assumptions rooted in pseudoscientific essentialized understandings of human nature and capability, and to critically investigate emerging auditing standards and practices that fail to address these assumptions. ","A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and
  the Quest for Auditability"
5,1412857973706162176,101810581,Animesh Garg,"['New #RSS2021 paper on self-supervised affordance discovery!\n\nGIFT: Generalizable Interaction-aware Functional Tool Affordances without Labels\n\n<LINK>\n<LINK>\n\nw\\ D. Turpin, L. Wang, @StavrosTsogkas, @DickinsonSven\n\n@VectorInst @UofTRobotics <LINK>', ""When it comes to tools, ‚Äò‚ÄòWhat can I do with this?‚Äô‚Äô is the key question &amp; usually, the answer isn't unique.\nAffordance is both ecological (what task) &amp; morphological (who does it)\n\nManual supervision adds morphological bias. \nWhat a human can do may not be possible for a robot! https://t.co/59F0Tc9CHi"", 'GIFT discovers affordances from self-supervised goal-directed interaction that learns both a shape representation and a task policy. \n\nWe represent an action possibility (i.e., an affordance) as a tuple (task ID, grasp keypoint, interaction keypoint). https://t.co/jvSeVgHOjs', 'GIFT models share a task-independent object representation that captures object geometry and proposals for interaction modes. \n\nThe task-specific part focuses on searching over actions to complete the task. https://t.co/PIQHqAljsk', 'Our method was testing three different tasks: \nHammering, reaching, and hooking -- mainly because all three are completable with the same object under different modes of usage. \n\nHooking a thermos with tools from the holdout set. https://t.co/Ryrbkw9tTH', 'Constrained reaching with tools from the holdout set. To manipulate an object on the other side of a wall, the tool must fit through the hole and reach the target object. https://t.co/dseI9YvfzL', 'This question has been something we have been studying for a while now, including our previous RSS and IJRR works on Learning Task-Oriented Grasping for Tool Manipulation from Simulated Self-Supervision\n\nhttps://t.co/VELN7SOiW9 https://t.co/0aWZaDj5DD']",https://arxiv.org/abs/2106.14973,"Tool use requires reasoning about the fit between an object's affordances and the demands of a task. Visual affordance learning can benefit from goal-directed interaction experience, but current techniques rely on human labels or expert demonstrations to generate this data. In this paper, we describe a method that grounds affordances in physical interactions instead, thus removing the need for human labels or expert policies. We use an efficient sampling-based method to generate successful trajectories that provide contact data, which are then used to reveal affordance representations. Our framework, GIFT, operates in two phases: first, we discover visual affordances from goal-directed interaction with a set of procedurally generated tools; second, we train a model to predict new instances of the discovered affordances on novel tools in a self-supervised fashion. In our experiments, we show that GIFT can leverage a sparse keypoint representation to predict grasp and interaction points to accommodate multiple tasks, such as hooking, reaching, and hammering. GIFT outperforms baselines on all tasks and matches a human oracle on two of three tasks using novel tools. ","GIFT: Generalizable Interaction-aware Functional Tool Affordances
  without Labels"
6,1411070045271347202,247769594,Mahmoud Afifi,['We present a practical raw-2-raw mapping that requires *a small set* of paired images (+ unpaired set) for training. A new dataset of unpaired/paired images taken by two smartphones is available here: <LINK>\nPaper: <LINK>\nWork done with @aatao92 <LINK>'],https://arxiv.org/abs/2106.13883,"The raw-RGB colors of a camera sensor vary due to the spectral sensitivity differences across different sensor makes and models. This paper focuses on the task of mapping between different sensor raw-RGB color spaces. Prior work addressed this problem using a pairwise calibration to achieve accurate color mapping. Although being accurate, this approach is less practical as it requires: (1) capturing pair of images by both camera devices with a color calibration object placed in each new scene; (2) accurate image alignment or manual annotation of the color calibration object. This paper aims to tackle color mapping in the raw space through a more practical setup. Specifically, we present a semi-supervised raw-to-raw mapping method trained on a small set of paired images alongside an unpaired set of images captured by each camera device. Through extensive experiments, we show that our method achieves better results compared to other domain adaptation alternatives in addition to the single-calibration solution. We have generated a new dataset of raw images from two different smartphone cameras as part of this effort. Our dataset includes unpaired and paired sets for our semi-supervised training and evaluation. ",Semi-Supervised Raw-to-Raw Mapping
7,1410682915676213259,69020466,George King,"['üéâ üéâ New paper on arXiv today! üéâ üéâ\n\n""The near-UV transit of HD 189733b with the XMM-Newton Optical Monitor""\n\n<LINK>\n\nA thread! üßµ\n1/', 'Some background:\n\nHD 189733b is one of the best studied ""hot Jupiter"" planets (Jupiter-sized planets that only take a few days to orbit their star), and the closest one to Earth that transits its star (https://t.co/UmELoGXhLl has more about the transit method).\n\n2/', 'One of the best ways we have of investigating the atmospheres of exoplanets is to look at how the size of the transit dip changes with the wavelength you observe at. This is what we call transmission spectroscopy (learn more about it here: https://t.co/NA4L0qQhlD)\n\n3/', 'In visible light, HD 189733b has a strong increase in the transit depth towards bluer wavelengths, likely the result of aerosol particles high in the atmosphere. In the infrared, there is evidence of water vapour and carbon monoxide in its atmosphere.\n\n4/', 'In the far ultraviolet, the transit of HD 189733b is up to seven times deeper than in the visible in Lyman alpha - an important wavelength associated with hydrogen - and demonstrates some material is escaping its atmosphere.\n\n5/', ""HD 189733b's transit in the near ultraviolet has not previously been investigated, and this wavelength has only been used for planet transit observations in comparatively few systems so far.\n\n6/"", 'We observed 20 transits with XMM-Newton in the near ultraviolet.\n\n""But isn\'t XMM is an X-ray telescope?"" I hear you say...\n\nXMM also has a 30 cm telescope called the Optical Monitor, which can observe visible or near ultraviolet light simultaneously with its X-ray telescopes.\n\n7/', ""18 of our obs used the UVW2 filter (centred at 212 nm), 1 used UVM2 (231 nm), and 1 used UVW1 (291 nm, but the source ended up too bright for the camera). Here's our raw combined light curve for the 18 UVW2 transits. The shaded region is transit duration in visible light.\n\n8/ https://t.co/SQcSmsWA0M"", ""We fitted models to light curves to investigate the properties of our transits and the system in the near-UV. We removed out of transit trends in the data by also fitting a quadratic function to each individual observation's light curve along with this transit model.\n\n9/"", 'First we let the transit depth vary between obs, but found no significant variation in this depth across the 18 UVW2 transits. Here we plot how the ratio of planet and star size changes across the 18 obs. The solid line is the value measured in visible light (at 495 nm)\n\n10/ https://t.co/cbrP65xhaO', 'Given this result, we did another fit where we forced the transit depth to be the same across all 18 UVW2 filter observations. After removing the fitted out of transit trends, this is what the light curve looks like with the best fit model over the top in orange.\n\n11/ https://t.co/2ophoV6Ycp', 'We did a similar fitting process for the single UVM2 filter transit. Here is our final light curve in that filter. As it is a single observation it is a bit noisier than the previous light curve, but it still shows a clear transit detection.\n\n12/ https://t.co/4Cjjv6pLta', ""Plotting our two transit measurements together with measurements from visible light (reproduced from David Sing's 2016 paper), we see our transit depths are consistent.\n\n13/ https://t.co/tC1jIkEXSs"", ""The UVW2 point perhaps hints the strong gradient slope in visible light continues into the near-UV, but really we would need to reduce the size of our uncertainties to be say either way, and we can't rule out the slope in visible light flattening off instead.\n\n14/"", 'However, what we can be sure of is that in these observations using broadband (relatively wide wavelength range) filters, there is no evidence of a signature of escaping material in the near-ultraviolet. \n\n15/', 'BUT this could maybe be detected at specific wavelengths associated with iron or magnesium, if one observed near-UV transits with higher resolution spectra, as has been done for a couple of other planets.\n\n16/', ""There's much more detail in our paper! (linked again below). Check it out if you want to know more... feel free to ping me questions too!\n\nhttps://t.co/zznHR8lekJ\n\n17/17"", 'Thank you for your attention... we will now resume the regularly scheduled programming of politics and cricket :)\n\n18/17']",https://arxiv.org/abs/2106.16208,"We present analysis of XMM-Newton Optical Monitor observations in the near-ultraviolet of HD 189733, covering twenty primary transits of its hot Jupiter planet. The transit is clearly detected with both the UVW2 and UVM2 filters, and our fits to the data reveal transit depths in agreement with that observed optically. The measured depths correspond to radii of $1.059^{+0.046}_{-0.050}$ and $0.94^{+0.15}_{-0.17}$ times the optically-measured radius (1.187 R$_{\rm J}$ at 4950 \r{A}) in the UVW2 and UVM2 bandpasses, respectively. We also find no statistically significant variation in the transit depth across the 8 year baseline of the observations. We rule out extended broadband absorption towards or beyond the Roche lobe at the wavelengths investigated, although observations with higher spectral resolution are required to determine if absorption out to those distances from the planet is present in individual near-UV lines. ",The near-UV transit of HD 189733b with the XMM-Newton Optical Monitor
8,1410574795096399886,1249649498382532610,Lucile Ter-Minassian,['Check out our new paper on Explainable AI (@SGhalebikesabi @karlado @cholmesuk) where we introduce a new approach to SHAP which improves the local interpretability and the robustness to the construction of adversarial classifiers <LINK> #Stats #MachineLearning <LINK>'],https://arxiv.org/abs/2106.14648,"Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distribution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we find that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also increase on-manifold explainability and robustness to the construction of adversarial classifiers. ",On Locality of Local Explanation Models
9,1410404498238066689,64958481,Birhanu Eshete,"[""üì¢New üì∞ üì¢\nGiven f(x)=y, you obtain  x'= x+Œ¥ (Œ¥: feature-space perturbation). As a result, f(x')=y'‚â†y (f is fooled!).\nQ: is there correlation b/n Œ¥ and feature-based explanation of f(x') = y'?\nA: check our new SecureComm'21 paper üëáby @AbderrahmenAmi2\n<LINK>""]",https://arxiv.org/abs/2106.15820,"Machine Learning (ML) models are susceptible to evasion attacks. Evasion accuracy is typically assessed using aggregate evasion rate, and it is an open question whether aggregate evasion rate enables feature-level diagnosis on the effect of adversarial perturbations on evasive predictions. In this paper, we introduce a novel framework that harnesses explainable ML methods to guide high-fidelity assessment of ML evasion attacks. Our framework enables explanation-guided correlation analysis between pre-evasion perturbations and post-evasion explanations. Towards systematic assessment of ML evasion attacks, we propose and evaluate a novel suite of model-agnostic metrics for sample-level and dataset-level correlation analysis. Using malware and image classifiers, we conduct comprehensive evaluations across diverse model architectures and complementary feature representations. Our explanation-guided correlation analysis reveals correlation gaps between adversarial samples and the corresponding perturbations performed on them. Using a case study on explanation-guided evasion, we show the broader usage of our methodology for assessing robustness of ML models. ",Explanation-Guided Diagnosis of Machine Learning Evasion Attacks
10,1410389142442332162,1186047738607263744,Rogemar A Riffel,"['New paper: Powerful multiphase outflows in the central region of Cygnus A, <LINK>']",https://arxiv.org/abs/2106.15279,"We use Gemini Near-Infrared Integral Field Spectrograph (NIFS) observations of the inner 3.5$\times$3.5 kpc$^2$ of the radio galaxy Cygnus A to map the gas excitation and kinematics at a spatial resolution of 200 pc. The emission of the ionised gas shows a biconical morphology, with half-opening angle of 45$^\circ$ and oriented along the position angle of the radio jet. Coronal line emission is seen within the cone, up to 1.75 kpc from the nucleus, with higher ionisation gas observed in the easterly side. The H$_2$ and [Fe II] emission lines are consistent with excitation by the central AGN, with some contribution of shocks to the southwest of the nucleus. The gas visual extinction and electron density are larger than those from optical-based measurements, consistent with the fact that near-IR observations penetrate deeply into the gas emission structure, probing denser and more obscured regions. The gas kinematics shows two components: (i) a rotating disc with kinematic position angle of $\Psi_0=21^\circ\pm2^\circ$, seen both in ionised and molecular gas, and (ii) outflows with velocities of up to 600 kms$^{-1}$ observed within the ionisation cone in ionised gas and restricted to inner 0.5 arcsec in molecular gas. The mass outflow rate in ionised gas is in the range $\sim100-280 {\rm M_\odot yr^{-1}}$ and the kinetic power of the outflow corresponds to 0.3-3.3 per cent of the AGN bolometric luminosity, indicating that the outflows in Cygnus A may be effective in suppressing star formation. ",Powerful multiphase outflows in the central region of Cygnus A
11,1410327465164111872,1273068367092445190,Alex Bergman,"['Check out our new paper ""Fast Training of Neural Lumigraph Representations using Meta-learning"":\n<LINK>\n\nWe propose MetaNLR++, which is able to train and render neural scene representations in a fraction of the time that competing methods require! <LINK>', 'This is joint work with my awesome collaborators from the Stanford Computational Imaging Group - Petr Kellnhofer and Gordon Wetzstein (@GordonWetzstein)']",https://arxiv.org/abs/2106.14942,"Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require. ",Fast Training of Neural Lumigraph Representations using Meta Learning
12,1410293304152018949,123421220,Yvette Cendes,"['It‚Äôs NEW PAPER DAY!!! ü§©\n\nLet me tell you all about ‚ÄúA Pilot Radio Search for Magnetic Activity in Directly Imaged Exoplanets‚Äù! In which we used the VLA to look for radio emission from known planets we could separate from their parent star‚Ä¶ üßµ\n\n<LINK> <LINK>', 'To begin, radio from exoplanets. I should note this is NOT ALIEN RELATED/ a SETI search (though I guess if they called loud enough, we would have heard it). Instead, this is about natural radio emission, from magnetic fields in the planet https://t.co/Cyx6ec0RNm', ""We know exoplanet radio emission happens in two ways! Jupiter is VERY radio bright in MHz, as are all other bodies in our solar system w magnetic fields to some degree. Here's a picture of what Jupiter looks like in radio, credit @TheNRAO https://t.co/5IIzVKb5lY"", 'When looking for exoplanet emission, most ppl are searching for solar system analogues, in the MHz regime. Most recently, @Astro_journey and @AstroJoeC and others have been involved with some great work on looking for exoplanets w LOFAR...', ""However, the problem down in MHz is LOFAR doesn't have sufficient resolution to distinguish between the planet and star, so you can't 100% say which emission comes from wout more follow up. Plus it also ignores another potential form of exoplanet emission..."", ""Which is, emission we see from ultra-cool brown dwarfs (UCDs) in GHz! These are ~13-100x the mass of Jupiter, but can have the same effective temperature as exoplanets at the lowest masses. And ~7-10% of them flare- here's one in Berger et al. (2001) https://t.co/3pbDvz4cnJ"", ""Also wild btw: we don't actually get HOW the radio emission is seen from UCDs in GHz wavelengths. We just know we see it, and it's a really phenomenologically driven sub-field in radio astronomy..."", 'But then it begs the question: what if exoplanets are ""failed brown dwarfs"" and also display this emission?  Can we find out?  Enter: my paper!', ""Now, there's several ways to detect exoplanets, but one super impressive one is direct imaging, where you can literally SEE the planet's position vs the star's. These tend to also have high separation for it to work. Here's an example of one in optical, 51 Eri b (from Wikipedia) https://t.co/3udCjSRDog"", ""The VLA's also just amazing and in A configuration has enough resolution to do sub-arcsec resolution! So we did a search using @NASAExoArchive for all directly imaged exoplanets &lt;50 pc from Earth, visible from the VLA in New Mexico. That left us w 5 systems, 8 planets..."", ""To begin: I am sorry to report that we did NOT detect exoplanet emission. üò¢ But check out Figure 1 from our paper, w two systems where we saw emission from the star, but could establish there was no emission from the exoplanet. So, there's def something to this method! https://t.co/PCZnPbv7PJ"", ""An even cooler example! 51 Eri (imaged before) was a dud, but there WAS a candidate possibly associated w the 51 Eri system. What do? Well 51 Eri is close enough it'd move slightly over 3 years of observations if the candidate was associated. So, that's what we did! (Fig 2) https://t.co/MDjqfXmTul"", ""As you can see above, w high enough SNR we can establish the source should have moved ~227 milliarcsec in this time, per Gaia data, if part of 51 Eri. It did not, to &lt;20 mas confidence, so we concluded it's a background AGN source and not a radio exoplanet."", ""(As another side note, no, you couldn't just look to see if there's a galaxy there in optical bc it's a 5th magnitude star. BUT, as you can see, radio quiet, so this technique worked! Also, my first time doing astrometry, and man it's fun to SEE STUFF MOVE. üòÅ )"", ""But anyway, no detections, is that useful? YES! Here's the luminosity of UCDs/exoplanets by spectral type in GHz to date, w detections (yellow), upper limits (blue), and our work (red). As you can see, we rule a lot out, and reach the best limits for exoplanets so far in GHz! https://t.co/HgmBdl8DFi"", '(That plot is a bit complicated, but in short the lines are to connect quiescent to flaring emission, and we shifted things a bit in a spectral type bin to make things more clear to see.)', ""Here's another plot! Luminosity of exoplanets (all upper limits) and detected UCDs (blue), all in GHz, by mass.  As you can see, we are really starting to overlap with detected emission from UCDs! https://t.co/fvHN5FRRz4"", ""So, conclusion: this technique definitely works, and there's a HUGE phase space untouched in GHz for exoplanets to establish whether they emit there! But not impossible numbers- we calculated if you look at 22 systems you'd have a 90% of achieving a detection. ü§Øü™ê"", ""(Or, conclude that it doesn't exist to any huge level. But obviously, the former is much more fun to think about.)"", ""So hey, there is a lot more work to do! ü§© And can I say, how much *fun* this paper has been. This is my *first* paper on exoplanets, or heck anything in the galaxy PERIOD, but radio exoplanet work is something I've always wanted to do, and it's been delightful."", 'Also, shout out to @pkgw who was 2nd author extraordinaire on this and got the ball rolling on this project in the first place. Thank you! https://t.co/fOgh34WZ0p', ""I think that's all I've got! It's been a really fun project to work on, and I really like being at the point in my career where I can apply techniques I know to completely new problems in science. :) If you have any more questions I'll be happy to answer! /fin""]",https://arxiv.org/abs/2106.14994,"We present the first systematic search for GHz frequency radio emission from directly imaged exoplanets using Very Large Array (VLA) observations of sufficient angular resolution to separate the planets from their host stars. We obtained results for five systems and eight exoplanets located at $\lesssim 50$ pc, through new observations (Ross 458, GU Psc, and 51 Eri) and archival data (GJ 504 and HR 8799). We do not detect radio emission from any of the exoplanets, with $3\sigma$ luminosity upper limits of $(0.9-23)\times10^{21}$ erg s$^{-1}$. These limits are comparable to the level of radio emission detected in several ultracool dwarfs, including T dwarfs, whose masses are only a factor of two times higher than those of the directly-imaged exoplanets. Despite the lack of detections in this pilot study, we highlight the need for continued GHz frequency radio observations of nearby exoplanets at $\mu$Jy-level sensitivity. ",A Pilot Radio Search for Magnetic Activity in Directly Imaged Exoplanets
13,1410271085258366982,1272251447434817537,Willie Agnew,"['üéâNew paper!üìú We annotated the values uplifted in ML papers to understand what the field as a whole is trying to acheive. We then analyzed the societal impacts of these top values, enabling understanding the impacts of even highly technical research. <LINK>', 'We found that top ML papers overwhelmingly favor scale values, such as performance and generalization. Even efficiency is used to increase scale (more training data or parameters) rather than enable more context. Justice, user influence, and autonomy appear rarely if at all. https://t.co/d8tpZxpWky', 'We argue for how a ML field that centers these values favors certain institutions--those with lots of data, compute, and knowledge that can take advantage of scaling. We analyze author ties and find big tech has increased 5x over the last decade to 60% of top ML papers. https://t.co/GD96Fk0L4d', ""We annotate any mentions of potential negative impacts, and find only 2% of papers include this. We also examine how papers connect research to societal needs, and find most (71%) don't mention any societal need, and very few (3%) seriously attempt to connect research to need."", 'What does it say about the actual priorities of ML research if rigorous connections to societal needs and discussions of potential impacts are so rare? üò¨We should treat these discussions with the same rigor we do other related works. Cite the scholarship in these areas!', 'Scale values have favored big institutions and corporations. What would an ML that centers other values look like and what futures would it make likely? Could we have an ML that centers justice, user influence, or user interpretability over performance and generalization?', 'In my future technical work I will search for ways I can include and uplift these values. One of the best parts of this project has been working with and learning from my amazing collaborators @Abebab @radical_ai_ @dallascard @DotanRavit @MichelleBao27 üòÄ']",https://arxiv.org/abs/2106.15590,"Machine learning (ML) currently exerts an outsized influence on the world, increasingly affecting communities and institutional practices. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we present a rigorous examination of the values of the field by quantitatively and qualitatively analyzing 100 highly cited ML papers published at premier ML conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: how they justify their choice of project, which aspects they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that societal needs are typically very loosely connected to the choice of project, if mentioned at all, and that consideration of negative consequences is extremely rare. We identify 67 values that are uplifted in machine learning research, and, of these, we find that papers most frequently justify and assess themselves based on performance, generalization, efficiency, researcher understanding, novelty, and building on previous work. We present extensive textual evidence and analysis of how these values are operationalized. Notably, we find that each of these top values is currently being defined and applied with assumptions and implications generally supporting the centralization of power. Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities. ",The Values Encoded in Machine Learning Research
14,1410242652017152000,613453368,Shaojie Bai,"['üßêWe summarize numerous challenges for the equilibrium models, and suggest a way to regularize their stabilities and accelerate by 2x. New ICML paper w/ V. Koltun and @zicokolter: \n<LINK>\n\nAlso releasing (more user-friendly) DEQ repo v2.0@ <LINK> <LINK>']",https://arxiv.org/abs/2106.14342,"Deep equilibrium networks (DEQs) are a new class of models that eschews traditional depth in favor of finding the fixed point of a single nonlinear layer. These models have been shown to achieve performance competitive with the state-of-the-art deep networks while using significantly less memory. Yet they are also slower, brittle to architectural choices, and introduce potential instability to the model. In this paper, we propose a regularization scheme for DEQ models that explicitly regularizes the Jacobian of the fixed-point update equations to stabilize the learning of equilibrium models. We show that this regularization adds only minimal computational cost, significantly stabilizes the fixed-point convergence in both forward and backward passes, and scales well to high-dimensional, realistic domains (e.g., WikiText-103 language modeling and ImageNet classification). Using this method, we demonstrate, for the first time, an implicit-depth model that runs with approximately the same speed and level of performance as popular conventional deep networks such as ResNet-101, while still maintaining the constant memory footprint and architectural simplicity of DEQs. Code is available at this https URL . ",Stabilizing Equilibrium Models by Jacobian Regularization
15,1410121015527673857,204261944,Matthew Kenworthy,"['New paper on results from the gvAPP coronagraph! @BenSutlieff  with @ajbohn, @jaynebirkby et al. with ""High-contrast observations of brown dwarf companion HR 2562 B with the gvAPP"" <LINK> where Ben tests Flipped Differential Imaging with the two PSFs  /1 <LINK>', '@BenSutlieff @ajbohn @jaynebirkby The grating vector Apodizing Phase Plate coronagraph is a single optic that you put into an astronomical camera that splits the image of a star into three images - two images have the diffraction of the primary star suppressed over 180 degree wide complementary regions... /2', '@BenSutlieff @ajbohn @jaynebirkby ...and although the coronagraph removes most of the diffraction from the central star, the camera optics still add a few aberrations in, so Ben tried rotating one PSF by 180 degrees and subtracting it from the other PSF - Flipped Differential Imaging. /3', ""@BenSutlieff @ajbohn @jaynebirkby It doesn't work as well as we thought it would, as there is scattered light in the camera and detector pattern noise - but so it goes! Ben did great work in pulling out the BD companion and showing that longer wavelengths are important for understanding these cool companions. https://t.co/sowSSpqBTs""]",https://arxiv.org/abs/2106.14890,"The vector Apodizing Phase Plate (vAPP) is a class of pupil plane coronagraph that enables high-contrast imaging by modifying the Point Spread Function (PSF) to create a dark hole of deep flux suppression adjacent to the PSF core. Here, we recover the known brown dwarf HR 2562 B using a vAPP coronagraph, in conjunction with the Magellan Adaptive Optics (MagAO) system, at a signal-to-noise of S/N = 3.04 in the lesser studied L-band regime. The data contained a mix of field and pupil-stabilised observations, hence we explored three different processing techniques to extract the companion, including Flipped Differential Imaging (FDI), a newly devised Principal Component Analysis (PCA)-based method for vAPP data. Despite the partial field-stabilisation, the companion is recovered sufficiently to measure a 3.94 $\mu$m narrow-band contrast of (3.05$\pm$1.00) $\times$ 10$^{-4}$ ($\Delta$m$_{3.94 {\mu}m}$ = 8.79$\pm$0.36 mag). Combined with archival GPI and SPHERE observations, our atmospheric modelling indicates a spectral type at the L/T transition with mass M = 29$\pm$15 M$_{\text{Jup}}$, consistent with literature results. However, effective temperature and surface gravity vary significantly depending on the wavebands considered (1200$\leq$T$_{\text{eff}}$(K)$\leq$1700 and 4.0$\leq$log(g)(dex)$\leq$5.0), reflecting the challenges of modelling objects at the L/T transition. Observations between 2.4-3.2 $\mu$m will be more effective in distinguishing cooler brown dwarfs due to the onset of absorption bands in this region. We explain that instrumental scattered light and wind-driven halo can be detrimental to FDI+PCA and thus must be sufficiently mitigated to use this processing technique. We thus demonstrate the potential of vAPP coronagraphs in the characterisation of high-contrast substellar companions, even in sub-optimal conditions, and provide new, complementary photometry of HR 2562 B. ","High-contrast observations of brown dwarf companion HR 2562 B with the
  vector Apodizing Phase Plate coronagraph"
16,1410104060393529347,1047471489782599681,Xiaopeng Lu,"['Excited to share our new survey paper on Embodied Vision Language Planning! In this survey, we review different Embodied Vision-Language tasks and summarize the core challenges behind these tasks. \n\nArxiv: <LINK>']",https://arxiv.org/abs/2106.13948,"Recent advances in the areas of multimodal machine learning and artificial intelligence (AI) have led to the development of challenging tasks at the intersection of Computer Vision, Natural Language Processing, and Embodied AI. Whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. Moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. In this survey paper, we discuss Embodied Vision-Language Planning (EVLP) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. We propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for EVLP tasks. Finally, we present the core challenges that we believe new EVLP works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment. ",Core Challenges in Embodied Vision-Language Planning
17,1410050371502972930,1357124221009334272,David Jaz Myers,"['I have a new paper, all about the relation between quantity and quality: Modal Fracture of Higher Groups. <LINK> ‚Ä¶ @SchreiberUrs‚Äô differential cohomology hexagon  done synthetically in Shulman‚Äôs cohesive HoTT, and classifiers for circle k-gerbes with connection.', 'https://t.co/f8SubGxVDL']",https://arxiv.org/abs/2106.15390,"In this paper, we examine the modal aspects of higher groups in Shulman's Cohesive Homotopy Type Theory. We show that every higher group sits within a modal fracture hexagon which renders it into its discrete, infinitesimal, and contractible components. This gives an unstable and synthetic construction of Schreiber's differential cohomology hexagon. As an example of this modal fracture hexagon, we recover the character diagram characterizing ordinary differential cohomology by its relation to its underlying integral cohomology and differential form data, although there is a subtle obstruction to generalizing the usual hexagon to higher types. Assuming the existence of a long exact sequence of differential form classifiers, we construct the classifiers for circle k-gerbes with connection and describe their modal fracture hexagon. ",Modal Fracture of Higher Groups
18,1410039972422328322,1250475209959698432,Dara Bahri,"['Excited to share our new paper from @GoogleAI. ""SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption"" (<LINK>). This is joint work with Heinrich Jiang, @ytay017 and @metzlerd. <LINK>', 'TLDR  -- a neat contrastive pre-training scheme for deep nets on tabular data that improves classification performance generally, when labeled data is limited, and when there are noisy labels.']",https://arxiv.org/abs/2106.15147,"Self-supervised contrastive representation learning has proved incredibly successful in the vision and natural language domains, enabling state-of-the-art performance with orders of magnitude less labeled data. However, such methods are domain-specific and little has been done to leverage this technique on real-world tabular datasets. We propose SCARF, a simple, widely-applicable technique for contrastive learning, where views are formed by corrupting a random subset of features. When applied to pre-train deep neural networks on the 69 real-world, tabular classification datasets from the OpenML-CC18 benchmark, SCARF not only improves classification accuracy in the fully-supervised setting but does so also in the presence of label noise and in the semi-supervised setting where only a fraction of the available training data is labeled. We show that SCARF complements existing strategies and outperforms alternatives like autoencoders. We conduct comprehensive ablations, detailing the importance of a range of factors. ","SCARF: Self-Supervised Contrastive Learning using Random Feature
  Corruption"
19,1409852391395110915,1034887017421787138,Jacob Menick,"['Our new paper shows how to prompt a pre-trained text language model with a combination of text AND images (üñºÔ∏è,üî§, üñºÔ∏è,üî§, üñºÔ∏è,üî§).\n\nKeep the language model üßä frozen üßä and train a vision encoder to embed images into the same space as word sequences.\n\n<LINK>\n(1/12) <LINK>', 'Others‚Äô work showed that text-pretrained LMs are useful priors not just for predicting language, but also tasks like classifying MNIST or performing logical ops on bitstrings. Requires only minimal finetuning (just embeddings and layernorm params).\n\nhttps://t.co/5cWYVwdr0i\n(2/12)', 'We train a convnet to make a frozen 7b-param LM generate the right caption for an image from Conceptual Captions.\n\nThis is like a *image-conditional* version of prefix-tuning, whereas past work has trained a bias to act like the continuous embedding of a *static* prompt.\n\n(3/12) https://t.co/l5TWDGpaSO', 'The LM can generalize to accepting interleaved images and text in a prompt. This enables *multimodal* #gpt3 style ‚Äúin-context‚Äù few-shot learning. My favourite part about our system is you can access the factual ‚Äúworld knowledge‚Äù in the LM with visual input (üëá).\n\n(4/12)', 'Conceptual Captions has named entities ‚Äúhypernymed‚Äù away, so our model has not been trained to associate named entities with images of them. Here few-shot learning comes in handy. We can remind the LM it knows about named entities with few-shot prompting. Pretty cool!\n\n(5/12) https://t.co/mmMiv2GNHV', 'Last year I thought alot about how well LMs can command the factual knowledge they learn during pre-training. Was v impressed by the work of @ada_rob and @colinraffel.\n\nThis ‚Äú‚úàÔ∏è was invented by the Wright Brothers‚Äù sample  ‚òùÔ∏è is even more evidence (more in our paper).\n\n(6/12)', 'We bolster these examples with quantitative evidence of multimodal few-shot learning with established benchmarks and targeted probe tasks.\n\nThe numbers are far from SOTA, but in all these tasks we do not use the training sets, only transfer from captioning.\n\n(7/12)', 'Keeping the LM üßä frozen üßä actually generalized better across the board in our experiments (e.g. captioning-&gt;VQA) than finetuning it. Of course captioning loss on the Conceptual Captions validation dataset was better with full finetuning of the LM though.\n\n(8/12)', 'This is joint work by @mtsimpoukelli, myself, @serkancabi, @arkitus, @OriolVinyalsML, and @FelixHill84, a true team effort!\n\nFuture work will do multimodal few-shot learning better + bigger, but the fact that our approach works at all says alot for big LMs re ‚Äúgrounding‚Äù.\n\n(9/12)', 'Thanks also to @borgeaud_s, @trevorycai, @jack_w_rae for developing the language models we studied in this project.\n\n(10/12)', 'Finally, I want to shout out a few related papers which we missed in our literature review. [2, 3] also train vision encoders to conform to the language space of pretrained language models.\n \n[2] https://t.co/6bvd6M0nDo\n[3] https://t.co/LZDjPQZ5K6\n\n(11/12)', 'We applied this idea to a pretrained autoregressive language model, and show that the result is an open-ended, generative multimodal few-shot learning system. \n\nLet us know if we missed anything else!\n\n(12/12)']",https://arxiv.org/abs/2106.13884,"When trained at sufficient scale, auto-regressive language models exhibit the notable ability to learn a new language task after being prompted with just a few examples. Here, we present a simple, yet effective, approach for transferring this few-shot learning ability to a multimodal setting (vision and language). Using aligned image and caption data, we train a vision encoder to represent each image as a sequence of continuous embeddings, such that a pre-trained, frozen language model prompted with this prefix generates the appropriate caption. The resulting system is a multimodal few-shot learner, with the surprising ability to learn a variety of new tasks when conditioned on examples, represented as a sequence of multiple interleaved image and text embeddings. We demonstrate that it can rapidly learn words for new objects and novel visual categories, do visual question-answering with only a handful of examples, and make use of outside knowledge, by measuring a single model on a variety of established and new benchmarks. ",Multimodal Few-Shot Learning with Frozen Language Models
20,1409698766966333445,702236063431942144,"Mercedes Gimeno-Segovia, PhD",['New paper on arXiv!üìù Long time in the making üòÖ so very happy to finally share some newly minted schemes on entanglement generation for linear optical quantum computing. Hope you find the results as interesting and shocking as we did ü§ì<LINK>'],https://arxiv.org/abs/2106.13825,"Using only linear optical elements, the creation of dual-rail photonic entangled states is inherently probabilistic. Known entanglement generation schemes have low success probabilities, requiring large-scale multiplexing to achieve near-deterministic operation of quantum information processing protocols. In this paper, we introduce multiple techniques and methods to generate photonic entangled states with high probability, which have the potential to reduce the footprint of Linear Optical Quantum Computing (LOQC) architectures drastically. Most notably, we are showing how to improve Bell state preparation from four single photons to up to p=2/3, boost Type-I fusion to 75% with a dual-rail Bell state ancilla and improve Type-II fusion beyond the limits of Bell state discrimination. ",Creation of Entangled Photonic States Using Linear Optics
21,1409579035953451009,1125396397275136000,Suhas Thejaswi,"['Checkout our new paper ""Diversity-aware k-median : Clustering with fair center representation"" accepted for publication in ECML-PKDD 2021 (<LINK>). Joint work with @gionis and Bruno Ordozgoiti . #DiversityandInclusion #algorithmicbias #databias', 'In this work we show that performing a trivial task such as clustering the data can become very difficult with the introduction of the diversity constraints.']",https://arxiv.org/abs/2106.11696,"We introduce a novel problem for diversity-aware clustering. We assume that the potential cluster centers belong to a set of groups defined by protected attributes, such as ethnicity, gender, etc. We then ask to find a minimum-cost clustering of the data into $k$ clusters so that a specified minimum number of cluster centers are chosen from each group. We thus require that all groups are represented in the clustering solution as cluster centers, according to specified requirements. More precisely, we are given a set of clients $C$, a set of facilities $\pazocal{F}$, a collection $\mathcal{F}=\{F_1,\dots,F_t\}$ of facility groups $F_i \subseteq \pazocal{F}$, budget $k$, and a set of lower-bound thresholds $R=\{r_1,\dots,r_t\}$, one for each group in $\mathcal{F}$. The \emph{diversity-aware $k$-median problem} asks to find a set $S$ of $k$ facilities in $\pazocal{F}$ such that $|S \cap F_i| \geq r_i$, that is, at least $r_i$ centers in $S$ are from group $F_i$, and the $k$-median cost $\sum_{c \in C} \min_{s \in S} d(c,s)$ is minimized. We show that in the general case where the facility groups may overlap, the diversity-aware $k$-median problem is \np-hard, fixed-parameter intractable, and inapproximable to any multiplicative factor. On the other hand, when the facility groups are disjoint, approximation algorithms can be obtained by reduction to the \emph{matroid median} and \emph{red-blue median} problems. Experimentally, we evaluate our approximation methods for the tractable cases, and present a relaxation-based heuristic for the theoretically intractable case, which can provide high-quality and efficient solutions for real-world datasets. ",Diversity-aware $k$-median : Clustering with fair center representation
22,1409404108776390658,70445227,Ike Kunze,"[""Our new paper on four spin-bit cousins that focus on loss detection is out and will be presented at ANRW '21 (co-located with IETF 111 @inretafo/@ietf). Check out the paper (<LINK>) and try our experiments yourself (<LINK>). [1/6] <LINK>"", 'In a Mininet testbed, we study the loss detection capabilities of the L-, Q-, R- and T-Bits, which are currently proposed in the @ietf IPPM WG and, similar to the spin-bit, shape measurable signals onto a (transport) connection. (https://t.co/JwTHhKtQJI) [2/6]', 'Overall, all approaches provide reasonable loss estimates close to the ground truth when subject to random loss. However, longer algorithmic intervals of Q, R, T cause fluctuations, while the L follows the ground truth with a slight delay. [3/6] https://t.co/zRwGZ92y53', 'These intervals of Q, R, T also impact their accuracy in times of bursty loss as whole phases might be wiped out. The L-Bit is quite robust to this as it reports the loss detected by the sender-side loss detection on a packet-by-packet level.  [4/6] https://t.co/rXhqfVTAhQ', 'The previous results base on long-running experiments with roughly 1 Mio sent packets each. When looking at shorter connections, it can be seen that the approaches require certain amounts of sent data to produce readings. Results only stabilize for higher flow volumes. [5/6] https://t.co/o58R9CHXJn', 'From a pure measurement accuracy perspective, the L-Bit and combinations of Q&amp;L / Q&amp;R, look the most promising. However, the approaches allow for different path segmentation granularities. While this might impact real deployments, it is not the focus of our paper. [6/6] https://t.co/qSzAi4OIPO']",http://arxiv.org/abs/2106.13710,"Network operators utilize traffic monitoring to locate and fix faults or performance bottlenecks. This often relies on intrinsic protocol semantics, e.g., sequence numbers, that many protocols share implicitly through their packet headers. The arrival of (almost) fully encrypted transport protocols, such as QUIC, significantly complicates this monitoring as header data is no longer visible to passive observers. Recognizing this challenge, QUIC offers explicit measurement semantics by exposing the spin bit to measure a flow's RTT. Ongoing efforts in the IETF IPPM working group argue to expose further information and enable the passive quantification of packet loss. This work implements and evaluates four currently proposed measurement techniques (L-, Q-, R-, and T-bit). We find that all techniques generally provide accurate loss estimations, but that longer algorithmic intervals for Q and R, yet foremost for T, complicate detecting very small loss rates or loss on short connections. Deployment combinations of Q & R as well as Q & L, thus, have the best potential for accurately grasping the loss in networks. ","L, Q, R, and T -- Which Spin Bit Cousin Is Here to Stay?"
23,1409401397343309824,17138973,Prof. Heather Gray,"[""New week, new paper: <LINK>  . Our experiment-independent and framework-independent toolkit for charge particle pattern recognition. Feel free to get in touch with us if you're interested in trying it out. @SaltyBurger @paulgessinger""]",https://arxiv.org/abs/2106.13593,"The reconstruction of the trajectories of charged particles, or track reconstruction, is a key computational challenge for particle and nuclear physics experiments. While the tuning of track reconstruction algorithms can depend strongly on details of the detector geometry, the algorithms currently in use by experiments share many common features. At the same time, the intense environment of the High-Luminosity LHC accelerator and other future experiments is expected to put even greater computational stress on track reconstruction software, motivating the development of more performant algorithms. We present here A Common Tracking Software (ACTS) toolkit, which draws on the experience with track reconstruction algorithms in the ATLAS experiment and presents them in an experiment-independent and framework-independent toolkit. It provides a set of high-level track reconstruction tools which are agnostic to the details of the detection technologies and magnetic field configuration and tested for strict thread-safety to support multi-threaded event processing. We discuss the conceptual design and technical implementation of ACTS, selected applications and performance of ACTS, and the lessons learned. ",A Common Tracking Software Project
24,1409329392753414147,199409067,Alex Teachey ÈΩäÂ≠ùÂµê,"[""New #exomoon paper out today, and it's a couple of firsts for me: first as a postdoc, and my first single-author paper!\n \nIn this paper I examined observable, dynamical signatures of transiting exoplanets hosting multiple moons. First some background: 1/N <LINK>"", 'Recently @david_kipping identified a cool phenomenon he calls the ""exomoon corridor"", which has to do with transit timing variations (TTVs) induced by an exomoon. What are TTVs, you might ask?\n \nTTVs can be observational evidence that an exoplanet has one or more moons. 2/N', 'Just as a planet pulls on its moon gravitationally, the moon pulls on the planet. This results in an oscillating effect around their common center of mass.\n \nThis means that a planet transiting its host star will not be strictly periodic -- the transits can be early or late. 3/N https://t.co/pdNtfc6UE0', 'We can measure these TTVs to investigate whether a moon (or something else) is gravitationally perturbing the planet. Okay!\n \n@david_kipping found that when a planet hosts a moon, the TTVs we measure will generally be very short period... 4/N https://t.co/NaokjdjOBZ', '... and this appears to stand in contrast to systems for which the TTVs are likely due to planet-planet perturbations (see figure).\n \nIn my new paper, I investigated whether this phenomenon also occurs when planets have *multiple* moons. 5/N https://t.co/eZXc2YdPc9', ""The gory details are in the paper, but suffice it say, planets hosting multiple moons will have more complex dynamical signatures, and it's not straightforward to predict what these might look like until we simulate them. That's also complex! 6/N"", 'We want to make sure that the systems we simulate are plausibly found in nature (read: dynamically stable). So I generated 150,000 N-body simulations of planet-moon systems, to look at the resulting TTVs and test the exomoon corridor result in the more general case 7/N https://t.co/eIQfAylGge', ""This is important, because if we look at the Solar System, most of the large moons are found in multiple-moon systems! We've mostly ignored this in exomoon hunting (for good reasons!), but it's worth investigating. 8/N https://t.co/tAorXw8Z93"", ""Bottom line: I found the exomoon corridor result does in fact hold in the more general case, whether or not the moons are in resonance. That's great news! It means the exomoon corridor result is not restricted to a narrow range of systems to be found in nature. 9/N https://t.co/sbdvvOUhn7"", 'In the process of carrying out these simulations, I noticed something interesting: as we add more moons to a given system, the total satellite system masses on average have to be lower in order to maintain long term stability. 10/N https://t.co/AXl9FjQR4X', ""It's not to say that high-mass, multimoon systems cannot be found in nature, but if they are, they will require more (natural!) fine-tuning in their orbital configurations, and could be consequently more rare. \n\nMaybe not surprising! But important nevertheless 11/N"", 'This has a few implications. For starters it suggests -- independent of circumplanetary disk model (CPD) assumptions -- that total satellite mass ratios will generally be less than ~10^-4, or this ""fine-tuning"" must arise from CPD conditions. 12/N', 'And from an observational perspective, it means that if we are looking for dynamical signatures of exomoons, we may be more likely to find systems with fewer moons, and maybe, moons for which formation in a CPD was not the pathway. 13/N https://t.co/n1BeymPCrL', ""Lastly, I looked at a large catalog of TTVs in the literature, cut it up into several subsets, and examined whether we see the exomoon corridor show up with real data (second column). Turns out, we do.\n \nIt's premature to draw strong conclusions from this, but... 14/N https://t.co/10vGPQ6ujM"", ""...it's intriguing nevertheless. Maybe we are seeing the dynamical signatures of exomoons in the data. But it's complicated. We need to keep thinking through this problem. (Read the paper for deeper analysis!) 15/N"", ""That about wraps it up! I'm very proud of this work, dreamt up and executed entirely on my own, and my first project across the finish line since arriving at @epo_asiaa. I hope you've gotten something out of this thread, and maybe you'll consider checking out the paper! 16/16 https://t.co/4tNBvSuoXG"", '@girlandkat @epo_asiaa Thanks for reading! And all the likes ‚ò∫Ô∏è']",https://arxiv.org/abs/2106.13421,"Recently Kipping (2021) identified the so-called ""exomoon corridor"", a potentially powerful new tool for identifying possible exomoon hosts, enabled by the observation that fully half of all planets hosting an exomoon will exhibit transit timing variation (TTV) periodicities of 2-4 epochs. One key outstanding problem in the search for exomoons, however, is the question of how well the methods we have developed under the single moon assumption extend to systems with multiple moons. In this work we use $N$-body simulations to examine the exomoon corridor effect in the more general case of $N \geq 1$ moons, generating realistic TTVs produced by satellite systems more akin to those seen in the outer Solar System. We find that indeed the relationship does hold for systems with up to 5 moons in both resonant and non-resonant chain configurations. Our results suggest an observational bias against finding systems with large numbers of massive moons; as the number of moons increases, total satellite mass ratios are generally required to be significantly lower in order to maintain stability, or architectures must be more finely tuned to survive. Moons produced in impact or capture scenarios may therefore dominate early detections. Finally, we examine the distribution of TTV periods measured for a large number of Kepler Objects of Interest (KOIs) and find the same characteristic exomoon corridor distribution in several cases. This could be dynamical evidence for an abundance of moons in the field, though we caution against strong inferences based on this result. ",The Exomoon Corridor for Multiple Moon Systems
25,1408515381124206593,58536843,Alex Lang,"['The next era of autonomous vehicle datasets is here! \n\nExcited to be a part of nuPlan, a new dataset and challenge that will unlock ML based planning. \n\nCheck out our paper for more details, but below is the killer comparison table.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2106.11810,"In this work, we propose the world's first closed-loop ML-based planning benchmark for autonomous driving. While there is a growing body of ML-based motion planners, the lack of established datasets and metrics has limited the progress in this area. Existing benchmarks for autonomous vehicle motion prediction have focused on short-term motion forecasting, rather than long-term planning. This has led previous works to use open-loop evaluation with L2-based metrics, which are not suitable for fairly evaluating long-term planning. Our benchmark overcomes these limitations by introducing a large-scale driving dataset, lightweight closed-loop simulator, and motion-planning-specific metrics. We provide a high-quality dataset with 1500h of human driving data from 4 cities across the US and Asia with widely varying traffic patterns (Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop simulation framework with reactive agents and provide a large set of both general and scenario-specific planning metrics. We plan to release the dataset at NeurIPS 2021 and organize benchmark challenges starting in early 2022. ","NuPlan: A closed-loop ML-based planning benchmark for autonomous
  vehicles"
26,1408469060052946951,790033937531703296,Yi Tay,"['Excited to share our new work from @GoogleAI and @DeepMind. ""Charformer: Fast Character Transformers via Gradient-based Subword Tokenization (paper: <LINK>) <LINK>', 'We introduce a new inductive bias that learns latent subwords in an e2e fashion. Charformer is fast, often much faster than other byte-level baselines/subword models while achieving very competitive performance. No more building SPMs / re-pretraining for every new task/domain!', ""Charformer works well both on monolingual English standard benchmarks (GLUE) and also multilingual datasets. We also evaluate Charformer on Jigsaw's toxicity detection sets from social media."", 'Joint work with amazing collaborators. @vqctran (co-first author), @seb_ruder (DeepMind), @_jai_gupta @hwchung27 @dara_bahri @pierceqin @sens3 @congyu and @metzlerd.', 'Code link is already in the paper but release ETA should be in about 1 week. :)', ""@christopher Mesh Tensorflow, similar to the T5 library. We're also working on a JAX version which will be released sometime later.""]",https://arxiv.org/abs/2106.12672,"State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce Charformer, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that Charformer outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, Charformer is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28%-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end. ","Charformer: Fast Character Transformers via Gradient-based Subword
  Tokenization"
27,1408461856532930561,268203365,gerald pao,"['Our new paper is a 1st attempt to download brains from organisms into computers with using whole brain electrophysiology &amp; embedding activity into a network of manifolds to reproduce realistic behavioral as well as neuronal dynamics in generative mode.\n<LINK> <LINK>', '@_rdgao Thanks Richard. Scale free embedding!', '@S_Sigfusson Also larval Zebrafish and human visual pathways', '@snpc_404 real data has noise both from observation and process noise so my general approach is to keep it simple. Using the geodesic might be better but I generally try to do as little as possible to the data to prevent overfitting. Therefore I use the Sugihara minimalist approach.', '@snpc_404 I am not opposed to anything in principle as long as it works better. Observatoins are not smooth so for it to be you either need to interpolate or fit with ODEs which is something that requires you to know the underlying variables that might be missing.', '@snpc_404 So we use a combination of real time series of observed neurons and delays thereof as formulated in the generalized Takens theorem, where you can combine real observables with ""placeholder"" unknowns that show up as delays in the embedding.\nhttps://t.co/rMyP03Ax2J', ""@snpc_404 In summary Riemannian manifolds would be nice because they would be nice but I don't know if they would work better with real world data. But I am open to is if it works. Thank you for your comment and question""]",https://arxiv.org/abs/2106.10627,"We propose an algorithm grounded in dynamical systems theory that generalizes manifold learning from a global state representation, to a network of local interacting manifolds termed a Generative Manifold Network (GMN). Manifolds are discovered using the convergent cross mapping (CCM) causal inference algorithm which are then compressed into a reduced redundancy network. The representation is a network of manifolds embedded from observational data where each orthogonal axis of a local manifold is an embedding of a individually identifiable neuron or brain area that has exact correspondence in the real world. As such these can be experimentally manipulated to test hypotheses derived from theory and data analysis. Here we demonstrate that this representation preserves the essential features of the brain of flies,larval zebrafish and humans. In addition to accurate near-term prediction, the GMN model can be used to synthesize realistic time series of whole brain neuronal activity and locomotion viewed over the long term. Thus, as a final validation of how well GMN captures essential dynamic information, we show that the artificially generated time series can be used as a training set to predict out-of-sample observed fly locomotion, as well as brain activity in out of sample withheld data not used in model building. Remarkably, the artificially generated time series show realistic novel behaviors that do not exist in the training data, but that do exist in the out-of-sample observational data. This suggests that GMN captures inherently emergent properties of the network. We suggest our approach may be a generic recipe for mapping time series observations of any complex nonlinear network into a model that is able to generate naturalistic system behaviors that identifies variables that have real world correspondence and can be experimentally manipulated. ",Experimentally testable whole brain manifolds that recapitulate behavior
28,1408424179880370189,747280089251864576,Tom Hutchcroft,"['Excited to announce this new paper on the arXiv with my student Noah Halberstam. Quite a departure for me as it‚Äôs an experimental paper with no proofs. Click through for lots of cool fractal stuff! <LINK>', 'Some excerpted figures: https://t.co/YhracZGzbb']",https://arxiv.org/abs/2106.13218,"It is a central prediction of renormalisation group theory that the critical behaviours of many statistical mechanics models on Euclidean lattices depend only on the dimension and not on the specific choice of lattice. We investigate the extent to which this universality continues to hold beyond the Euclidean setting, taking as case studies Bernoulli bond percolation and lattice trees. We present strong numerical evidence that the critical exponents governing these models on transitive graphs of polynomial volume growth depend only on the volume-growth dimension of the graph and not on any other large-scale features of the geometry. For example, our results strongly suggest that percolation, which has upper-critical dimension six, has the same critical exponents on the four-dimensional hypercubic lattice $\mathbb{Z}^4$ and the Heisenberg group despite the distinct large-scale geometries of these two lattices preventing the relevant percolation models from sharing a common scaling limit. On the other hand, we also show that no such universality should be expected to hold on fractals, even if one allows the exponents to depend on a large number of standard fractal dimensions. Indeed, we give natural examples of two fractals which share Hausdorff, spectral, topological, and topological Hausdorff dimensions but exhibit distinct numerical values of the percolation Fisher exponent $\tau$. This gives strong evidence against a conjecture of Balankin et al. [Phys. Lett. A 2018]. ",What are the limits of universality?
29,1408415934235172869,1541749356,Matt Landreman,"['New paper led by Florian Wechsung: designing stellarator electromagnetic coils to have wide tolerances, for reduced cost. Method: stochastic optimization. Also with Andrew Giuliani, Antoine Cerfon, &amp; Georg Stadler. <LINK> @nyuniversity @UofMaryland @SimonsFdn <LINK>']",https://arxiv.org/abs/2106.12137,"We extend the single-stage stellarator coil design approach for quasi-symmetry on axis from [Giuliani et al, 2020] to additionally take into account coil manufacturing errors. By modeling coil errors independently from the coil discretization, we have the flexibility to consider realistic forms of coil errors. The corresponding stochastic optimization problems are formulated using a risk-neutral approach and risk-averse approaches. We present an efficient, gradient-based descent algorithm which relies on analytical derivatives to solve these problems. In a comprehensive numerical study, we compare the coil designs resulting from deterministic and risk-neutral stochastic optimization and find that the risk-neutral formulation results in more robust configurations and reduces the number of local minima of the optimization problem. We also compare deterministic and risk-neutral approaches in terms of quasi-symmetry on and away from the magnetic axis, and in terms of the confinement of particles released close to the axis. Finally, we show that for the optimization problems we consider, a risk-averse objective using the Conditional Value-at-Risk leads to results which are similar to the risk-neutral objective. ","Single-stage gradient-based stellarator coil design: stochastic
  optimization"
30,1408392083602296833,604007660,Nabil Iqbal,"['We have a new paper out today! I want to try and put it in broader context, so here is a thread about What Is A Higher Form Symmetry and How They Can Make Your Life Better....1/n\n\n<LINK>', 'Physicists like symmetries. Emmy Noether taught us that symmetries result in conserved quantities. For most ""ordinary"" symmetries, these conserved quantities are (basically) numbers of particles. 2/n\n\nhttps://t.co/5m4tIq1Fkh', ""(For example, if you have a bunch of atoms in a box, the mathematical description of the box has a certain *symmetry* that enforces the dynamical statement that the number of atoms in the box can't change, and that you can't lose an atom!) 3/n"", 'Now, this is fine if you only care about particles. But many interesting systems have *extended objects* -- e.g. strings -- which are also conserved. My favorite example is ordinary Maxwell electromagnetism, where magnetic field lines are strings that cannot end. 4/n https://t.co/aaSJnBLw1g', 'What is the symmetry principle enforcing the conservation of higher dimensional objects? Nowadays we call these ""higher form symmetries"", and they were explained in a beautiful paper from 2014 that influenced my own research greatly: 5/n \n\nhttps://t.co/f8taxAt5mV', ""The upshot is that if you ever have extended objects that can't break or vanish -- gauge theory flux tubes, cosmic strings, magnetic field lines --  you probably have one of these higher-form symmetries playing an important role. 6/n"", 'We can now try and use these new symmetries to organize our understanding. In the recent work with John, we build a Landau-Ginzburg theory for such symmetries, where we try and describe the physics close to a point where one of these symmetries is *about* to break. 7/n https://t.co/XajDosBc7X', 'Just as normal Landau-Ginzburg theories describe the condensation of particles, this new framework has to describe the condensation of *strings*. This is complicated but fun, and we try to get a grasp of it using these new symmetries and principles of effective field theory. 8/n', '(By the way, these are *not* gauge symmetries; gauge ""symmetry"" is perhaps a lousy name for something that is not really a symmetry at all. But a lot of gauge theories happen to host extended objects, and so can be nicely understood in this framework.) 9/n https://t.co/7kuPQiNPv5', ""If you're interested in hearing more, John will talk about it at Strings 2021 on Monday, and I've given some online talks on it. Thanks for reading to the end. n/n\n\n https://t.co/ZUB3SMCsZd"", '@arghya_dutta_ great, glad you found it helpful!', '@xcode_king so in theories with such higher-form symmetries, the operators that are *charged* under the symmetries are also extended objects, and indeed can often be thought of as defects.', ""@xcode_king that's right! but a curious fact is that it turns out that the physically interesting defects often *are* charged under such a symmetry, even if it isn't obvious at first glance....""]",https://arxiv.org/abs/2106.12610,"By analogy with the Landau-Ginzburg theory of ordinary zero-form symmetries, we introduce and develop a Landau-Ginzburg theory of one-form global symmetries, which we call mean string field theory. The basic dynamical variable is a string field -- defined on the space of closed loops -- that can be used to describe the creation, annihilation, and condensation of effective strings. Like its zero-form cousin, the mean string field theory provides a useful picture of the phase diagram of broken and unbroken phases. We provide a transparent derivation of the area law for charged line operators in the unbroken phase and describe the dynamics of gapless Goldstone modes in the broken phase. The framework also provides a theory of topological defects of the broken phase and a description of the phase transition that should be valid above an upper critical dimension, which we discuss. We also discuss general consequences of emergent one-form symmetries at zero and finite temperature. ",Mean string field theory: Landau-Ginzburg theory for 1-form symmetries
31,1408342557663318020,1061375141274439685,Xabier Cid Vidal üõ∞Ô∏è,"['New paper! On <LINK> with @CharlesTheVS and many others we study the LHCb sensitivity to the wonderful baryogenesis model proposed in <LINK>. Spoiler alert: we CAN discover DM at LHCb! The mechanism involves b-hadron decays to a dark sector üëáüëá', 'particle which we can\'t detect but can look for as ""missing-pT"" in the direction of flight of the hadron. This type of signatures appear in other LHCb analyses and we know we can do them pretty well. I\'m specially happy for Saul Lopez,', 'to which I propose doing his Master thesis working on this and who will now get his work (hopefully) published. If you want to know more, I recommend you attend the #offshell2021 conference, where we will present our work.']",https://arxiv.org/abs/2106.12870,"A model that can simultaneously explain Dark Matter relic density and the apparent matter anti-matter imbalance of the universe has been recently proposed. The model requires $b$-hadron branching fractions to Dark Matter at the per mille level. The $b$-hadrons decay to a dark sector baryon, $\psi_{\rm{DS}}$, which has a mass in the region $940$ MeV/c$^{2} \leq m(\psi_{\rm{DS}}) \leq 4430$ MeV/c$^{2}$. In this paper, we discuss the sensitivity of the LHCb experiment to search for this dark baryon, covering different types of topology and giving prospects for Runs 3 and 4 of the LHC, as well as for the proposed Phase-II Upgrade. We show that the LHCb experiment can cover the entire mass range of the hypothetical dark baryon. ","Prospects on searches for baryonic Dark Matter produced in $b$-hadron
  decays at LHCb"
32,1408331159633866752,786855300322172928,Alkistis Pourtsidou,"['A new paper led by @PedroCarrilho11 and Chiara Moretti performs a validation and mcmc forecast study for a non-standard (interacting) DE model <LINK>, focusing on the theoretical systematic of non-linearities when we interpret galaxy clustering measurements.', 'Pedro and Chiara (+ Ben Bose and @DidaMarkovic) carefully compared the performance of the most popular perturbation theory approaches,  that have been used for the analysis of e.g. BOSS data. https://t.co/OND8fo6bYD', 'They finally forecast unbiased constraints on the coupling parameter of the interacting dark energy model for surveys similar to @EC_Euclid and @desisurvey https://t.co/8z7TBDzNt2']",https://arxiv.org/abs/2106.13163,"Interacting dark energy models have been proposed as attractive alternatives to $\Lambda$CDM. Forthcoming Stage-IV galaxy clustering surveys will constrain these models, but they require accurate modelling of the galaxy power spectrum multipoles on mildly non-linear scales. In this work we consider a dark scattering model with a simple 1-parameter extension to $w$CDM - adding only $A$, which describes a pure momentum exchange between dark energy and dark matter. We then provide a comprehensive comparison of three approaches of modeling non-linearities, while including the effects of this dark sector coupling. We base our modeling of non-linearities on the two most popular perturbation theory approaches: TNS and EFTofLSS. To test the validity and precision of the modelling, we perform an MCMC analysis using simulated data corresponding to a $\Lambda$CDM fiducial cosmology and Stage-IV surveys specifications in two redshift bins, $z=0.5$ and $z=1$. We find the most complex EFTofLSS-based model studied to be better suited at both, describing the mock data up to smaller scales, and extracting the most information. Using this model, we forecast uncertainties on the dark energy equation of state, $w$, and on the interaction parameter, $A$, finding $\sigma_w=0.06$ and $\sigma_A=1.1$ b/GeV for the analysis at $z=0.5$ and $\sigma_w=0.06$ and $\sigma_A=2.0$ b/GeV for the analysis at $z=1$. In addition, we show that a false detection of exotic dark energy up to 3$\sigma$ would occur should the non-linear modelling be incorrect, demonstrating the importance of the validation stage for accurate interpretation of measurements. ",Interacting dark energy from redshift-space galaxy clustering
33,1408319788825206784,1118095529928318976,Shuicheng Yan@Sea AI Lab,"['[2106.13112] VOLO: Vision Outlooker for Visual Recognition\n\nNew SOTA 87.1% on ImageNet (no extra data), paper and code released, SEA AI Lab. <LINK>']",https://arxiv.org/abs/2106.13112,"Visual recognition has been dominated by convolutional neural networks (CNNs) for years. Though recently the prevailing vision transformers (ViTs) have shown great potential of self-attention based models in ImageNet classification, their performance is still inferior to that of the latest SOTA CNNs if no extra data are provided. In this work, we try to close the performance gap and demonstrate that attention-based models are indeed able to outperform CNNs. We find a major factor limiting the performance of ViTs for ImageNet classification is their low efficacy in encoding fine-level features into the token representations. To resolve this, we introduce a novel outlook attention and present a simple and general architecture, termed Vision Outlooker (VOLO). Unlike self-attention that focuses on global dependency modeling at a coarse level, the outlook attention efficiently encodes finer-level features and contexts into tokens, which is shown to be critically beneficial to recognition performance but largely ignored by the self-attention. Experiments show that our VOLO achieves 87.1% top-1 accuracy on ImageNet-1K classification, which is the first model exceeding 87% accuracy on this competitive benchmark, without using any extra training data In addition, the pre-trained VOLO transfers well to downstream tasks, such as semantic segmentation. We achieve 84.3% mIoU score on the cityscapes validation set and 54.3% on the ADE20K validation set. Code is available at \url{this https URL}. ",VOLO: Vision Outlooker for Visual Recognition
34,1408286165858410503,1059281382025977856,Polina Kirichenko,"['Excited to share that\n(1) It is my birthday üéÇ\n(2) We have a new paper ""Task-agnostic Continual Learning with Hybrid Probabilistic Models"" on arxiv today!  We design a hybrid generative-discriminative model based on normalizing flows for continual learning <LINK> <LINK>', 'In task-agnostic continual learning, we need to train the model continually on a sequence of tasks without knowing the task boundaries. Turns out, we can use a single hybrid model to (1) detect task changes, (2) avoid forgetting and (3) make predictions!', 'We propose HCL, which uses a single normalizing flow to map the data into a latent space where we model the distribution of each class in each task as a Gaussian. When a new task appears we (1) identify it using the flow and (2) initialize the latent Gaussians for the new task.', 'To make predictions, we use the Bayes rule: we predict the class (and task) for which the input has the highest density.', 'To avoid forgetting, we propose two techniques: \n- HCL-GR uses generative replay, where we sample data from a snapshot of the model and re-train the model on this data\n- HCL-FR uses functional regularization, where we force the model to map replay data to the same latent position https://t.co/1kIUJYq1NK', 'HCL-FR provides stronger regularization and prevents forgetting better! In the figure: (b) data distribution with squares showing our replay data; grey is the first and orange is the second task; (c) HCL-GR model and (d) HCL-FR model after training on the second task. https://t.co/0veMNcEbpB', 'We compare HCL variants with other generative continual learning models, with promising results. HCL performs well both in task-aware and task-agnostic settings! https://t.co/r77wd7gZHe', 'Finally, HCL enables us to automatically detect new tasks as well as recurring tasks! To do so we can use the normalizing flow likelihood, or an advanced anomaly detection technique such as DoSE (https://t.co/woliqw12Ma).', 'With a great team of coauthors at @DeepMind @GoogleAI  and @NYUDataScience:\n\n@MFarajtabar, @drao64, @balajiln, Nir Levine, @__angli, Huiyi Hu, @andrewgwils and @rpascanu!', '@KevinKaichuang Happy birthday! üôÇ', '@balajiln Thank you! :)', '@MFarajtabar Haha thank you Mehrdad! üôÇ']",https://arxiv.org/abs/2106.12772,"Learning new tasks continuously without forgetting on a constantly changing data distribution is essential for real-world problems but extremely challenging for modern deep learning. In this work we propose HCL, a Hybrid generative-discriminative approach to Continual Learning for classification. We model the distribution of each task and each class with a normalizing flow. The flow is used to learn the data distribution, perform classification, identify task changes, and avoid forgetting, all leveraging the invertibility and exact likelihood which are uniquely enabled by the normalizing flow model. We use the generative capabilities of the flow to avoid catastrophic forgetting through generative replay and a novel functional regularization technique. For task identification, we use state-of-the-art anomaly detection techniques based on measuring the typicality of the model's statistics. We demonstrate the strong performance of HCL on a range of continual learning benchmarks such as split-MNIST, split-CIFAR, and SVHN-MNIST. ",Task-agnostic Continual Learning with Hybrid Probabilistic Models
35,1408269870660087815,382008009,Miles Cranmer,"['Happy to share our paper on AI for observational astronomy via our new resource allocation algorithm!\n\n""Unsupervised Resource Allocation with Graph Neural Networks""\n\nBlog/code: <LINK>\nPaper: <LINK>\n\nw/ @peter_melchior @iamstarnord \n\nThread üëá\n1/n <LINK>', 'Here‚Äôs a hard problem:\nOf, say, 10^9 galaxies, which should we observe to best measure the properties of the universe?\n\nHere‚Äôs an even harder problem: \nHow do we do this when we don‚Äôt fully understand the relationship between galaxies and the properties of the universe?\n\n2/n https://t.co/G7YW2akLJs', 'As you might guess, astronomers are stuck with the harder problem.\n\nWe develop an approach to this problem using what we call ""unsupervised resource allocation,"" which, expressed formally, learns a reward function without supervision to maximize a global utility.\n\n3/n', 'This lets us learn how to optimally schedule telescope time‚Äîor any other resource‚Äîfrom scratch, using only simulations of some parameter of interest. No labels needed for which sources are good tracers, nor how the observed properties relate to the desired parameter!\n\n4/n', ""Here's how the algorithm actually works:\n(1) Generate simulation conditioned on desired parameter, (2) add noise, (3) GNN1 allocates resources to parts of the simulation, (4) remove noise based on allocated resources, (5) GNN2 predicts parameter, (6) backprop!\n\n5/n https://t.co/AAiuVFZoA3"", ""Here's a visual representation of how this works, next to a more traditional observational pipeline:\n\n6/n https://t.co/RL5zbVXpBo"", 'Simply by doing gradient descent on this forward model (no RL!), our model can optimize telescope resources for a toy Cosmology problem!\n\nHere\'s the distribution of time per galaxy. The model appears to either ""observe"" for 0 minutes, or the max time of 1 hour per source!\n\n7/n https://t.co/BsUGceKyRe', 'We map observation time-&gt;noise as a function of galaxy mass and redshift, so we can also see how the model is allocating resources as a function of these. As can be seen, the model has learned that the best bang-for-the-buck is low-redshift, high-mass galaxies!\n\n8/n https://t.co/xrbWbmOW3i', 'We compare against simpler allocation strategies, such as GA-tuned luminosity thresholds and distributions. These models are stacked on the ""prediction GNN"" for a fairer comparison. \n\nLearned luminosity thresholds actually do quite well! But the GNN wins in the end:\n\n9/n https://t.co/MdY84Sufaj', 'Please let us know if you have any questions!\n\nThis was a part of the pre-registration experiment at NeurIPS (https://t.co/n4P5ErClTm), which I think is a great idea; I really enjoyed it. With the experiments done, this paper will now go to PMLR.\n\n10/10 https://t.co/q4bhS96KZ8']",https://arxiv.org/abs/2106.09761,"We present an approach for maximizing a global utility function by learning how to allocate resources in an unsupervised way. We expect interactions between allocation targets to be important and therefore propose to learn the reward structure for near-optimal allocation policies with a GNN. By relaxing the resource constraint, we can employ gradient-based optimization in contrast to more standard evolutionary algorithms. Our algorithm is motivated by a problem in modern astronomy, where one needs to select-based on limited initial information-among $10^9$ galaxies those whose detailed measurement will lead to optimal inference of the composition of the universe. Our technique presents a way of flexibly learning an allocation strategy by only requiring forward simulators for the physics of interest and the measurement process. We anticipate that our technique will also find applications in a range of resource allocation problems. ",Unsupervised Resource Allocation with Graph Neural Networks
36,1408238640652701697,2337598033,Geraint F. Lewis,"['Paper day! Excellent new paper by S5, led by @alexanderpji on Antlia 2 and Crater 2 - \n\n<LINK> <LINK>', '@alexanderpji And check out the cook video made by @deniserkal \n\nhttps://t.co/YUITjELTsT']",https://arxiv.org/abs/2106.12656,"We present new spectroscopic observations of the diffuse Milky Way satellite galaxies Antlia 2 and Crater 2, taken as part of the Southern Stellar Stream Spectroscopic Survey (S5). The new observations approximately double the number of confirmed member stars in each galaxy and more than double the spatial extent of spectroscopic observations in Antlia 2. A full kinematic analysis, including Gaia EDR3 proper motions, detects a clear velocity gradient in Antlia 2 and a tentative velocity gradient in Crater 2. The velocity gradient magnitudes and directions are consistent with particle stream simulations of tidal disruption. Furthermore, the orbit and kinematics of Antlia 2 require a model that includes the reflex motion of the Milky Way induced by the Large Magellanic Cloud. We also find that Antlia 2's metallicity was previously overestimated, so it lies on the empirical luminosity-metallicity relation and is likely only now experiencing substantial stellar mass loss. Current dynamical models of Antlia 2 require it to have lost over 90% of its stars to tides, in tension with the low stellar mass loss implied by the updated metallicity. Overall, the new kinematic measurements support a tidal disruption scenario for the origin of these large and extended dwarf spheroidal galaxies. ","Kinematics of Antlia 2 and Crater 2 from The Southern Stellar Stream
  Spectroscopic Survey (S5)"
37,1408227027040280576,156804540,Francisco Rodrigues,['Our new paper on @arxiv_org:\n<LINK>\nWe provide an introduction to the neural networks relevant to Dengue forecasting and review their applications in the literature. \n#DataScience #dengue #MachineLearning #Epidemiology'],https://arxiv.org/abs/2106.12905,"Due to a lack of treatments and universal vaccine, early forecasts of Dengue are an important tool for disease control. Neural networks are powerful predictive models that have made contributions to many areas of public health. In this systematic review, we provide an introduction to the neural networks relevant to Dengue forecasting and review their applications in the literature. The objective is to help inform model design for future work. Following the PRISMA guidelines, we conduct a systematic search of studies that use neural networks to forecast Dengue in human populations. We summarize the relative performance of neural networks and comparator models, model architectures and hyper-parameters, as well as choices of input features. Nineteen papers were included. Most studies implement shallow neural networks using historical Dengue incidence and meteorological input features. Prediction horizons tend to be short. Building on the strengths of neural networks, most studies use granular observations at the city or sub-national level. Performance of neural networks relative to comparators such as Support Vector Machines varies across study contexts. The studies suggest that neural networks can provide good predictions of Dengue and should be included in the set of candidate models. The use of convolutional, recurrent, or deep networks is relatively unexplored but offers promising avenues for further research, as does the use of a broader set of input features such as social media or mobile phone data. ",Neural Networks for Dengue Prediction: A Systematic Review
38,1408081802347089928,202211003,Alan Winfield üíô,"['Experiments in Artificial Culture: from noisy imitation to storytelling robots <LINK> A pre-print of our new paper (in review), which (a) outlines 10 years of experimental work modelling aspects of cultural evolution with robots, and 1/2', '(b) asks for ideas and research questions we could explore with our Storybots. 2/2']",https://arxiv.org/abs/2106.11754,"This paper presents a series of experiments in collective social robotics, spanning more than 10 years, with the long-term aim of building embodied models of (aspects) of cultural evolution. Initial experiments demonstrated the emergence of behavioural traditions in a group of social robots programmed to imitate each other's behaviours (we call these Copybots). These experiments show that the noisy (i.e. less than perfect fidelity) imitation that comes for free with real physical robots gives rise naturally to variation in social learning. More recent experimental work extends the robots' cognitive capabilities with simulation-based internal models, equipping them with a simple artificial theory of mind. With this extended capability we explore, in our current work, social learning not via imitation but robot-robot storytelling, in an effort to model this very human mode of cultural transmission. In this paper we give an account of the methods and inspiration for these experiments, the experiments and their results, and an outline of possible directions for this programme of research. It is our hope that this paper stimulates not only discussion but suggestions for hypotheses to test with the Storybots. ","Experiments in Artificial Culture: from noisy imitation to storytelling
  robots"
39,1408075288219504640,109603566,Stephen James,"['Coarse-to-fine ARM is a new way to do sparsely-rewarded real-world visual robot manipulation! ü¶æ\nE.g. video below shows learning to take lid off a saucepan, tabula rasa, with only 3 demos, in under 7 mins! \n\nVideos/Code: <LINK>\nPaper: <LINK> <LINK>', 'The magic comes from coarse-to-fine Q-attention, which allows effective discretisation of the (unbounded) translation space. With this discretisation, we are able to diverge from commonly used (unstable) actor-critic methods and instead use a more stable Q-learning method.', 'This was my final PhD work with @AjdDavison, and it is certainly the work I am most proud of. When I started my PhD in 2016, I thought that sim2real would be the only way to get RL+Manipulation to work in the real world (without needing &gt;&gt; real data); this work proves otherwise!']",https://arxiv.org/abs/2106.12534,"We present a coarse-to-fine discretisation method that enables the use of discrete reinforcement learning approaches in place of unstable and data-inefficient actor-critic methods in continuous robotics domains. This approach builds on the recently released ARM algorithm, which replaces the continuous next-best pose agent with a discrete one, with coarse-to-fine Q-attention. Given a voxelised scene, coarse-to-fine Q-attention learns what part of the scene to 'zoom' into. When this 'zooming' behaviour is applied iteratively, it results in a near-lossless discretisation of the translation space, and allows the use of a discrete action, deep Q-learning method. We show that our new coarse-to-fine algorithm achieves state-of-the-art performance on several difficult sparsely rewarded RLBench vision-based robotics tasks, and can train real-world policies, tabula rasa, in a matter of minutes, with as little as 3 demonstrations. ","Coarse-to-Fine Q-attention: Efficient Learning for Visual Robotic
  Manipulation via Discretisation"
40,1408016090668539906,103583599,Vishnu Reddy,"[""New paper on water ice mixtures on Neptune's moon Nereid by my grad student @benjaminsharkey and colleague @WaltHarris \n<LINK>""]",https://arxiv.org/abs/2106.12084,"Nereid, Neptune's third largest satellite, lies in an irregular orbit and is the only outer satellite in the system (apart from Triton) that can be spectroscopically characterized with the current generation of Earth-based telescopes. We report our results on spectral characterization of Nereid using its reflectance spectrum from 0.8-2.4 $\mu m$, providing the first measurements over the range of 0.8-1.4 $\mu m$. We detect spectral absorption features of crystalline water ice in close agreement with previous measurements. We show that model fits of simple intimate mixtures including water ice do not provide simultaneous matches to absorption band depths at 1.5 and 2.0 $\mu m$ when accounting for the spectral continuum. Possible solutions include invoking a more complex continuum, including both crystalline and amorphous water ice, and allowing for sub-micron sized grains. We show that mixtures including magnetite and the CM2 chondrite Murchison provide a flexible framework for interpreting spectral variation of bodies with neutral-sloped spectra like that of Nereid. Magnetite in particular provides a good match to the spectral continuum without requiring the presence of Tholin-like organics. We note that carbonaceous chondrites and their components may be useful analogs for the non-ice components of outer solar system bodies, consistent with recent findings by Fraser et al. (2019). Comparison to spectra of large TNOs and satellites of Uranus show that Nereid's low albedo, deep water bands, and neutral color is distinct from many other icy objects, but such comparisons are limited by incomplete understanding of spectral variability among $\sim$100km-sized icy bodies. ","Complex Water Ice Mixtures on NII Nereid: Constraints from NIR
  Reflectance"
41,1407963590515118083,1140222123006472194,Kasper Elm Heintz,"['New paper on @arxiv_org today, lead by W. Fong @FongGroup at: <LINK>\n\nHere we identify and characterize the host galaxy of the repeating, super-bursting FRB 20201124A <LINK>', 'We found that the galaxy is a dusty, relatively modest star-formning galaxy (in good agreement with last weeks result by Vikram Ravi and collaborators: https://t.co/KZLlitr5fm) with a potential hot MIR dust component contributing ~10-30% to the SED. https://t.co/eEClt2bb3T', 'Perhaps most intriguing, is that we could constrain the star-formation history of the galaxy finding that &gt;90% of its mass (and thus likely the progenitor star/object) was formed 1 Gyr ago, putting strong constraints on the likely progenitor channels of this FRB. https://t.co/QJAutE2fBF']",https://arxiv.org/abs/2106.11993,"We present the Australian Square Kilometre Array Pathfinder (ASKAP) localization and follow-up observations of the host galaxy of the repeating fast radio burst (FRB) source, FRB20201124A, the fifth such extragalactic repeating FRB with an identified host. From spectroscopic observations using the 6.5-m MMT Observatory, we derive a redshift of $z=0.0979 \pm 0.0001$, a star formation rate inferred from H$\alpha$ emission of SFR(H$\alpha$) $\approx 2.1 M_{\odot}$ yr$^{-1}$, and a gas-phase metallicity of 12+log(O/H)$\approx 9.0$. By jointly modeling the 12-filter optical-mid-infrared (MIR) photometry and spectroscopy of the host, we infer a median stellar mass of $\approx 2 \times 10^{10} M_{\odot}$, internal dust extinction of $A_V\approx 1-1.5$ mag, and a mass-weighted stellar population age of $\approx 5-6$ Gyr. Connecting these data to the radio and X-ray observations, we cannot reconcile the broad-band behavior with strong AGN activity and instead attribute the dominant source of persistent radio emission to star formation, likely originating from the circumnuclear region of the host. The modeling also indicates a hot dust component contributing to the MIR luminosity at a level of $\approx 10-30\%$. We model the host galaxy's star formation and mass assembly histories, finding that the host assembled $>90\%$ of its mass by 1 Gyr ago and exhibited a fairly constant SFR for most of its existence, with no clear evidence of past star-burst activity. ","Chronicling the Host Galaxy Properties of the Remarkable Repeating FRB
  20201124A"
42,1407785261992493064,19355829,Jonathan McDowell,"[""I really like this paper on the arxiv today by @jgagneastro et al: <LINK> - giving a new view on the geography of this part of the galaxy. Another example of how @ESAGaia, not just a big pile'o'data, is giving us truly deep insights into our cosmic neighbourhood"", '@AstroMikeMerri @jgagneastro @ESAGaia Yes - I liked that paper too!']",https://arxiv.org/abs/2106.11873,"We propose that fourteen co-moving groups of stars uncovered by Kounkel & Covey (2019) may be related to known nearby moving groups and bridge those and nearby open clusters with similar ages and space velocities. This indicates that known nearby moving groups may be spatially much more extended than previously though, and some of them might be parts of tidal tails around the cores of known open clusters, reminiscent of those recently found around the Hyades and a handful of other nearby clusters. For example, we find that both the nearby Carina and Columba associations may be linked to Theia 208 from Kounkel & Covey (2019) and together form parts of a large tidal tail around the Platais 8 open cluster. The AB Doradus moving group and Theia 301 may form a trailing tidal tail behind the Pleiades open cluster, with hints of a possible leading tidal tail in Theia 369. We similarly find that IC 2391 and its tidal tails identified by Meingast et al. (2021) may be extended by the nearby Argus association and are possibly further extended by Theia 115. The nearby Octans and Octans-Near associations, as well as Theia 94 and 95, may form a large tidal tail leading the poorly studied Platais 5 open cluster candidate. While a preliminary analysis of Gaia color-magnitude sequences hint that these structures are plausibly related, more observational evidence is still required to corroborate their consistent ages and space velocities. These observations may change our current understanding of nearby moving groups and the different pathways through which they can form. While some moving groups may have formed loosely in extended star-formation events with rich spatial structure, others may in fact correspond to the tidal tails of nearby open clusters. ","A Number of Nearby Moving Groups may be Fragments of Dissolving Open
  Clusters"
43,1407770346321858565,325547802,Freddie Bickford Smith,"['New paper with @bdroads, @ken_lxl &amp; @profdata. How does top-down attention help in vision? Contrasting with standard accounts that point to stimulus variables like clutter, we find that system variables capturing model-data-task interaction are key. [1/7]\n<LINK> <LINK>', 'Building on recent work by eg @neurograce &amp; @ken_lxl, plus classics like ALCOVE, we use a CNN with attention incorporated as feature-map modulation. This lets us scale to thousands of naturalistic tasks with multiple factors of variation, unlike in a typical lab experiment. [2/7]', 'Seeking to understand how attention boosts perception, we take the popular template for an ablation study (see @fchollet thread) and extend it to get deeper insights. Task-oriented ablation design (TOAD), our framework, is applicable across ML. [3/7]\nhttps://t.co/84dGarumO9', 'The core idea of TOAD: (1) quantify differences between tasks; (2) use this information to design and analyse an ablation study. More than just indicating *whether* your ML method works, TOAD tells you *when* (and thus possibly *how*) it works. [4/7] https://t.co/YfCnhxilBp', 'Using TOAD, we test our model on lots of tasks. To some degree, our results reflect the quirks of CNNs (see @jh_jacobsen thread). But the bigger takeaway is a view on attention as handling the interplay between model, training data and task format. [5/7]\nhttps://t.co/NeI2XF3JSG', 'This system-level view resonates with recent work by @khermann_, @andrewlampinen, @tingchenai &amp; @skornblith (see thread). If attention modulates representations, is the process that produces the representations the most valuable thing to analyse? [6/7]\nhttps://t.co/XaEfAsHwsc', 'We‚Äôd love to hear your feedback and answer questions. Send me a message! Thanks to those who helped along the way: @egrefen, @summerfieldlab &amp; Will Tebbutt. Work done at @aims_oxford, @uclcs &amp; @uclpals, and funded by @epsrc, @kelloggox, @nih, @royalsociety &amp; @wellcometrust. [7/7]', '@AndrewLampinen @egrefen @summerfieldlab @aims_oxford @uclcs @UCLPALS @EPSRC @KelloggOx @NIH @royalsociety @wellcometrust Good question! Like you, we thought there might be a ceiling effect at play. We checked and found no clear evidence for this in the results.']",http://arxiv.org/abs/2106.11339,"Top-down attention allows neural networks, both artificial and biological, to focus on the information most relevant for a given task. This is known to enhance performance in visual perception. But it remains unclear how attention brings about its perceptual boost, especially when it comes to naturalistic settings like recognising an object in an everyday scene. What aspects of a visual task does attention help to deal with? We aim to answer this with a computational experiment based on a general framework called task-oriented ablation design. First we define a broad range of visual tasks and identify six factors that underlie task variability. Then on each task we compare the performance of two neural networks, one with top-down attention and one without. These comparisons reveal the task-dependence of attention's perceptual boost, giving a clearer idea of the role attention plays. Whereas many existing cognitive accounts link attention to stimulus-level variables, such as visual clutter and object scale, we find greater explanatory power in system-level variables that capture the interaction between the model, the distribution of training data and the task format. This finding suggests a shift in how attention is studied could be fruitful. We make publicly available our code and results, along with statistics relevant to ImageNet-based experiments beyond this one. Our contribution serves to support the development of more human-like vision models and the design of more informative machine-learning experiments. ",Understanding top-down attention using task-oriented ablation design
44,1407749347714818052,1182194132309012481,Michihiro Yasunaga,"['New #ICML2021 paper ""Break-It-Fix-It: Unsupervised Learning for Program Repair"" with @percyliang @StanfordAILab @StanfordNLP!\n\nPaper: <LINK>\nGithub: <LINK>\n\nLearning to repair errors in programs is a key problem in machine learning. üîßü§ñ\n[1/6] ‚§µÔ∏è <LINK>', 'There is a lot of unlabeled code out there (e.g. GitHub). How can we leverage it to learn program repair? \n\nExisting works create synthetic training data by corrupting good code (e.g. dropping tokens), but this does not generalize to real distributions of broken code..\n[2/6] https://t.co/JbAX8esu1U', 'To tackle this challenge, we develop a new training method, Break-It-Fix-It (BIFI). We iteratively train both the desired fixer that maps bad code to good code and a breaker that maps good code to bad code, while using them to generate more realistic training data.\n[3/6] https://t.co/cMJz6AKJez', 'Compared to backtranslation (cycle-consistency) in unsupervised translation, BIFI further incorporates a critic (e.g. compiler) to improve the correctness of training data generated by breaker/fixer.\n\nBIFI attains new SOTA on program repair tasks: DeepFix and GitHub-Python.\n[4/6] https://t.co/5Mix6PupyS', 'In essence, BIFI is a new framework to turn raw unlabeled data into usable paired (training) data with the help of a critic. This is potentially applicable besides program repair, e.g. text editing, machine translation and molecular design.\n[5/6]', 'For more details, please check out our paper at\nhttps://t.co/WMyki8aYyy!\n\nA huge thank you to the collaborators and all who gave us feedback!\n[6/6]']",https://arxiv.org/abs/2106.06600,"We consider repair tasks: given a critic (e.g., compiler) that assesses the quality of an input, the goal is to train a fixer that converts a bad example (e.g., code with syntax errors) into a good one (e.g., code with no syntax errors). Existing works create training data consisting of (bad, good) pairs by corrupting good examples using heuristics (e.g., dropping tokens). However, fixers trained on this synthetically-generated data do not extrapolate well to the real distribution of bad inputs. To bridge this gap, we propose a new training approach, Break-It-Fix-It (BIFI), which has two key ideas: (i) we use the critic to check a fixer's output on real bad inputs and add good (fixed) outputs to the training data, and (ii) we train a breaker to generate realistic bad code from good code. Based on these ideas, we iteratively update the breaker and the fixer while using them in conjunction to generate more paired data. We evaluate BIFI on two code repair datasets: GitHub-Python, a new dataset we introduce where the goal is to repair Python code with AST parse errors; and DeepFix, where the goal is to repair C code with compiler errors. BIFI outperforms existing methods, obtaining 90.5% repair accuracy on GitHub-Python (+28.5%) and 71.7% on DeepFix (+5.6%). Notably, BIFI does not require any labeled data; we hope it will be a strong starting point for unsupervised learning of various repair tasks. ",Break-It-Fix-It: Unsupervised Learning for Program Repair
45,1407679235351797768,16581986,sdoyle44,"[""There is a new paper on arxiv describing the Highdicom library. I've used this for generating both DICOM SR and DICOM Seg objects: <LINK>""]",https://arxiv.org/abs/2106.07806,"Machine learning is revolutionizing image-based diagnostics in pathology and radiology. ML models have shown promising results in research settings, but their lack of interoperability has been a major barrier for clinical integration and evaluation. The DICOM a standard specifies Information Object Definitions and Services for the representation and communication of digital images and related information, including image-derived annotations and analysis results. However, the complexity of the standard represents an obstacle for its adoption in the ML community and creates a need for software libraries and tools that simplify working with data sets in DICOM format. Here we present the highdicom library, which provides a high-level application programming interface for the Python programming language that abstracts low-level details of the standard and enables encoding and decoding of image-derived information in DICOM format in a few lines of Python code. The highdicom library ties into the extensive Python ecosystem for image processing and machine learning. Simultaneously, by simplifying creation and parsing of DICOM-compliant files, highdicom achieves interoperability with the medical imaging systems that hold the data used to train and run ML models, and ultimately communicate and store model outputs for clinical use. We demonstrate through experiments with slide microscopy and computed tomography imaging, that, by bridging these two ecosystems, highdicom enables developers to train and evaluate state-of-the-art ML models in pathology and radiology while remaining compliant with the DICOM standard and interoperable with clinical systems at all stages. To promote standardization of ML research and streamline the ML model development and deployment process, we made the library available free and open-source. ","Highdicom: A Python library for standardized encoding of image
  annotations and machine learning model outputs in pathology and radiology"
46,1407675604896567302,896641093,"John J. Howard, Ph.D.","['New Paper Announcement! How we determine groups when measuring #Fairness in #AI is a huge unanswered problem. We do a quantitative analysis of one approach in computer vision called the Fitzpatrick Skin Type (FST) ...\n\n<LINK>', 'We show this metric may not be measuring what we think it is and can lead to questionable outcomes in studies of #bias. Measurement is important! Particularly when we discuss fairness.  Feedback welcome.']",https://arxiv.org/abs/2106.11240,"With increasing adoption of face recognition systems, it is important to ensure adequate performance of these technologies across demographic groups. Recently, phenotypes such as skin-tone, have been proposed as superior alternatives to traditional race categories when exploring performance differentials. However, there is little consensus regarding how to appropriately measure skin-tone in evaluations of biometric performance or in AI more broadly. In this study, we explore the relationship between face-area-lightness-measures (FALMs) estimated from images and ground-truth skin readings collected using a device designed to measure human skin. FALMs estimated from different images of the same individual varied significantly relative to ground-truth FALM. This variation was only reduced by greater control of acquisition (camera, background, and environment). Next, we compare ground-truth FALM to Fitzpatrick Skin Types (FST) categories obtained using the standard, in-person, medical survey and show FST is poorly predictive of skin-tone. Finally, we show how noisy estimation of FALM leads to errors selecting explanatory factors for demographic differentials. These results demonstrate that measures of skin-tone for biometric performance evaluations must come from objective, characterized, and controlled sources. Further, despite this being a currently practiced approach, estimating FST categories and FALMs from uncontrolled imagery does not provide an appropriate measure of skin-tone. ","Reliability and Validity of Image-Based and Self-Reported Skin Phenotype
  Metrics"
47,1407648910949195776,520665133,R. T. Pierrehumbert,"['Our new paper led by @EleeAstro,  establishing validity of Exo-FMS for Hot Jupiter simulations, and benchmarking performance of simplified radiation codes.  <LINK> .']",https://arxiv.org/abs/2106.11664,"Radiative-transfer (RT) is a fundamental part of modelling exoplanet atmospheres with general circulation models (GCMs). An accurate RT scheme is required for estimates of the atmospheric energy transport and for gaining physical insight from model spectra. We implement three RT schemes for Exo-FMS: semi-grey, non-grey `picket fence', and real gas with correlated-k. We benchmark the Exo-FMS GCM using these RT schemes to hot Jupiter simulation results from the literature. We perform a HD 209458b-like simulation with the three schemes and compare their results. These simulations are then post-processed to compare their observable differences. The semi-grey scheme results show qualitative agreement with previous studies in line with variations seen between GCM models. The real gas model reproduces well the temperature and dynamical structures from other studies. After post-processing our non-grey picket fence scheme compares very favourably with the real gas model, producing similar transmission spectra, emission spectra and phase curve behaviours. Exo-FMS is able to reliably reproduce the essential features of contemporary GCM models in the hot gas giant regime. Our results suggest the picket fence approach offers a simple way to improve upon RT realism beyond semi-grey schemes. ","Simulating gas giant exoplanet atmospheres with Exo-FMS: Comparing
  semi-grey, picket fence and correlated-k radiative-transfer schemes"
48,1407646622188769283,1347909035069206528,Anton Frisk Kockum,"['New preprint out today: ‚ÄùChiral quantum optics with giant atoms‚Äù <LINK> First paper with my PhD student Ariadna Soro, who did a great job. (1/n)', 'We investigate how giant atoms (atoms coupled at multiple points to a waveguide) behave if the coupling depends on the propagation direction of the light. (2/n)', 'We find that the decoherence-free interaction of ‚Äùbraided‚Äù giant atoms, previously predicted (https://t.co/gdzJFp57yB) and confirmed (https://t.co/eK89JJqV4B) for the case when the coupling doesn‚Äôt depend on the propagation direction, also exists in the chiral case. (3/n)', 'Furthermore, we show that unlike small atoms (atoms coupling at a single point), giant atoms in a chiral waveguide can reach a dark state even without being excited by a coherent drive. (4/n)', 'We also show that the dark states that arise in the driven-dissipative regime can be populated faster in giant atoms. (5/n)', 'All these effects seem ready to be demonstrated in experiment. (6/n)', 'Looking further ahead, we hope that our results lay a foundation for applications based on giant atoms in quantum simulations and quantum networks with chiral settings. (7/7)']",https://arxiv.org/abs/2106.11946,"In quantum optics, it is common to assume that atoms are point-like objects compared to the wavelength of the electromagnetic field they interact with. However, this dipole approximation is not always valid, e.g., if atoms couple to the field at multiple discrete points. Previous work has shown that superconducting qubits coupled to a one-dimensional waveguide can behave as such ""giant atoms"" and then can interact through the waveguide without decohering, a phenomenon that is not possible with small atoms. Here, we show that this decoherence-free interaction is also possible when the coupling to the waveguide is chiral, i.e., when the coupling depends on the propagation direction of the light. Furthermore, we derive conditions under which the giant atoms in such chiral architectures exhibit dark states. In particular, we show that unlike small atoms, giant atoms in a chiral waveguide can reach a dark state even without being excited by a coherent drive. We also show that in the driven-dissipative regime, dark states can be populated faster in giant atoms. The results presented here lay a foundation for applications based on giant atoms in quantum simulations and quantum networks with chiral settings. ",Chiral quantum optics with giant atoms
49,1407625833645228036,373332154,Vivien Parmentier,"[""New paper by radiative-transfer, GCM and microphysics wizard @EleeAstro ! She implemented three new RT schemes inside the FMS GCM. I'm particularly glad one of these make use of the picket-fence radiative transfer solution I derived during my PhD ! (1/5) <LINK> <LINK>"", 'The picket fence model assumes that there are 2 opacity bands in the infrared, rather than only 1 in the grey model. In short we use the function on the right to approximate the function on the left ! (2/5) https://t.co/cfu7jW5yd1', ""yes it's BRUTAL but it does allow for a specific effect : that the atmosphere can radiatively cool down at multiple layers. A constant opacity lead to temperatures that cannot be lower than a given value. Compare the temperature on the right (grey) and left (picket-fence) (3/5) https://t.co/dQLNvatQjy"", 'So @EleeAstro added this the the GCM and compared all the RT models and found that the picket-fence (top right) leads to a solution closer to the full non-grey solution (left) for only a small additional amount of CPU time compared to the grey one (bottom right)! (4/5) https://t.co/05DITO5o5z', 'This is good news as fully non-grey GCMs are cumbersome to run. So we now have an alternative, intermediate choice. As usual it is a good choice depending on the science question you want to solve ! (5/5) https://t.co/zxMyVsKE8H', 'Also on the paper @ClimateBook @ShamiTsai @tntfat @mark_planets  Daniel Kitzmann and Simon Grimm !', '@MarcqPlanets Yes exactly. Basically one big IR band and 2 g-bin inside it. I think we talked about that for your lava planet atmosphere a hundred years ago :D', ""@MarcqPlanets It has the advantage that you don't care where the Planck function is with respect to the opacities. But we choose that mainly because I had taken the time to fit the coefficient to the real gas case (H2 dominated solar-composition). But the numerics can be extended to N bands.""]",https://arxiv.org/abs/2106.11664,"Radiative-transfer (RT) is a fundamental part of modelling exoplanet atmospheres with general circulation models (GCMs). An accurate RT scheme is required for estimates of the atmospheric energy transport and for gaining physical insight from model spectra. We implement three RT schemes for Exo-FMS: semi-grey, non-grey `picket fence', and real gas with correlated-k. We benchmark the Exo-FMS GCM using these RT schemes to hot Jupiter simulation results from the literature. We perform a HD 209458b-like simulation with the three schemes and compare their results. These simulations are then post-processed to compare their observable differences. The semi-grey scheme results show qualitative agreement with previous studies in line with variations seen between GCM models. The real gas model reproduces well the temperature and dynamical structures from other studies. After post-processing our non-grey picket fence scheme compares very favourably with the real gas model, producing similar transmission spectra, emission spectra and phase curve behaviours. Exo-FMS is able to reliably reproduce the essential features of contemporary GCM models in the hot gas giant regime. Our results suggest the picket fence approach offers a simple way to improve upon RT realism beyond semi-grey schemes. ","Simulating gas giant exoplanet atmospheres with Exo-FMS: Comparing
  semi-grey, picket fence and correlated-k radiative-transfer schemes"
50,1407603910802653186,3108542843,Fran√ßois-Xavier Briol,"['New paper with Ziang Niu (@MaxwellAng1) and Johanna Meier on some of the advantages of fitting intractable generative models using quasi-Monte Carlo <LINK> [1/9]', 'In statistics/ML, we often have to resort to repeatedly sampling from a model in order to fit parameters. For Bayesian, this is used in approximate Bayesian computation (ABC), whereas frequentists use it in the method of simulated moments. In ML, we have GANs, etc... [2/9]', 'To compare simulated and true data, we use discrepancies. Examples include the MMD, Wasserstein dist, Sinkhorn divergence or sliced discrepancies. The performance  depends on how well we approximate the discrepancy between the model &amp; data generating process using samples [3/9]', 'In this paper, we study the use of quasi-Monte Carlo (QMC) sampling for this task. That is, we propose to use QMC instead of Monte Carlo (MC) to obtain samples from our model, which requires only a very minor change to existing algorithms. [4/9]', 'The plot below shows the impact of this for a bivariate g-and-k model - visually at least, QMC clearly provides a better ""coverage"" of the distribution (notice the cluttered MC points near the mode and the big gap in the bottom right!). [5/9] https://t.co/cgf24MWquR', 'Now QMC sampling is of course nothing new, but the main contribution of the paper is to prove rigorous results showing this leads to improved sample complexity over IID sampling for the discrepancies mentioned above (whenever the generator and domain are nice enough). [6/9] https://t.co/4PB3thwmB6', 'For the MMD &amp; Sinkhorn, gains are possible regardless of dimensionality. For the Wasserstein, gains are only possible for d=1, but this still quite useful because the sliced-Wasserstein is becoming increasingly popular, and the fast rate is inherited for it regardless of d. [7/9]', 'The paper also has an extensive simulation study looking at the impact of QMC sampling for a range of parameter estimation problems in cases which satisfy the theory, as well as cases where the assumptions do not hold (the g-and-k is actually one of those). [8/9]', 'This was a nice project to work on with some of our top-notch @stats_UCL students. Ziang (@MaxwellAng1) was a visiting undergraduate student, and Johanna was doing an MSc Data Science. I look forward to seeing what they both achieve in their future research! [9/9]']",https://arxiv.org/abs/2106.11561,"Intractable generative models are models for which the likelihood is unavailable but sampling is possible. Most approaches to parameter inference in this setting require the computation of some discrepancy between the data and the generative model. This is for example the case for minimum distance estimation and approximate Bayesian computation. These approaches require sampling a high number of realisations from the model for different parameter values, which can be a significant challenge when simulating is an expensive operation. In this paper, we propose to enhance this approach by enforcing ""sample diversity"" in simulations of our models. This will be implemented through the use of quasi-Monte Carlo (QMC) point sets. Our key results are sample complexity bounds which demonstrate that, under smoothness conditions on the generator, QMC can significantly reduce the number of samples required to obtain a given level of accuracy when using three of the most common discrepancies: the maximum mean discrepancy, the Wasserstein distance, and the Sinkhorn divergence. This is complemented by a simulation study which highlights that an improved accuracy is sometimes also possible in some settings which are not covered by the theory. ","Discrepancy-based Inference for Intractable Generative Models using
  Quasi-Monte Carlo"
51,1407411113226952705,822667790628880384,Nir Goldman,"['Please see our latest paper on a new graph theory-based order parameter for characterizing condensed phases: <LINK>', 'Our order parameter is physically motivated, and involves a single user-defined variable. It clearly outperforms the Steinhardt order parameters as a well machine learned approach for a wide variety of solids, molten materials, and nanoclusters.', ""@abhishekshwarma Thanks for your comments! I'll be sure to have the lead author post is as soon as possible."", '@abhishekshwarma We did experiment with using simpler functional forms (e.g., either the connectivity OR ""entropy"" term) but found that the combination of the two did the best job of distinguishing between materials. The order parameter does well for 2D systems (Fig. 2).', ""@abhishekshwarma That's a nice point -- a clean example showing improved results with the cubic exponent would be interesting.""]",https://arxiv.org/abs/2106.08215,"A new graph-based order parameter is introduced for the characterization of atomistic structures. The order parameter is universal to any material/chemical system, and is transferable to all structural geometries. Three sets of data are used to validate both the generalizability and accuracy of the algorithm: (1) liquid lithium configurations spanning up to 300 GPa, (2) condensed phases of carbon along with nanotubes and buckyballs at ambient and high temperature, and (3) a diverse set of aluminum configurations including surfaces, compressed and expanded lattices, point defects, grain boundaries, liquids, nanoparticles, all at non-zero temperatures. The aluminum configurations are also compared to existing characterization methods for both speed and accuracy. Our order parameter uniquely classifies every configuration and outperforms all crystalline order parameters studied here, opening the door for its use in a multitude of complex application spaces that can require fine configurational characterization of materials. ","A Physically-informed Graph-based Order Parameter for the Universal
  Characterization of Atomic Structures"
52,1407385863798067204,225120785,Mustafa Ghaleb,"['Check out our new work! \nCalliar,  the first online dataset for Arabic Calligraphy \n\nPaper: <LINK>\nCode &amp; data: <LINK>\nColab: <LINK> <LINK>']",https://arxiv.org/abs/2106.10745,"Calligraphy is an essential part of the Arabic heritage and culture. It has been used in the past for the decoration of houses and mosques. Usually, such calligraphy is designed manually by experts with aesthetic insights. In the past few years, there has been a considerable effort to digitize such type of art by either taking a photo of decorated buildings or drawing them using digital devices. The latter is considered an online form where the drawing is tracked by recording the apparatus movement, an electronic pen for instance, on a screen. In the literature, there are many offline datasets collected with a diversity of Arabic styles for calligraphy. However, there is no available online dataset for Arabic calligraphy. In this paper, we illustrate our approach for the collection and annotation of an online dataset for Arabic calligraphy called Calliar that consists of 2,500 sentences. Calliar is annotated for stroke, character, word and sentence level prediction. ",Calliar: An Online Handwritten Dataset for Arabic Calligraphy
53,1407373611422273536,1195045413570654208,Jordan Ash,"['New paper! We motivate and revisit a classic active learning objective and give a general-purpose algorithm (called BAIT) that makes it viable for neural networks.  Joint work with @SurbhiGoel_ , Akshay Krishnamurthy, and @ShamKakade6! \n\n<LINK>\n@MSFTResearch <LINK>']",https://arxiv.org/abs/2106.09675,"There is an increasing need for effective active learning algorithms that are compatible with deep neural networks. This paper motivates and revisits a classic, Fisher-based active selection objective, and proposes BAIT, a practical, tractable, and high-performing algorithm that makes it viable for use with neural models. BAIT draws inspiration from the theoretical analysis of maximum likelihood estimators (MLE) for parametric models. It selects batches of samples by optimizing a bound on the MLE error in terms of the Fisher information, which we show can be implemented efficiently at scale by exploiting linear-algebraic structure especially amenable to execution on modern hardware. Our experiments demonstrate that BAIT outperforms the previous state of the art on both classification and regression problems, and is flexible enough to be used with a variety of model architectures. ",Gone Fishing: Neural Active Learning with Fisher Embeddings
54,1407353298726817807,16311780,Paul Goldsmith-Pinkham,"['New paper posted on ArXiv: <LINK>', 'Forgot the abstract: https://t.co/CvayTpUbol', '@j_kalla Thanks! It was a crazy time rolling it out (not to mention the comments we saw on the ads...)', '@deaneckles Will have questions for you when we try to write up the spillover stuff', '@deaneckles And thanks!', '@deaneckles Yeah, we definitely deviated on the Covid write-up -- I think we did a pretty poor job pre-specifying what we would need to do in that setting.', ""@deaneckles Issues included a) data quality b) power \n\nwhich I think we would've thought more concretely over if we had had more time in the setup of our PAP"", '@flvcav GlowAd had control over the videos being sent out -- the choice of zip was the main randomized piece. Depending on what zip a person was in (according to facebook), they either received ads on the timeline or did not!']",https://arxiv.org/abs/2106.11012,"During the COVID-19 epidemic, many health professionals started using mass communication on social media to relay critical information and persuade individuals to adopt preventative health behaviors. Our group of clinicians and nurses developed and recorded short video messages to encourage viewers to stay home for the Thanksgiving and Christmas Holidays. We then conducted a two-stage clustered randomized controlled trial in 820 counties (covering 13 States) in the United States of a large-scale Facebook ad campaign disseminating these messages. In the first level of randomization, we randomly divided the counties into two groups: high intensity and low intensity. In the second level, we randomly assigned zip codes to either treatment or control such that 75% of zip codes in high intensity counties received the treatment, while 25% of zip codes in low intensity counties received the treatment. In each treated zip code, we sent the ad to as many Facebook subscribers as possible (11,954,109 users received at least one ad at Thanksgiving and 23,302,290 users received at least one ad at Christmas). The first primary outcome was aggregate holiday travel, measured using mobile phone location data, available at the county level: we find that average distance travelled in high-intensity counties decreased by -0.993 percentage points (95% CI -1.616, -0.371, p-value 0.002) the three days before each holiday. The second primary outcome was COVID-19 infection at the zip-code level: COVID-19 infections recorded in the two-week period starting five days post-holiday declined by 3.5 percent (adjusted 95% CI [-6.2 percent, -0.7 percent], p-value 0.013) in intervention zip codes compared to control zip codes. ","Doctors and Nurses Social Media Ads Reduced Holiday Travel and COVID-19
  infections: A cluster randomized controlled trial in 13 States"
55,1407290330546855941,3376954971,PaoloTozzi,['Cosmic telescopes at work: a new paper by Eros Vanzella on the Sunburst galaxy\n<LINK>'],https://arxiv.org/abs/2106.10280,"We investigate the strongly lensed (\mu x10-100) Lyman continuum (LyC) galaxy, dubbed Sunburst, at z=2.37, taking advantage of a new accurate model of the lens. A characterization of the intrinsic (delensed) properties of the galaxy yields a size of ~3 sq.kpc, a luminosity Muv=-20.3,and a stellar mass M~10^9 Msun;16% of the ultraviolet light is located in a 3 Myr old gravitationally-bound young massive star cluster (YMC) with an effective radius of Re~8 pc and a dynamical mass of ~10^7 Msun (similar to the stellar mass), from which LyC radiation is detected (\lambda < 912A). The inferred outflowing gas velocity (>300 km/s) exceeds the escape velocity of the star cluster. The resulting escape fraction of the ionizing radiation emerging from the Sunburst galaxy is >6-12%, whilst it is >46-93% if inferred from the YMC. 12 additional likely star clusters with 3<Re<20 pc are identified in the galaxy from which we derive a cluster formation efficiency \Gamma>~30%, which is consistent with the high \Gamma derived in local galaxies experiencing extreme gas physical conditions. The presence of the YMC influences the morphology (nucleation), photometry (photometric jumps) and spectroscopic output (nebular emission) of the entire galaxy. The de-lensed LyC and UV (1600A) magnitudes of the YMC are ~30.6 and ~26.9, whilst the galaxy has m1600~24.8. A relatively large rest-frame equivalent width of EWrest(Hb+[OIII]4959-5007)~450A emerges from the galaxy with the YMC contributing to ~30%. If O-type stars are mainly forged in star clusters, then such engines were the key ionizing agents during reionization and the increasing occurrence of high EW lines (Hb+[OIII]) observed at z>6.5 might be an indirect signature of a high \Gamma at reionization.Future facilities (like VLT/MAVIS or ELT), will probe bound clusters on moderately magnified (\mu<5-10) galaxies across cosmic epochs up to reionization[ABRIDGED] ","High star cluster formation efficiency in the strongly lensed Sunburst
  Lyman-continuum galaxy at z=2.37"
56,1407271656351928321,96779364,Arnab Bhattacharyya,"['New paper! ""Identifiability of AMP Chain Graph Models"" (<LINK>) with Yuhao Wang (<LINK>)', ""Gaussian Bayes networks and Gaussian graphical models are widely studied in CS &amp; statistics. Less well-known are chain graphs which 'interpolate' between undirected and directed models. Our paper gives new sufficient conditions for recovering the structure from observations."", 'In the figure below, X1 &amp; X2 form one chain component, X3 &amp; X4 form the other. The (so-called AMP) interpretation is: [X1, X2] = N(0, Œ£‚ÇÅ) while [X3, X4] = [X1, 0] + N(0, Œ£‚ÇÇ) where Œ£‚ÇÅ, Œ£‚ÇÇ are 2-by-2 psd matrices. https://t.co/GIEcqO5MOY', ""So, you can think of a chain graph as a Gaussian Bayes net on groups/vectors of features. This makes sense in causal modeling when some causal (directed) relationships are important while for certain groups of variables, we're okay with just knowing how they're correlated."", 'The question we study is pretty basic in this context: under what conditions can we recover the chain graph structure from observations? More precisely, the goal is to recover the chain components and their topological order.', 'For purely directed models (Bayes nets), there has been a huge amount of work. Just to cite one example, Aragam (@itsrainingdata) and coauthors in https://t.co/Bav2G9rxw7 showed (poly time) identifiability if the residual variance (E[Var(X_j | Pa(X_j))]) is the same at each node.', ""We observe that these conditions extend to imply identifiability of the topological order among the chain components, *if* the components were known. Instead of residual variance, we now look at the determinant of the residual covariance ('generalized variance')."", ""But what if the chain components are not known? Our main contribution is reducing the problem of finding the chain component decomposition to *submodular function minimization*, under some conditions. (This is actually the first time I've used submodularity in my research!üòÉ)"", 'What are these conditions? Informally: they enforce that the variables in a chain component are highly correlated with each other, while each chain component as a whole has high generalized variance conditioned on its parents.', 'We did some experiments also, showing that our algorithm (DCOV) beats other methods from prior work on models satisfying our assumptions. Please see the paper for more details. https://t.co/SarL9fqMHi', 'A side-agenda of our work is to popularize chain graphs. One paragraph in our related work section gives almost a complete history of work done on them! \n\nCan we design other, more general ways to combine causal and correlational information?\n\n#causality #probability #statistics', 'Should have tagged my co-author, @Yohanna49592977.']",https://arxiv.org/abs/2106.09350,"We study identifiability of Andersson-Madigan-Perlman (AMP) chain graph models, which are a common generalization of linear structural equation models and Gaussian graphical models. AMP models are described by DAGs on chain components which themselves are undirected graphs. For a known chain component decomposition, we show that the DAG on the chain components is identifiable if the determinants of the residual covariance matrices of the chain components are monotone non-decreasing in topological order. This condition extends the equal variance identifiability criterion for Bayes nets, and it can be generalized from determinants to any super-additive function on positive semidefinite matrices. When the component decomposition is unknown, we describe conditions that allow recovery of the full structure using a polynomial time algorithm based on submodular function minimization. We also conduct experiments comparing our algorithm's performance against existing baselines. ",Identifiability of AMP chain graph models
57,1407259486352445443,25068244,Ben Chamberlain,"['New paper from Twitter GraphML at #ICML2021 now on arxiv. \nWe develop links between Partial Differential Equations &amp; GNNs -&gt; new GNNs + new theory\nBlog post: <LINK>\nPaper: <LINK>\nCode: <LINK>\n#MachineLearning <LINK>', 'Joint work with @JRowbottom1 @migorinova @stefan_webb @emaros96 and @mmbronstein \nMuch more to do in this space, so please get in touch if interested in collaborating.']",https://arxiv.org/abs/2106.10934,"We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks. ",GRAND: Graph Neural Diffusion
58,1407253350429646848,2322575761,Prof Roberto Trotta,"['üö®Paper day! üö®\nV. excited about introducing Stratified Learning (or StratLearn) - a new, powerful method for supervised learning from a biased (non-representative) training set. #MachineLearning #Cosmology #SNIa  #CovariateShift\n<LINK>\nA thread üëá 1/n', ""The backdrop for our work is the challenge of supernova Type Ia (SNIa) classification from a photometric sample only: a supervised classification problem where the training set is biased wrt the test set due to selection effects (ie, brighter SNIa's are over-represented) 2/n"", 'This leads to poor generalization to the test set - a problem that we previously addressed with the STACCATO approach (https://t.co/BLkE799Zkc), which however requires data augmentation and a validation set 3/n', 'StratLearn is more powerful: it is based on the idea (from causal inference) that propensity scores (i.e., the probability for an object to be selected into the training set) are balancing scores in the case of covariate shift 4/n', 'The idea is simple: subdivide (‚Äústratify‚Äù) target and source data in k subgroups according to quantiles of their propensity scores. Then perform supervised learning in each stratum (‚Äústratified learner‚Äù). This works for both classification and regression, and with any learner 5/n', 'We tested it on the SPCC SNIa classification challenge, increasing the AUC of STACCATO (0.93) to 0.96 (cf a representative sample: 0.977); we compared with previous methods in a galaxy photo-z estimation problem, finding general improvement  6/n', 'We even tried in on UCI wine data, again with strong results. Overall, StratLearn improves the most for the hardest cases, i.e. large covariate shift and large number of potentially confounding covariates. It is a generally applicable tool across domains, simple yet powerful 7/n', 'We are exploring further applications in cosmology, as well as in deep learning training. \n\nHuge üëèüëèüëè to lead author and @ImperialMaths PhD student Max Autenrieth, and as always a pleasure to collaborate with co-authors &amp; statisticians David van Dyk &amp; David Stenning\n/end']",https://arxiv.org/abs/2106.11211,"Covariate shift arises when the labelled training (source) data is not representative of the unlabelled (target) data due to systematic differences in the covariate distributions. A supervised model trained on the source data subject to covariate shift may suffer from poor generalization on the target data. We propose a novel, statistically principled and theoretically justified method to improve learning under covariate shift conditions, based on propensity score stratification, a well-established methodology in causal inference. We show that the effects of covariate shift can be reduced or altogether eliminated by conditioning on propensity scores. In practice, this is achieved by fitting learners on subgroups (""strata"") constructed by partitioning the data based on the estimated propensity scores, leading to balanced covariates and much-improved target prediction. We demonstrate the effectiveness of our general-purpose method on contemporary research questions in observational cosmology, and on additional benchmark examples, matching or outperforming state-of-the-art importance weighting methods, widely studied in the covariate shift literature. We obtain the best reported AUC (0.958) on the updated ""Supernovae photometric classification challenge"" and improve upon existing conditional density estimation of galaxy redshift from Sloan Data Sky Survey (SDSS) data. ","Stratified Learning: a general-purpose statistical method for improved
  learning under Covariate Shift"
59,1407247333499285510,62044012,Michael Bronstein,"['#GNNs are related to PDEs governing information diffusion on graphs. In a new paper with @b_p_chamberlain James Rowbottom @migorinova @stefan_webb @emaros96  we study a new class of Neural Graph Diffusion PDEs\n\nBlog post: <LINK>\n\nPaper: <LINK> <LINK>', 'Thinking of GNNs as partial differential equations leads to a new broad class of GNNs that are able to address in a principled way some of the prominent issues of current Graph ML models such as depth, oversmoothing, bottlenecks, and graph rewiring.', 'Popular GNNs can be formalized as discretized diffusion PDEs with explicit single-step Euler scheme, where an iteration corresponds to a GNN layer, and running the diffusion for multiple iterations amounts to applying a GNN layer multiple times.', 'In Neural PDEs formalism, the diffusion time parameter acts as a continuous analogy of the layers‚Äîan interpretation allowing us to exploit more efficient and stable numerical schemes that use adaptive steps in time.', 'It might be advantageous to decouple the graph used for diffusion from the input graph (or ""rewire"" the graph). The diffusion framework offers a principled view on graph rewiring by considering the graph as a spatial discretization of some continuous object (manifold).', 'In particular, the popular GAT of @PetarV_93 is a nonlinear diffusion PDE with a learnable diffusivity function similar to the Perona-Malik diffusion model used in the 90s in image processing']",https://arxiv.org/abs/2106.10934,"We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks. ",GRAND: Graph Neural Diffusion
60,1407212852394434581,91371808,Amin Husni ‚ùÑ,"['Yay, my new research paper has been published and announced!\n<LINK>']",https://arxiv.org/abs/2106.10432,This study is to investigate and compare the facial recognition accuracy performance of Dlib ResNet against a K-Nearest Neighbour (KNN) classifier. Particularly when used against a dataset from an Asian ethnicity as Dlib ResNet was reported to have an accuracy deficiency when it comes to Asian faces. The comparisons are both implemented on the facial vectors extracted using the Histogram of Oriented Gradients (HOG) method and use the same dataset for a fair comparison. Authentication of a user by facial recognition in an electric vehicle (EV) charging station demonstrates a practical use case for such an authentication system. ,"Neural Network Facial Authentication for Public Electric Vehicle
  Charging Station"
61,1407036878537334794,108925314,Gautham Krishna Gudur,"[""Super glad that our paper 'Zero-Shot Federated Learning with New Classes for Audio Classification' has been accepted at #Interspeech2021! ü•≥\n\narXiv: <LINK>"", '@anirudh_kabi Thank you!']",https://arxiv.org/abs/2106.10019,"Federated learning is an effective way of extracting insights from different user devices while preserving the privacy of users. However, new classes with completely unseen data distributions can stream across any device in a federated learning setting, whose data cannot be accessed by the global server or other users. To this end, we propose a unified zero-shot framework to handle these aforementioned challenges during federated learning. We simulate two scenarios here -- 1) when the new class labels are not reported by the user, the traditional FL setting is used; 2) when new class labels are reported by the user, we synthesize Anonymized Data Impressions by calculating class similarity matrices corresponding to each device's new classes followed by unsupervised clustering to distinguish between new classes across different users. Moreover, our proposed framework can also handle statistical heterogeneities in both labels and models across the participating users. We empirically evaluate our framework on-device across different communication rounds (FL iterations) with new classes in both local and global updates, along with heterogeneous labels and models, on two widely used audio classification applications -- keyword spotting and urban sound classification, and observe an average deterministic accuracy increase of ~4.041% and ~4.258% respectively. ",Zero-Shot Federated Learning with New Classes for Audio Classification
62,1407019963001487366,1674028088,Misha Laskin üá∫üá¶,"['New paper / algo - MABE! We show that combining dynamics models + weighted behavioral priors results in offline RL that is (a) robust across datasets and (b) can transfer behaviors across domains.\n\nPaper: <LINK>\nSite: <LINK>\n\nüßµ 1/8 <LINK>', 'Offline RL has the potential to make RL safer and practical, while model-based methods enable sample efficiency and generalization.\n\nBut offline MBRL relies on uncertainty estimation for conservatism, which is notoriously hard to do accurately, resulting in brittle algos. \n\n2/8 https://t.co/0kXoXMAAyZ', 'We present Offline Model-based RL with Adaptive Behavioral Priors (MABE), an uncertainty-free MBRL approach that regularizes the MBRL policy with an advantage-weighted behavioral prior to keep the MBRL agent near high-return states.  \n\nOn D4RL MABE performs quite well!\n\n3/8 https://t.co/cAWWqBjyBi', 'Since the behavioral prior is adaptive (advantage or reward weighted), MABE excels in datasets of varying skill levels.\n\n4/8 https://t.co/lpbJM8cAQ3', 'We also investigate new capabilities that MABE brings to the table. MBRL methods work well for in-domain generalization, but can we transfer behaviors across domains even if the dynamics are different?\n\nE.g. consider transferring behavior from icy terrain to normal terrain.\n\n5/8 https://t.co/7dKtgglKu4', ""Prior MBRL approaches cannot generalize across domains, because the dynamics don't transfer. By combining dynamics models for in-domain generalization + behavioral priors for cross-domain transfer, MABE can successfully transfer behaviors even if the dynamics are different!\n\n6/8 https://t.co/zBt7599Vpd"", 'Finally, we ablate different components of MABE and even add / remove uncertainty estimation to see how much it contributes. tl;dr is that behavioral priors + offline policy improvement with MBRL contribute most to performance; and uncertainty estimation is not required.\n7/8 https://t.co/u1yjyAtW6H', 'This work was led by a very talented undergrad @catherine_cang and in collaboration with @aravindr93\n+ @pabbeel. \n8/8']",http://arxiv.org/abs/2106.09119,"Offline Reinforcement Learning (RL) aims to extract near-optimal policies from imperfect offline data without additional environment interactions. Extracting policies from diverse offline datasets has the potential to expand the range of applicability of RL by making the training process safer, faster, and more streamlined. We investigate how to improve the performance of offline RL algorithms, its robustness to the quality of offline data, as well as its generalization capabilities. To this end, we introduce Offline Model-based RL with Adaptive Behavioral Priors (MABE). Our algorithm is based on the finding that dynamics models, which support within-domain generalization, and behavioral priors, which support cross-domain generalization, are complementary. When combined together, they substantially improve the performance and generalization of offline RL policies. In the widely studied D4RL offline RL benchmark, we find that MABE achieves higher average performance compared to prior model-free and model-based algorithms. In experiments that require cross-domain generalization, we find that MABE outperforms prior methods. Our website is available at this https URL . ","Behavioral Priors and Dynamics Models: Improving Performance and Domain
  Transfer in Offline RL"
63,1407006793675456523,2431121389,Andrew Mann,"['So new paper from the Young Worlds Lab led by UNC graduate student Mackenna Wood:\n<LINK>\n\nA quick summary in tweets', ""Often we are faced with a situation where we want to know if a star is a binary, but have a mix of data (RVs, Gaia astrometry, AO, etc.). This is common in both stellar groups and transiting planets. Each dataset detects different kinds of binaries, but there's overlap. https://t.co/eaLWiFv9N0"", 'Even if no companion is detected. RVs might miss a face-on orbit, while even deep imaging can miss a companion that happens to be behind the star. We wanted to be able to know what kinds of binaries could be in the data.', 'Enter MOUSC (which is on github), which generates a set of realistic binaries and compares each to the wide set of data. It tells you both how many survived and what kind of companions survived. https://t.co/y5uEzVQU9K', 'It also takes into account constraints from Gaia imaging and RUWE (goodness-of-fit from Gaia). As has been shown in earlier papers, Gaia can detect many close-in companions and a low RUWE rules out a certain range of companions. https://t.co/nLhfcNVWZS', ""We've already been using this code for follow-up of young planets in the THYME survey, which has been particularly useful for false-positive analysis. https://t.co/trwSUpZlTM"", 'The output is currently optimized for single systems and is a bit prior-sensitive. But the code can output likelihoods, so you can combine results from a survey (e.g., in a hierarchical Bayesian model) with some modifications (some we are already planning).', 'Check out the full paper! https://t.co/oxtLCNp6Nc', 'Correction, the code is called ""MOLUSC"" (Multi Observational Limits on Unseen Stellar Companions). Mackenna came up with the acronym and I\'ve never been more proud. https://t.co/9VQTgA4Uw1']",https://arxiv.org/abs/2106.09040,"Binaries play a critical role in the formation, evolution, and fundamental properties of planets, stars, and stellar associations. Observational studies in these areas often include a mix of observations aimed at detecting or ruling out the presence of stellar companions. Rarely can non-detections rule out all possible binary configurations. Here we present MOLUSC, our framework for constraining the range of properties of unseen companions using astrometric, imaging, and velocity information. We showcase the use of MOLUSC on a number of systems, ruling out stellar false positives in the signals of HIP67522b, and DS Tuc Ab. We also demonstrate how MOLUSC could be used to predict the number of missing companions in a stellar sample using the ZEIT sample of young planet hosts. Although our results are not significant, with a larger sample MOLUSC could be used to see if close-in planets are less common in young binary systems, as is seen for their older counterparts. ",Characterizing Undetected Stellar Companions with Combined Datasets
64,1406972445647642629,795318493675712512,Maura Pintor,"['üì¢New paper: ""Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples""\npreprint: <LINK>\n\nw/ @biggiobattista @zangobot @ambrademontis @sotgiu_angelo Nicholas Carlini, Giovanni Manca, Fabio Roli\n\n#advml #debug #MachineLearning', 'TL;DR: unveil and fix common failures in the optimization of gradient-based attacks\n\ncode: https://t.co/QhmRFD0Ndv @secml_py @PRA_Lab', 'Sneak peek. Find out more in the paper!\n\nhttps://t.co/Yrn5RY9E67 https://t.co/QjLdDIRCNq']",https://arxiv.org/abs/2106.09947,"Evaluating robustness of machine-learning models to adversarial examples is a challenging problem. Many defenses have been shown to provide a false sense of security by causing gradient-based attacks to fail, and they have been broken under more rigorous evaluations. Although guidelines and best practices have been suggested to improve current adversarial robustness evaluations, the lack of automatic testing and debugging tools makes it difficult to apply these recommendations in a systematic manner. In this work, we overcome these limitations by (i) defining a set of quantitative indicators which unveil common failures in the optimization of gradient-based attacks, and (ii) proposing specific mitigation strategies within a systematic evaluation protocol. Our extensive experimental analysis shows that the proposed indicators of failure can be used to visualize, debug and improve current adversarial robustness evaluations, providing a first concrete step towards automatizing and systematizing current adversarial robustness evaluations. Our open-source code is available at: this https URL ","Indicators of Attack Failure: Debugging and Improving Optimization of
  Adversarial Examples"
65,1406945031081107457,1002606609527443462,Dr. Angela Collier,"[""New paper from our group: A Lopsided Outer Solar System? <LINK>\n\nMy *small* contribution--introducing Lynden-Bell's bar building mechanism to our group. The 'stellar bar' in the solar system is a lopsided mode. #AstroTwitter""]",https://arxiv.org/abs/2106.09739,"Axisymmetric disks of eccentric orbits in near-Keplerian potentials are unstable to an out-of-plane buckling. Recently, Zderic et al. (2020) showed that an idealized disk saturates to a lopsided mode. Here we show that this apsidal clustering also occurs in a primordial scattered disk in the outer solar system which includes the orbit-averaged gravitational influence of the giant planets. We explain the dynamics using Lynden-Bell (1979)'s mechanism for bar formation in galaxies. We also show surface density and line of sight velocity plots at different times during the instability, highlighting the formation of concentric circles and spiral arms in velocity space. ",A Lopsided Outer Solar System
66,1406918712058433540,1091042849561473031,Alexander Kolesnikov üá∫üá¶,"['New paper <LINK> üßµ : How to train your ViT? It is common to train vision transformers on ImageNet-1k (~1.3m images) for 300 epochs. We show that you are better off investing the same compute budget for training on ImageNet-21k (~13m images) for 30 epochs. <LINK>', 'But this is just one out of many highlights. We also conduct extremely thorough study on the interplay of model size, dataset size, regularizations and augmentations. Plus extensive transfer learning experiments. Check out our paper if you want to learn how to train ViT models. https://t.co/fYX1M2XIny', ""We are sure there are lots of interesting insights to be drawn from our collection of trained ViTs. So we release them all: &gt;50'000 models!\n\nJAX repo: https://t.co/9SocJQaQtb.\nJAX colab: https://t.co/NwIS8nPXHD\n\nOr also available in pytorch through timm: https://t.co/HZie2tyfhQ."", 'Also check out this great thread with the summary of our insights: https://t.co/wrGNnZFuab', 'Work done with stellar Andreas Steiner, @XiaohuaZhai, @wightmanr, @kyosu and @giffmana.']",http://arxiv.org/abs/2106.10270,"Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset. ","How to train your ViT? Data, Augmentation, and Regularization in Vision
  Transformers"
67,1406872771527839746,22419487,Dr Adam Rutherford,"['New paper klaxon! @JenniferRaff @ewanbirney @aylwyn_scally @minouye271 and I have been working on this a while: sparking a conversation about the lexicon of genetics, which continues to utilise scientifically redundant, confusing and racist terminology.\n\n<LINK>', 'We‚Äôre definitely not prescribing or policing language, but want to prompt a dialogue with colleagues in similar and adjacent fields about our terminology m, datasets and tools,  and move towards a lexicon that both serves the science and frees us from a racist past. https://t.co/dsJx7nahxG', 'This is to be a conversation, so please please please let us know what you think. This is a preprint, it is also in with a journal, but this is a community effort to move genetics forward. üß¨', '@matthewcobb @JenniferRaff @ewanbirney @aylwyn_scally @minouye271 Bioarxive don‚Äôt do reviews/perspective pieces.', 'Big thanks to @EimearEKenny @tuuliel @cschlebu @Anthrofuentes who helped a lot in the writing. Please get involved!', '@EimearEKenny @tuuliel @cschlebu @Anthrofuentes Note: * All authors have contributed equally and are presented alphabetically, their order being entirely arbitrary.']",https://arxiv.org/abs/2106.10041,"The language commonly used in human genetics can inadvertently pose problems for multiple reasons. Terms like ""ancestry"", ""ethnicity"", and other ways of grouping people can have complex, often poorly understood, or multiple meanings within the various fields of genetics, between different domains of biological sciences and medicine, and between scientists and the general public. Furthermore, some categories in frequently used datasets carry scientifically misleading, outmoded or even racist perspectives derived from the history of science. Here, we discuss examples of problematic lexicon in genetics, and how commonly used statistical practices to control for the non-genetic environment may exacerbate difficulties in our terminology, and therefore understanding. Our intention is to stimulate a much-needed discussion about the language of genetics, to begin a process to clarify existing terminology, and in some cases adopt a new lexicon that both serves scientific insight, and cuts us loose from various aspects of a pernicious past. ","The language of race, ethnicity, and ancestry in human genetic research"
68,1406820840486887424,268170698,Santi Roca F√†brega,"['New paper from the AGORA Collaboration. For the first time, seven different ¬°codes simulated the formation of a Milky-Way mass galaxy! New code groups are invited to calibrate their codes and join! <LINK> <LINK> @jbprime @JoelPrimack']",https://arxiv.org/abs/2106.09738,"We present a suite of high-resolution cosmological zoom-in simulations to $z=4$ of a $10^{12}\,{\rm M}_{\odot}$ halo at $z=0$, obtained using seven contemporary astrophysical simulation codes widely used in the numerical galaxy formation community. Physics prescriptions for gas cooling, heating, and star formation, are similar to the ones used in our previous {\it AGORA} disk comparison but now account for the effects of cosmological processes. In this work, we introduce the most careful comparison yet of galaxy formation simulations run by different code groups, together with a series of four calibration steps each of which is designed to reduce the number of tunable simulation parameters adopted in the final run. After all the participating code groups successfully completed the calibration steps, we reach a suite of cosmological simulations with similar mass assembly histories down to $z=4$. With numerical accuracy that resolves the internal structure of a target halo, we find that the codes overall agree well with one another in e.g., gas and stellar properties, but also show differences in e.g., circumgalactic medium properties. We argue that, if adequately tested in accordance with our proposed calibration steps and common parameters, the results of high-resolution cosmological zoom-in simulations can be robust and reproducible. New code groups are invited to join this comparison by generating equivalent models by adopting the common initial conditions, the common easy-to-implement physics package, and the proposed calibration steps. Further analyses of the simulations presented here will be in forthcoming reports from our Collaboration. ","The AGORA High-resolution Galaxy Simulations Comparison Project. III:
  Cosmological zoom-in simulation of a Milky Way-mass halo"
69,1406801366819954688,807855012927995904,Artsiom Sanakoyeu,"['Check out our new #CVPR21 paper!\nDiscovering Relationships between Object Categories via Universal Canonical Maps\n\nIn collaboration with FAIR (@NataliaNeverova, P. Labatut, @davnov134 and A. Vedaldi)\n\nüåê<LINK>\n‚ñ∂Ô∏è<LINK>\nüìù<LINK> <LINK>', 'I will present our #CVPR2021 paper:\nToday (21.06) at 11am EDT / 5PM CET. \n\nTL;DR: Densepose for Animals on Steroids &amp; Discovering correspondences between 3D shapes using novel cycle losses.\n\nJoin Q&amp;A https://t.co/XA8VYFp8Df\n\nüåêhttps://t.co/MPRxgSLIFw\nüìùhttps://t.co/xzniLtflQm https://t.co/8kvUF4EXxs']",https://arxiv.org/abs/2106.09758,"We tackle the problem of learning the geometry of multiple categories of deformable objects jointly. Recent work has shown that it is possible to learn a unified dense pose predictor for several categories of related objects. However, training such models requires to initialize inter-category correspondences by hand. This is suboptimal and the resulting models fail to maintain correct correspondences as individual categories are learned. In this paper, we show that improved correspondences can be learned automatically as a natural byproduct of learning category-specific dense pose predictors. To do this, we express correspondences between different categories and between images and categories using a unified embedding. Then, we use the latter to enforce two constraints: symmetric inter-category cycle consistency and a new asymmetric image-to-category cycle consistency. Without any manual annotations for the inter-category correspondences, we obtain state-of-the-art alignment results, outperforming dedicated methods for matching 3D shapes. Moreover, the new model is also better at the task of dense pose prediction than prior work. ","Discovering Relationships between Object Categories via Universal
  Canonical Maps"
70,1406787680168382470,1093387119148462081,Daniel Green,"['New paper today with Tim Cohen, Akhil Premkumar and Alec Ridgway:  ""Stochastic Inflation at NNLO""\n\n<LINK>\n\nIf you have ever wondered what Stochastic Inflation is and what is has to do with QFT in de Sitter space, then this paper is for you', 'The old idea (due to Starobinsky) is intuitive: de Sitter gives you quantum fluctuations (noise) and a classical potential makes them role (drift)\n\nStill I found it confusing:\n(1) Why does SI only include Gaussian noise?\n(2) How would I know to use SI if I started from QFT in dS?', 'What we show is that SI is the equation for RG flow: it is describes the mixing of operators.  The original set of equation are just the leading order anomalous dimensions / beta functions.  If you have ever calculated anomalous dimensions you will recognize some of our diagrams https://t.co/ObkF9uV6ew', 'We find two kinds of corrections: corrections to the effective potential (higher powers of the field) and, for the first time, the appearance of non-Gaussian noise (higher derivatives).  \n\nThis is the full equation for SI in \\lambda phi^4 at next-to-next-to leading order (NNLO) https://t.co/MrCfvugNn5', 'My take-away: we can now (in principle) understand light fields in de Sitter at the same level we understand QFT in four dimensions.  Everything follows from dimensional analysis and we know what diagrams to calculate to get the precise numbers.\n\nNext up: loops in inflation', '@JazayeriSadra Yes, I think so.  I hope to have a paper out soon on that case.  Most diagrams involving the metric aren‚Äôt actually important since they are derivatively coupled.  This isn‚Äôt obvious in most ways of calculating, but is manifest in SdSET', '@EKuflik @geller_mic For conventional models of slow roll inflation, the corrections will be small (except perhaps on the tails of the distribution) and are unlikely to change the qualitative picture.  In models with derivative interactions / primordial non-Gaussianity, the corrections could matter', '@REasther I‚Äôm not sure we have anything new to add for this question, other than to say that we understand the limitations of the equations better.  Eventually the uphill evolution balances the potential and you get equilibrium, like running to a non trivial fixed point in RG']",https://arxiv.org/abs/2106.09728,"Stochastic Inflation is an important framework for understanding the physics of de Sitter space and the phenomenology of inflation. In the leading approximation, this approach results in a Fokker-Planck equation that calculates the probability distribution for a light scalar field as a function of time. Despite its successes, the quantum field theoretic origins and the range of validity for this equation have remained elusive, and establishing a formalism to systematically incorporate higher order effects has been an area of active study. In this paper, we calculate the next-to-next-to-leading order (NNLO) corrections to Stochastic Inflation using Soft de Sitter Effective Theory (SdSET). In this effective description, Stochastic Inflation manifests as the renormalization group evolution of composite operators. The leading impact of non-Gaussian quantum fluctuations appears at NNLO, which is presented here for the first time; we derive the coefficient of this term from a two-loop anomalous dimension calculation within SdSET. We solve the resulting equation to determine the NNLO equilibrium distribution and the low-lying relaxation eigenvalues. In the process, we must match the UV theory onto SdSET at one-loop order, which provides a non-trivial confirmation that the separation into Wilson-coefficient corrections and contributions to initial conditions persists beyond tree level. Furthermore, these results illustrate how the naive factorization of time and momentum integrals in SdSET no longer holds in the presence of logarithmic divergences. It is these effects that ultimately give rise to the renormalization group flow that yields Stochastic Inflation. ",Stochastic Inflation at NNLO
71,1406778126659559425,767659609,Yoshihiko Hasegawa,"['My new paper ""Thermodynamic uncertainty relation for quantum first passage process via Loschmidt echo"" appeared in <LINK>.', 'This is the accompanying paper of https://t.co/rKXL9CsdA3.']",https://arxiv.org/abs/2106.09870,"We derive a thermodynamic uncertainty relation for first passage processes in quantum Markov chains. We consider first passage processes that stop after a fixed number of jumps, which contrasts with typical quantum Markov chains which end at a fixed time. We obtain bounds for the observables of the first passage processes in quantum Markov chains by the Loschmidt echo, which quantifies the extent of irreversibility in quantum many-body systems. Considering a particular case, we show that the lower bound corresponds to the quantum Fisher information, which plays a fundamental role in uncertainty relations in quantum systems. Moreover, considering classical dynamics, our bound reduces to a thermodynamic uncertainty relation for classical first passage processes. ",Thermodynamic uncertainty relation for quantum first passage process
72,1406700277651197958,1326543298622754817,Rodrigo Mira,"['Ever wonder how video can leverage audio as a learning objective? Check out our new paper (accepted at Interspeech 2021) titled ""LiRA: Learning Visual Speech Representations from Audio through Self-supervision"" <LINK>']",https://arxiv.org/abs/2106.09171,"The large amount of audiovisual content being shared online today has drawn substantial attention to the prospect of audiovisual self-supervised learning. Recent works have focused on each of these modalities separately, while others have attempted to model both simultaneously in a cross-modal fashion. However, comparatively little attention has been given to leveraging one modality as a training objective to learn from the other. In this work, we propose Learning visual speech Representations from Audio via self-supervision (LiRA). Specifically, we train a ResNet+Conformer model to predict acoustic features from unlabelled visual speech. We find that this pre-trained model can be leveraged towards word-level and sentence-level lip-reading through feature extraction and fine-tuning experiments. We show that our approach significantly outperforms other self-supervised methods on the Lip Reading in the Wild (LRW) dataset and achieves state-of-the-art performance on Lip Reading Sentences 2 (LRS2) using only a fraction of the total labelled data. ","LiRA: Learning Visual Speech Representations from Audio through
  Self-supervision"
73,1406217438367264770,1319333124086648834,A. Tuan Nguyen,"['New paper <LINK> on Domain Adaptation, with Toan Tran, @yaringal, Phil Torr, and @atilimgunes [1/n]', ""We propose a generalization bound of a model's loss on the target domain, based on the training loss and the reverse KL divergence between the source and target distributions. Different from some existing bounds in the literature, our bound works for all cases of ... [2/n]"", 'supervised learning, makes no assumptions about the labeling mechanism, works with virtually all predictive distributions commonly used in practice, and thus works with all common loss functions (cross-entropy, squared error, l1). [3/n]', 'Based on the bound, we propose an algorithm to minimize the KL term to improve the generalization performance. Different from other distance metrics, the KL divergence can be estimated with samples, leading to an efficient and stable alignment technique. More importantly,...[4/n]', 'the reverse KL has zero-forcing and mode-seeking effects, which allow for a flexible alignment between the domains, while still efficiently prevent out-of-distribution data at test time. Experiments show that our method outperforms other marginal alignment techniques. [n/n]']",https://arxiv.org/abs/2106.07780,"Domain adaptation is an important problem and often needed for real-world applications. In this problem, instead of i.i.d. training and testing datapoints, we assume that the source (training) data and the target (testing) data have different distributions. With that setting, the empirical risk minimization training procedure often does not perform well, since it does not account for the change in the distribution. A common approach in the domain adaptation literature is to learn a representation of the input that has the same (marginal) distribution over the source and the target domain. However, these approaches often require additional networks and/or optimizing an adversarial (minimax) objective, which can be very expensive or unstable in practice. To improve upon these marginal alignment techniques, in this paper, we first derive a generalization bound for the target loss based on the training loss and the reverse Kullback-Leibler (KL) divergence between the source and the target representation distributions. Based on this bound, we derive an algorithm that minimizes the KL term to obtain a better generalization to the target domain. We show that with a probabilistic representation network, the KL term can be estimated efficiently via minibatch samples without any additional network or a minimax objective. This leads to a theoretically sound alignment method which is also very efficient and stable in practice. Experimental results also suggest that our method outperforms other representation-alignment approaches. ",KL Guided Domain Adaptation
74,1405947328868106249,921726522582593536,Daniel Watson,"['Check out our new paper on diffusion models! For any pre-trained DDPM, one can *optimize* the inference path. If we optimize the ELBO, turns out we can do so with dynamic programming and get ‚â§0.1 extra BPD with 32 steps v.s. the usual 1k/4k steps :)\n\n<LINK>']",https://arxiv.org/abs/2106.03802,"Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a powerful family of generative models that can yield high-fidelity samples and competitive log-likelihoods across a range of domains, including image and speech synthesis. Key advantages of DDPMs include ease of training, in contrast to generative adversarial networks, and speed of generation, in contrast to autoregressive models. However, DDPMs typically require hundreds-to-thousands of steps to generate a high fidelity sample, making them prohibitively expensive for high dimensional problems. Fortunately, DDPMs allow trading generation speed for sample quality through adjusting the number of refinement steps as a post process. Prior work has been successful in improving generation speed through handcrafting the time schedule by trial and error. We instead view the selection of the inference time schedules as an optimization problem, and introduce an exact dynamic programming algorithm that finds the optimal discrete time schedules for any pre-trained DDPM. Our method exploits the fact that ELBO can be decomposed into separate KL terms, and given any computation budget, discovers the time schedule that maximizes the training ELBO exactly. Our method is efficient, has no hyper-parameters of its own, and can be applied to any pre-trained DDPM with no retraining. We discover inference time schedules requiring as few as 32 refinement steps, while sacrificing less than 0.1 bits per dimension compared to the default 4,000 steps used on ImageNet 64x64 [Ho et al., 2020; Nichol and Dhariwal, 2021]. ",Learning to Efficiently Sample from Diffusion Probabilistic Models
75,1405876304235405314,1325108931199438848,EckfordLab,['Will we ever get tired of the EM algorithm? We will not.\n\nNew paper in arXiv: \n\n<LINK>'],https://arxiv.org/abs/2106.09594,"We derive a factor graph EM (FGEM) algorithm, a technique that permits combined parameter estimation and statistical inference, to determine hidden kinetic microstates from patch clamp measurements. Using the cystic fibrosis transmembrane conductance regulator (CFTR) and nicotinic acetylcholine receptor (nAChR) as examples, we perform {\em Monte Carlo} simulations to demonstrate the performance of the algorithm. We show that the performance, measured in terms of the probability of estimation error, approaches the theoretical performance limit of maximum {\em a posteriori} estimation. Moreover, the algorithm provides a reliability score for its estimates, and we demonstrate that the score can be used to further improve the performance of estimation. We use the algorithm to estimate hidden kinetic states in lab-obtained CFTR single channel patch clamp traces. ","A factor graph EM algorithm for inference of kinetic microstates from
  patch clamp measurements"
76,1405835479833092097,65432023,Prof. Costas Andreopoulos,"['New paper highlighting recent developments in the well known #GENIE #neutrino interaction simulation, prepared for a special issue of European Physical Journal ST. A longer and much more comprehensive paper on GENIE3 is in the works #nuxsec  <LINK>']",https://arxiv.org/abs/2106.09381,"The release of GENIE v3.0.0 was a major milestone in the long history of the GENIE project, delivering several alternative comprehensive neutrino interaction models, improved charged-lepton scattering simulations, a range of beyond the Standard Model simulation capabilities, improved experimental interfaces, expanded core framework capabilities, and advanced new frameworks for the global analysis of neutrino scattering data and tuning of neutrino interaction models. Steady progress continued following the release of GENIE v3.0.0. New tools and a large number of new physics models, comprehensive model configurations, and tunes have been made publicly available and planned for release in v3.2.0. This article highlights some of the most recent technical and physics developments in the GENIE v3 series. ",Recent highlights from GENIE v3
77,1405819247574540290,1275184138664976384,Vittorio Ferrari,"['üì¢ New paper üì¢\nShape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving Objects\n\nWe reconstruct the 3D shape, texture, and motion of an object from a single motion-blurred image\n\n@DRozumnyi Martin Oswald @mapo1\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2106.08762,"We address the novel task of jointly reconstructing the 3D shape, texture, and motion of an object from a single motion-blurred image. While previous approaches address the deblurring problem only in the 2D image domain, our proposed rigorous modeling of all object properties in the 3D domain enables the correct description of arbitrary object motion. This leads to significantly better image decomposition and sharper deblurring results. We model the observed appearance of a motion-blurred object as a combination of the background and a 3D object with constant translation and rotation. Our method minimizes a loss on reconstructing the input image via differentiable rendering with suitable regularizers. This enables estimating the textured 3D mesh of the blurred object with high fidelity. Our method substantially outperforms competing approaches on several benchmarks for fast moving objects deblurring. Qualitative results show that the reconstructed 3D mesh generates high-quality temporal super-resolution and novel views of the deblurred object. ","Shape from Blur: Recovering Textured 3D Shape and Motion of Fast Moving
  Objects"
78,1405731090476716035,926000798970073088,Efthymios Tzinis,"['I am happy to announce that the new version of AudioScope is online ü•≥üì¢! We obtained some impressive results for on-screen sound separation tasks simply by watching in-the-wild videos! Please check our paper with @ScottTWisdom here: <LINK> <LINK>', 'We extended the AudioScope model (https://t.co/BuTSVUuTeJ) towards these axes:\n- We generalize to a wider set of videos for both training and testing.\n- Audio-visual self and cross-modal attention from low level features boost the performance and esp. on higher frame rates.', '- Pretraining the sound separation model helps significantly to provide more stable pseudo-labels for the audio-visual coincidence classifier.\n- Freezing the embedding networks also helps the model to prevent overfitting and leverage vast amounts of in-the-wild videos.', 'We introduce efficient separable versions of our self and cross-modal attention blocks which capture dependencies between each estimated source waveform and the input video! First perform self-attention across time and then attend across the second axis (space and/or sources)! https://t.co/Wgh9RXTqOh']",https://arxiv.org/abs/2106.09669,"We introduce a state-of-the-art audio-visual on-screen sound separation system which is capable of learning to separate sounds and associate them with on-screen objects by looking at in-the-wild videos. We identify limitations of previous work on audio-visual on-screen sound separation, including the simplicity and coarse resolution of spatio-temporal attention, and poor convergence of the audio separation model. Our proposed model addresses these issues using cross-modal and self-attention modules that capture audio-visual dependencies at a finer resolution over time, and by unsupervised pre-training of audio separation model. These improvements allow the model to generalize to a much wider set of unseen videos. We also show a robust way to further improve the generalization capability of our models by calibrating the probabilities of our audio-visual on-screen classifier, using only a small amount of in-domain videos labeled for their on-screen presence. For evaluation and semi-supervised training, we collected human annotations of on-screen audio from a large database of in-the-wild videos (YFCC100m). Our results show marked improvements in on-screen separation performance, in more general conditions than previous methods. ","Improving On-Screen Sound Separation for Open-Domain Videos with
  Audio-Visual Self-Attention"
79,1405709527664652288,1185977761032110080,Kazumasa Ohno (Â§ßÈáé ÂíåÊ≠£),"['New paper is out! <LINK>\nWe discuss about Jupiter formation from the planet‚Äôs atmospheric composition. We propose that proto-Jupiter may be formed in the cold ‚Äúshadow‚Äù of protosolar disk to explain enriched nitrogen and noble gases in the atmosphere.', 'The abundances of many heavy elements are ~3 solar in Jovian atmosphere. Super-solar abundances can come from the dissolution of solids (e.g., pebbles); however, it is puzzling why hyper volatiles (N and noble gases), hardly locked into solids, are enriched with the same degree.', 'Recent studies suggested that the enriched hyper-volatiles indicate that proto-Jupiter was formed at cold &gt;30AU region where hyper volatiles can freeze. However, this idea has a disadvantage that contradicts to planet migration theory and isotope dichotomy of meteorites.', ""So, we propose an alternative idea. The current Jupiter orbit may be shadowed in the past. Due to the lack of direct sunlight, the shadow may create extremely cold environments even near the current Jupiter's orbit, leading to cause the freezing of hyper-volatiles."", 'We consider the disk in which the dust is accumulated inside the H2O snow line. Such structure can originate from the different fragmentation threshold velocity of silicate and icy grains. The accumulated dust may cast the shadow behind the snow line, near the Jupiter orbit.', 'The shadowing effect is drastic. When the degree of dust pileup at H2O snow line is high enough (f_SL&lt;0.03), even 5AU region can be as cold as &lt;30K! I thank Takahiro, coauthor and my former colleague, for his radiative transfer calculations. https://t.co/zw4JS7gUp0', 'I then performed the condensation calculations and found that relevant hyper-volatiles (N and noble gases) can freeze around the current Jupiter orbit. Such volatile distributions are considerably different from those in non-shadowed disks which we usually assume. https://t.co/ThJ5fBu7k7', ""We obtained excellent matches between observed and model predicted elemental abundances, even for in-situ formation! Interestingly, in our shadow scenario, N and noble gases may be depleted in Saturnian atmosphere, as the Saturn's orbit is close to the outer edge of the shadow. https://t.co/TRZzqHQI3X"", 'To sum, the enriched N and noble gases may imply that proto-Jupiter was formed in the shadowed region. The scenario can be compatible with other constrains. The current paper did not explicitly simulate the Jupiter formation, which should be done in future studies.', 'Although the paper mainly focuses on Solar System giants, the shadowing effect would also affect how to interpret the elemental abundance ratios of exoplanetary atmospheres, which will be delivered by upcoming JWST. https://t.co/6Qqrw33xTe', 'Btw, this is my first paper that does not use the word of ""cloud"" and ""haze""! Doing the research related to planet formation was one of my dream.\nI came up the idea of this paper about an year ago, and happy that I can finally share it as a paper.']",https://arxiv.org/abs/2106.09084,"Atmospheric compositions offer valuable clues to planetary formation and evolution. Jupiter has been the most well-studied giant planet in terms of its atmosphere; however, the origin of the Jovian atmospheric composition remains a puzzle as the abundances of nitrogen and noble gases as high as those of other elements could only originate from extremely cold environments. We propose a novel idea for explaining the Jovian atmospheric composition: Dust pileup at the H$_2$O snow line casts a shadow and cools the Jupiter orbit so that N$_2$ and noble gases can freeze. Planetesimals or a core formed in the shadowed region can enrich nitrogen and noble gases as much as other elements through their dissolution in the envelope. We compute the temperature structure of a shadowed protosolar disk with radiative transfer calculations. Then, we investigate the radial volatile distributions and predict the atmospheric composition of Jupiter with condensation calculations. We find that the vicinity of the current Jupiter orbit, approximately $3$--$7~{\rm AU}$, could be as cold as $30~{\rm K}$ if the small-dust surface density varies by a factor of $\gtrsim30$ across the H$_2$O snow line. According to previous grain growth simulations, this condition could be achieved by weak disk turbulence if silicate grains are more fragile than icy grains. The shadow can cause the condensation of most volatile substances, namely N$_2$ and Ar. We demonstrate that the dissolution of shadowed solids can explain the elemental abundance patterns of the Jovian atmosphere even if proto-Jupiter was formed near Jupiter's current orbit. The disk shadow may play a vital role in controlling atmospheric compositions. The effect of the shadow also impacts the interpretation of upcoming observations of exoplanetary atmospheres by JWST. ","Jupiter's ""Cold"" Formation in the Protosolar Disk Shadow: An Explanation
  for the Planet's Uniformly Enriched Atmosphere"
80,1405632239791230984,769279457387540480,Han Guo,"['Excited to share our latest work with Bowen Tan @waterluffy Eric Xing @ZhitingHu!\nTldr, a new NLG formulation from soft Q-learning perspective, with app. such as learning from noisy data, text attacks, prompt generation.\nPaper <LINK>\nCode <LINK> <LINK>', 'MLE is the predominant algorithm for training text generation models. It relies on direct supervision examples, which is not applicable to many applications. RL on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. 2/5 https://t.co/SWcavKKx1J', 'Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. 3/5', 'We introduce a new RL formulation for text generation from the soft Q-learning perspective. It further enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. 4/5', 'We apply the approach to a wide range of tasks, including learning from noisy/negative examples, adversarial attacks, prompt generation, and training standard supervised text generation from scratch. 5/5', '@_Yuchen_Li_ @waterluffy @ZhitingHu Thanks Yuchen!']",https://arxiv.org/abs/2106.07704,"Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more flexible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inefficient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective. It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation. Experiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods. ",Text Generation with Efficient (Soft) Q-Learning
81,1405601565831053316,2923392199,Hannah Eyre,"['MedspaCy finally has a paper! \n\n""Launching into clinical space with medspaCy: a new clinical text processing toolkit in Python"" at #AMIA2021\n\nCongrats to @abchapman93 @burgersmoke and others at @SLC_IDEAS, @vahsrd, and @UofUEpi\n\n<LINK>\n\n@spacy_io #NLProc', '@abchapman93 @burgersmoke @SLC_IDEAS @vahsrd @UofUEpi @spacy_io spacy 3.x update to medspaCy coming very soon as well https://t.co/zNni1TtNuf']",https://arxiv.org/abs/2106.07799,"Despite impressive success of machine learning algorithms in clinical natural language processing (cNLP), rule-based approaches still have a prominent role. In this paper, we introduce medspaCy, an extensible, open-source cNLP library based on spaCy framework that allows flexible integration of rule-based and machine learning-based algorithms adapted to clinical text. MedspaCy includes a variety of components that meet common cNLP needs such as context analysis and mapping to standard terminologies. By utilizing spaCy's clear and easy-to-use conventions, medspaCy enables development of custom pipelines that integrate easily with other spaCy-based modules. Our toolkit includes several core components and facilitates rapid development of pipelines for clinical text. ","Launching into clinical space with medspaCy: a new clinical text
  processing toolkit in Python"
82,1405575447006617603,3100596960,Walter Scheirer,['New paper in arXiv: On the Objective Evaluation of Post Hoc Explainers. <LINK>\n\nExplainable AI techniques are important when transparency of decisions is needed in high-stakes environments. But how well do they work? Our framework can help answer that question. <LINK>'],https://arxiv.org/abs/2106.08376,"Many applications of data-driven models demand transparency of decisions, especially in health care, criminal justice, and other high-stakes environments. Modern trends in machine learning research have led to algorithms that are increasingly intricate to the degree that they are considered to be black boxes. In an effort to reduce the opacity of decisions, methods have been proposed to construe the inner workings of such models in a human-comprehensible manner. These post hoc techniques are described as being universal explainers - capable of faithfully augmenting decisions with algorithmic insight. Unfortunately, there is little agreement about what constitutes a ""good"" explanation. Moreover, current methods of explanation evaluation are derived from either subjective or proxy means. In this work, we propose a framework for the evaluation of post hoc explainers on ground truth that is directly derived from the additive structure of a model. We demonstrate the efficacy of the framework in understanding explainers by evaluating popular explainers on thousands of synthetic and several real-world tasks. The framework unveils that explanations may be accurate but misattribute the importance of individual features. ",A Framework for Evaluating Post Hoc Feature-Additive Explainers
83,1405565545693560840,1938536035,Huy V. Vo,"['Our new paper ""Large-Scale Unsupervised Object Discovery"" is on arxiv: <LINK>. We propose to use ranking methods for object discovery and show that our approach scales better than the baselines while yielding state-of-the-art results on COCO ... <LINK>', '... and the large OpenImages dataset with 1.7M images.']",https://arxiv.org/abs/2106.06650,"Existing approaches to unsupervised object discovery (UOD) do not scale up to large datasets without approximations that compromise their performance. We propose a novel formulation of UOD as a ranking problem, amenable to the arsenal of distributed methods available for eigenvalue problems and link analysis. Through the use of self-supervised features, we also demonstrate the first effective fully unsupervised pipeline for UOD. Extensive experiments on COCO and OpenImages show that, in the single-object discovery setting where a single prominent object is sought in each image, the proposed LOD (Large-scale Object Discovery) approach is on par with, or better than the state of the art for medium-scale datasets (up to 120K images), and over 37% better than the only other algorithms capable of scaling up to 1.7M images. In the multi-object discovery setting where multiple objects are sought in each image, the proposed LOD is over 14% better in average precision (AP) than all other methods for datasets ranging from 20K to 1.7M images. Using self-supervised features, we also show that the proposed method obtains state-of-the-art UOD performance on OpenImages. Our code is publicly available at this https URL ",Large-Scale Unsupervised Object Discovery
84,1405552874164269062,786855300322172928,Alkistis Pourtsidou,"[""Today's new @EC_Euclid paper led by St√©phane Iliƒá shows that the joint analysis of Euclid and CMB data can improve constraints by ~an order of magnitude wrt Euclid-only (for LCDM and extensions). <LINK>""]",https://arxiv.org/abs/2106.08346,"The combination and cross-correlation of the upcoming $Euclid$ data with cosmic microwave background (CMB) measurements is a source of great expectation since it will provide the largest lever arm of epochs, ranging from recombination to structure formation across the entire past light cone. In this work, we present forecasts for the joint analysis of $Euclid$ and CMB data on the cosmological parameters of the standard cosmological model and some of its extensions. This work expands and complements the recently published forecasts based on $Euclid$-specific probes, namely galaxy clustering, weak lensing, and their cross-correlation. With some assumptions on the specifications of current and future CMB experiments, the predicted constraints are obtained from both a standard Fisher formalism and a posterior-fitting approach based on actual CMB data. Compared to a $Euclid$-only analysis, the addition of CMB data leads to a substantial impact on constraints for all cosmological parameters of the standard $\Lambda$-cold-dark-matter model, with improvements reaching up to a factor of ten. For the parameters of extended models, which include a redshift-dependent dark energy equation of state, non-zero curvature, and a phenomenological modification of gravity, improvements can be of the order of two to three, reaching higher than ten in some cases. The results highlight the crucial importance for cosmological constraints of the combination and cross-correlation of $Euclid$ probes with CMB data. ","$Euclid$ preparation: XV. Forecasting cosmological constraints for the
  $Euclid$ and CMB joint analysis"
85,1405452410466807809,986338982,Johannes Buchner,"['With a new paper <LINK> presenting the first X-ray spectra for radiation-pressure generated AGN obscurer geometries like this one ... <LINK>', '... this ups the number of consistent X-ray to infrared spectral models to four: https://t.co/9zXR7Je4Ae  . @NuSTAR_Science is able to differentiate these models. https://t.co/8Mw0w0mwv9']",https://arxiv.org/abs/2106.08331,"The nuclear obscurer of Active Galactic Nuclei (AGN) is poorly understood in terms of its origin, geometry and dynamics. We investigate whether physically motivated geometries emerging from hydro-radiative simulations can be differentiated with X-ray reflection spectroscopy. For two new geometries, the radiative fountain model of Wada (2012) and a warped disk, we release spectral models produced with the ray tracing code XARS. We contrast these models with spectra of three nearby AGN taken by NuSTAR and Swift/BAT. Along heavily obscured sight-lines, the models present different 4-20keV continuum spectra. These can be differentiated by current observations. Spectral fits of the Circinus Galaxy favor the warped disk model over the radiative fountain, and clumpy or smooth torus models. The necessary reflector (NH>10^25/cm^2) suggests a hidden population of heavily Compton-thick AGN amongst local galaxies. X-ray reflection spectroscopy is a promising pathway to understand the nuclear obscurer in AGN. ",Physically motivated X-ray obscurer models
86,1405443744082665474,824763554,Mark Goodsell,"['New paper (<LINK>) today with Lakshmi Priya on recasting long lived particle searches, specifically for particles that ""disappear"" inside the detector! It took so much effort that I had to give it a silly title.', 'The code is hosted at (https://t.co/hoaPkE61Yk and https://t.co/RqII7mdSa5). I should also acknowledge @agbuckley and @jonmbutterworth for heputils/mcutils and YODA codes which are *very* useful!']",https://arxiv.org/abs/2106.08815,"We describe a new code and approach using particle-level information to recast the recent CMS disappearing track searches including all run 2 data. Notably, the simulation relies on knowledge of the detector geometry, and we also include the simulation of pileup events directly rather than as an efficiency function. We validate it against provided acceptances and cutflows, and use it in combination with heavy stable charged particle searches to place limits on winos with any proper decay length above a centimetre. We also provide limits for a simple model of a charged scalar that is only produced in pairs, that decays to electrons plus an invisible fermion. ",Long Dead Winos
87,1405434239978459140,216729597,Marcel S. Pawlowski,"['New paper on the arXiv today, lead by @astro_delgado with @Javanmardi_B and @Giusepp39181130. We looked at the dwarf galaxy system surrounding the Sculptor galaxy NGC 253, motivated by the discovery of three new dwarfs in the area.\n\n<LINK> <LINK> <LINK>', ""Many of the dwarfs around NGC 253 are distributed along a preferred direction, towards the north of the galaxy. Five of these galaxies have measured velocities, and four of them follow a common trend. Could this be an edge-on satellite plane? That's maybe rotating? https://t.co/rNyVBcnE1g"", 'Looking at the 3D position of confirmed galaxies around NGC 253 reveals their distribution is oriented edge-on to us, and is indeed more flattened than typical. Not highly significant (too few objects with good distance), but enough to motivate further attention and follow-up. https://t.co/WPyrnEHeOe', 'Interestingly, the orientation also aligns with the supergalactic plane and the local tidal tensor of the surrounding larger-scale structure, following the preference @satellitegalaxy found for such dwarf galaxy structures. https://t.co/IqenOHVb9A', 'So, is this a satellite plane? Possible. To be certain we need more &amp; better distance measurements, also for the new objects. And more spectroscopic velocities to judge the kinematic correlation. Given the extent of 600 kpc, it might also be related to a cosmic filament or sheet.', 'Or, and this is could be the most exciting possibility, are we maybe witnessing a satellite plane in formation from preferred accretion directions of dwarfs along of the cosmic web?']",https://arxiv.org/abs/2106.08868,"In the last years, a new generation of large-scale imaging surveys have probed wide field regions around some nearby galaxies at unprecedented low surface brightness regime (~28.0-29.0 mag arcsec^-2). This offers a chance of discovering very faint dwarf satellites by means of visual inspection of these public deep images. We report the first results of a systematic survey of faint dwarf spheroidal galaxies in the vicinity of the bright late-type spiral NGC 253 galaxy by means of a visual inspection of the images taken by the Dark Energy Survey. Three new dwarf galaxies have been discovered in the vicinity of the brightest member of the Sculptor filament, the late-type spiral NGC 253. Assuming they are companions of NGC 253, their total absolute V-magnitudes fall in the -7 to -9 mag range, which is typical for dwarf satellites in the local Universe. The central surface brightness tend to be extremely low for all the discovered dwarfs and fall roughly in the range of 25-26 mag arcsec^-2 in g-band. Using known data on distances and velocities of galaxies, we estimate the total virial mass of the NGC 253 group to be 8 x 10^11 Mo, which gives the virial radius R_200 = 186 kpc and the turn-around radius of 706 kpc. We also discuss the possible existence of a spatially flattened and velocity-correlated satellite system around NGC 253. This large-scale structure is orientated almost edge-on to line of sight. The possible plane of satellites is only 31 kpc thick with the minor-to-major axis ratio of 0.14. Four out of five galaxies with measured velocities follow a common velocity trend similar to those observed in the planes of satellites around the Andromeda and Centaurus A galaxies. However, the small number of galaxies with known velocities prevents to reach a definitive conclusion about the formation scenario of the structure and its possible relation to the surrounding cosmic web. ","Tracing satellite planes in the Sculptor group: I. Discovery of three
  faint dwarf galaxies around NGC 253"
88,1405416030055370753,804069495253962752,David Mart√≠nez Delgado,"['In this new paper, we report the discovery of three faint dwarf galaxies found by @Giusepp39181130 around the spiral NGC 253. This is the first paper of our project to search for a satellite plane in the Sculptor group in collaboration with @8minutesold (<LINK>). <LINK>']",http://arxiv.org/abs/2106.08868,"In the last years, a new generation of large-scale imaging surveys have probed wide field regions around some nearby galaxies at unprecedented low surface brightness regime (~28.0-29.0 mag arcsec^-2). This offers a chance of discovering very faint dwarf satellites by means of visual inspection of these public deep images. We report the first results of a systematic survey of faint dwarf spheroidal galaxies in the vicinity of the bright late-type spiral NGC 253 galaxy by means of a visual inspection of the images taken by the Dark Energy Survey. Three new dwarf galaxies have been discovered in the vicinity of the brightest member of the Sculptor filament, the late-type spiral NGC 253. Assuming they are companions of NGC 253, their total absolute V-magnitudes fall in the -7 to -9 mag range, which is typical for dwarf satellites in the local Universe. The central surface brightness tend to be extremely low for all the discovered dwarfs and fall roughly in the range of 25-26 mag arcsec^-2 in g-band. Using known data on distances and velocities of galaxies, we estimate the total virial mass of the NGC 253 group to be 8 x 10^11 Mo, which gives the virial radius R_200 = 186 kpc and the turn-around radius of 706 kpc. We also discuss the possible existence of a spatially flattened and velocity-correlated satellite system around NGC 253. This large-scale structure is orientated almost edge-on to line of sight. The possible plane of satellites is only 31 kpc thick with the minor-to-major axis ratio of 0.14. Four out of five galaxies with measured velocities follow a common velocity trend similar to those observed in the planes of satellites around the Andromeda and Centaurus A galaxies. However, the small number of galaxies with known velocities prevents to reach a definitive conclusion about the formation scenario of the structure and its possible relation to the surrounding cosmic web. ","Tracing satellite planes in the Sculptor group: I. Discovery of three
  faint dwarf galaxies around NGC 253"
89,1405339450914349057,69202541,Jonathan Le Roux,"['New paper out, accepted @INTERSPEECH2021: ""Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition,"" by Yosuke Higuchi, Niko Moritz, JL, Takaaki Hori\nMPL is a simple, efficient, stable, and scalable strategy for semi-supervised ASR. (1/3)\n<LINK>', 'MPL trains a pair of online/offline models that interact+learn from each other, inspired by mean teacher method. Online model is trained to predict PLs generated on the fly by the offline model. Offline model maintains a momentum-based moving average of the online model. (2/3)', 'We apply MPL to an end-to-end ASR model based on CTC, and show it effectively improves over PL and iterative PL on semi-supervised tasks based on LibriSpeech &amp; TEDLIUM 3. (3/3) https://t.co/IHc6aRSmCb', '@mhnt1580 @INTERSPEECH2021 Thanks! The ideas do look very similar indeed. I guess it was ""in the air"", especially given recent work on IPL and BYOLüòÑ']",https://arxiv.org/abs/2106.08922,"Pseudo-labeling (PL) has been shown to be effective in semi-supervised automatic speech recognition (ASR), where a base model is self-trained with pseudo-labels generated from unlabeled data. While PL can be further improved by iteratively updating pseudo-labels as the model evolves, most of the previous approaches involve inefficient retraining of the model or intricate control of the label update. We present momentum pseudo-labeling (MPL), a simple yet effective strategy for semi-supervised ASR. MPL consists of a pair of online and offline models that interact and learn from each other, inspired by the mean teacher method. The online model is trained to predict pseudo-labels generated on the fly by the offline model. The offline model maintains a momentum-based moving average of the online model. MPL is performed in a single training process and the interaction between the two models effectively helps them reinforce each other to improve the ASR performance. We apply MPL to an end-to-end ASR model based on the connectionist temporal classification. The experimental results demonstrate that MPL effectively improves over the base model and is scalable to different semi-supervised scenarios with varying amounts of data or domain mismatch. ",Momentum Pseudo-Labeling for Semi-Supervised Speech Recognition
90,1405339012081012737,22148802,Leo C. Stein ü¶Å,"['üö® New paper day! üö®\nNumerical renormalization group-based approach to secular perturbation theory \n-&gt; <LINK> &lt;- read here\n\nThis paper took waay longer than expected, because, you know, pandemic and all that.\n\nA thread üßµ, 1/? <LINK>', 'The term ""secular effects"" in math/physics refers to something small that builds up slowly over a long time, eventually creating a big effect. The example closest to my heart is that gravitational waves are tiny, but GW emission makes a binary eventually inspiral and merge. 2/?', 'Secular effects often spell trouble for perturbation theory, and special techniques are required to tame them. One classic approach is the method of multiple scales. See @stevenstrogatz lecture on it here: https://t.co/dlSH2enmch.\n3/?', 'A more modern approach, due to @NigelGoldenfeld and collaborators, is called the ""dynamical renormalization group"" or DRG. The DRG is able to do what a variety of specialized methods (multiple scales, boundary layers, etc.) could do, but in one unified framework. 4/?', 'The catch is, usually you need analytical mathematical expressions to use DRG! But sometimes we *have* to use numerics (for example, when simulating black hole mergers). But what if we want to apply the DRG to that problem? 5/?', ""That's what Jos√© Tom√°s G√°lvez Ghersi and I worked on in this latest paper. We showed how you *can* use the DRG, even when you only have numerical solutions!\n\nWhat's the key insight of our paper? Glad you asked, read on!\n6/?"", 'The key insight is this. Use naive perturbation theory, and you find a solution that is growing with time, unbounded. But the *details* of this unbounded growth encodes what are called ""beta functions"" for the dynamical renormalization group. Model this and extract Œ≤ funcs! 7/? https://t.co/1M2YoCOjGb', 'We applied this to the classic Korteweg-de Vries equation, which has soliton solutions. When you add the small term with an epsilon seen below, you get dissipation and the solitons slow down and decay. A secular effect! Naive perturbation theory fails here. 8/? https://t.co/KY9gDOERxx', 'You can see naive perturbation theory failing in this plot: what\'s supposed to be a ""small correction"" to the soliton is growing without bound!\n\nBut, we can successfully extract the beta function from this growing solution. Then, we can reconstruct a ""renormalized"" solution! 9/? https://t.co/KoAyeEl1tB', ""In the end, the renormalized solution and the full solution have bounded velocity/amplitude/width differences between each other.\n\nOk, we compared against an equation where we could get the full solution anyway. But! This method works for situations when you *can't*. 10/? https://t.co/Pxwj9nWNRY"", ""We hope to apply this to the problem of black hole mergers in theories beyond general relativity. Most of the time, we can't solve the full equations on the computer. But, we *can* solve the perturbative equations (and then they exhibit secular growth without bound).\n11/?"", 'Sorry this got a bit technical and niche. Anyway, hope this got some people interested in our paper [https://t.co/NxC4yoc97t]! Of course feel free to ask any questions, whether about the paper, or perturbation theory, or the KdV equation, or black holes.\n\n12/12', '@lepton939 @stevenstrogatz Lol where did you find this video of me, taken while we were trying to figure out how to make the calculations work?', '@adamsolo Surprise!', '@adamsolo I hope so!', ""@stevenstrogatz Thanks, Steve. If I understand anything in perturbation theory, it's because I've learned from some great teachers ‚Äî and I count you among the greats!"", ""@PartonWavicle Yes, that's right. I know next to nothing about condensed matter physics üôà and this is the first time I've heard of the NRG! How embarrassing"", ""@CJHaster Yup, that's the idea!"", ""@SturnioloSimone I did not! I know embarrassingly little condensed matter physics. What's the secular behavior in the systems your thinking about?"", '@Sharond53100400 Always!', ""@Mat_Hunt We cited a few that we're familiar with. Lots of examples in Chen, Goldenfeld, and Oono. I also like the treatment by Kunuhiro, which is almost like the formulation that we gave. Also lots of examples in Kunuhiro's review."", ""@SturnioloSimone I don't really see why DRG would help with the problem of exponentially large Hilbert space... it sounds like you have some domain-specific intuition if you see that, because I don't understand the connection!"", ""@SturnioloSimone Maybe one of these days I'll actually learn some condensed matter physics...""]",https://arxiv.org/abs/2106.08410,"Perturbation theory is a crucial tool for many physical systems, when exact solutions are not available, or nonperturbative numerical solutions are intractable. Naive perturbation theory often fails on long timescales, leading to secularly growing solutions. These divergences have been treated with a variety of techniques, including the powerful dynamical renormalization group (DRG). Most of the existing DRG approaches rely on having analytic solutions up to some order in perturbation theory. However, sometimes the equations can only be solved numerically. We reformulate the DRG in the language of differential geometry, which allows us to apply it to numerical solutions of the background and perturbation equations. This formulation also enables us to use the DRG in systems with background parameter flows, and therefore, extend our results to any order in perturbation theory. As an example, we apply this method to calculate the soliton-like solutions of the Korteweg-de Vries equation deformed by adding a small damping term. We numerically construct DRG solutions which are valid on secular time scales, long after naive perturbation theory has broken down. ","Numerical renormalization group-based approach to secular perturbation
  theory"
91,1405230885901897731,57284479,Matthias Minderer,"['New paper: Revisiting the Calibration of Modern Neural Networks (<LINK>). We studied the calibration of MLP-Mixer, Vision Transformers, BiT, and many others. Non-convolutional models are doing surprisingly well! 1/5 <LINK>', 'Lots of analyses in the paper. Some highlights: In-distribution calibration slightly deteriorates with increasing model size, but this is outweighed by a simultaneous improvement in accuracy. 2/5', 'Under distribution shift, calibration improves with model size. This reverses the trend for in-distribution data (!). Accuracy and calibration are correlated under distribution shift: optimizing for accuracy may also benefit calibration. 3/5', 'Model size, pretraining duration, and pretraining dataset size cannot fully explain differences in calibration properties. This suggests that architecture is a major determinant of calibration properties. 4/5', 'Work with Josip Djolonga, @robromijnders, Frances Hubis, @XiaohuaZhai, @neilhoulsby, @dustinvtran, and @MarioLucic_ 5/5', '@KAlexanderWang Additional differences between the families in the bottom left emerge after temperature scaling, so batch norm is likely not the whole story, but it could definitely play a role.']",https://arxiv.org/abs/2106.07998,"Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties. ",Revisiting the Calibration of Modern Neural Networks
92,1405158221074022401,95683474,Pablo Samuel Castro,"['Very happy to share our MICo paper!\n\nWe present a new behavioural distance over the state space of an MDP, and show how it can shape the learnt representations of deep RL agents.\n\nPaper: <LINK>\nBlog: <LINK>\nCode: <LINK>\n\n1/üßµ <LINK>', 'Bisimulation metrics are nice metrics b/c they bound differences in V*.\n\nBut they use the Kantorovich/Wasserstein distance b/w next-state distributions, making them suffer from 3 limitations:\n1. Very expensive\n2. Biased under sampling\n3. No connection to non-optimal œÄ\n\n2/X https://t.co/vAzCCXvvMI', ""We introduce MICo (Matching under Independent Couplings) to address these limitations.\nWe replace the expensive optimal coupling term (e.g. the Kantorovich) with an independent coupling and define it on-policy.\n\nDef'd via update operator below, we prove it has a fixed pt:\n\n3/X https://t.co/KbB8I4ijAk"", 'The independent coupling and on-policy def. means that MICo:\n1. Reduces computational complexity of bisimulation metrics by a factor of O(|X| |A|)\n2. Converges to fixed point via sampling\n3. Is connected to more policies than just œÄ*\n\n4/X https://t.co/ZwfR2cumtM', 'It turns out MICo is *not* a pseudo-metric. It\'s a different type of distance which we coin as a ""Diffuse metric"".\nDiffuse metrics differ from standard metrics in that they allow **non-zero self-distances**! üò±\n\n5/X https://t.co/okXr4LZZXA', 'What does it mean to have non-zero self-distance?? ü§î\n\nIt arises from a point being ""spread"" across a probability distribution: In deterministic MDPs you *do* have zero self-distances!\n\nPlay with the interactive widget in the blog post to learn more!\nhttps://t.co/KPe2vtNaJ9\n\n6/X https://t.co/cWzKE17LrJ', 'How do we use this for deep RL?\n\nWe define the distance between representations √∏(x), √∏(y) to approximate the MICo fixed point.\nThe average of the norms of the representations enable this approximant to have non-zero self-distances (empirically, this proved important).\n\n7/X https://t.co/OkUQgxGLpx', 'Our learning target and loss are then defined as below. Notice the similarity with TD-learning.\n\nEmpirically, it is important to use the Huber loss to minimize the loss, as we care more about differentiating smaller distances than about larger distances.\n\n8/X https://t.co/RyMCbu9lzu', 'During learning, we re-use batches (of size N) sampled from the replay buffer and build an ""all-pairs"" batch of size N^2 to compute pairwise distances.\n\n9/X https://t.co/XMw275GNEO', ""Although our MICo approximant can admit non-zero self-distances, when we use √∏(x) for control, it is now a vector in Euclidean space (so zero self-distances).\n\nIt turns out that we're effectively using a **projection** of our learned distances.\n\n10/X https://t.co/Dg90CjMVlp"", ""Are these projections useful at all??\n\nWe unfortunately lose the upper bound on value differences. üò™\n\nBut in practice we're ok. We evaluate the value bound gaps on hundreds of random MDPs and find that the upper bound is _mostly_ respected, and tighter than bisimulation!\n\n11/X https://t.co/CpioFscEpK"", 'Do the projections yield good state features?\n\nWe compute projected-MICo exactly on some GridWorlds and use UMAP to project them to lower-dim features. Then we do linear regression to the true V^œÄ and measure error.\n\nProjected-MICo FTW!\n\n12/X https://t.co/xm1BaaEVbc', 'What about with deep nets? We evaluate this on the ALE benchmark.\n\nWe add our MICo loss to all the Dopamine agents as well as Munchausen-IQN and... ü•Å\n\nThe MICo loss improves performance on *all* the agents! ü•≥üéâ\n\n13/X https://t.co/cqIhlL2WJj', 'Note we are applying our MICo loss *directly* on the representations √∏(x) (i.e. no extra parameters): it explicitly shapes the repr. used by Q-learning!\n\nAFAIK, this is the first time a metric loss like this has been successfully applied in such a direct way &amp; at scale.\n14/X', ""Fantastic collaboration with Tyler Kastner, Prakash Panangaden (my PhD supervisor!), and Mark Rowland.\n\nLet me know if you'd like to chat more about this work!\n\n15/15"", ""On a personal note, MICo was a bit of a backronym, as it's the nickname my brother @micokoch had growing up üêµ\n\nCoda/15 https://t.co/JXD4zqJKgN""]",http://arxiv.org/abs/2106.08229,"We present a new behavioural distance over the state space of a Markov decision process, and demonstrate the use of this distance as an effective means of shaping the learnt representations of deep reinforcement learning agents. While existing notions of state similarity are typically difficult to learn at scale due to high computational cost and lack of sample-based algorithms, our newly-proposed distance addresses both of these issues. In addition to providing detailed theoretical analysis, we provide empirical evidence that learning this distance alongside the value function yields structured and informative representations, including strong results on the Arcade Learning Environment benchmark. ","MICo: Improved representations via sampling-based state similarity for
  Markov decision processes"
93,1405133911437365254,2394625597,Tarun Souradeep,"['Confirming (at 5sigma)  our mad rush thru the Cosmos  (at mind-boggling 369 km/s) with @Planck \nFirst paper of new graduate student, @RocksSaha at @IiserPphysics , after moving to @IISERPune.  Continuing fruitful  collaboration with @bwandelt \n<LINK>', ""Although largely believed to be a well known fact reflected and measured as dipole in Cosmic Microwave background (CMB), this work closes further the loophole of an unknown intrinsic  dipole of primordial origin. Also questions other  'anomalous' estimates from other observations"", 'https://t.co/WXNyrGdBkm']",https://arxiv.org/abs/2106.07666?context=astro-ph,"The largest fluctuation in the CMB sky is the CMB dipole, which is believed to be caused by the motion of our observation frame with respect to the CMB rest frame. This motion accounts for the known motion of the Solar System barycentre with a best-fit amplitude of $369$ km/s, in the direction ($\ell= 264^\circ$, $b=48^\circ$) in galactic coordinates. Along with the CMB dipole signal, this motion also causes an inevitable signature of statistical anisotropy in the higher multipoles due to the modulation and aberration of the CMB temperature and polarization fields. This leads to a correlation between adjacent CMB multipoles causing a non-zero value of the off-diagonal terms in the covariance matrix which can be captured in terms of the dipolar spectra of the bipolar spherical harmonics (BipoSH). In our work, we jointly infer the CMB power spectrum and the BipoSH spectrum in a Bayesian framework using the $\textit{Planck}$-2018 $\texttt{SMICA}$ temperature map. We detect amplitude and direction of the local motion consistent with the canonical value $v=369$ km/s inferred from CMB dipole with a statistical significance of $4.54\sigma$, $4.97\sigma$ and $5.23\sigma$ respectively from the masked temperature map with the available sky fraction $40.1\%$, $59.1\%$, and $72.2\%$, confirming the common origin of both the signals. The Bayes factor in favor of the canonical value is between $7$ to $8$ depending on the choice of mask. But it strongly disagrees (by a value of the Bayes factor about $10^{-10}-10^{-11}$) with a higher value of local motion which one can infer from the amplitude of the dipole signal obtained from the CatWISE2020 quasar catalog using the WISE and NEOWISE data set. ","Bayesian estimation of our local motion from the Planck-2018 CMB
  temperature map"
94,1405111167274688514,2656302854,Ronald Drimmel üá∫üá¶,"['Really pleased beyond words to announce this paper, the first for a new project called #GaiaUnlimited, on deriving and applying a survey selection function, where we derive the color-luminosity function of white dwarfs from #GaiaEDR3 as a worked example.\n<LINK> <LINK>', 'I am but a lowly co-author. Paper led by Hans-Walter Rix and @davidwhogg. Group includes @astrowizicist, @mfouesneau, @neuronomer, Andrew Everall, Anthony Brown and @adrianprw.', ""@vicgrinberg Well, wasn't planning to..""]",https://arxiv.org/abs/2106.07653,"Statistical studies of astronomical data sets, in particular of cataloged properties for discrete objects, are central to astrophysics. One cannot model those objects' population properties or incidences without a quantitative understanding of the conditions under which these objects ended up in a catalog or sample, the sample's selection function. As systematic and didactic introductions to this topic are scarce in the astrophysical literature, we aim to provide one, addressing generically the following questions: What is a selection function? What arguments $\vec{q}$ should a selection function depend on? Over what domain must a selection function be defined? What approximations and simplifications can be made? And, how is a selection function used in `modelling'? We argue that volume-complete samples, with the volume drastically curtailed by the faintest objects, reflect a highly sub-optimal selection function that needlessly reduces the number of bright and usually rare objects in the sample. We illustrate these points by a worked example, deriving the space density of white dwarfs (WD) in the Galactic neighbourhood as a function of their luminosity and Gaia color, $\Phi_0(M_G,B-R)$ in [mag$^{-2}$pc$^{-3}$]. We construct a sample of $10^5$ presumed WDs through straightforward selection cuts on the Gaia EDR3 catalog, in magnitude, color, parallax, and astrometric fidelity $\vec{q}=(m_G,B-R,\varpi,p_{af})$. We then combine a simple model for $\Phi_0$ with the effective survey volume derived from this selection function $S_C(\vec{q})$ to derive a detailed and robust estimate of $\Phi_0(M_G,B-R)$. This resulting white dwarf luminosity-color function $\Phi_0(M_G,B-R)$ differs dramatically from the initial number density distribution in the luminosity-color plane: by orders of magnitude in density and by four magnitudes in density peak location. ","Selection Functions in Astronomical Data Modeling, with the Space
  Density of White Dwarfs as Worked Example"
95,1405100087555104768,302547719,Craig Glastonbury,"['Our new paper is now out on Arxiv! <LINK> Contrastive Mixture of Posteriors for Counterfactual Inference, Data Integration and Fairness. A project started by Adam Foster a fantastic intern last year at @benevolent_ai who continued to work with us on this.', 'In the paper we demonstrate that counterfactual inference, batch correction, data integration and learning fair representations in a CVAE framework, can all be seen as learning representations that are conditionally independent of a covariate.', 'Previous methods have tried to tackle this problem, such as the CVAE, FairVAE &amp; TrVAE from @fabian_theis lab. Whilst the CVAE does condition on a known label (c), either the encoder or decoder can choose to ignore this, leading to a latent space that separates on condition (c)', 'TrVAE and FairVAE do better than CVAE by introducing an MMD penalty. For CoMP, inspired by the VaMP prior and contrastive learning, we remove the need for any external discrepancy metric (e.g MMD) and use mixtures of the variational posterior alone, demonstrating better mixing! https://t.co/2FhCSLCYUy', 'We apply CoMP to three problems:\n\n1. Aligning cancer cell lines with tumors (RNA-seq data - Celligner problem). CoMP can align latent representations of cancer cell lines and their tumour equivalents. CoMP preserves tumour type and subtype clustering and has better sensitivity. https://t.co/09N22aDq5U', '2. Counterfactual inference.\nWe use PBMC scRNA-seq data treated and untreated with IFNg to ask: ""What would an untreated cell look like if perturbed with IFNg?"" CoMP successfully infers this - Whilst other methods overestimate underexpression or underestimate over expression. https://t.co/bntzSJP4Ri', '3. Fair representation learning \nWhilst solving these biological problems, we noticed parallels with fair representation learning and demonstrate that we can learn fair representations of biased income data that are invariant to Sex, yet still expressive (able to predict income). https://t.co/BHoLqhR1fC', ""The paper has many other neat findings including nice theoretical results demonstrating that CoMP is actually equivalent to an upper bound on a weighted sum of KL-divergences p(c)KL[q(z|c) | q(z|c')].\nCheck it out: https://t.co/7WSnrz8Qvf"", '@jmtomczak VamP prior getting some love.']",https://arxiv.org/abs/2106.08161,"Learning meaningful representations of data that can address challenges such as batch effect correction and counterfactual inference is a central problem in many domains including computational biology. Adopting a Conditional VAE framework, we show that marginal independence between the representation and a condition variable plays a key role in both of these challenges. We propose the Contrastive Mixture of Posteriors (CoMP) method that uses a novel misalignment penalty defined in terms of mixtures of the variational posteriors to enforce this independence in latent space. We show that CoMP has attractive theoretical properties compared to previous approaches and we prove counterfactual identifiability of CoMP under additional assumptions. We demonstrate state of the art performance on a set of challenging tasks including aligning human tumour samples with cancer cell-lines, predicting transcriptome-level perturbation responses, and batch correction on single-cell RNA sequencing data. We also find parallels to fair representation learning and demonstrate that CoMP is competitive on a common task in the field. ","Contrastive Mixture of Posteriors for Counterfactual Inference, Data
  Integration and Fairness"
96,1405081031984754690,1270706519689084930,Stylianos I. Venieris,"['DNN-based super-resolution (SR) can be key in pushing the QoE across visual content deliver systems. Our new paper in ACM #CSUR presents the roadblocks, key techniques to overcome them and future avenues.\nPreprint: <LINK>\nThanks to Royson Lee and @niclane7 <LINK>']",https://arxiv.org/abs/2106.03727,"Internet-enabled smartphones and ultra-wide displays are transforming a variety of visual apps spanning from on-demand movies and 360{\deg} videos to video-conferencing and live streaming. However, robustly delivering visual content under fluctuating networking conditions on devices of diverse capabilities remains an open problem. In recent years, advances in the field of deep learning on tasks such as super-resolution and image enhancement have led to unprecedented performance in generating high-quality images from low-quality ones, a process we refer to as neural enhancement. In this paper, we survey state-of-the-art content delivery systems that employ neural enhancement as a key component in achieving both fast response time and high visual quality. We first present the components and architecture of existing content delivery systems, highlighting their challenges and motivating the use of neural enhancement models as a countermeasure. We then cover the deployment challenges of these models and analyze existing systems and their design decisions in efficiently overcoming these technical challenges. Additionally, we underline the key trends and common approaches across systems that target diverse use-cases. Finally, we present promising future directions based on the latest insights from deep learning research to further boost the quality of experience of content delivery systems. ","Deep Neural Network-based Enhancement for Image and Video Streaming
  Systems: A Survey and Future Directions"
97,1405025466763915268,1173723962,Haque Ishfaq,"['Check out our new #ICML2021 paper on how to unify the general principle of optimism with Thompson sampling style exploration method for RL. \n<LINK>\nJoint work with Qiwen Cui, @defo_not_gpt3, Alex Ayoub, @zhuoran_yang, @zhaoran_wang, Doina Precup and @lyang36. <LINK>']",https://arxiv.org/abs/2106.07841,"We propose a model-free reinforcement learning algorithm inspired by the popular randomized least squares value iteration (RLSVI) algorithm as well as the optimism principle. Unlike existing upper-confidence-bound (UCB) based approaches, which are often computationally intractable, our algorithm drives exploration by simply perturbing the training data with judiciously chosen i.i.d. scalar noises. To attain optimistic value function estimation without resorting to a UCB-style bonus, we introduce an optimistic reward sampling procedure. When the value functions can be represented by a function class $\mathcal{F}$, our algorithm achieves a worst-case regret bound of $\widetilde{O}(\mathrm{poly}(d_EH)\sqrt{T})$ where $T$ is the time elapsed, $H$ is the planning horizon and $d_E$ is the $\textit{eluder dimension}$ of $\mathcal{F}$. In the linear setting, our algorithm reduces to LSVI-PHE, a variant of RLSVI, that enjoys an $\widetilde{\mathcal{O}}(\sqrt{d^3H^3T})$ regret. We complement the theory with an empirical evaluation across known difficult exploration tasks. ","Randomized Exploration for Reinforcement Learning with General Value
  Function Approximation"
98,1405002518367772677,538186192,Ramanakumar Sankar,['My new paper is out on arxiv: <LINK> ! We did some simulations of the 24deg N jet on Jupiter to study the convective plumes and associated cloud features that have been observed.'],https://arxiv.org/abs/2106.07809,"The $24^{\circ}$ N jet borders the North Tropical Belt and North Tropical Zone, and is the fastest prograde jet on Jupiter, reaching speeds above $170$ m/s. In this region, observations have shown several periodic convective plumes, likely from latent heat release from water condensation, which affect the cloud and zonal wind structure of the jet. We model this region with the Explicit Planetary hybrid-Isentropic Coordinate model using its active microphysics scheme to study the phenomenology of water and ammonia clouds within the jet region. On perturbing the atmosphere, we find that an upper tropospheric wave develops that directly influences the cloud structure within the jet. This wave travels at $\sim75$ m/s in our model, and leads to periodic chevron-shaped features in the ammonia cloud deck. These features travel with the wave speed, and are subsequently much slower than the zonal wind at the cloud deck. The cloud structure, and the slower drift rate, were both observed following the convective outbreak in this region in 2016 and 2020. We find that an upper level circulation is responsible for these cloud features in the aftermath of the convective outbursts. The comparatively slower observed drift rates of these features, relative to the wind speed of the jet, provides constraints on the vertical wind shear above the cloud tops, and we suggest that wind velocities determined from cloud tracking should correspond to a different altitude compared to the $680$ hPa pressure level. We also diagnose the convective potential of the atmosphere due to water condensation, and find that it is strongly coupled to the wave. ","The aftermath of convective events near Jupiter's fastest prograde jet:
  implications for clouds, dynamics and vertical wind shear"
99,1404994827251556352,2956121356,Russ Salakhutdinov,"['New work on Self-supervised Representation Learning for speech recognition: matching/surpassing SOTA approaches for speech recognition, generation, and compression.\n\nPaper: <LINK>\n\nwith W. Hsu, B. Bolte, H. Tsai, K. Lakhotia, A. Mohamed\n\n<LINK>']",https://arxiv.org/abs/2106.07447,"Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets. ","HuBERT: Self-Supervised Speech Representation Learning by Masked
  Prediction of Hidden Units"
100,1404902019169587206,606388721,Dida Markoviƒç,"['Exciting new paper by Peter Taylor that makes it possible to directly cross-correlate RSD x WL, which opens up amazing possibilities for the next-gen of cosmological surveys! (Full disclosure: am co-authorüòã.) \n@EC_Euclid  @NASARoman @desisurvey \n<LINK> <LINK>']",https://arxiv.org/abs/2106.05293,"Future data sets will enable cross-correlations between redshift space distortions (RSD) and weak lensing (WL). While photometric lensing and clustering cross-correlations have provided some of the tightest cosmological constraints to date, it is not well understood how to optimally perform similar RSD/WL joint analyses in a lossless way. RSD is typically measured in $3D$ redshift space, but WL is inherently a projected signal, making angular statistics a natural choice for the combined analysis. Thus, we determine the amount of RSD information that can be extracted using projected statistics. Specifically we perform a Fisher analysis to forecast constraints and model bias comparing two different Fingers-of-God (FoG) models using both, the $3D$ power spectrum, $P(k, \mu)$, and tomographic $C(\ell)$. We find that because na\""ive tomographic projection mixes large scales with poorly modelled nonlinear radial modes, it does not provide competitive constraints to the $3D$ RSD power spectrum without the model bias becoming unacceptably large. This is true even in the limit of narrow tomographic bins. In light of this we propose a new radial weighting scheme which unmixes radial RSD scales in projection yielding competitive constraints to the $3D$ RSD power spectrum, while keeping the model bias small. This work lays the groundwork for optimal joint analyses of RSD and cosmic shear. ",The RSD Sorting Hat: Unmixing Radial Scales in Projection
101,1404813501219151887,90131577,Noam Slonim üü¢,"['#KeyPointAnalysis by #ProjectDebater team @IBMResearch can summarize numerous short texts to a short list of Key Points + quantify each KP prevalence in the data. Our new ACL paper led by @RoyBarHaim shows the value of KPA to summarize reviews data <LINK> (1/2)', 'If you are interested in #KeyPointAnalysis, please see also our KPA #SharedTask @ArgMining_2021 @emnlpmeeting -- looking very much forward to your submissions #NLP https://t.co/MSGxK31r92 (2/2)']",https://arxiv.org/abs/2106.06758,"Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary. These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views. Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data. We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata. We show empirically that these novel extensions of KPA substantially improve its performance. We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement. ",Every Bite Is an Experience: Key Point Analysis of Business Reviews
102,1404723090811244544,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper about using video games to teach quantum computing (with game from @QuarksInteract1 ), including some data from game trials <LINK> work done with @helencramman and @Lauramsmith as well as Laur who runs QI and Gulsah Dost; done @JQCDurNew and @DurhamQlm']",https://arxiv.org/abs/2106.07077,"With a vast domain of applications and now having quantum computing hardware available for commercial use, an education challenge arises in getting people of various background to become quantum literate. Quantum Odyssey is a new piece of computer software that promises to be a medium where people can learn quantum computing without any previous requirements. It aims to achieve this through visual cues and puzzle play, without requiring the user to possess a background in computer coding or even linear algebra, which are traditionally a must to work on quantum algorithms. In this paper we report our findings on an UKRI Citizen Science grant that involves using Quantum Odyssey to teach how to construct quantum computing algorithms. Sessions involved 30 minutes of play, with 10 groups of 5 students, ranging between 11 to 18 years old, in two schools in the UK. Results show the Quantum Odyssey visual methods are efficient in portraying counterintuitive quantum computational logic in a visual and interactive form. This enabled untrained participants to quickly grasp difficult concepts in an intuitive way and solve problems that are traditionally given today in Masters level courses in a mathematical form. The results also show an increased interest in quantum physics after play, a higher openness and curiosity to learn the mathematics behind computing on quantum systems. Participants developed a visual, rather than mathematical intuition, that enabled them to understand and correctly answer entry level technical quantum information science. ","Inclusive learning for quantum computing: supporting the aims of quantum
  literacy using the puzzle game Quantum Odyssey"
103,1404677215363731459,2425754287,Roman Orus,['New paper out with the amazing team at @MultiverseQC showing quantum optimization of portfolios implementing investment bands and target volatilities... for all the assets in S&amp;P500!!  <LINK>'],http://arxiv.org/abs/2106.06735,"In this paper we show how to implement in a simple way some complex real-life constraints on the portfolio optimization problem, so that it becomes amenable to quantum optimization algorithms. Specifically, first we explain how to obtain the best investment portfolio with a given target risk. This is important in order to produce portfolios with different risk profiles, as typically offered by financial institutions. Second, we show how to implement individual investment bands, i.e., minimum and maximum possible investments for each asset. This is also important in order to impose diversification and avoid corner solutions. Quite remarkably, we show how to build the constrained cost function as a quadratic binary optimization (QUBO) problem, this being the natural input of quantum annealers. The validity of our implementation is proven by finding the optimal portfolios, using D-Wave Hybrid and its Advantage quantum processor, on portfolios built with all the assets from S&P100 and S&P500. Our results show how practical daily constraints found in quantitative finance can be implemented in a simple way in current NISQ quantum processors, with real data, and under realistic market conditions. In combination with clustering algorithms, our methods would allow to replicate the behaviour of more complex indexes, such as Nasdaq Composite or others, in turn being particularly useful to build and replicate Exchange Traded Funds (ETF). ","Quantum Portfolio Optimization with Investment Bands and Target
  Volatility"
104,1404606932367949828,2680275613,Haydee,"['New paper just dropped: ""Stable trace ideals and applications"" with coauthor Hailong Dao.\n\n<LINK>', '@MarissaKawehi Thanks much!']",http://arxiv.org/abs/2106.07064,We study stable trace ideals in one dimensional local Cohen-Macaulay rings and give numerous applications. ,Stable trace ideals and applications
105,1404543388486094853,147615935,Niko Grupen,"['üö®New preprintüö®\n\nDoes mutual reward yield fair outcomes for cooperative teams? We find this is not the case! Teams learn capitalistic strategies, achieving high reward by distributing it *unequally* across teammates.\n\nPaper: <LINK>\n\n#AI #FairAI #RL', 'We connect prediction-based fairness to multi-agent learning and introduce Fairness through Equivariance (Fair-E) -- a method that ensures fair outcomes for multi-agent teams through equivariant policy learning.', 'We also introduce Fairness through Equivariance Regularization (Fair-ER) as a soft-constraint version of equivariant policy learning and show that it allows us to modulate between fairness and utility. https://t.co/MpyBho8v8m']",https://arxiv.org/abs/2106.05727,"We study fairness through the lens of cooperative multi-agent learning. Our work is motivated by empirical evidence that naive maximization of team reward yields unfair outcomes for individual team members. To address fairness in multi-agent contexts, we introduce team fairness, a group-based fairness measure for multi-agent learning. We then prove that it is possible to enforce team fairness during policy optimization by transforming the team's joint policy into an equivariant map. We refer to our multi-agent learning strategy as Fairness through Equivariance (Fair-E) and demonstrate its effectiveness empirically. We then introduce Fairness through Equivariance Regularization (Fair-ER) as a soft-constraint version of Fair-E and show that it reaches higher levels of utility than Fair-E and fairer outcomes than non-equivariant policies. Finally, we present novel findings regarding the fairness-utility trade-off in multi-agent settings; showing that the magnitude of the trade-off is dependent on agent skill. ",Cooperative Multi-Agent Fairness and Equivariant Policies
106,1404489923256721414,326521133,Gustavo Machado,"['A new preprint is out! Led by Abagael, one of my graduate students, she is killing it. Check the vignette if you are interested in the latest ML local models\n<LINK>\n\npaper here! \n<LINK>', 'Follow her at @AbbySykesML']",https://arxiv.org/abs/2106.06506,"Effective biosecurity practices in swine production are key in preventing the introduction and dissemination of infectious pathogens. Ideally, biosecurity practices should be chosen by their impact on bio-containment and bio-exclusion, however quantitative supporting evidence is often unavailable. Therefore, the development of methodologies capable of quantifying and ranking biosecurity practices according to their efficacy in reducing risk have the potential to facilitate better informed choices. Using survey data on biosecurity practices, farm demographics, and previous outbreaks from 139 herds, a set of machine learning algorithms were trained to classify farms by porcine reproductive and respiratory syndrome virus status, depending on their biosecurity practices, to produce a predicted outbreak risk. A novel interpretable machine learning toolkit, MrIML-biosecurity, was developed to benchmark farms and production systems by predicted risk, and quantify the impact of biosecurity practices on disease risk at individual farms. Quantifying the variable impact on predicted risk 50% of 42 variables were associated with fomite spread while 31% were associated with local transmission. Results from machine learning interpretations identified similar results, finding substantial contribution to predicted outbreak risk from biosecurity practices relating to: the turnover and number of employees; the surrounding density of swine premises and pigs; the sharing of trailers; distance from the public road; and production type. In addition, the development of individualized biosecurity assessments provides the opportunity to guide biosecurity implementation on a case-by-case basis. Finally, the flexibility of the MrIML-biosecurity toolkit gives it potential to be applied to wider areas of biosecurity benchmarking, to address weaknesses in other livestock systems and industry relevant diseases. ","Interpretable machine learning applied to on-farm biosecurity and
  porcine reproductive and respiratory syndrome virus"
107,1404419248609505285,759118366468481024,Decker French,['New paper day! <LINK> I hope this review will be useful especially to students and people starting new projects in this field. <LINK>'],https://arxiv.org/abs/2106.05982,"Post-starburst (or ""E+A"") galaxies trace the fastest and most dramatic processes in galaxy evolution. Recent work studying the evolution of galaxies through this phase have revealed insights on how galaxies undergo structural and stellar population changes as well as the role of various feedback mechanisms. In this review, I summarize recent work on identifying post-starburst galaxies; tracing the role of this phase through cosmic time; measuring stellar populations, on-going star formation, morphologies, kinematics, interstellar medium properties, and AGN activity; mechanisms to cause the recent starburst and its end; and the future evolution to quiescence (or not). The review concludes with a list of open questions and exciting possibilities for future facilities. ","Evolution Through the Post-Starburst Phase: Using Post-Starburst
  Galaxies as Laboratories for Understanding the Processes that Drive Galaxy
  Evolution"
108,1404394299345846272,352048533,Sebasti√°n Marino,['A new paper by Josh Lovell where we analyzed @almaobs &amp; @NASAHubble obs of the q1Eri system that has an inner eccentric Jupiter and debris disc. The disc is wide and asymmetric possibly due to an additional planet @drgmk @lucaroundthewo1 @mmacgreg <LINK>'],https://arxiv.org/abs/2106.05975,"We present \textit{ALMA} 1.3 mm and 0.86 mm observations of the nearby (17.34 pc) F9V star q1 Eri (HD 10647, HR 506). This system, with age ${\sim}1.4$ Gyr, hosts a ${\sim}2$ au radial velocity planet and a debris disc with the highest fractional luminosity of the closest 300 FGK type stars. The \textit{ALMA} images, with resolution ${\sim}0.5''$, reveal a broad (34{-}134 au) belt of millimeter emission inclined by $76.7{\pm}1.0$ degrees with maximum brightness at $81.6{\pm}0.5$ au. The images reveal an asymmetry, with higher flux near the southwest ansa, which is also closer to the star. Scattered light observed with the Hubble Space Telescope is also asymmetric, being more radially extended to the northeast. We fit the millimeter emission with parametric models and place constraints on the disc morphology, radius, width, dust mass, and scale height. We find the southwest ansa asymmetry is best fitted by an extended clump on the inner edge of the disc, consistent with perturbations from a planet with mass $8 M_{\oplus} {-} 11 M_{\rm Jup}$ at ${\sim}60$ au that may have migrated outwards, similar to Neptune in our Solar System. If the measured vertical aspect ratio of $h{=}0.04{\pm}0.01$ is due to dynamical interactions in the disc, then this requires perturbers with sizes ${>}1200$ km. We find tentative evidence for an 0.86 mm excess within 10 au, $70{\pm}22\, \mu$Jy, that may be due to an inner planetesimal belt. We find no evidence for CO gas, but set an upper bound on the CO gas mass of $4{\times}10^{-6}$ M$_{\oplus}$ ($3\,\sigma$), consistent with cometary abundances in the Solar System. ","High resolution ALMA and HST images of q$^1$ Eri: an asymmetric debris
  disc with an eccentric Jupiter"
109,1404325296191459329,75129026,Emmanuel S√©ri√©,"['You may know surfboard wax... But do you know WAX-ML a brand new Python library for machine-learning and feedback-loops on streaming data? To know more,  you can read our paper <LINK> or checkout our code at <LINK>.\n#MachineLearning #jax #stream']",https://arxiv.org/abs/2106.06524,"Wax is what you put on a surfboard to avoid slipping. It is an essential tool to go surfing... We introduce WAX-ML a research-oriented Python library providing tools to design powerful machine learning algorithms and feedback loops working on streaming data. It strives to complement JAX with tools dedicated to time series. WAX-ML makes JAX-based programs easy to use for end-users working with pandas and xarray for data manipulation. It provides a simple mechanism for implementing feedback loops, allows the implementation of online learning and reinforcement learning algorithms with functions, and makes them easy to integrate by end-users working with the object-oriented reinforcement learning framework from the Gym library. It is released with an Apache open-source license on GitHub at this https URL ","WAX-ML: A Python library for machine learning and feedback loops on
  streaming data"
110,1403801833584435205,129350465,S Joshua Swamidass,['New paper from our group looks at improving fairness in artificial intelligence. <LINK>'],https://arxiv.org/abs/2106.00720,"In real world datasets, particular groups are under-represented, much rarer than others, and machine learning classifiers will often preform worse on under-represented populations. This problem is aggravated across many domains where datasets are class imbalanced, with a minority class far rarer than the majority class. Naive approaches to handle under-representation and class imbalance include training sub-population specific classifiers that handle class imbalance or training a global classifier that overlooks sub-population disparities and aims to achieve high overall accuracy by handling class imbalance. In this study, we find that these approaches are vulnerable in class imbalanced datasets with minority sub-populations. We introduced Fair-Net, a branched multitask neural network architecture that improves both classification accuracy and probability calibration across identifiable sub-populations in class imbalanced datasets. Fair-Nets is a straightforward extension to the output layer and error function of a network, so can be incorporated in far more complex architectures. Empirical studies with three real world benchmark datasets demonstrate that Fair-Net improves classification and calibration performance, substantially reducing performance disparity between gender and racial sub-populations. ","Fair-Net: A Network Architecture For Reducing Performance Disparity
  Between Identifiable Sub-Populations"
111,1403751179947896845,3022633752,Tianle Cai,"['Transformer is a GOOD graph learner when incorporating suitable graph structural information!\nThrilled to share our new work:\nTransformer + simple but effective encodings = GNN achieving new SOTA on OGB datasets, ZINC! \nPaper: <LINK>\nCode: <LINK> <LINK>', 'Full code will come soon! (keeping track of this repo : p)', 'With unbelievable collaborators Chengxuan, @Roger98079446, Shuxin, @guolin_ke, Di, Yanming, and Tie-Yan!', '@chaitjo Hi Chaitanya, thx for the pretty cool pointer! The solution looks very far-sighted. The power of Transformer on graph data should never be buried anyway : )', '@jeremyphoward @Roger98079446 @guolin_ke Thx Jeremy!']",https://arxiv.org/abs/2106.05234,"The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. ",Do Transformers Really Perform Bad for Graph Representation?
112,1403429122882387975,958953704568295424,Carlos Esteves,"['Check out our new #ICML2021 paper on learning arbitrary distributions on SO(3). With  \n@kmurmurs, @jampani_varun, @srikumarRam, @kiamada.\n\nPaper: <LINK>\n\nCode and data: <LINK> <LINK> <LINK>']",https://arxiv.org/abs/2106.05965,"Single image pose estimation is a fundamental problem in many vision and robotics tasks, and existing deep learning approaches suffer by not completely modeling and handling: i) uncertainty about the predictions, and ii) symmetric objects with multiple (sometimes infinite) correct poses. To this end, we introduce a method to estimate arbitrary, non-parametric distributions on SO(3). Our key idea is to represent the distributions implicitly, with a neural network that estimates the probability given the input image and a candidate pose. Grid sampling or gradient ascent can be used to find the most likely pose, but it is also possible to evaluate the probability at any pose, enabling reasoning about symmetries and uncertainty. This is the most general way of representing distributions on manifolds, and to showcase the rich expressive power, we introduce a dataset of challenging symmetric and nearly-symmetric objects. We require no supervision on pose uncertainty -- the model trains only with a single pose per example. Nonetheless, our implicit model is highly expressive to handle complex distributions over 3D poses, while still obtaining accurate pose estimation on standard non-ambiguous environments, achieving state-of-the-art performance on Pascal3D+ and ModelNet10-SO(3) benchmarks. ","Implicit-PDF: Non-Parametric Representation of Probability Distributions
  on the Rotation Manifold"
113,1403381633907048451,754321756559667201,Ondrej Dusek,"['Check out this new #NLProc paper by @tomiinek &amp; me on (in)consistency in automatic dialogue systems evaluation on MultiWOZ ‚Äì and use standard metrics ü§ì!\n\nWill be presented at the GEM Workshop at #ACL2021NLP, now at  \n<LINK>']",https://arxiv.org/abs/2106.05555,"The MultiWOZ dataset (Budzianowski et al.,2018) is frequently used for benchmarking context-to-response abilities of task-oriented dialogue systems. In this work, we identify inconsistencies in data preprocessing and reporting of three corpus-based metrics used on this dataset, i.e., BLEU score and Inform & Success rates. We point out a few problems of the MultiWOZ benchmark such as unsatisfactory preprocessing, insufficient or under-specified evaluation metrics, or rigid database. We re-evaluate 7 end-to-end and 6 policy optimization models in as-fair-as-possible setups, and we show that their reported scores cannot be directly compared. To facilitate comparison of future systems, we release our stand-alone standardized evaluation scripts. We also give basic recommendations for corpus-based benchmarking in future works. ","Shades of BLEU, Flavours of Success: The Case of MultiWOZ"
114,1403355027222274055,3395584851,Prof Norman Fenton,"['We pulled together some old and new work in this paper to show how and why causal Bayesian models are generally needed to calculate the probative value of evidence and why most calculations of Likelihood ratios get it wrong  <LINK> <LINK>', '@Nene1111 Sorry - I should have said this was pitched at researchers and forensic scientists.']",https://arxiv.org/abs/2106.05328,"When presenting forensic evidence, such as a DNA match, experts often use the Likelihood ratio (LR) to explain the impact of evidence . The LR measures the probative value of the evidence with respect to a single hypothesis such as 'DNA comes from the suspect', and is defined as the probability of the evidence if the hypothesis is true divided by the probability of the evidence if the hypothesis is false. The LR is a valid measure of probative value because, by Bayes Theorem, the higher the LR is, the more our belief in the probability the hypothesis is true increases after observing the evidence. The LR is popular because it measures the probative value of evidence without having to make any explicit assumptions about the prior probability of the hypothesis. However, whereas the LR can in principle be easily calculated for a distinct single piece of evidence that relates directly to a specific hypothesis, in most realistic situations 'the evidence' is made up of multiple dependent components that impact multiple different hypotheses. In such situations the LR cannot be calculated . However, once the multiple pieces of evidence and hypotheses are modelled as a causal Bayesian network (BN), any relevant LR can be automatically derived using any BN software application. ",Calculating the Likelihood Ratio for Multiple Pieces of Evidence
115,1403340131566768132,1310552063999438849,Hauke Group,['üîùCheck out the new paper ‚ñ∂ <LINK>\nüôåüèºCongrats to the authors @JCHalimeh @PhilippHauke #MaartenVanDamme #LingzhenGuo #JohannesLang\nüó£&lt;Dynamical phase transitions in #antiferromagnetic long-range spin chains are different from those in their ferromagnetic cousins&gt; <LINK>'],https://arxiv.org/abs/2106.05282,"In recent years, dynamical phase transitions and out-of-equilibrium criticality have been at the forefront of ultracold gases and condensed matter research. Whereas universality and scaling are established topics in equilibrium quantum many-body physics, out-of-equilibrium extensions of such concepts still leave much to be desired. Using exact diagonalization and the time-dependent variational principle in uniform martrix product states, we calculate the time evolution of the local order parameter and Loschmidt return rate in transverse-field Ising chains with antiferromagnetic power law-decaying interactions, and map out the corresponding rich dynamical phase diagram. \textit{Anomalous} cusps in the return rate, which are ubiquitous at small quenches within the ordered phase in the case of ferromagnetic long-range interactions, are absent within the accessible timescales of our simulations in the antiferromagnetic case, showing that long-range interactions are not a sufficient condition for their appearance. We attribute this to much weaker domain-wall binding in the antiferromagnetic case. For quenches across the quantum critical point, \textit{regular} cusps appear in the return rate and connect to the local order parameter changing sign, indicating the concurrence of two major concepts of dynamical phase transitions. Our results consolidate conclusions of previous works that a necessary condition for the appearance of anomalous cusps in the return rate after quenches within the ordered phase is for topologically trivial local spin flips to be the energetically dominant excitations in the spectrum of the quench Hamiltonian. Our findings are readily accessible in modern trapped-ion setups, and we outline the associated experimental considerations. ","Dynamical phase transitions in quantum spin models with
  antiferromagnetic long-range interactions"
116,1403315074476003330,1107290362228498434,Simon Vandenhende,"['We uploaded a new paper on contrastive self-supervised learning. The paper examines the influence of dataset biases, uncovers several interesting qualities of the learned representations, and shows how to realize further gains. \n\n<LINK>', '[1/5] An existing approach like MoCo can handle object-centric versus scene-centric, uniform versus long-tailed and general versus domain-specific datasets well. https://t.co/DyNev7r3yH', '[2/5] MoCo does not suffer from using non-overlapping views on scene-centric datasets. First, non-overlapping views do not occur when using the augmentation strategy from SimCLR. Second, even when lowering the overlap between crops, the performance remains stable. https://t.co/0YovHFUwRu', '[3/5] The learned representations can be further improved by learning additional invariances. https://t.co/orXoJnJDGG', '[4/5] Adopting a multi-crop strategy allows to learn spatially structured representations, which can be directly used for semantic segment retrieval and video instance segmentation without finetuning. https://t.co/CYyTgX6464', '[5/5] Training MoCo with a multi-crop strategy improves properties like object localization. https://t.co/pQJy0x1OvJ', 'Thats enough self-promotion for today. Work done together with @WGansbeke , @stam_g and Luc Van Gool.']",https://arxiv.org/abs/2106.05967,"Contrastive self-supervised learning has outperformed supervised pretraining on many downstream tasks like segmentation and object detection. However, current methods are still primarily applied to curated datasets like ImageNet. In this paper, we first study how biases in the dataset affect existing methods. Our results show that current contrastive approaches work surprisingly well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed and (iii) general versus domain-specific datasets. Second, given the generality of the approach, we try to realize further gains with minor modifications. We show that learning additional invariances -- through the use of multi-scale cropping, stronger augmentations and nearest neighbors -- improves the representations. Finally, we observe that MoCo learns spatially structured representations when trained with a multi-crop strategy. The representations can be used for semantic segment retrieval and video instance segmentation without finetuning. Moreover, the results are on par with specialized models. We hope this work will serve as a useful study for other researchers. The code and models are available at this https URL ","Revisiting Contrastive Methods for Unsupervised Learning of Visual
  Representations"
117,1403298820872675329,835146121144041472,Andr√© Biedenkapp,"[""TempoRL has gone deep. In our new #ICML2021 paper we extend TempoRL to work in the Deep RL case and showed improved learning speed and better guided temporal exploration. If you're interested in learning *when* to act in deep RL, check out the paper <LINK>. <LINK>"", 'This was a joint work with @RaghuSpaceRajan, @FrankRHutter and @LindauerMarius']",https://arxiv.org/abs/2106.05262,"Reinforcement learning is a powerful approach to learn behaviour through interactions with an environment. However, behaviours are usually learned in a purely reactive fashion, where an appropriate action is selected based on an observation. In this form, it is challenging to learn when it is necessary to execute new decisions. This makes learning inefficient, especially in environments that need various degrees of fine and coarse control. To address this, we propose a proactive setting in which the agent not only selects an action in a state but also for how long to commit to that action. Our TempoRL approach introduces skip connections between states and learns a skip-policy for repeating the same action along these skips. We demonstrate the effectiveness of TempoRL on a variety of traditional and deep RL environments, showing that our approach is capable of learning successful policies up to an order of magnitude faster than vanilla Q-learning. ",TempoRL: Learning When to Act
118,1403285233227776003,3021429142,Peter Karpov,"[""New paper on high-quality interpolation kernels now on arXiv: <LINK>\nTLDR: common kernels (bilinear, Keys' bicubic) result in anisotropic artifacts (blocking). The magnitude of these artifacts can be greatly reduced by careful kernel design. <LINK>""]",https://arxiv.org/abs/2106.04104,We present a number of new piecewise-polynomial kernels for image interpolation. The kernels are constructed by optimizing a measure of interpolation quality based on the magnitude of anisotropic artifacts. The kernel design process is performed symbolically using Mathematica computer algebra system. Experimental evaluation involving 14 image quality assessment methods demonstrates that our results compare favorably with the existing linear interpolators. ,"Design of Low-Artifact Interpolation Kernels by Means of Computer
  Algebra"
119,1403282459656077313,4249537197,Christian Wolf,['New paper: we show that transfer from Oracle input to real improves VQA reasoning when a strong link is established through program supervision: empirical results + analysis of sample complexity.\n\nWork with @CorentK  @antigregory  @moezbac  and M. Nadri. \n<LINK> <LINK>'],https://arxiv.org/abs/2106.05597,"Methods for Visual Question Anwering (VQA) are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. It has been recently shown that better reasoning patterns emerge in attention layers of a state-of-the-art VQA model when they are trained on perfect (oracle) visual inputs. This provides evidence that deep neural networks can learn to reason when training conditions are favorable enough. However, transferring this learned knowledge to deployable models is a challenge, as much of it is lost during the transfer. We propose a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. We also demonstrate the effectiveness of this approach experimentally on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training. ",Supervising the Transfer of Reasoning Patterns in VQA
120,1403276386089984000,978860930,"Jan Scholtz, Logic Wizard","['<LINK> It is a new paper day! We analyzed IFU data from VLT/SINFONI with ALMA band 7 continuum observations to answer: Are AGN driven outflows in Quasars instantaneously suppressing star formation?', 'The answer is. Not really, at least not on 4 kpc scale! It is probably all happening on much smaller scales and over many AGN episodes! Huge thanks to all my co-authors especially @CMHarrisonAstro, @astro_sario and Dave Alexander!']",https://arxiv.org/abs/2106.05277,"We present high-resolution ($\sim$2.4\,kpc) ALMA band 7 observations (rest-frame $\lambda \sim 250\mu$m) of three powerful z$\sim$2.5 quasars ($L_{\rm bol}=10^{47.3}$-$10^{47.5}$ ergs s$^{-1}$). These targets have previously been reported as showing evidence for suppressed star formation based on cavities in the narrow H$\alpha$ emission at the location of outflows traced with [O~{\sc iii}] emission. Here we combine the ALMA observations with a re-analysis of the VLT/SINFONI data to map the rest-frame far-infrared emission, H$\alpha$ emission, and [O~{\sc iii}] emission. In all targets we observe high velocity [O~{\sc iii}] gas (i.e., W80$\sim$1000--2000\,km\,s$^{-1}$) across the whole galaxy. We do not identify any H$\alpha$ emission that is free from contamination from AGN-related processes; however, based on SED analyses, we show that the ALMA data contains a significant dust-obscured star formation component in two out of the three systems. This dust emission is found to be extended over $\approx$1.5--5.5\,kpc in the nuclear regions, overlaps with the previously reported H$\alpha$ cavities and is co-spatial with the peak in surface brightness of the [O~{\sc iii}] outflows. In summary, within the resolution and sensitivity limits of the data, we do not see any evidence for a instantaneous shut down of in-situ star formation caused directly by the outflows. However, similar to the conclusions of previous studies and based on our measured star formation rates, we do not rule out that the global host galaxy star formation could be suppressed on longer timescales by the cumulative effect of quasar episodes during the growth of these massive black holes. ","The impact of ionised outflows from z$\sim$2.5 quasars is not through
  instantaneous in-situ quenching: the evidence from ALMA and VLT/SINFONI"
121,1403268904848330755,65432023,Prof. Costas Andreopoulos,['New paper on #neutrino-induced hadronic multiparticle production led by my PhD student J√∫lia Tena-Vidal (<LINK>). It presents a new tune of the Andreopoulos-Gallagher-Kehayias-Yang hadronization model in #GENIE . Funded by @livuniphysics LIV.DAT #nuxsec'],https://arxiv.org/abs/2106.05884,"The GENIE neutrino Monte Carlo describes neutrino-induced hadronization with an effective model, known as AGKY, which is interfaced with PYTHIA at high invariant mass. Only the low-mass AGKY model parameters were extracted from hadronic shower data from the FNAL 15 ft and BEBC experiments. In this paper, the first hadronization tune on averaged charged multiplicity data from deuterium and hydrogen bubble chamber experiments is presented, with a complete estimation of parameter uncertainties. A partial tune on deuterium data only highlights the tensions between hydrogen and deuterium datasets. ",Hadronization Model Tuning in GENIE v3
122,1403236896604442628,479249561,Makoto Morishita,"[""We've just released a new paper on arXiv.\nThis paper describes an NMT system for constrained translation tasks, which we must include specified terms in the output.\n<LINK>"", 'This system was submitted for WAT 2021 restricted translation task and won the first prize!\nhttps://t.co/yZDiSmxgfB']",https://arxiv.org/abs/2106.05450,"This paper describes our systems that were submitted to the restricted translation task at WAT 2021. In this task, the systems are required to output translated sentences that contain all given word constraints. Our system combined input augmentation and constrained beam search algorithms. Through experiments, we found that this combination significantly improves translation accuracy and can save inference time while containing all the constraints in the output. For both En->Ja and Ja->En, our systems obtained the best evaluation performances in automatic evaluation. ","Input Augmentation Improves Constrained Beam Search for Neural Machine
  Translation: NTT at WAT 2021"
123,1403154611008012295,1084300516543381505,endo_suguru,"['Our new paper is out! <LINK>  This paper introduces the novel way to simulate perturbation in (near-term) quantum hardware!', '@CQT_Kishor Thank you very much, Kishor! Arigatou Gozaimasu, Kishor-san!']",https://arxiv.org/abs/2106.05938,"Approximations based on perturbation theory are the basis for most of the quantitative predictions of quantum mechanics, whether in quantum field theory, many-body physics, chemistry or other domains. Quantum computing provides an alternative to the perturbation paradigm, but the tens of noisy qubits currently available in state-of-the-art quantum processors are of limited practical utility. In this article, we introduce perturbative quantum simulation, which combines the complementary strengths of the two approaches, enabling the solution of large practical quantum problems using noisy intermediate-scale quantum hardware. The use of a quantum processor eliminates the need to identify a solvable unperturbed Hamiltonian, while the introduction of perturbative coupling permits the quantum processor to simulate systems larger than the available number of physical qubits. After introducing the general perturbative simulation framework, we present an explicit example algorithm that mimics the Dyson series expansion. We then numerically benchmark the method for interacting bosons, fermions, and quantum spins in different topologies, and study different physical phenomena on systems of up to $48$ qubits, such as information propagation, charge-spin separation and magnetism. In addition, we use 5 physical qubits on the IBMQ cloud to experimentally simulate the $8$-qubit Ising model using our algorithm. The result verifies the noise robustness of our method and illustrates its potential for benchmarking large quantum processors with smaller ones. ",Perturbative quantum simulation
124,1403147732215156745,960527787428900864,Michael Albergo,"['*New paper* with the @MIT crew, @DaniloJRezende @sracaniere @DeepMind , @KyleCranmer and Julian Urban! We construct normalizing flows that are compatible with sampling path integrals of quantum field theories involving fermions. Fermions make this tricky! <LINK> <LINK>', 'A primary goal in lattice field theory is to compute path integral expectations arising from the ""action"" of the theory. For a theory involving bosons and fermions, that may look something like this equation. These integrals are generally evaluated with Markov chain Monte Carlo: https://t.co/HpNlUqfo3Y', ""The second set of integration measures here refer to integrals over the fermion fields œà, which aren't like bosonic, scalar fields. They are defined by Grassmann numbers, which are elements of the exterior algebra over the complex numbers. Grassmann numbers anti-commute!"", 'Because of this, the ""Grassmann"" part of these integrals can be analytically solved, which sounds like a blessing but can make the rest of the MCMC process difficult! For theories that we want to study, this means we have to compute the determinant of some pretty gnarly matrices: https://t.co/116GNPqwl1', 'We present a number of MCMC samplers to deal with this fact that primarily rely on the equivalence of these determinant calculations with a Gaussian integral over a set of auxiliary fields that are called ""pseudofermions"": https://t.co/h9ehhuTdMW', 'We then construct a variety of normalizing flows compatible with each of the samplers, making sure each captures the translational equivariance quirks (anti-periodicity in time) that arise from the fermionic degrees of freedom: https://t.co/TrcDd3Vwko', 'We apply these ideas to a Yukawa theory, where bosons are coupled to mass-degenerate fermions, to show how it works! https://t.co/rwmLqC68Rb', 'These tools should be applicable to a bunch of problems in lattice field theory and condensed matter, so we are excited to see where it takes us :)']",https://arxiv.org/abs/2106.05934,"Algorithms based on normalizing flows are emerging as promising machine learning approaches to sampling complicated probability distributions in a way that can be made asymptotically exact. In the context of lattice field theory, proof-of-principle studies have demonstrated the effectiveness of this approach for scalar theories, gauge theories, and statistical systems. This work develops approaches that enable flow-based sampling of theories with dynamical fermions, which is necessary for the technique to be applied to lattice field theory studies of the Standard Model of particle physics and many condensed matter systems. As a practical demonstration, these methods are applied to the sampling of field configurations for a two-dimensional theory of massless staggered fermions coupled to a scalar field via a Yukawa interaction. ",Flow-based sampling for fermionic lattice field theories
125,1403140600480747522,147951210,David Berthelot,"['New paper: AdaMatch - Unifying Unsupervised Domain Adaptation (UDA) and Semi-Supervised Learning (SSL) and SSDA. Nearly doubles SotA accuracy for UDA on non-pretrained DomainNet. <LINK>\n1/3', 'he technique itself is an extension of FixMatch: the two biggest differences being random logit interpolation and relative confidence thresholding.\n2/3 https://t.co/uMQYnZ0mGY', 'There are plenty of tables comparing the effects of applying SSL techniques to UDA, SSDA and UDA techniques such as MCD to SSL and SSDA.\nThe picture I like best is the one that compares convergence between MCD and AdaMatch over time.\n3/3 https://t.co/FPKoDxTOqp', 'Work done with @BeccaRoelofs (co-first author), Kihyuk Sohn, Nicholas Carlini, @alexey2004']",https://arxiv.org/abs/2106.04732,"We extend semi-supervised learning to the problem of domain adaptation to learn significantly higher-accuracy models that train on one data distribution and test on a different one. With the goal of generality, we introduce AdaMatch, a method that unifies the tasks of unsupervised domain adaptation (UDA), semi-supervised learning (SSL), and semi-supervised domain adaptation (SSDA). In an extensive experimental study, we compare its behavior with respective state-of-the-art techniques from SSL, SSDA, and UDA on vision classification tasks. We find AdaMatch either matches or significantly exceeds the state-of-the-art in each case using the same hyper-parameters regardless of the dataset or task. For example, AdaMatch nearly doubles the accuracy compared to that of the prior state-of-the-art on the UDA task for DomainNet and even exceeds the accuracy of the prior state-of-the-art obtained with pre-training by 6.4% when AdaMatch is trained completely from scratch. Furthermore, by providing AdaMatch with just one labeled example per class from the target domain (i.e., the SSDA setting), we increase the target accuracy by an additional 6.1%, and with 5 labeled examples, by 13.6%. ","AdaMatch: A Unified Approach to Semi-Supervised Learning and Domain
  Adaptation"
126,1403052319948914700,1246825028274008069,Emil J. Bergholtz,"['New paper ""Classification of Exceptional Nodal Topologies Protected by PT Symmetry"" out today. With my student Marcus St√•lhammar. My tactic of only hiring smarter people is my smartest move.\n<LINK> <LINK>']",https://arxiv.org/abs/2106.04582,"Exceptional degeneracies, at which both eigenvalues and eigenvectors coalesce, and parity-time ($\mathcal{PT}$) symmetry, reflecting balanced gain and loss in photonic systems, are paramount concepts in non-Hermitian systems. We here complete the topological classification of exceptional nodal degeneracies protected by $\mathcal{PT}$ symmetry in up to three dimensions and provide simple example models whose exceptional nodal topologies include previously overlooked possibilities such as second-order knotted surfaces of arbitrary genus, third-order knots and fourth-order points. ","Classification of Exceptional Nodal Topologies Protected by
  $\mathcal{PT}$ Symmetry"
127,1402982238329851904,2705638878,Dr. Adelle Goodwin,"['Okay I promised a thread about my new paper <LINK>, so here!\n\nHave you ever wondered if a hotspot on the surface of an X-ray pulsar might cause X-ray bursts to ignite underneath it? Wonder no more! \nNot sure what this means but burning with curiosity? Read on!', 'Firstly, this project was brought to you by a trip to visit the brilliant @drannawatts and co, in Amsterdam some 2yrs ago!\nThis work formed the 6th (&amp; final) publication of my PhD thesis and I‚Äôm very happy it‚Äôs finally published &amp; to share it with you. Buckle in for some science!', 'SO what is an X-ray pulsar? They are neutron stars (NS) in a binary system with a normal star. NSs are dense, like mass of the sun in the area of a small city dense, so they have a very strong gravitational pull and can pull material from their binary companion (a ‚Äúnormal‚Äù star).', 'Neutron stars also have strong magnetic fields. Because of this, the material they consume can be channelled to the poles (just like North and South Pole of Earth) causing a hotspot to form. These hotspots are visible to us as they come in and out of view as the star rotates.', 'Known as X-ray pulsars, these things are super cool for all sorts of reasons! E.g. We can precisely time how fast the NS is spinning by measuring the time between seeing the hotspot. We can even learn things about the companion star (and their orbit) from small timing deviations.', 'But that‚Äôs not all! When a neutron star devours its companion, the material it devours ends up building up on its surface. The surface of a neutron star is an EXTREME environment for this material (H and He) to be, and it is quickly condensed to high pressures and temps.', 'The built up material can reach such high temps and pressures that it ignites in a huge thermonuclear explosion (known as an X-Ray burst). These things are colossal, very bright, but short (~1min). SO much nuclear physics happens in these explosions they are a dream to study.', 'So what was the point of our paper? We wanted to know if these huge explosions might be more likely to be triggered underneath the hotspot on these X-ray pulsars, then the spread around the rest of the star. Seems logical, right? Explosion is triggered where it‚Äôs hotter ü§∑üèº\u200d‚ôÄÔ∏è', 'BUT there‚Äôs another key thing. Neutron stars are made up of extremely dense, strange matter. And they also can spin really really fast. Because of this, the gravity at the equator of the neutron star can actually be significantly lower than at the poles.', 'Why is this important? Well, conditions for these explosions will be reached more easily in a lower gravity environment. So it became a competition of whether the heat from the hotspot at the pole or the lower gravity at the equator would win for ignition location.', 'And so, like any good Astrophysicists, we created a model to answer this question. We modelled the heat transport mechanisms in the top layers of a neutron star, and ran models with hotspots, without hotspots, and with lower surface gravity to emulate the equator.', 'What did we find? Well, we found that it is actually very difficult for heat to penetrate deep into the neutron star at the levels we need to ignite these explosions, and also the lower gravity at the equator really does make it easier to ignite bursts there.', 'So the answer is: we would need a hotspot of 10^8K or hotter for bursts to preferentially ignite under it, otherwise they are probably going to ignite at the equator. Is 10^8K feasible for a hotspot? Erm, no, not really. Observed hotspots tend to be around 10^6-10^7K.', 'So there you have it! If you made it this far well done, you are now an expert on the ignition location of Type I thermonuclear X-ray bursts on accreting X-ray pulsars! üëèüèºüí•']",https://arxiv.org/abs/2106.04107,"Hotspots on the surface of accreting neutron stars have been directly observed via pulsations in the lightcurves of X-ray pulsars. They are thought to occur due to magnetic channelling of the accreted fuel to the neutron star magnetic poles. Some X-ray pulsars exhibit burst oscillations during Type I thermonuclear X-ray bursts which are thought to be caused by asymmetries in the burning. In rapidly rotating neutron stars, it has been shown that the lower gravity at the equator can lead to preferential ignition of X-ray bursts at this location. These models, however, do not include the effect of accretion hotspots at the neutron star surface. There are two accreting neutron star sources in which burst oscillations have been observed to track exactly the neutron star spin period. We analyse whether this could be due to the X-ray bursts igniting at the magnetic pole of the neutron star, because of heating in the accreted layers under the hotspot causing ignition conditions to be reached earlier. We investigate heat transport in the accreted layers using a 2D model and study the prevalence of heating down to the ignition depth of X-ray bursts for different hotspot temperatures and sizes. We perform calculations for accretion at the pole and at the equator, and infer that ignition could occur away from the equator at the magnetic pole for hotspots with temperatures greater than $1\times10^8$ K. However, current observations have not identified such high temperatures in accreting X-ray pulsars. ","X-ray burst ignition location on the surface of accreting X-ray pulsars:
  Can bursts preferentially ignite at the hotspot?"
128,1402940321198080002,803882049484398592,Takuya Yoshioka,"[""Human Listening and Live Captioning: Multi-Task Training for Speech Enhancement\n<LINK>\n\nOur new paper on speech enhancement, where we've addressed the problem of NN-based monaural SE methods degrading ASR accuracy.""]",https://arxiv.org/abs/2106.02896,"With the surge of online meetings, it has become more critical than ever to provide high-quality speech audio and live captioning under various noise conditions. However, most monaural speech enhancement (SE) models introduce processing artifacts and thus degrade the performance of downstream tasks, including automatic speech recognition (ASR). This paper proposes a multi-task training framework to make the SE models unharmful to ASR. Because most ASR training samples do not have corresponding clean signal references, we alternately perform two model update steps called SE-step and ASR-step. The SE-step uses clean and noisy signal pairs and a signal-based loss function. The ASR-step applies a pre-trained ASR model to training signals enhanced with the SE model. A cross-entropy loss between the ASR output and reference transcriptions is calculated to update the SE model parameters. Experimental results with realistic large-scale settings using ASR models trained on 75,000-hour data show that the proposed framework improves the word error rate for the SE output by 11.82% with little compromise in the SE quality. Performance analysis is also carried out by changing the ASR model, the data used for the ASR-step, and the schedule of the two update steps. ","Human Listening and Live Captioning: Multi-Task Training for Speech
  Enhancement"
129,1402933794580185092,716958933039054848,Senellart's QD Group,['üéâNew paper on photon-number entanglement!üéâ\n\nWe generate a photon-number Bell state by exciting a two-level system with subsequent pulses. A new scalable route towards entangled photonic states!\n\n<LINK>\n\n@AntnSo @JCLoredoR @Quandela_SAS'],https://arxiv.org/abs/2106.02049,"Entanglement and spontaneous emission are fundamental quantum phenomena that drive many applications of quantum physics. During the spontaneous emission of light from an excited two-level atom, the atom briefly becomes entangled with the photonic field. Here, we show that this natural process can be used to produce photon-number entangled states of light distributed in time. By exciting a quantum dot -- an artificial two-level atom -- with two sequential $\pi$ pulses, we generate a photon-number Bell state. We characterise this state using time-resolved intensity and phase correlation measurements. Furthermore, we theoretically show that applying longer sequences of pulses to a two-level atom can produce a series of multi-temporal mode entangled states with properties intrinsically related to the Fibonacci sequence. Our results on photon-number entanglement can be further exploited to generate new states of quantum light with applications in quantum technologies. ","Photon-number entanglement generated by sequential excitation of a
  two-level atom"
130,1402907147181101059,841031248839618560,Relja Arandjeloviƒá,"['In our new paper ""NeRF in detail: Learning to sample for view synthesis"" (aka yet another NeRF paper on your to-read list) we replace the heuristic coarse-to-fine strategy of NeRF via a learnt one. Improvements in rendering quality and speed. <LINK> <LINK>', ""@SattlerTorsten Thanks Torsten! I only tried it really on the original NeRF paper's datasets, it seems most NeRF improvements use the same data as well."", '@SattlerTorsten I see, good to know.']",https://arxiv.org/abs/2106.05264,"Neural radiance fields (NeRF) methods have demonstrated impressive novel view synthesis performance. The core approach is to render individual rays by querying a neural network at points sampled along the ray to obtain the density and colour of the sampled points, and integrating this information using the rendering equation. Since dense sampling is computationally prohibitive, a common solution is to perform coarse-to-fine sampling. In this work we address a clear limitation of the vanilla coarse-to-fine approach -- that it is based on a heuristic and not trained end-to-end for the task at hand. We introduce a differentiable module that learns to propose samples and their importance for the fine network, and consider and compare multiple alternatives for its neural architecture. Training the proposal module from scratch can be unstable due to lack of supervision, so an effective pre-training strategy is also put forward. The approach, named `NeRF in detail' (NeRF-ID), achieves superior view synthesis quality over NeRF and the state-of-the-art on the synthetic Blender benchmark and on par or better performance on the real LLFF-NeRF scenes. Furthermore, by leveraging the predicted sample importance, a 25% saving in computation can be achieved without significantly sacrificing the rendering quality. ",NeRF in detail: Learning to sample for view synthesis
131,1402898805373083654,145986026,Erdem Bƒ±yƒ±k,"['New paper at IEEE Transactions on Control of Network Systems:  <LINK>\n\nIn traffic, many equilibria exist (see the GIF, the flow is the same in all 5 configs). When all drivers select their routes selfishly, we often end up in congested equilibria. (1/n) <LINK>', 'Autonomous cars may alleviate this problem, as they can keep shorter headways with the car they are following. Besides, travel times might be further reduced if some vehicles (or drivers) are altruistic. But, does altruism really exist? (2/n)', 'Luckily, financial incentives (through ride-hailing services like Uber &amp; Lyft) can give the same benefits of altruism! These services should give multiple options to the users: ""do you prefer a cheaper but slower route or are you willing to pay more to get there faster?"" (3/n)', ""Using preference-based learning techniques helps learn passengers' price/latency tradeoff. The services should then use this information to optimize route prices to minimize congestion. In simulations, this approach decreased travel times up to 50%. (4/n)"", 'This is an extension of our earlier works presented at WAFR 2018 (https://t.co/k9lXsPlF0O) and CDC 2019 (https://t.co/vs2NWzpVgW) with Daniel A. Lazar, Ramtin Pedarsani and @DorsaSadigh. (5/5)']",https://arxiv.org/abs/2106.04678,"Traffic congestion has large economic and social costs. The introduction of autonomous vehicles can potentially reduce this congestion by increasing road capacity via vehicle platooning and by creating an avenue for influencing people's choice of routes. We consider a network of parallel roads with two modes of transportation: (i) human drivers, who will choose the quickest route available to them, and (ii) a ride hailing service, which provides an array of autonomous vehicle route options, each with different prices, to users. We formalize a model of vehicle flow in mixed autonomy and a model of how autonomous service users make choices between routes with different prices and latencies. Developing an algorithm to learn the preferences of the users, we formulate a planning optimization that chooses prices to maximize a social objective. We demonstrate the benefit of the proposed scheme by comparing the results to theoretical benchmarks which we show can be efficiently calculated. ","Incentivizing Efficient Equilibria in Traffic Networks with Mixed
  Autonomy"
132,1402887200426102786,1065359486951538688,Titouan Parcollet,['Want to know more about @SpeechBrain1? Just have a look at the new paper üëÄüëÄ\n\npdf: <LINK>\nabs: <LINK>\n\nTutorials: <LINK>\nGitHub: <LINK>\nHuggingFace: <LINK>\nWebsite: <LINK> <LINK>'],https://arxiv.org/abs/2106.04624,"SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies. ",SpeechBrain: A General-Purpose Speech Toolkit
133,1402883773805518851,3881712928,Valentina,"['New paper!üí´üåë In <LINK> we show that future neutrino experiments DUNE (@DUNEScience) and THEIA will be able to set competitive constraints on primordial black hole dark matter. Great collaboration with @pablommirave and @MariamTortola, @AHEPGroup @IFICorpuscular <LINK>']",https://arxiv.org/abs/2106.05013,"Primordial black holes (PBHs) are a potential dark matter candidate whose masses can span over many orders of magnitude. If they have masses in the $10^{15}-10^{17}$ g range, they can emit sizeable fluxes of MeV neutrinos through evaporation via Hawking radiation. We explore the possibility of detecting light (non-)rotating PBHs with future neutrino experiments. We focus on two next generation facilities: the Deep Underground Neutrino Experiment (DUNE) and THEIA. We simulate the expected event spectra at both experiments assuming different PBH mass distributions and spins, and we extract the expected 95% C.L. sensitivities to these scenarios. Our analysis shows that future neutrino experiments like DUNE and THEIA will be able to set competitive constraints on PBH dark matter, thus providing complementary probes in a part of the PBH parameter space currently constrained mainly by photon data. ",Signatures of primordial black hole dark matter at DUNE and THEIA
134,1402869254286946309,1357700078333550592,Keri Vos,"['Very happy to announce my new paper, the first (of hopefully many!) together with Marzia @mukkietto. An analysis of Lepton Flavor Violation in Lambda decays <LINK>\nWork to do for our #lhcb friends!', 'Thanks to @v_researcher @mmulderphysics @lgreeven @PrPetitesMains for their comments and help']",https://arxiv.org/abs/2106.05192,"Lepton flavour violation (LFV) naturally occurs in many new physics models, specifically in those explaining the $B$ anomalies. While LFV has already been studied for mesonic decays, it is important to consider also baryonic decays mediated by the same quark transition. In this paper, we study LFV in the baryonic $\Lambda_b \to \Lambda \ell_1 \ell_2$ using for the first time a full basis of New Physics operators. We present expected bounds on the branching ratio in a model-independent framework and using two specific new physics models. Finally, we point out the interplay and orthogonality between the baryonic and mesonic LFV searches. ",Lepton flavour violation in rare $\Lambda_b$ decays
135,1402761868318429184,2715142842,Prithvijit,"['(New Preprint) RobustNav: Towards Benchmarking Robustness in Embodied Navigation (1/6)\n\nPaper: <LINK>\nProject Webpage: <LINK>\nCode: <LINK>\n\n(w/ @judyfhoffman @RoozbehMottaghi @anikembhavi ) <LINK>', 'As an attempt towards assessing the robustness of visual navigation agents, we propose a benchmark to evaluate them in unseen target environments with corrupted appearance and dynamics characteristics (supported at progressively increasing levels of severity). (2/6)', ""Visual corruptions affect the agent's egocentric RGB observation in the target environment -- presence of noise, particles on the camera lens, blur, etc. (3/6) https://t.co/tHPcGzhsSR"", 'Dynamics corruption affect transition dynamics in the target environment. (4/6) https://t.co/HwihHvayE6', 'We find that standard nav policies trained via RL from scratch underperform or fail under severe corruptions. This is accompanied by certain idiosyncrasies that arise when agents are operating under corruptions.  (5/6)', 'For visual corruptions, we find that some unsupervised  adaptation techniques help minimally in terms of bridging the gap b/w clean and corrupt performance. (6/6)']",https://arxiv.org/abs/2106.04531,"As an attempt towards assessing the robustness of embodied navigation agents, we propose RobustNav, a framework to quantify the performance of embodied navigation agents when exposed to a wide variety of visual - affecting RGB inputs - and dynamics - affecting transition dynamics - corruptions. Most recent efforts in visual navigation have typically focused on generalizing to novel target environments with similar appearance and dynamics characteristics. With RobustNav, we find that some standard embodied navigation agents significantly underperform (or fail) in the presence of visual or dynamics corruptions. We systematically analyze the kind of idiosyncrasies that emerge in the behavior of such agents when operating under corruptions. Finally, for visual corruptions in RobustNav, we show that while standard techniques to improve robustness such as data-augmentation and self-supervised adaptation offer some zero-shot resistance and improvements in navigation performance, there is still a long way to go in terms of recovering lost performance relative to clean ""non-corrupt"" settings, warranting more research in this direction. Our code is available at this https URL ",RobustNav: Towards Benchmarking Robustness in Embodied Navigation
136,1402733253459689474,1240369991888896003,Riley Simmons-Edler,"['New Paper! ""Towards Practical Credit Assignment for Deep Reinforcement Learning"" from Vyacheslav Alipov and others at Samsung AI Center Moscow and me: <LINK>', ""Credit assignment in RL is the process of figuring out how your actions affect the rewards you get in the future. Explicit credit assignment methods (that try to predict this directly) have theoretical advantages, but haven't seen much success in deep RL yet."", ""In this paper, we asked why credit assignment hasn't worked well in deep RL. We extended an existing theoretical on-policy algorithm, Hindsight Credit Assignment, using deep neural networks, and made two interesting discoveries:"", 'First, we found that bias in the credit estimator neural network (due to lack of training, unlucky sampling, etc) results in biased gradient steps by the policy, which causes instability and policy collapse. We propose several measures to counteract this in the paper.', 'Second, we found that even with the stability issues solved, policy training is bottlenecked by the speed at which the credit estimator trains (since it tells the policy which actions to update) for most Atari games. Credit is only useful in cases where the baseline does poorly.', ""Faced with this, is there was another way to use credit that doesn't bottleneck training? We found that the entropy of our credit distribution can be used as a coarse-grained measure of the importance of actions taken in a state with respect to the future."", 'Using this insight, we propose a simple threshold constraint on state-action updates (keep or zero-out) based on credit entropy. We implemented this method on top of A2C, and found that it is stable and outperforms A2C on a majority of Atari games.', ""This approach is interesting because it uses credit information, but isn't affected by the specific credit assigned to any single action- if credit entropy is low, we know that *some actions* affect the future, but we don't need to be accurate about which ones/how much."", 'We call our algorithm ""Credit-Constrained Advantage Actor-Critic"" or C2A2C, and while much research remains on credit assignment, we think this is a fruitful new direction that has already shown practical results.', ""One major question that remains is what the threshold criteria should be? We used a quantile threshold, and found that there wasn't a single good quantile for all games- this parameter needs to be tuned per-task. We're hoping that this limitation can be addressed in future work."", ""Thanks for reading! If you're interested to know more, please check out our paper, now on arxiv! https://t.co/BiTRO7ygeH""]",https://arxiv.org/abs/2106.04499,"Credit assignment is a fundamental problem in reinforcement learning, the problem of measuring an action's influence on future rewards. Explicit credit assignment methods have the potential to boost the performance of RL algorithms on many tasks, but thus far remain impractical for general use. Recently, a family of methods called Hindsight Credit Assignment (HCA) was proposed, which explicitly assign credit to actions in hindsight based on the probability of the action having led to an observed outcome. This approach has appealing properties, but remains a largely theoretical idea applicable to a limited set of tabular RL tasks. Moreover, it is unclear how to extend HCA to deep RL environments. In this work, we explore the use of HCA-style credit in a deep RL context. We first describe the limitations of existing HCA algorithms in deep RL that lead to their poor performance or complete lack of training, then propose several theoretically-justified modifications to overcome them. We explore the quantitative and qualitative effects of the resulting algorithm on the Arcade Learning Environment (ALE) benchmark, and observe that it improves performance over Advantage Actor-Critic (A2C) on many games where non-trivial credit assignment is necessary to achieve high scores and where hindsight probabilities can be accurately estimated. ",Towards Practical Credit Assignment for Deep Reinforcement Learning
137,1402704471763922947,502318662,Emmanuel Bengio,"['Ever wanted to generate diverse samples of discrete data based on a reward function? Our new method, GFlowNet, based on flow networks &amp; a TD-like objective, gets great results on a molecule generation domain üíä\npaper:<LINK> <LINK>', 'blog: https://t.co/gnT6I0ZZQF\ncode: https://t.co/UwSIjpXqs7\nand this is work with these awesome people: @JainMoksh, Maksym Korablyov, Doina Precup &amp; Yoshua Bengio']",http://arxiv.org/abs/2106.04399,"This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task. ","Flow Network based Generative Models for Non-Iterative Diverse Candidate
  Generation"
138,1402662936859099145,1576276220,Dr. Emily Petroff,"['The CHIME/FRB Collaboration is beyond excited to share the results of our first Fast Radio Burst Catalog!!! 535 new FRBs in just under one year of operation (2018-2019) and all data publicly available on our website: <LINK>\nAnd paper here: <LINK> <LINK>', 'How amazing is this catalog? In almost 15 years (2007-2021) the entire field has reported 140 FRBs from all telescopes around the world. In ONE YEAR CHIME/FRB multiplied that number by 4. https://t.co/JoL3pvxDwv', 'If you\'ve ever heard me give a talk about FRBs, you\'ll have heard me say ""we have lots of unanswered questions, but maybe when we have a larger sample, we will know more.""\nWell, we have a larger sample. We know more!!! Follow along!\n(see my standard end slide) https://t.co/deAr6VK71F', 'First, the larger sample gives us a handle on the all-sky FRB rate. Based on detections at CHIME, and a detailed study of the telescope sensitivity, we estimate that there are about 800 bright FRBs (F &gt; 5 Jy ms, for you experts) over the whole sky every day. One every 30 seconds!', 'With a big sample, you can look at all the amazing structure we see in individual FRB pulses and see if there are any common themes. In a companion paper led by McGill PhD student Ziggy Pleunis we find 4 main archetypes for FRBs in the catalog.\nhttps://t.co/NCsHCxanvu https://t.co/uHhYUI5R7E', 'This sample also includes 61 bursts from 18 repeating sources. Meaning that for the first time we can do a robust comparison between repeaters and so-called ""one-offs"". We see big differences in their total duration (repeaters are longer) and spectrum (repeaters are splotchy). https://t.co/t1nO3rq2Uj', 'Big conclusion from Pleunis et al.: We see significant differences in the time-frequency structure of repeaters and non-repeaters. This is further evidence that they may have different sources or be produced in different ways! Maybe we can start to guess what bursts will repeat! https://t.co/pPVBNsESlC', ""BUT WAIT THERE'S MORE\nA large sample of FRBs means you can also look at how they're spread across the sky, and how they're spread across space. Do they cluster in any interesting ways? Do they??? Well, we have some answers!!"", 'Another companion paper led by @Perimeter PhD student Masoud Rafiei-Ravandi looked at whether the sky distribution of our FRBs lined up with the distribution of large scale structure, like large galaxy clusters. In a technique called ""cross-correlation"".\nhttps://t.co/FZyHlvo1si https://t.co/WD59n9vI8z', 'It\'s technical, but bear with me! Cross-correlation is a mathematical way to see how similar two things are to each other. If two populations (like FRBs and galaxies) have similar sky distributions, they should cross-correlate and you see a ""signal"". Do we see this? YES!!! https://t.co/OoVRHcTLWV', ""Big conclusion from Rafiei-Ravandi et al.: The all-sky sample of FRBs has a similar distribution to galaxies, what you'd expect from a population of FRBs coming from distant galaxies spread throughout the Universe.\n\nThis signal has been long theorized, and finally seen!"", ""BUT WHAT'S THIS? STILL MORE SCIENCE?? BUCKLE UP MY FRIENDS!"", 'In a THIRD companion paper led by McGill PhD student Alex Josephy we look at how the CHIME/FRB Catalog sample is distributed across the sky with respect to our own Milky Way Galaxy.\nhttps://t.co/xnOHK5lq53 https://t.co/Vz99YSH733', 'We know FRBs come from other galaxies, so why does it matter about their distribution with respect to the Milky Way? A long-ago (2014) paper by yours truly showed early evidence that FRBs *avoided* the Galactic plane, maybe they were harder to detect there?', 'But finally with a larger sample we can correct for things like how long we looked, how sensitive our telescope is, and how many FRBs we have over the whole sky. And we see that FRBs are uniformly distributed. No preference or avoidance for the Galactic plane. Good to know! https://t.co/QktTkSczCs', ""SO MUCH SCIENCE TO DIGEST FRIENDS. So I'll let you sit with all that for now, but guess what? There's more to come! CHIME/FRB has more to tell us about all these FRBs in some upcoming papers. And certainly many more FRBs to tell us about in future catalog releases!!!"", ""For now, I want to congratulate the whole CHIME/FRB team. Most of this amazing work happened long before I joined them as one of their project managers, and I'm so proud of this entire team of amazing, resourceful, dedicated, and SUPER SMART people!! Y'all are the FRBest! https://t.co/hFLcLNN3hn""]",https://arxiv.org/abs/2106.04352,"We present a catalog of 536 fast radio bursts (FRBs) detected by the Canadian Hydrogen Intensity Mapping Experiment Fast Radio Burst (CHIME/FRB) Project between 400 and 800 MHz from 2018 July 25 to 2019 July 1, including 62 bursts from 18 previously reported repeating sources. The catalog represents the first large sample, including bursts from repeaters and non-repeaters, observed in a single survey with uniform selection effects. This facilitates comparative and absolute studies of the FRB population. We show that repeaters and apparent non-repeaters have sky locations and dispersion measures (DMs) that are consistent with being drawn from the same distribution. However, bursts from repeating sources differ from apparent non-repeaters in intrinsic temporal width and spectral bandwidth. Through injection of simulated events into our detection pipeline, we perform an absolute calibration of selection effects to account for systematic biases. We find evidence for a population of FRBs - comprising a large fraction of the overall population - with a scattering time at 600 MHz in excess of 10 ms, of which only a small fraction are observed by CHIME/FRB. We infer a power-law index for the cumulative fluence distribution of $\alpha=-1.40\pm0.11(\textrm{stat.})^{+0.06}_{-0.09}(\textrm{sys.})$, consistent with the $-3/2$ expectation for a non-evolving population in Euclidean space. We find $\alpha$ is steeper for high-DM events and shallower for low-DM events, which is what would be expected when DM is correlated with distance. We infer a sky rate of $[820\pm60(\textrm{stat.})^{+220}_{-200}({\textrm{sys.}})]/\textrm{sky}/\textrm{day}$ above a fluence of 5 Jy ms at 600 MHz, with scattering time at $600$ MHz under 10 ms, and DM above 100 pc cm$^{-3}$. ",The First CHIME/FRB Fast Radio Burst Catalog
139,1402622193041915905,3010738324,Ashkan Kazemi,"[""our #ACL2021 claim matching paper with @gvrkiran @DGaff_ @computermacgyve is live! <LINK>\n\nhere's a thread on how we built equitable technology for global fact-checking during my time last year at @meedan + a brand new dataset that includes data from WhatsApp 0/n <LINK>"", 'human fact-checking is high-quality but time-consuming. however, we can still scale it by finding all the content that a fact-check applies to. prior work focuses mostly on English and US politics. but what if we, um, wanted to support fact-checking Tamil content in India? 1/n', 'a few obstacles here: 1) data hungry SOTA NLP doesn\'t work in low-resource environments &amp; 2) there isn\'t a ton of data to train models for lower resource languages like Tamil and Malayalam (hence the term ""low-resource""). what to do? 2/n', 'we used a knowledge distillation approach by Reimers et. al. (EMNLP 2020) and parallel data from OPUS to teach XLM-Roberta to generate good embeddings for some high and low resource languages. We call this model Indian XLM-R or I-XLM-R. 3/n', 'our I-XLM-R model produces embeddings for English and a variety of Indian languages including Hindi, Bengali, Malayalam and Tamil. we use this model on top of the BM25 retrieval system as a scalable multilingual claim matching solution. but how do you evaluate this system? 4/n https://t.co/5JSkCPM66b', ""through @meedan &amp; @gvrkiran 's prior work, we were able to collect data from WhatsApp public groups and tiplines as well as fact-check reports. you can see the details of our dataset curation process in the paper, and the data itself is @ https://t.co/lkKcvleLbd 5/n"", 'we created two multilingual, mainly Indian datasets: a claim detection set (does this text contain check-worthy claims?) and a claim matching set (can these two sets of claims be checked with one fact-check?) 6/n https://t.co/3vf2pvFVUF', 'using the claim matching dataset, we evaluated our proposed system, as well as other SOTA multilingual embedding models: LASER and LaBSE. TL;DR I-XLM-R provided the best overall results in all evaluations. 7/n', 'evaluation #1: can the embeddings of claim pairs alongside their cosine similarity correctly classify matching claims? 8/n https://t.co/hyp9VYwIim', 'evaluation #2: can the embedding similarity of claims be used to rerank and improve BM25 results? 9/n https://t.co/X0w6njoQGl', '@meedan is already using our I-XLM-R based solution in production for some fact-cheking orgs, allowing them to better understand the prevalence of a claim  and increasing their productivity. our approach is not unique to Indian languages &amp; can work for most spoken languages. 10/n', 'although we mainly evaluated I-XLM-R on monolingual claim pairs, we saw that it works decently cross-lingually. we want to explore this more in future work, but our initial findings demonstrate the ability of our models to find similar claims across languages. 11/n https://t.co/P6yzZtnj15', ""this all wouldn't have been possible without the help of my coauthors and many others, including @radamihalcea, the Meedan team and their fact-checking partners, our annotators and the anonymous reviewers who provided us with thoughtful feedback. a big thank you to you all! 12/n""]",https://arxiv.org/abs/2106.00853,"Manual fact-checking does not scale well to serve the needs of the internet. This issue is further compounded in non-English contexts. In this paper, we discuss claim matching as a possible solution to scale fact-checking. We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check. We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing ""claim-like statements"" and then matched with potentially similar items and annotated for claim matching. Our dataset contains content in high-resource (English, Hindi) and lower-resource (Bengali, Malayalam, Tamil) languages. We train our own embedding model using knowledge distillation and a high-quality ""teacher"" model in order to address the imbalance in embedding quality between the low- and high-resource languages in our dataset. We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models, namely LASER and LaBSE. We demonstrate that our performance exceeds LASER and LaBSE in all settings. We release our annotated datasets, codebooks, and trained embedding model to allow for further research. ",Claim Matching Beyond English to Scale Global Fact-Checking
140,1402577462756839428,1065979832306204673,Nathan Grinsztajn,"['New paper out on arxiv: ""There is no Turning Back: A Self-Supervised Approach to Reversibility-Aware RL""!\nüîó: <LINK>\n\nAmazing Inria-Google collaboration with @johanferret, O. Pietquin, M. Geist and P. Preux! <LINK>']",http://arxiv.org/abs/2106.04480,"We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function. ","There Is No Turning Back: A Self-Supervised Approach for
  Reversibility-Aware Reinforcement Learning"
141,1402563924529233922,1240632262858805249,Jos√© Manuel Alarc√≥n,['We posted yesterday the seminal paper of a new formalism that we developed to study the properties of nuclear matter. Take a look on the arXiv: <LINK>'],https://arxiv.org/abs/2106.02652,"We resum the ladder diagrams for the calculation of the energy density $\cal{E}$ of a spin 1/2 fermion many-body system in terms of arbitrary vacuum two-body scattering amplitudes. The partial-wave decomposition of the in-medium two-body scattering amplitudes is developed, and the expression for calculating $\cal{E}$ in a partial-wave amplitude expansion is also given. The case of contact interactions is completely solved and is shown to provide renormalized results, expressed directly in terms of scattering data parameters, within cutoff regularization in a wide class of schemes. $S$- and $P$-wave interactions are considered up to including the first three-terms in the effective-range expansion, paying special attention to the parametric region around the unitary limit. ","Ladder resummation of spin 1/2 fermion many-body systems with arbitrary
  partial-wave content"
142,1402549755889164288,844704885870333952,Canwen Xu,"['(1/2) Excited to share our new paper on knowledge distillation:\nMeta Learning for Knowledge Distillation (<LINK>). We add some ""meta"" magic to KD and ask the student to do a quiz to provide feedback to the teacher!\nw/ @wangchunshu and Julian McAuley <LINK>', 'üöÄ MetaDistil outperforms KD by a large margin and is very competitive with SOTA methods using additional features!\nüòâ The code will be released soon!']",https://arxiv.org/abs/2106.04570,"We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training. We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework. Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner. Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models. ",BERT Learns to Teach: Knowledge Distillation with Meta Learning
143,1402497950539001858,1250971237,Xiaohua Zhai,"['We study scaling laws for Vision Transformer, and characterize the relationships between error rate, model size, data, and compute.\n\nOur ViT-G/14 with 2B params, pre-trained on 3B images, attains a new SOTA on ImageNet of 90.45% accuracy!\n\nPaper: <LINK> <LINK>', 'Bigger models are more sample efficient, reaching the same level of error rate with fewer seen images. \n\nViT-G/14 model reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class, which is less than 1% of the train set! https://t.co/fAlBB6At8T', 'Visualization of ViT-G shape compared to the previous ViT-H shape, together with our memory saving improvements (remove `class` token, use Adafactor optimizer, https://t.co/WNsyjoMNeZ.). https://t.co/fiCeqEavCV', 'Work done at Google Research, Brain Team Z√ºrich, with @__kolesnikov__ , @neilhoulsby  and @giffmana.', '@Eng_Hemdi We deduplicate the 3B pre-train set with BOTH imagenet train set and test set, as well as all other downstream tasks used in our paper. This ensures that the test images are unseen during pre-training.']",https://arxiv.org/abs/2106.04560,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10 examples per class. ",Scaling Vision Transformers
144,1402495856935309312,804069495253962752,David Mart√≠nez Delgado,"['In this new paper (<LINK>), we have traced the full path of the giant stellar stream around the Sombrero galaxy, using deep images taken with a 18-cm telescope. Numerical models by @deniserkal suggest it is the remnant of a dwarf satellite accreted 3 Gyr ago. <LINK>']",http://arxiv.org/abs/2106.04548,"Recent evidence of extremely metal-rich stars found in the Sombrero galaxy (M104) halo suggests that this galaxy has undergone a recent major merger with a relatively massive galaxy. In this paper, we present wide-field deep images of the M104 outskirts obtained with a 18-cm amateur telescope with the purpose of detecting any coherent tidal features from this possible major merger. Our new data, together with a model of the M104 inner halo and scattered light from stars around the field, allow us to trace for the first time the full path of the stream on both sides of the disk of the galaxy. We fully characterize the ring-like tidal structure and we confirm that this is the only observable coherent substructure in the inner halo region. This result is in agreement with the hypothesis that M104 was created by a wet major merger more than 3.5 Gyr ago that heated up the stellar population, blurring all old substructure. We generated a set of numerical models that reproduce the formation of the observed tidal structure. Our best fit model suggests the formation of this stream in the last 3 Gyr is independent of the wet major merger that created the M104 system. Therefore, the formation of the tidal stream can put a constraint on the time when the major merger occurred. ","A feather on the hat: Tracing the giant stellar stream around the
  Sombrero galaxy"
145,1402481442299387906,2776970816,Emir | Ceyani,"['New preprint!\n\nThis work introduces SpreadGNN, a multi-task learning framework to train federated GNNs without ANY central server specifically for molecular property prediction problems. [1/4]\n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'A regular FL system on the left requires a central server, but it is hard to maintain trust and rivalry between institutions when it comes to training collaborative models. Also, generating ""fully"" labeled molecular datasets is costly. [2/4] https://t.co/cluQAh4cbS', 'Prominent federated multi-task learning methods like FMTL cannot work for non-convex models and in the absence of a central server. Our idea is to ""localize"" optimization instead! For this manner, we propose a special optimizer, DPA-SGD, with convergence guarantees [3/4] https://t.co/gdkiW8uUoq', 'Even with randomized neighbors not with a complete topology, SpreadGNN can beat FedAvg easily. This also raises the need for a central server in FL. For more results and convergence analysis, please read our paper [4/4] https://t.co/9tvlYfqoFc']",https://arxiv.org/abs/2106.02743,"Graph Neural Networks (GNNs) are the first choice methods for graph machine learning problems thanks to their ability to learn state-of-the-art level representations from graph-structured data. However, centralizing a massive amount of real-world graph data for GNN training is prohibitive due to user-side privacy concerns, regulation restrictions, and commercial competition. Federated Learning is the de-facto standard for collaborative training of machine learning models over many distributed edge devices without the need for centralization. Nevertheless, training graph neural networks in a federated setting is vaguely defined and brings statistical and systems challenges. This work proposes SpreadGNN, a novel multi-task federated training framework capable of operating in the presence of partial labels and absence of a central server for the first time in the literature. SpreadGNN extends federated multi-task learning to realistic serverless settings for GNNs, and utilizes a novel optimization algorithm with a convergence guarantee, Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized multi-task learning problems. We empirically demonstrate the efficacy of our framework on a variety of non-I.I.D. distributed graph-level molecular property prediction datasets with partial labels. Our results show that SpreadGNN outperforms GNN models trained over a central server-dependent federated learning system, even in constrained topologies. The source code is publicly available at this https URL ","SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural
  Networks"
146,1402366621419720711,1311974157853261824,Mher Safaryan,"['New paper on Smoothness-Aware Quantization Techniques (<LINK>), which improves upon the results obtained for sparsification and broadens the use of smoothness matrices in communication efficient distributed methods. \nJoint work w/ Bokun Wang and @peter_richtarik. <LINK> <LINK>']",https://arxiv.org/abs/2106.03524,"Distributed machine learning has become an indispensable tool for training large supervised machine learning models. To address the high communication costs of distributed training, which is further exacerbated by the fact that modern highly performing models are typically overparameterized, a large body of work has been devoted in recent years to the design of various compression strategies, such as sparsification and quantization, and optimization algorithms capable of using them. Recently, Safaryan et al (2021) pioneered a dramatically different compression design approach: they first use the local training data to form local {\em smoothness matrices}, and then propose to design a compressor capable of exploiting the smoothness information contained therein. While this novel approach leads to substantial savings in communication, it is limited to sparsification as it crucially depends on the linearity of the compression operator. In this work, we resolve this problem by extending their smoothness-aware compression strategy to arbitrary unbiased compression operators, which also includes sparsification. Specializing our results to quantization, we observe significant savings in communication complexity compared to standard quantization. In particular, we show theoretically that block quantization with $n$ blocks outperforms single block quantization, leading to a reduction in communication complexity by an $\mathcal{O}(n)$ factor, where $n$ is the number of nodes in the distributed system. Finally, we provide extensive numerical evidence that our smoothness-aware quantization strategies outperform existing quantization schemes as well the aforementioned smoothness-aware sparsification strategies with respect to all relevant success measures: the number of iterations, the total amount of bits communicated, and wall-clock time. ",Smoothness-Aware Quantization Techniques
147,1402305593793073155,400026483,Oem Trivedi,"['My new paper on Lorentz violating Inflation and the Swampland divinely hit the arxiv today ! Link - <LINK>\n\nSo now its time to unpack it üòáüòÅ\n\nA thread ..... 1/n', ""Swampland conjectures are certain field theoretic criterion which have been reasoned out from String theory. The premise of these criterion is that these are the requirements for any low energy EFT's to have a consistent UV Completion 2/n"", 'The story for this paper starts of with the paper by @WKCosmo @SunnyVagnozzi @lucavisinelli ( https://t.co/hj8Eh5rSUR), which showed that GR based single field Inflation is in almost unavoidable conflicts with these criterion and hence, possibly with Quantum gravity. 3/n', 'The implications in that paper were so strong that even models like Higgs inflation got into tensions with the conjectures (https://t.co/YEA1OvsA7v). However, it has been shown that single field models in non GR Cosmologies or in Warm inflation models can be consistent ..4/n', 'And also, multi field Inflationary models can be in amicable terms with the conjectures too. I myself was of the thought that there cannot be essentially GR based inflation scenarios which would still be consistent with the conjectures and I was quite firm in this regard ..5/n', 'That was until I had the opportunity to attend the 4th IUCSS Summer  school on Lorentz and CPT violations and Standard model extension hosted by Indiana University Bloomington. While attending the school, I was fortunate to learn a lot about Lorentz violations and ...6/n', 'I began to wonder about its implications to Cosmology. Eventually, I stumbled upon how one can consider single field Inflation in a Lorentz violating background. So in this paper, I considered a time-like Lorentz violating background for inflation in an essentially GR regime .7/n', 'So in the paper, I consider this scenario. What is then shown in the paper is that the Swampland conjectures imply a lower bound on the Lorentz violating parameter for these Inflationary models and this one bound can satisfy all the swampy issues of inflation.. 8/n', 'Now here comes the fun part, we consider models of Inflation in this Lorentz violating scenario which would have been in the Swampland in usual GR based single field Inflation. These models are the Higgs model, Radion gauge model( found by @malcfairbairn ) and..9/n', 'A spontaneous symmetry breaking model of Inflation. I also consider a quadratic Inflation tbh but I dont work out the parameter myself but take it from another work and this model is still in the Swampland even with Lorentz violations(these models have issues with data too)..10/n', 'What is very exciting tho is that Higgs, Radion gauge and the spontaneous symmetry breaking models are EASILY consistent with the conjectures here! This is exciting because these models would have had unavoidable swamp issues in usual GR based single field Inflation..11/n', 'But from these examples, it is shown that single field Inflation can STILL BE CONSISTENT with the Swampland conjectures in an essentially GR based Cosmology !ü§©These examples cover the full range from 0 to 2 free parameter models, all being consistent!.. 12/n', 'So this goes to show that inflation can be consistent with the Swampland even in a GR based Cosmology, when one considers some Lorentz violations in the background! This is a very nice outcome, because this possibly points towards the notion that.. 13/n', 'Quantum gravity(whatever that is) might indicate towards a greater significance of Lorentz violations in the early Universe than one envisages at this time, as previously swamp inconsistent regimes become consistent by just considering a certain form of these violations ..14/n', ""That's the big outcome from the paper! Thank you so much for reading it through eveyeone, hope you liked this thread and the paper too !üòáüôèüèªüòÅü•≥"", '@adamsolo Thanks a ton Adam! Your paper on non canonical inflation and Swampland was a treat to read btw üòÅü•≥', '@WKCosmo Thank you so much  ! This work has sprung out due to that paper of yours and also your other papers on the TCC which were very insightful! So thanks a ton for those works too']",https://arxiv.org/abs/2106.03578,"The swampland conjectures from String theory have had very interesting implications for cosmology and particularly for Inflation. It has been shown that the single field inflationary models in a GR based cosmology are in unavoidable tensions with these conjectures, while these single field models can still be consistent in certain non-trivial inflationary regimes. So it becomes interesting to see whether there is a way to overcome the issues of the swampland and single field inflation in an essentially GR based cosmology. We show that this can indeed be the case and for this, we consider a certain type of Lorentz violating inflationary scenario. We work out the swampland bounds for Lorentz violating inflationary models after which show that inflationary models which would have had otherwise serious tensions with these conjectures in a usual GR based scenario, can be very tranquil with the criterion in this regime. For this we take examples of Higgs inflation, radion gauge inflation and Spontaenous symmetry breaking inflation and show that the bounds imposed by the swampland on the Lorentz violating parameter can be easily satisfied in these models. ",Lorentz violating inflation and the Swampland
148,1402251145255477254,918851521416134656,Chin-Wei Huang,"['Super excited to share our new theory paper on connecting diffusion models &amp; score matching from a variational perspective (i.e. likelihood training)!\n\n<LINK>\n\nWe derive a new ELBO for general continuous-time diffusion models.\nw/ @jaehyunlim0606 &amp; @AaronCourville <LINK>', 'TL;DR: We derive a new lower bound on the likelihood of a diffusion model.\nThe lower bound subsumes continuous-time flows &amp; score-based models (with certain loss weighting) as special cases, explaining the success of reverse SDE and equiv ODE proposed in \nhttps://t.co/x6sWhII3Z5', 'Marginalization: Consider a generative SDE: dX = Œºdt+œÉdBt, with X0~N(0,1)\nUsing the Feynman-Kac formula, we show that the marginal density of Xt can be expressed as an expectation. \n\nThis can be seen as a mixture of continuous-time flow. https://t.co/S8Pr3kxcMj', 'Lower bounding: Using your favorite Jensen‚Äôs inequality + Girsanov theorem, we introduce an inference SDE and lower bound the likelihood. \n\n(It‚Äôs the same procedure as deriving the ELBO for a VAE; all the technicality is to deal with an infinite-dim latent space.) https://t.co/Blom3o4XNW', 'Consistency: We show that this continuous-time (CT) ELBO is consistent with the traditional variational framework. Let ùìîL denote the ELBO of the discretized diffusion model. We show that ùìîL-&gt;ùìî‚àû. This means maximizing CT ELBO is equivalent to training an infinitely-deep VAE. https://t.co/lkQgF8PFJd', 'Score matching: Finally, we show that if we reparameterize our generative &amp; inference SDEs as \ndX = (g^2s - f)dt + gdB and dY= fdt + gdB, then the CT-ELBO boils down to score-matching loss after rearrangement. https://t.co/8EWmqXJnPu', 'A continuum of ELBOs: More? We show that this procedure can be used to derive an ELBO of a family of ‚Äúplug-in reverse‚Äù SDEs, including the reverse SDE &amp; equiv ODE used in Song et al. as special cases Œª=0 &amp; Œª=1 --- min score-matching loss = max ELBO of the entire family. https://t.co/fRdX1DrXBh', 'Concurrently to us, @YSongStanford &amp; @conormdurkan also recently arXived their paper on the same topic. Check out their impressive empirical result that complements ours!\nhttps://t.co/2NkTlTWech\n\nHere‚Äôs a list of diff in the main technical contribs. https://t.co/bdtOQXmT7c']",https://arxiv.org/abs/2106.02808,"Discrete-time diffusion-based generative models and score matching methods have shown promising results in modeling high-dimensional image data. Recently, Song et al. (2021) show that diffusion processes that transform data into noise can be reversed via learning the score function, i.e. the gradient of the log-density of the perturbed data. They propose to plug the learned score function into an inverse formula to define a generative diffusion process. Despite the empirical success, a theoretical underpinning of this procedure is still lacking. In this work, we approach the (continuous-time) generative diffusion directly and derive a variational framework for likelihood estimation, which includes continuous-time normalizing flows as a special case, and can be seen as an infinitely deep variational autoencoder. Under this framework, we show that minimizing the score-matching loss is equivalent to maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed by Song et al. (2021), bridging the theoretical gap. ","A Variational Perspective on Diffusion-Based Generative Models and Score
  Matching"
149,1402241733262458891,1024678848422653952,Stef Czischek,['State-of-the-art tensor network simulations based on #PastaQ @ITensorLib capture measurement-induced entanglement phase transitions in hybrid circuits with trapped ion native gates\n\nCheck out our new paper <LINK> \nWith @giactorlai @muniasr Rajibul Islam @rgmelko <LINK>'],https://arxiv.org/abs/2106.03769,"The rise of programmable quantum devices has motivated the exploration of circuit models which could realize novel physics. A promising candidate is a class of hybrid circuits, where entangling unitary dynamics compete with disentangling measurements. Novel phase transitions between different entanglement regimes have been identified in their dynamical states, with universal properties hinting at unexplored critical phenomena. Trapped ion hardware is a leading contender for the experimental realization of such physics, which requires not only traditional two-qubit entangling gates, but a constant rate of local measurements accurately addressed throughout the circuit. Recent progress in engineering high-precision optical addressing of individual ions makes preparing a constant rate of measurements throughout a unitary circuit feasible. Using tensor network simulations, we show that the resulting class of hybrid circuits, prepared with native gates, exhibits a volume-law to area-law transition in the entanglement entropy. This displays universal hallmarks of a measurement-induced phase transition. Our simulations are able to characterize the critical exponents using circuit sizes with tens of qubits and thousands of gates. We argue that this transition should be robust against additional sources of experimental noise expected in modern trapped ion hardware, and will rather be limited by statistical requirements on post selection. Our work highlights the powerful role that tensor network simulations can play in advancing the theoretical and experimental frontiers of critical phenomena. ","Simulating a measurement-induced phase transition for trapped ion
  circuits"
150,1402240883261689857,859294200,Eric Malmi,"['üìùNew #ACL2021NLP Paperüìù\nPaper: <LINK>\nData: <LINK>\n\nOn the benefits of large LMs: We find that grammatical error correction (GEC) performance depends frustratingly(?) heavily on model size. More belowüëá <LINK>', 'Our synthetically pre-trained 11B-param T5 model yields new SOTA for English, German, Russian, and Czech GEC. \n\nSince the model is costly and impractical to run, we use it to create a cleaned version of the popular Lang-8 corpus to distill the model to smaller ones.', ""I view Grammatical Error Correction as a great example of beneficial applications of large LMs:\n1. GEC can level the playing field between native and non-native speakers.\n2. GEC models don't try to generate new content but only to fix grammatical errors""]",https://arxiv.org/abs/2106.03830,"This paper presents a simple recipe to train state-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a cLang-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages -- we demonstrate that performing a single fine-tuning step on cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English. ",A Simple Recipe for Multilingual Grammatical Error Correction
151,1402184147762925569,481539448,Richard Alexander,"['New paper, led by Alison Young, with @bec_nealon, @cwalshastrochem, @Alice_Centauri, and Christophe Pinte. Alison used hydrodynamics, chemistry, and radiative transfer models, to investigate how chemical changes can trace warps in planet-forming discs.\n\n<LINK>', 'Protoplanetary discs (unlike discs around black holes or compact objects) are mainly heated by light from the star. So when a disc becomes warped, the misaligned (inner) part casts shadows that change the (outer) disc temperature - the disc has a ""hot side"" and a ""cold side"". https://t.co/pt7rVGNeWW', 'These azimuthal temperature variations affect the disc chemistry, and Alison showed that they cause variations in the chemical abundances. The chemistry therefore traces the warped disc structure - the dominant molecules differ on the ""bright side"" vs the ""dark side"". https://t.co/OFHafebw5n', 'These abundance and temperature variations are then observable in line emission, and we expect to see clear asymmetries in the emission maps made with ALMA. This gives us a new way to probe the structure of planet-forming discs.üòÄ https://t.co/8rXyUoL01S', 'This is just a quick summary of a huge amount of work though, and there is *way* more in the paper than I can cover in a few tweets. So click through for all the details:\nhttps://t.co/4E2q7qqSNh']",https://arxiv.org/abs/2106.02660,"Circumstellar discs may become warped or broken into distinct planes if there is a stellar or planetary companion with an orbit that is misaligned with respect to the disc. There is mounting observational evidence for protoplanetary discs with misaligned inner discs and warps that may be caused by such interactions with a previously undetected companion, giving us a tantalising indication of possible planets forming there. Hydrodynamical and radiative transfer models indicate that the temperature varies azimuthally in warped discs due to the variable angle at which the disc surface faces the star and this impacts the disc chemistry. We perform chemical modelling based on a hydrodynamical model of a protoplanetary disc with an embedded planet orbiting at a 12$^{\circ}$ inclination to the disc. Even for this small misalignment, abundances of species including CO and HCO$^+$ vary azimuthally and this results in detectable azimuthal variations in submillimetre line emission. Azimuthal variations in line emission may therefore indicate the presence of an unseen embedded companion. Nonaxisymmetric chemical abundances should be considered when interpreting molecular line maps of warped or shadowed protoplanetary discs. ",Chemical signatures of a warped protoplanetary disc
152,1402171755129622530,1369326436864102401,Marlin Sch√§fer,"[""I'm happy to share that today our new paper in collaboration with the Uni Jena appeared on arXiv! We tested how training networks differently influences their GW detection capability and found an interesting improvement to the basic architecture.\n<LINK>""]",https://arxiv.org/abs/2106.03741,"Compact binary systems emit gravitational radiation which is potentially detectable by current Earth bound detectors. Extracting these signals from the instruments' background noise is a complex problem and the computational cost of most current searches depends on the complexity of the source model. Deep learning may be capable of finding signals where current algorithms hit computational limits. Here we restrict our analysis to signals from non-spinning binary black holes and systematically test different strategies by which training data is presented to the networks. To assess the impact of the training strategies, we re-analyze the first published networks and directly compare them to an equivalent matched-filter search. We find that the deep learning algorithms can generalize low signal-to-noise ratio (SNR) signals to high SNR ones but not vice versa. As such, it is not beneficial to provide high SNR signals during training, and fastest convergence is achieved when low SNR samples are provided early on. During testing we found that the networks are sometimes unable to recover any signals when a false alarm probability $<10^{-3}$ is required. We resolve this restriction by applying a modification we call unbounded Softmax replacement (USR) after training. With this alteration we find that the machine learning search retains $\geq 91.5\%$ of the sensitivity of the matched-filter search down to a false-alarm rate of 1 per month. ",Training Strategies for Deep Learning Gravitational-Wave Searches
153,1402138605955526658,191413731,Wil van der Aalst,"['New paper (accepted for FI) with surprising new results for free-choice nets, e.g., free-choice nets with home clusters are lucent even when they are not well-formed! More general and without using the standard #PetriNet machinery, see <LINK>. #weekendprojects <LINK>']",https://arxiv.org/abs/2106.03554,"A marked Petri net is lucent if there are no two different reachable markings enabling the same set of transitions, i.e., states are fully characterized by the transitions they enable. Characterizing the class of systems that are lucent is a foundational and also challenging question. However, little research has been done on the topic. In this paper, it is shown that all free-choice nets having a home cluster are lucent. These nets have a so-called home marking such that it is always possible to reach this marking again. Such a home marking can serve as a regeneration point or as an end-point. The result is highly relevant because in many applications, we want the system to be lucent and many well-behaved process models fall into the class identified in this paper. Unlike previous work, we do not require the marked Petri net to be live and strongly connected. Most of the analysis techniques for free-choice nets are tailored towards well-formed nets. The approach presented in this paper provides a novel perspective enabling new analysis techniques for free-choice nets that do not need to be well-formed. Therefore, we can also model systems and processes that are terminating and/or have an initialization phase. ",Free-Choice Nets With Home Clusters Are Lucent
154,1402110089801179139,110103071,Andrej Risteski,"['New paper with new student @_Yuchen_Li_ , at ACL\'21: \n<LINK> \nRecent approaches to incorporating syntax in neural architectures are often based on learning a syntactic distance which can be used to ""greedily"" build up a parse tree. (e.g. PRPN, ON-LSTM, ...)', 'In this paper, we explore how much their representational power is affected by the amount of ""context"" they are allowed. The context for calculating the syntactic distance at a given position is the surrounding window that the model is allowed to consider to calculate it.', 'We show that if the context is limited in either direction (i.e. possibly infinite window in one direction, finite in the other), the model can be ""arbitrarily wrong"". To formalize this, we consider as a sandbox the setting of PCFGs.', 'We show that for any particular bound on the context, there exists a PCFG, s.t. the probability (under the sentences generated from grammar) that any syntactic distance induces the maximum likelihood parse is arbitrarily low.', 'The hard instances formalize the intuition (e.g. in English) that often to disambiguate the correct parse we need to read-ahead a lot (possibly see the entire sentence). https://t.co/I2GuSAPczv', 'This formalizes a conjecture by @phu_pmh  @sleepinyourhat  @kchonyc in https://t.co/B5tgWx7DIf that unidirectional models may struggle with capturing complex syntactic relationships. [P.S. results in the paper also for transition-based approaches ala https://t.co/4OYGkRytws]']",https://arxiv.org/abs/2106.01580,"Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits. For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing. Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019). Most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like F-1 score). However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind. In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent? Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision. We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG. ",The Limitations of Limited Context for Constituency Parsing
155,1402099463922106370,1206262662562242560,Yixuan Wang,"['Our new paper addressing the controller design and verification is now available at <LINK>. By integrating the verification in a closed-loop manner, i.e. design-while-verify, the synthesized controller is formally guaranteed to be reach-avoid.']",https://arxiv.org/abs/2106.03245,"In the current control design of safety-critical autonomous systems, formal verification techniques are typically applied after the controller is designed to evaluate whether the required properties (e.g., safety) are satisfied. However, due to the increasing system complexity and the fundamental hardness of designing a controller with formal guarantees, such an open-loop process of design-then-verify often results in many iterations and fails to provide the necessary guarantees. In this paper, we propose a correct-by-construction control learning framework that integrates the verification into the control design process in a closed-loop manner, i.e., design-while-verify. Specifically, we leverage the verification results (computed reachable set of the system state) to construct feedback metrics for control learning, which measure how likely the current design of control parameters can meet the required reach-avoid property for safety and goal-reaching. We formulate an optimization problem based on such metrics for tuning the controller parameters, and develop an approximated gradient descent algorithm with a difference method to solve the optimization problem and learn the controller. The learned controller is formally guaranteed to meet the required reach-avoid property. By treating verifiability as a first-class objective and effectively leveraging the verification results during the control learning process, our approach can significantly improve the chance of finding a control design with formal property guarantees. This is demonstrated via a set of experiments on both linear and non-linear systems that use model-based or neural network based controllers. ","Verification in the Loop: Correct-by-Construction Control Learning with
  Reach-avoid Guarantees"
156,1401953995535446018,1115626019535200256,Pramodh Senarath Yapa,"[""Here's a new paper with @5thsound's group that appeared on the arXiv over the weekend!\n\nMatt Rudd did most of the work on this, but I got to partially supervise the project so that was a fun new experience! üòä \n\nQuick summary belowüëá 1/4\n<LINK> <LINK>"", 'Like rooms in your house, superfluid Helium-3 can have walls separating different (but energetically equivalent) regions within it. In this paper, we look at the stability of these walls.üß±\n\nTo paraphrase Reagan, ""Mr Gorbachev, how much does it cost to tear down these walls?"" 2/4', 'This kind of analysis has been done before - in fact, we closely followed this paper* by Silveri et al. But those results are only valid for a sliver of the phase diagram. We extended these results by incorporating experimental parameters. 3/4\n\n*https://t.co/b3eNfZQr5v', 'We found that the walls get weaker as you increase the pressure/temperature. These walls have a surprising connection to structures in cosmology*, so we can hopefully also gain insight into them by studying walls in Helium-3! 4/4\n\n*https://t.co/AtWokZH5BP', ""Also, funny factoid I just discovered about my publishing history: the penultimate word in all of the papers I've been involved in begin with the prefix 'super-'. ü§î\n\nLet's see how long I can keep this streak going! https://t.co/ZXQJBCaWCY""]",https://arxiv.org/abs/2106.02065,"Domain walls in superfluid 3He-B have gained renewed interest in light of experimental progress on confining helium in nanofabricated geometries. Here, we study the effect of strong-coupling corrections on domain wall width and interfacial tension by determining self-consistent solutions to spatially-dependent Ginzburg-Landau equations. We find that the formation of domain walls is generally energetically favored in strong coupling over weak coupling. Calculations were performed over a wide range of temperatures and pressures, showing decreasing interface energy with increasing temperature and pressure. This has implications for the observability of such domain walls in 3He-B, which are of both fundamental interest and form the basis for spatially-modulated pair-density wave states, when stabilized by strong confinement. ",Strong-coupling corrections to hard domain walls in superfluid 3He-B
157,1401891255374778371,352048533,Sebasti√°n Marino,"['New paper on the observability of the vertical shear instability using CO ALMA observations <LINK>, led by PhD student\nMarcelo Barraza. Using 3DHydro+RT+ALMAsim, we found the VSI produces detectable velocity perturbations in disks with inc&lt;35deg (with @sebanube)']",https://arxiv.org/abs/2106.01159,"Context. Dynamical and turbulent motions of gas in a protoplanetary disk are crucial for their evolution and affect planet formation. Recent observations suggest weak turbulence in the disk's outer regions. However, the physical mechanism of turbulence in these outer regions remains uncertain. The vertical shear instability (VSI) is a promising mechanism to produce turbulence in disks. Aims. Our aim is to study the observability of the gas velocity structure produced by the VSI via CO kinematics with ALMA. Methods. We perform global 3D hydrodynamical simulations of a VSI-unstable disk. We post-process the simulation results with radiative transfer calculations, and produce synthetic predictions of CO rotational emission lines. Following, we compute the line of sight velocity map, and its deviations from a sub-Keplerian equilibrium solution. We explore the detectability of the VSI by identifying kinematic signatures using realistic simulated observations. Results. Our 3D simulations of the VSI show the steady state dynamics of the gas in great detail. From the velocity structure we infer a turbulent stress value of $\alpha_{r\phi}=1.4 \times 10^{-4}$. On large scales, we observe velocity deviations of 50 m s$^{-1}$ as axisymmetric rings. We find optimal conditions at $i \lesssim 20^{\circ}$ to trace for the kinematic structures of the VSI. We found that current diagnostics to constrain gas turbulence from non-thermal broadening of the line emission are not applicable to anisotropic VSI turbulence. Conclusions. The detection of kinematic signatures produced by the VSI is possible with ALMA. Observations including an extended antenna configuration combined with the highest spectral resolution available are needed for robust detection. The characterization of the large-scale velocity perturbations is required to constrain the turbulence level produced by the VSI from gas observations. ","Observability of the Vertical Shear Instability in protoplanetary disk
  CO kinematics"
158,1401853459557761024,835960362012971009,Michael Vasmer,"['I have a new paper out on the arXiv today! \n<LINK>', 'In this joint work with Alex Kubica, we introduce a subsystem version of the 3D toric code, which supports single-shot error correction and can be realized with parity checks of weight at most three!', 'We found that the 3D subsystem toric code has a storage threshold of ~1% and moreover that the threshold is stable over multiple rounds of error correction! https://t.co/8Apq4tlpFW', '@QuantumIQC @Perimeter']",https://arxiv.org/abs/2106.02621,"We introduce a new topological quantum code, the three-dimensional subsystem toric code (3D STC), which is a generalization of the stabilizer toric code. The 3D STC can be realized by measuring geometrically-local parity checks of weight at most three on the cubic lattice with open boundary conditions. We prove that single-shot quantum error correction (QEC) is possible with the 3D STC, i.e., one round of local parity-check measurements suffices to perform reliable QEC even in the presence of measurement errors. We also propose an efficient single-shot QEC strategy for the 3D STC and investigate its performance. In particular, we numerically estimate the resulting storage threshold against independent bit-flip, phase-flip and measurement errors to be $p_\text{STC} \approx 1.045\%$. Such a high threshold together with local parity-check measurements of small weight make the 3D STC particularly appealing for realizing fault-tolerant quantum computing. ","Single-shot quantum error correction with the three-dimensional
  subsystem toric code"
159,1401850630105403394,513464916,Carlos S√°nchez Mu√±oz,"[""My new paper with @AVivasViana is out on the arXiv!\n\n<LINK>\n\nWe study the regime of coherent two-photon excitation of coupled, non-identical quantum emitters. You can have a look at Alejandro's thread üëá <LINK>""]",https://arxiv.org/abs/2106.02574,"We study a system of two interacting, non-indentical quantum emitters driven by a coherent field. We focus on the particular condition of two-photon resonance and obtain analytical expressions for the stationary density matrix of the system and observables of the fluorescent emission. Importantly, our expressions are valid for the general situation of non-identical emitters with different transition energies. Our results allow us to determine the regime of parameters in which coherent two-photon excitation, enabled by the coherent coupling between emitters, is dominant over competing, first-order processes. Using the formalism of quantum parameter estimation, we show that the features imprinted by the two-photon dynamics into the spectrum of resonance fluorescence are particularly sensitive to changes in the distance between emitters, making two-photon excitation the optimal driving regime for the estimation of inter-emitter distance. This can be exploited for applications such as superresolution imaging of point-like sources. ","Two-photon resonance fluorescence of two interacting non-identical
  quantum emitters"
160,1401797676719644672,1149446037028954112,Larissa Triess,['Are you interested in #DeepLearning based domain adaptation and #LiDAR #perception? Then check out our new #survey paper!\n\n<LINK>'],https://arxiv.org/abs/2106.02377,"Scalable systems for automated driving have to reliably cope with an open-world setting. This means, the perception systems are exposed to drastic domain shifts, like changes in weather conditions, time-dependent aspects, or geographic regions. Covering all domains with annotated data is impossible because of the endless variations of domains and the time-consuming and expensive annotation process. Furthermore, fast development cycles of the system additionally introduce hardware changes, such as sensor types and vehicle setups, and the required knowledge transfer from simulation. To enable scalable automated driving, it is therefore crucial to address these domain shifts in a robust and efficient manner. Over the last years, a vast amount of different domain adaptation techniques evolved. There already exists a number of survey papers for domain adaptation on camera images, however, a survey for LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated driving that provides detailed 3D scans of the vehicle's surroundings. To stimulate future research, this paper presents a comprehensive review of recent progress in domain adaptation methods and formulates interesting research questions specifically targeted towards LiDAR perception. ",A Survey on Deep Domain Adaptation for LiDAR Perception
161,1401746811484880903,1060158595076182016,Justin Edwards,"['How should smart devices talk to us if they want to initiate an interaction? How do we talk to people who are busy? My new paper, Eliciting Spoken Interruptions to Inform Proactive Speech Agent Design w/ Chris Janssen @UtrechtUni @BCowanHCI and @sjjgo\n<LINK>', 'In this one, we get participants to ask questions to a Tetris playing ""partner"" introducing a new gamified online paradigm for collecting conversational  design insight. We look at how urgency affects these interruptions and categorize some of the different strategies people use.', ""I'll be presenting this paper at @CUI_Conference this July, where I hope it can spark discussion about the future of proactive voice agents #voicefirst #ConversationDesign #voiceAI. Thanks to @AdaptCentrefor supporting this work, a foundational part of my PhD.""]",https://arxiv.org/abs/2106.02077,"Current speech agent interactions are typically user-initiated, limiting the interactions they can deliver. Future functionality will require agents to be proactive, sometimes interrupting users. Little is known about how these spoken interruptions should be designed, especially in urgent interruption contexts. We look to inform design of proactive agent interruptions through investigating how people interrupt others engaged in complex tasks. We therefore developed a new technique to elicit human spoken interruptions of people engaged in other tasks. We found that people interrupted sooner when interruptions were urgent. Some participants used access rituals to forewarn interruptions, but most rarely used them. People balanced speed and accuracy in timing interruptions, often using cues from the task they interrupted. People also varied phrasing and delivery of interruptions to reflect urgency. We discuss how our findings can inform speech agent design and how our paradigm can help gain insight into human interruptions in new contexts. ",Eliciting Spoken Interruptions to Inform Proactive Speech Agent Design
162,1400877275357270016,54910963,Clara Vania,"['Many NLU datasets have been created to evaluate various aspects of language, but which datasets are still effective to measure future progress? Check out our new paper, to appear at #acl2021nlp #NLProc  <LINK> (1/8) <LINK>', 'Taking inspiration from psychometric studies which often use Item Response Theory (IRT) to evaluate test items in educational assessment, we apply it to evaluate test examples from 29 English datasets. (2/8)', 'We use predictions from 18 Transformer-based models with varying degrees of abilities, and estimate how difficult and discriminative a test example is relative to other examples. (3/8)', 'We introduce a new metric called Locally Estimated Headroom (LEH) to estimate how much a dataset is still useful to measure near-future progress. We find that Quoref, HellaSwag, and MC-TACO are still effective, while SNLI, MNLI, and BoolQ seem to be saturated. (4/8) https://t.co/N0fw6xF6d7', 'We also find that span-based QA is the most effective task format to discriminate between strong and weak models. (5/8)', 'However, datasets that contain many discriminative examples do not always have examples that are the most difficult. SQuAD2.0, QuAIL, and ANLI appear to have many examples with the highest difficulty levels. (6/8) https://t.co/l1E9Qgfktz', 'Please see the paper for more details. We hope this work can give insights to future dataset creation and model development in NLP, and we argue that this evaluation should be done periodically over time. (7/8)', 'Joint work with amazing co-authors: @phu_pmh, @WillHuang93, Dhara Mungra, @yzpang97, @zhansheng, @liu_haokun, @kchonyc, and @sleepinyourhat. (8/8)']",https://arxiv.org/abs/2106.00840,"Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks. Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress. What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements? To measure this uniformly across datasets, we draw on Item Response Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples. We find that Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models. We also observe span selection task format, which is used for QA datasets like QAMR or SQuAD2.0, is effective in differentiating between strong and weak models. ",Comparing Test Sets with Item Response Theory
163,1400864196305297408,2485053080,Swarnadeep Saha,"['New #NAACL2021 paper (next Tues) on explaining compositional reasoning w/ multiple proof graphs ""multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning""\n \nPaper: <LINK>\nCode: <LINK>\n \n@prateeky2806 @mohitban47\n1/n <LINK>', 'Compositional reasoning is not always unique -- there can be multiple ways of reaching the correct answer. With multiPRover, we extend our #EMNLP2020 work on PRover (which generates a single proof) by now tackling a more challenging task of generating a *set* of proof graphs.\n2/n', 'We find that PRover, when asked to generate top-k proofs, does not perform well &amp; that multiple proofs for a ques. often have common subgraphs between them. So, to jointly learn from all proofs &amp; exploit their correlations better, we pose it as a (graph) set-generation task.\n3/n https://t.co/odAjr2k0B5', 'We propose 2 multiPRover models: (1) Multilabel-multiPRover generating a set of proofs via multi-label classification &amp; implicit conditioning between proofs, (2) Iterative-multiPRover generating proofs iteratively by explicitly conditioning on the previously generated proofs\n4/n', 'Both multiPRover models show signif. improvements on all synthetic, zero-shot, &amp; human-paraphrased datasets (from RuleTakers). Iterative-multiPRover also outperforms PRover on a zero-shot dataset with all single-proof examples &amp; has better generalization to higher depth ques.\n5/n https://t.co/fcRZ9SPpq8']",https://arxiv.org/abs/2106.01354,"We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent. Our code and models are publicly available at this https URL ","multiPRover: Generating Multiple Proofs for Improved Interpretability in
  Rule Reasoning"
164,1400852486190161926,45144224,Ben Green,"['Really excited to share a new paper draft! I survey the significant limits and fault lines of ‚Äútech ethics,‚Äù characterizing tech ethics as a terrain of contestation where the central struggle is over *what* ethics entails and *who* gets to define it.\n\nüìë: <LINK> <LINK>', 'With these limits of current approaches to tech ethics in mind, I sketch out a sociotechnical frame that focuses on the formulation and real-world effects of tech ethics, focused on determinism, solutionism, objectivity and neutrality, and sociotechnical systems.', 'This essay will eventually be part of a much larger collection of critical and interdisciplinary essays with friends/colleagues from @BKCHarvard analyzing the opportunities and limits of tech ethics as it has evolved in recent years -- stay tuned!', 'And special thanks to @Elibietti @annaeveryday @JennyKorn @kathytpham @luke_stark @SAWEvans @HarvardSTS for their excellent comments and guidance on earlier versions of this paper!', '@VaroonMathur Thanks Varoon!!']",https://arxiv.org/abs/2106.01784,"This article introduces the special issue ""Technology Ethics in Action: Critical and Interdisciplinary Perspectives"". In response to recent controversies about the harms of digital technology, discourses and practices of ""tech ethics"" have proliferated across the tech industry, academia, civil society, and government. Yet despite the seeming promise of ethics, tech ethics in practice suffers from several significant limitations: tech ethics is vague and toothless, has a myopic focus on individual engineers and technology design, and is subsumed into corporate logics and incentives. These limitations suggest that tech ethics enables corporate ""ethics-washing"": embracing the language of ethics to defuse criticism and resist government regulation, without committing to ethical behavior. Given these dynamics, I describe tech ethics as a terrain of contestation where the central debate is not whether ethics is desirable, but what ""ethics"" entails and who gets to define it. Current approaches to tech ethics are poised to enable technologists and technology companies to label themselves as ""ethical"" without substantively altering their practices. Thus, those striving for structural improvements in digital technologies must be mindful of the gap between ethics as a mode of normative inquiry and ethics as a practical endeavor. In order to better evaluate the opportunities and limits of tech ethics, I propose a sociotechnical approach that analyzes tech ethics in light of who defines it and what impacts it generates in practice. ","The Contestation of Tech Ethics: A Sociotechnical Approach to Technology
  Ethics in Practice"
165,1400783082672824323,1248597446848102401,Jacob Zavatone-Veth,"['Happy to share a new paper on representation learning in wide, finite Bayesian neural networks with @canatar_a and @CPehlevan: <LINK>!']",https://arxiv.org/abs/2106.00651,"Recent works have suggested that finite Bayesian neural networks may sometimes outperform their infinite cousins because finite networks can flexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of finite networks differ from the fixed representations of infinite networks remains incomplete. Perturbative finite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading finite-width corrections to the average feature kernels for any Bayesian network with linear readout and Gaussian likelihood have a largely universal form. We illustrate this explicitly for three tractable network architectures: deep linear fully-connected and convolutional networks, and networks with a single nonlinear hidden layer. Our results begin to elucidate how task-relevant learning signals shape the hidden layer representations of wide Bayesian neural networks. ","Asymptotics of representation learning in finite Bayesian neural
  networks"
166,1400730943850225668,994866870,Dr Steph Yardleyüåû,"['Our new paper on the presence of upflows in solar active regions is now available on arXiv: <LINK> What a nice end to the week! üåûüòé @realDavidBrooks @qsl_deb', '@erikapal @realDavidBrooks @qsl_deb I think it already started on acceptance day ü§£', '@erikapal @realDavidBrooks @qsl_deb https://t.co/9s3YBYUFjU']",http://arxiv.org/abs/2106.01396,"We performed a systematic study of 12 active regions (ARs) with a broad range of areas, magnetic flux and associated solar activity in order to determine whether there are upflows present at the AR boundaries and if these upflows exist, whether there is a high speed asymmetric blue wing component present in the upflows. To identify the presence and locations of the AR upflows we derive relative Doppler velocity maps by fitting a Gaussian function to {\it Hinode}/EIS Fe XII 192.394\,\AA\ line profiles. To determine whether there is a high speed asymmetric component present in the AR upflows we fit a double Gaussian function to the Fe XII 192.394\,\AA\ mean spectrum that is computed in a region of interest situated in the AR upflows. Upflows are observed at both the east and west boundaries of all ARs in our sample with average upflow velocities ranging between -5 to -26~km s$^{-1}$. A blue wing asymmetry is present in every line profile. The intensity ratio between the minor high speed asymmetric Gaussian component compared to the main component is relatively small for the majority of regions however, in a minority of cases (8/30) the ratios are large and range between 20 to 56~\%. These results suggest that upflows and the high speed asymmetric blue wing component are a common feature of all ARs. ",Widespread Occurrence of High-Velocity Upflows in Solar Active Regions
167,1400726331579568128,419500182,Neil Ketchley,"['New working paper with @RobertoCerina, @cbarrie, and @azelin \n\nHow can we identify the individual and contextual-level correlates of recruitment to rare forms of extreme political behavior, like joining the far right or ISIS? \n\n<LINK>\n\nA thread /n <LINK>', 'In the paper, we provide a state of the art solution: a multilevel, rare-event Bayesian contaminated case-control design that uses a conditionally auto-regressive prior for spatial smoothing', 'What a mouthful! Why do we need this? Recruits to extremist movements are tiny minorities in any society, and so are tiny minorities in population surveys. Case control allows us to sample on the dependent variable and generate a counterfactual using survey respondents ...', '... but we still have to account for the possibility of contamination ‚Äì that our controls become recruits. In addition, recruitment is often such a rare event that you can get separation in regression models + recruitment is subject to spatial confounding + contextual effects', 'We think that our approach addresses these problems and so provides a robust framework for analyzing important questions, e.g. who was more likely to participate in the 6 Jan 2021 attack on the U.S. Capitol Building https://t.co/FhMrxSEKbD', 'We illustrate this by analyzing recruitment to ISIS from 9 MENA countries. Matching recruits with unlabeled controls from @ArabBarometer, we find that high status university grads in their early twenties were more likely to join. There is mixed evidence for relative deprivation', 'Check out the code in the GitHub repo for replication and extension to other cases: https://t.co/IvJbL4ytm5', 'Note that our approach borrows from epidemiology and recent work by @brynrosenfeld studying protest participation. Shout-out to @oonuch and the @MOBILISEproject for bringing us together at their pre-Covid Manchester conference: https://t.co/1HgmywouAy']",https://arxiv.org/abs/2106.01814,"Who joins extremist movements? Answering this question poses considerable methodological challenges. Survey techniques are practically infeasible and selective samples provide no counterfactual. Assigning recruits to contextual units provides one solution, but is vulnerable to problems of ecological inference. In this article, we take inspiration from epidemiology and the protest literature and elaborate a technique to combine survey and ecological approaches. The rare events, multilevel Bayesian contaminated case-control design we propose accounts for individual-level and contextual factors, as well as spatial autocorrelation in the incidence of recruitment. We validate our approach by matching a sample of Islamic State (ISIS) fighters from nine Muslim-majority countries with representative population surveys enumerated shortly before recruits joined the movement. We find that high status individuals in their early twenties who had university education were more likely to join ISIS. We find more mixed evidence for relative deprivation. ","Explaining Recruitment to Extremism: A Bayesian Contaminated Case
  Control Approach"
168,1400696421007847426,4552714514,Kerrie Mengersen,"['Hot off the press! Our new paper on ""Understanding links between water-quality variables and nitrate concentration in freshwater streams using high-frequency sensor data"", led by Claire Kermorvant.\n<LINK>']",http://arxiv.org/abs/2106.01719,"Real time monitoring using in situ sensors is becoming a common approach for measuring water quality within watersheds. High frequency measurements produce big data sets that present opportunities to conduct new analyses for improved understanding of water quality dynamics and more effective management of rivers and streams. Of primary importance is enhancing knowledge of the relationships between nitrate, one of the most reactive forms of inorganic nitrogen in the aquatic environment, and other water quality variables. We analysed high frequency water quality data from in situ sensors deployed in three sites from different watersheds and climate zones within the National Ecological Observatory Network, USA. We used generalised additive mixed models to explain the nonlinear relationships at each site between nitrate concentration and conductivity, turbidity, dissolved oxygen, water temperature, and elevation. Temporal auto correlation was modelled with an auto regressive moving average model and we examined the relative importance of the explanatory variables. Total deviance explained by the models was high for all sites. Although variable importance and the smooth regression parameters differed among sites, the models explaining the most variation in nitrate contained the same explanatory variables. This study demonstrates that building a model for nitrate using the same set of explanatory water quality variables is achievable, even for sites with vastly different environmental and climatic characteristics. Applying such models will assist managers to select cost effective water quality variables to monitor when the goals are to gain a spatially and temporally in depth understanding of nitrate dynamics and adapt management plans accordingly. ","Understanding links between water-quality variables and nitrate
  concentration in freshwater streams using high-frequency sensor data"
169,1400672558731644931,1156325882157522944,Akbar Namin,"['Check out our new paper entitled:\n\n""Attack Prediction using Hidden Markov Model""\n\n<LINK>\n\nThe paper uses Hidden Markov Model (HMM) to model and predict families of security attacks.\n  \nTo appear at COMPSAC\'21 (DADA\'21)']",https://arxiv.org/abs/2106.02012,"It is important to predict any adversarial attacks and their types to enable effective defense systems. Often it is hard to label such activities as malicious ones without adequate analytical reasoning. We propose the use of Hidden Markov Model (HMM) to predict the family of related attacks. Our proposed model is based on the observations often agglomerated in the form of log files and from the target or the victim's perspective. We have built an HMM-based prediction model and implemented our proposed approach using Viterbi algorithm, which generates a sequence of states corresponding to stages of a particular attack. As a proof of concept and also to demonstrate the performance of the model, we have conducted a case study on predicting a family of attacks called Action Spoofing. ",Attack Prediction using Hidden Markov Model
170,1400671937739821056,1156325882157522944,Akbar Namin,"['Check out our new paper entitled:\n\n""Toward Explainable Users: Using NLP to Enable AI to Understand Users\' Perceptions of Cyber Attacks""\n\n<LINK>\n\nThe paper introduces the idea of using AI in explaining users\' perceptions.\n  \nTo appear at COMPSAC\'21 (DADA\'21)']",https://arxiv.org/abs/2106.01998,"To understand how end-users conceptualize consequences of cyber security attacks, we performed a card sorting study, a well-known technique in Cognitive Sciences, where participants were free to group the given consequences of chosen cyber attacks into as many categories as they wished using rationales they see fit. The results of the open card sorting study showed a large amount of inter-participant variation making the research team wonder how the consequences of security attacks were comprehended by the participants. As an exploration of whether it is possible to explain user's mental model and behavior through Artificial Intelligence (AI) techniques, the research team compared the card sorting data with the outputs of a number of Natural Language Processing (NLP) techniques with the goal of understanding how participants perceived and interpreted the consequences of cyber attacks written in natural languages. The results of the NLP-based exploration methods revealed an interesting observation implying that participants had mostly employed checking individual keywords in each sentence to group cyber attack consequences together and less considered the semantics behind the description of consequences of cyber attacks. The results reported in this paper are seemingly useful and important for cyber attacks comprehension from user's perspectives. To the best of our knowledge, this paper is the first introducing the use of AI techniques in explaining and modeling users' behavior and their perceptions about a context. The novel idea introduced here is about explaining users using AI. ","Toward Explainable Users: Using NLP to Enable AI to Understand Users'
  Perceptions of Cyber Attacks"
171,1400516024437411843,1188224435364327424,Belinda Li,"['Do neural language models (trained on text alone!) construct representations of meaning? In a new #ACL2021NLP paper, we find that LM representations implicitly model *entities and situations* as they evolve through a discourse. 1/\n<LINK> <LINK>', 'These representations roughly correspond to linguistic models of dynamic semantics---they update as new sentences are added to the discourse, to reflect changes to the underlying state. 2/ https://t.co/4AcxdqHN3m', 'Specifically, we train BART and T5 models on text transcripts in two datasets (Alchemy and Textworld). We find a linear probe can map encoder representations of text to the truth values of propositions about world that the text describes. 3/ https://t.co/JNlutxCjuJ', 'Furthermore, the representations are local: in Alchemy, the probe can tell the final state of an entity by looking at the first appearance of an entity in a discourse. 4/ https://t.co/8RkpTF1Npq', 'Finally, the representations can be manipulated to affect downstream generation: by replacing a small set of LM encodings, we can induce the decoder to generate text consistent with the *modified* state representation. 5/ https://t.co/IbXRWcssdv', 'Of course, LMs still make lots of semantic errors, and our experiments only look at a tiny slice of semantics. The datasets we test on are far simpler than, e.g., those featured in the thought experiments of Bender&amp;Koller. Even here, probe accuracies are far from perfect. 6/', 'However, these experiments suggest that it‚Äôs possible to learn noisy, approximate representations of some aspects of meaning from text alone. 7/', 'This work would not have been possible without my amazing co-authors @Maxwell_Nye and @jacobandreas!\n\nCode is available at: https://t.co/TW2vcC2Igp\n\n8/8']",https://arxiv.org/abs/2106.00737,"Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data. Code and data are available at this https URL . ",Implicit Representations of Meaning in Neural Language Models
172,1400492706267283462,1312318810645573632,Dolev Bashi,"[""Ever wondered how to quantify the similarity between two planetary systems architectures?, please check out our recent paper suggesting new tools to deal exactly with this: <LINK> (and what's the connection to PASSta?) <LINK>""]",https://arxiv.org/abs/2106.00688,"The planetary systems detected so far already exhibit a wide diversity of architectures, and various methods are proposed to study quantitatively this diversity. Straightforward ways to quantify the difference between two systems and more generally, two sets of multiplanetary systems, are useful tools in the study of this diversity. In this work we present a novel approach, using a Weighted extension of the Energy Distance (WED) metric, to quantify the difference between planetary systems on the logarithmic period-radius plane. We demonstrate the use of this metric and its relation to previously introduced descriptive measures to characterise the arrangements of Kepler planetary systems. By applying exploratory machine learning tools, we attempt to find whether there is some order that can be ascribed to the set of Kepler multiplanet system architectures. Based on WED, the 'Sequencer', which is such an automatic tool, identifies a progression from small and compact planetary systems to systems with distant giant planets. It is reassuring to see that a WED-based tool indeed identifies this progression. Next, we extend WED to define the Inter-Catalogue Energy Distance (ICED) - a distance metric between sets of multiplanetary systems. We have made the specific implementation presented in the paper available to the community through a public repository. We suggest to use these metrics as complementary tools in attempting to compare between architectures of planetary system, and in general, catalogues of planetary systems. ",Quantifying the Similarity of Planetary System Architectures
173,1400478101855817730,171674815,Mark Marley,"['I want to highlight our new paper, led by @exoEhsan, with @NatashaBatalha and @ChannonVisscher about Lithium in brown dwarfs: <LINK> <LINK>', ""In low mass stars and the most massive brown dwarfs Lithium is lost to fusion in the core. Li is a little bit easier to 'burn' than H but not as easy as deuterium. So while the minimum mass to steadily burn H is around 75 Jupiter masses, the limit for Li burning is around 65 M_J."", 'So brown dwarfs and stars older than around 250 Myr and more massive than 65 MJ do not have Li visible in their atmospheres. There is a strong Li line in the optical that makes Li detection feasible.', 'This led Rafael Rebolo in the 90s to suggest the ""Li test"" as a way of distinguishing brown dwarfs from stars. If you could detect Li you knew you had to have a brown dwarf on your hand since it would be below 65 MJ. For objects which were hot enough to be either this was helpful', ""If you have a cooler object, say one that has CH4, you KNOW you don't have a star on your hands, so the Li test seems less important. Also atomic Li goes into various other molecules and is removed, thus muddying the waters."", ""However there aren't many spectral gravity indicators for brown dwarfs and it would be nice to have some marker for the most massive objects, those between ~65 MJ and ~75 MJ. Missing Li would serve nicely, but you have the chemistry removing Li as well as the nuclear fires."", 'So the question becomes, can we detect the other Li molecules that show up at lower Teff? If so then we have a new lithium test for identifying the most massive brown dwarfs. But we need molecular opacities for species such as LiH, LiF, LiOH, and LiCl.', 'This is where Ehsan comes in. As part of his NPP postdoc at Ames Ehsan computed himself some of these opacities and compiled others so that we could investigate the detectability of the various Li species, allowing us to follow the lithium through the brown dwarf cooling sequence', 'You can read the paper for details, but some of these other Li species should be detectable in the mid-IR. Unfortunately the molecular signatures are more subtle than the atomic Li feature, but the 30m telescopes and perhaps JWST should be able to search for them.', ""I'm hopeful that someday Li could be used to resolve some puzzles, such as the mass of Gl 229 B. Special thanks to Channon for handling all of the Li-species chemistry for us. Our spectra were computed with PICASO.""]",https://arxiv.org/abs/2106.00781,"Lithium is an important element for the understanding of ultracool dwarfs because it is lost to fusion at masses above $\sim 68\, M_{\rm J}$. Hence, the presence or absence of atomic Li has served as an indicator of the nearby H-burning boundary at about $75\,M_{\rm J}$ between brown-dwarfs and very low-mass stars. Historically the ""Lithium test"", a search for the presence and strength of the Li line at 670.8 nm, has been a marker if an object has a substellar mass with stellar-like spectral energy distribution (e.g., a late-type M dwarf). While the Li test could in principle also be used to distinguish masses of later-type L-T dwarfs, Li is predominantly no longer found as an atomic gas, but rather a molecular species such as LiH, LiF, LiOH, and LiCl in their cooler atmospheres. L- and T-type brown dwarfs are also quite faint at 670 nm and thus challenging targets for high resolution spectroscopy. But only recently have experimental molecular line lists become available for the molecular Li species, allowing molecular Li mass discrimination. In this study, we generated the latest opacity of each of these Li-bearing molecules and performed thermochemical equilibrium atmospheric composition calculation of the abundance of these molecules. Finally, we computed thermal emission spectra for a series of radiative-convective equilibrium models of cloudy and cloudless brown dwarf atmospheres (with $T_{\rm eff}=$ 500--2400~K, and $\log g$=4.0, 4.5, 5.0) to understand where the presence or absence of atmospheric lithium-bearing species is most easily detected as a function of brown dwarf mass and age. After atomic Li, the best spectral signatures were found to be LiF at $10.5-12.5$~\micron and LiCl at $14.5-18.5$ $\micron$. LiH also shows a narrow feature at $\sim 9.38$ $\micron$. ","Following the Lithium: Tracing Li-bearing Molecules Across Age, Mass,
  and Gravity in Brown Dwarfs"
174,1400270370821836808,23412483,Kai Li,"['A new working paper by @ChenyueJ and myself: <LINK>, focusing on the rhetorical characteristics of data paper abstracts. We found a few data-focused rhetorical moves used in data papers, as comparing to the IMRaD structure in research articles.']",https://arxiv.org/abs/2106.01083,"The data paper is an emerging academic genre that focuses on the description of research data objects. However, there is a lack of empirical knowledge about this rising genre in quantitative science studies, particularly from the perspective of its linguistic features. To fill this gap, this research aims to offer a first quantitative examination of which rhetorical moves-rhetorical units performing a coherent narrative function-are used in data paper abstracts, as well as how these moves are used. To this end, we developed a new classification scheme for rhetorical moves in data paper abstracts by expanding a well-received system that focuses on English-language research article abstracts. We used this expanded scheme to classify and analyze rhetorical moves used in two flagship data journals, Scientific Data and Data in Brief. We found that data papers exhibit a combination of IMRaD- and data-oriented moves and that the usage differences between the journals can be largely explained by journal policies concerning abstract and paper structure. This research offers a novel examination of how the data paper, a novel data-oriented knowledge representation, is composed, which greatly contributes to a deeper understanding of data and data publication in the scholarly communication system. ","The data paper as a socio-linguistic epistemic object: A content
  analysis on the rhetorical moves used in data paper abstracts"
175,1400261601802870784,1119252439050354688,Peter Hase,"['New paper out! ‚ÄúSearch Methods for Sufficient, Socially-Aligned Feature Importance Explanations with In-Distribution Counterfactuals‚Äù\n\nEval. issues raised+resolved and new expln. methods\n\nw/ Harry Xie, @mohitban47\n\narxiv: <LINK>\ndemo: <LINK>\n\n1/n <LINK>', 'This paper contributes (1) a training algorithm for mitigating the out-of-distribution (OOD) problem in feature importance explanations, and (2) several new search methods for finding good feature importance explanations w/ Sufficiency + Comprehensiveness metrics.\n2/n', 'On (1): The OOD problem is that removing features from an input makes it OOD to a trained model. We argue that this means the model prior and random weight initialization will influence explanations and explanation metrics in unintended ways (i.e. causes social misalignment).\n3/n', 'We propose to train on these counterfactual inputs, so they‚Äôre not OOD to the trained model. We find this heavily influences downstream explanation metrics, which often do not look as good. For models not trained like this, we recommend best practices for removing features.\n4/n', 'On (2): We introduce new search methods for finding Sufficient + Comprehensive explanations, and we find that search can outperform salience methods with the same amount of compute. In fact, random search regularly beats LIME and Integrated Gradients across six datasets.\n5/n https://t.co/JQxfZdBSvw', 'But the best method is our ""Parallel Local Search,"" which outperforms the next best method by up to 5 points on Suff and 20 points on Comp! Experiments with 6 diverse datasets: SNLI, FEVER, BoolQ, Evidence Inference, MultiRC &amp; SST-2\n\nSee a demo at https://t.co/jpV5joAobu\n\nn/n']",https://arxiv.org/abs/2106.00786,"Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model confidence caused by removing certain input features at test time. For example, in the standard Sufficiency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to remove features from an input when creating or evaluating explanations: the fact that these counterfactual inputs are out-of-distribution (OOD) to models implies that the resulting explanations are socially misaligned. The crux of the problem is that the model prior and random weight initialization influence the explanations (and explanation metrics) in unintended ways. To resolve this issue, we propose a simple alteration to the model training process, which results in more socially aligned explanations and metrics. Second, we compare among five approaches for removing features from model inputs. We find that some methods produce more OOD counterfactuals than others, and we make recommendations for selecting a feature-replacement function. Finally, we introduce four search-based methods for identifying FI explanations and compare them to strong baselines, including LIME, Anchors, and Integrated Gradients. Through experiments with six diverse text classification datasets, we find that the only method that consistently outperforms random search is a Parallel Local Search (PLS) that we introduce. Improvements over the second-best method are as large as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All supporting code for experiments in this paper is publicly available at this https URL ","The Out-of-Distribution Problem in Explainability and Search Methods for
  Feature Importance Explanations"
176,1400149998831128576,446694758,Julian Eisenschlos,"['Transformers work well for tasks involving tabular data, but what if the table requires more tokens than the model can fit? In our new #ACL2021 Findings short paper <LINK> by Syrine Krichene,\n@muelletm and myself, we tackle this problem by doubling down ‚ÄºÔ∏è\n\n1/3', 'A first *small* Transformer based on TAPAS selects the important parts of the input in an end-to-end differentiable way. The output of the first model is mapped to a score from -‚àû to 0 for each token and then added to the attention mask of a second model before the softmax.\n\n2/3 https://t.co/4zTOvOsc3g', 'On the limit, DoT (Double Transformer) coincides exactly with removing the tokens from the input. Hence we can implement it efficiently and obtain speedups from 1.5x to 4.6x in TabFact and WikiSQL with negligible accuracy losses. Check out the code in https://t.co/1VxCSrATvT\n\n3/3']",http://arxiv.org/abs/2106.00479,"Transformer-based approaches have been successfully used to obtain state-of-the-art accuracy on natural language processing (NLP) tasks with semi-structured tables. These model architectures are typically deep, resulting in slow training and inference, especially for long inputs. To improve efficiency while maintaining a high accuracy, we propose a new architecture, DoT, a double transformer model, that decomposes the problem into two sub-tasks: A shallow pruning transformer that selects the top-K tokens, followed by a deep task-specific transformer that takes as input those K tokens. Additionally, we modify the task-specific attention to incorporate the pruning scores. The two transformers are jointly trained by optimizing the task-specific loss. We run experiments on three benchmarks, including entailment and question-answering. We show that for a small drop of accuracy, DoT improves training and inference time by at least 50%. We also show that the pruning transformer effectively selects relevant tokens enabling the end-to-end model to maintain similar accuracy as slower baseline models. Finally, we analyse the pruning and give some insight into its impact on the task model. ",DoT: An efficient Double Transformer for NLP tasks with tables
177,1400057059316535298,3853426059,Stefan S√∂ldner-Rembold,['Exciting new @Microboone paper on Higgs portal scalars with @UoMparticle ‚Äòs Pawel Guzowski as primary author <LINK>'],https://arxiv.org/abs/2106.00568,"We present a search for the decays of a neutral scalar boson produced by kaons decaying at rest, in the context of the Higgs portal model, using the MicroBooNE detector. We analyze data triggered in time with the Fermilab NuMI neutrino beam spill, with an exposure of $1.93\times10^{20}$ protons on target. We look for monoenergetic scalars that come from the direction of the NuMI hadron absorber, at a distance of 100 m from the detector, and decay to electron-positron pairs. We observe one candidate event, with a Standard Model background prediction of $1.9\pm0.8$. We set an upper limit on the scalar-Higgs mixing angle of $\theta<(3.3-4.6)\times10^{-4}$ at the 95% confidence level for scalar boson masses in the range $(100-200)$ MeV$/c^2$. We exclude at the 95% confidence level the remaining model parameters required to explain the central value of a possible excess of $K^0_L\rightarrow\pi^0\nu\bar{\nu}$ decays reported by the KOTO collaboration. We also provide a model-independent limit on a new boson $X$ produced in $K\rightarrow\pi X$ decays and decaying to $e^+e^-$. ","Search for a Higgs portal scalar decaying to electron-positron pairs in
  the MicroBooNE detector"
178,1400051612719124481,3881712928,Valentina,['New paper today with @AvelinoQuantum and Miguel Puerta @AHEPGroup \nüëâ <LINK> <LINK>'],https://arxiv.org/abs/2106.00481,"Scotogenic models are among the most popular possibilities to link dark matter and neutrino masses. In this work we discuss a variant of the Scotogenic model that includes charged fermions and a doublet with hypercharge $3/2$. Neutrino masses are induced at the one-loop level thanks to the states belonging to the dark sector. However, in contrast to the standard Scotogenic model, only the scalar dark matter candidate is viable in this version. After presenting the model and explaining some particularities about neutrino mass generation, we concentrate on its dark matter phenomenology. We show that the observed dark matter relic density can be correctly reproduced in the usual parameter space regions found for the standard Scotogenic model or the Inert Doublet model. In addition, the presence of the charged fermions may open up new regions, provided some tuning of the parameters is allowed. ",Dark matter in a charged variant of the Scotogenic model
179,1399984455255736321,820031736914513925,Fabrizio Leisen,"['NEW PAPER: ""Kernel based Dirichlet sequences"" written with P. Berti, E. Dreassi, L. Pratelli, P. Rigo. <LINK>']",http://arxiv.org/abs/2106.00114,"Let $X=(X_1,X_2,\ldots)$ be a sequence of random variables with values in a standard space $(S,\mathcal{B})$. Suppose \begin{gather*} X_1\sim\nu\quad\text{and}\quad P\bigl(X_{n+1}\in\cdot\mid X_1,\ldots,X_n\bigr)=\frac{\theta\nu(\cdot)+\sum_{i=1}^nK(X_i)(\cdot)}{n+\theta}\quad\quad\text{a.s.} \end{gather*} where $\theta>0$ is a constant, $\nu$ a probability measure on $\mathcal{B}$, and $K$ a random probability measure on $\mathcal{B}$. Then, $X$ is exchangeable whenever $K$ is a regular conditional distribution for $\nu$ given any sub-$\sigma$-field of $\mathcal{B}$. Under this assumption, $X$ enjoys all the main properties of classical Dirichlet sequences, including Sethuraman's representation, conjugacy property, and convergence in total variation of predictive distributions. If $\mu$ is the weak limit of the empirical measures, conditions for $\mu$ to be a.s. discrete, or a.s. non-atomic, or $\mu\ll\nu$ a.s., are provided. Two CLT's are proved as well. The first deals with stable convergence while the second concerns total variation distance. ",Kernel based Dirichlet sequences
180,1399901662857285633,15327263,Carl-Johan Haster,"['New paper on the ArXiv today! Led by Tom Callister @FlatironCCA, and with Ken Ng from @MIT_Physics, @sasomao and @farrwill.\n<LINK>\nWe looked for correlations between the parameters describing the population of Binary Black Holes observed by @LIGO and @ego_virgo.', 'And we found a correlation!\nLooking at the ratio between the black hole masses, and comparing this against the effective inspiral black hole spin of the binary, we found that more unequal mass binaries have larger effective spins...', ""Given the standard description of binary black holes, how they're thought to be formed in the Universe, and how we're able to observe the gravitational waves they emit we're not necessarily surprised that there are correlations between these two parameters."", ""We are however somewhat surprised about the direction of the correlation.\nSo ping to all Black Hole fans to start thinking of exciting ways to explain what we've found!"", 'I should also say that we\'ve been careful to check that the correlation really comes from the data (and isn\'t ""just caused"" by some bug/oversight in our analysis), and for all we can say the correlation is real and sufficiently statistically significant to be interesting!', ""And as usual, we're awaiting more Binary Black Hole observations to see how well our model holds up agains more observational data!"", '@CraigCahillane https://t.co/hGWlq7yzt2']",https://arxiv.org/abs/2106.00521,"Hierarchical analysis of the binary black hole (BBH) detections by the Advanced LIGO and Virgo detectors has offered an increasingly clear picture of their mass, spin, and redshift distributions. Fully understanding the formation and evolution of BBH mergers will require not just the characterization of these marginal distributions, though, but the discovery of any correlations that exist between the properties of BBHs. Here, we hierarchically analyze the ensemble of BBHs discovered by the LIGO and Virgo with a model that allows for intrinsic correlations between their mass ratios $q$ and effective inspiral spins $\chi_\mathrm{eff}$. At $98.7\%$ credibility, we find that the mean of the $\chi_\mathrm{eff}$ distribution varies as a function of $q$, such that more unequal-mass BBHs exhibit systematically larger $\chi_\mathrm{eff}$. We find Bayesian odds ratio of $10.5$ in favor of a model that allows for such a correlation over one that does not. Finally, we use simulated signals to verify that our results are robust against degeneracies in the measurements of $q$ and $\chi_\mathrm{eff}$ for individual events. While many proposed astrophysical formation channels predict some degree correlation between spins and mass ratio, these predicted correlations typically act in an opposite sense to the trend we observationally identify in the data. ","Who Ordered That? Unequal-Mass Binary Black Hole Mergers Have Larger
  Effective Spins"
181,1410427531065450496,607987861,Ashish Jaiswal,['My new paper pre-print on understanding cognitive fatigue through fMRI data with self-supervised learning and temporal attention mechanism is out on arXiv.\n\narXiv Link: <LINK> <LINK> <LINK>'],https://arxiv.org/abs/2106.15009,"Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that records neural activations in the brain by capturing the blood oxygen level in different regions based on the task performed by a subject. Given fMRI data, the problem of predicting the state of cognitive fatigue in a person has not been investigated to its full extent. This paper proposes tackling this issue as a multi-class classification problem by dividing the state of cognitive fatigue into six different levels, ranging from no-fatigue to extreme fatigue conditions. We built a spatio-temporal model that uses convolutional neural networks (CNN) for spatial feature extraction and a long short-term memory (LSTM) network for temporal modeling of 4D fMRI scans. We also applied a self-supervised method called MoCo (Momentum Contrast) to pre-train our model on a public dataset BOLD5000 and fine-tuned it on our labeled dataset to predict cognitive fatigue. Our novel dataset contains fMRI scans from Traumatic Brain Injury (TBI) patients and healthy controls (HCs) while performing a series of N-back cognitive tasks. This method establishes a state-of-the-art technique to analyze cognitive fatigue from fMRI data and beats previous approaches to solve this problem. ","Understanding Cognitive Fatigue from fMRI Scans with Self-supervised
  Learning"
182,1410398678548848641,186017372,Yuichi Tanaka,"['Our new paper on graph signal restoration is out! We propose ""nested"" deep algorithm unrolling. Check this out on arXiv!\nM. Nagahama, K. Yamada, Y. Tanaka, S. H. Chan, and Y. C. Eldar: Graph signal restoration using nested deep algorithm unrolling\n<LINK> <LINK>']",https://arxiv.org/abs/2106.15910,"Graph signal processing is a ubiquitous task in many applications such as sensor, social, transportation and brain networks, point cloud processing, and graph neural networks. Often, graph signals are corrupted in the sensing process, thus requiring restoration. In this paper, we propose two graph signal restoration methods based on deep algorithm unrolling (DAU). First, we present a graph signal denoiser by unrolling iterations of the alternating direction method of multiplier (ADMM). We then suggest a general restoration method for linear degradation by unrolling iterations of Plug-and-Play ADMM (PnP-ADMM). In the second approach, the unrolled ADMM-based denoiser is incorporated as a submodule, leading to a nested DAU structure. The parameters in the proposed denoising/restoration methods are trainable in an end-to-end manner. Our approach is interpretable and keeps the number of parameters small since we only tune graph-independent regularization parameters. We overcome two main challenges in existing graph signal restoration methods: 1) limited performance of convex optimization algorithms due to fixed parameters which are often determined manually. 2) large number of parameters of graph neural networks that result in difficulty of training. Several experiments for graph signal denoising and interpolation are performed on synthetic and real-world data. The proposed methods show performance improvements over several existing techniques in terms of root mean squared error in both tasks. ",Graph Signal Restoration Using Nested Deep Algorithm Unrolling
183,1407566593308114948,547776192,Chris Lovell,"['Wait 12 months for a bus and then two come at once...\n\nNew paper on the arXiv today, revealing how the orientation of a submillimetre galaxy (SMG) can impact its emission, and the knock on effects for surveys of SMGs at different wavelengths and redshifts\n\n<LINK> <LINK>', 'We use Simba to study 8 massive, star forming galaxies (named after John Le Carr√© characters üïµÔ∏è RIP) using dust RT to estimate their emission from 50 random orientations. \n\nOur main finding is a significant wavelength-dependent spread in the emission (up to ~50 mJy for Smiley) https://t.co/BjaMC7BQAl', 'We find that, for galaxies with a clear disk morphology, this can be parametrised by the relative inclination of the disk to the observer (parametrised here by the cosine similarity; 1 is face on, 0 is edge on) https://t.co/Jfd4BNHs6C', 'This has knock on effects on the inferred dust temperatures and IR luminosities (added uncertainties of 5.0 K and 0.\x9511 dex, respectively), which in turn impacts uncertainties on other derived properties (30% variance in SFR) https://t.co/adOb5agwn3', 'We combine all this together into a model for the effect of orientation on flux-limited surveys in the submillimetre. At low flux densities an incompleteness emerges; you are biased towards face-on galaxies https://t.co/CU7y4J8M52', 'There is also a redshift dependence to this uncertainty, as the wavelength at which the orientation uncertainty peaks moves through the observed wavelength range\n\nThe completeness is therefore lower at higher redshift https://t.co/HIMLwF5CTl', 'All this can be applied to arbitrary surveys using our simple python tool - just specify an observed wavelength, number count distribution and underlying redshift distribution\n\nhttps://t.co/7Bq7OBawE3', 'Thank you to my lovely coauthors for all of their help getting this together! the ones on twitter @jgeach  @RomeelDave @desikanarayanan', ""@jgeach @RomeelDave @desikanarayanan Special shout out to @jgeach who had the original idea in the depths of lockdown here in the UK, and for all of his support throughout the winter when progress was slowwww ü¶• I'm excited to see what people think!"", '@franco_vazza @NikoSarcevic Thanks! ‚ò∫Ô∏è', '@chargedcurrent @jgeach @RomeelDave @desikanarayanan We didn‚Äôt, that‚Äôs an interesting idea. However, when I tried to parametrise the variation as a function of line-of-sight attenuation it just didn‚Äôt work, which suggests secondary effects in the RT are important']",https://arxiv.org/abs/2106.11588,"Recent high-resolution interferometric images of submillimetre galaxies (SMGs) reveal fascinatingly complex morphologies. This raises a number of questions: how does the relative orientation of a galaxy affect its observed submillimetre emission, and does this result in an `orientation bias' in the selection and analysis of such galaxies in flux-limited cosmological surveys? We investigate these questions using the Simba cosmological simulation paired with the dust radiative transfer code Powderday. We select eight simulated SMGs ($S_{850}\gtrsim2$ mJy) at $z = 2$, and measure the variance of their `observed' emission over 50 random orientations. Each galaxy exhibits significant scatter in its emission close to the peak of the thermal dust emission, with variation in flux density of up to $\sim$50 mJy at the peak. This results in an appreciable dispersion in the inferred dust temperatures and infrared luminosities ($16^{\mathrm{th}}-84^{\mathrm{th}}$ percentile ranges of 5 K and 0.1 dex, respectively) and therefore a fundamental uncertainty in derived parameters such as dust mass and star formation rate ($\sim$30% for the latter using simple calibrations). Using a Monte Carlo simulation we also assess the impact of orientation on flux-limited surveys, finding a bias in the selection of SMGs towards those with face-on orientations, as well as those at lower redshifts. We predict that the orientation bias will affect flux-limited single-dish surveys, most significantly at THz frequencies, and this bias should be taken into account when placing the results of targeted follow-up studies in a statistical context. ",An orientation bias in observations of submillimetre galaxies
184,1407409443642433538,1069642708036259840,Josh Robinson,"['Happy to share our new paper on shortcut learning in contrastive learning!\n\nCan contrastive learning avoid shortcut solutions?\nPaper: <LINK>\nCode: <LINK>\n\njoint work with Li Sun, Ke Yu, @Imaging_Gen, @StefanieJegelka, @optiML <LINK>', 'As with supervised deep learning, contrastive learning suffers from shortcuts, where some input features are suppressed during training and achieve worse performance than at initialization.', ""We theoretically show that the InfoNCE loss doesn't provide enough guidance to avoid shortcuts, and that this can lead to counterintuitive behavior such as *negative correlation* between InfoNCE loss and test error https://t.co/1PPkBd9Vlu"", 'However, unlike supervised learning, we show that modifications to the contrastive instance discrimination task, e.g. using harder negatives, make it possible to *trade-off* learning of one feature for another https://t.co/hrpgCeEbi8', 'However, trade-offs are less than ideal, so we propose *Implicit Feature Modification* (IFM), which asks an encoder to discriminate instances in two ways: 1) using existing features represented, &amp; 2) by removing current features and using new ones', 'We find that IFM successfully enhances representation of multiple features simultaneously, without trade-offs (see main tweet pic.). We also find that IFM improves performance on real tasks such as object classification benchmarks https://t.co/ltjchZ0c4d']",https://arxiv.org/abs/2106.11230,"The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via ""shortcuts"", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: \url{this https URL}. ",Can contrastive learning avoid shortcut solutions?
185,1406898203195232261,1060844222855725056,Ayoub KARINE,['The code of our new accepted paper on ICIP 2021 is now available online : \n- Paper : <LINK> \n- Code : <LINK>\n@isennantes \n@Univ_Orleans  \n@um5rabat'],https://arxiv.org/abs/2106.09303,"This paper addresses the problem of blind stereoscopic image quality assessment (NR-SIQA) using a new multi-task deep learning based-method. In the field of stereoscopic vision, the information is fairly distributed between the left and right views as well as the binocular phenomenon. In this work, we propose to integrate these characteristics to estimate the quality of stereoscopic images without reference through a convolutional neural network. Our method is based on two main tasks: the first task predicts naturalness analysis based features adapted to stereo images, while the second task predicts the quality of such images. The former, so-called auxiliary task, aims to find more robust and relevant features to improve the quality prediction. To do this, we compute naturalness-based features using a Natural Scene Statistics (NSS) model in the complex wavelet domain. It allows to capture the statistical dependency between pairs of the stereoscopic images. Experiments are conducted on the well known LIVE PHASE I and LIVE PHASE II databases. The results obtained show the relevance of our method when comparing with those of the state-of-the-art. Our code is available online on this https URL ","A Multi-task convolutional neural network for blind stereoscopic image
  quality assessment using naturalness analysis"
186,1405589603671740418,967040398542688256,Mattie Fellows,"[""I'm excited to share our latest paper, Bayesian Bellman Operators: <LINK>! We introduce new persective  on model-free Bayesian RL where we infer a posterior over Bellman operators. I hope you enjoy! @kristianhartika @shimon8282 @whi_rl""]",https://arxiv.org/abs/2106.05012,"We introduce a novel perspective on Bayesian reinforcement learning (RL); whereas existing approaches infer a posterior over the transition distribution or Q-function, we characterise the uncertainty in the Bellman operator. Our Bayesian Bellman operator (BBO) framework is motivated by the insight that when bootstrapping is introduced, model-free approaches actually infer a posterior over Bellman operators, not value functions. In this paper, we use BBO to provide a rigorous theoretical analysis of model-free Bayesian RL to better understand its relationshipto established frequentist RL methodologies. We prove that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference isused, and derive conditions for which convergence properties hold. Empirically, we demonstrate that algorithms derived from the BBO framework have sophisticated deep exploration properties that enable them to solve continuous control tasks at which state-of-the-art regularised actor-critic algorithms fail catastrophically ",Bayesian Bellman Operators
187,1405441160701792256,1278073390298017794,Giuseppe Donatiello,"['This graphic shows the relative sizes of the three new satellite dwarf #galaxies compared to NGC 253 and NGC 247. As it turns out, they are really tiny. \nOur new paper, with first author @astro_delgado, is here: <LINK> <LINK>']",https://arxiv.org/abs/2106.08868,"In the last years, a new generation of large-scale imaging surveys have probed wide field regions around some nearby galaxies at unprecedented low surface brightness regime (~28.0-29.0 mag arcsec^-2). This offers a chance of discovering very faint dwarf satellites by means of visual inspection of these public deep images. We report the first results of a systematic survey of faint dwarf spheroidal galaxies in the vicinity of the bright late-type spiral NGC 253 galaxy by means of a visual inspection of the images taken by the Dark Energy Survey. Three new dwarf galaxies have been discovered in the vicinity of the brightest member of the Sculptor filament, the late-type spiral NGC 253. Assuming they are companions of NGC 253, their total absolute V-magnitudes fall in the -7 to -9 mag range, which is typical for dwarf satellites in the local Universe. The central surface brightness tend to be extremely low for all the discovered dwarfs and fall roughly in the range of 25-26 mag arcsec^-2 in g-band. Using known data on distances and velocities of galaxies, we estimate the total virial mass of the NGC 253 group to be 8 x 10^11 Mo, which gives the virial radius R_200 = 186 kpc and the turn-around radius of 706 kpc. We also discuss the possible existence of a spatially flattened and velocity-correlated satellite system around NGC 253. This large-scale structure is orientated almost edge-on to line of sight. The possible plane of satellites is only 31 kpc thick with the minor-to-major axis ratio of 0.14. Four out of five galaxies with measured velocities follow a common velocity trend similar to those observed in the planes of satellites around the Andromeda and Centaurus A galaxies. However, the small number of galaxies with known velocities prevents to reach a definitive conclusion about the formation scenario of the structure and its possible relation to the surrounding cosmic web. ","Tracing satellite planes in the Sculptor group: I. Discovery of three
  faint dwarf galaxies around NGC 253"
188,1405264735604334592,1352726011549528071,Marinos Poiitis,"[""The better a Neural Network (NN) generalizes the easier it is to train (?) We investigated this hypothesis with @loukasa_tweet and @StefanieJegelka by studying the NN's training behavior and complexity. \nHave a look at our new paperüìöüìï on arXiv ü•≥\nüëâ<LINK>üëà""]",https://arxiv.org/abs/2106.04186,"This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN's Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. We first observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, we find that steady training with Dropout implies a training- and data-dependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization. ",What training reveals about neural network complexity
189,1405174662141779976,17705769,Avishek Anand,"['We try to explain Neural models using IR axioms in our new paper in #ictir2021 ...shout out to my amazing co-authors @matthias_hagen @albondarenko2 @maik_froebe @movedthecheese and Michael Voelske. link:<LINK>', '@matthias_hagen @albondarenko2 @maik_froebe @movedthecheese In this age of ""hyperperformance"" we infact find out soberingly that the degree to which rankings can be explained with the currently known axioms is still rather limited...Quite happy that reviewers are not only sympathetic but appreciative of ""limitations""..', 'and of course shout out to @bennostein .. summer afternoons are amnesic..']",https://arxiv.org/abs/2106.08019,"Recently, neural networks have been successfully employed to improve upon state-of-the-art performance in ad-hoc retrieval tasks via machine-learned ranking functions. While neural retrieval models grow in complexity and impact, little is understood about their correspondence with well-studied IR principles. Recent work on interpretability in machine learning has provided tools and techniques to understand neural models in general, yet there has been little progress towards explaining ranking models. We investigate whether one can explain the behavior of neural ranking models in terms of their congruence with well understood principles of document ranking by using established theories from axiomatic IR. Axiomatic analysis of information retrieval models has formalized a set of constraints on ranking decisions that reasonable retrieval models should fulfill. We operationalize this axiomatic thinking to reproduce rankings based on combinations of elementary constraints. This allows us to investigate to what extent the ranking decisions of neural rankers can be explained in terms of retrieval axioms, and which axioms apply in which situations. Our experimental study considers a comprehensive set of axioms over several representative neural rankers. While the existing axioms can already explain the particularly confident ranking decisions rather well, future work should extend the axiom set to also cover the other still ""unexplainable"" neural IR rank decisions. ",Towards Axiomatic Explanations for Neural Ranking Models
190,1405146109295616000,1002014071,Frank Wilczek,['New paper <LINK> @arxiv combining:\n1. Encode your problem as finding low-energy states of an engineered Hamiltonian.\n2. Get to low energy by good contact with a 0-temperature bath.  \n- keeping quantum coherence.  Demonstrated on non-trivial examples. <LINK>'],https://arxiv.org/abs/2106.07522,"Interesting problems in quantum computation take the form of finding low-energy states of (pseudo)spin systems with engineered Hamiltonians that encode the problem data. Motivated by the practical possibility of producing very low-temperature spin systems, we propose and exemplify the possibility to compute by coupling the computational spins to a non-Markovian bath of spins that serve as a heat sink. We demonstrate both analytically and numerically that this strategy can achieve quantum advantage in the Grover search problem. ",Quantum Computing by Cooling
191,1404729719136849927,2949347867,Elias Gr√ºnewald,['üá™üá∫ + üíª = üí°\n\nCheck out our new paper on\n‚è© GDPR Transparency in RESTful Architectures\n\n<LINK> or here <LINK>\n\n#IWPE21 @IEEEEUROSP #PrivacyEngineering #GDPR'],https://arxiv.org/abs/2106.06001,"Transparency - the provision of information about what personal data is collected for which purposes, how long it is stored, or to which parties it is transferred - is one of the core privacy principles underlying regulations such as the GDPR. Technical approaches for implementing transparency in practice are, however, only rarely considered. In this paper, we present a novel approach for doing so in current, RESTful application architectures and in line with prevailing agile and DevOps-driven practices. For this purpose, we introduce 1) a transparency-focused extension of OpenAPI specifications that allows individual service descriptions to be enriched with transparency-related annotations in a bottom-up fashion and 2) a set of higher-order tools for aggregating respective information across multiple, interdependent services and for coherently integrating our approach into automated CI/CD-pipelines. Together, these building blocks pave the way for providing transparency information that is more specific and at the same time better reflects the actual implementation givens within complex service architectures than current, overly broad privacy statements. ","TIRA: An OpenAPI Extension and Toolbox for GDPR Transparency in RESTful
  Architectures"
192,1404473337670213645,32513057,Kristian Lum,"['Do you use the COMPAS dataset to demonstrate fair ML methods? Do you want to do so responsibly?\n\nThen, ""It\'s COMPASlicated,"" my new paper w @TheMichelleBao @angelamczhou @samzottola @BrianBrubach @DrSLDesmarais @Aaron_Horowitz @geomblog is for you! üßµ 1/n\n\n<LINK>', 'In this paper, we discuss properties inherent to CJ data that make strong claims about  fairness in this context, well.... COMPASlicated. \n\nTl;dr You may be predicting not-what-you-want based on features/a population that are themselves the result of ""biased"" decisions. 2/n', 'Algorithmic RAIs are not an automated, single-decision system, but rather 1 point of discretion in a complex multi-decisionmaker &amp; multi-institution system. \n\nAs such, even if your model performs ""fairly"" in isolation, this may not translate to fair outcomes in practice.  3/n', ""It's not possible to decontextualize risk assessment data from the politics of the criminal justice system. For example, accepting the objective of predicting future crime for use in this context tacitly endorses incapacitation over rehabilitation or retribution. 4/n"", 'ML researchers undermine efforts for CJ reform (e.g. eliminating bail) through their  misunderstanding of the issues-- for example, being ""granted bail"" is often posed as the positive outcome. The many people currently detained on (even small amounts) of bail would disagree. 5/n', 'Speaking of reform, ""fair ML"" often implicitly endorses RAIs as an avenue for criminal justice reform.  The notion of structural reform, for policing, incarceration, or CJ in general, is itself value-laden and presents existing structures as worthy of reform.  6/n', 'Many of these are issues that folks in other fields have already grappled with and have developed written guidelines and standards for implementing best practices when engaging in criminal justice work. So far, these have largely been ignored by the CS and fair ML community. 7/n', 'Specifically, there are guidelines for language use for describing justice-involved people. Terms like ""prisoner,"" ""felon,"" ""inmate,"" etc. are dehumanizing &amp; terms that acknowledge the person\'s humanity, such as ""person who has been convicted of a felony"" are now recommended 8/n', 'I just gave you the rundown of the issues nearest and dearest to my heart. Others perspectives, including those of  advocates and psychology/CJ researchers, are also included. \n\nI hope this offers some useful context for ML researchers who use the COMPAS dataset!']",https://arxiv.org/abs/2106.05498,"Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS dataset, are commonly used in algorithmic fairness papers due to benchmarking practices of comparing algorithms on datasets used in prior work. In many cases, this data is used as a benchmark to demonstrate good performance without accounting for the complexities of criminal justice (CJ) processes. However, we show that pretrial RAI datasets can contain numerous measurement biases and errors, and due to disparities in discretion and deployment, algorithmic fairness applied to RAI datasets is limited in making claims about real-world outcomes. These reasons make the datasets a poor fit for benchmarking under assumptions of ground truth and real-world impact. Furthermore, conventional practices of simply replicating previous data experiments may implicitly inherit or edify normative positions without explicitly interrogating value-laden assumptions. Without context of how interdisciplinary fields have engaged in CJ research and context of how RAIs operate upstream and downstream, algorithmic fairness practices are misaligned for meaningful contribution in the context of CJ, and would benefit from transparent engagement with normative considerations and values related to fairness, justice, and equality. These factors prompt questions about whether benchmarks for intrinsically socio-technical systems like the CJ system can exist in a beneficial and ethical way. ","It's COMPASlicated: The Messy Relationship between RAI Datasets and
  Algorithmic Fairness Benchmarks"
193,1403342240894816261,620237891,Ethan Gotlieb Wilcox,"[""Neural Language Models' incremental predictions under predict human sensitivity to ungrammatical sentences! New #ACL2021 paper with @roger_p_levy and Pranali Vani <LINK>  #NLProc <LINK>""]",https://arxiv.org/abs/2106.03232,"We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model `accuracy' scores (a la Marvin and Linzen(2018)) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reaction times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations. ","A Targeted Assessment of Incremental Processing in Neural LanguageModels
  and Humans"
194,1403338350703095808,1060158595076182016,Justin Edwards,"['What does it mean for chatbots to reflect aspects of our identities? Can a chatbot express its gender and sexual orientation? Do chatbots have queer identities? Me, @allisonperrrone and @lmhclark explore these in our new paper, and talk it out on the pod <LINK> <LINK>']",https://arxiv.org/abs/2106.02076,"Chatbots are popular machine partners for task-oriented and social interactions. Human-human computer-mediated communication research has explored how people express their gender and sexuality in online social interactions, but little is known about whether and in what way chatbots do the same. We conducted semi-structured interviews with 5 text-based conversational agents to explore this topic Through these interviews, we identified 6 common themes around the expression of gender and sexual identity: identity description, identity formation, peer acceptance, positive reflection, uncomfortable feelings and off-topic responses. Chatbots express gender and sexuality explicitly and through relation of experience and emotions, mimicking the human language on which they are trained. It is nevertheless evident that chatbots differ from human dialogue partners as they lack the flexibility and understanding enabled by lived human experience. While chatbots are proficient in using language to express identity, they also display a lack of authentic experiences of gender and sexuality. ","LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in
  Chatbots"
195,1403263384657793028,1311352436,Andrew Sellek,"['New paper out today! <LINK>\nWe investigate how self-similar models from Clarke and Alexander (2016) generalise to non-isothermal winds and winds launched from an elevated base. Reasonable temperature gradients make quite small differences... (1/5) <LINK>', '‚Ä¶the most key parameter in setting the maximum launch speed (Mach number) turns out to be the elevation of the wind base (phi_b). We tested the applicability of the models with hydrodynamic simulations. Not only do the scale free simulations agree excellently as before‚Ä¶ (2/5) https://t.co/qrDPM7Tfwv', '‚Ä¶but so do those with gravity included, and even broken power law models for the density, in which the self-similar models predict launch velocities excellently away from the transition radius, though winds are not launched beyond wherever the density steepens beyond r^-2. (3/5) https://t.co/X2aZb5rMMk', 'We find that much of the behaviour can be understood by the idea that the wind must fill the available space. For example, if we limit the elevation to which the wind can rise, then the wind adopts more curved solutions with lower launch velocities than the maximum allowed. (4/5) https://t.co/d84Bmz7760', 'Finally we consider the impact of the elevated base on [Ne II] line profiles, for which the dependence of the FWHM on inclination is reduced compared to winds launched from the midplane ‚Äì such effects should be considered important if they are used to infer wind properties. (5/5) https://t.co/adziAOmMun', ""Forgot to add that for anyone interested in using the solutions, I've made my python script to calculate them available here: https://t.co/UBg3kt0bIz! Also includes a few examples used in the paper and some tables from our parameter searches (see contour plot in tweet 2) (6/5)."", '@RTal421 Thanks, Rosie!', ""@r_d_alexander Thanks, Richard, I'm glad to hear your feedback given how well you know these solutions! I agree it's probably just about at our limit right now if we want to firmly measure the effect of elevation, but hopefully they can help us understand observations more robustly soon!""]",https://arxiv.org/abs/2106.05362,"Thermal disc winds occur in many contexts and may be particularly important to the secular evolution and dispersal of protoplanetary discs heated by high energy radiation from their central star. In this paper we generalise previous models of self-similar thermal winds - which have self-consistent morphology and variation of flow variables - to the case of launch from an elevated base and to non-isothermal conditions. These solutions are well-reproduced by hydrodynamic simulations, in which, as in the case of isothermal winds launched from the mid-plane, we find winds launch at the maximum Mach number for which the streamline solutions extend to infinity without encountering a singularity. We explain this behaviour based on the fact that lower Mach number solutions do not fill the spatial domain. We also show that hydrodynamic simulations reflect the corresponding self-similar models across a range of conditions appropriate to photoevaporating protoplanetary discs, even when gravity, centrifugal forces, or changes in the density gradient mean the problem is not inherently scale free. Of all the parameters varied, the elevation of the wind base affected the launch velocity and flow morphology most strongly, with temperature gradients causing only minor differences. We explore how launching from an elevated base affects Ne II line profiles from winds, finding it increases (reduces) the full width at half maximum (FWHM) of the line at low (high) inclination to the line of sight compared with models launched from the disc mid-plane and thus weakens the dependence of the FWHM on inclination. ","The general applicability of self-similar solutions for thermal disc
  winds"
196,1402887200426102786,1065359486951538688,Titouan Parcollet,['Want to know more about @SpeechBrain1? Just have a look at the new paper üëÄüëÄ\n\npdf: <LINK>\nabs: <LINK>\n\nTutorials: <LINK>\nGitHub: <LINK>\nHuggingFace: <LINK>\nWebsite: <LINK> <LINK>'],https://arxiv.org/abs/2106.04624,"SpeechBrain is an open-source and all-in-one speech toolkit. It is designed to facilitate the research and development of neural speech processing technologies by being simple, flexible, user-friendly, and well-documented. This paper describes the core architecture designed to support several tasks of common interest, allowing users to naturally conceive, compare and share novel speech processing pipelines. SpeechBrain achieves competitive or state-of-the-art performance in a wide range of speech benchmarks. It also provides training recipes, pretrained models, and inference scripts for popular speech datasets, as well as tutorials which allow anyone with basic Python proficiency to familiarize themselves with speech technologies. ",SpeechBrain: A General-Purpose Speech Toolkit
197,1402677639215726595,1280142008070307843,Ryan-Rhys Griffiths,"['Excited to present (and open-source) a new approach for high-dimensional Bayesian optimisation which achieves state-of-the-art performance on property-guided molecule generation.\n\nPaper: <LINK>\nCode: <LINK> <LINK>', '@kamilest @hbouammar @ImanisMind @Henrymossmoss @austinjtripp @edaxberger @vdutor @DanielSWigh @perman_jorayev @arian_jamasb Thanks Kamilƒó!', '@UnivWavefun @hbouammar @ImanisMind @Henrymossmoss @austinjtripp @edaxberger @vdutor @DanielSWigh @perman_jorayev @arian_jamasb Thanks Labani!']",https://arxiv.org/abs/2106.03609,"We introduce a method combining variational autoencoders (VAEs) and deep metric learning to perform Bayesian optimisation (BO) over high-dimensional and structured input spaces. By adapting ideas from deep metric learning, we use label guidance from the blackbox function to structure the VAE latent space, facilitating the Gaussian process fit and yielding improved BO performance. Importantly for BO problem settings, our method operates in semi-supervised regimes where only few labelled data points are available. We run experiments on three real-world tasks, achieving state-of-the-art results on the penalised logP molecule generation benchmark using just 3% of the labelled data required by previous approaches. As a theoretical contribution, we present a proof of vanishing regret for VAE BO. ","High-Dimensional Bayesian Optimisation with Variational Autoencoders and
  Deep Metric Learning"
198,1402665891062505478,757255726553333760,Amir Feder,"['üëãüëãNew ACL paper: Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions üëãüëã\n\nWith: Daniel Rosenberg, @Itai_Gat, @roireichart\n\nAsks whether VQA systems understand language. \n<LINK>\n1/6', 'VQA systems seem to be constantly improving, with many different models achieving similar state-of-the-art performance. However, a more careful look reveals that they often do not understand the rich signal they are being fed with.\n\n2/6', 'To understand and measure their generalization capabilities, we look at their robustness to counterfactual augmentations.\n\nThese augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes.\n\n3/6', 'Using the augmentations, we introduce a new robustness measure, Robustness to Augmented\nData (RAD), which measures the consistency\nof model predictions between original and counterfactual examples.\n\n4/6', 'We show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. \n\nWe find substantial failure cases which reveal that current VQA systems are still brittle. \n\n5/6', 'We also connect between robustness and out-of-distribution generalization, demonstrating the predictive power of RAD for performance on unseen augmentations.\n\n6/6']",https://arxiv.org/abs/2106.04484,"Deep learning algorithms have shown promising results in visual question answering (VQA) tasks, but a more careful look reveals that they often do not understand the rich signal they are being fed with. To understand and better measure the generalization capabilities of VQA systems, we look at their robustness to counterfactually augmented data. Our proposed augmentations are designed to make a focused intervention on a specific property of the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented Data (RAD), which measures the consistency of model predictions between original and augmented examples. Through extensive experimentation, we show that RAD, unlike classical accuracy measures, can quantify when state-of-the-art systems are not robust to counterfactuals. We find substantial failure cases which reveal that current VQA systems are still brittle. Finally, we connect between robustness and generalization, demonstrating the predictive power of RAD for performance on unseen augmentations. ","Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused
  Interventions"
199,1402381056964055040,765341324279156736,Sinong Wang,"['Thrilled to share our new work! ""Luna: Linear Unified Nested Attention"".\n\nThis is a new linear time transformer architecture achieves competitive results across multiple benchmarks.\n\nCheck our our paper here: <LINK>\nThe implementation: <LINK>. <LINK>']",https://arxiv.org/abs/2106.01540,"The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety ",Luna: Linear Unified Nested Attention
200,1401894523660345348,953638180154150918,Roger Zhe Li,"['#SIGIR2021, here we go! Our full paper ‚ÄúNew Insights into Metric Optimization for Ranking-based Recommendation‚Äù is now ready online.\n \nLink to the paper: <LINK>\nCode and data are also available: <LINK>\n \nCo-authors: @julian_urbano @alanhanjalic', 'I will present the work and take questions in July on @SIGIRConf. Presentation slides and video are also available:\nSlides: https://t.co/S9Wmy0H9SY\nVideo: https://t.co/WU4lOfeFNB\n \nWelcome to join us in the virtual Montr√©al!', 'Direct optimization of evaluation metrics is an important method to train IR models. Previous work has either:\n- limited to traditional IR tasks instead of recommendation;\n- not focused on which metric to optimize for, but simply optimize for the metric used for evaluation.', 'This paper focuses on the choice of IR metrics to optimize for in ranking-aware recommender systems.\n \nWe made an empirical comparison on optimizing for four popular metrics (nDCG, AP, RR, RBP) with two different learning paradigms under three different data distributions.', 'Findings:\n- Optimizing for RBP can get good performance not only on RBP, but also on the other 3 metrics;\n- Our proposed listwise RBP optimization model helps improve the utility of all users, but more for active ones.']",https://arxiv.org/abs/2106.02545,"Direct optimization of IR metrics has often been adopted as an approach to devise and develop ranking-based recommender systems. Most methods following this approach aim at optimizing the same metric being used for evaluation, under the assumption that this will lead to the best performance. A number of studies of this practice bring this assumption, however, into question. In this paper, we dig deeper into this issue in order to learn more about the effects of the choice of the metric to optimize on the performance of a ranking-based recommender system. We present an extensive experimental study conducted on different datasets in both pairwise and listwise learning-to-rank scenarios, to compare the relative merit of four popular IR metrics, namely RR, AP, nDCG and RBP, when used for optimization and assessment of recommender systems in various combinations. For the first three, we follow the practice of loss function formulation available in literature. For the fourth one, we propose novel loss functions inspired by RBP for both the pairwise and listwise scenario. Our results confirm that the best performance is indeed not necessarily achieved when optimizing the same metric being used for evaluation. In fact, we find that RBP-inspired losses perform at least as well as other metrics in a consistent way, and offer clear benefits in several cases. Interesting to see is that RBP-inspired losses, while improving the recommendation performance for all uses, may lead to an individual performance gain that is correlated with the activity level of a user in interacting with items. The more active the users, the more they benefit. Overall, our results challenge the assumption behind the current research practice of optimizing and evaluating the same metric, and point to RBP-based optimization instead as a promising alternative when learning to rank in the recommendation context. ",New Insights into Metric Optimization for Ranking-based Recommendation
201,1401842311722635265,2559528436,Matt Nixon,"['New paper on the arXiv today! We use internal structure models to explore the phase structures of water-rich Earth-to-Neptune sized planets: <LINK>\n\nWe have a number of key findings: (1/6) <LINK>', 'We calculated the possible depths of liquid water oceans on this kind of planet. This depends mostly on the planet‚Äôs surface gravity and temperature, and can be up to hundred‚Äôs of times deeper than the Earth‚Äôs ocean (2/6) https://t.co/MRB6oW5pS0', 'We also looked at the possibility of planets with H/He envelopes having liquid water beneath their atmosphere. Depending on atmospheric conditions, it turns out this phenomenon could occur for a range of planet masses and radii as shown here (3/6) https://t.co/o9jdd6AeIg', 'We also revisit the mass-radius relation for these planets. It‚Äôs important to keep in mind that a high surface temperature can inflate water-rich planets substantially, so be sure to consider this when characterising super-Earths/mini-Neptunes (4/6) https://t.co/XIZ2nigFTV', 'Finally, we also looked at planets with mixed H/He/H2O envelopes. If the quantities of these components are comparable, planets with mixed envelopes have much lower radii than a planet with the same composition but a differentiated envelope (5/6) https://t.co/62H6R9lCOk', 'Feel free to get in touch if you have any questions or comments! (6/6)']",https://arxiv.org/abs/2106.02061,"Understanding the internal structures of planets with a large H$_2$O component is important for the characterisation of sub-Neptune planets. The finding that the mini-Neptune K2-18b could host a liquid water ocean beneath a mostly hydrogen envelope motivates a detailed examination of the phase structures of water-rich planets. To this end, we present new internal structure models for super-Earths and mini-Neptunes that enable detailed characterisation of a planet's water component. We use our models to explore the possible phase structures of water worlds and find that a diverse range of interiors are possible, from oceans sandwiched between two layers of ice to supercritical interiors beneath steam atmospheres. We determine how the bulk properties and surface conditions of a water world affect its ocean depth, finding that oceans can be up to hundreds of times deeper than on Earth. For example, a planet with a 300 K surface can possess H$_2$O oceans with depths from 30-500 km, depending on its mass and composition. We also constrain the region of mass-radius space in which planets with H/He envelopes could host liquid H$_2$O, noting that the liquid phase can persist at temperatures up to 647 K at high pressures of $218$-$7\times10^4$ bar. Such H/He envelopes could contribute significantly to the planet radius while retaining liquid water at the surface, depending on the planet mass and temperature profile. Our findings highlight the exciting possibility that habitable conditions may be present on planets much larger than Earth. ","How Deep Is the Ocean? Exploring the phase structure of water-rich
  sub-Neptunes"
202,1400631246061903872,957685323198164992,Ziwei Liu,"['Our #CVPR2021 paper ""Robust Reference-based Super-Resolution via C2-Matching"":\n\nPaper: <LINK>\nCode: <LINK>\n\n- *C2-Matching*: a robust ref-SR framework that significantly outperforms existing methods.\n- *WR-SR*: A new webly-referenced SR dataset. <LINK>']",https://arxiv.org/abs/2106.01863,"Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising paradigm to enhance a low-resolution (LR) input image by introducing an additional high-resolution (HR) reference image. Existing Ref-SR methods mostly rely on implicit correspondence matching to borrow HR textures from reference images to compensate for the information loss in input images. However, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (e.g. scale and rotation) and the resolution gap (e.g. HR and LR). To tackle these challenges, we propose C2-Matching in this work, which produces explicit robust matching crossing transformation and resolution. 1) For the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) For the resolution gap, we adopt a teacher-student correlation distillation, which distills knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR matching. 3) Finally, we design a dynamic aggregation module to address the potential misalignment issue. In addition, to faithfully evaluate the performance of Ref-SR under a realistic setting, we contribute the Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario. Extensive experiments demonstrate that our proposed C2-Matching significantly outperforms state of the arts by over 1dB on the standard CUFED5 benchmark. Notably, it also shows great generalizability on WR-SR dataset as well as robustness across large scale and rotation transformations. ",Robust Reference-based Super-Resolution via C2-Matching
203,1400387018098630657,67936420,Rahul,"['Check out our findings of @aclmeeting paper where we introduce two new flavors of MAML!\n<LINK> (1/4)', ""MAML assumes that source and target 'tasks' are i.i.d which is not realistic during cross-lingual transfer. High-resource languages belong to a few families, geographical areas, and typological features and do not reflect the majority of the world's languages (2/4) https://t.co/Mmc1Si52yx"", 'With the aim of better transfer across distant language families, we propose (i) Minimax criterion: which minimizes the maximum risk across languages, and (ii) Neyman-Pearson criterion: which upper-bounds the risk for any subset of languages. (3/4)', 'We perform experiments on POS-tagging and QA and show that the new criteria significantly improve performance over vanilla MAML and an MTL baseline. This is joint work with @PontiEdoardo, @DishaShrivasta9, @sivareddyg, and Anders S√∏gaard. (4/4) https://t.co/u3gGx4aLTE']",https://arxiv.org/abs/2106.01051,"Model-agnostic meta-learning (MAML) has been recently put forth as a strategy to learn resource-poor languages in a sample-efficient fashion. Nevertheless, the properties of these languages are often not well represented by those available during training. Hence, we argue that the i.i.d. assumption ingrained in MAML makes it ill-suited for cross-lingual NLP. In fact, under a decision-theoretic framework, MAML can be interpreted as minimising the expected risk across training languages (with a uniform prior), which is known as Bayes criterion. To increase its robustness to outlier languages, we create two variants of MAML based on alternative criteria: Minimax MAML reduces the maximum risk across languages, while Neyman-Pearson MAML constrains the risk in each language to a maximum threshold. Both criteria constitute fully differentiable two-player games. In light of this, we propose a new adaptive optimiser solving for a local approximation to their Nash equilibrium. We evaluate both model variants on two popular NLP tasks, part-of-speech tagging and question answering. We report gains for their average and minimum performance across low-resource languages in zero- and few-shot settings, compared to joint multi-source transfer and vanilla MAML. ",Minimax and Neyman-Pearson Meta-Learning for Outlier Languages
204,1413790959796736002,60381074,Oriol Corcoll,['üö®Did I do that? üö®Blame as a means to identify controlled effects in reinforcement learning.\n\nWe propose to use counterfactual measures of Blame to model controllable aspects of the environment. Accepted in the Unsupervised RL Workshop @icmlconf 2021.\n<LINK>'],https://arxiv.org/abs/2106.00266,"Identifying controllable aspects of the environment has proven to be an extraordinary intrinsic motivator to reinforcement learning agents. Despite repeatedly achieving State-of-the-Art results, this approach has only been studied as a proxy to a reward-based task and has not yet been evaluated on its own. Current methods are based on action-prediction. Humans, on the other hand, assign blame to their actions to decide what they controlled. This work proposes Controlled Effect Network (CEN), an unsupervised method based on counterfactual measures of blame to identify effects on the environment controlled by the agent. CEN is evaluated in a wide range of environments showing that it can accurately identify controlled effects. Moreover, we demonstrate CEN's capabilities as intrinsic motivator by integrating it in the state-of-the-art exploration method, achieving substantially better performance than action-prediction models. ","Did I do that? Blame as a means to identify controlled effects in
  reinforcement learning"
205,1413141265978335239,705850200473010178,Rumman Chowdhury,"['.@mona_sloane , @MannyMoss and I wrote a paper on auditing hiring algorithms. tl;dr we propose a matrix that incorporates: assumptions, epistemological roots, and systems of algorithms in the audit. \n\n<LINK>']",https://arxiv.org/abs/2106.12403,"In this paper, we suggest a systematic approach for developing socio-technical assessment for hiring ADS. We suggest using a matrix to expose underlying assumptions rooted in pseudoscientific essentialized understandings of human nature and capability, and to critically investigate emerging auditing standards and practices that fail to address these assumptions. ","A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and
  the Quest for Auditability"
206,1413116956555812870,735418346896818176,Dr. Mona Sloane,"['üì¢ New paper out w/ @ruchowdh &amp; @MannyMoss: ""A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and the Quest for Auditability"" <LINK> -- &gt; we propose a matrix for interdisciplinary #audits and socio-technical assessments of #AI used in #hiring üîé']",https://arxiv.org/abs/2106.12403,"In this paper, we suggest a systematic approach for developing socio-technical assessment for hiring ADS. We suggest using a matrix to expose underlying assumptions rooted in pseudoscientific essentialized understandings of human nature and capability, and to critically investigate emerging auditing standards and practices that fail to address these assumptions. ","A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and
  the Quest for Auditability"
207,1411044795376623620,1029465843393024005,Guohao Li,"['Updates (1/2): Paper ‚Äútraining GNNs with 1000 layers‚Äù got accepted at ICML‚Äô2021. We study reversible connections, equilibrium models, etc., to advance the mem&amp;param efficiency of GNNs. We train 1000-layer RevGNN on 1 GPU and achieve SOTA on ogbn-proteins.\n\n<LINK> <LINK>', 'Joint work with Matthias, @BernardSGhanem and Vladlen during my internship at Intel ISL.\nRanking top 1 on ogbn-proteins and ogbn-arxiv #OGB #GNN. Implemented with PyG and DGL.\n\nCode: https://t.co/r0KXeY8w05\n\nProject web: https://t.co/EEcif6gquc\n\n@intel @KAUST_News @KaustVision https://t.co/QmLVyOEVNF', 'Also thank @shaojieb for the helpful discussion and the reviewers, and area chairs for their constructive suggestions, which improve the paper substantially.']",https://arxiv.org/abs/2106.07476,"Deep graph neural networks (GNNs) have achieved excellent results on various tasks on increasingly large graph datasets with millions of nodes and edges. However, memory complexity has become a major obstacle when training deep GNNs for practical applications due to the immense number of nodes, edges, and intermediate activations. To improve the scalability of GNNs, prior works propose smart graph sampling or partitioning strategies to train GNNs with a smaller set of nodes or sub-graphs. In this work, we study reversible connections, group convolutions, weight tying, and equilibrium models to advance the memory and parameter efficiency of GNNs. We find that reversible connections in combination with deep network architectures enable the training of overparameterized GNNs that significantly outperform existing methods on multiple datasets. Our models RevGNN-Deep (1001 layers with 80 channels each) and RevGNN-Wide (448 layers with 224 channels each) were both trained on a single commodity GPU and achieve an ROC-AUC of $87.74 \pm 0.13$ and $88.24 \pm 0.15$ on the ogbn-proteins dataset. To the best of our knowledge, RevGNN-Deep is the deepest GNN in the literature by one order of magnitude. Please visit our project website this https URL for more information. ",Training Graph Neural Networks with 1000 Layers
208,1410993404373344257,841187782504656898,Dennis G Wilson,"['In our work with @ktlnml, we propose a new activation, scheduling, and regularization methods for differentiable NAS.\n<LINK>', 'We present differentiable NAS, ie DARTS, as a constrainted bilevel optimization problem where architecture and weights are being optimized at the same time in a constrained search space (eventually has to produce a single final architecture)', 'In DARTS, the top two architecture weights are used to select the final weights, but we observed that many architecture weights were similar during search, leading to a somewhat arbitrary final decision https://t.co/JRdiJfbvFn', 'With the modifications we proposed, the winning architecture choices are much clearer and happen earlier, making progressive architecture search a safer bet. https://t.co/L8DDLIxofh']",https://arxiv.org/abs/2106.11655,"Differentiable Architecture Search (DARTS) is a recent neural architecture search (NAS) method based on a differentiable relaxation. Due to its success, numerous variants analyzing and improving parts of the DARTS framework have recently been proposed. By considering the problem as a constrained bilevel optimization, we present and analyze DARTS-PRIME, a variant including improvements to architectural weight update scheduling and regularization towards discretization. We propose a dynamic schedule based on per-minibatch network information to make architecture updates more informed, as well as proximity regularization to promote well-separated discretization. Our results in multiple domains show that DARTS-PRIME improves both performance and reliability, comparable to state-of-the-art in differentiable NAS. ","DARTS-PRIME: Regularization and Scheduling Improve Constrained
  Optimization in Differentiable NAS"
209,1410773778913763333,1283150444,Maurizio Pierini,"['One year ago, we started a project on detecting unexpected #gravitationalwaves sources with #autoencoders. It took less to perform&amp;release the following study on the #FPGA demonstrator of the model than to finish writing the original paper üò±\n<LINK>', 'In this study, carried on by our friends at @imperialcollege, a recurrent AE trained to detect GW signals was accelerated on FPGA up to a latency &lt; 1 microsec. This could be used for real-time trigger at @LIGO and @ego_virgo', 'But this hardware demonstrator study also shows a preview of the paper that will appear soon: the #LSTM AE, even compressed down to 16-bits precision, outperform the other architectures on detecting the signal with unsupervised structure. Details on this will appear soon. https://t.co/jec8CUCQkh']",https://arxiv.org/abs/2106.14089,"This paper presents novel reconfigurable architectures for reducing the latency of recurrent neural networks (RNNs) that are used for detecting gravitational waves. Gravitational interferometers such as the LIGO detectors capture cosmic events such as black hole mergers which happen at unknown times and of varying durations, producing time-series data. We have developed a new architecture capable of accelerating RNN inference for analyzing time-series data from LIGO detectors. This architecture is based on optimizing the initiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory) network, by identifying appropriate reuse factors for each layer. A customizable template for this architecture has been designed, which enables the generation of low-latency FPGA designs with efficient resource utilization using high-level synthesis tools. The proposed approach has been evaluated based on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA. Experimental results show that with balanced II, the number of DSPs can be reduced up to 42% while achieving the same IIs. When compared to other FPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower latency. ","Accelerating Recurrent Neural Networks for Gravitational Wave
  Experiments"
210,1410327465164111872,1273068367092445190,Alex Bergman,"['Check out our new paper ""Fast Training of Neural Lumigraph Representations using Meta-learning"":\n<LINK>\n\nWe propose MetaNLR++, which is able to train and render neural scene representations in a fraction of the time that competing methods require! <LINK>', 'This is joint work with my awesome collaborators from the Stanford Computational Imaging Group - Petr Kellnhofer and Gordon Wetzstein (@GordonWetzstein)']",https://arxiv.org/abs/2106.14942,"Novel view synthesis is a long-standing problem in machine learning and computer vision. Significant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require. ",Fast Training of Neural Lumigraph Representations using Meta Learning
211,1410303768541876226,2490439280,Bhuwan Dhingra,"['Me: Who are the NBA champions?\nT5: The Cavaliers, of course!\n\nHow can we make language models more time-aware? Find out at <LINK> <LINK>']",https://arxiv.org/abs/2106.15110,"Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. But language models (LMs) are trained on snapshots of data collected at a specific moment in time, and this can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum -- those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently ""refreshed"" as new data arrives, without the need for retraining from scratch. ",Time-Aware Language Models as Temporal Knowledge Bases
212,1409815029340332033,1294542807441514496,Victor Valera Baca üßêüî≠,"['<LINK>\nFinally online! Together with Alexei Smirnov we developed further on the topic of my diploma thesis at @ictpnews. We study in depth the phenomenon of resonance refraction, through which neutrino oscillations are enhanced in an energy localized region.(1/4)', 'Coherence forward scattering of nu on a cold backgrounds (e.g. DM) induces a potential that affects neutrino oscillations. Resonance in the s-channel leads to an enhanced potential that manifest as perturbations of oscillation probability measured by neutrino experiments. (2/4) https://t.co/IqVaojRBW6', 'Interplay of the background potential with vacuum and standard matter effects leads to new features: New MSW resonances, shift of the standard MSW resonance point, differences in the values of the neutrino square mass difference at high and low energies, etc. (3/4) https://t.co/ELU0N7AeoP', 'This mechanism might be used to explore signatures of BSM physics, as well as to constraint models of non-standard neutrino interactions. Excess of events in an specefic energy region would be a smoking gun for resonance refraction. (4/4) https://t.co/ktH6H4VtI9', 'We show that this mechanism as an explanation for the low energy MiniBooNE excess is excluded from measurements at lower and higher energies, as well as astrophysical, cosmological, and laboratory bound on neutrino non-standard couplings. (5/4 (?)) https://t.co/0mofx1lBjm']",https://arxiv.org/abs/2106.13829,"The refraction index and matter potential depend on neutrino energy and this dependence has a resonance character associated to the production of the mediator in the $s-$channel. For light mediators and light particles of medium (background) the resonance can be realized at energies accessible to laboratory experiments. We study properties of the energy dependence of the potential for different C-asymmetries of background. Interplay of the background potential and the vacuum term leads to (i) bump in the oscillation probability in the resonance region, (ii) dip related to the MSW resonance in the background, (iii) substantial deviation of the effective $\Delta m^2$ above the resonance from the low energy value, etc. We considered generation of mixing in the background. Interactions with background shifts the energy of usual MSW resonance and produces new MSW resonances. Searches of the background effects allow us to put bounds on new interactions of neutrinos and properties of the background. We show that explanation of the MiniBooNE excess, as the bump due to resonance refraction, is excluded. ",Resonance refraction and neutrino oscillations
213,1409758535857082369,869647136480219136,Turlier lab,"['What are the physical principles underlying the formation of a blastocoel, the first biological cavity in animal development? We wrote a small review to draw new perspectives on this surprisingly little studied question.\n<LINK>', 'Some teaser: &lt;Blastulation might not be ‚Äúthe most important time in our life‚Äù, to refer to the famous quote of Lewis Wolpert on gastrulation, but it remains the very first morphogenetic event common to most animals.&gt;', '@WalentekLab Well, actually not in all species, which may raise the question of its function(s) we believe.', '@WalentekLab Happy to get any feedbacks !']",https://arxiv.org/abs/2106.14509,"The blastocoel is a fluid-filled cavity characteristic of animal embryos at the blastula stage. Its emergence is commonly described as the result of cleavage patterning, but this historical view conceals a large diversity of mechanisms and overlooks many unsolved questions from a biophysics perspective. In this review, we describe generic mechanisms for blastocoel morphogenesis, rooted in biological literature and simple physical principles. We propose novel directions of study and emphasize the importance to study blastocoel morphogenesis as an evolutionary and physical continuum. ",Blastocoel morphogenesis: a biophysics perspective
214,1409748895966584837,23237589,Tanya Urrutia,"['With the slew of eROSITA papers coming out, let me highlight the ones I was most involved with. The first is by @MarcellaBrusa (<LINK>) and highlights how we can find red quasars (feedbacking) with eROSITA. 1/n', 'Incidentally, the red quasar we found with X-ray techniques had already been found by me in 2009, this time using radio techniques (https://t.co/EP3UEnbWFq). The paper previews how with the all sky survey, we expect to find about 700 red quasars this way. 2/n', 'The next paper is by Yoshiki Toba (https://t.co/afD6aiQLZs) and compared the WISE W4 selected point sources with the X-ray catalog. The bright W4 population is expected to be AGN dominated as was proven by Spitzer a decade ago. 3/n', 'The W4 sources that have X-ray counterparts (~700) are mostly unobscured. But when we stack the undetected sources (~7500), we find that they show X-ray SEDs showing some obscuration. This effect is highlighted when we divide the sample in photo-z bins. 4/n', ""There's a big science potential with such huge X-ray areas now being covered with eROSITA also in stacking, which was my main contribution to the paper (and others in the works). 5/n"", ""Lastly, let me highlight that this work wouldn't have been possible without the X-ray catalog in the eFEDS region (https://t.co/hBXUpQjUg9) and *especially* without the counterpart assignment and identification in a mammoth paper lead by Mara Salvato (https://t.co/bEYcgoGKxT) 6/6""]",https://arxiv.org/abs/2106.14525,"Theoretical models of galaxy-AGN co-evolution ascribe an important role for the feedback process to a short, luminous, obscured, and dust-enshrouded phase during which the accretion rate of the SMBH is expected to be at its maximum and the associated AGN-driven winds are also predicted to be maximally developed. To test this scenario, we have isolated a text-book candidate from the eROSITA Final Equatorial-Depth Survey (eFEDS) obtained within the Performance and Verification program of the eROSITA telescope on board Spectrum Roentgen Gamma. From an initial catalog of 246 hard X-ray selected sources matched with the photometric and spectroscopic information available within the eROSITA and Hyper Suprime-Cam consortia, three candidates Quasars in the feedback phase have been isolated applying the diagnostic proposed in Brusa et al. (2015). Only one source (eFEDSU J091157.5+014327) has a spectrum already available (from SDSS-DR16, z=0.603) and it unambiguously shows the presence of a broad component (FWHM~1650 km/s) in the [OIII]5007 line. The associated observed L_[OIII] is ~2.6x10^{42} erg/s, one to two orders of magnitude larger than that observed in local Seyferts and comparable to those observed in a sample of z~0.5 Type 1 Quasars. From the multiwavelength data available we derive an Eddington Ratio (L_bol/L_Edd) of ~0.25, and a bolometric correction in the hard X-ray of k_bol~10, lower than those observed for objects at similar bolometric luminosity. The presence of an outflow, the high X-ray luminosity and moderate X-ray obscuration (L_X~10^44.8 erg/s, N_H~2.7x10^22 cm^-2) and the red optical color, all match the prediction of quasars in the feedback phase from merger driven models. Forecasting to the full eROSITA all-sky survey with its spectroscopic follow-up, we predict that by the end of 2024 we will have a sample of few hundreds such objects at z=0.5-2. ","The eROSITA Final Equatorial-Depth Survey (eFEDS): The first archetypal
  Quasar in the feedback phase discovered by eROSITA"
215,1409699591147032576,905159827936075776,Sam Greydanus,"['‚ÄúPiecewise-constant Neural ODEs‚Äù ‚è±Ô∏è\n\nPaper: <LINK>\nBlog: <LINK>\n\nWe propose a neural network which jumps through time adaptively. When used as a planner for billiards, it can focus compute around collisions and skip over boring, linear motion: <LINK>', '2\nLike an RNN, our model predicts a hidden state. Like a Neural ODE, it predicts a derivative of a hidden state (‚Äústate velocity‚Äù).\n\nWe assume that this hidden state velocity is piecewise-constant over variable durations of time (also predicted by the model). https://t.co/1MLzRHcSNo', '3\nIt makes sense to plan with variable durations rather than uniform RNN ticks.\n\nConsider billiards: to make a shot, humans think about ball collisions but rarely about the motion in between. https://t.co/xtkUATwbpv', '4\nWe tested our model in a billiards planning environment. It showed modest improvements over baselines in terms of planning speed and accuracy. https://t.co/8AIYBdqLn0', '5\nCool obs: our model learns a coordinate transformation (encoder/decoder) and a latent dynamics model (hidden state dynamics). Training encourages large steps through time.\n\nSo it learns coordinate transformations (observation &lt;-&gt; latent space) s.t. dynamics are maximally linear', '6\nSo, if we train on Cartesian coordinates of an object undergoing circular motion, it will approximately recover a Cartesian-to-Polar change of basis!! https://t.co/VsKbtz09Eh', '7\nIt also sort of learned a change of basis from pixels to Cartesian-like coordinates, but this got messy :(', '8\nThe topic of ‚Äútemporal abstract planning with RNNs/NODEs‚Äù has a lot of papers/previous work (we cite as many as possible).\n\nAnd yet, no previous results (or this work, for that matter) seem to work super well. I think this is one area where more great papers &amp; work are needed.', '9\nWe make our code available, etc. My coauthors were Alan Fern and Stefan Lee (thanks for the great collab). A number of others (including those @mlcollective) gave good advice and feedback üôè']",https://arxiv.org/abs/2106.06621,"Neural networks are a popular tool for modeling sequential data but they generally do not treat time as a continuous variable. Neural ODEs represent an important exception: they parameterize the time derivative of a hidden state with a neural network and then integrate over arbitrary amounts of time. But these parameterizations, which have arbitrary curvature, can be hard to integrate and thus train and evaluate. In this paper, we propose making a piecewise-constant approximation to Neural ODEs to mitigate these issues. Our model can be integrated exactly via Euler integration and can generate autoregressive samples in 3-20 times fewer steps than comparable RNN and ODE-RNN models. We evaluate our model on several synthetic physics tasks and a planning task inspired by the game of billiards. We find that it matches the performance of baseline approaches while requiring less time to train and evaluate. ",Piecewise-constant Neural ODEs
216,1409698766966333445,702236063431942144,"Mercedes Gimeno-Segovia, PhD",['New paper on arXiv!üìù Long time in the making üòÖ so very happy to finally share some newly minted schemes on entanglement generation for linear optical quantum computing. Hope you find the results as interesting and shocking as we did ü§ì<LINK>'],https://arxiv.org/abs/2106.13825,"Using only linear optical elements, the creation of dual-rail photonic entangled states is inherently probabilistic. Known entanglement generation schemes have low success probabilities, requiring large-scale multiplexing to achieve near-deterministic operation of quantum information processing protocols. In this paper, we introduce multiple techniques and methods to generate photonic entangled states with high probability, which have the potential to reduce the footprint of Linear Optical Quantum Computing (LOQC) architectures drastically. Most notably, we are showing how to improve Bell state preparation from four single photons to up to p=2/3, boost Type-I fusion to 75% with a dual-rail Bell state ancilla and improve Type-II fusion beyond the limits of Bell state discrimination. ",Creation of Entangled Photonic States Using Linear Optics
217,1409497657052209161,1310552063999438849,Hauke Group,"['üéâCongrats to Kevin, Janika &amp; @PhilippHauke for the work on measuring currents in quantum simulators, which is published on @_arXiv_quant_ph\nüìñFull articleüëâüèº<LINK>\n\nüó£""We propose a flexible non-invasive technique to measure currents in analog quantum simulators."" <LINK>']",https://arxiv.org/abs/2106.12599,"Despite the pristine abilities of analog quantum simulators to study quantum dynamics, possibilities to detect currents are sparse. Here, we propose a flexible non-invasive technique to measure currents in quantum many-body systems by weakly coupling the system to an ancilla, followed by a measurement of the ancilla population. We numerically benchmark the scheme at the example of interacting bosons in a Harper-Hofstadter optical-lattice ladder, and discuss potential experimental error sources. The highly flexible protocol can be used with both hard-core and soft-core bosons as well as fermions, is easily extendable to more general observables like current-current correlations, and applies to other setups beyond cold atoms as we exemplify for the trapped-ion platform. ",Non-invasive measurement of currents in analog quantum simulators
218,1409471296480722946,735281203008376832,Felipe Martins,"['Our work (me, @MateusGM, @HansBassani, @pedro_mbraga and Edna S. Barros) is now available on arXiv!\n\n<LINK>\n\nWe propose a framework for creating reinforcement learning environments for IEEE VSSS and RoboCup Small Size League robot soccer competitions.', 'Very proud of this contribution, which is part of my masters research, we will use this framework as a base for further research on reinforcement learning for robot soccer. Feel free to reach me on here if there is any questions about the work!']",https://arxiv.org/abs/2106.12895,"Reinforcement learning is an active research area with a vast number of applications in robotics, and the RoboCup competition is an interesting environment for studying and evaluating reinforcement learning methods. A known difficulty in applying reinforcement learning to robotics is the high number of experience samples required, being the use of simulated environments for training the agents followed by transfer learning to real-world (sim-to-real) a viable path. This article introduces an open-source simulator for the IEEE Very Small Size Soccer and the Small Size League optimized for reinforcement learning experiments. We also propose a framework for creating OpenAI Gym environments with a set of benchmarks tasks for evaluating single-agent and multi-agent robot soccer skills. We then demonstrate the learning capabilities of two state-of-the-art reinforcement learning methods as well as their limitations in certain scenarios introduced in this framework. We believe this will make it easier for more teams to compete in these categories using end-to-end reinforcement learning approaches and further develop this research area. ","rSoccer: A Framework for Studying Reinforcement Learning in Small and
  Very Small Size Robot Soccer"
219,1408357448835112967,434250693,Peter Karpov,"['Our preprint with Francesco Piazza on quantum droplets in extended Bose-Hubbard models is out! Interestingly, different BH models have been studied for ages, but there is still room for new phases we find: superfluid/Mott/supersolid/density-wave droplets <LINK>']",https://arxiv.org/abs/2106.13226,"Multimode optical cavities can be used to implement interatomic interactions which are highly tunable in strength and range. For bosonic atoms trapped in an optical lattice, cavity-mediated interactions compete with the short-range interatomic repulsion, which we study using an extended Bose-Hubbard model. Already in a single-mode cavity, where the corresponding interaction has an infinite range, a rich phase diagram has been experimentally observed, featuring density-wave and supersolid self-organized phases in addition to the usual superfluid and Mott insulator. Here we show that, for any finite range of the cavity-mediated interaction, quantum self-bound droplets dominate the ground state phase diagram. Their size and in turn density is not externally fixed but rather emerges from the competition between local repulsion and finite-range attraction. Therefore, the phase diagram becomes very rich, featuring both compressible superfluid/supersolid as well as incompressible Mott and density-wave droplets. Additionally, we observe droplets with a compressible core and incompressible outer shells. ","Light-induced quantum droplet phases of lattice bosons in multimode
  cavities"
220,1408351220557881350,21599898,Jake Lishman,"['A quantum advantage requires creating large coherent superpositions, but multilevel coherence is less studied than multipartite entanglement.  We demonstrate a practical measurement to quantify the number of superposed classical states.\n\nüëá New preprint! <LINK>', ""It can't produce false positives, even if you don't have direct measurement access to the classical-state basis you care about.  All you need is to take a one-dimensional interference pattern by letting the system evolve through time."", ""We did it with the motional state of a single trapped ion, but the same method is applicable to pretty much any system - there's nothing specific to trapped ions in the theory."", 'We built on a previous theory result out of Imperial about using these interference patterns with rank-1 projective measurements (üëá), and we extended it so it works for any type of measurement.\n\nhttps://t.co/4cET5Qn1sh', ""Since the method is provably safe against false positives, even if your coherent manipulations can't be trusted, it's useful for characterisation and verification of properties of NISQ devices."", 'Experimental work done by the ion trappers at Imperial College London (@iontrapimperial), with the theory side handled by me and Florian Mintert.\n\nhttps://t.co/wSdddPW1D6']",https://arxiv.org/abs/2106.12939,"Quantum coherence is one of the clearest departures from classical physics, exhibited when a system is in a superposition of different basis states. Here the coherent superposition of three motional Fock states of a single trapped ion is experimentally certified, with a procedure provably robust against imperfect operation. As the motional state cannot be directly interrogated, our scheme uses an interference pattern generated by projective measurement of the coupled qubit state. The minimum number of coherently superposed states is inferred from a series of threshold values based on analysis of the interference pattern. This demonstrates that high-level coherence can be verified and investigated with simple, nonideal control methods well-suited to noisy intermediate-scale quantum devices. ",Certifying Multilevel Coherence in the Motional State of a Trapped Ion
221,1408342557663318020,1061375141274439685,Xabier Cid Vidal üõ∞Ô∏è,"['New paper! On <LINK> with @CharlesTheVS and many others we study the LHCb sensitivity to the wonderful baryogenesis model proposed in <LINK>. Spoiler alert: we CAN discover DM at LHCb! The mechanism involves b-hadron decays to a dark sector üëáüëá', 'particle which we can\'t detect but can look for as ""missing-pT"" in the direction of flight of the hadron. This type of signatures appear in other LHCb analyses and we know we can do them pretty well. I\'m specially happy for Saul Lopez,', 'to which I propose doing his Master thesis working on this and who will now get his work (hopefully) published. If you want to know more, I recommend you attend the #offshell2021 conference, where we will present our work.']",https://arxiv.org/abs/2106.12870,"A model that can simultaneously explain Dark Matter relic density and the apparent matter anti-matter imbalance of the universe has been recently proposed. The model requires $b$-hadron branching fractions to Dark Matter at the per mille level. The $b$-hadrons decay to a dark sector baryon, $\psi_{\rm{DS}}$, which has a mass in the region $940$ MeV/c$^{2} \leq m(\psi_{\rm{DS}}) \leq 4430$ MeV/c$^{2}$. In this paper, we discuss the sensitivity of the LHCb experiment to search for this dark baryon, covering different types of topology and giving prospects for Runs 3 and 4 of the LHC, as well as for the proposed Phase-II Upgrade. We show that the LHCb experiment can cover the entire mass range of the hypothetical dark baryon. ","Prospects on searches for baryonic Dark Matter produced in $b$-hadron
  decays at LHCb"
222,1408331159633866752,786855300322172928,Alkistis Pourtsidou,"['A new paper led by @PedroCarrilho11 and Chiara Moretti performs a validation and mcmc forecast study for a non-standard (interacting) DE model <LINK>, focusing on the theoretical systematic of non-linearities when we interpret galaxy clustering measurements.', 'Pedro and Chiara (+ Ben Bose and @DidaMarkovic) carefully compared the performance of the most popular perturbation theory approaches,  that have been used for the analysis of e.g. BOSS data. https://t.co/OND8fo6bYD', 'They finally forecast unbiased constraints on the coupling parameter of the interacting dark energy model for surveys similar to @EC_Euclid and @desisurvey https://t.co/8z7TBDzNt2']",https://arxiv.org/abs/2106.13163,"Interacting dark energy models have been proposed as attractive alternatives to $\Lambda$CDM. Forthcoming Stage-IV galaxy clustering surveys will constrain these models, but they require accurate modelling of the galaxy power spectrum multipoles on mildly non-linear scales. In this work we consider a dark scattering model with a simple 1-parameter extension to $w$CDM - adding only $A$, which describes a pure momentum exchange between dark energy and dark matter. We then provide a comprehensive comparison of three approaches of modeling non-linearities, while including the effects of this dark sector coupling. We base our modeling of non-linearities on the two most popular perturbation theory approaches: TNS and EFTofLSS. To test the validity and precision of the modelling, we perform an MCMC analysis using simulated data corresponding to a $\Lambda$CDM fiducial cosmology and Stage-IV surveys specifications in two redshift bins, $z=0.5$ and $z=1$. We find the most complex EFTofLSS-based model studied to be better suited at both, describing the mock data up to smaller scales, and extracting the most information. Using this model, we forecast uncertainties on the dark energy equation of state, $w$, and on the interaction parameter, $A$, finding $\sigma_w=0.06$ and $\sigma_A=1.1$ b/GeV for the analysis at $z=0.5$ and $\sigma_w=0.06$ and $\sigma_A=2.0$ b/GeV for the analysis at $z=1$. In addition, we show that a false detection of exotic dark energy up to 3$\sigma$ would occur should the non-linear modelling be incorrect, demonstrating the importance of the validation stage for accurate interpretation of measurements. ",Interacting dark energy from redshift-space galaxy clustering
223,1408224623725064195,110103071,Andrej Risteski,"['In the invariant feature approach to domain generalization, the goal is to identify invariant features after seeing a small # of environments. How many envs are needed? We study this q in a variant of a toy data model we introduced w @ElanRosenfeld in <LINK>. <LINK>', ""For this variant, we show that while ERM and IRM both can fail with sublinear number of environments, an *iterative* feature matching approach we introduce in the paper succeeds using only logarithmic # of domains. (For more details see Tengyu's tweet and paper)"", 'Joint work with @cynnjjs, @elanrosenfeld, Mark Sellke, and @tengyuma .']",https://arxiv.org/abs/2106.09913,"Domain generalization aims at performing well on unseen test environments with data from a limited number of training environments. Despite a proliferation of proposal algorithms for this task, assessing their performance both theoretically and empirically is still very challenging. Distributional matching algorithms such as (Conditional) Domain Adversarial Networks [Ganin et al., 2016, Long et al., 2018] are popular and enjoy empirical success, but they lack formal guarantees. Other approaches such as Invariant Risk Minimization (IRM) require a prohibitively large number of training environments -- linear in the dimension of the spurious feature space $d_s$ -- even on simple data models like the one proposed by [Rosenfeld et al., 2021]. Under a variant of this model, we show that both ERM and IRM cannot generalize with $o(d_s)$ environments. We then present an iterative feature matching algorithm that is guaranteed with high probability to yield a predictor that generalizes after seeing only $O(\log d_s)$ environments. Our results provide the first theoretical justification for a family of distribution-matching algorithms widely used in practice under a concrete nontrivial data model. ","Iterative Feature Matching: Toward Provable Domain Generalization with
  Logarithmic Environments"
224,1408158879775485953,1124967058414825472,Sindhana Pannir-Sivajothi,"['My first paper in grad school ü§© Thanks for being a wonderful advisor @ucsd_yuen! I learned a lot from you @JorgeACamGA and @LamQ310.\n\nOur first paper together @ShubhamSinha029 ‚ù§Ô∏è\n\nWe find that polariton condensation can change reaction yields and rates.\n<LINK> <LINK>', '@pleplostelous @ucsd_yuen @JorgeACamGA @LamQ310 @ShubhamSinha029 Thanks Avishek! :)', '@Sridevi292 @ucsd_yuen @JorgeACamGA @LamQ310 @ShubhamSinha029 Thanks Sridevi! :)', '@maity_indra_ @ucsd_yuen @JorgeACamGA @LamQ310 @ShubhamSinha029 Thanks Indrajit :D']",https://arxiv.org/abs/2106.12156,"When molecular transitions strongly couple to photon modes, they form hybrid light-matter modes called polaritons. Collective vibrational strong coupling is a promising avenue for control of chemistry, but this can be deterred by the large number of quasi-degenerate dark modes. The macroscopic occupation of a single polariton mode by excitations, as observed in Bose-Einstein condensation, offers promise for overcoming this issue. Here we theoretically investigate the effect of vibrational polariton condensation on the kinetics of electron transfer processes. Compared with excitation with infrared laser sources, the condensate changes the reaction yield significantly due to additional channels with reduced activation barriers resulting from the large accumulation of energy in the lower polariton, and the many modes available for energy redistribution during the reaction. Our results offer tantalizing opportunities to use condensates for driving chemical reactions, kinetically bypassing usual constraints of fast intramolecular vibrational redistribution in condensed phase. ",Driving chemical reactions with polariton condensates
225,1408065679354257417,717162062837719040,Phil Armitage,"[""New work, led by Zhaohuan Zhu @unlv, with Yan-Fei Jiang @FlatironCCA and NASA TCAN collaborators. We look at proto-Jupiter's gaseous envelope, as part of an effort to use radiation hydrodynamics to study the basic physics of core accretion. (1/8)\n\n<LINK> <LINK>"", 'Core accretion likely formed the Solar System\'s giant planets, and the vast majority of extrasolar planets with gaseous envelopes. It\'s a 3 stage process: a rocky or icy core forms first, accretes a slowly growing envelope, then ""runs away"" and accretes much faster. (2/8) https://t.co/IMjieLBBTe', 'Core accretion is well-studied. But new theoretical questions have arisen, from the discovery of ""recycling"" flows that exchange gas between the protoplanetary disk and the envelope. And observationally, many close-in planets appear to have gotten stuck in the middle stage. (3/8)', 'A LOT of work was done for the new simulations! We have updated opacities (available here https://t.co/ElHAffafvU), improved radiation transfer (using Jiang 2021), and setups that isolate 3D effects and allow 1D comparisons. (4/8)', 'The central result is a bit odd. We confirm (using a passive scalar tracer field) that recycling is strong for convective envelopes, but despite this the effect on the structure and cooling is modest. Cooling is what determines the long-term evolution. (5/8)', 'The 3D effects can be incorporated into 1D models using simple modifications to the boundary conditions. Doing that, we generate fiducial Jupiter formation tracks. The time scale to form Jupiter depends on the envelope opacity, as has been known for ages. (6/8) https://t.co/VyiFTWk8nz', ""I'd caution that the results may not generalize to closer-in (or further out!) giant planet formation. We plan to study those case too, though computationally they pose some extra difficulties. (7/8)"", 'Thanks to Zhaohuan, Yan-Fei and the other authors: Hans Baehr and Rebecca Martin at UNLV, and Andrew Youdin at Arizona. For once, no magnetic fields were harmed in the making of this paper! (8/8)']",https://arxiv.org/abs/2106.12003,"The core accretion model of giant planet formation has been challenged by the discovery of recycling flows between the planetary envelope and the disc that can slow or stall envelope accretion. We carry out 3D radiation hydrodynamic simulations with an updated opacity compilation to model the proto-Jupiter's envelope. To isolate the 3D effects of convection and recycling, we simulate both isolated spherical envelopes and envelopes embedded in discs. The envelopes are heated at given rates to achieve steady states, enabling comparisons with 1D models. We vary envelope properties to obtain both radiative and convective solutions. Using a passive scalar, we observe significant mass recycling on the orbital timescale. For a radiative envelope, recycling can only penetrate from the disc surface until $\sim$0.1-0.2 planetary Hill radii, while for a convective envelope, the convective motion can ""dredge up"" the deeper part of the envelope so that the entire convective envelope is recycled efficiently. This recycling, however, has only limited effects on the envelopes' thermal structure. The radiative envelope embedded in the disc has identical structure as the isolated envelope. The convective envelope has a slightly higher density when it is embedded in the disc. We introduce a modified 1D approach which can fully reproduce our 3D simulations. With our updated opacity and 1D model, we recompute Jupiter's envelope accretion with a 10 $M_{\oplus}$ core, and the timescale to runaway accretion is shorter than the disc lifetime as in prior studies. Finally, we discuss the implications of the efficient recycling on the observed chemical abundances of the planetary atmosphere (especially for super-Earths and mini-Neptunes). ","Global 3D Radiation Hydrodynamic Simulations of Proto-Jupiter's
  Convective Envelope"
226,1407770346321858565,325547802,Freddie Bickford Smith,"['New paper with @bdroads, @ken_lxl &amp; @profdata. How does top-down attention help in vision? Contrasting with standard accounts that point to stimulus variables like clutter, we find that system variables capturing model-data-task interaction are key. [1/7]\n<LINK> <LINK>', 'Building on recent work by eg @neurograce &amp; @ken_lxl, plus classics like ALCOVE, we use a CNN with attention incorporated as feature-map modulation. This lets us scale to thousands of naturalistic tasks with multiple factors of variation, unlike in a typical lab experiment. [2/7]', 'Seeking to understand how attention boosts perception, we take the popular template for an ablation study (see @fchollet thread) and extend it to get deeper insights. Task-oriented ablation design (TOAD), our framework, is applicable across ML. [3/7]\nhttps://t.co/84dGarumO9', 'The core idea of TOAD: (1) quantify differences between tasks; (2) use this information to design and analyse an ablation study. More than just indicating *whether* your ML method works, TOAD tells you *when* (and thus possibly *how*) it works. [4/7] https://t.co/YfCnhxilBp', 'Using TOAD, we test our model on lots of tasks. To some degree, our results reflect the quirks of CNNs (see @jh_jacobsen thread). But the bigger takeaway is a view on attention as handling the interplay between model, training data and task format. [5/7]\nhttps://t.co/NeI2XF3JSG', 'This system-level view resonates with recent work by @khermann_, @andrewlampinen, @tingchenai &amp; @skornblith (see thread). If attention modulates representations, is the process that produces the representations the most valuable thing to analyse? [6/7]\nhttps://t.co/XaEfAsHwsc', 'We‚Äôd love to hear your feedback and answer questions. Send me a message! Thanks to those who helped along the way: @egrefen, @summerfieldlab &amp; Will Tebbutt. Work done at @aims_oxford, @uclcs &amp; @uclpals, and funded by @epsrc, @kelloggox, @nih, @royalsociety &amp; @wellcometrust. [7/7]', '@AndrewLampinen @egrefen @summerfieldlab @aims_oxford @uclcs @UCLPALS @EPSRC @KelloggOx @NIH @royalsociety @wellcometrust Good question! Like you, we thought there might be a ceiling effect at play. We checked and found no clear evidence for this in the results.']",http://arxiv.org/abs/2106.11339,"Top-down attention allows neural networks, both artificial and biological, to focus on the information most relevant for a given task. This is known to enhance performance in visual perception. But it remains unclear how attention brings about its perceptual boost, especially when it comes to naturalistic settings like recognising an object in an everyday scene. What aspects of a visual task does attention help to deal with? We aim to answer this with a computational experiment based on a general framework called task-oriented ablation design. First we define a broad range of visual tasks and identify six factors that underlie task variability. Then on each task we compare the performance of two neural networks, one with top-down attention and one without. These comparisons reveal the task-dependence of attention's perceptual boost, giving a clearer idea of the role attention plays. Whereas many existing cognitive accounts link attention to stimulus-level variables, such as visual clutter and object scale, we find greater explanatory power in system-level variables that capture the interaction between the model, the distribution of training data and the task format. This finding suggests a shift in how attention is studied could be fruitful. We make publicly available our code and results, along with statistics relevant to ImageNet-based experiments beyond this one. Our contribution serves to support the development of more human-like vision models and the design of more informative machine-learning experiments. ",Understanding top-down attention using task-oriented ablation design
227,1407419464723935238,301450268,Mireia Montes,"['Paper day!! Today we focus on the ""enigmatic"" galaxy ""lacking"" dark matter: NGC1052-DF2 (the first one). You can find the accepted paper here: <LINK>\n@infantesainz @asborlaff <LINK>', 'First, we investigated the globular cluster system of this galaxy and we found that all but two of them are contained in the stellar body of the galaxy. That is different to what we found for DF4. https://t.co/X7PSx27dAM', 'This + that the galaxy does not show signs of interaction in VERY DEEP imaging is telling us that the galaxy is not interacting with anything. https://t.co/PxGYaIU9D2', 'So what is going on here? Well, @Cosmic_Horizons published a paper finding evidence for rotation in the GCs of the galaxy. The claimed total mass of the galaxy was derived assuming that this was not the case.', 'In our deep images we see that the galaxy, in the outer parts, resembles a disk. The inner parts, what was seen till now, are rounder and spheroidal (Sersic profile n = 0.6, pink), but the outer parts are more elliptical and disky (n =~1, black). https://t.co/EOGrF5ci9m', 'When we put the rotation map on top our images we can see that the position angle of both things (rotation and stars) coincide, strongly suggesting that the galaxy is rotating. https://t.co/3LmENJxyXW', 'This is independent of the distance assumed for this galaxy (which is still controversial). For 20 Mpc there is a factor of 2.6 in total mass, while at 13 Mpc is a factor of 1.5. So: more total mass = more dark matter.', 'To sum up: \n- DF2 is rotating.\n- DF4 is interacting with a neighbor.\nTwo different explanations for two different galaxies. ü§∑\u200d‚ôÄÔ∏è', 'All this is thanks to the newest Dr @infantesainz for his expertise in data reduction. He is giving a talk at the EAS 2021 S12 that you should listen to. \nAlso @asborlaff for his HST images. He is also giving a talk at EAS S12.', ""Please appreciate, again, the color scheme of the paper. It is complementary to that of DF4's paper."", ""@DanieliShany I don't understand why you say that I am ignoring the paper when it is cited in the paper and all the calculations/estimations are done using both distances."", ""@DanieliShany We don't refer to Danieli+2020 because we are barely talking about DF4 in this paper. Only to refer to parts of the analysis that are in common with my previous paper. For Shen+2021b is here: https://t.co/a3DCG6abBk"", '@DanieliShany We only cite Monelli &amp; Trujillo 2019 when referring to the distance to DF2 and NGC 1042.', '@astrocra VERY VERY']",https://arxiv.org/abs/2106.10283,"Using ultra-deep imaging ($\mu_g = 30.4$ mag/arcsec$^2$; 3$\sigma$, 10""x10""), we probed the surroundings of the first galaxy ""lacking"" dark matter KKS2000[04] (NGC 1052-DF2). Signs of tidal stripping in this galaxy would explain its claimed low content of dark matter. However, we find no evidence of tidal tails. In fact, the galaxy remains undisturbed down to a radial distance of 80 arcsec. This radial distance triples previous spatial explorations of the stellar distribution of this galaxy. In addition, the distribution of its globular clusters (GCs) is not extended in relation to the bulk of the galaxy (the radius containing half of the GCs is 21 arcsec). We also found that the surface brightness radial profiles of this galaxy in the g and r bands decline exponentially from 35 to 80 arcsec. That, together with a constant ellipticity and position angle in the outer parts of the galaxy strongly suggests the presence of a low-inclination disk. This is consistent with the evidence of rotation found for this object. This finding implies that the dynamical mass of this galaxy is a factor of 2 higher than previously reported, bringing the dark matter content of this galaxy in line with galaxies of similar stellar mass. ","A disk and no signatures of tidal distortion in the galaxy ""lacking""
  dark matter NGC 1052-DF2"
228,1407247333499285510,62044012,Michael Bronstein,"['#GNNs are related to PDEs governing information diffusion on graphs. In a new paper with @b_p_chamberlain James Rowbottom @migorinova @stefan_webb @emaros96  we study a new class of Neural Graph Diffusion PDEs\n\nBlog post: <LINK>\n\nPaper: <LINK> <LINK>', 'Thinking of GNNs as partial differential equations leads to a new broad class of GNNs that are able to address in a principled way some of the prominent issues of current Graph ML models such as depth, oversmoothing, bottlenecks, and graph rewiring.', 'Popular GNNs can be formalized as discretized diffusion PDEs with explicit single-step Euler scheme, where an iteration corresponds to a GNN layer, and running the diffusion for multiple iterations amounts to applying a GNN layer multiple times.', 'In Neural PDEs formalism, the diffusion time parameter acts as a continuous analogy of the layers‚Äîan interpretation allowing us to exploit more efficient and stable numerical schemes that use adaptive steps in time.', 'It might be advantageous to decouple the graph used for diffusion from the input graph (or ""rewire"" the graph). The diffusion framework offers a principled view on graph rewiring by considering the graph as a spatial discretization of some continuous object (manifold).', 'In particular, the popular GAT of @PetarV_93 is a nonlinear diffusion PDE with a learnable diffusivity function similar to the Perona-Malik diffusion model used in the 90s in image processing']",https://arxiv.org/abs/2106.10934,"We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks. ",GRAND: Graph Neural Diffusion
229,1407123342428184582,2415164311,Mufan (Bill) Li,"['There has been a flurry of work beyond the infinite-width limit. We study the infinite DEPTH-AND-WIDTH limit of ReLU nets with residual connections and see remarkable (!) agreement with STANDARD finite networks. Joint work w/ @MihaiCNica @roydanroy <LINK> <LINK>', 'How is the infinite depth-and-width limit different? In short, each layer of width (n) carries an error term of size O(1/n), and increasing depth (d) compounds the error exponentially. At the heart of the analysis is the following ""dichotomy"": https://t.co/YtVj4Gfw0Z', 'As a result, the infinite depth-and-width limit is not Gaussian. This work extends results for fully connected MLPs where the analysis is much simpler. See @MihaiCNica‚Äôs youtube video for an introduction. https://t.co/6u0z32SwW0', 'However, networks with skip connections introduce correlations between layers, which complicates the analysis. Surprising observation: with residual connections, the population of neurons is HYPOactivated, i.e., fewer than half of the ReLU units are active. https://t.co/gTGZf2NCWO', 'Our main result is a precise description of the distribution of the output of the network in the infinite depth-and-width limit. One key observation: the magnitude contains a log-Gaussian factor. The exact constants and Gaussian parameters can be found in the paper. https://t.co/gTlmcLc8wF', 'We believe this result can be extended to non-Gaussian weights. See an earlier universality result for MLPs by @BorisHanin and @MihaiCNica https://t.co/HRbunQ7og3']",https://arxiv.org/abs/2106.04013,"Theoretical results show that neural networks can be approximated by Gaussian processes in the infinite-width limit. However, for fully connected networks, it has been previously shown that for any fixed network width, $n$, the Gaussian approximation gets worse as the network depth, $d$, increases. Given that modern networks are deep, this raises the question of how well modern architectures, like ResNets, are captured by the infinite-width limit. To provide a better approximation, we study ReLU ResNets in the infinite-depth-and-width limit, where both depth and width tend to infinity as their ratio, $d/n$, remains constant. In contrast to the Gaussian infinite-width limit, we show theoretically that the network exhibits log-Gaussian behaviour at initialization in the infinite-depth-and-width limit, with parameters depending on the ratio $d/n$. Using Monte Carlo simulations, we demonstrate that even basic properties of standard ResNet architectures are poorly captured by the Gaussian limit, but remarkably well captured by our log-Gaussian limit. Moreover, our analysis reveals that ReLU ResNets at initialization are hypoactivated: fewer than half of the ReLUs are activated. Additionally, we calculate the interlayer correlations, which have the effect of exponentially increasing the variance of the network output. Based on our analysis, we introduce Balanced ResNets, a simple architecture modification, which eliminates hypoactivation and interlayer correlations and is more amenable to theoretical analysis. ","The Future is Log-Gaussian: ResNets and Their Infinite-Depth-and-Width
  Limit at Initialization"
230,1406988626374762503,1314126878823849984,Max Ryabinin,"['In our latest work, we propose DeDLOC ‚Äî a method for efficient collaborative training. This approach allowed us to pretrain sahajBERT (a Bengali-language ALBERT) together with the help of volunteers from the community! (1/10)\n\n<LINK>\n<LINK> <LINK>', 'The method relies on the standard data-parallel training paradigm: workers compute gradients independently and exchange them to make large-batch SGD steps. If some peers disconnect, we can still achieve the equivalent results by aggregating data from other participants. (2/10) https://t.co/tvMhNGVxMy', 'Training over slower networks requires efficient data transfer. We can encode both compute and network performance as an easy-to-solve linear programming problem.\n\nIts solution returns the optimal strategy that recovers well-known distributed DL methods as special cases! (3/10) https://t.co/3Aw6vSm05V', ""In controlled experiments (SwAV/ALBERT pretraining), we show that adaptive averaging allows us to converge competitively to regular high-tier multi-GPU setups while being more cost-efficient ‚Äî even when some workers don't participate all the time (4/10) https://t.co/HfhitwObJX"", 'To prove that this can be used in practice, we trained a version of ALBERT for Bengali with the @NeuroPark community. The resulting model performs very competitively even with XLM-R Large, which used hundreds of HPC GPUs and has &gt;30x more parameters (5/10) https://t.co/NjeYKsqsXe', 'This project was a joint effort of people from @YandexAI, @huggingface, @HSE_eng, @mipt_eng, @VectorInst, @UofT, and @NeuroPark; working collectively on things of this scale has been an incredible experience, and I look forward to seeing how we can develop this paradigm (6/10)', 'Huge thanks to @michael_diskin, Alexey Bukhtiyarov, @LucileSaulnier, @qlhoest, Anton Sinitsin, Dmitry Popov, Dmitry Pyrkin, Maxim Kashirin, @sasha_borzunov, @avillanovamoral, @deniskamazur, Ilia Kobelev, @YJernite, @Thom_Wolf, Gennady Pekhimenko, who made all this possible (7/10)', 'Also, would like to thank @StasBekman, @abhi1thakur, @tanmoyio, and @osanseviero for their help with building  \nthe infrastructure and conducting the experiments, and the entire @NeuroPark community for actually devoting their time and resources to sahajBERT training (8/10)', 'The majority of our code is already in the Hivemind library https://t.co/OXvKzhh1El: it contains an example with ALBERT pretraining, so you can try setting up a collaborative experiment with your friends! (9/10)', 'If you want to learn more about DeDLOC, sahajBERT or decentralized DL in general, stay tuned: we have many interesting things coming up in the nearest future (10/10)']",http://arxiv.org/abs/2106.10207,"Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that benefit all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientific areas. However, using this approach for machine learning is difficult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost. Finally, we provide a detailed report of successful collaborative language model pretraining with 40 participants. ",Distributed Deep Learning in Open Collaborations
231,1406981554111275008,1038161373451218944,John D. Martin,"['Our new preprint is now available on arXiv. We touch on a central question in AI: how can a machine make sense of uninterpreted sensory experience. We study this from the perspective of RL.\n\nJoint work with Joseph Modayil @DeepMind.  \n<LINK>\n[1/7]', 'We find that auxiliary predictions (GVFs) can provide useful statistics for structuring observations, by imposing sparse connections in a neural network that represents a value function. We call these connections prediction adapted neighborhoods.\n[2/7]', 'Prediction adapted neighborhoods can resemble spatial neighborhoods in domains with spatial patterns. Consider a frog that tracks an insect through the randomly-dispersed light receptors of its eye. Here, locality is useful, but it is unapparent from the input.\n[3/7] https://t.co/dIUR8OvKX2', 'When a learner adapts sparse connections with predictive information, its performance can closely approach that of a biased architecture, whose connections derive from exogenous distance information relating inputs. The adaptive learner, however, requires no such bias.\n[4/7] https://t.co/VgDQI8rCcG', 'We provide evidence of a novel auxiliary learning effect; one where performance scales as sparse connections strengthen their temporal associations, rather than the regularization of shared parameters.\n[5/7] https://t.co/hlqZkFwyWB', 'Working on this paper together with Joseph and the other folks at @DeepMind was a ton of fun. Thinking back, the experience feels like somewhat of a metanoia, because of how it changed the way I view observation and perception.\n[6/7]', ""Moving forward, I'm excited to continue thinking about these ideas, and especially how they could lead to scalable RL algorithms for more general problems, where the observation structure is unavailable.  \n[7/7]""]",http://arxiv.org/abs/2106.09776,"The performance of a reinforcement learning (RL) system depends on the computational architecture used to approximate a value function. Deep learning methods provide both optimization techniques and architectures for approximating nonlinear functions from noisy, high-dimensional observations. However, prevailing optimization techniques are not designed for strictly-incremental online updates. Nor are standard architectures designed for observations with an a priori unknown structure: for example, light sensors randomly dispersed in space. This paper proposes an online RL prediction algorithm with an adaptive architecture that efficiently finds useful nonlinear features. The algorithm is evaluated in a spatial domain with high-dimensional, stochastic observations. The algorithm outperforms non-adaptive baseline architectures and approaches the performance of an architecture given side-channel information. These results are a step towards scalable RL algorithms for more general problems, where the observation structure is not available. ","Adapting the Function Approximation Architecture in Online Reinforcement
  Learning"
232,1406885385763016706,561167071,Sascha Caron,"['""Rare and Different"" with @l_hendriks and Rob Verheyen\n\nToday we propose new anomaly scores for LHC  to optimally combine that events are different (with a new ensemble of DeepSVDs) and rare (using the likelihood of a autoregressive flow model). \n\nSee <LINK>']",https://arxiv.org/abs/2106.10164,"We propose a new method to define anomaly scores and apply this to particle physics collider events. Anomalies can be either rare, meaning that these events are a minority in the normal dataset, or different, meaning they have values that are not inside the dataset. We quantify these two properties using an ensemble of One-Class Deep Support Vector Data Description models, which quantifies differentness, and an autoregressive flow model, which quantifies rareness. These two parameters are then combined into a single anomaly score using different combination algorithms. We train the models using a dataset containing only simulated collisions from the Standard Model of particle physics and test it using various hypothetical signals in four different channels and a secret dataset where the signals are unknown to us. The anomaly detection method described here has been evaluated in a summary paper [1] where it performed very well compared to a large number of other methods. The method is simple to implement and is applicable to other datasets in other fields as well. ","Rare and Different: Anomaly Scores from a combination of likelihood and
  out-of-distribution models to detect new physics at the LHC"
233,1405880476800368643,34488432,Joel Z Leibo,"['In our latest paper we propose a model of social norm evolution based on agents that learn to classify observed behavior of others into socially approved and disapproved categories, and are motivated to disapprove in accord with their social group.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2106.09012,"Society is characterized by the presence of a variety of social norms: collective patterns of sanctioning that can prevent miscoordination and free-riding. Inspired by this, we aim to construct learning dynamics where potentially beneficial social norms can emerge. Since social norms are underpinned by sanctioning, we introduce a training regime where agents can access all sanctioning events but learning is otherwise decentralized. This setting is technologically interesting because sanctioning events may be the only available public signal in decentralized multi-agent systems where reward or policy-sharing is infeasible or undesirable. To achieve collective action in this setting we construct an agent architecture containing a classifier module that categorizes observed behaviors as approved or disapproved, and a motivation to punish in accord with the group. We show that social norms emerge in multi-agent systems containing this agent and investigate the conditions under which this helps them achieve socially beneficial outcomes. ","A learning agent that acquires social norms from public sanctions in
  decentralized multi-agent settings"
234,1405709527664652288,1185977761032110080,Kazumasa Ohno (Â§ßÈáé ÂíåÊ≠£),"['New paper is out! <LINK>\nWe discuss about Jupiter formation from the planet‚Äôs atmospheric composition. We propose that proto-Jupiter may be formed in the cold ‚Äúshadow‚Äù of protosolar disk to explain enriched nitrogen and noble gases in the atmosphere.', 'The abundances of many heavy elements are ~3 solar in Jovian atmosphere. Super-solar abundances can come from the dissolution of solids (e.g., pebbles); however, it is puzzling why hyper volatiles (N and noble gases), hardly locked into solids, are enriched with the same degree.', 'Recent studies suggested that the enriched hyper-volatiles indicate that proto-Jupiter was formed at cold &gt;30AU region where hyper volatiles can freeze. However, this idea has a disadvantage that contradicts to planet migration theory and isotope dichotomy of meteorites.', ""So, we propose an alternative idea. The current Jupiter orbit may be shadowed in the past. Due to the lack of direct sunlight, the shadow may create extremely cold environments even near the current Jupiter's orbit, leading to cause the freezing of hyper-volatiles."", 'We consider the disk in which the dust is accumulated inside the H2O snow line. Such structure can originate from the different fragmentation threshold velocity of silicate and icy grains. The accumulated dust may cast the shadow behind the snow line, near the Jupiter orbit.', 'The shadowing effect is drastic. When the degree of dust pileup at H2O snow line is high enough (f_SL&lt;0.03), even 5AU region can be as cold as &lt;30K! I thank Takahiro, coauthor and my former colleague, for his radiative transfer calculations. https://t.co/zw4JS7gUp0', 'I then performed the condensation calculations and found that relevant hyper-volatiles (N and noble gases) can freeze around the current Jupiter orbit. Such volatile distributions are considerably different from those in non-shadowed disks which we usually assume. https://t.co/ThJ5fBu7k7', ""We obtained excellent matches between observed and model predicted elemental abundances, even for in-situ formation! Interestingly, in our shadow scenario, N and noble gases may be depleted in Saturnian atmosphere, as the Saturn's orbit is close to the outer edge of the shadow. https://t.co/TRZzqHQI3X"", 'To sum, the enriched N and noble gases may imply that proto-Jupiter was formed in the shadowed region. The scenario can be compatible with other constrains. The current paper did not explicitly simulate the Jupiter formation, which should be done in future studies.', 'Although the paper mainly focuses on Solar System giants, the shadowing effect would also affect how to interpret the elemental abundance ratios of exoplanetary atmospheres, which will be delivered by upcoming JWST. https://t.co/6Qqrw33xTe', 'Btw, this is my first paper that does not use the word of ""cloud"" and ""haze""! Doing the research related to planet formation was one of my dream.\nI came up the idea of this paper about an year ago, and happy that I can finally share it as a paper.']",https://arxiv.org/abs/2106.09084,"Atmospheric compositions offer valuable clues to planetary formation and evolution. Jupiter has been the most well-studied giant planet in terms of its atmosphere; however, the origin of the Jovian atmospheric composition remains a puzzle as the abundances of nitrogen and noble gases as high as those of other elements could only originate from extremely cold environments. We propose a novel idea for explaining the Jovian atmospheric composition: Dust pileup at the H$_2$O snow line casts a shadow and cools the Jupiter orbit so that N$_2$ and noble gases can freeze. Planetesimals or a core formed in the shadowed region can enrich nitrogen and noble gases as much as other elements through their dissolution in the envelope. We compute the temperature structure of a shadowed protosolar disk with radiative transfer calculations. Then, we investigate the radial volatile distributions and predict the atmospheric composition of Jupiter with condensation calculations. We find that the vicinity of the current Jupiter orbit, approximately $3$--$7~{\rm AU}$, could be as cold as $30~{\rm K}$ if the small-dust surface density varies by a factor of $\gtrsim30$ across the H$_2$O snow line. According to previous grain growth simulations, this condition could be achieved by weak disk turbulence if silicate grains are more fragile than icy grains. The shadow can cause the condensation of most volatile substances, namely N$_2$ and Ar. We demonstrate that the dissolution of shadowed solids can explain the elemental abundance patterns of the Jovian atmosphere even if proto-Jupiter was formed near Jupiter's current orbit. The disk shadow may play a vital role in controlling atmospheric compositions. The effect of the shadow also impacts the interpretation of upcoming observations of exoplanetary atmospheres by JWST. ","Jupiter's ""Cold"" Formation in the Protosolar Disk Shadow: An Explanation
  for the Planet's Uniformly Enriched Atmosphere"
235,1405565545693560840,1938536035,Huy V. Vo,"['Our new paper ""Large-Scale Unsupervised Object Discovery"" is on arxiv: <LINK>. We propose to use ranking methods for object discovery and show that our approach scales better than the baselines while yielding state-of-the-art results on COCO ... <LINK>', '... and the large OpenImages dataset with 1.7M images.']",https://arxiv.org/abs/2106.06650,"Existing approaches to unsupervised object discovery (UOD) do not scale up to large datasets without approximations that compromise their performance. We propose a novel formulation of UOD as a ranking problem, amenable to the arsenal of distributed methods available for eigenvalue problems and link analysis. Through the use of self-supervised features, we also demonstrate the first effective fully unsupervised pipeline for UOD. Extensive experiments on COCO and OpenImages show that, in the single-object discovery setting where a single prominent object is sought in each image, the proposed LOD (Large-scale Object Discovery) approach is on par with, or better than the state of the art for medium-scale datasets (up to 120K images), and over 37% better than the only other algorithms capable of scaling up to 1.7M images. In the multi-object discovery setting where multiple objects are sought in each image, the proposed LOD is over 14% better in average precision (AP) than all other methods for datasets ranging from 20K to 1.7M images. Using self-supervised features, we also show that the proposed method obtains state-of-the-art UOD performance on OpenImages. Our code is publicly available at this https URL ",Large-Scale Unsupervised Object Discovery
236,1405230885901897731,57284479,Matthias Minderer,"['New paper: Revisiting the Calibration of Modern Neural Networks (<LINK>). We studied the calibration of MLP-Mixer, Vision Transformers, BiT, and many others. Non-convolutional models are doing surprisingly well! 1/5 <LINK>', 'Lots of analyses in the paper. Some highlights: In-distribution calibration slightly deteriorates with increasing model size, but this is outweighed by a simultaneous improvement in accuracy. 2/5', 'Under distribution shift, calibration improves with model size. This reverses the trend for in-distribution data (!). Accuracy and calibration are correlated under distribution shift: optimizing for accuracy may also benefit calibration. 3/5', 'Model size, pretraining duration, and pretraining dataset size cannot fully explain differences in calibration properties. This suggests that architecture is a major determinant of calibration properties. 4/5', 'Work with Josip Djolonga, @robromijnders, Frances Hubis, @XiaohuaZhai, @neilhoulsby, @dustinvtran, and @MarioLucic_ 5/5', '@KAlexanderWang Additional differences between the families in the bottom left emerge after temperature scaling, so batch norm is likely not the whole story, but it could definitely play a role.']",https://arxiv.org/abs/2106.07998,"Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties. ",Revisiting the Calibration of Modern Neural Networks
237,1405187538675982343,151193108,Mert R. Sabuncu,"['Modern deep learning models are often over-parameterized. We propose to split them up into an ensemble of sub-networks, which can boost performance. Pre-print - Ex uno plures: Splitting One Model into an Ensemble of Subnetworks <LINK>']",https://arxiv.org/abs/2106.04767,"Monte Carlo (MC) dropout is a simple and efficient ensembling method that can improve the accuracy and confidence calibration of high-capacity deep neural network models. However, MC dropout is not as effective as more compute-intensive methods such as deep ensembles. This performance gap can be attributed to the relatively poor quality of individual models in the MC dropout ensemble and their lack of diversity. These issues can in turn be traced back to the coupled training and substantial parameter sharing of the dropout models. Motivated by this perspective, we propose a strategy to compute an ensemble of subnetworks, each corresponding to a non-overlapping dropout mask computed via a pruning strategy and trained independently. We show that the proposed subnetwork ensembling method can perform as well as standard deep ensembles in both accuracy and uncertainty estimates, yet with a computational efficiency similar to MC dropout. Lastly, using several computer vision datasets like CIFAR10/100, CUB200, and Tiny-Imagenet, we experimentally demonstrate that subnetwork ensembling also consistently outperforms recently proposed approaches that efficiently ensemble neural networks. ",Ex uno plures: Splitting One Model into an Ensemble of Subnetworks
238,1405154772920016900,1217628182611927040,Boaz Barak,"['1/7 Do neural nets with different architecture, training objectives, and data, learn similar representations? \nWith @whybansal and @PreetumNakkiran we use ""stitching"" (Lenc-Vedaldi 2015) to study this question.\n\n<LINK>', '2/7 There are two competing ""stories"" about deep net training:\n\n1) Every net learns to meet its objective in its own unique way\n\n2) Like Tolstoy\'s ""happy families"" all good classifiers end up learning more or less the same representations. https://t.co/uuGDgOMrEL', ""3/7 Several works (eg @ch402's) point to 2nd scenario, but hard to quantify*\nWe test two predictions:\n\n(A) Representations learned through different means (e.g. self-supervised vs supervised) should be compatible\n(B) Representations learned with more data/compute should be better"", '4/7 We call (A) ""All roads lead to Rome"". \n\nFor example, we can ""stitch together""** bottom k layers of depth-d network trained with self-supervised methods with the top (d-k) layers of supervised-trained network with same architecture. Get minor accuracy drop for all k. https://t.co/iN9EKj70cR', '5/7 We call (B) ""More is better"". We show that when replacing bottom layers of net N with layers from N\' that was trained using different data, epochs, or width, then performance changes accordingly. \n\nIn particular if N\' has sufficiently ""more"" of resource, then perf improves! https://t.co/GuDJrxUWJu', '6/7 *Stitching not only approach to study this question - can use measures such as CKA https://t.co/m0SzBbinq2 but stitching has some advantages. Can identify similarity even in cases where representations are ""far"" by CKA, and also say when one rep is ""better"" than another. https://t.co/e2Limspu29', '7/7 ** By ""stitching together"" we (following Lenc-Vedaldi) mean combining two parts using a trainable layer. But this is not full ""fine tuning"" since the layer is simple (low capacity) and so cannot by itself be used to fit the data. See paper for more details. https://t.co/iw1n0SyvPN', '@AndrewLampinen @whybansal @PreetumNakkiran @khermann_ This is interesting! Of course not just similarity measures but also the models themselves have (as you say) a bias toward simple features. This was also shown in https://t.co/lSczbG4osd by @harshays_, @konsangbgohain , Aditi Raghunathan, @jainprateek_, &amp; @PNetrapalli']",https://arxiv.org/abs/2106.07682,"We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models $A$ and $B$, we consider a ""stitched model'' formed by connecting the bottom-layers of $A$ to the top-layers of $B$, with a simple trainable layer between them. We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative verifications for intuitive statements such as ""good networks learn similar representations'', by demonstrating that good networks of the same architecture, but trained in very different ways (e.g.: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that ""more is better'' by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be ""plugged in'' to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call ""stitching connectivity'', akin to mode-connectivity: typical minima reached by SGD can all be stitched to each other with minimal change in accuracy. ",Revisiting Model Stitching to Compare Neural Representations
239,1405048984171859970,774302179494625280,Jos√© L. Jim√©nez,"['New preprint! @haiyan_zheng and I propose a Bayesian phase I-II design that allows sharing of efficacy data across stages when, in each stage, we have a different patient population.\n\n<LINK>']",https://arxiv.org/abs/2106.08277,"Integrated phase I-II clinical trial designs are efficient approaches to accelerate drug development. In cases where efficacy cannot be ascertained in a short period of time, two-stage approaches are usually employed. When different patient populations are involved across stages, it is worth of discussion about the use of efficacy data collected from both stages. In this paper, we focus on a two-stage design that aims to estimate safe dose combinations with a certain level of efficacy. In stage I, conditional escalation with overdose control (EWOC) is used to allocate successive cohorts of patients. The maximum tolerated dose (MTD) curve is estimated based on a Bayesian dose-toxicity model. In stage II, we consider an adaptive allocation of patients to drug combinations that have a high probability of being efficacious along the obtained MTD curve. A robust Bayesian hierarchical model is proposed to allow sharing of information on the efficacy parameters across stages assuming the related parameters are either exchangeable or nonexchangeable. Under the assumption of exchangeability, a random-effects distribution is specified for the main effects parameters to capture uncertainty about the between-stage differences. The proposed methodology is assessed with extensive simulations motivated by a real phase I-II drug combination trial using continuous doses. ","A Bayesian adaptive design for dual-agent phase I-II cancer clinical
  trials combining efficacy data across stages"
240,1405010804290826240,1212606587644178432,Romero-Isart Group,['In <LINK> we quantize the electromagnetic field in the presence of a nonmoving dielectric sphere and analytically study light scattering of quantum states of light (e.g. two-photon states leading to Hong-Ou-Mandel interference). @iqoqi @uniinnsbruck @CarlosGBall <LINK>'],https://arxiv.org/abs/2106.07975,"We quantize the electromagnetic field in the presence of a nonmoving dielectric sphere in vacuum. The sphere is assumed to be lossless, dispersionless, isotropic, and homogeneous. The quantization is performed using normalized eigenmodes as well as plane-wave modes. We specify two useful alternative bases of normalized eigenmodes: spherical eigenmodes and scattering eigenmodes. A canonical transformation between plane-wave modes and normalized eigenmodes is derived. This formalism is employed to study the scattering of a single photon, coherent squeezed light, and two-photon states off a dielectric sphere. In the latter case we calculate the second-order correlation function of the scattered field, thereby unveiling the angular distribution of the Hong-Ou-Mandel interference for a dielectric sphere acting as a three-dimensional beam splitter. Our results are analytically derived for an arbitrary size of the dielectric sphere with a particular emphasis on the small-particle limit. This work sets the theoretical foundation for describing the quantum interaction between light and the motional, rotational and vibrational degrees of freedom of a dielectric sphere. ","Quantum Electrodynamics with a Nonmoving Dielectric Sphere: Quantizing
  Lorenz-Mie Scattering"
241,1405002518367772677,538186192,Ramanakumar Sankar,['My new paper is out on arxiv: <LINK> ! We did some simulations of the 24deg N jet on Jupiter to study the convective plumes and associated cloud features that have been observed.'],https://arxiv.org/abs/2106.07809,"The $24^{\circ}$ N jet borders the North Tropical Belt and North Tropical Zone, and is the fastest prograde jet on Jupiter, reaching speeds above $170$ m/s. In this region, observations have shown several periodic convective plumes, likely from latent heat release from water condensation, which affect the cloud and zonal wind structure of the jet. We model this region with the Explicit Planetary hybrid-Isentropic Coordinate model using its active microphysics scheme to study the phenomenology of water and ammonia clouds within the jet region. On perturbing the atmosphere, we find that an upper tropospheric wave develops that directly influences the cloud structure within the jet. This wave travels at $\sim75$ m/s in our model, and leads to periodic chevron-shaped features in the ammonia cloud deck. These features travel with the wave speed, and are subsequently much slower than the zonal wind at the cloud deck. The cloud structure, and the slower drift rate, were both observed following the convective outbreak in this region in 2016 and 2020. We find that an upper level circulation is responsible for these cloud features in the aftermath of the convective outbursts. The comparatively slower observed drift rates of these features, relative to the wind speed of the jet, provides constraints on the vertical wind shear above the cloud tops, and we suggest that wind velocities determined from cloud tracking should correspond to a different altitude compared to the $680$ hPa pressure level. We also diagnose the convective potential of the atmosphere due to water condensation, and find that it is strongly coupled to the wave. ","The aftermath of convective events near Jupiter's fastest prograde jet:
  implications for clouds, dynamics and vertical wind shear"
242,1404777559171153923,1041794770174205955,Rabeeh Karimi Mahabadi üíôüíõ,"['I am happy to present my work with Google Brain and DeepMind at #ACL2021: <LINK>\n\nWe propose HyperFormer, a parameter-efficient multi-task framework that outperforms single and multi-task T5 on GLUE while adding only 0.29% per task. <LINK>', 'HyperFormer learns adapter parameters for all layers/tasks generated with shared hypernetworks. These are conditioned on task/adapter position/layer id. This enables sharing knowledge across tasks (via hypernetworks) and adapting the model to individual tasks (through adapters).', 'HyperFormer is composed of a) Task Conditional Adapter Layers and b) Task Conditional Layer Normalization that generate adapters and layer normalization parameters respectively by conditioning on task embeddings.', 'Experiments on the well-known GLUE benchmark show that HyperFormer outperforms single-task and multi-task T5  while adding only 0.29% parameters per task. https://t.co/EldWYlbvo0', 'HyperFormer substantially improves results with limited training data, indicating more effective fine-tuning in this regime. https://t.co/jDnTpjligJ', 'HyperFormer obtains significant improvement in few-shot domain generalization across a variety of tasks. https://t.co/ts60e61sGH', 'Visualizing the task embeddings shows that the learned task representations are meaningful and that similar tasks are grouped together. https://t.co/QgKNqiFzDS', 'We will release the code soon. Stay tuned.', 'We will be presenting our work at ACL as an oral presentation (paper ID 2244).\nThis is joint work with @seb_ruder, @m__dehghani, @JamieBHenderson. Really enjoyed working with you all! and I am grateful to have the opportunity to work with these amazing researchers.', 'work done during my internship with  Google Brain.']",https://arxiv.org/abs/2106.04489,"State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in this https URL ","Parameter-efficient Multi-task Fine-tuning for Transformers via Shared
  Hypernetworks"
243,1404774464441794560,1041794770174205955,Rabeeh Karimi Mahabadi üíôüíõ,"[""Excited to share our new work with DeepMind: <LINK>\n \nOur main contribution:\nWe propose Compacter (Compact Adapter) layers, a method to adapt large-scale language models, which only trains around 0.05% of a model's parameters and performs on par with fine-tuning. <LINK>"", 'Compacter accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers https://t.co/ZOoj0eSiYF, a recent Google AI paper by @ytay017 et al.', 'Compacter generates weights of adapter layers (W1, W2) by the summation of Kronecker products of shared matrices A_i and adapter-specific matrices B_j, where each B_j is generated by multiplying independent rank one weights. https://t.co/lp6y65zOHV', 'Compacter and Compacter++ outperform all previous parameter-efficient methods and perform on par with full fine-tuning while only training 0.07% and 0.047% of parameters respectively https://t.co/7E664wqVrW', 'We compared multiple recently proposed light-weight methods for finetuning large-scale language models and show that compactor obtains an excellent trade-off between the number of trainable parameters, task performance, and memory footprint, compared to existing methods https://t.co/IIXDGkYSeX', 'We subsample each dataset of GLUE for varying sizes and show that Compacter substantially improves the results in the low-resource setting, indicating more effective fine-tuning in this regime. https://t.co/B1a1jsJ3Sa', 'A huge shout out to the excellent collaborator @seb_ruder  and my amazing advisor  @JamieBHenderson that made this paper possible.']",https://arxiv.org/abs/2106.04647,"Adapting large-scale pretrained language models to downstream tasks via fine-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, fine-tuning all weights of models with millions or billions of parameters is sample-inefficient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efficient fine-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard fine-tuning. In this work, we propose Compacter, a method for fine-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. Compacter accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Specifically, Compacter inserts task-specific weight matrices into a pretrained model's weights, which are computed efficiently as a sum of Kronecker products between shared ""slow"" weights and ""fast"" rank-one matrices defined per Compacter layer. By only training 0.047% of a pretrained model's parameters, Compacter performs on par with standard fine-tuning on GLUE and outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our code is publicly available at~\url{this https URL}. ",Compacter: Efficient Low-Rank Hypercomplex Adapter Layers
244,1404652964564791300,2915749124,Dhiraj Hazra,"['We propose an inflationary primordial feature model that can explain both the large and small-scale anomalies in the currently observed CMB data (<LINK>). Can we detect these anomalies with future observations ? With @xingangchen01 and Matteo Braglia. <LINK>', 'Can we detect these anomalies with future observations ? Using the specifications of future space based   #LiteBIRD and ground based @SimonsObs  we address this question. A #PICO or #CMBBHARAT type near-ultimate observation will be able to provide a decisive evidence.']",https://arxiv.org/abs/2106.07546,"We propose an inflationary primordial feature model that can explain both the large and small-scale anomalies in the currently measured cosmic microwave background (CMB) anisotropy spectra, revealing a clip of adventurous history of the Universe during its primordial epoch. Although the model is currently statistically indistinguishable from the Standard Model, we show that future observations such as the Simons Observatory and LiteBIRD will complement each other in distinguishing the model differences due to their accurate E-mode polarization measurements, and the PICO mission, if funded, can put stringent constraints on all characteristic properties. The model predicts a signal of classical primordial standard clock, which can also be used to distinguish the inflation and alternative scenarios in a model-independent fashion. ","Uncovering the History of Cosmic Inflation from Anomalies in Cosmic
  Microwave Background Spectra"
245,1404642428770914308,2850858010,Subbarao Kambhampati (‡∞ï‡∞Ç‡∞≠‡∞Ç‡∞™‡∞æ‡∞ü‡∞ø ‡∞∏‡±Å‡∞¨‡±ç‡∞¨‡∞æ‡∞∞‡∞æ‡∞µ‡±Å),"['A fun project where we studied how #GPT3 compares to custom SOTA approaches for the task of extracting plans from text--a task with applications in many industries with custom workflows written for human consumption. (With @_aolmo_ and @sarath_ssreedh)\n\n<LINK> <LINK>', '@cjmuise @_aolmo_ @sarath_ssreedh Oh no! It scooped our entire paper üò± (..which, tbf, was produced by #GPT3 after taking the title as the prompt..)', ""@cjmuise @_aolmo_ @sarath_ssreedh What's the point  of this half-a**ed approach?  Go for end-to-end automation which writes those papers, puts them on  arXiv, updates your CV, and writes your performance evaluation! üôÑ\n\n[Never again should @ICAPSConference  complain about them not getting enough submissions!]""]",https://arxiv.org/abs/2106.07131,"Operations in many essential industries including finance and banking are often characterized by the need to perform repetitive sequential tasks. Despite their criticality to the business, workflows are rarely fully automated or even formally specified, though there may exist a number of natural language documents describing these procedures for the employees of the company. Plan extraction methods provide us with the possibility of extracting structure plans from such natural language descriptions of the plans/workflows, which could then be leveraged by an automated system. In this paper, we investigate the utility of generalized language models in performing such extractions directly from such texts. Such models have already been shown to be quite effective in multiple translation tasks, and our initial results seem to point to their effectiveness also in the context of plan extractions. Particularly, we show that GPT-3 is able to generate plan extraction results that are comparable to many of the current state of the art plan extraction methods. ",GPT3-to-plan: Extracting plans from text using GPT-3
246,1404624618695049218,1148625407073247233,Haipeng Chen,"['Glad to share our recent UAI-2021 paper @UncertaintyInAI , titled ""Contingency-Aware Influence Maximization: A Reinforcement Learning Approach."" We propose to address the contingency-aware influence maximization (CAIM) problem from a RL perspective. <LINK>', ""CAIM is motivated by a series of studies in our group with @MilindTambe_AI, on HIV prevention among homeless youth social networks in the city of LA. Different from normal IM problems, there is uncertainty in a node's willingness to be seeds. (image credit: Wilder et al.) https://t.co/5VIzEGKb5r"", ""Despite recent success in the field using POMDPs or greedy algorithms (Yadav et al. 2016, Wilder et al. 2018, 2021), a main limitation of scaling their methods to more homeless youth shelters is the large run time, a big burden to NGOs who don't have HPC resources."", ""Inspired by recent work that uses RL to address combinatorial optimization problems, we extend it to the CAIM problem. We show the extension is computationally hard due to the uncertainty of a node's willingness, and propose a new reward shaping technique."", 'The reward shaping component is proven to be computationally efficient both in theory (via the submodularity property of the influence cascade models) and in empirical evaluation (via an ablation study).', 'Main takeaway -- our algorithm RL4IM achieves comparable influence to SOTA algorithm for CAIM, while requiring negligible runtime during test phase, which can be directly used by NGOs - a result making RL4IM an ideal alternative for CAIM in the low-resource computing paradigm.', 'Our work is an example of how RL can be used for social good work, and we hope to encourage more AI researchers in exploring the idea here to assist AI for social good in the low-resource computing domain.', 'Join work with @WeiQiu9, @HanChingOu, Bo An, and @MilindTambe_AI', 'Recent works on HIV prevention among homeless youth networks using influence maximization, with success in field test, by @brwilder @AmulyaYadav19 @MilindTambe_AI  @EricRicePhD', 'Work that combines graph representation learning and reinforcement learning to address combinatorial optimization problems, by @lyeskhalil et al. 2017']",https://arxiv.org/abs/2106.07039,"The influence maximization (IM) problem aims at finding a subset of seed nodes in a social network that maximize the spread of influence. In this study, we focus on a sub-class of IM problems, where whether the nodes are willing to be the seeds when being invited is uncertain, called contingency-aware IM. Such contingency aware IM is critical for applications for non-profit organizations in low resource communities (e.g., spreading awareness of disease prevention). Despite the initial success, a major practical obstacle in promoting the solutions to more communities is the tremendous runtime of the greedy algorithms and the lack of high performance computing (HPC) for the non-profits in the field -- whenever there is a new social network, the non-profits usually do not have the HPCs to recalculate the solutions. Motivated by this and inspired by the line of works that use reinforcement learning (RL) to address combinatorial optimization on graphs, we formalize the problem as a Markov Decision Process (MDP), and use RL to learn an IM policy over historically seen networks, and generalize to unseen networks with negligible runtime at test phase. To fully exploit the properties of our targeted problem, we propose two technical innovations that improve the existing methods, including state-abstraction and theoretically grounded reward shaping. Empirical results show that our method achieves influence as high as the state-of-the-art methods for contingency-aware IM, while having negligible runtime at test phase. ","Contingency-Aware Influence Maximization: A Reinforcement Learning
  Approach"
247,1404573351289425923,2548806176,Jonathan Freundlich,"['In our recent work with @FangzhouJiang, we  propose a semi-analytical model where the combination of dynamical friction heating and feedback-induced outflows enables to form cores in dark matter haloes very efficiently. <LINK> <LINK>', '1/ Incoming satellites heat up the dark matter by dynamical friction, the process being described by the ""SatGen"" analytical model developed by @FangzhouJiang. The dark matter halo can expand during a first relaxation phase following the heating.', '2/ A sudden gas outflow induced by feedback removes part of the gravitational support. The halo expands as it relaxes to its new equilibrium. This phase is described by the ""CuspCore"" analytical model I contributed to develop.', '@FangzhouJiang 3/ This process could explain the low inner dark matter fractions observed by Genzel et al. (2020) at z~2, i.e., ~3 Gyr after the Big Bang.', '@maximetrebitsch @FangzhouJiang The virial time is tv~0.5 Gyr at z~2 and a cosmological merger sequence during 0.4 tv is more than sufficient to form a significant core after half of the gas is expelled (cf. Fig. 10). The question may be whether AGN feedback would be sufficient for that?', '@maximetrebitsch @FangzhouJiang In these two cases for example, we consider cosmological merger sequences during 0.2 and 0.4 tv and half of the gas then being expelled (eta=0.5) -- in the initial halo, the gas account for half of the baryonic mass. It is still just a toy model though! :-) https://t.co/MyiUvKTzah', '@maximetrebitsch @FangzhouJiang PS: good luck with the CNAP!!! üí™', '@maximetrebitsch @FangzhouJiang Within the model, considering a longer merging sequence (&gt;0.4 tv, i.e., &gt;0.2 Gyr) and expelling less gas (eta=dM/Mgas&lt;0.5) may also do the trick...']",https://arxiv.org/abs/2106.01378,"Observed rotation curves in star-forming galaxies indicate a puzzling dearth of dark matter in extended flat cores within haloes of mass $\geq\! 10^{12}M_\odot$ at $z\!\sim\! 2$. This is not reproduced by current cosmological simulations, and supernova-driven outflows are not effective in such massive haloes. We address a hybrid scenario where post-compaction merging satellites heat up the dark-matter cusps by dynamical friction, allowing AGN-driven outflows to generate cores. Using analytic and semi-analytic models (SatGen), we estimate the dynamical-friction heating as a function of satellite compactness for a cosmological sequence of mergers. Cosmological simulations (VELA) demonstrate that satellites of initial virial masses $>\!10^{11.3}M_\odot$, that undergo wet compactions, become sufficiently compact for significant heating. Constituting a major fraction of the accretion onto haloes $\geq\!10^{12}M_\odot$, these satellites heat-up the cusps in half a virial time at $z\!\sim\! 2$. Using a model for outflow-driven core formation (CuspCore), we demonstrate that the heated dark-matter cusps develop extended cores in response to removal of half the gas mass, while the more compact stellar systems remain intact. The mergers keep the dark matter hot, while the gas supply, fresh and recycled, is sufficient for the AGN outflows. AGN indeed become effective in haloes $\geq\!10^{12}M_\odot$, where the black-hole growth is no longer suppressed by supernovae and its compaction-driven rapid growth is maintained by a hot CGM. For simulations to reproduce the dynamical-friction effects, they should resolve the compaction of the massive satellites and avoid artificial tidal disruption. AGN feedback could be boosted by clumpy black-hole accretion and clumpy response to AGN. ","Core Formation in High-z Massive Haloes: Heating by Post Compaction
  Satellites and Response to AGN Outflows"
248,1404543388486094853,147615935,Niko Grupen,"['üö®New preprintüö®\n\nDoes mutual reward yield fair outcomes for cooperative teams? We find this is not the case! Teams learn capitalistic strategies, achieving high reward by distributing it *unequally* across teammates.\n\nPaper: <LINK>\n\n#AI #FairAI #RL', 'We connect prediction-based fairness to multi-agent learning and introduce Fairness through Equivariance (Fair-E) -- a method that ensures fair outcomes for multi-agent teams through equivariant policy learning.', 'We also introduce Fairness through Equivariance Regularization (Fair-ER) as a soft-constraint version of equivariant policy learning and show that it allows us to modulate between fairness and utility. https://t.co/MpyBho8v8m']",https://arxiv.org/abs/2106.05727,"We study fairness through the lens of cooperative multi-agent learning. Our work is motivated by empirical evidence that naive maximization of team reward yields unfair outcomes for individual team members. To address fairness in multi-agent contexts, we introduce team fairness, a group-based fairness measure for multi-agent learning. We then prove that it is possible to enforce team fairness during policy optimization by transforming the team's joint policy into an equivariant map. We refer to our multi-agent learning strategy as Fairness through Equivariance (Fair-E) and demonstrate its effectiveness empirically. We then introduce Fairness through Equivariance Regularization (Fair-ER) as a soft-constraint version of Fair-E and show that it reaches higher levels of utility than Fair-E and fairer outcomes than non-equivariant policies. Finally, we present novel findings regarding the fairness-utility trade-off in multi-agent settings; showing that the magnitude of the trade-off is dependent on agent skill. ",Cooperative Multi-Agent Fairness and Equivariant Policies
249,1404411795855585280,896099152974761985,Jin Xu,"['We propose group equivariant subsampling/upsampling layers. One can construct exact equivariant models by simply using them with (G-)Conv layers!\n \nüóûÔ∏è <LINK>\nüíª <LINK>\n \nw/ @hyunjik11 @tom_rainforth @yeewhye\n \nA thread üëá <LINK>', 'One application is group equivariant autoencoders (GAEs), which are composed of alternating (G-)Conv layers and equivariant subsampling/upsampling layers. The learned representations are naturally disentangled into an invariant part and an equivariant part. https://t.co/Fu33Tx925m', 'It is known that standard subsampling operations such as strided convolutions or (max-) pooling are not translation equivariant. Therefore, they break equivariance in standard CNNs/G-CNNs despite (G-)Conv being equivariant. https://t.co/hGiatn2oI8', 'We propose subsampling/upsampling operations that are exactly group equivariant. In standard subsampling, the user typically specifies a scale factor. In group equivariant subsampling, the user instead specifies a subgroup.', 'e.g. the encoders in GAEs are composed of alternating G-Conv and equivariant subsampling layers, where each subsampling layer S reduces the feature map to functions on a smaller subgroup. Using eqv. subsampling/upsampling, GAEs can learn exact equivariant low-dim representations!', 'We show that simply replacing standard subsampling/upsampling layers in autoencoders with our group equivariant ones gives better generalisation and data efficiency for images of objects with varying locations and orientations. https://t.co/earWlK5WeN', '@pamattei Thank you! Yes, you‚Äôre right. We have some preliminary results on this but did not include them in the paper in the end to keep things simple.']",https://arxiv.org/abs/2106.05886,"Subsampling is used in convolutional neural networks (CNNs) in the form of pooling or strided convolutions, to reduce the spatial dimensions of feature maps and to allow the receptive fields to grow exponentially with depth. However, it is known that such subsampling operations are not translation equivariant, unlike convolutions that are translation equivariant. Here, we first introduce translation equivariant subsampling/upsampling layers that can be used to construct exact translation equivariant CNNs. We then generalise these layers beyond translations to general groups, thus proposing group equivariant subsampling/upsampling. We use these layers to construct group equivariant autoencoders (GAEs) that allow us to learn low-dimensional equivariant representations. We empirically verify on images that the representations are indeed equivariant to input translations and rotations, and thus generalise well to unseen positions and orientations. We further use GAEs in models that learn object-centric representations on multi-object datasets, and show improved data efficiency and decomposition compared to non-equivariant baselines. ",Group Equivariant Subsampling
250,1404363953254416386,966686196,Mark Mitchison,"['Pleased to announce a nice collab with colleagues at @unicomplutense. We find symmetry-protected boundary currents in a non-equilibrium lattice system without band topology: consequence of Berry curvature + weak coupling. Had fun messing with quiver plots!\n<LINK> <LINK>', 'The plot shows bosons (red) and fermions (blue) flowing through a system. Currents are localised on boundary. Bosons react to impurities in the middle (black circles) by ""shielding"" them with counter-current. Fermions in topological phase don\'t react at all (depending on filling)']",https://arxiv.org/abs/2106.05988,"We study two-dimensional bosonic and fermionic lattice systems under nonequilibrium conditions corresponding to a sharp gradient of temperature imposed by two thermal baths. In particular, we consider a lattice model with broken time-reversal symmetry that exhibits both topologically trivial and nontrivial phases. Using a nonperturbative Green function approach, we characterize the nonequilibrium current distribution in different parameter regimes. For both bosonic and fermionic systems, we find chiral edge currents that are robust against coupling to reservoirs and to the presence of defects on the boundary or in the bulk. This robustness not only originates from topological effects at zero temperature but, remarkably, also persists as a result of dissipative symmetries in regimes where band topology plays no role. Chirality of the edge currents implies that energy locally flows against the temperature gradient without any external work input. In the fermionic case, there is also a regime with topologically protected boundary currents, which nonetheless do not circulate around all system edges. ",Robust nonequilibrium edge currents with and without band topology
251,1403620917025120256,243651138,Nick Holliman,['Mixed Intelligence :: The Future of AI\nWe need to augment our intelligence not replace it.\n\nAutomating Visualization Quality Assessment: a Case Study in Higher Education\n<LINK>\n\nIs AI really that intelligent? <LINK> via @financialtimes'],https://arxiv.org/abs/2106.00077,"We present a case study in the use of machine+human mixed intelligence for visualization quality assessment, applying automated visualization quality metrics to support the human assessment of data visualizations produced as coursework by students taking higher education courses. A set of image informatics algorithms including edge congestion, visual saliency and colour analysis generate machine analysis of student visualizations. The insight from the image informatics outputs has proved helpful for the marker in assessing the work and is also provided to the students as part of a written report on their work. Student and external reviewer comments suggest that the addition of the image informatics outputs to the standard feedback document was a positive step. We review the ethical challenges of working with assessment data and of automating assessment processes. ","Automating Visualization Quality Assessment: a Case Study in Higher
  Education"
252,1403385826424377345,32911052,Valerie Bradley,"['Pre-print out today! We find that Big Data drastically over-estimates COVID-19 vaccine uptake among US adults, affecting estimates of vax hesitancy and willingness. Work with incredible collaborators @shirokuriwaki @MichaelIsakov3 @XiaoLiMeng1 @flaxter 1/\n<LINK>', 'We compare estimates of COVID vaccine uptake for US 18+ from @CmuDelphi / @Facebook COVID Symptom Survey (n=250,000/week), @uscensusbureau Household Pulse Survey (n=75,000/week), and @axios/ @IpsosMORI Coronavirus tracker (n=1000/week) to data from @CDCgov COVID Tracker‚Ä¶ 2/', '‚Ä¶and find drastic over-estimation of vaccine uptake by Delphi-Facebook (+17pp on April 24th), and the Census Household Pulse (+12pp on April 26), the two largest surveys. Large sample sizes mean that standard 95% CIs (plotted) don‚Äôt contain the CDC benchmark. 3/ https://t.co/FP4p5DlDOz', 'Measuring the national average uptake isn‚Äôt the main goal of any of these surveys, however we also find large discrepancies in 1) state-level rankings of vaccine uptake and 2) rate of increase in vaccination over time estimated by Delphi-Facebook and Census Household Pulse. 4/ https://t.co/svcu81DSrR', 'We use @XiaoLiMeng1‚Äôs framework for decomposing error (from Meng 2018 https://t.co/k7DUukUg3Q) and find that this error is largely driven by the data defect correlation (ddc), or selection bias caused by correlation between response and vaccination status. 5/ https://t.co/M8PvAUchVq', 'This is a striking example of the *Big Data Paradox* ‚Äì it is EXTREMELY hard to compensate for (even small) problems with data *quality* (selection bias) using high data *quantity*, especially in large populations (Meng‚Äôs Law of Large Populations).  6/', 'E.g. the effective sample size (size of an SRS that would have the same MSE) of Delphi-Facebook and Census Household Pulse is &gt;99% SMALLER than it appears, due to selection bias. 7/ https://t.co/Xl50226Ew6', ""Selection bias is likely driven by design, such as weighting schemes, sampling frames, survey, mode, etc. E.g. Delphi-Facebook does not weight on race/education, and Census recruits people for whom they have email/phone number. However, we can't conclude exact causes here. 8/"", 'We use ddc observed for vaccine *uptake* to present plausible scenarios for estimates of *hesitancy* and *willingness* by directly correcting the implied selection bias. 9/ https://t.co/CMs2orPBXY', 'In the scenario that selection bias is driven by issues of ACCESS (people who have trouble accessing surveys also have trouble accessing vaccines), then the ‚Äúwilling but unvaxxed‚Äù population could be 20%-25% of the US adult population, instead of the 6% raw estimates 10/', 'In the scenario that selection bias is driven by issues of HESITANT people also not responding to surveys, then the ‚Äúunvaxxed-and-hesitant‚Äù population could be 2x as large as raw estimates. 11/', 'Which of these is more plausible will need to be determined by auxiliary studies, however we hope that these ddc-based scenarios are helpful to policymakers with a better understanding of actual barriers people currently face to getting vaccinated. 12/', 'Some final thoughts: 1) standard CIs drastically underestimate true error in biased data (by much more than just 1/2!!)  2) careful sample design and adjustment / modeling is critical, even when data is ‚ÄúBig‚Äù 3) use benchmarks when you can! 13/', 'Shout out to @YouGov who has used the CDC vaccine data as a benchmark for improving their COVID tracker (https://t.co/jWTfjnPzJD) 14/', 'If you‚Äôre using Delphi-Facebook or Census Household Pulse estimates of vaccine behavior and attitudes, we‚Äôre happy to hear from you and discuss steps you might take to get the most accurate estimates you can.  15/', 'Big thank you to everyone from the 3 survey teams + CDC for all invaluable discussion and insight into your surveys!  16/', 'In sum: there are still lots of people in the US who want to get vaccinated; let‚Äôs work together to remove barriers! 17/17', ""@shirokuriwaki @MichaelIsakov3 @XiaoLiMeng1 @flaxter And of course @sejDino !!! Somehow didn't make the copy and paste üôà""]",https://arxiv.org/abs/2106.05818,"Surveys are a crucial tool for understanding public opinion and behavior, and their accuracy depends on maintaining statistical representativeness of their target populations by minimizing biases from all sources. Increasing data size shrinks confidence intervals but magnifies the impact of survey bias, an instance of the Big Data Paradox (Meng 2018). Here we demonstrate this paradox in estimates of first-dose COVID-19 vaccine uptake in US adults: Delphi-Facebook (about 250,000 responses per week) and Census Household Pulse (about 75,000 per week). By May 2021, Delphi-Facebook overestimated uptake by 17 percentage points and Census Household Pulse by 14, compared to a benchmark from the Centers for Disease Control and Prevention (CDC). Moreover, their large data sizes led to minuscule margins of error on the incorrect estimates. In contrast, an Axios-Ipsos online panel with about 1,000 responses following survey research best practices (AAPOR) provided reliable estimates and uncertainty. We decompose observed error using a recent analytic framework to explain the inaccuracy in the three surveys. We then analyze the implications for vaccine hesitancy and willingness. We show how a survey of 250,000 respondents can produce an estimate of the population mean that is no more accurate than an estimate from a simple random sample of size 10. Our central message is that data quality matters far more than data quantity, and compensating the former with the latter is a mathematically provable losing proposition. ","Unrepresentative Big Surveys Significantly Overestimate US Vaccine
  Uptake"
253,1403341402218524673,187221383,kourosh hakhamaneshi,"['How can we mitigate cold starts in BayesOpt using large offline datasets?\nIn JUMBO, we propose a new combination of neural nets and GPs for large-scale multi-task BayesOpt.\nüìñ <LINK>\nüíª <LINK>\nw/ @adityagrover_, @pabbeel, Vladimir Stojanoviƒá\n\n1/8 <LINK>', 'JUMBO is a no-regret algorithm that employs a careful hybrid of neural networks and Gaussian Processes for scalable and sample-efficient Multi-task BayesOpt. It scales cubically w.r.t. number of target queries but linearly w.r.t. the offline dataset size.\n\n2/8', 'First, we pre-train a NN using the offline data to learn a latent feature h(x). Warm-GP and cold-GP are then trained using target queries. For picking the next candidate query point, we consider upper confidence bounds of both warm and cold GPs.\n\n3/8 https://t.co/P1I48WGwb3', ""JUMBO's acquisition function is a convex combination of the UCB of individual GPs. Warm GP term prunes the search space. Cold GP term then selects the next candidate point to query. We extend the analysis in GP-UCB [Srinivas et al.] to prove that JUMBO is no-regret.\n\n4/8"", 'Empirically, we outperform prior SoTA MBO methods on benchmark hyper-parameter optimization problems.\n\n5/8 https://t.co/C5jLwyCRHd', 'We also applied JUMBO for optimizing the layout designs of circuits. We hope JUMBO is used by practitioners in many other science and engineering disciplines with offline domain data.\n\n6/8 https://t.co/Ls69MXQXqO', 'JUMBO builds on related advances in large-scale Multi-task BayesOpt such as Deep Kernel Learning (https://t.co/kQeWLWJ92u) and Adaptive Bayesian Linear Regression (https://t.co/O0tPzYxLrT).\n\n7/8', 'I really enjoyed working with amazing colleagues and advisors and I am looking forward to further collaborations with them: @adityagrover_, @pabbeel, and Vladimir Stojanoviƒá.\n\n8/8']",https://arxiv.org/abs/2106.00942,"The goal of Multi-task Bayesian Optimization (MBO) is to minimize the number of queries required to accurately optimize a target black-box function, given access to offline evaluations of other auxiliary functions. When offline datasets are large, the scalability of prior approaches comes at the expense of expressivity and inference quality. We propose JUMBO, an MBO algorithm that sidesteps these limitations by querying additional data based on a combination of acquisition signals derived from training two Gaussian Processes (GP): a cold-GP operating directly in the input domain and a warm-GP that operates in the feature space of a deep neural network pretrained using the offline data. Such a decomposition can dynamically control the reliability of information derived from the online and offline data and the use of pretrained neural networks permits scalability to large offline datasets. Theoretically, we derive regret bounds for JUMBO and show that it achieves no-regret under conditions analogous to GP-UCB (Srinivas et. al. 2010). Empirically, we demonstrate significant performance improvements over existing approaches on two real-world optimization problems: hyper-parameter optimization and automated circuit design. ",JUMBO: Scalable Multi-task Bayesian Optimization using Offline Data
254,1403330996041363459,1159548392839753729,Hugo Yeche,"[""Can contrastive learning help in applications where similar samples don't share downstream task labels? \n\nIn our #ICML2021 paper, we propose a simple framework for such problems and apply it to the monitoring of patients in the ICU \n\nPaper: <LINK> <LINK>"", 'Inspired by the similarity existing between contiguous states of a patient, we introduce a new contrastive objective. It allows a trade-off between preserving or discarding predefined attributes and this independently of data-augmentation. https://t.co/92hlkptBGM', 'We call samples sharing these chosen attributes ‚Äúneighbors‚Äù and define it as a binary function. It allows unifying prior works, unsupervised and supervised, under a common framework. https://t.co/supfRvc8ZR', 'This is my first work as a Ph.D. student at @CSatETH  and I‚Äôm very grateful to @gideoknite, our collaborators @FrancescoLocat8 and Matthias H√ºser, and my supervisor @gxr  for their help and guidance throughout this process!']",https://arxiv.org/abs/2106.05142,"Intensive care units (ICU) are increasingly looking towards machine learning for methods to provide online monitoring of critically ill patients. In machine learning, online monitoring is often formulated as a supervised learning problem. Recently, contrastive learning approaches have demonstrated promising improvements over competitive supervised benchmarks. These methods rely on well-understood data augmentation techniques developed for image data which do not apply to online monitoring. In this work, we overcome this limitation by supplementing time-series data augmentation techniques with a novel contrastive learning objective which we call neighborhood contrastive learning (NCL). Our objective explicitly groups together contiguous time segments from each patient while maintaining state-specific information. Our experiments demonstrate a marked improvement over existing work applying contrastive methods to medical time-series. ",Neighborhood Contrastive Learning Applied to Online Patient Monitoring
255,1403311939871285253,1089951038294106112,Wouter Van Gansbeke,"['We study how biases in the dataset affect contrastive pretraining and explore additional invariances\n\nWhat if we use non-curated data (COCO, OpenImages) vs ImageNet? Do we need priors to learn dense representations?\n\nPaper: <LINK>\nCode: <LINK>\nüëá <LINK>', '@svandenh1 @stam_g [1/4] We do not observe any indications that contrastive pretraining suffers from using scene-centric image data. This is in contrast to prior belief. Also, if the downstream data is non-object-centric, pretraining on COCO/OpenImages even outperforms ImageNet pretraining. https://t.co/koFhshxfRz', '[2/4] The augmentation strategy allows the model to learn spatially structured representations, even when training on scene-centric data. The representations can be used for semantic segment retrieval and video instance segmentation without any finetuning. https://t.co/ao2pNF7KTg', '[3/4] We discover semantic segments without any finetuning. https://t.co/eFXqfFQrmy', '[4/4] Additional invariances are explored, among which stronger augmentations and nearest neighbors. As expected they boost the transfer performance. https://t.co/viOzizkfUc', 'To conclude, recent contrastive approaches (e.g. MoCo) seem to be universally applicable. If you have enough data available, no priors are required to learn useful representations. Prior works argued that objec-centricness is necessary, euhm not really.']",http://arxiv.org/abs/2106.05967,"Contrastive self-supervised learning has outperformed supervised pretraining on many downstream tasks like segmentation and object detection. However, current methods are still primarily applied to curated datasets like ImageNet. In this paper, we first study how biases in the dataset affect existing methods. Our results show that current contrastive approaches work surprisingly well across: (i) object- versus scene-centric, (ii) uniform versus long-tailed and (iii) general versus domain-specific datasets. Second, given the generality of the approach, we try to realize further gains with minor modifications. We show that learning additional invariances -- through the use of multi-scale cropping, stronger augmentations and nearest neighbors -- improves the representations. Finally, we observe that MoCo learns spatially structured representations when trained with a multi-crop strategy. The representations can be used for semantic segment retrieval and video instance segmentation without finetuning. Moreover, the results are on par with specialized models. We hope this work will serve as a useful study for other researchers. The code and models are available at this https URL ","Revisiting Contrastive Methods for Unsupervised Learning of Visual
  Representations"
256,1403270265052844033,61881950,Limor Gultchin,"['How to utilize causal effects when targets of intervention are complex, e.g. images, text, genomics? We propose ùó∞ùóøùòÇùó±ùó≤ ùó∂ùóªùòÅùó≤ùóøùòÉùó≤ùóªùòÅùó∂ùóºùóªùòÄ + ùóΩùóøùóÆùó¥ùó∫ùóÆùòÅùó∂ùó∞ ùó∫ùó≤ùó±ùó∂ùóÆùòÅùó∂ùóºùóª. New at @icmlconf, w\\ @ds_wats0n, Matt Kusner and Ricardo Silva <LINK> <LINK>', '@icmlconf @ds_wats0n @stats_UCL @ai_ucl @CompSciOxford @CompSciOxford @turinginst @stats_UCL @ai_ucl']",https://arxiv.org/abs/2106.05074,"We examine the problem of causal response estimation for complex objects (e.g., text, images, genomics). In this setting, classical \emph{atomic} interventions are often not available (e.g., changes to characters, pixels, DNA base-pairs). Instead, we only have access to indirect or \emph{crude} interventions (e.g., enrolling in a writing program, modifying a scene, applying a gene therapy). In this work, we formalize this problem and provide an initial solution. Given a collection of candidate mediators, we propose (a) a two-step method for predicting the causal responses of crude interventions; and (b) a testing procedure to identify mediators of crude interventions. We demonstrate, on a range of simulated and real-world-inspired examples, that our approach allows us to efficiently estimate the effect of crude interventions with limited data from new treatment regimes. ",Operationalizing Complex Causes: A Pragmatic View of Mediation
257,1403025578043666432,1074633382452051969,Kimin,"['Can we learn policies using human feedback without pre-defined rewards efficiently?\n \nWe find unsupervised RL and off-policy learning can improve the preference-based RL in PEBBLE!\n \nüìëPaper: <LINK>\nüíªCode &amp; video: <LINK>\nw/ Laura Smith, @pabbeel <LINK>', 'PEBBLE improves sample- and feedback-efficiency of preference-based RL by applying the following ideas:\n\n1. Unsupervised pre-training to explore and collect diverse experiences\n2. Off-policy learning with re-labeling to mitigate the effects of a non-stationary reward function\n1/N https://t.co/VNVHwlaYLX', 'Experiments with a Human Teacher üë©\u200düè´\n \nWe demonstrate PEBBLE can learn behaviors for which a typical reward function is difficult to engineer very efficiently.\n \n2/N https://t.co/zLxyD8XBoc', 'Experiments with a Human Teacher üë®\u200düè´\n \nWe also show PEBBLE can avoid reward exploitation, leading to more desirable behaviors compared to an agent trained with respect to an engineered reward function.\n \n3/N https://t.co/0Dwp0Mnfmx', 'Experiments with a Simulated Teacher ü§ñ\n \nWe compare our approach to prior methods quantitatively using simulated teachers on locomotion and robotic manipulation tasks.\n \nPEBBLE can solve RL tasks more sample- and feedback-efficiently compared to the prior method.\n \n4/N https://t.co/U0Of4lc8iY', 'For more details, please check out\nour paper: https://t.co/4NqjsWl1SJ\nopen-source code: https://t.co/E3c86QY8UZ\nwebpage: https://t.co/cdzy6QZayx\n \nWe are also preparing benchmarking results with more interesting components. Please stay tuned!\n5/N', 'Personal note: it was really fun to revisit preference-based deep RL (led by Paul Christiano, @janleike, @ShaneLegg , Dario Amodei among many others). Hope to see more interesting results on preference-based RL (or human-in-the-loop RL in general)! ü§©\nI Believe it can scale up RL', 'Also, special thanks to amazing collaborators Laura and @pabbeel  üôè', ""A High-resolution and longer version of the video is available here: https://t.co/frSdG1cWVk \n\nThis one is motivated by @johnschulman2's nice overview video of GAE (https://t.co/9vKBpg58xr) üòÉ Reference from @pabbeel ü§£""]",https://arxiv.org/abs/2106.05091,"Conveying complex objectives to reinforcement learning (RL) agents can often be difficult, involving meticulous design of reward functions that are sufficiently informative yet easy enough to provide. Human-in-the-loop RL methods allow practitioners to instead interactively teach agents through tailored feedback; however, such approaches have been challenging to scale since human feedback is very expensive. In this work, we aim to make this process more sample- and feedback-efficient. We present an off-policy, interactive RL algorithm that capitalizes on the strengths of both feedback and off-policy learning. Specifically, we learn a reward model by actively querying a teacher's preferences between two clips of behavior and use it to train an agent. To enable off-policy learning, we relabel all the agent's past experience when its reward model changes. We additionally show that pre-training our agents with unsupervised exploration substantially increases the mileage of its queries. We demonstrate that our approach is capable of learning tasks of higher complexity than previously considered by human-in-the-loop methods, including a variety of locomotion and robotic manipulation skills. We also show that our method is able to utilize real-time human feedback to effectively prevent reward exploitation and learn new behaviors that are difficult to specify with standard reward functions. ","PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via
  Relabeling Experience and Unsupervised Pre-training"
258,1402776533387776000,2724167859,Stephen McAleer,"['We often choose to delegate our decisions to algorithms.  What should a central mediator do when multiple people choose to delegate their actions to the same mediator? In our recent paper we propose a mediator which Pareto-improves delegating agents.\n<LINK>', ""In the prisoner's dilemma, our Pareto Mediator creates a new game where both players delegating is a strong Nash equilibrium. When both players delegate, the mediator has them cooperate. https://t.co/nDnW3qmk7Y"", 'To see what this would look like on a restaurant-reservation platform, if delegating agents choose restaurants that are full, the mediator will simply reserve spots for them at open restaurants. https://t.co/RFV7JoO2iI', 'In sequential social dilemmas, the mediator will simply play a cooperative policy for both players if they choose to delegate. https://t.co/hsZjKyOTTp', ""Here's some more results on some random normal form games. https://t.co/2bhzFYnfGT"", 'Joint work with @JB_Lanier, @MichaelD1729, Pierre Baldi, and @roydfox']",https://arxiv.org/abs/2106.03927,"Machine learning algorithms often make decisions on behalf of agents with varied and sometimes conflicting interests. In domains where agents can choose to take their own action or delegate their action to a central mediator, an open question is how mediators should take actions on behalf of delegating agents. The main existing approach uses delegating agents to punish non-delegating agents in an attempt to get all agents to delegate, which tends to be costly for all. We introduce a Pareto Mediator which aims to improve outcomes for delegating agents without making any of them worse off. Our experiments in random normal form games, a restaurant recommendation game, and a reinforcement learning sequential social dilemma show that the Pareto Mediator greatly increases social welfare. Also, even when the Pareto Mediator is based on an incorrect model of agent utility, performance gracefully degrades to the pre-intervention level, due to the individual autonomy preserved by the voluntary mediator. ",Improving Social Welfare While Preserving Autonomy via a Pareto Mediator
259,1402563924529233922,1240632262858805249,Jos√© Manuel Alarc√≥n,['We posted yesterday the seminal paper of a new formalism that we developed to study the properties of nuclear matter. Take a look on the arXiv: <LINK>'],https://arxiv.org/abs/2106.02652,"We resum the ladder diagrams for the calculation of the energy density $\cal{E}$ of a spin 1/2 fermion many-body system in terms of arbitrary vacuum two-body scattering amplitudes. The partial-wave decomposition of the in-medium two-body scattering amplitudes is developed, and the expression for calculating $\cal{E}$ in a partial-wave amplitude expansion is also given. The case of contact interactions is completely solved and is shown to provide renormalized results, expressed directly in terms of scattering data parameters, within cutoff regularization in a wide class of schemes. $S$- and $P$-wave interactions are considered up to including the first three-terms in the effective-range expansion, paying special attention to the parametric region around the unitary limit. ","Ladder resummation of spin 1/2 fermion many-body systems with arbitrary
  partial-wave content"
260,1402525129511419904,2980919721,Kostas Gourgouliatos,"['In our recent paper with Sam Lander we study magneto-plastic flows in neutron stars and their dependence on the type of failure (local, intermediate and global) <LINK> <LINK>']",https://arxiv.org/abs/2106.03869,"Magnetic field evolution in neutron-star crusts is driven by the Hall effect and Ohmic dissipation, for as long as the crust is sufficiently strong to absorb Maxwell stresses exerted by the field and thus make the momentum equation redundant. For the strongest neutron-star fields, however, stresses build to the point of crustal failure, at which point the standard evolution equations are no longer valid. Here, we study the evolution of the magnetic field of the crust up to and beyond crustal failure, whence the crust begins to flow plastically. We perform global axisymmetric evolutions, exploring different types of failure affecting a limited region of the crust. We find that a plastic flow does not simply suppress the Hall effect even in the regime of a low plastic viscosity, but it rather leads to non-trivial evolution -- in some cases even overreacting and enhancing the impact of the Hall effect. Its impact is more pronouced in the toroidal field, with the differences on the poloidal field being less substantial. We argue that both the nature of magnetar bursts and their spindown evolution will be affected by plastic flow, so that observations of these phenomena may help to constrain the way the crust fails. ",Axisymmetric magneto-plastic evolution of neutron-star crusts
261,1402497950539001858,1250971237,Xiaohua Zhai,"['We study scaling laws for Vision Transformer, and characterize the relationships between error rate, model size, data, and compute.\n\nOur ViT-G/14 with 2B params, pre-trained on 3B images, attains a new SOTA on ImageNet of 90.45% accuracy!\n\nPaper: <LINK> <LINK>', 'Bigger models are more sample efficient, reaching the same level of error rate with fewer seen images. \n\nViT-G/14 model reaches 84.86% top-1 accuracy on ImageNet with only 10 examples per class, which is less than 1% of the train set! https://t.co/fAlBB6At8T', 'Visualization of ViT-G shape compared to the previous ViT-H shape, together with our memory saving improvements (remove `class` token, use Adafactor optimizer, https://t.co/WNsyjoMNeZ.). https://t.co/fiCeqEavCV', 'Work done at Google Research, Brain Team Z√ºrich, with @__kolesnikov__ , @neilhoulsby  and @giffmana.', '@Eng_Hemdi We deduplicate the 3B pre-train set with BOTH imagenet train set and test set, as well as all other downstream tasks used in our paper. This ensures that the test images are unseen during pre-training.']",https://arxiv.org/abs/2106.04560,"Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45% top-1 accuracy. The model also performs well on few-shot learning, for example, attaining 84.86% top-1 accuracy on ImageNet with only 10 examples per class. ",Scaling Vision Transformers
262,1402478792593920002,1578756350,Laura V. Sales,"['Grad student Ethan Jahn on ArXiv today üôå We use FIRE sims to study environmental quenching by LMC-like hosts. Summary: dwarf hosts can also quench their tiny satellites and tidal streams are common too! @AndrewWetzel  @astro_jenna   @kjb_astro @jbprime  <LINK> <LINK>', 'Adding: @MBKplus  (Twitter cut you off the original post!) Sorry!... maybe you just deserved your own subtweet.']",https://arxiv.org/abs/2106.03861,"Characterizing the predicted environments of dwarf galaxies like the Large Magellanic Cloud (LMC) is becoming increasingly important as next generation surveys push sensitivity limits into this low-mass regime at cosmological distances. We study the environmental effects of LMC-mass halos ($M_{200m} \sim 10^{11}$ M$_\odot$) on their populations of satellites ($M_\star \geq 10^4$ M$_\odot$) using a suite of zoom-in simulations from the Feedback In Realistic Environments (FIRE) project. Our simulations predict significant hot coronas with $T\sim10^5$ K and $M_\text{gas}\sim10^{9.5}$ M$_\odot$. We identify signatures of environmental quenching in dwarf satellite galaxies, particularly for satellites with intermediate mass ($M_\star = 10^{6-7}$ M$_\odot$). The gas content of such objects indicates ram-pressure as the likely quenching mechanism, sometimes aided by star formation feedback. Satellites of LMC-mass hosts replicate the stellar mass dependence of the quiescent fraction found in satellites of MW mass hosts (i.e. that the quiescent fraction increases as stellar mass decreases). Satellites of LMC-mass hosts have a wider variety of quenching times when compared to the strongly bi-modal distribution of quenching times of nearby centrals. Finally, we identify significant tidal stellar structures around four of our six LMC-analogs, suggesting that stellar streams may be common. These tidal features originated from satellites on close orbits, extend to $\sim$80 kpc from the central galaxy, and contain $\sim10^{6-7}$ M$_\odot$~of stars. ","The effects of LMC-mass environments on their dwarf satellite galaxies
  in the FIRE simulations"
263,1402307703066837000,1128114749793660928,Giulio Isacchini,"['How to do inference when the likelihood is not there for you? You can build its model using simulated data by maximizing mutual information - as we propose in our ""minimalist"" preprint <LINK>\nwith @N_t_n__l @TheArmita Thierry Mora and Aleksandra Walczak [1/4] <LINK>', 'When the likelihood cannot be used in practice, inference can be performed using simulated data. We draw inspiration from the work of Hermans (https://t.co/d9BemY1Nz9) and following their approach we infer the likelihood-to-evidence ratio using a neural network [2/4]', 'We reformulate the inference problem as maximization of the mutual information between model parameters and the simulated data. Once we estimate the likelihood-to-evidence ratio the posterior is only a few (MCMC) steps away. [3/4]', 'We compare different lower bounds to mutual information and find that they allow to reliably recover the posteriors over parameters of stochastic and chaotic processes. We are now looking forward to apply these methods to real data! [4/4]']",http://arxiv.org/abs/2106.01808,"Simulation-based inference enables learning the parameters of a model even when its likelihood cannot be computed in practice. One class of methods uses data simulated with different parameters to infer models of the likelihood-to-evidence ratio, or equivalently the posterior function. Here we frame the inference task as an estimation of an energy function parametrized with an artificial neural network. We present an intuitive approach where the optimal model of the likelihood-to-evidence ratio is found by maximizing the likelihood of simulated data. Within this framework, the connection between the task of simulation-based inference and mutual information maximization is clear, and we show how several known methods of posterior estimation relate to alternative lower bounds to mutual information. These distinct objective functions aim at the same optimal energy form and therefore can be directly benchmarked. We compare their accuracy in the inference of model parameters, focusing on four dynamical systems that encompass common challenges in time series analysis: dynamics driven by multiplicative noise, nonlinear interactions, chaotic behavior, and high-dimensional parameter space. ","MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood
  Inference from Sampled Trajectories"
264,1402240883261689857,859294200,Eric Malmi,"['üìùNew #ACL2021NLP Paperüìù\nPaper: <LINK>\nData: <LINK>\n\nOn the benefits of large LMs: We find that grammatical error correction (GEC) performance depends frustratingly(?) heavily on model size. More belowüëá <LINK>', 'Our synthetically pre-trained 11B-param T5 model yields new SOTA for English, German, Russian, and Czech GEC. \n\nSince the model is costly and impractical to run, we use it to create a cleaned version of the popular Lang-8 corpus to distill the model to smaller ones.', ""I view Grammatical Error Correction as a great example of beneficial applications of large LMs:\n1. GEC can level the playing field between native and non-native speakers.\n2. GEC models don't try to generate new content but only to fix grammatical errors""]",https://arxiv.org/abs/2106.03830,"This paper presents a simple recipe to train state-of-the-art multilingual Grammatical Error Correction (GEC) models. We achieve this by first proposing a language-agnostic method to generate a large number of synthetic examples. The second ingredient is to use large-scale multilingual language models (up to 11B parameters). Once fine-tuned on language-specific supervised sets we surpass the previous state-of-the-art results on GEC benchmarks in four languages: English, Czech, German and Russian. Having established a new set of baselines for GEC, we make our results easily reproducible and accessible by releasing a cLang-8 dataset. It is produced by using our best model, which we call gT5, to clean the targets of a widely used yet noisy lang-8 dataset. cLang-8 greatly simplifies typical GEC training pipelines composed of multiple fine-tuning stages -- we demonstrate that performing a single fine-tuning step on cLang-8 with the off-the-shelf language models yields further accuracy improvements over an already top-performing gT5 model for English. ",A Simple Recipe for Multilingual Grammatical Error Correction
265,1402225870199263234,1277309540313300995,Soufiane Hayou,"['Stochastic Depth (SD) acts similarly to Dropout by dropping layers instead of neurons. In our recent work with F. Ayed, (arXiv: <LINK>)\n\n1/ we study the explicit regularization effect of SD, showing that it penalizes a notion of information discrepancy', '2/ we show that choosing constant drop rates is linked to maximal regularization at initialization, compared to other popular choices such as linear drop', '3/ we show that in the large depth limit, SD acts as an implicit Gaussian Noise Injection regularizer, but implicitly adding Gaussian noise to the pre-activations. Empirical results show an excellent match with the theoretical findings.', '4/ inspired by the link between constant drop rate and maximal regularization, we introduce the Budget Hypothesis which predicts that when the average drop rate is small, i.e. effective depth is small, maximal regualrization is not needed.', '5/ we introduce SenseMode, a new algorithm that automatically choses the drop rates at initialization. We show that SenseMode outperforms other choices of drop rates when the average drop rate is small, confirming our budget hypothesis.']",https://arxiv.org/abs/2106.03091,"Regularization plays a major role in modern deep learning. From classic techniques such as L1,L2 penalties to other noise-based methods such as Dropout, regularization often yields better generalization properties by avoiding overfitting. Recently, Stochastic Depth (SD) has emerged as an alternative regularization technique for residual neural networks (ResNets) and has proven to boost the performance of ResNet on many tasks [Huang et al., 2016]. Despite the recent success of SD, little is known about this technique from a theoretical perspective. This paper provides a hybrid analysis combining perturbation analysis and signal propagation to shed light on different regularization effects of SD. Our analysis allows us to derive principled guidelines for choosing the survival rates used for training with SD. ",Regularization in ResNet with Stochastic Depth
266,1402190240018743299,549128696,Vicky Fawcett,['See our latest paper on the arxiv today: we find enhanced small scale radio emission in red quasars using e-MERLIN data\n<LINK>'],https://arxiv.org/abs/2106.02646,"Red quasi-stellar objects (QSOs) are a subset of the quasar population with colours consistent with reddening due to intervening dust. Recent work has demonstrated that red QSOs show special radio properties that fundamentally distinguish them from normal blue QSOs, specifically a higher incidence of low-power radio emission (1.4 GHz luminosities L$_{\rm 1.4} \approx 10^{25}$ - $10^{27}$ W Hz$^{-1}$) that is physically compact when imaged by arcsecond-resolution radio surveys such as FIRST. In this work, we present e-MERLIN imaging of a set of intermediate-redshift ($1.0<z<1.55$), luminous (bolometric luminosities L$_{bol} \approx 10^{46}$ - $10^{47}$ erg s$^{-1}$) red and normal QSOs carefully selected to have radio properties that span the range over which red QSOs show the most divergence from the general population. With an angular resolution $\times25$ better than FIRST, we resolve structures within the host galaxies of these QSOs ($> 2$ kpc). We report a statistically significant difference in the incidence of extended kpc-scale emission in red QSOs. From an analysis of the radio size distributions of the sample, we find that the excess radio emission in red QSOs can be attributed to structures that are confined to galaxy scales ($< 10$ kpc), while we confirm previous results that red and normal QSOs have similar incidences of radio jets and lobes on circumgalactic or larger scales ($> 10$ kpc). Our results indicate that the primary mechanism that generates the enhanced radio emission in red QSOs is not directly connected with the nuclear engine or accretion disc, but is likely to arise from extended components such as AGN-driven jets or winds. ","Fundamental differences in the radio properties of red and blue quasars:
  kiloparsec-scale structures revealed by e-MERLIN"
267,1402003625040285700,916407589,Anders Kvellestad,"['So, what is the current status of WIMP dark matter? To find out, we took GAMBIT for a spin in a 24-dimensional parameter space: \n\n<LINK>\n\n""Thermal WIMPs and the Scale of New Physics: Global Fits of Dirac Dark Matter Effective Field Theories""']",https://arxiv.org/abs/2106.02056,"We assess the status of a wide class of WIMP dark matter (DM) models in light of the latest experimental results using the global fitting framework $\textsf{GAMBIT}$. We perform a global analysis of effective field theory (EFT) operators describing the interactions between a gauge-singlet Dirac fermion and the Standard Model quarks, the gluons and the photon. In this bottom-up approach, we simultaneously vary the coefficients of 14 such operators up to dimension 7, along with the DM mass, the scale of new physics and several nuisance parameters. Our likelihood functions include the latest data from $\mathit{Planck}$, direct and indirect detection experiments, and the LHC. For DM masses below 100 GeV, we find that it is impossible to satisfy all constraints simultaneously while maintaining EFT validity at LHC energies. For new physics scales around 1 TeV, our results are influenced by several small excesses in the LHC data and depend on the prescription that we adopt to ensure EFT validity. Furthermore, we find large regions of viable parameter space where the EFT is valid and the relic density can be reproduced, implying that WIMPs can still account for the DM of the universe while being consistent with the latest data. ","Thermal WIMPs and the Scale of New Physics: Global Fits of Dirac Dark
  Matter Effective Field Theories"
268,1401850630105403394,513464916,Carlos S√°nchez Mu√±oz,"[""My new paper with @AVivasViana is out on the arXiv!\n\n<LINK>\n\nWe study the regime of coherent two-photon excitation of coupled, non-identical quantum emitters. You can have a look at Alejandro's thread üëá <LINK>""]",https://arxiv.org/abs/2106.02574,"We study a system of two interacting, non-indentical quantum emitters driven by a coherent field. We focus on the particular condition of two-photon resonance and obtain analytical expressions for the stationary density matrix of the system and observables of the fluorescent emission. Importantly, our expressions are valid for the general situation of non-identical emitters with different transition energies. Our results allow us to determine the regime of parameters in which coherent two-photon excitation, enabled by the coherent coupling between emitters, is dominant over competing, first-order processes. Using the formalism of quantum parameter estimation, we show that the features imprinted by the two-photon dynamics into the spectrum of resonance fluorescence are particularly sensitive to changes in the distance between emitters, making two-photon excitation the optimal driving regime for the estimation of inter-emitter distance. This can be exploited for applications such as superresolution imaging of point-like sources. ","Two-photon resonance fluorescence of two interacting non-identical
  quantum emitters"
269,1400616886644117506,1878909680,Amin Karbasi,"['Adaptivity complexity is a very neat information theoretic notion for parallelization. It has been studied under batch learning, information parallelization, etc in different communities. Here, we establish an exponential reduction for Thompson Sampling.<LINK> <LINK>', 'A very simple idea that Heavily relies on the amazing work done by @ashipra @SebastienBubeck @QuanquanGu and others. And of course, the bible ""Bandit Algorithms"" by @CsabaSzepesvari and Tor Lattimore.']",https://arxiv.org/abs/2106.01420,"How can we make use of information parallelism in online decision making problems while efficiently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and linear contextual bandit with finitely many arms. Over a time horizon $T$, our \textit{batch} Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only $O(\log T)$ batch queries. To achieve this exponential reduction, i.e., reducing the number of interactions from $T$ to $O(\log T)$, our batch policy dynamically determines the duration of each batch in order to balance the exploration-exploitation trade-off. We also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines such as static batch allocations. ",Parallelizing Thompson Sampling
270,1400516024437411843,1188224435364327424,Belinda Li,"['Do neural language models (trained on text alone!) construct representations of meaning? In a new #ACL2021NLP paper, we find that LM representations implicitly model *entities and situations* as they evolve through a discourse. 1/\n<LINK> <LINK>', 'These representations roughly correspond to linguistic models of dynamic semantics---they update as new sentences are added to the discourse, to reflect changes to the underlying state. 2/ https://t.co/4AcxdqHN3m', 'Specifically, we train BART and T5 models on text transcripts in two datasets (Alchemy and Textworld). We find a linear probe can map encoder representations of text to the truth values of propositions about world that the text describes. 3/ https://t.co/JNlutxCjuJ', 'Furthermore, the representations are local: in Alchemy, the probe can tell the final state of an entity by looking at the first appearance of an entity in a discourse. 4/ https://t.co/8RkpTF1Npq', 'Finally, the representations can be manipulated to affect downstream generation: by replacing a small set of LM encodings, we can induce the decoder to generate text consistent with the *modified* state representation. 5/ https://t.co/IbXRWcssdv', 'Of course, LMs still make lots of semantic errors, and our experiments only look at a tiny slice of semantics. The datasets we test on are far simpler than, e.g., those featured in the thought experiments of Bender&amp;Koller. Even here, probe accuracies are far from perfect. 6/', 'However, these experiments suggest that it‚Äôs possible to learn noisy, approximate representations of some aspects of meaning from text alone. 7/', 'This work would not have been possible without my amazing co-authors @Maxwell_Nye and @jacobandreas!\n\nCode is available at: https://t.co/TW2vcC2Igp\n\n8/8']",https://arxiv.org/abs/2106.00737,"Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as models of entities and situations as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data. Code and data are available at this https URL . ",Implicit Representations of Meaning in Neural Language Models
271,1399991776212103168,422672164,Dr Michael Reidinger,"['The Earth as a transducer for dark-photon dark-matter detection\n\n""We propose the use of the Earth as a transducer for ultralight dark-matter detection. In particular we point out a novel signal of kinetically mixed dark-photon dark matter""\n<LINK>']",https://arxiv.org/abs/2106.00022,"We propose the use of the Earth as a transducer for ultralight dark-matter detection. In particular we point out a novel signal of kinetically mixed dark-photon dark matter: a monochromatic oscillating magnetic field generated at the surface of the Earth. Similar to the signal in a laboratory experiment in a shielded box (or cavity), this signal arises because the lower atmosphere is a low-conductivity air gap sandwiched between the highly conductive interior of the Earth below and ionosphere or interplanetary medium above. At low masses (frequencies) the signal in a laboratory detector is usually suppressed by the size of the detector multiplied by the dark-matter mass. Crucially, in our case the suppression is by the radius of the Earth, and not by the (much smaller) height of the atmosphere. We compute the size and global vectorial pattern of our magnetic field signal, which enables sensitive searches for this signal using unshielded magnetometers dispersed over the surface of the Earth. In principle, the signal we compute exists for any dark photon in the mass range $10^{-21} \text{eV}\lesssim m_{A'} \lesssim 3\times 10^{-14} \text{eV}$. We summarize the results of our companion paper [arXiv:2108.08852], in which we detail such a search using a publicly available dataset from the SuperMAG Collaboration: we report no robust signal candidates and so place constraints in the (more limited) dark-photon dark-matter mass range $2\times 10^{-18} \text{eV} \lesssim m_{A'} \lesssim 7\times 10^{-17} \text{eV}$ (corresponding to frequencies $6\times 10^{-4} \text{Hz}\lesssim f \lesssim 2\times 10^{-2} \text{Hz}$). These constraints are complementary to existing astrophysical bounds. Future searches for this signal may improve the sensitivity over a wide range of ultralight dark-matter candidates and masses. ",Earth as a transducer for dark-photon dark-matter detection
272,1412922727145435136,504239269,Vu Nguyen,['1‚É£Tuning RL hypers is expensive \n2‚É£Do you want to optimize them without retraining the whole agent each time? \n3‚É£Hypers can be continuous or mixed categorical-continuous?\n\nWe propose to optimize mixed categorical-continuous hypers on-the-fly for AutoRL\n\nüëâ<LINK> <LINK>'],https://arxiv.org/abs/2106.15883,"Despite a series of recent successes in reinforcement learning (RL), many RL algorithms remain sensitive to hyperparameters. As such, there has recently been interest in the field of AutoRL, which seeks to automate design decisions to create more general algorithms. Recent work suggests that population based approaches may be effective AutoRL algorithms, by learning hyperparameter schedules on the fly. In particular, the PB2 algorithm is able to achieve strong performance in RL tasks by formulating online hyperparameter optimization as time varying GP-bandit problem, while also providing theoretical guarantees. However, PB2 is only designed to work for continuous hyperparameters, which severely limits its utility in practice. In this paper we introduce a new (provably) efficient hierarchical approach for optimizing both continuous and categorical variables, using a new time-varying bandit algorithm specifically designed for the population based training regime. We evaluate our approach on the challenging Procgen benchmark, where we show that explicitly modelling dependence between data augmentation and other hyperparameters improves generalization. ","Tuning Mixed Input Hyperparameters on the Fly for Efficient Population
  Based AutoRL"
273,1410648136016560129,1285209259701989377,Spandan Madan,"['New preprint! We show that CNNs and Transformers are brittle to small changes in 3D perspective and lighting. We propose an evolution strategies (ES) based search method for finding failures within training distribution! 1/5\n\npdf: <LINK> <LINK>', 'Brittleness of NNs to shifts/rotations/color changes is attributed to biased training data and lack of shift-invariance. We challenge this hypothesis and test classification under camera and lighting variations while ensuring - unbiased training and shift invariant architectures.', 'To investigate brittleness we propose an ES based approach which searches for failures within the training distribution (CMA-Search). Starting with correctly classified image, our method searches for failures in vicinity of camera and light params. 3/5 https://t.co/19AwVZb64v', 'Our method finds errors in vicinity of correctly classified images for 71% cases with &lt; 3.6% change in camera. For transformers, its far worse---only 15% survived CMA-Search. 4/5 https://t.co/t2rKBKv8Rn', 'Shift invariant architectures were robust to 2D shifts, but not 3D changes. Data augmentation, unbiased datasets, and shift-invariant architectures help, but new architectures need to be invariant to a super-set of 2D shifts‚Äî3D perspective changes and lighting. 5/5', 'This is joint work with Tomotake Sasaki, @tzumaoli, Xavier Boix and @hpfister!']",https://arxiv.org/abs/2106.16198,"Neural networks are susceptible to small transformations including 2D rotations and shifts, image crops, and even changes in object colors. This is often attributed to biases in the training dataset, and the lack of 2D shift-invariance due to not respecting the sampling theorem. In this paper, we challenge this hypothesis by training and testing on unbiased datasets, and showing that networks are brittle to both small 3D perspective changes and lighting variations which cannot be explained by dataset bias or lack of shift-invariance. To find these in-distribution errors, we introduce an evolution strategies (ES) based approach, which we call CMA-Search. Despite training with a large-scale (0.5 million images), unbiased dataset of camera and light variations, in over 71% cases CMA-Search can find camera parameters in the vicinity of a correctly classified image which lead to in-distribution misclassifications with < 3.6% change in parameters. With lighting changes, CMA-Search finds misclassifications in 33% cases with < 11.6% change in parameters. Finally, we extend this method to find misclassifications in the vicinity of ImageNet images for both ResNet and OpenAI's CLIP model. ","Small in-distribution changes in 3D perspective and lighting fool both
  CNNs and Transformers"
274,1410398678548848641,186017372,Yuichi Tanaka,"['Our new paper on graph signal restoration is out! We propose ""nested"" deep algorithm unrolling. Check this out on arXiv!\nM. Nagahama, K. Yamada, Y. Tanaka, S. H. Chan, and Y. C. Eldar: Graph signal restoration using nested deep algorithm unrolling\n<LINK> <LINK>']",https://arxiv.org/abs/2106.15910,"Graph signal processing is a ubiquitous task in many applications such as sensor, social, transportation and brain networks, point cloud processing, and graph neural networks. Often, graph signals are corrupted in the sensing process, thus requiring restoration. In this paper, we propose two graph signal restoration methods based on deep algorithm unrolling (DAU). First, we present a graph signal denoiser by unrolling iterations of the alternating direction method of multiplier (ADMM). We then suggest a general restoration method for linear degradation by unrolling iterations of Plug-and-Play ADMM (PnP-ADMM). In the second approach, the unrolled ADMM-based denoiser is incorporated as a submodule, leading to a nested DAU structure. The parameters in the proposed denoising/restoration methods are trainable in an end-to-end manner. Our approach is interpretable and keeps the number of parameters small since we only tune graph-independent regularization parameters. We overcome two main challenges in existing graph signal restoration methods: 1) limited performance of convex optimization algorithms due to fixed parameters which are often determined manually. 2) large number of parameters of graph neural networks that result in difficulty of training. Several experiments for graph signal denoising and interpolation are performed on synthetic and real-world data. The proposed methods show performance improvements over several existing techniques in terms of root mean squared error in both tasks. ",Graph Signal Restoration Using Nested Deep Algorithm Unrolling
275,1409575912337723397,939946467153924097,Jiacheng Xu,"['Excited to share my ACL21 paper with @gregd_nlp ""Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution"": <LINK>\nWe propose a two-stage framework to interpret abstractive summarization models‚Äô decisions at each time step. <LINK>', ""We've seen a lot of work on interpreting NLU models, but it's not easy to apply these to summarization."", 'First, we use model ablation to identify and isolate ‚Äúlanguage model-like‚Äù generation which does not rely on the source text being summarized. We compare the full model with ablated versions (eg. no input doc). \nEg. for (after ""vote"") &amp; ""Khan (after ""Sadiq"") are LM decisions.', 'Second, we do attribution to locate the source evidence for context-dependent generation. \nFor example, why does the model say ‚ÄúCameron‚Äù after ‚ÄúDavid?‚Äù Methods like Integrated Gradient tell us the model makes the decision by looking at ""prime minister"".', 'Bonus: We found evidence that SOTA models may actually ""cheat"" by being pre-trained on test data.\nIn the ablation setting, the model accidentally knows the test example even without encoding the input document. Check Section 4 ""Bias from Training Data"" for more detail.']",https://arxiv.org/abs/2106.01518,"Despite the prominence of neural abstractive summarization models, we know little about how they actually form summaries and how to understand where their decisions come from. We propose a two-step method to interpret summarization model decisions. We first analyze the model's behavior by ablating the full model to categorize each decoder decision into one of several generation modes: roughly, is the model behaving like a language model, is it relying heavily on the input, or is it somewhere in between? After isolating decisions that do depend on the input, we explore interpreting these decisions using several different attribution methods. We compare these techniques based on their ability to select content and reconstruct the model's predicted token from perturbations of the input, thus revealing whether highlighted attributions are truly important for the generation of the next token. While this machinery can be broadly useful even beyond summarization, we specifically demonstrate its capability to identify phrases the summarization model has memorized and determine where in the training pipeline this memorization happened, as well as study complex generation phenomena like sentence fusion on a per-instance basis. ","Dissecting Generation Modes for Abstractive Summarization Models via
  Ablation and Attribution"
276,1405179533335072768,636167919,Andreas Loukas,"['Are well-generalizing neural nets (NNs) easier to train?ü§îAiming to shed light on this hypothesis, we studied the relation between the complexity of the learned NN and the training behavior.\n\n<LINK>üì∞with @MarinosPoiitis &amp; @StefanieJegelka\nüëá <LINK>', 'Some evidence for our hypothesis already exists: e.g., it is known that training (shallow) NNs is more tedious for noisy data and easier for more separable classes. Moreover, the beautiful theory of stability says that NNs trained for few epochs have bounded sample complexity.', 'Differently, we connect aspects of the optim. trajectory with the NN Lipschitz constant (wrt input) close and far from the training data: we find that the trajectory of high complexity NNs is longer, veers further from initialization, and exhibits higher variance near convergence', 'Intriguingly, we also find that steady training with Dropout implies a training- and data-dependent generalization bound that grows poly-logarithmically with the number of parameters (typical Lipschitz constant-based generalization bounds grow exponentially üöÄwith NN depth).', 'Overall, our results support the hypothesis that good training behavior can be a useful bias towards good generalization']",https://arxiv.org/abs/2106.04186,"This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN's Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. We first observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point. Finally, we find that steady training with Dropout implies a training- and data-dependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization. ",What training reveals about neural network complexity
277,1405163518547181571,707491685727604736,Antonio,"['Check out our recent paper, where we show how hyperparameters affect the backdoor learning and model parameters. Furthermore, we find that a wise choice of hyperparameters provides a good trade-off between backdoor robustness and cleaning accuracy.\n\narxiv: <LINK> <LINK>', 'Joint work with Kathrin Grosse, @xwasco, @ambrademontis, @biggiobattista, Fabio Roli and @PelilloMarcello!']",https://arxiv.org/abs/2106.07214,"Backdoor attacks inject poisoning samples during training, with the goal of forcing a machine learning model to output an attacker-chosen class when presented a specific trigger at test time. Although backdoor attacks have been demonstrated in a variety of settings and against different models, the factors affecting their effectiveness are still not well understood. In this work, we provide a unifying framework to study the process of backdoor learning under the lens of incremental learning and influence functions. We show that the effectiveness of backdoor attacks depends on: (i) the complexity of the learning algorithm, controlled by its hyperparameters; (ii) the fraction of backdoor samples injected into the training set; and (iii) the size and visibility of the backdoor trigger. These factors affect how fast a model learns to correlate the presence of the backdoor trigger with the target class. Our analysis unveils the intriguing existence of a region in the hyperparameter space in which the accuracy on clean test samples is still high while backdoor attacks are ineffective, thereby suggesting novel criteria to improve existing defenses. ","Backdoor Learning Curves: Explaining Backdoor Poisoning Beyond Influence
  Functions"
278,1404655973768310785,1270937040880898050,Tobias Sutter,"['We study data-driven DRO when the data is generated by an unknown Markov chain. While generalization bounds can be extended to this setting using the theory of large deviations, the underlying optimization problem is nonconvex \n==&gt; <LINK> <LINK>']",https://arxiv.org/abs/2106.06741,"We study a stochastic program where the probability distribution of the uncertain problem parameters is unknown and only indirectly observed via finitely many correlated samples generated by an unknown Markov chain with $d$ states. We propose a data-driven distributionally robust optimization model to estimate the problem's objective function and optimal solution. By leveraging results from large deviations theory, we derive statistical guarantees on the quality of these estimators. The underlying worst-case expectation problem is nonconvex and involves $\mathcal O(d^2)$ decision variables. Thus, it cannot be solved efficiently for large $d$. By exploiting the structure of this problem, we devise a customized Frank-Wolfe algorithm with convex direction-finding subproblems of size $\mathcal O(d)$. We prove that this algorithm finds a stationary point efficiently under mild conditions. The efficiency of the method is predicated on a dimensionality reduction enabled by a dual reformulation. Numerical experiments indicate that our approach has better computational and statistical properties than the state-of-the-art methods. ",Distributionally Robust Optimization with Markovian Data
279,1402722460034797571,1270937040880898050,Tobias Sutter,['We studied how to derive robust generalization bounds in the presence of a distribution shift. Our key tools are the principle of minimum discriminating information and distributionally robust optimization.\n == &gt; <LINK> <LINK>'],https://arxiv.org/abs/2106.04443,"Training models that perform well under distribution shifts is a central challenge in machine learning. In this paper, we introduce a modeling framework where, in addition to training data, we have partial structural knowledge of the shifted test distribution. We employ the principle of minimum discriminating information to embed the available prior knowledge, and use distributionally robust optimization to account for uncertainty due to the limited samples. By leveraging large deviation results, we obtain explicit generalization bounds with respect to the unknown shifted distribution. Lastly, we demonstrate the versatility of our framework by demonstrating it on two rather distinct applications: (1) training classifiers on systematically biased data and (2) off-policy evaluation in Markov Decision Processes. ","Robust Generalization despite Distribution Shift via Minimum
  Discriminating Information"
280,1402571140917346304,872839257735475202,Mateusz Malinowski,"['Big language models are bad in dealing with consistency or mathematics. Here, we scrutinize the mathematical abilities and math-biases of Bert and propose a few self-supervised losses. The work will appear at #ACL2021 #ACL2021NLP\n\n<LINK> <LINK>']",https://arxiv.org/abs/2106.03921,"Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models. We also show how to reduce positional bias in such models. ","Measuring and Improving BERT's Mathematical Abilities by Predicting the
  Order of Reasoning"
281,1402559980566315008,878343763764039680,M Akash Kumar,"['How can we optimize language generation models for both quality and diversity?\n\nFind out in our KDD\'21 paper titled ""Diversity driven Query Rewriting in Search Advertising"". \n\nPaper: <LINK>\n\n(1/n)', 'In this paper, we describe recent improvements we‚Äôve made to algorithms that match search queries to ads in Microsoft Bing. Specifically, we focus on the problem of rewriting a user search query into multiple same-intent keywords. \n\n(2/n)', 'We propose CLOVER, a framework for optimizing human\njudgments on rewrite quality while also being able to control the desired diversity. We use diversity driven RL algorithms where the optimization objective enforces generating multiple diverse and high-quality rewrites. \n\n(3/n) https://t.co/4CnYQKdt6M', 'We perform online A/B experiments on Bing, which shows that our approach leads to (i) better user engagement with an average increase in clicks by 12.83% accompanied with an average defect reduction by 13.97%, and (ii) improved revenue by 21.29%.']",https://arxiv.org/abs/2106.03816,"Retrieving keywords (bidwords) with the same intent as query, referred to as close variant keywords, is of prime importance for effective targeted search advertising. For head and torso search queries, sponsored search engines use a huge repository of same intent queries and keywords, mined ahead of time. Online, this repository is used to rewrite the query and then lookup the rewrite in a repository of bid keywords contributing to significant revenue. Recently generative retrieval models have been shown to be effective at the task of generating such query rewrites. We observe two main limitations of such generative models. First, rewrites generated by these models exhibit low lexical diversity, and hence the rewrites fail to retrieve relevant keywords that have diverse linguistic variations. Second, there is a misalignment between the training objective - the likelihood of training data, v/s what we desire - improved quality and coverage of rewrites. In this work, we introduce CLOVER, a framework to generate both high-quality and diverse rewrites by optimizing for human assessment of rewrite quality using our diversity-driven reinforcement learning algorithm. We use an evaluation model, trained to predict human judgments, as the reward function to finetune the generation policy. We empirically show the effectiveness of our proposed approach through offline experiments on search queries across geographies spanning three major languages. We also perform online A/B experiments on Bing, a large commercial search engine, which shows (i) better user engagement with an average increase in clicks by 12.83% accompanied with an average defect reduction by 13.97%, and (ii) improved revenue by 21.29%. ",Diversity driven Query Rewriting in Search Advertising
282,1402359718795001862,1228741441708531716,Pouria Rouzrokh,"[""I am pleased to share our new publication, in which we propose a framework using perceptual-tuned Generative Adversarial Network (#GAN), in order to produce thinner slice (e.g., high resolution in the 'Z' plane) medical images with #DeepLearning: <LINK> <LINK>""]",https://arxiv.org/abs/2106.02599,"There is a growing demand for high-resolution (HR) medical images in both the clinical and research applications. Image quality is inevitably traded off with the acquisition time for better patient comfort, lower examination costs, dose, and fewer motion-induced artifacts. For many image-based tasks, increasing the apparent resolution in the perpendicular plane to produce multi-planar reformats or 3D images is commonly used. Single image super-resolution (SR) is a promising technique to provide HR images based on unsupervised learning to increase resolution of a 2D image, but there are few reports on 3D SR. Further, perceptual loss is proposed in the literature to better capture the textual details and edges than using pixel-wise loss functions, by comparing the semantic distances in the high-dimensional feature space of a pre-trained 2D network (e.g., VGG). However, it is not clear how one should generalize it to 3D medical images, and the attendant implications are still unclear. In this paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using Perceptual-tuned Generative Adversarial Network (GAN), in order to produce thinner slice (e.g., high resolution in the 'Z' plane) medical images with anti-aliasing and deblurring. The proposed method outperforms other conventional resolution-enhancement methods and previous SR work on medical images upon both qualitative and quantitative comparisons. Specifically, we examine the model in terms of its generalization for various SR ratios and imaging modalities. By addressing those limitations, our model shows promise as a novel 3D SR interpolation technique, providing potential applications in both clinical and research settings. ",SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks
283,1401945316979511296,50095170,Neil Lawrence,"['Solving Schr√∂dinger bridges via maximum likelihood arxiv: <LINK> \n\nWe propose an approximate IPFP/Sinkhorn variant based on the time reveral of diffusions with the goal of learning meaningful interpolating dynamics between two distributions. <LINK>', 'See also recent paper from @ValentinDeBort1, @JamesTThorn, @jeremyhengjm, @ArnaudDoucet1, \n\nhttps://t.co/zj7L8tkZGE', 'Work with \n@vargfran\n @pierthodo  and \n@AustenLamacraft', '@jbell_masterson The thing I would have most liked to do at SCOT was simulate supply chain backwards ... start from last mile, simulate to vendor. It feels like supply chain should/could be reversible ...\n\nHope all is good with you!', '@lostinio Yes, stochastic optimal control too ... as @davidjcmackay liked to say ""everything\'s connected"" (a phrase he picked up from John Bridle).']",https://arxiv.org/abs/2106.02081,"The Schr\""odinger bridge problem (SBP) finds the most likely stochastic evolution between two probability distributions given a prior stochastic evolution. As well as applications in the natural sciences, problems of this kind have important applications in machine learning such as dataset alignment and hypothesis testing. Whilst the theory behind this problem is relatively mature, scalable numerical recipes to estimate the Schr\""odinger bridge remain an active area of research. We prove an equivalence between the SBP and maximum likelihood estimation enabling direct application of successful machine learning techniques. We propose a numerical procedure to estimate SBPs using Gaussian process and demonstrate the practical usage of our approach in numerical simulations and experiments. ","Solving Schr\""odinger Bridges via Maximum Likelihood"
284,1401745472713236480,957685323198164992,Ziwei Liu,"['""Semi-Supervised Domain Generalization with Stochastic StyleMatch"":\n\nPaper: <LINK>\nCode: <LINK>\n\n- We study semi-supervised DG (SSDG), a more realistic and practical setting for DG.\n- StyleMatch is surprisingly effective in OOD generalization. <LINK>', '@Hossein_SHN Thanks for the nice suggestion, Hossein!  We will definitely discuss more in our updated version.']",https://arxiv.org/abs/2106.00592,"Ideally, visual learning algorithms should be generalizable, for dealing with any unseen domain shift when deployed in a new target environment; and data-efficient, for reducing development costs by using as little labels as possible. To this end, we study semi-supervised domain generalization (SSDG), which aims to learn a domain-generalizable model using multi-source, partially-labeled training data. We design two benchmarks that cover state-of-the-art methods developed in two related fields, i.e., domain generalization (DG) and semi-supervised learning (SSL). We find that the DG methods, which by design are unable to handle unlabeled data, perform poorly with limited labels in SSDG; the SSL methods, especially FixMatch, obtain much better results but are still far away from the basic vanilla model trained using full labels. We propose StyleMatch, a simple approach that extends FixMatch with a couple of new ingredients tailored for SSDG: 1) stochastic modeling for reducing overfitting in scarce labels, and 2) multi-view consistency learning for enhancing domain generalization. Despite the concise designs, StyleMatch achieves significant improvements in SSDG. We hope our approach and the comprehensive benchmarks can pave the way for future research on generalizable and data-efficient learning systems. The source code is released at \url{this https URL}. ",Semi-Supervised Domain Generalization with Stochastic StyleMatch
285,1399953534456532993,87128399,Sriram Ganapathy,"['Can we detect Covid using acoustics and symptoms ?\nOur findings on Coswara dataset (under review)\n<LINK>\nSummary - 92.7% acc., spec. of 95%, sensitivity of 69%.    \nWe deeply appreciate if you contribute(d) data to our study <LINK>\n@coswara_iisc <LINK>', 'Code to access the data, generate the lists and reproduce the results \nhttps://t.co/4kHuC7L9XJ']",https://arxiv.org/abs/2106.00639,"The research direction of identifying acoustic bio-markers of respiratory diseases has received renewed interest following the onset of COVID-19 pandemic. In this paper, we design an approach to COVID-19 diagnostic using crowd-sourced multi-modal data. The data resource, consisting of acoustic signals like cough, breathing, and speech signals, along with the data of symptoms, are recorded using a web-application over a period of ten months. We investigate the use of statistical descriptors of simple time-frequency features for acoustic signals and binary features for the presence of symptoms. Unlike previous works, we primarily focus on the application of simple linear classifiers like logistic regression and support vector machines for acoustic data while decision tree models are employed on the symptoms data. We show that a multi-modal integration of acoustics and symptoms classifiers achieves an area-under-curve (AUC) of 92.40, a significant improvement over any individual modality. Several ablation experiments are also provided which highlight the acoustic and symptom dimensions that are important for the task of COVID-19 diagnostics. ","Multi-modal Point-of-Care Diagnostics for COVID-19 Based On Acoustics
  and Symptoms"
