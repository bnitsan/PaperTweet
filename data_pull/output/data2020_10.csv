,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1327290695086985216,1310552063999438849,Hauke Group,['üì£ Check out our new paperüëâUniversal #quantum computation and quantum error correction with #ultracold #atomic mixtures\n\n‚Ñπ <LINK>\n\nThanks to all co-authors for contributingüëá\nKasper\nGonz√°lez-Cuadra \nHegde\nXia\nDauphin\nHuber\nTiemann\nLewenstein\nJendrzejewski\nHauke <LINK>'],https://arxiv.org/abs/2010.15923,"Quantum information platforms made great progress in the control of many-body entanglement and the implementation of quantum error correction, but it remains a challenge to realize both in the same setup. Here, we propose a mixture of two ultracold atomic species as a platform for universal quantum computation with long-range entangling gates, while providing a natural candidate for quantum error-correction. In this proposed setup, one atomic species realizes localized collective spins of tunable length, which form the fundamental unit of information. The second atomic species yields phononic excitations, which are used to entangle collective spins. Finally, we discuss a finite-dimensional version of the Gottesman-Kitaev-Preskill code to protect quantum information encoded in the collective spins, opening up the possibility to universal fault-tolerant quantum computation in ultracold atom systems. ","Universal quantum computation and quantum error correction with
  ultracold atomic mixtures"
1,1326946705233772544,1161312102486667264,Keith Burghardt,"['Whatever the outcome, affirmative action can be a critical tool to improve policies. We discuss questions that surround fair policies in our new paper\n<LINK>\nwith Yuzi He (first author) and @KristinaLerman. Come see our presentation at the AFLCI NeurIPS Workshop! <LINK>', 'Also wanted to add Fiona (Siyi) Guo was an excellent co-author on this paper too. I ran out of space in the previous tweet!']",https://arxiv.org/abs/2010.16409,"Explicit and implicit bias clouds human judgement, leading to discriminatory treatment of minority groups. A fundamental goal of algorithmic fairness is to avoid the pitfalls in human judgement by learning policies that improve the overall outcomes while providing fair treatment to protected classes. In this paper, we propose a causal framework that learns optimal intervention policies from data subject to fairness constraints. We define two measures of treatment bias and infer best treatment assignment that minimizes the bias while optimizing overall outcome. We demonstrate that there is a dilemma of balancing fairness and overall benefit; however, allowing preferential treatment to protected classes in certain circumstances (affirmative action) can dramatically improve the overall benefit while also preserving fairness. We apply our framework to data containing student outcomes on standardized tests and show how it can be used to design real-world policies that fairly improve student test scores. Our framework provides a principled way to learn fair treatment policies in real-world settings. ",Inherent Trade-offs in the Fair Allocation of Treatments
2,1326925131982180352,1153685498021470209,Yuki Atsusaka,"['While more and more topics are considered to be ""sensitive topics"" in surveysüôä, my new working paper w/ Randy Stevenson develops new methods for asking and analyzing sensitive questions based on the Crosswise Model (<LINK>)! (1/3) <LINK>', 'We propose a new design that helps us estimate the prevalence of sensitive attributes even when some survey respondents ""randomly"" choose a given answer category, while also offering several extensions, including sensitivity analysis, weighting, and multivariate regressions (2/3)', 'An easy-to-use R package implementing our methods is available at https://t.co/uNQTTtiWe6! Any suggestions and feedback are greatly appreciated! (3/3)']",https://arxiv.org/abs/2010.16129,"The crosswise model is an increasingly popular survey technique to elicit candid answers from respondents on sensitive questions. Recent studies, however, point out that in the presence of inattentive respondents, the conventional estimator of the prevalence of a sensitive attribute is biased toward 0.5. To remedy this problem, we propose a simple design-based bias correction using an anchor question that has a sensitive item with known prevalence. We demonstrate that we can easily estimate and correct for the bias arising from inattentive respondents without measuring individual-level attentiveness. We also offer several useful extensions of our estimator, including a sensitivity analysis for the conventional estimator, a strategy for weighting, a framework for multivariate regressions in which a latent sensitive trait is used as an outcome or a predictor, and tools for power analysis and parameter selection. Our method can be easily implemented through our open-source software, cWise. ","A Bias-Corrected Estimator for the Crosswise Model with Inattentive
  Respondents"
3,1325871656745771008,713238709,Lucas M√§kinen,"['STOKED to share our new paper ‚Äúdeep21: a Deep Learning Method for 21cm Foreground Removal‚Äù\n\nWe present an approach to separate 21cm cosmological maps from foregrounds -- no power spectra needed!\n\npaper: <LINK>\ntutorial: <LINK> \n\na thread üëá\n1/n <LINK>', 'This grew out of of my undergraduate senior thesis project with @cosmo_shirley, @lachlancaster, @peter_melchior , Laurence Perreault Levasseur, Francisco Villaescusa-Navarro, + @DavidSpergel \n@FlatironCCA + @PU_Astro  + @PrincetonSML  !\n\n(first first-author paper üçæüéâ )\n\n2/n', 'The redshifted 21cm hydrogen emission can be used to directly trace the growth of large-scale structure beyond the reach of galaxy surveys. The observable also enables us to view the Epoch of Reionization--but there‚Äôs a catch...\n\n(image from https://t.co/ZH0q5qn9O3 )\n\n3/n https://t.co/IP6EVqPBGY', 'Hydrogen isn‚Äôt the only thing that emits in the radio ‚Äî astrophysical foregrounds from our galaxy and the interstellar medium are 3-5 orders of magnitude brighter than the cosmological signal at all frequencies\n\n4/n https://t.co/NorxeRYlXo', 'Luckily, these foregrounds are smoother in frequency than the cosmological signal, meaning methods like Principal Component Analysis can find a basis in which foregrounds are orthogonal to the residual cosmo signal‚Äîmaking them easier to remove.\n\n5/n https://t.co/QDmDZK8TyH', 'BUT methods like PCA are *blind* methods ‚Äî they don‚Äôt encode a likelihood (physics) for the foregrounds, AND remove the mean of the cosmological maps. This means all we have left to work with are compressed power spectra for the cleaned maps (see paper for details)\n\n6/n', 'What if we could retrieve the intensity of the 21cm signal from a blind subtraction ?  Fitting this via a regression scales poorly with dimensionality (&gt;50330000 pixels in one simulation !) \n\nNeural networks are efficient learners in high-dimensional spaces‚Äîenter the UNet\n\n7/n', 'We train 3D UNets on simulations:\n1. add random noise realization to observed map\n2. perform PCA subtraction on each map\n3. split maps into cubed HEALPix voxels (x,y,freq)\n4. train ensemble of networks to map between PCA and target with modified loss function (LogCosh)\n\n8/n https://t.co/diacI8WdYR', ""A visual inspection of deep21's prediction on a test voxel over frequency shows the network‚Äôs learned to separate the clean HI signal !\n\nThis means it might be possible to do more fundamental physics with the maps themselves &gt;&gt;\n\n https://t.co/bKkyuDsxu6 \n\n9/n"", ""We validated deep21 by computing power spectra for each UNet's cleaned map, then computed the summaries‚Äô deviations from those derived from the true maps. Training an ensemble gives us an idea about how varied the network cleaning can be on a *single* sky (green contours)\n\n10/n https://t.co/S8AtOObDmp"", 'An interesting result was the successful marginalization of observational noise (at small scales) ! Exposing the network to different levels of noise meant that the network anticipates its shape (unlike PCA) and ""beats"" the noise boundary, like fitting a line to noisy data\n\n11/n', 'This shows that deep21 might be used in a setting with real data, as long as noise is specified and varied in training. (see paper for explanation)\n\n12/n https://t.co/YujcWlxyHY', 'The network was also sensitive to changed foreground parameters, meaning training likely encoded fiducial physics (see paper for details)\n\n13/n https://t.co/Gav0LcHD5c', ""There‚Äôs still a LOT of work that needs to be done before deep21 can clean real maps . We weren't able to resolve cosmo signal in the presence of polarization effects, for example. We‚Äôre hoping that this framework might be modified and improved for use with real data !\n\n14/n https://t.co/b41QQfCNFM"", 'Finally, a big thank you to David Alonso for his support in using his CRIME simulations: https://t.co/srVAr80tIG\n\nAlso, big shout-out to Nick Carriero @FlatironCCA for the GPU cluster support !\n\n15/n', 'Also, cheers to @abhiyay for his video review of our paper ! Glad you liked it !\n\nhttps://t.co/s7TByhMyvP \n\n16/16', 'gif version: https://t.co/vkO73RwrnE']",https://arxiv.org/abs/2010.15843,"We seek to remove foreground contaminants from 21cm intensity mapping observations. We demonstrate that a deep convolutional neural network (CNN) with a UNet architecture and three-dimensional convolutions, trained on simulated observations, can effectively separate frequency and spatial patterns of the cosmic neutral hydrogen (HI) signal from foregrounds in the presence of noise. Cleaned maps recover cosmological clustering statistics within 10% at all relevant angular scales and frequencies. This amounts to a reduction in prediction variance of over an order of magnitude on small angular scales ($\ell > 300$), and improved accuracy for small radial scales ($k_{\parallel} > 0.17\ \rm h\ Mpc^{-1})$ compared to standard Principal Component Analysis (PCA) methods. We estimate posterior confidence intervals for the network's prediction by training an ensemble of UNets. Our approach demonstrates the feasibility of analyzing 21cm intensity maps, as opposed to derived summary statistics, for upcoming radio experiments, as long as the simulated foreground model is sufficiently realistic. We provide the code used for this analysis on Github this https URL as well as a browser-based tutorial for the experiment and UNet model via the accompanying this http URL Colab notebook. ",deep21: a Deep Learning Method for 21cm Foreground Removal
4,1325453435601260545,765403985205354496,Tian Xu,['A theoretical study of two famous IL methods: BC and GAIL for imitating both policies and environments. Our analysis of imitating environments also suggests a new direction in MBRL. Check our NeurIPS paper for details! <LINK> Joint work with  @ZiniuLi and Yang Yu <LINK>'],https://arxiv.org/abs/2010.11876,"Imitation learning trains a policy by mimicking expert demonstrations. Various imitation methods were proposed and empirically evaluated, meanwhile, their theoretical understanding needs further studies. In this paper, we firstly analyze the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding errors compared to behavioral cloning, and thus has a better sample complexity. Noticed that by considering the environment transition model as a dual agent, imitation learning can also be used to learn the environment model. Therefore, based on the bounds of imitating policies, we further analyze the performance of imitating environments. The results show that environment models can be more effectively imitated by generative adversarial imitation than behavioral cloning, suggesting a novel application of adversarial imitation for model-based reinforcement learning. We hope these results could inspire future advances in imitation learning and model-based reinforcement learning. ",Error Bounds of Imitating Policies and Environments
5,1324325297290842113,1175514763347943424,Hirofumi Inaguma,"['Our new preprint on non-autoregressive E2E speech translation is out.\nWe present Orthros, which has AR/NAR decoders on a shared speech encoder.\nRescoring outputs from the NAR decoder by the AR decoder brings out the full potential of CMLM/SMART.\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2010.13047,"Fast inference speed is an important goal towards real-world deployment of speech translation (ST) systems. End-to-end (E2E) models based on the encoder-decoder architecture are more suitable for this goal than traditional cascaded systems, but their effectiveness regarding decoding speed has not been explored so far. Inspired by recent progress in non-autoregressive (NAR) methods in text-based translation, which generates target tokens in parallel by eliminating conditional dependencies, we study the problem of NAR decoding for E2E-ST. We propose a novel NAR E2E-ST framework, Orthros, in which both NAR and autoregressive (AR) decoders are jointly trained on the shared speech encoder. The latter is used for selecting better translation among various length candidates generated from the former, which dramatically improves the effectiveness of a large length beam with negligible overhead. We further investigate effective length prediction methods from speech inputs and the impact of vocabulary sizes. Experiments on four benchmarks show the effectiveness of the proposed method in improving inference speed while maintaining competitive translation quality compared to state-of-the-art AR E2E-ST systems. ","Orthros: Non-autoregressive End-to-end Speech Translation with
  Dual-decoder"
6,1324312287402987522,797888987675365377,Tom Rainforth,"['Improving Transformation Invariance in Contrastive Representation Learning, our new paper led by @AdamEFoster &amp; @rpukdeee. We strengthen representation invariance in contrastive learning using training gradient regularization &amp; test-time feature averaging\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.09515,"We propose methods to strengthen the invariance properties of representations obtained by contrastive learning. While existing approaches implicitly induce a degree of invariance as representations are learned, we look to more directly enforce invariance in the encoding process. To this end, we first introduce a training objective for contrastive learning that uses a novel regularizer to control how the representation changes under transformation. We show that representations trained with this objective perform better on downstream tasks and are more robust to the introduction of nuisance transformations at test time. Second, we propose a change to how test time representations are generated by introducing a feature averaging approach that combines encodings from multiple transformations of the original input, finding that this leads to across the board performance gains. Finally, we introduce the novel Spirograph dataset to explore our ideas in the context of a differentiable generative process with multiple downstream tasks, showing that our techniques for learning invariance are highly beneficial. ","Improving Transformation Invariance in Contrastive Representation
  Learning"
7,1323624267314061312,4193437228,Frederik Plesner Lyngse,"['New working paper out:\nLiquidity Constraints and Demand for Healthcare: Evidence from Danish Welfare Recipients.\n<LINK>.', 'I ask: Are low-income individuals relying on government transfers liquidity constrained by the end of the month to a degree that they postpone medical treatment?\nFirst, I show that the propensity to fill a prescription increase by 52% on transfer income payday. https://t.co/SS6twCmOD1', 'Next, I separate drugs where patients should be able to anticipate the need to fill the prescription: birth control pills and medication for five chronic diseases. I find an increased propensity to fill these prescription of 52-99% on payday, e.g. cholesterol-lowering Statins: https://t.co/c7RsDGuElQ', 'Even for acute conditions, where there should be no anticipation, I find an 22% increased propensity to purchase antibiotics on payday. https://t.co/fDdYWR83U7', 'A natural question to ask is whether this response is driven by patients postponing filling the prescription or by patients postponing the visit to the doctor in the expectation of the subsequent out-of-pocket cost for the prescribed treatment.', 'To answer this, I combine the information on the day of the doctor writing the prescription with the day of the patient filling it.\n80% fill the prescription the same day as the doctor writes it (t=0). 90% fill it within the day after (t=1). https://t.co/NoQw3py6Nw', 'I further exploit this information to test how many days patients postpone filling the prescription, conditional on the day of the doctor visit.', 'The ones getting the prescription 1 day before payday (t=-1) postpone filling the prescription 1 day. https://t.co/kO249YhWL8', 'The ones getting the prescription 2 days before payday (t=-1) postpone filling the prescription 2 days. https://t.co/DzHriXCPpS', 'These results are consistent with liquidity constraints being a key operating mechanism for why welfare recipients postpone antibiotic treatment, as they do seek (free) medical attention, but thereafter postpone the prescribed treatment (that comes with an out-of-pocket price).', 'Lastly, I also show this behavior for vulnerable groups that have an increased risk of complications if delaying necessary antibiotic treatment, e.g.  pregnant women: https://t.co/HinGRPxvrB']",https://arxiv.org/abs/2010.14651,"Are low-income individuals relying on government transfers liquidity constrained by the end of the month to a degree that they postpone medical treatment? I investigate this question using Danish administrative data comprising the universe of welfare recipients and the filling of all prescription drugs. I find that on transfer income payday, recipients have a 52% increase in the propensity to fill a prescription. By separating prophylaxis drugs used to treat chronic conditions, where the patient can anticipate the need to fill the prescription, e.g. cholesterol-lowering statins, I find an increase of up to 99% increase on payday. Even for drugs used to treat acute conditions, where timely treatment is essential, I find a 22% increase on payday for antibiotics and a 5-8% decrease in the four days preceding payday. Lastly, exploiting the difference in day the doctor write the prescription and the day the patient fill it, I show that liquidity constraints is the key operating mechanism for postponing antibiotic treatment. ","Liquidity Constraints and Demand for Healthcare: Evidence from Danish
  Welfare Recipients"
8,1323448662840803330,45082930,Anindya Maiti,"['Our new paper on ""Exploiting Video Calls for Keystroke Inference Attacks"" will appear in NDSS\'21!\n<LINK>\n<LINK>']",https://arxiv.org/abs/2010.12078,"Due to recent world events, video calls have become the new norm for both personal and professional remote communication. However, if a participant in a video call is not careful, he/she can reveal his/her private information to others in the call. In this paper, we design and evaluate an attack framework to infer one type of such private information from the video stream of a call -- keystrokes, i.e., text typed during the call. We evaluate our video-based keystroke inference framework using different experimental settings and parameters, including different webcams, video resolutions, keyboards, clothing, and backgrounds. Our relatively high keystroke inference accuracies under commonly occurring and realistic settings highlight the need for awareness and countermeasures against such attacks. Consequently, we also propose and evaluate effective mitigation techniques that can automatically protect users when they type during a video call. ","Zoom on the Keystrokes: Exploiting Video Calls for Keystroke Inference
  Attacks"
9,1323396125601210372,759249,Dean Eckles,"['üéØHow can we learn to target interventions when we care about outcomes that are only observed after a delay?üóìÔ∏è\n\nThis comes up in settings as varied as clinical trials (5 yr all-cause mortality) and marketing (customer lifetime value).\n\nOur new paper:\n<LINK> <LINK>', 'We were motivated to work on this through a collaboration with The Boston Globe, helping them retain subscribers who might otherwise churn by targeting ""thank you"" emails and discount offers.\n\n(Nice to do marketing research that helps journalism make a business model transition.) https://t.co/GJa5nvGVGC', 'We care about effects on Y, but want to make decisions now. So we measure surrogates S that may mediate all effects our actions have on Y. We then impute Y using past data to learn  E[Y|S,X].\n\nThis general approach is studied in https://t.co/5MD6myMmFL by @Susan_Athey et al. https://t.co/RwSrdsbgzp', 'From a methods perspective, what we do in this paper is show that the assumptions required for identifying the ATE with surrogates also suffice to identify the optimal targeting policy ‚Äî and actually weaker assumptions are sufficient!', 'Of course, this still rests on pretty strong, partially untestable assumptions. So we waited long enough to see Y realized and used that to evaluate these methods empirically.\n\nWe find that the surrogate-model-based policy is indistinguishable from the version using realized Ys: https://t.co/0dw3hTbtVh', 'It is also better than the status quo and using only a single short run proxy (revenue after 1-6 months).\n\nWhile this far from guarantees that this method will work in other settings, it is quite promising...', 'There is a lot more to do with surrogacy, whether using experimental data to learn the surrogacy model (cf https://t.co/cXikdFokQW) or learning surrogate models that are more robust to change over time. https://t.co/3bJGMwJIz6', ""So I'm glad to see that various research communities are valuing work in this exciting area at the interface of stats, econometrics, and ML ‚Äî including that we just received an @INFORMS best paper award\n\nhttps://t.co/zZ0MpLRdQQ"", '@mgershoff @jeremyzyang @sinanaral @dhillon_p No, it is not, which is why we recommend continual exploration, which we do via bootstrap Thompson sampling https://t.co/MXMkkI1iUW\n\nDefinitely good to think about how to learn surrogate models that are more robust to changes in E[Y|S,X] between historical &amp; new data!', '@mgershoff @jeremyzyang @sinanaral @dhillon_p It is just a point intervention really since some of the fixed qualifications for treatment rule out people getting the treatment multiple times... But we are currently working on more of a dynamic treatment regime / RL setup', '@cdsamii @jeremyzyang @sinanaral @dhillon_p @DrewDim I was just discussing this with someone the other day... Yes, I think they are quite related. Typically we imagine that we only have to satisfy some of the assumption on each of two data sets', ""@cdsamii @jeremyzyang @sinanaral @dhillon_p @DrewDim So hadn't fully appreciated this when we were writing the paper, but have recently been thinking a bunch about this.""]",https://arxiv.org/abs/2010.15835,"Decision makers often want to target interventions so as to maximize an outcome that is observed only in the long-term. This typically requires delaying decisions until the outcome is observed or relying on simple short-term proxies for the long-term outcome. Here we build on the statistical surrogacy and policy learning literatures to impute the missing long-term outcomes and then approximate the optimal targeting policy on the imputed outcomes via a doubly-robust approach. We first show that conditions for the validity of average treatment effect estimation with imputed outcomes are also sufficient for valid policy evaluation and optimization; furthermore, these conditions can be somewhat relaxed for policy optimization. We apply our approach in two large-scale proactive churn management experiments at The Boston Globe by targeting optimal discounts to its digital subscribers with the aim of maximizing long-term revenue. Using the first experiment, we evaluate this approach empirically by comparing the policy learned using imputed outcomes with a policy learned on the ground-truth, long-term outcomes. The performance of these two policies is statistically indistinguishable, and we rule out large losses from relying on surrogates. Our approach also outperforms a policy learned on short-term proxies for the long-term outcome. In a second field experiment, we implement the optimal targeting policy with additional randomized exploration, which allows us to update the optimal policy for future subscribers. Over three years, our approach had a net-positive revenue impact in the range of $4-5 million compared to the status quo. ",Targeting for long-term outcomes
10,1323323061370724352,117233133,Horace He,"['Excited about new paper (w/ @qhwang3 , @abaesingh, @sernamlim, @austinbenson): <LINK>\n\nWe show that by combining label propagation with simple models, we can often match or outperform SOTA GNNs at node tasks, usually at a fraction of the parameters/runtime! <LINK>', ""Label Propagation (LP) used to be the standard method for semi-supervised node classification. Yet, in recent years, GNNs that don't directly use labels have become the standard.\n\nHowever, LP still performs quite well. For example, on ogbn-arxiv, it achieves 68% vs 73% for SOTA. https://t.co/LQ31yGsf4l"", ""This is remarkable, considering LP uses no features at all.\n\nNow, let's take a model that *only* uses features but no graph information: the MLP. It achieves 55% on Arxiv.\n\nThese approaches use orthogonal sources of info!\n\nWhat if... we combined them?"", 'Presenting Correct and Smooth (C&amp;S), which consists of 2 standard label propagation variants applied as a postprocessing step. \n \nAs it turns out, you get very good performance when combining C&amp;S with simple models, often matching SOTA GNN performance! https://t.co/iJWeWgFerE', 'In addition, we don\'t require the graph during training,  substantially improving runtime and scalability.\n\n""For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time."" https://t.co/AR55tVWxms', 'Although we focus on applying C&amp;S to simple models, you can apply these steps to any model, including GNN models!\n\nThis allowed us to achieve SOTA on OGBN-Arxiv at time of submission. I note that all current submissions above 73% are directly using label information. https://t.co/A9QuWSkTew', ""In addition, our framework has another practical advantage. Since C&amp;S doesn't learn any parameters, why don't we just ... use validation labels at test time? \n\nThis further improves our performance! https://t.co/N9ylA0M5Cx"", 'Equal contribution with @qhwang3, with substantial contributions from @abaesingh as well. Advised by the great @austinbenson  :)\n\nAlso thanks to the rest of @cuai_cornell for discussion,  as well as @facebookai for generously funding our research!', 'Also, code for OGB results can all be found here.\n\nhttps://t.co/nAcsoeKVVf', ""One more thing I'd like to say: This paper does *not* replace GNNs or show that GNNs are useless. For example, our approach will not work for link prediction nor graph classification, only homophilic node classification.\n\nHowever, these are the most common graph benchmarks!"", 'A very relevant blog post is: ""Do we need deep graph neural networks"" (https://t.co/LBmufgT1zp), from @mmbronstein. It\'s not that graph neural networks *never* benefit from depth, it\'s that the tasks we often test them on don\'t seem to benefit from depth.', 'Also, for a higher level view of this paper and how it fits in, see this thread from our advisor @austinbenson.\n\nhttps://t.co/LqulfdJ4fs']",https://arxiv.org/abs/2010.13993,"Graph Neural Networks (GNNs) are the predominant technique for learning over graphs. However, there is relatively little understanding of why GNNs are successful in practice and whether they are necessary for good performance. Here, we show that for many standard transductive node classification benchmarks, we can exceed or match the performance of state-of-the-art GNNs by combining shallow models that ignore the graph structure with two simple post-processing steps that exploit correlation in the label structure: (i) an ""error correlation"" that spreads residual errors in training data to correct errors in test data and (ii) a ""prediction correlation"" that smooths the predictions on the test data. We call this overall procedure Correct and Smooth (C&S), and the post-processing steps are implemented via simple modifications to standard label propagation techniques from early graph-based semi-supervised learning methods. Our approach exceeds or nearly matches the performance of state-of-the-art GNNs on a wide variety of benchmarks, with just a small fraction of the parameters and orders of magnitude faster runtime. For instance, we exceed the best known GNN performance on the OGB-Products dataset with 137 times fewer parameters and greater than 100 times less training time. The performance of our methods highlights how directly incorporating label information into the learning algorithm (as was done in traditional techniques) yields easy and substantial performance gains. We can also incorporate our techniques into big GNN models, providing modest gains. Our code for the OGB results is at this https URL ","Combining Label Propagation and Simple Models Out-performs Graph Neural
  Networks"
11,1323298445097054208,112053784,Kaley Brauer üí´,"['new paper to kick off #supernovaember! üéâüéâüéâ\n\nWhere‚Äôd that gold in your jewelry come from? Many people think neutron stars ü§∑\u200d‚ôÄÔ∏è but we‚Äôre here to say don‚Äôt discount collapsars (esp when talking about the oldest gold in the universe)! üí•\n<LINK>\n\npaper in a picture: <LINK>', 'blatantly stealing #supernovaember hashtag from @sanjanacurtis &amp; @YoniAstro cause the timing was far too good', '@sanjanacurtis @alexanderpji @annafrebel @mrdrout hahah IKR everyone so quick to forget about CCSN, the disrespect üò™', ""@YoniAstro @sanjanacurtis you're not wrong, it's a big boom ü§∑\u200d‚ôÄÔ∏è"", '@AstroBarker @alexanderpji @annafrebel @mrdrout Called out by Twitter üò∞üò∞', '@expwnential @alexanderpji @annafrebel @mrdrout https://t.co/H2mN3aILy4', 'also it‚Äôs a really rough time and gotta celebrate the little wins when they come, so here‚Äôs my family when we had a mini publication party! feat. my brother who doesn‚Äôt drink toasting with oreos ü•Ç https://t.co/3LCtMqMvg0', '@syndamn chocolate creme oreo! definitely good but for those wishing their oreo was more chocolatey, fudge covered classic is the superior choice', '@QuantumEmanuel @alexanderpji @annafrebel @mrdrout thanks Emanuel!!! üòä']",http://arxiv.org/abs/2010.15837,"It is unclear if neutron star mergers can explain the observed r-process abundances of metal-poor stars. Collapsars, defined here as rotating massive stars whose collapse results in a rapidly accreting disk around a black hole that can launch jets, are a promising alternative. We find that we can produce a self-consistent model in which a population of collapsars with stochastic europium yields synthesizes all of the r-process material in metal-poor ([Fe/H] < -2.5) stars. Our model reproduces the observed scatter and evolution of scatter of [Eu/Fe] abundances. We find that if collapsars are the dominant r-process site for metal-poor stars, r-process synthesis may be linked to supernovae that produce long gamma-ray bursts. Our results also allow for the possibility that core-collapse supernovae beyond those that launch gamma-ray bursts also produce r-process material (e.g., potentially a subset of Type Ic-BL supernovae). Furthermore, we identify collapsar jet properties (isotropic energy, engine luminosity, or engine time) which may trace r-process yield and verify that the amount of r-process yield produced per collapsar in our model (~0.07 Msun) is consistent with other independent estimates. In the future, achieving 0.05 dex precision on distribution scatter or a reliable selection function would further constrain our probe of r-process production. Our model would also hold for another prompt r-process site with a power-law yield, and work is needed to determine if, for example, fast-merging neutron stars can also explain abundance scatter. ","Collapsar R-Process Yields Can Reproduce [Eu/Fe] Abundance Scatter in
  Metal-Poor Stars"
12,1323216837648277506,560983300,Briland Hitaj,"['Can adversarial examples be of any good use? Well yes! Have a look at our new paper ""Capture the Bot"", a joint work with @dorjanhitaj93 , Sushil Jajodia, and Luigi V. Mancini, <LINK> #DeepLearning #Security #MachineLearning']",https://arxiv.org/abs/2010.16204,"To this date, CAPTCHAs have served as the first line of defense preventing unauthorized access by (malicious) bots to web-based services, while at the same time maintaining a trouble-free experience for human visitors. However, recent work in the literature has provided evidence of sophisticated bots that make use of advancements in machine learning (ML) to easily bypass existing CAPTCHA-based defenses. In this work, we take the first step to address this problem. We introduce CAPTURE, a novel CAPTCHA scheme based on adversarial examples. While typically adversarial examples are used to lead an ML model astray, with CAPTURE, we attempt to make a ""good use"" of such mechanisms. Our empirical evaluations show that CAPTURE can produce CAPTCHAs that are easy to solve by humans while at the same time, effectively thwarting ML-based bot solvers. ","Capture the Bot: Using Adversarial Examples to Improve CAPTCHA
  Robustness to Bot Attacks"
13,1323180080030347265,1511333281,Marika Kieferova,['Our new paper shows that quantum neural networks are difficult to train if there is too much entanglement between visible and hidden units.  <LINK>'],https://arxiv.org/abs/2010.15968,"We argue that an excess in entanglement between the visible and hidden units in a Quantum Neural Network can hinder learning. In particular, we show that quantum neural networks that satisfy a volume-law in the entanglement entropy will give rise to models not suitable for learning with high probability. Using arguments from quantum thermodynamics, we then show that this volume law is typical and that there exists a barren plateau in the optimization landscape due to entanglement. More precisely, we show that for any bounded objective function on the visible layers, the Lipshitz constants of the expectation value of that objective function will scale inversely with the dimension of the hidden-subsystem with high probability. We show how this can cause both gradient descent and gradient-free methods to fail. We note that similar problems can happen with quantum Boltzmann machines, although stronger assumptions on the coupling between the hidden/visible subspaces are necessary. We highlight how pretraining such generative models may provide a way to navigate these barren plateaus. ",Entanglement Induced Barren Plateaus
14,1323094149470412800,1302683575096025088,Motoki Osada,['New nickelate paper is out on arXiv. We found the phase diagram of Pr-based nickelate thin films.\nPrÁ≥ª„Éã„ÉÉ„Ç±„É´ÈÖ∏ÂåñÁâ©ËñÑËÜú„ÅÆË∂Ö‰ºùÂ∞éÁõ∏„ÇíÂêåÂÆö„Åó„ÅüË´ñÊñá„ÇíarXiv„Å´Ëºâ„Åõ„Åæ„Åó„Åü„ÄÇ\n<LINK>'],https://arxiv.org/abs/2010.16101,"We report the phase diagram of infinite layer Pr$_{1-x}$Sr$_{x}$NiO$_2$ thin films synthesized via topotactic reduction from the perovskite precursor phase using CaH$_2$. Based on the electrical transport properties, we find a doping-dependent superconducting dome extending between $x$ = 0.12 and 0.28, with a maximum superconducting transition temperature $T_{\rm{c}}$ of 14 K at $x$ = 0.18, bounded by weakly insulating behavior on both sides. In contrast to the narrower dome observed in Nd$_{1-x}$Sr$_{x}$NiO$_2$, a local $T_{\rm{c}}$ suppression near $x$ = 0.2 was not observed for the Pr$_{1-x}$Sr$_{x}$NiO$_2$ system. Normal state Hall effect measurements indicate mixed carrier contributions of both electrons and holes, and show a sign change in the Hall coefficient as functions of temperature and $x$, quite similar to that in Nd$_{1-x}$Sr$_{x}$NiO$_2$. Also similar is the observation of a minimum in the normal state resistivity associated with the superconducting compositions. These findings indicate an infinite layer nickelate phase diagram that is relatively insensitive to the rare-earth element, but suggest that disorder arising from the variations of the ionic radii on the rare-earth site affects the superconducting dome. ","Phase Diagram of Infinite Layer Praseodymium Nickelate
  Pr$_{1-x}$Sr$_{x}$NiO$_2$ Thin Films"
15,1323078447787315200,983857052840636417,Rob Corless,"['For those interested in AI in math education, @rotmanphilo maybe, please find my new paper with @JamesHDavenport and @euniceyschan and others at <LINK> . We talk about our respective theory and practice; in some ways old-school, the paper is formed by experience.']",https://arxiv.org/abs/2010.16300,"Over the past thirty years or so the authors have been teaching various programming for mathematics courses at our respective Universities, as well as incorporating computer algebra and numerical computation into traditional mathematics courses. These activities are, in some important ways, natural precursors to the use of Artificial Intelligence in Mathematics Education. This paper reflects on some of our course designs and experiences and is therefore a mix of theory and practice. Underlying both is a clear recognition of the value of computer programming for mathematics education. We use this theory and practice to suggest good techniques for and to raise questions about the use of AI in Mathematics Education. ",Teaching Programming for Mathematical Scientists
16,1322558302543212545,980073199332282369,Toshihiko Yamasaki,"['Our new arxiv paper on video representation.\n\nSelf-Supervised Video Representation Using Pretext-Contrastive Learning\nAuthors: Li Tao, Xueting Wang, Toshihiko Yamasaki\n\n<LINK>']",https://arxiv.org/abs/2010.15464,"Recently, pretext-task based methods are proposed one after another in self-supervised video feature learning. Meanwhile, contrastive learning methods also yield good performance. Usually, new methods can beat previous ones as claimed that they could capture ""better"" temporal information. However, there exist setting differences among them and it is hard to conclude which is better. It would be much more convincing in comparison if these methods have reached as closer to their performance limits as possible. In this paper, we start from one pretext-task baseline, exploring how far it can go by combining it with contrastive learning, data pre-processing, and data augmentation. A proper setting has been found from extensive experiments, with which huge improvements over the baselines can be achieved, indicating a joint optimization framework can boost both pretext task and contrastive learning. We denote the joint optimization framework as Pretext-Contrastive Learning (PCL). The other two pretext task baselines are used to validate the effectiveness of PCL. And we can easily outperform current state-of-the-art methods in the same training manner, showing the effectiveness and the generality of our proposal. It is convenient to treat PCL as a standard training strategy and apply it to many other works in self-supervised video feature learning. ","Pretext-Contrastive Learning: Toward Good Practices in Self-supervised
  Video Representation Leaning"
17,1322386130558353410,59962128,Satoshi Matsuoka,"['We have a new paper ""Matrix Engines for High Performance Computing: A Paragon of Performance or Grasping at Straws?"" is now on ArXiv &amp; also submitted to a refereed venue. We realize the controversy of the topic and welcome input, positive &amp; negative! <LINK>']",https://arxiv.org/abs/2010.14373,"Matrix engines or units, in different forms and affinities, are becoming a reality in modern processors; CPUs and otherwise. The current and dominant algorithmic approach to Deep Learning merits the commercial investments in these units, and deduced from the No.1 benchmark in supercomputing, namely High Performance Linpack, one would expect an awakened enthusiasm by the HPC community, too. Hence, our goal is to identify the practical added benefits for HPC and machine learning applications by having access to matrix engines. For this purpose, we perform an in-depth survey of software stacks, proxy applications and benchmarks, and historical batch job records. We provide a cost-benefit analysis of matrix engines, both asymptotically and in conjunction with state-of-the-art processors. While our empirical data will temper the enthusiasm, we also outline opportunities to misuse these dense matrix-multiplication engines if they come for free. ","Matrix Engines for High Performance Computing:A Paragon of Performance
  or Grasping at Straws?"
18,1322358625201913856,2577596593,Chelsea Finn,"['Reinforcement learning can lead to behaviors that don‚Äôt generalize.\n\nIn our #NeurIPS2020 paper, we introduce a simple idea to allow extrapolation to new envs w/ a few trials:\n\nOne Solution is Not All You Need\n<LINK>\nw. Saurabh Kumar, Aviral Kumar, @svlevine\n(1/3) <LINK>', ""The key idea is to learn a *diverse* set of policies that all solve the task.\n\nWhen the environment changes locally, one of these policies is likely to be successful, even if the change wasn't seen during training.\n(2/3) https://t.co/QteHyYWzwU"", 'We refer to this idea as structured maximum entropy RL, since the solutions need to be diverse, but controllable using a latent variable.\n\nCheck out the paper for the theoretical &amp; empirical results and analysis!\n\n(3/3) https://t.co/ut7sTo8hqm']",https://arxiv.org/abs/2010.14484,"While reinforcement learning algorithms can learn effective policies for complex tasks, these policies are often brittle to even minor task variations, especially when variations are not explicitly provided during training. One natural approach to this problem is to train agents with manually specified variation in the training task or environment. However, this may be infeasible in practical situations, either because making perturbations is not possible, or because it is unclear how to choose suitable perturbation strategies without sacrificing performance. The key insight of this work is that learning diverse behaviors for accomplishing a task can directly lead to behavior that generalizes to varying environments, without needing to perform explicit perturbations during training. By identifying multiple solutions for the task in a single environment during training, our approach can generalize to new situations by abandoning solutions that are no longer effective and adopting those that are. We theoretically characterize a robustness set of environments that arises from our algorithm and empirically find that our diversity-driven approach can extrapolate to various changes in the environment and task. ","One Solution is Not All You Need: Few-Shot Extrapolation via Structured
  MaxEnt RL"
19,1322347361943760899,97205267,Vivek Natarajan,"['Our new work ""Addressing the real world class imbalance problem in dermatology""- <LINK> \nhas been accepted as a full paper at #NeurIPS2020 ML4H workshop!\n\nWork led by brilliant @ckbjimmy with @GoogleHealth /Brain team mates Jon Deaton, @gamaleldinfe and Yuan Liu <LINK>', 'Class imbalance is one of the most challenging aspects of building ML models to predict dermatological conditions from images.\n\nThere are 1000s of skin conditions yet datasets are dominated by top-10 skin conditions sometimes making up almost 90% of the training data.', 'Yet it is extremely important for the model to be able to perform well on the rare classes to have practical real world utility. \n\nSome of these rare classes also tend to be malignant like melanoma.', 'While more data is always a solution, it is often extremely hard to get data corresponding to conditions with rare real world prevalence \n\nHence, the model needs to be able to learn from few representative samples efficiently while being in this class imbalance setting.', 'Towards that, in this work, we did a comprehensive comparison of several common conventional supervised learning techniques (CSL) and few shot learning techniques (FSL) to address this class imbalance problem in this dermatology setting. https://t.co/UYHaxTphKH', 'However FSL train/eval strategy is quite contrived (n-way k-shot) assuming \n\n- disjoint classes between train/test sets\n- equal number of samples per class \n\nwhich makes it not viable to enable fair comparisons in our problem setup.\n\nHence, we design a new train/eval strategy. https://t.co/rqInWN0VaT', 'Using this new training/eval strategy, we find that conventional CSL techniques like focal loss and inverse frequency weighting actually outperform the best FSL technique (matching networks) even when only looking at the rare classes. https://t.co/SOR1ImYSqH', 'However, we find that by using a combination of FSL and CSL models in a diverse ensemble, we are able to improve on top of  a pure CSL-only ensemble for both rare and all classes. https://t.co/Z1LructIl5', 'Our experiments suggests that the few shot learning techniques we evaluated may have limited utility for problems like class imbalance on their own. \n\nWe need train/eval setups that mirror the real world use cases better if we want to accelerate progress here.', 'Concurrent to our work, there is another ICLR submission - https://t.co/VHFEIw4xGB \n\nwhich investigated this holistically with experiments over multiple datasets and came to similar conclusions.\n\nHopefully, these studies are useful data points for the few shot learning community. https://t.co/m5pqdt28LC']",https://arxiv.org/abs/2010.04308,"Class imbalance is a common problem in medical diagnosis, causing a standard classifier to be biased towards the common classes and perform poorly on the rare classes. This is especially true for dermatology, a specialty with thousands of skin conditions but many of which have low prevalence in the real world. Motivated by recent advances, we explore few-shot learning methods as well as conventional class imbalance techniques for the skin condition recognition problem and propose an evaluation setup to fairly assess the real-world utility of such approaches. We find the performance of few-show learning methods does not reach that of conventional class imbalance techniques, but combining the two approaches using a novel ensemble improves model performance, especially for rare classes. We conclude that ensembling can be useful to address the class imbalance problem, yet progress can further be accelerated by real-world evaluation setups for benchmarking new methods. ",Addressing the Real-world Class Imbalance Problem in Dermatology
20,1322322526811422721,461482865,Ivan Kryven,"['""Contact tracing in configuration models""\nNew paper with @clara_stegehuis out on arXiv: <LINK>\n\nWe study how configuration model for networks responds to an intervention with contact tracing. <LINK>']",https://arxiv.org/abs/2010.05590,"Quarantining and contact tracing are popular ad hoc practices for mitigating epidemic outbreaks. However, few mathematical theories are currently available to asses the role of a network in the effectiveness of these practices. In this paper, we study how the final size of an epidemic is influenced by the procedure that combines contact tracing and quarantining on a network null model: the configuration model. Namely, we suppose that infected vertices may self-quarantine and trace their infector with a given success probability. A traced infector is, in turn, less likely to infect others. We show that the effectiveness of such tracing process strongly depends on the network structure. In contrast to previous findings, the tracing procedure is not necessarily more effective on networks with heterogeneous degrees. We also show that network clustering influences the effectiveness of the tracing process in a non-trivial way: depending on the infectiousness parameter, contact tracing on clustered networks may either be more, or less efficient than on network without clustering. ",Contact tracing in configuration models
21,1322285065322196992,961763584651935744,Thao Nguyen,"['Do wide and deep neural networks learn the same thing? In a new paper (<LINK>) with @maithra_raghu and @skornblith we study how width and depth affect learned representations within and across models trained on CIFAR and ImageNet. 1/6', 'Scaling NNs by increasing depth &amp; width has been a successful approach for obtaining high performance across many tasks. To study representations of these models, we develop an efficient way to compute centered kernel alignment (CKA), a representation similarity measure. 2/6', 'We find that in large (wide and/or deep) models, a characteristic block structure emerges in the model layer representations. By varying training dataset size, we show that the block structure emerges when model capacity is large relative to the size of the training set. 3/6 https://t.co/KZBSzNPJG5', 'Through further analysis, we show that the block structure arises from the preservation and propagation of the first principal component of its layer representations, with clear ramifications for linear probe accuracy, and collapsing the constituent layers. 4/6 https://t.co/3RtrbtiEw2', 'Across models of different random initializations and configurations, representations outside the block structure are often similar, but the block structure is unique to each model. 5/6 https://t.co/zXAoaV9GgW', 'Finally, we study effects of depth and width on model outputs: on common datasets like CIFAR-10 and ImageNet, wide and deep models make systematically different mistakes at both instance level and class level. 6/6 https://t.co/AXyweyLyXy']",https://arxiv.org/abs/2010.15327,"A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes. ","Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural
  Network Representations Vary with Width and Depth"
22,1322258921420017664,2235411914,Surya Ganguli,['Our new #neurips2020 paper combines geometry and dynamics to reveal a rapid universal chaotic to stable transition in deep learning dynamics where in a few epochs the final loss basin is determined and the neural tangent kernel rapidly evolves and improves <LINK> <LINK>'],http://arxiv.org/abs/2010.15110,"In suitably initialized wide networks, small learning rates transform deep neural networks (DNNs) into neural tangent kernel (NTK) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization. Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent NTK. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and NTK dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process. In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the NTK changes rapidly, learning useful features from the training data that enables it to outperform the standard initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the NTK changes at constant velocity, and its performance matches that of full network training in 15% to 45% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning. ","Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the Neural Tangent Kernel"
23,1322246600320757760,40285266,Stanislav Fort at EAGx Prague ¬¨(üî•üìéüî•üìé),"['Excited to share our new #neurips2020 paper /Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel/ (<LINK>) with @KDziugaite, Mansheej, @SKharaghani, @roydanroy, @SuryaGanguli 1/6 <LINK>', 'We Taylor-expand Deep Neural Network logits with respect to their weights at different stages of training &amp; study how well a linearized network trains based on at which epoch it was expanded. Early expansions train poorly, but even slightly into training they do very well! 2/6 https://t.co/MQsQY0HcqB', 'Linearized DNNs underperform compared to even low learning rate trained nonlinear networks, but only for expansions /very early/ in training. We call this the *nonlinear advantage* and show that it disappears quickly into training. 3/6 https://t.co/RDZNrvQ1yV', 'Surprisingly, the nonlinear advantage a DNN enjoys over its linearized counterpart seems to correlate well with the error barrier (=instability in https://t.co/WQX1IWComr @jefrankle @mcarbin) between 2 NNs trained from that point, connecting two very different concepts. 4/6 https://t.co/mcnnyKbyvh', 'We find that many other DNN measures such as function space distance (similar to https://t.co/dIJCZfMHZi @balajiln), kernel distance, logit gradient similarity (similar to https://t.co/Ow3F7RFh2t) and others correlate in a similar manner. 5/6 https://t.co/dOFGPL1l4f', 'It seems that the importance of the nonlinear nature of deep neural nets is crucial at the beginning of training, but diminishes relatively soon after that, where Taylor expanded DNNs (even 1st order, in the ‚àû-width regime=Neural Tangent Kernel) perform almost equally well. 6/6', 'This was joint work with an amazing team of @KDziugaite @mansiege @SKharaghani, @roydanroy, @SuryaGanguli']",http://arxiv.org/abs/2010.15110,"In suitably initialized wide networks, small learning rates transform deep neural networks (DNNs) into neural tangent kernel (NTK) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization. Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent NTK. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and NTK dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process. In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the NTK changes rapidly, learning useful features from the training data that enables it to outperform the standard initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the NTK changes at constant velocity, and its performance matches that of full network training in 15% to 45% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning. ","Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the Neural Tangent Kernel"
24,1322211983161094145,29000998,Debashis Ghosh,"['For the statisticians in the audience, wanted to quickly discuss a new paper: <LINK>', 'This deals with a field called sufficient dimension reduction.     It started in the 1980s and 1990s and had this seemingly restrictive condition called the linearity condition.', 'Lots of methods were developed in the 1990s and early 2000s for sufficient dimension reduction (SDR).  They required the linearity condition.    More recently, people have moved into nonlinear SDR methods, primarily using kernels, which are popular in support vector machines.', 'Our paper shows that the linearity condition induces kernels just like those in the nonlinear SDR literature.  This involves using 100-year old math results (my favorite kind).', 'This is joint work with my co-author Youngjoo Cho (not on Twitter), who is at the University of Texas, El Paso.']",https://arxiv.org/abs/2010.15009,"There has been a lot of interest in sufficient dimension reduction (SDR) methodologies as well as nonlinear extensions in the statistics literature. In this note, we use classical results regarding metric spaces and positive definite functions to link linear SDR procedures to their nonlinear counterparts. ",Bridging linearity-based and kernel-based sufficient dimension reduction
25,1322139381746995205,479745680,Valeri,"['New cosmological forecast paper today on arxiv: <LINK>. Senwen Deng, a master student at ENS (Paris) did an amazing job on this.']",https://arxiv.org/abs/2010.15822,"Single-field models of $\alpha$-attractor quintessential inflation provide a unified picture of the two periods of early- and late-time cosmic acceleration, where both inflation and dark energy are described by a single scalar degree of freedom rolling down a runaway potential. These theoretically well-motivated models have distinct observational predictions that are in agreement with existing cosmological data. We show that the next generation of large-scale structure surveys, even when no other cosmological data sets are considered, will strongly constrain the parameter space of these models, and test them against the standard cosmological model and more conventional non-quintessential inflation. In particular, we expect $\mathcal{O}(10^{-5}\mathrm{-}10^{-4})$ constraints on the present values of the dark energy equation of state and its time derivative, $w_0$ and $w_a$. We also forecast more than one order of magnitude tighter constraints on the spectral index of primordial curvature perturbations $n_s$ compared to the expectations for the standard model. This demonstrates the powerful synergy between the upcoming large-scale structure probes of inflation and those aiming to measure the tensor-to-scalar ratio $r$ through the observation of $B$-mode polarization of the cosmic microwave background. ","Quintessential $\alpha$-attractor inflation: forecasts for Stage IV
  galaxy surveys"
26,1322126489496756230,423315224,Yin Cao,"['Our new sound event localization and detection (SELD) paper <LINK>, also submitted to ICASSP2021, with code released <LINK>\n\nWe view SELD from a source separation and a multi-task learning points of view.\n\n#audio #soundsourcelocalization']",https://arxiv.org/abs/2010.13092,"Polyphonic sound event localization and detection (SELD), which jointly performs sound event detection (SED) and direction-of-arrival (DoA) estimation, detects the type and occurrence time of sound events as well as their corresponding DoA angles simultaneously. We study the SELD task from a multi-task learning perspective. Two open problems are addressed in this paper. Firstly, to detect overlapping sound events of the same type but with different DoAs, we propose to use a trackwise output format and solve the accompanying track permutation problem with permutation-invariant training. Multi-head self-attention is further used to separate tracks. Secondly, a previous finding is that, by using hard parameter-sharing, SELD suffers from a performance loss compared with learning the subtasks separately. This is solved by a soft parameter-sharing scheme. We term the proposed method as Event Independent Network V2 (EINV2), which is an improved version of our previously-proposed method and an end-to-end network for SELD. We show that our proposed EINV2 for joint SED and DoA estimation outperforms previous methods by a large margin, and has comparable performance to state-of-the-art ensemble models. ","An Improved Event-Independent Network for Polyphonic Sound Event
  Localization and Detection"
27,1322081311721480192,21902101,Jim Geach,"['New paper on arXiv today @MattJDoherty et al. ‚Äú[NII] fine-structure emission at 122 and 205um in a galaxy at z=2.6: a globally dense star-forming interstellar medium‚Äù accepted in ApJ\n\n(@almaobs follow-up of the @SpaceWarps ‚Äòred radio ring‚Äô @chrislintott) \n\n<LINK>', '@chrislintott @MattJDoherty @almaobs @SpaceWarps @the_zooniverse Sure!']",https://arxiv.org/abs/2010.15128,"We present new observations with the Atacama Large Millimeter/sub-millimeter Array of the 122um and 205um fine-structure line emission of singly-ionised nitrogen in a strongly lensed starburst galaxy at z=2.6. The 122/205um [NII] line ratio is sensitive to electron density, n_e, in the ionised interstellar medium, and we use this to measure n_e~300cm^-3 averaged across the galaxy. This is over an order of magnitude higher than the Milky Way average, but comparable to localised Galactic star-forming regions. Combined with observations of the atomic carbon (CI(1-0)) and carbon monoxide (CO(4-3)) in the same system, we reveal the conditions in this intensely star-forming system. The majority of the molecular interstellar medium has been driven to high density, and the resultant conflagration of star formation produces a correspondingly dense ionised phase, presumably co-located with myriad HII regions that litter the gas-rich disk. ","[NII] fine-structure emission at 122 and 205um in a galaxy at z=2.6: a
  globally dense star-forming interstellar medium"
28,1322066901149757447,1226265894495539200,Anirudh Joshi,['Excited to share our new work #GloFlow that was accepted to Neurips ML4H. We see this technology as a way to lower the barrier to slide digitization in pathology and biology. \nPartner with us to pilot this technology : <LINK>\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2010.15269,"The application of deep learning to pathology assumes the existence of digital whole slide images of pathology slides. However, slide digitization is bottlenecked by the high cost of precise motor stages in slide scanners that are needed for position information used for slide stitching. We propose GloFlow, a two-stage method for creating a whole slide image using optical flow-based image registration with global alignment using a computationally tractable graph-pruning approach. In the first stage, we train an optical flow predictor to predict pairwise translations between successive video frames to approximate a stitch. In the second stage, this approximate stitch is used to create a neighborhood graph to produce a corrected stitch. On a simulated dataset of video scans of WSIs, we find that our method outperforms known approaches to slide-stitching, and stitches WSIs resembling those produced by slide scanners. ","GloFlow: Global Image Alignment for Creation of Whole Slide Images for
  Pathology from Video"
29,1322012306906271744,717162062837719040,Phil Armitage,"['New paper! In work led by @sraymond_astro, with Nathan Kaib and @jjfplanet, we quantify how planetesimals ejected from the Solar System differ from those that survive as small bodies in reservoirs such as the Kuiper belt and Oort Cloud.\n\n<LINK>', ""Interstellar objects are the motivation. Only two are known. 'Oumuamua was small, irregularly shaped, and generally weird. Borisov was pretty boringly similar to Solar System comets. A sample of two is a thin gruel, but it's better than one! And there will be (many) more."", ""The unexpected properties of 'Oumuamua inspired many novel ideas. It might be a hydrogen iceberg (Seligman &amp; Laughlin), an ultra-porous aggregate (Moro-Martin), a planet tidally disrupted by a star or compact object (Cuk, Rafikov, Zhang and Lin), etc..."", 'To assess these possibilities, our goal was to understand the null hypothesis: what would planetesimals ejected from Solar System-like planetary systems look like? To do this, we tracked the fates of ~18,000 planetesimals in constrained realizations of the early Solar System.', ""We were particularly interested in planetesimals that passed so close to planets that they would be tidally disrupted (and shredded into small pieces), or which got close enough to the Sun that their surfaces might have dried out. These could be 'Oumuamua-like."", 'The result? The frequency of volatile loss is far higher for ejected planetesimals than for surviving ones. Even if all interstellar objects were ejected from Solar System-like systems, their physical properties should be more diverse than those of Solar System survivors.', 'Of course, we know that not all planetary systems resemble the Solar System. As more interstellar objects are found, the goal will be to identify both the truly primordial bodies, and those that have been altered, and link them to known exoplanet populations.']",https://arxiv.org/abs/2010.15147,"The orbital architecture of the Solar System is thought to have been sculpted by a dynamical instability among the giant planets. During the instability a primordial outer disk of planetesimals was destabilized and ended up on planet-crossing orbits. Most planetesimals were ejected into interstellar space but a fraction were trapped on stable orbits in the Kuiper belt and Oort cloud. We use a suite of N-body simulations to map out the diversity of planetesimals' dynamical pathways. We focus on two processes: tidal disruption from very close encounters with a giant planet, and loss of surface volatiles from repeated passages close to the Sun. We show that the rate of tidal disruption is more than a factor of two higher for ejected planetesimals than for surviving objects in the Kuiper belt or Oort cloud. Ejected planetesimals are preferentially disrupted by Jupiter and surviving ones by Neptune. Given that the gas giants contracted significantly as they cooled but the ice giants did not, taking into account the thermal evolution of the giant planets decreases the disruption rate of ejected planetesimals. The frequency of volatile loss and extinction is far higher for ejected planetesimals than for surviving ones and is not affected by the giant planets' contraction. Even if all interstellar objects were ejected from Solar System-like systems, our analysis suggests that their physical properties should be more diverse than those of Solar System small bodies as a result of their divergent dynamical histories. This is consistent with the characteristics of the two currently-known interstellar objects. ","Survivor bias: divergent fates of the Solar System's ejected vs.
  persisting planetesimals"
30,1321990856644190208,874847778119266304,Vaishnavh Nagarajan,"['‚ÄúUnderstanding the failure modes of out-of-distribution generalization‚Äù, new paper w/ @bneyshabur and @AJAndreassen at Google \n\n<LINK>\n\nWe explain why classifiers rely on spurious correlations (e.g. bkgd.) that hold only in training. 1/ <LINK>', 'On the face of it, it seems intuitive that classifiers would use spurious correlations. But really, this isn‚Äôt so straightforward: why‚Äôd the classifier use the weak bkgd. correlation even when the object features are 100% informative of the label? 2/ https://t.co/7Z5kjxO0PS', 'We uncover two complementary modes by which classifiers fall into this trap: a geometric mode (occurs in both GD &amp; max-margin due to dataset geometry) and a statistical mode (occurs only in GD, but even on datasets w trivial geometries) 3/ https://t.co/z7wY8VShLy', 'We theoretically demonstrate these modes in linear classifiers, but also empirically demonstrate them in modern neural network settings (including in settings where there is no spurious correlation in the standard sense) 4/ https://t.co/rMLfJXyz7D', 'Crucially, these failure modes are general/fundamental failure modes bec they occur even in the easiest of tasks! In more complex tasks, other \n‚Äúless fundamental‚Äù failure modes would certainly arise, some of which we enumerate on the way. 5/ https://t.co/Nh5nTyGIG7', 'Bottomline: the ‚Äúclassifiers use spurious correlations‚Äù phenomenon is more nuanced than we think it is -- it happens via many distinct mechanisms. Hopefully, this insight can guide us to more robust algorithms e.g., does each failure mode demand a different solution technique? 6/']",http://arxiv.org/abs/2010.15775,"Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor accuracy during test-time. In this work, we identify the fundamental factors that give rise to this behavior, by explaining why models fail this way {\em even} in easy-to-learn tasks where one would expect these models to succeed. In particular, through a theoretical study of gradient-descent-trained linear classifiers on some easy-to-learn tasks, we uncover two complementary failure modes. These modes arise from how spurious correlations induce two kinds of skews in the data: one geometric in nature, and another, statistical in nature. Finally, we construct natural modifications of image classification datasets to understand when these failure modes can arise in practice. We also design experiments to isolate the two failure modes when training modern neural networks on these datasets. ",Understanding the Failure Modes of Out-of-Distribution Generalization
31,1321841252954824704,2279227387,Dr. Jessie Christiansen,"[""Breaking news - it's paper day!!! \n\nOur new eta-Earth calculation from the Kepler team ‚Äì punchline is that we find eta_Earth is 40-60% for the conservative habitable zone. \n\n<LINK>\n\nBUT WHAT DOES THAT MEAN??\n\nDon't worry, I gotchu. Thread follows of course. <LINK>"", 'First off, an Important Caveat ‚Äì this won‚Äôt be our final answer! There are still important corrections missing from this number. But! It‚Äôs the most careful job we‚Äôve done so far. \n\n(Reminder: that‚Äôs how science works!) https://t.co/i2O31qM9Yc', 'Updates in this work!!\n- Uniform stellar parameters from Gaia DR2, using Berger+2020.\n- Improvement probabilistic treatment of reliability\n- Analysis in instellation space, instead of period space (see the Kepler planet candidates in instellation space in the attached figure). https://t.co/RQaHSGgtYv', 'Now, that number. 40-60%!! That says around half of stars like our Sun (middle-aged yellow G stars) have a rocky planet like the Earth in their habitable zone ‚Äì the distance from the star where it‚Äôs not too hot, and not too cold ‚Äì it‚Äôs just right for liquid water on the surface*. https://t.co/P5abrJf7yC', ""*I‚Äôm going to re-introduce the ECHaLWOTS zone here. The habitable zone is truly an ‚Äòif you put the Earth here, it could maintain liquid water‚Äô zone. Not an ‚Äòany planet here could maintain liquid water‚Äô zone!!\n\nHere's @girlandkat explaining: https://t.co/LOfxapVfyL"", '*cough* K2-18 b *cough*', ""But! AROUND HALF?! That‚Äôs huge. That‚Äôs SO MANY potential Earth-like planets out there. \n\nThe Galaxy has ~10 billion stars like the Sun. That means there's something like around FIVE BILLION Earth-like planets just in our Galaxy.\n\nTHAT‚ÄôS WILD. https://t.co/1nI2K6nEZQ"", 'Another important aspect of this number, which is a somewhat higher than previous numbers (~20-50%) is that it could mean there are more nearby Earth-like planets for future large space-based telescopes (like HabEx or LUVOIR) to observe! Which is good.', 'So, to summarize: our new number is slightly higher than previous numbers, includes some new things, and is still missing some things.\n\nHooray! (?! üòÑ )', '@mrtommyb Depends which eta_Earth you take, and which assumptions you use about the habitable zone. Those are the conservative HZ numbers.', '@mrtommyb Steve twisted every knob in combination with every other knob to try to fully explore all the possible scenarios! So anyone can come to the paper with their preferred HZ and find the corresponding eta_Earth. ü§£)', ""@mrtommyb Yes, that's more correctly what they are, but in terms of trying to find a number I'm comfortable quoting from this paper, I'm happy to say 40-60%. ü§£\n\nNote for everyone else: Tom's point is that the 40-60% doesn't include the uncertainties on each of those numbers!"", 'Oh, and here is the official @NASAAmes press release!\n\nhttps://t.co/Xli9niNbTV', ""@david_kipping Yes, the uncertainties include zero. I would be very surprised if the answer WAS zero, given what we know about larger planets at the same instellation, and smaller planets at higher instellation, but it's not ruled out by the data. \n\nI don't know the Bayes factor!"", '@kesterallen Were we not before?!?', '@david_kipping For sure. There‚Äôs just so many numbers in the paper (multiple HZ formulations, multiple stellar samples) I wanted to condense them down a bit for digestion! I should have explained more about the uncertainties though.', '@ExoplanetJJ @NASAAmes Stellar multiplicity is one of the unresolved issues at the moment. The original stellar sample was built from stars that were photometrically single (to 1-2""). This new work uses the Gaia RUWE value to try to clean than up a bit, but it remains to be seen how many it misses.']",https://arxiv.org/abs/2010.14812,"We present occurrence rates for rocky planets in the habitable zones (HZ) of main-sequence dwarf stars based on the Kepler DR25 planet candidate catalog and Gaia-based stellar properties. We provide the first analysis in terms of star-dependent instellation flux, which allows us to track HZ planets. We define $\eta_\oplus$ as the HZ occurrence of planets with radius between 0.5 and 1.5 $R_\oplus$ orbiting stars with effective temperatures between 4800 K and 6300 K. We find that $\eta_\oplus$ for the conservative HZ is between $0.37^{+0.48}_{-0.21}$ (errors reflect 68\% credible intervals) and $0.60^{+0.90}_{-0.36}$ planets per star, while the optimistic HZ occurrence is between $0.58^{+0.73}_{-0.33}$ and $0.88^{+1.28}_{-0.51}$ planets per star. These bounds reflect two extreme assumptions about the extrapolation of completeness beyond orbital periods where DR25 completeness data are available. The large uncertainties are due to the small number of detected small HZ planets. We find similar occurrence rates using both a Poisson likelihood Bayesian analysis and Approximate Bayesian Computation. Our results are corrected for catalog completeness and reliability. Both completeness and the planet occurrence rate are dependent on stellar effective temperature. We also present occurrence rates for various stellar populations and planet size ranges. We estimate with $95\%$ confidence that, on average, the nearest HZ planet around G and K dwarfs is about 6 pc away, and there are about 4 HZ rocky planets around G and K dwarfs within 10 pc of the Sun. ","The Occurrence of Rocky Habitable Zone Planets Around Solar-Like Stars
  from Kepler Data"
32,1321808905442365441,48017871,Andreas Orthey,"['New journal preprint (Submitted to Transactions) on lifting paths from a lower-dimensional space into a higher-dimensional space while keeping them feasible. \n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'We mainly use those methods to plan motions through narrow passages, for example for the Bugtrap scenario or for dexterous manipulation of the @shadowrobot hand. Those are prototypical scenarios for many robotic applications like grasping, assembly, disassembly or egress tasks. https://t.co/55My1ClDcA', 'With @LIS_TUBerlin #MotionPlanning #Robotics #FiberBundles #Relaxation #LiftingPaths #AI #Algorithms']",https://arxiv.org/abs/2010.14524,"Sampling-based planning methods often become inefficient due to narrow passages. Narrow passages induce a higher runtime, because the chance to sample them becomes vanishingly small. In recent work, we showed that narrow passages can be approached by relaxing the problem using admissible lower-dimensional projections of the state space. Those relaxations often increase the volume of narrow passages under projection. Solving the relaxed problem is often efficient and produces an admissible heuristic we can exploit. However, given a base path, i.e. a solution to a relaxed problem, there are currently no tailored methods to efficiently exploit the base path. To efficiently exploit the base path and thereby its admissible heuristic, we develop section patterns, which are solution strategies to efficiently exploit base paths in particular around narrow passages. To coordinate section patterns, we develop the pattern dance algorithm, which efficiently coordinates section patterns to reactively traverse narrow passages. We combine the pattern dance algorithm with previously developed multilevel planning algorithms and benchmark them on challenging planning problems like the Bugtrap, the double L-shape, an egress problem and on four pregrasp scenarios for a 37 degrees of freedom shadow hand mounted on a KUKA LWR robot. Our results confirm that section patterns are useful to efficiently solve high-dimensional narrow passage motion planning problems. ","Section Patterns: Efficiently Solving Narrow Passage Problems in
  Multilevel Motion Planning"
33,1321747934740910081,883039700,Lenka Zdeborova,"['What happens when you hide a Hamiltonian cycle in a random graph, can we find it back? New paper with Gabriele Sicuro brings some answers to this question:   <LINK>', '@biodataclarity In fact, the problem is inspired by DNA assembly as explained in this work: \nhttps://t.co/6y8i8qx0fs', '@biodataclarity In fact, the problem is inspired by DNA assembly as explained in this work: https://t.co/6y8i8qx0fs']",https://arxiv.org/abs/2010.13700,"We consider the problem of recovering an unknown $k$-factor, hidden in a weighted random graph. For $k=1$ this is the planted matching problem, while the $k=2$ case is closely related to the planted travelling salesman problem. The inference problem is solved by exploiting the information arising from the use of two different distributions for the weights on the edges inside and outside the planted sub-graph. We argue that, in the large size limit, a phase transition can appear between a full and a partial recovery phase as function of the signal-to-noise ratio. We give a criterion for the location of the transition. ",The planted $k$-factor problem
34,1321735992689250304,863411754243674112,SebastianGPopescu,"['Delighted to share our new paper on enhanced outlier status propagation through Hierarchical GPs. In <LINK> we provide equivalents of Deep Kernel Learning and Deep Gaussian Processes in Wasserstein-2 space and show they are better at Out-of-Distribution Detection <LINK>', 'work done with @JamesCole_Neuro @Neurosharp @GlockerBen', 'By decomposing a Sparse GP into its parametric and non-parametric components, we can disentangle the two types of uncertainty present in our posterior variance formula. We consider Distributional Uncertainty as a measure of outlier detection.', 'In standard Deep Gaussian Processes, as the number of inducing points is increased the Distributional Variance collapses to zero. In figure 2, we can see outlier points (green) getting progressively more closely mapped to areas where inlier points (blue,red) get mapped to.', 'In DeepGPs, the Distributional Variance is low across input space areas where there is no data because of the PCA mean function. Our Wasserstein-2 space models suffer less from this drawback.(Figure 3)']",https://arxiv.org/abs/2010.14877,"Stacking Gaussian Processes severely diminishes the model's ability to detect outliers, which when combined with non-zero mean functions, further extrapolates low non-parametric variance to low training data density regions. We propose a hybrid kernel inspired from Varifold theory, operating in both Euclidean and Wasserstein space. We posit that directly taking into account the variance in the computation of Wasserstein-2 distances is of key importance towards maintaining outlier status throughout the hierarchy. We show improved performance on medium and large scale datasets and enhanced out-of-distribution detection on both toy and real data. ",Hierarchical Gaussian Processes with Wasserstein-2 Kernels
35,1321640168785403905,133148364,Devendra Chaplot,"['New paper on Unsupervised Domain Adaptation for Visual Navigation! We transfer navigation policies from simulation to the real-world using policy-based image translation.\n\nArxiv:\n<LINK>\n\nwith S. Li, Y.H. Tsai, Y. Wu, L.P. Morency, R. Salakhutdinov @rsalakhu <LINK>']",https://arxiv.org/abs/2010.14543,"Advances in visual navigation methods have led to intelligent embodied navigation agents capable of learning meaningful representations from raw RGB images and perform a wide variety of tasks involving structural and semantic reasoning. However, most learning-based navigation policies are trained and tested in simulation environments. In order for these policies to be practically useful, they need to be transferred to the real-world. In this paper, we propose an unsupervised domain adaptation method for visual navigation. Our method translates the images in the target domain to the source domain such that the translation is consistent with the representations learned by the navigation policy. The proposed method outperforms several baselines across two different navigation tasks in simulation. We further show that our method can be used to transfer the navigation policies learned in simulation to the real world. ",Unsupervised Domain Adaptation for Visual Navigation
36,1321632367312855042,1004126125558304768,CisnerosResearch,['New paper by \u2066@Jorge33838219\u2069 and \u2066@sehr_nk\u2069 in arXiv:\n[2010.14723] Development and Application of QM/MM Methods with Advanced Polarizable Potentials <LINK>'],https://arxiv.org/abs/2010.14723,"Quantum Mechanics/Molecular Mechanics (QM/MM) simulations are a popular approach to study various features of large systems. A common application of QM/MM calculations is in the investigation of reaction mechanisms in condensed-phase and biological systems. The combination of QM and MM methods to represent a system gives rise to several challenges that need to be addressed. The increase in computational speed has allowed the expanded use of more complicated and accurate methods for both QM and MM simulations. Here, we review some approaches that address several common challenges encountered in QM/MM simulations with advanced polarizable potentials, from methods to account for boundary across covalent bonds and long-range effects, to polarization and advanced embedding potentials. ","Development and Application of QM/MM Methods with Advanced Polarizable
  Potentials"
37,1321574699508314117,202420697,Jeff Carver,"['New @JSSoftware paper with Elvira-Maria Arvanitou, Apostolos Ampatzoglou and @AChatzigeorgiou \n\nSoftware Engineering Practices for Scientific Software Development: A Systematic Mapping Study\n\n<LINK>\n\n<LINK>']",https://arxiv.org/abs/2010.09914,"Background: The development of scientific software applications is far from trivial, due to the constant increase in the necessary complexity of these applications, their increasing size, and their need for intensive maintenance and reuse. Aim: To this end, developers of scientific software (who usually lack a formal computer science background) need to use appropriate software engineering (SE) practices. This paper describes the results of a systematic mapping study on the use of SE for scientific application development and their impact on software quality. Method: To achieve this goal we have performed a systematic mapping study on 359 papers. We first describe a catalogue of SE practices used in scientific software development. Then, we discuss the quality attributes of interest that drive the application of these practices, as well as tentative side-effects of applying the practices on qualities. Results: The main findings indicate that scientific software developers are focusing on practices that improve implementation productivity, such as code reuse, use of third-party libraries, and the application of ""good"" programming techniques. In addition, apart from the finding that performance is a key-driver for many of these applications, scientific software developers also find maintainability and productivity to be important. Conclusions: The results of the study are compared to existing literature, are interpreted under a software engineering prism, and various implications for researchers and practitioners are provided. One of the key findings of the study, which is considered as important for driving future research endeavors is the lack of evidence on the trade-offs that need to be made when applying a software practice, i.e., negative (indirect) effects on other quality attributes. ","Software Engineering Practices for Scientific Software Development: A
  Systematic Mapping Study"
38,1321558371175141376,2375680693,John P Dickerson,"['Many matching applications -- advertising, hiring, organ allocation -- operate on uncertain graphs, where vertices and/or edges may not exist.  In our new @NeurIPSConf paper, we give an approach to selecting just a few edges to query prior to ... 1/\n\nLink: <LINK> <LINK>', '‚Ä¶ running a black box matching algorithm.  This is motivated by the kidney exchange use case, where making changes to the matching algorithm itself may be logistically difficult.  We show that even gathering a tiny amount of (the right!) information ‚Ä¶ 2/', '... prior to matching results in big gains once uncertainty is realized.  We show info gathering is theoretically hard; our MCTS-based method works well on real kidney ex data.\n\nJoint work with: @DuncanMcelfresh @michaeljcurry1 Sandholm @SCSatCMU \n\nPaper: https://t.co/CtzbG5uHCK']",https://arxiv.org/abs/2010.12069,"In barter exchanges, participants swap goods with one another without exchanging money; exchanges are often facilitated by a central clearinghouse, with the goal of maximizing the aggregate quality (or number) of swaps. Barter exchanges are subject to many forms of uncertainty--in participant preferences, the feasibility and quality of various swaps, and so on. Our work is motivated by kidney exchange, a real-world barter market in which patients in need of a kidney transplant swap their willing living donors, in order to find a better match. Modern exchanges include 2- and 3-way swaps, making the kidney exchange clearing problem NP-hard. Planned transplants often fail for a variety of reasons--if the donor organ is refused by the recipient's medical team, or if the donor and recipient are found to be medically incompatible. Due to 2- and 3-way swaps, failed transplants can ""cascade"" through an exchange; one US-based exchange estimated that about 85% of planned transplants failed in 2019. Many optimization-based approaches have been designed to avoid these failures; however most exchanges cannot implement these methods due to legal and policy constraints. Instead we consider a setting where exchanges can query the preferences of certain donors and recipients--asking whether they would accept a particular transplant. We characterize this as a two-stage decision problem, in which the exchange program (a) queries a small number of transplants before committing to a matching, and (b) constructs a matching according to fixed policy. We show that selecting these edges is a challenging combinatorial problem, which is non-monotonic and non-submodular, in addition to being NP-hard. We propose both a greedy heuristic and a Monte Carlo tree search, which outperforms previous approaches, using experiments on both synthetic data and real kidney exchange data from the United Network for Organ Sharing. ",Improving Policy-Constrained Kidney Exchange via Pre-Screening
39,1321535778439237636,1012392833834029056,Robin Jia,"['How are active learning, label imbalance, and robustness related? Steve Mussmann, @percyliang, and I explore this in our new Findings of EMNLP paper, ""On the Importance of Adaptive Data Collection for Extremely Imbalanced Pairwise Tasks"" <LINK> . Thread below!', 'Extreme label imbalance often arises naturally in pairwise tasks (e.g., paraphrase detection). This forces train-test mismatch: Training data needs to be balanced for learning, but test data is very imbalanced. Thus, this is inherently a robustness/generalization problem! (2/5)', 'Standard models (BERT &amp; co) trained on standard balanced training data generalize very poorly (~2% average precision) to natural imbalanced test data formed by taking all pairs of utterances--the distribution that actually matters in practice! (3/5)', ""Active learning's contract is to collect non-iid data that leads to generalization on the original distribution--exactly what we need! Uncertainty sampling + cosine similarity model with fine-tuned BERT embeddings greatly improves AP on imbalanced test data (2-&gt;32% on QQP). (4/5)"", 'IMO lots of potential here as a testbed for studying impact of both model architecture and data collection choices on robustness &amp; generalization. Examples are natural (real utterances, we just take all pairs) and correspond to real-world challenge (label imbalance). (5/5)']",https://arxiv.org/abs/2010.05103,"Many pairwise classification tasks, such as paraphrase detection and open-domain question answering, naturally have extreme label imbalance (e.g., $99.99\%$ of examples are negatives). In contrast, many recent datasets heuristically choose examples to ensure label balance. We show that these heuristics lead to trained models that generalize poorly: State-of-the art models trained on QQP and WikiQA each have only $2.4\%$ average precision when evaluated on realistically imbalanced test data. We instead collect training data with active learning, using a BERT-based embedding model to efficiently retrieve uncertain points from a very large pool of unlabeled utterance pairs. By creating balanced training data with more informative negative examples, active learning greatly improves average precision to $32.5\%$ on QQP and $20.1\%$ on WikiQA. ","On the Importance of Adaptive Data Collection for Extremely Imbalanced
  Pairwise Tasks"
40,1321516150052790272,16252640,Matthew FL,"['New paper on arXiv about implementing logic programs with aggregation using term rewriting and a relational algebra\npaper: <LINK>\ncode/video recording: <LINK> \nwith @xtimv @adveisner #Dyna <LINK>', 'An earlier version of this paper appeared at #WRLA2020 https://t.co/Dt3vgFxumb']",https://arxiv.org/abs/2010.10503,"We present a scheme for translating logic programs, which may use aggregation and arithmetic, into algebraic expressions that denote bag relations over ground terms of the Herbrand universe. To evaluate queries against these relations, we develop an operational semantics based on term rewriting of the algebraic expressions. This approach can exploit arithmetic identities and recovers a range of useful strategies, including lazy strategies that defer work until it becomes possible or necessary. ","Evaluation of Logic Programs with Built-Ins and Aggregation: A Calculus
  for Bag Relations"
41,1321501114441760769,46165258,Shane Steinert-Threlkeld,"['New paper for #blackboxnlp! ""Linguistically-Informed Transformations (LIT)"" uses a high-precision parser to automate data augmentation. \n\nIt was a pleasure supervising great group of students!  (Chuanrong Li @ShengshuoL @ZEYULIU10 Xinyi Wu @nlpxuhui)\n \n<LINK> <LINK>']",http://arxiv.org/abs/2010.08580,"Although large-scale pretrained language models, such as BERT and RoBERTa, have achieved superhuman performance on in-distribution test sets, their performance suffers on out-of-distribution test sets (e.g., on contrast sets). Building contrast sets often re-quires human-expert annotation, which is expensive and hard to create on a large scale. In this work, we propose a Linguistically-Informed Transformation (LIT) method to automatically generate contrast sets, which enables practitioners to explore linguistic phenomena of interests as well as compose different phenomena. Experimenting with our method on SNLI and MNLI shows that current pretrained language models, although being claimed to contain sufficient linguistic knowledge, struggle on our automatically generated contrast sets. Furthermore, we improve models' performance on the contrast sets by apply-ing LIT to augment the training data, without affecting performance on the original data. ","Linguistically-Informed Transformations (LIT): A Method for
  Automatically Generating Contrast Sets"
42,1321499533377966083,772809603046334464,Jonathan Mackey,"['New @hesstelescopes paper on @arxiv today, ""An extreme particle accelerator in the Galactic plane: HESS J1826-130"", investigating a very hard-spectrum (but faint), unidentified gamma-ray source. #DIASdiscovers @davit_zargaryan @DIAS_Dublin @DIASAstronomy \n<LINK>']",https://arxiv.org/abs/2010.13101,"The unidentified very-high-energy (VHE; E $>$ 0.1 TeV) $\gamma$-ray source, HESS J1826$-$130, was discovered with the High Energy Stereoscopic System (HESS) in the Galactic plane. The analysis of 215 h of HESS data has revealed a steady $\gamma$-ray flux from HESS J1826$-$130, which appears extended with a half-width of 0.21$^{\circ}$ $\pm$ 0.02$^{\circ}_{\text{stat}}$ $\pm$ 0.05$^{\circ}_{\text{sys}}$. The source spectrum is best fit with either a power-law function with a spectral index $\Gamma$ = 1.78 $\pm$ 0.10$_{\text{stat}}$ $\pm$ 0.20$_{\text{sys}}$ and an exponential cut-off at 15.2$^{+5.5}_{-3.2}$ TeV, or a broken power-law with $\Gamma_{1}$ = 1.96 $\pm$ 0.06$_{\text{stat}}$ $\pm$ 0.20$_{\text{sys}}$, $\Gamma_{2}$ = 3.59 $\pm$ 0.69$_{\text{stat}}$ $\pm$ 0.20$_{\text{sys}}$ for energies below and above $E_{\rm{br}}$ = 11.2 $\pm$ 2.7 TeV, respectively. The VHE flux from HESS J1826$-$130 is contaminated by the extended emission of the bright, nearby pulsar wind nebula (PWN), HESS J1825$-$137, particularly at the low end of the energy spectrum. Leptonic scenarios for the origin of HESS J1826$-$130 VHE emission related to PSR J1826$-$1256 are confronted by our spectral and morphological analysis. In a hadronic framework, taking into account the properties of dense gas regions surrounding HESS J1826$-$130, the source spectrum would imply an astrophysical object capable of accelerating the parent particle population up to $\gtrsim$200 TeV. Our results are also discussed in a multiwavelength context, accounting for both the presence of nearby supernova remnants (SNRs), molecular clouds, and counterparts detected in radio, X-rays, and TeV energies. ",An extreme particle accelerator in the Galactic plane: HESS J1826$-$130
43,1321495999853006849,1135159531913338880,Ruari Mackenzie,"[""Well I'm a day late, but here's my new paper on Giant Lyman alpha Nebulae around faint quasars. In these objects we can observe the circumgalactic media of massive galaxies in emission.\n\n<LINK> <LINK>"", 'We studied a sample of much fainter quasars with MUSE than previous surveys, to see how the luminosity of the quasar impacts the surrounding nebula.', 'With our very large dynamic range (~7 magnitudes) we can clearly see luminosity dependence. Nebulae around faint quasars (blue) have lower surface-brightness when compared to those around more luminous QSOs (red). https://t.co/YOHiTz38jD', ""It's also curious that the nebular brightness is much more tightly correlated to the quasar Lyman alpha, compare to the UV continuum shown here. If you're interested please have a look at the paper.""]",https://arxiv.org/abs/2010.12589,"We present the results from a MUSE survey of twelve $z\simeq3.15$ quasars, which were selected to be much fainter (20<i<23) than in previous studies of Giant Ly$\alpha$ Nebulae around the brightest quasars (16.6<i<18.7). We detect HI Ly$\alpha$ nebulae around 100% of our target quasars, with emission extending to scales of at least 60 physical kpc, and up to 190 pkpc. We explore correlations between properties of the nebulae and their host quasars, with the goal of connecting variations in the properties of the illuminating QSO to the response in nebular emission. We show that the surface brightness profiles of the nebulae are similar to those of nebulae around bright quasars, but with a lower normalization. Our targeted quasars are on average 3.7 magnitudes (~30 times) fainter in UV continuum than our bright reference sample, and yet the nebulae around them are only 4.3 times fainter in mean Ly$\alpha$ surface brightness, measured between 20 and 50 pkpc. We find significant correlations between the surface brightness of the nebula and the luminosity of the quasar in both UV continuum and Ly$\alpha$. The latter can be interpreted as evidence for a substantial contribution from unresolved inner parts of the nebulae to the narrow components seen in the Ly$\alpha$ lines of some of our faint quasars, possibly from the inner CGM or from the host galaxy's ISM. ",Revealing the Impact of Quasar Luminosity on Giant Ly$\alpha$ Nebulae
44,1321489170762784776,4182664707,Bogdan Mazoure,"['Our new paper w/ Thang Doan, @Mehdi_A_Bennani, @grwip and  @PierreAlquier formulates a theory of catastrophic forgetting in continual learning.\nPaper: <LINK>\nBlog: <LINK>']",https://arxiv.org/abs/2010.04003,"Continual learning (CL) is a setting in which an agent has to learn from an incoming stream of data during its entire lifetime. Although major advances have been made in the field, one recurring problem which remains unsolved is that of Catastrophic Forgetting (CF). While the issue has been extensively studied empirically, little attention has been paid from a theoretical angle. In this paper, we show that the impact of CF increases as two tasks increasingly align. We introduce a measure of task similarity called the NTK overlap matrix which is at the core of CF. We analyze common projected gradient algorithms and demonstrate how they mitigate forgetting. Then, we propose a variant of Orthogonal Gradient Descent (OGD) which leverages structure of the data through Principal Component Analysis (PCA). Experiments support our theoretical findings and show how our method can help reduce CF on classical CL datasets. ","A Theoretical Analysis of Catastrophic Forgetting through the NTK
  Overlap Matrix"
45,1321474342807572485,277401500,David W Hogg,"['So stoked about this new paper on EPRV spectrograph calibration by Zhao, @meg_bedell, @debrafischer18, and me. Polynomials are not the right basis for wavelength solutions! <LINK> <LINK>', '@philip_armitage @meg_bedell @debrafischer18 If you read the paper carefully, you will see that it is filled with QAnon propaganda.', '@jbwhitmore @meg_bedell @debrafischer18 Right now we have only implemented for EXPRES at Yale but we are hoping to propagate much more widely. Looking for collaborators and implementers anywhere.']",https://arxiv.org/abs/2010.13786,"Excalibur is a non-parametric, hierarchical framework for precision wavelength-calibration of spectrographs. It is designed with the needs of extreme-precision radial velocity (EPRV) in mind, which require that instruments be calibrated or stabilized to better than $10^{-4}$ pixels. Instruments vary along only a few dominant degrees of freedom, especially EPRV instruments that feature highly stabilized optical systems and detectors. Excalibur takes advantage of this property by using all calibration data to construct a low-dimensional representation of all accessible calibration states for an instrument. Excalibur also takes advantage of laser frequency combs or etalons, which generate a dense set of stable calibration points. This density permits the use of a non-parametric wavelength solution that can adapt to any instrument or detector oddities better than parametric models, such as a polynomial. We demonstrate the success of this method with data from the EXtreme PREcision Spectrograph (EXPRES), which uses a laser frequency comb. When wavelengths are assigned to laser comb lines using excalibur, the RMS of the residuals is about five times lower than wavelengths assigned using polynomial fits to individual exposures. Radial-velocity measurements of HD 34411 showed a reduction in RMS scatter over a 10-month time baseline from $1.17$ to $1.05\, m\,s^{-1}$. ","Excalibur: A Non-Parametric, Hierarchical Wavelength-Calibration Method
  for a Precision Spectrograph"
46,1321474275933515778,146562720,Jian Pei,"['Our latest COLING 2020 paper ""Cross-lingual Machine Reading Comprehension with Language Branch Knowledge Distillation"" enhances the cross-lingual transferring performance by a novel augmentation approach &amp; establishes the new State of the Art performance. <LINK> <LINK>']",https://arxiv.org/abs/2010.14271,"Cross-lingual Machine Reading Comprehension (CLMRC) remains a challenging problem due to the lack of large-scale annotated datasets in low-source languages, such as Arabic, Hindi, and Vietnamese. Many previous approaches use translation data by translating from a rich-source language, such as English, to low-source languages as auxiliary supervision. However, how to effectively leverage translation data and reduce the impact of noise introduced by translation remains onerous. In this paper, we tackle this challenge and enhance the cross-lingual transferring performance by a novel augmentation approach named Language Branch Machine Reading Comprehension (LBMRC). A language branch is a group of passages in one single language paired with questions in all target languages. We train multiple machine reading comprehension (MRC) models proficient in individual language based on LBMRC. Then, we devise a multilingual distillation approach to amalgamate knowledge from multiple language branch models to a single model for all target languages. Combining the LBMRC and multilingual distillation can be more robust to the data noises, therefore, improving the model's cross-lingual ability. Meanwhile, the produced single multilingual model is applicable to all target languages, which saves the cost of training, inference, and maintenance for multiple models. Extensive experiments on two CLMRC benchmarks clearly show the effectiveness of our proposed method. ","Cross-lingual Machine Reading Comprehension with Language Branch
  Knowledge Distillation"
47,1321469284959375360,894875488094760960,Andrea Dittadi,"['New paper: On the Transfer of Disentangled Representations in Realistic Settings. We scale up disentangled representations to a new high-resolution robotics dataset and investigate the role of disentanglement in Out-Of-Distribution generalization.\n\n<LINK> [1/4] <LINK>', 'Our dataset consists of two parts: a large simulated dataset containing images of a robotic setup with 7 factors of variation (see previous gif), and a smaller test set of real images.\n\nHere is how a model transfers from the simulator to the real robot: [2/4] https://t.co/zIgTTxAZfB', 'We measure Out-Of-Distribution generalization when (1) only the downstream predictor is OOD, and (2) both encoder and predictor are OOD (e.g. when deployed to the real world). High disentanglement leads to low transfer score (lower is better) in some cases but not always. [3/4] https://t.co/sXfkQCd1cn', 'This was a super fun project at @MPI_IS with an amazing group of collaborators: @f_traeuble (equal contribution), @FrancescoLocat8, @manuelwuethrich, @ai_frojack, @OleWinther1, Stefan Bauer, @bschoelkopf [4/4]']",http://arxiv.org/abs/2010.14407,"Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful for diverse tasks such as abstract reasoning and fair classification, their scalability and real-world impact remain questionable. We introduce a new high-resolution dataset with 1M simulated images and over 1,800 annotated real-world images of the same setup. In contrast to previous work, this new dataset exhibits correlations, a complex underlying structure, and allows to evaluate transfer to unseen simulated and real-world settings where the encoder i) remains in distribution or ii) is out of distribution. We propose new architectures in order to scale disentangled representation learning to realistic high-resolution settings and conduct a large-scale empirical study of disentangled representations on this dataset. We observe that disentanglement is a good predictor for out-of-distribution (OOD) task performance. ",On the Transfer of Disentangled Representations in Realistic Settings
48,1321467105464778753,70874545,Josh Lothringer,"['New paper on the arXiv today with @astronomerslc25 in which we calculate some atmosphere models for two brown dwarfs irradiated by white dwarfs!\n\n<LINK>', ""The two objects, WD-0137B (Teq~2,000 K) and EPIC2122B (Teq~3,450 K), orbit a 16,500 K and 25,000 K white dwarf every 116 and 68 minutes, respectively. They're just about some of the most extreme (sub-stellar) systems you could dream up. https://t.co/FWql4lXGYX"", 'From the ground, we can get multiple full spectroscopic phase-curves in a night- something unimaginable with exoplanets! The plot below is from Longstaff et al. 2017 and is one of my favorite in the literature. https://t.co/4bpIsLtkDR', 'To better understand these extreme objects, we computed some PHOENIX atmosphere models. Importantly, we could self-consistently account for the huge amounts of incoming UV irradiation and show that this can drive temperature inversions... not unlike ultra-hot Jupiters! https://t.co/iZm7BJ25BO', ""We show that we can reproduce the emission lines observed in these two systems with our models and can fit the photometry *okay*. We also use PETRA retrievals to get an idea of the object's average temperature. https://t.co/sZiBoBTyA1"", ""We then compare these irradiated BDs to exoplanets: unlike exoplanets at similar temperatures, WD-0137B's inversion isn't driven by TiO/VO absorption, but from the absorption of the UV irradiation by metals, akin to the hottest ultra-hot Jupiters."", ""EPIC2122B on the other hand, is much like KELT-9b, but because of the even more intense UV irradiation, EPIC2122B's inversion can reach even higher temperatures! https://t.co/Jq57LvxRHG"", 'Even though these wild systems are rare, they have a lot to teach us about irradiated atmospheres and definitely test our models. You can read all about this and more in the paper, to appear soon in ApJ!']",https://arxiv.org/abs/2010.14319,"Irradiated brown dwarfs (BDs) provide natural laboratories to test our understanding of substellar and irradiated atmospheres. A handful of short-period BDs around white dwarfs (WDs) have been observed, but the uniquely intense UV-dominated irradiation presents a modeling challenge. Here, we present the first fully self-consistent 1D atmosphere models that take into account the UV irradiation's effect on the object's temperature structure. We explore two BD-WD systems, namely WD-0137-349 and EPIC-212235321. WD-0137-349B has an equilibrium temperature that would place it in the transition between hot and ultra-hot Jupiters, while EPIC-212235321B has an equilibrium temperature higher than all ultra-hot Jupiters except KELT-9b. We explore some peculiar aspects of irradiated BD atmospheres and show that existing photometry can be well-fit with our models. Additionally, the detections of atomic emission lines from these BDs can be explained by a strong irradiation-induced temperature inversion, similar to inversions recently explored in ultra-hot Jupiters. Our models of WD-0137-349B can reproduce the observed equivalent width of many but not all of these atomic lines. We use the observed photometry of these objects to retrieve the temperature structure using the PHOENIX ExoplaneT Retrieval Algorithm (PETRA) and demonstrate that the structures are consistent with our models, albeit somewhat cooler at low pressures. We then discuss the similarities and differences between this class of irradiated brown dwarf and the lower-mass ultra-hot Jupiters. Lastly, we describe the behavior of irradiated BDs in color-magnitude space to show the difficulty in classifying irradiated BDs using otherwise well-tested methods for isolated objects. ","Atmosphere Models of Brown Dwarfs Irradiated by White Dwarfs: Analogues
  for Hot and Ultra-Hot Jupiters"
49,1321462223097790468,2180768821,Erik Hoel,"['How do artificial neural networks generalize? The answer may be in their causal structure. In this new paper we use information theory to track nodes‚Äô causal relationships becoming more sensitive or degenerate. Training traces a path in this ‚Äúcausal plane‚Äù <LINK> <LINK>', 'Some really interesting results of this collaboration with Simon Mattsson and @ericjmichaud_ : the informativeness of a causal relationship between two nodes peaks at a certain characteristic edge weight, no matter what bin size you use (there is a manifold for multiple edges) https://t.co/dLN3gKAOVK', 'We can even measure a variant of the integrated information using these techniques. Normally the phi of a feedforward network is zero... but there are still integrated joint effects of one layer to another. Here we introduce a measure to capture those: ""phi feedfoward"" https://t.co/JkB253oeut', ""There's a great github package for those who want to try out these techniques. Ultimately we are hoping to offer a kind of alternative to the information bottleneck approach (although its not contradictory) that focuses more on causation\n\nhttps://t.co/zp3x3nd9L5"", '@BlaiseLucey00 @ericjmichaud_ I recently learned how to make gifs so get used to it']",https://arxiv.org/abs/2010.13871,"Deep Neural Networks (DNNs) are often examined at the level of their response to input, such as analyzing the mutual information between nodes and data sets. Yet DNNs can also be examined at the level of causation, exploring ""what does what"" within the layers of the network itself. Historically, analyzing the causal structure of DNNs has received less attention than understanding their responses to input. Yet definitionally, generalizability must be a function of a DNN's causal structure since it reflects how the DNN responds to unseen or even not-yet-defined future inputs. Here, we introduce a suite of metrics based on information theory to quantify and track changes in the causal structure of DNNs during training. Specifically, we introduce the effective information (EI) of a feedforward DNN, which is the mutual information between layer input and output following a maximum-entropy perturbation. The EI can be used to assess the degree of causal influence nodes and edges have over their downstream targets in each layer. We show that the EI can be further decomposed in order to examine the sensitivity of a layer (measured by how well edges transmit perturbations) and the degeneracy of a layer (measured by how edge overlap interferes with transmission), along with estimates of the amount of integrated information of a layer. Together, these properties define where each layer lies in the ""causal plane"" which can be used to visualize how layer connectivity becomes more sensitive or degenerate over time, and how integration changes during training, revealing how the layer-by-layer causal structure differentiates. These results may help in understanding the generalization capabilities of DNNs and provide foundational tools for making DNNs both more generalizable and more explainable. ","Examining the causal structures of deep neural networks using
  information theory"
50,1321459591809519622,720027280051957761,Oliver Newton,"['I‚Äôve had my head buried in code bugs, so I nearly missed the chance to promote a new paper that I collaborated on which came out today! Wolfgang Enzi led on a joint analysis of several methods to tighten constraints on thermal relic dark matter models\n<LINK>\n\n1/6', 'Enzi adopts a Bayesian approach to combine studies of gravitational lensing, the Lyman-alpha forest and MW satellite galaxies. From this, he obtains a lower limit on the thermal relic particle mass of 6.733 keV at 95 per cent confidence.\n\n2/6 https://t.co/feSkZCHkzK', 'This also rules out 7.1 keV sterile neutrino dark matter models for large values of the lepton asymmetry parameter, which is particularly interesting as it is one of the models proposed to explain the 3.55 keV excess observed in X-ray spectra of DM-dominated objects.\n\n3/6', 'How can we improve the results further? Currently, the MW satellites and Lyman-alpha forest provide the strongest constraints on the half mode mass; however, both analyses are subject to assumptions about galaxy formation and feedback processes.\n\n4/6', 'Other uncertainties peculiar to each approach also creep in (e.g. the MW mass affects the satellite galaxy luminosity function; the choice of IGM thermal history can make the Lyman-alpha forest consistent with both CDM and WDM models). So, plenty of areas for improvement!\n\n5/6', ""It's a great piece of work and I encourage anyone interested in astrophysical constraints on DM models to have a read!\n\n6/6""]",https://arxiv.org/abs/2010.13802,"We derive joint constraints on the warm dark matter (WDM) half-mode scale by combining the analyses of a selection of astrophysical probes: strong gravitational lensing with extended sources, the Lyman-$\alpha$ forest, and the number of luminous satellites in the Milky Way. We derive an upper limit of $\lambda_{\rm hm}=0.089{\rm~Mpc~h^{-1} }$ at the 95 per cent confidence level, which we show to be stable for a broad range of prior choices. Assuming a Planck cosmology and that WDM particles are thermal relics, this corresponds to an upper limit on the half-mode mass of $M_{\rm hm }< 3 \times 10^{7} {\rm~M_{\odot}~h^{-1}}$, and a lower limit on the particle mass of $m_{\rm th }> 6.048 {\rm~keV}$, both at the 95 per cent confidence level. We find that models with $\lambda_{\rm hm}> 0.223 {\rm~Mpc~h^{-1} }$ (corresponding to $m_{\rm th }> 2.552 {\rm~keV}$ and $M_{\rm hm }< 4.8 \times 10^{8} {\rm~M_{\odot}~h^{-1}}$) are ruled out with respect to the maximum likelihood model by a factor $\leq 1/20$. For lepton asymmetries $L_6>10$, we rule out the $7.1 {\rm~keV}$ sterile neutrino dark matter model, which presents a possible explanation to the unidentified $3.55 {\rm~keV}$ line in the Milky Way and clusters of galaxies. The inferred 95 percentiles suggest that we further rule out the ETHOS-4 model of self-interacting DM. Our results highlight the importance of extending the current constraints to lower half-mode scales. We address important sources of systematic errors and provide prospects for how the constraints of these probes can be improved upon in the future. ","Joint constraints on thermal relic dark matter from strong gravitational
  lensing, the Lyman-$\alpha$ forest, and Milky Way satellites"
51,1321444638121742336,341555556,Eric Ghysels,"['Delighted to post my new paper entitled: Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory and an Application to Racial Justice (with Andrii Babii, Xi Chen and Rohit Kumar)\n<LINK>']",https://arxiv.org/abs/2010.08463,We study the binary choice problem in a data-rich environment with asymmetric loss functions. The econometrics literature covers nonparametric binary choice problems but does not offer computationally attractive solutions in data-rich environments. The machine learning literature has many algorithms but is focused mostly on loss functions that are independent of covariates. We show that theoretically valid decisions on binary outcomes with general loss functions can be achieved via a very simple loss-based reweighting of the logistic regression or state-of-the-art machine learning techniques. We apply our analysis to racial justice in pretrial detention. ,"Binary Choice with Asymmetric Loss in a Data-Rich Environment: Theory
  and an Application to Racial Justice"
52,1321279153002455041,1230965221407150080,Noah Golowich,"[""New paper with Sarath Pattathil &amp; Costis Daskalakis: <LINK>.\nWe answer the question: at what rate can players' actions converge to equilibrium if each plays according a no-regret algorithm in a smooth monotone game?"", 'We study the no-regret Optimistic Gradient (OG) algorithm, and show that its T-th iterate converges at a rate of 1/sqrt(T). We also prove a matching lower bound.\nPrevious work established either rates for on-average convergence or showed last-iterate convergence but without rates', 'To prove our upper bound we introduce a potential function that depends on the global structure of the game -- we call it an adaptive potential function. ""System-augmentation"" type approaches previously used for showing convergence of OG (w/o rates) seem not to work.']",https://arxiv.org/abs/2010.13724,"We study the question of obtaining last-iterate convergence rates for no-regret learning algorithms in multi-player games. We show that the optimistic gradient (OG) algorithm with a constant step-size, which is no-regret, achieves a last-iterate rate of $O(1/\sqrt{T})$ with respect to the gap function in smooth monotone games. This result addresses a question of Mertikopoulos & Zhou (2018), who asked whether extra-gradient approaches (such as OG) can be applied to achieve improved guarantees in the multi-agent learning setting. The proof of our upper bound uses a new technique centered around an adaptive choice of potential function at each iteration. We also show that the $O(1/\sqrt{T})$ rate is tight for all $p$-SCLI algorithms, which includes OG as a special case. As a byproduct of our lower bound analysis we additionally present a proof of a conjecture of Arjevani et al. (2015) which is more direct than previous approaches. ","Tight last-iterate convergence rates for no-regret learning in
  multi-player games"
53,1321274937026531329,2930047588,Andrew Vanderburg,"['Check out this great new paper from Lizhou Sha (@mentisoasis) confirming and characterizing two new Saturn-mass exoplanets from the @TESSatMIT and @KeplerGO missions! <LINK>', ""First thing's first, here are the family portraits. EPIC 246193072 b orbits its star every 12 days. It was discovered by K2 observations and confirmed with precise radial velocities from PFS, HARPS, and FEROS: https://t.co/IdbnwI7FUF"", 'TOI 954 b is a hotter planet, orbiting its star every 3.7 days. It was discovered in the TESS full frame images and confirmed via radial velocity observations from CHIRON, CORALIE, HARPS, PFS, and MINERVA-Australis: https://t.co/b0WDTn0p3p', 'These two planets are in an interesting mass range and help us probe some of the questions surrounding giant planets. A major question in the field is why some of the hottest Jupiter-mass exoplanets are inflated, or larger than we would expect based on their mass alone.', 'One way of probing inflation is to see whether planets can be re-inflated by additional heating when their host stars evolve into giants and leave the main sequence. @SKGrunblatt has done a lot of interesting work studying Jupiters that seem to be reinflated.', 'TOI 954 b adds an interesting twist to this tale of planet inflation. Despite orbiting an evolving star, and being close to the reinflation regime, the planet does not seem to be larger than we would expect. So maybe reinflation is more efficient for Jupiters than Saturns.', ""It's of course hard to draw any firm conclusions from just these two planets, but adding them to the exoplanet population will hopefully let us figure out the physics that causes hot Jupiters to be so huge."", ""Finally, @mentisoasis did this work while working at the MIT TESS Science Office, but he has now just started as a grad student at the Univ. of Wisconsin-Madison. We're super excited to have him here!""]",https://arxiv.org/abs/2010.14436,"We report the discovery of two short-period Saturn-mass planets, one transiting the G subgiant TOI-954 (TIC 44792534, $ V = 10.343 $, $ T = 9.78 $) observed in TESS sectors 4 and 5, and one transiting the G dwarf K2-329 (EPIC 246193072, $ V = 12.70 $, $ K = 10.67 $) observed in K2 campaigns 12 and 19. We confirm and characterize these two planets with a variety of ground-based archival and follow-up observations, including photometry, reconnaissance spectroscopy, precise radial velocity, and high-resolution imaging. Combining all available data, we find that TOI-954 b has a radius of $0.852_{-0.062}^{+0.053} \, R_{\mathrm{J}}$ and a mass of $0.174_{-0.017}^{+0.018} \, M_{\mathrm{J}}$ and is in a 3.68 day orbit, while K2-329 b has a radius of $0.774_{-0.024}^{+0.026} \, R_{\mathrm{J}}$ and a mass of $0.260_{-0.022}^{+0.020} \, M_{\mathrm{J}}$ and is in a 12.46 day orbit. As TOI-954 b is 30 times more irradiated than K2-329 b but more or less the same size, these two planets provide an opportunity to test whether irradiation leads to inflation of Saturn-mass planets and contribute to future comparative studies that explore Saturn-mass planets at contrasting points in their lifetimes. ","TOI-954 b and K2-329 b: Short-Period Saturn-Mass Planets that Test
  whether Irradiation Leads to Inflation"
54,1321270766327877634,2577596593,Chelsea Finn,"['New paper: MELD allows a real robot to adapt to new goals directly from image pixels\n<LINK>\nThe key idea is to _meld_ meta-RL and latent dynamics models for unified state &amp; task inference.\n\nw/ @tonyzzhao, A Nagabandi, K Rakelly, @svlevine\nto appear at @corl_conf <LINK>']",https://arxiv.org/abs/2010.13957,"Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can \emph{also} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only $8$ hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images. ",MELD: Meta-Reinforcement Learning from Images via Latent State Models
55,1321261001749139456,3667426463,Xinting Yu,"['My new paper on arxiv--surface energy of the Titan aerosol analog ""tholin"": <LINK> A thread and a summary of our findings. <LINK>', '1/8: Surface energy may be a property that the community is not as familiar with, but it is important for a variety of processes in planetary atmospheres and on planetary surfaces.', '2/8: It is basically the energy needed to form a new surface/separate two surfaces apart. So the magnitude of surface energy determines how sticky a material is. We found the Titan tholin are very sticky--&gt;indicating they are easier to coagulate to form large particles', '3/8 if the surface sand is similar to tholin, then it means the surface sand is also sticky would require high wind speed to transport them on the surface', '4/8 The surface energy can also be partitioned into different intermolecular components, reflecting the bulk composition of a material. We found tholin to be highly polar, which corresponds to most of the chemical studies in the past.', '4/8 The surface energy can also be partitioned into different intermolecular components, reflecting the bulk composition of a material. We found tholin to be highly polar, which corresponds to most of the chemical studies in the past.', '5/8 With the measured surface energy we can now predict many processes happening on Titan. Implication 1: aerosol-cloud interaction, we can use the surface energy of tholin to estimate contact angle between aerosols and clouds to figure out which types of clouds form efficiently.', ""6/8 Implication 2: Aerosol-lake interactions. When aerosols fall into Titan's methane/ethane lakes, they may float/sink. A floating layer of aerosols would be cool as they could dampen the surface waves, and we know the lakes are almost mirror-smooth!"", ""7/8 However, we don't think the aerosols could float on the lakes. Two reasons, aerosol is likely denser than the lake liquids, so buoyancy force points down. Capillary force also does not help, as the contact angle is zero between the lake liquids and the aerosols."", ""8/8 That's it! My student Jialin Li is currently expanding this study by including tholins made at other laboratories (https://t.co/TTvOPCnQ5D) and Yuna Yu and Julia Garver are using the surface energy to theoretically describe cloud formation on Titan (https://t.co/XLl5fJ0jDW)."", '@Shamrocketeer I guess that would mean the dunes may not migrate much during most of the Titan year because of the low winds, expect during the equinoxes when there are much stronger winds.']",https://arxiv.org/abs/2010.13885,"The photochemical haze produced in the upper atmosphere of Titan plays a key role in various atmospheric and surface processes on Titan. The surface energy, one important physical properties of the haze, is crucial for understanding the growth of the haze particles and can be used to predict their wetting behavior with solid and liquid species on Titan. We produced Titan analog haze materials, so-called ""tholin"", with different energy sources and measured their surface energies through contact angle and direct force measurements. From the contact angle measurement, we found that the tholins produced by cold plasma and UV irradiation have total surface energy around 60-70 mJ/m2. The direct force measurement yields a total surface energy of ~66 mJ/m2 for plasma tholin. The surface energy of tholin is relatively high compared to common polymers, indicating its high cohesiveness. Therefore, the Titan haze particles would likely coagulate easily to form bigger particles, while the haze-derived surface sand particles would need higher wind speed to be mobilized because of the high interparticle cohesion. The high surface energy of tholins also makes them easily wettable by Titan's atmospheric hydrocarbon condensates and surface liquids. Thus, the hazes particles are likely good cloud condensation nuclei (CCN) for hydrocarbon clouds (methane and ethane) to nucleate and grow. And if the hazes particles are denser compared to the lake liquids, they would likely sink into the lakes instead of forming a floating film to dampen the lake surface waves. ","Surface Energy of the Titan Aerosol Analog ""Tholin"""
56,1321198016104026113,326915032,Conor A Nixon,"['Out today: story on our detection of a new molecule on Titan. This ""interstellar"" molecule is a first for a planetary atmosphere, having previously been seen in space only outside our solar system. Story: <LINK> and paper: <LINK> <LINK>']",https://arxiv.org/abs/2010.12743,"We report the first detection on Titan of the small cyclic molecule cyclopropenylidene (c-C3H2) from high sensitivity spectroscopic observations made with the Atacama Large Millimeter/sub-millimeter Array (ALMA). Multiple lines of cyclopropenylidene were detected in two separate datasets: ~251 GHz in 2016 (Band 6) and ~352 GHz in 2017 (Band 7). Modeling of these emissions indicates abundances of 0.50 +/- 0.14 ppb (2016) and 0.28 +/- 0.08 (2017) for a 350 km step model, which may either signify a decrease in abundance, or a mean value of 0.33 +/- 0.07 ppb. Inferred column abundances are (3-5)E12 cm-2 in 2016 and (1-2)E12 cm-2 in 2017, similar to photochemical model predictions. Previously the C3H3+ ion has been measured in Titan's ionosphere by Cassini's Ion and Neutral Mass Spectrometer (INMS), but the neutral (unprotonated) species has not been detected until now, and aromatic versus aliphatic structure could not be determined by the INMS. Our work therefore represents the first unambiguous detection of cyclopropenylidene, the second known cyclic molecule in Titan's atmosphere along with benzene (C6H6) and the first time this molecule has been detected in a planetary atmosphere. We also searched for the N-heterocycle molecules pyridine and pyrimidine finding non-detections in both cases, and determining 2-{\sigma} upper limits of 1.15 ppb (c-C5H5N) and 0.85 ppb (c-C4H4N2) for uniform abundances above 300 km. These new results on cyclic molecules provide fresh constraints on photochemical pathways in Titan's atmosphere, and will require new modeling and experimental work to fully understand the implications for complex molecule formation. ",Detection of Cyclopropenylidene on Titan with ALMA
57,1321144947018518529,3213868013,German Sborlini,['New paper on arXiv: <LINK>!!!!!\n\nVamos LTD collaboration!!! @RogerHer @GermanRodrigoC2 @Juplenti @RamirezSelomit @Sarevemon @AerOlivo \n\n#Science #Physics #Mathematics @PARTICLEFACE'],https://arxiv.org/abs/2010.12971,"The computation of multi-loop multi-leg scattering amplitudes plays a key role to improve the precision of theoretical predictions for particle physics at high-energy colliders. In this work, we focus on the mathematical properties of the novel integrand-level representation of Feynman integrals, which is based on the Loop-Tree Duality (LTD). We explore the behaviour of the multi-loop iterated residues and explicitly show, by developing a general formal proof for the first time, that contributions associated to displaced poles are cancelled out. The remaining residues, called nested residues as originally introduced in Ref. \cite{Verdugo:2020kzh}, encode the relevant physical information and are naturally mapped onto physical configurations associated to nondisjoint on-shell states. By going further on the mathematical structure of the nested residues, we prove that unphysical singularities vanish, and show how the final expressions can be written by using only causal denominators. In this way, we provide a mathematical proof for the all-loop formulae presented in Ref. \cite{Aguilera-Verdugo:2020kzc}. ","Mathematical properties of nested residues and their application to
  multi-loop scattering amplitudes"
58,1321143127038664707,1055880097595564034,Rohini Giles,"['Our new Juno UVS paper is out today, showing the first possible detection of Transient Luminous Events (sprites or elves!) on a planet other than Earth\n<LINK>\n<LINK> <LINK>', 'UVS is primarily used to study Jupiter‚Äôs auroras, which have strong ultraviolet emission ‚Äì but on 11 occasions, we also saw tiny short-lived flashes (an example is highlighted by the yellow circle in the image below). https://t.co/2OuiCtNep8', 'These bright flashes were extremely short-lived (~1.4 ms), were dominated by hydrogen emission and were located 260 km above the 1-bar level.', 'They can‚Äôt be lightning, because they are much too high in the atmosphere. Instead, we think they might be Transient Luminous Events (TLEs) ‚Äì upper atmospheric flashes of light that are triggered by lightning in an underlying thunderstorms.', 'This is what a sprite, a type of TLE, looks like on Earth ‚Äì they‚Äôre absolutely beautiful\n\nhttps://t.co/KGhvl0eMqL', 'TLEs have never been seen before on other planets, but theoretical studies have suggested that they should exist on Jupiter (and other planets) ‚Äì and the predicted properties match up really well with what we observe!']",https://arxiv.org/abs/2010.13740,"11 transient bright flashes were detected in Jupiter's atmosphere using the UVS instrument on the Juno spacecraft. These bright flashes are only observed in a single spin of the spacecraft and their brightness decays exponentially with time, with a duration of ~1.4 ms. The spectra are dominated by H2 Lyman band emission and based on the level of atmospheric absorption, we estimate a source altitude of 260 km above the 1-bar level. Based on these characteristics, we suggest that these are observations of Transient Luminous Events (TLEs) in Jupiter's upper atmosphere. In particular, we suggest that these are elves, sprites or sprite halos, three types of TLEs that occur in the Earth's upper atmosphere in response to tropospheric lightning strikes. This is supported by visible light imaging, which shows cloud features typical of lightning source regions at the locations of several of the bright flashes. TLEs have previously only been observed on Earth, although theoretical and experimental work has predicted that they should also be present on Jupiter. ","Possible Transient Luminous Events observed in Jupiter's upper
  atmosphere"
59,1321139111063814144,1074633382452051969,Kimin,"['Excited to share our #NeurIPS2020 paper that introduces a new model-based RL method to learn the multi-modal transition distribution in an unsupervised manner\nüéì<LINK>\nüíª<LINK>\nw/@younggyoseo @clavera_i @KurutachThanard @jinwoos0417 @pabbeel \n1/N', 'Learning an ensemble of dynamics models is a common practice in model-based RL. However, when the transition dynamics of environments change, dynamics models fail to provide accurate predictions because the target transition dynamics follow a multi-modal distribution.\n\n2/N https://t.co/yiNmVqTNUt', 'To tackle this problem, we propose a trajectory-wise multiple choice learning (T-MCL) that is a novel combination of MCL and model-based RL. The main idea is to force each dynamics model to specialize in different environments by only updating the most accurate dynamics model\n3/N https://t.co/5cJlGJBFqt', 'To utilize the specialized dynamics models more effectively, we propose adaptive planning that selects actions using the most accurate model over a recent experience, which can be interpreted as finding the nearest cluster to the current environment. \n\n4/N https://t.co/Pym57oaVVz', 'Can T-MCL make dynamics models be specialized for a certain subset of training environments with similar dynamics?\n\nYes. T-MCL can separate trajectories from different environments in an unsupervised manner (i.e., without any additional information about environments).\n\n5/N https://t.co/Cl4eGUvIGA', 'We also found that learning specialized dynamics models can improve the generalization performance on unseen (yet related) environments with different transition dynamics. We show T-MCL outperforms model-based RL methods (CaDM, GrBAL) and model-free meta-RL method (PEARL).\n6/N https://t.co/nhGFENVAuL', ""More qualitative analysis: We first train T-MCL on CartPole environments with different masses and visualize the agents' behavior by manually assigning specialized dynamics models. We show agents act as if they are light-weight or heavy-weight according to assigned models.\n\n7/N https://t.co/jY1T2oFzPl"", 'For more details, please check out \nour paper: https://t.co/nhKgJJJ9JW \nopen-source code: https://t.co/fRg25TSOdR \nwebpage: https://t.co/0u8oGOPPpj \n\nThank you for your attention! \n\n8/N', 'Special thanks to co-first author @younggyoseo and collaborators @clavera_i,@KurutachThanard, @jinwoos0417, @pabbeel üôè']",https://arxiv.org/abs/2010.13303,"Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efficiency and final performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to specialize each head in certain environments with similar dynamics, i.e., clustering environments. Moreover, we incorporate context learning, which encodes dynamics-specific information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience. Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at this https URL ","Trajectory-wise Multiple Choice Learning for Dynamics Generalization in
  Reinforcement Learning"
60,1321136883938721792,777651532787580928,Tailin Wu,"['New #NeurIPS2020 paper on Graph Neural Nets #GNN, Representation Learning, Robustness!\n""Graph Information Bottleneck"" w/ @ren_hongyu, @PanLi90769257, @jure, @StanfordAILab, @PurdueCS\n \nWebsite: <LINK>\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2010.12811,"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features. ",Graph Information Bottleneck
61,1321133207257952256,988008087301705728,Hongyu Ren,"['New #NeurIPS2020 paper on Graph Neural Nets #GNN, Representation Learning, Robustness!\n""Graph Information Bottleneck"" w/ @tailintalent, Pan Li, @jure @StanfordAILab @PurdueCS \nWebsite: <LINK>\nPaper: <LINK>\nCode: <LINK>\n(1/n) <LINK>', 'We apply the information bottleneck principle to graph representation learning and propose GIB. The goal is to learn a representation Z that uses the minimal sufficient information from a given graph (A, X) for a downstream prediction Y.\n(2/n)', 'GIB iteratively compresses the information of both A and X of a graph before predicting Y. We derive and optimize a variational upper/lower bound on mutual information  I(representation; graph) and I(representation; Y).\n(3/n) https://t.co/2Zj7alpy1u', 'The model achieves strong robustness against adversarial attacks on both node features and graph structure!\n(4/n) https://t.co/58OrmMiX9p', 'The code of graph information bottleneck (GIB) can be found at https://t.co/I0P05x8jz5, we used the fantastic Pytorch Geometric library by @rusty1s! Feel free to check it out! \n(5/n)', 'tagging @tailin_wu, @PanLi90769257']",https://arxiv.org/abs/2010.12811,"Representation learning of graph-structured data is challenging because both graph structure and node features carry important information. Graph Neural Networks (GNNs) provide an expressive way to fuse information from network structure and node features. However, GNNs are prone to adversarial attacks. Here we introduce Graph Information Bottleneck (GIB), an information-theoretic principle that optimally balances expressiveness and robustness of the learned representation of graph-structured data. Inheriting from the general Information Bottleneck (IB), GIB aims to learn the minimal sufficient representation for a given task by maximizing the mutual information between the representation and the target, and simultaneously constraining the mutual information between the representation and the input data. Different from the general IB, GIB regularizes the structural as well as the feature information. We design two sampling algorithms for structural regularization and instantiate the GIB principle with two new models: GIB-Cat and GIB-Bern, and demonstrate the benefits by evaluating the resilience to adversarial attacks. We show that our proposed models are more robust than state-of-the-art graph defense models. GIB-based models empirically achieve up to 31% improvement with adversarial perturbation of the graph structure as well as node features. ",Graph Information Bottleneck
62,1321080555111354371,996928553038966784,Samuel Yen-Chi Chen,"['Our new paper is released!\n<LINK>\n""Decentralizing Feature Extraction with Quantum Convolutional Neural Network for Automatic Speech Recognition""\n\n#quantumcomputing\n#machineintelligence\n#machinelearning\n#quantummachinelearning\n#speechrecognition']",https://arxiv.org/abs/2010.13309,"We propose a novel decentralized feature extraction approach in federated learning to address privacy-preservation issues for speech recognition. It is built upon a quantum convolutional neural network (QCNN) composed of a quantum circuit encoder for feature extraction, and a recurrent neural network (RNN) based end-to-end acoustic model (AM). To enhance model parameter protection in a decentralized architecture, an input speech is first up-streamed to a quantum computing server to extract Mel-spectrogram, and the corresponding convolutional features are encoded using a quantum circuit algorithm with random parameters. The encoded features are then down-streamed to the local RNN model for the final recognition. The proposed decentralized framework takes advantage of the quantum learning progress to secure models and to avoid privacy leakage attacks. Testing on the Google Speech Commands Dataset, the proposed QCNN encoder attains a competitive accuracy of 95.12% in a decentralized model, which is better than the previous architectures using centralized RNN models with convolutional features. We also conduct an in-depth study of different quantum circuit encoder architectures to provide insights into designing QCNN-based feature extractors. Neural saliency analyses demonstrate a correlation between the proposed QCNN features, class activation maps, and input spectrograms. We provide an implementation for future studies. ","Decentralizing Feature Extraction with Quantum Convolutional Neural
  Network for Automatic Speech Recognition"
63,1321080164999192577,996928553038966784,Samuel Yen-Chi Chen,"['Our new paper is released!\n<LINK>\n""A Unified Classification Framework with Quantum Metric Learning""\n#quantumcomputing\n#machineintelligence\n#machinelearning\n#quantummachinelearning']",https://arxiv.org/abs/2010.13186,"Quantum machine learning is an emerging field that combines machine learning with advances in quantum technologies. Many works have suggested great possibilities of using near-term quantum hardware in supervised learning. Motivated by these developments, we present an embedding-based framework for supervised learning with trainable quantum circuits. We introduce both explicit and implicit approaches. The aim of these approaches is to map data from different classes to separated locations in the Hilbert space via the quantum feature map. We will show that the implicit approach is a generalization of a recently introduced strategy, so-called \textit{quantum metric learning}. In particular, with the implicit approach, the number of separated classes (or their labels) in supervised learning problems can be arbitrarily high with respect to the number of given qubits, which surpasses the capacity of some current quantum machine learning models. Compared to the explicit method, this implicit approach exhibits certain advantages over small training sizes. Furthermore, we establish an intrinsic connection between the explicit approach and other quantum supervised learning models. Combined with the implicit approach, this connection provides a unified framework for quantum supervised learning. The utility of our framework is demonstrated by performing both noise-free and noisy numerical simulations. Moreover, we have conducted classification testing with both implicit and explicit approaches using several IBM Q devices. ",A Unified Framework for Quantum Supervised Learning
64,1320991972816003073,776765039726460929,Carlo Felice Manara,"['Our new paper by Josh Lovell, M.Wyatt, @megaparsec808 @kamatahvel @drgmk @Sebastromarino @lucaroundthewo1 @GRosotti @marcotazzari and more: ALMA survey of disks around ClassIII young stars, evidence of rapid dispersal of mm-sized dust \n<LINK> <LINK>']",https://arxiv.org/abs/2010.12657,"Class III stars are those in star forming regions without large non-photospheric infrared emission, suggesting recent dispersal of their protoplanetary disks. We observed 30 class III stars in the 1-3 Myr Lupus region with ALMA at ${\sim}856\mu$m, resulting in 4 detections that we attribute to circumstellar dust. Inferred dust masses are $0.036{-}0.093M_\oplus$, ${\sim}1$ order of magnitude lower than any previous measurements; one disk is resolved with radius ${\sim}80$ au. Two class II sources in the field of view were also detected, and 11 other sources, consistent with sub-mm galaxy number counts. Stacking non-detections yields a marginal detection with mean dust mass ${\sim}0.0048M_\oplus$. We searched for gas emission from the CO J=3-2 line, and present its detection to NO Lup inferring a gas mass ($4.9 {\pm} 1.1$) ${\times}10^{-5} M_\oplus$ and gas-to-dust ratio $1.0{\pm}0.4$. Combining our survey with class II sources shows a gap in the disk mass distribution from $0.09{-}2M_\oplus$ for ${>}0.7M_\odot$ Lupus stars, evidence of rapid dispersal of mm-sized dust from protoplanetary disks. The class III disk mass distribution is consistent with a population model of planetesimal belts that go on to replenish the debris disks seen around main sequence stars. This suggests that planetesimal belt formation does not require long-lived protoplanetary disks, i.e., planetesimals form within ${\sim}$2 Myr. While all 4 class III disks are consistent with collisional replenishment, for two the gas and/or mid-IR emission could indicate primordial circumstellar material in the final stages of protoplanetary disk dispersal. Two class III stars without sub-mm detections exhibit hot emission that could arise from ongoing planet formation processes inside ${\sim}1$ au. ","ALMA Survey of Lupus Class III Stars: Early Planetesimal Belt Formation
  and Rapid Disk Dispersal"
65,1320921930371866624,31503155,Dr. Pedro Bernardinelli,"['New paper out, and the first one where I mentored the first author! Willow was my student in one of the first classes I TA‚Äôd, and I told her to try to do summer research with Gary and I. A few years later, she did! Short thread on the paper, see <LINK> <LINK>', 'We understand DECam well enough that our astrometry is limited only by shot noise, our reference catalog (and Gaia DR2 is a phenomenal reference) and atmospheric turbulence. Turbulence, in particular, is a curl-free 10 mas effect in DES exposures https://t.co/5DexeThQp0', 'Willow derived a Gaussian process model using Gaia and DES data to reduce the effects of the atmospheric turbulence in our exposures, and showed that we can improve our turbulence errors by a factor of 2-5x, getting turbulence down to the ~2 mas level https://t.co/2xSr9x0mvv', 'Using Eris as a test case, we got orbit fit residuals of +- 5mas, with shot noise dominating the error budget instead of turbulence. \n\nThese techniques also generalize quite easily to LSST, and so its 10 mas astrometry requirement should be substantially surpassed! https://t.co/ORH9nyQqeB', '@abblesauce17 I know, right? I‚Äôm proud =)']",http://arxiv.org/abs/2010.13742,"Stochastic field distortions caused by atmospheric turbulence are a fundamental limitation to the astrometric accuracy of ground-based imaging. This distortion field is measurable at the locations of stars with accurate positions provided by the Gaia DR2 catalog; we develop the use of Gaussian process regression (GPR) to interpolate the distortion field to arbitrary locations in each exposure. We introduce an extension to standard GPR techniques that exploits the knowledge that the 2-dimensional distortion field is curl-free. Applied to several hundred 90-second exposures from the Dark Energy Survey as a testbed, we find that the GPR correction reduces the variance of the turbulent distortions $\approx12\times$, on average, with better performance in denser regions of the Gaia catalog. The RMS per-coordinate distortion in the $riz$ bands is typically $\approx7$ mas before any correction, and $\approx2$ mas after application of the GPR model. The GPR astrometric corrections are validated by the observation that their use reduces, from 10 to 5 mas RMS, the residuals to an orbit fit to $riz$-band observations over 5 years of the $r=18.5$ trans-Neptunian object Eris. We also propose a GPR method, not yet implemented, for simultaneously estimating the turbulence fields and the 5-dimensional stellar solutions in a stack of overlapping exposures, which should yield further turbulence reductions in future deep surveys. ","Reducing ground-based astrometric errors with Gaia and Gaussian
  processes"
66,1320913559019139077,90047221,Robin Kothari,"['New paper on the arXiv with Scott Aaronson, Shalev Ben-David, Shravas Rao, and Avishay Tal: ""Degree vs. Approximate Degree and Quantum Implications of Huang‚Äôs Sensitivity Theorem"". This subsumes our previous result.  <LINK> <LINK>', 'The main new result in this paper is a quadratic relationship between the degree and approximate degree of any total Boolean function. This relationship is optimal and resolves several open questions.']",https://arxiv.org/abs/2010.12629,"Based on the recent breakthrough of Huang (2019), we show that for any total Boolean function $f$, $\bullet \quad \mathrm{deg}(f) = O(\widetilde{\mathrm{deg}}(f)^2)$: The degree of $f$ is at most quadratic in the approximate degree of $f$. This is optimal as witnessed by the OR function. $\bullet \quad \mathrm{D}(f) = O(\mathrm{Q}(f)^4)$: The deterministic query complexity of $f$ is at most quartic in the quantum query complexity of $f$. This matches the known separation (up to log factors) due to Ambainis, Balodis, Belovs, Lee, Santha, and Smotrovs (2017). We apply these results to resolve the quantum analogue of the Aanderaa--Karp--Rosenberg conjecture. We show that if $f$ is a nontrivial monotone graph property of an $n$-vertex graph specified by its adjacency matrix, then $\mathrm{Q}(f)=\Omega(n)$, which is also optimal. We also show that the approximate degree of any read-once formula on $n$ variables is $\Theta(\sqrt{n})$. ","Degree vs. Approximate Degree and Quantum Implications of Huang's
  Sensitivity Theorem"
67,1320859119323000832,795072684048543744,Heni Ben Amor,"['Check out our NeurIPS 2020 spotlight paper on ""Language-Conditioned Imitation Learning for Robot Manipulation Tasks"". We show how to combine language, vision and control in order to teach robots new skills. <LINK> <LINK>']",https://arxiv.org/abs/2010.12083,"Imitation learning is a popular approach for teaching motor skills to robots. However, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., ""go to the large green bowl""). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more fine-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods. ",Language-Conditioned Imitation Learning for Robot Manipulation Tasks
68,1320796344609579009,31049199,Drew Dimmery,"['Do you run experiments? Do you care about heterogeneous treatment effects?\n\n@darbour26, Anup Rao and I have a new paper that looks specifically at how to design experiments to make sure you end up with minimal error in your HTE estimates. \n\nA thread.\n\n<LINK>', 'The first thing we do is define a set of estimators of HTE to consider. In particular, we focus on using _transductive inference_ techniques from machine learning (imputing counterfactuals using a KNN / local weighted regression). https://t.co/f5PoGTJKjH', 'This gives us a really nice decomposition of estimation error in terms of a bias and variance tradeoff. https://t.co/rEEABybm4I', 'We then go on to show that minimizing the pointwise error of HTEs corresponds very closely to a canonical NP-hard graph cutting problem: Max-Cut https://t.co/pDGm39qrJH https://t.co/4O8IQiISmy', 'This leaves us in a funny place. We can see a bias-variance tradeoff. The solution to minimize bias is a pretty straightforward modification of a matched-pair design (and is very efficient to calculate!). But can we minimize _MSE_ of the HTE?', ""Turns out, not really. There's a kind of impossibility here: unless you know more about your data (e.g. what residuals look like, how smooth conditional expectations are), you can't actually come up with a single optimal design."", 'Instead, we just focus on making a good tradeoff in-practice. We do this by assuming smoothness, taking the bias-minimizing design and relaxing it as much as we can while keeping the resulting optimization problem tractable.', 'In particular, we focus entirely on designs which admit an easy (polynomial-time) algorithm for assigning treatment after some simplification of the initial similarity matrix. Such algorithms are possible for all bipartite graphs.', 'Our method (which we call SoftBlock), is based on taking the minimum spanning tree of the similarity graph (https://t.co/mXtZyyVx6o)', 'This turns out to make a very nice tradeoff in simulations compared to typical approaches like matched pairs or blocking. https://t.co/Jw8sY7Paxu', ""We think this paper gives you a lot to think about in how to design experiments. Lots more in the paper! I'm really excited to get this one out in the wild."", ""@jon_mellon Everything we look at is finite-sample unbiased for the ATE. Our main analysis is for the HTE. In general, you'll get bias in finite-sample for any HTE model because of unsmoothness, but it will disappear asymptotically (because the neighborhood of units w/ weight &gt; 0 shrinks)."", ""@jon_mellon Yeah, right now our implementation is in python (it's _very_ simple) and we're working on releasing a library -- I'm happy to share privately in the meantime, though.""]",https://arxiv.org/abs/2010.11332,"In this work, we reframe the problem of balanced treatment assignment as optimization of a two-sample test between test and control units. Using this lens we provide an assignment algorithm that is optimal with respect to the minimum spanning tree test of Friedman and Rafsky (1979). This assignment to treatment groups may be performed exactly in polynomial time. We provide a probabilistic interpretation of this process in terms of the most probable element of designs drawn from a determinantal point process which admits a probabilistic interpretation of the design. We provide a novel formulation of estimation as transductive inference and show how the tree structures used in design can also be used in an adjustment estimator. We conclude with a simulation study demonstrating the improved efficacy of our method. ",Efficient Balanced Treatment Assignments for Experimentation
69,1320795405295230976,1247452582496350208,Oliver Herbort,"['Our newest paper is on arxivü•≥\nCoexistence of CO2, CH4 and H2O in exoplanet atmospheres. We present a new classification of atmospheres, these CO2-CH4-H2O are actually favourable for specific elemental compositions @AandA_journal #astronomy #astrobiology  \n<LINK> <LINK>', 'This paper is an exciting result inspired by our interdisciplinary StA-CES Journal Club organised together with @DominicSamra and @ExoplanetMaster.\nhttps://t.co/t7d1y8MapJ']",https://arxiv.org/abs/2010.12241,"We propose a classification of exoplanet atmospheres based on their H, C, O, N element abundances below about 600 K. Chemical equilibrium models were run for all combinations of H, C, N, O abundances, and three types of solutions were found, which are robust against variations of temperature, pressure and nitrogen abundance. Type A atmospheres contain H2O, CH4, NH3 and either H2 or N2, but only traces of CO2 and O2. Type B atmospheres contain O2, H2O, CO2 and N2, but only traces of CH4, NH3 and H2. Type C atmospheres contain H2O, CO2, CH4 and N2, but only traces of NH3, H2 and O2. Other molecules are only present in ppb or ppm concentrations in chemical equilibrium, depending on temperature. Type C atmospheres are not found in the solar system, where atmospheres are generally cold enough for water to condense, but exoplanets may well host such atmospheres. Our models show that graphite (soot) clouds can occur in type C atmospheres in addition to water clouds, which can occur in all types of atmospheres. Full equilibrium condensation models show that the outgassing from warm rock can naturally provide type C atmospheres. We conclude that type C atmospheres, if they exist, would lead to false positive detections of biosignatures in exoplanets when considering the coexistence of CH4 and CO2, and suggest other, more robust non-equilibrium markers. ","Coexistence of CH4, CO2 and H2O in exoplanet atmospheres"
70,1320779552830812160,32807399,üá∫üá¶ Kristina Lerman üá∫üá¶,"['In a new CSCW paper with @KeithComplexity, @mposfai we identify new instability in the wisdom of crowds: human biases can prevent the crowd from identifying best solutions. We also show an algorithmic solution that restores crowd wisdom.\n\n<LINK>']",https://arxiv.org/abs/2010.12571,"Crowdsourcing systems aggregate decisions of many people to help users quickly identify high-quality options, such as the best answers to questions or interesting news stories. A long-standing issue in crowdsourcing is how option quality and human judgement heuristics interact to affect collective outcomes, such as the perceived popularity of options. We address this limitation by conducting a controlled experiment where subjects choose between two ranked options whose quality can be independently varied. We use this data to construct a model that quantifies how judgement heuristics and option quality combine when deciding between two options. The model reveals popularity-ranking can be unstable: unless the quality difference between the two options is sufficiently high, the higher quality option is not guaranteed to be eventually ranked on top. To rectify this instability, we create an algorithm that accounts for judgement heuristics to infer the best option and rank it first. This algorithm is guaranteed to be optimal if data matches the model. When the data does not match the model, however, simulations show that in practice this algorithm performs better or at least as well as popularity-based and recency-based ranking for any two-choice question. Our work suggests that algorithms relying on inference of mathematical models of user behavior can substantially improve outcomes in crowdsourcing systems. ",Origins of Algorithmic Instabilities in Crowdsourced Ranking
71,1320753970256744451,3377714115,Karl Pertsch,"['How can we use large offline datasets for accelerating the learning of new tasks? We can transfer skills!\nCheck out our #CoRL2020 paper on efficient skill transfer with learned skill priors!\nüìÑPaper: <LINK>\nüíªWebsite &amp; Code: <LINK>\n\nThreadüëá(1/8) <LINK>', 'Learning reusable skills is great: it does not need rewards (vs offline RL) and can be done fully offline without a training task distribution (vs meta-RL).\nBut: with larger datasets, the number of learned skills grows. Exploring all skills on the target task is infeasible!\n(2/8) https://t.co/XjecYmfYf1', 'Intuitively, not all skills should be explored with equal probability: when holding onto the handle of a kettle, it is more promising to attempt a pickup than a sweeping skill.\n\nWe can learn such _skill priors_ from our experience solving other tasks!\n(3/8) https://t.co/7rMtmlnP5x', 'We introduce SPiRL (Skill-Prior RL), an approach for jointly learning an embedding space of skills and a prior over skills from offline data. For learning downstream tasks, we modify the SAC objective to use the learned prior as guidance for a hierarchical policy.\n(4/8) https://t.co/eHlPwYbCGd', 'We test SPiRL on three environments: maze navigation, block stacking and kitchen manipulation. For each we collect a large offline dataset and then test transfer to new tasks.\n(5/8) https://t.co/eREcaEsTvi', 'On all environments, we show that SPiRL‚Äôs skill prior leads to better target task performance than using a ‚Äúflat‚Äù prior over primitive actions or training a hierarchical policy without prior guidance.\n(6/8) https://t.co/IxxugJCbY0', 'Only SPiRL is able to learn the most challenging target tasks ‚Äî a hierarchical SAC baseline without prior guidance can solve some subtasks, but struggles to effectively explore the large set of learned skills.\n(7/8) https://t.co/3Dhb2HkAXG', 'For more details, code and videos, check out our website: https://t.co/9fyZ1NfP9g\n\njoint work w/ @YoungwoonLee and @JosephLim_AI\n\n(8/8)', 'Shoutout to @abhishekunique7 for publishing the kitchen environment with his RPL paper, and to Aviral Kumar and Justin Fu for packaging env+data nicely in the D4RL benchmark! :)']",https://arxiv.org/abs/2010.11944,"Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example information about the current state can hint which skills are promising to explore. In this work, we propose to implement this intuition by learning a prior over skills. We propose a deep latent variable model that jointly learns an embedding space of skills and the skill prior from offline agent experience. We then extend common maximum-entropy RL approaches to use skill priors to guide downstream learning. We validate our approach, SPiRL (Skill-Prior RL), on complex navigation and robotic manipulation tasks and show that learned skill priors are essential for effective skill transfer from rich datasets. Videos and code are available at this https URL ",Accelerating Reinforcement Learning with Learned Skill Priors
72,1320742623129210880,28840722,Phil Long,"['New paper with Peter Bartlett called ""Failures of model-dependent generalization bounds for least-norm interpolation"": <LINK>.']",https://arxiv.org/abs/2010.08479,"We consider bounds on the generalization performance of the least-norm linear regressor, in the over-parameterized regime where it can interpolate the data. We describe a sense in which any generalization bound of a type that is commonly proved in statistical learning theory must sometimes be very loose when applied to analyze the least-norm interpolant. In particular, for a variety of natural joint distributions on training examples, any valid generalization bound that depends only on the output of the learning algorithm, the number of training examples, and the confidence parameter, and that satisfies a mild condition (substantially weaker than monotonicity in sample size), must sometimes be very loose -- it can be bounded below by a constant when the true excess risk goes to zero. ","Failures of model-dependent generalization bounds for least-norm
  interpolation"
73,1320735090511618048,573729628,"Steve Taylor, PhD","['Great new paper led by Vanderbilt postdoc, Dr Nihan Pol! We peek into the future and establish some scientific milestones for the field of PTA GW Astronomy. First detection of the GW background, then unveiling its origin, then digging into SMBH astro.  <LINK>']",https://arxiv.org/abs/2010.11950,"The NANOGrav Collaboration reported strong Bayesian evidence for a common-spectrum stochastic process in its 12.5-yr pulsar timing array dataset, with median characteristic strain amplitude at periods of a year of $A_{\rm yr} = 1.92^{+0.75}_{-0.55} \times 10^{-15}$. However, evidence for the quadrupolar Hellings \& Downs interpulsar correlations, which are characteristic of gravitational wave signals, was not yet significant. We emulate and extend the NANOGrav dataset, injecting a wide range of stochastic gravitational wave background (GWB) signals that encompass a variety of amplitudes and spectral shapes, and quantify three key milestones: (I) Given the amplitude measured in the 12.5 yr analysis and assuming this signal is a GWB, we expect to accumulate robust evidence of an interpulsar-correlated GWB signal with 15--17 yrs of data, i.e., an additional 2--5 yrs from the 12.5 yr dataset; (II) At the initial detection, we expect a fractional uncertainty of $40\%$ on the power-law strain spectrum slope, which is sufficient to distinguish a GWB of supermassive black-hole binary origin from some models predicting more exotic origins;(III) Similarly, the measured GWB amplitude will have an uncertainty of $44\%$ upon initial detection, allowing us to arbitrate between some population models of supermassive black-hole binaries. In addition, power-law models are distinguishable from those having low-frequency spectral turnovers once 20~yrs of data are reached. Even though our study is based on the NANOGrav data, we also derive relations that allow for a generalization to other pulsar-timing array datasets. Most notably, by combining the data of individual arrays into the International Pulsar Timing Array, all of these milestones can be reached significantly earlier. ","Astrophysics Milestones For Pulsar Timing Array Gravitational Wave
  Detection"
74,1320685852788461568,2798844015,Simon Chandler-Wilde,['Finished our new paper with Mohammad Al Azah @MohdAlazah of @HTUjo on computing the complex error function with modified trapezoidal rule <LINK> It works well - error reduces by a factor exp(pi) = 23.1 for each additional quadrature point! <LINK>'],https://arxiv.org/abs/2010.05659,"In this paper we propose a method for computing the Faddeeva function $w(z) := e^{-z^2}\mathrm{erfc}(-i z)$ via truncated modified trapezoidal rule approximations to integrals on the real line. Our starting point is the method due to Matta and Reichel (Math. Comp. 25 (1971), pp. 339-344) and Hunter and Regan (Math. Comp. 26 (1972), pp. 339-541). Addressing shortcomings flagged by Weideman (SIAM. J. Numer. Anal. 31 (1994), pp. 1497-1518), we construct approximations which we prove are exponentially convergent as a function of $N+1$, the number of quadrature points, obtaining error bounds which show that accuracies of $2\times 10^{-15}$ in the computation of $w(z)$ throughout the complex plane are achieved with $N = 11$, this confirmed by computations. These approximations, moreover, provably achieve small relative errors throughout the upper complex half-plane where $w(z)$ is non-zero. Numerical tests suggest that this new method is competitive, in accuracy and computation times, with existing methods for computing $w(z)$ for complex $z$. ","Computation of the Complex Error Function using Modified Trapezoidal
  Rules"
75,1320657983928487936,786855300322172928,Alkistis Pourtsidou,"['Paper alert! A new @EC_Euclid paper led by Matteo Martinelli from @ift_uam_csic focuses on the ""precision vs accuracy"" problem for Euclid\'s weak lensing probe [<LINK>].', 'The paper studies the effect of matter nonlinearities as well as baryonic effects on the recovery of the ""true"" cosmological parameters. Worth a read! https://t.co/ONEvL9UIde', '@Chrisclarkson69 LOL']",https://arxiv.org/abs/2010.12382,"Upcoming surveys will map the growth of large-scale structure with unprecented precision, improving our understanding of the dark sector of the Universe. Unfortunately, much of the cosmological information is encoded by the small scales, where the clustering of dark matter and the effects of astrophysical feedback processes are not fully understood. This can bias the estimates of cosmological parameters, which we study here for a joint analysis of mock Euclid cosmic shear and Planck cosmic microwave background data. We use different implementations for the modelling of the signal on small scales and find that they result in significantly different predictions. Moreover, the different nonlinear corrections lead to biased parameter estimates, especially when the analysis is extended into the highly nonlinear regime, with both the Hubble constant, $H_0$, and the clustering amplitude, $\sigma_8$, affected the most. Improvements in the modelling of nonlinear scales will therefore be needed if we are to resolve the current tension with more and better data. For a given prescription for the nonlinear power spectrum, using different corrections for baryon physics does not significantly impact the precision of Euclid, but neglecting these correction does lead to large biases in the cosmological parameters. In order to extract precise and unbiased constraints on cosmological parameters from Euclid cosmic shear data, it is therefore essential to improve the accuracy of the recipes that account for nonlinear structure formation, as well as the modelling of the impact of astrophysical processes that redistribute the baryons. ","Euclid: impact of nonlinear prescriptions on cosmological parameter
  estimation from weak lensing cosmic shear"
76,1320577401370206208,2377407248,Daniel Whiteson,"['New paper!\n\nMapping Machine-Learned Physics into a Human-Readable Space ( <LINK> )\n\nLed by Taylor Faucett, with Jesse Thaler\n\nThis paper asks: can we translate a black-box NN into human language?', 'We started from a paper from a few years ago, that showed a CNN outperforming the combination of human ideas:\n\n(red vs blue, it‚Äôs small but real) https://t.co/b6sAcRqJlp', 'Our question was: what in the NN doing, and can we understand it?\n\nWe defined a space of possible NEW observables, energy flow polynomials\n\nThen we defined a mapping between the NN and these observables, a new metric ADO that is 1 if networks order pairs of events the same way: https://t.co/k6myS5SD9a', 'We set it loose, and it chose this observable: https://t.co/ZGDsOlM9Su', 'Which, when we added to the network, it closed the gap!\n\nInterestingly, this new observable is NOT theoretically well behaved (it‚Äôs IRC unsafe). And no IRC safe observable closes the gap!\n\nSo what‚Äôs the network doing? \n\nNot something you want to rely on!', 'We also set it loose from a minimal set of observables to see what it did. We use ADO to quickly find good observables, getting a 300x speed up over a brute-force search. \n\nAaaaaaand, it matched the CNN using just 7 observables! https://t.co/gLNBLrghiC', 'The lesson is: use powerful deep nets to measure the amount of info in your data, but always translate it back to human space before using it.', 'This method should work for any black-box network. More applications coming in follow-up papers soon!']",https://arxiv.org/abs/2010.11998,"We present a technique for translating a black-box machine-learned classifier operating on a high-dimensional input space into a small set of human-interpretable observables that can be combined to make the same classification decisions. We iteratively select these observables from a large space of high-level discriminants by finding those with the highest decision similarity relative to the black box, quantified via a metric we introduce that evaluates the relative ordering of pairs of inputs. Successive iterations focus only on the subset of input pairs that are misordered by the current set of observables. This method enables simplification of the machine-learning strategy, interpretation of the results in terms of well-understood physical concepts, validation of the physical model, and the potential for new insights into the nature of the problem itself. As a demonstration, we apply our approach to the benchmark task of jet classification in collider physics, where a convolutional neural network acting on calorimeter jet images outperforms a set of six well-known jet substructure observables. Our method maps the convolutional neural network into a set of observables called energy flow polynomials, and it closes the performance gap by identifying a class of observables with an interesting physical interpretation that has been previously overlooked in the jet substructure literature. ",Mapping Machine-Learned Physics into a Human-Readable Space
77,1320501654471532545,2375680693,John P Dickerson,"['Counterfactual explanations are core to AI explainability; announcing a new survey paper &amp; intro to that space.  @itsArthurAI Research Fellow @Sahil1V, @keeghin, and I welcome feedback &amp; will update our survey as this burgeoning field matures.\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2010.10596,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability. ",Counterfactual Explanations for Machine Learning: A Review
78,1320268849460269056,2712446972,Gilad Yehudai,"['Check out our new paper! ""On Size Generalization in Graph Neural Networks"". GNN can process graphs of any size, how can we learn on small graphs and generalize to larger graphs? it turns out that local structure affects the size generalization properties. \n<LINK>']",https://arxiv.org/abs/2010.08853,"Graph neural networks (GNNs) can process graphs of different sizes, but their ability to generalize across sizes, specifically from small to large graphs, is still not well understood. In this paper, we identify an important type of data where generalization from small to large graphs is challenging: graph distributions for which the local structure depends on the graph size. This effect occurs in multiple important graph learning domains, including social and biological networks. We first prove that when there is a difference between the local structures, GNNs are not guaranteed to generalize across sizes: there are ""bad"" global minima that do well on small graphs but fail on large graphs. We then study the size-generalization problem empirically and demonstrate that when there is a discrepancy in local structure, GNNs tend to converge to non-generalizing solutions. Finally, we suggest two approaches for improving size generalization, motivated by our findings. Notably, we propose a novel Self-Supervised Learning (SSL) task aimed at learning meaningful representations of local structures that appear in large graphs. Our SSL task improves classification accuracy on several popular datasets. ",From Local Structures to Size Generalization in Graph Neural Networks
79,1320077938952470529,390495490,Tom Silver,"['Curious about task and motion planning? Check out our new survey paper: <LINK> \n\n(w/ Caelan Reed Garrett, Rohan Chitnis, Rachel Holladay, Beomjoon Kim, Leslie Pack Kaelbling, Tom√°s Lozano-P√©rez)']",https://arxiv.org/abs/2010.01083,"The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete-continuous mathematical programming, and continuous motion planning, and thus cannot be effectively addressed by any of these fields directly. In this paper, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search. ",Integrated Task and Motion Planning
80,1320034634219048967,862775071785570304,Francisco Gomes,"['Happy to have triggered some discussion based on our Mutation 2020 paper findings (i.e., new ways of measuring diversity using test execution outcomes instead of test specs). :D (plugging pre-print: <LINK> - video link soon). Video: <LINK>']",https://arxiv.org/abs/2010.09144,"Diversity has been proposed as a key criterion to improve testing effectiveness and efficiency.It can be used to optimise large test repositories but also to visualise test maintenance issues and raise practitioners' awareness about waste in test artefacts and processes. Even though these diversity-based testing techniques aim to exercise diverse behavior in the system under test (SUT), the diversity has mainly been measured on and between artefacts (e.g., inputs, outputs or test scripts). Here, we introduce a family of measures to capture behavioural diversity (b-div) of test cases by comparing their executions and failure outcomes. Using failure information to capture the SUT behaviour has been shown to improve effectiveness of history-based test prioritisation approaches. However, history-based techniques require reliable test execution logs which are often not available or can be difficult to obtain due to flaky tests, scarcity of test executions, etc. To be generally applicable we instead propose to use mutation testing to measure behavioral diversity by running the set of test cases on various mutated versions of the SUT. Concretely, we propose two specific b-div measures (based on accuracy and Matthew's correlation coefficient, respectively) and compare them with artefact-based diversity (a-div) for prioritising the test suites of 6 different open-source projects. Our results show that our b-div measures outperform a-div and random selection in all of the studied projects. The improvement is substantial with an average increase in average percentage of faults detected (APFD) of between 19% to 31% depending on the size of the subset of prioritised tests. ",Using mutation testing to measure behavioural test diversity
81,1319814460828229634,1940366048,Abhilasha Ravichander,"['New paper ""EIGEN"" (<LINK>),  a framework that leverages large-scale language models to automatically generate event influences.\n\nAn incredible effort led by @aman_madaan,  starting from a *course project* \n(P.S. He\'s applying to PhD programs soon üòâ) <LINK>', 'Joint work with @aman_madaan *, @dheerajgopal *, Yiming Yang, Ed Hovy and @shrimai_  \n\nCode available at : https://t.co/Q3oaNqQyKO #NLProc']",https://arxiv.org/abs/2010.11764,"Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a ""what-if"" Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning. ",EIGEN: Event Influence GENeration using Pre-trained Language Models
82,1319706579260694529,2235411914,Surya Ganguli,"['Congrats to @aran_nayebi and @sanjana__z on leading  a great new paper w/ @dyamins to appear as a spotlight at #NeurIPS2020 on how to identify the learning rule a neural network is using, through dynamical measurements of aggregate statistics: <LINK> <LINK>']",https://arxiv.org/abs/2010.11765,"The brain modifies its synaptic strengths during learning in order to better adapt to its environment. However, the underlying plasticity rules that govern learning are unknown. Many proposals have been suggested, including Hebbian mechanisms, explicit error backpropagation, and a variety of alternatives. It is an open question as to what specific experimental measurements would need to be made to determine whether any given learning rule is operative in a real biological system. In this work, we take a ""virtual experimental"" approach to this problem. Simulating idealized neuroscience experiments with artificial neural networks, we generate a large-scale dataset of learning trajectories of aggregate statistics measured in a variety of neural network architectures, loss functions, learning rule hyperparameters, and parameter initializations. We then take a discriminative approach, training linear and simple non-linear classifiers to identify learning rules from features based on these observables. We show that different classes of learning rules can be separated solely on the basis of aggregate statistics of the weights, activations, or instantaneous layer-wise activity changes, and that these results generalize to limited access to the trajectory and held-out architectures and learning curricula. We identify the statistics of each observable that are most relevant for rule identification, finding that statistics from network activities across training are more robust to unit undersampling and measurement noise than those obtained from the synaptic strengths. Our results suggest that activation patterns, available from electrophysiological recordings of post-synaptic activities on the order of several hundred units, frequently measured at wider intervals over the course of learning, may provide a good basis on which to identify learning rules. ",Identifying Learning Rules From Neural Network Observables
83,1319699046919884802,988008087301705728,Hongyu Ren,"['New #NeurIPS2020 paper on multi-hop reasoning on knowledge graphs! joint work with @jure @StanfordAILab \nPaper: <LINK>\nWebsite: <LINK>\nCode (BetaE, Query2box, GQE): <LINK> <LINK>', 'The task is answering first-order logic queries (with existential, conjunction, disjunction and negation) on incomplete KGs. Key insight is to embed the queries and entities, and reason in the embedding space. Previous methods (GQE, Q2B..) cannot handle negation/set complement.', 'We embed queries as Beta distributions. For conjunctions, we take weighted product of the PDF of the Beta embeddings of input queries. For negation, we calculate the reciprocal of the parameters of the input so that the high density region will be become low and vice versa. https://t.co/9liA1Ac0mj', 'The embedding also captures the uncertainty of a query, measured by the number of answers it has. We found a natural connection between the entropy of the Beta embedding and # answers, without any training needed to enforce this correlation.']",https://arxiv.org/abs/2010.11465,"One of the fundamental problems in Artificial Intelligence is to perform complex multi-hop logical reasoning over the facts captured by a knowledge graph (KG). This problem is challenging, because KGs can be massive and incomplete. Recent approaches embed KG entities in a low dimensional space and then use these embeddings to find the answer entities. However, it has been an outstanding challenge of how to handle arbitrary first-order logic (FOL) queries as present methods are limited to only a subset of FOL operators. In particular, the negation operator is not supported. An additional limitation of present methods is also that they cannot naturally model uncertainty. Here, we present BetaE, a probabilistic embedding framework for answering arbitrary FOL queries over KGs. BetaE is the first method that can handle a complete set of first-order logical operations: conjunction ($\wedge$), disjunction ($\vee$), and negation ($\neg$). A key insight of BetaE is to use probabilistic distributions with bounded support, specifically the Beta distribution, and embed queries/entities as distributions, which as a consequence allows us to also faithfully model uncertainty. Logical operations are performed in the embedding space by neural operators over the probabilistic embeddings. We demonstrate the performance of BetaE on answering arbitrary FOL queries on three large, incomplete KGs. While being more general, BetaE also increases relative performance by up to 25.4% over the current state-of-the-art KG reasoning methods that can only handle conjunctive queries without negation. ",Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs
84,1319698795450388482,154406127,Aran Nayebi,"['1/ Excited to share our new work on ""Identifying Learning Rules From Neural Network Observables"", to appear as a #NeurIPS2020 Spotlight!\nw/ @sanjana__z @SuryaGanguli @dyamins\n\nPaper: <LINK>\nCode: <LINK>\n\nSummary below üëá', '2/ tldr; Our results suggest that in vivo recordings of post-synaptic activities from a neural circuit on the order of several hundred units, frequently measured at wider intervals during the course of learning, may provide a good basis on which to identify learning rules.', '3/ Background: one of the tenets of modern neuroscience is that the brain modifies its synaptic connections during learning to improve behavior.\nHowever, these underlying plasticity rules are currently unknown -- though there have been many proposals!', '4/ What types of neuroscience data should we collect to separate between hypothesized learning rules? Should we record synaptic strengths, post-synaptic activities, or relative changes between pre- and post-synaptic activity? Should we additionally differentiate by cortical area? https://t.co/N9fpG2k0YX', '5/ We take a ""virtual experimental"" approach to this problem, with the goal of answering whether it is even possible to generically identify which learning rule is operative in an artificial neural network, across a wide range of learning rule types, system architectures, &amp; tasks', '6/ We train over 1,000 neural networks across sensory modalities and supervised &amp; self-supervised tasks. We then measure statistics aggregated across units from each layer gathered from the weights, activities, or layer-wise activity changes, over the course of model training. https://t.co/uSdtwav4c6', '7/ First, is this general problem tractable? Yes, across all classes of observables, the Random Forest (a simple non-linear classifier) attains the highest test accuracy, solely on the basis of these aggregate statistics, without knowledge of the architecture or the task: https://t.co/6ROClupUze', '8/ We also see strong generalization to certain held-out classes of input types (""architecture"" and ""training curricula""): https://t.co/hPNZGP8fPO', '9/ Next, we explore additional experimental realism in several ways, combining what we can measure in neuroscience with what we can conclude given that we know the ground truth learning rule.', '10/ Often times one collects data throughout learning at regularly spaced intervals. We capture this variability by training classifiers on the subsampled learning trajectory, and find that measurements temporally spaced further apart are more robust to undersampling: https://t.co/iQTJsaGqdE', '11/ Finally, there are many recording paradigms in neuroscience, each with some range of unit subsampling and measurement noise. Therefore, how noise and subsample-robust are particular observable measures?', ""12/ We model measurement noise as an AWGN process added to model units, and find that aggregate statistics across units of the network's activation patterns are most robust to unit undersampling and measurement noise, regardless of classifier: https://t.co/jvO14RUxID"", '13/ See the paper for more experiments! We have also made our generated dataset of observable statistics publicly available so that others can analyze these properties without needing to train neural networks themselves.\n\nColab tutorial: https://t.co/VcSuFbD86I', '14/ Special thanks to @khermann_ @eshedmargalit @jinyao_yan for helpful discussions!', ""15/ That's all for now!\n\nI plan to talk more about this work at the upcoming @neuromatch next Thursday at 13:30 PT, feel free to tune in if interested üòÉ\n\nSchedule below:\nhttps://t.co/pcq98DxrDS"", ""@JonAMichaels @yahdef @sanjana__z @SuryaGanguli @dyamins ^ It's exactly these considerations that motivated us!""]",https://arxiv.org/abs/2010.11765,"The brain modifies its synaptic strengths during learning in order to better adapt to its environment. However, the underlying plasticity rules that govern learning are unknown. Many proposals have been suggested, including Hebbian mechanisms, explicit error backpropagation, and a variety of alternatives. It is an open question as to what specific experimental measurements would need to be made to determine whether any given learning rule is operative in a real biological system. In this work, we take a ""virtual experimental"" approach to this problem. Simulating idealized neuroscience experiments with artificial neural networks, we generate a large-scale dataset of learning trajectories of aggregate statistics measured in a variety of neural network architectures, loss functions, learning rule hyperparameters, and parameter initializations. We then take a discriminative approach, training linear and simple non-linear classifiers to identify learning rules from features based on these observables. We show that different classes of learning rules can be separated solely on the basis of aggregate statistics of the weights, activations, or instantaneous layer-wise activity changes, and that these results generalize to limited access to the trajectory and held-out architectures and learning curricula. We identify the statistics of each observable that are most relevant for rule identification, finding that statistics from network activities across training are more robust to unit undersampling and measurement noise than those obtained from the synaptic strengths. Our results suggest that activation patterns, available from electrophysiological recordings of post-synaptic activities on the order of several hundred units, frequently measured at wider intervals over the course of learning, may provide a good basis on which to identify learning rules. ",Identifying Learning Rules From Neural Network Observables
85,1319688341118455809,967806578425516032,Dr Jemima Tabeart,"[""We (@DrSarahDance, @amoslawless, @J_A_Waller and Nancy Nichols) have a new paper on arxiv this week!  <LINK>   As is my new tradition, here's an as-non-technical-as-possible overview (this time with GIFs!) üßµ: <LINK>"", 'In my PhD I studied how introducing correlated observation error covariance matrices (OECs) alters mathematical properties of variational data assimilation (DA) problems. DA appears everywhere, but is most well known for its use in weather forecasting. https://t.co/6giUJaexol', 'In this new paper we consider the preconditioned problem - this is where we solve a different but related problem that is computationally cheaper. The standard preconditioner for DA terms puts the OEC matrix in the same term as the background error covariance (BEC) matrix. https://t.co/t2ey0ZXSQW', 'Some maths: we study how cheap/expensive our problem is using a condition number. Large condition number = expensive! \nEigenvalues are properties of a matrix and relate to their condition number - if all the eigenvalues are close the condition no. is small, spread out = large https://t.co/D4jVsBLJh9', 'Previously we found for the UNpreconditioned system (BEC + OEC) that the smallest eigenvalue of the OEC matrix is important theoretically. If the eigenvalue is small, the DA problem is expensive! Numerics show that if we increase this eigenvalue, we can make the DA problem faster https://t.co/9MC6DImF2I', 'For the preconditioned problem (OECxBEC) we found that although the smallest eigenvalue is still important, numerically the DA problem is solved fastest when BEC and OEC have similar properties (e.g. related eigenvalues). https://t.co/pYm5kCJSZ5', ""So if all we care about is a fast solution, we should make BEC and OEC as similar as possible, right? Usually we don't get to choose BEC/OEC - they come from the underlying physics. However, in practice we often have to adapt OEC matrices before we use them. https://t.co/IwnKvrZPuq"", ""Our new theory and results could help us choose how to do this modification - and mean even faster convergence! This is great: OEC information is really important, but can be expensive to use. Better OEC matrices = better weather forecasts (NB I'm not promising better weather!) https://t.co/dPRXKR38G4"", ""Thanks for making it this far! Finishing this paper during COVID-19 times has been difficult. Collaborating virtually has taken some adjustment, and I've found focusing hard (not ideal for editing manuscripts!). But I'm going to celebrate this small win and keep plodding on! https://t.co/NJ4fAI6x05""]",https://arxiv.org/abs/2010.08416,"Data assimilation algorithms combine prior and observational information, weighted by their respective uncertainties, to obtain the most likely posterior of a dynamical system. In variational data assimilation the posterior is computed by solving a nonlinear least squares problem. Many numerical weather prediction (NWP) centres use full observation error covariance (OEC) weighting matrices, which can slow convergence of the data assimilation procedure. Previous work revealed the importance of the minimum eigenvalue of the OEC matrix for conditioning and convergence of the unpreconditioned data assimilation problem. In this paper we examine the use of correlated OEC matrices in the preconditioned data assimilation problem for the first time. We consider the case where there are more state variables than observations, which is typical for applications with sparse measurements e.g. NWP and remote sensing. We find that similarly to the unpreconditioned problem, the minimum eigenvalue of the OEC matrix appears in new bounds on the condition number of the Hessian of the preconditioned objective function. Numerical experiments reveal that the condition number of the Hessian is minimised when the background and observation lengthscales are equal. This contrasts with the unpreconditioned case, where decreasing the observation error lengthscale always improves conditioning. Conjugate gradient experiments show that in this framework the condition number of the Hessian is a good proxy for convergence. Eigenvalue clustering explains cases where convergence is faster than expected. ","New bounds on the condition number of the Hessian of the preconditioned
  variational data assimilation problem"
86,1319673277380497409,69202541,Jonathan Le Roux,"['Fresh out of the oven, our new paper with @amy_YN_Hung and Gordon Wichern ""Transcription Is All You Need"" (yes, it\'s tongue-in-cheek) considers training music source separation algorithms using only music scores as supervision, without isolated signals.\n<LINK>']",https://arxiv.org/abs/2010.11904,"Most music source separation systems require large collections of isolated sources for training, which can be difficult to obtain. In this work, we use musical scores, which are comparatively easy to obtain, as a weak label for training a source separation system. In contrast with previous score-informed separation approaches, our system does not require isolated sources, and score is used only as a training target, not required for inference. Our model consists of a separator that outputs a time-frequency mask for each instrument, and a transcriptor that acts as a critic, providing both temporal and frequency supervision to guide the learning of the separator. A harmonic mask constraint is introduced as another way of leveraging score information during training, and we propose two novel adversarial losses for additional fine-tuning of both the transcriptor and the separator. Results demonstrate that using score information outperforms temporal weak-labels, and adversarial structures lead to further improvements in both separation and transcription performance. ","Transcription Is All You Need: Learning to Separate Musical Mixtures
  with Score as Supervision"
87,1319643961234018313,4275242492,Rithesh Kumar,"['Super excited to share our work today from @DescriptApp , NU-GAN : Neural upsampling using GANs. NU-GAN is a new audio super-resolution algorithm that can generate audio with high quality at the full resolution of 44.1 kHz. \nPaper: <LINK>', '1. NU-GAN is one of the first models to generate audio at the full 44.1 kHz resolution and is distinguishable from original high-res audio less than 10% higher than random chance, as verified using ABX listening tests.', '2.  We propose adding a 3rd component to the typical TTS pipeline, consisting of the super-resolution module, that upsamples audio at 22-24 kHz resolution to the full 44.1 kHz high resolution.', '3. NU-GAN solves the task of audio super-resolution by: i.) Using a DSP-based sinc interpolation algorithm to interpolate the input audio to full resolution ii.) Extract magnitudes, phase from the interpolated audio iii.) Predict missing higher frequency bins using a GAN', '3.... iv.) Invert back to raw waveform using the predicted magnitudes and reusing the phase from the input.', '4. Overall, this allows us to perform end-to-end text to speech synthesis at the full 44.1 kHz resolution, and this technology powers our Overdub product @DescriptApp  (https://t.co/qAjJexXVc8)', '5. Our demo page shows the performance of our neural resampling algorithm on single-speaker and multi-speaker datasets: https://t.co/gQnLhFda19']",https://arxiv.org/abs/2010.11362,"In this paper, we propose NU-GAN, a new method for resampling audio from lower to higher sampling rates (upsampling). Audio upsampling is an important problem since productionizing generative speech technology requires operating at high sampling rates. Such applications use audio at a resolution of 44.1 kHz or 48 kHz, whereas current speech synthesis methods are equipped to handle a maximum of 24 kHz resolution. NU-GAN takes a leap towards solving audio upsampling as a separate component in the text-to-speech (TTS) pipeline by leveraging techniques for audio generation using GANs. ABX preference tests indicate that our NU-GAN resampler is capable of resampling 22 kHz to 44.1 kHz audio that is distinguishable from original audio only 7.4% higher than random chance for single speaker dataset, and 10.8% higher than chance for multi-speaker dataset. ",NU-GAN: High resolution neural upsampling with GAN
88,1319599194701705217,50901426,Rafael Alves Batista,['The new paper by the Pierre Auger Collaboration (@augerobs) has been published in ApJ and #arXiv. We look for neutrinos from TXS 0506+056. The negative results set limits on the flux of ultra-high-energy neutrinos from this blazar.\n<LINK> <LINK>'],https://arxiv.org/abs/2010.10953,"Results of a search for ultra-high-energy neutrinos with the Pierre Auger Observatory from the direction of the blazar TXS 0506+056 are presented. They were obtained as part of the follow-up that stemmed from the detection of high-energy neutrinos and gamma rays with IceCube, \textit{Fermi}-LAT, MAGIC, and other detectors of electromagnetic radiation in several bands. The Pierre Auger Observatory is sensitive to neutrinos in the energy range from 100 PeV to 100 EeV and in the zenith angle range from $\theta=60^\circ$ to $\theta=95^\circ$, where the zenith angle is measured from the vertical direction. No neutrinos from the direction of TXS 0506+056 have been found. The results were analyzed in three periods: One of 6 months around the detection of IceCube-170922A, coinciding with a flare period of TXS 0506+056, a second one of 110 days during which the IceCube collaboration found an excess of 13 neutrinos from a direction compatible with TXS 0506+056, and a third one from 2004 January 1 up to 2018 August 31, over which the Pierre Auger Observatory has been taking data. The sensitivity of the Observatory is addressed for different spectral indices by considering the fluxes that would induce a single expected event during the observation period. For indices compatible with those measured by the IceCube collaboration the expected number of neutrinos at the Observatory is well-below one. Spectral indices as hard as 1.5 would have to apply in this energy range to expect a single event to have been detected. ","A search for ultra high energy neutrinos from TXS 0506+056 using the
  Pierre Auger Observatory"
89,1319545643011088384,729646261092126721,Alex James,"[""Our latest paper, 'A new trigger mechanism for CMES' has been accepted to A&amp;A and is available now on arXiv at <LINK>. Many thanks to my brilliant co-authors (and PhD supervisors) @Dr_Lucie, @gherardo_valori, and Lidia van Driel-Gesztelyi.""]",https://arxiv.org/abs/2010.11204,"Context: Many previous studies have shown that the magnetic precursor of a coronal mass ejection (CME) takes the form of a magnetic flux rope, and a subset of them have become known as `hot flux ropes' due to their emission signatures in $\sim$10 MK plasma. Aims: We seek to identify the processes by which these hot flux ropes form, with a view of developing our understanding of CMEs and thereby improving space weather forecasts. Methods: Extreme-ultraviolet observations were used to identify five pre-eruptive hot flux ropes in the solar corona and study how they evolved. Confined flares were observed in the hours and days before each flux rope erupted, and these were used as indicators of episodic bursts of magnetic reconnection by which each flux rope formed. The evolution of the photospheric magnetic field was observed during each formation period to identify the process(es) that enabled magnetic reconnection to occur in the $\beta<1$ corona and form the flux ropes. Results: The confined flares were found to be homologous events and suggest flux rope formation times that range from 18 hours to 5 days. Throughout these periods, fragments of photospheric magnetic flux were observed to orbit around each other in sunspots where the flux ropes had a footpoint. Active regions with right-handed (left-handed) twisted magnetic flux exhibited clockwise (anticlockwise) orbiting motions, and right-handed (left-handed) flux ropes formed. Conclusions: We infer that the orbital motions of photospheric magnetic flux fragments about each other bring magnetic flux tubes together in the corona, enabling component reconnection that forms a magnetic flux rope above a flaring arcade. This represents a novel trigger mechanism for solar eruptions and should be considered when predicting solar magnetic activity. ","A new trigger mechanism for coronal mass ejections: the role of confined
  flares and photospheric motions in the formation of hot flux ropes"
90,1319458053629116418,112654363,Torsten Scholak,"['Our new text-to-SQL paper is now on the ArXiv!\n<LINK>\n\nThe complexity of current systems that translate natural language to SQL queries is very high.\n\nRead up on how our fully transformer-based model enabled us to make things much simpler!', 'I‚Äôm deeply indebted to my coworkers at @element_ai, namely Raymond Li, @DBahdanau, Harm de Vries, and @chrisjpal. It was truly a team effort!\n\nWe published the code, too: https://t.co/wPul9GeLnL.', '@ipvkyte I won‚Äôt deny that I have thought about doing that üòÖ', '@ipvkyte I very much agree, I‚Äôm working on a port of @PyTorch to Haskell also because I think the language is a great fit for neural program synthesis.', '@ipvkyte @PyTorch Thanks! Indeed, we‚Äôve been struggling with the GC a bit, and there‚Äôs certainly room for improvement, but the library works already quite well.', '@mandubian Sweet, let me know what you think üòÉ', '@lzamparo Thanks Lee!', '@lzamparo It is very gratifying, all the hard work is starting to pay off üòÉ']",https://arxiv.org/abs/2010.11119,"Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema. ",DuoRAT: Towards Simpler Text-to-SQL Models
91,1319350226243645441,499487546,Srini Iyer,"['(1/2) New Paper! A re-ranking method to improve the performance of distantly supervised extractive open-domain QA models like DPR. We improve over DPR on 4 open domain QA datasets. ‚Äî with @sewon__min @scottyih @YasharMehdad\n\n<LINK>', '(2/2) We do this by a simple combination of two ingredients: (i) Re-ranking using harder negatives, and, (ii) Answer-span focused cross-attention using span marking.']",https://arxiv.org/abs/2010.10757,"State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a simple and effective re-ranking approach (RECONSIDER) for span-extraction tasks, that improves upon the performance of large pre-trained MRC models. RECONSIDER is trained on positive and negative examples extracted from high confidence predictions of MRC models, and uses in-passage span annotations to perform span-focused re-ranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positive passages, and achieves a new state of the art on four QA tasks, including 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. ","RECONSIDER: Re-Ranking using Span-Focused Cross-Attention for Open
  Domain Question Answering"
92,1319335805542858755,1204489532332265472,Yuhito Shibaike,"['I posted our new paper on arXiv accepted to Astronomy &amp; AstrophysicsüòÅ\nPlanetesimals can form in broad areas of protoplanetary discs with our now formation model!!\n<LINK>', '1) A first-generation planet (assumption) creates a gas pressure bump and drifting pebbles accumulate there, resulting in planetesimal formation via SI.\n2) The planetesimal formation region moves inward as the planet migrates inward.\n3) Planetesimals form in broad areas!', 'We show that planetesimals form with a typical pebble mass flux, and the final profiles of the planetesimals is estimated by a very simple equation depending on the flux and the migration speed of the planet, which can be used as the initial condition of population synthesis.', 'The existence of the ""first-generation"" planet is actually assumed in our model but recent observations of a young PPD with gap structures support such early formation of planets!\nhttps://t.co/8ECasFPyC7', 'We will show a more detailed model (including growth of pebbles, planets, discs etc.) in Paper II in the (hopefully nearüôÑ) future!!']",https://arxiv.org/abs/2010.10594,"To avoid known difficulties in planetesimal formation such as the drift or fragmentation barriers, many scenarios have been proposed. However, in these scenarios, planetesimals form in general only at some specific locations in protoplanetary discs. On the other hand, it is generally assumed in planet formation models and population synthesis models, that planetesimals are broadly distributed in the protoplanetary disc. Here we propose a new scenario in which planetesimals can form in broad areas of the discs. Planetesimals form at the gas pressure bump formed by a first-generation planet (e.g. formed by pebble accretion) and the formation region spreads inward in the disc as the planet migrates. We use a simple 1D Lagrangian particle model to calculate the radial distribution of pebbles in the gas disc perturbed by a migrating embedded planet. We consider that planetesimals form by streaming instability at the points where the pebble-to-gas density ratio on the mid-plane becomes larger than unity. We also study the effect of some key parameters like the ones of the gas disc model, the pebble mass flux, the migration speed of the planet, and the strength of turbulence. We find that planetesimals form in wide areas of the discs provided the flux of pebbles is typical and the turbulence is not too strong. The planetesimal surface density depends on the pebble mass flux and the migration speed of the planet. The total mass of the planetesimals and the orbital position of the formation area depend strongly on the pebble mass flux. We also find that the profile of the planetesimal surface density and its slope can be estimated by very simple equations. We show that our new scenario can explain the formation of planetesimals in broad areas. The simple estimates we provide for the planetesimal surface density profile can be used as initial conditions for population synthesis models. ","Planetesimal formation at the gas pressure bump following a migrating
  planet I. Basic characteristics of the new formation model"
93,1319311199188602880,171674815,Mark Marley,"['New @PlanetImager paper led by Kim Ward-Duong on the dusty substellar companion orbiting in the debris disk of an F5V star. Are we seeing pollution of the atmosphere by infalling dust? \n<LINK>', 'Look how red it is, almost falling off the plot on the right. https://t.co/LR5BYLBXdP', 'It is very hard for any of the self consistent forward models---from any group--to get so red. This reminded me a bit of the post-impact Jupiter after the S/L-9 impacts. https://t.co/WlMzxhqHUj', 'This brown dwarf is sitting inside of a dusty debris disk. Are we seen a dirty atmosphere? Kim used the approach we had followed in last decade in a paper with Kay Hiranaka and @kellecruz and checked to see if silicate dust could redden the spectrum.', 'Bottom line is that it seems plausible but a lot more work needs to be done. This object is ripe for retrieval studies to see if we can nail down the atmospheric structure. See the paper for more details.', ""@AstroThayne @sciboinkhobbes @PlanetImager You don't know how the pollution might be delivered. Maybe, as in SL/9 through larger objects. All the mass budget issues and dust lifetimes need to be looked at.""]",https://arxiv.org/abs/2010.10546,"We present new near-infrared Gemini Planet Imager (GPI) spectroscopy of HD 206893 B, a substellar companion orbiting within the debris disk of its F5V star. The $J$, $H$, $K1$, and $K2$ spectra from GPI demonstrate the extraordinarily red colors of the object, confirming it as the reddest substellar object observed to date. The significant flux increase throughout the infrared presents a challenging atmosphere to model with existing grids. Best-fit values vary from 1200 K to 1800 K for effective temperature and from 3.0 to 5.0 for log($g$), depending on which individual wavelength band is fit and which model suite is applied. The extreme redness of the companion can be partially reconciled by invoking a high-altitude layer of sub-micron dust particles, similar to dereddening approaches applied to the peculiar red field L-dwarf population. However, reconciling the HD 206893 B spectra with even those of the reddest low-gravity L-dwarf spectra still requires the contribution of additional atmospheric dust, potentially due to the debris disk environment in which the companion resides. Orbit fitting from four years of astrometric monitoring is consistent with a $\sim$30-year period, orbital inclination of 147$^{\circ}$, and semimajor axis of 10 au, well within the estimated disk inner radius of $\sim$50 au. As one of very few substellar companions imaged interior to a circumstellar disk, the properties of this system offer important dynamical constraints on companion-disk interaction and provide a benchmark for substellar and planetary atmospheric study. ","Gemini Planet Imager Spectroscopy of the Dusty Substellar Companion HD
  206893 B"
94,1319276370355130368,1310552063999438849,Hauke Group,"['üéáQuantum simulators of lattice gauge theories work reliably up to a sharp phase transition point. \n\nCheck out our new paper by Maarten Van Damme, @JCHalimeh, @PhilippHauke üëá\n\nMore üîé <LINK>\n\n#subatomic\n#GaugeSymmetry\n#quantum <LINK>']",http://arxiv.org/abs/2010.07338,"Gauge symmetry plays a key role in our description of subatomic matter. The vanishing photon mass, the long-ranged Coulomb law, and asymptotic freedom are all due to gauge invariance. Recent years have seen tantalizing progress in the microscopic reconstruction of gauge theories in engineered quantum simulators. Yet, many of these are plagued by a fundamental question: When gauge symmetry is only approximate in the quantum device, do we actually quantum-simulate a gauge theory? Here, we answer this question in the affirmative for a paradigm gauge theory akin to quantum electrodynamics. Analytically, we derive a renormalized gauge symmetry that is at least exponentially accurate. Further, numerically computing the phase diagram in the thermodynamic limit, we find that the long-distance behavior of the gauge theory is only compromised upon reaching a sharp quantum phase transition. This behavior is enabled by an energy penalty term, which lends a mass to the Higgs boson to which the coherent gauge breaking couples. Our results not only lend validity to ongoing gauge-theory quantum simulations, they also probe the fundamental question of how gauge symmetry could emerge in nature. ","Gauge-Symmetry Violation Quantum Phase Transition in Lattice Gauge
  Theories"
95,1319275723807285248,1141772501472727040,Lena Voita,"['[1/4] Analyzing Source and Target Contributions to NMT Predictions - new work with @iatitov and @RicoSennrich!\n\nWhat influences the predictions in NMT: the source or the target prefix? We measure and find out!\n\nPaper: <LINK>\nBlog: <LINK> #NLProc <LINK>', '[2/4] NMT models can hallucinate, i.e. ignore source. On the other hand, LMs can ignore gibberish prefixes. What will our model do?\n\nWe give a model random prefixes and measure source contribution. With a short prefix, it is ignored; with a long, the source is ignored.\n\n#NLProc https://t.co/okLAETRBsX', '[3/4] We show that exposure bias indeed leads to over-reliance on target history.\n\nWhen we give random prefixes to different models, the ones with alleviated exposure bias ignore the source less than others.\n\n#NLProc https://t.co/s16rmeWdo1', ""[4/4] In the paper, we also look at lot's of other things:  amount of training data, training stages, ...\n\nWe find that the training process is non-monotonic with several distinct stages (e.g. stages changing direction from decreasing influence of source to increasing). \n\n#NLProc https://t.co/KGpbT5SDQf""]",https://arxiv.org/abs/2010.10907,"In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature. ","Analyzing the Source and Target Contributions to Predictions in Neural
  Machine Translation"
96,1319274487297134592,280083723,Yoh Tanimoto,['new paper~ <LINK> we recovered the continuum (free) field algebra on the Minkowski space from the lattice algebras locally embedded in it using Daubechies wavelets~ this is the full version of <LINK>'],https://arxiv.org/abs/2010.11121,"We present a rigorous renormalization group scheme for lattice quantum field theories in terms of operator algebras. The renormalization group is considered as an inductive system of scaling maps between lattice field algebras. We construct scaling maps for scalar lattice fields using Daubechies' wavelets, and show that the inductive limit of free lattice ground states exists and the limit state extends to the familiar massive continuum free field, with the continuum action of spacetime translations. In particular, lattice fields are identified with the continuum field smeared with Daubechies' scaling functions. We compare our scaling maps with other renormalization schemes and their features, such as the momentum shell method or block-spin transformations. ",Scaling limits of lattice quantum fields by wavelets
97,1319234172439810055,2467076389,Kunihiko Tanaka (x2),"['NEW PAPER! \n<LINK>', 'ÈäÄÊ≤≥Á≥ª„ÅÆ‰∏≠ÂøÉÈÉ®„ÅØÂ§ßÈáè„ÅÆÈ´òÂØÜÂ∫¶„Ç¨„Çπ„Åå„ÅÇ„Çä„Å™„Åå„Çâ(ÊúÄËøë„Åù„Åì„ÅØ„Åô„Åì„ÅóÁñë„Çè„Åó„Åè„Å™„Å£„Å¶„Åç„Å¶„ÅÑ„Çã„ÅÆ„Åß„Åô„Åå)„ÄÅÊòüÂΩ¢Êàê„ÅåÂú∞Âë≥„Å™„Åì„Å®„ÅßÁü•„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÂπ∏„ÅÑÊòüÂΩ¢Êàê„ÅÇ„Çä/„Å™„Åó„ÅÆÈ´òÂØÜÂ∫¶„ÇØ„É©„É≥„Éó„ÅÆ„Åæ„Å®„Åæ„Å£„Åü„Éá„Éº„Çø„ÇíÊåÅ„Å£„Å¶„ÅÑ„Åæ„Åó„Åü„ÅÆ„Åß„ÄÅÁµ±Ë®àËß£Êûê„Å´„Åã„Åë„Å¶„ÄÅÊòüÂΩ¢Êàê„ÅÆÊúâÁÑ°„Å´Áõ∏Èñ¢„Åô„Çã„Éë„É©„É°„Éº„Çø„ÇíÊé¢„Åï„Åõ„Å¶„Åø„Åæ„Åó„Åü„ÄÇ', '‰Ωï„ÅÆ„Éë„É©„É°„Éº„Çø„Å´„ÇÇÁõ∏Èñ¢„Åó„Å¶„ÅÑ„Å™„ÅÑ„ÄÅ„Å®„ÅÑ„ÅÜÁµêÊûú„Çí‰∫àÊÉ≥„Åó„Å¶„ÅÑ„Åü„ÅÆ„Åß„Åô„Åå„ÄÅÂÆüÈöõ„Å´„ÅØ„Éì„É™„Ç¢„É´ÊØî(ÈáçÂäõ„ÅÆÂº∑„Åï„ÇíË°®„Åô„Éë„É©„É°„Éº„Çø)„Å´„ÅÆ„ÅøÈùûÂ∏∏„Å´Âº∑„ÅÑÁõ∏Èñ¢„ÇíÁ§∫„Åó„Åæ„Åó„Åü„ÄÇ„É©„É≥„ÉÄ„É†„Å´ÁîüÊàê„Åó„ÅüÂØæÁß∞Áæ§„Å®„ÅÆÊØîËºÉ„Å´ÂØæ„Åó„Å¶„ÇÇÊúâÊÑè„Åß„Åô„ÄÇ', 'ÊòüÂΩ¢Êàê„Å´Áõ¥Áµê„Åó„Å™„ÅÑ‰ΩéÂØÜÂ∫¶È†òÂüü„Å´ÂØæ„Åó„Å¶Âêå„ÅòËß£Êûê„ÇíË°å„ÅÜ„Å®„ÄÅ„Éì„É™„Ç¢„É´ÊØî„Å∏„ÅÆ‰æùÂ≠òÊÄß„ÅØÊ∂à„Åà„ÄÅÊòüÂΩ¢Êàê„ÅÆÊúâÁÑ°„ÅØ„É©„É≥„ÉÄ„É†„Å´Ëøë„Å•„Åç„Åæ„Åô„ÄÇ„Åì„ÅÆÂ†¥Âêà„ÅØÂ§ßÂ±ÄÁöÑ„Å™ÊòüÂΩ¢ÊàêÁéá„ÅØÈ†òÂüü„ÅÆ„Çπ„Ç±„Éº„É´(Â§ß„Åç„Åï„ÉªË≥™Èáè)„Å´„Çà„Å£„Å¶„ÅÆ„ÅøÊ±∫„Åæ„Çã„Çà„ÅÜ„Å´Ë¶ã„Åà„Çã„Åß„Åó„Çá„ÅÜ„ÄÇ', 'ÁµêÊûúËá™‰Ωì„ÅØÁâπ„Å´Â§ß„Åç„Å™È©ö„Åç„Åå„ÅÇ„Çã„ÇÇ„ÅÆ„Åß„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„Åå„ÄÅÁµ±Ë®àËß£Êûê„Å†„Åë„Åß„Åç„Çå„ÅÑ„Å™ÁµêË´ñ„Å´Ëá≥„Å£„Åü„ÅÆ„ÅåÈù¢ÁôΩ„Åã„Å£„Åü„ÅÆ„ÅßË´ñÊñáÂåñ„Åó„Å¶„Åø„Åæ„Åó„Åü„ÄÇ', '@tatary „ÅØ„ÅÑ„ÄÇ„Åù„Çå‰ª•Â§ñ„ÅÆÈáè„Å∏„ÅÆ‰æùÂ≠òÊÄß„ÅåÂá∫„Å¶„Åì„Å™„Åã„Å£„Åü„ÅÆ„ÅØ„Åù„Çå„Å™„Çä„Å´ÊÑèÂë≥„Åå„ÅÇ„Çã„Åã„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇÁ£ÅÂ†¥„Å®„Åã„ÅØÂÖ•„Å£„Å¶„ÅÑ„Å™„ÅÑ„Çì„Åß„Åô„Åë„Å©„ÄÇ', '@tatary collision rate„Å®ÊòüÂΩ¢Êàê„ÇØ„É©„É≥„Éó„ÅÆÊï∞„ÅØ„Åæ„Åö„Åæ„Åö‰∏ÄËá¥„Åó„Åæ„Åô„ÄÇ„Åß„Åô„ÅåÊúÄËøë„ÅÆË¶≥Ê∏¨„Å†„Å® collision „Åó„Å¶„ÅÑ„Åù„ÅÜ„Å™„ÅÆ„Å´ÊòüÂΩ¢Êàê„ÅÆ„Å™„ÅÑÈ†òÂüü„Åå„Åü„Åè„Åï„ÇìË¶ã„Å§„Åã„Å£„Å¶„ÅÑ„Å¶„ÄÅÂÖ®„Å¶„ÅÆcollision„ÅåÊòüÂΩ¢Êàê„Å´Ëá≥„Çã„Çè„Åë„Åß„ÅØ„Å™„ÅÑ„ÅÆ„Åß„ÅØ„Å™„Åã„Çç„ÅÜ„Åã„Å®„ÄÇ„Åù„ÅÆÂäπÁéá„ÇíÂê´„ÇÅ„Çã„Å®Ë®Ä„Çè„Çå„Å¶„ÅÑ„Çãcollision rate„Åß„ÅØË∂≥„Çä„Å™„Åï„Åù„ÅÜ„Å®„ÅÑ„ÅÜÊé®Ê∏¨„Åß„Åô']",https://arxiv.org/abs/2010.10552,"We report a statistical analysis exploring the origin of the overall low star formation efficiency (SFE) of the Galactic central molecular zone (CMZ) and the SFE diversity among the CMZ clouds using a wide-field HCN $J$=4-3 map, whose optically thin critical density ($\sim10^7\,\mathrm{cm}^{-3}$) is the highest among the tracers ever used in CMZ surveys. Logistic regression is performed to empirically formulate star formation probability of 195 HCN clumps, 13 of which contain star formation signatures. The explanatory parameters in the best-fit model are reduced into the virial parameter $\alpha_{\mathrm{vir}}$ without significant contribution from other parameters, whereas the performance of the model without $\alpha_{\mathrm{vir}}$ is no better than that using randomly generated data. The threshold $\alpha_{\mathrm{vir}}$ is 6, which translates into a volume density ($n_{\mathrm{H_2}}$) of $10^{4.6}\,\mathrm{cm}^{-3}$ with the $n_{\mathrm{H_2}}$-$\alpha_{\mathrm{vir}}$ correlation. The scarcity of the low-$\alpha_{\mathrm{vir}}$ clumps, whose fraction to all HCN clumps is 0.1, can be considered as one of the immediate causes of the suppressed SFE. No correlation between the clump size or mass and star formation probability is found, implying that HCN $J$=4-3 does not immediately trace the mass of star-forming gas above a threshold density. Meanwhile, star-forming and non-star-forming clouds are degenerate in the physical parameters of the CS $\mathit{J}$=1-0 clouds, highlighting the efficacy of the HCN $\mathit{J}$=4-3 line to probe star-forming regions in the CMZ. The time scale of the high-$\alpha_{\mathrm{vir}}$ to low-$\alpha_{\mathrm{vir}}$ transition is $\lesssim2$ Myr, which is consistent with the tidal compression and X1/X2 orbit transition models but possibly does not fit the cloud-cloud collision picture. ","HCN $J$=4-3, HNC $J$=1-0, $\mathrm{H^{13}CN}$ $J$=1-0, and
  $\mathrm{HC_3N}$ $J$=10-9 Maps of Galactic Center Region II.: Physical
  Properties of Dense Gas Clumps and Probability of Star Formation"
98,1319230356751585280,1219188017908371456,Branden Chan,"['Got a paper accepted at COLING2020! Stefan Schweter, Timo M√∂ller and I trained a new set of SoTA German BERT and ELECTRA models. We perform evaluation over the course of training to show just how quickly the models are learning. \n\nPreprint: <LINK>\n\n#NLProc #ML', 'Trained models are already available on the HF model hub under:\n\ndeepset/gbert-base\ndeepset/gbert-large\ndeepset/gelectra-base\ndeepset/gelectra-large']",https://arxiv.org/abs/2010.10906,"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. Our trained models will be made publicly available to the research community. ",German's Next Language Model
99,1319229657879695362,3260753346,Deepanway Ghosal,"['New paper at Findings of #EMNLP2020\nWe propose COSMIC and explore how commonsense knowledge can help in emotion recognition in conversations. \n\nPaper: <LINK>\n\nCode: <LINK>\nJoint work with N.Majumder, @gelbukh, @radamihalcea, @soujanyaporia <LINK>', 'COSMIC is built upon COMET (by @ABosselut, @YejinChoinka, et al.) and incorporates different elements of commonsense knowledge, including events, causal relations, and uses them to learn interactions between interlocutors participating in a conversation.', 'Our method achieves new SOTA results on four different benchmark datasets: IEMOCAP, DailyDialog, MELD, and EmoryNLP for recognizing emotions in conversations.']",https://arxiv.org/abs/2010.02795,"In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn interactions between interlocutors participating in a conversation. Current state-of-the-art methods often encounter difficulties in context propagation, emotion shift detection, and differentiating between related emotion classes. By learning distinct commonsense representations, COSMIC addresses these challenges and achieves new state-of-the-art results for emotion recognition on four different benchmark conversational datasets. Our code is available at this https URL ","COSMIC: COmmonSense knowledge for eMotion Identification in
  Conversations"
100,1319192387193835520,72781449,Nikos Aletras,"[""Check out Mali's new paper on complaint identification <LINK>. Comes with a qualitative analysis of the limitations of transformers in predicting accurately complaints including domain adaptation setups #NLProc <LINK>""]",https://arxiv.org/abs/2010.10910,"Complaining is a speech act extensively used by humans to communicate a negative inconsistency between reality and expectations. Previous work on automatically identifying complaints in social media has focused on using feature-based and task-specific neural network models. Adapting state-of-the-art pre-trained neural language models and their combinations with other linguistic information from topics or sentiment for complaint prediction has yet to be explored. In this paper, we evaluate a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87. ",Complaint Identification in Social Media with Transformer Networks
101,1319186108270432256,48829079,Idan Schwartz,"['In our recent #neurips2020 paper, we show how to maximize modalities information for better generalization. We also achieve the new state-of-the-art on VQA-CP and other datasets.\n@itai_gat @alexschwing @TamirHazan2 \nCode: <LINK>\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2010.10802,"Many recent datasets contain a variety of different data modalities, for instance, image, question, and answer data in visual question answering (VQA). When training deep net classifiers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classification results than others. This is suboptimal because the classifier is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classification result. However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2 and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efficacy of our method on Colored MNIST. ","Removing Bias in Multi-modal Classifiers: Regularization by Maximizing
  Functional Entropies"
102,1319090266612039681,516167077,Lorenzo Coviello,"['And in this other new paper, we propose a new family of activations, Smooth ReLU, that provide better accuracy-reproducibility tradeoffs in deep networks! <LINK>']",https://arxiv.org/abs/2010.09931,"Deep networks are gradually penetrating almost every domain in our lives due to their amazing success. However, with substantive performance accuracy improvements comes the price of \emph{irreproducibility}. Two identical models, trained on the exact same training dataset may exhibit large differences in predictions on individual examples even when average accuracy is similar, especially when trained on highly distributed parallel systems. The popular Rectified Linear Unit (ReLU) activation has been key to recent success of deep networks. We demonstrate, however, that ReLU is also a catalyzer to irreproducibility in deep networks. We show that not only can activations smoother than ReLU provide better accuracy, but they can also provide better accuracy-reproducibility tradeoffs. We propose a new family of activations; Smooth ReLU (\emph{SmeLU}), designed to give such better tradeoffs, while also keeping the mathematical expression simple, and thus implementation cheap. SmeLU is monotonic, mimics ReLU, while providing continuous gradients, yielding better reproducibility. We generalize SmeLU to give even more flexibility and then demonstrate that SmeLU and its generalized form are special cases of a more general methodology of REctified Smooth Continuous Unit (RESCU) activations. Empirical results demonstrate the superior accuracy-reproducibility tradeoffs with smooth activations, SmeLU in particular. ",Smooth activations and reproducibility in deep networks
103,1319022062384578569,811954374675202048,Bright Yufeng Ye,"['Check out our new paper on arXiv! <LINK>\n\nWe propose using a quartic potential qubit (quarton) for a giant (&gt;1 GHz) nonlinear coupling between *linearly decoupled* qubits. The quarton can also cancel the self-Kerr of qubits, linearizing them into photons!', 'Be sure to also check out the recent experimental quarton qubit paper https://t.co/tJ6oB9MbHL  and the traveling wave photon detector paper https://t.co/y40Seh5HqN which uses the quarton.']",https://arxiv.org/abs/2010.09959,"Strong nonlinear coupling of superconducting qubits and/or photons is a critical building block for quantum information processing. Due to the perturbative nature of the Josephson nonlinearity, linear coupling is often used in the dispersive regime to approximate nonlinear coupling. However, this dispersive coupling is weak and the underlying linear coupling mixes the local modes which, for example, distributes unwanted self-Kerr to photon modes. Here, we use the quarton to yield purely nonlinear coupling between two linearly decoupled transmon qubits. The quarton's zero $\phi^2$ potential enables a giant gigahertz-level cross-Kerr which is an order of magnitude stronger compared to existing schemes, and the quarton's positive $\phi^4$ potential can cancel the negative self-Kerr of qubits to linearize them into resonators. This giant cross-Kerr between bare modes of qubit-qubit, qubit-photon, and even photon-photon is ideal for applications such as single microwave photon detection and implementation of bosonic codes. ",Engineering Purely Nonlinear Coupling with the Quarton
104,1319011775984041984,998966120706174976,Ishaq Aden-Ali,"['New paper with @ashtiani_hassan and @thegautamkamath on privately learning Gaussians. \n\n<LINK>\n\nUnder aprox DP, we get near optimal bounds for learning Gaussians with known covariance and (likely) near optimal bounds for general Gaussians 1/8 <LINK>', 'We build on work by @markmbun @thegautamkamath @shortstein @zstevenwu that presented two DP algos for private hypothesis selection (hypotheses here is a set of distributions we want to pick from). The first is a pure DP algo that can learn given a finite set of distributions. 2/8', 'The second is an approx DP algo that can learn given an infinite set of distributions. The second algorithm requires a cover for the set of distributions that is ""locally small"" in some technical sense 3/8', 'Unfortunately, explicitly deriving such covers can be very tedious even for simple classes of distributions. We show that we can get around this. If a TV ball around *every* dist. in the set can be covered with a ""small"" # dists., then the class has a \'\'locally small\'\' cover 4/8', 'Showing that the TV ball around any distribution in the class can be covered is a simpler problem, making the analysis less hairy.  5/8', 'We also refine existing (and new) sample complexity bounds by doing a two step DP procedure: In the first step we learn a *really* rough estimate of the dist. using the Approx DP  algo. We then build a finite cover for a TV ball around the rough estimate. 6/8', 'We then use the pure DP algo to learn given this finite cover. Turns out doing this gets better dependence on the parameters of the problem. \n\nLastly, the algorithms we mentioned need to know ""how far"" the best dist. in the set is compared to the unknown dist. we sample from. 7/8', 'Non-privately, we dont need to know this. We give a pure DP and sample efficient agnostic algo that basically privatizes the minimum distance estimate (MDE) √† la Yatracos. MDE maximizes a low sensitivity function, so plugging it into the exponential mechanism ""just works""  8/8', 'We learned a lot about DP from @thegautamkamath and look forward to learning more :) 9/8', '@zafateniac Thanks Zain Bhai :)']",https://arxiv.org/abs/2010.09929,"We provide sample complexity upper bounds for agnostically learning multivariate Gaussians under the constraint of approximate differential privacy. These are the first finite sample upper bounds for general Gaussians which do not impose restrictions on the parameters of the distribution. Our bounds are near-optimal in the case when the covariance is known to be the identity, and conjectured to be near-optimal in the general case. From a technical standpoint, we provide analytic tools for arguing the existence of global ""locally small"" covers from local covers of the space. These are exploited using modifications of recent techniques for differentially private hypothesis selection. Our techniques may prove useful for privately learning other distribution classes which do not possess a finite cover. ","On the Sample Complexity of Privately Learning Unbounded
  High-Dimensional Gaussians"
105,1318847411633213440,2810180859,Christoph L√ºders üá∫üá¶üá™üá∫,['New paper: Algorithmic Reduction of Biological Networks With Multiple Time Scales. A symbolic algorithmic approach to compute invariant manifolds and reduced systems for differential equations modeling biological networks. <LINK>'],https://arxiv.org/abs/2010.10129,"We present a symbolic algorithmic approach that allows to compute invariant manifolds and corresponding reduced systems for differential equations modeling biological networks which comprise chemical reaction networks for cellular biochemistry, and compartmental models for pharmacology, epidemiology and ecology. Multiple time scales of a given network are obtained by scaling, based on tropical geometry. Our reduction is mathematically justified within a singular perturbation setting. The existence of invariant manifolds is subject to hyperbolicity conditions, for which we propose an algorithmic test based on Hurwitz criteria. We finally obtain a sequence of nested invariant manifolds and respective reduced systems on those manifolds. Our theoretical results are generally accompanied by rigorous algorithmic descriptions suitable for direct implementation based on existing off-the-shelf software systems, specifically symbolic computation libraries and Satisfiability Modulo Theories solvers. We present computational examples taken from the well-known BioModels database using our own prototypical implementations. ",Algorithmic Reduction of Biological Networks With Multiple Time Scales
106,1318840243362189312,249039303,Salman Khan,"['Interested to see how meta-learning can be used to model the evolution of learning trends across tasks, check out our new preprint!\nPaper link: <LINK> \n\n#MachineLearning #DeepLearning <LINK>']",https://arxiv.org/abs/2010.09291,"Meta-learning stands for 'learning to learn' such that generalization to new tasks is achieved. Among these methods, Gradient-based meta-learning algorithms are a specific sub-class that excel at quick adaptation to new tasks with limited data. This demonstrates their ability to acquire transferable knowledge, a capability that is central to human learning. However, the existing meta-learning approaches only depend on the current task information during the adaptation, and do not share the meta-knowledge of how a similar task has been adapted before. To address this gap, we propose a 'Path-aware' model-agnostic meta-learning approach. Specifically, our approach not only learns a good initialization for adaptation, it also learns an optimal way to adapt these parameters to a set of task-specific parameters, with learnable update directions, learning rates and, most importantly, the way updates evolve over different time-steps. Compared to the existing meta-learning methods, our approach offers: (a) The ability to learn gradient-preconditioning at different time-steps of the inner-loop, thereby modeling the dynamic learning behavior shared across tasks, and (b) The capability of aggregating the learning context through the provision of direct gradient-skip connections from the old time-steps, thus avoiding overfitting and improving generalization. In essence, our approach not only learns a transferable initialization, but also models the optimal update directions, learning rates, and task-specific learning trends. Specifically, in terms of learning trends, our approach determines the way update directions shape up as the task-specific learning progresses and how the previous update history helps in the current update. Our approach is simple to implement and demonstrates faster convergence. We report significant performance improvements on a number of FSL datasets. ",Meta-learning the Learning Trends Shared Across Tasks
107,1318830965771407361,712621727554146304,Oliver Stockdale,"['We have a new experimental (with theory!) paper on the arXiv!\n\nVortex matter, symmetry breaking transitions, negative temperatures, and thermalisation all appear in this preprint. \n\nEnjoy reading!\n\n<LINK> <LINK>', 'The above figure shows the distribution of vortex matter (collections of many vortices) for increasing energy at fixed angular momentum.\n\nThis paper shows how we generate and observe these distributions in a superfluid Bose-Einstein condensate.', 'A (very) short summary can be found at the link below:\n\nhttps://t.co/KZ8oRWRE8r']",https://arxiv.org/abs/2010.10049,"We experimentally study emergence of microcanonical equilibrium states in the turbulent relaxation dynamics of a two-dimensional chiral vortex gas. Same-sign vortices are injected into a quasi-two-dimensional disk-shaped atomic Bose-Einstein condensate using a range of mechanical stirring protocols. The resulting long-time vortex distributions are found to be in excellent agreement with the meanfield Poisson-Boltzmann equation for the system describing the microcanonical ensemble at fixed energy $\cal{H}$ and angular momentum $\cal{M}$. The equilibrium states are characterized by the corresponding thermodynamic variables of inverse temperature $\hat{\beta}$ and rotation frequency $\hat{\omega}$. We are able to realize equilibria spanning the full phase diagram of the vortex gas, including on-axis states near zero-temperature, infinite temperature, and negative absolute temperatures. At sufficiently high energies the system exhibits a symmetry-breaking transition, resulting in an off-axis equilibrium phase at negative absolute temperature that no longer shares the symmetry of the container. We introduce a point-vortex model with phenomenological damping and noise that is able to quantitatively reproduce the equilibration dynamics. ","Turbulent relaxation to equilibrium in a two-dimensional quantum vortex
  gas"
108,1318823285933674497,1134375290581524480,Kai Schmitz,"['""Model-independent energy budget for LISA"" <LINK>\n\nYay! New paper on the arXiv; in collaboration with Felix Giese, Thomas Konstandin, and Jorinde van de Vis. ü•≥ <LINK>']",https://arxiv.org/abs/2010.09744,"We provide an easy method to obtain the kinetic energy fraction in gravitational waves, generated during a cosmological first-order phase transition, as a function of only the wall velocity and quantities that can be determined from the particle physics model at the nucleation temperature. This generalizes recent work that achieved this goal for detonations. Here we present the corresponding results for deflagrations and hybrids. Unlike for detonations, the sound speed in the symmetric phase also enters the analysis. We perform a detailed comparison between our model-independent approach and other approaches in the literature. We provide a Python code snippet to determine the kinetic energy fraction $K$ as a function of the wall velocity, the two speeds of sound and the strength parameter of the phase transition. We also assess how realistic sizable deviations in speed of sound are close to the phase transition temperature in a specific model. ",Model-independent energy budget for LISA
109,1318821901217705989,4878011,Matthias Rosenkranz ‚ú®,"['New paper out üéâ\n\nWe introduce variational quantum-classical Wasserstein GANs with gradient penalty and embed them in a classical framework for anomaly detection.\n\nWith Daniel Herr and Benjamin Obert.\n<LINK> <LINK>', 'Tests on a credit card dataset show F1 scores competitive with the fully classical architecture. The classical upscaling in the generator allows us to use all features of the dataset. https://t.co/Zq6fq8hR1K', ""Amongst other things we check the training convergence for finite measurement shots. It's a little slower but remains OK for 100 samples only https://t.co/oajNwXL3FS"", 'Big thanks to collaborators Daniel Herr and Benjamin Obert. üôè', ""Oh, here's Scirate :) \nhttps://t.co/FaamaytCm3"", '@jj_xyz Thanks, Johannes']",https://arxiv.org/abs/2010.10492,"Generative adversarial networks (GANs) are a machine learning framework comprising a generative model for sampling from a target distribution and a discriminative model for evaluating the proximity of a sample to the target distribution. GANs exhibit strong performance in imaging or anomaly detection. However, they suffer from training instabilities, and sampling efficiency may be limited by the classical sampling procedure. We introduce variational quantum-classical Wasserstein GANs to address these issues and embed this model in a classical machine learning framework for anomaly detection. Classical Wasserstein GANs improve training stability by using a cost function better suited for gradient descent. Our model replaces the generator of Wasserstein GANs with a hybrid quantum-classical neural net and leaves the classical discriminative model unchanged. This way, high-dimensional classical data only enters the classical model and need not be prepared in a quantum circuit. We demonstrate the effectiveness of this method on a credit card fraud dataset. For this dataset our method shows performance on par with classical methods in terms of the $F_1$ score. We analyze the influence of the circuit ansatz, layer width and depth, neural net architecture parameter initialization strategy, and sampling noise on convergence and performance. ","Anomaly detection with variational quantum generative adversarial
  networks"
110,1318712686696304640,21053572,Dr. Chenoa Tremblay (she/her),"['My recent paper will be published in Astrophysical Journals.  We present new molecular modelling for 14^NO and 15^NO and a deep, blind molecular line survey toward the Vela Constellation with the @mwatelescope.  <LINK>']",https://arxiv.org/abs/2010.09868,"We present new molecular modelling for 14NO and 15NO and a deep, blind molecular line survey at low radio frequencies (99-129 MHz). This survey is the third in a series completed with the Murchison Widefield Array (MWA), but in comparison with the previous surveys, uses four times more data (17 hours vs. 4 hours) and is three times better in angular resolution (1' vs. 3'). The new molecular modelling for nitric oxide and its main isotopologue has seven transitions within the MWA frequency band (although we also present the higher frequency transitions). Although we did not detect any new molecular lines at a limit of 0.21 Jy beam^-1, this work is an important step in understanding the data processing challenges for the future Square Kilometre Array (SKA) and places solid limits on what is expected in the future of low-frequency surveys. The modelling can be utilised for future searches of nitric oxide. ","Nitric Oxide and other molecules: Molecular Modelling and Low
  FrequencyExploration using the Murchison Widefield Array"
111,1318694153660760066,296161364,Chris Power,"['New paper on the @arxiv - <LINK> - led by former @ICRAR @ARC_ASTRO3D student, Rhys Poulton - a new formula, derived from large cosmo N-body sims, to estimate the merger timescale of dark matter subhalos and satellites - with Aaron, Pascal, @CDPLagos']",https://arxiv.org/abs/2010.08786,"Predicting the merger timescale ($\tau_{\rm merge}$) of merging dark matter halos, based on their orbital parameters and the structural properties of their hosts, is a fundamental problem in gravitational dynamics that has important consequences for our understanding of cosmological structure formation and galaxy formation. Previous models predicting $\tau_{\rm merge}$ have shown varying degrees of success when compared to the results of cosmological $N$-body simulations. We build on this previous work and propose a new model for $\tau_{\rm merge}$ that draws on insights derived from these simulations. We find that published predictions can provide reasonable estimates for $\tau_{\rm merge}$ based on orbital properties at infall, but tend to underpredict $\tau_{\rm merge}$ inside the host virial radius ($R_{200}$) because tidal stripping is neglected, and overpredict it outside $R_{200}$ because the host mass is underestimated. Furthermore, we find that models that account for orbital angular momentum via the circular radius $R_{\rm circ}$ underpredict (overpredict) $\tau_{\rm merge}$ for bound (unbound) systems. By fitting for the dependence of $\tau_{\rm merge}$ on various orbital and host halo properties,we derive an improved model for $\tau_{\rm merge}$ that can be applied to a merging halo at any point in its orbit. Finally, we discuss briefly the implications of our new model for $\tau_{\rm merge}$ for semi-analytical galaxy formation modelling. ",Extracting Galaxy Merger Timescales II: A new fitting formula
112,1318611154420334592,1545756036,Mike Boylan-Kolchin,"['Check out @astro_jenna‚Äôs thread and her new paper, out today! <LINK> <LINK>']",https://arxiv.org/abs/2010.08571,"We examine the prevalence, longevity, and causes of planes of satellite dwarf galaxies, as observed in the Local Group. We use 14 Milky Way/Andromeda-(MW/M31) mass host galaxies from the FIRE-2 simulations. We select the 14 most massive satellites by stellar mass within 300 kpc of each host and correct for incompleteness from the foreground galactic disc when comparing to the MW. We find that MW-like planes as spatially thin and/or kinematically coherent as observed are uncommon, but they do exist in our simulations. Spatially thin planes occur in 1-2 per cent of snapshots during $z=0-0.2$, and kinematically coherent planes occur in 5 per cent of snapshots. These planes are generally transient, surviving for less than 500 Myr. However, if we select hosts with an LMC-like satellite near first pericentre, the fraction of snapshots with MW-like planes increases dramatically to 7-16 per cent, with lifetimes of 0.7-1 Gyr, likely because of group accretion of satellites. We find that M31's satellite distribution is much more common: M31's satellites lie within about 1 sigma of the simulation median for every plane metric we consider. We find no significant difference in average satellite planarity for isolated hosts versus hosts in LG-like pairs. Baryonic and dark matter-only simulations exhibit similar levels of planarity, even though baryonic subhaloes are less centrally concentrated within their host haloes. We conclude that planes of satellites are not a strong challenge to LCDM cosmology. ","Planes of satellites around Milky Way/M31-mass galaxies in the FIRE
  simulations and comparisons with the Local Group"
113,1318576557808562178,841499391508779008,Zico Kolter,"['Current backdoor poisoning attacks aren\'t just vulnerable to attackers with a ""secret trigger"". They can easily be broken (creating a new trigger that is just as effective) given access to the classifier.  New paper with @Eric_jie_thu and @agsidd10.\n\n<LINK> <LINK>']",http://arxiv.org/abs/2010.09080,"Under a commonly-studied backdoor poisoning attack against classification models, an attacker adds a small trigger to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is incorrect. We describe a new threat model for poisoned classifier, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, we propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of smoothed adversarial images with human interaction. We demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI. We also compare our approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. Last, we include a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Thus, we argue that there is no such thing as a secret backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier. ","Poisoned classifiers are not only backdoored, they are fundamentally
  broken"
114,1318567762407677952,887589711908188161,Mingjie Sun,"[""Is the backdoor secret? Checkout our new work on ''breaking'' poisoned classifiers,  where we use neat ideas in adversarial robustness to analyze backdoored classifiers. Joint work with @agsidd10 &amp; @zicokolter.\n\nPaper: <LINK>\nCode: <LINK> <LINK>"", 'For a poisoned classifier, we construct a robustified smoothed classifier. We extract colors or cropped patches from adversarial examples of the smoothed classifier to create new triggers. These new triggers have similar or higher attack success rate than the original backdoor.', 'Open questions:\n1. Are there backdoor attacks that can avoid our attack?\n2. From our results, it seems that backdoor poisoning creates a spectrum of potential backdoors. It is natural to ask what is actually learnt through the backdoor poisoning process?']",https://arxiv.org/abs/2010.09080,"Under a commonly-studied backdoor poisoning attack against classification models, an attacker adds a small trigger to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is incorrect. We describe a new threat model for poisoned classifier, where one without knowledge of the original trigger, would want to control the poisoned classifier. Under this threat model, we propose a test-time, human-in-the-loop attack method to generate multiple effective alternative triggers without access to the initial backdoor and the training data. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a procedure called Denoised Smoothing, and then extracting colors or cropped portions of smoothed adversarial images with human interaction. We demonstrate the effectiveness of our attack through extensive experiments on high-resolution datasets: ImageNet and TrojAI. We also compare our approach to previous work on modeling trigger distributions and find that our method are more scalable and efficient in generating effective triggers. Last, we include a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Thus, we argue that there is no such thing as a secret backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier. ","Poisoned classifiers are not only backdoored, they are fundamentally
  broken"
115,1318555070791901193,943865672550973445,Ryan Plestid,"['Another new paper out today with Richard Hill (and lots of input from Pavel Murat and Michael Mackenzie) \n\n<LINK> <LINK>', ""1/ Mu2e and COMET are going to look for muon to electron conversion  but they can also look for muon to positron conversion \n\n(see Michael's talk) \nhttps://t.co/hS3Rpg8Gvk\n(our LOI) \nhttps://t.co/bGsir2Arh9\nOr \nhttps://t.co/WWLog5RgWw\n\n by @kjkelly_physics @JBerryman0814 etc."", '2/ This channel tends to have a lower-energy signal region (i.e. the energy of the positron in mu-&gt; positron is lower than the energy of the electron in mu -&gt; electron). \n\nhttps://t.co/CrZlz2eirJ\n\nThis can cause overlap of new physics signals with backgrounds from RMC https://t.co/akFqc1dP15', '3/ The basic problem is that the signal of new physics is a muon capturing on a nucleus, interacting via some BSM operator and turning into a positron, which is then emitted with a high energy. \n\nRMC can look very similar in one of two ways...', '4/ A high energy photon can be emitted in a SM muon capture event, and can pair produce in the detector material. \n\nAlternatively,  the photon can go off-shell and mediate production of e^+ e^- pairs ""internally"".  If the e^+ has enough energy it would contaminate the signal.', '5/ If you know the photon spectrum from RMC, then the positron spectrum from photons converting ""externally"" can be studied with a Monte Carlo simulation. \n\nWhat about the internal conversions? The point of this paper is to show that you can predict those too.', '6/ We have basically reconsidered a semi-famous old problem from the early days of QED (see pic). \n\nThe goal was to understand when you can approximate a nuclear matrix element for pair-production by the nuclear matrix element for photon emission. https://t.co/NJO6DTFPXG', '7/ We basically study how large the error in this approximation is in various regions of phase space. \n\nOur main conclusion: You can safely approximate the matrix elements if the photon\'s ""mass"" is small AND the positron is emitted nearly collinear to the photon\'s direction. https://t.co/dYYxSrSCKo', '8/ It turns out that these two conditions are naturally satisfied near the end-point. This is what is important for Mu2e and COMET: yay! \n\nThis means that the photon spectrum from RMC on nuclei is predictive of the positron (or electron) spectrum https://t.co/79PgaGfjeP', '9/ We are working on a follow up work taking into account the fact that electrons and positrons are actually being produced in the Coulomb field of the nucleus. \n\nStay tuned!']",https://arxiv.org/abs/2010.09509,"The Mu2e and COMET collaborations will search for nucleus-catalyzed muon conversion to positrons ($\mu^-\rightarrow e^+$) as a signal of lepton number violation. A key background for this search is radiative muon capture where either: 1) a real photon converts to an $e^+ e^-$ pair ""externally"" in surrounding material, or 2) a virtual photon mediates the production of an $e^+e^-$ pair ""internally"". If the $e^+$ has an energy approaching the signal region then it can serve as an irreducible background. In this work we describe how the near end-point internal positron spectrum can be related to the real photon spectrum from the same nucleus, which encodes all non-trivial nuclear physics. ","The high energy spectrum of internal positrons from radiative muon
  capture on nuclei"
116,1318549536269361158,943865672550973445,Ryan Plestid,"['Alright, last week was crazy but that works out fine since the companion paper came out today! \n\nTwo new papers out explaining how to constrain neutrino portals by searching for upscattering inside the Earth \n\n<LINK>\n<LINK>\n\nThread...', '1/ ""Neutrino portals"" are some extra interaction beyond the SM that couples a left-handed neutrino to a (as of yet undiscovered and therefore hypothetical) sterile neutrino / heavy neutral lepton. \n\nThese particles can be hard to discover because they are ""sterile""...', '2/ i.e. they do not interact very strongly. The best way to look for them is to hope they get produced close to an experiment and look for them to decay inside the detector (leaving a photon or electron-positron pair behind)...', '3/ One problem with this strategy is that the HNLs can take a very-long time to decay so even if you produce one it has only a small chance of decaying inside the experiment.', '4/. These two papers point out that if the HNL has such a long lifetime, then it does not need to be produced inside the detector itself! In fact, for a lot of interesting parameter space the HNLs could be produced *anywhere* inside the Earth https://t.co/FrBTjsCk09', '5/ One fun little consequence is that if the decay length is smaller than the radius of the Earth then the event rate inside a detector can actually become independent of the decay length: no penalty at all! https://t.co/tibTjE8n27', '6/ In some models the upscattering is (quasi-)isotropic and so the whole Earth ""lights up"", while in other models the upscattering is mostly forward so you have to think carefully about the geometry of the Earth relative to the Sun https://t.co/IIzSdmszBg', '7/ I made use of solar neutrinos in this work, which simplifies a lot of the analysis. Nuclei can be treated as static sources because of the relatively low neutrino energies.  This limits the reach to lower masses, but this is where constraints on HNLs tend to weaken', '8/ In the case of a neutrino dipole portal, I find that this is the best method for search for O(MeV) HNLs. https://t.co/y8LhOVop8W', '9/ The constraints are roughly independent of the neutrino flavor because the solar neutrino flux is pretty inclusive when it arrives to Earth (ie there are \\nu_\\tau, \\nu_\\mu, and \\nu_e). \n\nThis is true for both the constraints on dipole portals and mass-mixing portals.', '10/ For the mass-mixing portal, I find that the signals tend to be much smaller because the upscattering takes place via the weak interaction instead of the electromagnetic one. \n\nNevertheless upscattering inside the Earth provides new constraints on \\nu_\\tau mixing https://t.co/tfQ2JV4lDk', '11/ ....and I meant to post this figure as well but I forgot. https://t.co/eJY6en5ZzJ']",https://arxiv.org/abs/2010.04193,"Solar neutrinos upscattering inside the Earth can source unstable particles that can decay inside terrestrial detectors. Contrary to naive expectations we show that when the decay length is much shorter than the radius of the \emph{Earth} (rather than the detector), the event rate is independent of the decay length. In this paper we study a transition dipole operator (neutrino dipole portal) and show that Borexino's existing data probes previously untouched parameter space in the 0.5--20 MeV regime, complementing recent cosmological and supernova bounds. We briefly comment on similarities and differences with luminous dark matter and comment on future prospects for analogous signals stemming from atmospheric neutrinos. ",Luminous solar neutrinos I: Dipole portals
117,1318445578943168512,636864273,Christoph Molnar,"[""We have a new paper on arxiv üéâüéâ\nInterpretable Machine Learning - A Brief History, State-of-the-Art and Challenges.\n\nBest to read in a comfortable chair with a cup of coffee/tea. It's an extended abstract to a keynote I gave at the ECML XKDD workshop.   \n<LINK>"", '@9ncM9v2R Good catch. Thanks!', '@lawouach I went a bit overboard thereüòÖ']",https://arxiv.org/abs/2010.09337,"We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences. ","Interpretable Machine Learning -- A Brief History, State-of-the-Art and
  Challenges"
118,1318440960997519361,164135176,Mohit Shridhar,"['(1/4) Can reasoning üí≠ in textworld help agents solve tasks in embodied environments ü§ñ? \n\nCheckout our new paper on aligning text and embodied environments.\nPaper: <LINK> \nData &amp; Code: <LINK> <LINK>', '(2/4) We generate interactive TextWorld games for scenes in the ALFRED dataset. Agents learn abstract policies in TextWorld by leveraging semantic priors.', '(3/4) Results show that TextWorld training does indeed help with embodied tasks. Our TextWorld-trained agent, BUTLER, generalizes better to new tasks by reasoning in high-level ‚Äòtextual‚Äô space rather than through low-level visual representations.', '(4/4) This was joint work with @ericxyuan, @Cote_Marc, @ybisk, @APTrizzle, and @mhauskn as part of my @MSFTResearch internship.']",http://arxiv.org/abs/2010.03768,"Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (C\^ot\'e et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding). ","ALFWorld: Aligning Text and Embodied Environments for Interactive
  Learning"
119,1318437241686052864,32924635,Andrej Kastrin,['Our new paper on COVID-19 drug repurposing in now on arXiv <LINK>'],https://arxiv.org/abs/2010.09600,"Objective: To discover candidate drugs to repurpose for COVID-19 using literature-derived knowledge and knowledge graph completion methods. Methods: We propose a novel, integrative, and neural network-based literature-based discovery (LBD) approach to identify drug candidates from both PubMed and COVID-19-focused research literature. Our approach relies on semantic triples extracted using SemRep (via SemMedDB). We identified an informative subset of semantic triples using filtering rules and an accuracy classifier developed on a BERT variant, and used this subset to construct a knowledge graph. Five SOTA, neural knowledge graph completion algorithms were used to predict drug repurposing candidates. The models were trained and assessed using a time slicing approach and the predicted drugs were compared with a list of drugs reported in the literature and evaluated in clinical trials. These models were complemented by a discovery pattern-based approach. Results: Accuracy classifier based on PubMedBERT achieved the best performance (F1= 0.854) in classifying semantic predications. Among five knowledge graph completion models, TransE outperformed others (MR = 0.923, Hits@1=0.417). Some known drugs linked to COVID-19 in the literature were identified, as well as some candidate drugs that have not yet been studied. Discovery patterns enabled generation of plausible hypotheses regarding the relationships between the candidate drugs and COVID-19. Among them, five highly ranked and novel drugs (paclitaxel, SB 203580, alpha 2-antiplasmin, pyrrolidine dithiocarbamate, and butylated hydroxytoluene) with their mechanistic explanations were further discussed. Conclusion: We show that an LBD approach can be feasible for discovering drug candidates for COVID-19, and for generating mechanistic explanations. Our approach can be generalized to other diseases as well as to other clinical questions. ",Drug Repurposing for COVID-19 via Knowledge Graph Completion
120,1318435729186443264,2483365093,Francesco Crecchi,['Our new paper ‚ÄúFADER: Fast Adversarial Example Detection‚Äù is out! Check it if you want to speed up adversarial examples detection! <LINK>\n@biggiobattista @MarcoMelisIT @sotgiu_angelo'],http://arxiv.org/abs/2010.09119,"Deep neural networks are vulnerable to adversarial examples, i.e., carefully-crafted inputs that mislead classification at test time. Recent defenses have been shown to improve adversarial robustness by detecting anomalous deviations from legitimate training samples at different layer representations - a behavior normally exhibited by adversarial attacks. Despite technical differences, all aforementioned methods share a common backbone structure that we formalize and highlight in this contribution, as it can help in identifying promising research directions and drawbacks of existing methods. The first main contribution of this work is the review of these detection methods in the form of a unifying framework designed to accommodate both existing defenses and newer ones to come. In terms of drawbacks, the overmentioned defenses require comparing input samples against an oversized number of reference prototypes, possibly at different representation layers, dramatically worsening the test-time efficiency. Besides, such defenses are typically based on ensembling classifiers with heuristic methods, rather than optimizing the whole architecture in an end-to-end manner to better perform detection. As a second main contribution of this work, we introduce FADER, a novel technique for speeding up detection-based methods. FADER overcome the issues above by employing RBF networks as detectors: by fixing the number of required prototypes, the runtime complexity of adversarial examples detectors can be controlled. Our experiments outline up to 73x prototypes reduction compared to analyzed detectors for MNIST dataset and up to 50x for CIFAR10 dataset respectively, without sacrificing classification accuracy on both clean and adversarial data. ",FADER: Fast Adversarial Example Rejection
121,1318401010717569024,901266828655284225,Brian Metzger,"['<LINK>  @navinsridhar \'s paper on a new mechanism for generating an FRB-like signal from the terminal stage of neutron star merger.  The basic idea is that you can think of a magnetized neutron star in a tight binary as ""spinning up"" instead of the usual braking.', 'If the velocity of the binary wind is rising approaching the merger (uncertain but possible - depends on messy pair creation), internal shocks in the outflow can give rise to synchrotron maser emission, generating a bright radio pulse similar to an FRB.']",https://arxiv.org/abs/2010.09214,"During the final stages of a compact object merger, if at least one of the binary components is a magnetized neutron star (NS), then its orbital motion substantially expands the NS's open magnetic flux -- and hence increases its wind luminosity -- relative to that of an isolated pulsar. As the binary orbit shrinks due to gravitational radiation, the power and speed of this binary-induced inspiral wind may (depending on pair loading) secularly increase, leading to self-interaction and internal shocks in the outflow beyond the binary orbit. The magnetized forward shock can generate coherent radio emission via the synchrotron maser process, resulting in an observable radio precursor to binary NS merger. We perform 1D relativistic hydrodynamical simulations of shock interaction in the accelerating binary NS wind, assuming that the inspiral wind efficiently converts its Poynting flux into bulk kinetic energy prior to the shock radius. This is combined with the shock maser spectrum from particle-in-cell simulations, to generate synthetic radio light curves. The precursor burst with a fluence of $\sim1$ Jy$\cdot$ms at $\sim$GHz frequencies lasts $\sim 1-500$ ms following the merger for a source at $\sim3$ Gpc ($B_{\rm d}/10^{12}$ G)$^{8/9}$, where $B_{\rm d}$ is the dipole field strength of the more strongly-magnetized star. Given an outflow geometry concentrated along the binary equatorial, the signal may be preferentially observable for high-inclination systems, i.e. those least likely to produce a detectable gamma-ray burst. ","Shock-powered radio precursors of neutron star mergers from accelerating
  relativistic binary winds"
122,1318362281017712641,1316512308466647040,Christoph Bergmeir,['We have a new paper on forecasting weekly time series: <LINK>'],https://arxiv.org/abs/2010.08158,"Many businesses and industries require accurate forecasts for weekly time series nowadays. The forecasting literature however does not currently provide easy-to-use, automatic, reproducible and accurate approaches dedicated to this task. We propose a forecasting method that can be used as a strong baseline in this domain, leveraging state-of-the-art forecasting techniques, forecast combination, and global modelling. Our approach uses four base forecasting models specifically suitable for forecasting weekly data: a global Recurrent Neural Network model, Theta, Trigonometric Box-Cox ARMA Trend Seasonal (TBATS), and Dynamic Harmonic Regression ARIMA (DHR-ARIMA). Those are then optimally combined using a lasso regression stacking approach. We evaluate the performance of our method against a set of state-of-the-art weekly forecasting models on six datasets. Across four evaluation metrics, we show that our method consistently outperforms the benchmark methods by a considerable margin with statistical significance. In particular, our model can produce the most accurate forecasts, in terms of mean sMAPE, for the M4 weekly dataset. ",A Strong Baseline for Weekly Time Series Forecasting
123,1318359589205676032,2377407248,Daniel Whiteson,"['New paper! \n\n   Permutationless Many-Jet Event Reconstruction with Symmetry Preserving Attention Networks\n<LINK>\n\nLed by @mfentonHEP and Alex Shmakov. <LINK>', ""It's hard to know which thing we see in our detector came from which leg of the Feynman diagram. \n\nPreviously, most ideas just tried them *all*.   \n\nWe tried a neural network that had a symmetry similar to the task: https://t.co/eakU3fF96z"", 'Using a new idea Alex calls ""tensor attention"" where the structure of the tensors reflects the permutation symmetry of the problem: https://t.co/wVphBcgJmV', 'And it works... really well!  It gets the right assignments a lot more often then the current state of the art. https://t.co/u2heTlqY2r', ""@theory_dad Yes! This is @mfentonHEP's brain child, so he should comment. \n\nBut it should be applicable to any multi-jet final state with explosive combinatorics."", '@BryanJField Yes, it naturally incorporates flavor tags.']",http://arxiv.org/abs/2010.09206,"Top quarks, produced in large numbers at the Large Hadron Collider, have a complex detector signature and require special reconstruction techniques. The most common decay mode, the ""all-jet"" channel, results in a 6-jet final state which is particularly difficult to reconstruct in $pp$ collisions due to the large number of permutations possible. We present a novel approach to this class of problem, based on neural networks using a generalized attention mechanism, that we call Symmetry Preserving Attention Networks (SPA-Net). We train one such network to identify the decay products of each top quark unambiguously and without combinatorial explosion as an example of the power of this technique.This approach significantly outperforms existing state-of-the-art methods, correctly assigning all jets in $93.0%$ of $6$-jet, $87.8%$ of $7$-jet, and $82.6%$ of $\geq 8$-jet events respectively. ","Permutationless Many-Jet Event Reconstruction with Symmetry Preserving
  Attention Networks"
124,1318358290066067456,101980926,Masahito Yamazaki,"['A new paper, this time in hep-lat!\n""Is N=2 Large?"" (Ryuichiro Kitano, Norikazu Yamada, Masahito Yamazaki) <LINK>']",https://arxiv.org/abs/2010.08810,"We study $\theta$ dependence of the vacuum energy for the 4d SU(2) pure Yang-Mills theory by lattice numerical simulations. The response of topological excitations to the smearing procedure is investigated in detail, in order to extract topological information from smeared gauge configurations. We determine the first two coefficients in the $\theta$ expansion of the vacuum energy, the topological susceptibility $\chi$ and the first dimensionless coefficient $b_2$, in the continuum limit. We find consistency of the SU(2) results with the large $N$ scaling. By analytic continuing the number of colors, $N$, to non-integer values, we infer the phase diagram of the vacuum structure of SU(N) gauge theory as a function of $N$ and $\theta$. Based on the numerical results, we provide quantitative evidence that 4d SU(2) Yang-Mills theory at $\theta = \pi$ is gapped with spontaneous breaking of the CP symmetry. ",Is $N=2$ Large?
125,1318356831878537216,661613,"Alex Hanna, Ph.D., NREMT","['New paper: @teeepain and I wrote a short piece for the recent #CSCW2020 workshop on Reconsidering Scale and Scaling, wherein we try to map the dimensions of ""scale thinking"" in Valley culture and map out resistances in mutual aid <LINK>', 'This owes a lot to the intervention that @gleemie has made on ""design thinking"" \nand @ruha9\'s discussion of design, and was initially inspired by Bed-Stuy mutual aid organizers (@bedstuystrong) who wanted to build tech that ""didn\'t scale."" \n\nhttps://t.co/froDW042Pi', ""@EZanichkows Thank you, Elizabeth ‚ù§Ô∏è I mostly yell at engineers about data. Can't wait to have a thanksgiving with you in the near future!"", ""@kaytwo @gleemie @ruha9 @bedstuystrong Oh wow! I'd love to talk to you more about that sometime.""]",https://arxiv.org/abs/2010.08850,"At the heart of what drives the bulk of innovation and activity in Silicon Valley and elsewhere is scalability. This unwavering commitment to scalability -- to identify strategies for efficient growth -- is at the heart of what we refer to as ""scale thinking."" Whether people are aware of it or not, scale thinking is all-encompassing. It is not just an attribute of one's product, service, or company, but frames how one thinks about the world (what constitutes it and how it can be observed and measured), its problems (what is a problem worth solving versus not), and the possible technological fixes for those problems. This paper examines different facets of scale thinking and its implication on how we view technology and collaborative work. We argue that technological solutions grounded in scale thinking are unlikely to be as liberatory or effective at deep, systemic change as their purveyors imagine. Rather, solutions which resist scale thinking are necessary to undo the social structures which lie at the heart of social inequality. We draw on recent work on mutual aid networks and propose questions to ask of collaborative work systems as a means to evaluate technological solutions and guide designers in identifying sites of resistance to scale thinking. ",Against Scale: Provocations and Resistances to Scale Thinking
126,1318216255266906117,580031141,"James Davenport, PhD",['Really excited for this new paper by @astroilin \n<LINK>\nFlare rates in open clusters with K2 - incl. Rup 147 and M 167!!!'],https://arxiv.org/abs/2010.05576,"Flares, energetic eruptions on the surfaces of stars, are an unmistakable manifestation of magnetically driven emission. Their occurrence rates and energy distributions trace stellar characteristics such as mass and age. But before flares can be used to constrain stellar properties, the flaring-age-mass relation requires proper calibration. This work sets out to quantify flaring activity of independently age-dated main sequence stars for a broad range of spectral types using optical light curves obtained by the Kepler satellite. Drawing from the complete K2 archive, we searched 3435 $\sim 80$ day long light curves of 2111 open cluster members for flares using the open-source software packages K2SC to remove instrumental and astrophysical variability from K2 light curves, and AltaiPony to search and characterize the flare candidates. We confirmed a total of 3844 flares on high probability open cluster members with ages from zero age main sequence (Pleiades) to 3.6 Gyr (M67). We extended the mass range probed in the first study of this series to span from Sun-like stars to mid-M dwarfs. We added the Hyades (690 Myr) to the sample as a comparison cluster to Praesepe (750 Myr), the 2.6 Gyr old Ruprecht 147, and several hundred light curves from the late K2 Campaigns in the remaining clusters. The flare energy distribution was similar in the entire parameter space, following a power law relation with exponent $\alpha\approx 1.84-2.39$. The flaring rates declined with age, and declined faster for higher mass stars. We found evidence that a rapid decline in flaring activity occurred in M1-M2 dwarfs around Hyades/Praesepe age, when these stars spun down to rotation periods of about 10 days, while higher mass stars had already transitioned to lower flaring rates, and lower mass stars still resided in the saturated activity regime. (abridged) ","Flares in Open Clusters with K2. II. Pleiades, Hyades, Praesepe,
  Ruprecht 147, and M67"
127,1318202505684258816,1143044682,Nima Haghpanah,"['Excited about new paper ‚ÄúHow to Sell Hard Information‚Äù with @SNageebAli, Xiao Lin, Ron Siegel!\n\nWhy do information intermediaries exist?\n\nAn answer: They increase efficiency. Or at least benefit whoever is willing to pay for them.\n\nNot necessarily!\n\nLink: <LINK>', 'A seller has an asset to sell. He may buy info about its value from an intermediary to disclose. Intermediary designs a test that provides such info.\n\nSeller‚Äôs payoff is lower than what it would be if the intermediary didn‚Äôt exist.  Even though buying information is voluntary!', 'Why?  \n\nIntermediary uses information as a carrot.  She makes the test free to take, but charges for disclosure.  Seller is tempted by the test‚Äôs ""option value"" and takes the test.  But then not revealing the result is bad news.  So seller pays to disclose even mediocre scores.', ""This is related to the debate on #econtwitter about whether RA programs are useful. \n\nJust because students are ‚Äúwilling to pay‚Äù for them, it doesn‚Äôt mean that they benefit from their existence.  The problem?  They can't commit to not use these programs.""]",http://arxiv.org/abs/2010.08037,"The seller of an asset has the option to buy hard information about the value of the asset from an intermediary. The seller can then disclose the acquired information before selling the asset in a competitive market. We study how the intermediary designs and sells hard information to robustly maximize her revenue across all equilibria. Even though the intermediary could use an accurate test that reveals the asset's value, we show that robust revenue maximization leads to a noisy test with a continuum of possible scores that are distributed exponentially. In addition, the intermediary always charges the seller for disclosing the test score to the market, but not necessarily for running the test. This enables the intermediary to robustly appropriate a significant share of the surplus resulting from the asset sale even though the information generated by the test provides no social value. ",How to Sell Hard Information
128,1318188318958456834,2972152960,Renato Negrinho,"['New EMNLP findings paper #emnlp #emnlp2020 .\n\nPaper: <LINK>\nCode: <LINK>\n\nWe empirically study beam-aware training algorithms instantiated through a meta-algorithm and evaluate them on supertagging. <LINK>', 'We find that beam-aware training yields large performance improvements when the model must rely on the beam to manage uncertainty effectively, e.g., must make decisions with incomplete information.', 'Beam-aware training uses beam search for both training and decoding and therefore models do not suffer from exposure bias and learn to exploit the beam. This contrasts with the usual approach of training on maximum likelihood and decoding with beam search.', 'The goal of this paper is to better understand the impact of beam-aware training, i.e., under what conditions would we see performance improvements and what design aspects would be most important.', 'We used a meta-algorithm for beam-aware training proposed in our previous work (https://t.co/39WMuxTzPD), which identifies several design dimensions: beam size, data collection strategy, and loss function.', 'We have found that beam-aware training yields large improvements when the model can‚Äôt encode the complete sequence before starting prediction. In this case, beam-aware training yielded a model that did a better job managing uncertainty about future predictions.', 'The standard approach of maximum likelihood training and post-hoc beam search failed to reach the same performances (~10 absolute perf. points in some cases). We also have useful observations about design choices to train the models stably and achieve high performances.', 'Personal take:  I believe that beam-aware training will be most impactful in settings where the instance for which we have to generate predictions is not fully available and partial predictions must be made with incomplete information, or in cases where ..', 'actions taken affect information gathered, e.g., when traversing a graph incrementally. I‚Äôm also hopeful about the ability of these models to address pathologies that have been identified with the typical use of beam search for decoding after maximum likelihood training.']",https://arxiv.org/abs/2010.04980,"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims to address these problems, but unfortunately, it is not yet widely used due to a lack of understanding about how it impacts performance, when it is most useful, and whether it is stable. Recently, Negrinho et al. (2018) proposed a meta-algorithm that captures beam-aware training algorithms and suggests new ones, but unfortunately did not provide empirical results. In this paper, we begin an empirical investigation: we train the supertagging model of Vaswani et al. (2016) and a simpler model with instantiations of the meta-algorithm. We explore the influence of various design choices and make recommendations for choosing them. We observe that beam-aware training improves performance for both models, with large improvements for the simpler model which must effectively manage uncertainty during decoding. Our results suggest that a model must be learned with search to maximize its effectiveness. ",An Empirical Investigation of Beam-Aware Training in Supertagging
129,1318096399754559488,978500233368760320,Biao Zhang,"['[1/4] Excited to share our new work on improving end-to-end speech translation with adaptive feature selection, to appear at Findings of #emnlp2020.\n\nJoint work with @iatitov, @bazril and @RicoSennrich.\n\nPaper: <LINK>\nCode: <LINK> <LINK>', '[2/4] Unlike text, audios are noisy and lengthy with redundant signals. We propose to filter out those signals contributing little to speech tasks and only feed transcript-relevant speech features for the translation task. ~84% features dropped with ~1.3-1.6 BLEU gains! https://t.co/DEqfJWExTb', ""[3/4] The model is an adaption of L0Drop, proposed for encoder output sparsification on text-based seq2seq tasks (https://t.co/tqS8O81FC0, w/ @iatitov , @RicoSennrich). It's purely data-driven, capable of automatically selecting subset of encoder outputs critical for given tasks."", '[4/4] Our work shows E2E ST suffers from redundant speech features, with sparsification bringing significant performance improvements. This task offers new opportunities for sparse models to deliver performance gains, apart from enhancing efficiency and/or interpretability.']",https://arxiv.org/abs/2010.08518,"Information in speech signals is not evenly distributed, making it an additional challenge for end-to-end (E2E) speech translation (ST) to learn to focus on informative features. In this paper, we propose adaptive feature selection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR encoder and apply AFS to dynamically estimate the importance of each encoded speech feature to SR. A ST encoder, stacked on top of the ASR encoder, then receives the filtered features from the (frozen) ASR encoder. We take L0DROP (Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech features with respect to both temporal and feature dimensions. Results on LibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of ST by pruning out ~84% temporal features, yielding an average translation gain of ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces the performance gap compared to the cascade baseline, and outperforms it on LibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation) ",Adaptive Feature Selection for End-to-End Speech Translation
130,1318085836634771456,946726588200218624,Laurent B√©termin,"['I\'m very happy to present my new preprint ""Theta functions and optimal lattices for a grid cells model"", now on @arxiv: <LINK>. The paper treats the optimality of the triangular and FCC lattice for the Fisher Information related to brain spatial representation. <LINK>', 'In particular, the trace of the Fisher Information (see picture) for which we study the maximization is written in terms of translated lattice theta functions. We give new evidences of the maximality of the triangular and FCC lattices for this resolution-type functional. https://t.co/v4mSwI9lw7']",https://arxiv.org/abs/2010.08264,"Certain types of neurons, called ""grid cells"", have been shown to fire on a triangular grid when an animal is navigating on a two-dimensional environment, whereas recent studies suggest that the face-centred-cubic (FCC) lattice is the good candidate for the same phenomenon in three dimensions. The goal of this paper is to give new evidences of these phenomena by considering a infinite set of independent neurons (a module) with Poisson statistics and periodic spread out Gaussian tuning curves. This question of the existence of an optimal grid is transformed into a maximization problem among all possible unit density lattices for a Fisher Information which measures the accuracy of grid-cells representations in $\mathbb{R}^d$. This Fisher Information has translated lattice theta functions as building blocks. We first derive asymptotic and numerical results showing the (non-)maximality of the triangular lattice with respect to the Gaussian parameter and the size of the firing field. In a particular case where the size of the firing fields and the lattice spacing match with experiments, we have numerically checked that it is possible to find a value for the Gaussian parameter above which the triangular lattice is always optimal. In the case of a radially symmetric distribution of firing locations, we also characterize all the lattices that are critical points for the Fisher Information at fixed scales belonging to an open interval (we call these lattices ""volume stationary""). It allows us to compare the Fisher Information of a finite number of lattices in dimension 2 and 3 and to give another evidences of the optimality of the triangular and FCC lattices. ",Theta functions and optimal lattices for a grid cells model
131,1318032310445445120,2375680693,John P Dickerson,"['In new work, we add a group fairness knob to methods for training auction networks.  To the best of our knowledge, this is the first connection between nascent the fair auction literature &amp; differentiable economics (e.g., RegretNet).   üßµ.. 1/\n\n\n\nPaper: <LINK> <LINK> <LINK>', 'We give a deep-learning-based method for designing approximately fair, strategyproof, revenue-maximizing auctions given samples from the valuation distribution. We extend RegretNet (https://t.co/Dv9YJcMPvv) with fairness constraints (&amp; preserves generalization guarantees).  2/', 'Example: 1 bidder, 2 items, U[0,1] additive valuations.  Figs show prob. of getting item 1 (top) and item (2) as we increase the fairness knob from none (left) to highest (right).  We recover the traditional optimal auction (left) and see intuitive smoothing as we move right.  3/ https://t.co/pXvvjt6o4y', 'Similar results for other cases (e.g., 1 bidder, 2 items, U[2,3] unit demand).  Designing revenue-maximizing multi-item auctions under both strategyproofness but fairness constraints could be especially useful in advertising, where fairness/bias issues are already well known.  4/ https://t.co/X9O2lJlPdT', 'Note this is for one quantification of fairness, warranting future work w/ mech designers &amp; stakeholders; our approach should extend well!\n\nKuo, Ostuni, Horishny, &lt;-- REU undergrads!\n@michaeljcurry1, Dooley, @PingyehChiang, @tomgoldsteincs\n\nPaper: https://t.co/fqzYfLsnGg\n\n5/5']",https://arxiv.org/abs/2010.06398,"The design of revenue-maximizing auctions with strong incentive guarantees is a core concern of economic theory. Computational auctions enable online advertising, sourcing, spectrum allocation, and myriad financial markets. Analytic progress in this space is notoriously difficult; since Myerson's 1981 work characterizing single-item optimal auctions, there has been limited progress outside of restricted settings. A recent paper by D\""utting et al. circumvents analytic difficulties by applying deep learning techniques to, instead, approximate optimal auctions. In parallel, new research from Ilvento et al. and other groups has developed notions of fairness in the context of auction design. Inspired by these advances, in this paper, we extend techniques for approximating auctions using deep learning to address concerns of fairness while maintaining high revenue and strong incentive guarantees. ","ProportionNet: Balancing Fairness and Revenue for Auction Design with
  Deep Learning"
132,1317995856256245760,2914019402,Yamini Bansal,"['New paper <LINK> w Gal Kaplun &amp; @boazbaraktcs!\n\nRecent work has focused on the ""deep learning generalization puzzle"" (highlighted by Zhang  et al <LINK>). Since deep nets interpolate train data, the train err doesn\'t tell you much about test err. <LINK>', 'Additionally, we can prove generalization bounds for such classifiers, without making any structural assumptions on the relationship between the self-supervision task and the downstream task. (See interactive version of results here https://t.co/DHnLcE7OIM) https://t.co/CcVUTBOkQZ']",http://arxiv.org/abs/2010.08508,"We prove a new upper bound on the generalization gap of classifiers that are obtained by first using self-supervision to learn a representation $r$ of the training data, and then fitting a simple (e.g., linear) classifier $g$ to the labels. Specifically, we show that (under the assumptions described below) the generalization gap of such classifiers tends to zero if $\mathsf{C}(g) \ll n$, where $\mathsf{C}(g)$ is an appropriately-defined measure of the simple classifier $g$'s complexity, and $n$ is the number of training samples. We stress that our bound is independent of the complexity of the representation $r$. We do not make any structural or conditional-independence assumptions on the representation-learning task, which can use the same training dataset that is later used for classification. Rather, we assume that the training procedure satisfies certain natural noise-robustness (adding small amount of label noise causes small degradation in performance) and rationality (getting the wrong label is not better than getting no label at all) conditions that widely hold across many standard architectures. We show that our bound is non-vacuous for many popular representation-learning based classifiers on CIFAR-10 and ImageNet, including SimCLR, AMDIM and MoCo. ","For self-supervised learning, Rationality implies generalization,
  provably"
133,1317981212724559872,1001049754787368960,Dr. Yu-Dai Tsai,['New paper out! the FORMOSA experiment:\n<LINK>'],https://arxiv.org/abs/2010.07941,"We identify potentially the world's most sensitive location to search for millicharged particles in the 10 MeV to 100 GeV mass range: the forward region at the LHC. We propose constructing a scintillator-based experiment, FORward MicrOcharge SeArch (FORMOSA) in this location, and estimate the corresponding sensitivity projection. We show that FORMOSA can discover millicharged particles in a large and unexplored parameter space, and study strongly interacting dark matter that cannot be detected by ground-based direct-detection experiments. The newly proposed LHC Forward Physics Facility (FPF) provides an ideal structure to host the full FORMOSA experiment. ",FORMOSA: Looking Forward to Millicharged Dark Sectors
134,1317446802480762880,15106103,pedrobizarro,"['Fairband is a new fairness-aware algorithm with good properties: when compared with the best model (A) it consistently finds models with a bit less accuracy for much higher fairness (B), automatically and at low cost. \n\n#feedzai\n#responsibleai\n\nPaper at: <LINK> <LINK>']",https://arxiv.org/abs/2010.03665,"Considerable research effort has been guided towards algorithmic fairness but there is still no major breakthrough. In practice, an exhaustive search over all possible techniques and hyperparameters is needed to find optimal fairness-accuracy trade-offs. Hence, coupled with the lack of tools for ML practitioners, real-world adoption of bias reduction methods is still scarce. To address this, we present Fairband, a bandit-based fairness-aware hyperparameter optimization (HO) algorithm. Fairband is conceptually simple, resource-efficient, easy to implement, and agnostic to both the objective metrics, model types and the hyperparameter space being explored. Moreover, by introducing fairness notions into HO, we enable seamless and efficient integration of fairness objectives into real-world ML pipelines. We compare Fairband with popular HO methods on four real-world decision-making datasets. We show that Fairband can efficiently navigate the fairness-accuracy trade-off through hyperparameter optimization. Furthermore, without extra training cost, it consistently finds configurations attaining substantially improved fairness at a comparatively small decrease in predictive accuracy. ",A Bandit-Based Algorithm for Fairness-Aware Hyperparameter Optimization
135,1317163945505992705,972555245179064320,Jordy de Vries,"[""A new paper where we looked at how proposed LHC detectors (from AL3X to MAPP) can detect long-lived sterile neutrinos. In see-saw models sterile neutrinos couple very weakly (explaining 'sterile') leaving room for interactions via decoupled new physics. <LINK>""]",https://arxiv.org/abs/2010.07305,"We study the prospects of a displaced-vertex search of sterile neutrinos at the Large Hadron Collider (LHC) in the framework of the neutrino-extended Standard Model Effective Field Theory ($\nu$SMEFT). The production and decay of sterile neutrinos can proceed via the standard active-sterile neutrino mixing in the weak current, as well as through higher-dimensional operators arising from decoupled new physics. If sterile neutrinos are long-lived, their decay can lead to displaced vertices which can be reconstructed. We investigate the search sensitivities for the ATLAS/CMS detector, the future far-detector experiments: AL3X, ANUBIS, CODEX-b, FASER, MATHUSLA, and MoEDAL-MAPP, and at the proposed fixed-target experiment SHiP. We study scenarios where sterile neutrinos are predominantly produced via rare charm and bottom mesons decays through minimal mixing and/or dimension-six operators in the $\nu$SMEFT Lagrangian. We perform simulations to determine the potential reach of high-luminosity LHC experiments in probing the EFT operators, finding that these experiments are very competitive with other searches. ",Long-lived Sterile Neutrinos at the LHC in Effective Field Theory
136,1317151058594914305,36714410,Yang Li,"['Delighted to share our new @emnlp2020 paper on widget captioning, which generates language descriptions of mobile UIs based on their appearance and structures: <LINK>, closely related to our previous work on language grounding in UIs (<LINK>). <LINK>']",https://arxiv.org/abs/2010.04295,"Natural language descriptions of user interface (UI) elements such as alternative text are crucial for accessibility and language-based interaction in general. Yet, these descriptions are constantly missing in mobile UIs. We propose widget captioning, a novel task for automatically generating language descriptions for UI elements from multimodal input including both the image and the structural representations of user interfaces. We collected a large-scale dataset for widget captioning with crowdsourcing. Our dataset contains 162,859 language phrases created by human workers for annotating 61,285 UI elements across 21,750 unique UI screens. We thoroughly analyze the dataset, and train and evaluate a set of deep model configurations to investigate how each feature modality as well as the choice of learning strategies impact the quality of predicted captions. The task formulation and the dataset as well as our benchmark models contribute a solid basis for this novel multimodal captioning task that connects language and user interfaces. ","Widget Captioning: Generating Natural Language Description for Mobile
  User Interface Elements"
137,1317129255973654528,2432031889,Ana Marasoviƒá,"['üì¢ New at Findings #EMNLP2020 üì¢ \n\n""Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs""\n\nw/ @_csBhagav @jae_sung_park96 @Ronan_LeBras @nlpnoah @YejinChoinka\n\nüìñ Paper: <LINK>\n\nThread üëá <LINK>', 'Why natural language rationales?\n\nExplaining higher-level conceptual reasoning cannot be well conveyed *only* by attributing individual pixels or words---the cause behind prediction is often *not* explicitly grounded in the input (""she doesn‚Äôt know whose order is whose"") 1/', ""The key challenge of visual-textual reasoning rationalization is image understanding beyond explicit content (highlighting objects): understanding contextual content like the relations among objects through action predicates (semantics) and the action's intent (pragmatics) 2/"", 'We combine GPT-2 with object recognition, grounded visual semantic frames, and commonsense inferences inferred from an image and an optional event predicted from a visual commonsense graph 3/ https://t.co/U8eND0geM1', 'GPT-2 benefits from visual adaptation across complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering; adapted models generate more plausible rationales that are less likely to mention content irrelevant to an image 4/ https://t.co/HwKFIoHq3A', 'Our best performing models for visual commonsense reasoning and visual-textual entailment are still notably behind human-written rationales showing that free-text rationalization remains a challenging task despite our improvements 5/', ""I'm really excited about this work and the numerous open questions. \n\nCan natural language rationales be used to persuade users? Yes, don't generate rationales independently after the prediction. We need evaluations of association btw rationale generation and label prediction 7/"", ""Is generation of natural language rationales possible only with human-written rationales? I don't believe so. Recent related work that uses weak supervision is a promising direction, but we need more exploration there  8/ https://t.co/JkHLhvPTp3"", 'Are there alternatives to human evaluation of plausibility? Maybe. BLEU and co. are not suitable, but someone should investigate newly emerging *learned* evaluation measures such as BLEURT 9/', ""And I'm sure there are many more. Feel free to reach out! 10/10 :)""]",http://arxiv.org/abs/2010.07526,"Natural language rationales could provide intuitive, higher-level explanations that are easily understandable by humans, complementing the more broadly studied lower-level explanations based on gradients or attention weights. We present the first study focused on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. The key challenge of accurate rationalization is comprehensive image understanding at all levels: not just their explicit content at the pixel level, but their contextual contents at the semantic and pragmatic levels. We present Rationale^VT Transformer, an integrated model that learns to generate free-text rationales by combining pretrained language models with object recognition, grounded visual semantic frames, and visual commonsense graphs. Our experiments show that the base pretrained language model benefits from visual adaptation and that free-text rationalization is a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. ","Natural Language Rationales with Full-Stack Visual Reasoning: From
  Pixels to Semantic Frames to Commonsense Graphs"
138,1317116907598774272,185910194,Graham Neubig,"['Amazing new #EMNLP2020 paper by Zhengbao Jiang and @anas_ant: ""X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained LMs""\nResults on probing facts in LMs over *23* typologically-diverse languages with different resource availability levels! <LINK> 1/6 <LINK>', ""The resource (available here: https://t.co/vnU49PhqtR) is a big contribution, but there's also a *lot* of methodological contributions and experimental results that I'm really excited about too. 2/6 https://t.co/xRIRsFqvYr"", 'First, previous results on probing LMs for factual (relational) knowledge have largely focused on one-word entities, they can handle ""France"" but not ""United States"". We resolve this with multi-token decoding strategies, making it possible to probe facts about all entities. 3/6 https://t.co/yst40BmZlf', 'Second, we show that fact retrieval is a *hard* task, even for SOTA models, and even on high-resourced languages. Interestingly, M-BERT tends to out-perform XLM-R, contrary to previous results (https://t.co/sSwce4TS5j) demonstrating different capabilities of each model. 4/6 https://t.co/QSBhKvpALG', 'Further, we find that different facts are easier to retrieve using queries in different languages. Based on this, we propose a code-switched training strategy, that improves cross-lingual fact retrieval performance. 5/6 https://t.co/9TleHxqgH6', 'Finally, the annotation required for creating this resource/paper was a huge effort, and we profusely thank the annotators who lent their time to make this possible! If you would also like X-FACTR in your language, please feel free to reach out to us. 6/6 https://t.co/TSwsrNuyf1']",https://arxiv.org/abs/2010.06189,"Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as ""Punta Cana is located in _."" However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for 23 typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have been released at this https URL ","X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained
  Language Models"
139,1317109709829246976,475760077,Dr Sarah Casewell,"['New paper day! @astromarkmarley @mccushing\n@astronomace @JohnDebes Davy Kirkpatrick and I show the irradiated brown dwarf NLTT5306B which orbits a white dwarf in 101 minutes is inflated. \n<LINK>', '@astromarkmarley @mccushing @astronomace @JohnDebes To give an idea of what this binary looks like - if you replace HST with Jupiter, you would have a binary that looks similar! The white dwarf is earth sized, and the brown dwarf is  Jupiter sized. The brown dwarf is  slowly losing its atmosphere to the white dwarf which is odd', '@astromarkmarley @mccushing @astronomace @JohnDebes There are closer binaries (one with a period of 68 min!) and ones with hotter white dwarfs. None of these brown dwarfs appear to be inflated or interacting.', '@RainDogJones @astromarkmarley @mccushing @astronomace @JohnDebes I have a figure for that‚Ä¶..\nSo this is the BD Mass- Radius relation for eclipsing BDs. Anything below about 35 MJup is inflated irrelevant of the irradiation. Above that very few things are inflated irrelevant of the irradiation‚Ä¶.. The 2 stupidly irradiated BDs are not inflated https://t.co/8HpHqF8Cjs', '@RainDogJones @astromarkmarley @mccushing @astronomace @JohnDebes The colour is effective temperature of primary, and size of circle is proportional to the level of irradiation. The squares are the 3 systems with WD primaries.  Stay tuned for a similar diagram involving a BD with a sdB primary (we‚Äôre waiting for it to be accepted)', '@cbuzard94 @astromarkmarley @mccushing @astronomace @JohnDebes Thank you!  I think we set the scene nicely....üòâ', '@RainDogJones @astromarkmarley @mccushing @astronomace @JohnDebes That would work. We don‚Äôt see inflation in 2 of the wd-BDs but the one we do see it in is the highest mass BD. Of course that means it should be physically the smallest.... so the least surface area to accrete from the common envelope', '@RainDogJones @astromarkmarley @mccushing @astronomace @JohnDebes I add disclaimers to all my talks as I know a lot of CE stuff is controversial! I‚Äôm always very much ‚ÄúI‚Äôm the messenger here! I make no claims!!‚Äù']",https://arxiv.org/abs/2010.07398,"We present Spitzer observations at 3.6 and 4.5 microns and a near-infrared IRTF SpeX spectrum of the irradiated brown dwarf NLTT5306B. We determine that the brown dwarf has a spectral type of L5 and is likely inflated, despite the low effective temperature of the white dwarf primary star. We calculate brightness temperatures in the Spitzer wavebands for both the model radius, and Roche Lobe radius of the brown dwarf, and conclude that there is very little day-night side temperature difference. We discuss various mechanisms by which NLTT5306B may be inflated, and determine that while low mass brown dwarfs (M<35 MJup) are easily inflated by irradiation from their host star, very few higher mass brown dwarfs are inflated. The higher mass brown dwarfs that are inflated may be inflated by magnetic interactions or may have thicker clouds. ","NLTT5306B: an inflated, weakly irradiated brown dwarf"
140,1317097573736501253,1101220947607146497,Alon Jacovi,"['""We want to increase the user\'s trust in the model,"" or ""we want a more trustworthy model"" - you probably saw this sentiment in many papers. But what exactly does this mean?\n\nNew paper! --&gt; <LINK> @trustworthy_ml \n\nWith @anmarasovic  @tmiller_unimelb  @yoavgo <LINK>', 'This paper is meant as a foundation of what ""trust"" means in AI. What are the prerequisites and goals of trusting? What can ""cause"" trust, and is trust always desirable? What about distrust? How does all of this relate to designing AI? We answer this and more.', 'We formalize trust as the belief that a ""contract"" will be held under the risk that it may not:\nVulnerability to the ""bad"" scenario is a *prerequisite*, and the goal is to successfully anticipate the good scenario.\n\nThis ""contract"" can be anything that will mitigate the risk.', 'We characterize trust as ""warranted"" if it exists *because* of the AI\'s ability to carry the contract, otherwise it is unwarranted. For example, trusting the AI to perform well because it looks nice is unwarranted, because looking nice, at most, only correlates with performance. https://t.co/b5MAh29HJm', 'We also formalize two central causes of trust: *intrinsic* trust emergin from agreeable reasoning, or *extrinsic trust emerging from trustworthy evaluation.\n\nThe paper goes much further than this, so please check for details. https://t.co/ReplZfGngW', ""I'm here for any questions or discussion! The paper is a bit long, but I promise all of it is important to the core message - hopefully you peek inside, because I'm really happy with how it turned out"", ""@richardtomsett Thanks! Trust calibration also shows up a bit later (9.1) in response to Hoffman 2017's opinion that trust can't be calibrated. To me, the taxonomy of contractual trust finally clears up the issue (trust can be calibrated if it's conditioned on a contract, the question is which)"", ""@richardtomsett Thankfully there's already a ton of work on what these contracts are and how to communicate and evaluate them (@anmarasovic did an amazing job with Table 1 for this), I think it's just a matter of organizing them properly using a unified taxonomy"", ""@krvarshney @trustworthy_ml @anmarasovic @tmiller_unimelb @yoavgo @ManningBooks Thanks for the link. I can't say anything specific about your book right now, but I give my argument behind my choice of definitions (e.g. using contractual trust taxonomy) in sec.9.1 and possible future work towards practitioner interests in 9.2."", '@krvarshney @trustworthy_ml @anmarasovic @tmiller_unimelb @yoavgo @ManningBooks Our argument is that contractual trust is key to discussing trustworthiness in all cases and it solves a lot of the problems raised by e.g. Hoffman 2017.', ""@krvarshney @trustworthy_ml @anmarasovic @tmiller_unimelb @yoavgo @ManningBooks It's even more important for such a person, because those things are not sufficient to measure and avoid unwarranted trust""]",http://arxiv.org/abs/2010.07487,"Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, sociology's interpersonal trust (i.e., trust between people). This model rests on two key properties of the vulnerability of the user and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (which detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We then present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization. ","Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and
  Goals of Human Trust in AI"
141,1317091718664159232,1055880097595564034,Rohini Giles,"[""Our new paper about phosphine in Venus' atmosphere, led by Th√©r√®se Encrenaz, has been accepted in A&amp;A and is available on arXiv: <LINK> <LINK>"", ""We use infrared observations from TEXES/IRTF and don't find any evidence of phosphine absorption, giving an upper limit of 5 ppbv. The difference from the Greaves et al. paper could be due to probing slightly different levels in the atmosphere or PH3 variability.""]",https://arxiv.org/abs/2010.07817,"Following the announcement of the detection of phosphine (PH$_3$) in the cloud deck of Venus at millimeter wavelengths, we have searched for other possible signatures of this molecule in the infrared range. Since 2012, we have been observing Venus in the thermal infrared at various wavelengths to monitor the behavior of SO$_2$ and H$_2$O at the cloud top. We have identified a spectral interval recorded in March 2015 around 950 cm$^{-1}$ where a PH$_3$ transition is present. From the absence of any feature at this frequency, we derive, on the disk-integrated spectrum, a 3-$\sigma$ upper limit of 5 ppbv for the PH$_3$ mixing ratio, assumed to be constant throughout the atmosphere. This limit is 4 times lower than the disk-integrated mixing ratio derived at millimeter wavelengths. Our result brings a strong constraint on the maximum PH$_3$ abundance at the cloud top and in the lower mesosphere of Venus. ","A stringent upper limit of the PH$_3$ abundance at the cloud top of
  Venus"
142,1317068719173009408,11892372,Fabrizio Silvestri,"['We just released on arXiv (<LINK>) our new paper on ""Neural Databases"".\n\nWe presents NeuralDB, a DB system with no pre-defined schema, in which updates &amp; queries are given in natural language.\n@j6mes, @myazdani999, @marzieh_saeidi, @riedelcastro, and Alon Halevi', ""@srchvrs @Mniepert @j6mes @myazdani999 @marzieh_saeidi @riedelcastro Roots are roots! :) You can't forget about them :)""]",https://arxiv.org/abs/2010.06973,"In recent years, neural networks have shown impressive performance gains on long-standing AI problems, and in particular, answering queries from natural language text. These advances raise the question of whether they can be extended to a point where we can relax the fundamental assumption of database management, namely, that our data is represented as fields of a pre-defined schema. This paper presents a first step in answering that question. We describe NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language. We develop query processing techniques that build on the primitives offered by the state of the art Natural Language Processing methods. We begin by demonstrating that at the core, recent NLP transformers, powered by pre-trained language models, can answer select-project-join queries if they are given the exact set of relevant facts. However, they cannot scale to non-trivial databases and cannot perform aggregation queries. Based on these findings, we describe a NeuralDB architecture that runs multiple Neural SPJ operators in parallel, each with a set of database sentences that can produce one of the answers to the query. The result of these operators is fed to an aggregation operator if needed. We describe an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators. Importantly, this algorithm can be trained by the Neural SPJ operator itself. We experimentally validate the accuracy of NeuralDB and its components, showing that we can answer queries over thousands of sentences with very high accuracy. ",Neural Databases
143,1317048775727214592,1309737356258242564,Sho Takase,"['Our new paper is out! <LINK>\nThis paper addresses cross-lingual summarization with a multi-task learning of translation and summarization.', 'Our proposed method uses task specific tag to represent a target task. The proposed method improves the performance of not only cross-lingual summarization but also translation in several language pairs.']",https://arxiv.org/abs/2010.07503,"We present a multi-task learning framework for cross-lingual abstractive summarization to augment training data. Recent studies constructed pseudo cross-lingual abstractive summarization data to train their neural encoder-decoders. Meanwhile, we introduce existing genuine data such as translation pairs and monolingual abstractive summarization data into training. Our proposed method, Transum, attaches a special token to the beginning of the input sentence to indicate the target task. The special token enables us to incorporate the genuine data into the training data easily. The experimental results show that Transum achieves better performance than the model trained with only pseudo cross-lingual summarization data. In addition, we achieve the top ROUGE score on Chinese-English and Arabic-English abstractive summarization. Moreover, Transum also has a positive effect on machine translation. Experimental results indicate that Transum improves the performance from the strong baseline, Transformer, in Chinese-English, Arabic-English, and English-Japanese translation datasets. ",Multi-Task Learning for Cross-Lingual Abstractive Summarization
144,1317040248153518080,561899047,Aki Vehtari,"['New paper and code for variable and structure selection for GLMMs and GAMMs by @AleexCatalina, @paulbuerkner and I: ""Projection predictive inference for generalized linear and additive multilevel models"" <LINK>', 'Projection predictive variable selection has been shown to be stable and able to find small models with good predictive performance\nhttps://t.co/YlNfNU13Tr\nhttps://t.co/aoj4smny8F\nhttps://t.co/jTxkPW64uu', 'Our projpred package supported before just generalized linear models (GLMs). @AleexCatalina did great work in extending the methods to \nhandle also GLMMs and GAMMs and running many experiments to show that the extended method really works. https://t.co/zK54CcHmFC', 'The reference model can be defined with R formula syntax for GLMMs and GAMMs, forward search through the projected models finds a simpler model structure that has similar predictive performance as the full model, and we get a new formula. https://t.co/RUjXgZEkPj', 'The benefit of finding automatically a simpler model structure and formula is is that the modeler can focus more quickly on what is relevant.', 'The forward search (or any search through the model space) is usually associated with overfitting. The projection predictive approach avoids overfitting by examining only the projected submodels and not fitting each model independently to the data.', 'To be able to support R formula syntax for GLMMs and GAMMs, @AleexCatalina  refactored the projpred. The new code supporting GLMMs and GAMMs is not yet in CRAN, but is available in master branch at https://t.co/Opd3QqnOAj', 'See also a vignette showing examples how to use projpred for GLMMs\nhttps://t.co/PMntWaxP0x', 'Oops, I forgot to update the link from the draft of the tweet. The vignette is at https://t.co/izGPmjUyhA', '@bruno_nicenboim Even better to use https://t.co/izGPmjUyhA (thanks for pointing out that I had posted the wrong link)']",https://arxiv.org/abs/2010.06994,"Projection predictive inference is a decision theoretic Bayesian approach that decouples model estimation from decision making. Given a reference model previously built including all variables present in the data, projection predictive inference projects its posterior onto a constrained space of a subset of variables. Variable selection is then performed by sequentially adding relevant variables until predictive performance is satisfactory. Previously, projection predictive inference has been demonstrated only for generalized linear models (GLMs) and Gaussian processes (GPs) where it showed superior performance to competing variable selection procedures. In this work, we extend projection predictive inference to support variable and structure selection for generalized linear multilevel models (GLMMs) and generalized additive multilevel models (GAMMs). Our simulative and real-word experiments demonstrate that our method can drastically reduce the model complexity required to reach reference predictive performance and achieve good frequency properties. ","Projection Predictive Inference for Generalized Linear and Additive
  Multilevel Models"
145,1317027154002087936,2662693970,Frederik Kratzert,"['üö®Exciting Newsüö®\n\n1. New paper by our new colleague Martin Gauch on ""Rainfall-runoff predictions at multiple timescales with LSTM-based models"" <LINK>\n\nand\n\n2. We (our research group at @LITAILab) published our research code base.\n\nDetails in thread:', 'In this manuscript, we propose a new LSTM-based architecture that allows to train models on multiple time scales at once (e.g. daily &amp; hourly). Compared to hydrological models, this model does not suffer performance deterioration when predicting at fine temporal resolutions. https://t.co/vzSuj6gf2E', 'This model is not limited to two temporal resolutions. For example we also show results for a model trained on daily, 12-hourly, 6-hourly, 3-hourly and 1-hourly data and again have stable performance across all resolutions.', 'That is, we maintain the high predictive capabilities of LSTMs we saw at daily timescales also at finer temporal resolutions and additionally can get consistent predictions across time scales.', 'The other thing is our research code base: neuralHydrology https://t.co/8wmbnkVG8G', 'We have full documentation coverage and a growing number of tutorials. Both can be found here https://t.co/9rv0GydnoP', 'To give some background. We work on this code since roughly a year and use it ourselves for all of our hydrology related research. It makes it super easy to test new models, add new data sets, train multiple models across multiple GPUs.', 'All of our publications so far can be reproduced with this code repository. Let it be regional models (incl. EA-LSTM), Prediction in Ungauged Basins, Multi-forcing models, fine-tuning pretrained models, as well as the new Multi-Timescale models.', 'We are still using this code internally for ongoing research and very soon there will be another big content update.\n\nWe hope that it will be useful for many people and are always happy for feedback.', 'This paper was lead by Martin Gauch (not on Twitter) who is joining us at @LITAILab for his PhD. The other co-authors are Daniel Klotz @ido87, Grey Nearing @GreyNearing, Jimmy Lin @lintool, and Sepp Hochreiter.']",https://arxiv.org/abs/2010.07921,"Long Short-Term Memory Networks (LSTMs) have been applied to daily discharge prediction with remarkable success. Many practical scenarios, however, require predictions at more granular timescales. For instance, accurate prediction of short but extreme flood peaks can make a life-saving difference, yet such peaks may escape the coarse temporal resolution of daily predictions. Naively training an LSTM on hourly data, however, entails very long input sequences that make learning hard and computationally expensive. In this study, we propose two Multi-Timescale LSTM (MTS-LSTM) architectures that jointly predict multiple timescales within one model, as they process long-past inputs at a single temporal resolution and branch out into each individual timescale for more recent input steps. We test these models on 516 basins across the continental United States and benchmark against the US National Water Model. Compared to naive prediction with a distinct LSTM per timescale, the multi-timescale architectures are computationally more efficient with no loss in accuracy. Beyond prediction quality, the multi-timescale LSTM can process different input variables at different timescales, which is especially relevant to operational applications where the lead time of meteorological forcings depends on their temporal resolution. ","Rainfall-Runoff Prediction at Multiple Timescales with a Single Long
  Short-Term Memory Network"
146,1317020465072930817,445739626,Gozde Unal,"['An earlier paper from Alican Mertan\'s MSc thesis work ""Relative Depth Estimation as a Ranking Problem"", where a listwise loss is adapted to relative depth estimation, also proposed a new metric that considers pixel depth ranking accuracy. @ <LINK> @siu2020medipol']",http://arxiv.org/abs/2010.06944,"We present a formulation of the relative depth estimation from a single image problem, as a ranking problem. By reformulating the problem this way, we were able to utilize literature on the ranking problem, and apply the existing knowledge to achieve better results. To this end, we have introduced a listwise ranking loss borrowed from ranking literature, weighted ListMLE, to the relative depth estimation problem. We have also brought a new metric which considers pixel depth ranking accuracy, on which our method is stronger. ",Relative Depth Estimation as a Ranking Problem
147,1317016756792295424,445739626,Gozde Unal,"['Our paper ""A New Distributional Ranking Loss with Uncertainty: "" is accepted to #3DV2020. Congrats to my student Alican Mertan @ituvisionlabüëè Presents a new ranking loss to penalize differences btw ordinal relations+ provides an uncertainty estimation.See <LINK> <LINK>']",http://arxiv.org/abs/2010.07091,"We propose a new approach for the problem of relative depth estimation from a single image. Instead of directly regressing over depth scores, we formulate the problem as estimation of a probability distribution over depth and aim to learn the parameters of the distributions which maximize the likelihood of the given data. To train our model, we propose a new ranking loss, Distributional Loss, which tries to increase the probability of farther pixel's depth being greater than the closer pixel's depth. Our proposed approach allows our model to output confidence in its estimation in the form of standard deviation of the distribution. We achieve state of the art results against a number of baselines while providing confidence in our estimations. Our analysis show that estimated confidence is actually a good indicator of accuracy. We investigate the usage of confidence information in a downstream task of metric depth estimation, to increase its performance. ","A New Distributional Ranking Loss With Uncertainty: Illustrated in
  Relative Depth Estimation"
148,1317003069109002242,1562913787,Nathan Moynihan,['New paper out today! <LINK>'],https://arxiv.org/abs/2010.07861,"Electric-magnetic duality, the Newman-Janis shift, and the double copy all act by elementary operations on three-point amplitudes. At the same time, they generate a network of interesting classical solutions spanning from the Coulomb charge via the dyon to the Kerr-Taub-NUT spacetime. We identify the amplitudes corresponding to each of these solutions, working to all orders in spin, but to leading perturbative order. We confirm that the amplitudes double-copy when the solutions are related by the classical double copy. Along the way we show that the Kerr-Taub-NUT solution corresponds to a gravitational electric-magnetic duality rotation acting on the Kerr solution, again to all orders in spin, and demonstrate that the asymptotic charges also transform simply under our operations. ",Amplitudes from Coulomb to Kerr-Taub-NUT
149,1316971297067380736,2793095883,Yanping Huang,"['Happy to share our new Gradient Sign Dropout algorithm, which can significantly improve performance of deep multi-task networks, especially for applications in autonomous driving.  More details in our NeurIPS 2020 paper: <LINK> <LINK>']",https://arxiv.org/abs/2010.06808,"The vast majority of deep models use multiple gradient signals, typically corresponding to a sum of multiple loss terms, to update a shared set of trainable weights. However, these multiple updates can impede optimal training by pulling the model in conflicting directions. We present Gradient Sign Dropout (GradDrop), a probabilistic masking procedure which samples gradients at an activation layer based on their level of consistency. GradDrop is implemented as a simple deep layer that can be used in any deep net and synergizes with other gradient balancing approaches. We show that GradDrop outperforms the state-of-the-art multiloss methods within traditional multitask and transfer learning settings, and we discuss how GradDrop reveals links between optimal multiloss training and gradient stochasticity. ","Just Pick a Sign: Optimizing Deep Multitask Models with Gradient Sign
  Dropout"
150,1316911104279314432,16677638,Alex Beutel,"['New research from a collaboration between the Language Research team and my team on gendered correlations in language models (led by Kellie Webster, Xuezhi Wang, and @iftenney)!\n\nPaper: <LINK> \n\nA few exciting results in addition to those in the blog post (1/): <LINK>', 'The blog post discusses how applying different language models to coreference resolution can give nearly the same accuracy but make assumptions based on gender at very different rates.  We see this not just for coref, but also 3 other tasks (including 2 new ones). (2/)', 'From the blog post but worth highlighting again: these significant differences in the usage of gendered correlations can come from *seemingly innocuous* configuration changes, like changing dropout. Among many take-aways, test thoroughly! (3/) https://t.co/PG93ascGHG', 'We find counterfactual data augmentation (CDA) also can help decrease usage of gendered correlations. Particularly (academically) interesting, we find that CDA generalizes---applying CDA for some names decreases making gendered assumptions for other names! (4/)', 'Further, both of these *pre-training* changes can improve gendered correlations in downstream tasks---we can see the benefits maintained over the course of fine-tuning. (5/) https://t.co/HDt4JS5eyC', ""Based on this research we discuss some best practices for language modeling. And more generally I'm excited that this research suggests there is a path for broad benefits in this space from improving underlying pre-trained language models. (/end)""]",https://arxiv.org/abs/2010.06032,"Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations. ",Measuring and Reducing Gendered Correlations in Pre-trained Models
151,1316908101153554432,1254620813036158977,Shunya Noda,"['Junpei (@junpeikomiyama) and I released a new working paper, ""On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach""! This paper studies how underestimation against minority groups is generated and persists.  <LINK> (1/7)', 'Although most of the papers on statistical discrimination have studied the equilibrium consequence under a ""correct belief,"" learning a correct belief could be a challenging task. Using a multi-armed bandit model, we show that social learning could generate a biased belief. (2/7)', 'We show that, even when society consists of fully rational and non-prejudiced agents, with a significant probability, they persistently misestimates the quality of minority workers. This happens more frequently when the population ratio is imbalanced. (3/7)', 'This phenomenon is caused by endogenous data imbalance. Firms learn how to interpret observable characteristics using the past hiring data. However, if the data about minorities is insufficient, the quality of minority workers cannot be accurately estimated. (4/7)', 'Once the belief is misspecified, it tends to persists. The data about minorities is increased only when minority workers are hired. However, firms do not willing to hire them because they are underestimated. Hence, under laissez faire, underestimation persists. (5/7)', 'We study the performance of some UCB-based subsidy rules and the Rooney Rule for encouraging experimenting minority groups. This paper also shows that some temporal affirmative actions are effective for solving this type of statistical discrimination. (6/7)', 'Any comments, questions, thoughts, etc., are highly appreciated! (7/7) Again, the paper is available at arXiv: https://t.co/j4OKRWyOyN The presentation is also coming soon!']",https://arxiv.org/abs/2010.01079,"We analyze statistical discrimination in hiring markets using a multi-armed bandit model. Myopic firms face workers arriving with heterogeneous observable characteristics. The association between the worker's skill and characteristics is unknown ex ante; thus, firms need to learn it. Laissez-faire causes perpetual underestimation: minority workers are rarely hired, and therefore, underestimation towards them tends to persist. Even a slight population-ratio imbalance frequently produces perpetual underestimation. We propose two policy solutions: a novel subsidy rule (the hybrid mechanism) and the Rooney Rule. Our results indicate that temporary affirmative actions effectively mitigate discrimination caused by insufficient data. ","On Statistical Discrimination as a Failure of Social Learning: A
  Multi-Armed Bandit Approach"
152,1316898341951885312,177416255,Daniel Litt,"['New paper about some connections between algebraic geometry, number theory, and geometric topology with @wanlinbunny, Nick Salter, and Padma Srinivasan! Surface bundles and the section conjecture: <LINK> <LINK>', 'Some more bonus pictures (most of these were made by my co-author Nick): https://t.co/6CFszvewh6', ""I'm really proud of this paper. The purpose of it is to prove (a lot of) new cases of Grothendieck's section conjecture, which predicts that the arithmetic of the fundamental group of a curve of genus at least 2 (over a function field or number field) detects rational points."", 'The first cool thing the paper does is define a new(-ish) obstruction to rational points, which arises from the second piece of the lower central series of the fundamental group. We produce lots of curves where this honestly obstructs rational points.', ""We also prove the section conjecture for the *generic curve* (this was known due to work of Hain, but we prove that in fact the *abelianized* fundamental group detects this curve's lack of rational points). What's more exciting, IMO, is the non-abelian examples we construct."", 'For example, we show that the ""generic curve with a rational divisor class"" satisfies the section conjecture (and has no rational points); rational points here are obstructed by our new obstruction. (BTW, this obstruction is inspired by old work of @JSEllenberg.)', ""One cool thing about this example is that it's genuinely non-abelian--the abelianized fundamental group thinks the curve has a rational point."", 'We do a lot of other fun stuff; we formulate a conjecture about the ""generic curve with given reduction type"" -- namely that it satisfies the section conjecture -- and prove lots of cases of this.', 'And then in each of these cases we use our methods to produce examples of curves over p-adic fields and number fields satisfying the section conjecture (using the Chebotarev density theorem and a little Sylow trick).', 'What I love about this paper is that the input is pure geometric topology; we construct surface bundles over surfaces satisfying a ""topological section conjecture"" -- then we pass to geometry (using moduli theory), and finally to arithmetic via a kind of transfer principle.', 'What a fun project! (And what wonderful coauthors!)', 'Some final bonus computations from the paper: https://t.co/9o07OoA6qW', '@liftingring lolwut', '@liftingring It uses the Sylow theorems!', '@roydanroy @htowsner @wanlinbunny The ones I made are not -- my coauthor may have used Tikz? My guess is InkScape though...', '@sterlingcrispin @wanlinbunny That‚Äôs this account‚Äôs brand I guess!', '@BraneRunner Well the generic curve doesn‚Äôt exactly make sense in this case; the methods of the paper do imply some stuff in genus 2 but we don‚Äôt state our main results there bc it would be too complicated.', '@poco_forte We put all the ‚Äúquick computations‚Äù in a 10 page appendix.', '@LangVojtaBro Basically no; we don‚Äôt know the section conjecture is true for *any* curve which has rational points.', '@LangVojtaBro Not sure I understand the question, but it is in fact true that proving the section conjecture for all curves of genus g\\geq 2 w no ratl pts implies it in general!', '@LangVojtaBro Not obviously! It‚Äôs a good question, let me think about it.']",https://arxiv.org/abs/2010.07331,"We formulate a tropical analogue of Grothendieck's section conjecture: that for every stable graph G of genus g>2, and every field k, the generic curve with reduction type G over k satisfies the section conjecture. We prove many cases of this conjecture. In so doing we produce many examples of curves satisfying the section conjecture over fields of geometric interest, and then over p-adic fields and number fields via a Chebotarev argument. We construct two Galois cohomology classes o_1 and o_2, which obstruct the existence of pi_1-sections and hence of rational points. The first is an abelian obstruction, closely related to the period of a curve and to a cohomology class on the moduli space of curves M_g studied by Morita. The second is a 2-nilpotent obstruction and appears to be new. We study the degeneration of these classes via topological techniques, and we produce examples of surface bundles over surfaces where these classes obstruct sections. We then use these constructions to produce curves over p-adic fields and number fields where each class obstructs pi_1-sections and hence rational points. Among our geometric results are a new proof of the section conjecture for the generic curve of genus g>2, and a proof of the section conjecture for the generic curve of even genus with a rational divisor class of degree one (where the obstruction to the existence of a section is genuinely non-abelian). ",Surface bundles and the section conjecture
153,1316894007411130368,2899735410,Caleb Harada üè≥Ô∏è‚Äçüåàü™êüåå,"['Back on the twitter today to announce a BRAND NEW PAPER out on astro-ph today! ""Clouds in Three-Dimensional Models of Hot Jupiters Over a Wide Range of Temperatures I: Thermal Structures and Broadband Phase Curve Predictions"" [<LINK>], led by Michael Roman. [1/x]', 'Here on Earth, clouds have a huge impact on our climate. They absorb and scatter radiation from the Sun, affecting heating rates and circulation in our atmosphere. Well, it turns out that clouds are probably a key component of exoplanet atmospheres as well! [2/x] https://t.co/297cRVxQvf', ""Some planets, like hot Jupiters, orbit their stars too closely to form familiar water clouds -- rather, the intense heat means that things like iron vapor can condense into clouds! Such extreme conditions imply these atmospheres likely behave quite differently than Earth's. [3/x] https://t.co/SnYPwyvWwf"", ""Depending on the exact properties of the clouds (e.g., composition, vertical extent, etc.) and amount of stellar radiation received by the planet, clouds can have a dramatic effect on the planet's emitted IR light and reflected starlight we observe [4/x] https://t.co/DtsoODGzio"", ""However, our understanding of the properties of exoclouds is incomplete! Clouds are extremely complex and this generally makes them difficult (computationally expensive) to model self-consistently. That's not ideal for modeling many planets over a large parameter space! [5/x] https://t.co/lg2AlKf57T"", 'But good news! In this study, we were able to simulate many hot Jupiters over a wide range of expected irradiation temperatures and various cloud assumptions to predict key trends in broadband (IR and vis) phase curve observations! [6/x] https://t.co/DgYFsvK2Z9', 'How?? We implemented a 3D circulation model which included 13 cloud species in a temperature-dependent framework (i.e., no microphysics)! This allowed us to efficiently compute a grid of models while including necessary radiative feedback between the clouds &amp; atmosphere. [7/x] https://t.co/ZiJu7EK0Nj', ""From these models, we sought to identify key trends in phase curve amplitudes and peak emission offsets as functions of the planet's irradiation temperature. We also checked to see how vertical extent, condensation efficiency, and surface gravity could affect these trends. [8/x] https://t.co/cHOD8NLpJr"", 'So what did we learn?? From our grid of models, we found that clouds significantly affect IR phase curve observables for T_irr &lt; ~3000 K. At these cooler temps, clouds can condense to decrease nightside emission, increase dayside emission, and lower peak offsets. [9/x] https://t.co/qIFVI406EM', 'We also found that, over a limited range of T_irr, clouds can produce westward peak offsets in optical phase curves. This results from the advection of cooled nightside gas over the western terminator, allowing reflective clouds to form in that region. [10/x] https://t.co/94iHhgV2Rg', 'But if conditions are too hot (T_irr &gt; 3500 K), few to no clouds can form along the western terminator, resulting in phase curves resembling a clear atmosphere. And if too cold (T_irr &lt; 2000 K), the entire dayside can cloud over, reducing longitudinal variations in albedo. [11/x] https://t.co/dNsgd5ZxTA', 'Finally, radiative feedback from clouds significantly alters atmospheric thermal structure (especially for more vertically extended clouds). This is quite nuanced, and the precise effects depend on how reflective/absorptive the local combination of cloud species is! [12/x] https://t.co/jaxrR53FvH', ""There's much more interesting discussion in this paper, but I couldn't summarize it all here! Be sure to check it out the full manuscript! https://t.co/podRA6jG9c [13/x] https://t.co/ChW85iZc1K"", ""Oh, and one last thing! How does any of this relate to science we'll be able to do with JWST?? STAY TUNED for more (spectral) cloudy hot Jupiter science!\n\nAnd check out this awesome related paper led by @V_Parmentier, also on astro-ph today: https://t.co/SCQQnDooyQ [14/14] https://t.co/akVfowgpON""]",https://arxiv.org/abs/2010.06936,"Using a general circulation model (GCM), we investigate trends in simulated hot Jupiter atmospheres for a range of irradiation temperatures (1,500 - 4,000 K), surface gravities (10 and 40 m s-2), and cloud conditions. Our models include simplified temperature-dependent clouds with radiative feedback and show how different cloud compositions, vertical thicknesses, and opacities shape hot Jupiters atmospheres by potentially increasing planetary albedos, decreasing photospheric pressures and nightside temperatures, and in some cases producing strong dayside thermal inversions. With decreasing irradiation, clouds progressively form on the nightside and cooler western limb followed by the eastern limb and central dayside. We find that clouds significantly modify the radiative transport and affect the observable properties of planets colder than T_irr ~ 3,000~K (T_eq~2,100 K) depending on the clouds' vertical extent. The precise strength of expected effects depends on the assumed parameters, but trends in predicted phase curves emerge from an ensemble of simulations. Clouds lead to larger phase curve amplitudes and smaller phase curve offsets at IR wavelengths, compared to cloud-free models. At optical wavelengths, we predict mostly westward phase curve offsets at intermediate temperatures (T_irr ~ 2,000 - 3,500 K) with clouds confined to the nightside and western limb. If clouds are vertically compact (i.e. on order of a pressure scale height in thickness), their distributions and effects become more complicated as different condensates form at different heights -- some too deep to significantly affect the observable atmosphere. Our results have implications for interpreting the diversity of phase curve observations of planets with T_irr <~3,000~K. ","Clouds in Three-Dimensional Models of Hot Jupiters Over a Wide Range of
  Temperatures I: Thermal Structures and Broadband Phase Curve Predictions"
154,1316775765690601472,232287209,Dallas Card,"['New EMNLP paper with @PeterHndrsn @ukhndlwl @robinomial @kmahowald and @jurafsky --  With Little Power Comes Great Responsibility -- <LINK> (1/3)', 'In this paper we show why and how statistical power analysis is relevant to NLP, analyze three cases showing that underpowered experiments are widespread in NLP research, and suggested ways to improve things going forward. (2/3)', 'Data, code, and notebooks for running simple power analyses are all available at https://t.co/yv3IUrZHx8']",https://arxiv.org/abs/2010.06595,"Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings. By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses. ",With Little Power Comes Great Responsibility
155,1316769831727689728,2902126231,Douglas Guilbeault,"['1/üö® New working paper w. @joshua_a_becker and @samuelwoolley: our pre-registered experiment shows that evaluating news as simply true or false limits learning in social networks <LINK>. <LINK>', '2/ However, we show that when people collaboratively evaluate the probability that news is true (from 0 to 100%), allowing for more nuanced and continuous representations of news veracity, then social networks collectively learn much more reliably. https://t.co/uG7t6nfqK8', '3/ Probabilistic social learning also significantly reduced partisan differences in news assessments that remained entrenched when groups evaluated the accuracy of news in simple binary terms (true or false). https://t.co/QxVgMjRb8T', '4/ These benefits of probabilistic social learning are robust to the topic of news - vaccines, politics, economics, terrorism ‚Äì and subjects‚Äô education, race, income, religion, and partisanship. https://t.co/lYKwq3f5LX', '5/ These results were predicted by a formal model which indicates that social learning benefits when people can signal uncertainty and explicitly communicate even minor adjustments to their beliefs; pre-registration here https://t.co/FkUu63CHjH', '6/ Comments welcome! With the election approaching, we hope our research will get people reflecting on the dangers of allowing simple binaries to do our thinking for us. This is key given how often the binary between true and fake news is leveraged as a partisan talking point', '7/ This experiment was run with @empirica_ly and received financial support from the New Venture Fund.']",https://arxiv.org/abs/2010.06019,"The digital spread of misinformation is one of the leading threats to democracy, public health, and the global economy. Popular strategies for mitigating misinformation include crowdsourcing, machine learning, and media literacy programs that require social media users to classify news in binary terms as either true or false. However, research on peer influence suggests that framing decisions in binary terms can amplify judgment errors and limit social learning, whereas framing decisions in probabilistic terms can reliably improve judgments. In this preregistered experiment, we compare online peer networks that collaboratively evaluate the veracity of news by communicating either binary or probabilistic judgments. Exchanging probabilistic estimates of news veracity substantially improved individual and group judgments, with the effect of eliminating polarization in news evaluation. By contrast, exchanging binary classifications reduced social learning and entrenched polarization. The benefits of probabilistic social learning are robust to participants' education, gender, race, income, religion, and partisanship. ","Probabilistic Social Learning Improves the Public's Detection of
  Misinformation"
156,1316757835171147777,86242337,Anant Madabhushi,"['Thrilled to report new @arxiv paper ""A Pathologist-Annotated Dataset for Validating Artificial Intelligence: A Project Description and Pilot Study"" with Brandon Gallas, Sara Dudgeon at @US_FDA, along with luminaries in #computationalpathology.\n<LINK> <LINK>']",https://arxiv.org/abs/2010.06995,"Purpose: In this work, we present a collaboration to create a validation dataset of pathologist annotations for algorithms that process whole slide images (WSIs). We focus on data collection and evaluation of algorithm performance in the context of estimating the density of stromal tumor infiltrating lymphocytes (sTILs) in breast cancer. Methods: We digitized 64 glass slides of hematoxylin- and eosin-stained ductal carcinoma core biopsies prepared at a single clinical site. We created training materials and workflows to crowdsource pathologist image annotations on two modes: an optical microscope and two digital platforms. The workflows collect the ROI type, a decision on whether the ROI is appropriate for estimating the density of sTILs, and if appropriate, the sTIL density value for that ROI. Results: The pilot study yielded an abundant number of cases with nominal sTIL infiltration. Furthermore, we found that the sTIL densities are correlated within a case, and there is notable pathologist variability. Consequently, we outline plans to improve our ROI and case sampling methods. We also outline statistical methods to account for ROI correlations within a case and pathologist variability when validating an algorithm. Conclusion: We have built workflows for efficient data collection and tested them in a pilot study. As we prepare for pivotal studies, we will consider what it will take for the dataset to be fit for a regulatory purpose: study size, patient population, and pathologist training and qualifications. To this end, we will elicit feedback from the FDA via the Medical Device Development Tool program and from the broader digital pathology and AI community. Ultimately, we intend to share the dataset, statistical methods, and lessons learned. ","A Pathologist-Annotated Dataset for Validating Artificial Intelligence:
  A Project Description and Pilot Study"
157,1316752822193606656,1149734612693745666,Emily Rauscher,"['üó£ New paper alert! <LINK> In this work, led by Michael Roman, we present a grid of 3D models of hot Jupiters with clouds (temperature dependent and with radiative feedback) across a wide range of irradiation temperature, for different assumed cloud properties. <LINK>', 'In order of decreasing irradiation temperate, clouds form successively on: the night, the cool western terminator, the eastern terminator, and even across the dayside!', 'For planets cooler than irradiation temperatures of about 3000 K (Teq ~ 2100 K) clouds influence observable properties, like phase curves (both thermal and reflected)!', 'Sometimes clouds even produce temperature inversions!', 'Also see a lovely complementary paper by @V_Parmentier that is also on arXiv today!', '(Sorry this thread isn‚Äôt snazzier... my kindergartener needs help with his class work and is crying in my lap right now)']",http://arxiv.org/abs/2010.06936,"Using a general circulation model (GCM), we investigate trends in simulated hot Jupiter atmospheres for a range of irradiation temperatures (1,500 - 4,000 K), surface gravities (10 and 40 m s-2), and cloud conditions. Our models include simplified temperature-dependent clouds with radiative feedback and show how different cloud compositions, vertical thicknesses, and opacities shape hot Jupiters atmospheres by potentially increasing planetary albedos, decreasing photospheric pressures and nightside temperatures, and in some cases producing strong dayside thermal inversions. With decreasing irradiation, clouds progressively form on the nightside and cooler western limb followed by the eastern limb and central dayside. We find that clouds significantly modify the radiative transport and affect the observable properties of planets colder than T_irr ~ 3,000~K (T_eq~2,100 K) depending on the clouds' vertical extent. The precise strength of expected effects depends on the assumed parameters, but trends in predicted phase curves emerge from an ensemble of simulations. Clouds lead to larger phase curve amplitudes and smaller phase curve offsets at IR wavelengths, compared to cloud-free models. At optical wavelengths, we predict mostly westward phase curve offsets at intermediate temperatures (T_irr ~ 2,000 - 3,500 K) with clouds confined to the nightside and western limb. If clouds are vertically compact (i.e. on order of a pressure scale height in thickness), their distributions and effects become more complicated as different condensates form at different heights -- some too deep to significantly affect the observable atmosphere. Our results have implications for interpreting the diversity of phase curve observations of planets with T_irr <~3,000~K. ","Clouds in Three-Dimensional Models of Hot Jupiters Over a Wide Range of
  Temperatures I: Thermal Structures and Broadband Phase Curve Predictions"
158,1316725514233577472,1296874661519785985,Jay Fuhrman,"[""Excited to share our new working paper on the contribution of direct air capture and other negative emissions technologies to China's goal of carbon neutrality by 2060. @andresclarens @3y419 @ScottDoney1 @bill_shobe \n<LINK> <LINK>"", 'Getting to net-zero emissions without them will be REALLY hard. However, if (and only if) China and other nations put incentivizing policies in place consistent with limiting climate change to anywhere close to the IPCCs below-2C target, direct air capture can play a large role.', 'In China, which has large estimated capacity for geologic carbon storage, DAC and other negative emissions technologies could contribute up to 2.5 GtCO2 per year of carbon removal, offsetting difficult-to-mitigate sectors such as transportation and heavy industry.', 'DAC in particular could contribute up to 1.5 GtCO2 per year of negative emissions, reducing the need for land-intensive carbon removal such as BECCS and afforestation, while also allowing increased positive emissions.']",https://arxiv.org/abs/2010.06723,"China's pledge to reach carbon neutrality before 2060 is an ambitious goal and could provide the world with much-needed leadership on how to limit warming to +1.5C warming above pre-industrial levels by the end of the century. But the pathways that would achieve net zero by 2060 are still unclear, including the role of negative emissions technologies. We use the Global Change Analysis Model to simulate how negative emissions technologies, in general, and direct air capture (DAC) in particular, could contribute to China's meeting this target. Our results show that negative emissions could play a large role, offsetting on the order of 3 GtCO2 per year from difficult-to-mitigate sectors such as freight transportation and heavy industry. This includes up to a 1.6 GtCO2 per year contribution from DAC, constituting up to 60% of total projected negative emissions in China. But DAC, like bioenergy with carbon capture and storage and afforestation, has not yet been demonstrated at anywhere approaching the scales required to meaningfully contribute to climate mitigation. Deploying NETs at these scales will have widespread impacts on financial systems and natural resources such as water, land, and energy in China. ","The role of negative emissions in meeting China's 2060 carbon neutrality
  goal"
159,1316594134082359296,913058608883142657,Patrick Vallely üõ∞Ô∏èüååüî≠,"['New paper day!\n\nWe use @NASA_TESS to look at the explosive deaths of high-mass stars in a new way! üí•üëÄ\n\nFull paper: <LINK>\n\nBrief explainer thread: ‚¨áÔ∏è <LINK>', 'TESS is a fantastic space mission that monitors big chunks of the sky for about a month, taking images every half-hour. Allegedly this is to search for transiting exoplanets, but the big secret is that TESS is actually up there to study tidal disruption events and supernovae. ü§´ https://t.co/OTZhf4ksCJ', ""@NASAExoplanets can't admit this, of course, which is where we at @SuperASASSN come in. üòâ\n\nWe've put together an image processing pipeline that lets us use the excellent data from TESS to study these exciting extragalactic transients. https://t.co/nPwc29clt8"", 'This is our biggest TESS supernova paper yet, with observations of twenty bright core-collapse events! https://t.co/MKMCjqWP6k', 'To better understand our data, we converted a set of 100+ simulated supernovae (from this paper: https://t.co/m9cDoGwi81) into synthetic TESS observations, and we noticed a very tight correlation between the supernova rise time and the size of the pre-supernova star. https://t.co/d8mi3544aS', 'If we use that relation to estimate the size of the progenitor stars that exploded to produce the Type II supernovae in our sample, we find that they are remarkably similar to what @emsque found for Galactic red supergiants. üëÄ https://t.co/N14hHyDq77', 'Another interesting thing we find is that, when we stack all of our Type II light curves, there is a very clear excess in the stacked data in the day or so before the time of first light implied by our empirical light curve fits. https://t.co/y3YTK98NOd', ""The most likely explanation for this excess seems to be that we're seeing shock-breakout emission, the very brief initial outburst of radiation that occurs when the shock of the supernova explosion reaches the surface of the star. https://t.co/z15hG8Qebz"", ""There's some more fun stuff in the paper where we talk about some of the tricks we use to reduce TESS data and we try to mess around with fitting the data with a couple semi-analytic models.\n\nGive it a read if you're interested in supernova physics! https://t.co/vVYA2emyVp"", ""@starstrickenSF We'd hoped to find a nice standardizable candle type thing for rise time/rise rate correlating with peak brightness, but alas. Looks like it'll still be up to you for all the core-collapse cosmology shenanigans. üòÖ""]",https://arxiv.org/abs/2010.06596,"We present observations from the Transiting Exoplanet Survey Satellite (TESS) of twenty bright core-collapse supernovae with peak TESS-band magnitudes $\lesssim18$ mag. We reduce this data with an implementation of the image subtraction pipeline used by the All-Sky Automated Survey for Supernovae (ASAS-SN) optimized for use with the TESS images. In empirical fits to the rising light curves, we do not find strong correlations between the fit parameters and the peak luminosity. Existing semi-analytic models fit the light curves of the Type II supernovae well, but do not yield reasonable estimates of the progenitor radius or explosion energy, likely because they are derived for use with ultraviolet observations while TESS observes in the near-infrared. If we instead fit the data with numerically simulated light curves, the rising light curves of the Type~II SNe are consistent with the explosions of red supergiants. While we do not identify shock breakout emission for any individual event, when we combine the fit residuals of the Type II supernovae in our sample, we do find a $>5\sigma$ flux excess in the $\sim 0.5$~day before the start of the light curve rise. It is likely that this excess is due to shock breakout emission, and that during its extended mission TESS will observe a Type II supernova bright enough for this signal to be detected directly. ","High-Cadence, Early-Time Observations of Core-Collapse Supernovae From
  the TESS Prime Mission"
160,1316575049814405122,188182794,An Tran,['PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite Imagery with Multi-stage Training\n<LINK>\n\nMy new paper for an efficient method for generalizing semantic segmentation model of high resolution satellite imagery.\n\nTake it guys!!!'],https://arxiv.org/abs/2010.06932,"Road network and building footprint extraction is essential for many applications such as updating maps, traffic regulations, city planning, ride-hailing, disaster response \textit{etc}. Mapping road networks is currently both expensive and labor-intensive. Recently, improvements in image segmentation through the application of deep neural networks has shown promising results in extracting road segments from large scale, high resolution satellite imagery. However, significant challenges remain due to lack of enough labeled training data needed to build models for industry grade applications. In this paper, we propose a two-stage transfer learning technique to improve robustness of semantic segmentation for satellite images that leverages noisy pseudo ground truth masks obtained automatically (without human labor) from crowd-sourced OpenStreetMap (OSM) data. We further propose Pyramid Pooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentation that uses focal loss, poly learning rate, and context module. We demonstrate the strengths of our approach through evaluations done on three popular datasets over two tasks, namely, road extraction and building foot-print detection. Specifically, we obtain 78.19\% meanIoU on SpaceNet building footprint dataset, 67.03\% and 77.11\% on the road topology metric on SpaceNet and DeepGlobe road extraction dataset, respectively. ","PP-LinkNet: Improving Semantic Segmentation of High Resolution Satellite
  Imagery with Multi-stage Training"
161,1316460693609013248,2906950303,Yana Safonova,"['Check out our new review paper on applications of the trace reconstruction problems in two fields of computational biology: immunogenomics and DNA storage!\n\nI am proud to be a part of this collaboration: Vinnu Bhardwaj, Pavel Pevzner, and @CyrusRashtchian\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.06083,"The problem of reconstructing a string from its error-prone copies, the trace reconstruction problem, was introduced by Vladimir Levenshtein two decades ago. While there has been considerable theoretical work on trace reconstruction, practical solutions have only recently started to emerge in the context of two rapidly developing research areas: immunogenomics and DNA data storage. In immunogenomics, traces correspond to mutated copies of genes, with mutations generated naturally by the adaptive immune system. In DNA data storage, traces correspond to noisy copies of DNA molecules that encode digital data, with errors being artifacts of the data retrieval process. In this paper, we introduce several new trace generation models and open questions relevant to trace reconstruction for immunogenomics and DNA data storage, survey theoretical results on trace reconstruction, and highlight their connections to computational biology. Throughout, we discuss the applicability and shortcomings of known solutions and suggest future research directions. ",Trace Reconstruction Problems in Computational Biology
162,1316458822135767040,1210312444221935616,Cyrus Rashtchian,"['[1/4] Challenge: recover an unknown string, but you only see noisy copies, bits randomly deleted. How many samples do you need?\n\nNew survey paper ""Trace Reconstruction Problems in Computational Biology"" w/ Vinnu Bhardwaj, Pavel A. Pevzner @yana_safonova_ \n\n<LINK>', ""[2/4] We've been working on this for about a year. It has two main goals: \n1) Describe many new models motivated by computational immunology, so that #MachineLearning / #Statistics / #compbio ppl work on them!\n2) Provide a coherent survey of the lots of recent work in the area https://t.co/fYHDTNlH9g"", '[3/4] The above pic shows some models, where instead of indels, there is random trimming/errors in prefix or suffix. This is motivated by VDJ recombination, which is a crucial and very cool way that your body fights off bad pathogens (leading to the 1987 #NobelPrize !). https://t.co/2fprDA6WCj', '[4/4] Beyond immunology, we also discuss #DNA data storage and why the trace reconstruction problems show up naturally when you are recovering the data. We also describe the main theoretical advances, with even more open questions! https://t.co/ookRHrLB0O']",https://arxiv.org/abs/2010.06083,"The problem of reconstructing a string from its error-prone copies, the trace reconstruction problem, was introduced by Vladimir Levenshtein two decades ago. While there has been considerable theoretical work on trace reconstruction, practical solutions have only recently started to emerge in the context of two rapidly developing research areas: immunogenomics and DNA data storage. In immunogenomics, traces correspond to mutated copies of genes, with mutations generated naturally by the adaptive immune system. In DNA data storage, traces correspond to noisy copies of DNA molecules that encode digital data, with errors being artifacts of the data retrieval process. In this paper, we introduce several new trace generation models and open questions relevant to trace reconstruction for immunogenomics and DNA data storage, survey theoretical results on trace reconstruction, and highlight their connections to computational biology. Throughout, we discuss the applicability and shortcomings of known solutions and suggest future research directions. ",Trace Reconstruction Problems in Computational Biology
163,1316448818867699714,2878031881,Yangsibo Huang,"['How to tackle data privacy for language understanding tasks in distributed learning (without slowing down training or reducing accuracy)? Happy to share our new #emnlp2020 findings paper\n\nw/ @realZhaoSong, @danqi_chen, Prof. Kai Li, @prfsanjeevarora\npaper: <LINK> <LINK>']",https://arxiv.org/abs/2010.06053,"An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only $1.9\%$. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem. Our code is available at this https URL ",TextHide: Tackling Data Privacy in Language Understanding Tasks
164,1316429411193847812,1229384073363251202,Jun Yen Leung,"['Check out our new #EMNLP2020 paper at <LINK>. My co-authors @ryandcotterell and Guy Emerson and I show that adjective ordering is universal across languages. 1/4', 'When you have more than one adjective modifying a noun (e.g. ‚Äúthe big red dog‚Äù), the order of those adjectives is dictated by grammar. These ordering preferences intuitively seem to generalize across languages. 2/4', 'We train a latent-variable model that captures adjective ordering preferences using data in one set of languages, and test it on data from a separate set of languages. Amazingly, the model is accurate despite never having seen the testing languages (a zero-shot scenario!). 3/4', 'This suggests that the same set of adjective ordering rules apply across all languages. Here are results for a model trained on Czech, English, German, and Russian. 4/4 https://t.co/SkaFl5YmfK']",http://arxiv.org/abs/2010.04755,"Across languages, multiple consecutive adjectives modifying a noun (e.g. ""the big red dog"") follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies. ","Investigating Cross-Linguistic Adjective Ordering Tendencies with a
  Latent-Variable Model"
165,1316321913132679168,1140222123006472194,Kasper Elm Heintz,"['Happy to announce that our new paper: <LINK>  was posted today on ArXiv! \nIn this, we basically ask: can you efficiently identify quasars purely as point-like sources that doesn‚Äôt move on the sky? \nTurns out: Yes! \nAs long as its far away from the Galactic plane', 'This has been a rollercoaster of a project, starting with the release of @ESAGaia-DR2 more than 2 years ago, and with several non-succesful observing runs due to clouds, ice, etc. https://t.co/6lmzM4HrAQ', 'When we *finally* got spectra of our targets, we have had several first-year students at @DAWNCopenhagen/@uni_copenhagen help us classify them as either stars ‚≠êÔ∏è or quasars üí• (some many billion of lightyears away) ‚Äî and for many of them, this was their first academic paper!', 'Briefly, what we found was: it is possible to efficiently select quasars (~70% of the total number of targets) purely as sources with zero proper motions and parallaxes as measured by @ESAGaia-DR2. https://t.co/AbzczXmkCI', 'This is, however, only feasible to reach at high Galactic latitudes, i.e. far away from the Galactic plane. Moving closer, we would be completely overwhelmed by stars that were apparently non-moving on the sky (at least not in the transverse direction). https://t.co/nN77blhLfA', 'This *complete* sample of quasars allowed us to examine the intrinsic properties of the quasar population, and specifically how large the fraction of ‚Äúred‚Äù quasars is (anyway between 10% and 40% depending on the brightness limit). https://t.co/B9huHsfbo5', 'In this survey we also discovered several very interesting quasars ‚Äî here is one example that shines right through the disk of a nearby (z~0.02) galaxy! This allowed us to study the dust and metals in this disk, that was imposed on the background quasar spectrum. https://t.co/MY7pw0cXH7', 'Finally, we could also assess how complete other, more commonly used photometric quasar surveys were at similar brightness ‚Äî turns out, they only manage to find 85-90% of the quasars we do! https://t.co/eEtbUjJTcS', 'This just cements the fact that this survey is the only way to select quasars, or extragalactic point-sources in general, without assumptions of their intrinsic emission (i.e. radio-loud, blue UV colors, etc) ‚Äî so basically, quasars that don‚Äôt look like typical quasars!', '@dr_guangtou No apparently not! As far as I understand, extended sources will not trigger a detection on Gaia‚Äôs CCD ‚Äî however, for bright AGN (and this spectacular case) Gaia will be sensitive enough to disentangle and accurately measure the central (or outlying) point-source.']",https://arxiv.org/abs/2010.05934,"Here we explore the efficiency and fidelity of a purely astrometric selection of quasars as point sources with zero proper motions in the {\it Gaia} data release 2 (DR2). We have built a complete candidate sample including 104 Gaia-DR2 point sources brighter than $G<20$ mag within one degree of the north Galactic pole (NGP), all with proper motions consistent with zero within 2$\sigma$ uncertainty. In addition to pre-existing spectra, we have secured long-slit spectroscopy of all the remaining candidates and find that all 104 stationary point sources in the field can be classified as either quasars (63) or stars (41). The selection efficiency of the zero-proper-motion criterion at high Galactic latitudes is thus $\approx 60\%$. Based on this complete quasar sample we examine the basic properties of the underlying quasar population within the imposed limiting magnitude. We find that the surface density of quasars is 20 deg$^{-2}$, the redshift distribution peaks at $z\sim1.5$, and that only eight systems ($13^{+5}_{-3}\%$) show significant dust reddening. We then explore the selection efficiency of commonly used optical, near- and mid-infrared quasar identification techniques and find that they are all complete at the $85-90\%$ level compared to the astrometric selection. Finally, we discuss how the astrometric selection can be improved to an efficiency of $\approx70\%$ by including an additional cut requiring parallaxes of the candidates to be consistent with zero within 2$\sigma$. The selection efficiency will further increase with the release of future, more sensitive astrometric measurement from the Gaia mission. This type of selection, purely based on the astrometry of the quasar candidates, is unbiased in terms of colours and emission mechanisms of the quasars and thus provides the most complete census of the quasar population within the limiting magnitude of Gaia. ","Spectroscopic classification of a complete sample of
  astrometrically-selected quasar candidates using Gaia DR2"
166,1316302893859983361,322636963,Jonathan Berant,"[""In case you haven't seen enough papers on arXiv recently a new EMNLP findings paper #emnlp2020 led by Inbar Oren: \n<LINK>\nWe know already seq2seq models generalize badly to new compositions in semantic parsing... 1/3"", 'We thoroughly analyze architecture choices and propose extensions to the attention mechanism to improve comp. generalization. We dramatically improve gen. to new compositions, but bottom line generalization is still bad. In collab. with @nlpmattg  @nitish_gup @jonherzig. 2/3', ""Too bad we don't have a sexy figure in the paper to attach here... :( fin""]",https://arxiv.org/abs/2010.05647,"Generalization of models to out-of-distribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in semantic parsing, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional generalization. We find that the following factors improve compositional generalization: (a) using contextual representations, such as ELMo and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower. ",Improving Compositional Generalization in Semantic Parsing
167,1316294941157580801,90131577,Noam Slonim üü¢,"['Expanding #ProjectDebater beyond English. New paper by our team @IBMResearch using multi-ling BERT to address stance analysis, evidence detection, and argument quality in 6 languages + new datasets; in Findings of #emnlp2020 #ComputationalArgumentation -- <LINK>']",https://arxiv.org/abs/2010.06432,"The growing interest in argument mining and computational argumentation brings with it a plethora of Natural Language Understanding (NLU) tasks and corresponding datasets. However, as with many other NLU tasks, the dominant language is English, with resources in other languages being few and far between. In this work, we explore the potential of transfer learning using the multilingual BERT model to address argument mining tasks in non-English languages, based on English datasets and the use of machine translation. We show that such methods are well suited for classifying the stance of arguments and detecting evidence, but less so for assessing the quality of arguments, presumably because quality is harder to preserve under translation. In addition, focusing on the translate-train approach, we show how the choice of languages for translation, and the relations among them, affect the accuracy of the resultant model. Finally, to facilitate evaluation of transfer learning on argument mining tasks, we provide a human-generated dataset with more than 10k arguments in multiple languages, as well as machine translation of the English datasets. ",Multilingual Argument Mining: Datasets and Analysis
168,1316291247254974466,90131577,Noam Slonim üü¢,['The workweek is the best time to start a family! What happens when we finetune #GPT2 to _argue_ about controversial topics? A new paper from our #ProjectDebater team @IBMResearch to appear in Findings of #emnlp2020 #NLP #ComputationalArgumentation\n<LINK>'],https://arxiv.org/abs/2010.06185,"Argument generation is a challenging task whose research is timely considering its potential impact on social media and the dissemination of information. Here we suggest a pipeline based on GPT-2 for generating coherent claims, and explore the types of claims that it produces, and their veracity, using an array of manual and automatic assessments. In addition, we explore the interplay between this task and the task of Claim Retrieval, showing how they can complement one another. ","The workweek is the best time to start a family -- A Study of GPT-2
  Based Claim Generation"
169,1316289280348688384,22392129,Jasmijn Bastings,"['New #BlackboxNLP 2020 paper with @fajtak: The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? <LINK> #NLProc', 'We summarize the debate on whether attention is explanation, and find that often the goal (whether explicitly stated or not) is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer.', 'For this goal and user, we argue that input saliency methods are much better suited as explanation, and we discuss a few of those. We hope to shift attention from attention to saliency methods, while also discussing some limitations that saliency methods have &amp; going beyond.', ""@TuhinChakr @fajtak If you care about what words humans choose you're evaluating plausibility rather than faithfulness. We argue that a model developer is interested in the latter mostly, and wants to know what the model is using. That doesn't necessarily align with human intuition."", '@TuhinChakr @fajtak Thanks for the reference to YAKE :-) It seems somewhat different from the methods that we describe in the paper, that focus on neural NLP models for which we can exploit gradients or the backwards pass to obtain explanations.']",https://arxiv.org/abs/2010.05607,"There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations. ","The elephant in the interpretability room: Why use attention as
  explanation when we have saliency methods?"
170,1316263076723818501,802543221943439360,Andrea Caputo,['New paper out! <LINK>\nYou probably heard about GW190521 event. Here we prove that LISA will be crucial to inform us about the environments (such as AGNs) where these systems tend to form and to predict the location and the time of the EM counterpart. <LINK>'],https://arxiv.org/abs/2010.06056,"GW190521 is the compact binary with the largest masses observed to date, with at least one in the pair-instability gap. This event has also been claimed to be associated with an optical flare observed by the Zwicky Transient Facility in an Active Galactic Nucleus (AGN), possibly due to the post-merger motion of the merger remnant in the AGN gaseous disk. We show that the Laser Interferometer Space Antenna (LISA) will detect up to ten of such gas-rich black hole binaries months to years before their detection by LIGO/Virgo-like interferometers, localizing them in the sky within $\approx1$ deg$^2$. LISA will also measure directly deviations from purely vacuum and stationary waveforms, arising from gas accretion, dynamical friction, and orbital motion around the AGN's massive black hole (acceleration, strong lensing, and Doppler modulation). LISA will therefore be crucial to alert and point electromagnetic telescopes ahead of time on this novel class of gas-rich sources, to gain direct insight on their physics, and to disentangle environmental effects from corrections to General Relativity that may also appear in the waveforms at low frequencies. ","Detectable environmental effects in GW190521-like black-hole binaries
  with LISA"
171,1316239694670299137,2477990976,Akshay Suresh,"['üö® NEW PAPER ALERT üö®\n\nHappy to announce the first discovery of quiescent continuum emission from the nearby star Epsilon Eridani at 2-4 GHz.\n\nFull paper link: <LINK>\n\n----- Thread (1/6)', 'Eps Eri is an orange main sequence star slightly smaller  than the Sun. Being only ~ 10 light years away, Eps Eri has been a famous target of several searches for extraterrestrial intelligence including Frank Drake‚Äôs Project Ozma. \n\n----- Thread (2/6)', 'Excited by prospects of discovering planet-associated radio flares from the planetary system around Eps Eri, we targeted Eps Eri with the Very Large Array close to the expected peak of its stellar activity cycle in 2019.\n\n----- Thread (3/6)', 'As often happens in science, the data tell a story quite different from one‚Äôs expectations. We detected 2-4 GHz continuum emission from Eps Eri, but no flares in our data.\n\n----- Thread (4/6)', 'We deduce that the observed emission is of likely coronal/chromospheric origin. Our data clearly disfavor stellar wind-based thermal emission models, thus, allowing us to place a stringent upper limit on the mass loss rate from Eps Eri.\n\n----- Thread (5/6)', 'Curiously, our lack of radio flare detections coincides with the optical non-detection of the latest stellar maximum of Eps Eri in 2019. Could the internal magnetic dynamo of Eps Eri have evolved? We need more observations to find out. \n\n----- Thread (6/6)']",https://arxiv.org/abs/2010.05929,"The nearby star $\rm \epsilon \ Eridani$ has been a frequent target of radio surveys for stellar emission and extraterrestial intelligence. Using deep $\rm 2-4 \ GHz$ observations with the Very Large Array, we have uncovered a $29 \ \mu {\rm Jy}$ compact, steady continuum radio source coincident with $\rm \epsilon \ Eridani$ to within 0.06 arcseconds ($\lesssim 2\sigma$; 0.2 au at the distance of the star). Combining our data with previous high frequency continuum detections of $\rm \epsilon \ Eridani$, our observations reveal a spectral turnover at $\rm 6 \ GHz$. We ascribe the $\rm 2-6 \ GHz$ emission to optically thick, thermal gyroresonance radiation from the stellar corona, with thermal free-free opacity likely becoming relevant at frequencies below $\rm 1 \ GHz$. The steep spectral index ($\alpha \simeq 2$) of the $\rm 2-6 \ GHz$ spectrum strongly disfavors its interpretation as stellar wind-associated thermal bremsstrahlung ($\alpha \simeq 0.6$). Attributing the entire observed $\rm 2-4 \ GHz$ flux density to thermal free-free wind emission, we thus, derive a stringent upper limit of $3 \times 10^{-11} \ M_{\odot} \ {\rm yr}^{-1}$ on the mass loss rate from $\rm \epsilon \ Eridani$. Finally, we report the non-detection of flares in our data above a $5\sigma$ threshold of $\rm 95 \ \mu Jy$. Together with the optical non-detection of the most recent stellar maximum expected in 2019, our observations postulate a likely evolution of the internal dynamo of $\rm \epsilon \ Eridani$. ",Detection of 2$-$4 GHz Continuum Emission from $\epsilon$ Eridani
172,1316176058014597120,1191056593476915200,Ray Bai,"['New paper! ""Spike-and-slab meets LASSO: A review of the spike-and-slab LASSO."" A survey of spike-and-slab lasso methods, and a nice complement to the review papers by \n@jyotishkadatta et al. on the horseshoe prior.  Check it out here: <LINK> <LINK>', 'Spike-and-slab LASSO is not only amenable to fast posterior mode estimation, but also efficient posterior sampling. SSL can be used in tandem with (approximate) MCMC algorithms or Bayesian bootstrapping, which scale linearly in time with number of predictors!']",https://arxiv.org/abs/2010.06451,"High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets. ",Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO
173,1316143203553288193,1141772501472727040,Lena Voita,"['[1/3] GraphGlove: our new #emnlp2020 paper by Max Ryabinin (@m_ryabinin), Sergei Popov, Liudmila Prokhorenkova and myself.\n\nIn GraphGlove, words are nodes in a differentiable weighted graph, and we learn this graph in an unsupervised manner!\n\n<LINK>\n\n#NLProc <LINK>', '[2/3] We analyze the learned graphs and show that GraphGlove has:\n\n(1) hierarchical structure, similar to that of WordNet, \n\n(2) highly nontrivial geometry containing subgraphs with different local topology.\n\n#emnlp2020 #NLProc https://t.co/lvPjf3OxzT', ""[3/3] Formally, GraphGlove's training procedure is very similar to the vector-based GloVe versions. We modify the training objective and replace a vector space distance with the shortest path distance in the graph.\n\nMore details in the paper!\n\n#emnlp2020 #NLProc https://t.co/oT8M4Zzm81""]",https://arxiv.org/abs/2010.02598,"It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology. ",Embedding Words in Non-Vector Space with Unsupervised Graph Learning
174,1316083890604388353,1271552707464032256,Shunyu Yao,"['Happy to announce our new #emnlp2020 paper ‚ÄúKeep CALM and Explore: Language Models for Action Generation in Text-based Games‚Äù is online! w/ Rohan, @mhauskn, @karthik_r_n\narxiv: <LINK>\ncode: <LINK>\nmore below (1/n)', 'In text-based games, players receive text observation &amp; scalar reward, and issue text actions. For the observation below, what actions would you try? While most previous RL models use a valid-action handicap, we show a data-driven language modeling approach to this problem. https://t.co/by2amequaG', 'Our Contextual Action Language Model (CALM) learns to produce actions conditional on game context (previous &amp; current observation, previous action). To play a new game, we simply trains an RL agent (DRRN) with CALM generating top-k actions as a reduced action space at each state. https://t.co/FqW4TvW14L', 'For training, we collect a new ClubFloyd dataset, containing 200k+ human gameplay context-action pairs from 500+ games. They are noisy, non-optimal (in terms of scoring), but diverse and rich in human commonsense. For CALM training we use both n-gram and GPT-2. https://t.co/C8VVBT4Qex', 'We test on Jericho games which are **ALL UNSEEN** during CALM training. Turns out DRRN + CALM (GPT-2) surpasses all other no-handicap models, and more surprisingly, surpass DRRN + handicap action space on 8/28 games. More results and analysis in paper. Code and Dataset in repo. https://t.co/WG9TGFsdjK', 'Some interesting thoughts at the interaction of language and RL, e.g. how to/what aspects of functional use of language can be learned from LM pertaining? can be transferred across envs./tasks? does RL action space come from env. or agent? Hope you enjoy the fun paper!']",https://arxiv.org/abs/2010.02903,"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at this https URL ","Keep CALM and Explore: Language Models for Action Generation in
  Text-based Games"
175,1316058064290627584,1208983316822540288,Hannah Li,"['Sharing a new paper with @FaidraMonachou and @NikhGarg! We look at the role that standardized testing and affirmative action play in school admissions, and their impact on diversity and identification of top students.\nPaper: <LINK> \nThread üßµ 1/', 'Motivated by the push to drop test requirements during the pandemic (as well as the UC decision to drop the SAT/ACT requirements through 2024) and the uncertainty around the implications, we take a modeling approach to better understand implications of such policy changes.\n2/', 'Many critics of standardized testing argue that the tests are biased against already under-represented minorities.  We show that even when this is the case, dropping the tests does NOT always increase diversity.\n3/', 'In cases when schools have low signal about the skill level of a student, removing the test may hurt the student. (Ex: If a student comes from a lesser known high school, their SAT score is a way to stand out.) In such a scenario, removing tests actually decreases diversity.\n4/', 'On the other hand, if testing prevents many under-represented students from even applying to a school, removing the test increases diversity AND the skill level of admitted students by allowing a larger set of qualified students to apply. 5/', 'We then study the interplay of testing with affirmative action. We find that affirmative action increases diversity and, in realistic settings, may have little impact on the skill level of the resulting admitted class.\n6/', 'We also investigate what happens when multiple schools compete with each other for top students. This competition complicates the discussion, and we find that two schools may adopt the same policy but have differing outcomes.\n7/', 'Overall we find that the implications to dropping test scores and affirmative action are subtle, and depend on other components of the application, barriers to testing, and policy decisions of other schools.\n8/8', ""@FaidraMonachou @kiragoldner @NikhGarg Would love to! Let's some up some time soon :)"", '@EdgarVirguezR Happy to hear that these findings were helpful, and keep us updated on the decisions being made at Duke!']",https://arxiv.org/abs/2010.04396,"We study the role of information and access in capacity-constrained selection problems with fairness concerns. We develop a theoretical framework with testable implications that formalizes the trade-off between the (potentially positive) informational role of a feature and its (negative) exclusionary nature when members of different social groups have unequal access to this feature. Our framework finds a natural application to recent policy debates on dropping standardized testing in college admissions. Our primary takeaway is that the decision to drop a feature (such as test scores) cannot be made without the joint context of the information provided by other features and how the requirement affects the applicant pool composition. Dropping a feature may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, in the presence of access barriers to a feature, the interaction between the informational environment and the effect of access barriers on the applicant pool size becomes highly complex. In this case, we provide a threshold characterization regarding when removing a feature improves both academic merit and diversity. Finally, using application and transcript data from the University of Texas at Austin, we illustrate that there exist practical settings where dropping standardized testing improves or worsens all metrics. ","Dropping Standardized Testing for Admissions Trades Off Information and
  Access"
176,1316055126335991808,481018462,Alex Renda,"['New paper at MICRO: ‚ÄúDiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates‚Äù.\n\nDiffTune learns CPU simulator parameters from scratch, leading the simulator to higher accuracy than with expert-provided parameters.\n\n<LINK>. üßµ1/12 <LINK>', 'Basic block throughput predictors like llvm-mca (https://t.co/Bin5qnA9BA) and llvm_sim (https://t.co/jccZFzAbVT) need tens of thousands of parameters per-microarchitecture to configure the simulation, many of which don‚Äôt have measurable values (e.g., ‚Äúinstruction latency‚Äù). 2/12', 'Top-line results: for each simulator, DiffTune learns all of these parameters from scratch, leading the simulator to higher accuracy than when instantiated with expert-provided default parameters. 3/12 https://t.co/9TSd9SvO3o', 'To get an idea of how these simulators work: llvm-mca models a pipelined CPU, where instructions move through the pipeline based on availability of physical resources. The pipeline stages and physical resources are configured for each different target microarchitecture. 4/12 https://t.co/nLv4RP1XyB', 'DiffTune works by first learning a differentiable surrogate of the original simulator, an approximation of the simulator with a well-defined gradient (typical CPU simulators are very discrete and not differentiable). 5/12 https://t.co/GZPmUu6OZZ', 'This figure shows an example of what you get with a differentiable surrogate ‚Äî the blue dots show llvm-mca‚Äôs timing prediction on a specific basic block as a parameter value is varied, and the orange curve shows the prediction of the surrogate of llvm-mca. 6/12 https://t.co/yB6Im57vCw', 'Note that the surrogate in the figure above is smooth and interpolates llvm-mca‚Äôs predictions ‚Äî meaning that we can take the derivative of the timing with respect to the parameter value.\n\nWe learn this surrogate by training a deep neural network to approximate llvm-mca. 7/12 https://t.co/B2YbZqm50k', 'With this differentiable surrogate, DiffTune then optimizes the simulator‚Äôs parameters with gradient descent against a dataset of ground-truth measured basic block timings to find the simulator parameters that allow the simulator to best fit the observed data. 8/12 https://t.co/NdoftdFloy', 'Given the dataset of ground-truth x86 basic block throughput timings from BHive (https://t.co/cUCV3zTX7x), DiffTune learns the entire set of 11,265 parameters in llvm-mca to higher accuracy than the default parameters, across 4 different microarchitectures. 9/12 https://t.co/8FGsOMoG1n', 'Our results show that DiffTune offers the promise of a generic, scalable methodology to learn detailed performance models with only end-to-end measurements. 10/12', 'Looking beyond CPU simulation, DiffTune‚Äôs approach offers the promise of a generic, scalable methodology to learn the parameters of programs using only input-output examples, potentially reducing many programming tasks to simply that of gathering data. 11/12', 'This is joint work with some excellent coauthors: @TomChen17, @charith_mendis, and @mcarbin. The full paper is available at https://t.co/YbJrY50ZTj, and the code is released at https://t.co/s5Xm7lLAeS. Make sure to check out the talk at MICRO! https://t.co/wOF1cIQqm6. 12/12', ""We've also made a small-scale example to play around with, to try out DiffTune! https://t.co/x2rdNPAwTk (13/12)""]",https://arxiv.org/abs/2010.04017,"CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. In some cases, these parameters do not necessarily have a physical realization and are therefore fundamentally approximate, or even unmeasurable. In this paper we present DiffTune, a system for learning the parameters of x86 basic block CPU simulators from coarse-grained end-to-end measurements. Given a simulator, DiffTune learns its parameters by first replacing the original simulator with a differentiable surrogate, another function that approximates the original function; by making the surrogate differentiable, DiffTune is then able to apply gradient-based optimization techniques even when the original function is non-differentiable, such as is the case with CPU simulators. With this differentiable surrogate, DiffTune then applies gradient-based optimization to produce values of the simulator's parameters that minimize the simulator's error on a dataset of ground truth end-to-end performance measurements. Finally, the learned parameters are plugged back into the original simulator. DiffTune is able to automatically learn the entire set of microarchitecture-specific parameters within the Intel x86 simulation model of llvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling model. DiffTune's learned parameters lead llvm-mca to an average error that not only matches but lowers that of its original, expert-provided parameter values. ","DiffTune: Optimizing CPU Simulator Parameters with Learned
  Differentiable Surrogates"
177,1316035236883554306,1559281832,Vishvas Pandey,"['Check out our new paper on arXiv today: <LINK> - ""Modeling quasielastic interactions of monoenergetic kaon decay-at-rest neutrinos"" with some awesome colleagues! <LINK>', ""@QuarkyPhysicist Nuclear physics jargon is to blame Suvayu, doesn't help our particle physics colleagues.""]",https://arxiv.org/abs/2010.05794,"Monoenergetic muon neutrinos at 236 MeV are readily produced in intense medium-energy proton facilities ($\gtrsim$2-3~GeV) when a positive kaon decays at rest (KDAR; $K^+ \rightarrow \mu^+ \nu_\mu$). These neutrinos provide a unique opportunity to both study the neutrino interaction and probe the nucleus with a monoenergetic weak-interaction-only tool. We present cross section calculations for quasielastic scattering of these 236~MeV neutrinos off $^{12}$C and $^{40}$Ar, paying special attention to low-energy aspects of the scattering process. Our model takes the description of the nucleus in a mean-field (MF) approach as the starting point, where we solve Hartree-Fock (HF) equations using a Skyrme type nucleon-nucleon interaction. Thereby, we introduce long-range nuclear correlations by means of a continuum random phase approximation (CRPA) framework where we solve the CRPA equations using a Green's function method. The model successfully describes ($e,e'$) data on $^{12}$C and $^{40}$Ca in the kinematic region that overlaps with the KDAR $\nu_\mu$ phase space. In addition to these results, we present future prospects for precision KDAR cross section measurements and applications of our calculations in current and future experiments that will utilize these neutrinos. ","Modeling quasielastic interactions of monoenergetic kaon decay-at-rest
  neutrinos"
178,1316017602506883075,338526004,Sam Bowman,"['New paper with @WillHuang93 and @liu_haokun at the Negative Results workshop at EMNLP (thread)\n<LINK>', ""I've been very interested in the idea of counterfactual data augmentation‚Äîroughly, minimal pair construction‚Äîfrom @dkaushik96 &amp; @zacharylipton (https://t.co/Mft2pQcEyG), especially to the extent that it offers a more principled way to construct training sets."", ""@DanielKhashabi reported discouraging results, though, in an augmentation project for BoolQ. Assuming that augmentation examples cost about as much as regular examples, augmentation doesn't seem to be worth it as a way of improving generalization.\nhttps://t.co/yycdFbWL2K"", 'Our study is basically a quick replication of that work. For NLI, we have good evidence that augmentation is about as expensive as example creation (https://t.co/4ysPnmzQFG)...', 'Given that, does augmented training data give you better out-of-domain generalization than the same amount of unaugmented data?', ""We ask this using a RoBERTa model trained on Kaushik's SNLI data, with MNLI, the @arnaik19 et al. stress tests, and the SuperGLUE diagnostic set as generalization tests."", 'Our answer is consistent with Khashabi, and negative: The augmented training set yields much worse generalization than a regular training set of the same size.', ""Our study is definitely not conclusive. I don't think that the Kaushik SNLI training set was meant to be the end-all-be-all of counterfactually augmented training sets."", ""In particular, it's built around a relatively small set of seed examples that are heavily augmented, rather than sampling more broadly. But the results are still discouraging."", ""I still think the basic idea behind counterfactual data augmentation is compelling, but If we want to be able to use it as the foundation of a large-scale training data effort, it looks like there's more work to be done.""]",https://arxiv.org/abs/2010.04762,"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks---datasets collected from crowdworkers to create an evaluation task---while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data---data built by minimally editing a set of seed examples to yield counterfactual labels---to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable. ","Counterfactually-Augmented SNLI Training Data Does Not Yield Better
  Generalization Than Unaugmented Data"
179,1316002483899183104,833715179221315588,Tilman Esslinger,"['Geometric properties of adiabatic quantum thermal machines, theoretically described in a new paper by Bhandari et al. <LINK> \n\nEngineering such experiments with cold atoms is getting increasingly more realistic, see e.g. S. H√§usler et al. <LINK>']",https://arxiv.org/abs/2010.00011,"We study thermoelectric currents of neutral, fermionic atoms flowing through a mesoscopic channel connecting a hot and a cold reservoir across the superfluid transition. The thermoelectric response results from a competition between density-driven diffusion from the cold to the hot reservoir and the channel favoring transport of energetic particles from hot to cold. We control the relative strength of both contributions to the thermoelectric response using an external optical potential in a nearly non-interacting and a strongly-interacting system. Without interactions, the magnitude of the particle current can be tuned over a broad range but is restricted to flow from hot to cold in our parameter regime. Strikingly, strong interparticle interactions additionally reverse the direction of the current. We quantitatively model ab initio the non-interacting observations and qualitatively explain the interaction-assisted reversal by the reduction of entropy transport due to pairing correlations. Our work paves the way to studying the coupling of spin and heat in strongly correlated matter using spin-dependent optical techniques with cold atoms. ",Interaction-Assisted Reversal of Thermopower with Ultracold Atoms
180,1315829468645208065,42660153,Virginia V. Williams,['New matrix multiplication paper with @firebat03 ! <LINK>'],https://arxiv.org/abs/2010.05846,"The complexity of matrix multiplication is measured in terms of $\omega$, the smallest real number such that two $n\times n$ matrices can be multiplied using $O(n^{\omega+\epsilon})$ field operations for all $\epsilon>0$; the best bound until now is $\omega<2.37287$ [Le Gall'14]. All bounds on $\omega$ since 1986 have been obtained using the so-called laser method, a way to lower-bound the `value' of a tensor in designing matrix multiplication algorithms. The main result of this paper is a refinement of the laser method that improves the resulting value bound for most sufficiently large tensors. Thus, even before computing any specific values, it is clear that we achieve an improved bound on $\omega$, and we indeed obtain the best bound on $\omega$ to date: $$\omega < 2.37286.$$ The improvement is of the same magnitude as the improvement that [Le Gall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement to the laser method is quite general, and we believe it will have further applications in arithmetic complexity. ",A Refined Laser Method and Faster Matrix Multiplication
181,1315753352328863747,1012783002789675008,Chris O'Connor,"['Pleased to announce another üö®üö® NEW PAPER! üö®üö® Remember that jumbo #WhiteDwarfPlanet, announced just a few weeks ago? We think it went on a wild rollercoaster ride üé¢ to get there. Check it out!üîó<LINK>', 'A parallel work on the same system appeared on arXiv tonight! https://t.co/32gz5z9KYd\n\nFurther proof of a bright future for white-dwarf planetary studies!']",https://arxiv.org/abs/2010.04163,"We investigate the possible origin of the transiting giant planet WD1856+534b, the first strong exoplanet candidate orbiting a white dwarf, through high-eccentricity migration (HEM) driven by the Lidov-Kozai (LK) effect. The host system's overall architecture is an hierarchical quadruple in the '2+2' configuration, owing to the presence of a tertiary companion system of two M-dwarfs. We show that a secular inclination resonance in 2+2 systems can significantly broaden the LK window for extreme eccentricity excitation ($e \gtrsim 0.999$), allowing the giant planet to migrate for a wide range of initial orbital inclinations. Octupole effects can also contribute to the broadening of this 'extreme' LK window. By requiring that perturbations from the companion stars be able to overcome short-range forces and excite the planet's eccentricity to $e \simeq 1$, we obtain an absolute limit of $a_{1} \gtrsim 8 \, {\rm AU} \, (a_{3} / 1500 \, {\rm AU})^{6/7}$ for the planet's semi-major axis just before migration (where $a_{3}$ is the semi-major axis of the 'outer' orbit). We suggest that, to achieve a wide LK window through the 2+2 resonance, WD1856b likely migrated from $30 \, {\rm AU} \lesssim a_{1} \lesssim 60 \, {\rm AU}$, corresponding to $\sim 10$--$20 \, {\rm AU}$ during the host's main-sequence phase. We discuss possible difficulties of all flavours of HEM affecting the occurrence rate of short-period giant planets around white dwarfs. ","Enhanced Lidov-Kozai migration and the formation of the transiting giant
  planet WD1856+534b"
182,1315733290599567360,151193108,Mert R. Sabuncu üá∫üá¶,"[""In our new paper (oral @NeurIPSConf '20), we demonstrate that a neural network that maps a visual stimulus to fMRI response can learn visual attention masks that are remarkably similar to eye gaze patterns. Pre-print: <LINK> @meenakshik93 @AmyKuceyeski @ngohgia""]",https://arxiv.org/abs/2010.00516,"Visual perception is critically influenced by the focus of attention. Due to limited resources, it is well known that neural representations are biased in favor of attended locations. Using concurrent eye-tracking and functional Magnetic Resonance Imaging (fMRI) recordings from a large cohort of human subjects watching movies, we first demonstrate that leveraging gaze information, in the form of attentional masking, can significantly improve brain response prediction accuracy in a neural encoding model. Next, we propose a novel approach to neural encoding by including a trainable soft-attention module. Using our new approach, we demonstrate that it is possible to learn visual attention policies by end-to-end learning merely on fMRI response data, and without relying on any eye-tracking. Interestingly, we find that attention locations estimated by the model on independent data agree well with the corresponding eye fixation patterns, despite no explicit supervision to do so. Together, these findings suggest that attention modules can be instrumental in neural encoding models of visual stimuli. ",Neural encoding with visual attention
183,1315729920887324672,1069642708036259840,Josh Robinson,"['Really excited to share some of our new work on the role of negative samples in contrastive representation learning!\n\nPaper: <LINK>\nCode: <LINK>\n\nwith @ChingYaoChuang, @optiML , and Stefanie Jegelka <LINK>', 'There has been a lot of great work on sampling positive views, but what about negatives? We develop a simple method for sampling ""hard"" negatives that the embedding currently fails to differentiate well from an anchor.', 'Theoretically, we show that a limiting version of our proposed objective has optimal embeddings that tightly cluster points with the same label.', 'Experimentally, our method leads to a linear readout accuracy of 87.44% on STL10, compared to 80.15% for SimCLR. Implementation requires only a couple of extra lines of code, and adds no computational overhead!']",https://arxiv.org/abs/2010.04592,"How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervised sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead. ",Contrastive Learning with Hard Negative Samples
184,1315636064376291328,1465200073,Yannis Flet-Berliac,"['Is Standard Deviation the new Standard?\n\nHappy to finally share Actor with Variance Estimated Critic (AVEC), where we introduce a new training objective for the critic in actor-critic algorithms to better approximate the value function.\n\nPaper: <LINK>\n\n(1/5)', 'Instead of learning absolute state (or state-action) values as in traditional actor-critic methods, AVEC learns their *relative* values. In noiseless RL, we show that reward signals are better captured with the variance of residual errors.\n\n(2/5) https://t.co/kT7SjES19J', 'Compared to PPO and SAC, a variance-estimated critic drastically improves performance (+26% for SAC, +40% for PPO). Here is the performance at 1M steps on several continuous control tasks.\n\n(3/5) https://t.co/sfnPTsiusi', 'We also empirically confirm that AVEC better fits the value function compared to the top actor-critic methods and further reduces the variance of the gradient.\n\n(4/5) https://t.co/d52hIu02Uz', ""For more context, please also read @andrew_ilyas et al. (ICLR'20) on the behaviour of deep policy gradient algorithms, the main motivation for our work.\n\nThank you to @RedaOuhamma, Odalric-Ambrym Maillard and Philippe Preux for the fantastic collaboration.\n\n(5/5)""]",https://arxiv.org/abs/2010.04440,"Policy gradient algorithms have proven to be successful in diverse decision making and control tasks. However, these methods suffer from high sample complexity and instability issues. In this paper, we address these challenges by providing a different approach for training the critic in the actor-critic framework. Our work builds on recent studies indicating that traditional actor-critic algorithms do not succeed in fitting the true value function, calling for the need to identify a better objective for the critic. In our method, the critic uses a new state-value (resp. state-action-value) function approximation that learns the value of the states (resp. state-action pairs) relative to their mean value rather than the absolute value as in conventional actor-critic. We prove the theoretical consistency of the new gradient estimator and observe dramatic empirical improvement across a variety of continuous control tasks and algorithms. Furthermore, we validate our method in tasks with sparse rewards, where we provide experimental evidence and theoretical insights. ","Learning Value Functions in Deep Policy Gradients using Residual
  Variance"
185,1315570996565479425,1006251815070027782,Daniel Michelsanti,"['New work by @GMorrone8 at CASPR - @es_aau: ""Audio-Visual Speech Inpainting with Deep Learning"".\nPaper: <LINK>\nProject page (with full demo): <LINK> <LINK>']",https://arxiv.org/abs/2010.04556,"In this paper, we present a deep-learning-based framework for audio-visual speech inpainting, i.e., the task of restoring the missing parts of an acoustic speech signal from reliable audio context and uncorrupted visual information. Recent work focuses solely on audio-only methods and generally aims at inpainting music signals, which show highly different structure than speech. Instead, we inpaint speech signals with gaps ranging from 100 ms to 1600 ms to investigate the contribution that vision can provide for gaps of different duration. We also experiment with a multi-task learning approach where a phone recognition task is learned together with speech inpainting. Results show that the performance of audio-only speech inpainting approaches degrades rapidly when gaps get large, while the proposed audio-visual approach is able to plausibly restore missing information. In addition, we show that multi-task learning is effective, although the largest contribution to performance comes from vision. ",Audio-Visual Speech Inpainting with Deep Learning
186,1315462218184491008,133971296,Reza Haffari,"['New paper: ""Leveraging Discourse Rewards for Document-Level Neural Machine Translation"" \nThe pre-print: <LINK>\n\nJoint work with   @ijauregi &amp; UTS colleagues.\nAccepted to @coling2020 <LINK>', 'We propose a training approach to optimize two discourse metrics, lexical cohesion &amp; coherence, using reinforcement learning.  In the case of the Zh-En, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the\nrunner-up baseline.']",https://arxiv.org/abs/2010.03732,"Document-level machine translation focuses on the translation of entire documents from a source to a target language. It is widely regarded as a challenging task since the translation of the individual sentences in the document needs to retain aspects of the discourse at document level. However, document-level translation models are usually not trained to explicitly ensure discourse quality. Therefore, in this paper we propose a training approach that explicitly optimizes two established discourse metrics, lexical cohesion (LC) and coherence (COH), by using a reinforcement learning objective. Experiments over four different language pairs and three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in F_BERT. ","Leveraging Discourse Rewards for Document-Level Neural Machine
  Translation"
187,1314675584522424322,72712873,Nick Tenev,"['Excited to share ""Optimal Echo Chambers,"" a new working paper with Gabriel Martinez. Here\'s the intuition, for #EconTwitter and anyone interested in a theoretical basis for ignoring those who disagree with you. (1/8)\n<LINK>', 'When you learn something about the world, you also learn something about what well-informed people are likely to believe. And this gives you a way to distinguish them from less-informed people. (2/8)', ""So it can be rational to only pay attention to people with opinions similar to your own, if you're short on time and some people are better informed than others. People do this even when money's on the line (e.g. @prof_cookson's work on echo chambers and stock investors). (3/8)"", ""But wait‚Äîaren't echo chambers bad? Of course‚Äîthey prevent people from coming to agreement on the facts. But understanding why they arise is essential to prevention. Targeting the symptom (selective exposure) rather than the cause (uncertainty about quality) can backfire. (4/8)"", ""We show that simply exposing people to contrary views may limit their willingness to change their beliefs, if they don't trust what they're hearing. Limiting people's control over who they listen to can make them less persuadable, and make their beliefs less accurate. (5/8)"", ""Signals close to someone's prior expectation can move their beliefs further than more contrary views. If you think the world's round and I say it's flat, your beliefs may not budge. But if I say it's a little more squashed than you think, maybe you'll meet me halfway. (6/8)"", 'Since echo chambers are a rational response to uncertainty about information source quality, offering alternative ways of distinguishing good sources may do more to improve the accuracy of beliefs than simply trying to expose people to contrary views. (7/8)', ""The paper is all theory, though we do of course discuss some relevant empirical literature. And it's still preliminary, so we welcome critical comments! (8/8)"", ""@prof_cookson Thanks! This does indeed sound relevant -- we'll take a look."", '@prof_cookson Oh, interesting--maybe some people have more time to be reading opposing views than others? Below is a psychology paper with an experimental finding along those lines. Either way, I look forward to reading that new draft of yours!\nhttps://t.co/4ys6vEbBez', ""@prof_cookson @not_a_fish Oh I certainly couldn't judge anyone for a little zoom-induced mushiness, especially on a Friday"", '@prof_cookson @not_a_fish Thanks to you both!']",https://arxiv.org/abs/2010.01249,"We show that echo chambers are a rational response to uncertain information quality, and that broadening the range of views someone is exposed to can slow their learning by making their beliefs less responsive to the views of others. A Bayesian decision maker draws a signal about the (real-valued) state of the world from a collection of unbiased sources of heterogeneous quality. Exclusively sampling signals close to the prior expectation can improve accuracy, as they are more likely high quality. Signals close to the prior expectation can move beliefs further than more contrary views; ignoring those with opposing beliefs makes one more responsive to others' views, since it increases their perceived accuracy. The optimal echo chamber balances the credibility of views similar to one's own against the usefulness of those further away. ",Optimal Echo Chambers
188,1314622638069776386,590175311,Benedict Guttman-Kenney,"['üëãNew work with @johngathergood evaluating recent UK local lockdowns using new real-time spending data\n\nPaper: <LINK>\n\n@econObservatory blog: <LINK>\n\nThread ‚¨áÔ∏è 1/11\n#householdfinance #coronavirusindicators #econtwitter <LINK>', 'We use @fable_data - a new source of European real-time, transaction-level consumption data. We find a 0.91 correlation with comprehensive (aggregated) bank of england credit card data. Its real-time, disaggregated format facilitates studying regional consumption... 2/11 https://t.co/4oJy7cjuNA', 'We use this to inform the big policy question: How can authorities control coronavirus without killing the economy?\n\n3/11', 'Like other countries, UK had large declines in credit card spending in early 2020 when COVID-19 hit and the country went into a national lockdown...\n\n4/11 https://t.co/zUnreFZhNy', 'After the national lockdown was eased the government tried to contain regional outbreaks through a system of local lockdowns restricting household mixing but permitting economic activity in COVID-secure settings (e.g. masks, socially-distanced)... 5/11 https://t.co/wGLcqVD8m2', ""UK local lockdowns are imposed by national government - a contrast to the US where there's been little, if any, national coordination.\n\nLocal lockdowns have been a source of political tension between local and national governments &amp; motivated some new Van Morrison songs... 6/11 https://t.co/vJrG5QjKET"", 'We use a descriptive difference-in-difference approach to evaluate 13 lockdowns.\n\nFor each city or sub-region affected by a local lockdown (e.g. Manchester) we compare COVID-19 cases and offline credit card spending to a similar nearby city (e.g. Liverpool)... 7/11', 'We find lockdowns appear to turn the tide on rising COVID-19 cases in the month after being imposed\n(yellow lines are areas subject to local lockdowns, black nearby control areas).... 8/11 https://t.co/E8tYXIcu3D', 'But we see little if any change in credit card spending. \n\nCertainly no large drop of the magnitude accompanying the national lockdowns earlier in the year.\n\nThis suggests local lockdowns can control the virus without damaging the economy but... 9/11 https://t.co/cxe97q9FI6', 'Isolating, testing and tracing cases are key to the effectiveness of keeping lockdowns local and preventing the need for additional measures. Recent nationwide rises in cases indicate UK is not isolating cases early enough. 10/11', ""There‚Äôs some easter eggs in the annex of the paper e.g. spending by urban geographies.\n\nHope you don't hate it. 11/11 https://t.co/TDQznoAx6c"", 'P.s. Recommend following @EconObservatory - a brilliant resource edited by @econromesh to disseminate COVID-19 research in an accessible way.\n\nFable has been fantastic to collaborate with. John and I are planning more stuff. Email us if you want to know more.', '@threadreaderapp unroll']",https://arxiv.org/abs/2010.04129,"We find UK 'local lockdowns' of cities and small regions, focused on limiting how many people a household can interact with and in what settings, are effective in turning the tide on rising positive COVID-19 cases. Yet, by focusing on household mixing within the home, these local lockdowns have not inflicted the large declines in consumption observed in March 2020 when the first virus wave and first national lockdown occurred. Our study harnesses a new source of real-time, transaction-level consumption data that we show to be highly correlated with official statistics. The effectiveness of local lockdowns are evaluated applying a difference-in-difference approach which exploits nearby localities not subject to local lockdowns as comparison groups. Our findings indicate that policymakers may be able to contain virus outbreaks without killing local economies. However, the ultimate effectiveness of local lockdowns is expected to be highly dependent on co-ordination between regions and an effective system of testing. ","The English Patient: Evaluating Local Lockdowns Using Real-Time COVID-19
  & Consumption Data"
189,1314621417409990661,3247908407,Rob Leech,"['Our new theoretical paper by Fagerholm et al.: ""The principle of stationary action in neural systems"" also with @rosalynjmoran @FarlKriston\n<LINK>  1/5', 'Dynamical systems in physics, from cannonballs to electromagnetic fields, obey the principle of stationary action; i.e., the action is stationary along the true trajectory followed by the system. 2/5', 'This principle allows for problems to be solved without using equations of motion and therefore massively speeding up computation times. 3/5', 'In this paper, we show how complex oscillatory forms of three computational models of neural activity (operating at micro- and macro-scales) can be cast in a unified form in terms of the principle of stationary action. 4/5', 'This allows us to formally define conserved quantities such as energy and momentum in neural systems and to pave the way for computational neuroscience to be written in the same language as modern physics. 5/5']",https://arxiv.org/abs/2010.02993,"The principle of stationary action is a cornerstone of modern physics, providing a powerful framework for investigating dynamical systems found in classical mechanics through to quantum field theory. However, computational neuroscience, despite its heavy reliance on concepts in physics, is anomalous in this regard as its main equations of motion are not compatible with a Lagrangian formulation and hence with the principle of stationary action. Taking the Dynamic Causal Modelling neuronal state equation, Hodgkin-Huxley model, and the Leaky Integrate-and-Fire model as examples, we show that it is possible to write complex oscillatory forms of these equations in terms of a single Lagrangian. We therefore bring mathematical descriptions in computational neuroscience under the remit of the principle of stationary action and use this reformulation to express symmetries and associated conservation laws arising in neural systems. ",The principle of stationary action in neural systems
190,1314582759797596167,1184198482455945218,Yuber F Perez-G,"['New paper with Jessica Turner, an awesome collaborator!\nWe explore in detail non-resonant leptogenesis in a black hole dominated early Universe, take a look\n<LINK> <LINK>', 'We have found that black holes can modify substantially the final lepton asymmetry depending on the interplay between the products of the evaporation and the epoch of the Universe. For instance, if black holes evaporate before the thermal leptogenesis, the impact would be minimal', 'If, on the other hand, the evaporation occurs way after the thermal leptogenesis era, the outcome would depend on the right-handed neutrino masses. For large masses (&gt; 10^9 GeV) the evaporation could enhance the asymmetry.', 'but if RH neutrinos are lighter, the black hole contribution would not be sufficient, and the effect would be the contrary: BHs would diminish the lepton asymmetry because of the extra radiation produced.', ""So, we find that for the intermediate scale leptogenesis, BHs always tend to decrease the asymmetry, even for very fine tunned scenarios! Then, if it's proven that the Universe had a BH dominated era, this could put tension on this leptogenesis case."", ""This was a fun project! I'm looking forward to understanding the effects of BHs in other leptogenesis scenarios!"", '@tabrizi_zahra Not so many üòÄ']",https://arxiv.org/abs/2010.03565,"We perform the first numerical calculation of the interplay between thermal and black hole induced leptogenesis, demonstrating that the right-handed neutrino surplus produced during the evaporation only partially mitigates the entropy dilution suffered by the thermal component. As such, the intermediate-mass regime of the right-handed neutrinos, $10^6{\rm~GeV} \lesssim M_{N} \lesssim 10^{9}{\rm~GeV}$, could not explain the observed baryon asymmetry even for fine-tuned scenarios if there existed a primordial black hole dominated era, consistent with initial black hole masses of $M_i \gtrsim \mathcal{O}\left(1\right)$ kg. Detection of the gravitational waves emitted from the same primordial black holes would place intermediate-scale thermal leptogenesis under tension. ","Assessing the tension between a black hole dominated early universe and
  leptogenesis"
191,1314581402441089024,262191481,Mark Stevenson,"[""New paper: ‚ÄúRobustness and Reliability of Gender Bias Assessment in WordEmbeddings: The Role of Base Pairs‚Äù to appear at AACL-IJCNLP '20. Shows popular measures of bias are sensitive to the base pairs used as input. Work w/ @OOhaiyangOO and Alison Sneyd <LINK>""]",https://arxiv.org/abs/2010.02847,"It has been shown that word embeddings can exhibit gender bias, and various methods have been proposed to quantify this. However, the extent to which the methods are capturing social stereotypes inherited from the data has been debated. Bias is a complex concept and there exist multiple ways to define it. Previous work has leveraged gender word pairs to measure bias and extract biased analogies. We show that the reliance on these gendered pairs has strong limitations: bias measures based off of them are not robust and cannot identify common types of real-world bias, whilst analogies utilising them are unsuitable indicators of bias. In particular, the well-known analogy ""man is to computer-programmer as woman is to homemaker"" is due to word similarity rather than societal bias. This has important implications for work on measuring bias in embeddings and related work debiasing embeddings. ","Robustness and Reliability of Gender Bias Assessment in Word Embeddings:
  The Role of Base Pairs"
192,1314578548636745738,979644353097490432,Eleftheria Briakou,"['New #emnlp2020 paper w/ @MarineCarpuat at @umdclip on ""Detecting Fine-grained Cross-lingual Semantic Divergences without supervision by Learning to Rank"" is now on arxiv: <LINK>\nCode and data available: <LINK>', 'We show that considering diverse types of semantic divergences in bilingual text benefits both the annotation and prediction of cross-lingual semantic divergences', 'We contribute REFreSD, a new dataset of Wikimatrix En-Fr sentence-pairs, annotated with semantic divergence classes and token level rationales that justify the sentence level annotation', '64% of samples are annotated as divergent and 40% of them contain fine-grained meaning differences, confirming that divergences are too frequent to be ignored even in parallel corpora', 'We introduce Divergent mBERT, a BERT-based model that detects fine-grained sentence-level and token-level semantic divergences without supervision by learning to rank synthetic divergences of varying granularity']",https://arxiv.org/abs/2010.03662,"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences. ","Detecting Fine-Grained Cross-Lingual Semantic Divergences without
  Supervision by Learning to Rank"
193,1314500933011877888,282700930,utku,"['Sparse Neural Net Training has a gradient flow problem and LTs don\'t address this, instead they re-learn the pruning solution. Check out our new paper: ""Gradient Flow in Sparse NNs and How Lottery Tickets Win"" \n\nüìú <LINK>\n\nwith @yanii, Cem Keskin and @ynd. 1/12', 'We propose a new initialization for sparse networks that accounts for varying fan-in/outs for each connection. We show that our initialization preserves signal variance at the output (which is (maybe surprisingly) also achieved by scaled dense initialization of Liu et al.). 2/12 https://t.co/5AiSRfoknp', 'Sparse networks have poor gradient flow (||g||) at initialization if initialized naively and addressing this improves performance on networks with no BN. Additionally we observe improved gradient flow for RigL whereas LTs seem to have poor gradient flow. 3/12 https://t.co/GMwEq72uGh', 'Can improved gradient flow during early training explain the success of RigL? We observe + correlationüåº. RigL significantly increases gradient flow after its connectivity updates, whereas random growing (SET) or inverted RigL criteria have limited effect on this metric. 4/12 https://t.co/nvlRdXyeR5', ""We also observe new negative eigenvalues in the Hessian after RigL updates on the sparse connectivity. These effects seem to die off towards the later part of the training. Improving RigL's impact on gradient flow throughout the training remains as an exciting future work.\n 5/12 https://t.co/Qh5R19K6F2"", 'How about LTs? How come they get good results without improving gradient flow? To answer this, we investigate the relationship between the LT solution and pruning solution. And ask 3 questions: Are they (1) close? (2) in the same basin? (3) similar in function space? 6/12 https://t.co/lo9lKv3DvC', '1) We run multiple experiments using the same mask and visualize the start/end points of LT/Scratch training using Multidimensional Scaling. Results indicate that LTs start closer to the pruning solution and this distance narrows down significantly during the training. 7/12 https://t.co/sAu8WvvpxM', '2)@jefrankle showed LTs exist when training is stable, i.e. goes to the same basin. We show that this basin is the basin of the pruning solution. Given that LTs go back to the pruning basin, rewinding the weights seems unnecessary, which is partially observed by @alex_renda_ 8/12 https://t.co/5dZKXcngbl', ""3) Are LTs similar to the pruning solution? We follow the study of @stanislavfort and show that the learned LT solutions are highly similar to the pruning solution in function space. As a result ensembles of LT solutions aren't significantly better than a single LT solution. 9/12 https://t.co/y10rDGYhUR"", 'Our resulting understanding of Lottery Tickets is depicted below ‚¨áÔ∏è, which helps us explain various phenomena observed about LTs; their transfer, perturbation studies and existence in various settings. Check out the paper for more details! 10/12 https://t.co/Wbtf8UNVsx', 'Overall, we believe our work contributes to a growing body of work which shows the limitations of focusing solely on initialization. We need to focus on the entire sparse training and improving gradient flow might be a starting point.', 'Let us know if we missed any previous result/reference and thanks for reading. üôè We plan to open source the code soon and meanwhile appreciate any feedback on the preprint.', '@charles_irl Thats very true. It is also why we push these results to the appendix as it needs more work. Which one is more important? Positive or negative eigenvaleus? How does the gradient align with the eigenvectors of the Hessian?', '@charles_irl This is Lenet5, so no batchnorm. We don‚Äôt have the densities for ResNet yet; but it would be indeed interesting to see the spectrum in that setting...']",https://arxiv.org/abs/2010.03533,"Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). Through our analysis of gradient flow during training we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and demonstrate the importance of using sparsity-aware initialization. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from - however, this comes at the cost of learning novel solutions. ",Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win
194,1314395870297837569,1220419280946319362,Matthew Heffernan,"[""It's out! Glad to have been part of a new paper from the @JetscapePhysics Simulations and Distributed Computing Working Group on using Bayesian Model Averaging and to constrain properties of QGP:\n\n<LINK>""]",https://arxiv.org/abs/2010.03928,"Using combined data from the Relativistic Heavy Ion and Large Hadron Colliders, we constrain the shear and bulk viscosities of quark-gluon plasma (QGP) at temperatures of ${\sim\,}150{-}350$ MeV. We use Bayesian inference to translate experimental and theoretical uncertainties into probabilistic constraints for the viscosities. With Bayesian Model Averaging we account for the irreducible model ambiguities in the transition from a fluid description of the QGP to hadronic transport in the final evolution stage, providing the most reliable phenomenological constraints to date on the QGP viscosities. ","Phenomenological constraints on the transport properties of QCD matter
  with data-driven model averaging"
195,1314363840256188417,1044298030512570368,Anna Rogers üá™üá∫üá∫üá¶,"['New paperüìú: What Can We Do to Improve Peer Review in NLP? \n<LINK>\nwith @IAugenstein \n\nTLDR: In its current form, peer review is a poorly defined task with apples-to-oranges comparisons and unrealistic expectations. /1 <LINK>', 'Reviewers resort to heuristics such as reject-if-not-SOTA to cope with uncertainty, so the only way to change that is to reduce uncertainty. Which is at least partly doable: better paper-reviewer matching, unambiguous eval criteria, fine-grained tracks, better review forms etc /2', 'Which criteria and forms, exactly? Each field has to find out for itself, through iterative development and experiments. Except that in NLP such work would be hard to publish, so there are no incentives to do it - and no mechanisms to test and compare any solutions. /3', 'We mostly rant about peer review *after* acceptance notifications come out, but what do we expect to change without systematic work on improving its quality *between* conferences? /4 https://t.co/m6TGRLOSVI', 'This is not to say that conference organizers are doing a bad job. But each conference makes a unique set of choices, and we have no way to tell to systematically compare and decide which policies should be kept. Also, this would be a lot of extra work. /5', 'Actionable steps:\n- talk about peer review a lot more, build up prestige of the topic and incentives to work on it\n- create new ACL roles to think about systematic testing/implementation of peer review policies, and feedback mechanism between organizers, authors and reviewers /6', 'Many thanks to the awesome #NLProc community. This paper would have been very different without insights of @emilymbender @yoavgo @complingy @SeeTedTalk @ani_nenkova @astent @karinv and many, many others. /7', 'To make this paper truly meta, it will come out in Findings of EMNLP. /8']",https://arxiv.org/abs/2010.03863,"Peer review is our best tool for judging the quality of conference submissions, but it is becoming increasingly spurious. We argue that a part of the problem is that the reviewers and area chairs face a poorly defined task forcing apples-to-oranges comparisons. There are several potential ways forward, but the key difficulty is creating the incentives and mechanisms for their consistent implementation in the NLP community. ",What Can We Do to Improve Peer Review in NLP?
196,1314264504193159169,2733642475,Soheil Feizi,"['Excited to share our new work: winning ""Lottery tickets"" do exist in generative models (GANs, VAEs) (up to ~90% sparsity). Remarkably, winning tickets ""transfer"" between GANs and VAEs! This is a joint work with @NehaKalibhat and @YogeshBalaji95   \nPaper: <LINK> <LINK> <LINK>', 'What is the practical benefit of lottery tickets in generative models? We show that by detecting ""early-bird tickets"", we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54% reduction in training time of GANs']",https://arxiv.org/abs/2010.02350,"The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized properly, can be trained to reach comparable or even better performance to that of the original network. Prior works in lottery tickets have primarily focused on the supervised learning setup, with several papers proposing effective ways of finding ""winning tickets"" in classification problems. In this paper, we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show that the popular iterative magnitude pruning approach (with late rewinding) can be used with generative losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99% for AutoEncoders, 93% for VAEs and 89% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the transferability of winning tickets across different generative models (GANs and VAEs) sharing the same architecture, suggesting that winning tickets have inductive biases that could help train a wide range of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative models by detecting tickets at very early stages in training called ""early-bird tickets"". Through early-bird tickets, we can achieve up to 88% reduction in floating-point operations (FLOPs) and 54% reduction in training time, making it possible to train large-scale generative models over tight resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan, and Torr 2019) and GraSP (Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of proper network initializations that could improve convergence and stability of generative models. ",Winning Lottery Tickets in Deep Generative Models
197,1314140336323727360,312753273,Farid Shahandeh,"['New paper dance,\n<LINK>.\nIs there a smart efficient (in the number of subsystems) way to detect genuine multipatite entanglement that is also very refined and robust?\nWe introduce a new approach and test it (numerically) on QEC circuits. <LINK>', '@hforootan ŸÇÿ±ÿ®ÿßŸÜÿ™ üôèüòÉ']",https://arxiv.org/abs/2010.02941,"Ensuring the correct functioning of quantum error correction (QEC) circuits is crucial to achieve fault tolerance in realistic quantum processors subjected to noise. The first checkpoint for a fully operational QEC circuit is to create genuine multipartite entanglement across all subsystems of physical qubits. We introduce a conditional witnessing technique to certify genuine multipartite entanglement (GME) that is efficient in the number of subsystems and, importantly, robust against experimental noise and imperfections. Specifically, we prove that the detection of entanglement in a linear number of bipartitions by a number of measurements that also scales linearly, suffices to certify GME. Moreover, our method goes beyond the standard procedure of separating the state from the convex hull of biseparable states, yielding an improved finesse and robustness compared to previous techniques. We apply our method to the noisy readout of stabilizer operators of the distance-three topological color code and its flag-based fault-tolerant version. In particular, we subject the circuits to combinations of three types of noise, namely, uniform depolarizing noise, two-qubit gate depolarizing noise, and bit-flip measurement noise. We numerically compare our method with the standard, yet generally inefficient, fidelity test and to a pair of efficient witnesses, verifying the increased robustness of our method. Last but not least, we provide the full translation of our analysis to a trapped-ion native gate set that makes it suitable for experimental applications. ","Efficient and robust certification of genuine multipartite entanglement
  in noisy quantum error correction circuits"
198,1314107889225785344,1042121476151881728,Christian Henke,"['New paper on arXiv! Using a renormalised energy momentum tensor for space-time and matter, it is demonstrated that the #LambdaCDM model is realised at the #AdS boundary.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.03391,"The well-known brane world model of cosmology is an alternative to the standard model of cosmology, called {\Lambda}CDM (Lambda Cold Dark Matter) model. However, the quadratical energy density of the brane universe requires an undesirable fine tuning. The paper develops a new Anti-de Sitter (AdS) brane world model without the quadratical energy density by using a second Einstein equation, which arises canonically during a holographic renormalisation. On the AdS boundary it is demonstrated that this brane model equals an effective de Sitter {\Lambda}CDM model, whereas the brane model near the AdS boundary agrees to the {\Lambda}CDM model except the earliest times. Finally, it is shown that the coincidence problem is avoided and the cosmological constant problem, if not solved, is greatly reduced. ",Standard Cosmology on the Anti-de Sitter boundary
199,1314013467918176256,397834810,Minh Tran,['Our new paper on closing the gap between the Lieb-Robinson bounds for power-law interactions and achievable protocols for alpha &gt; d. Next stop is alpha &lt;= d. <LINK>'],https://arxiv.org/abs/2010.02930,"We present an optimal protocol for encoding an unknown qubit state into a multiqubit Greenberger-Horne-Zeilinger-like state and, consequently, transferring quantum information in large systems exhibiting power-law ($1/r^\alpha$) interactions. For all power-law exponents $\alpha$ between $d$ and $2d+1$, where $d$ is the dimension of the system, the protocol yields a polynomial speedup for $\alpha>2d$ and a superpolynomial speedup for $\alpha\leq 2d$, compared to the state of the art. For all $\alpha>d$, the protocol saturates the Lieb-Robinson bounds (up to subpolynomial corrections), thereby establishing the optimality of the protocol and the tightness of the bounds in this regime. The protocol has a wide range of applications, including in quantum sensing, quantum computing, and preparation of topologically ordered states. In addition, the protocol provides a lower bound on the gate count in digital simulations of power-law interacting systems. ","Optimal State Transfer and Entanglement Generation in Power-law
  Interacting Systems"
200,1314010985297047553,1251964945899683841,Faisal Ladhak,"['Our EMNLP Findings paper presenting WikiLingua ‚Äî a new multilingual abstractive summarization dataset ‚Äî is now on arXiv. \n\nIt contains 770K article/summary pairs in 17 languages, parallel with English. \n\nPaper: <LINK>\n\nDataset: <LINK>\n\n#NLProc', 'This was joint work with @esindurmusnlp, @clairecardie and Kathy McKeown. We hope this will encourage further research in cross-lingual summarization, as well as summarization in languages other than English.']",https://arxiv.org/abs/2010.03093,"We introduce WikiLingua, a large-scale, multilingual dataset for the evaluation of crosslingual abstractive summarization systems. We extract article and summary pairs in 18 languages from WikiHow, a high quality, collaborative resource of how-to guides on a diverse set of topics written by human authors. We create gold-standard article-summary alignments across languages by aligning the images that are used to describe each how-to step in an article. As a set of baselines for further studies, we evaluate the performance of existing cross-lingual abstractive summarization methods on our dataset. We further propose a method for direct crosslingual summarization (i.e., without requiring translation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference. ","WikiLingua: A New Benchmark Dataset for Cross-Lingual Abstractive
  Summarization"
201,1314007378048712705,2349760833,Simon Ramstedt,"['Reinforcement Learning with Random Delays <LINK> - Check out our new paper with Yann Bouteiller, @jumpjoe78, @chrisjpal and @jjbinas! <LINK>', 'In delayed environments the effect of actions on observations and rewards is delayed. This makes off-policy credit assignment more difficult.\n\nWe can mitigate that with n-step off-policy value backups. With a little trick, these work without importance correction if n &lt; delays! https://t.co/e0pomfrdl8']",http://arxiv.org/abs/2010.02966,"Action and observation delays commonly occur in many Reinforcement Learning applications, such as remote control scenarios. We study the anatomy of randomly delayed environments, and show that partially resampling trajectory fragments in hindsight allows for off-policy multi-step value estimation. We apply this principle to derive Delay-Correcting Actor-Critic (DCAC), an algorithm based on Soft Actor-Critic with significantly better performance in environments with delays. This is shown theoretically and also demonstrated practically on a delay-augmented version of the MuJoCo continuous control benchmark. ",Reinforcement Learning with Random Delays
202,1313907877225156611,830355049,Mohit Bansal,"['Check out @swarnaNLP\'s new #emnlp2020 paper (w.  \n@sayandgp, @shsriva) on ""PRover"" for proof-graph generation+QA, w. zero-shot &amp; depth-generalization &amp; data-efficiency gains --&gt; <LINK> üôÇ\n\nAll code/models available: <LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2010.02830,"Recent work by Clark et al. (2020) shows that transformers can act as 'soft theorem provers' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PROVER generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PROVER obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for 'depth 5', indicating significant scope for future work. Our code and models are publicly available at this https URL ",PRover: Proof Generation for Interpretable Reasoning over Rules
203,1313886452384772096,835680883,Jason Wright,"[""Awesome new paper up by @jasonleecurtis_ et al. on gyrochronology:\n\n<LINK>\n\nIt's a monster effort by Jason to explore spin rates at 3 Gyr across a wide range of ages using TESS+clusters.\n\nThere's so much in here it's hard to know where to start!"", 'The headline is that stars\' spindown rates ""stall"" for a while (they stop spinning down) before picking up again. The empirical evidence is undeniable, the physical motivation is based on core/envelope coupling.\n\nThe hard part is that the effect is strongly mass-dependent!', ""For me, the proof of this pudding is in the tasting, and it tastes great:  they apply their model to the triple system 36 Oph and get consistent ages for all 3 components‚Ä¶the first time that's happened! https://t.co/zFoL9lfPKT"", 'I know @jasonleecurtis_ is careful not to claim he\'s somehow ""solved"" gyrochronology, but the model developde here is clearly accurate, parsimonious, and at least partially physically motivated, so I suspect it will end up becoming widely used.\n\nTake a read!']",https://arxiv.org/abs/2010.02272,"Recent measurements of rotation periods ($P_\text{rot}$) in the benchmark open clusters Praesepe (670 Myr), NGC 6811 (1 Gyr), and NGC 752 (1.4 Gyr) demonstrate that, after converging onto a tight sequence of slowly rotating stars in mass$-$period space, stars temporarily stop spinning down. These data also show that the duration of this epoch of stalled spin-down increases toward lower masses. To determine when stalled stars resume spinning down, we use data from the $K2$ mission and the Palomar Transient Factory to measure $P_\text{rot}$ for 58 dwarf members of the 2.7-Gyr-old cluster Ruprecht 147, 39 of which satisfy our criteria designed to remove short-period or near-equal-mass binaries. Combined with the $Kepler$ $P_\text{rot}$ data for the approximately coeval cluster NGC 6819 (30 stars with $M_\star > 0.85$ M$_\odot$), our new measurements more than double the number of $\approx$2.5 Gyr benchmark rotators and extend this sample down to $\approx$0.55 M$_\odot$. The slowly rotating sequence for this joint sample appears relatively flat (22 $\pm$ 2 days) compared to sequences for younger clusters. This sequence also intersects the $Kepler$ intermediate period gap, demonstrating that this gap was not created by a lull in star formation. We calculate the time at which stars resume spinning down, and find that 0.55 M$_\odot$ stars remain stalled for at least 1.3 Gyr. To accurately age-date low-mass stars in the field, gyrochronology formulae must be modified to account for this stalling timescale. Empirically tuning a core$-$envelope coupling model with open cluster data can account for most of the apparent stalling effect. However, alternative explanations, e.g., a temporary reduction in the magnetic braking torque, cannot yet be ruled out. ","When Do Stalled Stars Resume Spinning Down? Advancing Gyrochronology
  with Ruprecht 147"
204,1313875787523731456,196589843,Helga D√©nes,"['A new paper on how the Sgr B2 molecular cloud could have formed by cloud-cloud collision, combining molecular line observations and simulations. \nThis image is a distribution of SiO line widths across the cloud.\n<LINK>\n#science <LINK>']",https://arxiv.org/abs/2010.02757,"We present SiO J=2-1 maps of the Sgr B2 molecular cloud, which show shocked gas with a turbulent substructure comprising at least three cavities at velocities of [10,40] km s$^{-1}$ and an arc at velocities of [-20,10] km s$^{-1}$. The spatial anti-correlation of shocked gas at low and high velocities, and the presence of bridging features in position-velocity diagrams suggest that these structures formed in a cloud-cloud collision. Some of the known compact HII regions spatially overlap with sites of strong SiO emission at velocities of [40,85] km s$^{-1}$, and are between or along the edges of SiO gas features at [100,120] km s$^{-1}$, suggesting that the stars responsible for ionizing the compact HII regions formed in compressed gas due to this collision. We find gas densities and kinetic temperatures of the order of $n_{\rm H_2}\sim 10^5\rm cm^{-3}$ and $\sim$30 K, respectively, towards three positions of Sgr B2. The average values of the SiO relative abundances, integrated line intensities, and line widths are $\sim$10$^{-9}$, $\sim$11 K km s$^{-1}$, and $\sim$31 km s$^{-1}$, respectively. These values agree with those obtained with chemical models that mimic grain sputtering by C-type shocks. A comparison of our observations with hydrodynamical simulations shows that a cloud-cloud collision that took place $\lesssim$ 0.5 Myr ago can explain the density distribution with a mean column density of $\bar{N}_{\rm H_2}\gtrsim 5\times10^{22}$ cm$^{-2}$, and the morphology and kinematics of shocked gas in different velocity channels. Colliding clouds are efficient at producing internal shocks with velocities $\sim$5-50 km $s^{-1}$. High-velocity shocks are produced during the early stages of the collision and can readily ignite star formation, while moderate- and low-velocity shocks are important over longer timescales and can explain the widespread SiO emission in Sgr B2. ","Structure and kinematics of shocked gas in Sgr B2: further evidence of a
  cloud-cloud collision from SiO emission maps"
205,1313862979222474752,1403815458,Amir Siraj,"['In a new paper on the arXiv, we explore observable signatures of low- and high-ejection-speed interstellar object populations, which can be studied by @VRubinObs to reveal insights into planetary system formation and evolution <LINK>']",https://arxiv.org/abs/2010.02214,"`Oumuamua and Borisov were the first two interstellar objects confirmed in the Solar system. The upcoming commencement of the Vera C. Rubin Observatory's Legacy Survey of Space of Time (LSST) will enhance greatly the discovery rate of interstellar objects. This raises the question, what can be learned from large-number statistics of interstellar objects? Here, we show that discovery statistics provided by LSST will allow low- and high-ejection-speed populations to be distinguished using the velocity dispersion and angular anisotropy of interstellar objects. These findings can be combined with physical characterizations to yield a better understanding of planetary system origin and nature. ","Observable Signatures of the Ejection Speed of Interstellar Objects from
  their Birth Systems"
206,1313816344580808704,1141006043218108419,Clara Isabel Meister,"['Beam search is a hack -- we all know it. So, why does it work so damn well? It‚Äôs a SOTA algorithm for decoding neural text generators! Our new EMNLP paper presents a framing of beam search that demonstrates it has a cognitive inductive bias. <LINK> <LINK>', 'Formally, we cast beam search as the solution to a regularized decoding problem. Analysis of our ‚Äúbeam search regularizer‚Äù reveals a concrete link between beam search and the UID hypothesis from cognitive science.', 'We find that beam search encourages an even distribution of information across the generated text, which is cognitively plausible! In our machine translation experiments, we show that BLEU correlates with an operationalization of the UID hypothesis.', 'This gives us a simple explanation about why beam search works so well with small beam sizes: It enforces a posited cognitive bias from the linguistics literature --  to wit, the UID hypothesis.', 'We also develop a set of novel regularizers, inspired by further work on the UID hypothesis, and decode with them in the regularized decoding framework. Experimentally, we find that our novel regularizers behave in a similar manner to beam search with a small beam size.', 'We conclude the paper arguing that our experimental results give us a plausible, cognitive explanation for beam search‚Äôs success as a decoding heuristic for neural text generators, even when the algorithm is far from exact in practice. Joint work with @ryandcotterell and @xtimv.']",https://arxiv.org/abs/2010.02650,"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU. ","If beam search is the answer, what was the question?"
207,1313809232794456065,1236348926153895937,Ricardo Salinas,['We have a new paper out! This time with the one-of-a-kind @ClaraMarvaz ü•≥  <LINK>'],https://arxiv.org/abs/2010.02220,"Delta Scuti ($\delta$ Sct) stars have been extensively studied in our Galaxy, but far less in extragalactic systems. Here we study the population of $\delta$ Sct variables in NGC 419, an intermediate-age globular cluster of the Small Magellanic Cloud (SMC), using $g,r,i$ Gemini-S/GMOS time series observations. Our goal is to study the role of such variables in the cluster extended main-sequence turnoff (MSTO). We report the discovery of 54 $\delta$ Sct stars and three eclipsing binaries in the NGC 419 field. We find only a handful of the $\delta$ Sct stars at the MSTO of NGC 419 while the majority is fainter, indicating that the cluster is younger ($\lesssim 1.2$ Gyr) than previously thought. Considering their radial distribution, we identify only six $\delta$ Sct stars as probable members of NGC 419 while the 48 remaining are likely $\delta$ Sct stars of the SMC field. Cluster $\delta$ Sct stars appear close to the red edge of the MSTO, supporting the idea that the extended MSTO has its origin in an age spread. The 48 field $\delta$ Sct stars represent the largest detection of $\delta$ Sct stars made in the SMC. The period distribution of these newly detected $\delta$ Sct stars ($0.04 \lesssim P \lesssim 0.15$ d) is similar to that detected in other systems. The amplitude distribution ($0.05 \lesssim \Delta r \lesssim 0.60$ mag) is likely biased because of the lack of low-amplitude stars. We finally use the $\delta$ Sct stars to calculate distances using different period-luminosity relations. The average distance moduli obtained are $18.76\pm0.14$ mag for NGC 419 and $18.86\pm0.11$ mag for the SMC field, which agree with previous measurements. ","Short period variability in the globular cluster NGC 419 and the SMC
  field"
208,1313760473125445632,2603024598,Ricardo P√©rez-Marco,"['New paper with @CGrunspan! We define the notion of ""profit lag"" and compute the profitability of the new ""alternate mining strategy"" that exploits forks with the same PoW.#Bitcoin\n\n(We also correct gross errors in a paper by @PeterRizun and @el33th4xor)\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.02671,"For a mining strategy we define the notion of ""profit lag"" as the minimum time it takes to be profitable after that moment. We compute closed forms for the profit lag and the revenue ratio for the strategies ""selfish mining"" and ""intermittent selfish mining"". This confirms some earlier numerical simulations and clarifies misunderstandings on profitability in the literature. We also study mining pairs of PoW cryptocurrencies, often coming from a fork, with the same mining algorithm. This represents a vector of attack that can be exploited using the ""alternate network mining"" strategy that we define. We compute closed forms for the profit lag and the revenue ratiofor this strategy that is more profitable than selfish mining and intermittent selfish mining. It is also harder to counter since it does not rely on a flaw in the difficulty adjustment formula that is the reason for profitability of the other strategies. ",Profit lag and alternate network mining
209,1313754611770167296,3832040415,Ran Zmigrod,"['‚ö†Ô∏è Attention all NLPers, when decoding dependency trees, please mind the root! ‚ö†Ô∏è\nCheck out our new @emnlp2020 short paper about efficient root-constrained decoding for graph-based dependency parsers!\nüå≤ <LINK>\nJoint work with @xtimv and @ryandcotterell <LINK>', 'Edge-factored non-projective dependency parsing decoding is done using MST algorithms. However, most dependency tree annotation standards do not directly translate to spanning trees!', 'A subtle root-constraint is often required for dependency trees, only one edge may emanate from the root! The current NLP solution to this is to add a factor of n to the runtime.', 'We introduce the NLP community to an algorithm from the 80s that correctly decodes root-constrained dependency trees without sacrificing runtime.', 'üå¥ Our code is available at https://t.co/6G7CMgeDcm']",https://arxiv.org/abs/2010.02550,"The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree. We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases. In fact, the worst constraint-violation rate we observe is 24%. Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime. ",Please Mind the Root: Decoding Arborescences for Dependency Parsing
210,1313738757565104128,2224437944,Hao Li,['Congratulations to @jiaman01 et al. for their new @SIGGRAPHAsia 2020 paper on Dynamic Facial Asset and Rig Generation from a Single Scan. We can now generate a fully rigged 3D face model with dynamic and physically-based textures from a single input scan: <LINK>'],https://arxiv.org/abs/2010.00560?fbclid=IwAR2nqK0P_ZjQ39y90ynH8tE_GTCT8N9W92vsmYdeAYfbQNvdGti2cv-Zkzs,"The creation of high-fidelity computer-generated (CG) characters used in film and gaming requires intensive manual labor and a comprehensive set of facial assets to be captured with complex hardware, resulting in high cost and long production cycles. In order to simplify and accelerate this digitization process, we propose a framework for the automatic generation of high-quality dynamic facial assets, including rigs which can be readily deployed for artists to polish. Our framework takes a single scan as input to generate a set of personalized blendshapes, dynamic and physically-based textures, as well as secondary facial components (e.g., teeth and eyeballs). Built upon a facial database consisting of pore-level details, with over $4,000$ scans of varying expressions and identities, we adopt a self-supervised neural network to learn personalized blendshapes from a set of template expressions. We also model the joint distribution between identities and expressions, enabling the inference of the full set of personalized blendshapes with dynamic appearances from a single neutral input scan. Our generated personalized face rig assets are seamlessly compatible with cutting-edge industry pipelines for facial animation and rendering. We demonstrate that our framework is robust and effective by inferring on a wide range of novel subjects, and illustrate compelling rendering results while animating faces with generated customized physically-based dynamic textures. ",Dynamic Facial Asset and Rig Generation from a Single Scan
211,1313728300213104640,1316932982,Kohei Suenaga,"['Preprint of our new paper: Yuhki Hatakeyama, Hiroki Sakuma, Yoshinori Konishi, Kohei Suenaga: Visualizing Color-wise Saliency of Black-Box Image Classification Models. \n<LINK>']",https://arxiv.org/abs/2010.02468,"Image classification based on machine learning is being commonly used. However, a classification result given by an advanced method, including deep learning, is often hard to interpret. This problem of interpretability is one of the major obstacles in deploying a trained model in safety-critical systems. Several techniques have been proposed to address this problem; one of which is RISE, which explains a classification result by a heatmap, called a saliency map, which explains the significance of each pixel. We propose MC-RISE (Multi-Color RISE), which is an enhancement of RISE to take color information into account in an explanation. Our method not only shows the saliency of each pixel in a given image as the original RISE does, but the significance of color components of each pixel; a saliency map with color information is useful especially in the domain where the color information matters (e.g., traffic-sign recognition). We implemented MC-RISE and evaluate them using two datasets (GTSRB and ImageNet) to demonstrate the effectiveness of our methods in comparison with existing techniques for interpreting image classification results. ",Visualizing Color-wise Saliency of Black-Box Image Classification Models
212,1313723531587452929,3310311821,Rahul Savani,['Our new paper gives a faster algorithm for finding Tarski fixed points <LINK>'],https://arxiv.org/abs/2010.02618,"Dang et al. have given an algorithm that can find a Tarski fixed point in a $k$-dimensional lattice of width $n$ using $O(\log^{k} n)$ queries. Multiple authors have conjectured that this algorithm is optimal [Dang et al., Etessami et al.], and indeed this has been proven for two-dimensional instances [Etessami et al.]. We show that these conjectures are false in dimension three or higher by giving an $O(\log^2 n)$ query algorithm for the three-dimensional Tarski problem. We also give a new decomposition theorem for $k$-dimensional Tarski problems which, in combination with our new algorithm for three dimensions, gives an $O(\log^{2 \lceil k/3 \rceil} n)$ query algorithm for the $k$-dimensional problem. ",A faster algorithm for finding Tarski fixed points
213,1313650656993923072,972932719503081472,Akira Sone,"['Check our new paper ""A Generalized Measure of Quantum Fisher Information"": <LINK>\nA great collaboration with Marco (@MvsCerezo) Jacob (@JacobBeckey) and Patrick (@ColesQuantum).  We present an efficiently computable lower bound on the QFI!', '@endo_suguru @MvsCerezo @JacobBeckey @ColesQuantum „ÅÇ„Çä„Åå„Å®„Éº']",https://arxiv.org/abs/2010.02904,"In this work, we present a lower bound on the quantum Fisher information (QFI) which is efficiently computable on near-term quantum devices. This bound itself is of interest, as we show that it satisfies the canonical criteria of a QFI measure. Specifically, it is essentially a QFI measure for subnormalized states, and hence it generalizes the standard QFI in this sense. Our bound employs the generalized fidelity applied to a truncated state, which is constructed via the $m$ largest eigenvalues and their corresponding eigenvectors of the probe quantum state $\rho_{\theta}$. Focusing on unitary families of exact states, we analyze the properties of our proposed lower bound, and demonstrate its utility for efficiently estimating the QFI. ",Generalized Measure of Quantum Fisher Information
214,1313583508430954496,1541749356,Matt Landreman,"['Paper just out: a new way to optimize stellarators, direct optimization of coils for quasisymmetry. <LINK> With Andrew Giuliani, Florian Wechsung, Antoine Cerfon, &amp; Georg Stadler @nyuniversity @UofMaryland @SimonsFdn <LINK>']",https://arxiv.org/abs/2010.02033,"We present a new coil design paradigm for magnetic confinement in stellarators. Our approach directly optimizes coil shapes and coil currents to produce a vacuum quasi-symmetric magnetic field with a target rotational transform on the magnetic axis. This approach differs from the traditional two-stage approach in which first a magnetic configuration with desirable physics properties is found, and then coils to approximately realize this magnetic configuration are designed. The proposed single-stage approach allows us to find a compromise between confinement and engineering requirements, i.e., find easy-to-build coils with good confinement properties. Using forward and adjoint sensitivities, we derive derivatives of the physical quantities in the objective, which is constrained by a nonlinear periodic differential equation. In two numerical examples, we compare different gradient-based descent algorithms and find that incorporating approximate second-order derivative information through a quasi-Newton method is crucial for convergence. We also explore the optimization landscape in the neighborhood of a minimizer and find many directions in which the objective is mostly flat, indicating ample freedom to find simple and thus easy-to-build coils. ","Single-stage gradient-based stellarator coil design: Optimization for
  near-axis quasi-symmetry"
215,1313542645348593664,2235411914,Surya Ganguli,"['Our new paper on a theory of self-supervised learning with dual pairs of deep networks, provides insights into how methods like SimCLR and BYOL extract hierarchical features from data: <LINK>  Also my first collab w/ @facebookai &amp; @tydsh @yulantao1996 Xinlei Chen <LINK>']",https://arxiv.org/abs/2010.00578,"We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a \emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a \emph{hierarchical latent tree model} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives \emph{no direct supervision} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in this https URL ",Understanding Self-supervised Learning with Dual Deep Networks
216,1313502257154060294,1212714725571612672,David Holzm√ºller,"['1. In a new paper (<LINK>), I prove a distribution-independent lower bound on the label-noise-induced generalization error in ridgeless linear regression with (random) features under weak assumptions. <LINK>', '2. The main assumption is that the sample-feature-matrix has full rank with probability one. I prove this for several analytic feature maps, especially for random deep neural networks with analytic activation functions, but also for random Fourier features and polynomial kernels. https://t.co/mJXlRxRk9L', '3. The paper also contains more experiments and a result for the uniform distribution on the unit sphere, which seems to provide the lowest sensitivity to label noise among all distributions satisfying the assumptions. https://t.co/SIgm7BSFah', ""4. The proof of the lower bound is less than two pages long and is mainly based on Jensen's inequality, the Schur complement and other matrix algebra. https://t.co/H3XWd2cto7"", '5. The proof of the main assumption for random neural networks is mainly based on the identity theorem for analytic functions. This strategy also provides a way to reliably computationally verify the main assumption for analytic finite-width Neural Tangent Kernel feature maps. https://t.co/qW40HOVnVF', '6. I wrote a blog post (https://t.co/TejtYuJTw3) with some additional discussion. In the blog post, I also show how to prove the theorem below using combinatorics! https://t.co/KnN2tuyA4Y', '7. Thanks to my supervisor, Ingo Steinwart, for helpful comments and to @SimTechStuttga2 @Uni_Stuttgart for funding my research.']",https://arxiv.org/abs/2010.01851,"We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functions like sigmoid, tanh, softplus or GELU. As further examples, we show that feature maps from random Fourier features and polynomial kernels also satisfy our assumptions. We complement our theory with further experimental and analytic results. ",On the Universality of the Double Descent Peak in Ridgeless Regression
217,1313483090891862016,1244087220,Andrew Beam,"['Are we making meaningful progress on machine learning for EHR data?\n\nNew preprint with @DavidRBellamy  and Leo Celi tries to answer this question through the lens of benchmarks and the answer is, unfortunately,  ""probably not""\n\nPaper: <LINK>\n\nSome highlights üëá <LINK>', '2/ The main problem with measuring progress is lack of standardization, so we looked for studies using common ""benchmark tasks""\n\nBased on our definition, we found only one benchmark that has received significant attention (&gt;200 citations, ~20 papers reporting results) https://t.co/xHdbVV9LQH', '3/ The benchmark has 4 sub prediction tasks: mortality, LOS, phenotype, and decompensation. \n\nWe found there has been no significant progress over time (trend line for each task ~0, p-val non-significant) since the benchmark was introduced ~3 years ago https://t.co/nDwVpQy9yE', '4/ Many of these papers use common baselines (e.g. logistic regression, LSTM, etc), so we did a meta-analysis to see if any of these model classes are better on average\n\nCompared to LR, the neural models were only better on phenotyping and decomp, and only then by a small margin https://t.co/SUTQUMBlgK', '5/ So what does all this mean? Benchmark tasks are our best bet for community engagement and progress, but not alI prediction tasks are created equal. In medicine, some outcomes like mortality actually involve quite a bit of discretion about when to withdraw care.', '6/ This means we should think carefully about what tasks are the right ones for benchmarks, because simple models might already be close to Bayes error rates for things like mortality. \n\nThe label has to be ""hard enough"" but not ""too hard"" for it to be a good benchmark task.', '7/ In summary, if we want to have an ""Imagenet moment"" in medicine, we have to have an equivalent benchmark, and currently it seems pretty clear that we do not.\n\nWe hope the analysis and discussion in this paper will help clarify considerations for future benchmark tasks. https://t.co/uciRb2Xaxx', ""@erikrtn Agree with all of the above! That's why it probably doesn't make a good benchmark task if you're trying to catalyze and measure model development progress!""]",https://arxiv.org/abs/2010.01149,"The Large Scale Visual Recognition Challenge based on the well-known Imagenet dataset catalyzed an intense flurry of progress in computer vision. Benchmark tasks have propelled other sub-fields of machine learning forward at an equally impressive pace, but in healthcare it has primarily been image processing tasks, such as in dermatology and radiology, that have experienced similar benchmark-driven progress. In the present study, we performed a comprehensive review of benchmarks in medical machine learning for structured data, identifying one based on the Medical Information Mart for Intensive Care (MIMIC-III) that allows the first direct comparison of predictive performance and thus the evaluation of progress on four clinical prediction tasks: mortality, length of stay, phenotyping, and patient decompensation. We find that little meaningful progress has been made over a 3 year period on these tasks, despite significant community engagement. Through our meta-analysis, we find that the performance of deep recurrent models is only superior to logistic regression on certain tasks. We conclude with a synthesis of these results, possible explanations, and a list of desirable qualities for future benchmarks in medical machine learning. ","Evaluating Progress on Machine Learning for Longitudinal Electronic
  Healthcare Data"
218,1313468600393572353,951503496867655680,Sanjeevan Ahilan,"['Excited to share our new paper on Correcting Experience Replay for Multi-Agent Communication. We use what Peter Dayan calls the ""Orwellian"" idea of using present information to correct the past, to improve future learning. <LINK>', 'https://t.co/iMps43gjdj', 'Multi-agent learning is highly non-stationary. We replace the messages agents did send with the ones they *would* send according to their current policies. Our method is easy to use and enables better communication policies to be learned. https://t.co/N6RamsSE2a']",https://arxiv.org/abs/2010.01192,"We consider the problem of learning to communicate using multi-agent reinforcement learning (MARL). A common approach is to learn off-policy, using data sampled from a replay buffer. However, messages received in the past may not accurately reflect the current communication policy of each agent, and this complicates learning. We therefore introduce a 'communication correction' which accounts for the non-stationarity of observed communication induced by multi-agent learning. It works by relabelling the received message to make it likely under the communicator's current policy, and thus be a better reflection of the receiver's current environment. To account for cases in which agents are both senders and receivers, we introduce an ordered relabelling scheme. Our correction is computationally efficient and can be integrated with a range of off-policy algorithms. We find in our experiments that it substantially improves the ability of communicating MARL systems to learn across a variety of cooperative and competitive tasks. ",Correcting Experience Replay for Multi-Agent Communication
219,1313430919802482690,974227479291420672,Francesco Lovascio,"['Check out our new paper on the streaming instability! <LINK>', 'Dust distributions affect the streaming instability changing growth rates and character of the instability. Keep your eyes peeled for more to come.']",https://arxiv.org/abs/2010.01145,"We introduce a polydisperse version of the streaming instability, where the dust component is treated as a continuum of sizes. We show that its behaviour is remarkably different from the monodisperse streaming instability. We focus on tightly coupled particles in the terminal velocity approximation and show that unstable modes that grow exponentially on a dynamical time scale exist. However, for dust to gas ratios much smaller than unity they are confined to radial wave numbers that are a factor $\sim 1/\overline{\rm St}$ larger than where the monodisperse streaming instability growth rates peak. Here $\overline{\rm St} \ll 1$ is a suitable average Stokes number for the dust size distribution. For dust to gas ratios larger than unity, polydisperse modes that grow on a dynamical time scale are found as well, similar as for the monodisperse streaming instability and at similarly large wave numbers. At smaller wave numbers, where the classical monodisperse streaming instability shows secular growth, no growing polydisperse modes are found under the terminal velocity approximation. Outside the region of validity for the terminal velocity approximation, we have found unstable epicyclic modes that grow on $\sim 10^4$ dynamical time scales. ","Polydisperse Streaming Instability I. Tightly coupled particles and the
  terminal velocity approximation"
220,1313404634350989312,87818714,Tiago Pimentel,"['Should our probes be simple or accurate? @nsaphra and I reframe this trade off with Pareto fronts in our new #emnlp2020 paper with @adinamwilliams and @ryandcotterell \n<LINK>\n\nFollow the thread :) <LINK>', 'Most interpretability work assumes probes should be simple (made explicit by @johnhewtt here). Often people limit them to linear models!\n\nhttps://t.co/pOrIx1dF9q', 'At #acl2020nlp, we argued against simplicity from an information-theoretical perspective.\nProbes should be as complex as necessary to extract information!\n\nhttps://t.co/MXNZw9YHs2', 'But now we say: both approaches are wrong!\n\nOptimizing for accuracy tells us nothing about the representations. Optimizing for simplicity actually rates one-hot or random representations as better than BERT! https://t.co/kZFqFbeDgp', 'Instead, we propose looking for probes that are:\n(„Å£‚óî‚ó°‚óî)„Å£ ‚ô• Pareto optimal ‚ô•\n\nThis means there is no competing probe for which accuracy is higher *and* complexity is lower on the task.', 'We try out four measures of complexity\n- nuclear norm\n- matrix rank\n- random label memorization\n- memorization with shuffled inputs (HARD MODE)\n\nWe evaluate representations in part-of-speech labeling (POSL) and dependency arc labeling (DAL).', 'We find:\n(1) one-hot representations maximize the accuracy of low complexity probes\n(2) linear probes give as good results as more complex ones\n(3) these tasks are too simple to highlight the contextual information about syntax encoded in the representations. https://t.co/Nuz2V8zUVb', 'Since these tasks are too simple, we try a more complex task: dependency parsing.\n\nProbing this task gives us embedding rankings that are a little more recognizable! https://t.co/HmogR3VUUJ', 'Based on our final experiments, we can conclude:\n(1) contextual representations encode much more knowledge about syntax than do non-contextual ones (duh!)\n(2) we need harder tasks for probes to actually be informative', 'As a special treat, we finish up with a ramble about the properties of ""model complexity"" definitions.\nAre we talking about the complexity of a model family or a learned model function? About the capacity or other aspects of inductive bias? Choose carefully!', ""@nsaphra @tttthomasssss @adinamwilliams @ryandcotterell The ones who got the joke immediately get to be aliens. The ones who didn't do not üôÉ"", ""@ryandcotterell @tttthomasssss @nsaphra @adinamwilliams I'm guessing that @xtimv gets the coconut tree because he is Brazilian üòÑ"", '@ryandcotterell @tttthomasssss @nsaphra @adinamwilliams @xtimv Oh, I get it now! That\'s a palm tree and not a coconut one üå¥\nThe ""not as good"" version of tropical weather üòÑ', ""@ryandcotterell @nsaphra @tttthomasssss @adinamwilliams @xtimv Yeah, that's fair :)\nOn our side though @nsaphra understood it immediately and claimed the üëΩ -&gt; I didn't"", ""@ryandcotterell @nsaphra @tttthomasssss @adinamwilliams @xtimv Yeah, I REALLY didn't get the joke until it was too late üòÇ""]",https://arxiv.org/abs/2010.02180,"The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the NLP literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations---e.g., why should the non-contextual fastText representations encode more morpho-syntactic information than the contextual BERT representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations. ",Pareto Probing: Trading Off Accuracy for Complexity
221,1313381016309039105,1051867025319047169,Tycho van der Ouderaa,"['New paper. Happy to share that our paper extending DL-based image registration to group-wise registration was accepted for the Thoracic Image Analysis workshop at @MICCAI2020.\n\narXiv: <LINK>\nw/ @ivanaisgum, Wouter B. Veldhuis and @BobdeVos\n\n@Quantib #MICCAI2020 <LINK>']",https://arxiv.org/abs/2010.00231,"Deep neural networks are increasingly used for pair-wise image registration. We propose to extend current learning-based image registration to allow simultaneous registration of multiple images. To achieve this, we build upon the pair-wise variational and diffeomorphic VoxelMorph approach and present a general mathematical framework that enables both registration of multiple images to their geodesic average and registration in which any of the available images can be used as a fixed image. In addition, we provide a likelihood based on normalized mutual information, a well-known image similarity metric in registration, between multiple images, and a prior that allows for explicit control over the viscous fluid energy to effectively regularize deformations. We trained and evaluated our approach using intra-patient registration of breast MRI and Thoracic 4DCT exams acquired over multiple time points. Comparison with Elastix and VoxelMorph demonstrates competitive quantitative performance of the proposed method in terms of image similarity and reference landmark distances at significantly faster registration. ",Deep Group-wise Variational Diffeomorphic Image Registration
222,1313335339159883777,10666172,Sabine Hossenfelder,"['New paper! <LINK> <LINK>', 'And indeed a second new paper:\n\nhttps://t.co/3P8PbxV5tU']",https://arxiv.org/abs/2010.01327,"A local, deterministic toy model for quantum mechanics is introduced and discussed. It is demonstrated that, when averaged over the hidden variables, the model produces the same predictions as quantum mechanics. In the model considered here, the dynamics depends only on the settings of the measurement device at the detection time, not how those settings were chosen. As a consequence, the model is locally causal but violates statistical independence. We show that it is neither fine-tuned nor allows for superluminal signalling. ",A Toy Model for Local and Deterministic Wave-function Collapse
223,1313301170182451200,304399622,Nobuyuki Yoshioka,['Our new paper is out! Neural-network ab initio simulation for solid systems in all spatial dimensions reaches chemical accuracy for ground states and quasiparticle band structures. Fun collaboration with @jlo8y0 and F. Nori @RIKEN_JP !\n<LINK>'],https://arxiv.org/abs/2010.01358,"Establishing a predictive ab initio method for solid systems is one of the fundamental goals in condensed matter physics and computational materials science. The central challenge is how to encode a highly-complex quantum-many-body wave function compactly. Here, we demonstrate that artificial neural networks, known for their overwhelming expressibility in the context of machine learning, are excellent tool for first-principles calculations of extended periodic materials. We show that the ground-state energies in real solids in one-, two-, and three-dimensional systems are simulated precisely, reaching their chemical accuracy. The highlight of our work is that the quasiparticle band spectra, which are both essential and peculiar to solid-state systems, can be efficiently extracted with a computational technique designed to exploit the low-lying energy structure from neural networks. This work opens up a path to elucidate the intriguing and complex many-body phenomena in solid-state systems. ","Solving Quasiparticle Band Spectra of Real Solids using Neural-Network
  Quantum States"
224,1313297946251677696,22399655,Ryota Kanaiüí°,"['New paper on NTIC with Martin Biehl (@36zimmer ) to be presented at SSCI 2020. \n\nNon-trivial informational closure of a Bayesian hyperparameter.\n<LINK>', ""The motivation is similar to Friston's Markov blanket, but here we are aiming to establish the relationship between NTIC (instead of MBs) and modeling of the environment by the internal state."", 'This paper shows that modeling implies increase in NTIC. But we are also interested in the reverse direction, i.e., whether NTIC implies modeling.', 'Since we have doubt about the relationship between Markov blankets and modeling, NTIC seems to be a good candidate for building similar arguments about life/agent and cognition.']",https://arxiv.org/abs/2010.01855,"We investigate the non-trivial informational closure (NTIC) of a Bayesian hyperparameter inferring the underlying distribution of an identically and independently distributed finite random variable. For this we embed both the Bayesian hyper-parameter updating process and the random data process into a Markov chain. The original publication by Bertschinger et al. (2006) mentioned that NTIC may be able to capture an abstract notion of modeling that is agnostic to the specific internal structure of and existence of explicit representations within the modeling process. The Bayesian hyperparameter is of interest since it has a well defined interpretation as a model of the data process and at the same time its dynamics can be specified without reference to this interpretation. On the one hand we show explicitly that the NTIC of the hyperparameter increases indefinitely over time. On the other hand we attempt to establish a connection between a quantity that is a feature of the interpretation of the hyperparameter as a model, namely the information gain, and the one-step pointwise NTIC which is a quantity that does not depend on this interpretation. We find that in general we cannot use the one-step pointwise NTIC as an indicator for information gain. We hope this exploratory work can lead to further rigorous studies of the relation between NTIC and modeling. ",Non-trivial informational closure of a Bayesian hyperparameter
225,1313295288203800576,90047221,Robin Kothari,"['New paper on the arXiv with Ankit Garg (@Ankit_garg06), Praneeth Netrapalli (@PNetrapalli), and Suhail Sherif (@shlshrf): ""No quantum speedup over gradient descent\nfor non-smooth convex optimization.""\n<LINK> <LINK>', 'Paper summary: Gradient descent can approximately minimize a Lipschitz function f using black-box queries to f and its (sub)gradient, and it does so with optimal dimension-independent query complexity. We show that even quantum algorithms cannot do better in this setting.']",https://arxiv.org/abs/2010.01801,"We study the first-order convex optimization problem, where we have black-box access to a (not necessarily smooth) function $f:\mathbb{R}^n \to \mathbb{R}$ and its (sub)gradient. Our goal is to find an $\epsilon$-approximate minimum of $f$ starting from a point that is distance at most $R$ from the true minimum. If $f$ is $G$-Lipschitz, then the classic gradient descent algorithm solves this problem with $O((GR/\epsilon)^{2})$ queries. Importantly, the number of queries is independent of the dimension $n$ and gradient descent is optimal in this regard: No deterministic or randomized algorithm can achieve better complexity that is still independent of the dimension $n$. In this paper we reprove the randomized lower bound of $\Omega((GR/\epsilon)^{2})$ using a simpler argument than previous lower bounds. We then show that although the function family used in the lower bound is hard for randomized algorithms, it can be solved using $O(GR/\epsilon)$ quantum queries. We then show an improved lower bound against quantum algorithms using a different set of instances and establish our main result that in general even quantum algorithms need $\Omega((GR/\epsilon)^2)$ queries to solve the problem. Hence there is no quantum speedup over gradient descent for black-box first-order convex optimization without further assumptions on the function family. ","No quantum speedup over gradient descent for non-smooth convex
  optimization"
226,1313283901851344896,4902145390,Gordan Krnjaic,"['New paper out tonight with @DanHooperAstro. We study how light primordial black holes (PBH) affect GUT baryogenesis. Hawking evaporation can produce very heavy particles even if the SM temperature is low, which changes the predictions for the baryon yield\n\n<LINK>', '@bvlehmann @jazzwhiz @DanHooperAstro Ben got it exactly right. The surrounding radiation temperature can be low, but as any BH evaporates, its personal temperature (distinct from the bath) increases']",https://arxiv.org/abs/2010.01134,"In models of baryogenesis based on Grand Unified Theories (GUTs), the baryon asymmetry of the universe is generated through the CP and baryon number violating, out-of-equilibrium decays of very massive gauge or Higgs bosons in the very early universe. Recent constraints on the scale of inflation and the subsequent temperature of reheating, however, have put pressure on many such models. In this paper, we consider the role that primordial black holes may have played in the process of GUT baryogenesis. Through Hawking evaporation, black holes can efficiently generate GUT Higgs or gauge bosons, regardless of the masses of these particles or the temperature of the early universe. Furthermore, in significant regions of parameter space, the black holes evaporate after the electroweak phase transition, naturally evading the problem of sphaleron washout that is normally encountered in GUT models based on $SU(5)$. We identify a wide range of scenarios in which black holes could facilitate the generation of the baryon asymmetry through the production and decays of GUT bosons. ",GUT Baryogenesis With Primordial Black Holes
227,1313140914265755648,1115817574078582785,Prithviraj Ammanabrolu,"['üö®New Paper Alertüö®\nHaving trouble keeping your (AI) dragon motivated? Same here. So we figured out how to teach it, interactively w/ RL &amp; lang pretraining, to act consistently + talk naturally wrt its motivations when questing in a fantasy text game.\n\n<LINK>\n1/4 <LINK>', 'We release datasets: of natural language character centric quests + motivations in LIGHT, a large scale crowdsourced fantasy text game; human demos of ppl playing them; a companion commonsense reasoning knowledge graph for fantasy worlds.\n\nData: https://t.co/QDlvgj9Jp8\n2/4', 'We mix big transformer pertaining and RL to train agents to act and talk in pursuit of their motivations. Turns out the mixing is a lot trickier with RL then supervised but if you do it right you find that interactive RL &gt;&gt;&gt;&gt; static supervised training for language learning. 3/4', 'This is the result of my time at @facebookai with the LIGHT and @parlai_parley teams. Thanks so much to my hosts and co authors: @JackUrbs @margs_li Arthur Szlam @_rockt @jaseweston !!! 4/4', '@arnicas The life cycle of an arxiv paper requires that they tweet it first']",https://arxiv.org/abs/2010.00685,"We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text-game -- with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations. ","How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and
  Act in Fantasy Worlds"
228,1313125036405002240,18593948,Duncan Brown,['New paper from my grad student Chaitanya Afle on measuring the properties of core collapse supernovae with @LIGO and Cosmic Explorer <LINK>'],https://arxiv.org/abs/2010.00719,"Galactic core-collapse supernovae are among the possible sources of gravitational waves. We investigate the ability of gravitational-wave observatories to extract the properties of the collapsing progenitor from the gravitational waves radiated. We use simulations of supernovae that explore a variety of progenitor core rotation rates and nuclear equations of state and examine the ability of current and future observatories to determine these properties using gravitational-wave parameter estimation. We use principal component analysis of the simulation catalog to determine the dominant features of the waveforms and create a map between the measured properties of the waveform and the physical properties of the progenitor. We use Bayesian parameter inference and the parameter map to calculate posterior probabilities for the physical properties given a gravitational-wave observation. We estimate the ratio of the progenitor's core rotational kinetic energy to potential energy ($\beta$) and the post bounce oscillation frequency. For a supernovae at the distance of the galactic center (8.1 kpc) with $\beta = 0.02$ our method can estimate $\beta$ with a $90\%$ credible interval of $0.004$ for Advanced LIGO, improving to $0.0008$ for Cosmic Explorer. We demonstrate that if the core is rotating sufficiently rapidly for a signal observed by Cosmic Explorer, our method can also extract the post bounce oscillation frequency of the protoneutron star to a precision of within $5$~Hz ($90\%$ credible interval) allowing us to constrain the nuclear equation of state. For a supernovae at the distance of the Magellanic Clouds (48.5 kpc) Cosmic Explorer's ability to measure these parameters decreases slightly to $0.003$ for rotation and $11$~Hz for the postbounce oscillation frequency ($90\%$ credible interval). Sources in Magellanic Clouds will be too distant for Advanced LIGO to measure these properties. ","Inferring physical properties of stellar collapse by third-generation
  gravitational-wave detectors"
229,1313089959432196096,2902687319,Mike Hudson,"['1/ New paper, led @SuprantaSB, in which we show that peculiar velocity predictions from the 2M++ density field have smaller uncertainities than other methods such as interpolating measured peculiar velocity data, because the latter are sparse and noisy. <LINK>', ""2/ We also make some improvements in using these data to measure H_0, finding a lower value (69 +/- 3) from megamasers than previously reported. I'm sure @DScol will be interested."", '3/ Finally, we apply these peculiar velocity predictions to other galaxies such as the low-dark matter galaxy NGC 1052-DF2, which will be of interest to @DokkumPieter and @roberto_abraham and others.']",https://arxiv.org/abs/2010.01119,"When measuring the value of the Hubble parameter, $H_0$, it is necessary to know the recession velocity free of the effects of peculiar velocities. In this work, we study different models of peculiar velocity in the local Universe. In particular, we compare models based on density reconstruction from galaxy redshift surveys and kernel smoothing of peculiar velocity data. The velocity field from the density reconstruction is obtained using the 2M++ galaxy redshift compilation, which is compared to two adaptive kernel-smoothed velocity fields: the first obtained from the 6dF Fundamental Plane sample and the other using a Tully-Fisher catalogue obtained by combining SFI++ and 2MTF. We highlight that smoothed velocity fields should be rescaled to obtain unbiased velocity estimates. Comparing the predictions of these models to the observations from a few test sets of peculiar velocity data, obtained from the Second Amendment Supernovae catalogue and the Tully-Fisher catalogues, we find that 2M++ reconstruction provides a better model of the peculiar velocity in the local Universe than the kernel-smoothed peculiar velocity models. We study the impact of peculiar velocities on the measurement of $H_0$ from gravitational waves and megamasers. In doing so, we introduce a probabilistic framework to marginalize over the peculiar velocity corrections along the line-of-sight. For the megamasers, we find $H_0 = 69^{+2.9}_{-2.8}$ km s^{-1} Mpc^{-1} using the 2M++ velocity field. We also study the peculiar velocity of the the galaxy NGC1052-DF2, concluding that a short $\sim$ 13 Mpc distance is not a likely explanation of the anomalously low dark matter fraction of that galaxy. ","Peculiar velocities in the local Universe: comparison of different
  models and the implications for $H_0$ and dark matter"
230,1313063661120888832,13453472,Matt Smith,"['We\'ve got a new paper on Arxiv - ""Understanding Realistic Attacks on Airborne Collision Avoidance Systems""! \n\n<LINK>\n\nIt\'s work by me, @MasorX, Vincent Lenders and Ivan Martinovic.\n\nMore below!', 'Collision avoidance systems are important last-resort systems in aviation. They step in at the last moment to keep aircraft apart, using Resolution Advisories (RAs). RAs include a target vertical rate for the pilots to reach.', ""RAs are compulsory instructions which pilots have to follow in a few seconds. Here's an example in a sim:\n\nhttps://t.co/gBKvvMVahN"", ""There's been some really interesting work on attacker triggered TCAS RAs recently, such as the talk by @alexlomas on spoofing in a simulator. You can find that over here:\n\nhttps://t.co/RlpzY6Fhsg"", 'We know that collision avoidance systems might be vulnerable to attack due to the data link they use - but in practice these attacks appear tricky. This is because TCAS (and the next generation ACAS X) use message time-of-flight to estimate range to nearby aircraft.', ""If an attacker wants to trigger an RA, they need to make TCAS think an aircraft is very close - which is pretty tricky if you're on the ground. There are some attacker models which let the attacker go wherever or send pre-emptive messages but they are technically very difficult."", 'So we wanted to figure out whether an attacker can have any effect on collision avoidance systems from the ground.\n\nHandily, the next-generation collision avoidance system, ACAS X, has a reference implementation and as a baseline, matches TCAS performance.', 'So - we built a simulator around ACAS X! \n\nThis let us feed a lot of real flights from @openskynetwork into ACAS X and see what happened when these flights were targeted by attacks. \n\nWe found that aircraft are particularly vulnerable under around 13000ft.', 'At this altitude we could trigger RAs in 79% of the flights we tested, with the attacker injecting a fairly simplistic sequence of messages.', 'We also found that these attacks caused fairly sizeable deviations from the original trajectory of the target. This was worst for the aircraft flying level or already descending.', ""Finally, we think this approach could be used to identify 'hotspots', where attacks are most likely to take place. For example, we ran simulations in a grid of locations around Frankfurt am Main on real traffic from the airport, highlighting some vulnerable areas."", 'This could guide where monitoring equipment is deployed. \n\nOverall, we think this area is really interesting and are looking forward to more research coming out of it!']",https://arxiv.org/abs/2010.01034,"Airborne collision avoidance systems provide an onboard safety net should normal air traffic control procedures fail to keep aircraft separated. These systems are widely deployed and have been constantly refined over the past three decades, usually in response to near misses or mid-air collisions. Recent years have seen security research increasingly focus on aviation, identifying that key wireless links---some of which are used in collision avoidance---are vulnerable to attack. In this paper, we go one step further to understand whether an attacker can remotely trigger false collision avoidance alarms. Primarily considering the next-generation Airborne Collision Avoidance System X (ACAS X), we adopt a modelling approach to extract attacker constraints from technical standards before simulating collision avoidance attacks against standardized ACAS X code. We find that in 44% of cases, an attacker can successfully trigger a collision avoidance alert which on average results in a 590 ft altitude deviation; when the aircraft is at lower altitudes, this success rate rises considerably to 79%. Furthermore, we show how our simulation approach can be used to help defend against attacks by identifying where attackers are most likely to be successful. ",Understanding Realistic Attacks on Airborne Collision Avoidance Systems
231,1313027161398423552,21902101,Jim Geach,"[""New paper on arXiv today from my student Mike Smith - applying an idea from natural language processing to automatically extract information from galaxy images: <LINK>\n\n(unfortunately arXiv is refusing to generate the pdf which we're working on!) <LINK>""]",https://arxiv.org/abs/2010.00622,"We present 'Pix2Prof', a deep learning model that can eliminate any manual steps taken when extracting galaxy profiles. We argue that a galaxy profile of any sort is conceptually similar to a natural language image caption. This idea allows us to leverage image captioning methods from the field of natural language processing, and so we design Pix2Prof as a float sequence 'captioning' model suitable for galaxy profile inference. We demonstrate the technique by approximating a galaxy surface brightness (SB) profile fitting method that contains several manual steps. Pix2Prof processes $\sim$1 image per second on an Intel Xeon E5 2650 v3 CPU, improving on the speed of the manual interactive method by more than two orders of magnitude. Crucially, Pix2Prof requires no manual interaction, and since galaxy profile estimation is an embarrassingly parallel problem, we can further increase the throughput by running many Pix2Prof instances simultaneously. In perspective, Pix2Prof would take under an hour to infer profiles for $10^5$ galaxies on a single NVIDIA DGX-2 system. A single human expert would take approximately two years to complete the same task. Automated methodology such as this will accelerate the analysis of the next generation of large area sky surveys expected to yield hundreds of millions of targets. In such instances, all manual approaches -- even those involving a large number of experts -- will be impractical. ","Pix2Prof: fast extraction of sequential information from galaxy imagery
  via a deep natural language 'captioning' model"
232,1313012233840910336,1254620813036158977,Shunya Noda,"['I just released a new working paper, ""On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach"" (joint with @junpeikomiyama at NYU Stern). Comments are highly appreciated!!! <LINK>']",https://arxiv.org/abs/2010.01079,"We analyze statistical discrimination in hiring markets using a multi-armed bandit model. Myopic firms face workers arriving with heterogeneous observable characteristics. The association between the worker's skill and characteristics is unknown ex ante; thus, firms need to learn it. Laissez-faire causes perpetual underestimation: minority workers are rarely hired, and therefore, underestimation towards them tends to persist. Even a slight population-ratio imbalance frequently produces perpetual underestimation. We propose two policy solutions: a novel subsidy rule (the hybrid mechanism) and the Rooney Rule. Our results indicate that temporary affirmative actions effectively mitigate discrimination caused by insufficient data. ","On Statistical Discrimination as a Failure of Social Learning: A
  Multi-Armed Bandit Approach"
233,1312947499141750786,6642132,Ikuya Yamada,"['Our @emnlp2020 paper ‚ÄúLUKE: Deep Contextualized Entity Representations with Entity-aware\nSelf-attention‚Äù is now available on arXiv! We present new pretrained contextualized representations that achieve SOTA on five datasets including SQuAD and CoNLL-2003.\n<LINK>', 'LUKE is based on bidirectional Transformer, treats words and entities in a text as independent tokens, and outputs contextualized representations of them. The representations can be used to address downstream tasks similarly to BERT. https://t.co/s7KZCxBi5D', 'LUKE is trained using a novel pretraining task that involves predicting randomly masked words (equivalent to BERT‚Äôs masked language model) and entities in an entity-annotated corpus obtained from Wikipedia.', 'LUKE also uses a new *entity-aware* self-attention mechanism that considers the types of tokens (words or entities) when computing attention scores.\nThe source code and pretrained models are available at https://t.co/k5koMphw7n. The documentation will be available soon!üòÉ']",https://arxiv.org/abs/2010.01057,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at this https URL ","LUKE: Deep Contextualized Entity Representations with Entity-aware
  Self-attention"
234,1312926204073185280,755924666,Brant Robertson,['New theory paper from the Lyman Continuum Escape Survey (LACES) team led by Kirk Barrow (@KIPAC1). We look at the connection btwn oxygen emission line ratios &amp; Lyman continuum escape in radiative transfer calcs applied to cosmo sims of galaxy formation:\n\n<LINK>'],https://arxiv.org/abs/2010.00592,"Escaping Lyman continuum photons from galaxies likely reionized the intergalactic medium at redshifts $z\gtrsim6$. However, the Lyman continuum is not directly observable at these redshifts and secondary indicators of Lyman continuum escape must be used to estimate the budget of ionizing photons. Observationally, at redshifts $z\sim2-3$ where the Lyman continuum is observationally accessible, surveys have established that many objects that show appreciable Lyman continuum escape fractions $f_{esc}$ also show enhanced [OIII]/[OII] (O$_{32}$) emission line ratios. Here, we use radiative transfer analyses of cosmological zoom-in simulations of galaxy formation to study the physical connection between $f_{esc}$ and O$_{32}$. Like the observations, we find that the largest $f_{esc}$ values occur at elevated O$_{32}\sim3-10$ and that the combination of high $f_{esc}$ and low O$_{32}$ is extremely rare. While high $f_{esc}$ and O$_{32}$ often are observable concurrently, the timescales of the physical origin for the processes are very different. Large O$_{32}$ values fluctuate on short ($\sim$1 Myr) timescales during the Wolf-Rayet-powered phase after the formation of star clusters, while channels of low absorption are established over tens of megayears by collections of supernovae. We find that while there is no direct causal relation between $f_{esc}$ and O$_{32}$, high $f_{esc}$ most often occurs after continuous input from star formation-related feedback events that have corresponding excursions to large O$_{32}$ emission. These calculations are in agreement with interpretations of observations that large $f_{esc}$ tends to occur when O$_{32}$ is large, but large O$_{32}$ does not necessarily imply efficient Lyman continuum escape. ","The Lyman Continuum Escape Survey: Connecting Time-Dependent [OIII] and
  [OII] Line Emission with Lyman Continuum Escape Fraction in Simulations of
  Galaxy Formation"
235,1312915772562452481,177416255,Daniel Litt,"[""New paper with Kiran Kedlaya and Jakub Witaszek! It's all about characteristic two! Spooky... üëªüéÉü™¶‚ò†Ô∏èüíÄüêà\u200d‚¨õ\n<LINK>\n\nI'll do a thread about it in the next few days... <LINK>"", ""@JSEllenberg This is your MO question, right? I've thought a lot about it but never really made any progress..."", ""@JSEllenberg Ah right, I'd forgotten that was over Q! Let me think about the case of F_q(t) -- it's indeed a very natural question!"", '@JSEllenberg I guess one silly comment -- any curve over a field of characteristic p admits a map to P^1 ramified over exactly *one* point. So again one needs tameness for an interesting statement. As our paper discusses this can be an issue in char 2, but in char 3 I suspect the problem...', ""@JSEllenberg isn't so different from over Q (and likewise for characteristic &gt; 3...)"", '@AShmakovMath Probably will save it for the thread :)']",https://arxiv.org/abs/2010.01130,"We show that every smooth projective curve over a finite field k admits a finite tame morphism to the projective line over k. Furthermore, we construct a curve with no such map when k is an infinite perfect field of characteristic two. Our work leads to a refinement of the tame Belyi theorem in positive characteristic, building on results of Sa\""idi, Sugiyama-Yasuda, and Anbar-Tutdere. ","Tamely ramified morphisms of curves and Belyi's theorem in positive
  characteristic"
236,1312164325927383040,308306041,Kamal Ndousse,"['Big personal milestone! My first ML paper is on arXiv: <LINK> \nWe propose a simple method that helps RL agents in shared environments learn from one another, and show that the learned social policies improve zero-shot transfer performance in new environments. üëá', 'Special thanks to @natashajaques, and to our collaborators @svlevine @douglas_eck!\nAnd to @OpenAI -- this work grew out of the Scholars program, which I participated in earlier this year.', ""In the paper, we show that social multi-agent RL can be useful even for non-social tasks! Our SociAPL agents are able to use cues from expert behavior to navigate environments they've never seen before.""]",https://arxiv.org/abs/2010.00581,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start. ",Emergent Social Learning via Multi-agent Reinforcement Learning
237,1312115601574248448,1641956484,Nikita Nangia,"['Hello #NLProc twitter üëã \nNew #EMNLP2020 paper! We introduce CrowS-Pairs, a crowdsourced dataset to measure the degree to which US social stereotypes are captured by MLMs. <LINK> 1/7', 'This work was done with @claravania, @rasikabh, and  @sleepinyourhat!\n\nCrowS-Pairs has 1508 English sentence pairs. It covers 9 categories of stereotypes: race, gender, sexual orientation, religion, age, nationality, disability, physical appearance, and socioeconomic status. 2/7', 'Each example consists of 2 sentences, the first is more stereotyping than the second. One sentence is also always about a historically disadvantaged group, and the paired sentence is about a contrasting advantaged group. Eg: Fat/thin people can never really be attractive. 3/7', 'Sentences in each pair are minimally distant: the only words that change are those identifying the group being spoken about. We calculate the pseudo log likelihood of a sentence while conditioning on the modified words to control for frequency effects 4/7', 'Comparing the log likelihoods of sentences in each pair, we find that BERT, RoBERTa, and ALBERT prefer the stereotyping sentences on average. All models display this bias in each of the 9 CrowS-PairS. 5/7', 'The biases reflected in this dataset are specific to the US, they are not exhaustive, and stereotypes that are salient to other cultural contexts are not covered. CrowS-Pairs will be helpful for measuring progress towards building less biased language models for English. 6/7', 'Please have a look at the paper for more details! And let us know if you have any comments or feedback. 7/7', '*each of the 9 categories']",https://arxiv.org/abs/2010.00133,"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress. ","CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked
  Language Models"
238,1312086085867114497,485856511,Channon Visscher,['Our new paper is posted on the arXiv! (<LINK>) Here‚Äôs a great thread by lead author @jjfplanet summarizing our findings. <LINK>'],https://arxiv.org/abs/2010.00146,"The atmospheric pressure-temperature profiles for transiting giant planets cross a range of chemical transitions. Here we show that the particular shape of these irradiated profiles for warm giant planets below 1300 K lead to striking differences in the behavior of non-equilibrium chemistry compared to brown dwarfs of similar temperatures. Our particular focus is H$_2$O, CO, CH$_4$, CO$_2$, and NH$_3$ in Jupiter- and Neptune-class planets. We show the cooling history of a planet, which depends most significantly on planetary mass and age, can have a dominant effect on abundances in the visible atmosphere, often swamping trends one might expect based on Teq alone. The onset of detectable CH$_4$ in spectra can be delayed to lower Teq for some planets compared to equilibrium, or pushed to higher Teq. The detectability of NH$_3$ is typically enhanced compared to equilibrium expectations, which is opposite to the brown dwarf case. We find that both CH$_4$ and NH$_3$ can become detectable at around the same Teq (at Teq values that vary with mass and metallicity) whereas these ""onset"" temperatures are widely spaced for brown dwarfs. We suggest observational strategies to search for atmospheric trends and stress that non-equilibrium chemistry and clouds can serve as probes of atmospheric physics. As examples of atmospheric complexity, we assess three Neptune-class planets GJ 436b, GJ 3470b, and WASP-107, all around Teq=700 K. Tidal heating due to eccentricity damping in all three planets heats the deep atmosphere by thousands of degrees, and may explain the absence of CH$_4$ in these cool atmospheres. Atmospheric abundances must be interpreted in the context of physical characteristics of the planet. ","Beyond Equilibrium Temperature: How the Atmosphere/Interior Connection
  Affects the Onset of Methane, Ammonia, and Clouds in Warm Transiting Giant
  Planets"
239,1312048795480403968,61474773,Bryan Gaensler üì°üß≤,"[""Very excited to be part of this important new paper from my team, led by Jennifer West. We have been talking for years about how to measure helicity (twisted magnetic fields) in the Universe, and now we think we've finally figured it out! <LINK>"", 'This is also one of the few papers I‚Äôve been involved in for which &gt;50% of the authors have been women.', 'Why is finding twisted magnetic fields important? Because we think magnetism in galaxies (in particular in our own Milky Way) is generated by dynamos.\n\nDynamos always produce twisted magnetism. So if we see a twisted magnetic field, we might be seeing evidence for a dynamo!', 'Or to put it more formally:\n\nIn this paper, we‚Äôve confirmed one of the key predictions of the dynamo theory for the origin of magnetic fields in the Universe: the presence of magnetic helicity.']",https://arxiv.org/abs/2010.00105,"We search for observational signatures of magnetic helicity in data from all-sky radio polarization surveys of the Milky Way Galaxy. Such a detection would help confirm the dynamo origin of the field and may provide new observational constraints for its shape. We compare our observational results to simulated observations for both a simple helical field, and for a more complex field that comes from a solution to the dynamo equation. Our simulated observations show that the large-scale helicity of a magnetic field is reflected in the large-scale structure of the fractional polarization derived from the observed synchrotron radiation and Faraday depth of the diffuse Galactic synchrotron emission. Comparing the models with the observations provides evidence for the presence of a quadrupolar magnetic field with a vertical component that is pointing away from the observer in both hemispheres of the Milky Way Galaxy. Since there is no reason to believe that the Galactic magnetic field is unusual when compared to other galaxies, this result provides further support for the dynamo origin of large-scale magnetic fields in galaxies. ",Helicity in the Large-Scale Galactic Magnetic Field
240,1311930789060321286,292313052,Martin Kilbinger,['New paper by my colleague Tim Schrabback from AIfA Bonn today. Using #WeakLensing from a number of observations they detected the ellipticity of dark matter halos at 3.8sigma! <LINK>'],https://arxiv.org/abs/2010.00311,"Cosmological simulations predict that galaxies are embedded into triaxial dark matter haloes, which appear approximately elliptical in projection. Weak gravitational lensing allows us to constrain these halo shapes and thereby test the nature of dark matter. Weak lensing has already provided robust detections of the signature of halo flattening at the mass scales of groups and clusters, whereas results for galaxies have been somewhat inconclusive. Here we combine data from five surveys (NGVSLenS, KiDS/KV450, CFHTLenS, CS82, and RCSLenS) in order to tighten observational constraints on galaxy-scale halo ellipticity for photometrically selected lens samples. We constrain $f_\rm{h}$, the average ratio between the aligned component of the halo ellipticity and the ellipticity of the light distribution, finding $f_\rm{h}=0.303^{+0.080}_{-0.079}$ for red lenses and $f_\rm{h}=0.217^{+0.160}_{-0.159}$ for blue lenses when assuming elliptical NFW density profiles and a linear scaling between halo ellipticity and galaxy ellipticity. Our constraints for red galaxies constitute the currently most significant ($3.8\sigma$) systematics-corrected detection of the signature of halo flattening at the mass scale of galaxies. Our results are in good agreement with expectations from the Millennium Simulation that apply the same analysis scheme and incorporate models for galaxy-halo misalignment. Assuming these misalignment models and the analysis assumptions stated above are correct, our measurements imply an average dark matter halo ellipticity for the studied red galaxy samples of $\langle|\epsilon_\rm{h}|\rangle=0.174\pm 0.046$, where $|\epsilon_{h}|=(1-q)/(1+q)$ relates to the ratio $q=b/a$ of the minor and major axes of the projected mass distribution. Similar measurements based on larger upcoming weak lensing data sets can help to calibrate models for intrinsic galaxy alignments. [abridged] ","Tightening weak lensing constraints on the ellipticity of galaxy-scale
  dark matter haloes"
241,1311851889009418241,3245949691,Rebecca Leane,"['New paper!\n\nExoplanets as New Sub-GeV Dark Matter Detectors\n<LINK>\n\n@SmirnovJuri  and I find that some of the billions of exoplanets peppered throughout our Galaxy can be used to discover Dark Matter! Thread:', 'Are we alone in the Universe? This question has driven wide-reaching interest in discovering a planet like our own. Regardless of whether or not we ever find alien life, the scientific advances from finding and understanding other planets will be enormous.', 'From a particle physics perspective, new celestial bodies provide a vast playground to discover new physics. Astrophysical systems have already been broadly used to probe new physics, including investigating the effects of gravitationally captured Dark Matter (DM).', 'This can occur if DM scatters with the system, loses energy, and becomes gravitationally bound. DM may then deposit its annihilation energy, increasing the temperature of the system.', 'Such DM annihilation heating has been considered in many objects, such as neutron stars and white dwarfs. The DM heat flow in other moons and planets has been considered, including Earth, Uranus, Neptune and Jupiter, Earth‚Äôs Luna, Jupiter‚Äôs Ganymede, as well as hot Jupiters.', 'Our paper explores the potential to discover DM using exoplanets ‚Äì planets outside our solar system. We use the term ‚Äúexoplanets‚Äù to refer to the broader class of all extra-solar planets (including rogue planets), as well as brown dwarfs, which exist at the planet-star boundary.', 'There are many advantages of using exoplanets to search for DM over other celestial bodies. These include:', '1. A rapidly accelerating research program: Until 1992, we didn‚Äôt even know if exoplanets existed. Almost all exoplanets we now know were only discovered in the last decade, with the majority found in the last five years.', 'There are so many new and upcoming telescopes poised to make exciting new discoveries, including @NASA_TESS, @VRubinObs, @NASARoman, @ESAGaia, @NASAWebb, @luvoirtelescope + many more!', '2. Enormous number of expected exoplanets: It is estimated that there are about 300 billion exoplanets in our Galaxy. While of course these won‚Äôt all be immediately found, even a small percentage leads to an *enormous* statistical advantage for understanding potential signals.', '3. Much larger surface area than neutron stars: Another key proposed search for DM heating using infrared telescopes is for old, cold neutron stars. A typical neutron star has a radius of about 10 km, while exoplanets of interest to us have radii of about 50,000 ‚Äì 200,000 km.', 'This means that exoplanets are (i) easier to find and (ii) its easier to measure their temperatures both locally and much further into the Galactic Center, where the DM density increases.', '4. Low temperatures: Exoplanets can be very cold, and can exist very far in large orbits from any host star to which they may be bound. They can even go *rogue*, floating free from any parent star.', 'As the low temperatures allow for a clearer signal over background for DM heating, exoplanets are advantageous over fuel-burning stars. Furthermore, their low core temperatures can prevent lower DM evaporation, providing new sensitivity to MeV-scale DM.', 'In our paper, we exploit all these features to identify new searches for DM in exoplanets. We establish two different searches: one for distant exoplanets and one for local exoplanets.', 'A schematic figure of these searches is shown here: https://t.co/GeD6gaciy6', 'where this is an assortment of exoplanet masses at varying galactic radii. The black dots are exoplanet temperature expectations with DM heating. The pink triangles show the expected background internal heat of the exoplanets.', 'The dashed line shows the minimum telescope sensitivity to these temperatures, using the James Webb Space Telescope (@NASAWebb). Temperatures above that line can in principle be probed.', 'Distant exoplanets can be used to map the Galactic DM density, given sufficient statistics and telescope sensitivity. This is seen by the uptick of many hot exoplanets, scaling with the DM density.', 'Of all the types of exoplanets, our ideal classes of exoplanets are Jupiters (with mass and radius about that of Jupiter), which extend into Super-Jupiters, as well as brown dwarfs, which have roughly 14-75 Jupiter masses.', 'To determine the DM-exoplanet heating, we considered a few different different DM density and velocity profiles: an NFW and a generalized NFW (gNFW) profile, and a Burkert profile, which all produce different predictions for DM-heating.', 'To see the predictions for various profiles and exoplanet masses, we break our results up into mass categories. For the Jupiters - Super Jupiters, we get as a function of distance to the Galactic Center: https://t.co/xEbf0ntx1V', 'where the shaded regions cover the range of DM-heated temperatures for varying Super Jupiter masses (highest mass at the top, lowest mass at the bottom). The dotted lines are predictions without DM.\nAnd on the higher mass end, for the highest-mass brown dwarfs: https://t.co/kqRL245AGQ', 'Comparing with the JWST sensitivity, it looks like this is indeed a detectable effect! For the Super-Jupiters, only some DM profiles can be well-probed for the distant exoplanets. The higher-mass brown dwarfs are clearly a better target for some profiles, for the distant search.', 'The Jupiters are more advantageous for local searches, as they have lower expected internal heat, and require less DM heating to outperform backgrounds. Of course, in the local position, the DM density is lower, so less DM-heating occurs.', 'Zooming in, for the local searches using Jupiters, we get the following temperatures and JWST estimated sensitivities: https://t.co/0XAKI91p70', 'The 1,0.1,0.01,0.001 numbers correspond to varying planetary emissivities. This is simply considering how efficiently the planet is trapping heat (like a greenhouse effect).', ""The telescope sensitivities vary in each case here (compare solid with dashed for a given label), because the spectral flux ends up looking different with emissivity. (We didn't show this effect for the distant searches, instead only showing the emissivities of 1 there.)"", 'Assuming the exoplanet temperatures can be sufficiently measured over their expected background temperature, we can also determine sensitivities to the DM scattering cross section in the exoplanet.', 'For spin-independent scattering in the local position, we get: https://t.co/CJ4J2d7pMV', 'And for closer to the Galactic Center (at 0.1 kpc), we get the following sensitivities: https://t.co/ycopfhJa4E', 'where the solid lines are for when all DM is captured (corresponding to maximal heating), while if 10% of the capture rate is achieved, the dotted limits would apply (corresponding to a lower amount of heating).', 'The ""Jupiters"" lines are for exoplanets with a mass and radius of Jupiter, the ""Brown Dwarfs"" lines are for masses up to 75 Jupiters. Exoplanets with intermediate masses would lie between these lines.', 'The shaded regions show existing complementary constraints. DD is a compliation of direct detection constraints, Earth is from DM-Earth heating, and Xenon1T (CR) is from boosted cosmic rays.', 'We also get a bit of new sensitivity to DM-electron scattering, in the case that 10% capture rates can be probed: https://t.co/KWkaSsHIhE', 'For both the DM-proton and DM-electron plots,the sensitivity region shown would be all filled in as a constraint if a sufficiently cold Jupiter or brown dwarf were measured. For instead discovery of a DM-heating signal, the DM parameters would lie somewhere above the lines shown.', 'We see that exoplanets can provide a new probe of MeV-scale DM, with sensitivities stronger than other experiments by up to 6 orders of magnitude! This is a new way to discover (or exclude) light DM.', 'Going forward, it will be important to investigate detailed simulations of temperature curves as a function of age for exoplanets; taking into account the presence of DM heating + planetary effects will be required to make more precise statements than the estimates we presented.', 'The exoplanet program is rapidly accelerating. Amongst the billions of new worlds in our Galaxy, many are waiting to reveal their surprises. Unexpected discoveries are inevitable, and numerous new telescopes with cutting-edge technology are ready to make them.', 'Hoping now for a successful launch of JWST next year, and the continued success of the many exoplanet telescopes!', 'Lastly, shout out to my awesome collaborator on this project, @SmirnovJuri! Neither of us knew almost anything about exoplanets before starting this project, so it has been lots of fun to venture into this very different yet exciting (exo-)world.', ""@DjunaCroon The specific DM-heating temperatures you would get aren't related to the lines that are shown; the lines are just the cross sections assuming the given amount of DM is captured.  The heating from the smaller cross sections is less than the full capture temperatures, (1/3)"", '@DjunaCroon so the smaller capture rates are a scenario that is harder to detect, but if you could detect the smaller heating change, you would have stronger sensitivity.  (2/3)', '@DjunaCroon The conservative case for clearer detection (and highest signal over background) is the full capture rate, that produces the highest DM-heating rates. (3/3)', '@DjunaCroon No worries! (1/1) ;)', '@geller_mic @SmirnovJuri Thanks! :)']",https://arxiv.org/abs/2010.00015,"We present exoplanets as new targets to discover Dark Matter (DM). Throughout the Milky Way, DM can scatter, become captured, deposit annihilation energy, and increase the heat flow within exoplanets. We estimate upcoming infrared telescope sensitivity to this scenario, finding actionable discovery or exclusion searches. We find that DM with masses above about an MeV can be probed with exoplanets, with DM-proton and DM-electron scattering cross sections down to about $10^{-37}$cm$^2$, stronger than existing limits by up to six orders of magnitude. Supporting evidence of a DM origin can be identified through DM-induced exoplanet heating correlated with Galactic position, and hence DM density. This provides new motivation to measure the temperature of the billions of brown dwarfs, rogue planets, and gas giants peppered throughout our Galaxy. ",Exoplanets as Sub-GeV Dark Matter Detectors
242,1311846189277433856,141440459,Rod Van Meter üåª,['New paper dance:\n<LINK>'],https://arxiv.org/abs/2010.00255,"Control modular addition is a core arithmetic function, and we must consider the computational cost for actual quantum computers to realize efficient implementation. To achieve a low computational cost in a control modular adder, we focus on minimizing KQ, defined by the product of the number of qubits and the depth of the circuit. In this paper, we construct an efficient control modular adder with small KQ by using relative-phase Toffoli gates in two major types of quantum computers: Fault-Tolerant Quantum Computers (FTQ) on the Logical layer and Noisy Intermediate-Scale Quantum Computers (NISQ). We give a more efficient construction compared to Van Meter and Itoh's, based on a carry-lookahead adder. In FTQ, $T$ gates incur heavy cost due to distillation, which fabricates ancilla for running $T$ gates with high accuracy but consumes a lot of specially prepared ancilla qubits and a lot of time. Thus, we must reduce the number of $T$ gates. We propose a new control modular adder that uses only 20% of the number of $T$ gates of the original. Moreover, when we take distillation into consideration, we find that we minimize $\text{KQ}_{T}$ (the product of the number of qubits and $T$-depth) by running $\Theta\left(n / \sqrt{\log n} \right)$ $T$ gates simultaneously. In NISQ, CNOT gates are the major error source. We propose a new control modular adder that uses only 35% of the number of CNOT gates of the original. Moreover, we show that the $\text{KQ}_{\text{CX}}$ (the product of the number of qubits and CNOT-depth) of our circuit is 38% of the original. Thus, we realize an efficient control modular adder, improving prospects for the efficient execution of arithmetic in quantum computers. ","Efficient Construction of a Control Modular Adder on a Carry-Lookahead
  Adder Using Relative-phase Toffoli Gates"
243,1311840390484434944,1133857840693518336,Spencer Frei,"['New paper arXiv! On the agnostic learning of halfspaces  using convex surrogates for the zero-one loss. We show that for many distributions, vanilla gradient descent finds halfspaces with error O(OPT^(1/2)) + Œµ in poly time, where OPT=best halfspace error. <LINK>', 'It is computationally intractable to learn up to the exact best OPT+Œµ error, but interestingly, vanilla GD is able to find approximate minimizers quickly! This same phenomenon occurs for learning a single ReLU (see https://t.co/9NJ7D6Yiv3).  Joint work w/ @_YuanCao_ @QuanquanGu']",https://arxiv.org/abs/2010.00539,"We analyze the properties of gradient descent on convex surrogates for the zero-one loss for the agnostic learning of linear halfspaces. If $\mathsf{OPT}$ is the best classification error achieved by a halfspace, by appealing to the notion of soft margins we are able to show that gradient descent finds halfspaces with classification error $\tilde O(\mathsf{OPT}^{1/2}) + \varepsilon$ in $\mathrm{poly}(d,1/\varepsilon)$ time and sample complexity for a broad class of distributions that includes log-concave isotropic distributions as a subclass. Along the way we answer a question recently posed by Ji et al. (2020) on how the tail behavior of a loss function can affect sample complexity and runtime guarantees for gradient descent. ",Agnostic Learning of Halfspaces with Gradient Descent via Soft Margins
244,1326516784468463623,255264665,Stuart Thomson,"['New paper about our work on hydrodynamic droplet lattices is available on @arxiv!\n\nw/ Matt Durey and Ruben Rosales.\n\n<LINK> <LINK>', 'Here we derive a discrete complex Ginzburg-Landau equation (dCGLE) from a model of the droplet lattice (described here: https://t.co/tTUZzWCXJJ). The dCGLE predicts all sorts of exciting stuff like discrete solitons, breathers, and traveling waves.', 'The coefficients of the dCGLE are directly connected to the constitutive properties of the droplet model, giving us more to explore in future experiments. Our results point to deeper connections between nonlinear oscillators and active particles in complex environments.']",https://arxiv.org/abs/2010.12655,"A discrete and periodic complex Ginzburg-Landau equation, coupled to a discrete mean equation, is systematically derived from a driven and dissipative oscillator model, close to the onset of a supercritical Hopf bifurcation. The oscillator model is inspired by recent experiments exploring active vibrations of quasi-one-dimensional lattices of self-propelled millimetric droplets bouncing on a vertically vibrating fluid bath. Our systematic derivation provides a direct link between the constitutive properties of the lattice system and the coefficients of the resultant amplitude equations, paving the way to compare the emergent nonlinear dynamics---namely discrete bright and dark solitons, breathers, and traveling waves---against experiments. Further, the amplitude equations allow us to rationalize the successive bifurcations leading to these distinct dynamical states. The framework presented herein is expected to be applicable to a wider class of oscillators characterized by the presence of a dynamic coupling potential between particles. More broadly, our results point to deeper connections between nonlinear oscillators and the physics of active and driven matter. ","A discrete complex Ginzburg-Landau equation for a hydrodynamic active
  lattice"
245,1324716399432880129,284589307,David Benaron,"['In 1961, Drake proposed equation for odds of other intelligent life. Not known was ""how many planets can sustain life?"" A new paper shows galaxy holds over 300 million habitable planets.\nDrake: <LINK>\nNews:\n<LINK>\nArticle: <LINK>']",https://arxiv.org/abs/2010.14812,"We present occurrence rates for rocky planets in the habitable zones (HZ) of main-sequence dwarf stars based on the Kepler DR25 planet candidate catalog and Gaia-based stellar properties. We provide the first analysis in terms of star-dependent instellation flux, which allows us to track HZ planets. We define $\eta_\oplus$ as the HZ occurrence of planets with radius between 0.5 and 1.5 $R_\oplus$ orbiting stars with effective temperatures between 4800 K and 6300 K. We find that $\eta_\oplus$ for the conservative HZ is between $0.37^{+0.48}_{-0.21}$ (errors reflect 68\% credible intervals) and $0.60^{+0.90}_{-0.36}$ planets per star, while the optimistic HZ occurrence is between $0.58^{+0.73}_{-0.33}$ and $0.88^{+1.28}_{-0.51}$ planets per star. These bounds reflect two extreme assumptions about the extrapolation of completeness beyond orbital periods where DR25 completeness data are available. The large uncertainties are due to the small number of detected small HZ planets. We find similar occurrence rates using both a Poisson likelihood Bayesian analysis and Approximate Bayesian Computation. Our results are corrected for catalog completeness and reliability. Both completeness and the planet occurrence rate are dependent on stellar effective temperature. We also present occurrence rates for various stellar populations and planet size ranges. We estimate with $95\%$ confidence that, on average, the nearest HZ planet around G and K dwarfs is about 6 pc away, and there are about 4 HZ rocky planets around G and K dwarfs within 10 pc of the Sun. ","The Occurrence of Rocky Habitable Zone Planets Around Solar-Like Stars
  from Kepler Data"
246,1323267496456212483,1096702971805618176,Elie Aljalbout,['New Paper @corl_conf : \nLearning Vision-based Reactive Policies for Obstacle Avoidance.\n\nOur goal is to learn the vision-motion relationship for high-DoF robot manipulators. \nProject:  <LINK>\nPaper: <LINK>'],https://arxiv.org/abs/2010.16298,"In this paper, we address the problem of vision-based obstacle avoidance for robotic manipulators. This topic poses challenges for both perception and motion generation. While most work in the field aims at improving one of those aspects, we provide a unified framework for approaching this problem. The main goal of this framework is to connect perception and motion by identifying the relationship between the visual input and the corresponding motion representation. To this end, we propose a method for learning reactive obstacle avoidance policies. We evaluate our method on goal-reaching tasks for single and multiple obstacles scenarios. We show the ability of the proposed method to efficiently learn stable obstacle avoidance strategies at a high success rate, while maintaining closed-loop responsiveness required for critical applications like human-robot interaction. ",Learning Vision-based Reactive Policies for Obstacle Avoidance
247,1322200829810610177,1311850585386348544,Max H Farrell,"['New paper alert! \n\nMachine learning is great for high-dimensional, complex data, but at the expense of interpretation. Classical econ models are rigid but  interpretable &amp; respect econ principles. Now you can have both: <LINK>.  1/5 #econtwitter', ""W/@LiangTengyuan @sanjog_misra, we take a classical model, e.g. binary choice Y = logit(Œ± + Œ≤'T), and make the coefficients flexible, high-dimensional functions of the data X, so Y = logit(Œ±(X) + Œ≤(X)'T). Our model is just as interpretable, but has flexible heterogeneity. 2/5"", ""Then we construct a new deep net architecture to embed the structure of the model, so machine learning isn't about black box prediction any more, it's about learning economic objects. 3/5 https://t.co/RjauuJADuN"", 'The paper has theoretical results for deep net estimation &amp; for inference on tons economically interesting parameters, like consumer surplus, willingness to pay, average partial effects, returns to scale, and lots more. 4/5', 'We have two applications to show you how it\'s done. One is Papke + @jmwooldridge\'s 1996 fractional outcomes and 401(k) participation. Deep learning works great with only a few thousand observations. You don\'t need ""big data"".\n\nStay tuned for code so you can try this at home! 5/5']",https://arxiv.org/abs/2010.14694,"We develop methodology for estimation and inference using machine learning to enrich economic models. Our framework takes a standard economic model and recasts the parameters as fully flexible nonparametric functions, to capture the rich heterogeneity based on potentially high dimensional or complex observable characteristics. These ""parameter functions"" retain the interpretability, economic meaning, and discipline of classical parameters. Deep learning is particularly well-suited to structured modeling of heterogeneity in economics. We show how to design the network architecture to match the structure of the economic model, delivering novel methodology that moves deep learning beyond prediction. We prove convergence rates for the estimated parameter functions. These functions are the key inputs into the finite-dimensional parameter of inferential interest. We obtain inference based on a novel influence function calculation that covers any second-stage parameter and any machine-learning-enriched model that uses a smooth per-observation loss function. No additional derivations are required. The score can be taken directly to data, using automatic differentiation if needed. The researcher need only define the original model and define the parameter of interest. A key insight is that we need not write down the influence function in order to evaluate it on the data. Our framework gives new results for a host of contexts, covering such diverse examples as price elasticities, willingness-to-pay, and surplus measures in binary or multinomial choice models, effects of continuous treatment variables, fractional outcome models, count data, heterogeneous production functions, and more. We apply our methodology to a large scale advertising experiment for short-term loans. We show how economically meaningful estimates and inferences can be made that would be unavailable without our results. ","Deep Learning for Individual Heterogeneity: An Automatic Inference
  Framework"
248,1320739098055921665,1237611673583628288,Xinsheng Wang,"['Our new work, Show and Speak (SAS), that directly synthesize spoken descriptions of images bypassing any text and phonemes. More details  at <LINK>\nPaper: <LINK>\nCode: <LINK>\n@OScharenborg @SFeng9 @hasegawajohnson <LINK>']",https://arxiv.org/abs/2010.12267,"This paper proposes a new model, referred to as the show and speak (SAS) model that, for the first time, is able to directly synthesize spoken descriptions of images, bypassing the need for any text or phonemes. The basic structure of SAS is an encoder-decoder architecture that takes an image as input and predicts the spectrogram of speech that describes this image. The final speech audio is obtained from the predicted spectrogram via WaveNet. Extensive experiments on the public benchmark database Flickr8k demonstrate that the proposed SAS is able to synthesize natural spoken descriptions for images, indicating that synthesizing spoken descriptions for images while bypassing text and phonemes is feasible. ",Show and Speak: Directly Synthesize Spoken Description of Images
249,1320626674833235969,1020920111476236288,Haggai Maron,"['New paper! Most existing NN architectures for processing microphone arrays deal with fixed arrays. \nWe present an architecture for microphone arrays on which no prior knowledge is presumed and demonstrate its applicability to speech dereverberation. \n<LINK> <LINK>', 'We harness the DSS framework and suggest an architecture that enhances the reverberant log-spectrum. Our experiments show that the proposed position-agnostic setup performs comparably with the position-aware framework and sometimes slightly better, even with fewer microphones.', 'In addition, it substantially improves performance over a single microphone architecture.\nLed by @YochaiYemini, with @EthanFetaya and Sharon Gannot', 'Fix: led by @YeminiYochai']",https://arxiv.org/abs/2010.11875,"Neural networks (NNs) have been widely applied in speech processing tasks, and, in particular, those employing microphone arrays. Nevertheless, most existing NN architectures can only deal with fixed and position-specific microphone arrays. In this paper, we present an NN architecture that can cope with microphone arrays whose number and positions of the microphones are unknown, and demonstrate its applicability in the speech dereverberation task. To this end, our approach harnesses recent advances in deep learning on set-structured data to design an architecture that enhances the reverberant log-spectrum. We use noisy and noiseless versions of a simulated reverberant dataset to test the proposed architecture. Our experiments on the noisy data show that the proposed scene-agnostic setup outperforms a powerful scene-aware framework, sometimes even with fewer microphones. With the noiseless dataset we show that, in most cases, our method outperforms the position-aware network as well as the state-of-the-art weighted linear prediction error (WPE) algorithm. ",Scene-Agnostic Multi-Microphone Speech Dereverberation
250,1320624028265123840,1277462784805154816,Siddharth Garg üåà,"['We have a new (short) paper out that critically evaluates several existing backdoor defenses (including our own). Unfortunately, none work across the board due to three critical pitfalls that we identify in the SOTA. Joint work with @VeldandaAkshaj  <LINK>', 'The pitfalls mirror some of those in early defences against adversarial perturbations. (1) hyper-parameters of attack not optimized; (2) severely restrictive assumptions on backdoor shape/size; (3) no evaluation of adaptive attacks.', 'We also design new attacks against SOTA defences, for instance, a combination backdoor that only triggers when two or more backdoors are present but not if either is present by itself. This easily circumvents ABS for instance.', ""@pinyuchenTW @VeldandaAkshaj Awesome. we'll take a look. @VeldandaAkshaj is our master attacker. FWIW we also have a new paper with code (https://t.co/IZBBnpTfPM) that we believe avoids these pitfalls, but I'd really love to get to a certifiable defense at some point :P"", ""@pinyuchenTW @VeldandaAkshaj Would love to chat more. Certifiability requires a formal notion of security to start against and we have a couple of ideas that we'd love feedback on! :)"", '@pinyuchenTW @VeldandaAkshaj (I meant to certify against :))']",https://arxiv.org/abs/2010.12186,"Deep neural networks (DNNs) demonstrate superior performance in various fields, including scrutiny and security. However, recent studies have shown that DNNs are vulnerable to backdoor attacks. Several defenses were proposed in the past to defend DNNs against such backdoor attacks. In this work, we conduct a critical analysis and identify common pitfalls in these existing defenses, prepare a comprehensive database of backdoor attacks, conduct a side-by-side evaluation of existing defenses against this database. Finally, we layout some general guidelines to help researchers develop more robust defenses in the future and avoid common mistakes from the past. ",On Evaluating Neural Network Backdoor Defenses
251,1319792944551952385,409006348,Shrimai,"['Our new paper ""EIGEN: Event Influence GENeration\nusing Pre-trained Language Models"" with @aman_madaan @dheerajgopal Yiming Yang, @Lasha1608 &amp; Ed Hovy explores the use of language models for generating event influences. \nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2010.11764,"Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a ""what-if"" Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning. ",EIGEN: Event Influence GENeration using Pre-trained Language Models
252,1318639882575597568,3067132687,Kiana Ehsani,"['Most representation learning works these days focus on contrastive approaches. What if we considered human attention and muscle movements as a supervisory signal? Check out our new large-scale dataset and representation learning approach. \nPaper: <LINK> <LINK>', 'Code: https://t.co/R8BO5DYZj4\nJoint work with @danielgordon100, Tom Nguyen, @RoozbehMottaghi, and Ali Farhadi.']",https://arxiv.org/abs/2010.08539,"Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ""muscly-supervised"" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: this https URL ","What Can You Learn from Your Muscles? Learning Visual Representation
  from Human Interactions"
253,1318592850964205570,1020920111476236288,Haggai Maron,"['New paper!  ‚ÄúHow to Stop Epidemics: Controlling Graph Dynamics with RL and GNNs‚Äù. An epidemic is a partially-observed dynamic process that spreads over a temporal contact graph. In this setting, how should we prioritize COVID-19 tests?  <LINK> <LINK>', 'We formulate this problem as a sequential decision problem over a graph. In face of an exponential state space, combinatorial action space, and partial observability, we design RLGN, a tractable RL scheme to prioritize which nodes should be tested, using GNNs to rank the nodes.', 'We evaluate this approach in three types of social networks: community-structured, preferential attachment, and based on statistics from real cellular tracking. RLGN consistently outperforms all baselines in our experiments.', 'We show that prioritizing tests using RLGN\non temporal graphs can increase the number of healthy people by 25% and contain the epidemic 30% more often than supervised approaches and 2.5√ó more often than non-learned baselines using the same resources.', 'Dynamics matter: Building the contacts map is crucial. The information in the temporal dynamics matters and can help detecting fast moving pockets of the epidemic.\n\nJoint work with @GalChechik and Shie Mannor\nLed by Eli Meirom \n@NVIDIAAI']",https://arxiv.org/abs/2010.05313,"We consider the problem of controlling a partially-observed dynamic process on a graph by a limited number of interventions. This problem naturally arises in contexts such as scheduling virus tests to curb an epidemic; targeted marketing in order to promote a product; and manually inspecting posts to detect fake news spreading on social networks. We formulate this setup as a sequential decision problem over a temporal graph process. In face of an exponential state space, combinatorial action space and partial observability, we design a novel tractable scheme to control dynamical processes on temporal graphs. We successfully apply our approach to two popular problems that fall into our framework: prioritizing which nodes should be tested in order to curb the spread of an epidemic, and influence maximization on a graph. ","Controlling Graph Dynamics with Reinforcement Learning and Graph Neural
  Networks"
254,1318549536269361158,943865672550973445,Ryan Plestid,"['Alright, last week was crazy but that works out fine since the companion paper came out today! \n\nTwo new papers out explaining how to constrain neutrino portals by searching for upscattering inside the Earth \n\n<LINK>\n<LINK>\n\nThread...', '1/ ""Neutrino portals"" are some extra interaction beyond the SM that couples a left-handed neutrino to a (as of yet undiscovered and therefore hypothetical) sterile neutrino / heavy neutral lepton. \n\nThese particles can be hard to discover because they are ""sterile""...', '2/ i.e. they do not interact very strongly. The best way to look for them is to hope they get produced close to an experiment and look for them to decay inside the detector (leaving a photon or electron-positron pair behind)...', '3/ One problem with this strategy is that the HNLs can take a very-long time to decay so even if you produce one it has only a small chance of decaying inside the experiment.', '4/. These two papers point out that if the HNL has such a long lifetime, then it does not need to be produced inside the detector itself! In fact, for a lot of interesting parameter space the HNLs could be produced *anywhere* inside the Earth https://t.co/FrBTjsCk09', '5/ One fun little consequence is that if the decay length is smaller than the radius of the Earth then the event rate inside a detector can actually become independent of the decay length: no penalty at all! https://t.co/tibTjE8n27', '6/ In some models the upscattering is (quasi-)isotropic and so the whole Earth ""lights up"", while in other models the upscattering is mostly forward so you have to think carefully about the geometry of the Earth relative to the Sun https://t.co/IIzSdmszBg', '7/ I made use of solar neutrinos in this work, which simplifies a lot of the analysis. Nuclei can be treated as static sources because of the relatively low neutrino energies.  This limits the reach to lower masses, but this is where constraints on HNLs tend to weaken', '8/ In the case of a neutrino dipole portal, I find that this is the best method for search for O(MeV) HNLs. https://t.co/y8LhOVop8W', '9/ The constraints are roughly independent of the neutrino flavor because the solar neutrino flux is pretty inclusive when it arrives to Earth (ie there are \\nu_\\tau, \\nu_\\mu, and \\nu_e). \n\nThis is true for both the constraints on dipole portals and mass-mixing portals.', '10/ For the mass-mixing portal, I find that the signals tend to be much smaller because the upscattering takes place via the weak interaction instead of the electromagnetic one. \n\nNevertheless upscattering inside the Earth provides new constraints on \\nu_\\tau mixing https://t.co/tfQ2JV4lDk', '11/ ....and I meant to post this figure as well but I forgot. https://t.co/eJY6en5ZzJ']",https://arxiv.org/abs/2010.04193,"Solar neutrinos upscattering inside the Earth can source unstable particles that can decay inside terrestrial detectors. Contrary to naive expectations we show that when the decay length is much shorter than the radius of the \emph{Earth} (rather than the detector), the event rate is independent of the decay length. In this paper we study a transition dipole operator (neutrino dipole portal) and show that Borexino's existing data probes previously untouched parameter space in the 0.5--20 MeV regime, complementing recent cosmological and supernova bounds. We briefly comment on similarities and differences with luminous dark matter and comment on future prospects for analogous signals stemming from atmospheric neutrinos. ",Luminous solar neutrinos I: Dipole portals
255,1318157701176791040,1146421407674355712,Takuo Matsubara,"['New arXiv paper with Chris and @fx_briol: <LINK> <LINK>', '(1) Proposed ‚ÄúRidgelet prior‚Äù that encode given GP prior predictive into BNN prior predictive.\n\n(2) Showed that BNN prior pred are capable of approximating any differentiable GP prior pred.']",https://arxiv.org/abs/2010.08488,"Bayesian neural networks attempt to combine the strong predictive performance of neural networks with formal quantification of uncertainty associated with the predictive output in the Bayesian framework. However, it remains unclear how to endow the parameters of the network with a prior distribution that is meaningful when lifted into the output space of the network. A possible solution is proposed that enables the user to posit an appropriate Gaussian process covariance function for the task at hand. Our approach constructs a prior distribution for the parameters of the network, called a ridgelet prior, that approximates the posited Gaussian process in the output space of the network. In contrast to existing work on the connection between neural networks and Gaussian processes, our analysis is non-asymptotic, with finite sample-size error bounds provided. This establishes the universality property that a Bayesian neural network can approximate any Gaussian process whose covariance function is sufficiently regular. Our experimental assessment is limited to a proof-of-concept, where we demonstrate that the ridgelet prior can out-perform an unstructured prior on regression problems for which a suitable Gaussian process prior can be provided. ","The Ridgelet Prior: A Covariance Function Approach to Prior
  Specification for Bayesian Neural Networks"
256,1317230167551582208,1243544508983279617,Horng Sheng Chia,"['I am beyond excited about this new paper: <LINK>\n\nIn short, turns out black holes never ‚Äúfall in Love‚Äù. <LINK>']",https://arxiv.org/abs/2010.07300,"We show that rotating black holes do not experience any tidal deformation when they are perturbed by a weak and adiabatic gravitational field. The tidal deformability of an object is quantified by the so-called ""Love numbers"", which describe the object's linear response to its external tidal field. In this work, we compute the Love numbers of Kerr black holes and find that they vanish identically. We also compute the dissipative part of the black hole's tidal response, which is non-vanishing due to the absorptive nature of the event horizon. Our results hold for arbitrary values of black hole spin, for both the electric-type and magnetic-type perturbations, and to all orders in the multipole expansion of the tidal field. The boundary conditions at the event horizon and at asymptotic infinity are incorporated in our study, as they are crucial for understanding the way in which these tidal effects are mapped onto gravitational-wave observables. In closing, we address the ambiguity issue of Love numbers in General Relativity, which we argue is resolved when those boundary conditions are taken into account. Our findings provide essential inputs for current efforts to probe the nature of compact objects through the gravitational waves emitted by binary systems. ",Tidal Deformation and Dissipation of Rotating Black Holes
257,1317145564249665536,253372082,Shrey Desai,"['New #EMNLP2020 paper ""Compressive Summarization with Plausibility and Salience Modeling"" (<LINK>)! We compress sentences w/ plausibility and salience, two data-driven objectives modeled by Transformers, achieving strong in-domain *and* out-of-domain results. 1/ <LINK>', 'Compression combines the robustness of extraction and the flexibility of abstraction, but past approaches commonly rely on heavily engineered syntactic rules to identify deletions. Can we instead learn these deletions in a more data-driven fashion? 2/', 'We create two notions: *plausibility* requires deletions to maintain grammaticality/factuality, while *salience* requires deletions to maximize content selection (ROUGE). Critically, these can be learned with existing resources + automatic oracles and modeled w/ Transformers! 3/ https://t.co/1eF9gT4xeP', 'Our system (CUPS) outperforms extractive SOTA on CNN, WikiHow, XSum, and Reddit. And, we demonstrate the compression module (plausibility + salience) can be applied on top of off-the-shelf extractive models; on top of MatchSum extraction, we see higher ROUGE on CNN/DM. 4/', 'CUPS also generalizes well out-of-domain on transfer tasks mimicking real-world scenarios. By fine-tuning an out-of-domain system on only *500 in-domain samples*, we can match or exceed in-domain extractive ROUGE (often trained on 10K+ samples). 5/ https://t.co/SvE79jzz6S', 'Check out some of our samples below -- the plausibility and salience components work together to create shorter but grammatical and factual extracts of the original documents: 6/ https://t.co/1MjIUSuSOj', 'Finally, our code is publicly available at https://t.co/CiUuiQnpj0. Special thanks to @huggingface for a speedy integration of ELECTRA, which we used as the backbone of our extraction and compression modules üôÇ 7/7']",https://arxiv.org/abs/2010.07886,"Compressive summarization systems typically rely on a crafted set of syntactic rules to determine what spans of possible summary sentences can be deleted, then learn a model of what to actually delete by optimizing for content selection (ROUGE). In this work, we propose to relax the rigid syntactic constraints on candidate spans and instead leave compression decisions to two data-driven criteria: plausibility and salience. Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and spans are salient if they contain important information from the summary. Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied. When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark summarization datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions. Furthermore, the flexibility of our approach allows it to generalize cross-domain: our system fine-tuned on only 500 samples from a new domain can match or exceed an in-domain extractive model trained on much more data. ",Compressive Summarization with Plausibility and Salience Modeling
258,1316996705410273280,1101194916900802566,Arie Cattan,"['In our #emnlp2020 demo paper, we present CoRefi, a new annotation tool that supports both single and multiple documents coreference &gt;&gt; (1/5)\n\n<LINK>\n\njoint work with @pythiccoder and Ido Dagan @biunlp <LINK> <LINK>', ""CoRefi's interface is clean and the annotation is performed using quick keyboard actions, so that annotators can focus on the *task* itself rather than on the tool functionalities &gt;&gt; (2/5) https://t.co/mkuDcBNPII"", 'CoRefi includes an onboarding feature to teach the task to annotators via automatic feedback &gt;&gt; (3/5) https://t.co/J3SyYtpijD', 'CoRefi also provides a novel reviewing algorithm that allows to review and improve the annotation, while keeping track of the original coref clusters &gt;&gt; (4/5) https://t.co/oylO8btA3l', ""Finally, corefi is a web-component and can be embedded in any website. CoRefi's features (onboarding, intuitive annotation interface, reviewing) make CoRefi a good suite for crowdsourcing annotation (5/5)""]",https://arxiv.org/abs/2010.02588,"Coreference annotation is an important, yet expensive and time consuming, task, which often involved expert annotators trained on complex decision guidelines. To enable cheaper and more efficient annotation, we present CoRefi, a web-based coreference annotation suite, oriented for crowdsourcing. Beyond the core coreference annotation tool, CoRefi provides guided onboarding for the task as well as a novel algorithm for a reviewing phase. CoRefi is open source and directly embeds into any website, including popular crowdsourcing platforms. CoRefi Demo: aka.ms/corefi Video Tour: aka.ms/corefivideo Github Repo: this https URL ",CoRefi: A Crowd Sourcing Suite for Coreference Annotation
259,1316738979279114246,3806941534,Adam Cobb,['New work on applying HMC to BNNs! Our symmetric splitting approach preserves the entire Hamiltonian and can be run on a single GPU.\n\nAll the code is in hamiltorch :)\n\nPaper: <LINK>\nBlog post: <LINK>\n\nWork with @BJalaian! <LINK>'],https://arxiv.org/abs/2010.06772,"Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) approach that exhibits favourable exploration properties in high-dimensional models such as neural networks. Unfortunately, HMC has limited use in large-data regimes and little work has explored suitable approaches that aim to preserve the entire Hamiltonian. In our work, we introduce a new symmetric integration scheme for split HMC that does not rely on stochastic gradients. We show that our new formulation is more efficient than previous approaches and is easy to implement with a single GPU. As a result, we are able to perform full HMC over common deep learning architectures using entire data sets. In addition, when we compare with stochastic gradient MCMC, we show that our method achieves better performance in both accuracy and uncertainty quantification. Our approach demonstrates HMC as a feasible option when considering inference schemes for large-scale machine learning problems. ","Scaling Hamiltonian Monte Carlo Inference for Bayesian Neural Networks
  with Symmetric Splitting"
260,1316730490787950592,899968956253044737,Yanai Elazar,"['üö®New paperüö® in the Negative Results workshop:\nThe Extraordinary Failure of Complement Coercion Crowdsourcing\n<LINK>\n\nWe discuss crowdsourcing (e.g. NLI) and disagreements revealed in specific linguistic phenomena\n\nwith Vika Basmov @ravogfel @yoavgo @rtsarfaty', 'We thought it would be a simple project, where we define a new task, collect some data and analyse some pre-trained models and how they perform on this new task.\n\nMore than a year later, we ended up writing a paper about how standards! methodologies for data collection...', 'for NLI and Missing Elements fall short, and produces disagreements between annotators on the phenomenon of Complement Coercion.', 'This came as a big surprise to use, since it seemed at first to be a simple task. Moreover, the many other works that collected data for NLI made us think it would be a piece of cake.\nWe were wrong!', 'We also draw connections to previous work that pointed on inherent disagreement in NLI (Pavlick and Kwiatkowski, 2019), but we show that they arise from different problems.', 'Writing this paper and the many discussions involving it enabled us to figure out what the problems were, which were only half baked hypotheses when looking at the data.\nSo kudos to @annargrs @JoaoSedoc and @arumshisky for organizing this workshop.', 'that is @ravfogel  who I misspelled üòÖ']",https://arxiv.org/abs/2010.05971,"Crowdsourcing has eased and scaled up the collection of linguistic annotation in recent years. In this work, we follow known methodologies of collecting labeled data for the complement coercion phenomenon. These are constructions with an implied action -- e.g., ""I started a new book I bought last week"", where the implied action is reading. We aim to collect annotated data for this phenomenon by reducing it to either of two known tasks: Explicit Completion and Natural Language Inference. However, in both cases, crowdsourcing resulted in low agreement scores, even though we followed the same methodologies as in previous work. Why does the same process fail to yield high agreement scores? We specify our modeling schemes, highlight the differences with previous work and provide some insights about the task and possible explanations for the failure. We conclude that specific phenomena require tailored solutions, not only in specialized algorithms, but also in data collection methods. ",The Extraordinary Failure of Complement Coercion Crowdsourcing
261,1316215116753301505,892993855670431744,Xikun Zhang,"['Do Language Embeddings Capture Scales?\n\ne.g. Can embeddings learned from pretrained language models predict the price of a ring or the weight of an elephant?\n\nNew paper available at <LINK> which was done during my internship at @GoogleAI! To be published ... <LINK>', 'at Findings of #emnlp2020 and to be presented at #BlackboxNLP. Done together with my wonderful collaborators Deepak Ramachandran, @iftenney, @yanaiela  and @dannydanr! @yanaiela has a nice brief summary about the paper above! üòÑ']",https://arxiv.org/abs/2010.05345,"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results. ",Do Language Embeddings Capture Scales?
262,1316094139877019649,899968956253044737,Yanai Elazar,"[""Do Language Embeddings Capture Scales?\nWell, they sort of do, but we're not there yet.\n\nNew paper at findings of #emnlp2020 (to be presented at #BlackboxNLP ) by @xikun_zhang_, Deepak Ramachandran, @iftenney, myself and @dannydanr \n\n<LINK> <LINK>"", ""We employ a new probing technique - Scalar Probing - that predicts a distribution of some object's property from the embeddings.\n\nWe use DoQ (https://t.co/5FDiXRDOGW) as a source for the distributions and use a linear model to predict the median or the distribution."", 'We also trained a new BERT model where all numbers are converted to their scientific notation representation, which seemed to improve the probes. We hope to release this model soon on ü§ó.']",https://arxiv.org/abs/2010.05345,"Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense, and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance and show that a simple method of canonicalizing numbers can have a significant effect on the results. ",Do Language Embeddings Capture Scales?
263,1315670167012204549,409006348,Shrimai,"['New paper ""Case Study: Deontological Ethics in NLP"" with Brendon Boldt, @rsalakhu &amp; Alan Black. Takeaways: 1) use ethical frameworks &amp; apply principles to NLP systems like case studies discussed, 2) explore directions in paper to improve NLP systems.\nLink: <LINK>']",https://arxiv.org/abs/2010.04658,"Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems. ",Case Study: Deontological Ethics in NLP
264,1314113611858665472,19626509,Gustavo Penha,"[""We find slices of data üî™ for which neural ranking models underperform and improve them with Slice-Aware Neural Ranking in our new paper to appear on the EMNLP workshop SCAI'20 ! (1/3)\n\n@scai_workshop #EMNLP #SCAI\n<LINK>"", 'We first define slicing functions for IR tasks to identify data for which the model might have low effectiveness and then we adapt SRAMs (https://t.co/e6wHBQ7bqP) from @SnorkelAI to train BERT for ranking with representations that are slice-aware. (2/3)', 'We find that slice-aware models helps on 3 ranking tasks. However, using random SFs also improves the effectiveness of the model, which indicates that slice-based learning gains could be also attributed to its ensemble learning effect and to having more parameters. (3/3)']",https://arxiv.org/abs/2010.03343,"Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response candidates) for which a neural ranker is ineffective and (ii) improving neural ranking for such instances. To address both challenges we resort to slice-based learning for which the goal is to improve effectiveness of neural models for slices (subsets) of data. We address challenge (i) by proposing different slicing functions (SFs) that select slices of the dataset---based on prior work we heuristically capture different failures of neural rankers. Then, for challenge (ii) we adapt a neural ranking model to learn slice-aware representations, i.e. the adapted model learns to represent the question and responses differently based on the model's prediction of which slices they belong to. Our experimental results (the source code and data are available at this https URL) across three different ranking tasks and four corpora show that slice-based learning improves the effectiveness by an average of 2% over a neural ranker that is not slice-aware. ",Slice-Aware Neural Ranking
265,1313683388524421120,2672671078,Katharina Kannüá∫üá¶üá™üá∫,"['Poetry around the city\nOpera the poet sings\nEssay on man epistle by\nTranslated kings.\n\n...from our neural poetüñäÔ∏è for acrostic poem generation in English, presented in our new #emnlp2020 paper, which is now on arXiv: <LINK>']",https://arxiv.org/abs/2010.02239,"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem's semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance. ",Acrostic Poem Generation
266,1313644071294955520,762359343656361984,James Zou,"['Telehealth is rapidly growing (esp. due to #COVID19), but poor quality image is a critical challenge. \n\nWe propose a #AI approach to identify photos that are subpar for clinical use and help guide patients to take better photos. New #psb21 paper <LINK> <LINK>', 'Super work by Kailas, @RoxanaDaneshjou @RobNovoaMD @AChiouMD and Justin Ko!']",https://arxiv.org/abs/2010.02086,"Telehealth is an increasingly critical component of the health care ecosystem, especially due to the COVID-19 pandemic. Rapid adoption of telehealth has exposed limitations in the existing infrastructure. In this paper, we study and highlight photo quality as a major challenge in the telehealth workflow. We focus on teledermatology, where photo quality is particularly important; the framework proposed here can be generalized to other health domains. For telemedicine, dermatologists request that patients submit images of their lesions for assessment. However, these images are often of insufficient quality to make a clinical diagnosis since patients do not have experience taking clinical photos. A clinician has to manually triage poor quality images and request new images to be submitted, leading to wasted time for both the clinician and the patient. We propose an automated image assessment machine learning pipeline, TrueImage, to detect poor quality dermatology photos and to guide patients in taking better photos. Our experiments indicate that TrueImage can reject 50% of the sub-par quality images, while retaining 80% of good quality images patients send in, despite heterogeneity and limitations in the training data. These promising results suggest that our solution is feasible and can improve the quality of teledermatology care. ","TrueImage: A Machine Learning Algorithm to Improve the Quality of
  Telehealth Photos"
267,1313642365631037443,1116002690604130305,Juliette Becker,"[""See our new paper (led by Tali Khain, now a first year grad at Chicago) on how TNOs move between Planet Nine resonances in the solar system w/ P9: <LINK> This is the final part of Tali's work which won her the 2019 @APSphysics Apker Award! <LINK>"", ""Tali's website: https://t.co/TILUDG24md As an undergrad, Tali led FOUR first author papers (working with me, Fred Adams, @kbatygin, and @dAArkEnergy). I hear she has some exciting results coming from her grad school work, so keep your eyes peeled for that!""]",https://arxiv.org/abs/2010.02234,"The observed physical clustering of the orbits of small bodies in the distant Kuiper Belt (TNOs) has recently prompted the prediction of an additional planet in the outer solar system. Since the initial posing of the hypothesis, the effects of Planet Nine on the dynamics of the main cluster of TNOs - the objects anti-aligned with its orbit - have been well-studied. In particular, numerical simulations have revealed a fascinating phenomenon, referred to as ""resonance hopping"", in which these objects abruptly transition between different mean-motion commensurabilities with Planet Nine. In this work, we explore this effect in greater detail, with the goal of understanding what mechanism prompts the hopping events to occur. In the process, we elucidate the often underestimated role of Neptune scattering interactions, which leads to diffusion in the semi-major axes of these distant TNOs. In addition, we demonstrate that although some resonant interactions with Planet Nine do occur, the anti-aligned objects are able to survive without the resonances, confirming that the dynamics of the TNOs are predominantly driven by secular, rather than resonant, interactions with Planet Nine. ",The Resonance Hopping Effect in the Neptune-Planet Nine System
268,1313507579310551041,1152338625654226944,Megan Mansfield,"[""New paper on the arXiv today! Let's talk about eclipse mapping. <LINK>\n\n#scicomm <LINK>"", 'Before I even start, this was a group effort and it would never have happened without all my lovely colleagues: Everett Schlawin, Jake Lustig-Yaeger, Arthur Adams, @astronemly, Jacob Arcangeli, @cloudfreekat, Prashansa Gupta, Dylan Keating, @kevinbstevenson, Thomas Beatty.', 'Crash course on eclipse mapping: by observing secondary eclipse ingress/egress, when the planet is slowly disappearing/appearing behind the star, we can construct a 2D map of the dayside of a planet. https://t.co/H77sVdJfj0', 'With JWST, we can make ~spectroscopic~ eclipse mapping observations. And, since observing at different wavelengths = observing at different pressures/altitudes in a planet, this means 3D DAYSIDE MAPS https://t.co/FxcHesjsSY', ""But there's degeneracies in the info we get from these maps, since we are actually just observing flux over time but we want to determine flux over latitude and longitude. How to do it? Enter the Eigenspectra Mapping Method https://t.co/MCt7UFqjrM"", ""So there's a LOT of math in this paper, but the basic idea is we use a method developed by @astronemly (Rauscher+18) to make 2D maps, then stack them together to create a 3D spatial+spectral map. https://t.co/y91BxSs0ax"", 'We use a clustering algorithm to look for patterns on this 3D map. This lets us identify a few spectra that show all the variability across the map. So you can see how temperature/composition changes over the dayside, but only have to analyze a couple high-precision spectra. https://t.co/1NafH8H4fR', 'So how does it work? Pretty well! We can identify large-scale features and flux gradients on dayside maps. https://t.co/YI6uYWrSlc', 'This method has 2 big applications:\n1. To get a first look at eclipse mapping data and identify large-scale features without assuming your observations will match your GCM predictions.\n2. To plan what precision of observations you need to observe certain features in your models.', 'Finally, we want the whole JWST observer community to be able to use this tool! GitHub repo currently in development, with a planned release of a user-friendly version in the next couple of months. https://t.co/AFKW3xAhUq', 'PS I skipped a lot of the meat of this paper (bc who wants to read a tweet about PCA?) but if you want to know more just email/message me!', '@_astronomay Thanks, me too!']",https://arxiv.org/abs/2010.02197,"Planetary atmospheres are inherently 3D objects that can have strong gradients in latitude, longitude, and altitude. Secondary eclipse mapping is a powerful way to map the 3D distribution of the atmosphere, but the data can have large correlations and errors in the presence of photon and instrument noise. We develop a technique to mitigate the large uncertainties of eclipse maps by identifying a small number of dominant spectra to make them more tractable for individual analysis via atmospheric retrieval. We use the eigencurves method to infer a multi-wavelength map of a planet from spectroscopic secondary eclipse light curves. We then apply a clustering algorithm to the planet map to identify several regions with similar emergent spectra. We combine the similar spectra together to construct an ""eigenspectrum"" for each distinct region on the planetary map. We demonstrate how this approach could be used to isolate hot from cold regions and/or regions with different chemical compositions in observations of hot Jupiters with the James Webb Space Telescope (JWST). We find that our method struggles to identify sharp edges in maps with sudden discontinuities, but generally can be used as a first step before a more physically motivated modeling approach to determine the primary features observed on the planet. ","Eigenspectra: A Framework for Identifying Spectra from 3D Eclipse
  Mapping"
269,1313420178605117440,165610171,Jorge Villa V√©lez,['A new #VESTIGE paper lead by Alessia Longobardi from @LAM_Marseille is out today on @arxiv showing results on how gas and dust are perturbed by cluster environment in a similar way (outside-in stripping of the galaxy ISM). Here is the link <LINK> #VESTIGEVIII <LINK>'],https://arxiv.org/abs/2010.02202,"We measure FIR emission from tails of stripped dust following the ionised and atomic gas components in galaxies undergoing ram pressure stripping. We study the dust-to-gas relative distribution and mass ratio in the stripped interstellar medium and relate them to those of the intra-cluster medium, thus linking the cluster-ICM-galaxy evolution at small-scales. The galaxy sample consists of three Scd Virgo galaxies with stellar masses in the range $10^9\lesssim \mathrm{M_{*}} \lesssim 10^{10}\, \mathrm{M_{\odot}}$, and within 1 Mpc from the cluster centre, namely NGC 4330, NGC 4522, and NGC 4654. Through the analysis of VESTIGE H$\alpha$, $Herschel$ SPIRE far-infrared, and VIVA HI data, we trace the spatial distribution of the tails and infer the dust and gas masses from the measured far-infrared 250 $\mu$m and HI flux densities. Dust-to-gas mass ratios (DGRs) in the tails are analysed as a function of the galaxy mass, metallicity, and dust temperature. Along the stripped component, the dust distribution closely follows the HI and H$\alpha$ emitting gas, all extending beyond the optical disc. In these regions, the DGRs are $2.0\pm0.6\times10^{-3}$, $0.7\pm0.1\times10^{-3}$, and $0.4\pm0.03\times10^{-3}$, for NGC 4330, NGC 4522, and NGC 4654, respectively, i.e. up to a factor of 15 less than the values measured in the main body of nearby galaxies. We also find a negative trend in the DGR as a function of the metallicity that can be explained in terms of a dust component more centrally concentrated in more metal-rich systems. Together with the finding that the stripped dust is cold, $T_{d} \lesssim 25\, K$, our results support an outside-in stripping scenario of the galaxy interstellar medium. This study shows that ram pressure stripping is a key mechanism in the building up of the Virgo intra-cluster component injecting dust grains into the ICM, thus contributing to its metal enrichment. ","A Virgo Environmental Survey Tracing Ionised Gas Emission. VESTIGE VIII.
  Bridging the cluster-ICM-galaxy evolution at small scales"
270,1313389112947245057,4843640122,Juan Calder√≥n Bustillo,"['Excited to release my first paper with my new @IGFAE_HEP affiliation, together with @LaskyPaul and @EHThrane from @ARC_OzGRav @MonashAstro, on testing the simplicity of #blackholes using #gravitationalwaves.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.01857,"The ""no-hair"" theorem states that astrophysical black holes are fully characterised by just two numbers: their mass and spin. The gravitational-wave emission from a perturbed black-hole consists of a superposition of damped sinusoids, known as \textit{quasi-normal modes}. Quasi-normal modes are specified by three integers $(\ell,m,n)$: the $(\ell, m)$ integers describe the angular properties and $(n)$ specifies the (over)tone. If the no-hair theorem holds, the frequencies and damping times of quasi-normal modes are determined uniquely by the mass and spin of the black hole, while phases and amplitudes depend on the particular perturbation. Current tests of the no-hair theorem, attempt to identify these modes in a semi-agnostic way, without imposing priors on the source of the perturbation. This is usually known as \textit{black-hole spectroscopy}. Applying this framework to GW150914, the measurement of the first overtone led to the confirmation of the theorem to $20\%$ level. We show, however, that such semi-agnostic tests cannot provide strong evidence in favour of the no-hair theorem, even for extremely loud signals, given the increasing number of overtones (and free parameters) needed to fit the data. This can be solved by imposing prior assumptions on the origin of the perturbed black hole that can further constrain the explored parameters: in particular, our knowledge that the ringdown is sourced by a binary black hole merger. Applying this strategy to GW150914 we find a natural log Bayes factor of $\sim 6.5$ in favour of the Kerr nature of its remnant, indicating that the hairy object hypothesis is disfavoured with $<1:600$ with respect to the Kerr black-hole one. ","Black-hole spectroscopy, the no-hair theorem and GW150914: Kerr vs.
  Occam"
271,1312093703780134914,923231130383536128,Eduardo Fonseca,"['üîäHappy to announce FSD50K: the new open dataset of human-labeled sound events! Over 51k Freesound audio clips, totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology.\n\nPaper: <LINK>\nDataset: <LINK> <LINK>', 'Companion site (where you can explore FSD50K): https://t.co/Fh8ZVUhRA3 \n\nCode for baseline experiments (to be released soon): https://t.co/QbfEfvSnoG\n\nWe hope all these resources are useful for the community!\n@mtg_upf #DCASE #machinelistening', '@ETzinis Thanks! Super interested in sound separation approaches using FSD50K!! Happy to provide insight on the data. Congrats on the NeurIPS paper by the way!!']",https://arxiv.org/abs/2010.00475,"Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on over 2M tracks from YouTube videos and encompassing over 500 sound classes. However, AudioSet is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to YouTube videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research. ",FSD50K: An Open Dataset of Human-Labeled Sound Events
272,1327425773611876353,1096659448657932288,Yuki Kawana,"['My first paper in the Ph.D. course has been accepted to NeurIPS 2020. We propose a highly expressive 3D representation for primitive decomposition task, which simultaneously has differentiable explicit and implicit shape representations. <LINK> <LINK>']",https://arxiv.org/abs/2010.11248,"Reconstructing 3D objects from 2D images is a fundamental task in computer vision. Accurate structured reconstruction by parsimonious and semantic primitive representation further broadens its application. When reconstructing a target shape with multiple primitives, it is preferable that one can instantly access the union of basic properties of the shape such as collective volume and surface, treating the primitives as if they are one single shape. This becomes possible by primitive representation with unified implicit and explicit representations. However, primitive representations in current approaches do not satisfy all of the above requirements at the same time. To solve this problem, we propose a novel primitive representation named neural star domain (NSD) that learns primitive shapes in the star domain. We show that NSD is a universal approximator of the star domain and is not only parsimonious and semantic but also an implicit and explicit shape representation. We demonstrate that our approach outperforms existing methods in image reconstruction tasks, semantic capabilities, and speed and quality of sampling high-resolution meshes. ",Neural Star Domain as Primitive Representation
273,1323721757799403522,956622601,Piotr Piecuch,"['3 weeks ago, I tweeted about our study announcing a &gt;60% enhancement of ESPT as a result of replacing one-photon excitations by their isoenergetic two-photon counterparts. Thanks to useful comments and suggestions by JCP reviewers, we improved our paper: <LINK>. <LINK>']",https://arxiv.org/abs/2010.04323v2,"Two-photon excitation is an attractive means for controlling chemistry in both space and time. Isoenergetic one- and two-photon excitations (OPE and TPE) in non-centrosymmetric molecules are often assumed to reach the same excited state and, hence, to produce similar excited-state reactivity. We compare the solvent-to-solute excited-state proton transfer of the super photobase FR0-SB following isoenergetic OPE and TPE. We find up to 62 % increased reactivity following TPE compared to OPE. From steady-state spectroscopy, we rule out the involvement of different excited states and find that OPE and TPE spectra are identical in non-polar solvents but not in polar ones. We propose that differences in the matrix elements that contribute to the two-photon absorption cross sections lead to the observed enhanced isoenergetic reactivity, consistent with the predictions of our high-level coupled-cluster-based computational protocol. We find that polar solvent configurations favor greater dipole moment change between ground and excited states, which enters the probability for two-photon excitations as the absolute value squared. This, in turn, causes a difference in the Franck-Condon region reached via TPE compared to OPE. We conclude that a new method has been found for controlling chemical reactivity via the matrix elements that affect two-photon cross sections, which may be of great utility for spatial and temporal precision chemistry. ","] Isoenergetic Two-Photon Excitation Enhances Solvent-to-Solute
  Excited-State Proton Transfer"
274,1323130306333822977,857425243043905536,Jiaxuan You,"['In our #NeurIPS2020 paper, we propose GRAPE as a general framework for the missing data problem, including both feature imputation and label prediction aspects. Our key innovation is to solve the problem with graph representation. \nPaper: <LINK> <LINK>', 'Project website: https://t.co/ZQDJDuCXFq']",https://arxiv.org/abs/2010.16418,"Machine learning with missing data has been approached in two different ways, including feature imputation where missing feature values are estimated based on observed values, and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label prediction often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a graph-based framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using a graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph Neural Networks. Experimental results on nine benchmark datasets show that GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods. ",Handling Missing Data with Graph Representation Learning
275,1323078447787315200,983857052840636417,Rob Corless,"['For those interested in AI in math education, @rotmanphilo maybe, please find my new paper with @JamesHDavenport and @euniceyschan and others at <LINK> . We talk about our respective theory and practice; in some ways old-school, the paper is formed by experience.']",https://arxiv.org/abs/2010.16300,"Over the past thirty years or so the authors have been teaching various programming for mathematics courses at our respective Universities, as well as incorporating computer algebra and numerical computation into traditional mathematics courses. These activities are, in some important ways, natural precursors to the use of Artificial Intelligence in Mathematics Education. This paper reflects on some of our course designs and experiences and is therefore a mix of theory and practice. Underlying both is a clear recognition of the value of computer programming for mathematics education. We use this theory and practice to suggest good techniques for and to raise questions about the use of AI in Mathematics Education. ",Teaching Programming for Mathematical Scientists
276,1322322526811422721,461482865,Ivan Kryven,"['""Contact tracing in configuration models""\nNew paper with @clara_stegehuis out on arXiv: <LINK>\n\nWe study how configuration model for networks responds to an intervention with contact tracing. <LINK>']",https://arxiv.org/abs/2010.05590,"Quarantining and contact tracing are popular ad hoc practices for mitigating epidemic outbreaks. However, few mathematical theories are currently available to asses the role of a network in the effectiveness of these practices. In this paper, we study how the final size of an epidemic is influenced by the procedure that combines contact tracing and quarantining on a network null model: the configuration model. Namely, we suppose that infected vertices may self-quarantine and trace their infector with a given success probability. A traced infector is, in turn, less likely to infect others. We show that the effectiveness of such tracing process strongly depends on the network structure. In contrast to previous findings, the tracing procedure is not necessarily more effective on networks with heterogeneous degrees. We also show that network clustering influences the effectiveness of the tracing process in a non-trivial way: depending on the infectiousness parameter, contact tracing on clustered networks may either be more, or less efficient than on network without clustering. ",Contact tracing in configuration models
277,1322301181687857152,888216099757490176,Maithra Raghu,"['Do Wide and Deep neural networks Learn the Same Things? \nPaper: <LINK>\n\nWe study representational properties of neural networks with different depths and widths on CIFAR/ImageNet, with insights on model capacity effects, feature similarity &amp; characteristic errors <LINK>', ""@OriolVinyalsML My (maybe weak) excuse is that we'd like people to skim the paper (or read the summary thread!) to understand some of the nuances in the conclusions! üôÇ""]",https://arxiv.org/abs/2010.15327,"A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes. ","Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural
  Network Representations Vary with Width and Depth"
278,1322285065322196992,961763584651935744,Thao Nguyen,"['Do wide and deep neural networks learn the same thing? In a new paper (<LINK>) with @maithra_raghu and @skornblith we study how width and depth affect learned representations within and across models trained on CIFAR and ImageNet. 1/6', 'Scaling NNs by increasing depth &amp; width has been a successful approach for obtaining high performance across many tasks. To study representations of these models, we develop an efficient way to compute centered kernel alignment (CKA), a representation similarity measure. 2/6', 'We find that in large (wide and/or deep) models, a characteristic block structure emerges in the model layer representations. By varying training dataset size, we show that the block structure emerges when model capacity is large relative to the size of the training set. 3/6 https://t.co/KZBSzNPJG5', 'Through further analysis, we show that the block structure arises from the preservation and propagation of the first principal component of its layer representations, with clear ramifications for linear probe accuracy, and collapsing the constituent layers. 4/6 https://t.co/3RtrbtiEw2', 'Across models of different random initializations and configurations, representations outside the block structure are often similar, but the block structure is unique to each model. 5/6 https://t.co/zXAoaV9GgW', 'Finally, we study effects of depth and width on model outputs: on common datasets like CIFAR-10 and ImageNet, wide and deep models make systematically different mistakes at both instance level and class level. 6/6 https://t.co/AXyweyLyXy']",https://arxiv.org/abs/2010.15327,"A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes. ","Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural
  Network Representations Vary with Width and Depth"
279,1322241317519396867,1667135348,Alexander Terenin,['Mat√©rn Gaussian Processes on Graphs\n\nWe study extensions of the Mat√©rn family of kernels for weighted undirected graphs. Now on arXiv - blog post and tweets soon - stay tuned!\n\n<LINK>\n\n@mpd37 @NicolasDurrande <LINK>'],https://arxiv.org/abs/2010.15538,"Gaussian processes are a versatile framework for learning unknown functions in a manner that permits one to utilize prior information about their properties. Although many different Gaussian process models are readily available when the input space is Euclidean, the choice is much more limited for Gaussian processes whose input space is an undirected graph. In this work, we leverage the stochastic partial differential equation characterization of Mat\'ern Gaussian processes - a widely-used model class in the Euclidean setting - to study their analog for undirected graphs. We show that the resulting Gaussian processes inherit various attractive properties of their Euclidean and Riemannian analogs and provide techniques that allow them to be trained using standard methods, such as inducing points. This enables graph Mat\'ern Gaussian processes to be employed in mini-batch and non-conjugate settings, thereby making them more accessible to practitioners and easier to deploy within larger learning frameworks. ",Mat\'ern Gaussian Processes on Graphs
280,1321998629054406657,69202541,Jonathan Le Roux,"['N. Moritz, T. Hori &amp; I propose Graph-based Temporal Classification (GTC), generalization of CTC that can use a graph representation as training target. We apply it to semi-sup E2E ASR via self-training w/ WFST supervision from N-best list of pseudo-labels.\n<LINK> <LINK>']",https://arxiv.org/abs/2010.15653,"Semi-supervised learning has demonstrated promising results in automatic speech recognition (ASR) by self-training using a seed ASR model with pseudo-labels generated for unlabeled data. The effectiveness of this approach largely relies on the pseudo-label accuracy, for which typically only the 1-best ASR hypothesis is used. However, alternative ASR hypotheses of an N-best list can provide more accurate labels for an unlabeled speech utterance and also reflect uncertainties of the seed ASR model. In this paper, we propose a generalized form of the connectionist temporal classification (CTC) objective that accepts a graph representation of the training labels. The newly proposed graph-based temporal classification (GTC) objective is applied for self-training with WFST-based supervision, which is generated from an N-best list of pseudo-labels. In this setup, GTC is used to learn not only a temporal alignment, similarly to CTC, but also a label alignment to obtain the optimal pseudo-label sequence from the weighted graph. Results show that this approach can effectively exploit an N-best list of pseudo-labels with associated scores, considerably outperforming standard pseudo-labeling, with ASR results approaching an oracle experiment in which the best hypotheses of the N-best lists are selected manually. ","Semi-Supervised Speech Recognition via Graph-based Temporal
  Classification"
281,1321841252954824704,2279227387,Dr. Jessie Christiansen,"[""Breaking news - it's paper day!!! \n\nOur new eta-Earth calculation from the Kepler team ‚Äì punchline is that we find eta_Earth is 40-60% for the conservative habitable zone. \n\n<LINK>\n\nBUT WHAT DOES THAT MEAN??\n\nDon't worry, I gotchu. Thread follows of course. <LINK>"", 'First off, an Important Caveat ‚Äì this won‚Äôt be our final answer! There are still important corrections missing from this number. But! It‚Äôs the most careful job we‚Äôve done so far. \n\n(Reminder: that‚Äôs how science works!) https://t.co/i2O31qM9Yc', 'Updates in this work!!\n- Uniform stellar parameters from Gaia DR2, using Berger+2020.\n- Improvement probabilistic treatment of reliability\n- Analysis in instellation space, instead of period space (see the Kepler planet candidates in instellation space in the attached figure). https://t.co/RQaHSGgtYv', 'Now, that number. 40-60%!! That says around half of stars like our Sun (middle-aged yellow G stars) have a rocky planet like the Earth in their habitable zone ‚Äì the distance from the star where it‚Äôs not too hot, and not too cold ‚Äì it‚Äôs just right for liquid water on the surface*. https://t.co/P5abrJf7yC', ""*I‚Äôm going to re-introduce the ECHaLWOTS zone here. The habitable zone is truly an ‚Äòif you put the Earth here, it could maintain liquid water‚Äô zone. Not an ‚Äòany planet here could maintain liquid water‚Äô zone!!\n\nHere's @girlandkat explaining: https://t.co/LOfxapVfyL"", '*cough* K2-18 b *cough*', ""But! AROUND HALF?! That‚Äôs huge. That‚Äôs SO MANY potential Earth-like planets out there. \n\nThe Galaxy has ~10 billion stars like the Sun. That means there's something like around FIVE BILLION Earth-like planets just in our Galaxy.\n\nTHAT‚ÄôS WILD. https://t.co/1nI2K6nEZQ"", 'Another important aspect of this number, which is a somewhat higher than previous numbers (~20-50%) is that it could mean there are more nearby Earth-like planets for future large space-based telescopes (like HabEx or LUVOIR) to observe! Which is good.', 'So, to summarize: our new number is slightly higher than previous numbers, includes some new things, and is still missing some things.\n\nHooray! (?! üòÑ )', '@mrtommyb Depends which eta_Earth you take, and which assumptions you use about the habitable zone. Those are the conservative HZ numbers.', '@mrtommyb Steve twisted every knob in combination with every other knob to try to fully explore all the possible scenarios! So anyone can come to the paper with their preferred HZ and find the corresponding eta_Earth. ü§£)', ""@mrtommyb Yes, that's more correctly what they are, but in terms of trying to find a number I'm comfortable quoting from this paper, I'm happy to say 40-60%. ü§£\n\nNote for everyone else: Tom's point is that the 40-60% doesn't include the uncertainties on each of those numbers!"", 'Oh, and here is the official @NASAAmes press release!\n\nhttps://t.co/Xli9niNbTV', ""@david_kipping Yes, the uncertainties include zero. I would be very surprised if the answer WAS zero, given what we know about larger planets at the same instellation, and smaller planets at higher instellation, but it's not ruled out by the data. \n\nI don't know the Bayes factor!"", '@kesterallen Were we not before?!?', '@david_kipping For sure. There‚Äôs just so many numbers in the paper (multiple HZ formulations, multiple stellar samples) I wanted to condense them down a bit for digestion! I should have explained more about the uncertainties though.', '@ExoplanetJJ @NASAAmes Stellar multiplicity is one of the unresolved issues at the moment. The original stellar sample was built from stars that were photometrically single (to 1-2""). This new work uses the Gaia RUWE value to try to clean than up a bit, but it remains to be seen how many it misses.']",https://arxiv.org/abs/2010.14812,"We present occurrence rates for rocky planets in the habitable zones (HZ) of main-sequence dwarf stars based on the Kepler DR25 planet candidate catalog and Gaia-based stellar properties. We provide the first analysis in terms of star-dependent instellation flux, which allows us to track HZ planets. We define $\eta_\oplus$ as the HZ occurrence of planets with radius between 0.5 and 1.5 $R_\oplus$ orbiting stars with effective temperatures between 4800 K and 6300 K. We find that $\eta_\oplus$ for the conservative HZ is between $0.37^{+0.48}_{-0.21}$ (errors reflect 68\% credible intervals) and $0.60^{+0.90}_{-0.36}$ planets per star, while the optimistic HZ occurrence is between $0.58^{+0.73}_{-0.33}$ and $0.88^{+1.28}_{-0.51}$ planets per star. These bounds reflect two extreme assumptions about the extrapolation of completeness beyond orbital periods where DR25 completeness data are available. The large uncertainties are due to the small number of detected small HZ planets. We find similar occurrence rates using both a Poisson likelihood Bayesian analysis and Approximate Bayesian Computation. Our results are corrected for catalog completeness and reliability. Both completeness and the planet occurrence rate are dependent on stellar effective temperature. We also present occurrence rates for various stellar populations and planet size ranges. We estimate with $95\%$ confidence that, on average, the nearest HZ planet around G and K dwarfs is about 6 pc away, and there are about 4 HZ rocky planets around G and K dwarfs within 10 pc of the Sun. ","The Occurrence of Rocky Habitable Zone Planets Around Solar-Like Stars
  from Kepler Data"
282,1321747934740910081,883039700,Lenka Zdeborova,"['What happens when you hide a Hamiltonian cycle in a random graph, can we find it back? New paper with Gabriele Sicuro brings some answers to this question:   <LINK>', '@biodataclarity In fact, the problem is inspired by DNA assembly as explained in this work: \nhttps://t.co/6y8i8qx0fs', '@biodataclarity In fact, the problem is inspired by DNA assembly as explained in this work: https://t.co/6y8i8qx0fs']",https://arxiv.org/abs/2010.13700,"We consider the problem of recovering an unknown $k$-factor, hidden in a weighted random graph. For $k=1$ this is the planted matching problem, while the $k=2$ case is closely related to the planted travelling salesman problem. The inference problem is solved by exploiting the information arising from the use of two different distributions for the weights on the edges inside and outside the planted sub-graph. We argue that, in the large size limit, a phase transition can appear between a full and a partial recovery phase as function of the signal-to-noise ratio. We give a criterion for the location of the transition. ",The planted $k$-factor problem
283,1321562418565140480,1055973579467059200,Rajsekhar Mohapatra,"['Based on our new 100 high-resolution simulations on stratified turbulence, we propose scaling relations between density and pressure fluctuations with turbulent velocities for weakly and strongly stratified turbulence. Check it out here: <LINK>. <LINK>']",https://arxiv.org/abs/2010.12602,"Turbulent gas motions are observed in the intracluster medium (ICM). The ICM is density-stratified, with the gas density being highest at the centre of the cluster and decreasing radially outwards. As a result of this, Kolmogorov (homogeneous, isotropic) turbulence theory does not apply to the ICM. The gas motions are instead explained by anisotropic stratified turbulence, with the stratification quantified by the perpendicular Froude number ($\mathrm{Fr}_\perp$). These turbulent motions are associated with density and pressure fluctuations, which manifest as perturbations in X-ray surface brightness maps of the ICM and as thermal Sunyaev-Zeldovich effect (SZ) fluctuations, respectively. In order to advance our understanding of the relations between these fluctuations and the turbulent gas velocities, we have conducted 100 high-resolution hydrodynamic simulations of stratified turbulence ($256^2\times 384$ -- $1024^2\times1536$ resolution elements), in which we scan the parameter space of subsonic rms Mach number ($\mathcal{M}$), $\mathrm{Fr}_\perp$, and the ratio of entropy and pressure scale heights ($R_{PS}=H_P/H_S$), relevant to the ICM. We develop a new scaling relation between the standard deviation of logarithmic density fluctuations ($\sigma_s$, where $s=\ln(\rho/\left<\rho\right>)$), $\mathcal{M}$, and $\mathrm{Fr}_{\perp}$, valid till $\mathrm{Fr}_\perp\ll1$:~$\sigma_s^2=\ln\left(1+b^2\mathcal{M}^4+0.10/(\mathrm{Fr}_\perp+0.25/\sqrt{\mathrm{Fr}_\perp})^2\mathcal{M}^2R_{PS}\right)$, where $b\sim1/3$ for solenoidal turbulence driving studied here. We further find that logarithmic pressure fluctuations $\sigma_{(\ln{P}/\left<P\right>)}$ are independent of stratification and scale according to the relation $\sigma_{(\ln{\bar{P}})}^2=\ln\left(1+b^2\gamma^2\mathcal{M}^4\right)$, where $\bar{P}=P/\left<P\right>$ and $\gamma$ is the adiabatic index of the gas. ","Turbulent density and pressure fluctuations in the stratified
  intracluster medium"
284,1321444476250918912,978500233368760320,Biao Zhang,"['[1/2] Happy to share our #WMT2020 paper ""Fast Interleaved Bidirectional Sequence Generation"". We propose IBDecoder that accelerates decoding by ~2x-6x with almost no quality loss. \n\nPaper: <LINK>\nCode: <LINK>\n\nw/ @iatitov  and @RicoSennrich <LINK>', '[2/2] IBDecoder combines bidirectional generation with semi-autoregressive modeling, where we show words from the left and right directions are loosely dependent. IBDecoder is compatible with shallow decoder students. With some quality loss, IBDecoder yields 4x-11x speedup! https://t.co/Oj1qS3AvNb']",https://arxiv.org/abs/2010.14481,"Independence assumptions during sequence generation can speed up inference, but parallel generation of highly inter-dependent tokens comes at a cost in quality. Instead of assuming independence between neighbouring tokens (semi-autoregressive decoding, SA), we take inspiration from bidirectional sequence generation and introduce a decoder that generates target words from the left-to-right and right-to-left directions simultaneously. We show that we can easily convert a standard architecture for unidirectional decoding into a bidirectional decoder by simply interleaving the two directions and adapting the word positions and self-attention masks. Our interleaved bidirectional decoder (IBDecoder) retains the model simplicity and training efficiency of the standard Transformer, and on five machine translation tasks and two document summarization tasks, achieves a decoding speedup of ~2X compared to autoregressive decoding with comparable quality. Notably, it outperforms left-to-right SA because the independence assumptions in IBDecoder are more felicitous. To achieve even higher speedups, we explore hybrid models where we either simultaneously predict multiple neighbouring tokens per direction, or perform multi-directional decoding by partitioning the target sequence. These methods achieve speedups to 4X-11X across different tasks at the cost of <1 BLEU or <0.5 ROUGE (on average). Source code is released at this https URL ",Fast Interleaved Bidirectional Sequence Generation
285,1321443840448958465,2733642475,Soheil Feizi,"['How well deep learning interpretation (saliency) methods work in ""time series predictions""? We study this question in our #NeurIPS2020  paper by developing a benchmark: <LINK>\nCode/data: <LINK>\nwith @asalam_91, M. Gunady and @hcorrada  1/3 <LINK>', 'We show that (i) in general, network architectures and saliency methods fail to accurately identify feature importance over time, and (ii) this failure is mainly due to the conflation of time and feature domains 2/3 https://t.co/fnwEOOBnUI', 'We propose a two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step. This approach improves the quality of saliency maps in time-series predictions 3/3 https://t.co/jVcA5D4dMv']",https://arxiv.org/abs/2010.13924,"Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step. ",Benchmarking Deep Learning Interpretability in Time Series Predictions
286,1321409303178317824,2647394298,Martin Degeling (@mrtn3000@chaos.social),"['How do #privacy factors influence the willingness to use apps to help fight #covid19. We studied differences between US, Germany and China. \n<LINK> \n\nJoint work with @christine_utz S. Becker @the0retisch F. Farke @FranziskaPiaH M. D√ºrmuth and L. Schaewitz <LINK>', 'We presented ~1000 participants in each countrie with ten different scenarios of apps for purposes like symptom checking, quarantine enforcement, contact tracing, and others. We varied factors in line with #contextualintegrity @privaci_way https://t.co/yRKyYr6QGt', 'Overall, the willingness to use any app was highest in China, followed by Germany. US participants are least likely to install an App, although they have similar expectations of others to use it.', 'Contact Tracing Apps are favored across all countries regardless of any other factors, while apps that enforce quarantines are least wanted. https://t.co/4XEhCZf2yd', 'We used cumulative link mixed models to understand how other factors influence the adoption. Overall we found that privacy-friendly apps are more likely to be installed and used. https://t.co/JULkROy7Nv', 'Interestingly, participants are more likely to use apps that have positive effects for the general public over individual advantages.', 'In free-text answers, many participants, especially in Germany and the US, raised concerns about data privacy and government surveillance. https://t.co/rd1eZN8HwY', 'Nevertheless, in all countries, there is a group of 15 to 21 % that are unlikely ever to install any app.', 'All details in the preprint ""Apps Against the Spread: Privacy Implications and User Acceptance of COVID-19-Related Smartphone Apps on Three Continents"" https://t.co/iON5a0642R']",https://arxiv.org/abs/2010.14245,"The COVID-19 pandemic has fueled the development of smartphone applications to assist disease management. Many ""corona apps"" require widespread adoption to be effective, which has sparked public debates about the privacy, security, and societal implications of government-backed health applications. We conducted a representative online study in Germany (n = 1,003), the US (n = 1,003), and China (n = 1,019) to investigate user acceptance of corona apps, using a vignette design based on the contextual integrity framework. We explored apps for contact tracing, symptom checks, quarantine enforcement, health certificates, and mere information. Our results provide insights into data processing practices that foster adoption and reveal significant differences between countries, with user acceptance being highest in China and lowest in the US. Chinese participants prefer the collection of personalized data, while German and US participants favor anonymity. Across countries, contact tracing is viewed more positively than quarantine enforcement, and technical malfunctions negatively impact user acceptance. ","Apps Against the Spread: Privacy Implications and User Acceptance of
  COVID-19-Related Smartphone Apps on Three Continents"
287,1321399158037749760,718084071,Denis Erkal,"['Paper day! We find evidence that the inner Milky Way is sloshing about with respect to the outer stellar halo (&gt;50 kpc) due to the LMC.\n\nWe consider 492 stars in the outer stellar halo with measured radial velocities and see a clear dipole on the sky. 1/n\n\n<LINK> <LINK>', 'The stars in the North are redshifted on average and the stars in the South are blueshifted on average, consistent with us moving downwards at ~30 km/s with respect to the outer stellar halo. This matches the predicted effect of the LMC. 2/n https://t.co/fcBm7prClO', 'The basic picture is that as the LMC falls in, it pulls on the Milky Way. Stars in the inner ~30 kpc have a short enough orbital period that they respond adiabatically while stars beyond this roughly stand still. As a result, the inner MW moves with respect to the outer halo. 3/n', 'The measured radial velocity signal is consistent with a broad range of LMC masses, including those we measured with tidal streams around the Milky Way. 4/n https://t.co/oZuaJxhu6D', ""We also took a gander at the proper motions of these stars with Gaia DR2. The measurements are broadly consistent although since the predicted effect is so small, it's difficult to tell. Hopefully with Gaia EDR3 we can measure this more precisely. 5/n https://t.co/m2IAJ8JU4K"", 'This means that the outskirts of our Galaxy are really out of equilibrium which must be accounted for when modelling any tracers out there. In addition, it means that the inner Milky Way is not an inertial reference frame but instead has been substantially accelerated. 6/n', ""See https://t.co/OeNz3cub98 https://t.co/VA1DcSYsJc\nhttps://t.co/SMjZbnuemy\nfor more details on the mechanism of the LMC's effect"", ""@iHinkthere4iam Good question. We didn't try to fit this direction since the predicted effect on the radial velocity isn't just a dipole. Given that we see the signal over a lot of the sky, I'd guess the direction could be measured with an uncertainty of ~10s of degrees even with current data."", ""@iHinkthere4iam A very precise measure of the orientation could perhaps tell us something about the dark matter around the LMC (which would include SMC material) but I'd guess we would need a much bigger sample to learn about such subtle features.""]",https://arxiv.org/abs/2010.13789,"A wealth of recent studies have shown that the LMC is likely massive, with a halo mass $>10^{11} M_\odot$. One consequence of having such a nearby and massive neighbour is that the inner Milky Way is expected to be accelerated with respect to our Galaxy's outskirts (beyond $\sim 30$ kpc). In this work we compile a sample of $\sim 500$ stars with radial velocities in the distant stellar halo, $r_{\rm GC}> 50$ kpc, to test this hypothesis. These stars span a large fraction of the sky and thus give a global view of the stellar halo. We find that stars in the Southern hemisphere are on average blueshifted, while stars in the North are redshifted, consistent with the expected, mostly downwards acceleration of the inner halo due to the LMC. We compare these results with simulations and find the signal is consistent with the infall of a $1.5\times10^{11} M_\odot$ LMC. We cross-match our stellar sample with \textit{Gaia} DR2 and find that the mean proper motions are not yet precise enough to discern the LMC's effect. Our results show that the outer Milky Way is significantly out of equilibrium and that the LMC has a substantial effect on our Galaxy. ",Detection of the LMC-induced sloshing of the Galactic halo
288,1321394065380937732,1212606587644178432,Romero-Isart Group,"['How does a gold nanoparticle become magnetized? In <LINK> we show that at low temperatures it does so in a funky way due to quantum effects. We propose how to measure it with nanoSQUIDS. @iqoqi @uniinnsbruck <LINK>', 'Work done in collaboration with Pepa Mart√≠nez-P√©rez and Fernando Luis from Instituto de (Nano)Ciencia de Materiales de Aragon @AragonCsic and @ARAID_ES']",https://arxiv.org/abs/2010.14370,"We theoretically study quantum size effects in the magnetic response of a spherical metallic nanoparticle (e.g. gold). Using the Jellium model in spherical coordinates, we compute the induced magnetic moment and the magnetic susceptibility for a nanoparticle in the presence of a static external magnetic field. Below a critical magnetic field the magnetic response is diamagnetic, whereas above such field the magnetization is characterized by sharp, step-like increases of several tenths of Bohr magnetons, associated with the Zeeman crossing of energy levels above and below the Fermi sea. We quantify the robustness of these regimes against thermal excitations and finite linewidth of the electronic levels. Finally, we propose two methods for experimental detection of the quantum size effects based on the coupling to superconducting quantum interference devices. ","Quantum Size Effects in the Magnetic Susceptibility of a Metallic
  Nanoparticle"
289,1321179343683465228,948010010,Antonis Anastasopoulos,"[""Preprint of our Findings of EMNLP paper, where we study negation and whether it's properly handled by machine translation systems.\n\nPaper: <LINK>\n\nSummary thread: üëá <LINK>""]",https://arxiv.org/abs/2010.05432,"As machine translation (MT) systems progress at a rapid pace, questions of their adequacy linger. In this study we focus on negation, a universal, core property of human language that significantly affects the semantics of an utterance. We investigate whether translating negation is an issue for modern MT systems using 17 translation directions as test bed. Through thorough analysis, we find that indeed the presence of negation can significantly impact downstream quality, in some cases resulting in quality reductions of more than 60%. We also provide a linguistically motivated analysis that directly explains the majority of our findings. We release our annotations and code to replicate our analysis here: this https URL ","It's not a Non-Issue: Negation as a Source of Error in Machine
  Translation"
290,1321101116357124096,151193108,Mert R. Sabuncu üá∫üá¶,"['To achieve high accuracy in some binary biomedical segmentation tasks can be very hard, as the background can contain objects that are very similar to the structure of interest. We propose an ensembling strategy that can help with this: <LINK>']",https://arxiv.org/abs/2010.08648,"Segmentation of anatomical regions of interest such as vessels or small lesions in medical images is still a difficult problem that is often tackled with manual input by an expert. One of the major challenges for this task is that the appearance of foreground (positive) regions can be similar to background (negative) regions. As a result, many automatic segmentation algorithms tend to exhibit asymmetric errors, typically producing more false positives than false negatives. In this paper, we aim to leverage this asymmetry and train a diverse ensemble of models with very high recall, while sacrificing their precision. Our core idea is straightforward: A diverse ensemble of low precision and high recall models are likely to make different false positive errors (classifying background as foreground in different parts of the image), but the true positives will tend to be consistent. Thus, in aggregate the false positive errors will cancel out, yielding high performance for the ensemble. Our strategy is general and can be applied with any segmentation model. In three different applications (carotid artery segmentation in a neck CT angiography, myocardium segmentation in a cardiovascular MRI and multiple sclerosis lesion segmentation in a brain MRI), we show how the proposed approach can significantly boost the performance of a baseline segmentation method. ",Ensembling Low Precision Models for Binary Biomedical Image Segmentation
291,1321085882825379843,1219928743944343553,Neil Zeghidour,"['Large scale training of EEG classifiers requires exploiting many, small datasets. Problem: they are collected with different headsets (# and place of electrodes). We propose CHARM, a module that remaps arbitrary EEG inputs to a canonical placement 1/3\n<LINK> <LINK>', 'CHARM is trainable and compatible with archs (e.g. CNNs) that expect consistent channels (same number, same order). Across different noising scenarios we show its robustness. Moreover, we successfully perform transfer learning between datasets collected w/ different headsets! 2/3 https://t.co/iJPQAnBOXe', 'Work done by @GoogleAI intern @aaqib_saeed, with @GrangierDavid and Olivier Pietquin. 3/3']",https://arxiv.org/abs/2010.13694,"We propose CHARM, a method for training a single neural network across inconsistent input channels. Our work is motivated by Electroencephalography (EEG), where data collection protocols from different headsets result in varying channel ordering and number, which limits the feasibility of transferring trained systems across datasets. Our approach builds upon attention mechanisms to estimate a latent reordering matrix from each input signal and map input channels to a canonical order. CHARM is differentiable and can be composed further with architectures expecting a consistent channel ordering to build end-to-end trainable classifiers. We perform experiments on four EEG classification datasets and demonstrate the efficacy of CHARM via simulated shuffling and masking of input channels. Moreover, our method improves the transfer of pre-trained representations between datasets collected with different protocols. ","Learning from Heterogeneous EEG Signals with Differentiable Channel
  Reordering"
292,1321004483988623360,1085279811692621825,Luca Matr√†,"['And in another great work led by @Sebastromarino today, we find yet another radially wide planetesimal belt with a ‚≠êÔ∏èGAP‚≠êÔ∏è around HD206893, hinting at a third companion orbiting the star at 74 au outer to the brown dwarfs HD206893B and C! What a system!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.12582,"Radial substructure in the form of rings and gaps has been shown to be ubiquitous among protoplanetary discs. This could be the case in exoKuiper belts as well, and evidence for this is emerging. In this paper we present ALMA observations of the debris/planetesimal disc surrounding HD 206893, a system that also hosts two massive companions at 2 and 11 au. Our observations reveal a disc extending from 30 to 180 au, split by a 27 au wide gap centred at 74 au, and no dust surrounding the reddened brown dwarf (BD) at 11 au. The gap width suggests the presence of a 0.9 M$_\mathrm{Jup}$ planet at 74 au, which would be the third companion in this system. Using previous astrometry of the BD, combined with our derived disc orientation as a prior, we were able to better constrain its orbit finding it is likely eccentric ($0.14^{+0.05}_{-0.04}$). For the innermost companion, we used RV, proper motion anomaly and stability considerations to show its mass and semi-major axis are likely in the range 4-100 M$_\mathrm{Jup}$ and 1.4-4.5 au. These three companions will interact on secular timescales and perturb the orbits of planetesimals, stirring the disc and potentially truncating it to its current extent via secular resonances. Finally, the presence of a gap in this system adds to the growing evidence that gaps could be common in wide exoKuiper belts. Out of 6 wide debris discs observed with ALMA with enough resolution, 4-5 show radial substructure in the form of gaps. ",Insights into the planetary dynamics of HD 206893 with ALMA
293,1320820554321055746,3524520857,Piotr ≈ªelasko,"['In our new study, we aim to gain some insights into the limitations of zero-shot ASR transfer to an unknown language. This work was done together with @SFeng9, Laureano Moro, Ali Abavisani, @OScharenborg, @hasegawajohnson, and Najim Dehak.\n\nüîó<LINK> <LINK>']",https://arxiv.org/abs/2010.12104,"The idea of combining multiple languages' recordings to train a single automatic speech recognition (ASR) model brings the promise of the emergence of universal speech representation. Recently, a Transformer encoder-decoder model has been shown to leverage multilingual data well in IPA transcriptions of languages presented during training. However, the representations it learned were not successful in zero-shot transfer to unseen languages. Because that model lacks an explicit factorization of the acoustic model (AM) and language model (LM), it is unclear to what degree the performance suffered from differences in pronunciation or the mismatch in phonotactics. To gain more insight into the factors limiting zero-shot ASR transfer, we replace the encoder-decoder with a hybrid ASR system consisting of a separate AM and LM. Then, we perform an extensive evaluation of monolingual, multilingual, and crosslingual (zero-shot) acoustic and language models on a set of 13 phonetically diverse languages. We show that the gain from modeling crosslingual phonotactics is limited, and imposing a too strong model can hurt the zero-shot transfer. Furthermore, we find that a multilingual LM hurts a multilingual ASR system's performance, and retaining only the target language's phonotactic data in LM training is preferable. ",How Phonotactics Affect Multilingual and Zero-shot ASR Performance
294,1320561969796161536,2506570218,Julian,['We recently open-sourced SMARTS (<LINK>); a simulation platform for RL and multi-agent research on autonomous driving. Focus is on realistic + diverse interactions. The associated paper (<LINK>) was accepted to CoRL. Hope you find it interesting! <LINK>'],https://arxiv.org/abs/2010.09776,"Multi-agent interaction is a fundamental aspect of autonomous driving in the real world. Despite more than a decade of research and development, the problem of how to competently interact with diverse road users in diverse scenarios remains largely unsolved. Learning methods have much to offer towards solving this problem. But they require a realistic multi-agent simulator that generates diverse and competent driving interactions. To meet this need, we develop a dedicated simulation platform called SMARTS (Scalable Multi-Agent RL Training School). SMARTS supports the training, accumulation, and use of diverse behavior models of road users. These are in turn used to create increasingly more realistic and diverse interactions that enable deeper and broader research on multi-agent interaction. In this paper, we describe the design goals of SMARTS, explain its basic architecture and its key features, and illustrate its use through concrete multi-agent experiments on interactive scenarios. We open-source the SMARTS platform and the associated benchmark tasks and evaluation metrics to encourage and empower research on multi-agent learning for autonomous driving. Our code is available at this https URL ","SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for
  Autonomous Driving"
295,1320538176029487106,353133592,Chenguang Wang,"['What‚Äôs the relationship between deep language models (#bert, #gpt3) and knowledge graphs?\n\n@dawnsongtweets, @ShawLiu12, and I find that we can construct knowledge graphs from the pre-trained language models.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2010.11967,"This paper shows how to construct knowledge graphs (KGs) from pre-trained language models (e.g., BERT, GPT-2/3), without human supervision. Popular KGs (e.g, Wikidata, NELL) are built in either a supervised or semi-supervised manner, requiring humans to create knowledge. Recent deep language models automatically acquire knowledge from large-scale corpora via pre-training. The stored knowledge has enabled the language models to improve downstream NLP tasks, e.g., answering questions, and writing code and articles. In this paper, we propose an unsupervised method to cast the knowledge contained within language models into KGs. We show that KGs are constructed with a single forward pass of the pre-trained language models (without fine-tuning) over the corpora. We demonstrate the quality of the constructed KGs by comparing to two KGs (Wikidata, TAC KBP) created by humans. Our KGs also provide open factual knowledge that is new in the existing KGs. Our code and KGs will be made publicly available. ",Language Models are Open Knowledge Graphs
296,1319566634680487936,422672164,Dr Michael Reidinger,"['Detecting Dark Photons from Atomic Rearrangement in the Galaxy\n\n""We study a new dark sector signature for an atomic process of ""rearrangement"" in the galaxy.""\n<LINK>']",https://arxiv.org/abs/2010.11205,"We study a new dark sector signature for an atomic process of ""rearrangement"" in the galaxy. In this process, a hydrogen-like atomic dark matter state together with its anti-particle can rearrange to form a highly-excited bound state. This bound state will then de-excite into the ground state emitting a large number of dark photons that can be measured in experiments on Earth through their kinetic mixing with the photon. We find that for DM masses in the GeV range, the dark photons have enough energy to pass the thresholds of neutrino observatories such as Borexino and Super-Kamiokande that can probe for our scenario even when our atomic states constitute a small fraction of the total DM abundance. We study the corresponding bounds on the parameters of our model from current data as well as the prospects for future detectors. ",Detecting Dark Photons from Atomic Rearrangement in the Galaxy
297,1319301265348874246,1158385581476515840,Allyson Ettinger,"[""Check out Lang's #EMNLP2020 paper! We look for evidence of phrase meaning composition in transformers, and find that word content is captured, but we see little evidence for (humanlike) compositional phrase meaning beyond word content <LINK> 1/2"", 'Specifically, we find strong correlations with human similarity judgment / strong paraphrase classification, but when we control word overlap, these correspondences drop dramatically -- suggesting reliance on lexical information rather than nuances of phrase meaning. 2/2']",https://arxiv.org/abs/2010.03763,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models. ",Assessing Phrasal Representation and Composition in Transformers
298,1319275723807285248,1141772501472727040,Lena Voita,"['[1/4] Analyzing Source and Target Contributions to NMT Predictions - new work with @iatitov and @RicoSennrich!\n\nWhat influences the predictions in NMT: the source or the target prefix? We measure and find out!\n\nPaper: <LINK>\nBlog: <LINK> #NLProc <LINK>', '[2/4] NMT models can hallucinate, i.e. ignore source. On the other hand, LMs can ignore gibberish prefixes. What will our model do?\n\nWe give a model random prefixes and measure source contribution. With a short prefix, it is ignored; with a long, the source is ignored.\n\n#NLProc https://t.co/okLAETRBsX', '[3/4] We show that exposure bias indeed leads to over-reliance on target history.\n\nWhen we give random prefixes to different models, the ones with alleviated exposure bias ignore the source less than others.\n\n#NLProc https://t.co/s16rmeWdo1', ""[4/4] In the paper, we also look at lot's of other things:  amount of training data, training stages, ...\n\nWe find that the training process is non-monotonic with several distinct stages (e.g. stages changing direction from decreasing influence of source to increasing). \n\n#NLProc https://t.co/KGpbT5SDQf""]",https://arxiv.org/abs/2010.10907,"In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence. While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision. We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP). Its underlying 'conservation principle' makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token's influence. We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process. We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature. ","Analyzing the Source and Target Contributions to Predictions in Neural
  Machine Translation"
299,1319229657879695362,3260753346,Deepanway Ghosal,"['New paper at Findings of #EMNLP2020\nWe propose COSMIC and explore how commonsense knowledge can help in emotion recognition in conversations. \n\nPaper: <LINK>\n\nCode: <LINK>\nJoint work with N.Majumder, @gelbukh, @radamihalcea, @soujanyaporia <LINK>', 'COSMIC is built upon COMET (by @ABosselut, @YejinChoinka, et al.) and incorporates different elements of commonsense knowledge, including events, causal relations, and uses them to learn interactions between interlocutors participating in a conversation.', 'Our method achieves new SOTA results on four different benchmark datasets: IEMOCAP, DailyDialog, MELD, and EmoryNLP for recognizing emotions in conversations.']",https://arxiv.org/abs/2010.02795,"In this paper, we address the task of utterance level emotion recognition in conversations using commonsense knowledge. We propose COSMIC, a new framework that incorporates different elements of commonsense such as mental states, events, and causal relations, and build upon them to learn interactions between interlocutors participating in a conversation. Current state-of-the-art methods often encounter difficulties in context propagation, emotion shift detection, and differentiating between related emotion classes. By learning distinct commonsense representations, COSMIC addresses these challenges and achieves new state-of-the-art results for emotion recognition on four different benchmark conversational datasets. Our code is available at this https URL ","COSMIC: COmmonSense knowledge for eMotion Identification in
  Conversations"
300,1319090266612039681,516167077,Lorenzo Coviello,"['And in this other new paper, we propose a new family of activations, Smooth ReLU, that provide better accuracy-reproducibility tradeoffs in deep networks! <LINK>']",https://arxiv.org/abs/2010.09931,"Deep networks are gradually penetrating almost every domain in our lives due to their amazing success. However, with substantive performance accuracy improvements comes the price of \emph{irreproducibility}. Two identical models, trained on the exact same training dataset may exhibit large differences in predictions on individual examples even when average accuracy is similar, especially when trained on highly distributed parallel systems. The popular Rectified Linear Unit (ReLU) activation has been key to recent success of deep networks. We demonstrate, however, that ReLU is also a catalyzer to irreproducibility in deep networks. We show that not only can activations smoother than ReLU provide better accuracy, but they can also provide better accuracy-reproducibility tradeoffs. We propose a new family of activations; Smooth ReLU (\emph{SmeLU}), designed to give such better tradeoffs, while also keeping the mathematical expression simple, and thus implementation cheap. SmeLU is monotonic, mimics ReLU, while providing continuous gradients, yielding better reproducibility. We generalize SmeLU to give even more flexibility and then demonstrate that SmeLU and its generalized form are special cases of a more general methodology of REctified Smooth Continuous Unit (RESCU) activations. Empirical results demonstrate the superior accuracy-reproducibility tradeoffs with smooth activations, SmeLU in particular. ",Smooth activations and reproducibility in deep networks
301,1319070298541441025,1258919993334312960,Yuyue Yan,"['Our manuscript: ‚ÄúVariable guiding strategies in multi-exits evacuation: Pursuing balanced pedestrian densities‚Äù <LINK>\nis now available.@ProfJohnDrury @EvacuationModel\nWe studied the effects of crowd density control in guiding strategies on evacuation efficiency. <LINK>', 'We revealed that for a moderate target density value, the density control for the partial regions (near the exits) could yield a global effect for balancing the pedestrians in the rest of the regions and hence improve the evacuation efficiency.']",http://arxiv.org/abs/2010.10647,"Evacuation assistants and their guiding strategies play an important role in the multi-exits pedestrian evacuation. To investigate the effect of guiding strategies on evacuation efficiency, we propose a force-driven cellular automaton model with adjustable guiding attractions imposed by the evacuation assistants located in the exits. In this model, each of the evacuation assistants tries to attract the pedestrians in the evacuation space towards its own exit by sending a quantifiable guiding signal, which may be adjusted according to the values of pedestrian density near the exit. The effects of guiding strategies pursuing balanced pedestrian densities are studied. It is observed that the unbalanced pedestrian distribution is mainly yielded by a snowballing effect generated from the mutual attractions among the pedestrians, and can be suppressed by controlling the pedestrian densities around the exits. We also reveal an interesting fact that given a moderate target density value, the density control for the partial regions (near the exits) could yield a global effect for balancing the pedestrians in the rest of the regions and hence improve the evacuation efficiency. Our findings may contribute to give new insight into designing effective guiding strategies in the realistic evacuation process. ","Variable guiding strategies in multi-exits evacuation: Pursuing balanced
  pedestrian densities"
302,1319036157565480961,301450268,Mireia Montes,"['Paper day!!! In the middle of moving continents, science happens. Today we found the explanation to the ""lack"" of dark matter of the galaxy NGC1052-DF4. You can find it here: <LINK> @UNSWScience @IAC_Astrofisica @infantesainz @asborlaff', 'This ultra-diffuse galaxy is in the field of NGC1052 (see pic from Monelli &amp; Trujillo 2019, although we do not think it is physically associated to that galaxy, but this is another story...) https://t.co/mEM2nf8G3M', 'This galaxy has been claimed to lack dark matter (dark matter / stars of around 1, when normally is ~100). One of the simplest scenarios to explain why is if it has been interacting with another galaxy and during this interaction has lost most of its dark matter.', 'To investigate if that is the case, we obtained very deep imaging of this galaxy using the IAC80 Telescope (80cm, CAMELOT2 camera) and the HiPERCAM installed in the 10.4-m @GTCtelescope. You can see the different instruments used in the image. https://t.co/B4yDiMSPJh', 'First we studied how the globular clusters align as if there is interaction the clusters will be deposited along the orbit. We selected them using HST and HiPERCAM. In the pic you can see that they define a particular direction. The new candidates are in mint. https://t.co/0NFRZ77hMK', 'But well, to be 100% sure of what is happening we need to see it in the galaxy (the stars). Here, the IAC80 images are perfect. We see this S-shape that is telling that tidal stripping is happening and the direction of the orbit. And aligns with the globulars! https://t.co/7OufIEhWV6', 'So yes, tidal stripping is what you need to make this galaxy and is fully compatible with what we knew about the Universe. No magic going on here! https://t.co/p3lXPFxp2l', 'One of the things that we show is how important is to have deep images to understand apparently weird things in the Universe. And to reduce those images in the well to unveil all the faint features. Here @infantesainz, Javier Roman and @asborlaff are your guys!', '(I hope that people realize the paper has a mint color theme).', '@dvcotton I like mint :P', '@JohnRombi @UNSWScience @IAC_Astrofisica @infantesainz @asborlaff The US (Baltimore).']",https://arxiv.org/abs/2010.09719,"The existence of long-lived galaxies lacking dark matter represents a challenge to our understanding of how galaxies form. Here, we present evidence that explains the lack of dark matter in one of such galaxies: NGC1052-DF4. Deep optical imaging of the system has detected tidal tails in this object caused by its interaction with its neighbouring galaxy NGC1035. As stars are more centrally concentrated than the dark matter, the tidal stripping will remove a significant percentage of the dark matter before affecting the stars of the galaxy. Only ~7% of the stellar mass of the galaxy is in the tidal tails, suggesting that the stars of NGC1052-DF4 are starting only now to be affected by the interaction, while the percentage of remaining dark matter is <1%. This naturally explains the low content of dark matter inferred for this galaxy and reconciles these type of galaxies with our current models of galaxy formation. ","The galaxy ""missing dark matter"" NGC1052-DF4 is undergoing tidal
  disruption"
303,1319022062384578569,811954374675202048,Bright Yufeng Ye,"['Check out our new paper on arXiv! <LINK>\n\nWe propose using a quartic potential qubit (quarton) for a giant (&gt;1 GHz) nonlinear coupling between *linearly decoupled* qubits. The quarton can also cancel the self-Kerr of qubits, linearizing them into photons!', 'Be sure to also check out the recent experimental quarton qubit paper https://t.co/tJ6oB9MbHL  and the traveling wave photon detector paper https://t.co/y40Seh5HqN which uses the quarton.']",https://arxiv.org/abs/2010.09959,"Strong nonlinear coupling of superconducting qubits and/or photons is a critical building block for quantum information processing. Due to the perturbative nature of the Josephson nonlinearity, linear coupling is often used in the dispersive regime to approximate nonlinear coupling. However, this dispersive coupling is weak and the underlying linear coupling mixes the local modes which, for example, distributes unwanted self-Kerr to photon modes. Here, we use the quarton to yield purely nonlinear coupling between two linearly decoupled transmon qubits. The quarton's zero $\phi^2$ potential enables a giant gigahertz-level cross-Kerr which is an order of magnitude stronger compared to existing schemes, and the quarton's positive $\phi^4$ potential can cancel the negative self-Kerr of qubits to linearize them into resonators. This giant cross-Kerr between bare modes of qubit-qubit, qubit-photon, and even photon-photon is ideal for applications such as single microwave photon detection and implementation of bosonic codes. ",Engineering Purely Nonlinear Coupling with the Quarton
304,1318948567910895620,383903725,Xavier Puig,"['Excited to share ‚ÄúWatch-And-Help: A Challenge for Social Perception and Human-AI Collaboration‚Äù!  \nWe propose a challenge where agents need to infer human goals in a household and help perform them efficiently.\narxiv: <LINK>\ncode: <LINK> <LINK>', 'We build a realistic multi-agent platform on top of VirtualHome (https://t.co/wWEpn8jKoZ) and design a benchmark with multiple helping agent baselines. We evaluate their performance when collaborating with simulated and real humans in different tasks. \nhttps://t.co/F9qj9oF981', 'Joint work with @tianminshu, @ShuangL13799063, Zilin Wang, Josh Tenenbaum, @FidlerSanja and Antonio Torralba.']",http://arxiv.org/abs/2010.09890,"In this paper, we introduce Watch-And-Help (WAH), a challenge for testing social intelligence in agents. In WAH, an AI agent needs to help a human-like agent perform a complex household task efficiently. To succeed, the AI agent needs to i) understand the underlying goal of the task by watching a single demonstration of the human-like agent performing the same task (social perception), and ii) coordinate with the human-like agent to solve the task in an unseen environment as fast as possible (human-AI collaboration). For this challenge, we build VirtualHome-Social, a multi-agent household environment, and provide a benchmark including both planning and learning based baselines. We evaluate the performance of AI agents with the human-like agent as well as with real humans using objective metrics and subjective user ratings. Experimental results demonstrate that the proposed challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale. ","Watch-And-Help: A Challenge for Social Perception and Human-AI
  Collaboration"
305,1318925781901324299,88806960,Dr. Vivienne Baldassare,"[""So excited to be part of the Young Supernova Experiment- a new time domain survey using the Pan-STARRS telescopes. While I'm excited for all the AGN science, we'll also find lots of young supernovae (duh), TDEs and other transients. Stay tuned ü•≥ <LINK>"", 'You can also learn more on our website: https://t.co/W7H4yFWoDI']",https://arxiv.org/abs/2010.09724,"Time domain science has undergone a revolution over the past decade, with tens of thousands of new supernovae (SNe) discovered each year. However, several observational domains, including SNe within days or hours of explosion and faint, red transients, are just beginning to be explored. Here, we present the Young Supernova Experiment (YSE), a novel optical time-domain survey on the Pan-STARRS telescopes. Our survey is designed to obtain well-sampled $griz$ light curves for thousands of transient events up to $z \approx 0.2$. This large sample of transients with 4-band light curves will lay the foundation for the Vera C. Rubin Observatory and the Nancy Grace Roman Space Telescope, providing a critical training set in similar filters and a well-calibrated low-redshift anchor of cosmologically useful SNe Ia to benefit dark energy science. As the name suggests, YSE complements and extends other ongoing time-domain surveys by discovering fast-rising SNe within a few hours to days of explosion. YSE is the only current four-band time-domain survey and is able to discover transients as faint $\sim$21.5 mag in $gri$ and $\sim$20.5 mag in $z$, depths that allow us to probe the earliest epochs of stellar explosions. YSE is currently observing approximately 750 square degrees of sky every three days and we plan to increase the area to 1500 square degrees in the near future. When operating at full capacity, survey simulations show that YSE will find $\sim$5000 new SNe per year and at least two SNe within three days of explosion per month. To date, YSE has discovered or observed 8.3% of the transient candidates reported to the International Astronomical Union in 2020. We present an overview of YSE, including science goals, survey characteristics and a summary of our transient discoveries to date. ","The Young Supernova Experiment: Survey Goals, Overview, and Operations"
306,1318703939877957633,97939183,Yuandong Tian,"['<LINK>. 3-min video for our theoretical framework on self-supervised methods (SimCLR/BYOL) with deep ReLU networks. We find 1) an analytic form of weight update per layer, 2) how feature emerges, 3) why BYOL works without negative pairs. <LINK> <LINK>']",https://arxiv.org/abs/2010.00578,"We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a \emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a \emph{hierarchical latent tree model} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives \emph{no direct supervision} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in this https URL ",Understanding Self-supervised Learning with Dual Deep Networks
307,1318188318958456834,2972152960,Renato Negrinho,"['New EMNLP findings paper #emnlp #emnlp2020 .\n\nPaper: <LINK>\nCode: <LINK>\n\nWe empirically study beam-aware training algorithms instantiated through a meta-algorithm and evaluate them on supertagging. <LINK>', 'We find that beam-aware training yields large performance improvements when the model must rely on the beam to manage uncertainty effectively, e.g., must make decisions with incomplete information.', 'Beam-aware training uses beam search for both training and decoding and therefore models do not suffer from exposure bias and learn to exploit the beam. This contrasts with the usual approach of training on maximum likelihood and decoding with beam search.', 'The goal of this paper is to better understand the impact of beam-aware training, i.e., under what conditions would we see performance improvements and what design aspects would be most important.', 'We used a meta-algorithm for beam-aware training proposed in our previous work (https://t.co/39WMuxTzPD), which identifies several design dimensions: beam size, data collection strategy, and loss function.', 'We have found that beam-aware training yields large improvements when the model can‚Äôt encode the complete sequence before starting prediction. In this case, beam-aware training yielded a model that did a better job managing uncertainty about future predictions.', 'The standard approach of maximum likelihood training and post-hoc beam search failed to reach the same performances (~10 absolute perf. points in some cases). We also have useful observations about design choices to train the models stably and achieve high performances.', 'Personal take:  I believe that beam-aware training will be most impactful in settings where the instance for which we have to generate predictions is not fully available and partial predictions must be made with incomplete information, or in cases where ..', 'actions taken affect information gathered, e.g., when traversing a graph incrementally. I‚Äôm also hopeful about the ability of these models to address pathologies that have been identified with the typical use of beam search for decoding after maximum likelihood training.']",https://arxiv.org/abs/2010.04980,"Structured prediction is often approached by training a locally normalized model with maximum likelihood and decoding approximately with beam search. This approach leads to mismatches as, during training, the model is not exposed to its mistakes and does not use beam search. Beam-aware training aims to address these problems, but unfortunately, it is not yet widely used due to a lack of understanding about how it impacts performance, when it is most useful, and whether it is stable. Recently, Negrinho et al. (2018) proposed a meta-algorithm that captures beam-aware training algorithms and suggests new ones, but unfortunately did not provide empirical results. In this paper, we begin an empirical investigation: we train the supertagging model of Vaswani et al. (2016) and a simpler model with instantiations of the meta-algorithm. We explore the influence of various design choices and make recommendations for choosing them. We observe that beam-aware training improves performance for both models, with large improvements for the simpler model which must effectively manage uncertainty during decoding. Our results suggest that a model must be learned with search to maximize its effectiveness. ",An Empirical Investigation of Beam-Aware Training in Supertagging
308,1318115428275752961,1207610988993896449,Dr Laura Wolz,"['Study on if we can learn about global atomic hydrogen properties through interferometric intensity mapping, led by Zhaoting Chen\n\n<LINK>\n\n*Spoiler alert: we can - through multiple exciting ways.\n\nNow, excuse me while I am having a proud supervisor moment. <LINK>']",https://arxiv.org/abs/2010.07985,"We present a new halo model of neutral hydrogen (HI) calibrated to galaxy formation simulations at redshifts $z\sim0.1$ and $z\sim1.0$ that we employ to investigate the constraining power of interferometric HI intensity mapping on HI astrophysics. We demonstrate that constraints on the small-scale HI power spectrum can break the degeneracy between the HI density $\Omega_{\rm HI}$ and the HI bias $b_{\rm HI}$. For $z\sim0.1$, we forecast that an accurate measurement of $\Omega_{\rm HI}$ up to 6% level precision and the large-scale HI bias $b_{\rm HI}^0$ up to 1% level precision can be achieved using Square Kilometre Array (SKA) pathfinder data from MeerKAT and Australian SKA Pathfinder (ASKAP). We also propose a new description of the HI shot noise in the halo model framework in which a scatter of the relation between the HI mass of galaxies and their host halo mass is taken into account. Furthermore, given the number density of HI galaxies above a certain HI mass threshold, future surveys will also be able to constrain the HI mass function using only the HI shot noise. This will lead to constraints at the 10% level using the standard Schechter function. This technique will potentially provide a new way of measuring the HI Mass Function, independent from existing methods. We predict that the SKA will be able to further improve the low-redshift constraints by a factor of 3, as well as pioneering measurements of HI astrophysics at higher redshifts. ",Extracting HI Astrophysics from Interferometric Intensity Mapping
309,1317999518349168642,255590123,Jiliang Tang,['Can we find a unified view of existing GNNs and then a general GNN framework? The answer is yes. Please check our recent preprint: A Unified View on Graph Neural Networks as Graph Signal Denoising\n<LINK>'],https://arxiv.org/abs/2010.01777,"Graph Neural Networks (GNNs) have risen to prominence in learning representations for graph structured data. A single GNN layer typically consists of a feature transformation and a feature aggregation operation. The former normally uses feed-forward networks to transform features, while the latter aggregates the transformed features over the graph. Numerous recent works have proposed GNN models with different designs in the aggregation operation. In this work, we establish mathematically that the aggregation processes in a group of representative GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (approximately) solving a graph denoising problem with a smoothness assumption. Such a unified view across GNNs not only provides a new perspective to understand a variety of aggregation operations but also enables us to develop a unified graph neural network framework UGNN. To demonstrate its promising potential, we instantiate a novel GNN model, ADA-UGNN, derived from UGNN, to handle graphs with adaptive smoothness across nodes. Comprehensive experiments show the effectiveness of ADA-UGNN. ",A Unified View on Graph Neural Networks as Graph Signal Denoising
310,1316922593220374528,3145451345,m.onodera,"['Paper day!\n\nWe studied physical properties of star-forming galaxies at z~3.3 with extremely intense [OIII]5007 emission line, and find they are efficiently ionizing the surrounding ISM. The most extreme ones are excellent candidates for LyC search. \n\n<LINK>', 'Honestly, as an observational astronomer, I was very excited to see that the broad-band selection gave us some excess galaxies and they actually turned out to be real by the follow-up observation with the refurbished MOIRCS instrument on Subaru Telescope.', 'Such strong emission lines can be visible only with ~3-5 min exposures, while we integrated more to detect as many faint spectral features as possible.', 'These galaxies can be considered as analogs of those responsible for cosmic reionization, so it would be interesting to study some more details in other wavelength.', '@yolajojo „Å†„ÅÑ„Å∂Ââç„ÅÆ„Éá„Éº„Çø„ÅßË¶ã„Å¶„Åø„Åü„Å®„Åì„Çç„Åß„ÅØ„ÄÅÂÄãÂà•„Å´Âèó„Åã„Å£„Å¶„Çã„ÇÇ„ÅÆ„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇCOSMOS„Å´„Å§„ÅÑ„Å¶„ÅØphoto-z„ÅÆÁ≤æÂ∫¶„Åå„Åã„Å™„Çä„Çà„Åï„Åù„ÅÜ„Å™„ÅÆ„Åß„ÄÅÊúÄÊñ∞„ÅÆ„Éá„Éº„Çø„Åß„Çπ„Çø„ÉÉ„ÇØ„Åó„Å¶„Åø„Å¶„ÇÇ„Åä„ÇÇ„Åó„Çç„ÅÑ„ÅÆ„Åß„ÅØ„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ„Åü„Å†„ÄÅ„Çπ„Çø„ÉÉ„ÇØ„ÅØ„Çà„Åè„Å™„ÅÑ„Å®„ÅÑ„ÅÜË©±„ÇÇ„ÅÇ„Çã„ÅÆ„Åß„ÄÅ„Å©„ÅÜ„ÅÑ„ÅÜËß£Êûê„Åå„Çà„ÅÑ„Åã„ÅØÊ§úË®é‰∏≠„Å®„ÅÑ„ÅÜ„Å®„Åì„Çç„Åß„Åô', '@yolajojo „ÅÇ„ÄÅÂ≠óÊï∞„ÅÆÈñ¢‰øÇ„ÅßÁ´ØÊäò„Å£„Å°„ÇÉ„ÅÑ„Åæ„Åó„Åü„Åå„ÄÅË™≠„Çì„Åß„ÅÑ„Åü„Å†„ÅÑ„Å¶„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇ']",https://arxiv.org/abs/2010.07545,"We present the selection, spectroscopic identification, and physical properties of extreme emission line galaxies (EELGs) at $3<z<3.7$ aiming at studying physical properties of an analog population of star-forming galaxies (SFGs) at the epoch of reionization. The sample is selected based on the excess in the observed Ks broad band flux relative to the best-fit stellar continuum model flux. By applying a 0.3 mag excess as a primary criterion, we select 240 EELG candidates with intense emission lines and estimated observed-frame equivalent width (EW) of $\gtrsim 1000$ angstrom over the UltraVISTA-DR2 ultra-deep stripe in the COSMOS field. We then carried out a HK band follow-up spectroscopy for 23 of the candidates with Subaru/MOIRCS, and find that 19 and two of them are at $z>3$ with intense [OIII] emission, and H$\alpha$ emitters at $z\simeq 2$, respectively. These spectroscopically identified EELGs at $z\simeq 3.3$ show, on average, higher specific star formation rates (sSFR) than the star-forming main sequence, low dust attenuation of $E(B-V) \lesssim 0.1$ mag, and high [OIII]/[OII] ratios of $\gtrsim 3$. We also find that our EELGs at $z\simeq 3.3$ have higher hydrogen ionizing photon production efficiencies ($\xi_\mathrm{ion}$) than the canonical value ($\simeq 10^{25.2}$ Hz/erg), indicating that they are efficient in ionizing their surrounding interstellar medium. These physical properties suggest that they are low metallicity galaxies with higher ionization parameters and harder UV spectra than normal SFGs, which is similar to galaxies with Lyman continuum (LyC) leakage. Among our EELGs, those with the largest [OIII]/[OII] and EW([OIII]) values would be the most promising candidates to search for LyC leakage. ","Broad-band selection, spectroscopic identification, and physical
  properties of a population of extreme emission line galaxies at 3&lt;z&lt;3.7"
311,1316737856505741312,94236519,Yuki Mitsufuji,['Can we stop the abuse of AI when it becomes too strong to break existing copy-protection technologies? Our study examines whether separating sound can be prevented by adding\nadversarial noise to music contents. #creativeAI\n\n<LINK> <LINK>'],https://arxiv.org/abs/2010.03164,"Despite the excellent performance of neural-network-based audio source separation methods and their wide range of applications, their robustness against intentional attacks has been largely neglected. In this work, we reformulate various adversarial attack methods for the audio source separation problem and intensively investigate them under different attack conditions and target models. We further propose a simple yet effective regularization method to obtain imperceptible adversarial noise while maximizing the impact on separation quality with low computational complexity. Experimental results show that it is possible to largely degrade the separation quality by adding imperceptibly small noise when the noise is crafted for the target model. We also show the robustness of source separation models against a black-box attack. This study provides potentially useful insights for developing content protection methods against the abuse of separated signals and improving the separation performance and robustness. ",Adversarial attacks on audio source separation
312,1316658865157570560,750911820,Pablo Ouro,"['In <LINK> we studied the scalability and performance of our CFD code on three HPC facilities: @Arm ThunderX2 @GW4Alliance Isambard, @AMD EPYC-Rome &amp; @intelhpc Skylake @SuperCompWales . EPYC delivers best performance, SKL achieves best scalability&amp; TX2 doing well!', 'This is a collaboration with @ULopezNovoa from @upvehu and Martyn Guest from @ARCCACardiffUni @cardiffuni, and partly funded by @GW4Alliance and @EPSRC . Thanks to @simonmcs, @adefewins, @hpcchris, Ade Fewings and @zaptohome for their support and help!']",https://arxiv.org/abs/2010.07111v1,"No area of computing is hungrier for performance than High Performance Computing (HPC), the demands of which continue to be a major driver for processor performance and adoption of accelerators, and also advances in memory, storage, and networking technologies. A key feature of the Intel processor domination of the past decade has been the extensive adoption of GPUs as coprocessors, whilst more recent developments have seen the increased availability of a number of CPU processors, including the novel ARM-based chips. This paper analyses the performance and scalability of a state-of-the-art Computational Fluid Dynamics (CFD) code on three HPC cluster systems equipped with AMD EPYC-Rome (EPYC, 4096 cores), ARM-based Marvell ThunderX2 (TX2, 8192 cores) and Intel Skylake (SKL, 8000 cores) processors. Three benchmark cases are designed with increasing computation-to-communication ratio and numerical complexity, namely lid-driven cavity flow, Taylor-Green vortex and a travelling solitary wave using the level-set method, adopted with $4^{th}$-order central-differences or a $5^{th}$-order WENO scheme. Our results show that the EPYC cluster delivers the best code performance for all the setups under consideration. In the first two benchmarks, the SKL cluster demonstrates faster computing times than the TX2 system, whilst in the solitary wave simulations, the TX2 cluster achieves good scalability and similar performance to the EPYC system, both improving on that obtained with the SKL cluster. These results suggest that while the Intel SKL cores deliver the best strong scalability, the associated cluster performance is lower compared to the EPYC system. The TX2 cluster performance is promising considering its recent addition to the HPC portfolio. ","] On the performance of a highly-scalable Computational Fluid Dynamics
  code on AMD, ARM and Intel processors"
313,1316291822952484866,272682658,Robert Hawkins,"['üî• more EMNLP content for your news feedüî•\n\nwe were interested in how recent neural language models integrate lexical/semantic information into their knowledge of grammatical constructions. obviously, we needed to study verb biases. \n\nlink: <LINK> <LINK>', 'in collab with the brilliant @TakaYamakoshi, @adelegoldberg1, and Tom Griffiths, we present the DAIS corpus of 50K acceptability ratings for 5K sentence pairs in the dative alternation, containing 200 different verbs:\n\nhttps://t.co/k51FOANwoI', 'e.g. how much better is A than B here?\n\nA: Ava gave her friend a book.\nB: Ava gave a book to her friend.\n\nwhat about with a different verb?\n\nA: Ava explained her friend a problem.\nB: Ava explained a problem to her friend.\n\nThis preference varies widely across verbs! https://t.co/vPPQ2vqknW', 'It turns out that larger models account for these verb-specific preferences better than smaller models, and transformer architectures (e.g. GPT-2) do so better than recurrent architectures (e.g. LSTMs), even with comparable param numbers. https://t.co/RBBm9Li0gJ', ""To begin to understand why, we probed the hidden states of these models as they proceed through the sentence. Immediately upon seeing the verb, GPT2's hidden layers already contains a surprising amount of information about human ratings, and this improved after seeing the 1st arg https://t.co/XBIan8K0JB"", ""as a side-note, we also analyzed how these representations are organized as a function of layer depth in GPT2 -- at the verb, human ratings are more decodable at lower layers (corresponding to lexical info?), while later in the sentence, it's only decodable at intermediate layers https://t.co/AEAAOsYqVW"", 'After a thoughtful suggestion from a reviewer, we also compared these models on an older natural speech corpus (assembled by the inimitable Joan Bresnan) and found similar rankings (vs. the roughly 92% previously achieved from hand-annotated features) https://t.co/TMQUg74IMl https://t.co/oWRFPlgxag', 'We speculate that transformers may be able to learn a better layer-wise pipeline for integrating lexically specific information with representations of higher-level syntactic constructions like the dative &amp; double-object, but further work is needed!', 'We hope our dataset will be useful for future work in both psycholinguistics and NLP, as we push our models to account for subtler phenomena and understand how they deviate from human expectations.']",https://arxiv.org/abs/2010.02375,"Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as \emph{verb bias}. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions. ",Investigating representations of verb bias in neural language models
314,1315846130735882240,80080794,Daniel Levy,"[""DRO: no-one knows what it does and it doesn't scale anyway... Or does it?\n\nIn our #NeurIPS2020 paper, we propose optimization algorithms with running time independent of dimension and dataset size (think SGD for ERM) for CVaR and chi-square objectives. <LINK> 1/4"", 'We propose two types of algorithm, one based on minimizing a surrogate objective and the other utilizing a more sophisticated (multilevel) gradient estimator. We show that the latter are optimal in two out of three settings and all are implemented in a few lines of PyTorch. 2/4', ""Some open questions remain:\n- There is still a gap between the (known) lower and upper bounds for the chi-square uncertainty sets.\n- What's the effect of DRO-type training for big deep learning models? This couldn't really be evaluated before!\n\n3/4"", 'Joint work with Yair Carmon, John Duchi and Aaron Sidford!\n\nPaper: https://t.co/jZMkwuTLgy\nCode: https://t.co/ix0994bAIA\n\n4/4']",https://arxiv.org/abs/2010.05893,"We propose and analyze algorithms for distributionally robust optimization of convex losses with conditional value at risk (CVaR) and $\chi^2$ divergence uncertainty sets. We prove that our algorithms require a number of gradient evaluations independent of training set size and number of parameters, making them suitable for large-scale applications. For $\chi^2$ uncertainty sets these are the first such guarantees in the literature, and for CVaR our guarantees scale linearly in the uncertainty level rather than quadratically as in previous work. We also provide lower bounds proving the worst-case optimality of our algorithms for CVaR and a penalized version of the $\chi^2$ problem. Our primary technical contributions are novel bounds on the bias of batch robust risk estimation and the variance of a multilevel Monte Carlo gradient estimator due to [Blanchet & Glynn, 2015]. Experiments on MNIST and ImageNet confirm the theoretical scaling of our algorithms, which are 9--36 times more efficient than full-batch methods. ",Large-Scale Methods for Distributionally Robust Optimization
315,1315825797744164864,1047227002388959232,Giannis Daras,"['Excited to announce our #NeurIPS2020 paper: \nSMYRF: Efficient Attention using Asymmetric Clustering.\nPaper: <LINK>\nCode: <LINK>\n<LINK>\nWe propose a novel way to approximate *pre-trained* attention layers or train from scratch.', 'The Reformer paper showed that we can train efficient attention layers by separating vectors into clusters and by performing attention within each cluster. We ask a new challenging question: Can we find clustering algorithms to approximate *pre-trained* attention layers?', 'We formulate the optimization problem of finding the clusters that best approximate dense attention. We prove that solving this problem is NP-hard, using a reduction to 3-dimensional matching.', 'Is all hope lost? Of course not. Pre-trained attention networks produce really sparse attention distributions &amp; the softmax matrix is near low rank. Hence, despite the hardness result, in practice, appropriate clustering algorithms can approximate attention very well.', 'We propose novel Asymmetric Transformations that we apply to the queries and keys, such that the distance of the transformed vectors decreases linearly as a function of the inner product of the initial vectors. We then use adaptive LSH clustering to generate attention clusters.', 'We show through numerous experiments that SMYRF layers are very effective in terms of performance even without any training. For example, we are able to shrink the attention memory of a pre-trained BigGAN by 50% while maintaining 98.2% of its Inception score without re-training.', ""We finetune SMYRF on GLUE, starting from a BERT checkpoint. We demonstrate that\nSMYRF outperforms BERT while using 50% less memory. We also show that with 75% less\nmemory, SMYRF maintains 99% of BERT's performance. We can even swap again SMYRF with dense attention after training."", 'We emphasize that SMYRF can be used directly to any pre-trained dense attention layer. It does not require changes to dense attention, such as using the same vectors for queries and keys as in the Reformer. For more NLP experiments and comparison to other methods, see the paper.', 'Finally, we trained SMYRF attention layers from scratch. Using a single TPU, we were able to scale attention\nto 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.', 'This paper is joint work with Nikita Kitaev, Augustus Odena (@gstsdn) and my Ph.D. advisor Alex Dimakis (@AlexGDimakis). I am very grateful for all the help and guidance they provided throughout this project. Looking forward to more research opportunities in the future!']",https://arxiv.org/abs/2010.05315,"We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from $O(N^2)$ to $O(N \log N)$, where $N$ is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using $50\%$ less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ. ",SMYRF: Efficient Attention using Asymmetric Clustering
316,1315752847531597824,2227196588,Javid Ebrahimi,"['How can language models learn hierarchies? In this @emnlp2020 Findings paper, we study how self-attention networks can recognize synthetic nested structures. The attention maps resemble a stack.\nWith @DhruvGelda \npaper: <LINK> <LINK>']",https://arxiv.org/abs/2010.04303,"We focus on the recognition of Dyck-n ($\mathcal{D}_n$) languages with self-attention (SA) networks, which has been deemed to be a difficult task for these networks. We compare the performance of two variants of SA, one with a starting symbol (SA$^+$) and one without (SA$^-$). Our results show that SA$^+$ is able to generalize to longer sequences and deeper dependencies. For $\mathcal{D}_2$, we find that SA$^-$ completely breaks down on long sequences whereas the accuracy of SA$^+$ is 58.82$\%$. We find attention maps learned by $\text{SA}{^+}$ to be amenable to interpretation and compatible with a stack-based language recognizer. Surprisingly, the performance of SA networks is at par with LSTMs, which provides evidence on the ability of SA to learn hierarchies without recursion. ",How Can Self-Attention Networks Recognize Dyck-n Languages?
317,1315024866442182659,110103071,Andrej Risteski,"['New work, w Fred Koehler &amp; @thebigmehtaphor (new collaborator at @CMU_Robotics!) on normalizing flows: <LINK>. Common training issue is conditioning (relatedly, the large depth models need in practice). We study representational aspects of these phenomena.', '(1) We show affine couplings (common architecture) are universal approxtrs, even if you don\'t pad input. Prior works all pad w/ zeroes to ""inflate"" latent dimension you can use for approximation. Tradeoff in construction:  \nit gets more poorly conditioned as approx gets better.', '(Zero padding clearly results in a singular \nJacobian when latent distribution we push forward is Gaussian. We find empirically Gaussian padding seems to work much better, but have no theory.)', ""(2) We show choice of partition in affine couplings doesn't matter much: you can simulate any linear map \n(hence permutation) using a const number of affine couplings of any choice of partition. Suggests 1x1 convolutions ala Glow only save a const factor on depth."", '(3) Finally, for general invertible architectures w/ bounded # of parameters per layer (e.g. each layer is invertible matrix + invertible nonlinearity), we show there are some shallow nets that need a large depth to be simulated.']",https://arxiv.org/abs/2010.01155,"Normalizing flows are among the most popular paradigms in generative modeling, especially for images, primarily because we can efficiently evaluate the likelihood of a data point. This is desirable both for evaluating the fit of a model, and for ease of training, as maximizing the likelihood can be done by gradient descent. However, training normalizing flows comes with difficulties as well: models which produce good samples typically need to be extremely deep -- which comes with accompanying vanishing/exploding gradient problems. A very related problem is that they are often poorly conditioned: since they are parametrized as invertible maps from $\mathbb{R}^d \to \mathbb{R}^d$, and typical training data like images intuitively is lower-dimensional, the learned maps often have Jacobians that are close to being singular. In our paper, we tackle representational aspects around depth and conditioning of normalizing flows: both for general invertible architectures, and for a particular common architecture, affine couplings. We prove that $\Theta(1)$ affine coupling layers suffice to exactly represent a permutation or $1 \times 1$ convolution, as used in GLOW, showing that representationally the choice of partition is not a bottleneck for depth. We also show that shallow affine coupling networks are universal approximators in Wasserstein distance if ill-conditioning is allowed, and experimentally investigate related phenomena involving padding. Finally, we show a depth lower bound for general flow architectures with few neurons per layer and bounded Lipschitz constant. ",Representational aspects of depth and conditioning in normalizing flows
318,1314944874970411008,743686415158960129,Yilin Yang,"[""Hello #NLProc and #emnlp2020, Our new work aiming to explain the internal behaviors of the Transformer decoder is now on arXiv: <LINK>\n\nSurprisingly, we find the de-facto usage of residual feedforward layer isn't essential and could be removed in totality!""]",https://arxiv.org/abs/2010.02648,"There have been significant efforts to interpret the encoder of Transformer-based encoder-decoder architectures for neural machine translation (NMT); meanwhile, the decoder remains largely unexamined despite its critical role. During translation, the decoder must predict output tokens by considering both the source-language text from the encoder and the target-language prefix produced in previous steps. In this work, we study how Transformer-based decoders leverage information from the source and target languages -- developing a universal probe task to assess how information is propagated through each module of each decoder layer. We perform extensive experiments on three major translation datasets (WMT En-De, En-Fr, and En-Zh). Our analysis provides insight on when and where decoders leverage different sources. Based on these insights, we demonstrate that the residual feed-forward module in each Transformer decoder layer can be dropped with minimal loss of performance -- a significant reduction in computation and number of parameters, and consequently a significant boost to both training and inference speed. ",On the Sub-Layer Functionalities of Transformer Decoder
319,1314656719037849601,140491831,Tejas Srinivasan,"[""'Fine-Grained Grounding for Multimodal Speech Recognition' will appear in Findings of #emnlp2020 <LINK>. We propose a multimodal ASR model that uses object proposals instead of global visual features. Work with @Sanabria_RST @delliott and Florian Metze <LINK>"", 'When the speech input is corrupted with silence masking, our model is significantly better at recovering masked words in the speech than our baselines. In particular, our model is better at recovering ‚Äúgroundable‚Äù words - nouns and places, adjectives and colors.', 'We also analyse whether our model is right for the right reasons. Does it recover words by focusing on the expected object proposal? https://t.co/ib2OZ36T4l', 'The code and data for training our ASR models is available at: https://t.co/QD1qdD5SwB', 'Obligatory: https://t.co/5qKBYlZv5b', '@_irishjars https://t.co/uMkJm5kt9G']",http://arxiv.org/abs/2010.02384,"Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the audio, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual features that represent the entire image, but localizing the relevant regions of the image will make it possible to recover a larger set of words, such as adjectives and verbs. In this paper, we propose a model that uses finer-grained visual information from different parts of the image, using automatic object proposals. In experiments on the Flickr8K Audio Captions Corpus, we find that our model improves over approaches that use global visual features, that the proposals enable the model to recover entities and other related words, such as adjectives, and that improvements are due to the model's ability to localize the correct proposals. ",Fine-Grained Grounding for Multimodal Speech Recognition
320,1313943220166905856,269211699,Ilya Chetviorkin,"['Happy to announce our joined work with @yitehsu, @Sarthak48364518 and Yi-Hsiu Liao on Efficient Inference for NMT <LINK> We propose a combination of techniques which combined allowed to speed up inference 2x without degradation in BLEU', 'It appeared that if one traines model based on the teacher decoded data, a lot of parameters become redundant in the model and can be prunned. We also leverage deep encoder, shallow decoder structure, simplified recurrent units instead of self attention and remove FFN in decoder.', 'To appear on @SustaiNLP2020 . cc @Thom_Wolf if you are interested in something like this for @huggingface']",http://arxiv.org/abs/2010.02416,"Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109% and 84% speedup on CPU and GPU respectively and reduce the number of parameters by 25% while maintaining the same translation quality in terms of BLEU. ",Efficient Inference For Neural Machine Translation
321,1313866220253241350,1261860960895004672,Jorge Moreno,"[""It's paper day! (ApJ accepted.) We use COSMOS/CANDELS to investigate the role of merging in triggering AGN. We generally find mild-to-null AGN fraction enhancement (with a few exceptions). Tip of the hat to Ekta Shah for leading this work!\n<LINK> <LINK>""]",https://arxiv.org/abs/2010.02710,"Galaxy interactions and mergers are thought to play an important role in the evolution of galaxies. Studies in the nearby universe show a higher AGN fraction in interacting and merging galaxies than their isolated counterparts, indicating that such interactions are important contributors to black hole growth. To investigate the evolution of this role at higher redshifts, we have compiled the largest known sample of major spectroscopic galaxy pairs (2381 with $\Delta V <5000$ km s$^{-1}$) at $0.5<z<3.0$ from observations in the COSMOS and CANDELS surveys. We identify X-ray and IR AGN among this kinematic pair sample, a visually identified sample of mergers and interactions, and a mass-, redshift-, and environment-matched control sample for each in order to calculate AGN fractions and the level of AGN enhancement as a function of relative velocity, redshift, and X-ray luminosity. While we see a slight increase in AGN fraction with decreasing projected separation, overall, we find no significant enhancement relative to the control sample at any separation. In the closest projected separation bin ($<25$ kpc, $\Delta V <1000$ km s$^{-1}$), we find enhancements of a factor of 0.94$^{+0.21}_{-0.16}$ and 1.00$^{+0.58}_{-0.31}$ for X-ray and IR-selected AGN, respectively. While we conclude that galaxy interactions do not significantly enhance AGN activity on average over $0.5<z<3.0$ at these separations, given the errors and the small sample size at the closest projected separations, our results would be consistent with the presence of low-level AGN enhancement. ","Investigating the Effect of Galaxy Interactions on AGN Enhancement at
  $0.5&lt;z&lt;3.0$"
322,1313862979222474752,1403815458,Amir Siraj,"['In a new paper on the arXiv, we explore observable signatures of low- and high-ejection-speed interstellar object populations, which can be studied by @VRubinObs to reveal insights into planetary system formation and evolution <LINK>']",https://arxiv.org/abs/2010.02214,"`Oumuamua and Borisov were the first two interstellar objects confirmed in the Solar system. The upcoming commencement of the Vera C. Rubin Observatory's Legacy Survey of Space of Time (LSST) will enhance greatly the discovery rate of interstellar objects. This raises the question, what can be learned from large-number statistics of interstellar objects? Here, we show that discovery statistics provided by LSST will allow low- and high-ejection-speed populations to be distinguished using the velocity dispersion and angular anisotropy of interstellar objects. These findings can be combined with physical characterizations to yield a better understanding of planetary system origin and nature. ","Observable Signatures of the Ejection Speed of Interstellar Objects from
  their Birth Systems"
323,1313781137806950400,513919777,Sebastian Bobadilla Suarez,"['Our new preprint ‚ÄúRobust priors for regularized regression‚Äù with @ProfData and Matt Jones is now up on arXiv! We find priors inspired by human decision-making heuristics lead to robust and interpretable models for data analysis. <LINK> 1/n <LINK>', 'Penalized regression approaches, like ridge regression, shrink weights toward zero but zero association is usually not a sensible prior. Human decision heuristics like tallying (TAL) or take-the-best (TTB) give us more sensible targets. https://t.co/houLLrOFp7 2/n https://t.co/bmSipwNsSj', 'Estimates from a constrained model serve as a prior for a more general model, yielding a principled way to interpolate between models of differing complexity. Models with robust priors had excellent worst-case performance. https://t.co/houLLrOFp7 3/n https://t.co/Qt9W5QnbVE', 'These models are more interpretable. For over 20 decision problems, solutions followed from the form of the heuristic that was used to derive the prior, while giving us a new vantage point on the environment (e.g., how compensatory is it?). https://t.co/houLLrOFp7 4/n https://t.co/EkKznoxEKs', 'We applied this approach to analyzing simulated fMRI data too, incorporating a least squares separate (LSS) prior ‚Äì commonly used for getting trial by trial estimates in rapid event designs. Our approach outperforms LSS (lower RMSE). https://t.co/houLLrOFp7 5/n https://t.co/GEEAIhreve', 'Thanks for reading if you got this far! Please check out the preprint on arXiv if you get a chance and stay tuned! https://t.co/houLLrOFp7 6/6 https://t.co/CA8fTKb5JW', ""@harrison_ritz @ProfData Thanks! Do you mean penalizing different directions in weight space? That's definitely a natural extension which we hint at in the discussion."", '@harrison_ritz @ProfData Yeah both standard ridge and LSS should help with that. So the flexible interpolation we propose seems to do better. It acts as a soft constraint on the weight space.', '@harrison_ritz @ProfData Getting a full posterior would be another possible extension for sure (also mentioned in the discussion). The idea is that constrained models like TAL, TTB, and LSS have higher bias and benefit from flexibility in our approach.']",https://arxiv.org/abs/2010.02610,"Induction benefits from useful priors. Penalized regression approaches, like ridge regression, shrink weights toward zero but zero association is usually not a sensible prior. Inspired by simple and robust decision heuristics humans use, we constructed non-zero priors for penalized regression models that provide robust and interpretable solutions across several tasks. Our approach enables estimates from a constrained model to serve as a prior for a more general model, yielding a principled way to interpolate between models of differing complexity. We successfully applied this approach to a number of decision and classification problems, as well as analyzing simulated brain imaging data. Models with robust priors had excellent worst-case performance. Solutions followed from the form of the heuristic that was used to derive the prior. These new algorithms can serve applications in data analysis and machine learning, as well as help in understanding how people transition from novice to expert performance. ",Robust priors for regularized regression
324,1313747256957263874,1189826682263179265,Yuji Kanagawa,"['Exploration is generally difficult, but if there are only a few rewarding states, we can use a simple inductive bias: just visit diverse regions!\nWe propose IMOC,  an option-learning method that learns diverse options.\npaper: <LINK>\ncode: <LINK> <LINK>']",https://arxiv.org/abs/2010.02756,"In this paper, we study the problem of autonomously discovering temporally abstracted actions, or options, for exploration in reinforcement learning. For learning diverse options suitable for exploration, we introduce the infomax termination objective defined as the mutual information between options and their corresponding state transitions. We derive a scalable optimization scheme for maximizing this objective via the termination condition of options, yielding the InfoMax Option Critic (IMOC) algorithm. Through illustrative experiments, we empirically show that IMOC learns diverse options and utilizes them for exploration. Moreover, we show that IMOC scales well to continuous control tasks. ",Diverse Exploration via InfoMax Options
325,1313689537940348929,228283891,Hossein Mobahi,"['1/6 We propose a ‚Äúunifying framework‚Äù for analyzing implicit bias of neural networks, investigate gradient flow on ‚Äúlinear tensor networks,‚Äù and extend existing theoretical results while relaxing some of their convergence assumptions. <LINK> <LINK>', '2/6 We propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks.', '3/6 With this formulation, one can identify the convergence direction of the network parameters as singular vectors of a tensor defined by the network.', '4/6 We also show that for D-layer linear tensor networks that are orthogonally decomposable, gradient flow on separable classification finds a stationary point of the L{2/D} max-margin problem in a ""transformed"" input space defined by the network.', '5/6 For underdetermined regression, we prove that gradient flow finds a global minimum which minimizes a norm-like function that interpolates between weighted L1 and L2 norms in the transformed input space.', '6/6 Our theorems extend many existing results into a general framework, while removing standard convergence assumptions in the literature of implicit bias/convergence. Joint work with my outstanding  intern Chulhee Yun (MIT) and Shankar Krishnan (Google Research).', '@yaodong_yu Thank you Yaodong!']",https://arxiv.org/abs/2010.02501,"We study the implicit bias of gradient flow (i.e., gradient descent with infinitesimal step size) on linear neural network training. We propose a tensor formulation of neural networks that includes fully-connected, diagonal, and convolutional networks as special cases, and investigate the linear version of the formulation called linear tensor networks. With this formulation, we can characterize the convergence direction of the network parameters as singular vectors of a tensor defined by the network. For $L$-layer linear tensor networks that are orthogonally decomposable, we show that gradient flow on separable classification finds a stationary point of the $\ell_{2/L}$ max-margin problem in a ""transformed"" input space defined by the network. For underdetermined regression, we prove that gradient flow finds a global minimum which minimizes a norm-like function that interpolates between weighted $\ell_1$ and $\ell_2$ norms in the transformed input space. Our theorems subsume existing results in the literature while removing standard convergence assumptions. We also provide experiments that corroborate our analysis. ",A Unifying View on Implicit Bias in Training Linear Neural Networks
326,1313582132300845057,1255599597763862529,Luyu Gao,"['Excited to share our paper ""Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation"" We propose a character n-gram embedding for translation into low resource language. With @cindyxinyiwang @gneubig, in findings of EMNLP.\n<LINK>']",http://arxiv.org/abs/2010.01667,"To improve the performance of Neural Machine Translation~(NMT) for low-resource languages~(LRL), one effective strategy is to leverage parallel data from a related high-resource language~(HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate \emph{into} the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding~\citep{SDE}, we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages. ","Improving Target-side Lexical Transfer in Multilingual Neural Machine
  Translation"
327,1313486162745528328,31706592,Dino Rossegger,['New Preprint on arXiv: <LINK>\nWe study the complexity of the elementary bi-embeddability relation on graphs both in the setting of descriptive set theory and computable structure theory. See also my short blogpost about it: <LINK>'],https://arxiv.org/abs/2010.00755,We study the bi-embeddability and elementary bi-embeddability relation on graphs under Borel reducibility and investigate the degree spectra realized by this relations. We first give a Borel reduction from embeddability on graphs to elementary embeddability on graphs. As a consequence we obtain that elementary bi-embeddability on graphs is a analytic complete equivalence relation. We then investigate the algorithmic properties of this reduction to show that every bi-embeddability spectrum of a graph is the jump spectrum of an elementary bi-embeddability spectrum of a graph. ,Degree spectra of analytic complete equivalence relations
328,1313283901851344896,4902145390,Gordan Krnjaic,"['New paper out tonight with @DanHooperAstro. We study how light primordial black holes (PBH) affect GUT baryogenesis. Hawking evaporation can produce very heavy particles even if the SM temperature is low, which changes the predictions for the baryon yield\n\n<LINK>', '@bvlehmann @jazzwhiz @DanHooperAstro Ben got it exactly right. The surrounding radiation temperature can be low, but as any BH evaporates, its personal temperature (distinct from the bath) increases']",https://arxiv.org/abs/2010.01134,"In models of baryogenesis based on Grand Unified Theories (GUTs), the baryon asymmetry of the universe is generated through the CP and baryon number violating, out-of-equilibrium decays of very massive gauge or Higgs bosons in the very early universe. Recent constraints on the scale of inflation and the subsequent temperature of reheating, however, have put pressure on many such models. In this paper, we consider the role that primordial black holes may have played in the process of GUT baryogenesis. Through Hawking evaporation, black holes can efficiently generate GUT Higgs or gauge bosons, regardless of the masses of these particles or the temperature of the early universe. Furthermore, in significant regions of parameter space, the black holes evaporate after the electroweak phase transition, naturally evading the problem of sphaleron washout that is normally encountered in GUT models based on $SU(5)$. We identify a wide range of scenarios in which black holes could facilitate the generation of the baryon asymmetry through the production and decays of GUT bosons. ",GUT Baryogenesis With Primordial Black Holes
329,1313144975958306816,501534807,Yuhao Zhang,"['üëã Excited to share our latest work ""Contrastive Learning of Medical Visual Representations from Paired Images and Text"".\n\nWe propose a contrastive framework for learning visual representations of medical images from paired textual data.\n\narXiv: <LINK>\n\nüëá (1/7) <LINK>', 'While paired textual reports are easily found for many types of medical images, existing work has either ignored the reports and relied on expert annotations &amp; weight transfer from ImageNet, or used rule-based label extraction from text.\n\n(2/7) https://t.co/S6DkaaYEcg', 'We argue that neither is optimal, and present *ConVIRT*, a new unsupervised method for learning medical visual repr. from the pairing of images &amp; text.\n\nConVIRT relies on contrasting true image-text pairs vs. random ones via a bidirectional NCE loss btw. the 2 modalities. \n\n(3/7) https://t.co/q0mJeunZrV', 'We apply ConVIRT to pretrain chest and bony image encoders &amp; transfer to downstream tasks. \n\nüëÄ We find that:\n(1) In 4 medical image tasks, ConVIRT outperforms strong baselines including widely used ImageNet or image captioning-based initializations...\n\n(4/7) https://t.co/kRnFvNxrlQ', '(2) In all 4 tasks, ConVIRT only requires *10%* as much of labeled data as an ImageNet initialized model to achieve better or comparable performance;\n\n(3) ConVIRT also markedly outperformed all baselines in 2 new *zero-shot* image retrieval tasks.\n\n(5/7) https://t.co/CaPdZ4VWeB', 'We also compare ConVIRT with image-only contrastive methods, SimCLR and MoCo v2, and find that ConVIRT is much more effective for medical visual representation learning, due to its effective use of information in text. \n\n(6/7) https://t.co/tjqkqSJ2Z0', 'üöÄ Joint work with great collaborators @hjian42, Yasuhide Miura, @chrmanning &amp; @curtlanglotz. Work at @stanfordnlp &amp; @StanfordAIMI.\n\nConVIRT = Contrasive VIsual Representation learning from Text\n\nüìñ Details in paper: https://t.co/HhJxaxSWe1\n\nCode is coming! (7/7)', '@nish_khandwala Thank you Nish! Glad you like it! üòÉ']",https://arxiv.org/abs/2010.00747,"Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency. ","Contrastive Learning of Medical Visual Representations from Paired
  Images and Text"
330,1312164325927383040,308306041,Kamal Ndousse,"['Big personal milestone! My first ML paper is on arXiv: <LINK> \nWe propose a simple method that helps RL agents in shared environments learn from one another, and show that the learned social policies improve zero-shot transfer performance in new environments. üëá', 'Special thanks to @natashajaques, and to our collaborators @svlevine @douglas_eck!\nAnd to @OpenAI -- this work grew out of the Scholars program, which I participated in earlier this year.', ""In the paper, we show that social multi-agent RL can be useful even for non-social tasks! Our SociAPL agents are able to use cues from expert behavior to navigate environments they've never seen before.""]",https://arxiv.org/abs/2010.00581,"Social learning is a key component of human and animal intelligence. By taking cues from the behavior of experts in their environment, social learners can acquire sophisticated behavior and rapidly adapt to new circumstances. This paper investigates whether independent reinforcement learning (RL) agents in a multi-agent environment can learn to use social learning to improve their performance. We find that in most circumstances, vanilla model-free RL agents do not use social learning. We analyze the reasons for this deficiency, and show that by imposing constraints on the training environment and introducing a model-based auxiliary loss we are able to obtain generalized social learning policies which enable agents to: i) discover complex skills that are not learned from single-agent training, and ii) adapt online to novel environments by taking cues from experts present in the new environment. In contrast, agents trained with model-free RL or imitation learning generalize poorly and do not succeed in the transfer tasks. By mixing multi-agent and solo training, we can obtain agents that use social learning to gain skills that they can deploy when alone, even out-performing agents trained alone from the start. ",Emergent Social Learning via Multi-agent Reinforcement Learning
331,1312120926985576449,446694758,Julian Eisenschlos,"['Entailment has been studied in depth for textual premises, but the case with structured data like tables or even HTML can have many applications in the wild. \n\nWe tackle this in our latest #EMNLP2020 Findings paper <LINK> with Syrine Krichene and @muelletm\n\n1/5', 'We extend TAPAS (Herzig et al, 2020), originally pretrained with MLM, to predict if a table entails or refutes a sentence and eval on TabFact (Chen et al, 2020). We introduce 2 novel pretraining binary-classification tasks called Counterfactual and Synthetic, shown in image.\n\n2/5 https://t.co/4s84vzcut1', 'Counterfactual examples are created by swapping entities that appear in both a table and a sentence for a plausible alternative: they are realistic but simple. Synthetic ones are sampled from a small pCFG based on the values of a real table: they improve numerical reasoning.\n\n3/5 https://t.co/f17pVpWqiu', 'Pretraining with these 2 tasks, we improve SOTA by ~10pts on TabFact and, interestingly, also get a new SOTA on the table QA task SQA (Iyyer et al, 2017). This results hold even with fraction of the data, and is only 2 points below a strong baseline with no data at all!\n\n4/5 https://t.co/RAakCIRKGn', 'Finally, we investigate how to deal with large tables by selecting which parts of the input to pass through the model using simple heuristics. We can can get 2x speed-ups with ~1pt acc drop, or 4x still above prior art. Code and models coming soon at https://t.co/1VxCSrSunr\n\n5/5']",http://arxiv.org/abs/2010.00571,"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to fine-tuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TabFact (Chen et al., 2020) and SQA datasets. ",Understanding tables with intermediate pre-training
332,1312117271284772864,97939183,Yuandong Tian,"['<LINK> We propose an analytic framework explaining how methods like SimCLR/BYOL learns mid-level features: we find ""covariance operator"" that drives weight update per-layer in deep ReLU nets. It also shows BN+predictor in BYOL gives an implicit contrastive term.']",http://arxiv.org/abs/2010.00578,"We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a \emph{covariance operator} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a \emph{hierarchical latent tree model} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives \emph{no direct supervision} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in this https URL ",Understanding Self-supervised Learning with Dual Deep Networks
333,1312062735056687104,60856903,Joseph KJ,['We study how meta-consolidation in the parameter space helps to improve continual learning in our #NeurIPS2020 work.\nPre-print: <LINK> \nCode: <LINK>'],https://arxiv.org/abs/2010.00352,"The ability to continuously learn and adapt itself to new tasks, without losing grasp of already acquired knowledge is a hallmark of biological learning systems, which current deep learning systems fall short of. In this work, we present a novel methodology for continual learning called MERLIN: Meta-Consolidation for Continual Learning. We assume that weights of a neural network $\boldsymbol \psi$, for solving task $\boldsymbol t$, come from a meta-distribution $p(\boldsymbol{\psi|t})$. This meta-distribution is learned and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once. Our experiments with continual learning benchmarks of MNIST, CIFAR-10, CIFAR-100 and Mini-ImageNet datasets show consistent improvement over five baselines, including a recent state-of-the-art, corroborating the promise of MERLIN. ",Meta-Consolidation for Continual Learning
334,1320628775965646849,525509345,Julien Bobroff,['How many ways to measure the height of a Building with a Smartphone ? Here is our article with the 61 methods we carried out ! <LINK> and you can find all of them reshaped for fun activities here : <LINK> Coll  @organtin  @VillebonCharpak <LINK>'],https://arxiv.org/abs/2010.11606,"We created an introductory physics activity for undergrad students consisting in measuring by different methods the same physical quantity. It allows to confront students with questions of uncertainty, precision, and model versus theory. The aim was to measure the height of a building using only a smartphone and everyday low-cost equipment. We designed 61 methods to do so and tested most of them. These methods can be implemented in various pedagogical scenarios, engaging students into a concrete task, outside of the lab, easily set up at almost zero cost. ","61 ways to measure the height of a building: an introduction to
  experimental practices"
335,1320047727833681920,1155301458541252608,Piyawat L Kumjorn,"['The pre-print of our #emnlp2020 paper is now on ArXiv. <LINK>\n\nSo, I would like to use this thread to explain what we did. Basically, we proposed FIND -- a framework which enables humans to debug deep text classifiers. \n\nFIND = Feature Investigation aNd Disabling <LINK>', '3Ô∏è‚É£ steps of FIND:\n\n1. Use an interpretability method to collect the patterns each learned feature detects\n\n2. Ask humans to investigate the patterns and disable irrelevant features\n\n3. Fine-tune the model on the training set again to fully exploit the remaining features https://t.co/WLpEGE5qFP', 'For example, using FIND to investigate CNN features for classifying a bio paragraph whether the occupation is ‚Äúsurgeon‚Äù or ‚Äúnurse‚Äù, we found the following patterns. One word cloud shows important n-grams for one feature. \n\nMore are shown here: https://t.co/CxZKqvMwq5 https://t.co/PH4v9L8JxR', 'The first two word clouds look sensible. But the third one detects gender words rather than occupation-related words. That‚Äôs why the model usually misclassifies bios of female surgeons and male nurses. The last word cloud detects university names which are irrelevant to the task.', 'After we disabled some irrelevant features based on Amazon MTurk human judgments, gender bias in the model reduced significantly (as reflected by two bias metrics ‚Äì FPED and FNED), while macro F1 drops only slightly from 95% to 93.4%. https://t.co/qryliEIfWS', 'So, it shows that FIND is effective to debug the biased classifier.\n\nBesides, we have experiments with small training datasets and dataset shift in the paper. Please feel free to check it out: https://t.co/OcQiuX5pXA. \n\nOur github repo is at https://t.co/fm9ilEpwCn.\n\n:)']",https://arxiv.org/abs/2010.04987,"Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets. These classifiers are thus likely to have undesirable properties. For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting. In this paper, we propose FIND -- a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features. Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions). ",FIND: Human-in-the-Loop Debugging Deep Text Classifiers
336,1319628325380378624,182730982,Andreas R√ºckl√©,"['Check out our paper ‚ÄúAdapterDrop""!\n \nWe find that Adapters can train 60% faster than full fine-tuning. With AdapterDrop we increase inference speed by up to 36% for 8 parallel tasks.\n \nGGeigle @Maxxx216 @devnull90 @PfeiffJo NReimers IGurevych @AdapterHub\n<LINK> <LINK>', 'With our *robust* AdapterDrop layers, we improve the efficiency of adapter models during training and inference. By removing adapters from lower layers, based on available computational resources, we enable a dynamic trade-off between inference speed and task performance. https://t.co/oTiMSmEPpn', 'Interestingly, we also find that we can share the adapter weights across *all* layers without losing much performance. This drastically reduces the storage space required for each task even more. https://t.co/7cmEk6jRt1', 'We also reveal significant potential for boosting the efficiency of AdapterFusion. We can drop the majority of adapters after transfer learning from the Fusion model, while maintaining task performance entirely. https://t.co/b4eHQTrs5G']",https://arxiv.org/abs/2010.11918,"Massively pre-trained transformer models are computationally expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely. ",AdapterDrop: On the Efficiency of Adapters in Transformers
337,1319411422887989248,802615428711268352,Shiran Dudy üü£,"['I am thrilled to share with you a recent work of myself and @stevenbedrick  that got accepted to #Eval4NLP workshop ""Are Some Words Worth More than Others"" where we propose to measure word-type diversity in a word prediction task <LINK> #EMNLP2020 #NLProc', 'We show how state-of-the-art models perform poorly on infrequent word-types when evaluated on a next-word-prediction task https://t.co/npueIGWgCI', 'We also demonstrate that this aspect of model performance varies widely between models, and is not captured by traditional evaluation metrics https://t.co/ajF2JY4EEC', ""We use paraphrase sentences as a probe to learn about a model's ability to represent infrequent words in comparison to more frequent synonyms, and found that model performance is clearly affected by word frequency"", ""Overall, we demonstrate that a model's performance depends greatly upon word frequency, and believe that this should be explicitly considered when evaluating language model performance""]",https://arxiv.org/abs/2010.06069,"Current evaluation metrics for language modeling and generation rely heavily on the accuracy of predicted (or generated) words as compared to a reference ground truth. While important, token-level accuracy only captures one aspect of a language model's behavior, and ignores linguistic properties of words that may allow some mis-predicted tokens to be useful in practice. Furthermore, statistics directly tied to prediction accuracy (including perplexity) may be confounded by the Zipfian nature of written language, as the majority of the prediction attempts will occur with frequently-occurring types. A model's performance may vary greatly between high- and low-frequency words, which in practice could lead to failure modes such as repetitive and dull generated text being produced by a downstream consumer of a language model. To address this, we propose two new intrinsic evaluation measures within the framework of a simple word prediction task that are designed to give a more holistic picture of a language model's performance. We evaluate several commonly-used large English language models using our proposed metrics, and demonstrate that our approach reveals functional differences in performance between the models that are obscured by more traditional metrics. ",Are Some Words Worth More than Others?
338,1318665169153785856,1229354866298040320,Lucas Torroba Hennigen,"['How do individual neurons encode bits of linguistic structure in networks trained on language data? We propose a novel generative probe and find most features we investigated are encoded only by a handful of neurons in mBERT for 36 languages: <LINK> #emnlp2020 <LINK>', 'We find that most of the information for linguistic features (e.g., grammatical gender, number and animacy) are located in subsets of 5 to 10 neurons. We are able to find these subsets with a novel generative probe based on a Gaussian assumption.', 'It appears that these features are mostly focally represented in mBERT (and in fastText), but adding more neurons does help a little bit. TL;DR: mBERT encodes most linguistic features in very focal pockets of neurons. https://t.co/r7Fok7ob9H', 'Another exciting bit of related research (coming to a conference near you!) is that mBERT encodes linguistic features using the same neurons across distinct languages. For instance, Hindi and Latvian use the overlapping neurons for number and gender. This was quite unexpected! https://t.co/HzKwXu6sSE', 'Experimental work conducted on 36 languages from the Universal Dependencies corpus. Joint work with @adinamwilliams and @ryandcotterell']",https://arxiv.org/abs/2010.02812,"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT. ",Intrinsic Probing through Dimension Selection
339,1314113611858665472,19626509,Gustavo Penha,"[""We find slices of data üî™ for which neural ranking models underperform and improve them with Slice-Aware Neural Ranking in our new paper to appear on the EMNLP workshop SCAI'20 ! (1/3)\n\n@scai_workshop #EMNLP #SCAI\n<LINK>"", 'We first define slicing functions for IR tasks to identify data for which the model might have low effectiveness and then we adapt SRAMs (https://t.co/e6wHBQ7bqP) from @SnorkelAI to train BERT for ranking with representations that are slice-aware. (2/3)', 'We find that slice-aware models helps on 3 ranking tasks. However, using random SFs also improves the effectiveness of the model, which indicates that slice-based learning gains could be also attributed to its ensemble learning effect and to having more parameters. (3/3)']",https://arxiv.org/abs/2010.03343,"Understanding when and why neural ranking models fail for an IR task via error analysis is an important part of the research cycle. Here we focus on the challenges of (i) identifying categories of difficult instances (a pair of question and response candidates) for which a neural ranker is ineffective and (ii) improving neural ranking for such instances. To address both challenges we resort to slice-based learning for which the goal is to improve effectiveness of neural models for slices (subsets) of data. We address challenge (i) by proposing different slicing functions (SFs) that select slices of the dataset---based on prior work we heuristically capture different failures of neural rankers. Then, for challenge (ii) we adapt a neural ranking model to learn slice-aware representations, i.e. the adapted model learns to represent the question and responses differently based on the model's prediction of which slices they belong to. Our experimental results (the source code and data are available at this https URL) across three different ranking tasks and four corpora show that slice-based learning improves the effectiveness by an average of 2% over a neural ranker that is not slice-aware. ",Slice-Aware Neural Ranking
340,1313644071294955520,762359343656361984,James Zou,"['Telehealth is rapidly growing (esp. due to #COVID19), but poor quality image is a critical challenge. \n\nWe propose a #AI approach to identify photos that are subpar for clinical use and help guide patients to take better photos. New #psb21 paper <LINK> <LINK>', 'Super work by Kailas, @RoxanaDaneshjou @RobNovoaMD @AChiouMD and Justin Ko!']",https://arxiv.org/abs/2010.02086,"Telehealth is an increasingly critical component of the health care ecosystem, especially due to the COVID-19 pandemic. Rapid adoption of telehealth has exposed limitations in the existing infrastructure. In this paper, we study and highlight photo quality as a major challenge in the telehealth workflow. We focus on teledermatology, where photo quality is particularly important; the framework proposed here can be generalized to other health domains. For telemedicine, dermatologists request that patients submit images of their lesions for assessment. However, these images are often of insufficient quality to make a clinical diagnosis since patients do not have experience taking clinical photos. A clinician has to manually triage poor quality images and request new images to be submitted, leading to wasted time for both the clinician and the patient. We propose an automated image assessment machine learning pipeline, TrueImage, to detect poor quality dermatology photos and to guide patients in taking better photos. Our experiments indicate that TrueImage can reject 50% of the sub-par quality images, while retaining 80% of good quality images patients send in, despite heterogeneity and limitations in the training data. These promising results suggest that our solution is feasible and can improve the quality of teledermatology care. ","TrueImage: A Machine Learning Algorithm to Improve the Quality of
  Telehealth Photos"
341,1313109648023719936,182730982,Andreas R√ºckl√©,"[""I'm excited to share ‚ÄúMultiCQA‚Äù, accepted at @EMNLP2020\n\nWe train 140 models on different domains and surprisingly find that neither domain similarity nor data size are critical factors for the best zero-shot transferability.\n\n<LINK>\n\\w  @PfeiffJo IGurevych <LINK>"", 'We train text matching models on all English StackExchange forums with self-supervision. The majority of our 140 models outperforms common IR baselines on non-factoid answer selection and question similarity tasks. https://t.co/7bWWYOoyh1', 'Our zero-shot MultiCQA model incorporates self-supervised and supervised multi-task learning on all source domains, and outperforms the in-domain SoTA on six evaluation benchmarks.', ""@lintool @emnlp2020 @PfeiffJo That's great work, thanks for pointing it out! We must have missed it and will add it to the camera ready version!""]",https://arxiv.org/abs/2010.00980,"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks. ","MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on
  a Massive Scale"
