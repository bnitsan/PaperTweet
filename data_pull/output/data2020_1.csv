,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1228049925587578881,1057452552,Duncan Watson-Parris,"['Our recent #ML paper describing a new, general physics emulator (DENSE; <LINK>) just had a really nice write-up in @sciencemagazine, check it out! <LINK> <LINK>', '@shoyer @rabernat @sciencemagazine Haha, yes there was some artistic license in the article! I suspect the biggest challenge in getting this complexity would just be the data throughput. We only needed fairly low-res runs to answer our question in this case anyway']",https://arxiv.org/abs/2001.08055,"Computer simulations are invaluable tools for scientific discovery. However, accurate simulations are often slow to execute, which limits their applicability to extensive parameter exploration, large-scale data analysis, and uncertainty quantification. A promising route to accelerate simulations by building fast emulators with machine learning requires large training datasets, which can be prohibitively expensive to obtain with slow simulations. Here we present a method based on neural architecture search to build accurate emulators even with a limited number of training data. The method successfully accelerates simulations by up to 2 billion times in 10 scientific cases including astrophysics, climate science, biogeochemistry, high energy density physics, fusion energy, and seismology, using the same super-architecture, algorithm, and hyperparameters. Our approach also inherently provides emulator uncertainty estimation, adding further confidence in their use. We anticipate this work will accelerate research involving expensive simulations, allow more extensive parameters exploration, and enable new, previously unfeasible computational discovery. ","Building high accuracy emulators for scientific simulations with deep
  neural architecture search"
1,1227688887528476672,4692890148,Dr. Ryan Alimo,"['Here is our new paper on ""Assistive Relative Pose Estimation for On-orbit Assembly using Convolutional Neural Networks"" <LINK> w/ @sdsonawani @asurobot Renaud Detry, and my former interns Daniel Jeong and Andrew Hess from @Caltech  @Columbia #ai #space @NASAJPL']",https://arxiv.org/abs/2001.10673,"Accurate real-time pose estimation of spacecraft or object in space is a key capability necessary for on-orbit spacecraft servicing and assembly tasks. Pose estimation of objects in space is more challenging than for objects on Earth due to space images containing widely varying illumination conditions, high contrast, and poor resolution in addition to power and mass constraints. In this paper, a convolutional neural network is leveraged to uniquely determine the translation and rotation of an object of interest relative to the camera. The main idea of using CNN model is to assist object tracker used in on space assembly tasks where only feature based method is always not sufficient. The simulation framework designed for assembly task is used to generate dataset for training the modified CNN models and, then results of different models are compared with measure of how accurately models are predicting the pose. Unlike many current approaches for spacecraft or object in space pose estimation, the model does not rely on hand-crafted object-specific features which makes this model more robust and easier to apply to other types of spacecraft. It is shown that the model performs comparable to the current feature-selection methods and can therefore be used in conjunction with them to provide more reliable estimates. ","Assistive Relative Pose Estimation for On-orbit Assembly using
  Convolutional Neural Networks"
2,1227615408900825091,1160578862419345408,"Takinoue Lab, Tokyo Tech","['Our new paper ""Surfactant concentration modulates the motion and placement of microparticles in an inhomogeneous electric field"" by M. Masukawa (@MarcosMasukawa ) et al. has been accepted to RSC Advances.\n(The paper can also be seen in arXiv: <LINK>) <LINK>', '@t_z_jia @MarcosMasukawa Thank you, Tony @t_z_jia']",https://arxiv.org/abs/2001.01359,"This study examined the effects of surfactants on the motion and positioning of microparticles in an inhomogeneous electric field. The microparticles were suspended in oil with a surfactant and the electric field was generated using sawtooth-patterned electrodes. The microparticles were trapped, oscillated, or attached to the electrodes. The proportion of microparticles in each state was defined by the concentration of surfactant and the voltage applied to the electrodes. Based on the trajectory of the microparticles in the electric field, a newly developed physical model in which the surfactant was adsorbed on the microparticles allowed the microparticles to be charged by contact with the electrodes, with either positive or negative charges, while the non-adsorbed surfactant micellizing in the oil contributed to charge relaxation. A simulation based on this model showed that the charging and charge relaxation, as modulated by the surfactant concentration, can explain the trajectories and proportion of the trapped, oscillating, and attached microparticles. These results will be useful for the development of novel self-assembly and transport technologies and colloids sensitive to electricity. ","Surfactant concentration modulates the motion and placement of
  microparticles in an inhomogeneous electric field"
3,1227517474662281216,1075649842955866114,Luca Cortese,['New paper by joint @ICRAR / China @SKA_telescope PhD student Wenkai Hu using HI stacking to show the importance of massive galaxies in the atomic hydrogen mass budget of the local Universe... and highlight tensions with current models. <LINK> @ARC_ASTRO3D <LINK>'],https://arxiv.org/abs/2001.09073,"We use spectral stacking to measure the contribution of galaxies of different masses and in different hierarchies to the cosmic atomic hydrogen (HI) mass density in the local Universe. Our sample includes 1793 galaxies at $z < 0.11$ observed with the Westerbork Synthesis Radio Telescope, for which Sloan Digital Sky Survey spectroscopy and hierarchy information are also available. We find a cosmic HI mass density of $\Omega_{\rm HI} = (3.99 \pm 0.54)\times 10^{-4} h_{70}^{-1}$ at $\langle z\rangle = 0.065$. For the central and satellite galaxies, we obtain $\Omega_{\rm HI}$ of $(3.51 \pm 0.49)\times 10^{-4} h_{70}^{-1}$ and $(0.90 \pm 0.16)\times 10^{-4} h_{70}^{-1}$, respectively. We show that galaxies above and below stellar masses of $\sim$10$^{9.3}$ M$_{\odot}$ contribute in roughly equal measure to the global value of $\Omega_{\rm HI}$. While consistent with estimates based on targeted HI surveys, our results are in tension with previous theoretical work. We show that these differences are, at least partly, due to the empirical recipe used to set the partition between atomic and molecular hydrogen in semi-analytical models. Moreover, comparing our measurements with the cosmological semi-analytic models of galaxy formation {\sc Shark} and GALFORM reveals gradual stripping of gas via ram pressure works better to fully reproduce the properties of satellite galaxies in our sample, than strangulation. Our findings highlight the power of this approach in constraining theoretical models, and confirm the non-negligible contribution of massive galaxies to the HI mass budget of the local Universe. ","The cosmic atomic hydrogen mass density as a function of mass and galaxy
  hierarchy from spectral stacking"
4,1225077329862500354,910621028212203521,Prof Rachel Oliver 🐯,"[""If you can't make Stefan Schulz's talk tomorrow, then read our preprint here instead: <LINK>\nWhy are green LEDs less efficient than blue ones? Major question with implications for more efficient solid state lighting, and this paper gives new insights! <LINK>""]",https://arxiv.org/abs/2001.09345,"We present a detailed theoretical analysis of the electronic and optical properties of c-plane InGaN/GaN quantum well structures with In contents ranging from 5% to 25%. Special attention is paid to the relevance of alloy induced carrier localization effects to the green gap problem. Studying the localization length and electron-hole overlaps at low and elevated temperatures, we find alloy-induced localization effects are crucial for the accurate description of InGaN quantum wells across the range of In content studied. However, our calculations show very little change in the localization effects when moving from the blue to the green spectral regime; i.e. when the internal quantum efficiency and wall plug efficiencies reduce sharply, for instance, the in-plane carrier separation due to alloy induced localization effects change weakly. We conclude that other effects, such as increased defect densities, are more likely to be the main reason for the green gap problem. This conclusion is further supported by our finding that the electron localization length is large, when compared to that of the holes, and changes little in the In composition range of interest for the green gap problem. Thus electrons may become increasingly susceptible to an increased (point) defect density in green emitters and as a consequence the nonradiative recombination rate may increase. ","Polar InGaN/GaN quantum wells: Revisiting the impact of carrier
  localization on the green gap problem"
5,1224607749012672512,2242569434,Matthijs Maas,"[""This week I'll be at @AIESConf in New York, to present 'Should #AIGovernance be Centralised?', the new paper by @LukaKemp , @pcihon and myself.  <LINK> -- we review regimes in environment, trade, security... to sketch considerations and trade-offs--"", 'Come check out our poster (a pretty one! with colour-coding!), and chat to me after our lightning talk on Friday (or literally any moment).', ""Also a shout out to my former student Suvradip Maitra, who's been accepted to present a really awesome piece on 'Indigenous Perspectives on AI'--go check him out if you're around\nhttps://t.co/AxutcQ5yn6""]",https://arxiv.org/abs/2001.03573,"Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate. ","Should Artificial Intelligence Governance be Centralised? Design Lessons
  from History"
6,1224516482861654017,1075649842955866114,Luca Cortese,"['New paper by former @ICRAR @UWAresearch postdoc Steven Janowiecki looking at the HI content of galaxies below the star-forming main sequence. For understanding the physics of SF quenching, we really need to look at HI. @ARC_ASTRO3D #xGASS  <LINK> <LINK>']",http://arxiv.org/abs/2001.06614,"We use HI and H2 global gas measurements of galaxies from xGASS and xCOLD GASS to investigate quenching paths of galaxies below the star formation main sequence (SFMS). We show that the population of galaxies below the SFMS is not a 1:1 match with the population of galaxies below the HI and H2 gas fraction scaling relations. Some galaxies in the transition zone (TZ) 1-sigma below the SFMS can be as HI-rich as those in the SFMS, and have on average longer gas depletion timescales. We find evidence for environmental quenching of satellites, but central galaxies in the TZ defy simple quenching pathways. Some of these so-called ""quenched"" galaxies may still have significant gas reservoirs and be unlikely to deplete them anytime soon. As such, a correct model of galaxy quenching cannot be inferred with SFR (or other optical observables) alone, but must include observations of the cold gas. We also find that internal structure (particularly, the spatial distribution of old and young stellar populations) plays a significant role in regulating the star formation of gas-rich isolated TZ galaxies, suggesting the importance of bulges in their evolution. ","xGASS: Cold gas content and quenching in galaxies below the star forming
  main sequence"
7,1224406752092770309,6334772,eytan bakshy,['Why do random embedding methods for high-dimensional Bayesian optimization produce inconsistent results? Find out in our new paper w/ @_bletham_ et al <LINK>. Implementation of new method now in Ax. See  <LINK> for usage + replication materials.'],https://arxiv.org/abs/2001.11659,"Bayesian optimization (BO) is a popular approach to optimize expensive-to-evaluate black-box functions. A significant challenge in BO is to scale to high-dimensional parameter spaces while retaining sample efficiency. A solution considered in existing literature is to embed the high-dimensional space in a lower-dimensional manifold, often via a random linear embedding. In this paper, we identify several crucial issues and misconceptions about the use of linear embeddings for BO. We study the properties of linear embeddings from the literature and show that some of the design choices in current approaches adversely impact their performance. We show empirically that properly addressing these issues significantly improves the efficacy of linear embeddings for BO on a range of problems, including learning a gait policy for robot locomotion. ","Re-Examining Linear Embeddings for High-Dimensional Bayesian
  Optimization"
8,1224367305498353664,1095182290689318912,Jon Zink,"['I have a new paper out today!\n\n""Scaling K2. II. Assembly of a Fully Automated C5 Planet Candidate Catalog Using EDI-Vetter""\n\n <LINK> \n\nThe first fully automated analysis of transiting K2 exoplanets. <LINK>', 'You need 4 ingredients to calculate an exoplanet occurrence rate: \n\n1. A uniform stellar sample. \n\n@kevinkhu just released a paper creating such a sample. \n\n""Scaling K2. I. Revised Parameters for 222,088 K2 Stars and a K2 Planet Radius Valley at 1.9 R⊕"" \nhttps://t.co/iMWNLUAPJf https://t.co/gf4EuFxe0x', '2. A uniformly detected planet sample.\n\nOur paper introduces EDI-Vetter. A fully automated vetting machine able to parse through the data and find the true planet signals. @EddieVedderRock\n\nWe found 75 candidates, including 8 new planets! https://t.co/wDr8TaTPCr', '3. A measure of the false negatives (the number of un-detected planets) in your sample. \n\nWe inject false signals into the data and see how many we recover. https://t.co/9JvIcIdmDn', '4. A measure of the false positives (the number of signals that look like planets but are not) in your sample. \n\nWe flip the light curves (removing all true planet signals) and run our detection pipeline looking for signals that look like planets. https://t.co/GRr60tnxzU', ""Put it all together and you've got the ingredients for an exoplanet occurrence rate.\n\nWatch out for an upcoming paper on the our first analysis of this K2 C5 data! https://t.co/VW93eWb2Ya"", 'Special thanks to all the collaborates on this project: \n@kevinkhu, @aussiastronomer, @AstroDressing, Ian Crossfield, Erik Petigura, @JoshuaSchlieder, and David Ciardi. \n\nLook out for more to come in the near future from the ""Scaling K2"" series!', '@EddieVedderRock @PearlJam']",https://arxiv.org/abs/2001.11515,"We present a uniform transiting exoplanet candidate list for Campaign 5 of the K2 mission. This catalog contains 75 planets with 7 multi-planet systems (5 double, 1 triple, and 1 quadruple planet system). Within the range of our search, we find 8 previously undetected candidates with the remaining 66 candidates overlapping 51% of the Kruse et al. study that manually vet Campaign 5 candidates. In order to vet our potential transit signals, we introduce the Exoplanet Detection Identification Vetter (EDI-Vetter), which is a fully automated program able to determine if a transit signal should be labeled as a false positive or a planet candidate. This automation allows us to create a statistically uniform catalog, ideal for planet occurrence rate measurements. When tested, the vetting software is able to ensure our sample is 94.2% reliable against systematic false positives. Additionally, we inject artificial transits at the light-curve-level of the raw K2 data and find the maximum completeness of our pipeline is 70% before vetting and 60% after vetting. For convenience of future occurrence rate studies, we include measurements of stellar noise (CDPP) and the three-transit window function for each target. This study is part of a larger survey of the K2 data set and the methodology which will be applied to the entirety of the K2 data set. ","Scaling K2. II. Assembly of a Fully Automated C5 Planet Candidate
  Catalog Using EDI-Vetter"
9,1224341303175131142,1146856263919292416,Julia E. Stawarz,"['Check out our new paper on inverse cascades in Hall MHD #turbulence, which was just accepted for publication! \nThe work looks at how small-scale plasma effects alter the generation of large-scale magnetic structure in turbulent systems. #space #plasma\n<LINK>']",https://arxiv.org/abs/2001.11625,"In turbulence, for neutral or conducting fluids, a large ratio of scales is excited because of the possible occurrence of inverse cascades to large, global scales together with direct cascades to small, dissipative scales, as observed in the atmosphere and oceans, or in the solar environment. In this context, using direct numerical simulations with forcing, we analyze scale dynamics in the presence of magnetic fields with a generalized Ohm's law including a Hall current. The ion inertial length epsilon_H serves as the control parameter at fixed Reynolds number. Both the magnetic and generalized helicity -- invariants in the ideal case -- grow linearly with time, as expected from classical arguments. The cross-correlation between the velocity and magnetic field grows as well, more so in relative terms for a stronger Hall current. We find that the helical growth rates vary exponentially with epsilon_H, provided the ion inertial scale resides within the inverse cascade range. These exponential variations are recovered phenomenologically using simple scaling arguments. They are directly linked to the wavenumber power-law dependence of generalized and magnetic helicity, k^(-2), in their inverse ranges. This illustrates and confirms the important role of the interplay between large and small scales in the dynamics of turbulent flows. ","Coupling large eddies and waves in turbulence: Case study of magnetic
  helicity at the ion inertial scale"
10,1224268965154869248,322636963,Jonathan Berant,"['New TACL paper involving a lot of hard work from my twitter-less student Tomer, along with great collab. at AI2 and TAU. Paper/website at <LINK> <LINK>  @megamor2 @yoavgo @nlpmattg @ankgup2 1/2 <LINK>', 'We define a mean. rep. (QDMR) that decomposes questions to a sequence of steps that can be executed against any context (image, text, DB),  crowdsource &gt;80K question-QDMR pairs using questions from 10 existing datasets, show usefulness for RC and release a QDMR parser. Enjoy! 2/2']",https://arxiv.org/abs/2001.11770v1,"Understanding natural language questions entails the ability to break down a question into the requisite steps for computing its answer. In this work, we introduce a Question Decomposition Meaning Representation (QDMR) for questions. QDMR constitutes the ordered list of steps, expressed through natural language, that are necessary for answering a question. We develop a crowdsourcing pipeline, showing that quality QDMRs can be annotated at scale, and release the Break dataset, containing over 83K pairs of questions and their QDMRs. We demonstrate the utility of QDMR by showing that (a) it can be used to improve open-domain question answering on the HotpotQA dataset, (b) it can be deterministically converted to a pseudo-SQL formal language, which can alleviate annotation in semantic parsing applications. Last, we use Break to train a sequence-to-sequence model with copying that parses questions into QDMR structures, and show that it substantially outperforms several natural baselines. ",] Break It Down: A Question Understanding Benchmark
11,1224176743121969158,17933536,Kevin Hardegree-Ullman,"['NEW PAPER ALERT!\n\n""Scaling K2. I. Revised Parameters for 222,088 K2 Stars and a K2 Planet Radius Valley at 1.9 R⊕""\n\n<LINK> (soon in ApJS!)\n\nThis is the first paper in the ""Scaling K2"" series, with the ultimate goal of computing planet occurrence rates. But... <LINK>', 'Before we can do anything with the planets, we must know something about the stars that host them: KNOW THY STAR, KNOW THY PLANET!\n\nIt is important to measure things like stellar radii, masses, temperatures, etc. in a homogeneous manner for occurrence rates. https://t.co/L2gLOXQ0yY', 'K2 (revitalized Kepler spacecraft) stared at a bunch of different fields along the ecliptic plane for ~80 days each in search of planets transiting their host star. \n\nTo measure the transiting planet radius, we first need to know the host star radius. https://t.co/qMxpcyPQSx', 'The Ecliptic Plane Input Catalog (EPIC; Huber+ 2016) characterized about ~300,000 stars for K2 mostly using proper motions and colors.\n\nThen, something completely expected happened. Gaia DR2 came and changed everything... https://t.co/nLWgCzWtji', 'Gaia measured distances to 1.3 billion stars! But Gaia is not alone in the era of Big Astronomical Data™. Pan-STARRS DR2 is 1.6 petabytes of photometry, and LAMOST DR5 has over 9 million spectra! https://t.co/HWloalVE4w', 'We updated some of the photometry in the EPIC using Pan-STARRS, used the https://t.co/8gnPkpb4mQ database to cross-match Gaia and K2 targets, and cross-matched the K2 targets with LAMOST to find ~27,000 K2 targets with a spectrum. https://t.co/ofBlTwRTGu', 'The LAMOST pipeline provides spectral types, temperatures, surface gravities, and metallicities for A, F, G, and K stars, but not M dwarfs. Luckily, I know a thing or two about M dwarfs (https://t.co/KKGbC5ctnn). https://t.co/jLh0OIcqQD', 'Using the spectroscopic target photometry and parameters as a training set, we used machine learning to derive spectral types, temperatures, surface gravities, and metallicities for targets without a spectrum. https://t.co/WTAc1dmvVG', 'We used the Stefan-Boltzmann law to compute radii for AFGK stars, and absolute magnitude-radius relations from Mann+ 2015 for M dwarfs. In total we have new stellar parameters for 222,088 stars! https://t.co/84nlRX5ZyI', 'Using our new stellar parameters, we re-derived radii for 299 confirmed and 517 candidate planets and something magical happened: WE FOUND A DEFINITIVE PLANET RADIUS GAP FOR THE FIRST TIME IN A DATA SET OTHER THAN KEPLER! https://t.co/IAlZkHbJhx', ""This means that the Kepler result is not a fluke, and since K2 probes different regions in the Galaxy, this result is not exclusive to the Kepler field!\n\nHere's how the radius valley looks in relation to incident stellar flux (left), and compared Kepler (right). https://t.co/zN5ka4wilO"", 'This result would not have been easily identifiable without a uniform set of stellar parameters. https://t.co/nw5qHdkTsB', 'Many thanks to @jonKzink, @aussiastronomer, @AstroDressing, David R. Ciardi, and @JoshuaSchlieder for their important contributions to this paper! https://t.co/Tl9tmNSHyD', 'BUT WAIT.... THERE\'S MORE!\n\nGo read ""Scaling K2. II. Assembly of a Fully Automated C5 Planet Candidate Catalog Using EDI-Vetter"", led by @jonKzink!\n\nhttps://t.co/jtvyB8u89w https://t.co/I2nEfPQjmf', 'K2 Planet Occurrence Rates, here we come... https://t.co/wGCqZ8VHy8', ""@amannastro It appears to be an artifact of the random forest. Few LAMOST targets were &lt;3500K and fainter than M_K=6, so it probably latched onto mostly warmer and sometimes brighter targets. There's definitely room for improvement on the photometric M dwarf Teffs."", '@amannastro Absolutely. I think K dwarfs are probably the best kept open secret in exoplanets. Probably a poor choice to down-weight for TESS.']",https://arxiv.org/abs/2001.11511,"Previous measurements of stellar properties for K2 stars in the Ecliptic Plane Input Catalog (EPIC; Huber et al. 2016) largely relied on photometry and proper motion measurements, with some added information from available spectra and parallaxes. Combining Gaia DR2 distances with spectroscopic measurements of effective temperatures, surface gravities, and metallicities from the Large Sky Area Multi-Object Fibre Spectroscopic Telescope (LAMOST) DR5, we computed updated stellar radii and masses for 26,838 K2 stars. For 195,250 targets without a LAMOST spectrum, we derived stellar parameters using random forest regression on photometric colors trained on the LAMOST sample. In total, we measured spectral types, effective temperatures, surface gravities, metallicities, radii, and masses for 222,088 A, F, G, K, and M-type K2 stars. With these new stellar radii, we performed a simple reanalysis of 299 confirmed and 517 candidate K2 planet radii from Campaigns 1--13, elucidating a distinct planet radius valley around 1.9 $R_{\oplus}$, a feature thus far only conclusively identified with Kepler planets, and tentatively identified with K2 planets. These updated stellar parameters are a crucial step in the process toward computing K2 planet occurrence rates. ","Scaling K2. I. Revised Parameters for 222,088 K2 Stars and a K2 Planet
  Radius Valley at 1.9 $R_{\oplus}$"
12,1224153805912268800,58605190,Jeremy Siek,['New paper draft:\nToward a Mechanized Compendium of Gradual Typing\n<LINK>\nin which I use Agda and lots of proof reuse to get 9 proofs of type safety for the price of 2.'],http://arxiv.org/abs/2001.11560,"The research on gradual typing has led to many variations on the Gradually Typed Lambda Calculus (GTLC) of Siek and Taha (2006) and its underlying cast calculus. For example, Wadler and Findler (2009) added blame tracking, Siek et al. (2009) investigated alternate cast evaluation strategies, and Herman et al. (2010) replaced casts with coercions for space-efficiency. The meta-theory for the GTLC has also expanded beyond type safety to include blame safety (Tobin-Hochstadt and Felleisen 2006), space consumption (Herman et al. 2010), and the gradual guarantees (Siek et al. 2015). These results have been proven for some variations of the GTLC but not others. Furthermore, researchers continue to develop variations on the GTLC but establishing all of the meta-theory for new variations is time consuming. This article identifies abstractions that capture similarities between many cast calculi in the form of two parameterized cast calculi, one for the purposes of language specification and the other to guide space-efficient implementations. The article then develops reusable meta-theory for these two calculi, proving type safety, blame safety, the gradual guarantees, and space consumption. Finally, the article instantiates this meta-theory for eight cast calculi including five from the literature and three new calculi. All of these definitions and theorems, including the two parameterized calculi, the reusable meta-theory, and the eight instantiations, are mechanized in Agda making extensive use of module parameters and dependent records to define the abstractions. ","Parameterized Cast Calculi and Reusable Meta-theory for Gradually Typed
  Lambda Calculi"
13,1223061740218281985,746440524052082688,Nicolas Delfosse,"['New paper revealing my two-step plan to reduce the cost of fault-tolerant quantum computing:\n      Step 1: Ask for better qubits.\n      Step 2: Use the Lazy Decoder to get a 1,500x reduction in decoding hardware and bandwidth.\n<LINK>\n\n#QuantumComputing', ""@arne_grimsmo Great @arne_grimsmo ;). So I need qubits with error rate &lt;10^-4. It's the regime needed for the Lazy Decoder.""]",https://arxiv.org/abs/2001.11427,"Extensive quantum error correction is necessary in order to scale quantum hardware to the regime of practical applications. As a result, a significant amount of decoding hardware is necessary to process the colossal amount of data required to constantly detect and correct errors occurring over the millions of physical qubits driving the computation. The implementation of a recent highly optimized version of Shor's algorithm to factor a 2,048-bits integer would require more 7 TBit/s of bandwidth for the sole purpose of quantum error correction and up to 20,000 decoding units. To reduce the decoding hardware requirements, we propose a fault-tolerant quantum computing architecture based on surface codes with a cheap hard-decision decoder, the lazy decoder, combined with a sophisticated decoding unit that takes care of complex error configurations. Our design drops the decoding hardware requirements by several orders of magnitude assuming that good enough qubits are provided. Given qubits and quantum gates with a physical error rate $p=10^{-4}$, the lazy decoder drops both the bandwidth requirements and the number of decoding units by a factor 50x. Provided very good qubits with error rate $p=10^{-5}$, we obtain a 1,500x reduction in bandwidth and decoding hardware thanks to the lazy decoder. Finally, the lazy decoder can be used as a decoder accelerator. Our simulations show a 10x speed-up of the Union-Find decoder and a 50x speed-up of the Minimum Weight Perfect Matching decoder. ","Hierarchical decoding to reduce hardware requirements for quantum
  computing"
14,1222942113425313792,2411222281,Dr./Prof. Meredith MacGregor,"['New paper alert! <LINK> - We present new millimeter flares detected from the M dwarf AU Mic with ALMA, and speculate about the origins of this previously unknown emission by placing these new detections in context with similar flares from Proxima Centauri.']",https://arxiv.org/abs/2001.10546,"We report on two millimeter flares detected by ALMA at 220 GHz from AU Mic, a nearby M dwarf. The larger flare had a duration of only $\sim35$ sec, with peak $L_{R}=2\times10^{15}$ erg s$^{-1}$ Hz$^{-1}$, and lower limit on linear polarization of $|Q/I|>0.12\pm0.04$. We examine the characteristics common to these new AU Mic events and those from Proxima Cen previously reported in MacGregor et al. (2018) - namely short durations, negative spectral indices, and significant linear polarization - to provide new diagnostics of conditions in outer stellar atmospheres and details of stellar flare particle acceleration. The event rates ($\sim20$ and $4$ events day$^{-1}$ for AU Mic and Proxima Cen, respectively) suggest that millimeter flares occur commonly but have been undetected until now. Analysis of the flare observing frequency and consideration of possible incoherent emission mechanisms confirms the presence of MeV electrons in the stellar atmosphere occurring as part of the flare process. The spectral indices point to a hard distribution of electrons. The short durations and lack of pronounced exponential decay in the light curve are consistent with formation in a simple magnetic loop, with radio emission predominating from directly precipitating electrons. We consider the possibility of both synchrotron and gyrosynchrotron emission mechanisms, although synchrotron is favored given the linear polarization signal. This would imply that the emission must be occurring in a low density environment of only modest magnetic field strength. A deeper understanding of this newly discovered and apparently common stellar flare mechanism awaits more observations with better-studied flare components at other wavelengths. ",Properties of M Dwarf Flares at Millimeter Wavelengths
15,1222834879764815872,1169196060130123782,Gergely Neu,"['is it possible to gain anything by abstaining from prediction in online learning?\n\nour new paper with the brilliant Nikita Zhivotovskiy shows for the first time (?) that MASSIVE improvements are possible!\n\n<LINK>', '... all without requiring that the cost of abstention be very small: it only needs to be slightly smaller than the cost of predicting a random label.', ""@amirsani here's my recommended probabilistic strategy: P[Pass] = 1"", ""**exclusive twitter-only content:**\n\nthe figure we tried to paste into the paper to illustrate the key proof idea, but couldn't manage to do so in overleaf :P\n\nenjoy! https://t.co/KwTr4vpGex""]",https://arxiv.org/abs/2001.10623,"In the setting of sequential prediction of individual $\{0, 1\}$-sequences with expert advice, we show that by allowing the learner to abstain from the prediction by paying a cost marginally smaller than $\frac 12$ (say, $0.49$), it is possible to achieve expected regret bounds that are independent of the time horizon $T$. We exactly characterize the dependence on the abstention cost $c$ and the number of experts $N$ by providing matching upper and lower bounds of order $\frac{\log N}{1-2c}$, which is to be contrasted with the best possible rate of $\sqrt{T\log N}$ that is available without the option to abstain. We also discuss various extensions of our model, including a setting where the sequence of abstention costs can change arbitrarily over time, where we show regret bounds interpolating between the slow and the fast rates mentioned above, under some natural assumptions on the sequence of abstention costs. ",Fast Rates for Online Prediction with Abstention
16,1222833648988065792,2902687319,Mike Hudson,"['We have new paper led by Tianyi Yang, with @nafshordi, on the luminous content and structure of filaments between massive haloes. Mass-to-light ratios of filaments similar to the cosmic average. <LINK> <LINK>', '@nafshordi And (what a coincidence!) I will present this at #LocalWeb20 tomorrow ;)']",https://arxiv.org/abs/2001.10943v1,"The cold dark matter model predicts that dark matter haloes are connected by filaments. Direct measurements of the masses and structure of these filaments are difficult, but recently several studies have detected these dark-matter-dominated filaments using weak lensing. Here we study the efficiency of galaxy formation within the filaments by measuring their total mass-to-light ratios and stellar mass fractions. Specifically, we stack pairs of Luminous Red Galaxies (LRGs) with a typical separation on the sky of $8 h^{-1}$ Mpc. We stack background galaxy shapes around pairs to obtain mass maps through weak lensing, and we stack galaxies from the Sloan Digital Sky Survey (SDSS) to obtain maps of light and stellar mass. To isolate the signal from the filament, we construct two matched catalogues of physical and non-physical (projected) LRG pairs, with the same distributions of redshift and separation. We then subtract the two stacked maps. Using LRG pair samples from the BOSS survey at two different redshifts, we find that the evolution of the mass in filament is consistent with the predictions from perturbation theory. The filaments are not entirely dark: their mass-to-light ratios ($M/L = 351\pm87$ in solar units in the $r$-band) and stellar mass fractions ($M_{\rm stellar}/M = 0.0073\pm0.0020$) are consistent with the cosmic values (and with their redshift evolutions) ",] How dark are filaments in the cosmic web?
17,1222805051699318784,130791303,Daniel Nüst,"['New preprint just out 📄🆕📣 ""The Rockerverse: Packages and Applications for Containerization with R""\n\n<LINK>\n\n@Docker #RockerProject #Containers #rstats #ReproducibleResearch\n\nJust in time for #rstudioconf2020 🙂\n\nFeedback welcome! <LINK> <LINK>', 'Big thanks go out to all 20 coauthors: @eddelbuettel @dominicjbennett @rcannood @davclark @daroczig @HoloMarkeD @_ColinFay @ellis_hughes @lopp_sean @benmarwick @skyetetra @heatherklus Hong Ooi @_inundata @noamross Lori Shepherd @niteshturaga Craig Willis @nanxstats C. Van Petegem']",https://arxiv.org/abs/2001.10641,"The Rocker Project provides widely used Docker images for R across different application scenarios. This article surveys downstream projects that build upon the Rocker Project images and presents the current state of R packages for managing Docker images and controlling containers. These use cases cover diverse topics such as package development, reproducible research, collaborative work, cloud-based data processing, and production deployment of services. The variety of applications demonstrates the power of the Rocker Project specifically and containerisation in general. Across the diverse ways to use containers, we identified common themes: reproducible environments, scalability and efficiency, and portability across clouds. We conclude that the current growth and diversification of use cases is likely to continue its positive impact, but see the need for consolidating the Rockerverse ecosystem of packages, developing common practices for applications, and exploring alternative containerisation software. ",The Rockerverse: Packages and Applications for Containerization with R
18,1222548229696499714,717162062837719040,Phil Armitage,"['New paper! In work led by Dan Gole, we find that modest levels of protoplanetary disk turbulence stop solids from collapsing into planetesimals. We think turbulence is strong close to the star, so this may hinder in situ formation of close-in exoplanets.\n\n<LINK>', 'Main difference compared to most prior work: we use small domains and force fluid turbulence with specified properties. This allows 10-100x better spatial resolution, at the expense of capturing large-scale structure in the turbulence (which may well be important).', 'Big picture: I still think the streaming instability is the best candidate planetesimal formation mechanism. But both analytic and simulation evidence now suggests that while the streaming instability is unavoidable, reaching conditions for gravitational collapse is non-trivial.', ""Planetesimal formation may need a chain of processes to concentrate solid material: zonal flows, vortices, the streaming instability... If so,  the efficiency goes down (may be OK observationally), and there may be places where planetesimals can't form at all.""]",https://arxiv.org/abs/2001.10000,"We study how the interaction between the streaming instability and intrinsic gas-phase turbulence affects planetesimal formation via gravitational collapse in protoplanetary disks. Turbulence impedes the formation of particle clumps by acting as an effective turbulent diffusivity, but it can also promote planetesimal formation by concentrating solids, for example in zonal flows. We quantify the effect of turbulent diffusivity using numerical simulations of the streaming instability in small local domains, forced with velocity perturbations that establish approximately Kolmogorov-like turbulence. We find that planetesimal formation is suppressed by turbulence once velocity fluctuations exceed $\langle \delta v^2 \rangle \simeq 10^{-3.5} - 10^{-3} c_s^2$. Turbulence whose strength is just below the threshold reduces the rate at which solids are bound into clumps. Our results suggest that the well-established turbulent thickening of the mid-plane solid layer is the primary mechanism by which turbulence influences planetesimal formation and that planetesimal formation requires a mid-plane solid-to-gas ratio $\epsilon \gtrsim 0.5$. We also quantify the initial planetesimal mass function using a new clump-tracking method to determine each planetesimal mass shortly after collapse. For models in which planetesimals form, we show that the mass function is well-described by a broken power law, whose parameters are robust to the inclusion and strength of imposed turbulence. Turbulence in protoplanetary disks is likely to substantially exceed the threshold for planetesimal formation at radii where temperatures $T \gtrsim 10^3 \ {\rm K}$ lead to thermal ionization. Planetesimal formation may therefore be unviable in the inner disk out to 2-3 times the dust sublimation radius. ","Turbulence Regulates the Rate of Planetesimal Formation via
  Gravitational Collapse"
19,1222512069754478593,1215586752980824065,Amos Turchet,['new paper is out!\n\n<LINK>\n\n#arXiv #DiophantineGeometry #CampanaConjectures #Research'],https://arxiv.org/abs/2001.10229,"We construct a family of fibered threefolds $X_m \to (S , \Delta)$ such that $X_m$ has no \'etale cover that dominates a variety of general type but it dominates the orbifold $(S,\Delta)$ of general type. Following Campana, the threefolds $X_m$ are called \emph{weakly special} but not \emph{special}. The Weak Specialness Conjecture predicts that a weakly special variety defined over a number field has a potentially dense set of rational points. We prove that if $m$ is big enough the threefolds $X_m$ present behaviours that contradict the function field and analytic analogue of the Weak Specialness Conjecture. We prove our results by adapting the recent method of Ru and Vojta. We also formulate some generalizations of known conjectures on exceptional loci that fit into Campana's program and prove some cases over function fields. ",Nonspecial varieties and Generalized Lang-Vojta conjectures
20,1222432508928843776,2901786546,Pau Ramos,"['#GaiaDR2 keeps on delivering! Our new paper is now on the ArXiv: <LINK>\n\nIn the work led by @AntojaTeresa we have detected the Sagittarius stream all around the sky from proper motions alone. The result: a ~300.000 stars sample (dwarf+tails) [1/3] <LINK>', 'And all that... Without downloading a single star!! Instead, we pulled the proper motion histograms directly from the #GaiaMission Archive and processed them on the fly.\n\nYou can find more info at https://t.co/1Z659VaPAW. Stay tuned for news in the upcoming weeks! [2/3]', 'Once we isolated the proper motion peaks belonging to Sagittarius, we finally downloaded the individual stars of the stream. The catalogue will be available soon...\n\nIn the meantime, we leave you with this colour-magnitude diagram [3/3]. https://t.co/q72uZ67zV4']",https://arxiv.org/abs/2001.10012,"We aim to measure the proper motion along the Sagittarius stream that is the missing piece to determine its full 6D phase space coordinates. We conduct a blind search of over-densities in proper motion from Gaia DR2 in a broad region around the Sagittarius stream by applying wavelet transform techniques. We find that for most of the sky patches, the highest intensity peaks delineate the path of the Sagittarius stream. The 1500 peaks identified depict a continuous sequence spanning almost $2\pi$ in the sky, only obscured when the stream crosses the Galactic disk. Altogether, around $100\,000$ stars potentially belong to the stream as indicated by a coarse inspection of the colour-magnitude diagrams. From these stars, we determine the proper motion along the Sagittarius stream, making it the proper motion sequence with the largest span and continuity ever measured for a stream. A first comparison with existing N-body models of the stream reveals some discrepancies, especially near the pericentre of the trailing arm and an overestimation of the total proper motion for the leading arm. Our study can be the starting point for determining the variation of the population of stars along the stream, the distance to the stream with red clump stars, and the solar motion. It will also allow a much better measurement of the Milky Way potential. ",An all-sky proper motion map of the Sagittarius stream using Gaia DR2
21,1222359627968499712,2766925212,Andrew Childs,"['New paper from Daochen Wang (@JointQuICS) answers the question ""can graph properties have exponential quantum speedup?"" with a resounding ""it depends."" <LINK>', '@JointQuICS (See also https://t.co/bSle537nQa from Ben-David and Podder.)']",http://arxiv.org/abs/2001.10520,"Quantum computers can sometimes exponentially outperform classical ones, but only for problems with sufficient structure. While it is well known that query problems with full permutation symmetry can have at most polynomial quantum speedup -- even for partial functions -- it is unclear how far this condition must be relaxed to enable exponential speedup. In particular, it is natural to ask whether exponential speedup is possible for (partial) graph properties, in which the input describes a graph and the output can only depend on its isomorphism class. We show that the answer to this question depends strongly on the input model. In the adjacency matrix model, we prove that the bounded-error randomized query complexity $R$ of any graph property $\mathcal{P}$ has $R(\mathcal{P}) = O(Q(\mathcal{P})^{6})$, where $Q$ is the bounded-error quantum query complexity. This negatively resolves an open question of Montanaro and de Wolf in the adjacency matrix model. More generally, we prove $R(\mathcal{P}) = O(Q(\mathcal{P})^{3l})$ for any $l$-uniform hypergraph property $\mathcal{P}$ in the adjacency matrix model. In direct contrast, in the adjacency list model for bounded-degree graphs, we exhibit a promise problem that shows an exponential separation between the randomized and quantum query complexities. ",Can graph properties have exponential quantum speedup?
22,1222347158541651968,40108176,Zhangqi Yin,['Our new paper on time crystal:  <LINK>'],https://arxiv.org/abs/2001.10187,"Time crystal is defined as a phase of matter spontaneously exhibiting a periodicity in time. Previous studies focused on discrete quantum time crystals under periodic drive. Here, we propose a time crystal model based on a levitated charged nanoparticle in a static magnetic field without drive. Both the classical time crystal in thermal equilibrium and the quantum time crystal in the ground state can emerge in the spin rotational mode, under the strong magnetic field or the large charge-to-mass ratio limit. Besides, for the first time, the \emph{time polycrystal} is defined and naturally appears in this model. Our model paves a way for realizing time crystals in thermal equilibrium. ","Classical and quantum time crystals in a levitated nanoparticle without
  drive"
23,1222344363528900608,1177063549606203394,Tommi Tenkanen,"['A new paper out! Here I present an introduction to cosmic inflation in the context of ""Palatini gravity"" and review some previous studies on the topic:  <LINK>\n\nThe Palatini theory is an interesting alternative to the usual ""metric"" theory of gravity. 1/', 'In the ""metric"" case only the metric tensor determines the geometry of space-time, whereas in the ""Palatini"" (also known as ""metric-affine"") case both the metric and the space-time connection are, a priori, independent variables. 2/', ""In the context of Einstein's General Relativity these are just two different ways to formulate the same theory but in scenarios where there are field(s) that are coupled non-minimally to gravity or the gravitational sector is otherwise extended, that is not the case. 3/"", 'In that case, they are two intrinsically different models of gravity. Currently we do not know which one is the right one. 4/', 'Because the two models are different, assumptions of the underlying gravitational degrees of freedom (i.e. is just the metric or both the metric and the connection which are important) can have a big impact on the observational consequences of cosmic inflation. 5/', 'And that\'s a great thing, because it means that in the context of a given model (say, ""Higgs inflation""), cosmic inflation gives us a way to probe the gravitational degrees of freedom at very high energies! 6/', 'In the paper I discuss what, in particular, a detection of primordial gravitational waves could tell us about the underlying theory of gravity. Fingers crossed something will be detected! 7/', ""@rittenhousewest Unfortunately not, at least not by default. I'm sure it would provide more motivation for models where DM is more strongly entwined with space-time geometry but at this point I'm not able to say what a detection could imply of properties of such DM in general.""]",https://arxiv.org/abs/2001.10135,"We present an introduction to cosmic inflation in the context of Palatini gravity, which is an interesting alternative to the usual metric theory of gravity. In the latter case only the metric $g_{\mu\nu}$ determines the geometry of space-time, whereas in the former case both the metric and the space-time connection $\Gamma^\lambda_{\mu\nu}$ are a priori independent variables - a choice which can lead to a theory of gravity different from the metric one. In scenarios where the field(s) responsible for cosmic inflation are coupled non-minimally to gravity or the gravitational sector is otherwise extended, assumptions of the underlying gravitational degrees of freedom can have a big impact on the observational consequences of inflation. We demonstrate this explicitly by reviewing several interesting and well-motivated scenarios including Higgs inflation, $R^2$ inflation, and $\xi$-attractor models. We also discuss some prospects for future research and argue why $r=10^{-3}$ is a particularly important goal for future missions that search for signatures of primordial gravitational waves. ","Tracing the high energy theory of gravity: an introduction to Palatini
  inflation"
24,1222268281932828672,989251872107085824,Quoc Le,"['New paper: Towards a Human-like Open-Domain Chatbot. Key takeaways:\n\n1. ""Perplexity is all a chatbot needs"" ;)\n2. We\'re getting closer to a high-quality chatbot that can chat about anything\n\nPaper: <LINK>\nBlog: <LINK> <LINK>', '@xpearhead @lmthang You can find some sample conversations with the bot here:\nhttps://t.co/SP9HO0HpL9']",https://arxiv.org/abs/2001.09977,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated. ",Towards a Human-like Open-Domain Chatbot
25,1222171902078341120,147411178,spencer woody,['I have a new paper out for sensitivity analysis for treatment effect estimation when the proper controls are unknown: <LINK>'],https://arxiv.org/abs/2001.07256,"When constructing a model to estimate the causal effect of a treatment, it is necessary to control for other factors which may have confounding effects. Because the ignorability assumption is not testable, however, it is usually unclear which set of controls is appropriate, and effect estimation is generally sensitive to this choice. A common approach in this case is to fit several models, each with a different set of controls, but it is difficult to reconcile inference under the multiple resulting posterior distributions for the treatment effect. Therefore we propose a two-stage approach to measure the sensitivity of effect estimation with respect to control specification. In the first stage, a model is fit with all available controls using a prior carefully selected to adjust for confounding. In the second stage, posterior distributions are calculated for the treatment effect under nested sets of controls by propagating posterior uncertainty in the original model. We demonstrate how our approach can be used to detect the most significant confounders in a dataset, and apply it in a sensitivity analysis of an observational study measuring the effect of legalized abortion on crime rates. ","Bayesian inference for treatment effects under nested subsets of
  controls"
26,1222144666272845826,4328253554,Nicolas Won,"['Check out our new paper ""Compounding the performance accuracy of assembled techniques in a convolutional neural network"". Our proposed ResNet-50 shows an  improvement in top-1 accuracy from 76.3% to 82.78%.\n<LINK>', 'code: https://t.co/vri7o2v9cR']",https://arxiv.org/abs/2001.06268,"Recent studies in image classification have demonstrated a variety of techniques for improving the performance of Convolutional Neural Networks (CNNs). However, attempts to combine existing techniques to create a practical model are still uncommon. In this study, we carry out extensive experiments to validate that carefully assembling these techniques and applying them to basic CNN models (e.g. ResNet and MobileNet) can improve the accuracy and robustness of the models while minimizing the loss of throughput. Our proposed assembled ResNet-50 shows improvements in top-1 accuracy from 76.3\% to 82.78\%, mCE from 76.0\% to 48.9\% and mFR from 57.7\% to 32.3\% on ILSVRC2012 validation set. With these improvements, inference throughput only decreases from 536 to 312. To verify the performance improvement in transfer learning, fine grained classification and image retrieval tasks were tested on several public datasets and showed that the improvement to backbone network performance boosted transfer learning performance significantly. Our approach achieved 1st place in the iFood Competition Fine-Grained Visual Recognition at CVPR 2019, and the source code and trained models are available at this https URL ","Compounding the Performance Improvements of Assembled Techniques in a
  Convolutional Neural Network"
27,1222108003295539200,520665133,R. T. Pierrehumbert,['New paper from group. Graham Lee and team simulate brown dwarf WDO-137-349B in 2 hour orbit around 16000K white dwarf. <LINK> . Talk about extreme solar systems!'],https://arxiv.org/abs/2001.06558,"Context: White dwarf - Brown dwarf short period binaries (P$_{\rm orb}$ $\lesssim$ 2 hours) are some of the most extreme irradiated atmospheric environments known. These systems offer an opportunity to explore theoretical and modelling efforts of irradiated atmospheres different to typical hot Jupiter systems. Aims: We aim to investigate the three dimensional atmospheric structural and dynamical properties of the Brown dwarf WD0137-349B. Methods: We use the three dimensional GCM model Exo-FMS, with a dual-band grey radiative-transfer scheme to model the atmosphere of WD0137-349B. The results of the GCM model are post-processed using the three dimensional Monte Carlo radiative-transfer model \textsc{cmcrt}. Results: Our results suggest inefficient day-night energy transport and a large day-night temperature contrast for WD0137-349B. Multiple flow patterns are present, shifting energy asymmetrically eastward or westward depending on their zonal direction and latitude. Regions of overturning are produced on the western terminator. We are able to reproduce the start of the system near-IR emission excess at $\gtrsim$ 1.95 $\mu$m as observed by the GNIRS instrument. Our model over predicts the IR phase curve fluxes by factors of $\approx$1-3, but generally fits the shape of the phase curves well. Chemical kinetic modelling using \textsc{vulcan} suggests a highly ionised region at high altitudes can form on the dayside of the Brown dwarf. Conclusions: We present a first attempt at simulating the atmosphere of a short period White dwarf - Brown dwarf binary in a 3D setting. Further studies into the radiative and photochemical heating from the UV irradiation is required to more accurately capture the energy balance inside the Brown dwarf atmosphere. Cloud formation may also play an important role in shaping the emission spectra of the Brown dwarf. ",Simplified 3D GCM modelling of the irradiated brown dwarf WD0137-349B
28,1221986010407395328,478401196,Matthew Talia,['New paper out! Hit me up if you would like to know more\n\n<LINK> <LINK>'],https://arxiv.org/abs/2001.09156,"We use the state-of-the-art high-resolution cosmological simulations by IllustrisTNG to derive the velocity distribution and local density of dark matter in galaxies like our Milky Way and find a substantial spread in both quantities. Next we use our findings to examine the sensitivity to the dark matter velocity profile of underground searches using electron scattering in germanium and silicon targets. We find that sub-GeV dark matter search is strongly affected by these uncertainties, unlike nuclear recoil searches for heavier dark matter, especially in multiple electron-hole modes, for which the sensitivity to the scattering cross-section is also weaker. Therefore, by improving the sensitivity to lower ionization thresholds not only projected sensitivities will be boosted but also the dependence on the astrophysical uncertainties will become significantly reduced. ","Impact of uncertainties in the halo velocity profile on direct detection
  of sub-GeV dark matter"
29,1221717163108634625,735386827578875904,siegfried Vanaverbek,['A new paper from the HOYS project accepted for publication in MNRAS. \nFuture work involves big challenges in data science and AI.\n<LINK>'],https://arxiv.org/abs/2001.05570,"The HOYS citizen science project conducts long term, multifilter, high cadence monitoring of large YSO samples with a wide variety of professional and amateur telescopes. We present the analysis of the light curve of V1490Cyg in the Pelican Nebula. We show that colour terms in the diverse photometric data can be calibrated out to achieve a median photometric accuracy of 0.02mag in broadband filters, allowing detailed investigations into a variety of variability amplitudes over timescales from hours to several years. Using GaiaDR2 we estimate the distance to the Pelican Nebula to be 870$^{+70}_{-55}$pc. V1490Cyg is a quasi-periodic dipper with a period of 31.447$\pm$0.011d. The obscuring dust has homogeneous properties, and grains larger than those typical in the ISM. Larger variability on short timescales is observed in U and R$_c-$H$\alpha$, with U-amplitudes reaching 3mag on timescales of hours, indicating the source is accreting. The H$\alpha$ equivalent width and NIR/MIR colours place V1490Cyg between CTTS/WTTS and transition disk objects. The material responsible for the dipping is located in a warped inner disk, about 0.15AU from the star. This mass reservoir can be filled and emptied on time scales shorter than the period at a rate of up to 10$^{-10}$M$_\odot$/yr, consistent with low levels of accretion in other T Tauri stars. Most likely the warp at this separation from the star is induced by a protoplanet in the inner accretion disk. However, we cannot fully rule out the possibility of an AA Tau-like warp, or occultations by the Hill sphere around a forming planet. ","A survey for variable young stars with small telescopes: II -- Mapping a
  protoplanetary disk with stable structures at 0.15 AU"
30,1221714698812477440,882303076505456642,Timon Emken,"[""Fresh off the press: Our new #paper on next-generation's direct sub-GeV #DarkMatter searches with semiconductors on today's @arxiv.\n\n<LINK>\n\nWe study future experiments' discovery reaches using frequentist and #MonteCarlo methods. <LINK>"", ""@arxiv The paper's emphasis is on the improved statistical treatment, the background modeling, and a study of astrophysical uncertainties, e.g. of the local escape velocity of our galaxy.\n\nNote: The figure does not show projected constraints, but discovery reaches of future experiments. https://t.co/BRCsbnwS3j"", '@arxiv It is the hard work of four Bachelor students here at @ChalmersPhysics that made this work possible.', '@BradleyKavanagh @arxiv Thanks!\n\nYeah, I might as well embrace the weird arXiv numbers sooner than later.']",https://arxiv.org/abs/2001.08910,"We compute the projected sensitivity to dark matter (DM) particles in the sub-GeV mass range of future direct detection experiments using germanium and silicon semiconductor targets. We perform this calculation within the dark photon model for DM-electron interactions using the likelihood ratio as a test statistic, Monte Carlo simulations, and background models that we extract from recent experimental data. We present our results in terms of DM-electron scattering cross section values required to reject the background only hypothesis in favour of the background plus DM signal hypothesis with a statistical significance, $\mathcal{Z}$, corresponding to 3 or 5 standard deviations. We also test the stability of our conclusions under changes in the astrophysical parameters governing the local space and velocity distribution of DM in the Milky Way. In the best-case scenario, when a high-voltage germanium detector with an exposure of $50$ kg-year and a CCD silicon detector with an exposure of $1$ kg-year and a dark current rate of $1\times10^{-7}$ counts/pixel/day have simultaneously reported a DM signal, we find that the smallest cross section value compatible with $\mathcal{Z}=3$ ($\mathcal{Z}=5$) is about $8\times10^{-42}$ cm$^2$ ($1\times10^{-41}$ cm$^2$) for contact interactions, and $4\times10^{-41}$ cm$^2$ ($7\times10^{-41}$ cm$^2$) for long-range interactions. Our sensitivity study extends and refine previous works in terms of background models, statistical methods, and treatment of the underlying astrophysical uncertainties. ","Projected sensitivity to sub-GeV dark matter of next-generation
  semiconductor detectors"
31,1221462779065962498,532752544,Michael Lopez,"['New paper with @lyhuStatree and @BatmanGu looking at Bayesian Additive Regression Trees for causal inference with multiple treatments and binary outcome <LINK> <LINK>', '@lauretig @lyhuStatree @BatmanGu Code from our initial version is here -- going to be updated after (hopeful) publication https://t.co/ipgEuFNTNl']",https://arxiv.org/abs/2001.06483,"There is a dearth of robust methods to estimate the causal effects of multiple treatments when the outcome is binary. This paper uses two unique sets of simulations to propose and evaluate the use of Bayesian Additive Regression Trees (BART) in such settings. First, we compare BART to several approaches that have been proposed for continuous outcomes, including inverse probability of treatment weighting (IPTW), targeted maximum likelihood estimator (TMLE), vector matching and regression adjustment. Results suggest that under conditions of non-linearity and non-additivity of both the treatment assignment and outcome generating mechanisms, BART, TMLE and IPTW using generalized boosted models (GBM) provide better bias reduction and smaller root mean squared error. BART and TMLE provide more consistent 95 per cent CI coverage and better large-sample convergence property. Second, we supply BART with a strategy to identify a common support region for retaining inferential units and for avoiding extrapolating over areas of the covariate space where common support does not exist. BART retains more inferential units than the generalized propensity score based strategy, and shows lower bias, compared to TMLE or GBM, in a variety of scenarios differing by the degree of covariate overlap. A case study examining the effects of three surgical approaches for non-small cell lung cancer demonstrates the methods. ","Estimation of Causal Effects of Multiple Treatments in Observational
  Studies with a Binary Outcome"
32,1221053952357257217,446634627,Roberto Santana,"['In our new paper ""On the human evaluation of audio adversarial examples""  (<LINK>) we  show that distortion metrics proposed for evaluating audio adversarial examples can be misleading. More accurate metrics that reflect human perception are needed.']",https://arxiv.org/abs/2001.08444,"Human-machine interaction is increasingly dependent on speech communication. Machine Learning models are usually applied to interpret human speech commands. However, these models can be fooled by adversarial examples, which are inputs intentionally perturbed to produce a wrong prediction without being noticed. While much research has been focused on developing new techniques to generate adversarial perturbations, less attention has been given to aspects that determine whether and how the perturbations are noticed by humans. This question is relevant since high fooling rates of proposed adversarial perturbation strategies are only valuable if the perturbations are not detectable. In this paper we investigate to which extent the distortion metrics proposed in the literature for audio adversarial examples, and which are commonly applied to evaluate the effectiveness of methods for generating these attacks, are a reliable measure of the human perception of the perturbations. Using an analytical framework, and an experiment in which 18 subjects evaluate audio adversarial examples, we demonstrate that the metrics employed by convention are not a reliable measure of the perceptual similarity of adversarial examples in the audio domain. ",On the human evaluation of audio adversarial examples
33,1220650806657142784,1090904571797491712,Timo Kepp,"['Check out our new work on deep learning-based analysis of retinal OCT images from a novel low-cost OCT device for home monitoring.\n\nIt will be presented at SPIE Medical Imaging, Houston, Tuesday, Paper 11314-56, 4:30 PM\n\n<LINK>', 'This was a joint work with the Medical Laser Center Lübeck and the University Eye Hospital Kiel.']",https://arxiv.org/abs/2001.08480,"The treatment of age-related macular degeneration (AMD) requires continuous eye exams using optical coherence tomography (OCT). The need for treatment is determined by the presence or change of disease-specific OCT-based biomarkers. Therefore, the monitoring frequency has a significant influence on the success of AMD therapy. However, the monitoring frequency of current treatment schemes is not individually adapted to the patient and therefore often insufficient. While a higher monitoring frequency would have a positive effect on the success of treatment, in practice it can only be achieved with a home monitoring solution. One of the key requirements of a home monitoring OCT system is a computer-aided diagnosis to automatically detect and quantify pathological changes using specific OCT-based biomarkers. In this paper, for the first time, retinal scans of a novel self-examination low-cost full-field OCT (SELF-OCT) are segmented using a deep learning-based approach. A convolutional neural network (CNN) is utilized to segment the total retina as well as pigment epithelial detachments (PED). It is shown that the CNN-based approach can segment the retina with high accuracy, whereas the segmentation of the PED proves to be challenging. In addition, a convolutional denoising autoencoder (CDAE) refines the CNN prediction, which has previously learned retinal shape information. It is shown that the CDAE refinement can correct segmentation errors caused by artifacts in the OCT image. ","Segmentation of Retinal Low-Cost Optical Coherence Tomography Images
  using Deep Learning"
34,1220504185524764672,405482317,Prof. Lisa Harvey-Smith,"[""A new paper led by Chika Ogbodo was released today (I am a co-author) showing the strength and orientation of magnetic fields in the Milky Way. The magnetic fields in space are measured by looking at 'masers' - microwave lasers in space that occur in gas.\n<LINK> <LINK>"", 'The light from these space lasers is split into two colours by the magnetic field in the cloud of gas where it originates. This allows us to measure its strength and direction.', 'Another way to measure the magnetic fields in space is to observe the light from pulsars (weird stars that appear to pulse, because they are spinning). The way that light is spread out by magnetism in our Galaxy can also be studied in this way.', 'Congrats to Chika, to his supervisors @JimiGreen, Jo Dawson and the rest of the team.', '@JimiGreen @Macquarie_Uni @CSIRO_ATNF', ""@ScotIncGrowth The universe may be infinite but I will never make sense of Brexit I'm afraid.""]",http://arxiv.org/abs/2001.06180,"From targeted observations of ground-state OH masers towards 702 Multibeam (MMB) survey 6.7-GHz methanol masers, between Galactic longitudes 186$^{\circ}$ through the Galactic centre to 20$^{\circ}$, made as part of the `MAGMO' project, we present the physical and polarisation properties of the 1720-MHz OH maser transition, including the identification of Zeeman pairs. We present 10 new and 23 previously catalogued 1720-MHz OH maser sources detected towards star formation regions. In addition, we also detected 16 1720-MHz OH masers associated with supernova remnants and two sites of diffuse OH emission. Towards the 33 star formation masers, we identify 44 Zeeman pairs, implying magnetic field strengths ranging from $-$11.4 to $+$13.2 mG, and a median magnetic field strength of $|B_{LOS}|$ $\sim$ 6 mG. With limited statistics, we present the in-situ magnetic field orientation of the masers and the Galactic magnetic field distribution revealed by the 1720-MHz transition. We also examine the association statistics of 1720-MHz OH SFR masers with other ground-state OH masers, excited-state OH masers, class I and class II methanol masers and water masers, and compare maser positions with mid-infrared images of the parent star forming regions. Of the 33 1720-MHz star formation masers, ten are offset from their central exciting sources, and appear to be associated with outflow activity. ","MAGMO: Polarimetry of 1720-MHz OH Masers towards Southern Star Forming
  Regions"
35,1220416549917511689,7984662,Clayton Shonkwiler,"['New paper: “Expected distances on manifolds of partially oriented flags”, with Chris Peterson and my student Brenden Balch <LINK> <LINK>']",https://arxiv.org/abs/2001.07854,"Flag manifolds are generalizations of projective spaces and other Grassmannians: they parametrize flags, which are nested sequences of subspaces in a given vector space. These are important objects in algebraic and differential geometry, but are also increasingly being used in data science, where many types of data are properly understood as subspaces rather than vectors. In this paper we discuss partially oriented flag manifolds, which parametrize flags in which some of the subspaces may be endowed with an orientation. We compute the expected distance between random points on some low-dimensional examples, which we view as a statistical baseline against which to compare the distances between particular partially oriented flags coming from geometry or data. ",Expected Distances on Manifolds of Partially Oriented Flags
36,1220350729413627910,899746531904901121,Tom Everitt,"['Thanks to structural causal models, we now a more precise understanding of incentives in causal influence diagrams\n\nblog post: <LINK>\narXiv:\n<LINK>']",https://arxiv.org/abs/2001.07118,"Which variables does an agent have an incentive to control with its decision, and which variables does it have an incentive to respond to? We formalise these incentives, and demonstrate unique graphical criteria for detecting them in any single decision causal influence diagram. To this end, we introduce structural causal influence models, a hybrid of the influence diagram and structural causal model frameworks. Finally, we illustrate how these incentives predict agent incentives in both fairness and AI safety applications. ",The Incentives that Shape Behaviour
37,1220269837911560192,838517941259771904,Christian Hirsch,"['How to make sense of densest particle-packing, optimal cache placement or minimal matching if the volume is infinite and the input random? Find out more about this and many more optimization problems in our new paper <LINK> w/t Bartek Błaszczyszyn <LINK>']",https://arxiv.org/abs/2001.08074,"Many specific problems ranging from theoretical probability to applications in statistical physics, combinatorial optimization and communications can be formulated as an optimal tuning of local parameters in large systems of interacting particles. Using the framework of stationary point processes in the Euclidean space, we pose it as a problem of an optimal stationary marking of a given stationary point process. The quality of a given marking is evaluated in terms of scores calculated in a covariant manner for all points in function of the proposed marked configuration. In the absence of total order of the configurations of scores, we identify intensity-optimality and local optimality as two natural ways for defining optimal stationary marking. We derive tightness and integrability conditions under which intensity-optimal markings exist and further stabilization conditions making them equivalent to locally optimal ones. We present examples motivating the proposed, general framework. Finally, we discuss various possible approaches leading to uniqueness results. ",Optimal stationary markings
38,1220248971618156544,583141846,Nicola Lo Gullo,['New paper out! Great collaboration with  the CNRS in Grenoble to make and study a heat valve at nanoscale and in the mK regime. @turkuquantum @PhysUtu @QTFinland @CNR \n\n<LINK>'],https://arxiv.org/abs/2001.08183,"We demonstrate gate control of electronic heat flow in a thermally-biased single-quantum-dot junction. Electron temperature maps taken in the immediate vicinity of the junction, as a function of the gate and bias voltages applied to the device, reveal clearly defined Coulomb diamond patterns revealing a maximum heat transfer right at the charge degeneracy point. The non-trivial bias and gate dependence of this heat valve results from both the quantum nature of the dot at the heart of device and its strong coupling to leads. ",A Single-Quantum-Dot Heat Valve
39,1220234517572534277,40108176,Zhangqi Yin,['Our new paper on quantum error correction cods: <LINK>'],https://arxiv.org/abs/2001.07998,"Quantum error correction plays an important role in fault-tolerant quantum information processing. It is usually difficult to experimentally realize quantum error correction, as it requires multiple qubits and quantum gates with high fidelity. Here we propose a simple quantum error-correcting code for the detected amplitude damping channel. The code requires only two qubits. We implement the encoding, the channel, and the recovery on an optical platform, the IBM Q System, and a nuclear magnetic resonance system. For all of these systems, the error correction advantage appears when the damping rate exceeds some threshold. We compare the features of these quantum information processing systems used and demonstrate the advantage of quantum error correction on current quantum computing platforms. ",Testing a Quantum Error-Correcting Code on Various Platforms
40,1219882582562811905,1107290362228498434,Simon Vandenhende,['A new paper on multi-task learning :) \n<LINK>'],https://arxiv.org/abs/2001.06902,"In this paper, we argue about the importance of considering task interactions at multiple scales when distilling task information in a multi-task learning setup. In contrast to common belief, we show that tasks with high affinity at a certain scale are not guaranteed to retain this behaviour at other scales, and vice versa. We propose a novel architecture, namely MTI-Net, that builds upon this finding in three ways. First, it explicitly models task interactions at every scale via a multi-scale multi-modal distillation unit. Second, it propagates distilled task information from lower to higher scales via a feature propagation module. Third, it aggregates the refined task features from all scales via a feature aggregation unit to produce the final per-task predictions. Extensive experiments on two multi-task dense labeling datasets show that, unlike prior work, our multi-task model delivers on the full potential of multi-task learning, that is, smaller memory footprint, reduced number of calculations, and better performance w.r.t. single-task learning. The code is made publicly available: this https URL ",MTI-Net: Multi-Scale Task Interaction Networks for Multi-Task Learning
41,1219805362133569536,36438289,Dr. Patrícia,['My new paper is now on Arxiv: it is about NGC 613! One of the richest galactic nuclei! \n\nMeu paper está no Arxiv: é sobre NGC 613! Um dos núcleos galácticos mais ricos já estudado!\n\n<LINK>\n\n#astronomy #MNRAS #astroph #arxiv #agn #astronomia  #astrophysics'],https://arxiv.org/abs/2001.07328,"In this paper, we report a detailed study with a variety of data from optical, near-infrared, X-ray, and radio telescopes of the nuclear region of the galaxy NGC 613 with the aim of understanding its complexity. We detected an extended stellar emission in the nucleus that, at first, appears to be, in the optical band, two stellar nuclei separated by a stream of dust. The active galactic nucleus (AGN) is identified as a variable point-like source between these two stellar components. There is a central hard X-ray emission and an extended soft X-ray emission that closely coincides with the ionization cone, as seen in the [O III]$\lambda$5007 emission. The centroid of the [O I]$\lambda$6300 emission does not coincide with the AGN, being shifted by 0.24 arcsec towards the ionization cone; this shift is probably caused by a combination of differential dust extinction together with emission and reflection in the ionization cone. The optical spectra extracted from the central region are typical of low-ionization nuclear emission-line regions. We also identify 10 H II regions, eight of them in a star forming ring that is visible in Br$\gamma$, [Fe II]$\lambda$16436 and molecular CO(3-2) images observed in previous studies. Such a ring also presents weak hard X-ray emission, probably associated with supernova remnants, not detected in other studies. The position of the AGN coincides with the centre of a nuclear spiral (detected in previous works) that brings gas and dust from the bar to the nucleus, causing the high extinction in this area. ",The nuclear region of NGC 613. I -- Multiwavelength analysis
42,1219404910862704640,939589825602228224,Leah Jenks,['New paper on the ArXiv today! Q: What happens to gravitational waves if the cosmological constant is no longer a constant? A: Some weird sh!t \n\n<LINK>'],https://arxiv.org/abs/2001.06373,"In recent work minimal theories allowing the variation of the cosmological constant by means of a balancing torsion, have been proposed. It was found that such theories contain parity violating homogeneous and isotropic solutions, due to a torsion structure called the Cartan spiral staircase. Their dynamics are controlled by Euler and Pontryagin quasi-topological terms in the action. Here we show that such theories predict a dramatically different picture for gravitational wave fluctuations in the parity violating branch. If the dynamics are ruled solely by the Euler-type term, then linear tensor mode perturbations are entirely undetermined, hinting at a new type of gauge invariance. The Pontryagin term not only permits for phenomenologically sounder background solutions (as found in previous literature), but for realistic propagation of gravitational wave modes. We discuss the observational constraints and predictions of these theories. ",Gravity waves in parity-violating Copernican Universes
43,1219203535373393920,637447809,martin biehl,"['Our new paper is on arxiv: <LINK> \nThis is work with Felix A Pollock and @kanair.\nIt is a criticism of the ""Life as we know it"" (LAWKI) paper by Friston, 2013 (<LINK>). \n\nThread. \n\n1/10', 'LAWKI argues that the existence of a ""Markov blanket"" in an ergodic random dynamical system is sufficient for the internal coordinates \\lambda of the Markov blanket to perform (or look like they perform) Bayesian inference on the external coordinates \\psi.    \n2/n', ""We identify technical problems that undermine LAWKI's justification for an interpretation as Bayesian inference. \n\nThese problems only concern the relation between Markov blankets and Bayesian inference. They do not affect Friston's approach to constructing agents for a \n3/n"", 'given environment/POMDP which is also called free energy principle or active inference. \n\nInstead of trying to construct an agent for a given environment LAWKI tries to give conditions for the existence of a Bayesian agent (and its environment) *inside* a given system.\n4/n', 'LAWKI:\nExistence of a Bayesian agent \n&lt;=&gt; internal coordinates \\lamba of Markov blanket perform Bayesian inference on external coordinates \\psi\n&lt;=&gt; \\lambda parameterize pdist q(\\psi|\\lambda) that becomes equal to ergodic cond pdist p*(\\psi|s,a,\\lambda) due to system dynamics.\n5/n', 'We do not discuss these equivalences. We look at the arguments used in LAWKI to derive the last statement. These arguments contain technical errors. This means that LAWKI does not show that the Markov blanket is sufficient for a Bayesian agent. But maybe it is anyway...\n6/n', 'By now https://t.co/xBIto1nURq (FEPP) and https://t.co/AkPKrTt9H3 appeared which argue the same for a different Bayesian agent notion.\n\nHowever, one error we identify in LAWKI is not entirely fixed in FEPP (see below). We do not know whether this is  of consequence.\n \n7/n', 'What we show (fig):\n\n- first error: dropping relevant components of the gradient of the log of the ergodic pdist p*\n\n- second error: vanishing gradient of KL div does not imply equality of q and p*\n\n- alternative to first error: makes the Free Energy Lemma (FEL) impossible\n\n8/n https://t.co/lJEdEv5vDI', 'Also shown in the paper but not in the figure above: \n\n- FEPP alternative to first error also wrong\n\n- proof of FEL flawed\n\n- generalized coordinate version in https://t.co/AUNDOK0naD also contains first error and a different but also flawed FEL proof\n\n9/n', 'We hope our will eventually lead to a clarification of these issues. Or at least to a revelation of the reasons for the newer formulations in FEPP and https://t.co/AkPKrTt9H3 \nNo such reasons are given in these newer papers.\n\n10/10\n\n@FarlKriston']",https://arxiv.org/abs/2001.06408,"We summarize the original formulation of the free energy principle, and highlight some technical issues. We discuss how these issues affect related results involving generalised coordinates and, where appropriate, mention consequences for and reveal, up to now unacknowledged, differences to newer formulations of the free energy principle. In particular, we reveal that various definitions of the ""Markov blanket"" proposed in different works are not equivalent. We show that crucial steps in the free energy argument which involve rewriting the equations of motion of systems with Markov blankets, are not generally correct without additional (previously unstated) assumptions. We prove by counterexample that the original free energy lemma, when taken at face value, is wrong. We show further that this free energy lemma, when it does hold, implies equality of variational density and ergodic conditional density. The interpretation in terms of Bayesian inference hinges on this point, and we hence conclude that it is not sufficiently justified. Additionally, we highlight that the variational densities presented in newer formulations of the free energy principle and lemma are parameterised by different variables than in older works, leading to a substantially different interpretation of the theory. Note that we only highlight some specific problems in the discussed publications. These problems do not rule out conclusively that the general ideas behind the free energy principle are worth pursuing. ",A Technical Critique of Some Parts of the Free Energy Principle
44,1219068633596469248,348159742,Blair Bilodeau,"['New paper (<LINK>) with Dave Stanford. For the first time, we provide a characterization of the effect on high priority customers of introducing a delay to an Accumulating Priority Queue.', 'We apply this to a common key performance measure used for Ontario emergency department wait times. This provides a framework for ED admin to optimally select the next patient to be seen.']",https://arxiv.org/abs/2001.06054,"We provide the first analytical expressions for the expected waiting time of high-priority customers in the delayed APQ by exploiting a classical conservation law for work-conserving queues. Additionally, we describe an algorithm to compute the expected waiting times of both low-priority and high-priority customers, which requires only the truncation of sums that converge quickly in our experiments. These insights are used to demonstrate how the accumulation rate and delay level should be chosen by health care practitioners to optimize common key performance indicators (KPIs). In particular, we demonstrate that for certain nontrivial KPIs, an accumulating priority queue with a delay of zero is always preferable. Finally, we present a detailed investigation of the quality of an exponential approximation to the high-priority waiting time distribution, which we use to optimize the choice of queueing parameters with respect to both classes' waiting time distributions. ","High-Priority Expected Waiting Times in the Delayed Accumulating
  Priority Queue with Applications to Health Care KPIs"
45,1219043738137309186,1085377606327943169,Jonathan B. Curtis,"['New paper alert! <LINK> \n\nThis one is about how to realize analogues of exotic “Newton-Cartan” spacetimes (not found in nature) in ultra cold quantum gases!', '@jbcurtis76 It’s element-ary']",https://arxiv.org/abs/2001.05496,"It is well established that linear dispersive modes in a flowing quantum fluid behave as though they are coupled to an Einstein-Hilbert metric and exhibit a host of phenomena coming from quantum field theory in curved space, including Hawking radiation. We extend this analogy to any nonrelativistic Goldstone mode in a flowing spinor Bose-Einstein condensate. In addition to showing the linear dispersive result for all such modes, we show that the quadratically dispersive modes couple to a special nonrelativistic spacetime called a Newton-Cartan geometry. The kind of spacetime (Einstein-Hilbert or Newton-Cartan) is intimately linked to the mean-field phase of the condensate. To illustrate the general result, we further provide the specific theory in the context of a pseudo-spin-1/2 condensate where we can tune between relativistic and nonrelativistic geometries. We uncover the fate of Hawking radiation upon such a transition: it vanishes and remains absent in the Newton-Cartan geometry despite the fact that any fluid flow creates a horizon for certain wave numbers. Finally, we use the coupling to different spacetimes to compute and relate various energy and momentum currents in these analog systems. While this result is general, present day experiments can realize these different spacetimes including the magnon modes for spin-1 condensates such as $^{87}$Rb, $^{7}$Li, $^{41}$K (Newton-Cartan), and $^{23}$Na (Einstein-Hilbert). ","Analog spacetimes from nonrelativistic Goldstone modes in spinor
  condensates"
46,1218152708798144512,1140222123006472194,Kasper Elm Heintz,"['New paper is online @arxiv_org today!!  (see here: <LINK>) \nThis work was done in collaboration with @darach, where we, for the first time, measure directly the molecular gas mass conversion factor in high-redshift galaxies (thread below). \n#AstroTwitter', 'But first, a huge shout-out to @Sydonahi, for producing the beautiful Figure 1 of our paper — for more of her fantastic work, see here: https://t.co/tlzqMt0WJI', 'Paper background: In molecular clouds, the *by far* most abundant molecule is H_2. However, since it has no permanent dipole moment, and can only be rotationally excited at much higher temperatures than that of the cold molecular gas, it will only emit weakly. 1/n', 'Therefore, other molecular gas tracers such as CO and neutral atomic carbon (CI) have been used instead, to estimate the *total* molecular gas mass using some conversion factor. However, these have only been directly constrained in the Milky Way and for some nearby galaxies. 2/n', 'These estimates are typically based on virial mass estimates of the molecular clouds compared to their CO or CI emission. This is, however, not possible to do outside the local Universe. Instead, what we did in this paper is to use cosmic lighthouses such as GRBs and quasars 3/n', 'shining through molecular clouds in high-redshift galaxies, to measure the relative abundance of CI to H2. \nIt turns out, not only can we reproduce the average CI-to-H2 conversion factor observed in the Milky Way at solar metallicities, but we also observe 4/', 'A clear metallicity evolution, exactly as predicted by hydrodynamical simulations. All these results point to the fact that the properties of molecular clouds follow universal scaling relations with metallicity, both in the local and high-redshift Universe. 5/n', 'Finally, this technique provides one more way to map the observed properties between the absorption- and emission-selected galaxy populations. \n6/n, where n=6.', '@Sydonahi @arxiv_org @darach Thanks! And thanks again for the fantastic figure 🤗 I really believe these kinds of infographics fits nicely into a paper', '@Sydonahi @arxiv_org @darach That is, more people should start doing it! 🤗']",https://arxiv.org/abs/2001.05770v1,"The amount of cold, molecular gas in high-redshift galaxies is typically inferred from proxies of molecular hydrogen (H$_2$), such as carbon monoxide (CO) or neutral atomic carbon ([CI]) and molecular gas mass conversion factors. The use of these proxies, however, relies on modeling and observations that have not been directly measured outside the local universe. Here, we use recent samples of high-redshift gamma-ray burst (GRB) and quasar molecular gas absorbers to determine this conversion factor $\alpha_{\rm [CI]}$ from the column density of H$_2$, which gives us the mass per unit column, and the [CI]($J=1$) column density, which provides the luminosity per unit column. This technique allows us to make direct measurements of the relative abundances in high-redshift absorption-selected galaxies. Our sample spans redshifts of z=1.9-3.4 and covers two orders of magnitude in gas-phase metallicity. We find that $\alpha_{\rm [CI]}$ scales linearly with the metallicity: $\log \alpha_{\rm [CI]} = -1.13\times \log(Z/Z_{\odot}) + 1.33$, with a scatter of $\sigma_{\alpha_{\rm [CI]}} = 0.2$ dex. Using a sample of emission-selected galaxies at z~0-5, with both [CI] and CO line detections, we apply the $\alpha_{\rm [CI]}$ conversion to derive independent estimates of the molecular gas mass and $\alpha_{\rm CO}$. We find a remarkable agreement between the molecular gas masses inferred from the absorption-derived $\alpha_{\rm [CI]}$ compared to typical $\alpha_{\rm CO}$-based estimates, and an inferred metallicity evolution of $\alpha_{\rm CO}$ that is consistent with $\alpha_{\rm [CI]}$ and previous estimates from the literature. These results thus support the use of the absorption-derived $\alpha_{\rm [CI]}$ conversion factor for emission-selected star-forming galaxies and demonstrate that both methods probe the same universal properties of molecular gas in the local and high-redshift universe. ","] Direct measurement of the [CI] luminosity to molecular gas mass
  conversion factor in high-redshift star-forming galaxies"
47,1218101064613974017,1013746596222308352,Trifon Trifonov,['[2001.05942] A public HARPS radial velocity database corrected for systematic errors. — Our new paper announcing a new HARPS public database of 3000 stars with a total of 212 000 precise RV measurements (and more) is on the AstroPH. Please spread around!!! <LINK>'],https://arxiv.org/abs/2001.05942,"Context. The HARPS spectrograph provides state-of-the-art stellar radial velocity (RV) measurements with a precision down to 1 m/s. The spectra are extracted with a dedicated data-reduction software (DRS) and the RVs are computed by CCF with a numerical mask. Aims. The aim of this study is three-fold: (i) Create easy access to the public HARPS RV data set. (ii) Apply the new public SERVAL pipeline to the spectra, and produce a more precise RV data set. (iii) Check whether the precision of the RVs can be further improved by correcting for small nightly systematic effects. Methods. For each star observed with HARPS, we downloaded the publicly available spectra from the ESO archive and recomputed the RVs with SERVAL. We then computed nightly zero points (NZPs) by averaging the RVs of quiet stars. Results. Analysing the RVs of the most RV-quiet stars, whose RV scatter is < 5 m/s, we find that SERVAL RVs are on average more precise than DRS RVs by a few percent. We find three significant systematic effects, whose magnitude is independent of the software used for the RV derivation: (i) stochastic variations with a magnitude of 1 m/s; (ii) long-term variations, with a magnitude of 1 m/s and a typical timescale of a few weeks; and (iii) 20-30 NZPs significantly deviating by a few m/s. In addition, we find small (< 1 m/s) but significant intra-night drifts in DRS RVs before the 2015 intervention, and in SERVAL RVs after it. We confirm that the fibre exchange in 2015 caused a discontinuous RV jump, which strongly depends on the spectral type of the observed star: from 14 m/s for late F-type stars, to -3 m/s for M dwarfs. Conclusions. Our NZP-corrected SERVAL RVs can be retrieved from a user-friendly, public database. It provides more than 212 000 RVs for about 3000 stars along with many auxiliary information, NZP corrections, various activity indices, and DRS-CCF products. ",A public HARPS radial velocity database corrected for systematic errors
48,1218072112142987264,953655661430222848,Moritz Schauer,"['New preprint: Joris Bierkens, Sebastiano Grazzi (first paper!), Frank van der Meulen, M.S.: A piecewise deterministic Monte Carlo method for diffusion bridges <LINK> <LINK>', 'Julia implementation at https://t.co/sIPYpVrtK7 #julialang https://t.co/XWPNxaTV80']",https://arxiv.org/abs/2001.05889,"We introduce the use of the Zig-Zag sampler to the problem of sampling conditional diffusion processes (diffusion bridges). The Zig-Zag sampler is a rejection-free sampling scheme based on a non-reversible continuous piecewise deterministic Markov process. Similar to the L\'evy-Ciesielski construction of a Brownian motion, we expand the diffusion path in a truncated Faber-Schauder basis. The coefficients within the basis are sampled using a Zig-Zag sampler. A key innovation is the use of the fully local Algorithm for the Zig-Zag sampler that allows to exploit the sparsity structure implied by the dependency graph of the coefficients and by the subsampling technique to reduce the complexity of the algorithm. We illustrate the performance of the proposed methods in a number of examples. ",A piecewise deterministic Monte Carlo method for diffusion bridges
49,1218003572325068800,17373048,Rodrigo Nemmen,"['Exciting new paper: this is the first study of the low-luminosity AGN population in gamma-rays with @NASAFermi. Led by PhD student Raniere Menezes. MNRAS, in press <LINK>', 'A thread will follow soon summarizing the results']",https://arxiv.org/abs/2001.03184,"The majority of the activity around nearby (z ~ 0) supermassive black holes is found in low-luminosity active galactic nuclei (LLAGN), the most of them being classified as low ionization nuclear emission regions. Although these sources are well studied from radio up to X-rays, they are poorly understood in gamma-rays. In this work we take advantage of the all sky-surveying capabilities of the Large Area Telescope on board Fermi Gamma ray Space Telescope to study the whole Palomar sample of LLAGN in gamma-rays. Precisely, the four radio-brightest LLAGN in the sample are identified as significant gamma-ray emitters, all of which are recognized as powerful Fanaroff-Riley I galaxies. These results suggest that the presence of powerful radio jets is of substantial importance for observing a significant gamma-ray counterpart even if these jets are misaligned with respect to the line of sight. We also find that most of the X-ray-brightest LLAGN do not have a significant gamma-ray and strong radio emission, suggesting that the X-rays come mainly from the accretion flow in these cases. A detailed analysis of the spectral energy distributions (SEDs) of NGC 315 and NGC 4261, both detected in gamma-rays, is provided where we make a detailed comparison between the predicted hadronic gamma-ray emission from a radiatively inefficient accretion flow (RIAF) and the gamma-ray emission from a leptonic jet-dominated synchrotron self-Compton (SSC) model. Both SEDs are better described by the SSC model while the RIAF fails to explain the gamma-ray observations. ",Gamma-ray observations of low-luminosity active galactic nuclei
50,1217990000777867264,3018751880,Prof. Katelin Schutz,"['New paper out tonight! <LINK> The short summary: there is too much dark matter substructure in halos (inferred gravitationally) for it to be very fuzzy, more granularity is needed', ""This granularity can only be accommodated if the de Broglie wavelength of dark matter is sufficiently short, otherwise the dark matter gets smeared out and the observations can't be explained. This puts a lower bound on the mass of dark matter particles, 2.1 x 10^-21 eV"", ""Other bounds of similar strength exist, but it's always great to have independent corroboration with different observations, analyses, systematics, and assumptions. Maybe you could poke holes in one limit, but it's hard to poke holes in several at the same time"", ""This mass limit is high enough that it precludes some of the funkier phenomena that could have happened if dark matter were allowed to be lighter. Sadly, Nature doesn't care whether dark matter behaves in crazy cool ways, Nature just does its thing and doesn't care what we think"", ""... that's science! Another day, another theory of dark matter to test! https://t.co/SzKfUeW0Im"", '@acollierastro Thank you! Tbh, pressing submit was one of the top 5 scariest things I’ve ever done 😂']",https://arxiv.org/abs/2001.05503,"Warm dark matter has recently become increasingly constrained by observational inferences about the low-mass end of the subhalo mass function, which would be suppressed by dark matter free streaming in the early Universe. In this work, we point out that a constraint can be placed on ultralight bosonic dark matter (often referred to as ""fuzzy dark matter"") based on similar considerations. Recent limits on warm dark matter from strong gravitational lensing of quasars and from fluctuations in stellar streams separately translate to a lower limit of $\sim 2.1 \times 10^{-21}$ eV on the mass of an ultralight boson comprising all dark matter. These limits are complementary to constraints on ultralight dark matter from the Lyman-$\alpha$ forest and are subject to a completely different set of assumptions and systematic uncertainties. Taken together, these probes strongly suggest that dark matter with a mass $\sim 10^{-22}$ eV is not a viable way to reconcile differences between cold dark matter simulations and observations of structure on small scales. ",The Subhalo Mass Function and Ultralight Bosonic Dark Matter
51,1217983086136303616,104529881,Diogo Souto,"['Our new paper is now available on the arXiv!\n""Stellar Characterization of M-dwarfs from the \n@APOGEEsurvey\n  Survey: A Calibrator Sample for the M-dwarf Metallicities""\n<LINK> 👇', '@APOGEEsurvey We studied 21 M-dwarfs where 11 are in binaries systems with a warmer companion and the other 10 had interferometric measurements of the stellar radius from the literature.', '@APOGEEsurvey We determine atmospheric parameters and metallicity for all stars based only on the spectra. The metallicity results compare pretty well with the literature for the warmer stars.', '@APOGEEsurvey The results presented here will be used to future calibrations of the APOGEE/ASPCAP pipeline, if we need so. \nCheck it out! =D']",https://arxiv.org/abs/2001.05597,"We present spectroscopic determinations of the effective temperatures, surface gravities and metallicities for 21 M-dwarfs observed at high-resolution (R $\sim$ 22,500) in the \textit{H}-band as part of the SDSS-IV APOGEE survey. The atmospheric parameters and metallicities are derived from spectral syntheses with 1-D LTE plane parallel MARCS models and the APOGEE atomic/molecular line list, together with up-to-date H$_{2}$O and FeH molecular line lists. Our sample range in $T_{\rm eff}$ from $\sim$ 3200 to 3800K, where eleven stars are in binary systems with a warmer (FGK) primary, while the other 10 M-dwarfs have interferometric radii in the literature. We define an $M_{K_{S}}$--Radius calibration based on our M-dwarf radii derived from the detailed analysis of APOGEE spectra and Gaia DR2 distances, as well as a mass-radius relation using the spectroscopically-derived surface gravities. A comparison of the derived radii with interferometric values from the literature finds that the spectroscopic radii are slightly offset towards smaller values, with $\Delta$ = -0.01 $\pm$ 0.02 $R{\star}$/$R_{\odot}$. In addition, the derived M-dwarf masses based upon the radii and surface gravities tend to be slightly smaller (by $\sim$5-10\%) than masses derived for M-dwarf members of eclipsing binary systems for a given stellar radius. The metallicities derived for the 11 M-dwarfs in binary systems, compared to metallicities obtained for their hotter FGK main-sequence primary stars from the literature, shows excellent agreement, with a mean difference of [Fe/H](M-dwarf - FGK primary) = +0.04 $\pm$ 0.18 dex, confirming the APOGEE metallicity scale derived here for M-dwarfs. ","Stellar Characterization of M-dwarfs from the APOGEE Survey: A
  Calibrator Sample for the M-dwarf Metallicities"
52,1217793783003320323,1003652696723873792,Max Gaspari,"['New paper published in Nature Astronomy! \nWe review the major open questions tied to feedback and feeding of supermassive black holes, and advocate for a multiscale, multiwavelength, and interdisciplinary community.\n#BlackHoleWeather #BlackHoles #astronomy\n<LINK> <LINK>']",https://arxiv.org/abs/2001.04985,"Supermassive black hole (SMBH) feeding and feedback processes are often considered as disjoint and studied independently at different scales, both in observations and simulations. We encourage to adopt and unify three physically-motivated scales for feeding and feedback (micro - meso - macro ~ mpc - kpc - Mpc), linking them in a tight multiphase self-regulated loop. We pinpoint the key open questions related to this global SMBH unification problem, while advocating for the extension of novel mechanisms best observed in massive halos (such as chaotic cold accretion) down to low-mass systems. To solve such challenges, we provide a set of recommendations that promote a multiscale, multiwavelength, and interdisciplinary community. ","Linking Macro, Meso, and Micro Scales in Multiphase AGN Feeding and
  Feedback"
53,1217742677560741888,813753716,Louise Welsh,['New paper out today using ESPRESSO to study the carbon isotope ratio in near-pristine gas 💫. Unexpected result... could the most metal-poor DLAs be affected by reionisation quenching?  <LINK> <LINK>'],https://arxiv.org/abs/2001.04983v1,"Using science verification observations obtained with ESPRESSO at the Very Large Telescope (VLT) in 4UT mode, we report the first bound on the carbon isotope ratio 12C/13C of a quiescent, near-pristine damped Ly-alpha (DLA) system at z=2.34. We recover a limit log10(12C/13C) > +0.37 (2 sigma). We use the abundance pattern of this DLA, combined with a stochastic chemical enrichment model, to infer the properties of the enriching stars, finding the total gas mass of this system to be log10(M_gas/M_sun)=6.3+1.4-0.9 and the total stellar mass to be log10(M_*/M_sun)=4.8+/-1.3. The current observations disfavour enrichment by metal-poor Asymptotic Giant Branch (AGB) stars with masses <2.4 Msun, limiting the epoch at which this DLA formed most of its enriching stars. Our modelling suggests that this DLA formed very few stars until >1 Gyr after the cosmic reionization of hydrogen and, despite its very low metallicity (~1/1000 of solar), this DLA appears to have formed most of its stars in the past few hundred Myr. Combining the inferred star formation history with evidence that some of the most metal-poor DLAs display an elevated [C/O] ratio at redshift z<3, we suggest that very metal-poor DLAs may have been affected by reionization quenching. Finally, given the simplicity and quiescence of the absorption features associated with the DLA studied here, we use these ESPRESSO data to place a bound on the possible variability of the fine-structure constant, Delta alpha/alpha=(-1.2 +/- 1.1)x10^-5. ",] A bound on the 12C/13C ratio in near-pristine gas with ESPRESSO
54,1217575974897946625,18773226,Roei Schuster,"['In a new paper, we literally ""make war mean peace"".\xa0Our attack on AI perturbs corpora to change word “meanings” that get encoded in vector embeddings like #W2V.\n\nWith @TalSchuster , Yoav Meri, Vitaly Shmatikov. To appear at IEEE S&amp;P\n@IEEESSP #SP20 #NLProc\n<LINK> <LINK>']",https://arxiv.org/abs/2001.04935,"Word embeddings, i.e., low-dimensional vector representations such as GloVe and SGNS, encode word ""meaning"" in the sense that distances between words' vectors correspond to their semantic proximity. This enables transfer learning of semantics for a variety of natural language processing tasks. Word embeddings are typically trained on large public corpora such as Wikipedia or Twitter. We demonstrate that an attacker who can modify the corpus on which the embedding is trained can control the ""meaning"" of new and existing words by changing their locations in the embedding space. We develop an explicit expression over corpus features that serves as a proxy for distance between words and establish a causative relationship between its values and embedding distances. We then show how to use this relationship for two adversarial objectives: (1) make a word a top-ranked neighbor of another word, and (2) move a word from one semantic cluster to another. An attack on the embedding can affect diverse downstream tasks, demonstrating for the first time the power of data poisoning in transfer learning scenarios. We use this attack to manipulate query expansion in information retrieval systems such as resume search, make certain names more or less visible to named entity recognition models, and cause new words to be translated to a particular target word regardless of the language. Finally, we show how the attacker can generate linguistically likely corpus modifications, thus fooling defenses that attempt to filter implausible sentences from the corpus using a language model. ",Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning
55,1217459930954981376,53464710,Eric Wong,"['1/ New paper on an old topic: turns out, FGSM works as well as PGD for adversarial training!* \n\n*Just avoid catastrophic overfitting, as seen in picture\n\nPaper: <LINK>\nCode: <LINK>\n\nJoint work with @_leslierice and @zicokolter to be at #ICLR2020 <LINK>', ""2/ Summary: Changing the initialization to be uniformly random is the main contributor towards successful FGSM adversarial training.\n\nGenerated adversarial examples need to be able to actually span the entire threat model, but otherwise don't need to be that strong for training."", '3/ Save your valuable time with cyclic learning rates and mixed precision! These techniques can train robust CIFAR10 and ImageNet in 6 min and 12 hrs using FGSM adv training.\n\nSuper easy to incorporate (just add a 3-4 lines of code), and can accelerate any training method.', '4/ Did you try FGSM before and it didn\'t work? It probably failed due to ""catastrophic overfitting"": plotting the learning curves reveals that, if done incorrectly, FGSM adv training learns a robust classifier, up until it suddenly and rapidly deteriorates within a single epoch.']",https://arxiv.org/abs/2001.03994,"Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45% robust accuracy to PGD attacks with $\epsilon=8/255$ in 6 minutes, and a robust ImageNet classifier with 43% robust accuracy at $\epsilon=2/255$ in 12 hours, in comparison to past work based on ""free"" adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as ""catastrophic overfitting"" which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at this https URL ",Fast is better than free: Revisiting adversarial training
56,1217456466040442880,2654165034,Jason M Pittman,['Think you can reliably recognize #password strength? Think again. Check out my new #research paper: <LINK>'],http://arxiv.org/abs/2001.04930,"The purpose of this study was to measure whether participant education, profession, and technical skill level exhibited a relationship with identification of password strength. Participants reviewed 50 passwords and labeled each as weak or strong. A Chi-square test of independence was used to measure relationships between education, profession, technical skill level relative to the frequency of weak and strong password identification. The results demonstrate significant relationships across all variable combinations except for technical skill and strong passwords which demonstrated no relationship. This research has three limitations. Data collection was dependent upon participant self-reporting and has limited externalized power. Further, the instrument was constructed under the assumption that all participants could read English and understood the concept of password strength. Finally, we did not control for external tool use (i.e., password strength meter). The results build upon existing literature insofar as the outcomes add to the collective understanding of user perception of passwords in specific and authentication in general. Whereas prior research has explored similar areas, such work has done so by having participants create passwords. This work measures perception of pre-generated passwords. The results demonstrate a need for further investigation into why users continue to rely on weak passwords. The originality of this work rests in soliciting a broad spectrum of participants and measuring potential correlations between participant education, profession, and technical skill level. ",Shades of Perception- User Factors in Identifying Password Strength
57,1217435303184650240,4639078397,John Wise,"['New paper day! D. Skinner (@drenniks) has her 1st 1st-author paper! Pop III multiplicity is the norm and their host halos are pretty resistant to UV backgrounds. They may be more common than previously thought. More metal, more BHs. Check it out! <LINK> <LINK>', '@toomanyspectra @drenniks Thanks! Next on the list: r-process enrichment from NSMs!']",https://arxiv.org/abs/2001.04480,"The formation of Population III (Pop III) stars is a critical step in the evolution of the early universe. To understand how these stars affected their metal-enriched descendants, the details of how, why and where Pop III formation takes place needs to be determined. One of the processes that is assumed to greatly affect the formation of Pop III stars is the presence of a Lyman-Werner (LW) radiation background, that destroys H$_2$, a necessary coolant in the creation of Pop III stars. Self-shielding can alleviate the effect the LW background has on the H$_2$ within haloes. In this work, we perform a cosmological simulation to study the birthplaces of Pop III stars, using the adaptive mesh refinement code Enzo. We investigate the distribution of host halo masses and its relationship to the LW background intensity. Compared to previous work, haloes form Pop III stars at much lower masses, up to a factor of a few, due to the inclusion of H$_2$ self-shielding. We see no relationship between the LW intensity and host halo mass. Most haloes form multiple Pop III stars, with a median number of four, up to a maximum of 16, at the instance of Pop III formation. Our results suggest that Pop III star formation may be less affected by LW radiation feedback than previously thought and that Pop III multiple systems are common. ","Cradles of the first stars: self-shielding, halo masses, and
  multiplicity"
58,1217353286187016192,1084300516543381505,endo_suguru,['<LINK>\n\nWe uploaded a new paper to arxiv!'],https://arxiv.org/abs/2001.04891,"Quantum error mitigation (QEM) is vital for noisy intermediate-scale quantum (NISQ) devices. While most conventional QEM schemes assume discrete gate-based circuits with noise appearing either before or after each gate, the assumptions are inappropriate for describing realistic noise that may have strong gate-dependence and complicated nonlocal effects, and general computing models such as analog quantum simulators. To address these challenges, we first extend the scenario, where each computation process, being either digital or analog, is described by a continuous time evolution. For noise from imperfections of the engineered Hamiltonian or additional noise operators, we show it can be effectively suppressed by a novel stochastic QEM method. Since our method only assumes accurate single qubit controls, it is applicable to all digital quantum computers and various analog simulators. Meanwhile, errors in the mitigation procedure can be suppressed by leveraging the Richardson extrapolation method. As we numerically test our method with various Hamiltonians under energy relaxation and dephasing noise and digital quantum circuits with additional two-qubit crosstalk, we show an improvement of simulation accuracy by two orders. We assess the resource cost of our scheme and conclude the feasibility of accurate quantum computing with NISQ devices. ","Mitigating realistic noise in practical noisy intermediate-scale quantum
  devices"
59,1217287301920849922,775759968221868032,Bo Wang,['Amazed to receive the email from #deepai that our new preprint <LINK> is on trend!  The new paper pioneers to develop novel GCNs on unsupervised image retrieval and achieve better accuracies on many large scale image datasets without any human labeling. <LINK>'],https://arxiv.org/abs/2001.01284,"Diffusion has shown great success in improving accuracy of unsupervised image retrieval systems by utilizing high-order structures of image manifold. However, existing diffusion methods suffer from three major limitations: 1) they usually rely on local structures without considering global manifold information; 2) they focus on improving pair-wise similarities within existing images input output transductively while lacking flexibility to learn representations for novel unseen instances inductively; 3) they fail to scale to large datasets due to prohibitive memory consumption and computational burden due to intrinsic high-order operations on the whole graph. In this paper, to address these limitations, we propose a novel method, Graph Diffusion Networks (GRAD-Net), that adopts graph neural networks (GNNs), a novel variant of deep learning algorithms on irregular graphs. GRAD-Net learns semantic representations by exploiting both local and global structures of image manifold in an unsupervised fashion. By utilizing sparse coding techniques, GRAD-Net not only preserves global information on the image manifold, but also enables scalable training and efficient querying. Experiments on several large benchmark datasets demonstrate effectiveness of our method over state-of-the-art diffusion algorithms for unsupervised image retrieval. ","Learning Global and Local Consistent Representations for Unsupervised
  Image Retrieval via Deep Graph Diffusion Networks"
60,1217219860398321665,830355049,Mohit Bansal,"['Check out this new #AAAI2020 paper ""Multi-Source Domain Adaptation for Text Classification via DistanceNet-Bandits"" by @HanGuo97 @Ramakanth1729 --&gt; <LINK>\n\n<LINK>\n<LINK>\nPS. Han is on the PhD admissions market!\n\n@RealAAAI @UNCNLP <LINK>']",https://arxiv.org/abs/2001.04362,"Domain adaptation performance of a learning algorithm on a target domain is a function of its source domain error and a divergence measure between the data distribution of these two domains. We present a study of various distance-based measures in the context of NLP tasks, that characterize the dissimilarity between domains based on sample estimates. We first conduct analysis experiments to show which of these distance measures can best differentiate samples from same versus different domains, and are correlated with empirical results. Next, we develop a DistanceNet model which uses these distance measures, or a mixture of these distance measures, as an additional loss function to be minimized jointly with the task's loss function, so as to achieve better unsupervised domain adaptation. Finally, we extend this model to a novel DistanceNet-Bandit model, which employs a multi-armed bandit controller to dynamically switch between multiple source domains and allow the model to learn an optimal trajectory and mixture of domains for transfer to the low-resource target domain. We conduct experiments on popular sentiment analysis datasets with several diverse domains and show that our DistanceNet model, as well as its dynamic bandit variant, can outperform competitive baselines in the context of unsupervised domain adaptation. ","Multi-Source Domain Adaptation for Text Classification via
  DistanceNet-Bandits"
61,1217158388485775360,176202072,'(Robert Smith),"['Ever wondered why @rigetti’s hybrid quantum systems are so dang fast? Careful &amp; deliberate design, a steady hand, and a bunch of technical stuff. Read all about in a new paper by @pkaralekas, @niktezak, @ringspectrum, Colm Ryan, @themarcusps, and me:\n\n<LINK>', 'Spoiler Alert: There are many things that make the system fast, but one big one is the optimizing compiler paired with a balanced source programming language! I’d hate to keep the compiler—called quilc—all to myself, so we put it on GitHub!\n\nhttps://t.co/9ziQH9RIKI']",https://arxiv.org/abs/2001.04449,"In order to support near-term applications of quantum computing, a new compute paradigm has emerged--the quantum-classical cloud--in which quantum computers (QPUs) work in tandem with classical computers (CPUs) via a shared cloud infrastructure. In this work, we enumerate the architectural requirements of a quantum-classical cloud platform, and present a framework for benchmarking its runtime performance. In addition, we walk through two platform-level enhancements, parametric compilation and active qubit reset, that specifically optimize a quantum-classical architecture to support variational hybrid algorithms (VHAs), the most promising applications of near-term quantum hardware. Finally, we show that integrating these two features into the Rigetti Quantum Cloud Services (QCS) platform results in considerable improvements to the latencies that govern algorithm runtime. ","A quantum-classical cloud platform optimized for variational hybrid
  algorithms"
62,1217062421082316801,3351977373,Alex Clark,['New paper up on the arXiv today! Great collaboration with Dara McCutcheon @QETLabsBristol and Jake Iles-Smith @sheffielduni on phonon dephasing in molecules. Congrats to PhD students Chloe and Ross on a brilliant paper! <LINK> @ImperialPhysics @Imperial_IMSE'],https://arxiv.org/abs/2001.04365,"Organic molecules have recently gained attention as novel sources of single photons. We present a joint experiment--theory analysis of the temperature-dependent emission spectra, zero-phonon linewidth, and second-order correlation function of light emitted from a single molecule. We observe spectra with a zero-phonon-line together with several additional sharp peaks, broad phonon sidebands, and a strongly temperature dependent homogeneous broadening. Our model includes both localised vibrational modes of the molecule and a thermal phonon bath, which we include non-perturbatively, and is able capture all observed features. For resonant driving we measure Rabi oscillations that become increasingly damped with temperature, which our model naturally reproduces. Our results constitute an essential characterisation of the photon coherence of these promising molecules, paving the way towards their use in future quantum information applications. ",Phonon-induced optical dephasing in single organic molecules
63,1217037416709660677,2603024598,Ricardo Pérez-Marco,"['First paper of a series on Gamma functions. We give a new definition of Euler Gamma function that is more natural from the complex analysis and transalgebraic point of view.\n\n""On the definition of Euler Gamma function""\n\n<LINK>\n<LINK>']",https://arxiv.org/abs/2001.04445,"We present a new definition of Euler Gamma function. From the complex analysis and transalgebraic viewpoint, it is a natural characterization in the space of finite order meromorphic functions. We show how the classical theory and formulas develop naturally, and we discuss the relation with other definitions. We show in a companion article, that this definition generalizes to higher gamma functions and provides an unifying framework for their definition which is more natural than the usual Bohr-Mollerup or Lerch approaches. ",On the definition of Euler Gamma function
64,1217009522046357504,80569756,Andreas Brunthaler,"['A new paper about the proper motion of the supermassive black hole at the center for our Galaxy: \n\n<LINK>\n\nThe observations span 18 years now and limit the motion to &lt; 1 km/s. This corresponds to a lower limit of the mass of 1 Million Solar masses (Msol) (1/2)', 'Combined with the size from EHT observations, this gives a density that is within a factor of 3 of the of the General Relativity limit for a black hole. \nFurthermore, intermediate mass black holes more massive than 30,000 Msol between 0.003 and 0.1 pc are excluded. (2/2)']",http://arxiv.org/abs/2001.04386,"We report measurements with the Very Long Baseline Array of the proper motion of Sgr A* relative to two extragalactic radio sources spanning 18 years. The apparent motion of Sgr A* is -6.411 +/- 0.008 mas/yr along the Galactic plane and -0.219 +/- 0.007 mas/yr toward the North Galactic Pole. This apparent motion can almost entirely be attributed to the effects of the Sun's orbit about the Galactic center. Removing these effects yields residuals of -0.58 +/- 2.23 km/s in the direction of Galactic rotation and -0.85 +/- 0.75 km/s toward the North Galactic Pole. A maximum-likelihood analysis of the motion, both in the Galactic plane and perpendicular to it, expected for a massive object within the Galactic center stellar cluster indicates that the radiative source, Sgr A*, contains more than about 25% of the gravitational mass of 4 x 10^6 Msun deduced from stellar orbits. The intrinsic size of Sgr A* is comparable to its Schwarzschild radius, and the implied mass density of >4 x 10^23 Msun/pc^-3 very close to that expected for a black hole, providing overwhelming evidence that it is indeed a super-massive black hole. Finally, the existence of ""intermediate-mass"" black holes more massive than 3 x 10^4 Msun between approximately 0.003 and 0.1 pc from Sgr A*are excluded. ","The Proper Motion of Sagittarius A*: III. The Case for a Supermassive
  Black Hole"
65,1216901561114222593,1075649842955866114,Luca Cortese,"['New paper out today by @ICRAR @UWAresearch @ARC_ASTRO3D  PhD student @awattsup_ \nshowing that HI asymmetries are associated with gas-poor satellites galaxies, and highlighting the importance of signal-to-noise when studying HI profiles shape. #xGASS  <LINK> <LINK>']",https://arxiv.org/abs/2001.04037,"We present an analysis of asymmetries in global HI spectra from the extended GALEX Arecibo SDSS Survey (xGASS), a stellar mass-selected and gas fraction-limited survey which is representative of the HI properties of galaxies in the local Universe. We demonstrate that the asymmetry in a HI spectrum is strongly linked to its signal-to-noise meaning that, contrary to what was done in previous works, asymmetry distributions for different samples cannot be compared at face value. We develop a method to account for noise-induced asymmetry and find that the typical galaxy detected by xGASS exhibits higher asymmetry than what can be attributed to noise alone, with 37% of the sample showing asymmetry greater than 10% at an 80% confidence level. We find that asymmetric galaxies contain, on average, 29% less HI mass compared to their symmetric counterparts matched in both stellar mass and signal-to-noise. We also present clear evidence that satellite galaxies, as a population, exhibit more asymmetric HI spectra than centrals and that group central galaxies show a slightly higher rate of HI asymmetries compared to isolated centrals. All these results support a scenario in which environmental processes, in particular those responsible for gas removal, are the dominant driver of asymmetry in xGASS. ","xGASS: Robust quantification of asymmetries in global HI spectra and
  their relationship to environmental processes"
66,1216775386601861121,28734416,Sebastian Risi,"['Happy our new paper ""Revealing Neural Network Bias to Non-Experts Through Interactive Counterfactual Examples"" w/ @chelmyers, @efreed52, @larispardo, @anushayfurqan, and @jichenz is now on arXiv: <LINK> @ITUkbh @DrexelUniv <LINK>', 'Traditionally, non-experts have little control in uncovering potential social bias in the algorithms that may impact their lives. We present a preliminary design for an interactive visualization tool, to reveal biases in neural networks.', ""The tool combines counterfactual examples and clustering of the networks' activation patterns to empower non-experts to detect bias. For most of us, this is the first foray\xa0into fair AI methods, so any pointers to other relevant work or suggestions are greatly appreciated."", ""@drscotthawley @mahimapushkarna Thanks! And we'll have a look at that tool.""]",https://arxiv.org/abs/2001.02271,"AI algorithms are not immune to biases. Traditionally, non-experts have little control in uncovering potential social bias (e.g., gender bias) in the algorithms that may impact their lives. We present a preliminary design for an interactive visualization tool CEB to reveal biases in a commonly used AI method, Neural Networks (NN). CEB combines counterfactual examples and abstraction of an NN decision process to empower non-experts to detect bias. This paper presents the design of CEB and initial findings of an expert panel (n=6) with AI, HCI, and Social science experts. ","Revealing Neural Network Bias to Non-Experts Through Interactive
  Counterfactual Examples"
67,1216753540728672256,1143059450359513088,Rahul Sharma,['New paper:\nPresent results from the spectral and timing study of Accreting millisecond X-ray pulsar using AstroSat <LINK>'],https://arxiv.org/abs/2001.03594,"SAX J1748.9-2021 is a transient accretion powered millisecond X-ray pulsar located in the Globular cluster NGC 6440. We report on the spectral and timing analysis of SAX J1748.9-2021 performed on AstroSat data taken during its faint and short outburst of 2017. We derived the best-fitting orbital solution for the 2017 outburst and obtained an average local spin frequency of 442.361098(3) Hz. The pulse profile obtained from 3-7 keV and 7-20 keV energy bands suggest constant fractional amplitude ~0.5% for fundamental component, contrary to previously observed energy pulse profile dependence. Our AstroSat observations revealed the source to be in a hard spectral state. The 1-50 keV spectrum from SXT and LAXPC on-board AstroSat can be well described with a single temperature blackbody and thermal Comptonization. Moreover, we found that the combined spectra from XMM-Newton (EPIC-PN) and AstroSat (SXT+LAXPC) indicated the presence of reflection features in the form of iron (Fe K${\alpha}$) line that we modeled with the reflection model xillvercp. One of the two X-ray burst observed during the AstroSat/LAXPC observation showed hard X-ray emission (>30 keV) due to Compton up-scattering of thermal photons by the hot corona. Time resolved analysis performed on the bursts revealed complex evolution in emission radius of blackbody for second burst suggestive of mild photospheric radius expansion. ","A broadband look of the Accreting Millisecond X-ray Pulsar SAX
  J1748.9-2021 using AstroSat and XMM-Newton"
68,1216700308270743552,1114972720826068992,Danny Horta Darrington 🇺🇦,"['Check out our new paper on the chemical compositions of accreted and ""in situ"" Galactic Globular Clusters found in APOGEE. We use alpha/Fe vs Fe/H abundances to infer GC origin, and consequently constrain the mass assembly of the Galaxy <LINK> <LINK>']",https://arxiv.org/abs/2001.03177,"Studies of the kinematics and chemical compositions of Galactic globular clusters (GCs) enable the reconstruction of the history of star formation, chemical evolution, and mass assembly of the Galaxy. Using the latest data release (DR16) of the SDSS/APOGEE survey, we identify 3,090 stars associated with 46 GCs. Using a previously defined kinematic association, we break the sample down into eight separate groups and examine how the kinematics-based classification maps into chemical composition space, considering only $\alpha$ (mostly Si and Mg) elements and Fe. Our results show that: (i) The loci of both in situ and accreted subgroups in chemical space match those of their field counterparts; (ii) GCs from different individual accreted subgroups occupy the same locus in chemical space. This could either mean that they share a similar origin or that they are associated with distinct satellites which underwent similar chemical enrichment histories; (iii) The chemical compositions of the GCs associated with the low orbital energy subgroup defined by Massari and collaborators is broadly consistent with an in situ origin. However, at the low metallicity end, the distinction between accreted and in situ populations is blurred; (iv) Regarding the status of GCs whose origin is ambiguous, we conclude the following: the position in Si-Fe plane suggests an in situ}origin for Liller 1 and a likely accreted origin for NGC 5904 and NGC 6388. The case of NGC 288 is unclear, as its orbital properties suggest an accretion origin, its chemical composition suggests it may have formed in situ. ","The Chemical Compositions of Accreted and {\it in situ} Galactic
  Globular Clusters According to SDSS/APOGEE"
69,1216134530572148737,33315710,Austin Benson,"['(1/7) Here is a new paper on hypergraph cuts by @n_veldt, the twitterless Jon Kleinberg, and me that I am very excited about: <LINK>\n \nmin s-t graph cut is one of the basic problems we learn about in an algo class.\n \nWhat if we want min s-t _hypergraph_ cut?', ""@n_veldt (2/7) Given a bipartition of nodes of a graph, an edge can only be cut in one way: one node on each side of the cut.\n \nWith hyperegraphs, it's complicated. There are many ways to cut a hyperedge. With a 4-node hyperedge {u,v,w,x} we might care about {u}, {v,w,x} vs. {u,v}, {w,x}"", '@n_veldt (3/7) In 1973, Lawler first studied the case where one only cares if all nodes in a hyperedge are on the same side of the cut or not: {u}, {v,w,x} or {u,v}, {w,x} would both incur the same cost.\n \nLawler showed how to reduce this min s-t hypergraph cut to min s-t _graph_ cut.', '@n_veldt (4/7) What if we want different costs for different types of hyperedge cuts?\n \nOne natural way is what we call cardinality-based penalties: we care about the number of nodes on each side of the cut (with symmetry, just the number of nodes on the “small” side of the partition).', '@n_veldt (5/7) With cardinality-based,\n{u}, {v,w,x} has a cost of w1 (1 node on small side)\n{u,v}, {w,x} has a cost of w2 (2 nodes on small side)\n \nIf w1 &lt;= w2 &lt;= 2 * w1 (more generally, if the weights look submodular), we show that min s-t hypergraph cut reduces to graph cut.', '@n_veldt (6/7) Moreover, if the penalties are cardinality-based, the only min s-t hypergraph cut problems that reduce to min s-t graph cuts are ones where the weights look submodular.\n \nThere are also pretty simple hard min s-t hypergraph cut problems. The case w2 &lt; w1 is NP-hard.', '@n_veldt (7/7) We have lots more in the paper, including 10 concrete open questions at the end.\n \nWe tried to survey the many ways in which various hypergraph cuts have appeared in the literature. Let us know if you have more references!', ""@UthsavC @n_veldt Good question --- one that I have thought about a fair amount but unfortunately don't have a great answer for yet.\n\nI think the Chan et al. JACM 2016 paper that you cite provides some answers, although the diffusion process there is not a simple random walk.""]",https://arxiv.org/abs/2001.02817,"The minimum $s$-$t$ cut problem in graphs is one of the most fundamental problems in combinatorial optimization, and graph cuts underlie algorithms throughout discrete mathematics, theoretical computer science, operations research, and data science. While graphs are a standard model for pairwise relationships, hypergraphs provide the flexibility to model multi-way relationships, and are now a standard model for complex data and systems. However, when generalizing from graphs to hypergraphs, the notion of a ""cut hyperedge"" is less clear, as a hyperedge's nodes can be split in several ways. Here, we develop a framework for hypergraph cuts by considering the problem of separating two terminal nodes in a hypergraph in a way that minimizes a sum of penalties at split hyperedges. In our setup, different ways of splitting the same hyperedge have different penalties, and the penalty is encoded by what we call a splitting function. Our framework opens a rich space on the foundations of hypergraph cuts. We first identify a natural class of cardinality-based hyperedge splitting functions that depend only on the number of nodes on each side of the split. In this case, we show that the general hypergraph $s$-$t$ cut problem can be reduced to a tractable graph $s$-$t$ cut problem if and only if the splitting functions are submodular. We also identify a wide regime of non-submodular splitting functions for which the problem is NP-hard. We also analyze extensions to multiway cuts with at least three terminal nodes and identify a natural class of splitting functions for which the problem can be reduced in an approximation-preserving way to the node-weighted multiway cut problem in graphs, again subject to a submodularity property. Finally, we outline several open questions on general hypergraph cut problems. ",Hypergraph Cuts with General Splitting Functions
70,1215657011028877312,855945227718230016,Tarraneh Eftekhari,['Our new paper compares the compact radio sources discovered by Amy Reines+19 to the persistent radio source associated with the first repeating fast radio burst and finds that they may share a common origin!  @edobergerhvd @bluekilonova @pkgw  <LINK>'],https://arxiv.org/abs/2001.02688,"The discovery of a persistent radio source coincident with the first repeating fast radio burst, FRB 121102, and offset from the center of its dwarf host galaxy has been used as evidence for a link with young millisecond magnetars born in superluminous supernovae (SLSNe) or long-duration gamma-ray bursts (LGRBs). A prediction of this scenario is that compact radio sources offset from the centers of dwarf galaxies may serve as signposts for at least some FRBs. Recently, Reines et al. 2019 presented the discovery of 20 such radio sources in nearby ($z\lesssim 0.055$) dwarf galaxies, and argued that these cannot be explained by emission from HII regions, normal supernova remnants, or normal radio supernovae. Instead, they attribute the emission to accreting wandering massive black holes. Here, we explore the alternative possibility that these sources are analogs of FRB 121102. We compare their properties -- radio luminosities, spectral energy distributions, light curves, ratios of radio-to-optical flux, and spatial offsets -- to FRB 121102, a few other well-localized FRBs, and potentially related systems, and find that these are all consistent as arising from the same population. We further compare their properties to the magnetar nebula model used to explain FRB 121102, as well as to theoretical off-axis LGRB light curves, and find overall consistency. Finally, we find a consistent occurrence rate relative to repeating FRBs and LGRBs. We outline key follow-up observations to further test these possible connections. ","Wandering Massive Black Holes or Analogs of the First Repeating Fast
  Radio Burst?"
71,1215581262448152576,929973145,Dr David Sobral 💫🌌,"['New LEGA-C paper by Justin Cole, Rachel Bezanson et al. <LINK> - Stellar Kinematics and Environment at z~0.8 in the LEGA-C Survey and comparison with the local Universe: Massive, Slow-Rotators are Built First in Over-dense Environments. <LINK>']",https://arxiv.org/abs/2001.02695,"In this letter, we investigate the impact of environment on integrated and spatially-resolved stellar kinematics of a sample of massive, quiescent galaxies at intermediate redshift ($0.6<z<1.0$). For this analysis, we combine photometric and spectroscopic parameters from the UltraVISTA and Large Early Galaxy Astrophysics Census (LEGA-C) surveys in the COSMOS field and environmental measurements. We analyze the trends with overdensity (1+$\delta$) on the rotational support of quiescent galaxies and find no universal trends at either fixed mass or fixed stellar velocity dispersion. This is consistent with previous studies of the local Universe; rotational support of massive galaxies depends primarily on stellar mass. We highlight two populations of massive galaxies ($\log M_\star/M_\odot\geq11$) that deviate from the average mass relation. First, the most massive galaxies in the most under-dense regions ($(1+\delta)\leq1$) exhibit elevated rotational support. Similarly, at the highest masses ($\log M_\star/M_\odot\geq11.25$) the range in rotational support is significant in all but the densest regions. This corresponds to an increasing slow-rotator fraction such that only galaxies in the densest environments ($(1+\delta)\geq3.5$) are primarily (90$\pm$10\%) slow-rotators.This effect is not seen at fixed velocity dispersion, suggesting minor merging as the driving mechanism: only in the densest regions have the most massive galaxies experienced significant minor merging, building stellar mass and diminishing rotation without significantly affecting the central stellar velocity dispersion. In the local Universe, most massive galaxies are slow-rotators, regardless of environment, suggesting minor merging occurs at later cosmic times $(z\lesssim0.6)$ in all but the most dense environments. ","Stellar Kinematics and Environment at z~0.8 in the LEGA-C Survey:
  Massive, Slow-Rotators are Built First in Overdense Environments"
72,1215486344547319809,452384386,Sebastien Bubeck,['How to trap a gradient flow?<LINK>\n\nPumped about this 1st paper of the decade! W. my intern Dan Mikulincer we found a new way to search for stationary points of non-convex fcts. *Gradient Flow Trapping* is optimal in 2D/improves SOTA in 3D. 4+D remains wide open! <LINK>'],https://arxiv.org/abs/2001.02968,"We consider the problem of finding an $\varepsilon$-approximate stationary point of a smooth function on a compact domain of $\mathbb{R}^d$. In contrast with dimension-free approaches such as gradient descent, we focus here on the case where $d$ is finite, and potentially small. This viewpoint was explored in 1993 by Vavasis, who proposed an algorithm which, for any fixed finite dimension $d$, improves upon the $O(1/\varepsilon^2)$ oracle complexity of gradient descent. For example for $d=2$, Vavasis' approach obtains the complexity $O(1/\varepsilon)$. Moreover for $d=2$ he also proved a lower bound of $\Omega(1/\sqrt{\varepsilon})$ for deterministic algorithms (we extend this result to randomized algorithms). Our main contribution is an algorithm, which we call gradient flow trapping (GFT), and the analysis of its oracle complexity. In dimension $d=2$, GFT closes the gap with Vavasis' lower bound (up to a logarithmic factor), as we show that it has complexity $O\left(\sqrt{\frac{\log(1/\varepsilon)}{\varepsilon}}\right)$. In dimension $d=3$, we show a complexity of $O\left(\frac{\log(1/\varepsilon)}{\varepsilon}\right)$, improving upon Vavasis' $O\left(1 / \varepsilon^{1.2} \right)$. In higher dimensions, GFT has the remarkable property of being a logarithmic parallel depth strategy, in stark contrast with the polynomial depth of gradient descent or Vavasis' algorithm. In this higher dimensional regime, the total work of GFT improves quadratically upon the only other known polylogarithmic depth strategy for this problem, namely naive grid search. We augment this result with another algorithm, named \emph{cut and flow} (CF), which improves upon Vavasis' algorithm in any fixed dimension. ",How to trap a gradient flow
73,1215471658049396738,1710697381,Diego F. Torres,"['New year, first paper today, a study on gamma2 Velorum See it in <LINK> Hints of orbital variability are found, indicating maximum flux from the binary during apastron passage. <LINK>']",https://arxiv.org/abs/2001.02708,"Context. Colliding wind binaries are massive systems featuring strong, interacting stellar winds which may act as particle accelerators. Therefore, such binaries are good candidates for detection at high energies. However, only the massive binary Eta Carinae has been firmly associated with a gamma-ray signal. A second system, gamma^2 Velorum, is positionally coincident with a gamma-ray source, but unambiguous identification remains lacking. Aims. Observing orbital modulation of the flux would establish an unambiguous identification of the binary gamma^2 Velorum as the gamma-ray source detected by the Fermi Large Area Telescope (Fermi-LAT). Methods. We have used more than 10 years of observations with Fermi-LAT. Events are folded with the orbital period of the binary to search for variability. Systematic errors that might arise from the strong emission of the nearby Vela pulsar are studied by comparing with a more conservative pulse-gated analysis. Results. Hints of orbital variability are found, indicating maximum flux from the binary during apastron passage. Conclusions. Our analysis strengthens the possibility that gamma-rays are produced in gamma^2 Velorum, most likely as a result of particle acceleration in the wind collision region. The observed orbital variability is consistent with predictions from recent MHD simulations, but contrasts with the orbital variability from Eta Carinae, where the peak of the light curve is found at periastron. ",Hints of gamma-ray orbital variability from gamma^2 Velorum
74,1215461144334360583,1162081,Carlos Baquero,"['In this new paper we developed a simple, fast and space-efficient representation of a sliding window over an unbounded stream: Age-Partitioned Bloom Filters <LINK>', 'Previous best solutions to approximate membership queries on sliding windows were by dictionary based approaches. Our new proposal is a direct extension of bloom filters and allows very simple implementations, without dictionaries and with just a few accesses to memory.']",https://arxiv.org/abs/2001.03147,"Bloom filters (BF) are widely used for approximate membership queries over a set of elements. BF variants allow removals, sets of unbounded size or querying a sliding window over an unbounded stream. However, for this last case the best current approaches are dictionary based (e.g., based on Cuckoo Filters or TinyTable), and it may seem that BF-based approaches will never be competitive to dictionary-based ones. In this paper we present Age-Partitioned Bloom Filters, a BF-based approach for duplicate detection in sliding windows that not only is competitive in time-complexity, but has better space usage than current dictionary-based approaches (e.g., SWAMP), at the cost of some moderate slack. APBFs retain the BF simplicity, unlike dictionary-based approaches, important for hardware-based implementations, and can integrate known improvements such as double hashing or blocking. We present an Age-Partitioned Blocked Bloom Filter variant which can operate with 2-3 cache-line accesses per insertion and around 2-4 per query, even for high accuracy filters. ",Age-Partitioned Bloom Filters
75,1215376615288365056,956742472599666688,"Bijan Fakhri, PhD","[""Check out our paper accepted to ICSM'19 on Foveated Haptic Gaze, a new way to interact with 3D visual environments using solely haptics. #HCI #accessibility \n<LINK> <LINK>""]",https://arxiv.org/abs/2001.01824,"As digital worlds become ubiquitous via video games, simulations, virtual and augmented reality, people with disabilities who cannot access those worlds are becoming increasingly disenfranchised. More often than not the design of these environments focuses on vision, making them inaccessible in whole or in part to people with visual impairments. Accessible games and visual aids have been developed but their lack of prevalence or unintuitive interfaces make them impractical for daily use. To address this gap, we present Foveated Haptic Gaze, a method for conveying visual information via haptics that is intuitive and designed for interacting with real-time 3-dimensional environments. To validate our approach we developed a prototype of the system along with a simplified first-person shooter game. Lastly we present encouraging user study results of both sighted and blind participants using our system to play the game with no visual feedback. ",Foveated Haptic Gaze
76,1215271012067160071,69202541,Jonathan Le Roux,"[""Niko Moritz, Takaaki Hori, and I have a new paper out on arXiv on streaming ASR with Transformer. We obtain what we believe to be new SOTA for online end-to-end ASR on LibriSpeech, with 2.7% and 7.0% WER on the 'clean' and 'other' test sets. <LINK>""]",https://arxiv.org/abs/2001.02674,"Encoder-decoder based sequence-to-sequence models have demonstrated state-of-the-art results in end-to-end automatic speech recognition (ASR). Recently, the transformer architecture, which uses self-attention to model temporal context information, has been shown to achieve significantly lower word error rates (WERs) compared to recurrent neural network (RNN) based system architectures. Despite its success, the practical usage is limited to offline ASR tasks, since encoder-decoder architectures typically require an entire speech utterance as input. In this work, we propose a transformer based end-to-end ASR system for streaming ASR, where an output must be generated shortly after each spoken word. To achieve this, we apply time-restricted self-attention for the encoder and triggered attention for the encoder-decoder attention mechanism. Our proposed streaming transformer architecture achieves 2.8% and 7.2% WER for the ""clean"" and ""other"" test data of LibriSpeech, which to our knowledge is the best published streaming end-to-end ASR result for this task. ",Streaming automatic speech recognition with the transformer model
77,1215258112384475136,19149703,Karina Voggel ✨🔭🏃🏼‍♀️,"['Today our new paper on how we can find globular clusters with the help of @ESAGaia is out on the arxiv! with @anilcseth @caprastro &amp; @sand_dave <LINK>', '@ESAGaia @anilcseth @caprastro @sand_dave We tried to find globular clusters in Centaurus A by using colour and astrometric excess factors in Gaia DR2. We realized that star clusters in nearby galaxies do not appear like standard point sources in Gaia. https://t.co/nkpMAxKJZ9', '@ESAGaia @anilcseth @caprastro @sand_dave Normally these excess factors are used as a quality assessment tool in Gaia to purge out bad sources!', '@ESAGaia @anilcseth @caprastro @sand_dave We have followe up a few candidates, and identified 5 brand new GCs in the outskirts of CenA. They actually are clearly visible as star clusters in good seeing imaging but we had not found them because the outer Halo of Gaia spans several square degrees on the sky. https://t.co/xDCaSyUi7X', '@ESAGaia @anilcseth @caprastro @sand_dave These newly confirmed clusters (Blue datapoints) are now the record holders for the three most distant known GCs in CenA! This shows the power of this Gaia method to identify good candidates in the distant outskirts. https://t.co/YmA7KTnz3X', '@ESAGaia @anilcseth @caprastro @sand_dave And we expect that this @ESAGaia method is applicable to find bright GCs in most Local Volume galaxies out to 25Mpc. Especially in those sparse outer Halos of galaxies! https://t.co/8xVH1LvTF3', '@ESAGaia @anilcseth @caprastro @sand_dave On top of that we also show that the excess factors are directly correlated to the physical size of the GCs. Meaning that you can use the excess factors to get a rough size estimate of their sizes without the need for high-resolution HST imaging. https://t.co/qlkSS1SIHx']",https://arxiv.org/abs/2001.02243,"Tidally stripped galaxy nuclei and luminous globular clusters (GCs) are important tracers of the halos and assembly histories of nearby galaxies, but are difficult to reliably identify with typical ground-based imaging data. In this paper we present a new method to find these massive star clusters using Gaia DR2, focusing on the massive elliptical galaxy Centaurus A (Cen A). We show that stripped nuclei and globular clusters are partially resolved by Gaia at the distance of Cen A, showing characteristic astrometric and photometric signatures. We use this selection method to produce a list of 632 new candidate luminous clusters in the halo of Cen A out to a projected radius of 150 kpc. Adding in broadband photometry and visual examination improves the accuracy of our classification. In a spectroscopic pilot program we have confirmed 5 new luminous clusters, which includes the 7th and 10th most luminous GC in Cen\,A. Three of the newly discovered GCs are further away from Cen A in than all previously known GCs. Several of these are compelling candidates for stripped nuclei. We show that our novel Gaia selection method retains at least partial utility out to distances of 25 Mpc and hence is a powerful tool for finding and studying star clusters in the sparse outskirts of galaxies in the local universe. ","A Gaia-based catalog of candidate stripped nuclei and luminous globular
  clusters in the halo of Centaurus A"
78,1215243432802779136,216729597,Marcel S. Pawlowski,"[""Another day, another new paper on the arXiv. Check out @KatjaFah's fantastic work on the nuclear star clusters of two CenA dwarf galaxies (with @VoltarCH, @lellifede, @DrMLyubenova): <LINK>\n\nShe also has you covered with a thread summarizing the main findings 👇 <LINK>""]",https://arxiv.org/abs/2001.02241,"Studies of nucleated dwarf galaxies can constrain the scenarios for the formation and evolution of nuclear star clusters (NSC) in low-mass galaxies and give us insights on the origin of ultra compact dwarf galaxies (UCDs). We report the discovery of a NSC in the dwarf galaxy KKs58 and investigate its properties together with those of another NSC in KK197. Both NSCs are hosted by dwarf elliptical galaxies of the Centaurus group. Combining ESO VLT MUSE data with photometry from VLT FORS2, CTIO Blanco DECam, and HST ACS, as well as high-resolution spectroscopy from VLT UVES, we analyse the photometric, kinematic and stellar population properties of the NSCs and their host galaxies. We confirm membership of the NSCs based on their radial velocities and location close to the galaxy centres. We also confirm the membership of two globular clusters (GCs) and detect oblate rotation in the main body of KK197. Based on high signal-to-noise spectra taken with MUSE of the NSCs of both KKs58 and KK197 we measure low metallicities, [Fe/H] = $-1.75 \pm 0.06$ dex and [Fe/H] = $-1.84 \pm 0.05$ dex, and stellar masses of $7.3 \times 10^5 M_\odot$ and $1.0 \times 10^6 M_\odot$, respectively. Both NSCs are more metal-poor than their hosts that have metallicities of $-1.35 \pm 0.23$ dex (KKs58) and $-0.84 \pm 0.12$ dex (KK197). This can be interpreted as NSC formation via the in-spiral of GCs. The masses, sizes and metallicities of the two NSCs place them among other NSCs, but also among the known UCDs of the Centaurus group. This indicates that NSCs might constitute the progenitors of a part of the low-mass UCDs, although their properties are almost indistinguishable from typical GCs. ","Metal-poor nuclear star clusters in two dwarf galaxies near Centaurus A
  suggesting formation from the in-spiraling of globular clusters"
79,1215216368196431873,4665536483,James Grant,"['Paper with @DLeslieLancs accepted to AISTATS! “On Thompson Sampling for Smoother-than-Lipschitz Bandits” provides new regret bounds in bandit problems with smooth reward functions <LINK> Excited to present this work in Palermo in June!', '@storiLucy @DLeslieLancs Thanks Lucy!!', '@Boukouva1Alexis @DLeslieLancs Thanks Alexis!!']",https://arxiv.org/abs/2001.02323,"Thompson Sampling is a well established approach to bandit and reinforcement learning problems. However its use in continuum armed bandit problems has received relatively little attention. We provide the first bounds on the regret of Thompson Sampling for continuum armed bandits under weak conditions on the function class containing the true function and sub-exponential observation noise. Our bounds are realised by analysis of the eluder dimension, a recently proposed measure of the complexity of a function class, which has been demonstrated to be useful in bounding the Bayesian regret of Thompson Sampling for simpler bandit problems under sub-Gaussian observation noise. We derive a new bound on the eluder dimension for classes of functions with Lipschitz derivatives, and generalise previous analyses in multiple regards. ",On Thompson Sampling for Smoother-than-Lipschitz Bandits
80,1214965581318885382,1094750915444310016,Alex Pizzuto,"['New paper on the arXiv! <LINK>\n\nThis paper has been the ""side project"" that I\'ve spent the last ~2 years on with the wonderful Anastasia Barbano and @IbrahimSafa1. Still in disbelief that research I did was interesting enough to get interviewed for 😊 <LINK>']",https://arxiv.org/abs/2001.01737,"During the first three flights of the Antarctic Impulsive Transient Antenna (ANITA) experiment, the collaboration detected several neutrino candidates. Two of these candidate events were consistent with an ultra-high-energy up-going air shower and compatible with a tau neutrino interpretation. A third neutrino candidate event was detected in a search for Askaryan radiation in the Antarctic ice, although it is also consistent with the background expectation. The inferred emergence angle of the first two events is in tension with IceCube and ANITA limits on isotropic cosmogenic neutrino fluxes. Here, we test the hypothesis that these events are astrophysical in origin, possibly caused by a point source in the reconstructed direction. Given that any ultra-high-energy tau neutrino flux traversing the Earth should be accompanied by a secondary flux in the TeV-PeV range, we search for these secondary counterparts in seven years of IceCube data using three complementary approaches. In the absence of any significant detection, we set upper limits on the neutrino flux from potential point sources. We compare these limits to ANITA's sensitivity in the same direction and show that an astrophysical explanation of these anomalous events under standard model assumptions is severely constrained regardless of source spectrum. ","A search for IceCube events in the direction of ANITA neutrino
  candidates"
81,1214844593075871744,136262002,Michael Black,"['Our new AAAI paper on Chained Representation Cycling is now online:  <LINK>  To enable unsupervised learning of 3D human pose from images, we break the problem into a series of transformations (cycles) between increasingly abstract representations. <LINK>']",https://arxiv.org/abs/2001.01613,"The goal of many computer vision systems is to transform image pixels into 3D representations. Recent popular models use neural networks to regress directly from pixels to 3D object parameters. Such an approach works well when supervision is available, but in problems like human pose and shape estimation, it is difficult to obtain natural images with 3D ground truth. To go one step further, we propose a new architecture that facilitates unsupervised, or lightly supervised, learning. The idea is to break the problem into a series of transformations between increasingly abstract representations. Each step involves a cycle designed to be learnable without annotated training data, and the chain of cycles delivers the final solution. Specifically, we use 2D body part segments as an intermediate representation that contains enough information to be lifted to 3D, and at the same time is simple enough to be learned in an unsupervised way. We demonstrate the method by learning 3D human pose and shape from un-paired and un-annotated images. We also explore varying amounts of paired data and show that cycling greatly alleviates the need for paired data. While we present results for modeling humans, our formulation is general and can be applied to other vision problems. ","Chained Representation Cycling: Learning to Estimate 3D Human Pose and
  Shape by Cycling Between Representations"
82,1214467266831515649,1160578862419345408,"Takinoue Lab, Tokyo Tech","['Our new paper entitled ""Surfactant concentration modulates the motion and placement of microparticles in an inhomogeneous electric fieldl"" by Marcos Masukawa (@MarcosMasukawa), Masayuki Hayakawa, *Masahiro Takinoue has been uploaded on arXiv.\n<LINK> <LINK>']",https://arxiv.org/abs/2001.01359,"This study examined the effects of surfactants on the motion and positioning of microparticles in an inhomogeneous electric field. The microparticles were suspended in oil with a surfactant and the electric field was generated using sawtooth-patterned electrodes. The microparticles were trapped, oscillated, or attached to the electrodes. The proportion of microparticles in each state was defined by the concentration of surfactant and the voltage applied to the electrodes. Based on the trajectory of the microparticles in the electric field, a newly developed physical model in which the surfactant was adsorbed on the microparticles allowed the microparticles to be charged by contact with the electrodes, with either positive or negative charges, while the non-adsorbed surfactant micellizing in the oil contributed to charge relaxation. A simulation based on this model showed that the charging and charge relaxation, as modulated by the surfactant concentration, can explain the trajectories and proportion of the trapped, oscillating, and attached microparticles. These results will be useful for the development of novel self-assembly and transport technologies and colloids sensitive to electricity. ","Surfactant concentration modulates the motion and placement of
  microparticles in an inhomogeneous electric field"
83,1214431462323130369,801743,Neil Ernst,"['New paper for #saner20 from my students and I : cross-dataset design discussion mining. Preprint: <LINK> Replication: <LINK> We look at how well classifiers do across project datasets (ok, not great) when labeling design.', 'Big congrats to my students Alvi and Karan who worked very hard on this one. Thanks to the ICSME and SANER reviewers who had some great comments that improved the paper a lot.', 'And finally thanks to @joaobrunet, @gnviviani and others who made it easy for us to build on their work. Have a citation! 😃😃😃😃', '@joaobrunet @gnviviani About 2 tweets earlier :)']",https://arxiv.org/abs/2001.01424,"Being able to identify software discussions that are primarily about design, which we call design mining, can improve documentation and maintenance of software systems. Existing design mining approaches have good classification performance using natural language processing (NLP) techniques, but the conclusion stability of these approaches is generally poor. A classifier trained on a given dataset of software projects has so far not worked well on different artifacts or different datasets. In this study, we replicate and synthesize these earlier results in a meta-analysis. We then apply recent work in transfer learning for NLP to the problem of design mining. However, for our datasets, these deep transfer learning classifiers perform no better than less complex classifiers. We conclude by discussing some reasons behind the transfer learning approach to design mining. ",Cross-Dataset Design Discussion Mining
84,1214379087390593024,59038273,Dr. Rosie ʻAnolani Alegado #ShutDownRedHill,"['(1) NEW PAPER: “A Native Hawaiian-led summary of the current impact of constructing the Thirty Meter Telescope on Maunakea” by @sara_kahanamoku, me, @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro &amp; @astrocanuck.  <LINK> (1/n)', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck We talk about the impacts of Maunakea astronomy research on non-astronomy communities, esp. Native Hawaiians. Maunakea is a lightning-rod issue for the Hawaiian community, and should spur scientists to 1) reflect on the human impacts &amp; externalities of their research, and (2/n)', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck 2) learn more about the Maunakea issue. Luckily, there are plenty of papers you can read to get info: ours is one of eight Hawaiian-led papers submitted to the NAS that talk about Maunakea from a variety of perspectives.  https://t.co/HOr5JEm9HF (3/n)', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck We submitted this packet of Hawaiian-led papers to the NAS for the Astro2020 Decadal review, which determines funding priorities for the field of astronomy.', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck TMT is not the only astro project on Indigenous land &amp; we urge scientists of all fields to consider their ethical &amp; moral obligations to Indigenous people when designing research projects. (5/n)', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck Our first recommendation is to immediately STOP TMT progress &amp; restart dialogue dialogue w/ Maunakea protectors to better understand their position &amp; improve the processes of seeking consent. This includes understanding the terms of refusal for TMT and future projects. (6/n)', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck Other recommendations aim to provide a path forward for astronomers and scientists of all fields. Science must be just and ethical, &amp; consider the communities of people that it impacts. (7/n)', '@sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck We need ethical guidelines for research, especially research on Indigenous land and among minoritized communities. See our recs for moving towards #PonoScience (8/n) https://t.co/M9bXqgBJtS', '@GarmireGroup @sara_kahanamoku @akkagawa @kteabam @shaka_lulu @IBJIYONGI @MiaDoesAstro @astrocanuck Thank you for standing in solidarity with us! #ScientistsforMaunakea']",https://arxiv.org/abs/2001.00970,"Maunakea, the proposed site of the Thirty Meter Telescope (TMT), is a lightning-rod topic for Native Hawaiians, Hawaii residents, and the international astronomy community. In this paper we, Native Hawaiian natural scientists and allies, identify historical decisions that impact current circumstances on Maunakea and provide approaches to acknowledging their presence. Our aim is to provide an Indigenous viewpoint centered in Native Hawaiian perspectives on the impacts of the TMT project on the Hawaiian community. We summarize the current Maunakea context from the perspective of the authors who are trained in the natural sciences (inclusive of and beyond astronomy and physics), the majority of whom are Native Hawaiian or Indigenous. We highlight three major themes in the conflict surrounding TMT: 1) physical demonstrations and the use of law enforcement against the protectors of Maunakea; 2) an assessment of the benefit of Maunakea astronomy to Native Hawaiians; and 3) the disconnect between astronomers and Native Hawaiians. We close with general short- and long- term recommendations for the astronomy community, which represent steps that can be taken to re-establish trust and engage in meaningful reciprocity and collaboration with Native Hawaiians and other Indigenous communities. Our recommendations are based on established best principles of free, prior, and informed consent and researcher-community interactions that extend beyond transactional exchanges. We emphasize that development of large-scale astronomical instrumentation must be predicated on consensus from the local Indigenous community about whether development is allowed on their homelands. Proactive steps must be taken to center Indigenous voices in the earliest stages of project design. ","A Native Hawaiian-led summary of the current impact of constructing the
  Thirty Meter Telescope on Maunakea"
85,1214377760413511681,1950210188,Dr. Ashley T. Rubin,"['Congrats to @RubinActual and his @uhmanoa colleagues for their paper (incl. @BenShappee), ""Does Gravity Fall Down?"" It offers a new test of general relativity, which is pretty neat.  <LINK>', '@RubinActual @uhmanoa @BenShappee Sorry, Deep, I left you off! @palebluedot2275']",https://arxiv.org/abs/2001.01710,"We present a novel test of general relativity (GR): measuring the geometric component of the time delay due to gravitational lensing. GR predicts that photons and gravitational waves follow the same geodesic paths and thus experience the same geometric time delay. We show that for typical systems, the time delays are tens of seconds, and thus can dominate over astrophysical delays in the timing of photon emission. For the case of GW 170817, we use a multi-plane lensing code to evaluate the time delay due to four massive halos along the line of sight. From literature mass and distance measurements of these halos, we establish at high confidence (significantly greater than 5 sigma) that the gravitational waves of GW 170817 underwent gravitational deflection to arrive within 1.7 seconds of the photons. ","Does Gravity Fall Down? Evidence for Gravitational Wave Deflection Along
  the Line of Sight to GW 170817"
86,1214211737928552448,1048984881131401217,Cora Dvorkin,['New paper in the new decade: <LINK> @harvardphysics @Harvard @HarvardITC'],https://arxiv.org/abs/2001.00584,"Angular cosmological correlators are infamously difficult to compute due to the highly oscillatory nature of the projection integrals. Motivated by recent development on analytic approaches to cosmological perturbation theory, in this paper we present an efficient method for computing cosmological four-point correlations in angular space, generalizing previous works on lower-point functions. This builds on the FFTLog algorithm that approximates the matter power spectrum as a sum over power-law functions, which makes certain momentum integrals analytically solvable. The computational complexity is drastically reduced for correlators in a ""separable"" form---we define a suitable notion of separability for cosmological trispectra, and derive formulas for angular correlators of different separability classes. As an application of our formalism, we compute the angular galaxy trispectrum at tree level, with and without primordial non-Gaussianity. This includes effects of redshift space distortion and bias parameters up to cubic order. We also compute the non-Gaussian covariance of the angular matter power spectrum due to the connected four-point function, beyond the Limber approximation. We demonstrate that, in contrast to the standard lore, the Limber approximation can fail for the non-Gaussian covariance computation even for large multipoles. ",Cosmological Angular Trispectra and Non-Gaussian Covariance
87,1213995194871361543,14981648,The Disordered Cosmos by Chanda Prescod-Weinstein,"[""To build on @uahikea's tweet: tonight I, along with @niais @astrocanuck @shaka_lulu and @iamstarnord have a new white paper up on the arXiv with recommendations for anticolonial astronomy. We include something specifically about this (click image to read).\n<LINK> <LINK> <LINK>"", ""@uahikea @niais @astrocanuck @shaka_lulu @iamstarnord Since my account is locked, you're all welcome to reproduce the tweet exactly and just add a second tweet saying it's originally a tweet from my account :-)"", '@uahikea @niais @astrocanuck @shaka_lulu @iamstarnord If you look closely at the citations you will find @imaniperry, CLR James and Walter Rodney']",https://arxiv.org/abs/2001.00674,"This white paper explains that professional astronomy has benefited from settler colonial white supremacist patriarchy. We explicate the impact that this has had on communities which are not the beneficiaries of colonialism and white supremacy. We advocate for astronomers to reject these benefits in the future, and we make proposals regarding the steps involved in rejecting colonialist white supremacy's benefits. We center ten recommendations on the timely issue of what to do about the Thirty Meter Telescope (TMT) on Maunakea in Hawaii. This paper is written in solidarity with and support of efforts by Native Hawaiian scientists (e.g. Kahanamoku et al. 2019). ","Reframing astronomical research through an anticolonial lens -- for TMT
  and beyond"
88,1213049206748913664,268409056,Bernardo Furtado,"['New fun Holidays\' paper: ""Talent, Luck, Context and Perspective on Success: a disaggregating simulation using Risk"" <LINK>. Code here: <LINK> <LINK>']",https://arxiv.org/abs/2001.00034,"We propose a controlled simulation within a competitive sum-zero environment as a proxy for disaggregating components of success. Given a simulation of the Risk board game, we consider (a) Talent to be one of three rule-based strategies used by players; (b) Context as the setting of each run of the game with opponents' strategies, goals and luck; and (c) Perspective as the objective of each player. Success is attained when a first player conquers its goal. We simulate 100,000 runs of an agent-based model and analyze the results. The simulation results strongly suggest that luck, talent and context are all relevant to determine success. Perspective -- as the description of the goal that defines success -- is not. As such, we present a quantitative, reproducible environment in which we are able to significantly separate the concepts, reproducing previous results of the literature and adding arguments for context and perspective. Finally, we also find that the simulation offers insights on the relevance of resilience and opportunity. ","Contributions of Talent, Perspective, Context and Luck to Success"
89,1212988781537554432,2416760538,Peter Gao,"['New paper! ""Deflating Super-Puffs: Impact of Photochemical Hazes on the Observed Mass-Radius Relationship of Low Mass Planets."" Just in time for #AAS235! <LINK>\n\nThis was one of the most fun projects I\'ve ever done, as I got to stretch my theorist wings! (Thread)', 'In collaboration with Xi Zhang (@ucsc prof, my former @Caltech office mate and wedding officiant, latest Ron Greeley Award winner), we tackled photochemistry, haze microphysics, atmospheric loss, and the mass-radius relationship of all sub-Neptunes with temperatures &lt; 800 K.', 'The paper is also in response to a recent discovery that several of the puffiest super-puffs have flat transmission spectra, even though we expect their puffy atmospheres to exhibit huge spectral features: \n\nhttps://t.co/BaR0EYZv1I', 'What is the implication of this for their atmosphere, interior, formation, and evolution? What are super-puffs anyway? Are they related to the much more numerous sub-Neptunes (planets with radii between Earth and Neptune), or are they their own beasts?', ""But let's step back and think about what a flat transmission spectrum means. Often it means frustration, as we get no information on the composition of these atmospheres. However, there is a more insidious problem."", ""In a clear atmosphere, transmitted light can probe as deep as the molecular opacity allows it. This includes contributions from various molecules, collisionally induced absorption, and scattering. Given a composition, we should have a pretty good idea of where we're probing https://t.co/TO8FlPZKQv"", ""Once we have an estimate of the pressures probed, then given reasonable interior compositions (rock, ice, gas) and constraints from the mass and radius, we can get pretty good ideas of what we're looking at."", ""But alas, nature doesn't make things easy, because atmospheres are not clear. Physical and chemical processes in planetary atmosphere naturally generate aerosols - cloud and haze particles - that contribute to the total atmospheric opacity."", 'Their distribution is dependent on numerous parameters, including aerosol composition, transport processes, and atmospheric thermal structure. They can form high enough in an atmosphere that no molecular features show up in transmission due to low gas densities above them.', ""And this is the key: Once we get a flat spectrum, we have no idea what pressures we're actually probing besides some upper limit. Therefore, the radius we have inferred could be wildly off from what it would be if the planet were clear. https://t.co/qj6OffiaEl"", ""Now, this is a problem, because while we have probed many planets using transmission spectroscopy and a fair number have shown spectral features (and a lot haven't), the mass majority of planets we have found have only radius measurements at one wavelength: the Kepler band."", 'Kepler has measured the radii of thousands of planets, but most are too far away to get good transmission spectra. It is with Kepler data + mass measurements from multiple hard working groups around the world that we are able to construct mass-radius diagrams of exoplanets.', 'Here is one I made using data from the @NASAExoArchive, specifically for low mass (&lt;10M_Earth), temperate (&lt;800 K) planets with radius &gt;1R_Earth (the black line is the computed mass-radius relationship of Earth-like planets): https://t.co/6GPJT9Cj1a', 'As one can see, while there are quite a few planets clustered below 3-4R_Earth (sub-Neptunes), the Super-Puffs, shown in the blue symbols, are much bigger. \n\nTheir large size and low gravity also present another problem for some of them: Atmospheric loss.', 'One of the major ways low mass, warm planets lose their atmospheres is through photoevaporation: high energy photons impinging on their atmosphere launching an escaping wind. The lower the gravity and higher the atmospheric density at the launch point, the greater the loss. https://t.co/AncBEyCwTg', 'Super-puffs, with their extended atmospheres caused by low gravity due to their large radii + low mass are perfect candidates for rapid atmospheric loss.', 'And so we constructed planet structure models for sub-Neptunes and super puffs and calculated their atmospheric lifetimes. Here is what happens when we put them on top of the observations: https://t.co/KbWhXnwTRv', 'Every multi-colored curve here represents the change in radius as we add more gas onto an Earth-like core and the gray lines are constant gas-fraction lines. The colors of the curves = atmospheric lifetimes. Red = lifetimes &lt; 100 kyr. Some of these super-puffs should not exist!', 'And thus the impetus for our paper: Maybe these planets are not as ""big"" as we have assumed, because they\'re being propped up by high altitude hazes, which in turn generate flat transmission spectra.', 'If this is the case, then the atmospheric density at the escape wind launch point would be much lower, resulting in longer atmospheric lifetimes.', ""But wait, there's more: Since so many sub-Neptunes have only one radius measurement (Kepler), can we trust them? What if they all had hazes and are actually smaller than we have assumed? \n\nAnd so Xi and I set out to assess the impact of haze on all low-mass planets!"", 'We combined our structure models, complete with an escaping wind, with a simple photochemical model and a haze microphysics model. What we found was that there were three haze regimes: https://t.co/mNC3pSHD3V', 'On the left, is the ""boring"" case. The planet is not losing atmosphere very fast; haze is being generated at high altitudes and fall, growing by coagulation as they do so.', 'On the right is the exploding planet case. The escaping wind is so strong that haze has no time to form before they are blown into space.', 'Now, the middle case is interesting: The planet is losing mass, but the escape wind is just fast enough to keep larger particles aloft, but slow enough to allow for particle growth until they can fall out.', 'This results in higher haze opacity at high altitudes, much more so than either of the left and right cases. The best part is that this condition is met very simply: The escape wind speed just has to match the fall speed of ~10-100 nm particles!', 'In other words, we can relate the middle case to planetary parameters. We show in the paper that the haziest planets, and those that experience the most radius enhancement due to haze, cuts across the atmospheric mass-core mass-atmospheric lifetime diagram like so: https://t.co/GqYl0tx9C8', 'Basically, planets that can survive for &lt;1 Gyr is ripe for this. More stable planets have less radius enhancement (but still some, on the order of a few to 10%), while less stable planets are long gone.', ""Putting this in terms of the mass radius diagram, we get the following. Here I've put the clear case in as well so you can compare. https://t.co/AA9P4xKuaK"", 'Now everything* works! The atmospheric lifetime can be explained if there are high haze layers fooling us into thinking that these planets have more gas than they actually do.', '*Ok so not EVERTHING works - as always there are caveats, including uncertainties in the deep interior temperature of the atmospheres, haze optical properties, and haze production pathways. More research needed for those!', 'Another important - and disturbing - point is that, because of these high altitude hazes, we predict that super-puffs and many sub-Neptunes in general should have little-to-no spectral features in transmission in the optical and near-infrared. When I say little, I mean &lt;50 ppm.', ""And that's for the most optimistic prediction. &lt;10 ppm is more typical. However, this doesn't mean there aren't larger features at longer wavelengths, but we'll have to wait for @NASAWebb for that."", 'I should also point out that the small spectral features is more than due to the haze itself, but also photochemistry destroying many familiar (water, methane) molecular absorbers above the haze. After all, the haze forms from photochemistry.', 'A few more things of note:\n\n1. Because of the falling opacity of haze particles with increasing wavelength, mid-infrared observations of super-puffs should prepare for a much smaller planet, maybe by a factor of 2. https://t.co/l08qk6jsZP', ""2. Since the hazes optimal for boosting radius occurs for planets that are relatively young, the youngest planets we have found so far and which are large could actually be super-puffs, especially if they don't have tight mass constraints. Something for @amannastro to look into!"", '3. The haze opacity dominates the upper atmosphere by orders of magnitude, and so haze heating and cooling could be important in controlling the upper atmosphere energetics and even atmospheric loss itself. https://t.co/GpxN1NZ4cE', 'Lastly, and perhaps the most profound: As the planets age, they will shrink due to cooling of their interiors. This, combined with atmospheric loss, should eventually shut down the escape wind and reduce the haze enhancement of the planet radius.', 'The resulting change in radius vs. time could be huge, by nearly an order of magnitude. And, since our computed gas mass fractions of several super-puffs are now relatively low, they could shrink to become sub-Neptunes!', 'At the same time, some sub-Neptunes may have once been massive super-puffs, despite their relatively small size now. The fact that we could connect these two populations in this way blew my mind.', 'Looking ahead, the potential for new theoretical and observational investigations of sub-Neptunes and super-puffs in light of our work is great. In addition to the mid-IR observations of Webb, there could also be new revelations through emitted and reflected light observations', ""Observations of young planets will be key to seeing whether they're all hazy, and whether haze is being carried off to space by escaping winds. \n\nMass constraints are vital."", ""At the same time, it would be good to nail down how these planets actually evolve temporally, and how haze affects atmospheric loss (if at all). \n\nCan't wait to pursue these in the next decade!"", '@Exotides Thanks!!', '@ExoEhsan Nope! We just looked at Sun-like (and early Sun-like) XUV fluxes']",https://arxiv.org/abs/2001.00055,"The observed mass-radius relationship of low-mass planets informs our understanding of their composition and evolution. Recent discoveries of low mass, large radii objects (""super-puffs"") have challenged theories of planet formation and atmospheric loss, as their high inferred gas masses make them vulnerable to runaway accretion and hydrodynamic escape. Here we propose that high altitude photochemical hazes could enhance the observed radii of low-mass planets and explain the nature of super-puffs. We construct model atmospheres in radiative-convective equilibrium and compute rates of atmospheric escape and haze distributions, taking into account haze coagulation, sedimentation, diffusion, and advection by an outflow wind. We develop mass-radius diagrams that include atmospheric lifetimes and haze opacity, which is enhanced by the outflow, such that young (~0.1-1 Gyr), warm (T$_{eq}$ $\geq$ 500 K), low mass objects ($M_c$ < 4M$_{\rm Earth}$) should experience the most apparent radius enhancement due to hazes, reaching factors of three. This reconciles the densities and ages of the most extreme super-puffs. For Kepler-51b, the inclusion of hazes reduces its inferred gas mass fraction to <10%, similar to that of planets on the large radius side of the sub-Neptune radius gap. This suggests that Kepler-51b may be evolving towards that population, and that some warm sub-Neptunes may have evolved from super-puffs. Hazes also render transmission spectra of super-puffs and sub-Neptunes featureless, consistent with recent measurements. Our hypothesis can be tested by future observations of super-puffs' transmission spectra at mid-infrared wavelengths, where we predict that the planet radius will be half of that observed in the near-infrared. ","Deflating Super-Puffs: Impact of Photochemical Hazes on the Observed
  Mass-Radius Relationship of Low Mass Planets"
90,1224584985274912768,2438091938,Rasmus Pagh ☮ 🇺🇦,"['Want to estimate the number of people outside the intersection of two sets, known by different parties? With differential privacy? New joint paper with my great student Nina Mesing Stausholm on ""sketchy privacy"".\n<LINK>']",https://arxiv.org/abs/2001.11932,"A powerful feature of linear sketches is that from sketches of two data vectors, one can compute the sketch of the difference between the vectors. This allows us to answer fine-grained questions about the difference between two data sets. In this work, we consider how to construct sketches for weighted $F_0$, i.e., the summed weights of the elements in the data set, that are small, differentially private, and computationally efficient. Let a weight vector $w\in(0,1]^u$ be given. For $x\in\{0,1\}^u$ we are interested in estimating $\Vert x\circ w\Vert_1$ where $\circ$ is the Hadamard product (entrywise product). Building on a technique of Kushilevitz et al.~(STOC 1998), we introduce a sketch (depending on $w$) that is linear over GF(2), mapping a vector $x\in \{0,1\}^u$ to $Hx\in\{0,1\}^\tau$ for a matrix $H$ sampled from a suitable distribution $\mathcal{H}$. Differential privacy is achieved by using randomized response, flipping each bit of $Hx$ with probability $p<1/2$. We show that for every choice of $0<\beta < 1$ and $\varepsilon=O(1)$ there exists $p<1/2$ and a distribution $\mathcal{H}$ of linear sketches of size $\tau = O(\log^2(u)\varepsilon^{-2}\beta^{-2})$ such that: 1) For random $H\sim\mathcal{H}$ and noise vector $\varphi$, given $Hx + \varphi$ we can compute an estimate of $\Vert x\circ w\Vert_1$ that is accurate within a factor $1\pm\beta$, plus additive error $O(\log(u)\varepsilon^{-2}\beta^{-2})$, with probability $1-1/u$, and 2) For every $H\sim\mathcal{H}$, $Hx + \varphi$ is $\varepsilon$-differentially private over the randomness in $\varphi$. The special case $w=(1,\dots,1)$ is unweighted $F_0$. Our results both improve the efficiency of existing methods for unweighted $F_0$ estimating and extend to a weighted generalization. We also give a distributed streaming implementation for estimating the size of the union between two input streams. ",Efficient Differentially Private $F_0$ Linear Sketching
91,1222092944989138944,4809182422,James S. Jenkins,"[""Go check out our paper led by @astrodillo on a new 'hot rock' discovery, a terrestrial planet orbiting a nearby small M dwarf star. A lot of future characterization potential here!!  <LINK> <LINK>""]",https://arxiv.org/abs/2001.09175,"We report the detection of a transiting super-Earth-sized planet (R=1.39+-0.09 Rearth) in a 1.4-day orbit around L 168-9 (TOI-134),a bright M1V dwarf (V=11, K=7.1) located at 25.15+-0.02 pc. The host star was observed in the first sector of the Transiting Exoplanet Survey Satellite (TESS) mission and, for confirmation and planet mass measurement, was followed up with ground-based photometry, seeing-limited and high-resolution imaging, and precise radial velocity (PRV) observations using the HARPS and PFS spectrographs. Combining the TESS data and PRV observations, we find the mass of L168-9 b to be 4.60+-0.56 Mearth, and thus the bulk density to be 1.74+0.44-0.33 times larger than that of the Earth. The orbital eccentricity is smaller than 0.21 (95% confidence). This planet is a Level One Candidate for the TESS Mission's scientific objective - to measure the masses of 50 small planets - and is one of the most observationally accessible terrestrial planets for future atmospheric characterization. ","A hot terrestrial planet orbiting the bright M dwarf L 168-9 unveiled by
  TESS"
92,1222017379925925889,330151669,Amy X Zhang,"['📢New paper at #CSCW2020 with Michael Muller and Dakuo Wang on ""How do Data Science Workers Collaborate? Roles, Workflows, and Tools"". We conduct a survey of industry data science/ML workers (technical and non-technical) to understand how they collaborate. <LINK> <LINK>', 'We examine how different roles collaborate with each other, and at different stages in the data science workflow. We also consider tool usage over the course of a project, particularly as it relates to code/data documentation and re-use practices. https://t.co/ZZzpMdUBgs', 'We find:\n- extensive collaboration across all stages of a data science workflow\n- but fewer collaborations with non-technical workers during important middle stages, such as feature engineering\n- bias detection and mitigation primarily a technical matter https://t.co/ZZzpMdUBgs', '- lower usage of documentation &amp; discussion tools during feature engineering and bias mitigation stages\n- overall low rates of data documentation\n- differences in documentation + code reading/re-use practices based on tooling (GitHub vs Jupyter vs scripts) https://t.co/ZZzpMdUBgs', 'We conclude with ideas on how to design data science tools with collaboration in mind, including easier ways to record code and data provenance, improving transparency of decision-making, and overall involving more non-technical and indirect stakeholders. https://t.co/ZZzpMdUBgs']",https://arxiv.org/abs/2001.06684,"Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions. ","How do Data Science Workers Collaborate? Roles, Workflows, and Tools"
93,1221747249245491202,3313806489,Tim Roberts,"[""New paper.  Presented without frivolous comment.\n\n<LINK>\n\nOh, what the hell.  Here's Ruby, who we are dog sitting at the moment. <LINK>""]",https://arxiv.org/abs/2001.08752,"Ultraluminous X-ray sources (ULXs) are a class of accreting compact objects with X-ray luminosities above 1e39 erg/s. The ULX population counts several hundreds objects but only a minor fraction is well studied. Here we present a detailed analysis of all ULXs hosted in the galaxy NGC 7456. It was observed in X-rays only once in the past (in 2005) by XMM-Newton, but the observation was short and strongly affected by high background. In 2018, we obtained a new, deeper (~90 ks) XMM-Newton observation that allowed us to perform a detailed characterization of the ULXs hosted in the galaxy. ULX-1 and ULX-2, the two brightest objects (Lx~(6-10)e39 erg/s), have spectra that can be described by a two-thermal component model as often found in ULXs. ULX-1 shows also one order of magnitude in flux variability on short-term timescales (hundreds to thousand ks). The other sources (ULX-3 and ULX-4) show flux changes of at least an order of magnitude, and these objects may be candidate transient ULXs although longer X-ray monitoring or further studies are required to ascribe them to the ULX population. In addition, we found a previously undetected source that might be a new candidate ULX (labelled as ULX-5) with a luminosity of ~1e39 erg/s and hard power-law spectral shape, whose nature is still unclear and for which a background Active Galactic Nucleus cannot be excluded. We discuss the properties of all the ULXs in NGC 7456 within the framework of super-Eddington accretion onto stellar mass compact objects. Although no pulsations were detected, we cannot exclude that the sources host neutron stars. ",The Ultraluminous X-ray sources population of the galaxy NGC 7456
94,1220446554613604364,824418390005731329,Heng Yang,"['Super excited to share new work “TEASER: Fast and Certifiable Point Cloud Registration” with Jingnan Shi and @lucacarlone1 \n\nPaper: <LINK>\nCode: <LINK>\n\nTEASER is the first algorithm of its kind in many practical and theoretical aspects: <LINK> <LINK>', 'Practice:\n1. Robustness: robust to &gt;99% outliers \n2. Safety: Certifiably correct in the sense of global optimality \n3. Efficiency: milliseconds\n4. Correspondence-free: first algorithm to solve correspond.-free registration using all-to-all correspond. due to extreme robustness', 'Theory:\n1. Relaxation: first tight semidefinite relaxation in the presence of outliers (binary variables). Real data is not our enemy!\n2. Certification: first fast optimality certification using alternating projections to convex sets, 200 times faster than solving a large SDP', '3. Invariant measurements: first formal decoupling of scale, rotation and translation estimation\n4. Performance guarantees: first formal bounds with respect to ground truth, under mild “estimation contract” We hope this paper could illustrate the idea of certifiable algorithms!']",https://arxiv.org/abs/2001.07715,"We propose the first fast and certifiable algorithm for the registration of two sets of 3D points in the presence of large amounts of outlier correspondences. We first reformulate the registration problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem is still non-convex and combinatorial in nature, we show that (i) TLS scale and (component-wise) translation estimation can be solved in polynomial time via adaptive voting, (ii) TLS rotation estimation can be relaxed to a semidefinite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates, and (iii) the graph-theoretic framework allows drastic pruning of outliers by finding the maximum clique. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While solving large SDP relaxations is typically slow, we develop a second fast and certifiable algorithm, named TEASER++, that uses graduated non-convexity to solve the rotation subproblem and leverages Douglas-Rachford Splitting to efficiently certify global optimality. For both algorithms, we provide theoretical bounds on the estimation errors, which are the first of their kind for robust registration problems. Moreover, we test their performance on standard, object detection, and the 3DMatch benchmarks, and show that (i) both algorithms dominate the state of the art and are robust to more than 99% outliers, (ii) TEASER++ can run in milliseconds, and (iii) TEASER++ is so robust it can also solve problems without correspondences, where it largely outperforms ICP and it is more accurate than Go-ICP while being orders of magnitude faster. ",TEASER: Fast and Certifiable Point Cloud Registration
95,1220380764879179777,3323459854,Ronnie Clark,"[""If you haven't already, check out our new work where we use #DeepFactors in a full factor graph formulation for solving dense monocular SLAM! w/ @czarnowskij , Tristan Laidlow, @AjdDavison. Paper: <LINK>. Appearing in RAL and @icra2020. <LINK>""]",https://arxiv.org/abs/2001.05049,"The ability to estimate rich geometry and camera motion from monocular imagery is fundamental to future interactive robotics and augmented reality applications. Different approaches have been proposed that vary in scene geometry representation (sparse landmarks, dense maps), the consistency metric used for optimising the multi-view problem, and the use of learned priors. We present a SLAM system that unifies these methods in a probabilistic framework while still maintaining real-time performance. This is achieved through the use of a learned compact depth map representation and reformulating three different types of errors: photometric, reprojection and geometric, which we make use of within standard factor graph software. We evaluate our system on trajectory estimation and depth reconstruction on real-world sequences and present various examples of estimated dense geometry. ",DeepFactors: Real-Time Probabilistic Dense Monocular SLAM
96,1217485792441962496,884827261,Daniel Tamayo,"['New paper on how chaos fundamentally limits how accurately we can know how long a given planetary system will take to go unstable: <LINK>. Led by Naireen Hussain, an amazing undergraduate at the University of Toronto. Thread:', ""We typically take a given planetary configuration, run an N-body integration and quote how long it took to go unstable. But these systems are chaotic, so if you offset by machine precision and ran again, you'd get a different answer. How much can we trust a single value? 🤔"", 'In violently unstable systems this can depend a lot on the initial configuration: cool vis. of diff initial conditions for black hole triples by Johan Samsing. Diff colors are collision, ejection etc. There are fractal regions where diff outcomes are almost on top of one another! https://t.co/AzkWxhnhEa', ""But for exoplanets, we're typically interested in Kepler compact systems with several small planets. Here you don't need 1, but many kicks between planets to reach instability, so you might expect things to settle out into nice statistical distributions that apply more widely."", ""We've been working a lot on making reliable long-term stability predictions on such systems, and have a big dataset of N-body integrations. Here we took a couple hundred initial conditions, and for each one, ran a cloud of realizations initially offset by machine precision."", ""If we plot histograms of all the different realizations' instability times, we find that they do reach well defined, lognormal distributions. The width of these distributions are the error bars we should be quoting when we use N-body to measure a configuration's instability time. https://t.co/reqEmm77eP"", 'Perhaps most interestingly, we find that diff histograms like the above, corresponding to diff initial conditions, have very similar lognormal widths, peaked at 0.4 dex. This is surprising given that the centers (means) vary from 10^4 - 10^7 orbits for different systems.', ""It's also very convenient. It means that when you measure an instability time t with N-body, then the 'error bar' on that value is about a factor of 3 in either direction, for a very wide range of initial conditions. This is a useful number when stating results!"", ""The paper spends time testing this on other results, like those of @AstroAlysa and TRAPPIST-1, and then trying to understand outliers. Exceptions are systems that go unstable quickly &amp; don't have enough chaotic timescales to settle to lognormals, but these are cheap to integrate!"", ""We'd love to understand what gives rise to these universal distributions, and have some ideas with @AstroAlysa !"", ""@yalinewich I agree that's interesting! Rice et al. (2018) did a nice similar analysis &amp; looked at some of that. But their focus was after the 1st close encounter, which is where we stop our integrations. Nice thing is that all our simulation archives are online, so you can check all params!"", '@semaphore_P machine precision can be a problem...if you used floats &amp; your integrator had biased roundoff errors, you would get garbage results over 10^9 orbits. @hannorein and I have spent significant time ensuring REBOUND errors are unbiased! But below some thresh you should just see chaos', '@semaphore_P @hannorein which is the regime we study in the paper!']",https://arxiv.org/abs/2001.04606,"Instabilities in compact planetary systems are generically driven by chaotic dynamics. This implies that an instability time measured through direct N-body integration is not exact, but rather represents a single draw from a distribution of equally valid chaotic trajectories. In order to characterize the ""errors"" on reported instability times from direct N-body integrations, we investigate the shape and parameters of the instability time distributions (ITDs) for ensembles of shadow trajectories that are initially perturbed from one another near machine precision. We find that in the limit where instability times are long compared to the Lyapunov (chaotic) timescale, ITDs approach remarkably similar lognormal distributions with standard deviations ~0.43 $\pm$ 0.16 dex, despite the instability times varying across our sample from $10^4-10^8$ orbits. We find excellent agreement between these predictions, derived from ~450 closely packed configurations of three planets, and a much wider validation set of ~10,000 integrations, as well as on ~20,000 previously published integrations of tightly packed five-planet systems, and a seven-planet resonant chain based on TRAPPIST-1, despite their instability timescales extending beyond our analyzed timescale. We also test the boundary of applicability of our results on dynamically excited versions of our Solar System. These distributions define the fundamental limit imposed by chaos on the predictability of instability times in such planetary systems. It provides a quantitative estimate of the intrinsic error on an N-body instability time imprinted by chaos, approximately a factor of 3 in either direction. ","Fundamental limits from chaos on instability time predictions in compact
  planetary systems"
97,1217119136754606080,2420562302,Ulrik Lyngs,"['New #CHI2020 paper (@klukoff @ozzulak W Seymour @EthicsWildfire @Marenka @junszhao @emax @Nigel_Shadbolt) on how design interventions change behaviour and perceived control on Facebook. tl;dr: Large effects of goal reminders and removing the newsfeed <LINK> \n1/13 <LINK>', ""Why did we do this?  \n\nMuch research, especially among students, has focused on 'Problematic Facebook Use', i.e. use that is associated with e.g. lower academic achievement, or anxiety and depression --- and difficulties with self-control seems to be a main cause \n2/13 https://t.co/cXTID9xjob"", 'Meanwhile, many tools on browser extension stores cater to users who struggle with self-control on Facebook.  \nThese offer interventions such as removing the newsfeed, automatically logging oneself out after a time limit is hit, providing goal reminders, etc. \n3/13 https://t.co/Exp47DXFbn', ""Do these interventions actually work, and could they inform what it means to 'design for digital wellbeing' on social media?  \nWe don't know - there are no studies. So we did a study of popular browser extensions for self-control on Facebook on the Chrome Web Store. \n\n4/13"", ""We assigned university students (n=58) to one of three conditions:  \n(i) goal reminders (typing in one's goal when visiting the site, then being reminded every few mins),  \n(ii) removing the newsfeed,  \n(iii) control condition (turning the background white) \n\n5/13 https://t.co/EFkAyA3h70"", 'To tackle measurement challenges related to self-report, we triangulated three types of evidence:  \nwe (i) logged actual behaviour,  \n(ii) administered fortnightly surveys,  \n(iii) conducted semi-structured interviews by the end of the study \n\n6/13 https://t.co/RqWoIwXwag', ""The qualitative data showed that both interventions helped participants stay on task and avoid unintended use:  \n- goal reminders helped people 'snap out' of unintended behaviour \n- removing the newsfeed stopped unintended behaviours from being triggered in the first place \n\n7/13"", 'The downsides:  \n- goal reminders were often experienced as annoying \n- some participants were afraid of missing out on information when their newsfeed was removed \n\n8/13', ""The logged behaviour supported participants' reported experience:  \n- removing the newsfeed led to shorter visits (r = 0.75) \n- goal reminders led to less daily time spent (r = 0.63), fewer visits (r = 0.63), and a trend towards shorter visits (d = 0.51). \n\n9/13 https://t.co/lypleTW1XM"", 'In the interviews, almost all participants said they felt conflicted about Facebook:  \n- it was an ongoing source of distraction and self-control struggles \n- but it was too useful to avoid \n\n10/13', '- they readily suggested design solutions, from controls for filtering the newsfeed, to blocking games or videos\n- but they *did not trust Facebook to provide any useful solutions*, because this was seen as going against their business interests. \n\n11/13', 'Our paper is exploratory and should be followed by confirmatory studies --- you can find materials, data, analysis scripts, and the R Markdown source file for the paper at https://t.co/5SGsFg90sM \n\n12/13 https://t.co/RjPyEr3Lyd', ""We hope our paper can help the discussion around \n(i) what 'designing for digital wellbeing' might mean in practice,  \n(ii) what methods we can use to produce more reliable and meaningful research on 'screentime' and wellbeing (cf. @OrbenAmy's great work)\n\n13/13""]",https://arxiv.org/abs/2001.04180,"Beyond being the world's largest social network, Facebook is for many also one of its greatest sources of digital distraction. For students, problematic use has been associated with negative effects on academic achievement and general wellbeing. To understand what strategies could help users regain control, we investigated how simple interventions to the Facebook UI affect behaviour and perceived control. We assigned 58 university students to one of three interventions: goal reminders, removed newsfeed, or white background (control). We logged use for 6 weeks, applied interventions in the middle weeks, and administered fortnightly surveys. Both goal reminders and removed newsfeed helped participants stay on task and avoid distraction. However, goal reminders were often annoying, and removing the newsfeed made some fear missing out on information. Our findings point to future interventions such as controls for adjusting types and amount of available information, and flexible blocking which matches individual definitions of 'distraction'. ","'I Just Want to Hack Myself to Not Get Distracted': Evaluating Design
  Interventions for Self-Control on Facebook"
98,1217107716746801152,1070545175942959105,Tom McCoy,"['New paper: ""Does syntax need to grow on trees? Sources of hierarchical inductive bias in sequence-to-sequence networks"" w/ @Bob_Frank &amp; @TalLinzen to appear in TACL\n\nPaper <LINK>\nWebsite <LINK>\n\nInterested in syntactic generalization? Read on! 1/ <LINK>', '@bob_frank @tallinzen For 2 syntactic tasks, we train models on training sets that are ambiguous between two rules: one rule based on hierarchical structure and one based on linear order. \n\n2/12', '@bob_frank @tallinzen We then test the models on examples where the rules make different predictions to see whether they are biased toward linear or hierarchical rules. We do this for 100 re-runs of each model type to control for variability.\n\n3/12', '@bob_frank @tallinzen All the models that we train achieve similar (and very high) scores on the in-distribution test set; that is, they have mastered the types of examples that they have seen. \n\nBut performance varies dramatically on generalizing to novel types of examples.\n\n4/12', '@bob_frank @tallinzen We first investigate standard, sequential recurrent neural networks that vary in their recurrent unit and type of attention. On one task (question formation), both factors can dramatically affect generalization performance:\n\n5/12 https://t.co/iOnmFRlSCR', '@bob_frank @tallinzen However, on a second task (tense reinflection), all types of sequential models generalized poorly. This suggests that none of these models have a hierarchical bias, because if they did we would expect such a bias to manifest across tasks.\n\n6/12 https://t.co/mHLEg7DwJz', '@bob_frank @tallinzen By contrast, tree-structured RNNs generalize very well on both tasks, suggesting that they do have a hierarchical bias:\n\n7/12 https://t.co/xCZ3amFMPz', '@bob_frank @tallinzen Maybe the tree-based models are simply incapable of learning a linear rule? \n\nNo; when we train these models on the linear rule, they learn it with high accuracy. Similarly, sequential RNNs can easily learn the hierarchical rule when given evidence for it.\n\n8/12', '@bob_frank @tallinzen Maybe the tree models only perform better because they have been given more information (i.e., they have the parse trees)? \n\nNo; when we give a sequential GRU the correct parse trees (in the form of brackets in the input and output sequences), it still generalizes poorly.\n\n9/12', '@bob_frank @tallinzen Main takeaways:\n-All aspects of a model can qualitatively affect its syntactic generalization\n-However, a hierarchical bias-which has been argued to underlie children’s acquisition of syntax-only arose in models whose inputs and computations were governed by tree structure\n\n10/12', '@bob_frank @tallinzen Other results in the paper show:\n-Qualitative differences between GRUs and LSTMs\n-Qualitative effects of hidden size and learning rate\n-Findings that the decoder must be tree-structured, but the encoder does not need to be\n-Effects of multitask learning\n\n11/12', '@bob_frank @tallinzen For all these results, with takeaways for both linguists and NLP researchers, see  https://t.co/mHrIDmWc0N and https://t.co/JUyYjCXRFd\n\n12/12']",https://arxiv.org/abs/2001.03632,"Learners that are exposed to the same training data might generalize differently due to differing inductive biases. In neural network models, inductive biases could in theory arise from any aspect of the model architecture. We investigate which architectural factors affect the generalization behavior of neural sequence-to-sequence models trained on two syntactic tasks, English question formation and English tense reinflection. For both tasks, the training set is consistent with a generalization based on hierarchical structure and a generalization based on linear order. All architectural factors that we investigated qualitatively affected how models generalized, including factors with no clear connection to hierarchical structure. For example, LSTMs and GRUs displayed qualitatively different inductive biases. However, the only factor that consistently contributed a hierarchical bias across tasks was the use of a tree-structured model rather than a model with sequential recurrence, suggesting that human-like syntactic generalization requires architectural syntactic structure. ","Does syntax need to grow on trees? Sources of hierarchical inductive
  bias in sequence-to-sequence networks"
99,1216313905452064769,831805863035863040,Midas Nouwens,"['New paper (for #CHI2020) on dark patterns in consent pop-ups after the GDPR. We (w/ I Liccardi @mikarv @karger L Kagal) scraped 10K UK sites and found the biggest pop-up providers are configured illegally 88.2% of the time, with big impacts on answers\n<LINK>\n\n1/9 <LINK>', '5 pop-up (consent management platform) firms have ~58% of the UK market (@QuantCast\n@TrustArc @Crownpeak @CookiebotCybot @OneTrust).\n \nOnly 11.8%(!) meet even the most basic(!) legal UX reqs for valid consent: no pre-checked boxes, explicit consent, reject as easy as accept.\n\n2/9 https://t.co/UL5m5TBmKw', 'This is before you even look at what actually happens (or doesn’t happen) after you click accept/reject, which @CelestinMatte @nataliabielova @Cristianapt have shown can be very little: https://t.co/J6PYYPdymX\n \n3/9', ""To see how designs affect acceptance rates, we injected fake pop-ups into participants' sites (n=40), comparing:\n- banners vs. barriers;\n- presence vs. absence of reject button on 1st page;\n- 0 vs 1 clicks required to access vendor/purpose controls\n \n4/9"", 'We found that removing the decline button from the 1st page increased consent by ~23%pts. In fact, anything on the 2nd page might as well not exist: 93.1% of interactions were limited to the 1st page.\n \n5/9 https://t.co/3XA6uUWzVY', 'Showing more options on 1st page than just Accept or Decline always decreased consent: showing vendors by 20%pts; purposes by 8.8%pts; and both by 11.9%pts.\n \nThis follows what @ChristineUtz @mrtn3000 @sascha_fahl @floschaub @thorstenholz\nfound: https://t.co/WGZNWMeuJn\n \n6/9 https://t.co/6KE7QAFAk9', 'Enforcement is lacking and DPAs lack budget/staff (https://t.co/ZDBnPlfw1R) — and perhaps don’t know where to start. Regulating 3rd party consent services to not allow misconfiguration could be a much more effective way to increase compliance than targeting individual sites.\n7/9', ""If you don't want to wait for enforcement, we also built a browser extension that automatically answers consent pop-ups for you based on your privacy preferences. https://t.co/gw7Vdd9Lk5\n \n8/9"", 'If scientific papers are not your thing, @riptari has written up our findings in a rigorous and thorough way here: https://t.co/l9pMvysSqp\n \n9/9', ""@PrivacyMatters I know... Here's a cached version though! https://t.co/6QeMpv9Og2"", '@el_keogh Generally, yes! Websites can of course build their own, but most use a third party ""consent managment platform"". These often come with a back-end advertising service, where they facilitate real-time ad bidding etc', ""@drt3ch @mikarv @karger @PrivacyPrivee @Carleton_U Amazing, thanks! I'd also be happy to make the scraper/experiment/R code available to them if they want!"", '@andreapernici @mikarv @karger Reminds me of @Quantcast press release saying ""more than 10,000 domains worldwide have deployed Quantcast Choice, generating an average consent rate among consumers of more than 90 percent"". Begs the question: legally valid consent? https://t.co/VVsbv2XhTU', ""@BCowanHCI Thanks! Couldn't have done it without your help! https://t.co/Qpql5zGS4F""]",https://arxiv.org/abs/2001.02479,"New consent management platforms (CMPs) have been introduced to the web to conform with the EU's General Data Protection Regulation, particularly its requirements for consent when companies collect and process users' personal data. This work analyses how the most prevalent CMP designs affect people's consent choices. We scraped the designs of the five most popular CMPs on the top 10,000 websites in the UK (n=680). We found that dark patterns and implied consent are ubiquitous; only 11.8% meet the minimal requirements that we set based on European law. Second, we conducted a field experiment with 40 participants to investigate how the eight most common designs affect consent choices. We found that notification style (banner or barrier) has no effect; removing the opt-out button from the first page increases consent by 22--23 percentage points; and providing more granular controls on the first page decreases consent by 8--20 percentage points. This study provides an empirical basis for the necessary regulatory action to enforce the GDPR, in particular the possibility of focusing on the centralised, third-party CMP services as an effective way to increase compliance. ","Dark Patterns after the GDPR: Scraping Consent Pop-ups and Demonstrating
  their Influence"
100,1215661685391814657,737089971807485952,Serena Booth,"[""It's still WIP, but I am quite fond of our new paper: <LINK>. We play around with probabilistic programming to find in-distribution adversarial examples. Joint work with Yilun, @ankitjs, @julie_a_shah from @MITRobotics. Come see us Feb 7 at AAAI StarAI workshop!""]",https://arxiv.org/abs/2001.03076,"Though neural network models demonstrate impressive performance, we do not understand exactly how these black-box models make individual predictions. This drawback has led to substantial research devoted to understand these models in areas such as robustness, interpretability, and generalization ability. In this paper, we consider the problem of exploring the prediction level sets of a classifier using probabilistic programming. We define a prediction level set to be the set of examples for which the predictor has the same specified prediction confidence with respect to some arbitrary data distribution. Notably, our sampling-based method does not require the classifier to be differentiable, making it compatible with arbitrary classifiers. As a specific instantiation, if we take the classifier to be a neural network and the data distribution to be that of the training data, we can obtain examples that will result in specified predictions by the neural network. We demonstrate this technique with experiments on a synthetic dataset and MNIST. Such level sets in classification may facilitate human understanding of classification behaviors. ","Sampling Prediction-Matching Examples in Neural Networks: A
  Probabilistic Programming Approach"
101,1213227972431245317,29931309,Gillian Hadfield,['Our new paper (with @jackclarkSF) setting out a proposal for regulatory markets as a more agile approach to AI (and other global/complex technology) regulation now up on arxiv.  <LINK>'],https://arxiv.org/abs/2001.00078,We propose a new model for regulation to achieve AI safety: global regulatory markets. We first sketch the model in general terms and provide an overview of the costs and benefits of this approach. We then demonstrate how the model might work in practice: responding to the risk of adversarial attacks on AI models employed in commercial drones. ,Regulatory Markets for AI Safety
102,1224298279418187776,1173845537612881920,Francesc Lluis,"['We propose a deep-learning method for sound field reconstruction that offers advantages in three directions: works with low number of mics, accommodates irregular mics distributions, and has efficient inference\n\nPaper:  <LINK>\nData &amp; Code: <LINK> <LINK>', '@neokaplanis In the end, yes!:) 🔊🎙️']",http://arxiv.org/abs/2001.11263,"In this paper, a deep-learning-based method for sound field reconstruction is proposed. It is shown the possibility to reconstruct the magnitude of the sound pressure in the frequency band 30-300 Hz for an entire room by using a very low number of irregularly distributed microphones arbitrarily arranged. Moreover, the approach is agnostic to the location of the measurements in the Euclidean space. In particular, the presented approach uses a limited number of arbitrary discrete measurements of the magnitude of the sound field pressure in order to extrapolate this field to a higher-resolution grid of discrete points in space with a low computational complexity. The method is based on a U-net-like neural network with partial convolutions trained solely on simulated data, which itself is constructed from numerical simulations of Green's function across thousands of common rectangular rooms. Although extensible to three dimensions and different room shapes, the method focuses on reconstructing a two-dimensional plane of a rectangular room from measurements of the three-dimensional sound field. Experiments using simulated data together with an experimental validation in a real listening room are shown. The results suggest a performance which may exceed conventional reconstruction techniques for a low number of microphones and computational requirements. ",Sound field reconstruction in rooms: inpainting meets super-resolution
103,1224132348708413441,6502132,Sheryl (Shane) Hermoso ⚡️,"['A few months ago, our team did a small project on “Cyber Attack Detection and Machine Learning”. The results are quite interesting, so we published it online. If you like reading academic papers, you may find it here: <LINK>. #netsec #netflow #machinelearning', '@rom great idea sir, i can try to set it up. had less than 8 wks to crunch data on this paper.', '@rom future project. this works only on labelled dataset for now. &gt;:)']",https://arxiv.org/abs/2001.06309,"Cybersecurity attacks are growing both in frequency and sophistication over the years. This increasing sophistication and complexity call for more advancement and continuous innovation in defensive strategies. Traditional methods of intrusion detection and deep packet inspection, while still largely used and recommended, are no longer sufficient to meet the demands of growing security threats. As computing power increases and cost drops, Machine Learning is seen as an alternative method or an additional mechanism to defend against malwares, botnets, and other attacks. This paper explores Machine Learning as a viable solution by examining its capabilities to classify malicious traffic in a network. First, a strong data analysis is performed resulting in 22 extracted features from the initial Netflow datasets. All these features are then compared with one another through a feature selection process. Then, our approach analyzes five different machine learning algorithms against NetFlow dataset containing common botnets. The Random Forest Classifier succeeds in detecting more than 95% of the botnets in 8 out of 13 scenarios and more than 55% in the most difficult datasets. Finally, insight is given to improve and generalize the results, especially through a bootstrapping technique. ",Cyber Attack Detection thanks to Machine Learning Algorithms
104,1222661233377075200,2875482557,Dr Laura McKemmish,"[""Now in a single gif: our group's newest paper, led by Anna-Maree Syme. How we can study fundamental physics using #compchem! Full paper on arxiv here <LINK> with pretty version journal link here <LINK> <LINK>""]",https://arxiv.org/abs/2001.06121,"Astrophysical molecular spectroscopy is an important method of searching for new physics through probing the variation of the proton-to-electron mass ratio, $\mu$, with existing constraints limiting variation to a fractional change of less than 10$^{-17}$/year. To improve on this constraint and therefore provide better guidance to theories of new physics, new molecular probes will be useful. These probes must have spectral transitions that are observable astrophysically and have different sensitivities to variation in the proton-to-electron mass ratio. Here, we concisely detail how astrophysical observations constrain the set of potential molecular probes and promising sensitive transitions based on how the frequency and intensity of these transitions align with available telescopes and observational constraints. Our detailed investigation focuses on rovibronic transitions in astrophysical diatomic molecules, using the spectroscopic models of 11 diatomics to identify sensitive transitions and probe how they generally arise in real complex molecules with many electronic states and fine structure. While none of the 11 diatomics investigated have sensitive transitions likely to be astrophysically observable, we have found that at high temperatures (1000 K) five of these diatomics have a significant number of low intensity sensitive transitions arising from an accidental near-degeneracy between vibrational levels in the ground and excited electronic state. This insight enables screening of all astrophysical diatomics as potential probes of proton-to-electron mass variation, with CN, CP, SiN and SiC being the most promising candidates for further investigation for sensitivity in rovibronic transitions. ","Diatomic rovibronic transitions as potential probes for
  proton-to-electron mass ratio across cosmological time"
105,1222576254865625088,1610691422,Patrick Schwab,"['Using smartphone data from the Floodlight Open study (n=774), we trained deep learning models that identify digital biomarkers for multiple sclerosis, and showed that these digital biomarkers contain significant  signal (AUC=0.88) for diagnosing MS\n\nLink: <LINK> <LINK>']",https://arxiv.org/abs/2001.09748,"Multiple sclerosis (MS) affects the central nervous system with a wide range of symptoms. MS can, for example, cause pain, changes in mood and fatigue, and may impair a person's movement, speech and visual functions. Diagnosis of MS typically involves a combination of complex clinical assessments and tests to rule out other diseases with similar symptoms. New technologies, such as smartphone monitoring in free-living conditions, could potentially aid in objectively assessing the symptoms of MS by quantifying symptom presence and intensity over long periods of time. Here, we present a deep-learning approach to diagnosing MS from smartphone-derived digital biomarkers that uses a novel combination of a multilayer perceptron with neural soft attention to improve learning of patterns in long-term smartphone monitoring data. Using data from a cohort of 774 participants, we demonstrate that our deep-learning models are able to distinguish between people with and without MS with an area under the receiver operating characteristic curve of 0.88 (95% CI: 0.70, 0.88). Our experimental results indicate that digital biomarkers derived from smartphone data could in the future be used as additional diagnostic criteria for MS. ","A Deep Learning Approach to Diagnosing Multiple Sclerosis from
  Smartphone Data"
106,1222548229696499714,717162062837719040,Phil Armitage,"['New paper! In work led by Dan Gole, we find that modest levels of protoplanetary disk turbulence stop solids from collapsing into planetesimals. We think turbulence is strong close to the star, so this may hinder in situ formation of close-in exoplanets.\n\n<LINK>', 'Main difference compared to most prior work: we use small domains and force fluid turbulence with specified properties. This allows 10-100x better spatial resolution, at the expense of capturing large-scale structure in the turbulence (which may well be important).', 'Big picture: I still think the streaming instability is the best candidate planetesimal formation mechanism. But both analytic and simulation evidence now suggests that while the streaming instability is unavoidable, reaching conditions for gravitational collapse is non-trivial.', ""Planetesimal formation may need a chain of processes to concentrate solid material: zonal flows, vortices, the streaming instability... If so,  the efficiency goes down (may be OK observationally), and there may be places where planetesimals can't form at all.""]",https://arxiv.org/abs/2001.10000,"We study how the interaction between the streaming instability and intrinsic gas-phase turbulence affects planetesimal formation via gravitational collapse in protoplanetary disks. Turbulence impedes the formation of particle clumps by acting as an effective turbulent diffusivity, but it can also promote planetesimal formation by concentrating solids, for example in zonal flows. We quantify the effect of turbulent diffusivity using numerical simulations of the streaming instability in small local domains, forced with velocity perturbations that establish approximately Kolmogorov-like turbulence. We find that planetesimal formation is suppressed by turbulence once velocity fluctuations exceed $\langle \delta v^2 \rangle \simeq 10^{-3.5} - 10^{-3} c_s^2$. Turbulence whose strength is just below the threshold reduces the rate at which solids are bound into clumps. Our results suggest that the well-established turbulent thickening of the mid-plane solid layer is the primary mechanism by which turbulence influences planetesimal formation and that planetesimal formation requires a mid-plane solid-to-gas ratio $\epsilon \gtrsim 0.5$. We also quantify the initial planetesimal mass function using a new clump-tracking method to determine each planetesimal mass shortly after collapse. For models in which planetesimals form, we show that the mass function is well-described by a broken power law, whose parameters are robust to the inclusion and strength of imposed turbulence. Turbulence in protoplanetary disks is likely to substantially exceed the threshold for planetesimal formation at radii where temperatures $T \gtrsim 10^3 \ {\rm K}$ lead to thermal ionization. Planetesimal formation may therefore be unviable in the inner disk out to 2-3 times the dust sublimation radius. ","Turbulence Regulates the Rate of Planetesimal Formation via
  Gravitational Collapse"
107,1221714698812477440,882303076505456642,Timon Emken,"[""Fresh off the press: Our new #paper on next-generation's direct sub-GeV #DarkMatter searches with semiconductors on today's @arxiv.\n\n<LINK>\n\nWe study future experiments' discovery reaches using frequentist and #MonteCarlo methods. <LINK>"", ""@arxiv The paper's emphasis is on the improved statistical treatment, the background modeling, and a study of astrophysical uncertainties, e.g. of the local escape velocity of our galaxy.\n\nNote: The figure does not show projected constraints, but discovery reaches of future experiments. https://t.co/BRCsbnwS3j"", '@arxiv It is the hard work of four Bachelor students here at @ChalmersPhysics that made this work possible.', '@BradleyKavanagh @arxiv Thanks!\n\nYeah, I might as well embrace the weird arXiv numbers sooner than later.']",https://arxiv.org/abs/2001.08910,"We compute the projected sensitivity to dark matter (DM) particles in the sub-GeV mass range of future direct detection experiments using germanium and silicon semiconductor targets. We perform this calculation within the dark photon model for DM-electron interactions using the likelihood ratio as a test statistic, Monte Carlo simulations, and background models that we extract from recent experimental data. We present our results in terms of DM-electron scattering cross section values required to reject the background only hypothesis in favour of the background plus DM signal hypothesis with a statistical significance, $\mathcal{Z}$, corresponding to 3 or 5 standard deviations. We also test the stability of our conclusions under changes in the astrophysical parameters governing the local space and velocity distribution of DM in the Milky Way. In the best-case scenario, when a high-voltage germanium detector with an exposure of $50$ kg-year and a CCD silicon detector with an exposure of $1$ kg-year and a dark current rate of $1\times10^{-7}$ counts/pixel/day have simultaneously reported a DM signal, we find that the smallest cross section value compatible with $\mathcal{Z}=3$ ($\mathcal{Z}=5$) is about $8\times10^{-42}$ cm$^2$ ($1\times10^{-41}$ cm$^2$) for contact interactions, and $4\times10^{-41}$ cm$^2$ ($7\times10^{-41}$ cm$^2$) for long-range interactions. Our sensitivity study extends and refine previous works in terms of background models, statistical methods, and treatment of the underlying astrophysical uncertainties. ","Projected sensitivity to sub-GeV dark matter of next-generation
  semiconductor detectors"
108,1219961900324327430,36396172,Diego R. Amancio,"['Our new preprint is out: ""Modeling Economic Networks with Firm-to-Firm Wire Transfers"" <LINK>\n\n""We study a novel economic network comprised of wire transfers (electronic payment transactions) among the universe of firms in Brazil (6.2 million firms). ""']",https://arxiv.org/abs/2001.06889,"We study a novel economic network (supply chain) comprised of wire transfers (electronic payment transactions) among the universe of firms in Brazil (6.2 million firms). We construct a directed and weighted network in which vertices represent cities and edges connote pairwise economic dependence between cities. Cities (vertices) represent the collection of all firms in that location and links denote intercity wire transfers. We find a high degree of economic integration among cities in the trade network, which is consistent with the high degree of specialization found across Brazilian cities. We are able to identify which cities have a dominant role in the entire supply chain process using centrality network measures. We find that the trade network has a disassortative mixing pattern, which is consistent with the power-law shape of the firm size distribution in Brazil. After the Brazilian recession in 2014, we find that the disassortativity becomes even stronger as a result of the death of many small firms and the consequent concentration of economic flows on large firms. Our results suggest that recessions have a large impact on the trade network with meaningful and heterogeneous economic consequences across municipalities. We run econometric exercises and find that courts efficiency plays a dual role. From the customer perspective, it plays an important role in reducing contractual frictions as it increases economic transactions between different cities. From the supplier perspective, cities that are central suppliers to the supply chain seem to use courts inefficiency as a lawsuit barrier from their customers. ",Modeling Supply-Chain Networks with Firm-to-Firm Wire Transfers
109,1218278961748291584,802543221943439360,Andrea Caputo,"['<LINK>\nMy first paper of 2020 out! We study the effect of accretion on black holes binaries, focusing on multiband prospects and possibility to detect electromagnetic counterparts. Take a look! #BlackHole #Ligo #physics']",https://arxiv.org/abs/2001.03620,"We study the impact of gas accretion on the orbital evolution of black-hole binaries initially at large separation in the band of the planned Laser Interferometer Space Antenna (LISA). We focus on two sources: (i)~stellar-origin black-hole binaries~(SOBHBs) that can migrate from the LISA band to the band of ground-based gravitational-wave observatories within weeks/months; and (ii) intermediate-mass black-hole binaries~(IMBHBs) in the LISA band only. Because of the large number of observable gravitational-wave cycles, the phase evolution of these systems needs to be modeled to great accuracy to avoid biasing the estimation of the source parameters. Accretion affects the gravitational-wave phase at negative ($-4$) post-Newtonian order, and is therefore dominant for binaries at large separations. If accretion takes place at the Eddington or at super-Eddington rate, it will leave a detectable imprint on the dynamics of SOBHBs. In optimistic astrophysical scenarios, a multiwavelength strategy with LISA and a ground-based interferometer can detect about $10$ (a few) SOBHB events for which the accretion rate can be measured at $50\%$ ($10\%$) level. In all cases the sky position can be identified within much less than $0.4\,{\rm deg}^2$ uncertainty. Likewise, accretion at $\gtrsim 10\%$ ($\gtrsim 100\%$) of the Eddington rate can be measured in IMBHBs up to redshift $z\approx 0.1$ ($z\approx 0.5$), and the position of these sources can be identified within less than $0.01\,{\rm deg}^2$ uncertainty. Altogether, a detection of SOBHBs or IMBHBs would allow for targeted searches of electromagnetic counterparts to black-hole mergers in gas-rich environments with future X-ray detectors (such as Athena) and radio observatories (such as SKA). ","Gravitational-wave detection and parameter estimation for accreting
  black-hole binaries and their electromagnetic counterpart"
110,1218241830355193857,788067417918365696,Harshitha Machiraju,"['"" A Little Fog for a Large Turn""\nWe tested popular autonomous navigation models with adversarially generated weather conditions and find that they still have a long way to go... @wacv2020 \nPreprint: <LINK>\nWebsite: <LINK>']",https://arxiv.org/abs/2001.05873,"Small, carefully crafted perturbations called adversarial perturbations can easily fool neural networks. However, these perturbations are largely additive and not naturally found. We turn our attention to the field of Autonomous navigation wherein adverse weather conditions such as fog have a drastic effect on the predictions of these systems. These weather conditions are capable of acting like natural adversaries that can help in testing models. To this end, we introduce a general notion of adversarial perturbations, which can be created using generative models and provide a methodology inspired by Cycle-Consistent Generative Adversarial Networks to generate adversarial weather conditions for a given image. Our formulation and results show that these images provide a suitable testbed for steering models used in Autonomous navigation models. Our work also presents a more natural and general definition of Adversarial perturbations based on Perceptual Similarity. ",A Little Fog for a Large Turn
111,1217132405552644096,4111874585,Manuel Rigger,"['We propose Pivoted Query Synthesis (PQS), a new approach for finding logic bugs in DBMS (see <LINK>). Using PQS, we found ~100 previously unknown (and many critical) bugs in widely-used DBMS (e.g., SQLite3, MySQL, and PostgreSQL). Work with @zhendongsu. <LINK>', 'PQS effectively tackles both test query and oracle generation. Its core idea is to choose a randomly-selected ""pivot row"" and generate a query whose result set must contain the pivot row. If the result set returned by the DBMS fails to fetch the pivot row, a bug is found.', 'To our knowledge, this is the largest and most successful testing campaign against such production DBMS (https://t.co/aB0tUYgEv0 lists all PQS bugs). Stay tuned for more work on testing DBMS. For example, our ongoing work has found an additional 100+ bugs via another new approach', 'I also want to highlight the great work done by the DBMS developers. The SQLite developers in particular fixed bugs very quickly, which is why we focused on testing SQLite.']",https://arxiv.org/abs/2001.04174,"Relational databases are used ubiquitously. They are managed by database management systems (DBMS), which allow inserting, modifying, and querying data using a domain-specific language called Structured Query Language (SQL). Popular DBMS have been extensively tested by fuzzers, which have been successful in finding crash bugs. However, approaches to finding logic bugs, such as when a DBMS computes an incorrect result set, have remained mostly untackled. Differential testing is an effective technique to test systems that support a common language by comparing the outputs of these systems. However, this technique is ineffective for DBMS, because each DBMS typically supports its own SQL dialect. To this end, we devised a novel and general approach that we have termed Pivoted Query Synthesis. The core idea of this approach is to automatically generate queries for which we ensure that they fetch a specific, randomly selected row, called the pivot row. If the DBMS fails to fetch the pivot row, the likely cause is a bug in the DBMS. We tested our approach on three widely-used and mature DBMS, namely SQLite, MySQL, and PostgreSQL. In total, we reported 123 bugs in these DBMS, 99 of which have been fixed or verified, demonstrating that the approach is highly effective and general. We expect that the wide applicability and simplicity of our approach will enable the improvement of robustness of many DBMS. ",Testing Database Engines via Pivoted Query Synthesis
112,1217013297876217857,1015270156501647362,Karan Molaverdikhani,"[""Remember we showed even Ultra-hot jupiters are possibly cloudy? (<LINK>) Today you'll find on the arxiv that they are also possibly not at thermochemical equilibrium and likely we only see them that way through transmission geometry! (<LINK>) <LINK>"", 'The third paper on this series (by awesome @MartianColonist et al.) will show how much these physicochemical processes would change the spectra.\n\nThese papers initiated during the Cloud Academy I and Cloud Academy II is on the way! Stay tuned for more exciting results! 💫']",https://arxiv.org/abs/2001.03668,"The atmospheres of ultra-hot Jupiters are commonly considered to be at thermochemical equilibrium. We aim to provide disequilibrium chemistry maps for a global understanding of the chemistry in HAT-P-7b's atmosphere and assess the importance of disequilibrium chemistry on UHJs. We apply a hierarchical modelling approach utilising 97 1D atmospheric profiles from 3D GCM of HAT-P-7b. For each 1D profile, we evaluate our kinetic cloud formation model consistently with the local gas-phase composition in chemical equilibrium. We then evaluate quenching results from a zeroth-order approximation in comparison to a kinetic gas-phase approach. We find that the zeroth-order approach of estimating quenching points agrees well with the full gas-kinetic modeling results. Chemical disequilibrium has the greatest effect on the nightside and morning abundance of species such as H, H$_2$O, CH$_4$, CO$_2$, HCN, and all C$_n$H$_m$ molecules; heavier C$_n$H$_m$ molecules are more affected by disequilibrium processes. CO abundance, however, is affected only marginally. While dayside abundances also notably change, those around the evening terminator of HAT-P-7b are the least affected by disequilibrium processes. The latter finding may partially explain the consistency of observed transmission spectra of UHJs with atmospheres in thermochemical equilibrium. Photochemistry only negligibly affects molecular abundances and quenching levels. In general, the quenching points of HAT-P-7b's atmosphere are at much lower pressures in comparison to the cooler hot-jupiters. We propose several avenues to look for the effect of disequilibrium processes on UHJs that are, in general, based on abundance and opacity measurements at different local times. It remains a challenge to completely disentangle this from the chemical effects of clouds and that of a primordial non-solar abundance. ","Understanding the atmospheric properties and chemical composition of the
  ultra-hot Jupiter HAT-P-7b II. Mapping the effects of gas kinetics"
113,1216739447447810051,1216454926349418496,Isaiah Santistevan,"['Just got my first, first-author paper on the arxiv! We determined formation times and studied the build-up of MW/M31-mass galaxies in the FIRE sims across cosmic time: <LINK>', 'By examining when galaxies transition from mostly merger/accretion dominated growth, to mostly in-situ growth, the main progenitor of the host galaxy “forms/emerges” around z ~ 3-4 (11.6-12.2 Gyr ago) https://t.co/lApvqNdyAN', 'About 100 galaxies with stellar mass over 1e5 solar masses formed a typical MW/M31-mass galaxy and its satellite population; this is ~ 5x more galaxies than the surviving population! https://t.co/LQM5XqCtsY', 'Finally, galaxies that are in LG-like environments (i.e., with a massive companion; in dotted lines) form earlier than isolated MW/M31-mass galaxies (solid lines)! https://t.co/lbFg9BMIEv', ""Many more details are in the paper if you're interested. Special thanks to @AndrewWetzel @kjb_astro @JossBlandHawtho @MBKplus and everyone else who helped out!""]",https://arxiv.org/abs/2001.03178,"Surveys of the Milky Way (MW) and M31 enable detailed studies of stellar populations across ages and metallicities, with the goal of reconstructing formation histories across cosmic time. These surveys motivate key questions for galactic archaeology in a cosmological context: when did the main progenitor of a MW/M31-mass galaxy form, and what were the galactic building blocks that formed it? We investigate the formation times and progenitor galaxies of MW/M31-mass galaxies using the FIRE-2 cosmological simulations, including 6 isolated MW/M31-mass galaxies and 6 galaxies in Local Group (LG)-like pairs at z = 0. We examine main progenitor ""formation"" based on two metrics: (1) transition from primarily ex-situ to in-situ stellar mass growth and (2) mass dominance compared to other progenitors. We find that the main progenitor of a MW/M31-mass galaxy emerged typically at z ~ 3-4 (11.6-12.2 Gyr ago), while stars in the bulge region (inner 2 kpc) at z = 0 formed primarily in a single main progenitor at z < 5 (< 12.6 Gyr ago). Compared with isolated hosts, the main progenitors of LG-like paired hosts emerged significantly earlier (\Delta z ~ 2, \Delta t ~ 1.6 Gyr), with ~ 4x higher stellar mass at all z > 4 (> 12.2 Gyr ago). This highlights the importance of environment in MW/M31-mass galaxy formation, especially at early times. Overall, about 100 galaxies with M_star > 10^5 M_sun formed a typical MW/M31-mass system. Thus, surviving satellites represent a highly incomplete census (by ~ 5x) of the progenitor population. ","The Formation Times and Building Blocks of Milky Way-mass Galaxies in
  the FIRE Simulations"
114,1216489565013319682,4873857094,Dominik Zumbuhl,"['On arXiv Machine learning automatic tuning into double quantum dot regime. We demonstrate an algorithm navigating the entire, large parameter space of a gate-defined dot to find e.g. double dot regimes. This also allows quantification device variability... <LINK> <LINK>']",https://arxiv.org/abs/2001.02589,"Device variability is a bottleneck for the scalability of semiconductor quantum devices. Increasing device control comes at the cost of a large parameter space that has to be explored in order to find the optimal operating conditions. We demonstrate a statistical tuning algorithm that navigates this entire parameter space, using just a few modelling assumptions, in the search for specific electron transport features. We focused on gate-defined quantum dot devices, demonstrating fully automated tuning of two different devices to double quantum dot regimes in an up to eight-dimensional gate voltage space. We considered a parameter space defined by the maximum range of each gate voltage in these devices, demonstrating expected tuning in under 70 minutes. This performance exceeded a human benchmark, although we recognise that there is room for improvement in the performance of both humans and machines. Our approach is approximately 180 times faster than a pure random search of the parameter space, and it is readily applicable to different material systems and device architectures. With an efficient navigation of the gate voltage space we are able to give a quantitative measurement of device variability, from one device to another and after a thermal cycle of a device. This is a key demonstration of the use of machine learning techniques to explore and optimise the parameter space of quantum devices and overcome the challenge of device variability. ","Machine learning enables completely automatic tuning of a quantum device
  faster than human experts"
115,1215258112384475136,19149703,Karina Voggel ✨🔭🏃🏼‍♀️,"['Today our new paper on how we can find globular clusters with the help of @ESAGaia is out on the arxiv! with @anilcseth @caprastro &amp; @sand_dave <LINK>', '@ESAGaia @anilcseth @caprastro @sand_dave We tried to find globular clusters in Centaurus A by using colour and astrometric excess factors in Gaia DR2. We realized that star clusters in nearby galaxies do not appear like standard point sources in Gaia. https://t.co/nkpMAxKJZ9', '@ESAGaia @anilcseth @caprastro @sand_dave Normally these excess factors are used as a quality assessment tool in Gaia to purge out bad sources!', '@ESAGaia @anilcseth @caprastro @sand_dave We have followe up a few candidates, and identified 5 brand new GCs in the outskirts of CenA. They actually are clearly visible as star clusters in good seeing imaging but we had not found them because the outer Halo of Gaia spans several square degrees on the sky. https://t.co/xDCaSyUi7X', '@ESAGaia @anilcseth @caprastro @sand_dave These newly confirmed clusters (Blue datapoints) are now the record holders for the three most distant known GCs in CenA! This shows the power of this Gaia method to identify good candidates in the distant outskirts. https://t.co/YmA7KTnz3X', '@ESAGaia @anilcseth @caprastro @sand_dave And we expect that this @ESAGaia method is applicable to find bright GCs in most Local Volume galaxies out to 25Mpc. Especially in those sparse outer Halos of galaxies! https://t.co/8xVH1LvTF3', '@ESAGaia @anilcseth @caprastro @sand_dave On top of that we also show that the excess factors are directly correlated to the physical size of the GCs. Meaning that you can use the excess factors to get a rough size estimate of their sizes without the need for high-resolution HST imaging. https://t.co/qlkSS1SIHx']",https://arxiv.org/abs/2001.02243,"Tidally stripped galaxy nuclei and luminous globular clusters (GCs) are important tracers of the halos and assembly histories of nearby galaxies, but are difficult to reliably identify with typical ground-based imaging data. In this paper we present a new method to find these massive star clusters using Gaia DR2, focusing on the massive elliptical galaxy Centaurus A (Cen A). We show that stripped nuclei and globular clusters are partially resolved by Gaia at the distance of Cen A, showing characteristic astrometric and photometric signatures. We use this selection method to produce a list of 632 new candidate luminous clusters in the halo of Cen A out to a projected radius of 150 kpc. Adding in broadband photometry and visual examination improves the accuracy of our classification. In a spectroscopic pilot program we have confirmed 5 new luminous clusters, which includes the 7th and 10th most luminous GC in Cen\,A. Three of the newly discovered GCs are further away from Cen A in than all previously known GCs. Several of these are compelling candidates for stripped nuclei. We show that our novel Gaia selection method retains at least partial utility out to distances of 25 Mpc and hence is a powerful tool for finding and studying star clusters in the sparse outskirts of galaxies in the local universe. ","A Gaia-based catalog of candidate stripped nuclei and luminous globular
  clusters in the halo of Centaurus A"
116,1220281875018575874,185700691,Yuri Lavinas🇧🇷🏴󠁧󠁢󠁥󠁮󠁧󠁿🇯🇵🏴󠁧󠁢󠁳󠁣󠁴󠁿,"['We publish a study on #multiobjective in Arvix, titled ""MOEA/D with Random Partial Update Strategy"". We found that slower population dynamics affect MOEA/D in interesting ways 😀! Come take a look!!\n\n<LINK>']",https://arxiv.org/abs/2001.06980,"Recent studies on resource allocation suggest that some subproblems are more important than others in the context of the MOEA/D, and that focusing on the most relevant ones can consistently improve the performance of that algorithm. These studies share the common characteristic of updating only a fraction of the population at any given iteration of the algorithm. In this work we investigate a new, simpler partial update strategy, in which a random subset of solutions is selected at every iteration. The performance of the MOEA/D using this new resource allocation approach is compared experimentally against that of the standard MOEA/D-DE and the MOEA/D with relative improvement-based resource allocation. The results indicate that using the MOEA/D with this new partial update strategy results in improved HV and IGD values, and a much higher proportion of non-dominated solutions, particularly as the number of updated solutions at every iteration is reduced. ",MOEA/D with Random Partial Update Strategy
117,1220048478111354880,3779423657,Shen Yan,['We also find the MixMatch based approach works pretty well on unsupervised domain adaptation for standard image benchmarks and time-series data.  <LINK> @D_Berthelot_ML <LINK>'],https://arxiv.org/abs/2001.00677,"Unsupervised domain adaptation studies the problem of utilizing a relevant source domain with abundant labels to build predictive modeling for an unannotated target domain. Recent work observe that the popular adversarial approach of learning domain-invariant features is insufficient to achieve desirable target domain performance and thus introduce additional training constraints, e.g. cluster assumption. However, these approaches impose the constraints on source and target domains individually, ignoring the important interplay between them. In this work, we propose to enforce training constraints across domains using mixup formulation to directly address the generalization performance for target data. In order to tackle potentially huge domain discrepancy, we further propose a feature-level consistency regularizer to facilitate the inter-domain constraint. When adding intra-domain mixup and domain adversarial learning, our general framework significantly improves state-of-the-art performance on several important tasks from both image classification and human activity recognition. ",Improve Unsupervised Domain Adaptation with Mixup Training
118,1216313905452064769,831805863035863040,Midas Nouwens,"['New paper (for #CHI2020) on dark patterns in consent pop-ups after the GDPR. We (w/ I Liccardi @mikarv @karger L Kagal) scraped 10K UK sites and found the biggest pop-up providers are configured illegally 88.2% of the time, with big impacts on answers\n<LINK>\n\n1/9 <LINK>', '5 pop-up (consent management platform) firms have ~58% of the UK market (@QuantCast\n@TrustArc @Crownpeak @CookiebotCybot @OneTrust).\n \nOnly 11.8%(!) meet even the most basic(!) legal UX reqs for valid consent: no pre-checked boxes, explicit consent, reject as easy as accept.\n\n2/9 https://t.co/UL5m5TBmKw', 'This is before you even look at what actually happens (or doesn’t happen) after you click accept/reject, which @CelestinMatte @nataliabielova @Cristianapt have shown can be very little: https://t.co/J6PYYPdymX\n \n3/9', ""To see how designs affect acceptance rates, we injected fake pop-ups into participants' sites (n=40), comparing:\n- banners vs. barriers;\n- presence vs. absence of reject button on 1st page;\n- 0 vs 1 clicks required to access vendor/purpose controls\n \n4/9"", 'We found that removing the decline button from the 1st page increased consent by ~23%pts. In fact, anything on the 2nd page might as well not exist: 93.1% of interactions were limited to the 1st page.\n \n5/9 https://t.co/3XA6uUWzVY', 'Showing more options on 1st page than just Accept or Decline always decreased consent: showing vendors by 20%pts; purposes by 8.8%pts; and both by 11.9%pts.\n \nThis follows what @ChristineUtz @mrtn3000 @sascha_fahl @floschaub @thorstenholz\nfound: https://t.co/WGZNWMeuJn\n \n6/9 https://t.co/6KE7QAFAk9', 'Enforcement is lacking and DPAs lack budget/staff (https://t.co/ZDBnPlfw1R) — and perhaps don’t know where to start. Regulating 3rd party consent services to not allow misconfiguration could be a much more effective way to increase compliance than targeting individual sites.\n7/9', ""If you don't want to wait for enforcement, we also built a browser extension that automatically answers consent pop-ups for you based on your privacy preferences. https://t.co/gw7Vdd9Lk5\n \n8/9"", 'If scientific papers are not your thing, @riptari has written up our findings in a rigorous and thorough way here: https://t.co/l9pMvysSqp\n \n9/9', ""@PrivacyMatters I know... Here's a cached version though! https://t.co/6QeMpv9Og2"", '@el_keogh Generally, yes! Websites can of course build their own, but most use a third party ""consent managment platform"". These often come with a back-end advertising service, where they facilitate real-time ad bidding etc', ""@drt3ch @mikarv @karger @PrivacyPrivee @Carleton_U Amazing, thanks! I'd also be happy to make the scraper/experiment/R code available to them if they want!"", '@andreapernici @mikarv @karger Reminds me of @Quantcast press release saying ""more than 10,000 domains worldwide have deployed Quantcast Choice, generating an average consent rate among consumers of more than 90 percent"". Begs the question: legally valid consent? https://t.co/VVsbv2XhTU', ""@BCowanHCI Thanks! Couldn't have done it without your help! https://t.co/Qpql5zGS4F""]",https://arxiv.org/abs/2001.02479,"New consent management platforms (CMPs) have been introduced to the web to conform with the EU's General Data Protection Regulation, particularly its requirements for consent when companies collect and process users' personal data. This work analyses how the most prevalent CMP designs affect people's consent choices. We scraped the designs of the five most popular CMPs on the top 10,000 websites in the UK (n=680). We found that dark patterns and implied consent are ubiquitous; only 11.8% meet the minimal requirements that we set based on European law. Second, we conducted a field experiment with 40 participants to investigate how the eight most common designs affect consent choices. We found that notification style (banner or barrier) has no effect; removing the opt-out button from the first page increases consent by 22--23 percentage points; and providing more granular controls on the first page decreases consent by 8--20 percentage points. This study provides an empirical basis for the necessary regulatory action to enforce the GDPR, in particular the possibility of focusing on the centralised, third-party CMP services as an effective way to increase compliance. ","Dark Patterns after the GDPR: Scraping Consent Pop-ups and Demonstrating
  their Influence"
119,1215661685391814657,737089971807485952,Serena Booth,"[""It's still WIP, but I am quite fond of our new paper: <LINK>. We play around with probabilistic programming to find in-distribution adversarial examples. Joint work with Yilun, @ankitjs, @julie_a_shah from @MITRobotics. Come see us Feb 7 at AAAI StarAI workshop!""]",https://arxiv.org/abs/2001.03076,"Though neural network models demonstrate impressive performance, we do not understand exactly how these black-box models make individual predictions. This drawback has led to substantial research devoted to understand these models in areas such as robustness, interpretability, and generalization ability. In this paper, we consider the problem of exploring the prediction level sets of a classifier using probabilistic programming. We define a prediction level set to be the set of examples for which the predictor has the same specified prediction confidence with respect to some arbitrary data distribution. Notably, our sampling-based method does not require the classifier to be differentiable, making it compatible with arbitrary classifiers. As a specific instantiation, if we take the classifier to be a neural network and the data distribution to be that of the training data, we can obtain examples that will result in specified predictions by the neural network. We demonstrate this technique with experiments on a synthetic dataset and MNIST. Such level sets in classification may facilitate human understanding of classification behaviors. ","Sampling Prediction-Matching Examples in Neural Networks: A
  Probabilistic Programming Approach"
120,1215455053042970624,15921177,Deepti Ghadiyaram,"['We look at the downside of classifiers relying heavily on the co-occurring context  and propose two novel ways to mitigate such co-occurring bias to ensure the model is learning from the right thing, with no additional annotations! <LINK>']",https://arxiv.org/abs/2001.03152,"Existing models often leverage co-occurrences between objects and their context to improve recognition accuracy. However, strongly relying on context risks a model's generalizability, especially when typical co-occurrence patterns are absent. This work focuses on addressing such contextual biases to improve the robustness of the learnt feature representations. Our goal is to accurately recognize a category in the absence of its context, without compromising on performance when it co-occurs with context. Our key idea is to decorrelate feature representations of a category from its co-occurring context. We achieve this by learning a feature subspace that explicitly represents categories occurring in the absence of context along side a joint feature subspace that represents both categories and context. Our very simple yet effective method is extensible to two multi-label tasks -- object and attribute classification. On 4 challenging datasets, we demonstrate the effectiveness of our method in reducing contextual bias. ","Don't Judge an Object by Its Context: Learning to Overcome Contextual
  Bias"
121,1215314320189317120,2587793395,Jeannette (Jamie) Garcia,"['New study up on arXiv this week! Check out our results on quantum chemistry calculations for salts formed during operation in lithium-sulfur batteries with @Daimler. Here we demonstrate a dipole moment calculation using #IBMQ hardware: <LINK>.', '@Daimler Also, check out my corresponding blog discussing our study and why we’re exploring chemistry with quantum computers: https://t.co/JjeNqPeeu6.']",https://arxiv.org/abs/2001.01120,"Quantum chemistry simulations of some industrially relevant molecules are reported, employing variational quantum algorithms for near-term quantum devices. The energies and dipole moments are calculated along the dissociation curves for lithium hydride (LiH), hydrogen sulfide, lithium hydrogen sulfide and lithium sulfide. In all cases we focus on the breaking of a single bond, to obtain information about the stability of the molecular species being investigated. We calculate energies and a variety of electrostatic properties of these molecules using classical simulators of quantum devices, with up to 21 qubits for lithium sulfide. Moreover, we calculate the ground-state energy and dipole moment along the dissociation pathway of LiH using IBM quantum devices. This is the first example, to the best of our knowledge, of dipole moment calculations being performed on quantum hardware. ","Quantum Chemistry Simulations of Dominant Products in Lithium-Sulfur
  Batteries"
