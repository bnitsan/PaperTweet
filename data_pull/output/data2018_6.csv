,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1017901369804673025,223923566,David Van Horn,['New paper: Theorem Proving for All: Equational Reasoning in Liquid Haskell w/ @nikivazou @nomeata @haskellhutt &amp; William Kunkel to appear at Haskell Symposium 2018 @icfp_conference @strangeloop_stl \n<LINK>'],https://arxiv.org/abs/1806.03541,"Equational reasoning is one of the key features of pure functional languages such as Haskell. To date, however, such reasoning always took place externally to Haskell, either manually on paper, or mechanised in a theorem prover. This article shows how equational reasoning can be performed directly and seamlessly within Haskell itself, and be checked using Liquid Haskell. In particular, language learners --- to whom external theorem provers are out of reach --- can benefit from having their proofs mechanically checked. Concretely, we show how the equational proofs and derivations from Graham's textbook can be recast as proofs in Haskell (spoiler: they look essentially the same). ","Functional Pearl: Theorem Proving for All (Equational Reasoning in
  Liquid Haskell)"
1,1017373588033298432,927837253,Emtiyaz Khan,"['I will talk about our new work on ""Bayesian deep learning using weight-perturbation in Adam"" at #icml2018 in ""Deep Learning (Bayesian) 2"" session at 4:50pm in room A4. Paper here <LINK> Slides here <LINK> Code here <LINK> 1/6 <LINK>', 'Short summary: Gaussian mean-field variational inference by running Adam on the MLE objective and making the following changes: perturb the weights. Second, add a contribution from the prior, and use a small minibatch size. 2/6', 'This result is a direct consequence of using natural-gradients instead of gradients. The mean is equal to the parameter returned by Adam, and the variance can be obtained from the scale vector. Perturbation is due to the sampling from variational distribution. 3/6', ""Small minibatches are due to 'a square of sum of gradients' approximation in Adam for the second-order information. See theorem 1 in the paper. 4/6 https://t.co/Bff8Grqprd"", 'We also propose VadaGrad and Variational Adaptive Newton (VAN) method for variational optimization (or what @beenwrekt calls Random search). This work is cool because the variance of the search distribution is automatically adapted. Also see https://t.co/0Izj1HsmKI 5/6 https://t.co/axKWYyHgnH', 'Also check out a very similar work by @Guodzh @DavidDuvenaud @RogerGrosse https://t.co/vuPtmVm26o They have done some interesting things with KFAC. 6/6', '7/6 Tweeting is hard.']",https://arxiv.org/abs/1806.04854,"Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization. ",Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam
2,1015689145602199553,5382332,Georg Langs,['New paper (TMI) on Keypoint Transfer for Fast Whole-Body Segmentation in medical imaging data by Christian Wachinger @wonnga also using @VisceralEU data set: <LINK>'],https://arxiv.org/abs/1806.08723,"We introduce an approach for image segmentation based on sparse correspondences between keypoints in testing and training images. Keypoints represent automatically identified distinctive image locations, where each keypoint correspondence suggests a transformation between images. We use these correspondences to transfer label maps of entire organs from the training images to the test image. The keypoint transfer algorithm includes three steps: (i) keypoint matching, (ii) voting-based keypoint labeling, and (iii) keypoint-based probabilistic transfer of organ segmentations. We report segmentation results for abdominal organs in whole-body CT and MRI, as well as in contrast-enhanced CT and MRI. Our method offers a speed-up of about three orders of magnitude in comparison to common multi-atlas segmentation, while achieving an accuracy that compares favorably. Moreover, keypoint transfer does not require the registration to an atlas or a training phase. Finally, the method allows for the segmentation of scans with highly variable field-of-view. ",Keypoint Transfer for Fast Whole-Body Segmentation
3,1013166589536063488,3409898008,〈 Berger | Dillon 〉,"['If you want some light reading on invariant tensors in gauge theories, check out my new paper here:  <LINK> <LINK>']",https://arxiv.org/abs/1806.04332,"Invariant tensors play an important role in gauge theories, for example, in dualities of N=1 gauge theories. However, for theories with fields in representations larger than the fundamental, the full set of invariant tensors is often difficult to construct. We present a new approach to the construction of these tensors, and use it to find the complete set of invariant tensors of a theory of SO(3) with fields in the symmetric tensor representation. ",Invariant Tensors in Gauge Theories
4,1012726654261702658,19510090,Julian Togelius,"['Deep reinforcement learning overfits. Often, a trained network can only play the particular level(s) you trained it on! In our new paper, we show how to train more general networks with procedural level generation, generating progressively harder levels.\n<LINK> <LINK>', 'The paper, written by @nojustesen @ruben_torrado @FilipoGiovanni @Amidos2006, me and @risi1979, builds on the General Video Game AI framework, which includes more than a hundred different games and lets you easily modify games and levels (and generate new ones).', ""We also build on ours and others' research on procedural content generation for games, a research field studying algorithms that can create new game content such as levels. Useful not only for game development but also for AI testing.\nMore on PCG:\nhttps://t.co/d7Y1fTFomc"", 'The level generators we use in our paper allow for generating levels for three different games, with different difficulty levels. So we start training agents on very simple levels, and as soon as they learn to play these levels well we increase the difficulty level. https://t.co/y6J7VEIDid', 'By training this way, we not only find agents that generalize better to unseen levels, but we can also learn to play hard levels which we could not learn to play if we started from scratch. https://t.co/ZwQmsUcLsg', 'We are taking the old idea of increasing the difficulty as the agent improves, which has variously been called incremental evolution, staged learning and curriculum learning, and combining it with procedural content generation.', 'Our results point to the need for variable environments for reinforcement learning. Using procedural content generation when learning to play games seems to be more or less necessary to achieve policies that are not brittle and specialized.', 'When training on a single game with fixed, small set of levels, you are setting yourself up for overfitting. If your performance evaluation is based on the same set of levels, you are testing on the training set, which is considered a big no-no in machine learning (but not RL?).', 'In particular, this applies to the very popular practice of training agents to play Atari games in the ALE framework. Our results suggest that doing so encourages overfitting, and learning very brittle strategies.', 'In other words, reinforcement learning researchers - including but not limited to those working on games - should adopt procedural level generation as a standard practice. The @gvgai framework provides a perfect platform for this.', 'Our previous paper explaining the GVGAI learning track framework which we use for this research can be found here:\nhttps://t.co/yq4ehVfyPr', ""@FredrikHeintz That's the next step!"", '@mtrc This is a beautiful picture...', '@hardmaru @robo_skills Thanks! I agree, even a little bit of random variation certainly helps, and thanks for pointing to that bipedal walker environment - we should cite it. I do believe though that the more thorough the PCG is, the more we challenge the generalization capacity of the agent.', '@hardmaru @robo_skills I\'ve long wanted to build an environment which gradually generalizes and complexifies forever until you get to ""actually general"" intelligence. Let\'s say that @gvgai is a baby step in that direction...', '@mtrc Oh yes. Might need to edit it down so it fits in a minute while we describe the algorithm over it...\nhttps://t.co/OWOiRQGoqO', '@hardmaru @ThomasMiconi @robo_skills @gvgai @kenneth0stanley Definitely! Competitive coevolution of levels and controllers (where either of the parts is either evolution, gradient descent or possible something else) has been on my list for a long time now. If only I had more people...', '@iandanforth @hardmaru @ThomasMiconi @robo_skills @gvgai @kenneth0stanley That is an interesting approach to research collaboration. Not sure what I think of it...']",https://arxiv.org/abs/1806.10729,"Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human. ","Illuminating Generalization in Deep Reinforcement Learning through
  Procedural Level Generation"
5,1012711910616289281,1400175774,Victor Veitch,"['New paper ""Empirical Risk Minimization and Stochastic Gradient Descent for Relational Data"" does what it says on the label. An easy and statistically sane approach to handling machine learning on big, complicated relational datasets. <LINK>  @blei_lab <LINK>']",https://arxiv.org/abs/1806.10701,"Empirical risk minimization is the main tool for prediction problems, but its extension to relational data remains unsolved. We solve this problem using recent ideas from graph sampling theory to (i) define an empirical risk for relational data and (ii) obtain stochastic gradients for this empirical risk that are automatically unbiased. This is achieved by considering the method by which data is sampled from a graph as an explicit component of model design. By integrating fast implementations of graph sampling schemes with standard automatic differentiation tools, we provide an efficient turnkey solver for the risk minimization problem. We establish basic theoretical properties of the procedure. Finally, we demonstrate relational ERM with application to two non-standard problems: one-stage training for semi-supervised node classification, and learning embedding vectors for vertex attributes. Experiments confirm that the turnkey inference procedure is effective in practice, and that the sampling scheme used for model specification has a strong effect on model performance. Code is available at this https URL ","Empirical Risk Minimization and Stochastic Gradient Descent for
  Relational Data"
6,1012673397027954688,50901426,Rafael Alves Batista,['My new paper: “Cosmogenic photon and neutrino fluxes in the Auger era”\nWe fit the UHECR spectrum and composition and derive bands for the expected photon and neutrino fluxes.\n<LINK>'],https://arxiv.org/abs/1806.10879,"The interaction of ultra-high-energy cosmic rays (UHECRs) with pervasive photon fields generates associated cosmogenic fluxes of neutrinos and photons due to photohadronic and photonuclear processes taking place in the intergalactic medium. We perform a fit of the UHECR spectrum and composition measured by the Pierre Auger Observatory for four source emissivity scenarios: power-law redshift dependence with one free parameter, active galactic nuclei, gamma-ray bursts, and star formation history. We show that negative source emissivity evolution is favoured if we treat the source evolution as a free parameter. In all cases, the best fit is obtained for relatively hard spectral indices and low maximal rigidities, for compositions at injection dominated by intermediate nuclei (nitrogen and silicon groups). In light of these results, we calculate the associated fluxes of neutrinos and photons. Finally, we discuss the prospects for the future generation of high-energy neutrino and gamma-ray observatories to constrain the sources of UHECRs. ",Cosmogenic photon and neutrino fluxes in the Auger era
7,1012668342505820162,613425571,Joseph Anderson,['New paper on ASASSN-18km/SN2018bsz on the archive: <LINK> A carbon-rich SLSN!'],https://arxiv.org/abs/1806.10609,"Super-luminous supernovae (SLSNe) are rare events defined as being significantly more luminous than normal terminal stellar explosions. The source of the extra powering needed to achieve such luminosities is still unclear. Discoveries in the local Universe (i.e. $z<0.1$) are scarce, but afford dense multi-wavelength observations. Additional low-redshift objects are therefore extremely valuable. We present early-time observations of the type I SLSN ASASSN-18km/SN~2018bsz. These data are used to characterise the event and compare to literature SLSNe and spectral models. Host galaxy properties are also analysed. Optical and near-IR photometry and spectroscopy were analysed. Early-time ATLAS photometry was used to constrain the rising light curve. We identified a number of spectral features in optical-wavelength spectra and tracked their time evolution. Finally, we used archival host galaxy photometry together with HII region spectra to constrain the host environment. ASASSN-18km/SN~2018bsz is found to be a type I SLSN in a galaxy at a redshift of 0.0267 (111 Mpc), making it the lowest-redshift event discovered to date. Strong CII lines are identified in the spectra. Spectral models produced by exploding a Wolf-Rayet progenitor and injecting a magnetar power source are shown to be qualitatively similar to ASASSN-18km/SN~2018bsz, contrary to most SLSNe-I that display weak/non-existent CII lines. ASASSN-18km/SN~2018bsz displays a long, slowly rising, red 'plateau' of $>$26 days, before a steeper, faster rise to maximum. The host has an absolute magnitude of --19.8 mag ($r$), a mass of M$_{*}$ = 1.5$^{+0.08}_{-0.33}$ $\times$10$^{9}$ M$_{\odot}$ , and a star formation rate of = 0.50$^{+2.22}_{-0.19}$ M$_{\odot}$ yr$^{-1}$. A nearby HII region has an oxygen abundance (O3N2) of 8.31$\pm$0.01 dex. ","A nearby superluminous supernova with a long pre-maximum 'plateau' and
  strong CII features"
8,1012609594110218240,1362984396,Dino Pedreschi,['Open the Black Box: Data-Driven Explanation of Black Box Decision Systems. Check out the new paper by @kdd_lab <LINK> \n#opentheblackbox #explainableAI @annamonreale @rikdrive8s @ruggieris @lucpappalard Fosca Giannotti Franco Turini @SoBigData <LINK>'],http://arxiv.org/abs/1806.09936,"Black box systems for automated decision making, often based on machine learning over (big) data, map a user's features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases hidden in the algorithms, due to human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We introduce the local-to-global framework for black box explanation, a novel approach with promising early results, which paves the road for a wide spectrum of future developments along three dimensions: (i) the language for expressing explanations in terms of highly expressive logic-based rules, with a statistical and causal interpretation; (ii) the inference of local explanations aimed at revealing the logic of the decision adopted for a specific instance by querying and auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of the many local explanations into simple global ones, with algorithms that optimize the quality and comprehensibility of explanations. ",Open the Black Box Data-Driven Explanation of Black Box Decision Systems
9,1012490399196336128,1648277749,Greg Warrington,"[""A short new gerrymandering paper on the definition of packed/cracked\nvoters as referred to in last week's SCOTUS opinions for Gill v. Whitford.\n\n<LINK> <LINK>""]",https://arxiv.org/abs/1806.11074,"The actions of packing and cracking are central to the construction of gerrymandered district plans. The US Supreme Court opinion in Gill v. Whitford makes clear that vote dilution arguments require showing that individual voters have been packed or cracked. In this article we provide precise definitions of what it means for a voter to be packed or cracked. These definitions, which depend crucially on the existence of at least one comparator plan, are illustrated using a simple hypothetical example. We also explore who might be considered packed or cracked for congressional plans in Maryland and North Carolina, and for the current state assembly plan in Wisconsin. ",Packed voters and cracked voters
10,1012410454327480320,1012405681435992064,Robyn Sanderson,"[""Okay, I'll get on Twitter for this. Hello world! Use our new data! \nPaper: <LINK>  \nData: <LINK> \nWebsite: <LINK> <LINK>""]",http://arxiv.org/abs/1806.10564,"With Gaia Data Release 2, the astronomical community is entering a new era of multidimensional surveys of the Milky Way. This new phase-space view of our Galaxy demands new tools for comparing observations to simulations of Milky-Way-mass galaxies in a cosmological context, to test the physics of both dark matter and galaxy formation. We present ananke, a framework for generating synthetic phase-space surveys from high-resolution baryonic simulations, and use it to generate a suite of synthetic surveys resembling Gaia DR2 in data structure, magnitude limits, and observational errors. We use three cosmological simulations of Milky-Way-mass galaxies from the Latte suite of the Feedback In Realistic Environments (FIRE) project, which feature self-consistent clustering of star formation in dense molecular clouds and thin stellar/gaseous disks in live cosmological halos with satellite dwarf galaxies and stellar halos. We select three solar viewpoints from each simulation to generate nine synthetic Gaia-like surveys. We sample synthetic stars by assuming each star particle (of mass 7070 $M_{\odot}$) represents a single stellar population. At each viewpoint, we compute dust extinction from the simulated gas metallicity distribution and apply a simple error model to produce a synthetic Gaia-like survey that includes both observational properties and a pointer to the generating star particle. We provide the complete simulation snapshot at $z = 0$ for each simulated galaxy. We describe data access points, the data model, and plans for future upgrades. These synthetic surveys provide a tool for the scientific community to test analysis methods and interpret Gaia data. ","Synthetic Gaia surveys from the FIRE cosmological simulations of Milky
  Way-mass galaxies"
11,1012386205189668865,26131740,Yuxing Sun,"['#stanfordnlp RT gneubig: New #ACL2018 paper on StructVAE, semi-supervised learning with structured latent variables: <LINK>\nA new shift-reduce method for seq2tree neural models, semantic parsing results robust to small data, and nice an… <LINK>']",https://arxiv.org/abs/1806.07832,"Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semisupervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models. ","StructVAE: Tree-structured Latent Variable Models for Semi-supervised
  Semantic Parsing"
12,1012346514708967426,90437330,Hiroshi Saruwatari,"[""Our preprint paper on Independent Deeply Learned Matrix Analysis (IDLMA) is now available. This is the world's first integration of DNN and independent linear factor analysis. Also, new parametric heavy tailed source model is introduced.\n\n<LINK>""]",https://arxiv.org/abs/1806.10307,"In this paper, we address a multichannel audio source separation task and propose a new efficient method called independent deeply learned matrix analysis (IDLMA). IDLMA estimates the demixing matrix in a blind manner and updates the time-frequency structures of each source using a pretrained deep neural network (DNN). Also, we introduce a complex Student's t-distribution as a generalized source generative model including both complex Gaussian and Cauchy distributions. Experiments are conducted using music signals with a training dataset, and the results show the validity of the proposed method in terms of separation accuracy and computational cost. ","Independent Deeply Learned Matrix Analysis for Multichannel Audio Source
  Separation"
13,1012240076439539712,483095843,Alexander Peach,"[""Excited to announce my new paper, based on Erik Verlinde's new theory of emergent gravity which claims to do away with the existence of dark matter.\n\n<LINK>""]",https://arxiv.org/abs/1806.10195,"In this work, a clear connection is made between E. Verlinde's recent theory of emergent gravity in de Sitter space and the earlier work that described emergent gravity using holographic screens. A modified (non)holographic screen scenario is presented, wherein the screen fails to encode an emergent mass in the bulk ""unemerged"" part of space for sufficiently large length-scales, where the volume-law of the non-holographic bulk degrees of freedom overtakes the area-law scaling of the entropy of the screen. Within this framework, we can describe both an emergent dark gravitational force, which scales like $\frac{1}{r}$, and also a version of the baryonic Tully-Fisher relation. We therefore recast these results within an emergent gravity framework in which there is an explicit violation of holography for sufficiently large length-scales. ",Emergent Dark Gravity from (Non)Holographic Screens
14,1012227137854681089,292313052,Martin Kilbinger,"[""Weak lensers: Do you need to calibrate shear bias from simulations? Check out Arnau Pujol's new paper <LINK> to reduce the number of necessary gaalxy images by 2-3 orders of magnitude!""]",https://arxiv.org/abs/1806.10537,"We present a new method to estimate shear measurement bias in image simulations that significantly improves the precision with respect to current techniques. Our method is based on measuring the shear response for individual images. We generated sheared versions of the same image to measure how the galaxy shape changes with the small applied shear. This shear response is the multiplicative shear bias for each image. In addition, we also measured the individual additive bias. Using the same noise realizations for each sheared version allows us to compute the shear response at very high precision. The estimated shear bias of a sample of galaxies is then the average of the individual measurements. The precision of this method leads to an improvement with respect to previous methods concerned with the precision of estimates of multiplicative bias since our method is not affected by noise from shape measurements, which until now has been the dominant uncertainty. As a consequence, the method does not require shape-noise suppression for a precise estimation of shear multiplicative bias. Our method can be readily used for numerous applications such as shear measurement validation and calibration, reducing the number of necessary simulated images by a few orders of magnitude to achieve the same precision. ","A highly precise shear bias estimator independent of the measured shape
  noise"
15,1012139214996004864,21611239,Sean Carroll,"['What can you do if you want to talk about something like ""position"" and ""momentum,"" but you have a quantum theory in a finite-dimensional Hilbert space? Generalized Clifford Algebra to the rescue. New paper with @ashmeetastro.\n<LINK>']",https://arxiv.org/abs/1806.10134,"The finite entropy of black holes suggests that local regions of spacetime are described by finite-dimensional factors of Hilbert space, in contrast with the infinite-dimensional Hilbert spaces of quantum field theory. With this in mind, we explore how to cast finite-dimensional quantum mechanics in a form that matches naturally onto the smooth case, especially the recovery of conjugate position/momentum variables, in the limit of large Hilbert-space dimension. A natural tool for this task are the Generalized Pauli operators (GPO). Based on an exponential form of Heisenberg's canonical commutation relation, the GPO offers a finite-dimensional generalization of conjugate variables without relying on any a priori structure on Hilbert space. We highlight some features of the GPO, its importance in studying concepts such as spread induced by operators, and point out departures from infinite-dimensional results (possibly with a cutoff) that might play a crucial role in our understanding of quantum gravity. We introduce the concept of ""Operator Collimation,"" which characterizes how the action of an operator spreads a quantum state along conjugate directions. We illustrate these concepts with a worked example of a finite-dimensional harmonic oscillator, demonstrating how the energy spectrum deviates from the familiar infinite-dimensional case. ","Modeling Position and Momentum in Finite-Dimensional Hilbert Spaces via
  Generalized Pauli Operators"
16,1012135946894970880,3018751880,Prof. Katelin Schutz,"['On a positive note, I have a new paper out today about a thermal #darkmatter production mechanism w Yonit Hochberg, Eric Kuflik, Robert McGehee, and Hitoshi Murayama (@sleptogenesis). Really fun working with the SIMP crew doing some real #particlephysics! <LINK>']",https://arxiv.org/abs/1806.10139,"Dark matter could be a thermal relic comprised of strongly interacting massive particles (SIMPs), where $3 \rightarrow 2$ interactions set the relic abundance. Such interactions generically arise in theories of chiral symmetry breaking via the Wess-Zumino-Witten term. In this work, we show that an axion-like particle can successfully maintain kinetic equilibrium between the dark matter and the visible sector, allowing the requisite entropy transfer that is crucial for SIMPs to be a cold dark matter candidate. Constraints on this scenario arise from beam dump and collider experiments, from the cosmic microwave background, and from supernovae. We find a viable parameter space when the axion-like particle is close in mass to the SIMP dark matter, with strong-scale masses of order a few hundred MeV. Many planned experiments are set to probe the parameter space in the near future. ",SIMPs through the axion portal
17,1012126279871627265,2902658140,Sander Dieleman,"['Stacking WaveNet autoencoders on top of each other leads to raw audio models that can capture long-range structure in music. Check out our new paper: <LINK>\n\nListen to some minute-long piano music samples: <LINK> <LINK>', 'more unconditional samples and reconstructions are available here: https://t.co/Mu6Cp11LKw']",https://arxiv.org/abs/1806.10474,"Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds. ","The challenge of realistic music generation: modelling raw audio at
  scale"
18,1011998887370870784,869876111336914944,Nana Liu,['Our new paper on continuous-variable blind quantum computation is now out on the arxiv! Many thanks to my wonderful collaborators! <LINK>'],https://arxiv.org/abs/1806.09137,"We present a verifiable and blind protocol for assisted universal quantum computing on continuous-variable (CV) platforms. This protocol is highly experimentally-friendly to the client, as it only requires Gaussian-operation capabilities from the latter. Moreover, the server is not required universal quantum-computational power either, its only function being to supply the client with copies of a single-mode non-Gaussian state. Universality is attained based on state-injection of the server's non-Gaussian supplies. The protocol is automatically blind because the non-Gaussian resource requested to the server is always the same, regardless of the specific computation. Verification, in turn, is possible thanks to an efficient non-Gaussian state fidelity test where we assume identical state preparation by the server. It is based on Gaussian measurements by the client on the injected states, which is potentially interesting on its own. The division of quantum hardware between client and server assumed here is in agreement with the experimental constraints expected in realistic schemes for CV cloud quantum computing. ","Client-friendly continuous-variable blind and verifiable quantum
  computing"
19,1011874938901458944,518919325,Antal Haans,['New paper available with @alecorbetta and others on the crowd steering effectiveness of arrow signage. Large scale field experiment (~270000 participants) @GlowEindhoven with high precision tracking of individual walking trajectories: <LINK> <LINK>'],http://arxiv.org/abs/1806.09801,"We introduce ""Moving Light"": an unprecedented real-life crowd steering experiment that involved about 140.000 participants among the visitors of the Glow 2017 Light Festival (Eindhoven, NL). Moving Light targets one outstanding question of paramount societal and technological importance: ""can we seamlessly and systematically influence routing decisions in pedestrian crowds?"" Establishing effective crowd steering methods is extremely relevant in the context of crowd management, e.g. when it comes to keeping floor usage within safety limits (e.g. during public events with high attendance) or at designated comfort levels (e.g. in leisure areas). In the Moving Light setup, visitors walking in a corridor face a choice between two symmetric exits defined by a large central obstacle. Stimuli, such as arrows, alternate at random and perturb the symmetry of the environment to bias choices. While visitors move in the experiment, they are tracked with high space and time resolution, such that the efficiency of each stimulus at steering individual routing decisions can be accurately evaluated a posteriori. In this contribution, we first describe the measurement concept in the Moving Light experiment and then we investigate quantitatively the steering capability of arrow indications. ",A large-scale real-life crowd steering experiment via arrow-like stimuli
20,1011681101306351616,230157762,Thorsten Sommer,['#Blockchain and #Education? We propose a #blockchain for the worldwide organization of student #achievements. Read our new paper: <LINK> Our goal is to encourage discussion in the communities. Spread the idea and start the discussion.'],https://arxiv.org/abs/1806.09335,"Staying abroad during their studies is increasingly popular for students. However, there are various challenges for both students and universities. One important question for students is whether or not achievements performed at different universities can be taken into account for either enrolling at a foreign university or for completing the studies at their home university. In addition to university achievements, an increasing proportion of the 195 million students worldwide increasingly receive certificates from MOOCs or other social media services. The integration of such services into university teaching is still in the initial stages and presents some challenges. In this paper we describe the idea to manage all these study achievements worldwide in a blockchain, which might solve the national and international challenges regarding the recognition of student achievements. The aim of this paper is to encourage discussion in the global community instead of presenting a finished concept. Some of the open research questions are: How to ensure student data protection, how to deal with fraud and how to deal with the possibility that students can analytically calculate the easiest way through their studies? ","Request for Comments: Proposal of a Blockchain for the Automatic
  Management and Acceptance of Student Achievements"
21,1011549383471575041,797888987675365377,Tom Rainforth,"['Check out our new paper ""Inference Trees: Adaptive Inference with Exploration"" <LINK>.  We introduce a completely new class of adaptive inference algorithms that uses ideas from Monte Carlo tree search and carries out target exploration of the parameter space <LINK>', '@jwvdm @yeewhye @frankdonaldwood @hyang144']",https://arxiv.org/abs/1806.09550,"We introduce inference trees (ITs), a new class of inference methods that build on ideas from Monte Carlo tree search to perform adaptive sampling in a manner that balances exploration with exploitation, ensures consistency, and alleviates pathologies in existing adaptive methods. ITs adaptively sample from hierarchical partitions of the parameter space, while simultaneously learning these partitions in an online manner. This enables ITs to not only identify regions of high posterior mass, but also maintain uncertainty estimates to track regions where significant posterior mass may have been missed. ITs can be based on any inference method that provides a consistent estimate of the marginal likelihood. They are particularly effective when combined with sequential Monte Carlo, where they capture long-range dependencies and yield improvements beyond proposal adaptation alone. ",Inference Trees: Adaptive Inference with Exploration
22,1011482990109646848,34709963,Daniel Lowd,"['New paper (w/ J. Ebrahimi + D. Dou):\n""On Adversarial Examples for Character-Level Neural Machine Translation""\n\nLearn how to break machine translation, one character at a time.\nArea Chair Favorite for #coling2018!\n\narXiv: <LINK> \nCode: <LINK>']",https://arxiv.org/abs/1806.09030,"Evaluating on adversarial examples has become a standard procedure to measure robustness of deep learning models. Due to the difficulty of creating white-box adversarial examples for discrete text input, most analyses of the robustness of NLP models have been done through black-box adversarial examples. We investigate adversarial examples for character-level neural machine translation (NMT), and contrast black-box adversaries with a novel white-box adversary, which employs differentiable string-edit operations to rank adversarial changes. We propose two novel types of attacks which aim to remove or change a word in a translation, rather than simply break the NMT. We demonstrate that white-box adversarial examples are significantly stronger than their black-box counterparts in different attack scenarios, which show more serious vulnerabilities than previously known. In addition, after performing adversarial training, which takes only 3 times longer than regular training, we can improve the model's robustness significantly. ",On Adversarial Examples for Character-Level Neural Machine Translation
23,1011425834006188032,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper is out. <LINK>  The QCD axion becomes ALP through the adiabatic conversion in the early Universe. Its contribution to DM is suppressed by the mass ratio, and the ALP coupling to photons can be enhanced by a few orders of magnitude. <LINK>']",https://arxiv.org/abs/1806.09551,"We revisit the adiabatic conversion between the QCD axion and axion-like particle (ALP) at level crossing, which can occur in the early universe as a result of the existence of a hypothetical mass mixing. This is similar to the Mikheyev-Smirnov-Wolfenstein effect in neutrino oscillations. After refining the conditions for the adiabatic conversion to occur, we focus on a scenario where the ALP produced by the adiabatic conversion of the QCD axion explains the observed dark matter abundance. Interestingly, we find that the ALP decay constant can be much smaller than the ordinary case in which the ALP is produced by the realignment mechanism. As a consequence, the ALP-photon coupling is enhanced by a few orders of magnitude, which is advantageous for the future ALP and axion-search experiments using the ALP-photon coupling. ","Enhanced photon coupling of ALP dark matter adiabatically converted from
  the QCD axion"
24,1011372857266298880,573729628,"Steve Taylor, PhD","['New paper out on constructing binary black-hole population models using population synthesis simulations 😀. We stitch together distributions from lots of simulations to constrain stellar progenitor conditions: <LINK>, <LINK>']",https://arxiv.org/abs/1806.08365,"Catalogs of stellar-mass compact binary systems detected by ground-based gravitational-wave instruments (such as Advanced LIGO and Advanced Virgo) will offer insights into the demographics of progenitor systems and the physics guiding stellar evolution. Existing techniques approach this through phenomenological modeling, discrete model selection, or model mixtures. Instead, we explore a novel technique that mines gravitational-wave catalogs to directly infer posterior probability distributions of the hyper-parameters describing formation and evolutionary scenarios (e.g. progenitor metallicity, kick parameters, and common-envelope efficiency). We use a bank of compact-binary population synthesis simulations to train a Gaussian-process emulator that acts as a prior on observed parameter distributions (e.g. chirp mass, redshift, rate). This emulator slots into a hierarchical population inference framework to extract the underlying astrophysical origins of systems detected by Advanced LIGO and Advanced Virgo. Our method is fast, easily expanded with additional simulations, and can be adapted for training on arbitrary population synthesis codes, as well as different detectors like LISA. ","Mining Gravitational-wave Catalogs To Understand Binary Stellar
  Evolution: A New Hierarchical Bayesian Framework"
25,1011354535720341504,2397275310,Valerie Higgins,"[""You can read about the efforts of the @Fermilab Archives to document the important contributions of non-scientific staff to the lab's work in my new paper on arXiv: <LINK>""]",https://arxiv.org/abs/1806.07338,"Science, especially large-scale basic research, is a collaborative endeavor, often drawing on the skills of people from a wide variety of disciplines. These people include not just scientists, but also administrators, engineers, and many others. Fermilab, a Department of Energy National Laboratory and the United States' premier particle physics laboratory, exemplifies this kind of research; many of its high-energy physics experiments involve hundreds of collaborators from all over the world. The Fermilab Archives seeks to document the history of the lab and the unique scientific research its staff and visitors perform. Adequately documenting the lab's work often requires us to go far beyond things like the writings and correspondence of scientists to also capture the administrative and social histories of the experiments and the context in which they were performed. At Fermilab, we have sought to capture these elements of the lab's activities through an oral history program that focuses on support staff as well as physicists and collection development choices that recognize the importance of records documenting the cultural life of the lab. These materials are not merely supplementary, but rather essential documentation of the many types of labor that go into the planning and execution of an experiment or the construction of an accelerator and the context in which this work is performed. ","It Takes a Village: Documenting the Contributions of Non-Scientific
  Staff to Scientific Research"
26,1011233057058287616,36396172,Diego R. Amancio,"['Our new preprint is out: ""Paragraph-based complex networks: application to document classification and authenticity verification"".  <LINK>. Paper with \u2066@hfarruda\u2069 et al.']",https://arxiv.org/abs/1806.08467,"With the increasing number of texts made available on the Internet, many applications have relied on text mining tools to tackle a diversity of problems. A relevant model to represent texts is the so-called word adjacency (co-occurrence) representation, which is known to capture mainly syntactical features of texts.In this study, we introduce a novel network representation that considers the semantic similarity between paragraphs. Two main properties of paragraph networks are considered: (i) their ability to incorporate characteristics that can discriminate real from artificial, shuffled manuscripts and (ii) their ability to capture syntactical and semantic textual features. Our results revealed that real texts are organized into communities, which turned out to be an important feature for discriminating them from artificial texts. Interestingly, we have also found that, differently from traditional co-occurrence networks, the adopted representation is able to capture semantic features. Additionally, the proposed framework was employed to analyze the Voynich manuscript, which was found to be compatible with texts written in natural languages. Taken together, our findings suggest that the proposed methodology can be combined with traditional network models to improve text classification tasks. ","Paragraph-based complex networks: application to document classification
  and authenticity verification"
27,1011163397403152384,270962976,Vincenzo Lomonaco,"['Our new paper ""Continuous Learning in Single-Incremental-Task Scenarios"" is out: <LINK>\n\nIn this work we propose a new, very light CL strategy denoted as ""AR1"" outperforming existing regularization strategies by a good margin on iCIFAR-100 and CORe50! <LINK>']",https://arxiv.org/abs/1806.08568,"It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in term of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin. ",Continuous Learning in Single-Incremental-Task Scenarios
28,1011054350255435776,15612654,Alan Stern,"['This is a review paper summarizing the first New Horizons Kuiper Belt Extended Mission, in the journal Space Science Reviews. Enjoy! \n\n<LINK> <LINK>', '@adam_ford1 Yes. Good until the 2030s!']",https://arxiv.org/abs/1806.08393,"The central objective of the New Horizons prime mission was to make the first exploration of Pluto and its system of moons. Following that, New Horizons has been approved for its first extended mission, which has the objectives of extensively studying the Kuiper Belt environment, observing numerous Kuiper Belt Objects (KBOs) and Centaurs in unique ways, and making the first close flyby of the KBO 486958 2014 MU69. This review summarizes the objectives and plans for this approved mission extension, and briefly looks forward to potential objectives for subsequent extended missions by New Horizons. ",The New Horizons Kuiper Belt Extended Mission
29,1011045264843530241,839913287240278020,Joey Rodriguez,"['New Paper Alert!: a discovery of a compact multi-planet system with a misaligned ultra short period planet. Up to six possible planets orbiting this late k-star. @NASAKepler @KeplerGO @GeertHub <LINK>', '#K2Mission']",https://arxiv.org/abs/1806.08368,"We report the discovery of a compact multi-planet system orbiting the relatively nearby (78pc) and bright ($K=8.9$) K-star, K2-266 (EPIC248435473). We identify up to six possible planets orbiting K2-266 with estimated periods of P$_b$ = 0.66, P$_{.02}$ = 6.1, P$_c$ = 7.8, P$_d$ = 14.7, P$_e$ = 19.5, and P$_{.06}$ = 56.7 days and radii of R$_P$ = 3.3 R$_{\oplus}$, 0.646 R$_{\oplus}$, 0.705 R$_{\oplus}$, 2.93 R$_{\oplus}$, 2.73 R$_{\oplus}$, and 0.90 R$_{\oplus}$, respectively. We are able to confirm the planetary nature of two of these planets (d & e) from analyzing their transit timing variations ($m_d= 8.9_{-3.8}^{+5.7} M_\oplus$ and $m_e=14.3_{-5.0}^{+6.4} M_\oplus$), confidently validate the planetary nature of two other planets (b & c), and classify the last two as planetary candidates (K2-266.02 & .06). From a simultaneous fit of all 6 possible planets, we find that K2-266 b's orbit has an inclination of 75.32$^{\circ}$ while the other five planets have inclinations of 87-90$^{\circ}$. This observed mutual misalignment may indicate that K2-266 b formed differently from the other planets in the system. The brightness of the host star and the relatively large size of the sub-Neptune sized planets d and e make them well-suited for atmospheric characterization efforts with facilities like the Hubble Space Telescope and upcoming James Webb Space Telescope. We also identify an 8.5-day transiting planet candidate orbiting EPIC248435395, a co-moving companion to K2-266. ","A Compact Multi-Planet System With A Significantly Misaligned Ultra
  Short Period Planet"
30,1010987227759894529,19120978,Bilal Farooq,['New paper from #LiTRANS on #DistractedPedestrian #TravelBehaviour using #VR published in #TransportResearchPartF. Preprint available at: <LINK>'],https://arxiv.org/abs/1806.06454,"A novel head-mounted virtual immersive/interactive reality environment (VIRE) is utilized to evaluate the behaviour of participants in three pedestrian road crossing conditions while 1) not distracted, 2) distracted with a smartphone, and 3) distracted with a smartphone with a virtually implemented safety measure on the road. Forty-two volunteers participated in our research who completed thirty successful (complete crossing) trials in blocks of ten trials for each crossing condition. For the two distracted conditions, pedestrians are engaged in a maze-solving game on a virtual smartphone, while at the same time checking the traffic for a safe crossing gap. For the proposed safety measure, smart flashing and color changing LED lights are simulated on the crosswalk to warn the distracted pedestrian who initiates crossing. Surrogate safety measures as well as speed information and distraction attributes such as direction and orientation of participant's head were collected and evaluated by employing a Multinomial Logit (MNL) model. Results from the model indicate that females have more dangerous crossing behaviour especially in distracted conditions; however, the smart LED treatment reduces this negative impact. Moreover, the number of times and the percentage of duration the head was facing the smartphone during a trial and a waiting time respectively increase the possibility of unsafe crossings; though, the proposed treatment reduces the safety crossing rate. Hence, our study shows that the smart LED light safety treatment indeed improves the safety of distracted pedestrians and enhances the successful crossing rate. ","Impact of Smartphone Distraction on Pedestrians' Crossing Behaviour: An
  Application of Head-Mounted Immersive Virtual Reality"
31,1010297258745745408,133148364,Devendra Chaplot,"['New paper on Learning Cognitive Models using Neural Networks on arxiv: <LINK>. We found that learning in neural networks is indicative of human learning based on student performance data in Intelligent Tutoring Systems. With C. MacLellan, @rsalakhu, K. Koedinger <LINK>']",https://arxiv.org/abs/1806.08065,"A cognitive model of human learning provides information about skills a learner must acquire to perform accurately in a task domain. Cognitive models of learning are not only of scientific interest, but are also valuable in adaptive online tutoring systems. A more accurate model yields more effective tutoring through better instructional decisions. Prior methods of automated cognitive model discovery have typically focused on well-structured domains, relied on student performance data or involved substantial human knowledge engineering. In this paper, we propose Cognitive Representation Learner (CogRL), a novel framework to learn accurate cognitive models in ill-structured domains with no data and little to no human knowledge engineering. Our contribution is two-fold: firstly, we show that representations learnt using CogRL can be used for accurate automatic cognitive model discovery without using any student performance data in several ill-structured domains: Rumble Blocks, Chinese Character, and Article Selection. This is especially effective and useful in domains where an accurate human-authored cognitive model is unavailable or authoring a cognitive model is difficult. Secondly, for domains where a cognitive model is available, we show that representations learned through CogRL can be used to get accurate estimates of skill difficulty and learning rate parameters without using any student performance data. These estimates are shown to highly correlate with estimates using student performance data on an Article Selection dataset. ",Learning Cognitive Models using Neural Networks
32,1010272600688349184,138046236,Carole Mundell,"['Superb new @nrao radio paper led by @Harvard Kate Alexander (<LINK>) ""An Unexpectedly Small Emission Region Size Inferred from Strong High-Frequency Diffractive Scintillation in GRB 161219B""  <LINK>']",https://arxiv.org/abs/1806.08017,"We present Karl G. Jansky Very Large Array radio observations of the long gamma-ray burst GRB 161219B ($z=0.147$) spanning $1-37$ GHz. The data exhibit unusual behavior, including sharp spectral peaks and minutes-timescale large-amplitude variability centered at $20$ GHz and spanning the full frequency range. We attribute this behavior to scattering of the radio emission by the turbulent ionized Galactic interstellar medium (ISM), including both diffractive and refractive scintillation. However, the scintillation is much stronger than predicted by a model of the Galactic electron density distribution (NE2001); from the measured variability timescale and decorrelation bandwidth we infer a scattering measure of $SM\approx {(8-70)\times 10^{-4}}$ kpc m$^{-20/3}$ (up to ${25}$ times larger than predicted in NE2001) and a scattering screen distance of $d_{\rm scr}\approx {0.2-3}$ kpc. We infer an emission region size of $\theta_s \approx {0.9-4}$ $\mu$as ($\approx {(1-4)}\times 10^{16}$ cm) at $\approx4$ days, and find that prior to 8 days the source size is an order of magnitude smaller than model predictions for a uniformly illuminated disk or limb-brightened ring, indicating a slightly off-axis viewing angle or significant substructure in the emission region. Simultaneous multi-hour broadband radio observations of future GRB afterglows will allow us to characterize the scintillation more completely, and hence to probe the observer viewing angle, the evolution of the jet Lorentz factor, the structure of the afterglow emission regions, and ISM turbulence at high Galactic latitudes. ","An Unexpectedly Small Emission Region Size Inferred from Strong
  High-Frequency Diffractive Scintillation in GRB 161219B"
33,1010248183291830272,121881387,Claudia Flores-Saviaga,"['Political trolls have been designing successful memes to support Trump on his immigration decisions. But, how do these trolls organize? Our new #ICWSM paper uncovers the importance of providing historic facts to drive participation <LINK> <LINK>']",https://arxiv.org/abs/1806.00429,"Political trolls initiate online discord not only for the lulz (laughs) but also for ideological reasons, such as promoting their desired political candidates. Political troll groups recently gained spotlight because they were considered central in helping Donald Trump win the 2016 US presidential election, which involved difficult mass mobilizations. Political trolls face unique challenges as they must build their own communities while simultaneously disrupting others. However, little is known about how political trolls mobilize sufficient participation to suddenly become problems for others. We performed a quantitative longitudinal analysis of more than 16 million comments from one of the most popular and disruptive political trolling communities, the subreddit /r/The\_Donald (T\D). We use T_D as a lens to understand participation and collective action within these deviant spaces. In specific, we first study the characteristics of the most active participants to uncover what might drive their sustained participation. Next, we investigate how these active individuals mobilize their community to action. Through our analysis, we uncover that the most active employed distinct discursive strategies to mobilize participation, and deployed technical tools like bots to create a shared identity and sustain engagement. We conclude by providing data-backed design implications for designers of civic media. ","Mobilizing the Trump Train: Understanding Collective Action in a
  Political Trolling Community"
34,1010193330435653634,268169845,Andrés Asensio Ramos,"['We uploaded to arxiv our new submitted paper in which we use deep learning to do real time image correction in solar physics, also with code <LINK>\n<LINK>']",https://arxiv.org/abs/1806.07150,"The quality of images of the Sun obtained from the ground are severely limited by the perturbing effect of the turbulent Earth's atmosphere. The post-facto correction of the images to compensate for the presence of the atmosphere require the combination of high-order adaptive optics techniques, fast measurements to freeze the turbulent atmosphere and very time consuming blind deconvolution algorithms. Under mild seeing conditions, blind deconvolution algorithms can produce images of astonishing quality. They can be very competitive with those obtained from space, with the huge advantage of the flexibility of the instrumentation thanks to the direct access to the telescope. In this contribution we leverage deep learning techniques to significantly accelerate the blind deconvolution process and produce corrected images at a peak rate of ~100 images per second. We present two different architectures that produce excellent image corrections with noise suppression while maintaining the photometric properties of the images. As a consequence, polarimetric signals can be obtained with standard polarimetric modulation without any significant artifact. With the expected improvements in computer hardware and algorithms, we anticipate that on-site real-time correction of solar images will be possible in the near future. ",Real-time multiframe blind deconvolution of solar images
35,1010063699808268288,16079444,Ying-Jer Kao,['New paper with Kai-Shin and Yi-Ping\nTunneling-induced restoration of classical degeneracy in quantum kagome  ice\n\n<LINK>'],https://arxiv.org/abs/1806.08145,"Quantum effect is expected to dictate the behaviour of physical systems at low temperature. For quantum magnets with geometrical frustration, quantum fluctuation usually lifts the macroscopic classical degeneracy, and exotic quantum states emerge. However, how different types of quantum processes entangle wave functions in a constrained Hilbert space is not well understood. Here, we study the topological entanglement entropy (TEE) and the thermal entropy of a quantum ice model on a geometrically frustrated kagome lattice. We find that the system does not show a $Z_2$ topological order down to extremely low temperature, yet continues to behave like a classical kagome ice with finite residual entropy. Our theoretical analysis indicates an intricate competition of off-diagonal and diagonal quantum processes leading to the quasi-degeneracy of states and effectively, the classical degeneracy is restored. ","Tunneling-induced restoration of classical degeneracy in quantum kagome
  ice"
36,1010053202128011264,157973000,Michael Pfarrhofer,['New paper on uncertainty shocks and state-level income inequality in the US.\n<LINK> @WU_econ'],https://arxiv.org/abs/1806.08278,"In this paper, we explore the relationship between state-level household income inequality and macroeconomic uncertainty in the United States. Using a novel large-scale macroeconometric model, we shed light on regional disparities of inequality responses to a national uncertainty shock. The results suggest that income inequality decreases in most states, with a pronounced degree of heterogeneity in terms of shapes and magnitudes of the dynamic responses. By contrast, some few states, mostly located in the West and South census region, display increasing levels of income inequality over time. We find that this directional pattern in responses is mainly driven by the income composition and labor market fundamentals. In addition, forecast error variance decompositions allow for a quantitative assessment of the importance of uncertainty shocks in explaining income inequality. The findings highlight that volatility shocks account for a considerable fraction of forecast error variance for most states considered. Finally, a regression-based analysis sheds light on the driving forces behind differences in state-specific inequality responses. ","The transmission of uncertainty shocks on income inequality: State-level
  evidence from the United States"
37,1009952110547529729,3333301606,"Keith Hawkins, Ph.D.","['The fastest moving stars in the Galaxy are flying thru space so fast that they are running away, escaping from the Galaxy. How a star can be accelerated to these speeds is not understood. In a new paper, I used chemical tagging to determine their origins!<LINK> <LINK>']",https://arxiv.org/abs/1806.07907,"The fastest moving stars provide insight into several fundamental properties of the Galaxy, including the escape velocity as a function of Galactocentric radius, the total mass, and the nature and frequency of stellar encounters with the central supermassive black hole. The recent second data release of Gaia has allowed the identification of new samples of stars with extreme velocities. Discrimination among the possible origins of these stars is facilitated by chemical abundance information. We here report the results from our high-resolution spectroscopic followup, using the Apache Point Observatory, of five late-type `hypervelocity' star candidates, characterised by total Galactic rest-frame velocities between 500-600 km/s and estimated to have a probability larger than 50% to be unbound from the Milky Way. Our new results confirm the Gaia DR2 radial velocities to within 1 km/s. We derived stellar atmospheric parameters and chemical abundances for several species including $\alpha$-elements (Mg, Ti, Si, Ca), Fe-peak elements (Fe, Ni, Co, Cr, Mn), neutron-capture elements (Sr, Y, Zr, Ba, La, Nd, Eu) and odd-Z elements (Na, Al, K, V, Cu, Sc). We find that all stars observed are metal-poor giants with -2 $\leq$ [Fe/H] $\leq$ -1 dex and are chemically indistinguishable from typical halo stars. Our results are supported by the chemical properties of four additional stars with extreme space motions which were observed by existing spectroscopic surveys. We conclude that these stars are simply the high-velocity tail of the stellar halo and effectively rule out more exotic origins such as from the Galactic centre or the Large Magellanic Cloud. ","The Fastest Travel Together: Chemical Tagging of the Fastest Stars in
  Gaia DR2 to the Stellar Halo"
38,1009812511191777280,101980926,Masahito Yamazaki,['My new paper on (rather unexpected) supersymmetry enhancement:\n<LINK>'],https://arxiv.org/abs/1806.07714,"We conjecture infrared emergent $\mathcal{N}=4$ supersymmetry for a class of three-dimensional $\mathcal{N}=2$ $U(1)$ gauge theories coupled with a single chiral multiplet. One example is the case where $U(1)$ gauge group has the Chern-Simons level $-\frac{3}2$ and the chiral multiplet has gauge charge $+1$. Other examples are related to this example either by known dualities or rescaling the Abelian gauge field. We give three independent evidences for the conjecture: 1) exact match between the central charges of the $U(1)$ R-symmetry current and the $U(1)$ topological symmetry current, 2) semi-classical construction of the $\mathcal{N}=4$ stress-tensor multiplet, and 3) an IR duality between a direct product of the two copies of the 3d theory on the one hand, and an $\mathcal{N}=4$ theory obtained by gauging the diagonal $SU(2)$ flavor symmetry of the $T[SU(2)]$ theory, on the other. The duality in 3) follows from geometrical aspects of the 3d-3d correspondence. ",Three-dimensional gauge theories with supersymmetry enhancement
39,1009791778264936448,185910194,Graham Neubig,"['New #ACL2018 paper on StructVAE, semi-supervised learning with structured latent variables: <LINK>\nA new shift-reduce method for seq2tree neural models, semantic parsing results robust to small data, and nice analysis of why semi-supervised learning works! <LINK>']",https://arxiv.org/abs/1806.07832,"Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semisupervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models. ","StructVAE: Tree-structured Latent Variable Models for Semi-supervised
  Semantic Parsing"
40,1009761473852133376,923132721760649216,kosukekurosawa,"['Our new paper has been accepted for publication in Icarus. <LINK>\nWe constructed a fully-analytical model to predict impact outcomes, including ejecta velocity distribution and transient crater radii under a given impact condition.']",https://arxiv.org/abs/1806.07665,"Impact craters are among the most prominent topographic features on planetary bodies. Crater scaling laws allow us to extract information about the impact histories on the host bodies. The pi-group scaling laws have been constructed based on the point-source approximation, dimensional analysis, and the results from impact experiments. Recent impact experiments, however, demonstrated that the scaling parameters themselves exhibits complex behavior against the change in the impact conditions and target properties. Here, we propose an alternative, fully analytical method to predict impact outcomes based on impact cratering mechanics. This approach is based on the Maxwell Z-model and the residual velocity. We present analytical expressions of (1) the proportionality relation between the ejection velocity and the ejection position, (2) the radius of a growing crater as a function of time, and (3) the transient crater radii. Since we focused on obtaining analytical solutions, a number of simplifications are employed. Due to the simplifications in the strength model, the accuracy of the prediction in the strength-dominated regime is relatively low. Our model reproduces the power-law behavior of the ejecta velocity distribution and the approximate time variation of a growing crater. The predicted radii under typical impact conditions mostly converge to a region between the two typical scaling lines by the pi-group scaling laws, strongly supporting the notion that the new method is one of the simplest ways to predict impact outcomes, as it provides analytical solutions. Our model could serve as a quick-look tool to estimate the impact outcome under a given set of conditions, and it might provide new insights into the nature of impact excavation processes. ","Impact cratering mechanics: A forward approach to predicting ejecta
  velocity distribution and transient crater radii"
41,1009729620231573506,23000769,Christopher Conselice,"['On astro-ph today we posted a paper where we argue that individual galaxy halo masses can be measured using a new hybrid theory/observational approach.    <LINK>', 'One result is the following -- the stellar mass to halo mass ratio is relatively constant up to z~3 which has implications for how galaxies acquire their dark and baryonic matter. https://t.co/OPnPLvnDx4', 'Another is that we find a third parameter in the relation between stellar and halo mass which is the time the bulk of the mass of a halo is formed. The earlier the formation, the higher the stellar to halo mass ratio. #DarkMatter #galaxies']",https://arxiv.org/abs/1806.07752,"We use a hybrid observational/theoretical approach to study the relation between galaxy kinematics and the derived stellar and halo masses of galaxies up to z=3 as a function of stellar mass, redshift and morphology. Our observational sample consists of a concatenation of 1125 galaxies with kinematic measurements at 0.4<z<3 from long-slit and integral-field studies. We investigate several ways to measure halo masses from observations based on results from semi-analytical models, showing that galaxy halo masses can be retrieved with a scatter of ~0.4 dex by using only stellar masses. We discover a third parameter, relating to the time of the formation of the halo, which reduces the scatter in the relation between the stellar and halo masses, such that systems forming earlier have a higher stellar mass to halo mass ratio, which we also find observationally. We find that this scatter correlates with morphology, such that early-type, or older stellar systems, have higher M_*/M_halo ratios. We furthermore show using this approach, and through weak lensing and abundance matching, that the ratio of stellar to halo mass does not significantly evolve with redshift at 1<z<3. This is evidence for the regulated hierarchical assembly of galaxies such that the ratio of stellar to dark matter mass remains approximately constant since $z = 2$. We use these results to show that the dark matter accretion rate evolves from $dM_{\rm halo}/d{\rm t} \sim 4000$ M_solar year$^{-1}$ at $z \sim 2.5$, to a few 100 M_solar year$^{-1}$ by $z \sim 0.5$. ","The Halo Masses of Galaxies to $z\sim 3$: A Hybrid Observational and
  Theoretical Approach"
42,1009718370047021056,2289472260,Ben Glocker,"[""Excited to see @KostasKamnitsas's new paper on semi-supervised learning (<LINK>) has been accepted as a long oral at #ICML2018 @icmlconf @BioMedIAICL @ICComputing \n@MSFTResearchCam <LINK> <LINK>""]",https://arxiv.org/abs/1806.02679,"We present a novel cost function for semi-supervised learning of neural networks that encourages compact clustering of the latent space to facilitate separation. The key idea is to dynamically create a graph over embeddings of labeled and unlabeled samples of a training batch to capture underlying structure in feature space, and use label propagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the graph that regularizes the latent space to form a single compact cluster per class, while avoiding to disturb existing clusters during optimization. We evaluate our approach on three benchmarks and compare to state-of-the art with promising results. Our approach combines the benefits of graph-based regularization with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily applied to existing networks to enable an effective use of unlabeled data. ",Semi-Supervised Learning via Compact Latent Space Clustering
43,1009615338944843781,901303999529312256,Quanquan Gu,"['Does there exist a faster algorithm than SVRG and SCSG for finding first-order stationary points in nonconvex optimization? Yes, check out our new paper on stochastic nested variance-reduced gradient methods (SNVRG)  at <LINK> <LINK>']",https://arxiv.org/abs/1806.07811,"We study finite-sum nonconvex optimization problems, where the objective function is an average of $n$ nonconvex functions. We propose a new stochastic gradient descent algorithm based on nested variance reduction. Compared with conventional stochastic variance reduced gradient (SVRG) algorithm that uses two reference points to construct a semi-stochastic gradient with diminishing variance in each iteration, our algorithm uses $K+1$ nested reference points to build a semi-stochastic gradient to further reduce its variance in each iteration. For smooth nonconvex functions, the proposed algorithm converges to an $\epsilon$-approximate first-order stationary point (i.e., $\|\nabla F(\mathbf{x})\|_2\leq \epsilon$) within $\tilde O(n\land \epsilon^{-2}+\epsilon^{-3}\land n^{1/2}\epsilon^{-2})$ number of stochastic gradient evaluations. This improves the best known gradient complexity of SVRG $O(n+n^{2/3}\epsilon^{-2})$ and that of SCSG $O(n\land \epsilon^{-2}+\epsilon^{-10/3}\land n^{2/3}\epsilon^{-2})$. For gradient dominated functions, our algorithm also achieves better gradient complexity than the state-of-the-art algorithms. Thorough experimental results on different nonconvex optimization problems back up our theory. ",Stochastic Nested Variance Reduction for Nonconvex Optimization
44,1009454717217443841,252867237,Juan Miguel Arrazola,['New paper from @XanaduAI: Continuous-variable quantum neural networks <LINK> \nIt includes my favourite figure ever on a paper I have worked on: images of tetrominos encoded in two-mode quantum states. <LINK>'],https://arxiv.org/abs/1806.06871,"We introduce a general method for building neural networks on quantum computers. The quantum neural network is a variational quantum circuit built in the continuous-variable (CV) architecture, which encodes quantum information in continuous degrees of freedom such as the amplitudes of the electromagnetic field. This circuit contains a layered structure of continuously parameterized gates which is universal for CV quantum computation. Affine transformations and nonlinear activation functions, two key elements in neural networks, are enacted in the quantum network using Gaussian and non-Gaussian gates, respectively. The non-Gaussian gates provide both the nonlinearity and the universality of the model. Due to the structure of the CV model, the CV quantum neural network can encode highly nonlinear transformations while remaining completely unitary. We show how a classical network can be embedded into the quantum formalism and propose quantum versions of various specialized model such as convolutional, recurrent, and residual networks. Finally, we present numerous modeling experiments built with the Strawberry Fields software library. These experiments, including a classifier for fraud detection, a network which generates Tetris images, and a hybrid classical-quantum autoencoder, demonstrate the capability and adaptability of CV quantum neural networks. ",Continuous-variable quantum neural networks
45,1009450537866223616,20916144,Cyrus Samii,"['W/ @MikeGechter, @RDehejia, and Pop-Eleches, new paper on formally assessing the quality of extrapolation-based policy recommendations derived from RCTs, structural estimates, &amp; other methods: <LINK> Work in progress, so comments welcomed.', '@JustinSandefur Here is the first version of the paper that we discussed. More additions coming soon, but this sets out the key ideas.']",https://arxiv.org/abs/1806.07016,"We derive a formal, decision-based method for comparing the performance of counterfactual treatment regime predictions using the results of experiments that give relevant information on the distribution of treated outcomes. Our approach allows us to quantify and assess the statistical significance of differential performance for optimal treatment regimes estimated from structural models, extrapolated treatment effects, expert opinion, and other methods. We apply our method to evaluate optimal treatment regimes for conditional cash transfer programs across countries where predictions are generated using data from experimental evaluations in other countries and pre-program data in the country of interest. ","Evaluating Ex Ante Counterfactual Predictions Using Ex Post Causal
  Inference"
46,1009422742842556416,21638577,Tom Runia,"['Our new journal paper ""Repetition Estimation"" is now available as preprint on arXiv. Come talk to us at the #CVPR2018 Image Motion session this Thursday! (with @cgmsnoek and Arnold Smeulders) <LINK> <LINK>']",https://arxiv.org/abs/1806.06984,"Visual repetition is ubiquitous in our world. It appears in human activity (sports, cooking), animal behavior (a bee's waggle dance), natural phenomena (leaves in the wind) and in urban environments (flashing lights). Estimating visual repetition from realistic video is challenging as periodic motion is rarely perfectly static and stationary. To better deal with realistic video, we elevate the static and stationary assumptions often made by existing work. Our spatiotemporal filtering approach, established on the theory of periodic motion, effectively handles a wide variety of appearances and requires no learning. Starting from motion in 3D we derive three periodic motion types by decomposition of the motion field into its fundamental components. In addition, three temporal motion continuities emerge from the field's temporal dynamics. For the 2D perception of 3D motion we consider the viewpoint relative to the motion; what follows are 18 cases of recurrent motion perception. To estimate repetition under all circumstances, our theory implies constructing a mixture of differential motion maps: gradient, divergence and curl. We temporally convolve the motion maps with wavelet filters to estimate repetitive dynamics. Our method is able to spatially segment repetitive motion directly from the temporal filter responses densely computed over the motion maps. For experimental verification of our claims, we use our novel dataset for repetition estimation, better-reflecting reality with non-static and non-stationary repetitive motion. On the task of repetition counting, we obtain favorable results compared to a deep learning alternative. ",Repetition Estimation
47,1009358916227133440,19729661,Dr Alice Harpole,"['New paper on arxiv with @IanHawke: <LINK>. We looked at whether a subsonic burning wave hitting a wall of fluid moving perpendicularly &amp; near the speed of light could become supersonic (technically yes, but unlikely to happen IRL).']",https://arxiv.org/abs/1806.07301,"Type I X-ray bursts are thermonuclear burning events which occur on the surfaces of accreting neutron stars. Burning begins in a localised spot in the star's ocean layer before propagating across the entire surface as a deflagration. On the scale of the entire star, the burning front can be thought of as discontinuity. To model this, we investigated the reactive Riemann problem for relativistic deflagrations and detonations and developed a numerical solver. Unlike for the Newtonian Riemann problem, where only the velocity perpendicular to the interface is relevant, in the relativistic case the tangential velocity becomes coupled through the Lorentz factor and can alter the waves present in the solution. We investigated whether a fast tangential velocity may be able to cause a deflagration wave to transition to a detonation. We found that such a transition is possible, but only for tangential velocities that are a significant fraction of the speed of light or for systems already on the verge of transitioning. Consequently, it is highly unlikely that this transition would occur for a burning front in a neutron star ocean without significant contributions from additional multidimensional effects. ","Effects of tangential velocity in the reactive relativistic Riemann
  problem"
48,1009227980982218752,3222736855,Reed Lab,"['A preprint of our new paper testing an underdominance system 200 generations after it was established is available on arxiv: ""Robust, safe, and reversible ribosomal protein gene drive""\n<LINK>']",https://arxiv.org/abs/1806.05304,"Despite the advent of several novel, synthetic gene drive mechanisms and their potential to one-day control a number of devastating diseases, among other applications, practical use of these systems remains contentious and risky. In particular, there is little in the way of empirical evidence of the long-term robustness of these synthetic systems against mutational breakdown. Rather, most existing systems are either known or predicted to be susceptible to rapid inactivation, though methodological designs continue to be refined. Here we evaluate a currently existing synthetic, underdominance-based gene drive system 200+ generations after it was first established in a laboratory colony of Drosophila melanogaster. Not only do we find that the system is still functioning as designed, we also show evidence that disruptions to the genetic construct are highly likely to be removed by natural selection, contributing to the system's robust, long-term stability. This stability appears to be a result of a fundamental relationship between ribosomal proteins (a novel target of the system) and natural cellular defenses that protect against cancer development. As far as we are aware, this is the longest continually functioning synthetic gene drive system thus verified, making it highly appropriate for additional research into its eventual suitability for field trials. Due to inherent properties of this gene drive, it is also likely to be adaptable for use in many different species. The insect lines established and used to test this system have been deposited at a Drosophila stock center, and are available to labs for further, independent testing. ","RPM-Drive: A robust, safe, and reversible gene drive system that remains
  functional after 200+ generations"
49,1009124417996247040,106843613,Jacob Haqq Misra,['Just over 200 more years until our technology starts to directly heat the planet (as long as global energy consumption rates continue to grow) #thermodynamics #anthropocene\n\nRead more in my new paper with @BrendanLMullan \n<LINK> <LINK>'],https://arxiv.org/abs/1806.06474,"Von Hoerner (1975) examined the effects of human population growth and predicted agricultural, environmental, and other problems from observed growth rate trends. Using straightforward calculations, VH75 predicted the ""doomsday"" years for these scenarios (2020-2050), when we as a species should run out of space or food, or induce catastrophic anthropogenic climate change through thermodynamically unavoidable direct heating of the planet. Now that over four decades have passed, in this paper we update VH75. We perform similar calculations as that work, with improved data and trends in population growth, food production, energy use, and climate change. For many of the impacts noted in VH75 our work amounts to pushing the ""doomsday"" horizon back to the 2300s-2400s (or much further for population-driven interstellar colonization). This is largely attributable to using worldwide data that exhibit smaller growth rates of population and energy use in the last few decades. While population-related catastrophes appear less likely than in VH75, our continued growth in energy use provides insight into possible future issues. We find that, if historic trends continue, direct heating of the Earth will be a substantial contributor to climate change by about 2260, regardless of the energy source used, coincident with our transition to a Kardashev type-I civilization. We also determine that either an increase of Earth's global mean temperature of 12K will occur or an unreasonably high fraction of the planet will need to be covered by solar collectors by about 2400 to keep pace with our growth in energy use. We further discuss the implications in terms of interstellar expansion, the transition to type II and III civilizations, SETI, and the Fermi Paradox. We conclude that the ""sustainability solution"" to the Fermi Paradox is a compelling possibility. ","Population Growth, Energy Use, and the Implications for the Search for
  Extraterrestrial Intelligence"
50,1009120965513629696,106843613,Jacob Haqq Misra,"['Climate on an #exomoon would be drastically different than Earth, with infrared radiation from the giant planet complementing sunlight as an energy source. \n\nRead more in my new paper with @DrReneHeller\n<LINK>']",https://arxiv.org/abs/1806.06822,"Recent studies have shown that large exomoons can form in the accretion disks around super-Jovian extrasolar planets. These planets are abundant at about 1 AU from Sun-like stars, which makes their putative moons interesting for studies of habitability. Technological advances could soon make an exomoon discovery with Kepler or the upcoming CHEOPS and PLATO space missions possible. Exomoon climates might be substantially different from exoplanet climates because the day-night cycles on moons are determined by the moon's synchronous rotation with its host planet. Moreover, planetary illumination at the top of the moon's atmosphere and tidal heating at the moon's surface can be substantial, which can affect the redistribution of energy on exomoons. Using an idealized general circulation model with simplified hydrologic, radiative, and convective processes, we calculate surface temperature, wind speed, mean meridional circulation, and energy transport on a 2.5 Mars-mass moon orbiting a 10-Jupiter-mass at 1 AU from a Sun-like star. The strong thermal irradiation from a young giant planet causes the satellite's polar regions to warm, which remains consistent with the dynamically-driven polar amplification seen in Earth models that lack ice-albedo feedback. Thermal irradiation from young, luminous giant planets onto water-rich exomoons can be strong enough to induce water loss on a planet, which could lead to a runaway greenhouse. Moons that are in synchronous rotation with their host planet and do not experience a runaway greenhouse could experience substantial polar melting induced by the polar amplification of planetary illumination and geothermal heating from tidal effects. ","Exploring exomoon atmospheres with an idealized general circulation
  model"
51,1009088433418067970,3885912072,Marshall Johnson,"['New paper alert! <LINK> In this paper we report the discovery, confirmation, and characterization of two new giant planets using the @NASAKepler K2 extended mission.', 'EPIC 246911830 b is a hot Jupiter transiting a late F star. Although the host star is relatively faint, there are a number of interesting things about the system. First, there is a likely stellar companion, probably a mid-M dwarf at a projected separation of about 400 AU. https://t.co/yjPlh6Rxkl', 'Second, it is the first K2 hot Jupiter with a secondary eclipse detected in the K2 light curve. This allowed us to show that the planet is relatively reflective--it has a geometric albedo of ~0.2. https://t.co/1J3J2iZLK7', 'The other planet is EPIC 201498078 b, a warm Saturn transiting a relatively bright (V=10.5), 8.8-billion-year-old G star at the main sequence turn-off. With an 11.6-day orbit, this is one of the brighter K2 host stars with a long-period planet. https://t.co/Fn71cQ8exn', ""These discoveries were made by the KESPRINT collaboration, &amp; wouldn't have been possible without a lot of great work by other members of the collaboration (most aren't on Twitter, but including @oscaribv &amp; @vaneylenv ), &amp; thanksto @skyientist &amp; @WtnNori for follow-up lightcurves"", 'And thanks to @justesen for all of his work on the stellar characterization!']",https://arxiv.org/abs/1806.06099,"We present the discovery and confirmation of two new transiting giant planets from the Kepler extended mission K2. K2-260 b is a hot Jupiter transiting a $V=12.7$ F6V star in K2 Field 13, with a mass and radius of $M_{\star}=1.39_{-0.06}^{+0.05} M_{\odot}$ and $R_{\star}=1.69 \pm 0.03 R_{\odot}$. The planet has an orbital period of $P=2.627$ days, and a mass and radius of $M_P=1.42^{+0.31}_{-0.32} M_J$ and $R_P=1.552^{+0.048}_{-0.057} R_J$. This is the first K2 hot Jupiter with a detected secondary eclipse in the Kepler bandpass, with a depth of $71 \pm 15$ ppm, which we use to estimate a geometric albedo of $A_g\sim0.2$. We also detected a candidate stellar companion at 0.6"" from K2-260; we find that it is very likely physically associated with the system, in which case it would be an M5-6V star at a projected separation of $\sim400$ AU. K2-261 b is a warm Saturn transiting a bright ($V=10.5$) G7IV/V star in K2 Field 14. The host star is a metal-rich ([Fe/H]$=0.36 \pm 0.06$), mildly evolved $1.10_{-0.02}^{+0.01} M_{\odot}$ star with $R_{\star}=1.65 \pm 0.04 R_{\odot}$. Thanks to its location near the main sequence turn-off, we can measure a relatively precise age of $8.8_{-0.3}^{+0.4}$ Gyr. The planet has $P=11.633$ days, $M_P=0.223 \pm 0.031 M_J$, and $R_P=0.850^{+0.026}_{-0.022} R_J$, and its orbit is eccentric ($e=0.39 \pm 0.15$). Its brightness and relatively large transit depth make this one of the best known warm Saturns for follow-up observations to further characterize the planetary system. ","K2-260 b: a hot Jupiter transiting an F star, and K2-261 b: a warm
  Saturn around a bright G star"
52,1008986390464483328,120285811,Rob Crain,"[""Exciting new paper from the E-MOSAICS collaboration between @UniHeidelberg and @LJMU. You can use the MW's globular clusters to reconstruct its assembly history: <LINK>"", '(cc @LJMU_Astro, @LJMUResearch)']",https://arxiv.org/abs/1806.05680,"We use the age-metallicity distribution of 96 Galactic globular clusters (GCs) to infer the formation and assembly history of the Milky Way (MW), culminating in the reconstruction of its merger tree. Based on a quantitative comparison of the Galactic GC population to the 25 cosmological zoom-in simulations of MW-mass galaxies in the E-MOSAICS project, which self-consistently model the formation and evolution of GC populations in a cosmological context, we find that the MW assembled quickly for its mass, reaching $\{25,50\}\%$ of its present-day halo mass already at $z=\{3,1.5\}$ and half of its present-day stellar mass at $z=1.2$. We reconstruct the MW's merger tree from its GC age-metallicity distribution, inferring the number of mergers as a function of mass ratio and redshift. These statistics place the MW's assembly $\textit{rate}$ among the 72th-94th percentile of the E-MOSAICS galaxies, whereas its $\textit{integrated}$ properties (e.g. number of mergers, halo concentration) match the median of the simulations. We conclude that the MW has experienced no major mergers (mass ratios $>$1:4) since $z\sim4$, sharpening previous limits of $z\sim2$. We identify three massive satellite progenitors and constrain their mass growth and enrichment histories. Two are proposed to correspond to Sagittarius (few $10^8~{\rm M}_\odot$) and the GCs formerly associated with Canis Major ($\sim10^9~{\rm M}_\odot$). The third satellite has no known associated relic and was likely accreted between $z=0.6$-$1.3$. We name this enigmatic galaxy $\textit{Kraken}$ and propose that it is the most massive satellite ($M_*\sim2\times10^9~{\rm M}_\odot$) ever accreted by the MW. We predict that $\sim40\%$ of the Galactic GCs formed ex-situ (in galaxies with masses $M_*=2\times10^7$-$2\times10^9~{\rm M}_\odot$), with $6\pm1$ being former nuclear clusters. ","The formation and assembly history of the Milky Way revealed by its
  globular cluster population"
53,1008946714844323840,1008316626012459008,Robert Schulz,['The new paper is now available on arXiv 😀 <LINK>'],https://arxiv.org/abs/1806.06653,"The energetic feedback that is generated by radio jets in active galactic nuclei (AGNs) has been suggested to be able to produce fast outflows of atomic hydrogen (HI) gas that can be studied in absorption at high spatial resolution. We have used the Very Large Array (VLA) and a global very-long-baseline-interferometry (VLBI) array to locate and study in detail the HI outflow discovered with the Westerbork Synthesis Radio Telescope (WSRT) in the re-started radio galaxy 3C 236. We confirm, from the VLA data, the presence of a blue-shifted wing of the HI with a width of $\sim1000\mathrm{\,km\,s^{-1}}$. This HI outflow is partially recovered by the VLBI observation. In particular, we detect four clouds with masses of $0.28\text{-}1.5\times 10^4M_\odot$ with VLBI that do not follow the regular rotation of most of the HI. Three of these clouds are located, in projection, against the nuclear region on scales of $\lesssim 40\mathrm{\,pc}$, while the fourth is co-spatial to the south-east lobe at a projected distance of $\sim270\mathrm{\,pc}$. Their velocities are between $150$ and $640\mathrm{\,km\,s^{-1}}$ blue-shifted with respect to the velocity of the disk-related HI. These findings suggest that the outflow is at least partly formed by clouds, as predicted by some numerical simulations and originates already in the inner (few tens of pc) region of the radio galaxy. Our results indicate that all of the outflow could consist of many clouds with perhaps comparable properties as the ones detected, distributed also at larger radii from the nucleus where the lower brightness of the lobe does not allow us to detect them. However, we cannot rule out the presence of a diffuse component of the outflow. The fact that 3C 236 is a low excitation radio galaxy, makes it less likely that the optical AGN is able to produce strong radiative winds leaving the radio jet as the main driver for the HI outflow. ","Mapping the neutral atomic hydrogen gas outflow in the restarted radio
  galaxy 3C 236"
54,1008940585372151809,39525395,Aditya Grover,['Check out our new @icmlconf paper on algorithms and applications for unsupervised representation learning of policies in multiagent systems: <LINK> w/ @alshedivat @rejuvyesh #YuraBurda #HarrisonEdwards @OpenAI Accepted as a long talk at #ICML2018!'],http://arxiv.org/abs/1806.06464,"Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems. Prior work in agent modeling has largely been task-specific and driven by hand-engineering domain-specific prior knowledge. We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a representation learning problem. Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies. We demonstrate empirically the utility of the proposed framework in (i) a challenging high-dimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning. ",Learning Policy Representations in Multiagent Systems
55,1008904075318923265,22399655,Ryota Kanai💡,"['Our new paper from Araya by @ildefons and myself, now on arXiv.\n ""A unified strategy for implementing curiosity and empowerment driven reinforcement learning""\n<LINK>']",https://arxiv.org/abs/1806.06505v1,"Although there are many approaches to implement intrinsically motivated artificial agents, the combined usage of multiple intrinsic drives remains still a relatively unexplored research area. Specifically, we hypothesize that a mechanism capable of quantifying and controlling the evolution of the information flow between the agent and the environment could be the fundamental component for implementing a higher degree of autonomy into artificial intelligent agents. This paper propose a unified strategy for implementing two semantically orthogonal intrinsic motivations: curiosity and empowerment. Curiosity reward informs the agent about the relevance of a recent agent action, whereas empowerment is implemented as the opposite information flow from the agent to the environment that quantifies the agent's potential of controlling its own future. We show that an additional homeostatic drive is derived from the curiosity reward, which generalizes and enhances the information gain of a classical curious/heterostatic reinforcement learning agent. We show how a shared internal model by curiosity and empowerment facilitates a more efficient training of the empowerment function. Finally, we discuss future directions for further leveraging the interplay between these two intrinsic rewards. ","] A unified strategy for implementing curiosity and empowerment driven
  reinforcement learning"
56,1008883946879414272,35346624,Jaroslaw Zola,['New paper with Marina Blanton @iamskaran and Ah Reum Kang on privacy-preserving ML analysis of distributed medical data <LINK> We outline the platform and show how it can be used to learn BN while protecting data without modifying it'],http://arxiv.org/abs/1806.06477,"Objective: To enable privacy-preserving learning of high quality generative and discriminative machine learning models from distributed electronic health records. Methods and Results: We describe general and scalable strategy to build machine learning models in a provably privacy-preserving way. Compared to the standard approaches using, e.g., differential privacy, our method does not require alteration of the input biomedical data, works with completely or partially distributed datasets, and is resilient as long as the majority of the sites participating in data processing are trusted to not collude. We show how the proposed strategy can be applied on distributed medical records to solve the variables assignment problem, the key task in exact feature selection and Bayesian networks learning. Conclusions: Our proposed architecture can be used by health care organizations, spanning providers, insurers, researchers and computational service providers, to build robust and high quality predictive models in cases where distributed data has to be combined without being disclosed, altered or otherwise compromised. ",Privacy Preserving Analytics on Distributed Medical Data
57,1008876058651787264,901303999529312256,Quanquan Gu,"['Can Adam generalize as well as SGD with momentum while still training faster? No, but Padam can. Check out our new paper: Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks：<LINK>']",https://arxiv.org/abs/1806.06763,"Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, despite the nice property of fast convergence, have been observed to generalize worse than stochastic gradient descent (SGD) with momentum in training deep neural networks. This leaves how to close the generalization gap of adaptive gradient methods an open problem. In this work, we show that adaptive gradient methods such as Adam, Amsgrad, are sometimes ""over adapted"". We design a new algorithm, called Partially adaptive momentum estimation method, which unifies the Adam/Amsgrad with SGD by introducing a partial adaptive parameter $p$, to achieve the best from both worlds. We also prove the convergence rate of our proposed algorithm to a stationary point in the stochastic nonconvex optimization setting. Experiments on standard benchmarks show that our proposed algorithm can maintain a fast convergence rate as Adam/Amsgrad while generalizing as well as SGD in training deep neural networks. These results would suggest practitioners pick up adaptive gradient methods once again for faster training of deep neural networks. ","Closing the Generalization Gap of Adaptive Gradient Methods in Training
  Deep Neural Networks"
58,1008874374361780224,2956121356,Russ Salakhutdinov,"['New #icml2018 paper: Gated Path Planning Networks <LINK>\nMore importantly, Lisa Lee @rl_agent is also releasing the code replicating experiments in the paper!\n<LINK> <LINK>']",https://arxiv.org/abs/1806.06408,"Value Iteration Networks (VINs) are effective differentiable path planning modules that can be used by agents to perform navigation while still maintaining end-to-end differentiability of the entire architecture. Despite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other optimization problems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs couple recurrent convolutions with an unconventional max-pooling activation. From this perspective, we argue that standard gated recurrent update equations could potentially alleviate the optimization issues plaguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to empirically outperform VIN on a variety of metrics such as learning speed, hyperparameter sensitivity, iteration count, and even generalization. Furthermore, we show that this performance gap is consistent across different maze transition types, maze sizes and even show success on a challenging 3D environment, where the planner is only provided with first-person RGB images. ",Gated Path Planning Networks
59,1008700708303130626,1020088099,Umberto Picchini,"['new paper by Samuel Wiqvist, Julie Forman and myself on accelerating delayed-acceptance MCMC for expensive targets. <LINK> This is the first of a series of nine tweets. Keep scrolling... \\1 <LINK>', 'this is the first paper by Samuel Wiqvist. *Congratulations* Samuel, and thanks for all the hard work! Follow him at @samuel_wiqvist for more goodies  \\2', 'Delayed-acceptance MCMC (DA-MCMC) is a well studied methodology to accelerate MCMC sampling from arbitrary distributions, when the target is computationally expensive. Of course this is relevant for Bayesian inference with expensive likelihoods.  \\3', 'We provide an accelerated (and approximate) delayed acceptance algorithm named ADA-MCMC. While regular DA-MCMC has two-stages, where stage 1 rejects ""bad"" proposals without using the expensive likelihood, and only uses the latter at second stage to accept/or reject...  \\4', 'instead ADA-MCMC finds ways to (sometimes) avoid the expensive likelihood also at the second stage. That is a proposal can be accepted without computing the expensive likelihood. This introduces approximation in the sampling but also an acceleration in the computations. \\5', 'How worth is using ADA-MCMC?  In a case study on Bayesian inference, we access the expensive likelihood function from 2 to 6 times less often than in standard DA-MCMC.  \\6', 'Will this provide a significant speed up? If the likelihood is very expensive, definitely yes. If it is instead fairly cheap to evaluate then the acceleration is negligible. So try it out if your target is expensive. Our likelihoods were not super expensive. Performances vary  \\7', 'The good news is that ADA-MCMC uses the same constructs you would need to run a standard DA-MCMC, with little overhead computations. So there is not much work to be added \\8', 'You may also be interested in our application to protein folding data (25,000 observations) using stochastic differential equations, and comparisons to some kind of pseudo-marginal (particle) MCMC methods. \\9']",https://arxiv.org/abs/1806.05982,"Delayed-acceptance Markov chain Monte Carlo (DA-MCMC) samples from a probability distribution via a two-stages version of the Metropolis-Hastings algorithm, by combining the target distribution with a ""surrogate"" (i.e. an approximate and computationally cheaper version) of said distribution. DA-MCMC accelerates MCMC sampling in complex applications, while still targeting the exact distribution. We design a computationally faster, albeit approximate, DA-MCMC algorithm. We consider parameter inference in a Bayesian setting where a surrogate likelihood function is introduced in the delayed-acceptance scheme. When the evaluation of the likelihood function is computationally intensive, our scheme produces a 2-4 times speed-up, compared to standard DA-MCMC. However, the acceleration is highly problem dependent. Inference results for the standard delayed-acceptance algorithm and our approximated version are similar, indicating that our algorithm can return reliable Bayesian inference. As a computationally intensive case study, we introduce a novel stochastic differential equation model for protein folding data. ",Accelerating delayed-acceptance Markov chain Monte Carlo algorithms
60,1008682116555657216,855945227718230016,Tarraneh Eftekhari,"['New paper led by Ben Margalit on the engines of fast radio bursts, super-luminous supernovae, and gamma-ray bursts out on the arxiv today! <LINK>']",https://arxiv.org/abs/1806.05690,"Young, rapidly spinning magnetars are invoked as central engines behind a diverse set of transient astrophysical phenomena, including gamma-ray bursts (GRB), super-luminous supernovae (SLSNe), fast radio bursts (FRB), and binary neutron star (NS) mergers. However, a barrier to direct confirmation of the magnetar hypothesis is the challenge of directly observing non-thermal emission from the central engine at early times (when it is most powerful and thus detectable) due to the dense surrounding ejecta. We present CLOUDY calculations of the time-dependent evolution of the temperature and ionization structure of expanding supernova or merger ejecta due to photo-ionization by a magnetar engine, in order to study the escape of X-rays (absorbed by neutral gas) and radio waves (absorbed by ionized gas), as well as to assess the evolution of the local dispersion measure due to photo-ionization. We find that ionization breakout does not occur if the engine's ionizing luminosity decays rapidly, and that X-rays typically escape the oxygen-rich ejecta of SLSNe only on $\sim 100 \, {\rm yr}$ timescales, consistent with current X-ray non-detections. We apply these results to constrain engine-driven models for the binary NS merger GW170817 and the luminous transient ASASSN-15lh. In terms of radio transparency and dispersion measure constraints, the repeating FRB 121102 is consistent with originating from a young, $\gtrsim 30-100 \, {\rm yr}$, magnetar similar to those inferred to power SLSNe. We further show that its high rotation measure can be produced within the same nebula that is proposed to power the quiescent radio source observed co-located with FRB 121102. Our results strengthen previous work suggesting that at least some FRBs may be produced by young magnetars, and motivate further study of engine powered transients. ","Unveiling the Engines of Fast Radio Bursts, Super-Luminous Supernovae,
  and Gamma-Ray Bursts"
61,1008518473474084866,15362595,BenneHolwerda,['DEVILS is in the redshifts. <LINK> new paper!'],https://arxiv.org/abs/1806.05808,"The Deep Extragalactic VIsible Legacy Survey (DEVILS) is a large spectroscopic campaign at the Anglo-Australian Telescope (AAT) aimed at bridging the near and distant Universe by producing the highest completeness survey of galaxies and groups at intermediate redshifts ($0.3<z<1.0$). Our sample consists of $\sim$60,000 galaxies to Y$<$21.2mag, over $\sim$6deg$^{2}$ in three well-studied deep extragalactic fields (Cosmic Origins Survey field, COSMOS, Extended Chandra Deep Field South, ECDFS and the X-ray Multi-Mirror Mission Large-Scale Structure region, XMM-LSS - all Large Synoptic Survey Telescope deep-drill fields). This paper presents the broad experimental design of DEVILS. Our target sample has been selected from deep Visible and Infrared Survey Telescope for Astronomy (VISTA) Y-band imaging (VISTA Deep Extragalactic Observations, VIDEO and UltraVISTA), with photometry measured by ProFound. Photometric star/galaxy separation is done on the basis of NIR colours, and has been validated by visual inspection. To maximise our observing efficiency for faint targets we employ a redshift feedback strategy, which continually updates our target lists, feeding back the results from the previous night's observations. We also present an overview of the initial spectroscopic observations undertaken in late 2017 and early 2018. ","Deep Extragalactic VIsible Legacy Survey (DEVILS): Motivation, Design
  and Target Catalogue"
62,1007701382974115840,783061931334766596,Kay L. Kirkpatrick \🥄\🥄,"['We just posted a new paper on the arxiv: <LINK>', '@Eric0Lawton Please let me know if there’s anything I can try to explain :)', '@Eric0Lawton I have recent bio/philosophy oriented papers if you’re interested...', '@Eric0Lawton I can send them by email']",https://arxiv.org/abs/1806.05282,"Lattice spin models in statistical physics are used to understand magnetism. Their Hamiltonians are a discrete form of a version of a Dirichlet energy, signifying a relationship to the Harmonic map heat flow equation. The Gibbs distribution, defined with this Hamiltonian, is used in the Metropolis-Hastings (M-H) algorithm to generate dynamics tending towards an equilibrium state. In the limiting situation when the inverse temperature is large, we establish the relationship between the discrete M-H dynamics and the continuous Harmonic map heat flow associated with the Hamiltonian. We show the convergence of the M-H dynamics to the Harmonic map heat flow equation in two steps: First, with fixed lattice size and proper choice of proposal size in one M-H step, the M-H dynamics acts as gradient descent and will be shown to converge to a system of Langevin stochastic differential equations (SDE). Second, with proper scaling of the inverse temperature in the Gibbs distribution and taking the lattice size to infinity, it will be shown that this SDE system converges to the deterministic Harmonic map heat flow equation. Our results are not unexpected, but show remarkable connections between the M-H steps and the SDE Stratonovich formulation, as well as reveal trajectory-wise out of equilibrium dynamics to be related to a canonical PDE system with geometric constraints. ",Limiting Behaviors of High Dimensional Stochastic Spin Ensembles
63,1007551169206652928,174298756,Adel Bibi,['New paper out about improving SAGA with an optimal interpolation with GD and minibatch SAGA. We also provide a way to compute the optimal minibatch for SAGA with a linear speed up in batch size.\n<LINK>'],https://arxiv.org/abs/1806.05633,"We develop and analyze a new algorithm for empirical risk minimization, which is the key paradigm for training supervised machine learning models. Our method---SAGD---is based on a probabilistic interpolation of SAGA and gradient descent (GD). In particular, in each iteration we take a gradient step with probability $q$ and a SAGA step with probability $1-q$. We show that, surprisingly, the total expected complexity of the method (which is obtained by multiplying the number of iterations by the expected number of gradients computed in each iteration) is minimized for a non-trivial probability $q$. For example, for a well conditioned problem the choice $q=1/(n-1)^2$, where $n$ is the number of data samples, gives a method with an overall complexity which is better than both the complexity of GD and SAGA. We further generalize the results to a probabilistic interpolation of SAGA and minibatch SAGA, which allows us to compute both the optimal probability and the optimal minibatch size. While the theoretical improvement may not be large, the practical improvement is robustly present across all synthetic and real data we tested for, and can be substantial. Our theoretical results suggest that for this optimal minibatch size our method achieves linear speedup in minibatch size, which is of key practical importance as minibatch implementations are used to train machine learning models in practice. Moreover, empirical evidence suggest that a linear speedup in minibatch size can be attained with a parallel implementation. ",Improving SAGA via a Probabilistic Interpolation with Gradient Descent
64,1007423146125914113,2800204849,Andrew Gordon Wilson,"['Improving Consistency-Based Semi-Supervised Learning with Weight Averaging\nOur new paper (+code!): <LINK> \nBy analyzing loss geometry, we achieve record semi-supervised results, including 95% accuracy on CIFAR-10 with only 4000 labels! <LINK>']",https://arxiv.org/abs/1806.05594,"Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to the previous best result in the literature of 6.3%. ","There Are Many Consistent Explanations of Unlabeled Data: Why You Should
  Average"
65,1007382589425045505,929791330519322624,Black in AI,"['Congratulations to Ousmane Dia, AI Researcher at @element_ai and colleagues for their new paper on Bayesian Model-Agnostic Meta-Learning \nPaper: <LINK>']",https://arxiv.org/abs/1806.03836,"Learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning due to the model uncertainty inherent in the problem. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines scalable gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. During fast adaptation, the method is capable of learning complex uncertainty structure beyond a point estimate or a simple Gaussian approximation. In addition, a robust Bayesian meta-update mechanism with a new meta-loss prevents overfitting during meta-update. Remaining an efficient gradient-based meta-learner, the method is also model-agnostic and simple to implement. Experiment results show the accuracy and robustness of the proposed method in various tasks: sinusoidal regression, image classification, active learning, and reinforcement learning. ",Bayesian Model-Agnostic Meta-Learning
66,1007153130310701056,776765039726460929,Carlo Felice Manara,"['Another paper today, lead by the great Feng Long, with @GregHerczeg @ilaria_pascucci @danielapai @GijsMulders myself and others: <LINK> \nNew deeper observations of ChaI disks with @almaobs lead to new detections and to understanding of properties of fainter disks']",https://arxiv.org/abs/1806.04826,"ALMA surveys of nearby star-forming regions have shown that the dust mass in the disk is correlated with the stellar mass, but with a large scatter. This scatter could indicate either different evolutionary paths of disks or different initial conditions within a single cluster. We present ALMA Cycle 3 follow-up observations for 14 Class II disks that were low S/N detections or non-detections in our Cycle 2 survey of the $\sim 2$ Myr-old Chamaeleon I star-forming region. With 5 times better sensitivity, we detect millimeter dust continuum emission from six more sources and increase the detection rate to 94\% (51/54) for Chamaeleon I disks around stars earlier than M3. The stellar-disk mass scaling relation reported in \citet{pascucci2016} is confirmed with these updated measurements. Faint outliers in the $F_{mm}$--$M_*$ plane include three non-detections (CHXR71, CHXR30A, and T54) with dust mass upper limits of 0.2 M$_\oplus$ and three very faint disks (CHXR20, ISO91, and T51) with dust masses $\sim 0.5$ M$_\oplus$. By investigating the SED morphology, accretion property and stellar multiplicity, we suggest for the three millimeter non-detections that tidal interaction by a close companion ($<$100 AU) and internal photoevaporation may play a role in hastening the overall disk evolution. The presence of a disk around only the secondary star in a binary system may explain the observed stellar SEDs and low disk masses for some systems. ","An ALMA Survey of faint disks in the Chamaeleon I star-forming region:
  Why are some Class II disks so faint?"
67,1006920075402719234,151193108,Mert R. Sabuncu 🇺🇦,['Machine learning models are increasingly applied to resting-state functional MRI data. We propose a new way to use convolutional neural network architectures for connectome-based prediction. We report excellent accuracy on ABIDE. Paper under review: <LINK>'],https://arxiv.org/abs/1806.04209,"Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a diagnostic or prognostic tool for a wide variety of conditions, such as autism, Alzheimer's disease, and stroke. While a growing number of studies have demonstrated the promise of machine learning algorithms for rs-fMRI based clinical or behavioral prediction, most prior models have been limited in their capacity to exploit the richness of the data. For example, classification techniques applied to rs-fMRI often rely on region-based summary statistics and/or linear models. In this work, we propose a novel volumetric Convolutional Neural Network (CNN) framework that takes advantage of the full-resolution 3D spatial structure of rs-fMRI data and fits non-linear predictive models. We showcase our approach on a challenging large-scale dataset (ABIDE, with N > 2,000) and report state-of-the-art accuracy results on rs-fMRI-based discrimination of autism patients and healthy controls. ","3D Convolutional Neural Networks for Classification of Functional
  Connectomes"
68,1006873190507974656,2613619922,byron wallace,"[""EBM-NLP: A new corpus to support work on NLP for evidence-based medicine (5k clinical trial abstracts w/detailed annotations). ACL '18 paper: <LINK>. Corpus website: <LINK>. w/Ben Nye @ani_nenkova @ijmarshall @996roma @yinfeiy @jessyjli""]",https://arxiv.org/abs/1806.04185,"We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the `PICO' elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine. ","A Corpus with Multi-Level Annotations of Patients, Interventions and
  Outcomes to Support Language Processing for Medical Literature"
69,1006762157978071040,10666172,Sabine Hossenfelder,['We have a new paper: Predicting Citation Counts with a Neural Network <LINK> <LINK>'],https://arxiv.org/abs/1806.04641,We here describe and present results of a simple neural network that predicts individual researchers' future citation counts based on a variety of data from the researchers' past. For publications available on the open access-server arXiv.org we find a higher predictability than previous studies. ,Predicting Citation Counts with a Neural Network
70,1006710712910139393,948010010,Antonis Anastasopoulos,"['We collected a new #nlp corpus: 114 stories in an endangered language, Griko, with translations in Italian.\nTime to work on the extremely low-resource end!\nWe started with POS tagging (the test set has gold annotations!) in our #coling2018 resource paper\n<LINK>', 'You can download the resource here: https://t.co/L9m4Ym50Q0']",https://arxiv.org/abs/1806.03757,"Most work on part-of-speech (POS) tagging is focused on high resource languages, or examines low-resource and active learning settings through simulated studies. We evaluate POS tagging techniques on an actual endangered language, Griko. We present a resource that contains 114 narratives in Griko, along with sentence-level translations in Italian, and provides gold annotations for the test set. Based on a previously collected small corpus, we investigate several traditional methods, as well as methods that take advantage of monolingual data or project cross-lingual POS tags. We show that the combination of a semi-supervised method with cross-lingual transfer is more appropriate for this extremely challenging setting, with the best tagger achieving an accuracy of 72.9%. With an applied active learning scheme, which we use to collect sentence-level annotations over the test set, we achieve improvements of more than 21 percentage points. ","Part-of-Speech Tagging on an Endangered Language: a Parallel
  Griko-Italian Resource"
71,1006699758755426304,2935607927,Peter Hull,"['New(ish) working paper with Kirill Borusyak and Xavier Jaravel: ""Quasi-experimental Shift-share Research Designs"" \n\n<LINK>\n\nComments very welcome! A brief summary thread follows <LINK>', 'This paper updates a May 2017 note on how to think about identification in ""shift-share"" or ""Bartik"" IVs as coming from quasi-random variation in the aggregate shocks \n\nAlthough this seems to often be the intuition behind shift-share IV, we haven\'t yet seen it formalized https://t.co/FNnPy4yT9T', 'We first show that even though Bartik IV is estimated in the ""location"" space, its validity condition can be expressed in the ""industry"" space (i.e. the space of shocks g). \n\nFor shift-share IV validity, the covariance of g and relevant unobservables should be close to zero. https://t.co/UOxCdZ16A0', 'We then derive sufficient shock-level assumptions for this condition to hold. We think of shocks as being as-good-as-randomly assigned, with the impact of each shock shrinking as the sample grows\n\nWe also consider extensions with conditionally random shocks and panel variation https://t.co/KnSIVVUByQ', 'Lastly we caution that estimating aggregate shocks in the IV sample can make shift-share IV inconsistent, even when the shocks are random \n\nThis probably is analogous to the classic inconsistency of many-instrument 2SLS and can similarly be solved by split-sample shock estimation https://t.co/ZUFgWRh3cA', 'We hope this helps clarify how one may think of identification in shift-share IVs as ""coming from"" a natural shock experiment, and conclude with some practical advice for such settings\n\nOf course this is work in progress, so its helpfulness is up to you! Please send your thoughts']",https://arxiv.org/abs/1806.01221,"Many studies use shift-share (or ``Bartik'') instruments, which average a set of shocks with exposure share weights. We provide a new econometric framework for shift-share instrumental variable (SSIV) regressions in which identification follows from the quasi-random assignment of shocks, while exposure shares are allowed to be endogenous. The framework is motivated by an equivalence result: the orthogonality between a shift-share instrument and an unobserved residual can be represented as the orthogonality between the underlying shocks and a shock-level unobservable. SSIV regression coefficients can similarly be obtained from an equivalent shock-level regression, motivating shock-level conditions for their consistency. We discuss and illustrate several practical insights of this framework in the setting of Autor et al. (2013), estimating the effect of Chinese import competition on manufacturing employment across U.S. commuting zones. ",Quasi-Experimental Shift-Share Research Designs
72,1006520093466136576,2869101210,Jenn Wortman Vaughan,"[""Can a bandit algorithm's exploration process lead to unfair outcomes? New COLT paper with Manish Raghavan, Alex Slivkins, and @zstevenwu now on arXiv.\n\n<LINK>""]",https://arxiv.org/abs/1806.00543,"Online learning algorithms, widely used to power search and content optimization on the web, must balance exploration and exploitation, potentially sacrificing the experience of current users for information that will lead to better decisions in the future. Recently, concerns have been raised about whether the process of exploration could be viewed as unfair, placing too much burden on certain individuals or groups. Motivated by these concerns, we initiate the study of the externalities of exploration - the undesirable side effects that the presence of one party may impose on another - under the linear contextual bandits model. We introduce the notion of a group externality, measuring the extent to which the presence of one population of users impacts the rewards of another. We show that this impact can in some cases be negative, and that, in a certain sense, no algorithm can avoid it. We then study externalities at the individual level, interpreting the act of exploration as an externality imposed on the current user of a system by future users. This drives us to ask under what conditions inherent diversity in the data makes explicit exploration unnecessary. We build on a recent line of work on the smoothed analysis of the greedy algorithm that always chooses the action that currently looks optimal, improving on prior results to show that a greedy approach almost matches the best possible Bayesian regret rate of any other algorithm on the same problem instance whenever the diversity conditions hold, and that this regret is at most $\tilde{O}(T^{1/3})$. Returning to group-level effects, we show that under the same conditions, negative group externalities essentially vanish under the greedy algorithm. Together, our results uncover a sharp contrast between the high externalities that exist in the worst case, and the ability to remove all externalities if the data is sufficiently diverse. ","The Externalities of Exploration and How Data Diversity Helps
  Exploitation"
73,1006513802588934145,842036161585545217,Richard McKinley,['New paper with Andreas Hess (@4h355) coming out of the @ScanNeuroradBE : Synthetic Perfusion Maps: Imaging Perfusion Deficits in DSC-MRI with\n  Deep Learning  <LINK>'],http://arxiv.org/abs/1806.03848,"In this work, we present a novel convolutional neural net- work based method for perfusion map generation in dynamic suscepti- bility contrast-enhanced perfusion imaging. The proposed architecture is trained end-to-end and solely relies on raw perfusion data for inference. We used a dataset of 151 acute ischemic stroke cases for evaluation. Our method generates perfusion maps that are comparable to the target maps used for clinical routine, while being model-free, fast, and less noisy. ","Synthetic Perfusion Maps: Imaging Perfusion Deficits in DSC-MRI with
  Deep Learning"
74,1006468060608385025,2676457430,MAGIC telescopes 🌴🌺,"['new MAGIC paper accepted in A&amp;A!This work is focused on the radio galaxy NGC 1275, the  central galaxy of the Perseus cluster, and on its fast variability in gamma-rays.\n<LINK>\npicture credit: Hubble/ESA <LINK>']",https://arxiv.org/abs/1806.01559,"We report on the detection of flaring activity from the Fanaroff-Riley I radio galaxy NGC 1275 in very-high-energy (VHE, E $>$ 100 GeV) gamma rays with the MAGIC telescopes. Observations were performed between 2016 September and 2017 February as part of a monitoring program. The brightest outburst with $\sim1.5$ times the Crab Nebula flux above 100 GeV (C.U.) was observed during the night between 2016 December 31 and 2017 January 1 (fifty times higher than the mean previously measured in two observational campaigns between 2009 and 2011). Significant variability of the day-by-day light curve was measured, the shortest flux-doubling time-scales was found to be of $(611\pm101)$ min. The combined spectrum of the MAGIC data during the strongest flare state and simultaneous data from the Fermi-LAT around 2017 January 1 follows a power-law with an exponential cutoff at the energy $(492\pm35)$ GeV. Simultaneous optical flux density measurements in the R-band obtained with the KVA telescope are also presented and the correlation between the optical and gamma-ray emission is investigated. Due to possible internal pair-production, the fast flux variability constrains the Doppler factor to values which are inconsistent with a large viewing angle as observed in the radio band. We investigate different scenarios for the explanation of fast gamma-ray variability, namely emission from: magnetospheric gaps, relativistic blobs propagating in the jet (mini-jets) or external cloud (or star) entering the jet. We find that the only plausible model to account for the luminosities here observed would be the production of gamma rays in a magnetospheric gap around the central black hole only in the eventuality of an enhancement of the magnetic field threading the hole from its equipartition value with the gas pressure in the accretion flow. ",Gamma-ray flaring activity of NGC 1275 in 2016-2017 measured by MAGIC
75,1006382116593168389,322636963,Jonathan Berant,"[""Check my student Mor's new #coling2018 paper. We train an agent to navigate in long Wikipedia pages using the structure of the page to answer questions. <LINK> #taunlp <LINK>""]",https://arxiv.org/abs/1806.03529,"Reading comprehension models are based on recurrent neural networks that sequentially process the document tokens. As interest turns to answering more complex questions over longer documents, sequential reading of large portions of text becomes a substantial bottleneck. Inspired by how humans use document structure, we propose a novel framework for reading comprehension. We represent documents as trees, and model an agent that learns to interleave quick navigation through the document tree with more expensive answer extraction. To encourage exploration of the document tree, we propose a new algorithm, based on Deep Q-Network (DQN), which strategically samples tree nodes at training time. Empirically we find our algorithm improves question answering performance compared to DQN and a strong information-retrieval (IR) baseline, and that ensembling our model with the IR baseline results in further gains in performance. ",Learning to Search in Long Documents Using Document Structure
76,1006336750300459008,759118366468481024,Decker French,"['New paper today! <LINK> A thread:', 'We used stellar population modeling to study the recent starbursts in post-starburst galaxies. Now that the starbursts have ended, we can characterize how much stellar mass was produced, whether there was one burst or two, and the duration of the starburst.', 'But, the most interesting thing we can determine is the time elapsed since the starburst ended (the “post-starburst age”).', 'Comparing the ages of post-starburst galaxies lets us put them on a timeline after the starburst. We used the ages of post-starburst galaxies with molecular gas measurements from my 2015 sample, @astrokatey’s sample, and @kerowlands’s sample. https://t.co/kY4TXtrgI2', 'We see a significant decline in the molecular gas fraction after the starburst. This is cool for two reasons:', '(1) the decline rate is too fast to be accounted for by star-formation, even assuming a heavy impact from stellar feedback. It looks like AGN feedback might be acting during this phase.', 'Others have seen AGN driving outflows and winds before, but this is the first time we’ve seen the molecular gas being truly *depleted* from the galaxy as a whole!', '(2) The decline in gas fraction solves the problem of how post-starburst galaxies could look like early types in a few Gyr, which they otherwise will. Given the current gas depletion rates, the post-starbursts could resemble early type galaxies in their gas properties in 1-2 Gyr.', 'Aside from the ages, we also find some neat trends in the starburst properties. Lower stellar mass post-starbursts have stronger starbursts and are more likely to have had multiple recent bursts.', 'This might be because stellar feedback is stronger in lower mass galaxies, or that lower mass galaxies are less likely to have a bulge to stabilize the gas during a merger first passage.', 'If you want to read more, the main results are in section 4 (the starburst properties) and section 5 (the molecular gas trends).']",https://arxiv.org/abs/1806.03301,"Detailed modeling of the recent star formation histories (SFHs) of post-starburst (or ""E+A"") galaxies is impeded by the degeneracy between the time elapsed since the starburst ended (post-burst age), the fraction of stellar mass produced in the burst (burst strength), and the burst duration. To resolve this issue, we combine GALEX ultraviolet photometry, SDSS photometry and spectra, and new stellar population synthesis models to fit the SFHs of 532 post-starburst galaxies. In addition to an old stellar population and a recent starburst, 48% of the galaxies are best fit with a second recent burst. Lower stellar mass galaxies (log M$_\star$/M$_\odot<10.5$) are more likely to experience two recent bursts, and the fraction of their young stellar mass is more strongly anti-correlated with their total stellar mass. Applying our methodology to other, younger post-starburst samples, we identify likely progenitors to our sample and examine the evolutionary trends of molecular gas and dust content with post-burst age. We discover a significant (4$\sigma$) decline, with a 117-230 Myr characteristic depletion time, in the molecular gas to stellar mass fraction with the post-burst age. The implied rapid gas depletion rate of 2-150 M$_\odot$yr$^{-1}$ cannot be due to current star formation, given the upper limits on the current SFRs in these post-starbursts. Nor are stellar winds or SNe feedback likely to explain this decline. Instead, the decline points to the expulsion or destruction of molecular gas in outflows, a possible smoking gun for AGN feedback. ","Clocking the Evolution of Post-Starburst Galaxies: Methods and First
  Results"
77,1006308638540132352,2800204849,Andrew Gordon Wilson,"['Our new paper, Probabilistic FastText for Multi-Sense Word Embeddings, is appearing as an oral at #ACL2018, with code! <LINK>\nWe learn density embeddings that account for sub-word structure and multiple senses. Joint with Ben Athiwaratkun and @AnimaAnandkumar! <LINK>']",https://arxiv.org/abs/1806.02901,"We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share statistical strength across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, the proposed model is the first to achieve multi-sense representations while having enriched semantics on rare words. ",Probabilistic FastText for Multi-Sense Word Embeddings
78,1006219231900229633,8143682,Jure Leskovec,['Graph Convolutional Neural Networks for Web-Scale Recommender Systems. Our new #KDD2018 paper with @PinterestEng. <LINK>'],https://arxiv.org/abs/1806.01973,"Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures. ",Graph Convolutional Neural Networks for Web-Scale Recommender Systems
79,1006190571407462401,2805395378,Sofia Moschou,['New paper on astro-ph: <LINK>\nWhen can a CME escape? Suppressed .vs. Escaping CMEs! <LINK>'],https://arxiv.org/abs/1806.02828,"We present results from a set of numerical simulations aimed at exploring the mechanism of coronal mass ejection (CME) suppression in active stars by an overlying large-scale magnetic field. We use a state-of-the-art 3D magnetohydrodynamic (MHD) code which considers a self-consistent coupling between an Alfv\'en wave-driven stellar wind solution, and a first-principles CME model based on the eruption of a flux-rope anchored to a mixed polarity region. By replicating the driving conditions used in simulations of strong solar CMEs, we show that a large-scale dipolar magnetic field of $75$ G is able to fully confine eruptions within the stellar corona. Our simulations also consider CMEs exceeding the magnetic energy used in solar studies, which are able to escape the large-scale magnetic field confinement. The analysis includes a qualitative and quantitative description of the simulated CMEs and their dynamics, which reveals a drastic reduction of the radial speed caused by the overlying magnetic field. With the aid of recent observational studies, we place our numerical results in the context of solar and stellar flaring events. In this way, we find that this particular large-scale magnetic field configuration establishes a suppression threshold around $\sim$$3 \times 10^{32}$~erg in the CME kinetic energy. Extending the solar flare-CME relations to other stars, such CME kinetic energies could be typically achieved during erupting flaring events with total energies larger than $6 \times 10^{32}$ erg (GOES class $\sim$X70). ","Suppression of Coronal Mass Ejections in active stars by an overlying
  large-scale magnetic field: A numerical study"
80,1006115349119537152,814077886374232064,Enrico Gavagnin,['Our new paper about the invasion speed of cell migration models with realistic cell cycle representations is now out on arXiv! @Kit_Yates_Maths <LINK> <LINK>'],https://arxiv.org/abs/1806.03140,"Cell proliferation is typically incorporated into stochastic mathematical models of cell migration by assuming that cell divisions occur after an exponentially distributed waiting time. Experimental observations, however, show that this assumption is often far from the real cell cycle time distribution (CCTD). Recent studies have suggested an alternative approach to modelling cell proliferation based on a multi-stage representation of the CCTD. In order to validate and parametrise these models, it is important to connect them to experimentally measurable quantities. In this paper we investigate the connection between the CCTD and the speed of the collective invasion. We first state a result for a general CCTD, which allows the computation of the invasion speed using the Laplace transform of the CCTD. We use this to deduce the range of speeds for the general case. We then focus on the more realistic case of multi-stage models, using both a stochastic agent-based model and a set of reaction-diffusion equations for the cells' average density. By studying the corresponding travelling wave solutions, we obtain an analytical expression for the speed of invasion for a general N-stage model with identical transition rates, in which case the resulting cell cycle times are Erlang distributed. We show that, for a general N-stage model, the Erlang distribution and the exponential distribution lead to the minimum and maximum invasion speed, respectively. This result allows us to determine the range of possible invasion speeds in terms of the average proliferation time for any multi-stage model. ","The invasion speed of cell migration models with realistic cell cycle
  time distributions"
81,1006042556432601089,59083454,Dr. Rosita Kokotanekova,['*New paper* with @colinsnodgrass @pedrolacerda <LINK> We look for period changes of 3 large comets and discuss why large nuclei tend to survive longer. We also find hints that the level of surface erosions of JFCs could be distinguished using ground observations.'],https://arxiv.org/abs/1806.02897,"Rotational spin-up due to outgassing of comet nuclei has been identified as a possible mechanism for considerable mass-loss and splitting. We report a search for spin changes for three large Jupiter-family comets (JFCs): 14P/Wolf, 143P/Kowal-Mrkos, and 162P/Siding Spring. None of the three comets has detectable period changes, and we set conservative upper limits of 4.2 (14P), 6.6 (143P) and 25 (162P) minutes per orbit. Comparing these results with all eight other JFCs with measured rotational changes, we deduce that none of the observed large JFCs experiences significant spin changes. This suggests that large comet nuclei are less likely to undergo rotationally-driven splitting, and therefore more likely to survive more perihelion passages than smaller nuclei. We find supporting evidence for this hypothesis in the cumulative size distributions of JFCs and dormant comets, as well as in recent numerical studies of cometary orbital dynamics. We added 143P to the sample of 13 other JFCs with known albedos and phase-function slopes. This sample shows a possible correlation of increasing phase-function slopes for larger geometric albedos. Partly based on findings from recent space missions to JFCs, we hypothesise that this correlation corresponds to an evolutionary trend for JFCs. We propose that newly activated JFCs have larger albedos and steeper phase functions, which gradually decrease due to sublimation-driven erosion. If confirmed, this could be used to analyse surface erosion from ground and to distinguish between dormant comets and asteroids. ","Implications of the Small Spin Changes Measured for Large Jupiter-Family
  Comet Nuclei"
82,1006003780737957889,989160650327908353,Elad Hazan,['How to implement provable full-matrix Adaptive Regularization efficiently?  A new paper from Princeton Brain team:\n<LINK>'],https://arxiv.org/abs/1806.02958,"Adaptive regularization methods pre-multiply a descent direction by a preconditioning matrix. Due to the large number of parameters of machine learning problems, full-matrix preconditioning methods are prohibitively expensive. We show how to modify full-matrix adaptive regularization in order to make it practical and effective. We also provide a novel theoretical analysis for adaptive regularization in non-convex optimization settings. The core of our algorithm, termed GGT, consists of the efficient computation of the inverse square root of a low-rank matrix. Our preliminary experiments show improved iteration-wise convergence rates across synthetic tasks and standard deep learning benchmarks, and that the more carefully-preconditioned steps sometimes lead to a better solution. ",Efficient Full-Matrix Adaptive Regularization
83,1005860721912139777,2614166177,Lorenz A. Gilch 🇪🇺,['It seems that @FIFAcom changed to Elo-System: <LINK>\nmaybe due to our new #FifaWorldCup2018 research paper <LINK> which gives mathematical-statistical prediction models @Know_Center'],https://arxiv.org/abs/1806.01930,We propose an approach for the analysis and prediction of a football championship. It is based on Poisson regression models that include the Elo points of the teams as covariates and incorporates differences of team-specific effects. These models for the prediction of the FIFA World Cup 2018 are fitted on all football games on neutral ground of the participating teams since 2010. Based on the model estimates for single matches Monte-Carlo simulations are used to estimate probabilities for reaching the different stages in the FIFA World Cup 2018 for all teams. We propose two score functions for ordinal random variables that serve together with the rank probability score for the validation of our models with the results of the FIFA World Cups 2010 and 2014. All models favor Germany as the new FIFA World Champion. All possible courses of the tournament and their probabilities are visualized using a single Sankey diagram. ,On Elo based prediction models for the FIFA Worldcup 2018
84,1005845563911364614,23724401,Daniel Jiang,"['New paper with Yijia Wang on using a ""structured actor-critic"" method for solving a public health inventory (e.g., naloxone) dispensing problem: <LINK> <LINK>']",https://arxiv.org/abs/1806.02490,"Public health organizations face the problem of dispensing treatments (i.e., vaccines, antibiotics, and others) to groups of affected populations through ""points-of-dispensing"" (PODs) during emergency situations, typically in the presence of complexities like demand stochasticity, heterogenous utilities (e.g., for vaccine distribution, certain segments of the population may need to be prioritized), and limited storage. We formulate a hierarchical Markov decision process (MDP) model with two levels of decisions (and decision-makers): the upper-level decisions come from an inventory planner that ""controls"" a lower-level dynamic problem, which optimizes dispensing decisions that take into consideration the heterogeneous utility functions of the random set of PODs. We then derive structural properties of the MDP model and propose an approximate dynamic programming (ADP) algorithm that leverages structure in both the policy and the value space (state-dependent basestocks and concavity, respectively). The algorithm can be considered an actor-critic method; to our knowledge, this paper is the first to jointly exploit policy and value structure within an actor-critic framework. We prove that the policy and value function approximations each converge to their optimal counterparts with probability one and provide a comprehensive numerical analysis showing improved empirical convergence rates when compared to other ADP techniques. Finally, we show how an aggregation-based version of our algorithm can be applied in a realistic case study for the problem of dispensing naloxone (an overdose reversal drug) via first responders amidst the ongoing opioid crisis. ",Structured Actor-Critic for Managing Public Health Points-of-Dispensing
85,1005251141200957440,19510090,Julian Togelius,"[""You know what's better than video games for testing your RL algorithms? Why, a framework where you can easily make new games and levels! You can now use General Video Game AI @gvgai with @OpenAI gym. In a first paper, we test deep RL algos on GVGAI games.\n<LINK> <LINK>"", 'There are over 100 games in the @gvgai repository, and new ones are made every time we run a new round of the competition. You can make your own new games or levels in mere minutes, and you can easily make perturbations to existing games or levels, or write your own generators!', 'A large number of planning agents exists for the @gvgai games, and in our new paper we compare the performance of deep RL agents with planning agents. In one case we have a forward model with no training time, and vice versa. Results? Yes, they are interesting. Read the paper :)', 'The new learning track of the @gvgai competition challenges you to develop a learning agent that learns to play a game it has not seen before. So you can not tailor your learning algorithm to the specific game.\nhttps://t.co/1rZEqFSPUy', ""That's right, no testing on the training set."", ""There's also the level generation track, which is about generating good levels for existing games, and the rule generation track, which is about generating game rules for existing levels. And of course the planning track (single player or two-player)."", 'The new paper is by Ruben Rodriguez Torrado, @FilipoGiovanni, @_JialinLiu, @diego_pliebana and myself.\nhttps://t.co/yq4ehVfyPr\nWe see this as a platform for much new research to come, as it enables RL research beyond fixed sets of games with fixed levels.', 'Other key contributors to the wider @gvgai project include @squashstar, @b_gum22, @spysamot, @Amidos2006 and Tom Schaul.', ""@MLFreak1 @gvgai @OpenAI There's plenty of research on AI for generating levels and adapting to the player - see https://t.co/nm5CZyfXmS""]",https://arxiv.org/abs/1806.02448,"The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search. In this paper, we describe how we interface GVGAI to the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of GVGAI games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions. ",Deep Reinforcement Learning for General Video Game AI
86,1005197869085753346,925019749456719872,Leilani,"['Our new review paper on ""explaining explanations"" provides new insights, definitions, and best practices for explainable AI and interpretability moving forward.  <LINK>']",https://arxiv.org/abs/1806.00069,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence. ","Explaining Explanations: An Overview of Interpretability of Machine
  Learning"
87,1005134970120294402,890454487,Karl Tuyls,['Our new paper on Evaluation is on arXiv: <LINK>\n\nCheck it out! With @dbalduzzi'],https://arxiv.org/abs/1806.02643,"Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents. ",Re-evaluating Evaluation
88,1005094065522794497,176191683,Olga Zamora,['Our new @APOGEEsurvey paper on M-dwarfs: 44 New &amp; Known M Dwarf Multiples\n<LINK>'],https://arxiv.org/abs/1806.02395,"Binary stars make up a significant portion of all stellar systems. Consequently, an understanding of the bulk properties of binary stars is necessary for a full picture of star formation. Binary surveys indicate that both multiplicity fraction and typical orbital separation increase as functions of primary mass. Correlations with higher order architectural parameters such as mass ratio are less well constrained. We seek to identify and characterize double-lined spectroscopic binaries (SB2s) among the 1350 M dwarf ancillary science targets with APOGEE spectra in the SDSS-III Data Release 13. We measure the degree of asymmetry in the APOGEE pipeline cross-correlation functions (CCFs), and use those metrics to identify a sample of 44 high-likelihood candidate SB2s. At least 11 of these SB2s are known, having been previously identified by Deshapnde et al, and/or El Badry et al. We are able to extract radial velocities (RVs) for the components of 36 of these systems from their CCFs. With these RVs, we measure mass ratios for 29 SB2s and 5 SB3s. We use Bayesian techniques to fit maximum likelihood (but still preliminary) orbits for 4 SB2s with 8 or more distinct APOGEE observations. The observed (but incomplete) mass ratio distribution of this sample rises quickly towards unity. Two-sided Kolmogorov-Smirnov tests and probabilities of 18.3% and 18.7%, demonstrating that the mass ratio distribution of our sample is consistent with those measured by Pourbaix et al. and Fernandez et al., respectively. ","44 New & Known M Dwarf Multiples In The SDSS-III/APOGEE M Dwarf
  Ancillary Science Sample"
89,1005069574662959104,322586399,Adam Glos,['New paper concerning spectral similarities between @barabasi-Albert  model and Chung-Lu model. It seems they are not so different.   <LINK> <LINK>'],https://arxiv.org/abs/1806.01569,"In the paper we have analyzed spectral similarity between Barab\'asi-Albert and Chung-lu models. We have shown the similarity of spectral distribution for sufficiently large Barab\'asi-Albert parameter value. Contrary, extreme eigenvalues and principal eigenvector are not similar for those model. We provide applications of obtained results related to the spectral graph theory and efficiency of quantum spatial search ",Spectral similarity for Barab\'asi-Albert and Chung-Lu models
90,1005001401041346560,933826478038544384,Mark Williams,['Another *new* paper submission from the LHCb charm physics group. What is the lifetime of the recently discovered doubly-charmed-doubly-charged baryon? Should be long-lived as it decays weakly - results confirm this! <LINK> @LHCbExperiment @LHCbPhysics <LINK>'],https://arxiv.org/abs/1806.02744,"The first measurement of the lifetime of the doubly charmed baryon $\Xi_{cc}^{++}$ is presented, with the signal reconstructed in the final state $\Lambda_c^+ K^- \pi^+ \pi^+$. The data sample used corresponds to an integrated luminosity of $1.7\,\mathrm{fb}^{-1}$, collected by the LHCb experiment in proton-proton collisions at a centre-of-mass energy of $13\mathrm{\,Te\kern -0.1em V}$. The $\Xi_{cc}^{++}$ lifetime is measured to be $0.256\,^{+0.024}_{-0.022}{\,\rm (stat)\,} \pm 0.014 {\,\rm(syst)}\mathrm{\,ps}$. ",Measurement of the lifetime of the doubly charmed baryon $\Xi_{cc}^{++}$
91,1004988853210505221,960703608361644032,Simon Graham,"['Have a read of our new MILD-Net paper, to be presented at the exciting @midl_amsterdam conference <LINK>']",https://arxiv.org/abs/1806.01963,"The analysis of glandular morphology within colon histopathology images is an important step in determining the grade of colon cancer. Despite the importance of this task, manual segmentation is laborious, time-consuming and can suffer from subjectivity among pathologists. The rise of computational pathology has led to the development of automated methods for gland segmentation that aim to overcome the challenges of manual segmentation. However, this task is non-trivial due to the large variability in glandular appearance and the difficulty in differentiating between certain glandular and non-glandular histological structures. Furthermore, a measure of uncertainty is essential for diagnostic decision making. To address these challenges, we propose a fully convolutional neural network that counters the loss of information caused by max-pooling by re-introducing the original image at multiple points within the network. We also use atrous spatial pyramid pooling with varying dilation rates for preserving the resolution and multi-level aggregation. To incorporate uncertainty, we introduce random transformations during test time for an enhanced segmentation result that simultaneously generates an uncertainty map, highlighting areas of ambiguity. We show that this map can be used to define a metric for disregarding predictions with high uncertainty. The proposed network achieves state-of-the-art performance on the GlaS challenge dataset and on a second independent colorectal adenocarcinoma dataset. In addition, we perform gland instance segmentation on whole-slide images from two further datasets to highlight the generalisability of our method. As an extension, we introduce MILD-Net+ for simultaneous gland and lumen segmentation, to increase the diagnostic power of the network. ","MILD-Net: Minimal Information Loss Dilated Network for Gland Instance
  Segmentation in Colon Histology Images"
92,1004985512778698752,55509201,Germain Forestier,"['New conference paper ""Evaluating surgical skills from kinematic data using convolutional neural networks"" available here: <LINK> to be presented at @MICCAI2018 with @hassanfawaz93 @jjmweber @pa_muller #deeplearning #surgery #training\n<LINK>']",https://arxiv.org/abs/1806.02750,"The need for automatic surgical skills assessment is increasing, especially because manual feedback from senior surgeons observing junior surgeons is prone to subjectivity and time consuming. Thus, automating surgical skills evaluation is a very important step towards improving surgical practice. In this paper, we designed a Convolutional Neural Network (CNN) to evaluate surgeon skills by extracting patterns in the surgeon motions performed in robotic surgery. The proposed method is validated on the JIGSAWS dataset and achieved very competitive results with 100% accuracy on the suturing and needle passing tasks. While we leveraged from the CNNs efficiency, we also managed to mitigate its black-box effect using class activation map. This feature allows our method to automatically highlight which parts of the surgical task influenced the skill prediction and can be used to explain the classification and to provide personalized feedback to the trainee. ","Evaluating surgical skills from kinematic data using convolutional
  neural networks"
93,1004972968076857344,972555245179064320,Jordy de Vries,"['New paper (<LINK>) today about effective theories and (again) neutrinoless double beta decay. The paper is about connecting high-scale sources of lepton-number violation to the very low-energy scales of nuclear experiments. This results in this metro-map. <LINK>', '@BradleyKavanagh Powerpoint....']",https://arxiv.org/abs/1806.02780,"We present a master formula describing the neutrinoless-double-beta decay ($0\nu\beta\beta$) rate induced by lepton-number-violating (LNV) operators up to dimension nine in the Standard Model Effective Field Theory. We provide an end-to-end framework connecting the possibly very high LNV scale to the nuclear scale, through a chain of effective field theories. Starting at the electroweak scale, we integrate out the heavy Standard Model degrees of freedom and we match to an $SU(3)_c\otimes U(1)_{\mathrm{em}}$ effective theory. After evolving the resulting effective Lagrangian to the QCD scale, we use chiral perturbation theory to derive the lepton-number-violating chiral Lagrangian. The chiral Lagrangian is used to derive the two-nucleon $0\nu\beta\beta$ transition operators to leading order in the chiral power counting. Based on renormalization arguments we show that in various cases short-range two-nucleon operators need to be enhanced to leading order. We show that all required nuclear matrix elements can be taken from existing calculations. Our final result is a master formula that describes the $0\nu\beta\beta$ rate in terms of phase-space factors, nuclear matrix elements, hadronic low-energy constants, QCD evolution factors, and high-energy LNV Wilson coefficients, including all the interference terms. Our master formula can be easily matched to any model where LNV originates at energy scales above the electroweak scale. As an explicit example, we match our formula to the minimal left-right-symmetric model in which contributions of operators of different dimension compete, and we discuss the resulting phenomenology. ","A neutrinoless double beta decay master formula from effective field
  theory"
94,1004890078592712704,1271414598,Nathan Goldbaum,"[""I'd like to announce a new library I've been working on recently called unyt: <LINK>. Myself along with @powersoffour @astrojaz, @Xarthisius, and @annalikesstars submitted a paper about it to @JOSS_TheOJ which you can read on the arxiv: <LINK>"", ""unyt is an evolution of yt.units. The goodness of yt.units has been factored out into its own pure-python library that depends only on SymPy and NumPy. I'm often told that yt.units is people's favorite part of yt, now it's easier than ever to use."", 'The paper contains a performance benchmark comparing unyt with Pint and astropy.units (two other Python libraries that provide a unit system). For most operations, unyt is the fastest library available', ""(Although shortly after I alerted the astropy developers by opening an issue about the benchmark results, they've already opened some PRs to speed up a couple operations.)"", ""I'd love it if you give unyt a try. We worked really hard on the documentation and tests, including making sure the API docs all have examples and getting the test coverage to 100%."", ""If you have comments, suggestions, or you run into issues, please open an issue on github. If you're interested in contributing, pull requests are very welcome. It's a fun codebase to hack on."", ""@powersoffour @astrocrash in @yt_astro names for things aren't acceptable unless they have at least three meanings""]",https://arxiv.org/abs/1806.02417,"Software that processes real-world data or that models a physical system must have some way of managing units. While simple approaches like the understood convention that all data are in a unit system (such as the MKS SI unit system) do work in practice, they are fraught with possible sources of error both by developers and users of the software. In this paper we present unyt, a Python library based on NumPy and SymPy for handling data that has units. It is designed both to aid quick interactive calculations and to be tightly integrated into a larger Python application or library. We compare unyt with two other Python libraries for handling units, Pint and astropy.units, and find that unyt is faster, has higher test coverage, and has fewer lines of code. ","unyt: Handle, manipulate, and convert data with units in Python"
95,1004645897148555264,926551285788233728,Vincent Fortuin,['Our new paper on discrete representation learning on time series with self-organizing map variational autoencoders is online: <LINK>'],https://arxiv.org/abs/1806.02199,"High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty. We evaluate our model in terms of clustering performance and interpretability on static (Fashion-)MNIST data, a time series of linearly interpolated (Fashion-)MNIST images, a chaotic Lorenz attractor system with two macro states, as well as on a challenging real world medical time series application on the eICU data set. Our learned representations compare favorably with competitor methods and facilitate downstream tasks on the real world data. ",SOM-VAE: Interpretable Discrete Representation Learning on Time Series
96,1004514572416495616,5620142,Edward Grefenstette 🇪🇺,"['Happy to share our new @DeepMindAI paper on AGILE, a method for training agents to follow language instructions by jointly learning a reward model from examples. No more template languages, or problems with hard/impossible to code reward functions!\n<LINK>', '@tkasasagi @DeepMindAI Well, at least awesome enough to attract stellar interns like @DBahdanau, who did a very large chunk of this work. The co-authors all contributed greatly as well, of course 🙂', '@kchonyc @DeepMindAI Yes. Need to guard against degeneration present in other adversarial objectives (e.g. GANs etc), hence the buffer rejection method.', '@SeeTedTalk @DeepMindAI @LeonDerczynski 1) Most/all of our papers are submitted to peer reviewed conferences and/or journals.\n2) Researchers are incentivised to produce good and impactful research, but there is no one metric for measuring this. It certainly does not boil down to number of papers, citations, etc.', '@SeeTedTalk @DeepMindAI @LeonDerczynski PS. Thanks for your interest in our work!', '@SeeTedTalk @DeepMindAI @LeonDerczynski Yes. No such policy for NIPS/ICML/ICLR/etc...']",https://arxiv.org/abs/1806.01946,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples. ",Learning to Understand Goal Specifications by Modelling Reward
97,1004381748099272704,1940366048,Abhilasha Ravichander,"['Can we sanity-check our NLI Models? New Paper at #COLING2018 on ""Stress Test Evaluation for Natural Language Inference"" : <LINK>.  Joint work with @LTIatCMU @isrcmu researchers, evaluation exercises neural NLI models on large-scale stress tests of their failures: <LINK>']",https://arxiv.org/abs/1806.00692,"Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed ""stress tests"" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area. ",Stress Test Evaluation for Natural Language Inference
98,1004304559433486336,933826478038544384,Mark Williams,"['New LHCb paper searches for (undiscovered) CP Violation in charm, here in D⁰ → KS⁰ KS⁰ which can have large (~1%) CPV from SM sources alone ⇒ good discovery mode. Sadly, measurement is consistent with CP symmetry - more data needed! <LINK> @LHCbExperiment <LINK>']",https://arxiv.org/abs/1806.01642,"A measurement of the time-integrated $CP$ asymmetry in $D^0\rightarrow K^0_S K^0_S$ decays is reported. The data correspond to an integrated luminosity of about $2$ fb$^{-1}$ collected in 2015-2016 by the LHCb collaboration in $pp$ collisions at a centre-of-mass energy of $13$ TeV. The $D^0$ candidate is required to originate from a $D^{\ast +} \rightarrow D^0 \pi^+$ decay, allowing the determination of the flavour of the $D^0$ meson using the pion charge. The $D^0 \rightarrow K^{+}K^{-}$ decay, which has a well measured $CP$ asymmetry, is used as a calibration channel. The $CP$ asymmetry for $D^0\rightarrow K^0_S K^0_S$ is measured to be \begin{equation*} \mathcal{A}^{CP}(D^0\rightarrow K^0_S K^0_S) = (4.3\pm 3.4\pm 1.0)\%, \end{equation*} where the first uncertainty is statistical and the second is systematic. This result is combined with the previous LHCb measurement at lower centre-of-mass energies to obtain \begin{equation*} \mathcal{A}^{CP}(D^0\rightarrow K^0_S K^0_S) = (2.3\pm 2.8\pm 0.9)\%. \end{equation*} ","Measurement of the time-integrated $CP$ asymmetry in $D^0 \rightarrow
  K^0_S K^0_S$ decays"
99,1004300818412113920,20703003,Peter B Denton,"[""With #Neutrino2018 going on it's a good time to ask: What actually is the probability for a neutrino to oscillate? Stephen Parke and I just put out a new paper continuing our work on understanding these formulas with a great @UChicago student Xining Zhang. <LINK> <LINK>""]",https://arxiv.org/abs/1806.01277,"We further develop a simple and compact technique for calculating the three flavor neutrino oscillation probabilities in uniform matter density. By performing additional rotations instead of implementing a perturbative expansion we significantly decrease the scale of the perturbing Hamiltonian and therefore improve the accuracy of zeroth order. We explore the relationship between implementing additional rotations and that of performing a perturbative expansion. Based on our analysis, independent of the size of the matter potential, we find that the first order perturbation expansion can be replaced by two additional rotations and a second order perturbative expansion can be replaced by one more rotation. Numerical tests have been applied and all the exceptional features of our analysis have been verified. ","Rotations Versus Perturbative Expansions for Calculating Neutrino
  Oscillation Probabilities in Matter"
100,1004243643262091264,22399655,Ryota Kanai💡,['A new paper from us on how to make AI bored. With Yen and Acer. “\nBoredom-driven curious learning by Homeo-Heterostatic Value Gradients”\n<LINK>'],http://arxiv.org/abs/1806.01502,"This paper presents the Homeo-Heterostatic Value Gradients (HHVG) algorithm as a formal account on the constructive interplay between boredom and curiosity which gives rise to effective exploration and superior forward model learning. We envisaged actions as instrumental in agent's own epistemic disclosure. This motivated two central algorithmic ingredients: devaluation and devaluation progress, both underpin agent's cognition concerning intrinsically generated rewards. The two serve as an instantiation of homeostatic and heterostatic intrinsic motivation. A key insight from our algorithm is that the two seemingly opposite motivations can be reconciled---without which exploration and information-gathering cannot be effectively carried out. We supported this claim with empirical evidence, showing that boredom-enabled agents consistently outperformed other curious or explorative agent variants in model building benchmarks based on self-assisted experience accumulation. ",Boredom-driven curious learning by Homeo-Heterostatic Value Gradients
101,1004007216683331586,2541954109,Victoria Krakovna,"['New arXiv paper: Measuring and avoiding side effects using relative reachability, <LINK>\n\nWe introduce a general side effects measure that avoids introducing bad incentives that occur in current approaches.\n\nAccompanying blog post: <LINK>']",http://arxiv.org/abs/1806.01186,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives. ",Penalizing side effects using stepwise relative reachability
102,1003946953892352000,704799523,francesca dominici,['New paper led by Georgia Papadogeorgou: \nA Causal Exposure Response Function with Local Adjustment for  Confounding <LINK> <LINK>'],https://arxiv.org/abs/1806.00928,"The Clean Air Act mandates that the National Ambient Air Quality Standards (NAAQS) must be routinely assessed to protect populations based on the latest science. Therefore, researchers should continue to address whether exposure to levels of air pollution below the NAAQS is harmful to human health. The contentious nature surrounding environmental regulations urges us to cast this question within a causal inference framework. Parametric and semi-parametric regression approaches have been used to estimate the exposure-response (ER) curve between ambient air pollution and health outcomes. Most of these approaches are not formulated within a causal framework, adjust for the same covariates across all levels of exposure, and do not account for model uncertainty. We introduce a Bayesian framework for the estimation of a causal ER curve called LERCA (Local Exposure Response Confounding Adjustment), which allows for different confounders and different strength of confounding at the different exposure levels; and propagates uncertainty regarding confounders' selection and the shape of the ER. LERCA provides a principled way of assessing the covariates' confounding importance at different exposure levels, providing researchers with information regarding the variables to adjust for in regression models. Using simulations, we show that state of the art approaches perform poorly in estimating the ER curve in the presence of local confounding. LERCA is used to evaluate the relationship between exposure to ambient PM2.5 and cardiovascular hospitalizations for 5,362 zip codes in the US, while adjusting for a potentially varying set of confounders across the exposure range. Ambient PM2.5 leads to an increase in cardiovascular hospitalization rates when focusing at the low exposure range. Our results indicate that there is no threshold for the effect of PM2.5 on cardiovascular hospitalizations. ","A causal exposure response function with local adjustment for
  confounding: Estimating health effects of exposure to low levels of ambient
  fine particulate matter"
103,1003921998404341761,3077144869,Preethi Lahoti,['Interested in learning data representations that are geared towards #fairness for #individuals (and can be learnt independent of the underlying machine learning task)? Check out our new paper with (Gerhard Weikum and Krishna Gummadi) available on arxiv: <LINK>'],https://arxiv.org/abs/1806.01059,"People are rated and ranked, towards algorithmic decision making in an increasing number of applications, typically based on machine learning. Research on how to incorporate fairness into such tasks has prevalently pursued the paradigm of group fairness: giving adequate success rates to specifically protected groups. In contrast, the alternative paradigm of individual fairness has received relatively little attention, and this paper advances this less explored direction. The paper introduces a method for probabilistically mapping user records into a low-rank representation that reconciles individual fairness and the utility of classifiers and rankings in downstream applications. Our notion of individual fairness requires that users who are similar in all task-relevant attributes such as job qualification, and disregarding all potentially discriminating attributes such as gender, should have similar outcomes. We demonstrate the versatility of our method by applying it to classification and learning-to-rank tasks on a variety of real-world datasets. Our experiments show substantial improvements over the best prior work for this setting. ","iFair: Learning Individually Fair Data Representations for Algorithmic
  Decision Making"
104,1003893981925793804,815300326127534080,Jaan Aru,"['The new paper from @DeepMindAI argues that combinatorial generalization must be a top priority for AI to achieve human-like abilities. The paper presents the ""graph network"" that enables manipulating structured knowledge and producing structured behaviors <LINK>', 'it\'s also nice to see @DeepMindAI saying ""we reject the false choice between ""hand-engineering"" and ""end-to-end"" learning, and instead advocate for an approach which benefits from their complementary strengths"". Hopefully encouraging to @GaryMarcus and other critical voices!']",https://arxiv.org/abs/1806.01261,"Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between ""hand-engineering"" and ""end-to-end"" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice. ","Relational inductive biases, deep learning, and graph networks"
105,1003796574768640000,1214528593,Miles Brundage,"['New paper: ""Accounting for the Neglected Dimensions of AI Progress"": <LINK> \n\nJoint work with Fernando Martínez-Plumed, Shahar Avin, Allan Dafoe, Seán Ó hÉigeartaigh, and José Hernández-Orallo.  \n\n(1/n) <LINK>', 'We analyze the (often hidden) costs of AI systems, and develop/apply a framework for more rigorously accounting for these costs. These costs include computing power, data, human oversight, etc. Often performance improves at the expense of one or more such dimensions. (2/n) https://t.co/gaMKMQYBOr', 'Based on this idea, we describe the idea of a Pareto frontier for particular tasks, with some new algorithms pushing this frontier and others moving around within it. (3/n) https://t.co/RFNP1GHGUN', 'These costs are borne by various actors at different stages in AI development, and vary across algorithms. We discuss issues of reproducibility, replication, etc. in this framework. (4/n) https://t.co/MTyhZlaFIf', 'Often, these costs are not described in papers and other research outputs, which can make it hard to assess the improvement(s) associated with new research, if any. (5/n) https://t.co/LkYGLWRb4v', 'Finally, we apply this framework to two case studies: the Alpha* line of Go-playing systems, and algorithms applied to Atari games in the Arcade Learning Environment. We present the multidimensional utility space in these domains, using what data we could find or infer. (6/n) https://t.co/Dz5ZjQiVJQ', 'For more explanation of all of this, see the paper! https://t.co/uvD4sTITqR (7/7)']",https://arxiv.org/abs/1806.00610,"We analyze and reframe AI progress. In addition to the prevailing metrics of performance, we highlight the usually neglected costs paid in the development and deployment of a system, including: data, expert knowledge, human oversight, software resources, computing cycles, hardware and network facilities, development time, etc. These costs are paid throughout the life cycle of an AI system, fall differentially on different individuals, and vary in magnitude depending on the replicability and generality of the AI solution. The multidimensional performance and cost space can be collapsed to a single utility metric for a user with transitive and complete preferences. Even absent a single utility function, AI advances can be generically assessed by whether they expand the Pareto (optimal) surface. We explore a subset of these neglected dimensions using the two case studies of Alpha* and ALE. This broadened conception of progress in AI should lead to novel ways of measuring success in AI, and can help set milestones for future progress. ",Accounting for the Neglected Dimensions of AI Progress
106,1003598563253866501,835789494,Charlie Nash,"[""Want to understand your model's representations? Invert them with an autoregressive model: <LINK>\n\nNew paper with Nate Kushman and Chris Williams. <LINK>""]",https://arxiv.org/abs/1806.00400,"We present a method for feature interpretation that makes use of recent advances in autoregressive density estimation models to invert model representations. We train generative inversion models to express a distribution over input features conditioned on intermediate model representations. Insights into the invariances learned by supervised models can be gained by viewing samples from these inversion models. In addition, we can use these inversion models to estimate the mutual information between a model's inputs and its intermediate representations, thus quantifying the amount of information preserved by the network at different stages. Using this method we examine the types of information preserved at different layers of convolutional neural networks, and explore the invariances induced by different architectural choices. Finally we show that the mutual information between inputs and network layers decreases over the course of training, supporting recent work by Shwartz-Ziv and Tishby (2017) on the information bottleneck theory of deep learning. ","Inverting Supervised Representations with Autoregressive Neural Density
  Models"
107,1003564531338137601,957689165902118912,Alexandre Dauphin,"['Our new paper on applying machine learning to the many-body localization problem is out. In this work, the ML algorithm allows one to extract the phase transition with 100 times less average than previous methods.\n<LINK>']",https://arxiv.org/abs/1806.00419,"We identify a new ""order parameter"" for the disorder driven many-body localization (MBL) transition by leveraging artificial intelligence. This allows us to pin down the transition, as the point at which the physics changes qualitatively, from vastly fewer disorder realizations and in an objective and cleaner way than is possible with the existing zoo of quantities. Contrary to previous studies, our method is almost entirely unsupervised. A game theoretic process between neural networks defines an adversarial setup with conflicting objectives to identify what characteristic features to base efficient predictions on. This reduces the numerical effort for mapping out the phase diagram by a factor of ~100x. This approach of automated discovery is applicable specifically to poorly understood phase transitions and exemplifies the potential of machine learning assisted research in physics. ","Automated discovery of characteristic features of phase transitions in
  many-body localization"
108,1003446483633336320,22737971,Thomas Connor,"[""Oh hey, new paper I'm on! Fantastic X-ray analysis of giant ellipticals led by Kiran Lakhchaura. Check it out!\n<LINK>""]",https://arxiv.org/abs/1806.00455,"We present a study of the thermal structure of the hot X-ray emitting atmospheres for a sample of 49 nearby X-ray and optically bright elliptical galaxies using {\it Chandra} X-ray data. We focus on the connection between the properties of the hot X-ray emitting gas and the cooler H$\alpha$+[NII] emitting phase, and the possible role of the latter in the AGN (Active Galactic Nuclei) feedback cycle. We do not find evident correlations between the H$\alpha$+[NII] emission and global properties such as X-ray luminosity, mass of hot gas, and gas mass fraction. We find that the presence of H$\alpha$+[NII] emission is more likely in systems with higher densities, lower entropies, shorter cooling times, shallower entropy profiles, lower values of min($t_{\rm cool}/t_{\rm ff}$), and disturbed X-ray morphologies (linked to turbulent motions). However, we see no clear separations in the observables obtained for galaxies with and without optical emission line nebulae. The AGN jet powers of the galaxies with X-ray cavities show hint of a possible weak positive correlation with their H$\alpha$+[NII] luminosities. This correlation and the observed trends in the thermodynamic properties may result from chaotic cold accretion (CCA) powering AGN jets, as seen in some high-resolution hydrodynamic simulations. ","Thermodynamic properties, multiphase gas and AGN feedback in a large
  sample of giant ellipticals"
109,1003438335363203072,22399655,Ryota Kanai💡,"['Our new paper led by Nicholas Guttenberg (@ngutten) ""Being curious about the answers to questions: novelty search with learned attention"". \nTo be presented at ALIFE 2018 @alifelab.\n<LINK>']",https://arxiv.org/abs/1806.00201,"We investigate the use of attentional neural network layers in order to learn a `behavior characterization' which can be used to drive novelty search and curiosity-based policies. The space is structured towards answering a particular distribution of questions, which are used in a supervised way to train the attentional neural network. We find that in a 2d exploration task, the structure of the space successfully encodes local sensory-motor contingencies such that even a greedy local `do the most novel action' policy with no reinforcement learning or evolution can explore the space quickly. We also apply this to a high/low number guessing game task, and find that guessing according to the learned attention profile performs active inference and can discover the correct number more quickly than an exact but passive approach. ","Being curious about the answers to questions: novelty search with
  learned attention"
110,1017444023076818944,482434888,Yingjie Hu,"['Our new GIScience paper has been selected as the best full paper candidate: <LINK> This work analyzed more than 110,000 POI names in 7 US metropolitan areas, and systematically examined their changes with place types and distances. #GIScience #GIS #Geospatial <LINK>']",https://arxiv.org/abs/1806.08040,"While Points Of Interest (POIs), such as restaurants, hotels, and barber shops, are part of urban areas irrespective of their specific locations, the names of these POIs often reveal valuable information related to local culture, landmarks, influential families, figures, events, and so on. Place names have long been studied by geographers, e.g., to understand their origins and relations to family names. However, there is a lack of large-scale empirical studies that examine the localness of place names and their changes with geographic distance. In addition to enhancing our understanding of the coherence of geographic regions, such empirical studies are also significant for geographic information retrieval where they can inform computational models and improve the accuracy of place name disambiguation. In this work, we conduct an empirical study based on 112,071 POIs in seven US metropolitan areas extracted from an open Yelp dataset. We propose to adopt term frequency and inverse document frequency in geographic contexts to identify local terms used in POI names and to analyze their usages across different POI types. Our results show an uneven usage of local terms across POI types, which is highly consistent among different geographic regions. We also examine the decaying effect of POI name similarity with the increase of distance among POIs. While our analysis focuses on urban POI names, the presented methods can be generalized to other place types as well, such as mountain peaks and streets. ","An empirical study on the names of points of interest and their changes
  with geographic distance"
111,1012975619737620480,735386827578875904,siegfried Vanaverbek,"[""Our new paper on the secular dimming of Tabby's star has just been published:\n<LINK>\nSeveral datasets have been combined, including data from Astrolab IRIS.""]",https://arxiv.org/abs/1806.09911,"The star KIC 8462852 (Boyajian's Star) displays both fast dips of up to 20% on time scales of days, plus long-term secular fading by up to 19% on time scales from a year to a century. We report on CCD photometry of KIC 8462852 from 2015.75 to 2018.18, with 19,176 images making for 1,866 nightly magnitudes in BVRI. Our light curves show a continuing secular decline (by 0.023 +- 0.003 mags in the B-band) with three superposed dips with duration 120-180 days. This demonstrates that there is a continuum of dip durations from a day to a century, so that the secular fading is seen to be by the same physical mechanism as the short-duration Kepler dips. The BVRI light curves all have the same shape, with the slopes and amplitudes for VRI being systematically smaller than in the B-band by factors of 0.77 +- 0.05, 0.50 +- 0.05, and 0.31 +- 0.05. We rule out any hypothesis involving occultation of the primary star by any star, planet, solid body, or optically thick cloud. But these ratios are the same as that expected for ordinary extinction by dust clouds. This chromatic extinction implies dust particle sizes going down to ~0.1 micron, suggesting that this dust will be rapidly blown away by stellar radiation pressure, so the dust clouds must have formed within months. The modern infrared observations were taken at a time when there was at least 12.4% +- 1.3% dust coverage (as part of the secular dimming), and this is consistent with dimming originating in circumstellar dust. ","The KIC 8462852 Light Curve From 2015.75 to 2018.18 Shows a Variable
  Secular Decline"
112,1011892443443355649,746249674869346304,Dmitry Meshkov,"['It is generally accepted that missing of loops inside a smart contract language means that it is not Turing-complete. Our new paper <LINK> dispels this myth.', 'One of the most interesting practical result is that it is easy to build Turing-complete language without runtime cost analysis (e.g. gas in #ethereum), making smart contracts much more secure.']",https://arxiv.org/abs/1806.10116,"Turing-completeness of smart contract languages in blockchain systems is often associated with a variety of language features (such as loops). In opposite, we show that Turing-completeness of a blockchain system can be achieved through unwinding the recursive calls between multiple transactions and blocks instead of using a single one. We prove it by constructing a simple universal Turing machine using a small set of language features in the unspent transaction output (UTXO) model, with explicitly given relations between input and output transaction states. Neither unbounded loops nor possibly infinite validation time are needed in this approach. ",Self-Reproducing Coins as Universal Turing Machine
113,1011508341024182272,3066116999,Adam Barrett,"['New paper: ""Measuring Integrated Information: Comparison of Candidate Measures in Theory and Simulation"" <LINK>']",https://arxiv.org/abs/1806.09373,"Integrated Information Theory (IIT) is a prominent theory of consciousness that has at its centre measures that quantify the extent to which a system generates more information than the sum of its parts. While several candidate measures of integrated information (`$\Phi$') now exist, little is known about how they compare, especially in terms of their behaviour on non-trivial network models. In this article we provide clear and intuitive descriptions of six distinct candidate measures. We then explore the properties of each of these measures in simulation on networks consisting of eight interacting nodes, animated with Gaussian linear autoregressive dynamics. We find a striking diversity in the behaviour of these measures -- no two measures show consistent agreement across all analyses. Further, only a subset of the measures appear to genuinely reflect some form of dynamical complexity, in the sense of simultaneous segregation and integration between system components. Our results help guide the operationalisation of IIT and advance the development of measures of integrated information that may have more general applicability. ","Measuring Integrated Information: Comparison of Candidate Measures in
  Theory and Simulation"
114,1011410064408760321,321794593,José G. Fernández-Trincado,"['Our new @APOGEE paper announced on ArXiv <LINK>, results from my Master student ☺️ <LINK>']",https://arxiv.org/abs/1806.09575,"IC 166 is an intermediate-age open cluster ($\sim 1$ Gyr) which lies in the transition zone of the metallicity gradient in the outer disc. Its location, combined with our very limited knowledge of its salient features, make it an interesting object of study. We present the first high-resolution spectroscopic and precise kinematical analysis of IC 166, which lies in the outer disc with $R_{GC} \sim 12.7$ kpc. High resolution \textit{H}-band spectra were analyzed using observations from the SDSS-IV Apache Point Observatory Galactic Evolution Experiment (APOGEE) survey. We made use of the Brussels Automatic Stellar Parameter (BACCHUS) code to provide chemical abundances based on a line-by-line approach for up to eight chemical elements (Mg, Si, Ca, Ti, Al, K, Mn and Fe). The $\alpha-$element (Mg, Si, Ca and whenever available Ti) abundances, and their trends with Fe abundances have been analysed for a total of 13 high-likelihood cluster members. No significant abundance scatter was found in any of the chemical species studied. Combining the positional, heliocentric distance, and kinematic information we derive, for the first time, the probable orbit of IC 166 within a Galactic model including a rotating boxy bar, and found that it is likely that IC 166 formed in the Galactic disc, supporting its nature as an unremarkable Galactic open cluster with an orbit bound to the Galactic plane. ","A chemical and kinematical analysis of the intermediate-age open cluster
  IC 166 from APOGEE and Gaia DR2"
115,1010244714484850688,68493084,sadia afroz,"['Our new paper explores blocking beyond censorship: blocking\nvisitors from the EU to avoid GDPR compliance, blocking\nbased upon the visitor’s country, and blocking due to\nsecurity concerns: <LINK> (will be presented at #foci18 workshop @USENIXSecurity)']",https://arxiv.org/abs/1806.00459,"This paper examines different reasons the websites may vary in their availability by location. Prior works on availability mostly focus on censorship by nation states. We look at three forms of server-side blocking: blocking visitors from the EU to avoid GDPR compliance, blocking based upon the visitor's country, and blocking due to security concerns. We argue that these and other forms of blocking warrant more research. ","A Bestiary of Blocking: The Motivations and Modes behind Website
  Unavailability"
116,1007618336329551873,933084565895286786,Dan Hooper,"['(1/6) I just put out a new paper, in which I indulge in some wide-eyed futurism. Let me walk you though the idea.\n<LINK> <LINK>', ""@physicspod Sure, I'd be happy to chat."", '@physicspod Twitter is fine, but feel free to use email if you prefer.', '@dwsNY @JenLucPiquant Guilty!']",https://arxiv.org/abs/1806.05203,"The presence of dark energy in our universe is causing space to expand at an accelerating rate. As a result, over the next approximately 100 billion years, all stars residing beyond the Local Group will fall beyond the cosmic horizon and become not only unobservable, but entirely inaccessible, thus limiting how much energy could one day be extracted from them. Here, we consider the likely response of a highly advanced civilization to this situation. In particular, we argue that in order to maximize its access to useable energy, a sufficiently advanced civilization would chose to expand rapidly outward, build Dyson Spheres or similar structures around encountered stars, and use the energy that is harnessed to accelerate those stars away from the approaching horizon and toward the center of the civilization. We find that such efforts will be most effective for stars with masses in the range of $M\sim (0.2-1) M_{\odot}$, and could lead to the harvesting of stars within a region extending out to several tens of Mpc in radius, potentially increasing the total amount of energy that is available to a future civilization by a factor of several thousand. We also discuss the observable signatures of a civilization elsewhere in the universe that is currently in this state of stellar harvesting. ","Life Versus Dark Energy: How An Advanced Civilization Could Resist the
  Accelerating Expansion of the Universe"
117,1007506891730243584,702608666944475136,Mohsen Ghafoorian,['Our new paper with my colleagues at TomTom: EL-GAN substantially stabilizes adversarial training while ensuring structural quality in semantic segmentation.\n<LINK> <LINK>'],https://arxiv.org/abs/1806.05525,"Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold. ","EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane
  Detection"
118,1007448975514001409,2423179856,Edward Raff,"[""New paper with @jsylvest on how we as a community might look at improving the amount of fair ML in practice. <LINK> Will be presenting @icmlconf ML-Debates. Looking forward to Jared's new paper haiku""]",https://arxiv.org/abs/1806.05250,"Machine learning practitioners are often ambivalent about the ethical aspects of their products. We believe anything that gets us from that current state to one in which our systems are achieving some degree of fairness is an improvement that should be welcomed. This is true even when that progress does not get us 100% of the way to the goal of ""complete"" fairness or perfectly align with our personal belief on which measure of fairness is used. Some measure of fairness being built would still put us in a better position than the status quo. Impediments to getting fairness and ethical concerns applied in real applications, whether they are abstruse philosophical debates or technical overhead such as the introduction of ever more hyper-parameters, should be avoided. In this paper we further elaborate on our argument for this viewpoint and its importance. ",What About Applied Fairness?
119,1006811970690080769,992779795,Kristof De Mey,['New @ugent paper online: Prediction of the @FIFAWorldCup – A random forest approach with an emphasis on estimated team ability parameters. The model slightly favors #Spain 🇪🇸before the defending champion #Germany.🇩🇪 But... ➡️<LINK> #Statistics #Analytics <LINK>'],https://arxiv.org/abs/1806.03208,"In this work, we compare three different modeling approaches for the scores of soccer matches with regard to their predictive performances based on all matches from the four previous FIFA World Cups 2002 - 2014: Poisson regression models, random forests and ranking methods. While the former two are based on the teams' covariate information, the latter method estimates adequate ability parameters that reflect the current strength of the teams best. Within this comparison the best-performing prediction methods on the training data turn out to be the ranking methods and the random forests. However, we show that by combining the random forest with the team ability parameters from the ranking methods as an additional covariate we can improve the predictive power substantially. Finally, this combination of methods is chosen as the final model and based on its estimates, the FIFA World Cup 2018 is simulated repeatedly and winning probabilities are obtained for all teams. The model slightly favors Spain before the defending champion Germany. Additionally, we provide survival probabilities for all teams and at all tournament stages as well as the most probable tournament outcome. ","Prediction of the FIFA World Cup 2018 - A random forest approach with an
  emphasis on estimated team ability parameters"
120,1006425985393315840,738769492122214400,Johannes Lischner,"['X-ray photoemission is a useful technique for studying #catalysis, but analyzing spectra is challenging. In our new paper, we use DFT to calculate core-electron binding energies to make life easier for experimentalists: <LINK>. <LINK>', 'Big thanks to @bluebananna and @photoelectrons for motivating this work!']",https://arxiv.org/abs/1806.03895,"Core-level X-ray Photoelectron Spectroscopy (XPS) is often used to study the surfaces of heterogeneous copper-based catalysts, but the interpretation of measured spectra, in particular the assignment of peaks to adsorbed species, can be extremely challenging. In this study we demonstrate that first principles calculations using the delta Self Consistent Field (delta-SCF) method can be used to guide the analysis of experimental core-level spectra of complex surfaces relevant to heterogeneous catalysis. Specifically, we calculate core-level binding energy shifts for a series of adsorbates on Cu(111) and show that the resulting C1s and O1s binding energy shifts for adsorbed CO, CO2, C2H4, HCOO, CH3O, H2O, OH and a surface oxide on Cu(111) are in good overall agreement with the experimental literature. In the few cases where the agreement is less good, the theoretical results may indicate the need to re-examine experimental peak assignments. ","Core electron binding energies of adsorbates on Cu(111) from
  first-principles calculations"
121,1006096760744370176,20309837,Michael Veale,"['‘Debiasing’/FATML methods assume ML modellers hold sensitive data (eg ethnicity, sexuality). Privacy problem. Our new #ICML2018 paper uses secure multiparty computation to train ‘fair’ models without seeing these, and allows regulators to verify decisions. <LINK> <LINK>', ""(it's also important to remember throughout this that 'debiasing' approaches only are appropriate in very narrow situations, and are no silver bullet for  socio-technical harms and concerns that may involve machine learning)"", 'this might be of interest to @zacharylipton @realjoshkroll @tforcworc @realhamed @Miles_Brundage @mort___ @ruggieris']",https://arxiv.org/abs/1806.03281,"Recent work has explored how to train machine learning models which do not discriminate against any subgroup of the population as determined by sensitive attributes such as gender or race. To avoid disparate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disparate impact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-party computation which allow us to avoid both. By encrypting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its outputs verified and held to account, without users revealing their sensitive attributes. ",Blind Justice: Fairness with Encrypted Sensitive Attributes
122,1004670223474216960,710610891058716673,Jan Leike,"['New paper on teaching RL agents to understand the meaning of instructions. Instead of manually specifying rewards, we learn them from goal-state examples. With @DBahdanau, Felix Hill, @edwardfhughes, @pushmeet, and @egrefen! <LINK>', ""@oliverlemon @DBahdanau @edwardfhughes @pushmeet @egrefen As I understand it BABBLE is a very different approach: it doesn't learn reward functions and doesn't receive instructions, only dialogue goals which evaluated using an explicitly specified reward function."", '@oliverlemon @DBahdanau @edwardfhughes @pushmeet @egrefen The point of our method is that it allows us to learn to execute instructions that we cannot easily specify reward functions for.']",https://arxiv.org/abs/1806.01946,"Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples. ",Learning to Understand Goal Specifications by Modelling Reward
123,1004362469215752192,185910194,Graham Neubig,"['New #COLING2018 paper (by @arnaik19/@Lasha1608) on ""Stress Test Evaluation for Natural Language Inference"" <LINK>\nAdds noise (e.g. ""not"" in the hypothesis) to existing datasets and breaks SOTA NLI systems. Use the data to check that methods are actually learning! <LINK>']",https://arxiv.org/abs/1806.00692,"Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed ""stress tests"" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area. ",Stress Test Evaluation for Natural Language Inference
124,1017702254026117120,2541954109,Victoria Krakovna,"['I\'m presenting ""Measuring and avoiding side effects using relative reachability"" at #ICML2018 Goal Specifications for RL workshop (Sat 10:50am in A7). We propose a general way to penalize agents for causing side effects without introducing bad incentives. <LINK>']",https://arxiv.org/abs/1806.01186,"How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives. ",Penalizing side effects using stepwise relative reachability
125,1013981432983638016,319443920,Christoph Simon,"[""Quantum physics for everyone. We hope that some people might use this for their own 'quantum physics for poets' classes, and others might just find it interesting to read. <LINK>""]",https://arxiv.org/abs/1806.09958,"Quantum physics, which describes the strange behavior of light and matter at the smallest scales, is one of the most successful descriptions of reality, yet it is notoriously inaccessible. Here we provide an approachable explanation of quantum physics using simple thought experiments. We derive all relevant quantum predictions using minimal mathematics, without introducing the advanced calculations that are typically used to describe quantum physics. We focus on the two key surprises of quantum physics, namely wave-particle duality, a term that was introduced to capture the fact that single quantum particles in some respects behave like waves and in other respects like particles, and entanglement, which applies to two or more quantum particles and brings out the inherent contradiction between quantum physics and seemingly obvious assumptions regarding the nature of reality. Following arguments originally made by John Bell and Lucien Hardy, we show that the so-called local hidden variables are inadequate at explaining the behavior of entangled quantum particles. This means that one either has to give up on hidden variables, i.e., the idea that the outcomes of measurements on quantum particles are determined before an experiment is actually carried out, or one has to relinquish the principle of locality, which requires that no causal influences should be faster than the speed of light and is a cornerstone of Einstein's theory of relativity. Finally, we describe how these remarkable predictions of quantum physics have been confirmed in experiments. We have successfully used the present approach in a course that is open to all undergraduate students at the University of Calgary, without any prerequisites in mathematics or physics. ","Understanding quantum physics through simple experiments: from
  wave-particle duality to Bell's theorem"
126,1013673745800671232,75249390,Axel Maas,"['We have published a preliminary study on how phenomenology based on the field theory of #BroutEnglertHiggs physics, improving on standard methods, would work, see <LINK>\n\nStill a toy theory, but the (qualitative!) differences are remarkable #np3']",https://arxiv.org/abs/1806.11373,"Even at weak coupling the physical, observable spectrum of gauge theories with a Brout-Englert-Higgs effect can deviate from the elementary one of perturbation theory. This can be analytically described and treated using the Fr\""ohlich-Morchio-Strocchi mechanism. We confirm this by lattice simulation for an SU(3) gauge theory with a fundamental scalar, a toy model for grand unification. We also show that this has experimentally observable consequence, e.g., in scattering cross-sections of lepton collisions in this toy model. ",On observable particles in theories with a Brout-Englert-Higgs effect
127,1012557852932825088,883039700,Lenka Zdeborova,['What types of phase transitions can you find in computational problems? We had a lot of fun looking into the question: <LINK>'],https://arxiv.org/abs/1806.11013,"Many inference problems, notably the stochastic block model (SBM) that generates a random graph with a hidden community structure, undergo phase transitions as a function of the signal-to-noise ratio, and can exhibit hard phases in which optimal inference is information-theoretically possible but computationally challenging. In this paper we refine this description by emphasizing the existence of more generic phase diagrams with a hybrid-hard phase in which it is computationally easy to reach a non-trivial inference accuracy, but computationally hard to match the information theoretically optimal one. We support this discussion by quantitative expansions of the functional cavity equations that describe inference problems on sparse graphs. These expansions shed light on the existence of hybrid-hard phases, for a large class of planted constraint satisfaction problems, and on the question of the tightness of the Kesten-Stigum (KS) bound for the associated tree reconstruction problem. Our results show that the instability of the trivial fixed point is not a generic evidence for the Bayes-optimality of the message passing algorithms. We clarify in particular the status of the symmetric SBM with 4 communities and of the tree reconstruction of the associated Potts model: in the assortative (ferromagnetic) case the KS bound is always tight, whereas in the disassortative (antiferromagnetic) case we exhibit an explicit criterion involving the degree distribution that separates a large degree regime where the KS bound is tight and a low degree regime where it is not. We also investigate the SBM with 2 communities of different sizes, a.k.a. the asymmetric Ising model, and describe quantitatively its computational gap as a function of its asymmetry, and a version of the SBM with 2 groups of communities. We complement this study with numerical simulations of the Belief Propagation algorithm. ",Typology of phase transitions in Bayesian inference problems
128,1011769394748243968,16714100,Cayman Unterborn,"['I wrote a note (to tune of gif). For TRAPPIST-1, we agree with Grimm et al., 2018 (&lt;5% H2O) only if the planets’ cores are much smaller than Earth. We find 3 are definitely very wet. The other 4 are consistent with having no Fe core at all or lots of H2O.\n<LINK> <LINK>', 'Cc my lovely co-authors @natalie_hinkel and @Deschscoveries']",http://arxiv.org/abs/1806.10084,"After publication of our initial mass-radius-composition models for the TRAPPIST-1 system in Unterborn et al. (2018), the planet masses were updated in Grimm et al. (2018). We had originally adopted the data set of Wang et al., 2017 who reported different densities than the updated values. The differences in observed density change the inferred volatile content of the planets. Grimm et al. (2018) report TRAPPIST-1 b, d, f, g, and h as being consistent with <5 wt% water and TRAPPIST-1 c and e has having largely rocky interiors. Here, we present updated results recalculating water fractions and potential alternative compositions using the Grimm et al., 2018 masses. Overall, we can only reproduce the results of Grimm et al., 2018 of planets b, d and g having small water contents if the cores of these planets are small (<23 wt%). We show that, if the cores for these planets are roughly Earth-sized (33 wt%), significant water fractions up to 40 wt% are possible. We show planets c, e, f, and h can have volatile envelopes between 0-35 wt% that are also consistent with being totally oxidized and lacking an Fe-core entirely. We note here that a pure MgSiO$_3$ planet (Fe/Mg = 0) is not the true lowest density end-member mass-radius curve for determining the probability of a planet containing volatiles. All planets that are rocky likely contain some Fe, either within the core or oxidized in the mantle. We argue the true low density end-member for oxidizing systems is instead a planet with the lowest reasonable Fe/Mg and completely core-less. Using this logic, we assert that planets b, d and g likely must have significant volatile layers because the end-member planet models produce masses too high even when uncertainties in both mass and radius are taken into account. ",Updated Compositional Models of the TRAPPIST-1 Planets
129,1011681101306351616,230157762,Thorsten Sommer,['#Blockchain and #Education? We propose a #blockchain for the worldwide organization of student #achievements. Read our new paper: <LINK> Our goal is to encourage discussion in the communities. Spread the idea and start the discussion.'],https://arxiv.org/abs/1806.09335,"Staying abroad during their studies is increasingly popular for students. However, there are various challenges for both students and universities. One important question for students is whether or not achievements performed at different universities can be taken into account for either enrolling at a foreign university or for completing the studies at their home university. In addition to university achievements, an increasing proportion of the 195 million students worldwide increasingly receive certificates from MOOCs or other social media services. The integration of such services into university teaching is still in the initial stages and presents some challenges. In this paper we describe the idea to manage all these study achievements worldwide in a blockchain, which might solve the national and international challenges regarding the recognition of student achievements. The aim of this paper is to encourage discussion in the global community instead of presenting a finished concept. Some of the open research questions are: How to ensure student data protection, how to deal with fraud and how to deal with the possibility that students can analytically calculate the easiest way through their studies? ","Request for Comments: Proposal of a Blockchain for the Automatic
  Management and Acceptance of Student Achievements"
130,1011163397403152384,270962976,Vincenzo Lomonaco,"['Our new paper ""Continuous Learning in Single-Incremental-Task Scenarios"" is out: <LINK>\n\nIn this work we propose a new, very light CL strategy denoted as ""AR1"" outperforming existing regularization strategies by a good margin on iCIFAR-100 and CORe50! <LINK>']",https://arxiv.org/abs/1806.08568,"It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in term of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin. ",Continuous Learning in Single-Incremental-Task Scenarios
131,1009853749446172672,888216099757490176,Maithra Raghu,"['Very excited about our latest preprint: <LINK>, joint work with @arimorcos and Samy Bengio. We apply Canonical Correlation (CCA) to study the representational similarity between memorizing and generalizing networks, and also examine the training dynamics of RNNs. <LINK>']",https://arxiv.org/abs/1806.05759,"Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al., 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations. ","Insights on representational similarity in neural networks with
  canonical correlation"
132,1007633027122380800,563661699,Prof Alex Hill,"['Check out my paper with @astro_curator @jcibanezm and Andrea Gatto. We find that the pressure of interstellar gas stays in the range where cold dense atomic gas (which might eventually form stars) can exist even when we change the heating rate by ~10. <LINK> 1/6 <LINK>', 'Technical details: Happens because any gas that gets turbulently compressed has two options: expand and heat at the same pressure, or heat to become (less) cold gas at the same density (raising the pressure). Latter happens *much* faster, so pressure tends to go up. 2/6', 'This is supported by observations that cold hydrogen gas is found a long way out in the Milky Way disk, where the pressure ought to be too low for cold gas to form. 3/6', 'Has implications for cosmological simulations which don’t resolve the cold gas: the pressure in the warm gas should be kept high enough for cold gas to form even far out in galactic disks. 4/6', 'Our data (22 GB) is available through the @AMNH library at https://t.co/BP6gQsNmGz So pull out @yt_astro and have at it! 5/6', 'This paper started with failure: alternative title would be “What we learned when I couldn’t get my simulations to work for two years”. 6/6', '@astrocurator (7/6)']",https://arxiv.org/abs/1806.05571,"We investigate the impact of the far ultraviolet (FUV) heating rate on the stability of the three-phase interstellar medium using three-dimensional simulations of a $1$ kpc$^2$, vertically-extended domain. The FUV heating rate sets the range of thermal pressures across which the cold ($\sim10^2$ K) and warm ($\sim10^4$ K) neutral media (CNM and WNM) can coexist in equilibrium. Even absent a variable star formation rate regulating the FUV heating rate, the gas physics keeps the pressure in the two-phase regime: because radiative heating and cooling processes happen on shorter timescales than sound wave propagation, turbulent compressions tend to keep the interstellar medium within the CNM-WNM pressure regime over a wide range of heating rates. The thermal pressure is set primarily by the heating rate with little influence from the hydrostatics. The vertical velocity dispersion adjusts as needed to provide hydrostatic support given the thermal pressure: when the turbulent pressure $\langle\rho\rangle\sigma_z^2$ is calculated over scales $\gtrsim500$ pc, the thermal plus turbulent pressure approximately equals the weight of the gas. The warm gas volume filling fraction is $0.2<f_w<0.8$ over a factor of less than three in heating rate, with $f_w$ near unity at higher heating rates and near zero at lower heating rates. We suggest that cosmological simulations that do not resolve the CNM should maintain an interstellar thermal pressure within the two-phase regime. ","Effect of the heating rate on the stability of the three-phase
  interstellar medium"
133,1006920075402719234,151193108,Mert R. Sabuncu 🇺🇦,['Machine learning models are increasingly applied to resting-state functional MRI data. We propose a new way to use convolutional neural network architectures for connectome-based prediction. We report excellent accuracy on ABIDE. Paper under review: <LINK>'],https://arxiv.org/abs/1806.04209,"Resting-state functional MRI (rs-fMRI) scans hold the potential to serve as a diagnostic or prognostic tool for a wide variety of conditions, such as autism, Alzheimer's disease, and stroke. While a growing number of studies have demonstrated the promise of machine learning algorithms for rs-fMRI based clinical or behavioral prediction, most prior models have been limited in their capacity to exploit the richness of the data. For example, classification techniques applied to rs-fMRI often rely on region-based summary statistics and/or linear models. In this work, we propose a novel volumetric Convolutional Neural Network (CNN) framework that takes advantage of the full-resolution 3D spatial structure of rs-fMRI data and fits non-linear predictive models. We showcase our approach on a challenging large-scale dataset (ABIDE, with N > 2,000) and report state-of-the-art accuracy results on rs-fMRI-based discrimination of autism patients and healthy controls. ","3D Convolutional Neural Networks for Classification of Functional
  Connectomes"
134,1006655938797932547,3918111614,Oriol Vinyals,['Graph Neural Networks / Relational Networks are models worth studying. We wrote a pretty comprehensive review about them which I hope you will find helpful (code forthcoming!). <LINK> <LINK>'],https://arxiv.org/abs/1806.01261,"Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between ""hand-engineering"" and ""end-to-end"" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice. ","Relational inductive biases, deep learning, and graph networks"
135,1006042556432601089,59083454,Dr. Rosita Kokotanekova,['*New paper* with @colinsnodgrass @pedrolacerda <LINK> We look for period changes of 3 large comets and discuss why large nuclei tend to survive longer. We also find hints that the level of surface erosions of JFCs could be distinguished using ground observations.'],https://arxiv.org/abs/1806.02897,"Rotational spin-up due to outgassing of comet nuclei has been identified as a possible mechanism for considerable mass-loss and splitting. We report a search for spin changes for three large Jupiter-family comets (JFCs): 14P/Wolf, 143P/Kowal-Mrkos, and 162P/Siding Spring. None of the three comets has detectable period changes, and we set conservative upper limits of 4.2 (14P), 6.6 (143P) and 25 (162P) minutes per orbit. Comparing these results with all eight other JFCs with measured rotational changes, we deduce that none of the observed large JFCs experiences significant spin changes. This suggests that large comet nuclei are less likely to undergo rotationally-driven splitting, and therefore more likely to survive more perihelion passages than smaller nuclei. We find supporting evidence for this hypothesis in the cumulative size distributions of JFCs and dormant comets, as well as in recent numerical studies of cometary orbital dynamics. We added 143P to the sample of 13 other JFCs with known albedos and phase-function slopes. This sample shows a possible correlation of increasing phase-function slopes for larger geometric albedos. Partly based on findings from recent space missions to JFCs, we hypothesise that this correlation corresponds to an evolutionary trend for JFCs. We propose that newly activated JFCs have larger albedos and steeper phase functions, which gradually decrease due to sublimation-driven erosion. If confirmed, this could be used to analyse surface erosion from ground and to distinguish between dormant comets and asteroids. ","Implications of the Small Spin Changes Measured for Large Jupiter-Family
  Comet Nuclei"
136,1004868521736769538,469536044,Gourav Khullar,"[""IT'S OUT! IT IS OUT. We studied a bunch of super high redshift galaxy clusters - and my 1st first-author paper is up on @arxiv! \nPlease give it a read ☺️@SPTelescope @UChicagoAstro #AAS232\n\n<LINK> <LINK>"", ""Don't know what the coolest thing about yesterday was - the chat with high school researchers about my poster, clusters and CCD flat-fielding, submitting a paper about an extremely challenging project, or reaching 500 followers on Tweeeter 🎺🎷🥁🎸 https://t.co/gzPRHWsLpK"", 'This was challenging work. Understanding these distant massive objects using ground based spectroscopy meant getting low signal datasets that were giving us a hard time. @LCOAstro data, and more than 4 years of collab. work well spent.', ""We double the no. of confirmed 'massive' clusters known beyond z&gt;1.2. My hope is that this helps people studying the properties of cluster members in this epoch 😁"", 'After a peer review process, we also hope the paper finds its place in ApJ @AAS_Publishing!']",https://arxiv.org/abs/1806.01962,"We present spectroscopic confirmation of five galaxy clusters at $1.25 < \textit{z} < 1.5$, discovered in the $2500$ deg$^{2}$ South Pole Telescope Sunyaev-Zel'dovich (SPT-SZ) survey. These clusters, taken from a mass-limited sample with a nearly redshift independent selection function, have multi-wavelength follow-up imaging data from the X-ray to near-infrared, and currently form the most homogeneous massive high-redshift cluster sample known. We identify $44$ member galaxies, along with $25$ field galaxies, among the five clusters, and describe the full set of observations and data products from Magellan/LDSS3 multi-object spectroscopy of these cluster fields. We briefly describe the analysis pipeline, and present ensemble analyses of cluster member galaxies that demonstrate the reliability of the measured redshifts. We report $\textit{z} = 1.259, 1.288, 1.316, 1.401$ and $1.474$ for the five clusters from a combination of absorption-line (Ca II H$\&$K doublet - $3968,3934$ {\AA}) and emission-line ([OII] $3727,3729$ {\AA}) spectral features. Moreover, the calculated velocity dispersions yield dynamical cluster masses in good agreement with SZ masses for these clusters. We discuss the velocity and spatial distributions of passive and [OII]-emitting galaxies in these clusters, showing that they are consistent with velocity segregation and biases observed in lower redshift SPT clusters. We identify modest [OII] emission and pronounced CN and H$\delta$ absorption in a stacked spectrum of $28$ passive galaxies with Ca II H$\&$K-derived redshifts. This work increases the number of spectroscopically-confirmed SZ-selected galaxy clusters at $\textit{z} > 1.25$ from three to eight, further demonstrating the efficacy of SZ selection for the highest redshift massive clusters, and enabling detailed study of these systems. ","Spectroscopic Confirmation of Five Galaxy Clusters at z &gt; 1.25 in the
  2500 sq. deg. SPT-SZ Survey"
137,1004639392575737857,30198750,Gunnar Rätsch,['New work on Boosting Black Box Variational Inference w/ @MPI_IS. We show boosting VI satisfies smoothness assumption sufficient for convergence of Frank-Wolfe alg &amp; propose to maximize Residual ELBO -&gt; allows black box implementation of boosting subroutine <LINK>'],https://arxiv.org/abs/1806.02185,"Approximating a probability density in a tractable manner is a central task in Bayesian statistics. Variational Inference (VI) is a popular technique that achieves tractability by choosing a relatively simple variational family. Borrowing ideas from the classic boosting framework, recent approaches attempt to \emph{boost} VI by replacing the selection of a single density with a greedily constructed mixture of densities. In order to guarantee convergence, previous works impose stringent assumptions that require significant effort for practitioners. Specifically, they require a custom implementation of the greedy step (called the LMO) for every probabilistic model with respect to an unnatural variational family of truncated distributions. Our work fixes these issues with novel theoretical and algorithmic insights. On the theoretical side, we show that boosting VI satisfies a relaxed smoothness assumption which is sufficient for the convergence of the functional Frank-Wolfe (FW) algorithm. Furthermore, we rephrase the LMO problem and propose to maximize the Residual ELBO (RELBO) which replaces the standard ELBO optimization in VI. These theoretical enhancements allow for black box implementation of the boosting subroutine. Finally, we present a stopping criterion drawn from the duality gap in the classic FW analyses and exhaustive experiments to illustrate the usefulness of our theoretical and algorithmic contributions. ",Boosting Black Box Variational Inference
138,1004357171734294528,318288924,Salim Arslan,['See our latest preprint to find out how we use graph convolutional networks and class activations to identify brain regions (ROIs) with application to functional connectivity driven sex classification. Now online at <LINK> w/ @s0f1ra @GlockerBen @DanielRueckert <LINK>'],https://arxiv.org/abs/1806.01764,"Graph convolutional networks (GCNs) allow to apply traditional convolution operations in non-Euclidean domains, where data are commonly modelled as irregular graphs. Medical imaging and, in particular, neuroscience studies often rely on such graph representations, with brain connectivity networks being a characteristic example, while ultimately seeking the locus of phenotypic or disease-related differences in the brain. These regions of interest (ROIs) are, then, considered to be closely associated with function and/or behaviour. Driven by this, we explore GCNs for the task of ROI identification and propose a visual attribution method based on class activation mapping. By undertaking a sex classification task as proof of concept, we show that this method can be used to identify salient nodes (brain regions) without prior node labels. Based on experiments conducted on neuroimaging data of more than 5000 participants from UK Biobank, we demonstrate the robustness of the proposed method in highlighting reproducible regions across individuals. We further evaluate the neurobiological relevance of the identified regions based on evidence from large-scale UK Biobank studies. ","Graph Saliency Maps through Spectral Convolutional Networks: Application
  to Sex Classification with Brain Connectivity"
139,1003550825795477505,978638177048162305,Mirko Signorelli,"['In this paper, we propose a model-based clustering method for populations of networks that can be used to identify subpopulations of networks that share certain topological properties of interest: <LINK> <LINK>']",https://arxiv.org/abs/1806.00225,"Until recently obtaining data on populations of networks was typically rare. However, with the advancement of automatic monitoring devices and the growing social and scientific interest in networks, such data has become more widely available. From sociological experiments involving cognitive social structures to fMRI scans revealing large-scale brain networks of groups of patients, there is a growing awareness that we urgently need tools to analyse populations of networks and particularly to model the variation between networks due to covariates. We propose a model-based clustering method based on mixtures of generalized linear (mixed) models that can be employed to describe the joint distribution of a populations of networks in a parsimonious manner and to identify subpopulations of networks that share certain topological properties of interest (degree distribution, community structure, effect of covariates on the presence of an edge, etc.). Maximum likelihood estimation for the proposed model can be efficiently carried out with an implementation of the EM algorithm. We assess the performance of this method on simulated data and conclude with an example application on advice networks in a small business. ",Model-based clustering for populations of networks
140,1006163488102846467,750727628294848512,Malena Rice,['!!! A paper including some of my undergrad honors thesis work with the @PlanetImager team has been accepted! Read on to learn about the HD 35841 debris disk and how we can use direct imaging + inverse modeling to study disk geometries &amp; compositions: <LINK>'],https://arxiv.org/abs/1806.02904,"We present new high resolution imaging of a light-scattering dust ring and halo around the young star HD 35841. Using spectroscopic and polarimetric data from the Gemini Planet Imager in H-band (1.6 microns), we detect the highly inclined (i=85 deg) ring of debris down to a projected separation of ~12 au (~0.12"") for the first time. Optical imaging from HST/STIS shows a smooth dust halo extending outward from the ring to >140 au (>1.4""). We measure the ring's scattering phase function and polarization fraction over scattering angles of 22-125 deg, showing a preference for forward scattering and a polarization fraction that peaks at ~30% near the ansae. Modeling of the scattered-light disk indicates that the ring spans radii of ~60-220 au, has a vertical thickness similar to that of other resolved dust rings, and contains grains as small as 1.5 microns in diameter. These models also suggest the grains have a low porosity, are more likely to consist of carbon than astrosilicates, and contain significant water ice. The halo has a surface brightness profile consistent with that expected from grains pushed by radiation pressure from the main ring onto highly eccentric but still bound orbits. We also briefly investigate arrangements of a possible inner disk component implied by our spectral energy distribution models, and speculate about the limitations of Mie theory for doing detailed analyses of debris disk dust populations. ","Direct Imaging of the HD 35841 Debris Disk: A Polarized Dust Ring from
  Gemini Planet Imager and an Outer Halo from HST/STIS"
