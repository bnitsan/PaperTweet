,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1391075507115003906,4241309441,Pete Bartram,['Our new paper is up looking at the behaviour of compact exoplanet systems after an instability event. \n<LINK> <LINK>'],https://arxiv.org/abs/2104.13658,"Recent observational missions have uncovered a significant number of compact multi-exoplanet systems. The tight orbital spacing of these systems has led to much effort being applied to the understanding of their stability; however, a key limitation of the majority of these studies is the termination of simulations as soon as the orbits of two planets cross. In this work we explore the stability of compact, three-planet systems and continue our simulations all the way to the first collision of planets to yield a better understanding of the lifetime of these systems. We perform over $25,000$ integrations of a Sun-like star orbited by three Earth-like secondaries for up to a billion orbits to explore a wide parameter space of initial conditions in both the co-planar and inclined cases, with a focus on the initial orbital spacing. We calculate the probability of collision over time and determine the probability of collision between specific pairs of planets. We find systems that persist for over $10^8$ orbits after an orbital crossing and show how the post-instability survival time of systems depends upon the initial orbital separation, mutual inclination, planetary radius, and the closest encounter experienced. Additionally, we examine the effects of very small changes in the initial positions of the planets upon the time to collision and show the effect that the choice of integrator can have upon simulation results. We generalise our results throughout to show both the behaviour of systems with an inner planet initially located at $1$ AU and $0.25$ AU. ","Orbital stability of compact three-planet systems, II: Post-instability
  impact behaviour"
1,1390703744409550851,1178403596158808067,Onno Kampman,"[""New paper with @neurosamuel presented at @SEDL_workshop today, in which we discuss ideas and insights from psychology's reproducibility crisis and translate them to the context and terminology of machine learning research\n<LINK>""]",https://arxiv.org/abs/2104.08878,"In the early 2010s, a crisis of reproducibility rocked the field of psychology. Following a period of reflection, the field has responded with radical reform of its scientific practices. More recently, similar questions about the reproducibility of machine learning research have also come to the fore. In this short paper, we present select ideas from psychology's reformation, translating them into relevance for a machine learning audience. ","Perspectives on Machine Learning from Psychology's Reproducibility
  Crisis"
2,1390611975290372100,961446288,Samuel Bell,"['Can Machine Learning improve its practices by taking inspiration from Psychology?  Our new paper certainly thinks so!\n\nCheck out ""Perspectives on Machine Learning from Psychology\'s Reproducibility Crisis"" on arxiv now: <LINK> <LINK>', ""@AdrianAskelund Thanks Adrian! Totally agreed - many of these issues are far from solved, and the proposed practices often seem hard to apply in practice. Still, they're good first steps, and certainly an improvement over the status quo!""]",https://arxiv.org/abs/2104.08878,"In the early 2010s, a crisis of reproducibility rocked the field of psychology. Following a period of reflection, the field has responded with radical reform of its scientific practices. More recently, similar questions about the reproducibility of machine learning research have also come to the fore. In this short paper, we present select ideas from psychology's reformation, translating them into relevance for a machine learning audience. ","Perspectives on Machine Learning from Psychology's Reproducibility
  Crisis"
3,1390399246084476934,185910194,Graham Neubig,"['MetaXL is a new method for cross-lingual transfer to extremely low-resource languages that works by meta-learning transformation functions to improve gradient alignment between source and target languages. See our #NAACL2021 paper! <LINK> 1/2 <LINK>', 'Code is also available: https://t.co/pBauwCGnbu\nGreat work by @xiamengzhou, Guoqing Zheng, and other collaborators at @MSFTResearch! 2/2']",https://arxiv.org/abs/2104.07908,"The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an under-studied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages - without access to large-scale monolingual corpora or large amounts of labeled data - for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL. ","MetaXL: Meta Representation Transformation for Low-resource
  Cross-lingual Learning"
4,1389773569329025024,915008528246435840,Rohin Shah,"['New #ICLR2021 paper by @davlindner, me, @pabbeel and @ancadianadragan, where we learn rewards from the state of the world. This HalfCheetah was trained from a single state sampled from a balancing policy!\n\nüí° Blog: <LINK>\nüìë Paper: <LINK>\n\n(1/5) <LINK>', 'Given an optimized state, we infer which goals would lead one to enter that state. We previously introduced Reward Learning by Simulating the Past (RLSP), which works in gridworlds. Now we scale it up using function approximation to get Deep RLSP. https://t.co/D08S9quMDr\n\n(2/5)', 'To simulate the past, we learn an inverse dynamics model that predicts past states, and an inverse policy that predicts past actions. Chaining these together allows our algorithm to simulate the past.\n\n(3/5) https://t.co/9dUuILU8tf', 'We train a featurization using self-supervised learning on random rollout data, and use a linear reward on top of the features. Gradient ascent pushes the reward to produce behavior similar to what happened in the past. (4/5) https://t.co/BZAK9rdFKf', 'üé• More videos on our website: https://t.co/yEXerw2Z6p\nüí° Read our blog post: https://t.co/VhuaYKAo4c\nüìë And the paper for details: https://t.co/jZbmzDPYJq\nüíª Code: https://t.co/BIJ0FcDHqz\n\nVisit our poster at #ICLR2021 poster session 10, Thursday (May 06) at 08:00 UTC.\n\n(5/5)']",https://arxiv.org/abs/2104.03946,"Since reward functions are hard to specify, recent work has focused on learning policies from human feedback. However, such approaches are impeded by the expense of acquiring such feedback. Recent work proposed that agents have access to a source of information that is effectively free: in any environment that humans have acted in, the state will already be optimized for human preferences, and thus an agent can extract information about what humans want from the state. Such learning is possible in principle, but requires simulating all possible past trajectories that could have led to the observed state. This is feasible in gridworlds, but how do we scale it to complex tasks? In this work, we show that by combining a learned feature encoder with learned inverse models, we can enable agents to simulate human actions backwards in time to infer what they must have done. The resulting algorithm is able to reproduce a specific skill in MuJoCo environments given a single state sampled from the optimal policy for that skill. ",Learning What To Do by Simulating the Past
5,1389432020967690243,1351642759405383680,Mark Miller,"['New paper with Felix Schoeller, @royesal and @FarlKriston titled ‚ÄúTrust as Extended Control: Active Inference and User Feedback During Human-Robot Collaboration‚Äù is preprinted now! <LINK>. Exploring the predictive dynamics underlaying human-robot collaborations.']",https://arxiv.org/abs/2104.11153,"To interact seamlessly with robots, users must infer the causes of a robot's behavior and be confident about that inference. Hence, trust is a necessary condition for human-robot collaboration (HRC). Despite its crucial role, it is largely unknown how trust emerges, develops, and supports human interactions with nonhuman artefacts. Here, we review the literature on trust, human-robot interaction, human-robot collaboration, and human interaction at large. Early models of trust suggest that trust entails a trade-off between benevolence and competence, while studies of human-to-human interaction emphasize the role of shared behavior and mutual knowledge in the gradual building of trust. We then introduce a model of trust as an agent's best explanation for reliable sensory exchange with an extended motor plant or partner. This model is based on the cognitive neuroscience of active inference and suggests that, in the context of HRC, trust can be cast in terms of virtual control over an artificial agent. In this setting, interactive feedback becomes a necessary component of the trustor's perception-action cycle. The resulting model has important implications for understanding human-robot interaction and collaboration, as it allows the traditional determinants of human trust to be defined in terms of active inference, information exchange and empowerment. Furthermore, this model suggests that boredom and surprise may be used as markers for under and over-reliance on the system. Finally, we examine the role of shared behavior in the genesis of trust, especially in the context of dyadic collaboration, suggesting important consequences for the acceptability and design of human-robot collaborative systems. ","Trust as Extended Control: Active Inference and User Feedback During
  Human-Robot Collaboration"
6,1389240757115133952,1012125662117851136,Edward Kennedy,"['.@manjarips &amp; I have a new paper on nonparametric estimation of population size:\n\n<LINK>\n\nThis is a classic problem w/ a long history &amp; lots of important applications - our paper focuses on how to flexibly &amp; efficiently incorporate complex covariate information <LINK>', 'We make 4 main contributions:\n\n- nonparametric efficiency bound (ie best possible estimation error)\n\n- new doubly robust estimator, w/non-asymptotic error guarantees\n\n- method for turning generic capture % estimates into pop size CI\n\n- analysis of impact of armed conflict in Peru']",https://arxiv.org/abs/2104.14091,"Estimation of population size using incomplete lists (also called the capture-recapture problem) has a long history across many biological and social sciences. For example, human rights and other groups often construct partial and overlapping lists of victims of armed conflicts, with the hope of using this information to estimate the total number of victims. Earlier statistical methods for this setup either use potentially restrictive parametric assumptions, or else rely on typically suboptimal plug-in-type nonparametric estimators; however, both approaches can lead to substantial bias, the former via model misspecification and the latter via smoothing. Under an identifying assumption that two lists are conditionally independent given measured covariate information, we make several contributions. First, we derive the nonparametric efficiency bound for estimating the capture probability, which indicates the best possible performance of any estimator, and sheds light on the statistical limits of capture-recapture methods. Then we present a new estimator, and study its finite-sample properties, showing that it has a double robustness property new to capture-recapture, and that it is near-optimal in a non-asymptotic sense, under relatively mild nonparametric conditions. Next, we give a method for constructing confidence intervals for total population size from generic capture probability estimators, and prove non-asymptotic near-validity. Finally, we study our methods in simulations, and apply them to estimate the number of killings and disappearances attributable to different groups in Peru during its internal armed conflict between 1980 and 2000. ",Doubly robust capture-recapture methods for estimating population size
7,1389211086923124737,796541007177404416,Andrew Burkhardt,"['I got a new paper out! This time: the detection of indene (C9H8), a pure hydrocarbon PAH! GOTHAM so far only detected rings with a CN group, so finding rings with only C&amp;H could allow us to infer the abundance of symmetric species like benzene/naphthalene <LINK>', 'Indene was also found to be the most abundant of ring found in GOTHAM, even the original C6H5CN, which has important implications for multi-ring and PAH formation.', 'For the models, as before, we found there is WAY more of these aromatic molecules than our models predict. But, we discuss some promising avenues\xa0that GOTHAM is currently exploring to get our models to make more rings!', ""I obviously didn't do this alone: this includes some sweet new lab work by Bryan Changala &amp; @astrochembrett, improvements to the spectral stacking procedure by @cmmmsubmm &amp; Ryan Loomis, and helping me pretend to be a chemist by @cnshingledecker &amp;  @Ilsastellar with the modeling."", 'Thanks as always to @GreenBankObserv for the fantastic support of our GOTHAM project.']",https://arxiv.org/abs/2104.15117,"Polycyclic Aromatic Hydrocarbons (PAHs) have long been invoked in the study of interstellar and protostellar sources, but the unambiguous identification of any individual PAH has proven elusive until very recently. As a result, the formation mechanisms for this important class of molecules remain poorly constrained. Here we report the first interstellar detection of a pure hydrocarbon PAH, indene (C$_9$H$_8$), as part of the GBT Observations of TMC-1: Hunting for Aromatic Molecules (GOTHAM) survey. This detection provides a new avenue for chemical inquiry, complementing the existing detections of CN-functionalized aromatic molecules. From fitting the GOTHAM observations, indene is found to be the most abundant organic ring detected in TMC-1 to date. And from astrochemical modeling with NAUTILUS, the observed abundance is greater than the model's prediction by several orders of magnitude suggesting that current formation pathways in astrochemical models are incomplete. The detection of indene in relatively high abundance implies related species such as cyanoindene, cyclopentadiene, toluene, and styrene may be detectable in dark clouds. ","Discovery of the Pure Polycyclic Aromatic Hydrocarbon Indene
  ($c$-C$_9$H$_8$) with GOTHAM Observations of TMC-1"
8,1389145027415314432,179222978,Yoav Len,"['A new paper with Nathan Ilten, in which we introduce a tropical version of the Gauss map, sending a point of a curve to the point in Grassmannian representing its tangent line. <LINK>']",https://arxiv.org/abs/2104.15059,"We consider the tropicalization of tangent lines to a complete intersection curve $X$ in $\mathbb{P}^n$. Under mild hypotheses, we describe the tropicalization of the image of the Gauss map of $X$ in terms of the tropicalizations of the hypersurfaces cutting out $X$. We apply this to obtain descriptions of the tropicalization of the dual variety $X^*$ and tangential variety $\tau(X)$ of $X$. In particular, we are able to compute the degrees of $X^*$ and $\tau(X)$ and the Newton polytope of $\tau(X)$ without using any elimination theory. ",Tropical tangents for complete intersection curves
9,1389046859667226637,1232357695124049921,Brian Okorn,"['Skip retraining your vision system every time you find a new object. ZePHyR can estimate the 6D pose of novel objects, not seen at training time. #ICRA2021\n\nProject: <LINK>\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2104.13526,"Pose estimation is a basic module in many robot manipulation pipelines. Estimating the pose of objects in the environment can be useful for grasping, motion planning, or manipulation. However, current state-of-the-art methods for pose estimation either rely on large annotated training sets or simulated data. Further, the long training times for these methods prohibit quick interaction with novel objects. To address these issues, we introduce a novel method for zero-shot object pose estimation in clutter. Our approach uses a hypothesis generation and scoring framework, with a focus on learning a scoring function that generalizes to objects not used for training. We achieve zero-shot generalization by rating hypotheses as a function of unordered point differences. We evaluate our method on challenging datasets with both textured and untextured objects in cluttered scenes and demonstrate that our method significantly outperforms previous methods on this task. We also demonstrate how our system can be used by quickly scanning and building a model of a novel object, which can immediately be used by our method for pose estimation. Our work allows users to estimate the pose of novel objects without requiring any retraining. Additional information can be found on our website this https URL ",ZePHyR: Zero-shot Pose Hypothesis Rating
10,1389013404027736066,101980926,Masahito Yamazaki,"['My new paper!\n""Chern-Simons Invariants from Ensemble Averages""\n(Meer Ashwinkumar, Matthew Dodelson, Abhiram Kidambi, Jacob M. Leedom, Masahito Yamazaki)\n<LINK>']",https://arxiv.org/abs/2104.14710,"We discuss ensemble averages of two-dimensional conformal field theories associated with an arbitrary indefinite lattice with integral quadratic form $Q$. We provide evidence that the holographic dual after the ensemble average is the three-dimensional Abelian Chern-Simons theory with kinetic term determined by $Q$. The resulting partition function can be written as a modular form, expressed as a sum over the partition functions of Chern-Simons theories on lens spaces. For odd lattices, the dual bulk theory is a spin Chern-Simons theory, and we identify several novel phenomena in this case. We also discuss the holographic duality prior to averaging in terms of Maxwell-Chern-Simons theories. ",Chern-Simons Invariants from Ensemble Averages
11,1388549201819312129,1661813766,Mehdi Kamani,"['New paper! We introduce a first-order algorithm:\n‚Ä¢ Converges to a point on the #ParetoFrontier with the desired level of trade-offs in #MultiobjectiveOptimization\n‚Ä¢ Traces other points on the Pareto frontier\n‚Ä¢ SOTA results on #Fairness aware learning\n<LINK> <LINK>', 'Using the proposed Preference-based Pareto Descent Optimization, unlike other approaches we can trace other points on the Pareto frontier using only first-order information while converging to the desired point on that set.\n#MultiobjectiveOptimization #Pareto https://t.co/ylx4Lqgd7F', ""The application of this approach to the #fairness problem shows SOTA performance with finding many points on the Pareto frontier for better and enhanced decision making in terms of fairness. Solutions found by our algorithm mostly dominate other SOTA's solutions.\n\nPlease RT! https://t.co/c3f1c8HaZn""]",https://arxiv.org/abs/2104.01634,"As algorithmic decision-making systems are becoming more pervasive, it is crucial to ensure such systems do not become mechanisms of unfair discrimination on the basis of gender, race, ethnicity, religion, etc. Moreover, due to the inherent trade-off between fairness measures and accuracy, it is desirable to learn fairness-enhanced models without significantly compromising the accuracy. In this paper, we propose Pareto efficient Fairness (PEF) as a suitable fairness notion for supervised learning, that can ensure the optimal trade-off between overall loss and other fairness criteria. The proposed PEF notion is definition-agnostic, meaning that any well-defined notion of fairness can be reduced to the PEF notion. To efficiently find a PEF classifier, we cast the fairness-enhanced classification as a bilevel optimization problem and propose a gradient-based method that can guarantee the solution belongs to the Pareto frontier with provable guarantees for convex and non-convex objectives. We also generalize the proposed algorithmic solution to extract and trace arbitrary solutions from the Pareto frontier for a given preference over accuracy and fairness measures. This approach is generic and can be generalized to any multicriteria optimization problem to trace points on the Pareto frontier curve, which is interesting by its own right. We empirically demonstrate the effectiveness of the PEF solution and the extracted Pareto frontier on real-world datasets compared to state-of-the-art methods. ","Pareto Efficient Fairness in Supervised Learning: From Extraction to
  Tracing"
12,1387952437823221763,1614231872,Aaron Fisher,"['New paper alert! \n\nA way to augment moving window models with features that describe long-term patterns, such as ""time since last instance of state k.""\n\n<LINK>\n\nIt\'s based on optimizing a well-known, but heuristic method for actigraphy (Webster\'s rescoring rules) <LINK>', ""Please feel welcome to DM me (or reply here, up to you!) if you have any thoughts, questions, criticisms or comments. They'd be very much appreciated.""]",https://arxiv.org/abs/2104.14291,"Analyzing temporal data (e.g., wearable device data) requires a decision about how to combine information from the recent and distant past. In the context of classifying sleep status from actigraphy, Webster's rescoring rules offer one popular solution based on the long-term patterns in the output of a moving-window model. Unfortunately, the question of how to optimize rescoring rules for any given setting has remained unsolved. To address this problem and expand the possible use cases of rescoring rules, we propose rephrasing these rules in terms of epoch-specific features. Our features take two general forms: (1) the time lag between now and the most recent [or closest upcoming] bout of time spent in a given state, and (2) the length of the most recent [or closest upcoming] bout of time spent in a given state. Given any initial moving window model, these features can be defined recursively, allowing for straightforward optimization of rescoring rules. Joint optimization of the moving window model and the subsequent rescoring rules can also be implemented using gradient-based optimization software, such as Tensorflow. Beyond binary classification problems (e.g., sleep-wake), the same approach can be applied to summarize long-term patterns for multi-state classification problems (e.g., sitting, walking, or stair climbing). We find that optimized rescoring rules improve the performance of sleep-wake classifiers, achieving accuracy comparable to that of certain neural network architectures. ","Optimizing Rescoring Rules with Interpretable Representations of
  Long-Term Information"
13,1387585352374767617,174173896,Yoshinari Fujinuma,"['Check out our new paper <LINK> w/ @mhagiwara on joint readability assessment (on the six CEFR levels) for words and documents with a GCN.\nWe observe and exploit what we call the ""recursive relationship between word and document difficulty"". <LINK>']",https://arxiv.org/abs/2104.13103,"Readability or difficulty estimation of words and documents has been investigated independently in the literature, often assuming the existence of extensive annotated resources for the other. Motivated by our analysis showing that there is a recursive relationship between word and document difficulty, we propose to jointly estimate word and document difficulty through a graph convolutional network (GCN) in a semi-supervised fashion. Our experimental results reveal that the GCN-based method can achieve higher accuracy than strong baselines, and stays robust even with a smaller amount of labeled data. ",Semi-Supervised Joint Estimation of Word and Document Readability
14,1387575617680445442,2956121356,Russ Salakhutdinov,"['New work on Document-Grounded Text Generation, focusing on Wikipedia update generation and Dialogue response generation tasks. \n\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2104.12714,"Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks. ",Focused Attention Improves Document-Grounded Generation
15,1387568794650902529,2337598033,Geraint F. Lewis,"['Paper day! New results from the S5 survey on the demise of a dwarf galaxy in the Galactic halo!\n\nLed by Terese Hansen, authors include @alexanderpji @sazabi_li @andycaseydesign\n@deniserkal @kwkuehn @dougalmackey @FadAstra @norashipp @JossBlandHawtho\n\n<LINK> <LINK>']",https://arxiv.org/abs/2104.13883,"The recently discovered Indus stellar stream exhibits a diverse chemical signature compared to what is found for most other streams due to the abundances of two outlier stars, Indus$\_$0 and Indus$\_$13. Indus$\_$13, exhibits an extreme enhancement in rapid neutron-capture ($r$-)process elements with $\mathrm{[Eu/Fe]} = +1.81$. It thus provides direct evidence of the accreted nature of $r$-process enhanced stars. In this paper we present a detailed chemical analysis of the neutron-capture elements in Indus$\_$13, revealing the star to be slightly actinide poor. The other outlier, Indus$\_0$, displays a globular cluster-like signature with high N, Na, and Al abundances, while the rest of the Indus stars show abundances compatible with a dwarf galaxy origin. Hence, Indus$\_0$ provides the first chemical evidence of a fully disrupted dwarf containing a globular cluster. We use the chemical signature of the Indus stars to discuss the nature of the stream progenitor which was likely a chemically evolved system, with a mass somewhere in the range from Ursa Minor to Fornax. ","${S}^5$: The destruction of a bright dwarf galaxy as revealed by the
  chemistry of the Indus stellar stream"
16,1387473961362104323,311165615,Julian Posada,"[""New paper! ‚ÄúWe Haven't Gone Paperless Yet: Why the Printing Press Can Help Us Understand Data and AI.‚Äù \nCo-written with @wendyhwong and @nicholas_weller. Part of my work as an RA for @TorontoSRI. Final version available in the proceedings of @AIESConf. <LINK>""]",https://arxiv.org/abs/2104.12731,"How should we understand the social and political effects of the datafication of human life? This paper argues that the effects of data should be understood as a constitutive shift in social and political relations. We explore how datafication, or quantification of human and non-human factors into binary code, affects the identity of individuals and groups. This fundamental shift goes beyond economic and ethical concerns, which has been the focus of other efforts to explore the effects of datafication and AI. We highlight that technologies such as datafication and AI (and previously, the printing press) both disrupted extant power arrangements, leading to decentralization, and triggered a recentralization of power by new actors better adapted to leveraging the new technology. We use the analogy of the printing press to provide a framework for understanding constitutive change. The printing press example gives us more clarity on 1) what can happen when the medium of communication drastically alters how information is communicated and stored; 2) the shift in power from state to private actors; and 3) the tension of simultaneously connecting individuals while driving them towards narrower communities through algorithmic analyses of data. ","We Haven't Gone Paperless Yet: Why the Printing Press Can Help Us
  Understand Data and AI"
17,1387404853006188555,2894532745,Priya L. Donti,"['Deep learning methods often struggle to satisfy hard constraints, limiting their practical use in domains such as power systems.\n\nWe tackle this challenge in our new #ICLR2021 paper on DL for approximate optimization:\n<LINK>\n\nw/ @david_rolnick &amp; @zicokolter\n\n1/ <LINK>', 'Quick notation: Our goal is to approximate optimization problems of the form shown here, where the inputs x parameterize the objective function and constraints, and we‚Äôre trying to output an optimal decision variable y. 2/ https://t.co/HB84LNFdRM', 'Our deep learning framework for this problem, DC3, has two main parts: (1) equality completion, which ensures that the equality constraints of our optimization problem are satisfied, and (2) inequality correction, which aims to satisfy any violated inequality constraints. 3/ https://t.co/333h50g3lU', '(1) Equality completion: Given x, rather than directly outputting the full-dimensional optimization solution y, we first output a subset of the variables. We then infer the remaining variables via the equality constraints. This ensures the equality constraints are satisfied. 4/ https://t.co/5HZbbpj8Kn', '(2) Inequality correction: Next, we aim to satisfy any violated inequality constraints in a way that makes sure the equality constraints *stay* satisfied. We do this by taking gradient steps along the manifold of points satisfying the equality constraints. 5/ https://t.co/IaIl3YU7Zy', 'Together, steps (1) and (2) yield an output that is feasible with respect to all constraints. These steps are also fully differentiable (notably, if the equality constraints represent an implicit function, you can use implicit differentiation techniques). 6/', 'We train the network end-to-end to minimize a ‚Äúsoft loss‚Äù function that captures the optimization objective as well as a soft version of the constraints. (Subtle point: This latter part helps improve the convergence of the inequality correction procedure.) 7/ https://t.co/RS5itD8c4h', 'We demonstrate our method in three different settings - convex QPs, a synthetic nonconvex setting, and a power system optimization problem called AC optimal power flow. We find that our method performs well in terms of objective value, while satisfying the constraints. 8/ https://t.co/4yLQLqbr11', 'E.g., for AC optimal power flow, we find that we‚Äôre about 10x faster than the traditional optimizer, with only a 0.22% optimality gap. And while all the other deep learning methods generally fail to satisfy the constraints, our method *does* satisfy these constraints. 9/ https://t.co/oT4eoKTMEd', 'In summary, we show that DC3 is able to incorporate (potentially non-convex) equality and inequality constraints into DL-based optimization algorithms. Bonus: Since DC3 is fully differentiable, the learned approximators can also be used within broader deep learning pipelines! 10/ https://t.co/9R4Zshdn7S', 'If you‚Äôre interested in chatting further about this work, @david_rolnick and I will be presenting our poster at #ICLR2021 on Tue, May 4 from 9-11am Pacific, in poster room 5B. Hope to see you there! 11/11 https://t.co/1wipWaY4fH', '@unsorsodicorda @david_rolnick @zicokolter Quite different settings, actually! A commonality is that they do both use implicit layers. The robust control work uses implicit layers to enforce Lyapunov decrease conditions via projection. DC3 uses an implicit layer to solve/differentiate thru implicit equality constraints.', ""@HassanLHijazi @david_rolnick @zicokolter Thanks, and great question! No current plans to participate, but I'd be game to coordinate with existing teams for whom this could be useful.\n\nIf the ACOPF solver doesn't need to be fully differentiable, @kyrib's model may be also be a good choice: https://t.co/ILNh8etS3L"", ""@kyrib @nandofioretto @david_rolnick @zicokolter Thanks @nandofioretto! And yup, we predict p_g at the generator buses and |v| at the generator and slack buses, given p_d and q_d at all buses and voltage angle at the slack bus. (Same setup as @kyrib's paper, Section 2: https://t.co/ILNh8etS3L)"", ""@kyrib @nandofioretto @david_rolnick @zicokolter That said, technically, our method doesn't require uniqueness of the solutions to the power flow equations -- just that we can find solutions (i.e., that we have a set of values for which the implicit function theorem holds)."", ""@kyrib @nandofioretto @david_rolnick @zicokolter We did keep the slack generator non-fixed, but there was still some brittleness in the naive Newton-Raphson solver. We ended up just implementing the version that MATPOWER does (see Section 4.1: https://t.co/KQUDyi8GWi), which is what's described in Appendix C."", '@unsorsodicorda @david_rolnick @zicokolter You can sort of think of a learned DC3 model as being a cheaper replacement to a traditional implicit optimization layer (e.g., you could use this method to learn an approximate differentiable optimizer, and then stick that in in place of the ""real"" differentiable optimizer)', '@unsorsodicorda @david_rolnick @zicokolter But then perhaps confusingly, DC3 itself uses an implicit layer to actually learn this approximation (though the implicit layer DC3 uses is much simpler - it uses Newton-Raphson to solve nonlinear inequality constraints, and then differentiates thru these constraints at the soln)', ""@unsorsodicorda @david_rolnick @zicokolter I suppose simpler is in the eye of the beholder, in that they're conceptually different. Traditional differentiable convex optimization layers involve writing down KKT conditions/fixed point equations and implicitly differentiating through those. 1/"", '@unsorsodicorda @david_rolnick @zicokolter DC3 involves a feedforward neural network + a (potentially implicit) equality constraint solver + gradient steps to resolve the inequality constraints.']",https://arxiv.org/abs/2104.12225,"Large optimization problems with hard constraints arise in many settings, yet classical solvers are often prohibitively slow, motivating the use of deep networks as cheap ""approximate solvers."" Unfortunately, naive deep learning approaches typically cannot enforce the hard constraints of such problems, leading to infeasible solutions. In this work, we present Deep Constraint Completion and Correction (DC3), an algorithm to address this challenge. Specifically, this method enforces feasibility via a differentiable procedure, which implicitly completes partial solutions to satisfy equality constraints and unrolls gradient-based corrections to satisfy inequality constraints. We demonstrate the effectiveness of DC3 in both synthetic optimization tasks and the real-world setting of AC optimal power flow, where hard constraints encode the physics of the electrical grid. In both cases, DC3 achieves near-optimal objective values while preserving feasibility. ",DC3: A learning method for optimization with hard constraints
18,1387239706933424131,49693598,Ajinkya Kale,['New paper alert! Joint work with @AdobeResearch \nMultimodal Contrastive Training for Visual Representation Learning\n<LINK> <LINK>'],https://arxiv.org/abs/2104.12836,"We develop an approach to learning visual representations that embraces multimodal data, driven by a combination of intra- and inter-modal similarity preservation objectives. Unlike existing visual pre-training methods, which solve a proxy prediction task in a single domain, our method exploits intrinsic data properties within each modality and semantic information from cross-modal correlation simultaneously, hence improving the quality of learned visual representations. By including multimodal training in a unified framework with different types of contrastive losses, our method can learn more powerful and generic visual features. We first train our model on COCO and evaluate the learned visual representations on various downstream tasks including image classification, object detection, and instance segmentation. For example, the visual representations pre-trained on COCO by our method achieve state-of-the-art top-1 validation accuracy of $55.3\%$ on ImageNet classification, under the common transfer protocol. We also evaluate our method on the large-scale Stock images dataset and show its effectiveness on multi-label image tagging, and cross-modal retrieval tasks. ",Multimodal Contrastive Training for Visual Representation Learning
19,1387212947257233415,91634245,Brad Marston,['A new paper by Zekun Zhuang with some small assistance from me entitled ‚ÄúSpin Transport in Quantum Spin-Orbital Liquids‚Äù has appeared on the arXiv.   See <LINK>'],https://arxiv.org/abs/2104.12887,"Quantum spin-orbital liquids (QSOLs) are a novel phase of matter, similar to quantum spin liquids, with quantum fluctuations in both spin and orbital degrees of freedom. We use non-equilibrium Green's function theory to study out-of-equilibrium spin transport in an exactly solvable QSOL model put forward by Yao and Lee. We find that the spin transport problem can be mapped to that of a free fermion problem with effective fermionic baths that have rapidly varying density of states. In the gapless phase, the spin current $I_s-V_s$ relation is thus highly nonlinear, while in the chiral gapped phase, the spin current conductance is quantized to be $1/2\pi$ provided that the contacts are sufficiently wide. The quantized conductance is a signature of the topological nature of the chiral gapped QSOL. ",Spin Transport in Quantum Spin-Orbital Liquids
20,1387109048345001987,1008944276431036416,Boris Ivanovic,"['New paper on arXiv!! In it, we propose a traj. forecasting method (HAICU) that propagates semantic uncertainty from upstream perception through the model, releasing a new dataset for investigating Perceptual Uncertainty in Prediction (PUP) along the way! <LINK> <LINK>', 'The results of a great collaboration between @StanfordASL, @StanfordEng, and @ToyotaResearch! In particular, with researchers from the machine learning research team: Kuan-Hui Lee, @ptokmakov, @wulfebw, @rowantmc, @adnothing, as well as the one and only @MarcoPavoneSU!']",https://arxiv.org/abs/2104.12446,"Reasoning about the future behavior of other agents is critical to safe robot navigation. The multiplicity of plausible futures is further amplified by the uncertainty inherent to agent state estimation from data, including positions, velocities, and semantic class. Forecasting methods, however, typically neglect class uncertainty, conditioning instead only on the agent's most likely class, even though perception models often return full class distributions. To exploit this information, we present HAICU, a method for heterogeneous-agent trajectory forecasting that explicitly incorporates agents' class probabilities. We additionally present PUP, a new challenging real-world autonomous driving dataset, to investigate the impact of Perceptual Uncertainty in Prediction. It contains challenging crowded scenes with unfiltered agent class probabilities that reflect the long-tail of current state-of-the-art perception systems. We demonstrate that incorporating class probabilities in trajectory forecasting significantly improves performance in the face of uncertainty, and enables new forecasting capabilities such as counterfactual predictions. ","Heterogeneous-Agent Trajectory Forecasting Incorporating Class
  Uncertainty"
21,1387078221707939841,1062741167371141123,Neel Guha,"['Just how much does domain-specific pretraining help for legal NLP tasks? We studied this by creating ‚ÄúCaseHOLD‚Äù -- a new benchmark for precedential reasoning in law. \n\nPaper: <LINK>\nBlog: <LINK>\n1/6 <LINK>', 'Strangely, existing work has noticed that pretraining BERT on legal opinions produces only marginal improvements for legal NLP benchmarks. We hypothesized that existing benchmarks were either too easy, or had language that was too dissimilar from that found in legal opinions.\n2/6', 'We thus created CaseHOLD: a benchmark of 53,000+ multiple choice questions corresponding to the task of determining the appropriate holding to cite for a legal argument. As lawyers may recognize, CaseHOLD replicates a task essential to the practice of law.\n3/6 https://t.co/DuZc5rljlI', 'In evaluations, we found that legal domain specific pretraining (over 3,446,187 judicial opinions) produced substantial performance improvements over CaseHOLD, but only marginal improvements for other benchmarks! \n4/6 https://t.co/Xq1MiazMMI', 'If you‚Äôre interested in learning more about the CaseHOLD dataset, our pre-training process, or our analysis of the Legal-BERT model -- check out the full paper!  You can also download our models and the CaseHOLD dataset from here: https://t.co/HDxuQK0D9r\n5/6', ""This work was done by @lucia__zheng, Brandon Anderson, @PeterHndrsn, and @DanHo1. Special thanks to the following for invaluable help. Please reach out if you have questions or feedback -- we're very excited about further work in this direction!\n6/6 https://t.co/FScxN7gbCA""]",https://arxiv.org/abs/2104.08671,"While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (using corpus of approximately 3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2% on F1, representing a 12% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language. ","When Does Pretraining Help? Assessing Self-Supervised Learning for Law
  and the CaseHOLD Dataset"
22,1387031166247743491,3306943245,Konstantin Klemmer,"['New (short) paper out today, accepted at #AIMOCC workshop @ #ICLR2021! Preliminary results from our work-in-progress on conditional GANs for simulating extreme weather patterns.\n\nJoint work with @sudipansaha, Matthias Kahl, @linylinx and @xiaoxiang_zhu. \n\n<LINK> <LINK>']",https://arxiv.org/abs/2104.12469,"Deep generative models are increasingly used to gain insights in the geospatial data domain, e.g., for climate data. However, most existing approaches work with temporal snapshots or assume 1D time-series; few are able to capture spatio-temporal processes simultaneously. Beyond this, Earth-systems data often exhibit highly irregular and complex patterns, for example caused by extreme weather events. Because of climate change, these phenomena are only increasing in frequency. Here, we proposed a novel GAN-based approach for generating spatio-temporal weather patterns conditioned on detected extreme events. Our approach augments GAN generator and discriminator with an encoded extreme weather event segmentation mask. These segmentation masks can be created from raw input using existing event detection frameworks. As such, our approach is highly modular and can be combined with custom GAN architectures. We highlight the applicability of our proposed approach in experiments with real-world surface radiation and zonal wind data. ","Generative modeling of spatio-temporal weather patterns with extreme
  event conditioning"
23,1386985667343884288,104207361,Amos Folarin,"['üö®New Paper (preprint)üö® We explored the use of Nearby Bluetooth Device Count (NBDC) remote monitoring as a means to predict depressive states in RADAR-CNS @yuezhou_zhang @RADARCNS @radar_base  @richdobson #Depression\n<LINK> <LINK>', 'We hypothesize that NBDC (a complex signal) may encode aspects of the local personal environment that could impact or be impacted by depressive state symptoms. https://t.co/Yn7I7MW9P4', '2nd order statistical features, multiscale entropy and frequency domain features were generated from 2886 14-day PHQ-8 intervals from 316 participants. We investigated how much a baseline forecasting model (prior interval PHQ-8) was improved using NBDC features.']",https://arxiv.org/abs/2104.12407,"The Bluetooth sensor embedded in mobile phones provides an unobtrusive, continuous, and cost-efficient means to capture individuals' proximity information, such as the nearby Bluetooth devices count (NBDC). The continuous NBDC data can partially reflect individuals' behaviors and status, such as social connections and interactions, working status, mobility, and social isolation and loneliness, which were found to be significantly associated with depression by previous survey-based studies. This paper aims to explore the NBDC data's value in predicting depressive symptom severity as measured via the 8-item Patient Health Questionnaire (PHQ-8). The data used in this paper included 2,886 bi-weekly PHQ-8 records collected from 316 participants recruited from three study sites in the Netherlands, Spain, and the UK as part of the EU RADAR-CNS study. From the NBDC data two weeks prior to each PHQ-8 score, we extracted 49 Bluetooth features, including statistical features and nonlinear features for measuring periodicity and regularity of individuals' life rhythms. Linear mixed-effect models were used to explore associations between Bluetooth features and the PHQ-8 score. We then applied hierarchical Bayesian linear regression models to predict the PHQ-8 score from the extracted Bluetooth features. A number of significant associations were found between Bluetooth features and depressive symptom severity. Compared with commonly used machine learning models, the proposed hierarchical Bayesian linear regression model achieved the best prediction metrics, R2= 0.526, and root mean squared error (RMSE) of 3.891. Bluetooth features can explain an extra 18.8% of the variance in the PHQ-8 score relative to the baseline model without Bluetooth features (R2=0.338, RMSE = 4.547). ","Predicting Depressive Symptom Severity through Individuals' Nearby
  Bluetooth Devices Count Data Collected by Mobile Phones: A Preliminary
  Longitudinal Study"
24,1386952043764633600,1263870728870469632,Enrico Ronca,['Our new paper on Vibrational strong coupling effects on chemical reactivity is out on ArXiv.\n\n<LINK>'],https://arxiv.org/abs/2104.12429,"Strong light-matter interaction in cavity environments is emerging as a promising approach to control chemical reactions in a non-intrusive and efficient manner. The underlying mechanism that distinguishes between steering, accelerating, or decelerating a chemical reaction has, however, remained unclear, hampering progress in this frontier area of research. We leverage quantum-electrodynamical density-functional theory to unveil the microscopic mechanism behind the experimentally observed reduced reaction rate under cavity induced resonant vibrational strong light-matter coupling. We observe multiple resonances and obtain the thus far theoretically elusive but experimentally critical resonant feature for a single strongly-coupled molecule undergoing the reaction. While we do not explicitly account for collective coupling or intermolecular interactions, the qualitative agreement with experimental measurements suggests that our conclusions can be largely abstracted towards the experimental realization. Specifically, we find that the cavity mode acts as mediator between different vibrational modes. In effect, vibrational energy localized in single bonds that are critical for the reaction is redistributed which ultimately inhibits the reaction. ","Shining Light on the Microscopic Resonant Mechanism Responsible for
  Cavity-Mediated Chemical Reactivity"
25,1386933153835663362,2503999452,Arnau Rios,"['New paper today on #ArXiv <LINK> with @BindingBlocks colleagues @AleStyle81 @amromero92 @PhysicsatYork @UNCPhysics. We provide free, easy-to-use worksheets to develop #Outreach activities on #NeutronStars &amp; discuss the #ExoticPhysics involved in these objects <LINK>', 'Activity 1 (Minimum Mass) continues our past work on #LiquidDrop #BindingEnergy activities in https://t.co/CkJd8ZM5M2 (published here https://t.co/NfTASN08p4). Activity 2 (Maximum mass) uses an analytical model for the #Interior of #NeutronStars https://t.co/KS37xWnuxF', 'We fill the gap in past literature by focusing on activities for #Alevel students. #Outreach work to discuss recent advances by @LIGO, #NICER &amp; future #SpaceMissions. Limits of theoretical modelling can also be assessed. Watch this space for more, updated #worksheets! https://t.co/9gXhcSArzw']",https://arxiv.org/abs/2104.12449,"We introduce two simple online activities to explore the physics of neutron stars. These provide an introduction to the basic properties of compact objects, like their masses and radii, for secondary school students. The first activity explores the idea of the minimum mass of a neutron star. It is directly linked to the concept of binding energy and follows on from our previous activities. The second activity focuses on the maximum mass of neutron stars using a solvable model of the neutron star interior. The activities are based on spreadsheets, provided as Supplementary Material, and can be easily adapted to different levels, age groups and discussion topics. In particular, these activities can naturally lead towards discussions on extrapolations and limits of theoretical models. ","From nuclei to neutron stars: simple binding energy computer modelling
  in the classroom (part 2)"
26,1386849717938802691,321794593,Jos√© G. Fern√°ndez-Trincado,['Our new accepted paper using APOGEE-2 Southern spectra from The Ir√©n√©e du Pont Telescope (@LCOAstro)  toward my favorite Globular Cluster Omega Centauri is today on ArXiv üëâ<LINK> üëá <LINK>'],https://arxiv.org/abs/2104.12075,"We study the multiple populations of $\omega$ Cen by using the abundances of Fe, C, N, O, Mg, Al, Si, K, Ca, and Ce from the high-resolution, high signal-to-noise (S/N$>$70) spectra of 982 red giant stars observed by the SDSS-IV/APOGEE-2 survey. We find that the shape of the Al-Mg and N-C anticorrelations changes as a function of metallicity, continuous for the metal-poor groups, but bimodal (or unimodal) at high metallicities. There are four Fe populations, similar to what has been found in previously published investigations, but we find seven populations based on Fe, Al, and Mg abundances. The evolution of Al in $\omega$ Cen is compared to its evolution in the Milky Way and in five representative globular clusters. We find that the distribution of Al in metal-rich stars of $\omega$ Cen closely follows what is observed in the Galaxy. Other $\alpha-$elements and C, N, O, and Ce are also compared to the Milky Way, and significantly elevated abundances are observed over what is found in the thick disk for almost all elements. However, we also find some stars with high metallicity and low [Al/Fe], suggesting that $\omega$ Cen could be the remnant core of a dwarf galaxy, but the existence of these peculiar stars needs an independent confirmation. We also confirm the increase in the sum of CNO as a function of metallicity previously reported in the literature and find that the [C/N] ratio appears to show opposite correlations between Al-poor and Al-rich stars as a function of metallicity. ","Homogeneous Analysis of Globular Clusters from the APOGEE Survey with
  the BACCHUS Code $-$ III. $\omega$ Cen"
27,1386651580297060354,1176867972163559424,MartinHuber,['Our new paper (joint with D. Imhof) on #DeepLearning for detecting #cartels. Applies #convolutional #neuralnets for image recognition to flag cartel participants based on graphs depicting price (or bid) interactions: <LINK> #EconTwitter #ArtificialIntelligence <LINK>'],https://arxiv.org/abs/2104.11142,"Adding to the literature on the data-driven detection of bid-rigging cartels, we propose a novel approach based on deep learning (a subfield of artificial intelligence) that flags cartel participants based on their pairwise bidding interactions with other firms. More concisely, we combine a so-called convolutional neural network for image recognition with graphs that in a pairwise manner plot the normalized bid values of some reference firm against the normalized bids of any other firms participating in the same tenders as the reference firm. Based on Japanese and Swiss procurement data, we construct such graphs for both collusive and competitive episodes (i.e when a bid-rigging cartel is or is not active) and use a subset of graphs to train the neural network such that it learns distinguishing collusive from competitive bidding patterns. We use the remaining graphs to test the neural network's out-of-sample performance in correctly classifying collusive and competitive bidding interactions. We obtain a very decent average accuracy of around 90% or slightly higher when either applying the method within Japanese, Swiss, or mixed data (in which Swiss and Japanese graphs are pooled). When using data from one country for training to test the trained model's performance in the other country (i.e. transnationally), predictive performance decreases (likely due to institutional differences in procurement procedures across countries), but often remains satisfactorily high. All in all, the generally quite high accuracy of the convolutional neural network despite being trained in a rather small sample of a few 100 graphs points to a large potential of deep learning approaches for flagging and fighting bid-rigging cartels. ","Deep learning for detecting bid rigging: Flagging cartel participants
  based on convolutional neural networks"
28,1386636119706685442,16837428,John Stott,['New paper from @chanmc_astro and myself. Using machine learning to predict galaxy cluster redshifts. Its called z-sequence which rhymes with red sequence not pea sequence üòâ  <LINK>'],https://arxiv.org/abs/2104.11335,"We introduce Z-Sequence, a novel empirical model that utilises photometric measurements of observed galaxies within a specified search radius to estimate the photometric redshift of galaxy clusters. Z-Sequence itself is composed of a machine learning ensemble based on the k-nearest neighbours algorithm. We implement an automated feature selection strategy that iteratively determines appropriate combinations of filters and colours to minimise photometric redshift prediction error. We intend for Z-Sequence to be a standalone technique but it can be combined with cluster finders that do not intrinsically predict redshift, such as our own DEEP-CEE. In this proof-of-concept study we train, fine-tune and test Z-Sequence on publicly available cluster catalogues derived from the Sloan Digital Sky Survey. We determine the photometric redshift prediction error of Z-Sequence via the median value of $|\Delta z|/(1+z)$ (across a photometric redshift range of $0.05 \le \textit{z} \le 0.6$) to be $\sim0.01$ when applying a small search radius. The photometric redshift prediction error for test samples increases by 30-50 per cent when the search radius is enlarged, likely due to line-of-sight interloping galaxies. Eventually, we aim to apply Z-Sequence to upcoming imaging surveys such as the Legacy Survey of Space and Time to provide photometric redshift estimates for large samples of as yet undiscovered and distant clusters. ","Z-Sequence: Photometric redshift predictions for galaxy clusters with
  sequential random k-nearest neighbours"
29,1386566348512366593,820031736914513925,Fabrizio Leisen,"['New paper: ""Bayesian predictive inference without a prior"" with P. Berti, E. Dreassi, L. Pratelli, P. Rigo. \n<LINK>']",http://arxiv.org/abs/2104.11643,"Let $(X_n:n\ge 1)$ be a sequence of random observations. Let $\sigma_n(\cdot)=P\bigl(X_{n+1}\in\cdot\mid X_1,\ldots,X_n\bigr)$ be the $n$-th predictive distribution and $\sigma_0(\cdot)=P(X_1\in\cdot)$ the marginal distribution of $X_1$. In a Bayesian framework, to make predictions on $(X_n)$, one only needs the collection $\sigma=(\sigma_n:n\ge 0)$. Because of the Ionescu-Tulcea theorem, $\sigma$ can be assigned directly, without passing through the usual prior/posterior scheme. One main advantage is that no prior probability has to be selected. In this paper, $\sigma$ is subjected to two requirements: (i) The resulting sequence $(X_n)$ is conditionally identically distributed, in the sense of Berti, Pratelli and Rigo (2004); (ii) Each $\sigma_{n+1}$ is a simple recursive update of $\sigma_n$. Various new $\sigma$ satisfying (i)-(ii) are introduced and investigated. For such $\sigma$, the asymptotics of $\sigma_n$, as $n\rightarrow\infty$, is determined. In some cases, the probability distribution of $(X_n)$ is also evaluated. ",Bayesian predictive inference without a prior
30,1386387633752977411,963873866392186882,Philipp Schindler,"['New paper on heating effects in #iontraps @uniinnsbruck <LINK> We had a detailed look into how an ion crystal can melt from a collision with background gas, and how to re-crystallize. We found a surprisingly simple model that is validated by the experiment. <LINK>']",https://arxiv.org/abs/2104.10623,"We investigate the energy dynamics of non-crystallized (melted) ions, confined in a Paul trap. The non-periodic Coulomb interaction experienced by melted ions forms a medium for non-conservative energy transfer from the radio-frequency (rf) field to the ions, a process known as rf heating. We study rf heating by analyzing numerical simulations of non-crystallized ion motion in Paul trap potentials, in which the energy of the ions' secular motion changes at discrete intervals, corresponding to ion-ion collisions. The analysis of these collisions is used as a basis to derive a simplified model of rf heating energy dynamics, from which we conclude that the rf heating rate is predominantly dependent on the rf field strength. We confirm the predictability of the model experimentally: Two trapped $^{40}$Ca$^{+}$ ions are deterministically driven to melt, and their fluorescence rate is used to infer the ions' energy. From simulation and experimental results, we generalize which experimental parameters are required for efficient recrystallization of melted trapped ions. ",RF-induced heating dynamics of non-crystallized trapped ions
31,1385642187073806336,40285266,Stanislav Fort at EAGx Prague ¬¨(üî•üìéüî•üìé),"['Our new paper is out <LINK> led by @james_r_lucas! Linear interpolation init-&gt;optimum often has ~monotonically decreasing loss &amp; this also holds on paths from unrelated(!) inits. Joint work w @james_r_lucas, @juhan_bae, @michaelrzhang, Rich Zemel, @RogerGrosse <LINK>']",http://arxiv.org/abs/2104.11044,"Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. (2014) persists in spite of the non-convex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network - providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g. network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties. ","Analyzing Monotonic Linear Interpolation in Neural Network Loss
  Landscapes"
32,1385631776798879747,69202541,Jonathan Le Roux,"['New paper by Takaaki Hori, Niko Moritz, Chiori Hori, and myself, ""Advanced Long-context End-to-end Speech Recognition Using Context-expanded Transformers,"" with new SOTA 17.3% CER on HKUST and 12.0%/6.3% WER on CallHome/Switchboard.\n<LINK>']",https://arxiv.org/abs/2104.09426,"This paper addresses end-to-end automatic speech recognition (ASR) for long audio recordings such as lecture and conversational speeches. Most end-to-end ASR models are designed to recognize independent utterances, but contextual information (e.g., speaker or topic) over multiple utterances is known to be useful for ASR. In our prior work, we proposed a context-expanded Transformer that accepts multiple consecutive utterances at the same time and predicts an output sequence for the last utterance, achieving 5-15% relative error reduction from utterance-based baselines in lecture and conversational ASR benchmarks. Although the results have shown remarkable performance gain, there is still potential to further improve the model architecture and the decoding process. In this paper, we extend our prior work by (1) introducing the Conformer architecture to further improve the accuracy, (2) accelerating the decoding process with a novel activation recycling technique, and (3) enabling streaming decoding with triggered attention. We demonstrate that the extended Transformer provides state-of-the-art end-to-end ASR performance, obtaining a 17.3% character error rate for the HKUST dataset and 12.0%/6.3% word error rates for the Switchboard-300 Eval2000 CallHome/Switchboard test sets. The new decoding method reduces decoding time by more than 50% and further enables streaming ASR with limited accuracy degradation. ","Advanced Long-context End-to-end Speech Recognition Using
  Context-expanded Transformers"
33,1385631300124557314,1296601241527758853,Mingyu Kang,"['Happy to write my first tweet on my first academic paper  w/ @bichenzh @Huang_Shilin @Gausswang @kenbrownquantum and more - now on arxiv! <LINK>\nWe present a new method of pulse optimization for two-qubit gates in trapped ion systems, using batch optimization.', 'Fun fact: 7 of 8 authors have ""ANG"" in their name, with \n@kenbrownquantum being the unfortunate symmetry breaker. The Ang-Gang!', ""@MJBiercuk Thank you Prof. Biercuk! I studied many papers you co-authored while writing this paper, so it's an honor hearing from you! I've seen this framework mentioned in our group meeting. I'll take a closer look!"", ""@MJBiercuk Thanks a lot!! Sounds really interesting. I'll def check the arxiv!""]",https://arxiv.org/abs/2104.06887,"Two-qubit gates in trapped-ion quantum computers are generated by applying spin-dependent forces that temporarily entangle the internal state of the ion with its motion. Laser pulses are carefully designed to generate a maximally entangling gate between the ions while minimizing any residual entanglement between the motion and the ion. The quality of the gates suffers when the actual experimental parameters differ from the ideal case. Here, we improve the robustness of frequency-modulated M{\o}lmer-S{\o}rensen gates to motional mode-frequency offsets by optimizing the average performance over a range of systematic errors using batch optimization. We then compare this method with frequency-modulated gates optimized for ideal parameters that include an analytic robustness condition. Numerical simulations show good performance up to 12 ions, and the method is experimentally demonstrated on a two-ion chain. ","Batch Optimization of Frequency-Modulated Pulses for Robust Two-qubit
  Gates in Ion Chains"
34,1385607983284080644,367022967,Sharief Hendricks,"['New Paper: Automated Tackle Injury Risk Assessment in Contact-Based Sports -- A Rugby Union Example with @UnitAfrican accepted in @IEEEXplore Computer Society Conference on Computer Vision and Pattern Recognition Workshops <LINK> #DeepLearning üèâüî¨ <LINK>', 'https://t.co/NPOwLJMaDZ']",https://arxiv.org/abs/2104.10916,"Video analysis in tackle-collision based sports is highly subjective and exposed to bias, which is inherent in human observation, especially under time constraints. This limitation of match analysis in tackle-collision based sports can be seen as an opportunity for computer vision applications. Objectively tracking, detecting and recognising an athlete's movements and actions during match play from a distance using video, along with our improved understanding of injury aetiology and skill execution will enhance our understanding how injury occurs, assist match day injury management, reduce referee subjectivity. In this paper, we present a system of objectively evaluating in-game tackle risk in rugby union matches. First, a ball detection model is trained using the You Only Look Once (YOLO) framework, these detections are then tracked by a Kalman Filter (KF). Following this, a separate YOLO model is used to detect persons/players within a tackle segment and then the ball-carrier and tackler are identified. Subsequently, we utilize OpenPose to determine the pose of ball-carrier and tackle, the relative pose of these is then used to evaluate the risk of the tackle. We tested the system on a diverse collection of rugby tackles and achieved an evaluation accuracy of 62.50%. These results will enable referees in tackle-contact based sports to make more subjective decisions, ultimately making these sports safer. ","Automated Tackle Injury Risk Assessment in Contact-Based Sports -- A
  Rugby Union Example"
35,1385396035195858944,3377160202,Djuna Croon,"['New paper! \n\nNon-perturbative methods for false vacuum decay\n<LINK>\nwith the amazing Eleanor Hall (@quarkygirl) and Hitoshi Muruyama (@sleptogenesis) \n\nWe propose a new (non-perturbative!) technique to calculate false vacuum decay rates. Important, because...', ""...accurate false vacuum decay calculations are needed to predict the resulting gravitational wave spectra. \n\nI'll try to give a brief explanation below, but I also highly recommend Nell's excellent slides on the topic: https://t.co/PlOhCeZI4H at #DarkSectorRainbow last month..."", '...So, accurately calculating the gravitational wave spectrum from a first order phase transition is a big challenge.\n\nExisting methods struggle particularly with strong coupling. Why?\n\nThe usual false vacuum decay formalism is well-defined for tree-level bounces, but...', ""...for radiatively induced phase-transitions, it needs modifications. A related issue is that an all-orders calculation of the effective action is manifestly convex. \n\nWhat does that mean? A convex potential doesn't have more than one minimum -&gt; no first order phase transition..."", '...""coarse-graining"" in momentum scale is a useful solution. However, the accuracy of coarse graining usually depends on a large ratio of scales or a weak coupling. \n\nWe propose an alternative, where we enforce locality in field space (see fig) rather than in momentum space... https://t.co/peSC3QKVDP', '...that can be done in the language of the functional renormalization group, as we show. \n\nMoreover, we work out a simple example, which we compare to the result found in other methods. As expected, the difference creep in at stronger coupling... https://t.co/tY9KJU24Mo', ""...We hope to develop this method further, and eventually study things like confinement / chiral symmetry breaking and the resulting gravitational wave spectra. I can't wait to learn more. \n\nA big thank you to my wonderful collaborators! It has been an absolute joy ‚ù§Ô∏è"", 'Oops, MurAyama, apologies!!']",https://arxiv.org/abs/2104.10687,"We propose a simple non-perturbative formalism for false vacuum decay using functional methods. We introduce the quasi-stationary effective action, a bounce action that non-perturbatively incorporates radiative corrections and is robust to strong couplings. The quasi-stationary effective action obeys an exact flow equation in a modified functional renormalization group with a motivated regulator functional. We demonstrate the use of this formalism in a simple toy model and compare our result with that obtained in perturbation theory. ",Non-perturbative methods for false vacuum decay
36,1385294499631538181,2390686466,Andrei Novitskiiüá∫üá¶,['Our new paper on tailoring the transport properties of BiCuSeO oxyselenides by increasing bond ionicity. \n\n<LINK>\n\n#thermoelectrics'],https://arxiv.org/abs/2104.10509,"In this study, we demonstrate that introducing of rare-earth elements, $R$ = La or Pr, into the Bi-O charge reservoir layer of BiCuSeO leads to an increase of both, the charge carrier concentration and the effective mass. Although the charge carrier mobility slightly decreases upon Bi$^{3+}$ to $R^{3+}$ substitution, the electronic transport properties are significantly improved in a broad temperature range from 100 K to 800 K. In particular, the electrical resistivity decreases by two times, while the Seebeck coefficient drops from 323 ${\mu}$V K$^{-1}$ to 238 ${\mu}$V K$^{-1}$ at 800 K. Thus, a power factor of nearly 3 ${\mu}$W cm$^{-1}$ K$^{-2}$ is achieved for Bi$_{0.92}$La$_{0.08}$CuSeO sample at 800 K. Meanwhile, a noticeable decrease of the lattice thermal conductivity is observed for the substituted samples, which can be attributed to the enhanced point defect scattering mostly originated from atomic mass fluctuations between $R$ and Bi. Ultimately, a maximum $zT$ value of nearly 0.34 at 800 K is obtained for the Bi$_{0.92}$La$_{0.08}$CuSeO sample, which is ~30% higher than that of pristine BiCuSeO. ","Influence of Bi substitution with rare-earth elements on the transport
  properties of BiCuSeO oxyselenides"
37,1385275202515333121,997610236587294721,Brendan Dyck,"['New ApJL paper with @World_of_Wade and @richardmpalin is now on arXiv: The effect of core formation on surface composition and planetary habitability <LINK>', 'To narrow down the search for habitable worlds we should settle on a few key variables that, with luck, we can actually measure 1/10', ""One of these key variables is the amount of iron in a rocky planet's mantle, which greatly influences the thickness and composition of its outer crust 2/10 https://t.co/aPd4Fm24uq"", 'Iron is unique amongst the main planet-forming elements because it can either form a metallic core (Fe) or bond with oxygen and reside in the mantle (FeO) 3/10', 'How iron is distributed between a planet‚Äôs core and mantle is governed by redox conditions during accretion that vary with distance from its star 4/10', 'So, planets with identical compositions can result in a wide range of core sizes and mantle iron contents ‚Äì we see this in our solar system with Mercury, Earth and Mars 5/10', 'While we can‚Äôt measure mantle iron content directly without probes or samples, we can make reasonable predictions of how large a planet‚Äôs core is and then work out the amount of mantle FeO. 6/10', 'How does all this relate to habitability? Well if we know mantle FeO (&amp; mass, radius) we can calculate the thickness and mineralogy of its crust 7/10', 'Planets with a large iron core (think Mercury) will develop thin crusts that can‚Äôt cycle water or other volatile elements 8/10', 'Conversely, a small iron core (think Mars) leads to thick crusts that can stabilize hydrous minerals like amphibole and serpentine ‚Äì potentially a one-way sponge! 9/10', 'So, while a planet‚Äôs orbit may lie within a ‚Äòhabitable zone‚Äô its accretionary history might ultimately render it inhabitable. Good news is we can work that out before sending üöÄüõ∞Ô∏èüòÉ 10/10', '@nexssinfo @ubcnews @OxUniEarthSci']",https://arxiv.org/abs/2104.10612,"The melt productivity of a differentiated planet's mantle is primarily controlled by its iron content, which is itself approximated by the planet's core mass fraction (CMF). Here we show that estimates of an exo-planet's CMF allows robust predictions of the thickness, composition and mineralogy of the derivative crust. These predicted crustal compositions allow constraints to be placed on volatile cycling between surface and the deep planetary interior, with implications for the evolution of habitable planetary surfaces. Planets with large, terrestrial-like, CMFs ($\geq$0.32) will exhibit thin crusts that are inefficient at transporting surface water and other volatiles into the underlying mantle. By contrast, rocky planets with smaller CMFs ($\leq$0.24) and higher, Mars-like, mantle iron contents will develop thick crusts capable of stabilizing hydrous minerals, which can effectively sequester volatiles into planetary interiors and act to remove surface water over timescales relevant to evolution. The extent of core formation has profound consequences for the subsequent planetary surface environment and may provide additional constraints in the hunt for habitable, Earth-like exo-planets. ","The effect of core formation on surface composition and planetary
  habitability"
38,1385240077790482437,707086933,Yuguang Chen (ÈôàÊò±ÂÖâ),"['Time for a new paper ad! This time, we present a major step forward for the KBSS-KCWI survey. That is systematically observing z~2 SF galaxies in the KBSS survey using KCWI, something I have been working on for &gt;3 years to bring it to this stage. \n<LINK>', ""Let's set the stage. At low redshift, it is now well established observationally that cold gas absorbers do not distribute around galaxies isotropically. This is caused by the preferential direction of inflow and outflow in the CGM. (IC: Bordoloi+11, P√©roux+20) https://t.co/LuxZiDXwy8"", 'But nobody knows whether this same trend applies to either 1) cold-gas emission, e.g., Lya, or 2) CGM at high redshift. So we find 59 galaxies in our sample, which have been observed by both KCWI and some high-spatial-resolution imaging (HST or AO assisted Keck).', 'We measured the projected morphological major axes of the galaxies using the high-resolution images, and compare whether Lya emission is different in the directions of galaxy major and minor axes. https://t.co/Rs34sx4D88', 'To do so, we also introduced the ""CP2D spectra"" made from IFU data cubes. Basically, they are maps of the averaged Lya emission as a function of projected distance and wavelength. https://t.co/NiFciU5YIR', 'In the end, we found very little evidence that Lya emission is statistically related to its relative direction to the host galaxies. In fact, within 30 kpc around galaxies, Lya emission is dominated by outflow + resonant scattering in all directions. https://t.co/MxBDroy2Bw', 'This suggests that CGM at z&gt;2 is kinematically different from those of z&lt;1. The galaxy tends to not have ""disky"" rotational structure, and the gas in the CGM does not have a stable angular momentum.', 'Meanwhile, we saw a tiny marginal detection that the blueshifted Lya emission is enhanced along the galaxy minor axis for galaxies with weak overall Lya emission. Could this be a sign that angular momentum starts to form around galaxies that first enter adulthood? https://t.co/oM2re0voZD', 'Stay tuned for our future publications from the KBSS-KCWI survey! Many thanks to my coauthors (including @crosstrainor @allison_strom @GwenCRudie) and many hard-working folks from @keckobservatory, the only observatory that could make this survey possible.']",https://arxiv.org/abs/2104.10173,"We present the first statistical analysis of kinematically-resolved, spatially-extended Ly$\alpha$ emission around $z = 2-3$ galaxies in the Keck Baryonic Structure Survey (KBSS) using the Keck Cosmic Web Imager (KCWI). Our sample of 59 star-forming galaxies ($z_\mathrm{med} = 2.29$) comprises the subset with typical KCWI integration times of ~5 hours and with existing imaging data from the Hubble Space Telescope and/or adaptive optics-assisted integral field spectroscopy. The high resolution images were used to evaluate the azimuthal dependence of the diffuse Ly$\alpha$ emission with respect to the stellar continuum within projected galactocentric distances of $\lesssim 30$ proper kpc. We introduce cylindrically-projected 2D spectra (CP2D) that map the averaged Ly$\alpha$ spectral profile over a specified range of azimuthal angle, as a function of impact parameter around galaxies. The averaged CP2D spectrum of all galaxies shows clear signatures of Ly$\alpha$ resonant scattering by outflowing gas. We stacked the CP2D spectra of individual galaxies over ranges of azimuthal angle with respect to their major axes. The extended Ly$\alpha$ emission along the galaxy principal axes are statistically indistinguishable, with residual asymmetry of $\le$ 2% ($\sim 2 \sigma$) of the integrated Ly$\alpha$ emission. The symmetry implies that the Ly$\alpha$ scattering medium is dominated by outflows in all directions within 30 kpc. Meanwhile, we find that the blueshifted component of Ly$\alpha$ emission is marginally stronger along galaxy minor axes for galaxies with relatively weak Ly$\alpha$ emission. We speculate that this weak directional dependence of Ly$\alpha$ emission becomes discernible only when the Ly$\alpha$ escape fraction is low. These discoveries highlight the need for similar analyses in simulations with Ly$\alpha$ radiative transfer modeling. ","The KBSS-KCWI Survey: The connection between extended Ly$\alpha$ halos
  and galaxy azimuthal angle at $z\sim 2-3$"
39,1385232534586413059,382961853,Jo Dunkley,"[""New @ACT_Pol paper led by Sigurd Naess today on the Arxiv <LINK>  - looking for Planet 9. So, no, Sigurd et al didn't find it, but pretty fun that we can look for it with a telescope designed to study signals from the Big Bang! #NSFfunded"", 'Why does this work? Well, to look for light-patterns from the Big Bang we survey millimeter-wave light coming from more than half the sky. A Planet 9 lurking in the distant parts of the Solar System would be a bit warm, and would also send out millimeter light.', 'Since it is planet-warmth and not reflected sunlight that @ACT_Pol looks for, the sensitivity goes down as distance squared, not distance to power 4. Here are flux limits from Naess et al (bold red), compared to predictions for a 5-10 Earth-mass Planet 9. https://t.co/mLO5JmHOdm', 'So, using these new data @ACT_Pol can exclude around 10-20% of the expected Planet 9 parameter space. With new data from @ACT_Pol and then @SimonsObs, we‚Äôll get more sensitive and can look deeper, for this and other Solar System objects! #NSFfunded']",https://arxiv.org/abs/2104.10264,"We use Atacama Cosmology Telescope (ACT) observations at 98 GHz (2015--2019), 150 GHz (2013--2019) and 229 GHz (2017--2019) to perform a blind shift-and-stack search for Planet 9. The search explores distances from 300 AU to 2000 AU and velocities up to 6.3 arcmin per year, depending on the distance. For a 5 Earth-mass Planet 9 the detection limit varies from 325 AU to 625 AU, depending on the sky location. For a 10 Earth-mass planet the corresponding range is 425 AU to 775 AU. The search covers the whole 18,000 square degrees of the ACT survey, though a slightly deeper search is performed for the parts of the sky consistent with Planet 9's expected orbital inclination. No significant detections are found, which is used to place limits on the mm-wave flux density of Planet 9 over much of its orbit. Overall we eliminate roughly 17% and 9% of the parameter space for a 5 and 10 Earth-mass Planet 9 respectively. We also provide a list of the 10 strongest candidates from the search for possible follow-up. More generally, we exclude (at 95% confidence) the presence of an unknown Solar system object within our survey area brighter than 4--12 mJy (depending on position) at 150 GHz with current distance $300 \text{ AU} < r < 600 \text{ AU}$ and heliocentric angular velocity $1.5'/\text{yr} < v \cdot \frac{500 \text{ AU}}{r} < 2.3'\text{yr}$, corresponding to low-to-moderate eccentricities. These limits worsen gradually beyond 600 AU, reaching 5--15 mJy by 1500 AU. ",The Atacama Cosmology Telescope: A search for Planet 9
40,1385123457323704321,526115229,Kevin Heng,"['[2104.10462] ‚ÄúA transit survey to search for planets around hot subdwarfs: I. methods and performance tests on light curves from Kepler, K2, TESS, and CHEOPS‚Äù Beautifully led by Valerie. New \u2066@ESA_CHEOPS\u2069 paper. <LINK>']",https://arxiv.org/abs/2104.10462,"Context. Hot subdwarfs experienced strong mass loss on the Red Giant Branch (RGB) and are now hot and small He-burning objects. Aims. In this project we aim to perform a transit survey in all available light curves of hot subdwarfs from space-based telescopes (Kepler, K2, TESS, and CHEOPS), with our custom-made pipeline SHERLOCK, in order to determine the occurrence rate of planets around these stars, as a function of orbital period and planetary radius. Methods. In this first paper, we perform injection-and-recovery tests of synthetic transits for a selection of representative Kepler, K2 and TESS light curves, to determine which transiting bodies, in terms of object radius and orbital period, we will be able to detect with our tools. We also provide such estimates for CHEOPS data, which we analyze with the pycheops package. Results. Transiting objects with a radius $\lesssim$ 1.0 $R_{\Earth}$ can be detected in most of Kepler, K2 and CHEOPS targets for the shortest orbital periods (1 d and below), reaching values as small as $\sim$0.3 $R_{\Earth}$ in the best cases. Reaching sub-Earth-sized bodies is achieved only for the brightest TESS targets, and the ones observed during a significant number of sectors. We also give a series of representative results for farther and bigger planets, for which the performances strongly depend on the target magnitude, the length and the quality of the data. Conclusions. The TESS sample will provide the most important statistics for the global aim of measuring the planet occurrence rate around hot subdwarfs. The Kepler, K2 and CHEOPS data will allow us to search for planetary remnants, i.e. very close and small (possibly disintegrating) objects, which would have partly survived the engulfment in their red giant host. ","A transit survey to search for planets around hot subdwarfs: I. methods
  and performance tests on light curves from Kepler, K2, TESS, and CHEOPS"
41,1385111798899216384,19048020,Sean Lawton,['New Paper: <LINK>'],https://arxiv.org/abs/2104.05589,"Let G be a compact Lie group or a complex reductive affine algebraic group. We explore induced mappings between G-character varieties of surface groups by mappings between corresponding surfaces. It is shown that these mappings are generally Poisson. We also given an effective algorithm to compute the Poisson bi-vectors when G=SL(2,C). We demonstrate this algorithm by explicitly calculating the Poisson bi-vector for the 5-holed sphere, the first example for an Euler characteristic -3 surface. ",Poisson maps between character varieties: gluing and capping
42,1384921824107143174,706374159102685184,Rana X Adhikari,"['Our new paper, led by @PMA Burke Fellow Hang Yu,  is out: <LINK>\n\none-liner abstract:\n""Nonlinear Regression w ML can help us find the Neutron Star mergers about a minute before they merge: Thunder before Lightning.""\n@TaikaWaititi @AJemaineClement']",https://arxiv.org/abs/2104.09438,"The success of the multi-messenger astronomy relies on gravitational-wave observatories like LIGO and Virgo to provide prompt warning of merger events involving neutron stars (including both binary neutron stars and neutron-star-black-holes), which further depends critically on the low-frequency sensitivity of LIGO as a typical binary neutron star stays in this band for minutes. However, the current sub-60 Hz sensitivity of LIGO has not yet reached its design target and the excess noise can be more than an order of magnitude below 20 Hz. It is limited by nonlinearly coupled noises from auxiliary control loops which are also nonstationary, posing challenges to realistic early-warning pipelines. Nevertheless, machine-learning-based neural networks provide ways to simultaneously improve the low-frequency sensitivity and mitigate its nonstationarity, and detect the real-time gravitational-wave signal with a very short computational time. We propose to achieve this by inputting both the main gravitational-wave readout and key auxiliary witnesses to a compound neural network. Using simulated data with characteristic representing the real LIGO detectors, our machine-learning-based neural networks can reduce nonlinearly coupled noise by about a factor of 5 and allows a typical binary neutron star (neutron-star-black-hole) to be detected 100 s (10 s) before the merger at a distance of 40 Mpc (160 Mpc). If one can further reduce the noise to the fundamental limit, our neural networks can achieve detection out to a distance of 80 Mpc and 240 Mpc for binary neutron stars and neutron-star-black-holes, respectively. It thus demonstrates that utilizing machine-learning-based neural networks is a promising direction for the timely detection of the coalescence of electromagnetically bright LIGO/Virgo sources. ","Early warning of coalescing neutron-star and neutron-star-black-hole
  binaries from nonstationary noise background using neural networks"
43,1384903392380792835,1015712980275748864,Konstantin Antipin,"['My new paper ""Construction of genuinely multipartite entangled subspace and the associated bounds on entanglement measures for mixed states""  <LINK>    The approach is inspired by tensor diagrams.', 'Thanks to @JacobBiamonte, the author of ""Lectures on quantum tensor networks"", and to @coecke and @AleksKissinger, the authors of ""Picturing quantum processes"", for introducing me to the field']",https://arxiv.org/abs/2104.09664,"Genuine entanglement is the strongest form of multipartite entanglement. Genuinely entangled pure states contain entanglement in every bipartition and as such can be regarded as a valuable resource in the protocols of quantum information processing. A recent direction of research is the construction of genuinely entangled subspaces -- the class of subspaces consisting entirely of genuinely multipartite entangled pure states. In this paper we present several methods of construction of such subspaces including those of maximal possible dimension. The approach is based on the correspondence between bipartite entangled subspaces and quantum channels of a certain type. The examples include maximal subspaces for systems of three qubits, four qubits, three qutrits. We also provide lower bounds on two entanglement measures for mixed states, the concurrence and the convex-roof extended negativity, which are directly connected with the projection on genuinely entangled subspaces. ","Construction of genuinely multipartite entangled subspaces and the
  associated bounds on entanglement measures for mixed states"
44,1384854467116404736,91051114,Kristen M. Scott,['In our new paper we look at whether BERT can be used to measure specific opinions expressed on Twitter about specific COVID measures in Belgium w\\ @pieterdelobelle\nüìÉpaper <LINK>\nü§ñcode + @huggingface models <LINK>\nüåêblog <LINK> <LINK>'],http://arxiv.org/abs/2104.09947,"We classify seven months' worth of Belgian COVID-related Tweets using multilingual BERT and relate them to their governments' COVID measures. We classify Tweets by their stated opinion on Belgian government curfew measures (too strict, ok, too loose). We examine the change in topics discussed and views expressed over time and in reference to dates of related events such as implementation of new measures or COVID-19 related announcements in the media. ","Measuring Shifts in Attitudes Towards COVID-19 Measures in Belgium Using
  Multilingual BERT"
45,1384820227368792074,1379812899670085635,Dror Moran,"['New paper: Deep Permutation Equivariant Structure from Motion.\n \nWe propose an architecture that, given a set of point tracks in multiple images of a scene, recovers both the camera params and 3D structure by minimizing an unsupervised reprojection loss\n\n<LINK> <LINK>', 'Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. https://t.co/wIJn2v7gJk', 'Our architecture supports two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments indicate that our method accurately recovers pose and structure, on par with state-of-the-art methods. Here are several examples: (cameras in red) https://t.co/lpWqyJ3Clk']",https://arxiv.org/abs/2104.06703,"Existing deep methods produce highly accurate 3D reconstructions in stereo and multiview stereo settings, i.e., when cameras are both internally and externally calibrated. Nevertheless, the challenge of simultaneous recovery of camera poses and 3D scene structure in multiview settings with deep networks is still outstanding. Inspired by projective factorization for Structure from Motion (SFM) and by deep matrix completion techniques, we propose a neural network architecture that, given a set of point tracks in multiple images of a static scene, recovers both the camera parameters and a (sparse) scene structure by minimizing an unsupervised reprojection loss. Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. Notably, our method does not require initialization of camera parameters or 3D point locations. We test our architecture in two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments, conducted on a variety of datasets in both internally calibrated and uncalibrated settings, indicate that our method accurately recovers pose and structure, on par with classical state of the art methods. Additionally, we show that a pre-trained network can be used to reconstruct novel scenes using inexpensive fine-tuning with no loss of accuracy. ",Deep Permutation Equivariant Structure from Motion
46,1384778065037959170,50247894,Audrius Alkauskas,"['Our new paper that presents a comprehensive theory and ab initio calculations of photoionization mechanisms of nitrogen-vacancy (NV) centers in diamond.\n\n<LINK>\n\nIn collaboration with @PW_edu, @unirostock, and @PhysicsANU <LINK>', 'In particular, we suggest that right after the ionization from the 3E state the NV center transitions into the 4A2 state of the neutral defect, in contrast to previous suggestions. This explains spin polarization found in ESR experiments on this state.', 'We also present a new methodology to calculate photoionization cross sections using modern electronic structure methods. It is based on dense k-point meshes, band unfolding, and interpolation. Developed by @LukasRazinkovas', '@einulf @LukasRazinkovas Thanks, Linai.', '@igordownunder @PW_edu @unirostock @PhysicsANU All in successionüòÄ Some of us are not superhumans like the one down under üòÑ']",http://arxiv.org/abs/2104.09144,"We present ab-initio calculations of photoionization thresholds and cross sections of the negatively charged nitrogen-vacancy (NV) center in diamond from the ground $^{3}\!A_2$ and the excited $^{3}\!E$ states. We show that after the ionization from the $^{3}\!E$ level the NV center transitions into the metastable $^{4}\!A_2$ electronic state of the neutral defect. We reveal how spin polarization of $\mathrm{NV}^{-}$ gives rise to spin polarization of the $^{4}\!A_2$ state, providing an explanation of electron spin resonance experiments. We obtain smooth photoionization cross sections by employing dense $k$-point meshes for the Brillouin zone integration together with the band unfolding technique to rectify the distortions of the band structure induced by artificial periodicity of the supercell approach. Our calculations provide a comprehensive picture of photoionization mechanisms of $\mathrm{NV}^{-}$. They will be useful in interpreting and designing experiments on charge-state dynamics at NV centers. In particular, we offer a consistent explanation of recent results of spin-to-charge conversion of NV centers. ","Photoionization of negatively charged NV centers in diamond: theory and
  ab initio calculations"
47,1384764614412492800,850306776352395265,Mario Reig,"['(1/ 5)   New paper today!\n\n<LINK>\n\nDue to the complexity of the extra-dim. space, String Theory generically suggests the existence of many axion particles, the Axiverse. The number of these particles can be rather large, easily O(100) in most compactifications.', '(2/5) It is generically believed that some of these axions behave as dark matter (DM) while others might describe our current epoch of accelerated expansion as dynamical dark energy (DE). However, with such a plethora of axions one usually finds...', '(3/5) ..  an overproduction of DM and a picture of DE which is not fully consistent. In this work I show that the same physics that regulates the axion DM overproduction is an excellent candidate to set the initial conditions for a consistent picture of dynamical axion DE.', '(4/5) The framework offers indications about the maximal temperature that our Universe reached in the past and about its fundamental scale. Also gives hints about the fate of the Universe, it is predicted that it will reenter an era of matter domination after the current epoch.', '(5/5) Finally and most important, it gives us homework to do: one is challenged to obtain a consistent mechanism of long and low-scale inflation. This is left as an exercise for the reader üòú']",https://arxiv.org/abs/2104.09923,"In addition to spectacular signatures such as black hole superradiance and the rotation of CMB polarization, the plenitude of axions appearing in the string axiverse may have potentially dangerous implications. An example is the cosmological overproduction of relic axions and moduli by the misalignment mechanism, more pronounced in regions where the signals mentioned above may be observable, that is for large axion decay constant. In this work, we study the minimal requirements to soften this problem and show that the fundamental requirement is a long period of low-scale inflation. However, in this case, if the inflationary Hubble scale is lower than around $O(100)$ eV, no relic DM axion is produced in the early Universe. Cosmological production of some axions may be activated, via the misalignment mechanism, if their potential minimum changes between inflation and today. As a particular example, we study in detail how the maximal-misalignment mechanism dilutes the effect of dangerous axions and allows the production of axion DM in a controlled way. In this case, the potential of the axion that realises the mechanism shifts by a factor $\Delta\theta=\pi$ between the inflationary epoch and today, and the axion starts to oscillate from the top of its potential. We also show that axions with masses $m_a\sim O(1-100)\, H_0$ realising the maximal-misalignment mechanism generically behave as dark energy with a decay constant that can take values well below the Planck scale, avoiding problems associated to super-Planckian scales. Finally, we briefly study the basic phenomenological implications of the mechanism and comment on the compatibility of this type of maximally-misaligned quintessence with the swampland criteria. ",The Stochastic Axiverse
48,1384744804412796930,1140222123006472194,Kasper Elm Heintz,"['New paper out on @arxiv_org today lead by Consuelo Nu√±ez ‚Äî We were able to rule out that one-off FRBs are *not* accompanied by SLSN, and the brightest sub-types of Type Ia SNe, TDEs and GRB SNe ‚Äî so maybe not that cataclysmic of events after all üí•\n\n<LINK>']",https://arxiv.org/abs/2104.09727,"Fast Radio Bursts (FRBs) are extremely energetic pulses of millisecond duration and unknown origin. In order to understand the phenomenon that emits these pulses, targeted and untargeted searches have been performed for multi-wavelength counterparts, including the optical. The objective of this work is to search for optical transients at the position of 8 well-localized FRBs, after the arrival of the burst on different time-scales (typically at one day, several months, and one year after FRB detection) in order to compare with known transient optical light curves. We used the Las Cumbres Observatory Global Telescope Network (LCOGT), which allows us to promptly take images owing to its network of twenty-three telescopes working around the world. We used a template subtraction technique on all the images we collected at different epochs. We have divided the subtractions into two groups, in one group we use the image of the last epoch as a template and in the other group we use the image of the first epoch as a template. We have searched for bright optical transients at the localizations of the FRBs (<1 arcsec) in the template subtracted images. We have found no optical transients, so we have set limiting magnitudes of optical counterparts. Typical limiting magnitudes in apparent (absolute) magnitudes for our LCOGT data are ~22 (-19) mag in the r-band. We have compared our limiting magnitudes with light curves of superluminous supernovae (SLSNe), type Ia supernovae (SNe), supernovae associated with gamma-ray bursts (GRB SNe), a kilonova, and tidal disruption events (TDEs). We rule out that FRBs are associated with SLSN at a confidence of ~99.9%. We can also rule out the brightest sub-types of type Ia SNe, GRB SNe and TDEs (under some conditions) at similar confidence, though we cannot exclude scenarios where FRBs are associated with the faintest sub-type of each of these transient classes. ",Constraining bright optical counterparts of Fast Radio Bursts
49,1384691983214592004,310916361,Farnik Nikakhtar,"['üö®New paper alertüö® delighted to share our new paper ""New families in our Solar neighborhood: applying Gaussian Mixture models for objective classification of structures in the Milky Way and in simulations""\n\n<LINK>', 'We have constructed a Gaussian mixture model for both simulated (Gaia-APOGEE mocks from FIRE-2) and observed (Gaia-APOGEE DR16) stars in the solar neighborhood using velocities and metallicities; in both catalogs, the best-fit GMM uses *five* independent components! https://t.co/5cvwLSBigP', 'This paper also serves as the data release paper for the new APOGEE-Gaia synthetic surveys using ananke (https://t.co/7rXQ318tb0), which will be made public as a value-added catalog with SDSS-IV DR17', 'Many thanks to my co-authors @astrorobyn; @AndrewWetzel; @sloebman; @sanjib_sharma1; @rareflwr41; @ted_mackereth; Vijith Jacob Poovelil; @zastronomerski; @anabonaca; @_sarahmartell_; Henrik J√∂nsson; and Claude-Andre Faucher-Giguere']",https://arxiv.org/abs/2104.08394,"The standard picture of galaxy formation motivates the decomposition of the Milky Way into 3--4 stellar populations with distinct kinematic and elemental abundance distributions: the thin disk, thick disk, bulge, and stellar halo. To test this idea, we construct a Gaussian mixture model (GMM) for both simulated and observed stars in the Solar neighborhood, using measured velocities and iron abundances (i.e., an augmented Toomre diagram) as the distributions to be decomposed. We compare results for the Gaia-APOGEE DR16 crossmatch catalog of the Solar neighborhood with those from a suite of synthetic Gaia-APOGEE crossmatches constructed from FIRE-2 cosmological simulations of Milky Way-mass galaxies. We find that in both the synthetic and real data, the best-fit GMM uses five independent components, some of whose properties resemble the standard populations predicted by galaxy formation theory. Two components can be identified unambiguously as the thin disk and another as the halo. However, instead of a single counterpart to the thick disk, there are three intermediate components with different age and alpha abundance distributions (although these data are not used to construct the model). We use decompositions of the synthetic data to show that the classified components indeed correspond to stars with different origins. By analogy with the simulated data, we show that our mixture model of the real Gaia-APOGEE crossmatch distinguishes the following components: (1) a classic thin disk of young stars on circular orbits (46%), (2) thin disk stars heated by interactions with satellites (22%), (3, 4) two components representing the velocity asymmetry of the alpha-enhanced thick disk (27%), and (5) a stellar halo consistent with early, massive accretion (4%). ","New families in our Solar neighborhood: applying Gaussian Mixture models
  for objective classification of structures in the Milky Way and in
  simulations"
50,1384553358141313024,751326416495517697,Nandan Thakur,"['üö®New paper alert üö®\nüçª BEIR: a heterogeneous benchmark for IR. 17 datasets, 9 tasks with diverse domains. 9 SOTA retrieval models evaluated in a zero-shot setup.\n\n w/ @Nils_Reimers @arueckle @abhesrivas, IG at @UKPLab \n\npdf: <LINK>\nMore details, code üëá\n#NLProc <LINK>', '[1/7] BEIR covers diverse datasets and tasks in retrieval. From existing work, evaluation can often be narrow, sometimes for a single task (question-answering) or on a specific domain (Wikipedia). To tackle the problem we include a variety of diverse tasks and datasets. https://t.co/m7wTjiPS8j', '[2/7] We evaluate 9 state-of-the-art retrieval architectures üöÄ. One lexical, one sparse, five dense-retrieval, and two reranking models all in a zero-shot evaluation setup. We observe often in-domain performance (on MSMARCO) does not correlate well with zero-shot performances. https://t.co/A7gWKpOckY', '[3/7] BM25 is a strong baseline for the zero-shot evaluation. It is the third-best performing model ü§ØReranking models like BM25+CE and ColBERT generalize well but at the expense of bulky indexes and slow retrieval. This hinders their practical applicability in real-world tasks. https://t.co/h0MN2VfR7R', '[4/7] Dense models such as SBERT and ANCE  computationally efficient, but are unable to generalize or perform well in BEIR. SBERT and ANCE are observed to perform well with a strong domain overlap between the target dataset and source dataset (MSMARCO). https://t.co/YHtUCHc7ya', '[5/7] We surprisingly find dense retrievers trained with cosine-similarity tend to retrieve smaller-length documents compared to dot-product. This sometimes leads to an improvement in performance or a drastic drop of 15.3 points as seen in TREC-COVID. https://t.co/Bx7cZmgnGL', '[6/7] We provide an easy-to-use Google Colab example for everyone to get started with the BEIR benchmark. We interactively explore the loading of datasets and evaluation with lexical, dense, and reranking models.\n\nColab: https://t.co/VE9MQdiIQp', '[7/7] Finally, we also provide a well-documented GitHub repository, where you will find easy-to-use examples for evaluation. With BEIR, you can easily add your own model or evaluate a new dataset using the GitHub repository.\n\nPyPI: pip install beir\nhttps://t.co/Z9EZ7yT5F9']",https://arxiv.org/abs/2104.08663,"Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at this https URL ","BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information
  Retrieval Models"
51,1384552647382888453,774170436057731073,Alexis Conneau,"['Happy to share our work ""Large-Scale Self- and Semi-Supervised Learning for Speech Translation""\n\nNew state of the art (+2.6 BLEU on average) on CoVoST-V2 with a simple approach, lot of unannotated data and less supervision\n\nPaper: <LINK>\nModels coming soon', 'We effectively leverage large quantities of unlabeled speech and text data in different and complementary ways\n\nWe explore both pretraining (wav2vec 2.0) and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl', 'Different to existing work, our approach does not leverage any other supervision than ST data. \n\nWe will make code and models publicly available.\n\nJoint work with @ChanghanWang Anne Wu @juanmiguelpino @ZloiAlexei @MichaelAuli']",https://arxiv.org/abs/2104.06678,"In this paper, we improve speech translation (ST) through effectively leveraging large quantities of unlabeled speech and text data in different and complementary ways. We explore both pretraining and self-training by using the large Libri-Light speech audio corpus and language modeling with CommonCrawl. Our experiments improve over the previous state of the art by 2.6 BLEU on average on all four considered CoVoST 2 language pairs via a simple recipe of combining wav2vec 2.0 pretraining, a single iteration of self-training and decoding with a language model. Different to existing work, our approach does not leverage any other supervision than ST data. Code and models will be publicly released. ",Large-Scale Self- and Semi-Supervised Learning for Speech Translation
52,1384532885168922624,1220419280946319362,Matthew Heffernan,"[""As promised last night, I'm happy to announce my new paper - a tutorial on applying Bayesian techniques to evaluate guidance in introductory physics labs <LINK>"", '2/x One key aspect is that rigorous Bayesian modeling should follow a workflow that involves detailed checking of the modelling process to make sure that good science comes out. What better way to illustrate this workflow than a familiar teaching problem?', '3/x Another key aspect: guidance in a quantitative field should be quantitative. What Bayesian model selection does is evaluate if the data shows a preference between two models of equal complexity or between a simple model and a more sophisticated model with more parameters.', ""4/x Current advice: Stop using sin(theta)~theta when theta&gt;15 deg in radians. But if students can't resolve the discrepancy at this level, what's the point? My work uses realistic uncertainties from labs at @McGillUPhysics to evaluate this for the first time."", '5/x What my work also does is describe how to approach Bayesian problems in a simple case so that researchers or students interested in evaluating higher-dimensional models have a guide to follow that uses a familiar, low-dimensional example.', '6/x. Thanks are due to my advisor, Charles Gale, and to professors/fellow students both @McGillUPhysics and elsewhere who gave feedback &amp; encouraged me to publish this work as a single-author. This kind of mentorship goes a long way and means a lot to an early-career researcher.']",https://arxiv.org/abs/2104.08621,"Physics increasingly relies on Bayesian techniques for systematic data analysis and model-to-data comparison. This paper describes how these methods can be implemented to answer questions of relevance to teaching laboratories. It demonstrates the Bayesian approach to statistical modeling and model selection in a step-by-step workflow. The simple pendulum provides a demonstration with the precision commonly seen in the introductory laboratory. This is used to provide realistic quantitative guidance for model preference between the small angle approximation and more complicated formula. This extends the simple pendulum literature's focus beyond comparing individual idealized assessments of different approximations and provides actionable, data-driven guidance for teaching laboratory design. ",How about that Bayes: Bayesian techniques and the simple pendulum
53,1384484495760400386,1373680069,Sam Partee,['New paper on arXiv! Our work with @NCAR_Science \n\nUsing Machine Learning at Scale in HPC Simulations with SmartSim: an Application to Ocean Climate Modeling\n\nPaper: <LINK>\nCode: <LINK>\n\nMore to come.'],https://arxiv.org/abs/2104.09355,"We demonstrate the first climate-scale, numerical ocean simulations improved through distributed, online inference of Deep Neural Networks (DNN) using SmartSim. SmartSim is a library dedicated to enabling online analysis and Machine Learning (ML) for traditional HPC simulations. In this paper, we detail the SmartSim architecture and provide benchmarks including online inference with a shared ML model on heterogeneous HPC systems. We demonstrate the capability of SmartSim by using it to run a 12-member ensemble of global-scale, high-resolution ocean simulations, each spanning 19 compute nodes, all communicating with the same ML architecture at each simulation timestep. In total, 970 billion inferences are collectively served by running the ensemble for a total of 120 simulated years. Finally, we show our solution is stable over the full duration of the model integrations, and that the inclusion of machine learning has minimal impact on the simulation runtimes. ","Using Machine Learning at Scale in HPC Simulations with SmartSim: An
  Application to Ocean Climate Modeling"
54,1384459287452323842,29955721,Lamberto Ballan,"['Check out our new paper on arXiv: Conditional Variational Capsule Network for Open Set Recognition!\nWe also released code, data splits, etc. and addressed reproducibility (that on this task is really a serious issue)\nPaper: <LINK>\nProject: <LINK> <LINK>']",https://arxiv.org/abs/2104.09159,"In open set recognition, a classifier has to detect unknown classes that are not known at training time. In order to recognize new categories, the classifier has to project the input samples of known classes in very compact and separated regions of the features space for discriminating samples of unknown classes. Recently proposed Capsule Networks have shown to outperform alternatives in many fields, particularly in image recognition, however they have not been fully applied yet to open-set recognition. In capsule networks, scalar neurons are replaced by capsule vectors or matrices, whose entries represent different properties of objects. In our proposal, during training, capsules features of the same known class are encouraged to match a pre-defined gaussian, one for each class. To this end, we use the variational autoencoder framework, with a set of gaussian priors as the approximation for the posterior distribution. In this way, we are able to control the compactness of the features of the same class around the center of the gaussians, thus controlling the ability of the classifier in detecting samples from unknown classes. We conducted several experiments and ablation of our model, obtaining state of the art results on different datasets in the open set recognition and unknown detection tasks. ",Conditional Variational Capsule Network for Open Set Recognition
55,1384396211248013312,56468788,Ehsan Adeli,"['Check our paper @CVPR 2021: Metadata Normalization (MDN), a new batch-level operation (end2end training) to correct the influence of metadata (#bias, #confounder, you name it) on feature distributions. W/ @drfeifei @jcniebles et al.\n<LINK>\n<LINK> <LINK>', 'Code will be released soon: https://t.co/BSvhIFzbBg\nMany thanks to the team from @StanfordAILab @StanfordSVL @StanfordMed, especially Mandy Lu and Qingyu Zhao, for this great work!']",https://arxiv.org/abs/2104.09052,"Batch Normalization (BN) and its variants have delivered tremendous success in combating the covariate shift induced by the training step of deep learning methods. While these techniques normalize feature distributions by standardizing with batch statistics, they do not correct the influence on features from extraneous variables or multiple distributions. Such extra variables, referred to as metadata here, may create bias or confounding effects (e.g., race when classifying gender from face images). We introduce the Metadata Normalization (MDN) layer, a new batch-level operation which can be used end-to-end within the training framework, to correct the influence of metadata on feature distributions. MDN adopts a regression analysis technique traditionally used for preprocessing to remove (regress out) the metadata effects on model features during training. We utilize a metric based on distance correlation to quantify the distribution bias from the metadata and demonstrate that our method successfully removes metadata effects on four diverse settings: one synthetic, one 2D image, one video, and one 3D medical image dataset. ",Metadata Normalization
56,1384377593755996162,513512479,Guillermo Navas-Palencia,"['Happy to share my new paper, ""Optimal Counterfactual Explanations for Scorecard modelling"". This work presents MIPs to generate multiple counterfactual explanations with diversity constraints simultaneously for scorecard models. <LINK>\n#orms #MachineLearning #XAI', 'The presented algorithm will be part of OptBinning 0.11.0. Providing tools to enhance the explainability of scorecard models with a binary or continuous target.']",https://arxiv.org/abs/2104.08619,"Counterfactual explanations is one of the post-hoc methods used to provide explainability to machine learning models that have been attracting attention in recent years. Most examples in the literature, address the problem of generating post-hoc explanations for black-box machine learning models after the rejection of a loan application. In contrast, in this work, we investigate mathematical programming formulations for scorecard models, a type of interpretable model predominant within the banking industry for lending. The proposed mixed-integer programming formulations combine objective functions to ensure close, realistic and sparse counterfactuals using multi-objective optimization techniques for a binary, probability or continuous outcome. Moreover, we extend these formulations to generate multiple optimal counterfactuals simultaneously while guaranteeing diversity. Experiments on two real-world datasets confirm that the presented approach can generate optimal diverse counterfactuals addressing desired properties with assumable CPU times for practice use. ",Optimal Counterfactual Explanations for Scorecard modelling
57,1384316150318714880,2337598033,Geraint F. Lewis,"['Paper day! Is it an accreted galaxy, or is it not? New results on the Nyx Stream from @galahsurvey with @FadAstra @_sarahmartell_ @astrowizicist @JontiHorner and many others.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2104.08684,"The results from the ESA Gaia astrometric mission and deep photometric surveys have revolutionized our knowledge of the Milky Way. There are many ongoing efforts to search these data for stellar substructure to find evidence of individual accretion events that built up the Milky Way and its halo. One of these newly identified features, called Nyx, was announced as an accreted stellar stream traveling in the plane of the disk. Using a combination of elemental abundances and stellar parameters from the GALAH and APOGEE surveys, we find that the abundances of the highest likelihood Nyx members are entirely consistent with membership of the thick disk, and inconsistent with a dwarf galaxy origin. We conclude that the postulated Nyx stream is most probably a high-velocity component of the Milky Way's thick disk. With the growing availability of large data sets including kinematics, stellar parameters, and detailed abundances, the probability of detecting chance associations increases, and hence new searches for substructure require confirmation across as many data dimensions as possible. ","The GALAH Survey: No chemical evidence of an extragalactic origin for
  the Nyx stream"
58,1384246978834419717,843188676834164737,Yevgen Chebotar üá∫üá¶,"['Excited to present our new work on Actionable Models, an approach for learning functional understanding of the world via goal-conditioned Q-functions in a fully-offline setting!\n\npaper: <LINK>\nwebsite: <LINK>\n<LINK>', 'Our method enables a real-world robotic system to accomplish a wide range of visually indicated tasks and acquires rich representations that can be used to accelerate learning of downstream tasks.', 'Blogpost: https://t.co/ECB1ySaXFr\n\nTogether with @hausman_k, Yao Lu, @xiao_ted, Dmitry Kalashnikov, Jake Varley, @AlexIrpan, @ben_eysenbach, @ryancjulian, @chelseabfinn, @svlevine']",https://arxiv.org/abs/2104.07749,"We consider the problem of learning useful robotic skills from previously collected offline data without access to manually specified rewards or additional online exploration, a setting that is becoming increasingly important for scaling robot learning by reusing past robotic data. In particular, we propose the objective of learning a functional understanding of the environment by learning to reach any goal state in a given dataset. We employ goal-conditioned Q-learning with hindsight relabeling and develop several techniques that enable training in a particularly challenging offline setting. We find that our method can operate on high-dimensional camera images and learn a variety of skills on real robots that generalize to previously unseen scenes and objects. We also show that our method can learn to reach long-horizon goals across multiple episodes through goal chaining, and learn rich representations that can help with downstream tasks through pre-training or auxiliary objectives. The videos of our experiments can be found at this https URL ","Actionable Models: Unsupervised Offline Reinforcement Learning of
  Robotic Skills"
59,1384184301613289478,4747768463,Cam Buzard,"[""My new paper has been accepted for publication!! It's about how our choice of observation nights might have a lot to do with how well we can see planets - Key point: try for when the star is in the telluric reference frame <LINK>"", ""This was originally a quick figure in my first paper, and I'm so glad my advisor wanted to pull it out and dig into it a bit deeper"", 'I had some fun with this work and owe thanks to many people, specifically thanks to @kellecruz and #bdnyc for teaching me about stats and the KS test which I relied on heavily here!', '@abehmard thank you!!! ü•∞']",https://arxiv.org/abs/2104.07790,"Cross correlation analyses of high resolution spectroscopic data have recently shown great success in directly detecting planetary signals and enabling the characterization of their atmospheres. One such technique aims to observe a system at multiple epochs and combine the measured planetary radial velocities from each epoch into a measurement of the planetary Keplerian orbital velocity $K_p$, constituting a direct detection of the planetary signal. Recent work has shown that in few-epoch ($\sim$5) data sets, unintended structure can arise at a high level, obscuring the planetary detection. In this work, we look to simulations to examine whether there are ways to reduce this structured noise in few-epoch data sets by careful planning of observations. The choice of observation date allows observers to select the primary (stellar) velocity - through a set systemic velocity and chosen barycentric velocity - and the planetary orbital phase, and so we focus on the effects of these two parameters. We find that epochs taken when the primary velocity is near zero, and the stellar lines remain relatively fixed to the telluric rest-frame, greatly reduce the level of structured noise and allow for much stronger planetary detections, on average more than twice the significance of detections made with epochs using randomly selected primary velocities. Following these results, we recommend that observers looking to build up high-resolution multi-epoch data sets target nights when their system has a near-zero primary velocity. ","Primary Velocity and Orbital Phase Effects on Planetary Detectability
  from Small Epoch Number Data Sets"
60,1384156678006870018,2601605436,Wouter Ryssens,"[""New paper üì∞! Normally, I like to wait until a paper is actually published to post here, but I'm just so excited to have submitted this particular one. Warning: EXTREMELY technical content.  1/13\n<LINK>"", 'Energy density functionals (EDFs) have a long history in nuclear structure: these objects allow us to calculate nuclei all over the nuclear chart, from very light to very heavy. EDF-based models have been ubiquitous and rather successful over the past decades. 2/13', 'However, even modern-day EDFs are very far from perfect, and people have been coming up with various extensions for a while now. One of these ideas is to include a whole bunch of new densities in their formulation. 3/13', 'Traditional EDFs rely on the normal density, a kinetic density, a current or two and perhaps a few others. All of these can be formed by applying up to two derivatives on the single-particle wave-functions that characterize individual nucleons. 4/13', 'If we however throw a bunch more derivatives at the nucleons, then suddenly we can construct a whole boatload more densities. People have been proposing to do exactly this for a while now, thinking about densities with four (N2LO) or six gradients (N3LO).\n5/13', 'Actually constructing the ""right"" EDF with more gradients is not so easy though, as all these gradients can combine with one another. Your preferred choice of densities from which you want to build a functional, is but one of a very large set of (equivalent) possibilities. 6/13', 'While such choices make no difference to the underlying physics, they can have dire consequences for us puny mortals. For one, we don\'t want to include ""useless"" densities, we only want densities that offer useful degrees of freedom. 7/13', ""For another, we don't want to increase the computational burden more than necessary, we want to choose densities that need the least amount of memory to store and the smallest amount of CPU time to calculate. 8/13"", ""Also, we don't want to make mistakes when writing things down. Traditionally, one just uses letters to denote different densities, which are generally not linked to its mathematical structure. If you are anything like me, you will make MANY mistakes manipulating these things.9/13"", 'In this paper, M. Bender and I propose a solution to all of these problems: a new notation that\n(i) can be extended to any order of gradients\n(ii) links the mathematical structure to the symbol of the density\n(iii) allows one to ""see"" which densities are ""useless"" 10/13', 'We use this notation to clearly identify suboptimal choices of the past, and present a new (again, physically equivalent) formulation of the N2LO functional that is much easier to handle formally and numerically. 11/13', ""We've also implemented this N2LO functional in a three-dimensional structure code, and we have performed the very first non-spherically symmetric calculations using this type of extended functionals. 12/13"", 'In particular, we find that the SN2LO1 parameterization of the extended N2LO shares successes and failures with the traditional EDFs. More work is definitely needed, but we think this work will make it easier to ""see the forest through the trees"". 13/13', '@MatthiasFilez Well, 1/3rd is appendices üòÖ']",https://arxiv.org/abs/2104.07697,"There is an ongoing quest to improve on the spectroscopic quality of nuclear energy density functionals (EDFs) of the Skyrme type through extensions of its traditional form. One direction for such activities is the inclusion of terms of higher order in gradients in the EDF. We report on exploratory symmetry-breaking calculations performed for an extension of the Skyrme EDF that includes central terms with four gradients at next-to-next-to-leading order (N2LO) and for which the high-quality parametrization SN2LO1 has been constructed recently [P. Becker et al, Phys. Rev. C 96, 044330 (2017)]. Up to now, the investigation of such functionals with higher-order terms was limited to infinite matter and spherically symmetric configurations of singly- and doubly-magic nuclei. We address here nuclei and phenomena that require us to consider axial and non-axial deformation, both for reflection-symmetric and also reflection-asymmetric shapes, as well as the breaking of time-reversal invariance. Achieving these calculations demanded a number of formal developments. These all resulted from the formulation of the N2LO EDF requiring the introduction of new local densities with additional gradients that are not present in the EDF at NLO. Their choice is not unique, but can differ in the way the gradients are coupled. While designing a numerical implementation of N2LO EDFs in Cartesian 3d coordinate-space representation, we have developed a novel definition and a new unifying notation for normal and pair densities that contain gradients at arbitrary order. The resulting scheme resolves several issues with some of the choices that have been made for local densities in the past, in particular when breaking time-reversal symmetry. Guided by general practical considerations, we propose an alternative form of the N2LO contribution to the Skyrme EDF that is built from a different set of densities. ","Skyrme pseudopotentials at next-to-next-to-leading order Construction of
  local densities and first symmetry-breaking calculations"
61,1384150415206326274,919538082,samantha scibelli,"['My new organics paper is out on arXiv! Click the link if you wanna learn about all the cool organic molecules I detected in a young starless core ü§©üì°üß™‚ú® <LINK> #phdchat #astrochemistry #astronomy', '@That_Astro_Chic Thanks girl ‚ò∫Ô∏è']",https://arxiv.org/abs/2104.07683,"Determining the level of chemical complexity within dense starless and gravitationally bound prestellar cores is crucial for constructing chemical models, which subsequently constrain the initial chemical conditions of star formation. We have searched for complex organic molecules (COMs) in the young starless core L1521E, and report the first clear detection of dimethyl ether (CH$_3$OCH$_3$), methyl formate (HCOOCH$_3$), and vinyl cyanide (CH$_2$CHCN). Eight transitions of acetaldehyde (CH$_3$CHO) were also detected, five of which (A states) were used to determine an excitation temperature to then calculate column densities for the other oxygen-bearing COMs. If source size was not taken into account (i.e., if filling fraction was assumed to be one), column density was underestimated, and thus we stress the need for higher resolution mapping data. We calculated L1521E COM abundances and compared them to other stages of low-mass star formation, also finding similarities to other starless/prestellar cores, suggesting related chemical evolution. The scenario that assumes formation of COMs in gas-phase reactions between precursors formed on grains and then ejected to the cold gas via reactive desorption was tested and was unable to reproduce observed COM abundances, with the exception of CH$_3$CHO. These results suggest that COMs observed in cold gas are formed not by gas-phase reactions alone, but also through surface reactions on interstellar grains. Our observations present a new, unique challenge for existing theoretical astrochemical models. ",Detection of Complex Organic Molecules in Young Starless Core L1521E
62,1384044780204609546,820031736914513925,Fabrizio Leisen,"['New paper: ""New perspectives on knockoffs construction"" written with P. Berti, E. Dreassi, L. Pratelli, P. Rigo.  <LINK>']",http://arxiv.org/abs/2104.07752,"Let $\Lambda$ be the collection of all probability distributions for $(X,\widetilde{X})$, where $X$ is a fixed random vector and $\widetilde{X}$ ranges over all possible knockoff copies of $X$ (in the sense of \cite{CFJL18}). Three topics are developed in this note: (i) A new characterization of $\Lambda$ is proved; (ii) A certain subclass of $\Lambda$, defined in terms of copulas, is introduced; (iii) The (meaningful) special case where the components of $X$ are conditionally independent is treated in depth. In real problems, after observing $X=x$, each of points (i)-(ii)-(iii) may be useful to generate a value $\widetilde{x}$ for $\widetilde{X}$ conditionally on $X=x$. ",New perspectives on knockoffs construction
63,1383953935363309577,1093387119148462081,Daniel Green,['Very cool new paper from Lagu√´ et al <LINK> (including @reneehlozek).  Constraints on ultra light axion dark matter from galaxy surveys.  Another nice example of novel analyses of BOSS data exceeding the sensitivity of the CMB. <LINK>'],https://arxiv.org/abs/2104.07802,"Ultralight axions and other bosons are dark matter candidates present in many high energy physics theories beyond the Standard Model. In particular, the string axiverse postulates the existence of up to $\mathcal{O}(100)$ light scalar bosons constituting the dark sector. Considering a mixture of axions and cold dark matter, we obtain upper bounds for the axion relic density $\Omega_a h^2 < 0.004$ for axions of mass $10^{-31}\;\mathrm{eV}\leq m_a \leq 10^{-26}\;\mathrm{eV}$ at 95% confidence. We also improve existing constraints by a factor of over 4.5 and 2.1 for axion masses of $10^{-25}$ eV and $10^{-32}$ eV, respectively. We use the Fourier-space galaxy clustering statistics from the Baryon Oscillation Spectroscopic Survey (BOSS) and demonstrate how galaxy surveys break important degeneracies in the axion parameter space compared to the cosmic microwave background (CMB). We test the validity of the effective field theory of large-scale structure approach to mixed ultralight axion dark matter by making our own mock galaxy catalogs and find an anisotropic ultralight axion signature in the galaxy quadrupole. We also observe an enhancement of the linear galaxy bias from 1.8 to 2.4 when allowing for 5% of the dark matter to be composed of a $10^{-28}$ eV axion in our simulations. Finally, we develop an augmented interpolation scheme allowing a fast computation of the axion contribution to the linear matter power spectrum leading to a 70% reduction of the computational cost for the full Monte Carlo Markov chains analysis. ",Constraining Ultralight Axions with Galaxy Surveys
64,1383948271102566401,922847904058011649,Romy Rodr√≠guez,['Excellent new paper by Diego Godoy-Rivera on the precise characterization of a large sample of subgiants in the TESS continuous viewing zones.  <LINK>'],https://arxiv.org/abs/2104.07679,"Given their location on the Hertzsprung-Russell (HR) diagram, thoroughly characterized subgiant stars can place stringent constraints on a wide range of astrophysical problems. Accordingly, they are prime asteroseismic targets for the Transiting Exoplanet Survey Satellite (TESS) mission. In this work, we infer stellar properties for a sample of 347 subgiants located in the TESS Continuous Viewing Zones (CVZs), which we select based on their likelihood of showing asteroseismic oscillations. We investigate how well they can be characterized using classical constraints (photometry, astrometry), and validate our results using spectroscopic values. We derive luminosities, effective temperatures, and radii with mean 1$\sigma$ random (systematic) uncertainties of 4.5% (2%), 33 K (60 K), and 2.2% (2%), as well as more model-dependent quantities such as surface gravities, masses, and ages. We use our sample to demonstrate that subgiants are ideal targets for mass and age determination based on HR diagram location alone, discuss the advantages of stellar parameters derived from a detailed characterization over widely available catalogs, show that the generally used 3D extinction maps tend to overestimate the extinction for nearby stars (distance $\lesssim$ 500 pc), and find a correlation that supports the rotation-activity connection in post main sequence stars. The complementary roles played by classical and asteroseismic data sets will open a window to unprecedented astrophysical studies using subgiant stars. ","Testing the Limits of Precise Subgiant Characterization with APOGEE and
  Gaia: Opening a Window to Unprecedented Astrophysical Studies"
65,1383906743307051010,1036512677051412480,Claudio Gallicchio,"['new paper out! üòçüòç\nwe study deep randomized graph neural networks based on stable dynamics &amp; graph pooling!\naccepted for journal publication, here in pre-print on arxiv: <LINK>\n\nspecial thanks to @FilippoMariaBi1, a great co-author for this work! <LINK>', '@luigidisotto @FilippoMariaBi1 thanks!']",https://arxiv.org/abs/2104.04710,"We propose a deep Graph Neural Network (GNN) model that alternates two types of layers. The first type is inspired by Reservoir Computing (RC) and generates new vertex features by iterating a non-linear map until it converges to a fixed point. The second type of layer implements graph pooling operations, that gradually reduce the support graph and the vertex features, and further improve the computational efficiency of the RC-based GNN. The architecture is, therefore, pyramidal. In the last layer, the features of the remaining vertices are combined into a single vector, which represents the graph embedding. Through a mathematical derivation introduced in this paper, we show formally how graph pooling can reduce the computational complexity of the model and speed-up the convergence of the dynamical updates of the vertex features. Our proposed approach to the design of RC-based GNNs offers an advantageous and principled trade-off between accuracy and complexity, which we extensively demonstrate in experiments on a large set of graph datasets. ",Pyramidal Reservoir Graph Neural Network
66,1383090332401799171,2485053080,Swarnadeep Saha,"['Excited to share our new work on ""ExplaGraphs: An Explanation Graph Generation Task for Structured Commonsense Reasoning""! Has been a long effort and a great learning experience too üôÇ\n\nJoint work w. @prateeky2806 @lbauer119 @mohitban47 @uncnlp\nPaper: <LINK>\n1/5 <LINK>', 'Commonsense reasoning tasks are usually discriminative, thus failing to eval. model\'s ability to reason+explain preds with underlying knowledge(+allowing shortcuts). We propose a new ""generative+structured"" task for generating ""commonsense expl. graphs"" for stance prediction. 2/5', 'Graphs are structured, so explicitly explain+eval model reasoning by visually laying out relevant context &amp; commonsense knowldg edges/chains/subgraphs. We collect a dataset of non-trivial, complete, unambiguous explanations thru Collect-Judge-Refine graph-annotation framework 3/5', ""We also propose a multi-level evaluation framework that checks for generated graph's structural + semantic correctness (as judged by stance inference capability given the belief and graph) + predicted graph‚Äôs plausibility wrt gold graphs + each individual edge's importance. 4/5"", 'Initial baselines w. BART+T5 show that they fail to generate meaningful expln graphs, leaving large gap with human performance; &amp; we hope this will encourage future work by community on better structured models for challenging new commonsense graph-based expln generation task 5/5', ""@aman_madaan @prateeky2806 @lbauer119 @mohitban47 @uncnlp Thanks, @aman_madaan for the appreciation and the pointer (we'll cite it in our next version).""]",https://arxiv.org/abs/2104.07644,"Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model's ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be ""right for the right reasons"". In this work, we present ExplaGraphs, a new generative and structured commonsense-reasoning task (and an associated dataset) of explanation graph generation for stance prediction. Specifically, given a belief and an argument, a model has to predict if the argument supports or counters the belief and also generate a commonsense-augmented graph that serves as non-trivial, complete, and unambiguous explanation for the predicted stance. We collect explanation graphs through a novel Create-Verify-And-Refine graph collection framework that improves the graph quality (up to 90%) via multiple rounds of verification and refinement. A significant 79% of our graphs contain external commonsense nodes with diverse structures and reasoning depths. Next, we propose a multi-level evaluation framework, consisting of automatic metrics and human evaluation, that check for the structural and semantic correctness of the generated graphs and their degree of match with ground-truth graphs. Finally, we present several structured, commonsense-augmented, and text generation models as strong starting points for this explanation graph generation task, and observe that there is a large gap with human performance, thereby encouraging future work for this new challenging task. ExplaGraphs will be publicly available at this https URL ","ExplaGraphs: An Explanation Graph Generation Task for Structured
  Commonsense Reasoning"
67,1383073302697099264,1204291434842595328,Cl√©ment Rebuffel,"['Happy to have contributed to QuestEval, with a new paper in collaboration with @ThomasScialom: \n\nData-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation!\n\nPaper is available on ArXiv: <LINK>\n\n1/2', 'Your goal: measure Semantic Matching between generated text and structured input using QuestEval\n\nYour issue: in-domain Question Generation/Answering copora are needed to train QuestEval\n\nOur solution: build synthetic QG/QA datasets for any Data-to-Text Generation task!\n\n2/2 https://t.co/NVfu7bMCIR']",https://arxiv.org/abs/2104.07555,"QuestEval is a reference-less metric used in text-to-text tasks, that compares the generated summaries directly to the source text, by automatically asking and answering questions. Its adaptation to Data-to-Text tasks is not straightforward, as it requires multimodal Question Generation and Answering systems on the considered tasks, which are seldom available. To this purpose, we propose a method to build synthetic multimodal corpora enabling to train multimodal components for a data-QuestEval metric. The resulting metric is reference-less and multimodal; it obtains state-of-the-art correlations with human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's code and models available for reproducibility purpose, as part of the QuestEval project. ","Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic
  Evaluation"
68,1383068160329449481,1215310334,Timo Schick,"['üéâ New paper üéâ In ""Generating Datasets with Pretrained Language Models"", we introduce DINOü¶ï and show how LMs can create entire datasets from scratch if provided with instructions. These datasets can be used to train much smaller models #NLProc\n\nüìÑ Paper: <LINK> <LINK>']",https://arxiv.org/abs/2104.07540,"To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets. ",Generating Datasets with Pretrained Language Models
69,1383068017123336196,22148802,Leo C. Stein ü¶Å,"['üö® New preprint day! üéâ\n""Comparing Remnant Properties from Horizon Data and Asymptotic Data in Numerical Relativity""\nIozzo, Khera, Stein, et al.\n<LINK>\nWhat\'s this paper all about? A üßµ for #BlackHoleWeek 1/8 <LINK>', 'When two black holes merge, we get one remnant black hole and a bunch of gravitational waves. Measuring the properties of the remnant is very important ‚Äî it tells us things like its mass, spin, or ""Will this black hole be kicked out of its host galaxy?"" 2/8 https://t.co/wx5671nwop', 'Previously, numerical relativity codes measured these remnant properties in the ""bulk"" of the spacetime ‚Äî more precisely, as integrals over its (apparent) horizon. Some of these integrals are well justified, others are not!\nBut, there\'s another way to measure those properties\n3/8', 'We can go infinitely far away, to ""asymptotic future null infinity,"" where the gravitational waves end up. There, we have well-defined integrals for mass, momentum, and angular momentum (technically: BMS charges). 4/8', 'To perform those integrals at future null infinity, we needed a robust code that takes data from the bulk of the spacetime to its boundary. This step is called Cauchy-characteristic evolution (CCE) and our improved code has been around for ~1 year (https://t.co/UvbK6mpYGA) 5/8', 'So now that we have good asymptotic data, we can do those charge integrals ""at infinity"" correctly. Of course we compare to the old method on the horizons, and find really great agreement 6/8 https://t.co/G7pTOd8SVj', 'Moreover, we have a good theoretical understanding for *why* the two approaches should agree ‚Äî but only for some of the remnant quantities! This has to do with symmetries and our patron saint, Dr. Emmy Noether 7/8 https://t.co/IRYfAA9uXP', ""In the future, we'll use the asymptotic quantities instead of the horizon remnant quantities, because we trust them more. The simulation data will be made public, and the code to do this is already in our python package, 'scri'.\n\nhttps://t.co/6ajt3d5Lk2\n\nHappy #BlackHoleWeek! 8/8"", '@CJHandmer Indeed! We have another paper in the works where we do care about the supertranslations (not in this paper, though ‚Äî here we just needed a Poincare subalgebra of BMS)']",https://arxiv.org/abs/2104.07052,"We present a new study of remnant black hole properties from 13 binary black hole systems, numerically evolved using the Spectral Einstein Code. The mass, spin, and recoil velocity of each remnant were determined quasi-locally from apparent horizon data and asymptotically from Bondi data $(h, \psi_4, \psi_3, \psi_2, \psi_1)$ computed at future null infinity using SpECTRE's Cauchy characteristic evolution. We compare these independent measurements of the remnant properties in the bulk and on the boundary of the spacetime, giving insight into how well asymptotic data are able to reproduce local properties of the remnant black hole in numerical relativity. We also discuss the theoretical framework for connecting horizon quantities to asymptotic quantities and how it relates to our results. This study recommends a simple improvement to the recoil velocities reported in the Simulating eXtreme Spacetimes waveform catalog, provides an improvement to future surrogate remnant models, and offers new analysis techniques for evaluating the physical accuracy of numerical simulations. ","Comparing Remnant Properties from Horizon Data and Asymptotic Data in
  Numerical Relativity"
70,1383054369097011204,952949678533849088,Kareem El-Badry,"['New paper. We found a new (sort of) type of interacting binary star! <LINK> <LINK>', ""It's a white dwarf and a tidally distorted, bloated, stripped helium core. It looks like a cataclysmic variable (white dwarf accreting from a normal star), but the donor star is much hotter than any known cataclysmic variable donor. https://t.co/fPC1gbUn9Z"", ""Conversely, it's cooler, more bloated, and more tidally distorted than known low-mass white dwarfs. https://t.co/e4Gr95qqBl"", 'There are a few other known stars with similar temperature and luminosity (‚ÄúsdA‚Äù stars) but they are not close being mass transferring and are mostly in wider binaries.', ""We think this binary was a cataclysmic variable with a donor that started transferring mass to its white dwarf companion *just* at the end of the main sequence. Now it's becoming an extremely low-mass white dwarf. https://t.co/Qb6EDLUKOF"", 'In a few Gyr, the binary will shrink to extremely short periods, where it appear as an ultracompact white dwarf binary or maybe an ""AM CVn"" (ultra short period, mass-transferring) system. https://t.co/9N7zR1hZHW', ""There are more of these transitional systems to be characterized! We're looking forward to mapping the population. \n\nthanks to @kenjshen, @thomkupfer, @bigticketdw, and other coauthors not on twitter."", '@PNeunteufel @kenjshen In the best-fit models, there is still a ~0.005 Msun H-burning envelope (which is why the contraction is slow). There are no shell flashes for the best-fit mass of 0.15 Msun, but there would be for &gt;0.18 Msun or so. Thanks!']",https://arxiv.org/abs/2104.07033,"We present LAMOST J0140355+392651 (hereafter J0140), a close ($P_{\rm orb} = 3.81$ hours) binary containing a bloated, low-mass ($M \approx 0.15 M_{\odot}$) proto-white dwarf (WD) and a massive ($M\approx 0.95\,M_{\odot}$) WD companion. The system's optical light curve is dominated by large-amplitude ellipsoidal variability but also exhibits additional scatter, likely driven by pulsations. The proto-WD is cooler ($T_{\rm eff} = 6800\pm 100$ K) and more puffy ($\log\left[g/\left({\rm cm\,s^{-2}}\right)\right]=4.74\pm0.07$) than any known extremely low mass (ELM) WD, but hotter than any known cataclysmic variable (CV) donor. It either completely or very nearly fills its Roche lobe ($R/R_{{\rm Roche\,lobe}}=0.99\pm0.01$), suggesting ongoing or recently terminated mass transfer. No dwarf nova-like outbursts have been observed. The spectrum is dominated by the proto-WD but shows tentative hints of H$\alpha$ emission, perhaps due to accretion onto the massive WD. The properties of the system are well-matched by MESA binary evolution models of CVs with donors that underwent significant nuclear evolution before the onset of mass transfer. In these models, the bloated proto-WD is either still losing mass via stable Roche lobe overflow or was doing so until very recently. In either case, it is evolving toward higher temperatures at near-constant luminosity to become an ELM WD. If the system is detached, mass transfer likely ended when the donor became too hot for magnetic braking to remain efficient. Evolutionary models predict that the binary will shrink to $P_{\rm orb}\lesssim 10$ minutes within a few Gyr, when it will either merge or become an AM CVn binary. J0140 provides an observational link between the formation channels of CVs, ELM WDs, detached ultracompact WD binaries, and AM CVn systems. ","LAMOST J0140355+392651: An evolved cataclysmic variable donor
  transitioning to become an extremely low mass white dwarf"
71,1383046204599189504,73090248,Prof. Danushka Bollegala,"['New paper on multilingual counterfactual detection in product reviews <LINK> with @JamesONeil21 Ryuichi Kiryo, Motoko Kubota and Polina Rozenshtein  (1/n)', 'Statements such as ""I wished this shirt was available in red"" express counterfatuals  and often indicate that the attribute under consideration (i.e. red) is not present for the product under considersation (i.e. shirt). (2/n)', 'It is important to be able to detect such counterfactuals in IR because using a bag of words model covering all words would result in returning this shirt (which is not available in red) for a user searching for ""red shirts"". (3/n)', 'We annotate counterfactuals for English, German and Japanese languages and train classifiers using different sentence encoder models. Interestingly, MLMs seem to be robust against the clue phrases compared to lexicalised models. (4/n=4)']",https://arxiv.org/abs/2104.06893,"Counterfactual statements describe events that did not or cannot take place. We consider the problem of counterfactual detection (CFD) in product reviews. For this purpose, we annotate a multilingual CFD dataset from Amazon product reviews covering counterfactual statements written in English, German, and Japanese languages. The dataset is unique as it contains counterfactuals in multiple languages, covers a new application area of e-commerce reviews, and provides high quality professional annotations. We train CFD models using different text representation methods and classifiers. We find that these models are robust against the selectional biases introduced due to cue phrase-based sentence selection. Moreover, our CFD dataset is compatible with prior datasets and can be merged to learn accurate CFD models. Applying machine translation on English counterfactual examples to create multilingual data performs poorly, demonstrating the language-specificity of this problem, which has been ignored so far. ","I Wish I Would Have Loved This One, But I Didn't -- A Multilingual
  Dataset for Counterfactual Detection in Product Reviews"
72,1382992132793860097,1014270138621808641,Paolo Cremonese,"[""New paper out on #GravitationalWaves! With @jmezquiagabravo and dr. Salzano.\n\nMain point is: mass-sheet degeneracy affects also GW lensing! But there is a solution.\n\nGive it a read if you're interested in #GravitationalWaves or #Lensing!\n\n<LINK> <LINK>""]",http://arxiv.org/abs/2104.07055,"The mass-sheet degeneracy is a well-known problem in gravitational lensing which limits our capability to infer astrophysical lens properties or cosmological parameters from observations. As the number of gravitational wave observations grows, detecting lensed events will become more likely, and to assess how the mass-sheet degeneracy may affect them is crucial. Here we study both analytically and numerically how the lensed waveforms are affected by the mass-sheet degeneracy computing the amplification factor from the diffraction integral. In particular, we differentiate between the geometrical optics, wave optics and interference regimes, focusing on ground-based gravitational waves detectors. In agreement with expectations of gravitational lensing of electromagnetic radiation, we confirm how, in the geometrical optics scenario, the mass-sheet degeneracy cannot be broken with only one lensed image. However, we find that in the interference regime, and in part in the wave-optics regime, the mass-sheet degeneracy can be broken with only one lensed waveform thanks to the characteristic interference patterns of the signal. Finally, we quantify, through template matching, how well the mass-sheet degeneracy can be broken. We find that, within present GW detector sensitivities and considering signals as strong as those which have been detected so far, the mass-sheet degeneracy can lead to a $1\sigma$ uncertainty on the lens mass of $\sim 12\%$. With these values the MSD might still be a problematic issue. But in case of signals with higher signal-to-noise ratio, the uncertainty can drop to $\sim 2\%$, which is less than the current indeterminacy achieved by dynamical mass measurements. ","Breaking the mass-sheet degeneracy with gravitational wave interference
  in lensed events"
73,1382970840535425024,977906884886827008,Marcos Mari√±o,"['Peacock patterns are everywhere! In my last paper with Jie Gu, we study these patterns in topological string theory. They can be used to define new integer invariants of the topological string.\n<LINK> <LINK>']",https://arxiv.org/abs/2104.07437,"Topological string theory near the conifold point of a Calabi-Yau threefold gives rise to factorially divergent power series which encode the all-genus enumerative information. These series lead to infinite towers of singularities in their Borel plane (also known as ""peacock patterns""), and we conjecture that the corresponding Stokes constants are integer invariants of the Calabi-Yau threefold. We calculate these Stokes constants in some toric examples, confirming our conjecture and providing in some cases explicit generating functions for the new integer invariants, in the form of q-series. Our calculations in the toric case rely on the TS/ST correspondence, which promotes the asymptotic series near the conifold point to spectral traces of operators, and makes it easier to identify the Stokes data. The resulting mathematical structure turns out to be very similar to the one of complex Chern-Simons theory. In particular, spectral traces correspond to state integral invariants and factorize in holomorphic/anti-holomorphic blocks. ",Peacock patterns and new integer invariants in topological string theory
74,1382968473953918978,328430286,Jad C. Halimeh,['New paper <LINK>. We show analytically and numerically that #GaugeInvariance can be enforced at volume-independent protection strength in the thermodynamic limit and for large link spin lengths in ultracold-atom setups of #QLM #LatticeGaugeTheories. @HaukeGroup <LINK>'],https://arxiv.org/abs/2104.07040,"Although gauge invariance is a postulate in fundamental theories of nature such as quantum electrodynamics, in quantum-simulation implementations of gauge theories it is compromised by experimental imperfections. In a recent work [Halimeh and Hauke, \href{this https URL}{Phys. Rev. Lett. \textbf{125}, 030503 (2020)}], it has been shown in finite-size spin-$1/2$ quantum link lattice gauge theories that upon introducing an energy-penalty term of sufficiently large strength $V$, unitary gauge-breaking errors at strength $\lambda$ are suppressed $\propto\lambda^2/V^2$ up to all accessible evolution times. Here, we show numerically that this result extends to quantum link models in the thermodynamic limit and with larger spin-$S$. As we show analytically, the dynamics at short times is described by an \textit{adjusted} gauge theory up to a timescale that is at earliest $\tau_\text{adj}\propto\sqrt{V/V_0^3}$, with $V_0$ an energy factor. Moreover, our analytics predicts that a renormalized gauge theory dominates at intermediate times up to a timescale $\tau_\text{ren}\propto\exp(V/V_0)/V_0$. In both emergent gauge theories, $V$ is volume-independent and scales at worst $\sim S^2$. Furthermore, we numerically demonstrate that robust gauge invariance is also retained through a single-body gauge-protection term, which is experimentally straightforward to implement in ultracold-atom setups and NISQ devices. ",Reliability of lattice gauge theories in the thermodynamic limit
75,1382960353148026883,301178769,Preslav Nakov,"['New #NLProc paper preprint üìù, in which we study the Role of Context in Detecting Previously Fact-Checked Claims (Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov) <LINK> #factchecking #fakenews @firojalam04 @giodsm <LINK>']",https://arxiv.org/abs/2104.07423,"Recent years have seen the proliferation of disinformation and fake news online. Traditional approaches to mitigate these issues is to use manual or automatic fact-checking. Recently, another approach has emerged: checking whether the input claim has previously been fact-checked, which can be done automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. Here, we focus on claims made in a political debate and we study the impact of modeling the context of the claim: both on the source side, i.e., in the debate, as well as on the target side, i.e., in the fact-checking explanation document. We do this by modeling the local context, the global context, as well as by means of co-reference resolution, and multi-hop reasoning over the sentences of the document describing the fact-checked claim. The experimental results show that each of these represents a valuable information source, but that modeling the source-side context is most important, and can yield 10+ points of absolute improvement over a state-of-the-art model. ",The Role of Context in Detecting Previously Fact-Checked Claims
76,1382953793134923781,507704346,Isabelle Augenstein,['New #NLProc paper: quantifying gender bias towards politicians in X-ling language models\n\ntl;dr:\nüó£Ô∏ègender bias is highly lang-dependent\nü§îlarger models are not significantly more gender-biased\n\n<LINK>\n#NLProc @karstanczak @sagnikrayc @tpimentelms @ryandcotterell <LINK>'],https://arxiv.org/abs/2104.07505,"While the prevalence of large pre-trained language models has led to significant improvements in the performance of NLP systems, recent research has demonstrated that these models inherit societal biases extant in natural language. In this paper, we explore a simple method to probe pre-trained language models for gender bias, which we use to effect a multi-lingual study of gender bias towards politicians. We construct a dataset of 250k politicians from most countries in the world and quantify adjective and verb usage around those politicians' names as a function of their gender. We conduct our study in 7 languages across 6 different language modeling architectures. Our results demonstrate that stance towards politicians in pre-trained language models is highly dependent on the language used. Finally, contrary to previous findings, our study suggests that larger language models do not tend to be significantly more gender-biased than smaller ones. ","Quantifying Gender Bias Towards Politicians in Cross-Lingual Language
  Models"
77,1382949643621961729,507704346,Isabelle Augenstein,"['New #NLProc paper preprint üìù, in which we present MoLE (Mixture-of-Experts w. Label Embeddings)\n\nüë∑unsupervised out-of-domain prediction of user-defined labels\nüîßparameter-efficient\nüöÄsizable performance gains in an eval on 16 stance detection datasets\n\n<LINK> <LINK>', ""This is work with some of the research team at @checkstep -- @mhardalov @rnav_arora @preslav_nakov \nIf you're interested in hearing more about it, come to my EACL invited talk at the AdaptNLP workshop next week! @barbara_plank  \nhttps://t.co/074rbmA5sa""]",https://arxiv.org/abs/2104.07467,"Stance detection concerns the classification of a writer's viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance. ",Cross-Domain Label-Adaptive Stance Detection
78,1382862830509113347,717162062837719040,Phil Armitage,"['1/ New paper! In work led by Rebecca Martin @unlv we look at how misaligned circumplanetary disks couple to planetary spin, potentially leading to primordial obliquity.\n\n<LINK>', '2/ Earlier, with Zhaohuan Zhu, Chao-Chin Yang, and Hans Baehr, we found that circumplanetary disks can be unstable to tilting due to their interaction with the stellar tidal potential. Simulations confirm that this can work given the right disk conditions. https://t.co/2ziMj1DIcw', '3/ The new paper uses an approximate, but (I think!) quite neat, way to track the spin. We evolve the circumplanetary disk using linear warp-wave equations, including stellar tidal effects and planetary oblateness. The regime (wave-like) is correct, linearity is an approximation.', ""4/ The bottom line is that if the disk becomes misaligned, *and* if it has a large enough mass, then the planetary spin axis tracks the evolution of the disk tilt pretty closely. It's certainly not clear if this happens, but it's a possible way to generate a primordial obliquity.""]",https://arxiv.org/abs/2104.06479,"Detached circumplanetary disks are unstable to tilting as a result of the stellar tidal potential. We examine how a tilted circumplanetary disk affects the evolution of the spin axis of an oblate planet. The disk is evolved using time-dependent equations for linear wave-like warp evolution, including terms representing the effect of the tidal potential and planetary oblateness. For a disk with a sufficiently large mass, we find that the planet spin quickly aligns to the misaligned disk. The tilt of the planetary spin axis then increases on the same timescale as the disk. This can be an efficient mechanism for generating primordial obliquity in giant planets. We suggest that directly imaged exoplanets at large orbital radii, where the disk mass criterion is more likely to be satisfied, could have significant obliquities due to the tilt instability of their circumplanetary disks. ",Primordial giant planet obliquity driven by a circumplanetary disk
79,1382638807233748994,804069495253962752,David Mart√≠nez Delgado,"['In our new paper (<LINK>), we introduce the Stellar Stream Legacy Survey, a systematic search for tidal streams in nearby galaxies with the exquisite imaging from the @desisurvey and @theDESurvey. (Art credit: J. Josephides, Swinburne Astronomy Productions) <LINK>']",http://arxiv.org/abs/2104.06071,"Mergers and tidal interactions between massive galaxies and their dwarf satellites are a fundamental prediction of the Lambda-Cold Dark Matter cosmology. These events are thought to provide important observational diagnostics of nonlinear structure formation. Stellar streams in the Milky Way and Andromeda are spectacular evidence for ongoing satellite disruption. However, constructing a statistically meaningful sample of tidal streams beyond the Local Group has proven a daunting observational challenge, and the full potential for deepening our understanding of galaxy assembly using stellar streams has yet to be realised. Here we introduce the Stellar Stream Legacy Survey, a systematic imaging survey of tidal features associated with dwarf galaxy accretion around a sample of ~3100 nearby galaxies within z~0.02, including about 940 Milky Way analogues. Our survey exploits public deep imaging data from the DESI Legacy Imaging Surveys, which reach surface brightness as faint as ~29 mag/arcsec^2 in the r band. As a proof of concept of our survey, we report the detection and broad-band photometry of 24 new stellar streams in the local Universe. We discuss how these observations can yield new constraints on galaxy formation theory through comparison to mock observations from cosmological galaxy simulations. These tests will probe the present-day mass assembly rate of galaxies, the stellar populations and orbits of satellites, the growth of stellar halos and the resilience of stellar disks to satellite bombardment. ",Hidden Depths in the Local Universe: the Stellar Stream Legacy Survey
80,1382619276830466048,3134039644,Jess Morley,"[""It's #preprint time! Here is my new paper with @dr_c_morton, Kassandra Karpathakis, @RosariaTaddeo &amp; @Floridi - an initial synthesis of requirements set out in lit 4 evaluating AI CDS - designed to provide the theory 4 the many policy convos in this space: <LINK>""]",https://arxiv.org/abs/2104.06910,"The potential presented by Artificial Intelligence (AI) for healthcare has long been recognised by the technical community. More recently, this potential has been recognised by policymakers, resulting in considerable public and private investment in the development of AI for healthcare across the globe. Despite this, excepting limited success stories, real-world implementation of AI systems into front-line healthcare has been limited. There are numerous reasons for this, but a main contributory factor is the lack of internationally accepted, or formalised, regulatory standards to assess AI safety and impact and effectiveness. This is a well-recognised problem with numerous ongoing research and policy projects to overcome it. Our intention here is to contribute to this problem-solving effort by seeking to set out a minimally viable framework for evaluating the safety, acceptability and efficacy of AI systems for healthcare. We do this by conducting a systematic search across Scopus, PubMed and Google Scholar to identify all the relevant literature published between January 1970 and November 2020 related to the evaluation of: output performance; efficacy; and real-world use of AI systems, and synthesising the key themes according to the stages of evaluation: pre-clinical (theoretical phase); exploratory phase; definitive phase; and post-market surveillance phase (monitoring). The result is a framework to guide AI system developers, policymakers, and regulators through a sufficient evaluation of an AI system designed for use in healthcare. ","Towards a framework for evaluating the safety, acceptability and
  efficacy of AI systems for health: an initial synthesis"
81,1382587672577851394,2563532985,Prof Anna Watts,"[""New NICER collaboration paper! The 3rd in the series of 'supporting' papers providing context to the mass and radius inference that we published late 2019 for PSR J0030+0451:   <LINK>\n\nYes, it's a trilogy.  Clich√©s we are!  Bogdanov et al. Part III"", 'We had hoped to have this one out a lot sooner, but pandemic. üôè']",https://arxiv.org/abs/2104.06928,"We describe the X-ray pulse profile models we use, and how we use them, to analyze Neutron Star Interior Composition Explorer (NICER) observations of rotation-powered millisecond pulsars to obtain information about the mass-radius relation of neutron stars and the equation of state of the dense matter in their cores. Here we detail our modeling of the observed profile of PSR J0030+0451 that we analyzed in Miller et al. (2019) and Riley et al. (2019) and describe a cross-verification of computations of the pulse profiles of a star with R/M 3, in case stars this compact need to be considered in future analyses. We also present our early cross-verification efforts of the parameter estimation procedures used by Miller et al. (2019) and Riley et al. (2019) by analyzing two distinct synthetic data sets. Both codes yielded credible regions in the mass-radius plane that are statistically consistent with one another and both gave posterior distributions for model parameter values consistent with the values that were used to generate the data. We also summarize the additional tests of the parameter estimation procedure of Miller et al. (2019) that used synthetic pulse profiles and the NICER pulse profile of PSR J0030+0451. We then illustrate how the precision of mass and radius estimates depends on the pulsar's spin rate and the size of its hot spot by analyzing four different synthetic pulse profiles. Finally, we assess possible sources of systematic error in these estimates made using this technique, some of which may warrant further investigation. ","Constraining the Neutron Star Mass--Radius Relation and Dense Matter
  Equation of State with NICER. III. Model Description and Verification of
  Parameter Estimation Codes"
82,1382555060870139905,326864126,Shota Notsu,"['Our new paper (with @cwalshastrochem etc.) has come out on today\'s astro-ph!\n\n""X-ray induced chemistry of water and related molecules in low-mass protostellar envelopes"", Notsu et al. 2021, A&amp;A in press.\n<LINK>\n#Astrochemistry #ALMA #ngVLA']",https://arxiv.org/abs/2104.06878,"Recent water line observations toward several low-mass protostars suggest low water gas fractional abundances in the inner warm envelopes. Water destruction by X-rays has been proposed to influence the water abundances in these regions, but the detailed chemistry, including the nature of alternative oxygen carriers, is not yet understood. In this study, we aim to understand the impact of X-rays on the composition of low-mass protostellar envelopes, focusing specifically on water and related oxygen bearing species. We compute the chemical composition of two low-mass protostellar envelopes using a 1D gas-grain chemical reaction network, under various X-ray field strengths. According to our calculations, outside the water snowline, the water gas abundance increases with $L_{\mathrm{X}}$. Inside the water snowline, water maintains a high abundance of $\sim 10^{-4}$ for small $L_{\mathrm{X}}$, with water and CO being the dominant oxygen carriers. For large $L_{\mathrm{X}}$, the water gas abundances significantly decrease just inside the water snowline (down to $\sim10^{-8}-10^{-7}$) and in the innermost regions ($\sim10^{-6}$). For these cases, the O$_{2}$ and O gas abundances reach $\sim 10^{-4}$ within the water snowline, and they become the dominant oxygen carriers. The HCO$^{+}$ and CH$_{3}$OH abundances, which have been used as tracers of the water snowline, significantly increase/decrease within the water snowline, respectively, as the X-ray fluxes become larger. The abundances of some other dominant molecules, such as CO$_{2}$, OH, CH$_{4}$, HCN, and NH$_{3}$, are also affected by strong X-ray fields, especially within their own snowlines. These X-ray effects are larger in lower density envelope models. Future observations of water and related molecules (using e.g., ALMA and ngVLA) will access the regions around protostars where such X-ray induced chemistry is effective. ","X-ray induced chemistry of water and related molecules in low-mass
  protostellar envelopes"
83,1382527487868071945,443618018,Satoshi INOUE,['Our new paper! \n<LINK>'],https://arxiv.org/abs/2104.06639,"Solar active region 12673 produced two successive X-class flares (X2.2 and X9.3) approximately 3 hours apart in September 2017. The X9.3 was the recorded largest solar flare in Solar Cycle 24. In this study we perform a data-constrained magnetohydrodynamic simulation taking into account the observed photospheric magnetic field to reveal the initiation and dynamics of the X2.2 and X9.3 flares. According to our simulation, the X2.2 flare is first triggered by magnetic reconnection at a local site where at the photosphere the negative polarity intrudes into the opposite-polarity region. This magnetic reconnection expels the innermost field lines upward beneath which the magnetic flux rope is formed through continuous reconnection with external twisted field lines. Continuous magnetic reconnection after the X2.2 flare enhances the magnetic flux rope, which is lifted up and eventually erupts via the torus instability. This gives rise to the X9.3 flare. ","An MHD Modeling of Successive X2.2 and X9.3 Solar Flares of 2017
  September 6"
84,1382483856339603459,710263466351681536,Max Vladymyrov üá∫üá¶,"['New paper! üìú\n\nWe introduce BLUR (Bidirectional Learning Update Rules), a novel optimization framework that parametrizes forward and back-propagation via a set of very low-dimensional meta-learned matrices.\n\n<LINK>\n\nüßµ to follow...', 'We first demonstrate that a classical neural net can be seen as a special case of a 2-state network (one for activations and one for gradients) with synapses updated according to a Hebbian rule. https://t.co/LUsSqxUCER', 'We then generalize this to a multi-state bidirectional network, where interactions between the states are controlled by a small set of meta-learned parameters called the ""genome"". \n\nNo loss function &amp; no backprop gradients required! https://t.co/SHPwl58SXc', 'As an additional bonus: BLUR can learn with asymmetric forward and backward synapses (removing the ‚Äúweight transport problem‚Äù and hinting at biological plausibility), using activations on the backward pass and more! https://t.co/NHv1nC5HyD', 'We demonstrate that our system can learn faster than SGD, with genomes trained using a standard off-the-shelf optimizer, such as Adam. \n\nWhat‚Äôs more, genomes  also can be trained with non-differentiable objectives (like accuracy) using Evolutionary training. https://t.co/K974IpuggD', 'Genomes meta-trained on MNIST can generalize to multiple other datasets, even for different input resolutions, number of classes or different architectures. https://t.co/sTDESDi2s3', 'We hope this work will open doors for a more general paradigm where the learning process itself is:\n- deeply parametrized by meta-training data;\n- not restricted by backpropagation, including but not limited to, having explicit loss functions and gradients.', 'Joint work with my coworkers:\nMark Sandler,\nAndrey Zhmoginov,\nNolan Miller,\nAndrew Jackson,\nTom Madams, \nBlaise Ag√ºera y Arcas @blaiseaguera']",https://arxiv.org/abs/2104.04657,"In this paper, we introduce a new type of generalized neural network where neurons and synapses maintain multiple states. We show that classical gradient-based backpropagation in neural networks can be seen as a special case of a two-state network where one state is used for activations and another for gradients, with update rules derived from the chain rule. In our generalized framework, networks have neither explicit notion of nor ever receive gradients. The synapses and neurons are updated using a bidirectional Hebb-style update rule parameterized by a shared low-dimensional ""genome"". We show that such genomes can be meta-learned from scratch, using either conventional optimization techniques, or evolutionary strategies, such as CMA-ES. Resulting update rules generalize to unseen tasks and train faster than gradient descent based optimizers for several standard computer vision and synthetic tasks. ",Meta-Learning Bidirectional Update Rules
85,1382372443629654018,750411947661811712,Dr. Sarah Pearson,"[""Check out our new Stellar Stream Legacy Survey paper lead by @astro_delgado: <LINK>. We report the detection and broad-band photometry of 24 new stellar\nstreams. I'm very excited for the science to come with statistical samples of external galaxy streams in hand. <LINK>""]",https://arxiv.org/abs/2104.06071,"Mergers and tidal interactions between massive galaxies and their dwarf satellites are a fundamental prediction of the Lambda-Cold Dark Matter cosmology. These events are thought to provide important observational diagnostics of nonlinear structure formation. Stellar streams in the Milky Way and Andromeda are spectacular evidence for ongoing satellite disruption. However, constructing a statistically meaningful sample of tidal streams beyond the Local Group has proven a daunting observational challenge, and the full potential for deepening our understanding of galaxy assembly using stellar streams has yet to be realised. Here we introduce the Stellar Stream Legacy Survey, a systematic imaging survey of tidal features associated with dwarf galaxy accretion around a sample of ~3100 nearby galaxies within z~0.02, including about 940 Milky Way analogues. Our survey exploits public deep imaging data from the DESI Legacy Imaging Surveys, which reach surface brightness as faint as ~29 mag/arcsec^2 in the r band. As a proof of concept of our survey, we report the detection and broad-band photometry of 24 new stellar streams in the local Universe. We discuss how these observations can yield new constraints on galaxy formation theory through comparison to mock observations from cosmological galaxy simulations. These tests will probe the present-day mass assembly rate of galaxies, the stellar populations and orbits of satellites, the growth of stellar halos and the resilience of stellar disks to satellite bombardment. ",Hidden Depths in the Local Universe: the Stellar Stream Legacy Survey
86,1382290905269420033,786855300322172928,Alkistis Pourtsidou,"['New @EC_Euclid paper led by A. Pocino, looking at how to optimise the survey for GCphot and GGL analyses. Importantly, the analysis is using very realistic photo-z distributions based on the state-of-the-art Flagship simuation...have a look! <LINK>']",https://arxiv.org/abs/2104.05698,"The accuracy of photometric redshifts (photo-zs) particularly affects the results of the analyses of galaxy clustering with photometrically-selected galaxies (GCph) and weak lensing. In the next decade, space missions like Euclid will collect photometric measurements for millions of galaxies. These data should be complemented with upcoming ground-based observations to derive precise and accurate photo-zs. In this paper, we explore how the tomographic redshift binning and depth of ground-based observations will affect the cosmological constraints expected from Euclid. We focus on GCph and extend the study to include galaxy-galaxy lensing (GGL). We add a layer of complexity to the analysis by simulating several realistic photo-z distributions based on the Euclid Consortium Flagship simulation and using a machine learning photo-z algorithm. We use the Fisher matrix formalism and these galaxy samples to study the cosmological constraining power as a function of redshift binning, survey depth, and photo-z accuracy. We find that bins with equal width in redshift provide a higher Figure of Merit (FoM) than equipopulated bins and that increasing the number of redshift bins from 10 to 13 improves the FoM by 35% and 15% for GCph and its combination with GGL, respectively. For GCph, an increase of the survey depth provides a higher FoM. But the addition of faint galaxies beyond the limit of the spectroscopic training data decreases the FoM due to the spurious photo-zs. When combining both probes, the number density of the sample, which is set by the survey depth, is the main factor driving the variations in the FoM. We conclude that there is more information that can be extracted beyond the nominal 10 tomographic redshift bins of Euclid and that we should be cautious when adding faint galaxies into our sample, since they can degrade the cosmological constraints. ","Euclid preparation: XII. Optimizing the photometric sample of the Euclid
  survey for galaxy clustering and galaxy-galaxy lensing analyses"
87,1382265915111260163,52147188,Guadalupe Ca√±as Herrera,"['I work a lot, but when you see the results all the pain disappears.\n\nNew paper about inflation and forecasting with alpha-attractors!\n\nPS: thanks to @sant87casas for the useful commentsüòç\n\n<LINK>']",https://arxiv.org/abs/2104.06398,"We study here the observational constraints on single-field inflationary models achievable with the next generation of CMB experiments. We particularly focus on a Stage-IV like experiment and forecasts its constraints on inflationary parameters in the context of $\alpha$-attractor inflation comprising a large class of single-field models. To tailor our forecasts we use as a fiducial model the results obtained with current CMB and LSS data, assuming the $\alpha$-model a priori. We found that current CMB data are able to place a tight bound on the ratio of the tensor-to-scalar ratio with the alpha parameter $r/\alpha = 3.87^{+0.78}_{-0.94}\cdot 10^{-3}$ and on the running of the scalar index $\alpha_S = -6.4^{+1.6}_{-1.3}\cdot 10^{-4}$ with a value of the scalar index consistent with current constraints. In the optimistic scenario of detection of primordial gravitational waves in the CMB B-mode polarization power spectra, we found that CMB-S4 will be able to achieve a $15\%$ bound on the value of the parameter $\alpha = 1.01^{+0.14}_{-0.18}$. This bound clearly show the ability of CMB-S4 to constrain not only the energy scale of inflation but also the shape of its potential. Enlarging the baseline model to also include the neutrino sector merely reduce the accuracy of $5\%$ leading to $\alpha = 1.07^{+0.18}_{-0.23}$ so that our main conclusions are still valid. ",Current and Future constraints on single-field $\alpha$-Attractor model
88,1382259710850633728,1138762581164855298,Christoph Ternes,"['New paper today, <LINK> , with @ntinaValentina and Andre de Gouv√™a. This is a follow up to our previous work on decoherence. We  calculate better bounds on the neutrino wave packet width and show the robustness of standard  parameters in presence of decoherence.']",https://arxiv.org/abs/2104.05806,"Reactor experiments are well suited to probe the possible loss of coherence of neutrino oscillations due to wave-packets separation. We combine data from the short-baseline experiments Daya Bay and the Reactor Experiment for Neutrino Oscillation (RENO) and from the long baseline reactor experiment KamLAND to obtain the best current limit on the reactor antineutrino wave-packet width, $\sigma > 2.1 \times 10^{-4}$ nm at 90% CL. We also find that the determination of standard oscillation parameters is robust, i.e., it is mostly insensitive to the presence of hypothetical decoherence effects once one combines the results of the different reactor neutrino experiments. ",Combined analysis of neutrino decoherence at reactor experiments
89,1382255139776520197,988837119408967680,Thuerey Group at TUM,['Our new paper on flow reconstructions from single videos via differentiable physics &amp; rendering (a CVPR oral) is now on arxiv <LINK>. Key ingredients are a learned self-supervision (GAN-style) and a global formulation for the flow transport. <LINK>'],http://arxiv.org/abs/2104.06031,"We propose a novel method to reconstruct volumetric flows from sparse views via a global transport formulation. Instead of obtaining the space-time function of the observations, we reconstruct its motion based on a single initial state. In addition we introduce a learned self-supervision that constrains observations from unseen angles. These visual constraints are coupled via the transport constraints and a differentiable rendering step to arrive at a robust end-to-end reconstruction algorithm. This makes the reconstruction of highly realistic flow motions possible, even from only a single input view. We show with a variety of synthetic and real flows that the proposed global reconstruction of the transport process yields an improved reconstruction of the fluid motion. ",Global Transport for Fluid Reconstruction with Learned Self-Supervision
90,1382235552108384257,1109745957346963456,Rub√©n Moreno-Bote,"['New paper: ‚ÄòDeep imagination is a close to optimal policy for planning in large decision trees under limited resources‚Äô, with my student @ChiaraMastrogi3 \nHow to allocate finite sample capacity in a very large decision tree? Broadly or deeply?\n<LINK> <LINK>', 'We consider a Markov Decision Process (MDP) that operates in two consecutive phases having different actions. The first phase is a learning or exploration phase, while the second one is an exploitation phase. Samples are allocated in one-shot. https://t.co/SnFOVOa2VE', 'We derive a diffusion-maximization algorithm for stochastically and binarily rewarded trees that runs in polynomial time, much better than the exponential complexity of backwards induction. https://t.co/ap4SvF61Nc', 'Solution:\nDepth dominates over breadth in large regions of parameter space.  \n(a) Value of playing optimally a tree as a function of capacity C and reward probability p\n(b) Optimal number of sampled branches b* as a function of C and p. The large plateau corresponds to the b=2 https://t.co/w55GsC4TVy', 'Depth dominates over breadth in large regions of parameter space.  \n(a) Value of playing optimally a tree as a function of capacity C and reward probability p\n(b) Optimal number of sampled branches b* as a function of C and p. The large plateau corresponds to b= 2 https://t.co/EFPZVfmCok', 'Happy to receive any comments!']",https://arxiv.org/abs/2104.06339,"Many decisions involve choosing an uncertain course of actions in deep and wide decision trees, as when we plan to visit an exotic country for vacation. In these cases, exhaustive search for the best sequence of actions is not tractable due to the large number of possibilities and limited time or computational resources available to make the decision. Therefore, planning agents need to balance breadth (exploring many actions at each level of the tree) and depth (exploring many levels in the tree) to allocate optimally their finite search capacity. We provide efficient analytical solutions and numerical analysis to the problem of allocating finite sampling capacity in one shot to large decision trees. We find that in general the optimal policy is to allocate few samples per level so that deep levels can be reached, thus favoring depth over breadth search. In contrast, in poor environments and at low capacity, it is best to broadly sample branches at the cost of not sampling deeply, although this policy is marginally better than deep allocations. Our results provide a theoretical foundation for the optimality of deep imagination for planning and show that it is a generally valid heuristic that could have evolved from the finite constraints of cognitive systems. ","Deep imagination is a close to optimal policy for planning in large
  decision trees under limited resources"
91,1382233477794439170,1355052822,Benjamin Alldritt,"['New paper @arxiv (<LINK>), integrating Bayesian inference with scanning probe experiments for robust identification of surface adsorbate configurations, with J. J√§rvi, O. Krejƒç√≠, M. Todoroviƒá, P. Liljeroth (@AaltoAtoms), P. Rinke. @AaltoResearch']",http://arxiv.org/abs/2104.05302,"Controlling the properties of organic/inorganic materials requires detailed knowledge of their molecular adsorption geometries. This is often unattainable, even with current state-of-the-art tools. Visualizing the structure of complex non-planar adsorbates with atomic force microscopy (AFM) is challenging, and identifying it computationally is intractable with conventional structure search. In this fresh approach, cross-disciplinary tools are integrated for a robust and automated identification of 3D adsorbate configurations. Bayesian optimization is employed with first-principles simulations for accurate and unbiased structure inference of multiple adsorbates. The corresponding AFM simulations then allow fingerprinting adsorbate structures that appear in AFM experimental images. In the instance of bulky (1S)-camphor adsorbed on the Cu(111) surface, three matching AFM image contrasts are found, which allow correlating experimental image features to distinct cases of molecular adsorption. ","Integrating Bayesian Inference with Scanning Probe Experiments for
  Robust Identification of Surface Adsorbate Configurations"
92,1382151767631167489,296161364,Chris Power,"[""New paper on the @arxiv today - <LINK> - by @ICRAR @arc_astro3d's @awattsup_, examining the connection between gas and star formation properties and whether or not they might be consider HI asymmetric - with Barbara Catinella, @astroquokka, and Sara Ellison. <LINK>""]",https://arxiv.org/abs/2104.05995,"Observations have revealed that disturbances in the cold neutral atomic hydrogen (HI) in galaxies are ubiquitous, but the reasons for these disturbances remain unclear. While some studies suggest that asymmetries in integrated HI spectra (global HI asymmetry) are higher in HI-rich systems, others claim that they are preferentially found in HI-poor galaxies. In this work, we utilise the ALFALFA and xGASS surveys, plus a sample of post-merger galaxies, to clarify the link between global HI asymmetry and the gas properties of galaxies. Focusing on star-forming galaxies in ALFALFA, we find that elevated global HI asymmetry is not associated with a change in the HI content of a galaxy, and that only the galaxies with the highest global HI asymmetry show a small increase in specific star-formation rate (sSFR). However, we show that the lack of a trend with HI content is because ALFALFA misses the gas-poor tail of the star-forming main-sequence. Using xGASS to obtain a sample of star-forming galaxies that is representative in both sSFR and HI content, we find that global HI asymmetric galaxies are typically more gas-poor than symmetric ones at fixed stellar mass, with no change in sSFR. Our results highlight the complexity of the connection between galaxy properties and global HI asymmetry. This is further confirmed by the fact that even post-merger galaxies show both symmetric and asymmetric HI spectra, demonstrating that merger activity does not always lead to an asymmetric global HI spectrum. ","On the relationship between gas content, star-formation, and global HI
  asymmetry of galaxies on the star-forming main-sequence"
93,1382146239052066817,321794593,Jos√© G. Fern√°ndez-Trincado,"['Our new accepted paper today on ArXiv üëâ""APOGEE view of the globular cluster NGC 6544"" by Gran et al. and the APOGEE team üòÄ üëâ<LINK> üëá <LINK>']",https://arxiv.org/abs/2104.05865,"The second phase of the APOGEE survey is providing near-infrared, high-resolution, high signal-to-noise spectra of stars in the halo, disk, bar and bulge of the Milky Way. The near-infrared spectral window is especially important in the study of the Galactic bulge, where stars are obscured by the dust and gas of the disk in its line-of-sight. We present a chemical characterisation of the globular cluster NGC 6544 with high-resolution spectroscopy. The characterisation of the cluster chemical fingerprint, given its status of ""interloper"" towards the Galactic bulge and clear signatures of tidal disruption in its core is crucial for future chemical tagging efforts. Cluster members were selected from the DR16 of the APOGEE survey, using chemo-dynamical criteria of individual stars. A sample of 23 members of the cluster was selected. An analysis considering the intra-cluster abundance variations, known anticorrelations is given. According to the RGB content of the cluster, the iron content and $\alpha$-enhancement are [Fe/H] $= -1.44 \pm 0.04$ dex and [$\alpha$/Fe] $= 0.20 \pm 0.04$ dex, respectively. Cluster members show a significant spread in [Fe/H] and [Al/Fe] that is larger than expected based on measurement errors. An [Al/Fe] spread, signal of an Mg-Al anticorrelation is observed and used to constraint the cluster mass budget, along with C, N, Mg, Si, K, Ca, and Ce element variations are discussed. Across all the analysed evolutionary stages (RGB and AGB), about $\sim2/3$ (14 out of 23) show distinct chemical patterns, possibly associated with second-generation stars. ",APOGEE view of the globular cluster NGC 6544
94,1382137534407340033,369569444,Takahiro TERADA (ÂØ∫Áî∞ ÈöÜÂ∫É),"['My new paper:  ‚ÄúMinimal Supergravity Inflation without Slow Gravitino‚Äù <LINK> [hep-th]', 'Recently, ‚ÄúCatastrophic production of slow gravitinos‚Äù by Kolb, Long, Mcdonough (2021) got some attention.  This was originally studied by us in 1701.03106.  I propose a way to avoid this problem but still obtain the supergravity inflation with the minimal degrees of freedom.', 'We utilize a recently proposed cubic nilpotent superfield Œ¶ ((Œ¶-Œ¶*)^3=0) to realize inflation in supergravity with the minimal degrees of freedom: the inflaton, graviton, and massive gravitino.', 'As an advantage, the resultant model is free from the catastrophic production of gravitinos due to its vanishing ‚Äúsound speed‚Äù. However, the model suffers from the standard gravitino problem, and its viability depends on the mass spectrum and the thermal history of the universe.', 'The original ‚ÄúMinimal Supergravity Inflation‚Äù model is by Carrasco, Kallosh, Linde (2015) and Ferrara, Kallosh, Thaler (2015), which is also related to Kahn, Roberts, Thaler (2015).  I used a recently proposed constraint (S+S*)^3=0 by Aldabergenov, Chatrabhuti, and Isono (2021).']",https://arxiv.org/abs/2104.05731,"We utilize a recently proposed cubic nilpotent superfield to realize inflation in supergravity with the minimal degrees of freedom: the inflaton, graviton, and massive gravitino. As an advantage, the resultant model is free from the catastrophic production of gravitinos due to its vanishing propagation speed. However, the model suffers from the standard gravitino problem, and its viability depends on the mass spectrum and the thermal history of the universe. ",Minimal Supergravity Inflation without Slow Gravitino
95,1382078749802389512,791705191175360512,Niels Warburton,"['Our new paper out today details the first fully relativistic EMRI template model fast enough to do parameter estimation with. As a demonstration we explore the accuracy of the semi-relativistic kludge waveform generation methods. <LINK> <LINK>', 'For certain regions of the parameter space, using semi-relativistic amplitudes to recover an injected full relativistic waveform can lead to biases in the posterior distribution https://t.co/VigCMuXLwY', 'The fully relativistic model is part of a new FastEMRIWaveform framework that provides a unified Python interface to generate EMRI waveform templates in the solar system barycenter frame. This makes it very easy for data analysts to upgrade to new models as they become available. https://t.co/7VS6ylgVWQ', 'The fully relativistic model is fast. Using GPU acceleration you can generate a year-long EMRI waveform in less than 100ms. All the code is open source in the BHPToolkit: https://t.co/ie7SABWp3U', 'The fully relativistic model, first introduced in our letter, https://t.co/TiAckbhy88, is only for inspirals into non-rotating black holes. The methods we used should extend to Kerr and incorporate GSF corrections as they become available. https://t.co/xcRJYsWxlz', ""For the LISA Data Challenge it is useful to have waveform models that are extensive in parameter space so whilst we wait for fully relativistic Kerr waveforms we also include an updated GPU accel'd Augment Analytic Kludge model in the FEW framework.""]",https://arxiv.org/abs/2104.04582,"We present the FastEMRIWaveforms (FEW) package, a collection of tools to build and analyze extreme mass ratio inspiral (EMRI) waveforms. Here, we expand on the Physical Review Letter that introduced the first fast and accurate fully-relativistic EMRI waveform template model. We discuss the construction of the overall framework; constituent modules; and the general methods used to accelerate EMRI waveforms. Because the fully relativistic FEW model waveforms are for now limited to eccentric orbits in the Schwarzschild spacetime, we also introduce an improved Augmented Analytic Kludge (AAK) model that describes generic Kerr inspirals. Both waveform models can be accelerated using graphics processing unit (GPU) hardware. With the GPU-accelerated waveforms in hand, a variety of studies are performed including an analysis of EMRI mode content, template mismatch, and fully Bayesian Markov Chain Monte Carlo-based EMRI parameter estimation. We find relativistic EMRI waveform templates can be generated with fewer harmonic modes ($\sim10-100$) without biasing signal extraction. However, we show for the first time that extraction of a relativistic injection with semi-relativistic amplitudes can lead to strong bias and anomalous structure in the posterior distribution for certain regions of parameter space. ","FastEMRIWaveforms: New tools for millihertz gravitational-wave data
  analysis"
96,1382032130172198914,520665133,R. T. Pierrehumbert,"[""New paper by ERC Exocondense fellow Maxence LeFevre (in @ClimateBook group), on convection in Proxima Centauri b's atmosphere.  <LINK> . In collaboration with Martin Turbet""]",https://arxiv.org/abs/2104.05559,"A large fraction of known terrestrial-size exoplanets located in the Habitable Zone of M-dwarfs are expected to be tidally-locked. Numerous efforts have been conducted to study the climate of such planets, using in particular 3-D Global Climate Models (GCM). One of the biggest challenges in simulating such an extreme environment is to properly represent the effects of sub-grid convection. Most GCMs use either a simplistic convective-adjustment parametrization or sophisticated (e.g., mass flux scheme) Earth-tuned parametrizations. One way to improve the representation of convection is to study convection using Convection Resolving numerical Models (CRMs), with an fine spatial resolution . In this study, we developed a CRM coupling the non-hydrostatic dynamical core WRF with the radiative transfer and cloud/precipitation models of the LMD-Generic climate model to study convection and clouds on tidally-locked planets, with a focus on Proxima b. Simulations were performed for a set of 3 surface temperatures (corresponding to three different incident fluxes) and 2 rotation rates, assuming an Earth-like atmosphere. The main result of our study is that while we recover the prediction of GCMs that (low-altitude) cloud albedo increases with increasing stellar flux, the cloud feedback is much weaker due to transient aggregation of convection leading to low partial cloud cover. ","3D convection-resolving model of temperate, tidally-locked exoplanets"
97,1382020321121734657,3389619551,Bhaskar Krishnamachari,"['Our new paper with @arvin_hekmati and colleagues studies #COVID19  spread in a university through modeling and #simulations based on real data. It shows #masks reduce new cases by 3.6x and hybrid classes with 20% occupancy reduce new cases by about 2.15x.  <LINK>', ""The paper will appear at the IEEE ICC'21 COVI-COM workshop. https://t.co/BTPRlJI3sK"", 'Here is a short video by @arvin_hekmati explaining our paper on modeling and simulating COVID-19 infection spread on a university campus  : https://t.co/mBElOHmBQm']",https://arxiv.org/abs/2104.04129,"Airborne transmission is now believed to be the primary way that COVID-19 spreads. We study the airborne transmission risk associated with holding in-person classes on university campuses. We utilize a model for airborne transmission risk in an enclosed room that considers the air change rate for the room, mask efficiency, initial infection probability of the occupants, and also the activity level of the occupants. We introduce, and use for our evaluations, a metric $R_0^{eff}$ that represents the ratio of new infections that occur over a week due to classroom interactions to the number of infected individuals at the beginning of the week. This can be seen as a surrogate for the well-known $R_0$ reproductive number metric, but limited in scope to classroom interactions and calculated on a weekly basis. The simulations take into account the possibility of repeated in-classroom interactions between students throughout the week. We presented model predictions were generated using Fall 2019 and Fall 2020 course registration data at a large US university, allowing us to evaluate the difference in transmission risk between in-person and hybrid programs. We quantify the impact of parameters such as reduced occupancy levels and mask efficacy. Our simulations indicate that universal mask usage results in an approximately $3.6\times$ reduction in new infections through classroom interactions. Moving 90% of the classes online leads to about $18\times$ reduction in new cases. Reducing class occupancy to 20%, by having hybrid classes, results in an approximately $2.15-2.3\times$ further reduction in new infections. ","Simulation-Based Analysis of COVID-19 Spread Through Classroom
  Transmission on a University Campus"
98,1381951465737093126,1685737398,F√°bio Cruz,"['New preprint of a recent paper w/ Thomas Grismayer and @luis_os @golp_ipfn @istecnico on a new kinetic instability in a special plasma equilibrium relevant in pulsar magnetospheres is now out: <LINK> This will appear soon on PRE. #PaperDay <LINK>', 'In a nutshell: a DC (null wavenumber) inductive mode is converted into electrostatic modes at near pump harmonics. The energy is converter into larger and larger harmonics over time, eventually depleting the pump! https://t.co/vPP9fR3eo8', 'This was an interesting plasma physics problem: having a null wavenumber, the pump is not subject to other fundamental depletion/damping mechanisms such as Landau damping or wave-breaking.  This instability can be seen as a *kind of* forced resonant harmonic oscillator.', '@TolmanLibby Thanks Libby! Hope all is well with you.']",http://arxiv.org/abs/2104.04490,"A uniform in space, oscillatory in time plasma equilibrium sustained by a time-dependent current density is analytically and numerically studied resorting to particle-in-cell simulations. The dispersion relation is derived from the Vlasov equation for oscillating equilibrium distribution functions, and used to demonstrate that the plasma has an infinite number of unstable kinetic modes. This instability represents a new kinetic mechanism for the decay of the initial mode of infinite wavelength (or equivalently null wavenumber), for which no classical wave breaking or Landau damping exists. The relativistic generalization of the instability is discussed. In this regime, the growth rate of the fastest growing unstable modes scales with $\gamma_T^{-1/2}$, where $\gamma_T$ is the largest Lorentz factor of the plasma distribution. This result hints that this instability is not as severely suppressed for large Lorentz factor flows as purely streaming instabilities. The relevance of this instability in inductive electric field oscillations driven in pulsar magnetospheres is discussed. ",Kinetic instability in inductively oscillatory plasma equilibrium
99,1381905200928198658,1310552063999438849,Hauke Group,['üì¢New paper out!\nüôåüèªCongrats to #KevinGeier &amp; @PhilippHauke\nüîé<LINK>\nüó£Non-Hermitian linear response gives access to unequal-time anti-commutators. It provides an unbiased way of probing fluctuation-dissipation relations in #quantum many-body systems\n@ERC_Research <LINK>'],https://arxiv.org/abs/2104.03983,"Quantum many-body systems are characterized by their correlations. While equal-time correlators and unequal-time commutators between operators are standard observables, the direct access to unequal-time anti-commutators poses a formidable experimental challenge. Here, we propose a general technique for measuring unequal-time anti-commutators using the linear response of a system to a non-Hermitian perturbation. We illustrate the protocol at the example of a Bose-Hubbard model, where the approach to thermal equilibrium in a closed quantum system can be tracked by measuring both sides of the fluctuation-dissipation relation. We relate the scheme to the quantum Zeno effect and weak measurements, and illustrate possible implementations at the example of a cold-atom system. Our proposal provides a way of characterizing dynamical correlations in quantum many-body systems with potential applications in understanding strongly correlated matter as well as for novel quantum technologies. ","From non-Hermitian linear response to dynamical correlations and
  fluctuation-dissipation relations in quantum many-body systems"
100,1381893390355300356,1179672664002183168,Ahmet Iscen,"['New paper on arxiv!\nClass-Balanced Distillation for Long-Tailed Visual Recognition\n<LINK>\n\nWith Andr√© Araujo, @BoqingGo and @CordeliaSchmid <LINK>', 'We propose a new method which combines the advantages of instance and class-balanced sampling by distilling the feature representations of multiple teachers with different characteristics.']",https://arxiv.org/abs/2104.05279,"Real-world imagery is often characterized by a significant imbalance of the number of images per class, leading to long-tailed distributions. An effective and simple approach to long-tailed visual recognition is to learn feature representations and a classifier separately, with instance and class-balanced sampling, respectively. In this work, we introduce a new framework, by making the key observation that a feature representation learned with instance sampling is far from optimal in a long-tailed setting. Our main contribution is a new training method, referred to as Class-Balanced Distillation (CBD), that leverages knowledge distillation to enhance feature representations. CBD allows the feature representation to evolve in the second training stage, guided by the teacher learned in the first stage. The second stage uses class-balanced sampling, in order to focus on under-represented classes. This framework can naturally accommodate the usage of multiple teachers, unlocking the information from an ensemble of models to enhance recognition capabilities. Our experiments show that the proposed technique consistently outperforms the state of the art on long-tailed recognition benchmarks such as ImageNet-LT, iNaturalist17 and iNaturalist18. ",Class-Balanced Distillation for Long-Tailed Visual Recognition
101,1381830904515993600,930764003277643777,Matias Quiroz,['Today I will present our work new work on subsampling MCMC for multivariate time series in #ABCinSvalbard. Slides here: <LINK> and paper here: <LINK>.'],https://arxiv.org/abs/2104.02134,"Spectral subsampling MCMC was recently proposed to speed up Markov chain Monte Carlo (MCMC) for long stationary univariate time series by subsampling periodogram observations in the frequency domain. This article extends the approach to stationary multivariate time series. It also proposes a multivariate generalisation of the autoregressive tempered fractionally differentiated moving average model (ARTFIMA) and establishes some of its properties. The new model is shown to provide a better fit compared to multivariate autoregressive moving average models for three real world examples. We demonstrate that spectral subsampling may provide up to two orders of magnitude faster estimation, while retaining MCMC sampling efficiency and accuracy, compared to spectral methods using the full dataset. ",Spectral Subsampling MCMC for Stationary Multivariate Time Series
102,1381818144952627204,30650518,Sergei Belousov,"['Today, my new article was published:\n\nMobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis\n\nPaper: <LINK>\nCode: <LINK>\nPython library: <LINK>\nSample videos: <LINK>']",https://arxiv.org/abs/2104.04767,"In recent years, the use of Generative Adversarial Networks (GANs) has become very popular in generative image modeling. While style-based GAN architectures yield state-of-the-art results in high-fidelity image synthesis, computationally, they are highly complex. In our work, we focus on the performance optimization of style-based generative models. We analyze the most computationally hard parts of StyleGAN2, and propose changes in the generator network to make it possible to deploy style-based generative networks in the edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer parameters and is x9.5 less computationally complex than StyleGAN2, while providing comparable quality. ","MobileStyleGAN: A Lightweight Convolutional Neural Network for
  High-Fidelity Image Synthesis"
103,1381692581290110978,1284311543648329728,Maitraiyee Tiwari,"[""Our new paper describing the first results of  @SOFIAtelescope's legacy program (FEEDBACK) was accepted by ApJ. We study the stellar feedback in RCW49 by characterizing its wind driven shell. \n@AstroRamsey \n\narXiv: <LINK> <LINK>""]",http://arxiv.org/abs/2104.04276,"We unveil the stellar wind driven shell of the luminous massive star-forming region of RCW 49 using SOFIA FEEDBACK observations of the [CII] 158 $\mu$m line. The complementary dataset of the $^{12}$CO and $^{13}$CO J = 3 - 2 transitions is observed by the APEX telescope and probes the dense gas toward RCW 49. Using the spatial and spectral resolution provided by the SOFIA and APEX telescopes, we disentangle the shell from a complex set of individual components of gas centered around RCW 49. We find that the shell of radius ~ 6 pc is expanding at a velocity of 13 km s$^{-1}$ toward the observer. Comparing our observed data with the ancillary data at X-Ray, infrared, sub-millimeter and radio wavelengths, we investigate the morphology of the region. The shell has a well defined eastern arc, while the western side is blown open and is venting plasma further into the west. Though the stellar cluster, which is ~ 2 Myr old gave rise to the shell, it only gained momentum relatively recently as we calculate the shell's expansion lifetime ~ 0.27 Myr, making the Wolf-Rayet star WR20a a likely candidate responsible for the shell's re-acceleration. ","SOFIA FEEDBACK survey: exploring the dynamics of the stellar wind driven
  shell of RCW 49"
104,1381615526431035401,1958546466,David Jennings,"['New paper on estimating correlations in quantum channels, non-separability and benchmarking: <LINK>\nCongrats to Matt on an excellent 1st paper, and thanks to Cristina @quantumcris at @cambridgecqc for her brilliant work on this!']",https://arxiv.org/abs/2104.04352,"The ability to transfer coherent quantum information between systems is a fundamental component of quantum technologies and leads to coherent correlations within the global quantum process. However correlation structures in quantum channels are less studied than those in quantum states. Motivated by recent techniques in randomized benchmarking, we develop a range of results for efficient estimation of correlations within a bipartite quantum channel. We introduce sub-unitarity measures that are invariant under local changes of basis, generalize the unitarity of a channel, and allow for the analysis of coherent information exchange within channels. Using these, we show that unitarity is monogamous, and provide a novel information-disturbance relation. We then define a notion of correlated unitarity that quantifies the correlations within a given channel. Crucially, we show that this measure is strictly bounded on the set of separable channels and therefore provides a witness of non-separability. Finally, we describe how such measures for effective noise channels can be efficiently estimated within different randomized benchmarking protocols. We find that the correlated unitarity can be estimated in a SPAM-robust manner for any separable quantum channel, and show that a benchmarking/tomography protocol with mid-circuit resets can reliably witness non-separability for sufficiently small reset errors. The tools we develop provide information beyond that obtained via simultaneous randomized benchmarking and so could find application in the analysis of cross-talk and coherent errors in quantum devices. ","Estimation of correlations and non-separability in quantum channels via
  unitarity benchmarking"
105,1381561087682871297,926506527216865281,Aymeric Vi√©,['New paper: Population network structure impacts genetic algorithm optimisation performance (and even improves it!)\n<LINK>'],https://arxiv.org/abs/2104.04254,"A genetic algorithm (GA) is a search method that optimises a population of solutions by simulating natural evolution. Good solutions reproduce together to create better candidates. The standard GA assumes that any two solutions can mate. However, in nature and social contexts, social networks can condition the likelihood that two individuals mate. This impact of population network structure over GAs performance is unknown. Here we introduce the Networked Genetic Algorithm (NGA) to evaluate how various random and scale-free population networks influence the optimisation performance of GAs on benchmark functions. We show evidence of significant variations in performance of the NGA as the network varies. In addition, we find that the best-performing population networks, characterised by intermediate density and low average shortest path length, significantly outperform the standard complete network GA. These results may constitute a starting point for network tuning and network control: seeing the network structure of the population as a parameter that can be tuned to improve the performance of evolutionary algorithms, and offer more realistic modelling of social learning. ","Population network structure impacts genetic algorithm optimisation
  performance"
106,1381522458642161666,216729597,Marcel S. Pawlowski,"['Paper day! ""Gaia EDR3 proper motions of Milky Way dwarfs I: 3D Motions and Orbits"" led by Hefan Li and Francois Hammer: <LINK>\n\nIt presents proper motions for 46 MW dSph satellite galaxies based on Gaia Early Data Release 3, and some implications of the new data. <LINK>', 'We provide the proper motions (PM) and their random+systematic uncertainties, implied velocity components, and orbital properties in a variety of possible Milky Way potentials, including the chances that an object is unbound from the MW. https://t.co/ccPUfBNq7j', 'We follow the methodology by Fritz+(2018, https://t.co/cdD8lW0Mnp) and have 37 dSphs in common with that study. Good news: the PMs of the studies agree well (as expected ~1/3 of systems have PMs differing by &gt;1œÉ), but the PM uncertainties have improved by about a factor of 2.5! https://t.co/YlcijsPvp8', ""However, in their research note McConnachie &amp; Venn (2020, https://t.co/VcKXz1Tjke) also provide  updated proper motions based on Gaia EDR3. Again we find good agreement, but their errors are smaller. However, note that they don't provide systematic errors, while we include them. https://t.co/qwpSh4NgdT"", 'I said satellite galaxies above. Are all dSphs indeed bound to the MW? That depends on the potential. Orbits were calculated in 4 potentials consistent with the MW rotation curve. In the massive ones most dSphs are bound, but for the more extreme low-mass ones many might not be. https://t.co/CPvYLos0pb', 'Like in previous works, the new PMs again give an excess of dSphs close to their pericenters. The lines are for the two most massive assumed MW potentials. A more massive MW helps a bit, but ‚Ä¶ https://t.co/kfdC6Ln6Ox', ""‚Ä¶ we tried to pick volume-complete samples, so the most obvious way out, discovering more dSphs at larger distances where they are close to their apocenters, isn't a likely way to alleviate this."", ""Finally, we of course also looked at the alignment with the Vast Polar Structure, the satellite plane around the Milky Way (pink circles). Of the 40 dSphs that can have aligned orbital poles, 11 clearly don't align, 20 align, and 9 are consistent but not well constrained. https://t.co/72FdBwafJV"", 'A real strength is the similarity of this study to Fritz et al. (2018), which allows to judge the impact of improved PM errors by directly comparing the inferred orbital alignments for the 37 dSphs in common between the two studies: ‚Ä¶', ""Fritz+ found 14 (7) to be likely (certainly) aligned, we now find 16 (10). So both numbers grew with better data. The counter-orbiting fraction decreased, too. All signs that there's an underlying correlation among a substantial fraction (~1/2 to 2/3) of the MW satellites."", ""There's of course a lot more that can be done with this new data. We're already busy working on several related studies, so stay tuned!""]",https://arxiv.org/abs/2104.03974,"Based on Gaia Early Data Release 3 (EDR3), we estimate the proper motions for 46 dwarf spheroidal galaxies (dSphs) of the Milky Way. The uncertainties in proper motions, determined by combining both statistical and systematic errors, are smaller by a factor 2.5, when compared with Gaia Data Release 2. We have derived orbits in four Milky Way potential models that are consistent with the MW rotation curve, with total mass ranging from $2.8\times10^{11}$ $M_{\odot}$ to $15\times10^{11}$ $M_{\odot}$. Although the type of orbit (ellipse or hyperbola) are very dependent on the potential model, the pericenter values are firmly determined, largely independent of the adopted MW mass model. By analyzing the orbital phases, we found that the dSphs are highly concentrated close to their pericenter, rather than to their apocenter as expected from Kepler's law. This may challenge the fact that most dSphs are Milky Way satellites, or alternatively indicates an unexpected large number of undiscovered dSphs lying very close to their apocenters. Between half and two thirds of the satellites have orbital poles that indicate them to orbit along the Vast Polar Structure (VPOS), with the vast majority of these co-orbiting in a common direction also shared by the Magellanic Clouds, which is indicative of a real structure of dSphs. ",Gaia EDR3 proper motions of Milky Way dwarfs I: 3D Motions and Orbits
107,1380793392490283010,14697496,Thomas Steiner,"[""New arXiv paper üìÑ: Accessing HID Devices on the Web With the #WebHID API: How to play the Chrome Dino Game by Jumping With a Nintendo Joy-Con Controller in One's Pocket. ü¶ñ <LINK>. For @TheWebConf Developers Track.""]",https://arxiv.org/abs/2104.02392,"In this demonstration, we show how special hardware like Nintendo Joy-Con controllers can be made accessible from the Web through the new WebHID API. This novel technology proposal allows developers to write Web drivers in pure JavaScript that talk to Human Interface Device (HID) devices via the HID protocol. One such example of a driver has been realized in the project Joy-Con-WebHID, which allows for fun pastimes like playing the Google Chrome browser's offline dinosaur game by jumping. This works thanks to the accelerometers built into Joy-Con controllers whose signals are read out by the driver and used to control the game character in the browser. A video of the experience is available. ","Accessing HID Devices on the Web With the WebHID API: How to play the
  Chrome Dino Game by Jumping With a Nintendo Joy-Con Controller in One's
  Pocket"
108,1380582300971196418,24053629,Balaji Lakshminarayanan,"['Excited to announce our new paper ""Does Your Dermatology Classifier Know What It Doesn\'t Know?"", led by @abzz4ssj @jessierenjie @jimwinkens along with an awesome set of collaborators @GoogleAI @GoogleHealth @DeepMind.\n\nPaper: <LINK>\n\nThread 1/n: <LINK>', ""2/n \n\nFor medical imaging, it's not only sufficient for a model to have good test accuracy on i.i.d. inputs, but also important for the model to detect inputs that do not belong to one of the training classes, which is challenging due to the long-tail nature of unseen conditions. https://t.co/e7amQbhVAt"", '3/n\n\nWe propose hierarchical outlier detection loss that leverages ""known outliers"" by performing a coarse inlier vs outlier classification as well as fine-grained classification. https://t.co/ADugiZkQgW', '4/n \n\nWe show that the hierarchical outlier detection loss   improves OOD detection and outperforms methods which collapse outliers from different classes into the same ""reject bucket"" https://t.co/S8GPKlMEbv', '5/n \n\nWe also show that diverse ensembles improve OOD detection, which highlights that it may be beneficial to use different pre-training strategies for members of the ensemble. https://t.co/asyD3273GG', 'n/n\n\nTo mimic downstream impact, we evaluate models using a cost matrix which unifies accuracy and abstention on OOD/misclassified inputs. We show that our approach consistently outperforms the baseline.\n\nFor more details and results, check out our paper! https://t.co/I237izZ1K5 https://t.co/FAYKMN5WyH']",https://arxiv.org/abs/2104.03829,"We develop and rigorously evaluate a deep learning based system that can accurately classify skin conditions while detecting rare conditions for which there is not enough data available for training a confident classifier. We frame this task as an out-of-distribution (OOD) detection problem. Our novel approach, hierarchical outlier detection (HOD) assigns multiple abstention classes for each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate the effectiveness of the HOD loss in conjunction with modern representation learning approaches (BiT, SimCLR, MICLe) and explore different ensembling strategies for further improving the results. We perform an extensive subgroup analysis over conditions of varying risk levels and different skin types to investigate how the OOD detection performance changes over each subgroup and demonstrate the gains of our framework in comparison to baselines. Finally, we introduce a cost metric to approximate downstream clinical impact. We use this cost metric to compare the proposed method against a baseline system, thereby making a stronger case for the overall system effectiveness in a real-world deployment scenario. ","Does Your Dermatology Classifier Know What It Doesn't Know? Detecting
  the Long-Tail of Unseen Conditions"
109,1380506400607047684,280624416,Alberto Caimo,['New preprint of the Bergm v5 tutorial paper (to appear in JSS):  <LINK> <LINK>'],https://arxiv.org/abs/2104.02444,"Recent advances in computational methods for intractable models have made network data increasingly amenable to statistical analysis. Exponential random graph models (ERGMs) emerged as one of the main families of models capable of capturing the complex dependence structure of network data in a wide range of applied contexts. The Bergm package for R has become a popular package to carry out Bayesian parameter inference, missing data imputation, model selection and goodness-of-fit diagnostics for ERGMs. Over the last few years, the package has been considerably improved in terms of efficiency by adopting some of the state-of-the-art Bayesian computational methods for doubly-intractable distributions. Recently, version 5 of the package has been made available on CRAN having undergone a substantial makeover, which has made it more accessible and easy to use for practitioners. New functions include data augmentation procedures based on the approximate exchange algorithm for dealing with missing data, adjusted pseudo-likelihood and pseudo-posterior procedures, which allow for fast approximate inference of the ERGM parameter posterior and model evidence for networks on several thousands nodes. ",Statistical Network Analysis with Bergm
110,1380469687230795780,78913886,parfait Atchad√©,"['New paper <LINK>! We propose a convolutional filter that takes advantage of the classic #ML experience, quantum effects and the variational principle to enhance #CNN. I like to thank @XanaduAI @pennylaneai @awscloud &amp; team for hosting #Qhack21 #QML #ai #Quantum <LINK>']",http://arxiv.org/abs/2104.03418,"Convolutional Neural Networks (CNN) are used mainly to treat problems with many images characteristic of Deep Learning. In this work, we propose a hybrid image classification model to take advantage of quantum and classical computing. The method will use the potential that convolutional networks have shown in artificial intelligence by replacing classical filters with variational quantum filters. Similarly, this work will compare with other classification methods and the system's execution on different servers. The algorithm's quantum feasibility is modelled and tested on Amazon Braket Notebook instances and experimented on the Pennylane's philosophy and framework. ",Quantum Enhanced Filter: QFilter
111,1380459343586455553,1020053469787566082,Alan Karthikesalingam,"['Our new paper tackles an important safety hurdle for ML from code to clinic- ‚Äúhow does your dermatology classifier know what it doesn‚Äôt know?‚Äù In clinical practice patients may present with conditions unseen by ML systems in training, causing errors <LINK> 1/2', 'Detecting these previously-unseen conditions is challenging but enables safer management, eg deferral to clinician experts. Joint work @DeepMind @GoogleHealth @GoogleAI with @abzz4ssj @jimwinkens @vivnat @pwnic @balajiln @JanFreyberg @TaylanCemgilML @_basilM @AziziShekoofeh et al']",http://arxiv.org/abs/2104.03829,"We develop and rigorously evaluate a deep learning based system that can accurately classify skin conditions while detecting rare conditions for which there is not enough data available for training a confident classifier. We frame this task as an out-of-distribution (OOD) detection problem. Our novel approach, hierarchical outlier detection (HOD) assigns multiple abstention classes for each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate the effectiveness of the HOD loss in conjunction with modern representation learning approaches (BiT, SimCLR, MICLe) and explore different ensembling strategies for further improving the results. We perform an extensive subgroup analysis over conditions of varying risk levels and different skin types to investigate how the OOD detection performance changes over each subgroup and demonstrate the gains of our framework in comparison to baselines. Finally, we introduce a cost metric to approximate downstream clinical impact. We use this cost metric to compare the proposed method against a baseline system, thereby making a stronger case for the overall system effectiveness in a real-world deployment scenario. ","Does Your Dermatology Classifier Know What It Doesn't Know? Detecting
  the Long-Tail of Unseen Conditions"
112,1380453933274005506,47059480,Daniela Huppenkothen,"['New paper alert! I was going to tweet about this yesterday, but then I got a bit distracted by ERC shenanigans. \n\nSo, here‚Äôs my new paper with @matteobachetti on simulation-based inference to mitigate dead time in X-ray detectors. <LINK> <LINK>', 'Why, you might ask? Well, dead time is an annoying effect in X-ray telescopes that changes the variability properties for bright sources. Like all the wavy structure in the periodogram below (the peak is real, though). ‚¨áÔ∏è‚¨áÔ∏è‚¨áÔ∏è https://t.co/jn1lMkD5hu', 'If your instrument has more than one identical detectors, like NuSTAR, there are methods you can use. But what if your detector doesn‚Äôt? What if dead time is variable, or energy-dependent, or otherwise complicated? https://t.co/3Lb7Vkwltc', 'Well, we‚Äôve got a solution for you: simulation-based inference! If you can simulate dead time-affected data for your detector, you can use those simulations to train a neural network to approximate the posterior probability. Magic! (not really, but it kind of feels like it) https://t.co/Ajy4l97mDj', 'This works on simulations, and also on real data from GRS 1915+105, the strangest of black holes. You can make sampling quite fast, which allows you do make probabilistic versions of dynamical power spectra! üòé https://t.co/N3ddylBNO8', 'Note: this in principle works on other types of instrumental biases, too, as long as you can simulate them. For example, data with gaps when you‚Äôre trying to look at long time scales! https://t.co/b9pLvUVNSg', ""This paper has been years in the making! It started with a question about the cospectrum that @matteobachetti  asked me at a conference back in 2017, that's been keeping me busy ever since. The seeds were planted when @KyleCranmer explained likelihood-free inference to me."", 'It took until this year, because just in the last couple of years there‚Äôs been really cool advances in neural network-based density estimation for simulation-based inference, which made this computationally feasible.', 'The paper is still under review, so I‚Äôm very happy to receive feedback, comments, suggestions. You can also play around with it yourself in the notebooks in this repo (though I should really clean that up a bit more): \nhttps://t.co/mpH7vcB6mH']",https://arxiv.org/abs/2104.03278,"Because many of our X-ray telescopes are optimized towards observing faint sources, observations of bright sources like X-ray binaries in outburst are often affected by instrumental biases. These effects include dead time and photon pile-up, which can dramatically change the statistical inference of physical parameters from these observations. While dead time is difficult to take into account in a statistically consistent manner, simulating dead time-affected data is often straightforward. This structure makes the issue of inferring physical properties from dead time-affected observations fall into a class of problems common across many scientific disciplines. There is a growing number of methods to address them under the name of Simulation-Based Inference (SBI), aided by new developments in density estimation and statistical machine learning. In this paper, we introduce SBI as a principled way to infer variability properties from dead time-affected light curves. We use Sequential Neural Posterior Estimation to estimate the posterior probability for variability properties. We show that this method can recover variability parameters on simulated data even when dead time is variable, and present results of an application of this approach to NuSTAR observations of the galactic black hole X-ray binary GRS 1915+105. ","Accurate X-ray Timing in the Presence of Systematic Biases With
  Simulation-Based Inference"
113,1380445635187466246,1359421250431516683,Dr. Eva Laplace,"[""It's new paper day! With 2D supernova explosion simulations, we find that stars that get stripped by a binary companion tend to explode more easily than stars of the same initial mass that remain single all their life.\nMore details at <LINK>""]",https://arxiv.org/abs/2104.03317,"Most massive stars experience binary interactions in their lifetimes that can alter both the surface and core structure of the stripped star with significant effects on their ultimate fate as core-collapse supernovae. However, core-collapse supernovae simulations to date have focused almost exclusively on the evolution of single stars. We present a systematic simulation study of single and binary-stripped stars with the same initial mass as candidates for core-collapse supernovae (11 - 21 M$_{\odot}$). Generally, we find that binary-stripped stars core tend to be less compact, with a more prominent, deeper silicon/oxygen interface, and explode preferentially to the corresponding single stars of the same initial mass. Such a dichotomy of behavior between these two modes of evolution would have important implications for supernovae statistics, including the final neutron star masses, explosion energies, and nucleosynthetic yields. Binary-stripped remnants are also well poised to populate the possible mass gap between the heaviest neutron stars and the lightest black holes. Our work presents an improvement along two fronts, as we self-consistently account for the pre-collapse stellar evolution and the subsequent explosion outcome. Even so, our results emphasize the need for more detailed stellar evolutionary models to capture the sensitive nature of explosion outcome. ",Binary-Stripped Stars as Core-Collapse Supernovae Progenitors
114,1380437828056707077,3094610676,Pranav Rajpurkar,"['Radiology report labelers have significantly improved over last couple of years.\n\nBUT, do these improvements translate to better chest X-ray diagnosis models?\n\nOur new short paper shows that they DO: <LINK>\n\n@saahil9jain @AkshaySmit @stanfordnlp \n\n1/5 <LINK>', 'We first compare the performance of the #CheXpert, #CheXbert, and #VisualCheXbert models on the task of extracting high quality chest X-ray image labels from radiology reports.\n\nWe show that the VisualCheXbert labeler significantly outperforms other labeling approaches.\n\n2/5 https://t.co/qBSAguu0fd', 'After training DL models for chest X-ray interpretation using various report labelers...\n\nWe show that a model trained on higher quality labels generated by VisualCheXbert outperforms models trained on other labels in detecting medical conditions from chest X-rays.\n\n3/5 https://t.co/YfYVgzjcQV', 'Given the statistically significant overall improvement achieved by a model using a higher quality report labeler, our work suggests that recent improvements in radiology report labeling can translate to improvements in chest X-ray diagnosis.\n\n4/5', 'After spending more than a year improving the quality of radiology report labelers, we are excited to see how better labeling can improve the performance of chest X-ray diagnosis models!\n\nRead more about our work here: https://t.co/36EzKSPrCw\n\n5/5']",https://arxiv.org/abs/2104.00793,"Although deep learning models for chest X-ray interpretation are commonly trained on labels generated by automatic radiology report labelers, the impact of improvements in report labeling on the performance of chest X-ray classification models has not been systematically investigated. We first compare the CheXpert, CheXbert, and VisualCheXbert labelers on the task of extracting accurate chest X-ray image labels from radiology reports, reporting that the VisualCheXbert labeler outperforms the CheXpert and CheXbert labelers. Next, after training image classification models using labels generated from the different radiology report labelers on one of the largest datasets of chest X-rays, we show that an image classification model trained on labels from the VisualCheXbert labeler outperforms image classification models trained on labels from the CheXpert and CheXbert labelers. Our work suggests that recent improvements in radiology report labeling can translate to the development of higher performing chest X-ray classification models. ","Effect of Radiology Report Labeler Quality on Deep Learning Models for
  Chest X-Ray Interpretation"
115,1380425758426988546,453048459,andres ferraro,"['New paper out! ""Enriched Music Representations with Multiple Cross-modal Contrastive Learning"" in IEEE SPL w/ Favory, @drosos_kostas, Kim and @di_bogdanov \n\npaper: <LINK>\ncode: <LINK>\ndata: <LINK>']",https://arxiv.org/abs/2104.00437,"Modeling various aspects that make a music piece unique is a challenging task, requiring the combination of multiple sources of information. Deep learning is commonly used to obtain representations using various sources of information, such as the audio, interactions between users and songs, or associated genre metadata. Recently, contrastive learning has led to representations that generalize better compared to traditional supervised methods. In this paper, we present a novel approach that combines multiple types of information related to music using cross-modal contrastive learning, allowing us to learn an audio feature from heterogeneous data simultaneously. We align the latent representations obtained from playlists-track interactions, genre metadata, and the tracks' audio, by maximizing the agreement between these modality representations using a contrastive loss. We evaluate our approach in three tasks, namely, genre classification, playlist continuation and automatic tagging. We compare the performances with a baseline audio-based CNN trained to predict these modalities. We also study the importance of including multiple sources of information when training our embedding model. The results suggest that the proposed method outperforms the baseline in all the three downstream tasks and achieves comparable performance to the state-of-the-art. ","Enriched Music Representations with Multiple Cross-modal Contrastive
  Learning"
116,1380321084453568517,2756561793,Zili Shen,"['Pretty galaxy picture ahead! Get ready for the deepest HST image of the mysterious NGC1052-DF2, combined from 40 orbits of data. This is part of our new paper measuring the distance to this galaxy with the tip of the red giant branch method: <LINK> <LINK>', ""We use this new data to construct a color-magnitude diagram, and you can see the red giant branch by eye! (In case you don't know what I'm talking about, it's in between the green dashed lines.) The tip of the red giant branch is located around 27.5 magnitude. https://t.co/mBJwSmFiEQ"", 'From our model that accounts for contamination and photometric errors, the best-fit TRGB magnitude is 27.67 mag and the distance is 22.1 Mpc. https://t.co/F79KIQBBeg', 'In addition, we also found that we could measure the relative distance to NGC1052-DF4, which has been analyzed previously by @DanieliShany. We found that these two galaxies are 2.1 Mpc apart, so they cannot both be in close proximity to NGC1052, the central galaxy in the group.', 'These two galaxies remain a puzzle to be solved, and new twists keep emerging in this story! Exciting times.']",https://arxiv.org/abs/2104.03319,"The large and diffuse galaxies NGC1052-DF2 and NGC1052-DF4 have been found to have very low dark matter content and a population of luminous globular clusters. Accurate distance measurements are key to interpreting these observations. Recently, the distance to NGC1052-DF4 was found to be $20.0\pm 1.6$ Mpc by identifying the tip of the red giant branch (TRGB) in 12 orbits of Hubble Space Telescope (HST) Advanced Camera for Surveys (ACS) imaging. Here we present 40 orbits of HST ACS data for NGC1052-DF2 and use these data to measure its TRGB. The TRGB is readily apparent in the color-magnitude diagram. Using a forward model that incorporates photometric uncertainties, we find a TRGB magnitude of $m_{\rm F814W, TRGB} = 27.67 \pm 0.10$ mag. The inferred distance is $D_{\rm TRGB} = 22.1 \pm 1.2$ Mpc, consistent with the previous surface brightness fluctuation distances to the bright elliptical galaxy NGC1052. The new HST distance rules out the idea that some of NGC1052-DF2's unusual properties can be explained if it were at $\sim 13$ Mpc; instead, it implies that the galaxy's globular clusters are even more luminous than had been derived using the previous distance of 20 Mpc. The distance from NGC1052-DF2 to NGC1052-DF4 is well-determined at $2.1\pm 0.5$ Mpc, significantly larger than the virial diameter of NGC1052. We discuss the implications for formation scenarios of the galaxies and for the external field effect, which has been invoked to explain the intrinsic dynamics of these objects in the context of modified Newtonian dynamics. ","A Tip of the Red Giant Branch Distance of $22.1 \pm 1.2$ Mpc to the Dark
  Matter Deficient Galaxy NGC1052-DF2 from 40 Orbits of Hubble Space Telescope
  Imaging"
117,1380312235906596871,2361934191,Louigi Addario-Berry,"[""New paper just out w/ my colleague Jessica Lin &amp; our postdoc Erin Beckman. Title: Cooperative motion in one dimension.\n\nHere's a thread summarizing our results and techniques (we use numerical PDE results to prove limit theorems for random processes).\n<LINK>\n1/17"", ""Here's the model. Start with an any integer-valued random variable X‚ÇÄ. \nFor each n‚â•0, let X‚Çô(1),...,X‚Çô(m) be independent copies of X‚Çô. \nIf X‚Çô= X‚Çô(1)=...= X‚Çô(m) then set X‚Çô‚Çä‚ÇÅ=X‚Çô+B where B is a Bernoulli(p) random variable. Otherwise, set X‚Çô‚Çä‚ÇÅ=X‚Çô.\n\n2/17"", 'The picture to have in mind is that X‚Çô is ""trying"" to make a Bernoulli-distributed step, but needs the help of m friends in order to do so. The friends are at locations X‚Çô(1),...,X‚Çô(m), and a step is possibly iff they are all in the same location as X‚Çô.\n\n3/17', 'We prove that X‚Çô is asymptotically Beta((m+1)/m,1)-distributed, after scaling by a constant times n^{-1/(m+1)}.\n\nThis is a surprising behaviour; the process shows subdiffusive behaviour, random to first order, but with compact support on that order.\n\n4/17', 'The method of proof is the most interesting part of the paper to me. The def. of X‚Çô yields a recurrence for the values p‚Åø‚Çñ:=Prob(X‚Çô=k):\n\np‚Åø‚Å∫¬π‚Çñ=p‚Åø‚Çñ - q((p‚Åø‚Çñ)·µê‚Å∫¬π-(p‚Åø‚Çñ‚Çã‚ÇÅ)·µê‚Å∫¬π).\n\nTo have X‚Çô‚Çä‚ÇÅ=k, either X‚Çô=k (&amp; no move occurs) or X‚Çô=k=1 (&amp; a move occurs). \n\n5/17', 'At the level of the cumulative distribution function of X‚Çô this becomes \n\nF‚Åø‚Å∫¬π‚Çñ = F‚Åø‚Çñ - q(F‚Åø‚Çñ-F‚Åø‚Çñ‚Çã‚ÇÅ)·µê‚Å∫¬π. \n\nViewing n as time and k as space, this is a discretization of (finite difference scheme for) the nonlinear Hamilton-Jacobi equation\n\nu‚Çú+q u‚Çì·µê‚Å∫¬π = 0. \n\n6/17', ""There are various solution theories for such PDEs.  The theory of viscosity solutions for them, developed by Crandall &amp; Lions, is one useful notion of weak sol'n, w/ the advantage that it includes convergence results for finite difference schemes for the PDEs, like we have!\n\n7/17"", 'A disadvantage of the Crandall-Lions theory is that it requires Lipschitz continuous initial data; our initial data, which is a rescaling of a fixed cumulative distribution function, basically corresponds to a Heaviside function.\n\n8/17', 'To deal with this, we need to use both the Crandall-Lions theory, and their finite approximation results, as well as another solution theory, developed by Barron and Jensen, which applies to discontinuous initial data. (The two theories coincide when they are both defined.)\n\n9/17', 'The PDE results allow us to prove convergence theorems for suitably ""mollified"" initial conditions. We then use stochastic coupling arguments to extend from the smoothed initial conditions to general initial data. \n\n10/17', ""This is the second paper I've written which uses numerical PDE results to prove distributional convergence for a random process; it generalizes the results of the first paper (which essentially handle the case m=1 of the cooperative motion).\n\n11/17"", 'I\'m excited by this approach &amp; I think it has the potential to be used much more generally. E.g. Sourav Chatterjee (https://t.co/UWKGoeb6YR) recently announced that he &amp; Takis Souganidis use Crandall-Lions in a similar way to find the scaling limit of  ""deterministic KPZ"".\n\n12/17', 'It also raises the prospect of ""sending information the other way"": proving limit theorems for probabilistic evolutions and thereby deducing stability or convergence for numerical approximation schemes that correspond to those evolutions. \n\n13/17', ""I don't think this is a pipe dream; probabilistically, it is quite clear that the results we prove should hold in much more generality than we prove them (e.g. for more general step sizes). From the numerical PDE perspective, that would be a new convergence result!\n\n14/17"", 'There are also many natural questions about the probabilistic model itself. We prove that if the Bernoulli B is replaced by a positive integer multiple of B, then the limit is not necessarily Beta-distributed; there are lattice-effects that can persist in the limit.\n\n15/17', 'When the step size is non-negative and not too heavy-tailed, and there are no lattice effects, we conjecture that the Beta((m+1)/m,1) scaling limit always holds. \n\nWhen the step size can be negative, we are not sure what happens, but it should be interesting!\n\n16/17', 'And, as suggested by the title of the paper, we only study the one-dimensional case. It would be very natural to try to figure out what happens for d &gt; 1!. \n\nFor more details, and more open questions, take a look at the paper :-).\n17/17']",https://arxiv.org/abs/2104.03369,"We prove distributional convergence for a family of random processes on $\mathbb{Z}$, which we call asymmetric cooperative motions. The model generalizes the ""totally asymmetric hipster random walk"" introduced in [Addario-Berry, Cairns, Devroye, Kerriou and Mitchell, 2020]. We present a novel approach based on connecting a temporal recurrence relation satisfied by the cumulative distribution functions of the process to the theory of finite difference schemes for Hamilton-Jacobi equations [Crandall and Lyons, 1984]. We also point out some surprising lattice effects that can persist in the distributional limit, and propose several generalizations and directions for future research. ",Asymmetric cooperative motion in one dimension
118,1380267902109016065,55027942,Michael Hammer,"['I HAVE A NEW PAPER!!!!! :-)\nNow you can find out once and for all which planets trigger longer-lived vortices: low-mass or high-mass? <LINK> @linminkai \n\nThe answer is...\n(1/n) <LINK>', 'Past studies have just assumed that high-mass planets should create stronger vortices.\nAnd that makes sense, right? Sharp gap edges ===&gt;&gt;&gt; Stronger vortices.\nAnd higher-mass planets create sharper gap edges!\n\n(2/n) https://t.co/mhv0YK8XgS', 'But, what we find is that, for a given disc aspect ratio (H/R), planets generate initial vortices with roughly the same lifetimes regardless of planet mass. The main difference for the &lt;&lt;&lt; total lifetime &gt;&gt;&gt; is that the vortex can re-form in some cases!\n\n(3/n) https://t.co/4a0gSSmGVt', 'When can the vortex re-form? We find that only happens if the gap edge remains close to the planet. Since lower-mass planets open narrower gaps, that happens much more naturally when the planet is lower-mass.\n\n(4/n) https://t.co/1364Xvb52N', 'That leads to our first key result:\nBecause only low-mass planets can re-trigger vortices, they should be accompanied by vortices for much longer!\n\n(5/n) https://t.co/08eNq79r0T', 'But wait, you might ask, what do you mean by ‚Äúhigh-mass‚Äù and ‚Äúlow-mass‚Äù? What determines the cutoff between the two regimes?\nThe answer is the gap-opening mass: M_planet ~ M_star * (H/R)^3\n\n(6/n) https://t.co/3Pq5cg0Dvm', 'That leads to our second key result:\nPlanets below the gap-opening mass trigger longer-lived vortices, or more specifically, planets in thicker discs with larger aspect ratios (H/R) trigger longer-lived vortices.\n\nBut the reason is totally different!\n(7/n)', 'With large values of H/R, shocks from the planet‚Äôs spiral waves are much weaker. As a result, the planet can‚Äôt disrupt the vortex, and the initial vortex survives for many thousands of orbits without even needing to re-form!\n\n(8/n) https://t.co/PLFuz6lXK5', 'In these H/R &gt;= 0.08 cases, we find a new scenario where the initial vortex never stops growing and ultimately migrates outwards, allowing a second vortex to form in its place.\nThus, one planet can create two vortices, which could potentially explain discs like MWC 758!\n\n(9/n) https://t.co/12I9MqAP8q', 'We also explored how the supply of dust affects the appearance of the vortex.\n\nWhereas most previous studies have supplied the vortex with unlimited dust, we tried cutting off the dust supply because asymmetries are usually the outermost feature in a disc.\n\n(10/n)', 'With a limited supply of dust, we found that elongated vortices don‚Äôt necessarily appear elongated!\n\nIt‚Äôs also possible for the dust in the vortex to split into two peaks, which resembles its appearance with dust feedback.\nThis pattern may have been seen in HD 142527.\n\n(11/n) https://t.co/xRjIlEs0x7', 'We note that all of these results are mainly for vortices in very low viscosity discs (\\alpha ~ 10^-5).\nHigher disc viscosities (\\alpha &gt; 10^-4) can still kill vortices rather quickly regardless of planet mass or disc H/R.\n\n(12/n)', 'With our finding that even low-mass planets can generate very long-lived vortices, you might be wondering: Do we see a lot of crescent-shaped vortices in real ALMA observations?\n\nThe answer is actually: No!\n\n(13/n)', 'With the long lifetimes in our study, we would expect to see at least 2 or 3 vortices in a sample of six discs in Taurus from Long+ 2018, 2019.\n\nBut none of those discs have asymmetries!\n\n(14/n) https://t.co/1UwCWIawAU', 'The general dearth of observed asymmetries seems to suggest that our low-viscosity simulations do not match with observations.\nThis adds support for any sort of mechanism that shortens vortex lifetimes, such as something even as simple as having a higher viscosity.\n\n(15/n)', 'Overall, I really liked working on this because it was my first paper where I came up with the idea for the project!\n\n(16/n)', 'Originally, it was going to include planet migration too, but the case with a fixed planet was complicated enough to fill a whole paper.\n\nNow that we worked out the simple case, we can start adding more physics effects like migration, feedback, etc. for follow-up paper(s).\n(17/n)', 'Also overall, you can find a write-up of the paper on my website, complete with the one thing none of us can live without: movies!\n\nhttps://t.co/npaLSf0grq\n(18/n)\nn = 18']",https://arxiv.org/abs/2104.02782,"Recent ALMA observations have found many protoplanetary discs with rings that can be explained by gap-opening planets less massive than Jupiter. Meanwhile, recent studies have suggested that protoplanetary discs should have low levels of turbulence. Past computational work on low-viscosity discs has hinted that these two developments might not be self-consistent because even low-mass planets can be accompanied by vortices instead of conventional double rings. We investigate this potential discrepancy by conducting hydrodynamic simulations of growing planetary cores in discs with various aspect ratios ($H/r=0.04$, 0.06, 0.08) and viscosities ($1.5 \times 10^{-5} \lesssim \alpha \lesssim 3 \times 10^{-4}$), having these cores accrete their gas mass directly from the disc. With $\alpha < 10^{-4}$, we find that sub-Saturn-mass planets in discs with $H/r \le 0.06$ are more likely to be accompanied by dust asymmetries compared to Jupiter-mass planets because they can trigger several generations of vortices in succession. We also find that vortices with $H/r = 0.08$ survive $>6000$ planet orbits regardless of the planet mass or disc mass because they are less affected by the planet's spiral waves. We connect our results to observations and find that the outward migration of vortices with $H/r \ge 0.08$ may be able to explain the cavity in Oph IRS 48 or the two clumps in MWC 758. Lastly, we show that the lack of observed asymmetries in the disc population in Taurus is unexpected given the long asymmetry lifetimes in our low viscosity simulations ($\alpha \sim 2 \times 10^{-5}$), a discrepancy we suggest is due to these discs having higher viscosities. ",Which planets trigger longer-lived vortices: low-mass or high-mass?
119,1380241443998949376,918263452698841088,KemperLab,"['A new preprint from our group!  Efekan K√∂kc√º\'s first paper, ""Fixed Depth Hamiltonian Simulation via Cartan Decomposition""\n\n<LINK>']",https://arxiv.org/abs/2104.00728,"Simulating spin systems on classical computers is challenging for large systems due to the significant memory requirements. This makes Hamiltonian simulation by quantum computers a promising option due to the direct representation of quantum states in terms of its qubits. Standard algorithms for time evolution on quantum computers require circuits whose depth grows with time. We present a new algorithm, based on Cartan decomposition of the Lie algebra generated by the Hamiltonian, that generates a circuit with time complexity O(1) for ordered and disordered models of n spins. We highlight our algorithm for special classes of models where an O(n^2)-gate circuit emerges. Compared to product formulas with significantly larger gate counts, our algorithm drastically improves simulation precision. Our algebraic technique sheds light on quantum algorithms and will reduce gate requirements for near term simulation. ",Fixed Depth Hamiltonian Simulation via Cartan Decomposition
120,1380218022825185280,799279982233014273,Chen Wang,['Glad to share our new ICRA 2021 paper. It is one of the first Semantic RGB-D SLAM systems that run in real-time on a low-power embedded platform and provide high localization accuracy in dynamic environments. <LINK> <LINK>'],https://arxiv.org/abs/2104.01316,"Most of the existing visual SLAM methods heavily rely on a static world assumption and easily fail in dynamic environments. Some recent works eliminate the influence of dynamic objects by introducing deep learning-based semantic information to SLAM systems. However such methods suffer from high computational cost and cannot handle unknown objects. In this paper, we propose a real-time semantic RGB-D SLAM system for dynamic environments that is capable of detecting both known and unknown moving objects. To reduce the computational cost, we only perform semantic segmentation on keyframes to remove known dynamic objects, and maintain a static map for robust camera tracking. Furthermore, we propose an efficient geometry module to detect unknown moving objects by clustering the depth image into a few regions and identifying the dynamic regions via their reprojection errors. The proposed method is evaluated on public datasets and real-world conditions. To the best of our knowledge, it is one of the first semantic RGB-D SLAM systems that run in real-time on a low-power embedded platform and provide high localization accuracy in dynamic environments. ",Towards Real-time Semantic RGB-D SLAM in Dynamic Environments
121,1380192406427987968,998635839109427201,Marika McCarthy,"['New paper on the arxiv, and coming soon to an ApJ near you!  McCarthy, Longcope and Malanushenko, ""Multi-spacecraft observations of coronal loops to verify a force-free field reconstruction and infer loop cross sections"" <LINK>']",https://arxiv.org/abs/2104.02722,"Active region EUV loops are believed to trace a subset of magnetic field lines through the corona. Malanushenko et al. (2009) proposed a method, using loop images and line-of-sight photospheric magnetograms, to infer the three-dimensional shape and field strength along each loop. McCarthy et al. (2019) used this novel method to compute the total magnetic flux interconnecting a pair of active regions observed by SDO/AIA. They adopted the common assumption that each loop had a circular cross section. The accuracy of inferred shape and circularity of cross sections can both be tested using observations of the same loops from additional vantage points as provided by STEREO/EUVI. Here, we use multiple viewing angles to confirm the three-dimensional structure of loops. Of 151 viable cases, 105 (69.5%) matched some form of visible coronal structure when viewed approximately in quadrature. A loop with a circular cross-section should appear of a similar width in different perspectives. In contradiction to this, we find a puzzling lack of correlation between loop diameters seen from different perspectives, even an anti-correlation in some cases. Features identified as monolithic loops in AIA may, in fact, be more complex density enhancements. The 30.5% of reconstructions from AIA which did not match any feature in EUVI might be such enhancements. Others may be genuine loop structures, but with elliptical cross sections. We observe an anti-correlation between diameter and brightness, lending support to the latter hypothesis. Of 13 suitable for width analysis, four loops are consistent with non-circular cross sections, where we find anti-correlation in both comparisons. ","Multi-spacecraft observations of coronal loops to verify a force-free
  field reconstruction and infer loop cross sections"
122,1380161640348463105,69202541,Jonathan Le Roux,"['Niko Moritz, Takaaki Hori, and I have a new paper out, ""Capturing Multi-Resolution Context by Dilated Self-Attention"", on reducing complexity of self-attention layers. ASR results on par w/ full-seq. SA at fraction of cost. To be presented at #ICASSP2021.\n<LINK>']",https://arxiv.org/abs/2104.02858,"Self-attention has become an important and widely used neural network component that helped to establish new state-of-the-art results for various applications, such as machine translation and automatic speech recognition (ASR). However, the computational complexity of self-attention grows quadratically with the input sequence length. This can be particularly problematic for applications such as ASR, where an input sequence generated from an utterance can be relatively long. In this work, we propose a combination of restricted self-attention and a dilation mechanism, which we refer to as dilated self-attention. The restricted self-attention allows attention to neighboring frames of the query at a high resolution, and the dilation mechanism summarizes distant information to allow attending to it with a lower resolution. Different methods for summarizing distant frames are studied, such as subsampling, mean-pooling, and attention-based pooling. ASR results demonstrate substantial improvements compared to restricted self-attention alone, achieving similar results compared to full-sequence based self-attention with a fraction of the computational costs. ",Capturing Multi-Resolution Context by Dilated Self-Attention
123,1380146225652015107,930764003277643777,Matias Quiroz,"['New paper on arXiv <LINK>. Subsampling MCMC now gives decent speed ups (approx 100 fold compared to full data MCMC) on highly complex multivariate time series models time series with fractional tempered differencing to model semi-long memory', 'With @matvil, @robertjk59', 'And @SalomoneRob']",https://arxiv.org/abs/2104.02134,"Spectral subsampling MCMC was recently proposed to speed up Markov chain Monte Carlo (MCMC) for long stationary univariate time series by subsampling periodogram observations in the frequency domain. This article extends the approach to stationary multivariate time series. It also proposes a multivariate generalisation of the autoregressive tempered fractionally differentiated moving average model (ARTFIMA) and establishes some of its properties. The new model is shown to provide a better fit compared to multivariate autoregressive moving average models for three real world examples. We demonstrate that spectral subsampling may provide up to two orders of magnitude faster estimation, while retaining MCMC sampling efficiency and accuracy, compared to spectral methods using the full dataset. ",Spectral Subsampling MCMC for Stationary Multivariate Time Series
124,1380106474475696129,886584833237028864,Joseph Bowles,['Our new paper on how to use neural networks to modify quantum circuit cost landscapes and avoid local minima is on the arXiv today!\n\n<LINK>\n<LINK> <LINK>'],https://arxiv.org/abs/2104.02955,"Variational Quantum Algorithms have emerged as a leading paradigm for near-term quantum computation. In such algorithms, a parameterized quantum circuit is controlled via a classical optimization method that seeks to minimize a problem-dependent cost function. Although such algorithms are powerful in principle, the non-convexity of the associated cost landscapes and the prevalence of local minima means that local optimization methods such as gradient descent typically fail to reach good solutions. In this work we suggest a method to improve gradient-based approaches to variational quantum circuit optimization, which involves coupling the output of the quantum circuit to a classical neural network. The effect of this neural network is to peturb the cost landscape as a function of its parameters, so that local minima can be escaped or avoided via a modification to the cost landscape itself. We present two algorithms within this framework and numerically benchmark them on small instances of the Max-Cut optimization problem. We show that the method is able to reach deeper minima and lower cost values than standard gradient descent based approaches. Moreover, our algorithms require essentially the same number of quantum circuit evaluations per optimization step as the standard approach since, unlike the gradient with respect to the circuit, the neural network updates can be estimated in parallel via the backpropagation method. More generally, our approach suggests that relaxing the cost landscape is a fruitful path to improving near-term quantum computing algorithms. ","Avoiding local minima in Variational Quantum Algorithms with Neural
  Networks"
125,1380093483621421061,898575285121159168,Alexandre Santerne üá™üá∫,"['New paper on #HIP41378 reporting the detection from the ground of the 19h-long transit of planet f by @NextGenTransits. Interestingly, the planet exhibits large #TTVs. The next transit will be observed by @NASAHubble this May. üî≠üõ∞Ô∏èü™ê\n<LINK> <LINK>', ""@V_Parmentier @NextGenTransits @NASAHubble Unfortunately, for visibility reasons @ESA_CHEOPS won't be able to observe a transit of #HIP41378 f üòû""]",https://arxiv.org/abs/2104.03159,"HIP 41378 f is a temperate $9.2\pm0.1 R_{\oplus}$ planet with period of 542.08 days and an extremely low density of $0.09\pm0.02$ g cm$^{-3}$. It transits the bright star HIP 41378 (V=8.93), making it an exciting target for atmospheric characterization including transmission spectroscopy. HIP 41378 was monitored photometrically between the dates of 2019 November 19 and November 28. We detected a transit of HIP 41378 f with NGTS, just the third transit ever detected for this planet, which confirms the orbital period. This is also the first ground-based detection of a transit of HIP 41378 f. Additional ground-based photometry was also obtained and used to constrain the time of the transit. The transit was measured to occur 1.50 hours earlier than predicted. We use an analytic transit timing variation (TTV) model to show the observed TTV can be explained by interactions between HIP 41378 e and HIP 41378 f. Using our TTV model, we predict the epochs of future transits of HIP 41378 f, with derived transit centres of T$_{C,4} = 2459355.087^{+0.031}_{-0.022}$ (May 2021) and T$_{C,5} = 2459897.078^{+0.114}_{-0.060}$ (Nov 2022). ","A transit timing variation observed for the long-period extremely low
  density exoplanet HIP 41378f"
126,1380057597387083777,1032283657015316480,Josh Dorrington,"['What do donuts and Atlantic jet regimes have in common? They both exhibit nontrivial topology!\n\nNew paper from my co-authors and me on the arxiv, arguing for a topological definition of regime dynamics using persistent homology\n\n<LINK>', '@KristianStromm2 ,@kneppkatt and the elusive Mat\n Chantry']",https://arxiv.org/abs/2104.03196,"It has long been suggested that the mid-latitude atmospheric circulation possesses what has come to be known as `weather regimes', loosely categorised as regions of phase space with above-average density and/or extended persistence. Their existence and behaviour has been extensively studied in meteorology and climate science, due to their potential for drastically simplifying the complex and chaotic mid-latitude dynamics. Several well-known, simple non-linear dynamical systems have been used as toy-models of the atmosphere in order to understand and exemplify such regime behaviour. Nevertheless, no agreed-upon and clear-cut definition of a `regime' exists in the literature. We argue here for an approach which equates the existence of regimes in a dynamical system with the existence of non-trivial topological structure of the system's attractor. We show using persistent homology, an algorithmic tool in topological data analysis, that this approach is computationally tractable, practically informative, and identifies the relevant regime structure across a range of examples. ",A topological perspective on weather regimes
127,1380045688734638080,147554162,Galaxy Map,['New paper on nearby solar twins visible from the northern hemisphere. <LINK> <LINK>'],https://arxiv.org/abs/2104.02806,"Solar twins are key in different areas of astrophysics, however only just over a hundred were identified and well-studied in the last two decades. In this work, we take advantage of the very precise \textit{Gaia} (DR2/EDR3), Tycho and 2MASS photometric systems to create the Inti survey of new solar twins in the Northern Hemisphere. The spectra of our targets were initially obtained with spectrographs of moderate resolution (ARCES and Goodman spectrographs with $R$ = 31500 and 11930, respectively) to find the best solar twin candidates and then observed at McDonald Observatory with higher resolving power (TS23, $R$ = 60000) and signal-to-noise ratio (SNR $\sim$ 300-500). The stellar parameters were estimated through the differential spectroscopic equilibrium relative to the Sun, which allow us to achieve a high internal precision ($\sigma(T_{\rm{eff}})$ = 15 K, $\sigma(\log g)$ = 0.03 dex, $\sigma$([Fe/H]) = 0.01 dex, and $\sigma(v_{t})$ = 0.03 km s$^{-1}$). We propose a new class of stars with evolution similar to the Sun: \textit{solar proxy}, which is useful to perform studies related to the evolution of the Sun, such as its rotational and magnetic evolution. Its definition is based on metallicity ($-$0.15 dex $\leq$ [Fe/H] $\leq$ +0.15 dex) and mass (0.95 M$_{\odot}$ $\leq$ M $\leq$ 1.05 M$_{\odot}$) constraints, thus assuring that the star follows a similar evolutionary path as the Sun along the main sequence. Based on this new definition, we report 70 newly identified solar proxies, 46 solar analogs and 13 solar-type stars. In addition, we identified 9 \textit{close solar twins} whose stellar parameters are the most similar to those of the Sun. ",Searching for new solar twins: The Inti survey for the Northern Sky
128,1379996091287011329,1067925193249759233,Mohammad Norouzi,['New paper led by @wchan212: \n\nSpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network\n<LINK> <LINK>'],http://arxiv.org/abs/2104.02133,"We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\% WER on AMI-IHM, 4.7\% WER on Switchboard, 8.3\% WER on CallHome, and 1.3\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\% WER without a language model, which compares to 38.6\% WER to a strong HMM baseline with a language model. ","SpeechStew: Simply Mix All Available Speech Recognition Data to Train
  One Large Neural Network"
129,1379961984406278149,916864821122830336,MinKai Lin,"['New paper led by @theAstroHammer taking an in-depth look into growing planets, vortices, and observational signatures. @epo_asiaa  <LINK> <LINK>']",https://arxiv.org/abs/2104.02782,"Recent ALMA observations have found many protoplanetary discs with rings that can be explained by gap-opening planets less massive than Jupiter. Meanwhile, recent studies have suggested that protoplanetary discs should have low levels of turbulence. Past computational work on low-viscosity discs has hinted that these two developments might not be self-consistent because even low-mass planets can be accompanied by vortices instead of conventional double rings. We investigate this potential discrepancy by conducting hydrodynamic simulations of growing planetary cores in discs with various aspect ratios ($H/r=0.04$, 0.06, 0.08) and viscosities ($1.5 \times 10^{-5} \lesssim \alpha \lesssim 3 \times 10^{-4}$), having these cores accrete their gas mass directly from the disc. With $\alpha < 10^{-4}$, we find that sub-Saturn-mass planets in discs with $H/r \le 0.06$ are more likely to be accompanied by dust asymmetries compared to Jupiter-mass planets because they can trigger several generations of vortices in succession. We also find that vortices with $H/r = 0.08$ survive $>6000$ planet orbits regardless of the planet mass or disc mass because they are less affected by the planet's spiral waves. We connect our results to observations and find that the outward migration of vortices with $H/r \ge 0.08$ may be able to explain the cavity in Oph IRS 48 or the two clumps in MWC 758. Lastly, we show that the lack of observed asymmetries in the disc population in Taurus is unexpected given the long asymmetry lifetimes in our low viscosity simulations ($\alpha \sim 2 \times 10^{-5}$), a discrepancy we suggest is due to these discs having higher viscosities. ",Which planets trigger longer-lived vortices: low-mass or high-mass?
130,1379796042401603590,338526004,Sam Bowman,"['üö® New cranky position paper alert! üö®\n(#naacl2021, mini-thread, <LINK>) <LINK>', ""George Dahl helped us plan out what turned into the MultiNLI dataset, and in the almost five years since that project started, I've had an ongoing conversation with him about what it means for a benchmark dataset to be effective."", ""It's a tricky issue, and these discussions have influenced my thinking quite a bit, so with George's help, I decided to write up the position that we've roughly converged on."", 'On the cranky side, we argue that adversarial filtering is good at making datasets that look hard by the numbers, but that these datasets can drift arbitrarily far away from measuring the thing that we actually want to measure.', ""On the positive side, we lay out four difficult but relatively familiar challenges that we'll need to face in order to build benchmarks that will allow us to responsibly measure further progress on language understanding, ..."", '... and sketch some ideas that we think will allow us to make progress on all four challenges. https://t.co/gWQ0GGUoG5', 'Thanks to the audiences that sat through half-baked versions of these ideas at talks or commented on our drafts!', ""And, of course, the goals in this paper are goals that I'm personally interested in working toward. A decent chunk of my attention these days is going toward figuring out how to build hard, fair test sets. If that's something you're thinking about as well, I'd be happy to talk."", ""@ChrisGPotts Yep! And ANLI, for example, does seem to be a pretty good/diverse dataset for NLI. I'm not sure that's a full rebuttal, though."", ""@ChrisGPotts We're arguing that the adversarial filtering approach doesn't absolve you of the need to worry about coverage, and that as systems get better, it'll start to get in the way of good coverage. (Plus, adversarial filtering plus *static* leaderboards creates broken incentives.)"", '@evanmiltenburg Oof. Thanks!', ""@ChrisGPotts MultiNLI: I'm not sure what to say about the matched/mismatched contrast here. Generally, though, I think it worked to spur progress while systems were still bad at the skills that it focused on, ..."", ""@ChrisGPotts ... but that we've hit the limits of how well we can evaluate NLI with the kind of messy/artifacty/repetitive data that you get out of crowdsourcing with standard incentives and quality control practices. Improving those practices seems like the obvious next step to me there."", ""@ChrisGPotts I agree that our choice to remove tasks from SuperGLUE that BERT already solved is inconsistent with the argument that we're making now, and I wouldn't do things quite that way if I were building another similar benchmark."", ""@ChrisGPotts Second sentence: Yep, but it seems to be getting much harder to build these 'good enough for now' benchmarks in a fair way. We're trying to figure out what it'd look like to build something that we can trust to be more definitive/lasting."", ""@ChrisGPotts MNLI: Mismatched isn't adversarial in the strong adversarial-model sense that we're most directly arguing against..."", '@ChrisGPotts ...but it would be impacted by our (maybe counterintuitive) argument that OOD test sets become less valuable if we can really get coverage and quality right.', '@srchvrs We didn\'t emphasize this, but I think ""hard enough"" isn\'t something we should be optimizing for. If we think that a benchmark is big, broad-coverage, and faithful to a task, and it turns out that it\'s not hard enough, that means we\'re done!', ""@srchvrs In practice it'll be hard to fully guarantee that we have perfect coverage, but I think we can get close enough to be able to take 'human parity' results more seriously than we do now, ..."", '@srchvrs ... such that if we get such a result, instead of just looking for harder examples along the same lines, the obvious thing to do is to actually set our sights higher by choosing a more ambitious task setting, adding constraints (like few-shot), ...', '@srchvrs ... or expanding the languages/varieties we test on.', '@brendan642 @rachelrudinger Exactly.', '@brendan642 Yep! The huge gap btw. the MNLI test set and those numbers is really clear evidence that MNLI has issues with validity/coverage.', ""@ChrisGPotts We're pretty attached to the goal of having one (or a few) metrics for a task that are clearly the right metric and safe to hill-climb on. It's the same thing we were aiming for with *GLUE."", ""@ChrisGPotts It's nearly impossible to get this right perfectly  (Goodhart's law, as you say), and the upsides aren't that obvious for researchers in the field, but I think that aiming for this goal can really help us benefit from work by people from neighboring fields."", ""@ChrisGPotts You can fault us for using pretty broad terms in the title when we really are focused on a fairly focused style of evaluation/experiment in the main body, but I'm really not convinced that static adversarially-filtered data collection works in the setting we're describing."", ""@ChrisGPotts In particular, the use of an adversary model that's in the class of models you want to compare seems really dangerous. You can get almost arbitrary flips to leaderboard rankings that way (vs. a comparable test set with a different adversary), ..."", ""@ChrisGPotts ... and you can wind up incentivizing models that are different from the adversary without being broadly better. I don't see a way to fully eliminate that risk, and the XLNet results in the ANLI paper suggest that it's already here to some extent."", '@ChrisGPotts Agreed. We come at issues with ambiguity from a slightly different angle, but I think this is a helpful point.', ""@srchvrs I guess I'd argue that our current classification datasets are mostly pretty artifact-ey, so we're probably not done! But otherwise agree."", ""@brendan642 No, sorry. You'll have to scrape them. (I've done it in past years, but not recently.) Nobody currently in my group has the app development expertise/bandwidth to make significant changes to our backend. üòï"", ""@srchvrs @emelin_denis Plus, classification is still an unsolved problem with real applications. I think NLG is worth doing, and NLG evaluation is an important research problem, but I don't see an argument for it to completely supplant straight NLU."", ""@ChrisGPotts Agreed‚ÄîI'm calling this a 'cranky' paper in part because we're arguing that we should try to get better coverage of those tail phenomena directly, but we're not giving a detailed agenda for how to do so."", ""@ChrisGPotts That seems right. I'm making the bet that the weird effects from *static* adversarially-filtered benchmarks will distort incentives more than Goodhart's law will."", ""@brendan642 We wrote a lot about this, but didn't agree on that much, so wound up pruning it down to this: https://t.co/KHMqnFiwov"", '@brendan642 I definitely agree that most good research questions require much more nuanced evaluation work than just a benchmark, but I think there are loads of cases where benchmarks make doing a credible evaluation much easier.', ""@brendan642 I think this is clearest for papers where 'good task performance' is necessary for some argument but not the goal. Consider BERTology, worst-case robustness, or efficiency work‚Äîyou generally want to show that you're working with a model that's basically reasonable/useful."", ""@evanmiltenburg @davidschlangen These do look quite relevant, thanks! We're very close to the camera-ready deadline, but I'll see if I can make time to take a look."", '@miserlis_ @brendan642 @EntilZhaPR @ihsgnef I second the endorsement of this talk.', ""@rtk254 @nelsonfliu I agree that they're useful for prototyping and debugging/analysis, but I think those goals are orthogonal to what we're talking about. If you have a task for which artificial data can form a good target benchmark, then you're probably not doing *natural* lanugage processing."", ""@rtk254 @nelsonfliu The Liu result there is really interesting, but I'm not sure I see an argument for making artificial datasets the sole/primary standard evaluations that people try to hill-climb on for some NLP task.""]",https://arxiv.org/abs/2104.02145,"Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias. ",What Will it Take to Fix Benchmarking in Natural Language Understanding?
131,1379789578505236484,1317002454375059456,Paula Garc√≠a Molina,"['Check out my new paper <LINK> where we show how to use quantum computers to solve partial differential equations. Thank you @jjgarciaripoll for making this possible!', '@alvarotejerom @jjgarciaripoll Gracias!!üòÑ', '@ManuQPhys @jjgarciaripoll Muchas gracias Manu!', '@JorgeCasanovaM2 @jjgarciaripoll Muchas gracias!']",https://arxiv.org/abs/2104.02668,"In this work, we develop a highly efficient representation of functions and differential operators based on Fourier analysis. Using this representation, we create a variational hybrid quantum algorithm to solve static, Schr\""odinger-type, Hamiltonian partial differential equations (PDEs), using space-efficient variational circuits, including the symmetries of the problem, and global and gradient-based optimizers. We use this algorithm to benchmark the performance of the representation techniques by means of the computation of the ground state in three PDEs, i.e., the one-dimensional quantum harmonic oscillator, and the transmon and flux qubits, studying how they would perform in ideal and near-term quantum computers. With the Fourier methods developed here, we obtain low infidelities of order $10^{-4}-10^{-5}$ using only three to four qubits, demonstrating the high compression of information in a quantum computer. Practical fidelities are limited by the noise and the errors of the evaluation of the cost function in real computers, but they can also be improved through error mitigation techniques. ","Quantum Fourier analysis for multivariate functions and applications to
  a class of Schr\""odinger-type partial differential equations"
132,1379762621956681730,494212643,Aayush Saxena,"['Paper alert!! Our new study constraining the X-ray binary populations in galaxies at z&gt;3 using VANDELS is out now: <LINK>\n\nThe unique combo of VANDELS and 7Ms Chandra data enables probing X-rays from normal galaxies at super high redshifts! Some key plots below: <LINK>', '@Rohan_Naidu Cheers! We do find some interesting deviations from the redshift evolution expected purely from Z evolution, which may be indicative of some other fun stuff in high-z galaxies that impacts XRB output...']",https://arxiv.org/abs/2104.02624,"We use VANDELS spectroscopic data overlapping with the $\simeq$7 Ms Chandra Deep Field South survey to extend studies of high-mass X-ray binary systems (XRBs) in 301 normal star-forming galaxies in the redshift range $3 < z < 5.5$. Our analysis evaluates correlations between X-ray luminosities ($L_X$), star formation rates (SFR) and stellar metallicities ($Z_\star$) to higher redshifts and over a wider range in galaxy properties than hitherto. Using a stacking analysis performed in bins of both redshift and SFR for sources with robust spectroscopic redshifts without AGN signatures, we find convincing evolutionary trends in the ratio $L_X$/SFR to the highest redshifts probed, with a stronger trend for galaxies with lower SFRs. Combining our data with published samples at lower redshift, the evolution of $L_X$/SFR to $z\simeq5$ proceeds as $(1 + z)^{1.03 \pm 0.02}$. Using stellar metallicities derived from photospheric absorption features in our spectroscopic data, we confirm indications at lower redshifts that $L_X$/SFR is stronger for metal-poor galaxies. We use semi-analytic models to show that metallicity dependence of $L_X$/SFR alone may not be sufficient to fully explain the observed redshift evolution of X-ray emission from high-mass XRBs, particularly for galaxies with SFR $<30$ $M_\odot$ yr$^{-1}$. We speculate that the discrepancy may arise due to reduced overall stellar ages in the early Universe leading to higher $L_X$/SFR for the same metallicity. We use our data to define the redshift-dependent contribution of XRBs to the integrated X-ray luminosity density and, in comparison with models, find that the contribution of high-mass XRBs to the cosmic X-ray background at $z>6$ may be $\gtrsim 0.25$ dex higher than previously estimated. ","The VANDELS Survey: New constraints on the high-mass X-ray binary
  populations in normal star-forming galaxies at 3 &lt; z &lt; 5.5"
133,1379737640388472834,14327235,Jack Valmadre,"['New paper on arxiv!\nLocal metrics for multi-object tracking\n<LINK>\n\nWe propose metrics for MOT where tracks only need to be correct within a finite temporal horizon. Varying the horizon reveals the distance at which trackers make association errors\n\n1/n', 'A difficult question in MOT is how to measure accuracy when some degree of association error is tolerable. Various association metrics have been proposed over the years (MOTA, HOTA, mutual info, Rand index, ...) and each defines an implicit trade-off with detection\n\n2/n', 'Local metrics provide an alternative mechanism to explicitly specify the trade-off between detection and association. Instead of choosing an association metric and combining it with a detection metric, benchmarks can just choose one (or more) temporal horizon(s)\n\n3/n', 'We further propose a decomposition of tracker errors into detection (false-negative and false-positive) and association (split and merge). Together with the temporal horizon, these constitute valuable tools for analysing and comparing MOT algorithms\n\n4/n', 'Shown here: Average Local Tracking Accuracy (ALTA) as a function of temporal horizon for state-of-the-art trackers on MOT17. The metric ranges from pure detection (horizon &lt; 1 frame) to strict tracking (horizon &gt; seq len). Asterisks denote the use of a custom detector\n\n5/n https://t.co/aVWbRoqvjD']",https://arxiv.org/abs/2104.02631,"This paper introduces temporally local metrics for Multi-Object Tracking. These metrics are obtained by restricting existing metrics based on track matching to a finite temporal horizon, and provide new insight into the ability of trackers to maintain identity over time. Moreover, the horizon parameter offers a novel, meaningful mechanism by which to define the relative importance of detection and association, a common dilemma in applications where imperfect association is tolerable. It is shown that the historical Average Tracking Accuracy (ATA) metric exhibits superior sensitivity to association, enabling its proposed local variant, ALTA, to capture a wide range of characteristics. In particular, ALTA is better equipped to identify advances in association independent of detection. The paper further presents an error decomposition for ATA that reveals the impact of four distinct error types and is equally applicable to ALTA. The diagnostic capabilities of ALTA are demonstrated on the MOT 2017 and Waymo Open Dataset benchmarks. ",Local Metrics for Multi-Object Tracking
134,1379631725165223937,844704885870333952,Canwen Xu,"['Trained a Chinese BERT and want to know how it knows about the common sense and world knowledge? DogWhistle is a new dataset suitable for testing PLMs. It studies the cant/jargons with a game setting. Check out our #naacl2021 paper here: <LINK>\n@wangchunshu <LINK>', 'üíØYou can submit to the leaderboard here: https://t.co/Hzbe1SRo3X \nü§óBaseline code (powered by @huggingface) is available here: https://t.co/TtRnrlpWbu']",http://arxiv.org/abs/2104.02704,"Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks. The code is available at this https URL The data and leaderboard are available at this https URL ","Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with
  Common Sense and World Knowledge"
135,1379598021575409664,3245949691,Rebecca Leane,"['New paper out today!\n\nFirst Analysis of Jupiter in Gamma Rays and a New Search for Dark Matter\n<LINK>\ntogether with Tim Linden (@trlinden).\n\nWe analyze Jupiter in gamma rays for the first time, and perform a new dark matter search! Story time:', 'King of the Roman gods, Jupiter, commanded lightning, thunder, and storms. Analogous to the Greek god Zeus, he exerted his power with lightning bolts as weapons. Highlighting his stature, Jupiter was often portrayed ruling on a throne with a royal scepter in hand.', 'His luminous wrath won him the name of one of the brightest objects in the sky, Iovis Stella (the star of Jupiter). Today, it is known as Jupiter, which is the heaviest and largest planet in our Solar System.', ""Despite Jupiter's known existence for thousands of years, it has never been properly studied in gamma rays. For the first time, we perform a dedicated search for Jupiter‚Äôs lightning bolts (gamma rays) with the Fermi Gamma-Ray Space Telescope."", 'There are a few reasons to be interested in whether or not Jupiter shines in gamma rays. Firstly, there are astrophysical reasons these gamma rays could exist. For example, the gamma rays could be produced through acceleration of cosmic rays in Jovian magnetic fields.', ""They could also be produced due to interaction of galactic cosmic rays with Jupiter‚Äôs atmosphere. So, detecting gamma rays could give us new information about Jupiter's properties."", ""There is another possibility. There could be a new signal of dark matter annihilation, that has been secretly emanating from Jupiter all this time. But I'll get to that later. Lets first just jump straight to it -- what does Jupiter look like in gamma rays?"", 'For the first time ever, here it is in 1-3 GeV energies: https://t.co/XvoefC0NqT', ""The top-left figure shows the total counts of a 45 degree region around Jupiter, as it has moved through its orbit. Top-right is the data-driven background (the same part of the sky, but when Jupiter isn't there)."", 'Bottom-left is the residual map (counts - background) and bottom-right shows you the size/position of Jupiter to Fermi at this energy. So, the residual map should have lit up at that Jupiter position. However, at these energies, there is no clear excess of gamma rays.', 'Here it is again at the lowest end of our analysis, as per above but for 10-15 MeV energies: https://t.co/lkDReYoDM6', 'We can study the flux in each energy bin, to derive the first ever upper limits on the Jovian gamma-ray flux: https://t.co/mwLGWwS4KY', 'The blue line is zero flux, black dots are our data points. At first it might look like there are several excesses here, but note that importantly the energy bins are *highly* correlated, so mostly there is nothing significant here.', 'The shape of the bumps primarily arise due to (i) exposure of Fermi increasing at higher energies, meaning that fluctuations look more significant at lower energies (and therefore the ""damping"" effect with increasing energy), and', '(ii) significant energy-dispersion of the low-energy Fermi data, which smears the true Jovian energy flux between multiple energy bins.', ""Note there is a statistically significant excess in the lowest energy bins, at ~5 sigma. We emphasize however that this is not robust; we are really stretching Fermi's limits to analyze such low energy gammas. New systematic studies are needed here, which is outside our scope."", ""So, no clear excesses in Jovian gamma rays. What else can we do with this new analysis? Well, I said earlier that there could be a new signal of Dark Matter (DM) which would make Jupiter shine in gamma rays. We don't see excess gammas, so we can constrain such a scenario."", 'Not only might this happen, but we point out that Jupiter can be one of the *best* DM detectors if DM has some particular properties, for a few reasons.', 'Firstly, because Jupiter has a large surface area compared to other solar system planets, it can capture *more* DM. This happens as DM in the Galactic halo passes through Jupiter, scatters, loses energy, and becomes gravitationally captured. More DM means a larger DM signal.', 'You might then wonder why not just use the even bigger (and very close by) Sun. Well, the second advantage is that because Jupiter has a cooler core than the Sun, it gives the DM particles less of a thermal kick.', 'This in part can stop lighter DM from evaporating out of Jupiter, which would have evaporated out of the Sun. These properties of large size and cool core temperature make Jupiter an optimal celestial target for sub-GeV DM.', 'Now, once this light DM is captured, it can start to annihilate. If it annihilates to particles with a sufficiently long lifetime or boost, the particles can decay outside the Jovian surface, into detectable gamma rays which can be seen by Fermi.', 'This is illustrated by our cartoon here: https://t.co/BgJlM8eAlA', 'So, we go ahead and set the first limits on the DM-proton scattering cross section from annihilation to long-lived particles in Jupiter, assuming they decay to gamma rays: https://t.co/vqKCIt2ifE', 'With the assumption of this long-lived/boosted mediator, we can outperform direct detection by 10 orders of magnitude! We see Jupiter has sensitivity where the Sun drops off, though with less cross section reach, primarily as Jupiter is smaller + therefore has a smaller DM flux.', 'There are a few caveats in our new Jupiter DM limits. Primarily, we emphasize that in DM model-dependent contexts, these bounds can weaken, both in their cross section sensitivities and lower end of the DM mass reach (you can read about those in our paper).', 'Overall, we emphasize that we aim to show the optimal constraining power of Jovian gamma rays on DM, rather than model-dependent realizations of these bounds. This strong sensitivity motivates new DM model-dependent studies for Jupiter!', ""Looking forward, it will be interesting to see if upcoming MeV gamma-ray telescopes such as AMEGO and e-ASTROGAM find any Jovian gamma rays, especially at the lower end of our analysis, where Fermi's performance suffers. Maybe Jupiter still has some secrets to share..."", 'Many thanks to my awesome collaborator on this project, Tim Linden! This was a very enjoyable joint venture between @SLAClab and @TheOKC.', '@Katelinsaurus Thanks Katelin! Yeah the assumption is that the DM scatters off the Jovian protons to get captured, so the sensitivity requires there to be some DM-proton coupling.', '@Katelinsaurus Totally agree about the model dependence -- DD and ID are totally different setups, so with a specific model the interplay of the limits can look pretty different (we discuss this briefly in the paper).']",https://arxiv.org/abs/2104.02068,"We present the first dedicated gamma-ray analysis of Jupiter, using 12 years of data from the Fermi Telescope. We find no robust evidence of gamma-ray emission, and set upper limits of $\sim10^{-9}~$GeV cm$^{-2} $s$^{-1}$ on the Jovian gamma-ray flux. We point out that Jupiter is an advantageous dark matter (DM) target due to its large surface area (compared to other solar system planets), and cool core temperature (compared to the Sun). These properties allow Jupiter to both capture and retain lighter DM, providing a complementary probe of sub-GeV DM. Our analysis focuses on the annihilation of sub-GeV DM to long-lived particles, which can escape the Jovian surface and decay into gamma rays. In this regime, we constrain DM-proton scattering cross-sections as low as $10^{-41}~$cm$^2$, which is up to ten orders of magnitude more sensitive than direct detection, and subject to fewer astrophysical uncertainties than other limits in this parameter space. Our work motivates follow-up studies with upcoming MeV telescopes such as AMEGO and e-ASTROGAM. ",First Analysis of Jupiter in Gamma Rays and a New Search for Dark Matter
136,1379541880124362754,1273462312968609792,Derek Lim,"['Our new paper: ‚ÄúNew Benchmarks for Learning on Non-Homophilous Graphs‚Äù in the Graph Learning Benchmarks Workshop at WWW 2021.\n@xiuyu_l @Felix_Hohne @sernamlim\narXiv: <LINK>\nData/Code: <LINK>\n1/ <LINK>', 'A lot of graph-structured data exhibit homophily, where connected nodes are often of the same class (say, for some class that we wish to predict). A lot of common benchmarks are homophilous, and many graph learning methods leverage homophily assumptions.\n2/', 'But not all real-world data is homophilous. A collection of non-homophilous graph data (Pei et al. 2019) has driven recent work in GNNs for non-homophilous settings. However, these datasets face some issues‚Ä¶\n3/', 'These prior datasets are tiny (183 to 7600 nodes) and are from a limited range of application areas. Just like issues in GNN benchmarking on small homophilous datasets, these non-homophilous datasets may not be that informative of graph learning performance.\n4/', 'We propose larger non-homophilous datasets (up to 2.9 million nodes) from diverse application areas. These datasets have been studied in different contexts in the past, though we introduce new node labels (i.e. new prediction tasks) and/or new node features for some.\n5/ https://t.co/aDdY8sNeku', 'The compatibility matrices for our proposed datasets (where the i,j entry is the proportion of edges from nodes of class i that are connected to nodes of class j) show that they exhibit a wide variety of relationships between graph topology and node labels.\n6/ https://t.co/FsaKLFzD5m', 'We benchmark a lot of methods on our datasets, including standard GNNs, newer GNNs for non-homophilous settings, and oft-overlooked simple methods that perform rather well, like LINK (logistic regression on adjacency) and 2-hop label propagation methods.\n7/ https://t.co/aU3iEacGaf', 'Other nice light resources:\nPost on recent non-homophilous GNN work: https://t.co/vWKj8WagRa\nDiscussions on homophily in benchmarks and some non-homophilous methods: https://t.co/qf7srhJyHc\n8/', 'Still, homophily is super prevalent in graph-structured data. We hope that our work helps improve graph learning for those data that are non-homophilous, and aids in evaluation of the non-homophilous methods from recent/ future works.\n9/9']",https://arxiv.org/abs/2104.01404,"Much data with graph structures satisfy the principle of homophily, meaning that connected nodes tend to be similar with respect to a specific attribute. As such, ubiquitous datasets for graph machine learning tasks have generally been highly homophilous, rewarding methods that leverage homophily as an inductive bias. Recent work has pointed out this particular focus, as new non-homophilous datasets have been introduced and graph representation learning models better suited for low-homophily settings have been developed. However, these datasets are small and poorly suited to truly testing the effectiveness of new methods in non-homophilous settings. We present a series of improved graph datasets with node label relationships that do not satisfy the homophily principle. Along with this, we introduce a new measure of the presence or absence of homophily that is better suited than existing measures in different regimes. We benchmark a range of simple methods and graph neural networks across our proposed datasets, drawing new insights for further research. Data and codes can be found at this https URL ",New Benchmarks for Learning on Non-Homophilous Graphs
137,1379475290154356738,55348425,Ajay Jain,"['Check out our new paper - we put NeRF on a diet! Given just 1 to 8 images, DietNeRF renders consistent novel views of an object using prior knowledge from large visual encoders like CLIP ViT.\n \n<LINK>\n<LINK>\nw/ Matthew Tancik, @pabbeel 1/', 'Given photos of a scene from many viewpoints, NeRF learns a volumetric representation that can be rendered from novel perspectives. NeRF works well given ~20-100 photos, but often not with a few (8, on left). DietNeRF adds an auxiliary loss that removes most artifacts (right). 2/ https://t.co/C0fhAQRQCs', 'The core problem is that NeRF computes loss in pixel space, so renderings need to align pixel-wise with an observation. However, *a bulldozer is a bulldozer from any viewpoint*: images from different viewpoints share high-level semantic properties like object identity. 3/ https://t.co/uboZq3DY8X', 'Our DietNeRF regularizes the NeRF scene representation with a semantic consistency loss, computed in *a feature space*. This allows us to compare renderings from arbitrary poses. We use pre-trained CLIP and ImageNet Vision Transformers in experiments. 4/ https://t.co/EfgtLqnOsZ', 'pixelNeRF tackled the same problem by training NeRF on multiple similar scenes. This allows generalization to new scenes with only a few views. Using our loss, ""DietPixelNeRF"" synthesizes novel views with higher perceptual quality from *only a single monocular photo*. https://t.co/35tLQt4I2H', 'Our paper has details and bonus results, including extrapolation to completely unseen regions and tips for making this fast. Watch our video explanation for a quick overview: https://t.co/DzWKXUCLFD 6/6', 'cc @_parasj @AravSrinivas @akanazawa @pathak2206  @aditij @alexyu00 @jon_barron @_pratul_ - thank you for feedback along the way!']",https://arxiv.org/abs/2104.00677,"We present DietNeRF, a 3D neural scene representation estimated from a few images. Neural Radiance Fields (NeRF) learn a continuous volumetric representation of a scene through multi-view consistency, and can be rendered from novel viewpoints by ray casting. While NeRF has an impressive ability to reconstruct geometry and fine details given many images, up to 100 for challenging 360{\deg} scenes, it often finds a degenerate solution to its image reconstruction objective when only a few input views are available. To improve few-shot quality, we propose DietNeRF. We introduce an auxiliary semantic consistency loss that encourages realistic renderings at novel poses. DietNeRF is trained on individual scenes to (1) correctly render given input views from the same pose, and (2) match high-level semantic attributes across different, random poses. Our semantic loss allows us to supervise DietNeRF from arbitrary poses. We extract these semantics using a pre-trained visual encoder such as CLIP, a Vision Transformer trained on hundreds of millions of diverse single-view, 2D photographs mined from the web with natural language supervision. In experiments, DietNeRF improves the perceptual quality of few-shot view synthesis when learned from scratch, can render novel views with as few as one observed image when pre-trained on a multi-view dataset, and produces plausible completions of completely unobserved regions. ",Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis
138,1379274668381724673,168143135,Jeongmin Lee,"[""Our new paper accepted at AI in Medicine (AIME) 2021 conf is online <LINK> What's new? When you have event sequence data with high variability like ones form EHRs, do fine-tuning with past history of its own on-the-fly. It will greatly increase predictability.""]",https://arxiv.org/abs/2104.01787,"Clinical event sequences consist of thousands of clinical events that represent records of patient care in time. Developing accurate prediction models for such sequences is of a great importance for defining representations of a patient state and for improving patient care. One important challenge of learning a good predictive model of clinical sequences is patient-specific variability. Based on underlying clinical complications, each patient's sequence may consist of different sets of clinical events. However, population-based models learned from such sequences may not accurately predict patient-specific dynamics of event sequences. To address the problem, we develop a new adaptive event sequence prediction framework that learns to adjust its prediction for individual patients through an online model update. ","Neural Clinical Event Sequence Prediction through Personalized Online
  Adaptive Learning"
139,1379106204379648002,1068545181576773632,Kenneth Brown,"['New paper on ""Hidden Inverses"" on the arXiv <LINK> with @bichenzh @SwarnadeepMaju3 @DriptoDebroy , and other #DukeQuantumCenter researchers. Joint work between my lab and Jungsang Kim\'s lab sponsored by @NSF @IARPAnews @doescience @energy @EPiQCExpedition', 'What is a hidden inverse?  In quantum computing, we often choose gates (unitary operators) that are their own inverse. That is unlikely for unitary operators.', 'Normally we imagine the gate G comes from some time dynamics: G=exp(-iHt). Then the inverse is Ginv=exp(iHt). This has the nice feature of rolling the dynamics forward and then rolling it back.', 'For most systems, these familiar self inverse gates, Hadamard, CNOT, CPhase, are made from gates that are not self inverses, CNOT=A*B*C.  Then we can construct CNOTinv=Cinv*Binv*Ainv.', 'If there are no coherent or systematic errors, then CNOT and CNOTinv are the same gate.  When there are coherent or systematic errors this is not the case, and choosing between these choices can change circuit fidelity.', 'It connects to previous work in the group on stabilizer slicing (hidden inverses on subspaces https://t.co/L1409Prlie) and crafting two-qubit composite gates (https://t.co/Jo3nu2ptCD).', 'We show in this paper theoretical advantages for quantum simulation primitives and experimental advantage for phase and amplitude noise for two-qubit circuits.', 'It connects to the broader literature on control and dynamic decoupling.  It shares properties with randomized compiling (reduce coherent errors by changing the circuit, https://t.co/hi0zx3n9zz) but can outperform randomized compiling if the structure of the noise is known.', '@nmrqip Yes. The larger point that I want to make is that there are many ways to create any gate. Despite our best attempts, it is still useful to use more than one way and which way to use is context dependent. I think the good news is that we can make decisions based on local context.', '@nmrqip Agreed. The amplitude work is easy to map from system to system. The detuning work outside of two-level systems is harder. We made some progress on detuning errors in bichromatic fields for Molmer-Sorensen ion trap gates but it is not portable to other systems.']",https://arxiv.org/abs/2104.01119,"Coherent gate errors are a concern in many proposed quantum computing architectures. These errors can be effectively handled through composite pulse sequences for single-qubit gates, however, such techniques are less feasible for entangling operations. In this work, we benchmark our coherent errors by comparing the actual performance of composite single-qubit gates to the predicted performance based on characterization of individual single-qubit rotations. We then propose a compilation technique, which we refer to as hidden inverses, that creates circuits robust to these coherent errors. We present experimental data showing that these circuits suppress both overrotation and phase misalignment errors in our trapped ion system. ",Hidden Inverses: Coherent Error Cancellation at the Circuit Level
140,1378033555922554882,71156816,Chris Mattmann,"['Check out our new paper! Many-to-English Machine Translation Tools, Data, and Pretrained Models @thammegowda doing things! <LINK>']",https://arxiv.org/abs/2104.00290,"While there are more than 7000 languages in the world, most translation research efforts have targeted a few high-resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec, and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages. ","Many-to-English Machine Translation Tools, Data, and Pretrained Models"
141,1378003733230063617,23974975,Thomas Beckers,['Check out our new paper on safe online learning-based control of multi-agent systems <LINK> or watch the video <LINK>\n@PennEngineers @GRASPlab'],http://arxiv.org/abs/2104.00130,"Formation control algorithms for multi-agent systems have gained much attention in the recent years due to the increasing amount of mobile and aerial robotic swarms. The design of safe controllers for these vehicles is a substantial aspect for an increasing range of application domains. However, parts of the vehicle's dynamics and external disturbances are often unknown or very time-consuming to model. To overcome this issue, we present a safe formation control law for multiagent systems based on double integrator dynamics by using Gaussian Processes for an online learning of the unknown dynamics. The presented approach guarantees a bounded error to desired formations with high probability, where the bound is explicitly given. A numerical example highlights the effectiveness of the learning-based formation control law. ","Safe Online Learning-based Formation Control of Multi-Agent Systems with
  Gaussian Processes"
142,1377976999122178051,939589825602228224,Leah Jenks,"['New paper ‚Äúthe Chern-Simons Caps for Rotating Black Holes‚Äù! I‚Äôm so lucky to have worked on this project with my advisor, Stephon Alexander @stephstem , Greg Gabadadze and Nico Yunes! For an explanation of the super cool science, here‚Äôs a üßµ<LINK>', 'Dynamical Chern-Simons gravity is an extension to general relativity which is motivated from both particle physics and string theory. It can be treated as an effective quantum field theory - we take this approach and first show the parameter space where this is valid', 'We then look at a rotating black hole solution in dCS and find an intriguing result - there are two regions at the north and south poles that have unusual dynamics', 'Namely, due to the presence of the Chern-Simons term, geodesics are unfocused in these domains', 'For some physical intuition these ‚Äòcaps‚Äô extend to 20 degrees on either side of the poles and to 3% of the black hole‚Äôs size. For the BH in M87 this is approximately 3 AU!', 'Why is this important? Well, the Hawking-Penrose singularly theorem relies on the focusing of geodesics, implying that the singularity theorem can‚Äôt be so straightforwardly applied in these regions.', 'This doesn‚Äôt necessarily point to singularity avoidance in dCS, but opens up a wide range of questions about further exploring dynamics in the region. Stay tuned for future work!']",https://arxiv.org/abs/2104.00019,"We study the dynamical Chern-Simons gravity as an effective quantum field theory, and discuss a broad range of its parameter space where the theory is valid. Within that validity range, we show that slowly rotating black holes acquire novel geometric structures due to the gravitational dynamical Chern-Simons term. In particular, the rotating black hole solutions get endowed with two, cap-like domains, emanating from the north and south poles in the standard Boyer-Lindquist coordinates. The domains extend out to a distance that is approximately a few percent of the black hole's size. The cap-like domains have an unusual equation of state, pointing to non-standard dynamics within the caps. In particular, the focusing condition for geodesics is violated in those domains. This in turn implies that the Hawking-Penrose singularity theorem cannot be straightforwardly applied to hypothetical probe matter placed within the Chern-Simons caps. ",The Chern-Simons Caps for Rotating Black Holes
143,1377958224570253312,91634245,Brad Marston,['Our new paper on ‚ÄúResistively detected NMR as a probe of the topological nature of conducting edge/surface states‚Äù with Zekun Zhuang as lead author is now on the arXiv.  <LINK>'],https://arxiv.org/abs/2104.00146,"Electron spins in edge or surface modes of topological insulators (TIs) with strong spin-orbit coupling cannot be directly manipulated with microwaves due to the locking of electron spin to its momentum. We show by contrast that a resistively detected nuclear magnetic resonance (RDNMR) based technique can be used to probe the helical nature of surface conducting states. In such experiments, one applies a radio frequency (RF) field to reorient nuclear spins that then couple to electronic spins by the hyperfine interaction. The spin of the boundary electrons can thereby be modulated, resulting in changes in conductance at nuclear resonance frequencies. Here, we demonstrate that the conductivity is sensitive to the direction of the applied magnetic field with respect to the helicity of the electrons. This dependence of the RDNMR signal on angle probes the nature of the conductive edge or surface states. In the case of 3D TI in the quantum Hall regime, we establish that the dominant mechanism responsible for the conductance change in a RDNMR experiment is based on the Overhauser field effect. Our findings indicate that the same physics underlying the use of RDNMR to probe TI states also enables us to use RF control of nuclear spins to coherently manipulate topologically protected states which could be useful for a new generation of devices. ","Resistively detected NMR as a probe of the topological nature of
  conducting edge/surface states"
144,1377943395604267013,78913886,parfait Atchad√©,"['I‚Äôm glad to share my new paper, quantum Case Based-Reasoning (qCBR).\nI aim to solve the combinatorial optimization problems (#Logistics, #industry, #Fintech, #medical, #pharmaceutical, etc.) as human do. Thanks to #qibo @qiskit @KetPuntoG my team #DS4DS\n<LINK> <LINK>']",https://arxiv.org/abs/2104.00409,"Case-Based Reasoning (CBR) is an artificial intelligence approach to problem-solving with a good record of success. This article proposes using Quantum Computing to improve some of the key processes of CBR, such that a Quantum Case-Based Reasoning (qCBR) paradigm can be defined. The focus is set on designing and implementing a qCBR based on the variational principle that improves its classical counterpart in terms of average accuracy, scalability and tolerance to overlapping. A comparative study of the proposed qCBR with a classic CBR is performed for the case of the Social Workers' Problem as a sample of a combinatorial optimization problem with overlapping. The algorithm's quantum feasibility is modelled with docplex and tested on IBMQ computers, and experimented on the Qibo framework. ",quantum Case-Based Reasoning (qCBR)
145,1377933596808212483,795877354266456064,KoheiKamadaPhys,['Our new paper on the chiral effect on the gravitational waves appeared. Jun‚Äôya did a good job. <LINK>'],https://arxiv.org/abs/2104.00583,"Gravitational counterpart of the chiral magnetic effect, which is referred as the chiral gravitational effect, can also be of interest in a cosmological setup. In this study, we investigate this effect in the time-dependent chiral asymmetric fermion background and in the expanding spacetime by formulating the effective action of gravitational waves. We also analyze the anomaly equation to see how the backreaction from gravitational waves to thermal chiral plasma occurs. We find that the non-trivial time dependence of chiral chemical potential, which can be induced in some scenarios of baryogenesis, is the key ingredient of the chiral gravitational effect. It turns out that the ""memory"" of the effect is imprinted on the high frequency gravitational waves propagating in the plasma. Cosmological implications and potential effects on the gravitational wave observation are briefly discussed. ",Chiral gravitational effect in time-dependent backgrounds
146,1377858154793435136,335479530,Mattia Walschaers,"['New @arxiv paper! \nTogether with the team of Qiongyi He in Beijing, we derive monogamy relations for the amount Wigner negativity that can be remotely created. We also show that stronger steering does not always create more Wigner negativity. \n<LINK>', 'Big shout out to Yu and Shuheng for the hard work they put into these results!']",https://arxiv.org/abs/2104.00451,"Wigner negativity, as a well-known indicator of nonclassicality, plays an essential role in quantum computing and simulation using continuous-variable systems. Recently, it has been proven that Einstein-Podolsky-Rosen steering is a prerequisite to generate Wigner negativity between two remote modes. Motivated by the demand of real-world quantum network, here we investigate the shareability of generated Wigner negativity in the multipartite scenario from a quantitative perspective. By establishing a monogamy relation akin to the generalized Coffman-Kundu-Wootters inequality, we show that the amount of Wigner negativity cannot be freely distributed among different modes. Moreover, for photon subtraction -- one of the main experimentally realized non-Gaussian operations -- we provide a general method to quantify the remotely generated Wigner negativity. With this method, we find that there is no direct quantitative relation between the Gaussian steerability and the amount of generated Wigner negativity. Our results pave the way for exploiting Wigner negativity as a valuable resource for numerous quantum information protocols based on non-Gaussian scenario. ","Quantification of Wigner Negativity Remotely Generated via
  Einstein-Podolsky-Rosen Steering"
147,1389644597530447872,4098537032,Asher Trockman,"['Check out our new method for orthogonalizing convolutional layers at #ICLR2021. Compared to similar layers, ours is often faster and improves clean &amp; certifiably-robust accuracy.\n\nPaper: <LINK>\nCode: <LINK>\n\nWith @zicokolter <LINK>']",https://arxiv.org/abs/2104.07167,"Recent work has highlighted several advantages of enforcing orthogonality in the weight layers of deep networks, such as maintaining the stability of activations, preserving gradient norms, and enhancing adversarial robustness by enforcing low Lipschitz constants. Although numerous methods exist for enforcing the orthogonality of fully-connected layers, those for convolutional layers are more heuristic in nature, often focusing on penalty methods or limited classes of convolutions. In this work, we propose and evaluate an alternative approach to directly parameterize convolutional layers that are constrained to be orthogonal. Specifically, we propose to apply the Cayley transform to a skew-symmetric convolution in the Fourier domain, so that the inverse convolution needed by the Cayley transform can be computed efficiently. We compare our method to previous Lipschitz-constrained and orthogonal convolutional layers and show that it indeed preserves orthogonality to a high degree even for large convolutions. Applied to the problem of certified adversarial robustness, we show that networks incorporating the layer outperform existing deterministic methods for certified defense against $\ell_2$-norm-bounded adversaries, while scaling to larger architectures than previously investigated. Code is available at this https URL ",Orthogonalizing Convolutional Layers with the Cayley Transform
148,1389019649032003585,1166165104808931328,Zahra Tabrizi,"[""I'm so glad to have this new paper out. Here we've shown that using the Lmu-Ltau model, the mass-coupling region which alleviates the Hubble tension and explains the IceCube gap as well as the g-2 band could all be coinciding. \n<LINK> <LINK>""]",https://arxiv.org/abs/2104.15136,"In light of the recent Muon $g-2$ experiment data from Fermilab, we investigate the implications of a gauged $L_{\mu} - L_{\tau}$ model for high energy neutrino telescopes. It has been suggested that a new gauge boson at the MeV scale can both account for the Muon $g-2$ data and alleviate the tension in the Hubble parameter measurements. It also strikes signals at IceCube from the predicted resonance scattering between high-energy neutrinos and the cosmic neutrino background. We revisit this model based on the latest IceCube shower data, and perform a four-parameter fit to find a preferred region. While the data are consistent with the absence of resonant signatures from secret interactions, we find the preferred region consistent with the muon $g-2$ anomaly and Hubble tension. We demonstrate that future neutrino telescopes such as IceCube-Gen2 can probe this unique parameter space, and point out that successful measurements would infer the neutrino mass with $0.05~{\rm eV}\lesssim \Sigma m_\nu\lesssim 0.3~{\rm eV}$. ","High-energy cosmic neutrinos as a probe of the vector mediator scenario
  in light of the muon $g-2$ anomaly and Hubble tension"
149,1388122708349755394,2180768821,Erik Hoel,"['One thing I want to stress about this new paper of mine is that I think it provides an ""umbrella"" framework of how emergence occurs. By tracking how information is rendered redundant during reduction, it\'s really a meta-theory about information conversion\n<LINK> <LINK>', ""We use the mutual information *because* the information decomposition allows us to see precisely this change from synergistic information to redundant information following reduction, not because the MI is a special or particularly interesting  measure (in many cases it's not)"", ""@scottmstirling If you don't believe me or don't get it, I don't have time to try to convince you, sorry""]",https://arxiv.org/abs/2104.13368,"Is reduction always a good scientific strategy? Does it always lead to a gain in information? The very existence of the special sciences above and beyond physics seems to hint no. Previous research has shown that dimension reduction (macroscales) can increase the dependency between elements of a system (a phenomenon called ""causal emergence""). However, this has been shown only for specific measures like effective information or integrated information. Here, we provide an umbrella mathematical framework for emergence based on information conversion. Specifically, we show evidence that a macroscale can have more of a certain type of information than its underlying microscale. This is because macroscales can convert information from one type to another. In such cases, reduction to a microscale means the loss of this type of information. We demonstrate this using the well-understood mutual information measure applied to Boolean networks. By using the partial information decomposition, the mutual information can be decomposed into redundant, unique, and synergistic information atoms. Then by introducing a novel measure of the synergy bias of a given decomposition, we are able to show that the synergy component of a Boolean network's mutual information can increase at macroscales. This can occur even when there is no difference in the total mutual information between a macroscale and its underlying microscale, proving information conversion. We relate this broad framework to previous work, compare it to other theories, and argue it complexifies any notion of universal reduction in the sciences, since such reduction would likely lead to a loss of synergistic information in scientific models. ",Emergence as the conversion of information: A unifying theory
150,1388111851960119297,1227633169622556672,Valentin Hofmann,"[""‚ú®NEW PAPER‚ú®\nWe've all heard about polarizing issues like gun control, but can we detect them automatically? Short answer: yes! We introduce Slap4slip, a framework that achieves this with minimal supervision (w/ Janet Pierrehumbert &amp; @HinrichSchuetze). <LINK> /1 <LINK>"", 'We model the polarization of concepts along the dimensions of ideological agenda setting and framing. For framing, we draw upon insights from social psychology and moral foundations theory. /2', 'To determine the most polarizing concepts, we combine graph neural networks with structured sparsity. Working with data from Reddit, we find that ""mainstream"" was one of the most polarizing concepts in 2019! /3', 'Our model also learns ideology embeddings for individual nodes on the social network that capture temporal ideological dynamics. Want to know what subreddit hijacking is? Check out the paper! /4']",https://arxiv.org/abs/2104.08829,"The increasing polarization of online political discourse calls for computational tools that are able to automatically detect and monitor ideological divides in social media. Here, we introduce a minimally supervised method that directly leverages the network structure of online discussion forums, specifically Reddit, to detect polarized concepts. We model polarization along the dimensions of agenda setting and framing, drawing upon insights from moral psychology. The architecture we propose combines graph neural networks with structured sparsity and results in representations for concepts and subreddits that capture phenomena such as ideological radicalization and subreddit hijacking. We also create a new dataset of political discourse covering 12 years and more than 600 online groups with different ideologies. ","Modeling Ideological Agenda Setting and Framing in Polarized Online
  Groups with Graph Neural Networks and Structured Sparsity"
151,1388033706967904258,268337552,Nicolas Kourtellis,"['Preprint of our new paper proposing the 1st Privacy-Preserving Federated Learning Framework with TEEs, accepted @ACMMobiSys 2021, here: <LINK>\n@VincentMo6,@realhamed,@minoskt,@_EduardMarin_,@Diego_Perino \nPowered by @TEFresearch,@concordiah2020,\n@accordion_h2020 <LINK>']",https://arxiv.org/abs/2104.14380,"We propose and implement a Privacy-preserving Federated Learning ($PPFL$) framework for mobile systems to limit privacy leakages in federated learning. Leveraging the widespread presence of Trusted Execution Environments (TEEs) in high-end and mobile devices, we utilize TEEs on clients for local training, and on servers for secure aggregation, so that model/gradient updates are hidden from adversaries. Challenged by the limited memory size of current TEEs, we leverage greedy layer-wise training to train each model's layer inside the trusted area until its convergence. The performance evaluation of our implementation shows that $PPFL$ can significantly improve privacy while incurring small system overheads at the client-side. In particular, $PPFL$ can successfully defend the trained model against data reconstruction, property inference, and membership inference attacks. Furthermore, it can achieve comparable model utility with fewer communication rounds (0.54$\times$) and a similar amount of network traffic (1.002$\times$) compared to the standard federated learning of a complete model. This is achieved while only introducing up to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in $PPFL$'s client-side. ","PPFL: Privacy-preserving Federated Learning with Trusted Execution
  Environments"
152,1387225114383552516,367297219,Melanie Mitchell,"['New paper by me: ""Why AI is Harder Than We Think"".  <LINK>  \n\nFeedback is welcome! <LINK>', '@zxul767 Hard to summarize in a tweet, but I wrote about what understanding entails in Part 5 of my book: https://t.co/2PurH8BLMx', '@FelixHill84 Thanks for feedback!']",https://arxiv.org/abs/2104.12871,"Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (""AI spring"") and periods of disappointment, loss of confidence, and reduced funding (""AI winter""). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense. ",Why AI is Harder Than We Think
153,1386613549242404865,1097098670552559616,Andr√© Melo,"['New paper out today with @FatemiValla and @AkhmerovAnton: \n\n""Multiplet supercurrent in Josephson tunneling circuits""\n<LINK>\n\nSummary below! <LINK>', 'The Josephson effect allows DC supercurrent to flow in 2-terminal Josephson junctions at *zero* voltage. At *finite* voltages, no DC supercurrent can flow because charge transfers across terminals cost energy! (an AC supercurrent can still flow -- this is the AC Josephson effect)', 'However, in multi-terminal junctions there are energy-conserving processes that result in supercurrent at finite commensurate voltagesü§ØE.g. a 3 terminal junction with two terminals biased at n‚ÇÅV‚ÇÅ  = -n‚ÇÇ V‚ÇÇ will transfer n‚ÇÅ + n‚ÇÇ Cooper pairs to the third (grounded) terminal', 'Mechanisms put forward to explain this rely on nonlocal Andreev states that extend to multiple terminals\nand mediate charge transfers through nonlocal Andreev processes. However, this is subject to large dissipation and sensitive to the microscopic details of the junction‚òπÔ∏è', 'A natural question arises here. Are there other processes that mediate multiplet supercurrent? A clue: https://t.co/LBd7GldnTQ shows that Weyl physics can be encoded in collective electronic electronic modes in tunneling JJ circuits (rather than multi-terminal Andreev states)ü§î', 'Our work shows the answer is yes: simple Josephson tunneling circuits can also carry multiplet supercurrent! The nice thing is that these circuits are easy to fab and measure in the lab -- they are standard building blocks of superconducting devices. https://t.co/e6XACS66oa', 'There are two fun limits here. First, multiplet supercurrent flows even when the charging energy of the central island is zero, that is when the circuit behaves classically.', 'Second (and most suprising IMO), multiplet supercurrent can still flow when the charging energy is infinite. This implies the supercurrent is carried by sequential charge transfers!\n\n8/N https://t.co/86JwCSESRB', 'Lastly, we show there is a geometric contribution to the supercurrent that arises due to finite Berry curvature. Because this contribution scales linearly with the voltage, it can be distinguished from the regular adiabatic contribution. https://t.co/asHYZmjQyS', 'Thanks for reading! Code available on Zenodo (and also Binder-able): https://t.co/HW4AO9jKrs']",https://arxiv.org/abs/2104.11239,"The multi-terminal Josephson effect allows DC supercurrent to flow at finite commensurate voltages. Existing proposals to realize this effect rely on nonlocal Andreev processes in superconductor-normal-superconductor junctions. However, this approach requires precise control over microscopic states and is obscured by dissipative current. We show that standard tunnel Josephson circuits also support multiplet supercurrent mediated only by local tunneling processes. Furtheremore, we observe that the supercurrents persist even in the high charging energy regime in which only sequential Cooper transfers are allowed. Finally, we demonstrate that the multiplet supercurrent in these circuits has a quantum geometric component that is distinguinshable from the well-known adiabatic contribution. ",Multiplet supercurrent in Josephson tunneling circuits
154,1384512694183661572,65432023,Prof. Costas Andreopoulos,['New #neutrino x-section pheno paper by my PhD student J√πlia Tena-Vidal <LINK>. First in a series of papers on the new tunes of the well-known #GENIE MC. Next: Hadronization. Funded by @livuniphysics LIV.DAT. Initial work supported by an @IPPP_Durham associateship'],https://arxiv.org/abs/2104.09179,"We summarise the results of a study performed within the GENIE global analysis framework, revisiting the GENIE bare-nucleon cross-section tuning and, in particular, the tuning of a) the inclusive cross-section, b) the cross-section of low-multiplicity inelastic channels (single-pion and double-pion production), and c) the relative contributions of resonance and non-resonance processes to these final states. The same analysis was performed with several different comprehensive cross-section model sets available in GENIE Generator v3. In this work we performed a careful investigation of the observed tensions between exclusive and inclusive data, and installed analysis improvements to handle systematics in historic data. All tuned model configurations discussed in this paper are available through public releases of the GENIE Generator. With this paper we aim to support the consumers of these physics tunes by providing comprehensive summaries of our alternate model constructions, of the relevant datasets and their systematics, and of our tuning procedure and results. ",Neutrino-Nucleon Cross-Section Model Tuning in GENIE v3
155,1384364473981538304,716990588109852672,Yingtong Dou,['üîîNew paper out! #MachineLearning \n\nWe have extended our #CIKM20 paper to a 43-page journal submission (<LINK>) which investigates how different #ReinforcementLearning algorithms help #GNN to select informative neighbors.\n\nCode: <LINK>'],https://arxiv.org/abs/2104.07886,"Graph Neural Networks (GNNs) have been widely used for the representation learning of various structured graph data. While promising, most existing GNNs oversimplified the complexity and diversity of the edges in the graph, and thus inefficient to cope with ubiquitous heterogeneous graphs, which are typically in the form of multi-relational graph representations. In this paper, we propose RioGNN, a novel Reinforced, recursive and flexible neighborhood selection guided multi-relational Graph Neural Network architecture, to navigate complexity of neural network structures whilst maintaining relation-dependent representations. We first construct a multi-relational graph, according to the practical task, to reflect the heterogeneity of nodes, edges, attributes and labels. To avoid the embedding over-assimilation among different types of nodes, we employ a label-aware neural similarity measure to ascertain the most similar neighbors based on node attributes. A reinforced relation-aware neighbor selection mechanism is developed to choose the most similar neighbors of a targeting node within a relation before aggregating all neighborhood information from different relations to obtain the eventual node embedding. Particularly, to improve the efficiency of neighbor selecting, we propose a new recursive and scalable reinforcement learning framework with estimable depth and width for different scales of multi-relational graphs. RioGNN can learn more discriminative node embedding with enhanced explainability due to the recognition of individual importance of each relation via the filtering threshold mechanism. Comprehensive experiments on real-world graph data and practical tasks demonstrate the advancements of effectiveness, efficiency and the model explainability, as opposed to other comparative GNN models. ","Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural
  Networks"
156,1383002791849951234,2728854028,Philipp Dufter,"['New paper at #NAACL2021ü•≥\nTakeaway: for restricted cloze queries use small vocab plus expensive composition function (BERT) *OR* learn plenty of cheap representations with fastText\n@KassnerNora @HinrichSchuetze\nüìú <LINK> \nüñ•Ô∏è <LINK>\n #NLProc #LAMA', '@mariusmosbach @KassnerNora @HinrichSchuetze Good point - I agree that frequency certainly plays a role here. We have not done an analysis on that (yet).', 'Speaking of #LAMA\nCheck out multilingual LAMA covering 53 languages, which will \nbe presented next week at #EACL2021\n\n@KassnerNora, @HinrichSchuetze\nüìú https://t.co/72iz5EWO3n\nü§ó-datasets: m_lama\n\nSuper interesting contemporary work on multilingual LAMA: https://t.co/DEJQIvgzoc']",https://arxiv.org/abs/2104.07094,"Recent research investigates factual knowledge stored in large pretrained language models (PLMs). Instead of structural knowledge base (KB) queries, masked sentences such as ""Paris is the capital of [MASK]"" are used as probes. The good performance on this analysis task has been interpreted as PLMs becoming potential repositories of factual knowledge. In experiments across ten linguistically diverse languages, we study knowledge contained in static embeddings. We show that, when restricting the output space to a candidate set, simple nearest neighbor matching using static embeddings performs better than PLMs. E.g., static embeddings perform 1.6% points better than BERT while just using 0.3% of energy for training. One important factor in their good comparative performance is that static embeddings are standardly learned for a large vocabulary. In contrast, BERT exploits its more sophisticated, but expensive ability to compose meaningful representations from a much smaller subword vocabulary. ",Static Embeddings as Efficient Knowledge Bases?
157,1382522020844425217,1082668696202502144,Corey J Nolet,['I‚Äôm happy to announce the pre-print for our new paper ‚ÄúSemiring Primitives for Sparse Neighborhood Methods on the GPU.‚Äù We show that semirings can enable many important neighborhood machine learning methods and provide a novel implementation on the GPU. <LINK>'],https://arxiv.org/abs/2104.06357,"High-performance primitives for mathematical operations on sparse vectors must deal with the challenges of skewed degree distributions and limits on memory consumption that are typically not issues in dense operations. We demonstrate that a sparse semiring primitive can be flexible enough to support a wide range of critical distance measures while maintaining performance and memory efficiency on the GPU. We further show that this primitive is a foundational component for enabling many neighborhood-based information retrieval and machine learning algorithms to accept sparse input. To our knowledge, this is the first work aiming to unify the computation of several critical distance measures on the GPU under a single flexible design paradigm and we hope that it provides a good baseline for future research in this area. Our implementation is fully open source and publicly available as part of the RAFT library of GPU-accelerated machine learning primitives (this https URL). ",GPU Semiring Primitives for Sparse Neighborhood Methods
158,1381780891270311947,321794593,Jos√© G. Fern√°ndez-Trincado,"['Our new accepted paper Today on ArXiv üëâ ""The Rotation of Selected Globular Clusters and the Differential Rotation of M3 in Multiple Populations from the SDSS-IV APOGEE-2 Survey"" by L√°szl√≥, Szabolcs, et al. üëâ<LINK> üëá <LINK>']",https://arxiv.org/abs/2104.04524,"In this paper, we analyze 10 globular clusters in order to measure their rotational properties by using high precision radial velocity data from the SDSS-IV APOGEE-2 survey. Out of the 10 clusters we were able to successfully measure the rotation speed and position angle of the rotation axis for 9 clusters (M2, M3, M5, M12, M13, M15, M53, M92, M107). The comparison between our results and previous ones shows a really good agreement within our uncertainties. For four of the globular clusters, M3, M13, M5 and M15, we separated the sample into two generation of stars using their [Al/Fe] abundances and examined the kinematic features of these generations separately from one another. In case of M3, we found significant difference between the rotational properties of first and second populations, confirming for the first time the predictions of several numerical simulations from the literature. The other three clusters (M5, M13, M15) also show smaller deviation between the two groups of stars, but those deviations are comparable to our errors. ","The Rotation of Selected Globular Clusters and the Differential Rotation
  of M3 in Multiple Populations from the SDSS-IV APOGEE-2 Survey"
159,1381421121447297032,321794593,Jos√© G. Fern√°ndez-Trincado,"['Our new paper accepted in ApJ, based on Nitrogen Rich Field Stars is today on ArXiv. This one with data that I observed during three nights on the Magellan Clay telescope (Chile) - High-resolution (R = 30000) optical spectra üòÄüëâ<LINK> üëá <LINK>']",https://arxiv.org/abs/2104.04265,"We measure chemical abundances for over 20 elements of 15 N-rich field stars with high resolution ($R \sim 30000$) optical spectra. We find that Na, Mg, Al, Si, and Ca abundances of our N-rich field stars are mostly consistent with those of stars from globular clusters (GCs). Seven stars are estimated to have [Al/Fe$]>0.5$, which is not found in most GC ""first generation"" stars. On the other hand, $\alpha$ element abundances (especially Ti) could show distinguishable differences between in situ stars and accreted stars. We discover that one interesting star, with consistently low [Mg/Fe], [Si/Fe], [Ca/Fe], [Ti/Fe], [Sc/Fe], [V/Fe], and [Co/Fe], show similar kinematic and [Ba/Eu] as other stars from the dissolved dwarf galaxy ""$Gaia$-Sausage-Enceladus"". The $\alpha$-element abundances and the iron-peak element abundances of the N-rich field stars with metallicities $-1.25 \le {\rm [Fe/H]} \le -0.95$ show consistent values with Milky Way field stars rather than stars from dwarf galaxies, indicating that they were formed in situ. In addition, the neutron capture elements of N-rich field stars show that most of them could be enriched by asymptotic giant branch (AGB) stars with masses around $3 - 5\, M_{\odot}$. ",Chemical Tagging N-rich Field Stars with High-resolution Spectroscopy
160,1380322125903360002,1115946897431244800,Darsh J Shah,"[""What do we do when sources of information aren't in full agreement? Check out our new paper -&gt; Nutribullets Hybrid <LINK>.\nWe generate summaries of multiple documents which have varying degrees of consensus.  @BarzilayRegina @taolei15949106 @liliyu_lili <LINK>"", 'NAACL 2021, Camera Ready!']",https://arxiv.org/abs/2104.03465,"We present a method for generating comparative summaries that highlights similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health -- rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization -- while being equally fluent. ",Nutribullets Hybrid: Multi-document Health Summarization
161,1380099795612733442,888857981403582465,Xiaoxiang ZHU,"['Interested in a new #AI4EO task - multi-scene classification in single aerial images? \n\nWe @Zhu_XLab @ai4eo_de  are sharing a large-scale benchmark, called #MultiScene, composed of 100,000  high-resolution aerial images. Stay tuned!\n\nLink to paper: <LINK> <LINK>']",https://arxiv.org/abs/2104.02846,"Aerial scene recognition is a fundamental research problem in interpreting high-resolution aerial imagery. Over the past few years, most studies focus on classifying an image into one scene category, while in real-world scenarios, it is more often that a single image contains multiple scenes. Therefore, in this paper, we investigate a more practical yet underexplored task -- multi-scene recognition in single images. To this end, we create a large-scale dataset, called MultiScene, composed of 100,000 unconstrained high-resolution aerial images. Considering that manually labeling such images is extremely arduous, we resort to low-cost annotations from crowdsourcing platforms, e.g., OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and incorrectness, which introduce noise into image labels. To address this issue, we visually inspect 14,000 images and correct their scene labels, yielding a subset of cleanly-annotated images, named MultiScene-Clean. With it, we can develop and evaluate deep networks for multi-scene recognition using clean data. Moreover, we provide crowdsourced annotations of all images for the purpose of studying network learning with noisy labels. We conduct experiments with extensive baseline models on both MultiScene-Clean and MultiScene to offer benchmarks for multi-scene recognition in single images and learning from noisy labels for this task, respectively. To facilitate progress, we make our dataset and trained models available on this https URL ","MultiScene: A Large-scale Dataset and Benchmark for Multi-scene
  Recognition in Single Aerial Images"
162,1379357627134705665,1062005076204642305,Toby Ord,"['The Edges of Our Universe\n\nI‚Äôve just released a new paper exploring the largest-scale causal structure of our universe and its implications for what spacefaring civilisations could ever achieve.\n\n<LINK>\n1/ <LINK>', 'The paper answers questions such as:\n\n‚Ä¢ How large is the universe?\n‚Ä¢ What exactly is the observable universe?\n‚Ä¢ Will we ever be able to detect things outside it?\n‚Ä¢ If so, what are the ultimate limits of observability?\n‚Ä¶\n2/', '‚Ä¶\n‚Ä¢ Are there fundamental limits on how far we could travel through space? \n‚Ä¢ How far away does something have to be such that it is completely causally separate from us?\n‚Ä¢ How do these boundaries relate to each other?\n‚Ä¢ How do they change with time?\n3/', 'Some conclusions may be surprising:\n\n‚Ä¢ Many galaxies currently outside the observable universe will become observable.\n‚Ä¢ &lt;5% of the galaxies we can currently observe could ever be affected by us, and this is shrinking all the time.\n‚Ä¶\n4/', '‚Ä¶\n‚Ä¢ But we *can* affect some galaxies that are receding from us faster than the speed of light.\n‚Ä¢ There is a fundamental split in the longterm history of the universe in 150 billion years‚Äô time between an era of connection and an era of isolation.\n5/', ""I highlight a region that is as important as the observable universe, but which doesn‚Äôt yet have a standard name. I propose calling it the 'affectable universe'. This is the part of the universe we can causally affect ‚Äî and it is substantially smaller. \n6/"", 'It is like a time-reversed mirror image of the observable universe ‚Äî the difference between past and future, knowledge and action, sensors and effectors, what we can see and what can see us. Each year the observable universe grows, but the affectable universe shrinks.  \n7/', 'I also highlight other important parts of the universe, including the ‚Äòeventually observable universe‚Äô (what we could ever see from here) and the ‚Äòultimately observable universe‚Äô (what we could ever see).\n\nI explain them with spacetime diagrams, culminating in this one:\n8/ https://t.co/qFUR27rV60', 'Surprisingly only high school mathematics is needed to understand and calculate these limits, providing a rare situation where the cutting edge physics can be explained to a general audience without compromising its accuracy.\n9/', 'The paper is mainly an attempt to clarify and explain existing frontiers of knowledge on these topics. But along the way there are a few contributions I think are novel ‚Äî especially in drawing attention to the physical importance of some of the limits and their implications.\n10/', 'I started the paper in 2015, but paused it in 2016 to write The Precipice. I managed to include a little material in the final chapter, but it has been nice to return to this project, to show how the physics really works and what it implies.\n11/11\nhttps://t.co/xWNj5Sa8fg', '@geoffreyirving 5 light years. We lost about a billionth of the volume ‚Äî roughly 20 galaxies.', '@matheusd_tech @geoffreyirving @elonmusk The growing cold may in fact be good news ‚Äî see the section on the limits of computation.']",https://arxiv.org/abs/2104.01191,"This paper explores the fundamental causal limits on how much of the universe we can observe or affect. It distinguishes four principal regions: the affectable universe, the observable universe, the eventually observable universe, and the ultimately observable universe. It then shows how these (and other) causal limits set physical bounds on what spacefaring civilisations could achieve over the longterm future. ",The Edges of Our Universe
163,1378058929079345155,128608141,Thamme Gowda,"['Check out our new work; collaboration w/ Zhao Zhang, @chrismattmann, @jonathanmay:\nPaper: <LINK> \nTL;DR: Introducing our tools for machine translation; showing their use by creating a model that translates 500 languages to English!\nDemo: <LINK>', 'Our tools are:\n1. MTData: downloads datasets from many sources https://t.co/Srrkv0XLDw \n2. NLCodec: for byte-pair-encoding, dataset storage+retrieval on large datasets (using @ApacheSpark) https://t.co/Z6tZtCzJkx \n3. RTG: our NMT toolkit based on @PyTorch https://t.co/wHXLQ6Odr2', 'use cases for pre-trained MT models: the web and social media analysis and retrieval beyond English. Docker images are available so you may use them for translating *many* langs to English.\nhttps://t.co/OTqX6U4DYd \n\nAlso being integrated to @ApacheTika  https://t.co/1L9YvjFzMS', 'An additional use case (also one of my favorite areas of research) is improving the translation of low-resource languages. We have been impressed with the fine-tuning of our model in low resource settings. \nhttps://t.co/7JlSY5aIdv https://t.co/9RYuhA5Zof']",https://arxiv.org/abs/2104.00290,"While there are more than 7000 languages in the world, most translation research efforts have targeted a few high-resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec, and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages. ","Many-to-English Machine Translation Tools, Data, and Pretrained Models"
164,1389575126186926081,1284869663881670656,Javier Robledo Moreno,"['(1/2) Our latest work with @ageorges44 and Johannes Flick is now out on the arXiv! We study the possibility of reconstructing the band gap of materials from the knowledge of the electron density using #NeuralNetworks and supervised learning.\n\n<LINK> <LINK>', '(2/2) Of course, inspired by the Hohenberg-Kohn theorem. We also propose an improvement to the widely used Behler-Parrinello architecture with deep sets!']",https://arxiv.org/abs/2104.14351,"A remarkable consequence of the Hohenberg-Kohn theorem of density functional theory is the existence of an injective map between the electronic density and any observable of the many electron problem in an external potential. In this work, we study the problem of predicting a particular observable, the band gap of semiconductors and band insulators, from the knowledge of the local electronic density. Using state-of-the-art machine learning techniques, we predict the experimental band gaps from computationally inexpensive density functional theory calculations. We propose a modified Behler-Parrinello (BP) architecture that greatly improves the model capacity while maintaining the symmetry properties of the BP architecture. Using this scheme, we obtain band gaps at a level of accuracy comparable to those obtained with state of the art and computationally intensive hybrid functionals, thus significantly reducing the computational cost of the task. ",Machine learning band gaps from the electron density
165,1388232296310874114,543184346,Markus Freitag,"['Happy to announce our most recent paper: Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation <LINK> - We\nre-evaluated the outputs from WMT20 using in-context MQM annotations from professional translators. <LINK>', 'We propose a standard MQM scoring scheme appropriate for high-quality MT and use it to acquire ratings by professional translators for WMT20. We use them as a ‚Äúplatinum‚Äù standard for comparison to different human evaluation methodologies and crowd-worker evaluations.', 'We carry out the largest MQM research study for MT to date, scoring the outputs of top systems, including human generated translations for Chinese to English and English to German.', 'Our study shows that crowd-worker human evals (as conducted by WMT) have low\ncorrelation with MQM, and the resulting system-level rankings are quite different. This\ncasts doubt on previous conclusions made on the basis of crowd-worker evaluation, especially for high-quality MT. https://t.co/WvnokwCWst', 'We further come to the surprising finding that many automatic metrics, and in particular embedding-based ones, already outperform crowd-worker human evaluation. https://t.co/FV9T5Hyz0E', 'Unlike ratings acquired by crowd workers and ratings acquired by professional translators on simpler human evaluation methodologies, MQM labels acquired with professional translators show a large gap between the quality of human and machine generated translations.', 'Furthermore, we characterize the current error types in human and machine translations, highlighting which error types are responsible for the difference between the two. https://t.co/rynf8QcLl8', 'We release all ratings acquired in this study to encourage further research on this dataset\nfor both human evaluation and automatic evaluation. https://t.co/9Get0LoAbb', 'This is joint work with George Foster, @GrangierDavid, @vireshratnakar, Qijun Tan, Wolfgang Macherey @googleresearch #NLProc']",http://arxiv.org/abs/2104.14478,"Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research. ","Experts, Errors, and Context: A Large-Scale Study of Human Evaluation
  for Machine Translation"
166,1387996041723760640,422672164,Dr Michael Reidinger,"['Shapes of Milky-Way-Mass Galaxies with Self-Interacting Dark Matter\n\n""we study a suite of cosmological-baryonic simulations of Milky-Way (MW)-mass galaxies""\n<LINK>']",https://arxiv.org/abs/2104.14069,"Self-interacting dark matter (SIDM) models offer one way to reconcile inconsistencies between observations and predictions from collisionless cold dark matter (CDM) models on dwarf-galaxy scales. In order to incorporate the effects of both baryonic and SIDM interactions, we study a suite of cosmological-baryonic simulations of Milky-Way (MW)-mass galaxies from the Feedback in Realistic Environments (FIRE-2) project where we vary the SIDM self-interaction cross-section $\sigma/m$. We compare the shape of the main dark matter (DM) halo at redshift $z=0$ predicted by SIDM simulations (at $\sigma/m=0.1$, $1$, and $10$ cm$^2$ g$^{-1}$) with CDM simulations using the same initial conditions. In the presence of baryonic feedback effects, we find that SIDM models do not produce the large differences in the inner structure of MW-mass galaxies predicted by SIDM-only models. However, we do find that the radius where the shape of the total mass distribution begins to differ from that of the stellar mass distribution is dependent on $\sigma/m$. This transition could potentially be used to set limits on the SIDM cross-section in the MW. ",Shapes of Milky-Way-Mass Galaxies with Self-Interacting Dark Matter
167,1387929898099134473,2800204849,Andrew Gordon Wilson,"['What are Bayesian neural network posteriors really like? With high fidelity HMC, we study approximate inference quality, generalization, cold posteriors, priors, and more. \n<LINK>\nWith @Pavel_Izmailov, @sharadvikram, and Matthew D. Hoffman. 1/10 <LINK>', 'We show that Bayesian neural networks reassuringly provide good generalization, outperforming deep ensembles, standard training, and many approximate inference procedures, even with a single chain. 2/10 https://t.co/sSeW6NGSRX', 'However, we find that BNNs are surprisingly poor at OOD generalization, even worse than SGD, despite the popularity of approximate inference in this setting, and the relatively good performance of BNNs for OOD detection. 3/10 https://t.co/7EYqt1JIDe', 'Even though deep ensembles are often talked about as a ""non-Bayesian"" alternative to standard approximate inference, we find they approximate the HMC predictive distribution better than MFVI, and about as well as standard SGLD. 4/10 https://t.co/Wmbp4rFCnJ', 'There has been much attention lately on ""cold posteriors"" in BDL, where the posterior raised to a power 1/T with T&lt;1 can lead to better results. We see little evidence for a general cold posterior effect, which we find is largely due to data augmentation. 5/10 https://t.co/eXkRopCpRs', 'We explored Gaussian, mixture of Gaussian, and heavy-tailed logistic priors, which performed similarly, although the heavy-tailed priors did slightly better. We also found performance relatively insensitive to the scale of the Gaussian prior... 6/10 https://t.co/REonWDwHvJ', '...these results highlight the relative importance of the architecture compared to the distribution over weights in defining the induced prior over functions. Indeed, other work shows that even standard Gaussian priors have many useful properties: https://t.co/midasGNPYn. 7/10 https://t.co/yrDjvkO20m', 'We present many other results, including mixing in function space vs. weight space, posterior geometry and mode connecting paths, single chain vs. multi-chain...!  8/10 https://t.co/efyU00gjIz', 'Many of the results, both positive and negative for BDL, are contrary to conventional wisdom. 9/10 https://t.co/8naizY9p35', 'We worked hard to obtain these HMC samples, which we plan to release as a public resource, as a reference for evaluating more practical alternatives to HMC, and for researchers to explore their own questions around  approximate inference in BDL. 10/10']",https://arxiv.org/abs/2104.14421,"The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full-batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a ""cold posterior"" effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC methods can provide good generalization, they provide distinct predictive distributions from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference. ",What Are Bayesian Neural Network Posteriors Really Like?
168,1387310497591033856,928533461861675009,John Bamberg,"['In work with Jesse, Michael, and Gordon, we managed to find the first known examples of synchronising symmetry groups that are ""diagonal type"". We\'re still pretty chuffed about this: <LINK>']",https://arxiv.org/abs/2104.13355,"Every synchronising permutation group is primitive and of one of three types: affine, almost simple, or diagonal. We exhibit the first known example of a synchronising diagonal type group. More precisely, we show that $\mathrm{PSL}(2,q)\times \mathrm{PSL}(2,q)$ acting in its diagonal action on $\mathrm{PSL}(2,q)$ is separating, and hence synchronising, for $q=13$ and $q=17$. Furthermore, we show that such groups are non-spreading for all prime powers $q$. ",Synchronising primitive groups of diagonal type exist
169,1387216699976347649,14382917,Emil Lenc,"['Nice work by @HIgalaxies - another ORC to add to our collection! Hopefully, as we find more of these, we can eventually work out what these things actually are. <LINK> @CSIRO_ATNF <LINK>']",https://arxiv.org/abs/2104.13055,"We present the discovery of another Odd Radio Circle (ORC) with the Australian Square Kilometre Array Pathfinder (ASKAP) at 944 MHz. The observed radio ring, ORC J0102-2450, has a diameter of ~70 arcsec or 300 kpc, if associated with the central elliptical galaxy DES J010224.33-245039.5 (z ~ 0.27). Considering the overall radio morphology (circular ring and core) and lack of ring emission at non-radio wavelengths, we investigate if ORC J0102-2450 could be the relic lobe of a giant radio galaxy seen end-on or the result of a giant blast wave. We also explore possible interaction scenarios, for example, with the companion galaxy, DES J010226.15-245104.9, located in or projected onto the south-eastern part of the ring. We encourage the search for further ORCs in radio surveys to study their properties and origin. ","Discovery of a new extragalactic circular radio source with ASKAP: ORC
  J0102-2450"
170,1387109048345001987,1008944276431036416,Boris Ivanovic,"['New paper on arXiv!! In it, we propose a traj. forecasting method (HAICU) that propagates semantic uncertainty from upstream perception through the model, releasing a new dataset for investigating Perceptual Uncertainty in Prediction (PUP) along the way! <LINK> <LINK>', 'The results of a great collaboration between @StanfordASL, @StanfordEng, and @ToyotaResearch! In particular, with researchers from the machine learning research team: Kuan-Hui Lee, @ptokmakov, @wulfebw, @rowantmc, @adnothing, as well as the one and only @MarcoPavoneSU!']",https://arxiv.org/abs/2104.12446,"Reasoning about the future behavior of other agents is critical to safe robot navigation. The multiplicity of plausible futures is further amplified by the uncertainty inherent to agent state estimation from data, including positions, velocities, and semantic class. Forecasting methods, however, typically neglect class uncertainty, conditioning instead only on the agent's most likely class, even though perception models often return full class distributions. To exploit this information, we present HAICU, a method for heterogeneous-agent trajectory forecasting that explicitly incorporates agents' class probabilities. We additionally present PUP, a new challenging real-world autonomous driving dataset, to investigate the impact of Perceptual Uncertainty in Prediction. It contains challenging crowded scenes with unfiltered agent class probabilities that reflect the long-tail of current state-of-the-art perception systems. We demonstrate that incorporating class probabilities in trajectory forecasting significantly improves performance in the face of uncertainty, and enables new forecasting capabilities such as counterfactual predictions. ","Heterogeneous-Agent Trajectory Forecasting Incorporating Class
  Uncertainty"
171,1387078221707939841,1062741167371141123,Neel Guha,"['Just how much does domain-specific pretraining help for legal NLP tasks? We studied this by creating ‚ÄúCaseHOLD‚Äù -- a new benchmark for precedential reasoning in law. \n\nPaper: <LINK>\nBlog: <LINK>\n1/6 <LINK>', 'Strangely, existing work has noticed that pretraining BERT on legal opinions produces only marginal improvements for legal NLP benchmarks. We hypothesized that existing benchmarks were either too easy, or had language that was too dissimilar from that found in legal opinions.\n2/6', 'We thus created CaseHOLD: a benchmark of 53,000+ multiple choice questions corresponding to the task of determining the appropriate holding to cite for a legal argument. As lawyers may recognize, CaseHOLD replicates a task essential to the practice of law.\n3/6 https://t.co/DuZc5rljlI', 'In evaluations, we found that legal domain specific pretraining (over 3,446,187 judicial opinions) produced substantial performance improvements over CaseHOLD, but only marginal improvements for other benchmarks! \n4/6 https://t.co/Xq1MiazMMI', 'If you‚Äôre interested in learning more about the CaseHOLD dataset, our pre-training process, or our analysis of the Legal-BERT model -- check out the full paper!  You can also download our models and the CaseHOLD dataset from here: https://t.co/HDxuQK0D9r\n5/6', ""This work was done by @lucia__zheng, Brandon Anderson, @PeterHndrsn, and @DanHo1. Special thanks to the following for invaluable help. Please reach out if you have questions or feedback -- we're very excited about further work in this direction!\n6/6 https://t.co/FScxN7gbCA""]",https://arxiv.org/abs/2104.08671,"While self-supervised learning has made rapid advances in natural language processing, it remains unclear when researchers should engage in resource-intensive domain-specific pretraining (domain pretraining). The law, puzzlingly, has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique. We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help. To address this, we first present CaseHOLD (Case Holdings On Legal Decisions), a new dataset comprised of over 53,000+ multiple choice questions to identify the relevant holding of a cited case. This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective (F1 of 0.4 with a BiLSTM baseline). Second, we assess performance gains on CaseHOLD and existing legal NLP datasets. While a Transformer architecture (BERT) pretrained on a general corpus (Google Books and Wikipedia) improves performance, domain pretraining (using corpus of approximately 3.5M decisions across all courts in the U.S. that is larger than BERT's) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD (gain of 7.2% on F1, representing a 12% improvement on BERT) and consistent performance gains across two other legal tasks. Third, we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus: the level of performance increase in three legal tasks was directly tied to the domain specificity of the task. Our findings inform when researchers should engage resource-intensive pretraining and show that Transformer-based architectures, too, learn embeddings suggestive of distinct legal language. ","When Does Pretraining Help? Assessing Self-Supervised Learning for Law
  and the CaseHOLD Dataset"
172,1385636121217019920,935236213,Gavin Lamb,"['Submitted paper.\nHere we show how the shape of the jet break as measured by a smoothly broken power law function can contain information about the lateral structure of a GRB jet.\n\nWe use this method for 20 GRB afterglows and find a ~50/50 split!\nü§î \n\n<LINK>', 'Some of the data has gaps or variability that could bias the fits, however, 5 GRBs are well sampled.\nThese have a 3:2 split smooth-edged:sharp-edged\n\nI wonder, do some GRB jets have more/less lateral structure than others? What could cause this?', 'The 50/50 split is between smooth-edged jet structures, such as Gaussian or powerlaw. And sharp-edged jet structures, such as a top-hat or two-component (spine and sheath).']",https://arxiv.org/abs/2104.11099,"We investigate the shape of the jet break in within-beam gamma-ray burst (GRB) optical afterglows for various lateral jet structure profiles. We consider cases with and without lateral spreading and a range of inclinations within the jet core half-opening angle, $\theta_c$. We fit model and observed afterglow lightcurves with a smoothly-broken power-law function with a free-parameter $\kappa$ that describes the sharpness of the break. We find that the jet break is sharper ($\kappa$ is greater) when lateral spreading is included than in the absence of lateral spreading. For profiles with a sharp-edged core, the sharpness parameter has a broad range of $0.1\lesssim\kappa\lesssim4.6$, whereas profiles with a smooth-edged core have a narrower range of $0.1\lesssim\kappa\lesssim2.2$ when models both with and without lateral spreading are included. For sharp-edged jets, the jet break sharpness depends strongly on the inclination of the system within $\theta_c$, whereas for smooth-edged jets, $\kappa$ is more strongly dependent on the size of $\theta_c$. Using a sample of 20 GRBs we find nine candidate smooth-edged jet structures and eight candidate sharp-edged jet structures, while the remaining three are consistent with either. The shape of the jet break, as measured by the sharpness parameter $\kappa$, can be used as an initial check for the presence of lateral structure in within-beam GRBs where the afterglow is well-sampled at and around the jet-break time. ",GRB jet structure and the jet break
173,1385396035195858944,3377160202,Djuna Croon,"['New paper! \n\nNon-perturbative methods for false vacuum decay\n<LINK>\nwith the amazing Eleanor Hall (@quarkygirl) and Hitoshi Muruyama (@sleptogenesis) \n\nWe propose a new (non-perturbative!) technique to calculate false vacuum decay rates. Important, because...', ""...accurate false vacuum decay calculations are needed to predict the resulting gravitational wave spectra. \n\nI'll try to give a brief explanation below, but I also highly recommend Nell's excellent slides on the topic: https://t.co/PlOhCeZI4H at #DarkSectorRainbow last month..."", '...So, accurately calculating the gravitational wave spectrum from a first order phase transition is a big challenge.\n\nExisting methods struggle particularly with strong coupling. Why?\n\nThe usual false vacuum decay formalism is well-defined for tree-level bounces, but...', ""...for radiatively induced phase-transitions, it needs modifications. A related issue is that an all-orders calculation of the effective action is manifestly convex. \n\nWhat does that mean? A convex potential doesn't have more than one minimum -&gt; no first order phase transition..."", '...""coarse-graining"" in momentum scale is a useful solution. However, the accuracy of coarse graining usually depends on a large ratio of scales or a weak coupling. \n\nWe propose an alternative, where we enforce locality in field space (see fig) rather than in momentum space... https://t.co/peSC3QKVDP', '...that can be done in the language of the functional renormalization group, as we show. \n\nMoreover, we work out a simple example, which we compare to the result found in other methods. As expected, the difference creep in at stronger coupling... https://t.co/tY9KJU24Mo', ""...We hope to develop this method further, and eventually study things like confinement / chiral symmetry breaking and the resulting gravitational wave spectra. I can't wait to learn more. \n\nA big thank you to my wonderful collaborators! It has been an absolute joy ‚ù§Ô∏è"", 'Oops, MurAyama, apologies!!']",https://arxiv.org/abs/2104.10687,"We propose a simple non-perturbative formalism for false vacuum decay using functional methods. We introduce the quasi-stationary effective action, a bounce action that non-perturbatively incorporates radiative corrections and is robust to strong couplings. The quasi-stationary effective action obeys an exact flow equation in a modified functional renormalization group with a motivated regulator functional. We demonstrate the use of this formalism in a simple toy model and compare our result with that obtained in perturbation theory. ",Non-perturbative methods for false vacuum decay
174,1385300649915555842,301377645,Ga≈°per Begu≈°,"['A new preprint on interpreting individual convolutional layers in CNNs trained on speech:\n\n<LINK>\n\nWe propose a technique to interpret and visualize intermediate layers and observe causal effect between linguistically meaningful latent variables and conv.layers. <LINK>', 'The proposed technique allows acoustic analysis of intermediate layers: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information.', 'Paper with Alan Zhou, an amazing undergraduate student @UCBerkeley. \n\n#ML #NLProc #speech']",https://arxiv.org/abs/2104.09489,"This paper presents a technique to interpret and visualize intermediate layers in CNNs trained on raw speech data in an unsupervised manner. We argue that averaging over feature maps after ReLU activation in each convolutional layer yields interpretable time-series data. By linearly interpolating individual latent variables to marginal levels outside of the training range, we further argue that we are able to observe a causal relationship between individual latent variables that encode linguistically meaningful units and activations in intermediate convolutional layers. The proposed technique allows acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. Observing the causal effect between linear interpolation and the resulting changes in intermediate layers can reveal how individual variables get transformed into spikes in activation in intermediate layers.We train and probe internal representations on two models -- a bare WaveGAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). The proposal also allows testing of higher-level morphophonological alternations such as reduplication (copying). In short, using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in different convolutional layers. ","Interpreting intermediate convolutional layers of CNNs trained on raw
  speech"
175,1385242417012613123,981250608190644224,Lisa Bauer,"['We\'ll present our #EACL2021 paper ""Identify, Align, Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"" on Apr23!üôÇ \nJoin us: Oral/QA session Zoom8B 8-9am, Gather3A 9-11am EDT\ncc @mohitban47\n\n<LINK>\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2104.10193,"Integrating external knowledge into commonsense reasoning tasks has shown progress in resolving some, but not all, knowledge gaps in these tasks. For knowledge integration to yield peak performance, it is critical to select a knowledge graph (KG) that is well-aligned with the given task's objective. We present an approach to assess how well a candidate KG can correctly identify and accurately fill in gaps of reasoning for a task, which we call KG-to-task match. We show this KG-to-task match in 3 phases: knowledge-task identification, knowledge-task alignment, and knowledge-task integration. We also analyze our transformer-based KG-to-task models via commonsense probes to measure how much knowledge is captured in these models before and after KG integration. Empirically, we investigate KG matches for the SocialIQA (SIQA) (Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0 (Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al., 2019a), ConceptNet (Speer et al., 2017), and an automatically constructed instructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we are able to demonstrate that ATOMIC, an event-inference focused KG, is the best match for SIQA and MCScript2.0, and that the taxonomic ConceptNet and WikiHow-based KGs are the best matches for PIQA across all 3 analysis phases. We verify our methods and findings with human evaluation. ","Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense
  Reasoning Tasks"
176,1385232534586413059,382961853,Jo Dunkley,"[""New @ACT_Pol paper led by Sigurd Naess today on the Arxiv <LINK>  - looking for Planet 9. So, no, Sigurd et al didn't find it, but pretty fun that we can look for it with a telescope designed to study signals from the Big Bang! #NSFfunded"", 'Why does this work? Well, to look for light-patterns from the Big Bang we survey millimeter-wave light coming from more than half the sky. A Planet 9 lurking in the distant parts of the Solar System would be a bit warm, and would also send out millimeter light.', 'Since it is planet-warmth and not reflected sunlight that @ACT_Pol looks for, the sensitivity goes down as distance squared, not distance to power 4. Here are flux limits from Naess et al (bold red), compared to predictions for a 5-10 Earth-mass Planet 9. https://t.co/mLO5JmHOdm', 'So, using these new data @ACT_Pol can exclude around 10-20% of the expected Planet 9 parameter space. With new data from @ACT_Pol and then @SimonsObs, we‚Äôll get more sensitive and can look deeper, for this and other Solar System objects! #NSFfunded']",https://arxiv.org/abs/2104.10264,"We use Atacama Cosmology Telescope (ACT) observations at 98 GHz (2015--2019), 150 GHz (2013--2019) and 229 GHz (2017--2019) to perform a blind shift-and-stack search for Planet 9. The search explores distances from 300 AU to 2000 AU and velocities up to 6.3 arcmin per year, depending on the distance. For a 5 Earth-mass Planet 9 the detection limit varies from 325 AU to 625 AU, depending on the sky location. For a 10 Earth-mass planet the corresponding range is 425 AU to 775 AU. The search covers the whole 18,000 square degrees of the ACT survey, though a slightly deeper search is performed for the parts of the sky consistent with Planet 9's expected orbital inclination. No significant detections are found, which is used to place limits on the mm-wave flux density of Planet 9 over much of its orbit. Overall we eliminate roughly 17% and 9% of the parameter space for a 5 and 10 Earth-mass Planet 9 respectively. We also provide a list of the 10 strongest candidates from the search for possible follow-up. More generally, we exclude (at 95% confidence) the presence of an unknown Solar system object within our survey area brighter than 4--12 mJy (depending on position) at 150 GHz with current distance $300 \text{ AU} < r < 600 \text{ AU}$ and heliocentric angular velocity $1.5'/\text{yr} < v \cdot \frac{500 \text{ AU}}{r} < 2.3'\text{yr}$, corresponding to low-to-moderate eccentricities. These limits worsen gradually beyond 600 AU, reaching 5--15 mJy by 1500 AU. ",The Atacama Cosmology Telescope: A search for Planet 9
177,1385194253199028226,1318905364851679232,Andr√©s Chacoma,"['New preprint out today ""Stochastic model for football\'s collective dynamics"" (<LINK>). We surveyed a database containing body-sensors traces, where we observed statistical patterns that we used to propose a stochastic model for the players\' motion in the field. <LINK>']",http://arxiv.org/abs/2104.10272,"In this paper, we study collective interaction dynamics emerging in the game of football-soccer. To do so, we surveyed a database containing body-sensors traces measured during three professional football matches, where we observed statistical patterns that we used to propose a stochastic model for the players' motion in the field. The model, which is based on linear interactions, captures in good approximation the spatiotemporal dynamics of a football team. Our theoretical framework, therefore, becomes an effective analytical tool to uncover the underlying cooperative mechanisms behind the complexity of football plays. Moreover, we showed that it can provide handy theoretical support for coaches to evaluate teams' and players' performances in both training sessions and competitive scenarios. ",Stochastic model for football's collective dynamics
178,1384820227368792074,1379812899670085635,Dror Moran,"['New paper: Deep Permutation Equivariant Structure from Motion.\n \nWe propose an architecture that, given a set of point tracks in multiple images of a scene, recovers both the camera params and 3D structure by minimizing an unsupervised reprojection loss\n\n<LINK> <LINK>', 'Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. https://t.co/wIJn2v7gJk', 'Our architecture supports two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments indicate that our method accurately recovers pose and structure, on par with state-of-the-art methods. Here are several examples: (cameras in red) https://t.co/lpWqyJ3Clk']",https://arxiv.org/abs/2104.06703,"Existing deep methods produce highly accurate 3D reconstructions in stereo and multiview stereo settings, i.e., when cameras are both internally and externally calibrated. Nevertheless, the challenge of simultaneous recovery of camera poses and 3D scene structure in multiview settings with deep networks is still outstanding. Inspired by projective factorization for Structure from Motion (SFM) and by deep matrix completion techniques, we propose a neural network architecture that, given a set of point tracks in multiple images of a static scene, recovers both the camera parameters and a (sparse) scene structure by minimizing an unsupervised reprojection loss. Our network architecture is designed to respect the structure of the problem: the sought output is equivariant to permutations of both cameras and scene points. Notably, our method does not require initialization of camera parameters or 3D point locations. We test our architecture in two setups: (1) single scene reconstruction and (2) learning from multiple scenes. Our experiments, conducted on a variety of datasets in both internally calibrated and uncalibrated settings, indicate that our method accurately recovers pose and structure, on par with classical state of the art methods. Additionally, we show that a pre-trained network can be used to reconstruct novel scenes using inexpensive fine-tuning with no loss of accuracy. ",Deep Permutation Equivariant Structure from Motion
179,1384772023943979008,3232627976,Khyati Malhan,"['Paper day! We study the LMS-1 stellar stream of the Milky Way using #GaiaEDR3. Both the structural and orbital properties of LMS-1 make it a very interesting fossil remnant of the early formation of our Galaxy !! \nw/ @Anke_NL, @nfmartin1980 and others.\n<LINK> <LINK>', 'LMS-1 dwarf galaxy stream has a very small orbital radius (r_apo~20kpc, r_peri~10kpc) compared to other dwarf galaxy streams, and even intact dwarf galaxies, of the Milky Way. If the Galaxy grew ""inside-out"", this implies that LMS-1 was possibly accreted early (~&gt;8-10 Gyrs ago). https://t.co/LZj33YFK2d', 'The previous figure also shows that the globular clusters NGC 5053 and NGC 5024 have very similar orbits as LMS-1. Moreover, the remarkable overlap in the orbits of NGC 5053 and LMS-1 further suggests that NGC 5053 was probably the ""nuclear"" star cluster of LMS-1\'s parent galaxy.', 'In addition to the orbit overlap, even the metallicities ([Fe/H]) of NGC 5024 and NGC 5053 overlap with that of LMS-1. Additionally, this figure also highlights Pal 5 and stream ""Indus"", as these objects also fall within the [Fe/H] range of LMS-1. https://t.co/blsvSpMVqA', ""Interestingly, all of these objects (NGC 5053, NGC 5024, Palomar 5 and Indus) overlap with LMS-1 in the energy-action (E,J) space. This strong orbit-[Fe/H] coincidence indicates that at least NGC 5053, NGC 5024 and Indus were possibly accreted inside the LMS-1's parent galaxy. https://t.co/FvMw3Cc2Wl"", 'LMS-1 has a very large velocity dispersion, both along the ""tangential"" and the ""line-of-sight"" directions (see figure attached). This velocity dispersion measurement is even higher than that of the massive Sagittarius stream (vdisp_Sgr~13 kms-1) https://t.co/Z56Cp1zsPN', 'Putting analysis in perspective: Milky Way is populated by 3 dwarf galaxy streams that lie along ""prograde"" and very ""polar"" orbits: 1) LMS-1 (occupying the inner ~10-20 kpc of the halo), 2) Cetus (intermediate ~25-35 kpc), and 3) Sagittarius (outer ~15-100 kpc). See figure! https://t.co/YedwuFdLEN']",https://arxiv.org/abs/2104.09523,"Stellar streams produced from dwarf galaxies provide direct evidence of the hierarchical formation of the Milky Way. Here, we present the first comprehensive study of the ""LMS-1"" stellar stream, that we detect by searching for wide streams in the Gaia EDR3 dataset using the STREAMFINDER algorithm. This stream was recently discovered by Yuan et al. (2020). We detect LMS-1 as a $60\deg$ long stream to the north of the Galactic bulge, at a distance of $\sim 20$ kpc from the Sun, together with additional components that suggest that the overall stream is completely wrapped around the inner Galaxy. Using spectroscopic measurements from LAMOST, SDSS and APOGEE, we infer that the stream is very metal poor (${\rm \langle [Fe/H]\rangle =-2.1}$) with a significant metallicity dispersion ($\sigma_{\rm [Fe/H]}=0.4$), and it possesses a large radial velocity dispersion (${\rm \sigma_v=20 \pm 4\,km\,s^{-1}}$). These estimates together imply that LMS-1 is a dwarf galaxy stream. The orbit of LMS-1 is close to polar, with an inclination of $75\deg$ to the Galactic plane. Both the orbit and metallicity of LMS-1 are remarkably similar to the globular clusters NGC 5053, NGC 5024 and the stellar stream ""Indus"". These findings make LMS-1 an important contributor to the stellar population of the inner Milky Way halo. ",Evidence of a dwarf galaxy stream populating the inner Milky Way Halo
180,1384507239482920962,959316810448318464,Theo O'Neill,"['My first paper is on arXiv today!  We used ALMA observations to study dense ‚Äúcores‚Äù of dust and gas within nearby protoclusters.  If you‚Äôre interested in star formation, the core mass function, and/or fitting power-laws, take a look!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2104.08861,"The stellar initial mass function (IMF) is fundamental for many areas of astrophysics, but its origin remains poorly understood. It may be inherited from the core mass function (CMF) or arise as a result of more chaotic, competitive accretion. Dense, gravitationally bound cores are seen in molecular clouds and some observations have suggested that the CMF is similar in shape to the IMF, though translated to higher masses by a factor of $\sim3$. Here we measure the CMF in 28 dense clumps within 3.5 kpc that are likely to be central regions of massive protoclusters, observed via $1.3\:{\rm{mm}}$ dust continuum emission by the ALMAGAL project. We identify 222 cores using the dendrogram algorithm with masses ranging from 0.04 to $252\:M_{\odot}$. We apply completeness corrections for flux and number recovery, estimated from core insertion and recovery experiments. At higher masses, the final derived CMF is well described by a single power law of the form $dN/d\:{\textrm{log}}\:M\propto\:M^{-\alpha}$ with $\alpha\simeq0.94\pm0.08$. However, we find evidence of a break in this power-law behavior between $\sim5$ and $15\:M_{\odot}$, which is, to our knowledge, the first time such a break has been found in distant ($\gtrsim 1$ kpc) regions by ALMA. We compare this massive protocluster CMF with those derived using the same methods in the G286 protocluster and a sample of Infrared Dark Clouds. The massive protocluster CMF is significantly different, i.e., containing more massive cores, which is a potential indication of the role of environment on the CMF and IMF. ","The Core Mass Function Across Galactic Environments. III. Massive
  Protoclusters"
181,1383906743307051010,1036512677051412480,Claudio Gallicchio,"['new paper out! üòçüòç\nwe study deep randomized graph neural networks based on stable dynamics &amp; graph pooling!\naccepted for journal publication, here in pre-print on arxiv: <LINK>\n\nspecial thanks to @FilippoMariaBi1, a great co-author for this work! <LINK>', '@luigidisotto @FilippoMariaBi1 thanks!']",https://arxiv.org/abs/2104.04710,"We propose a deep Graph Neural Network (GNN) model that alternates two types of layers. The first type is inspired by Reservoir Computing (RC) and generates new vertex features by iterating a non-linear map until it converges to a fixed point. The second type of layer implements graph pooling operations, that gradually reduce the support graph and the vertex features, and further improve the computational efficiency of the RC-based GNN. The architecture is, therefore, pyramidal. In the last layer, the features of the remaining vertices are combined into a single vector, which represents the graph embedding. Through a mathematical derivation introduced in this paper, we show formally how graph pooling can reduce the computational complexity of the model and speed-up the convergence of the dynamical updates of the vertex features. Our proposed approach to the design of RC-based GNNs offers an advantageous and principled trade-off between accuracy and complexity, which we extensively demonstrate in experiments on a large set of graph datasets. ",Pyramidal Reservoir Graph Neural Network
182,1383049651792842752,73090248,Prof. Danushka Bollegala,"['We propose a novel social bias evaluation measure, All Unmasked Likelihood (AUL), for evaluating masked language model biases. <LINK> w/ @MasahiroKaneko_   We show that AUL is better suited for evaluating MLM biases using CrowsPairs and StereoSet datasets (1/n)', 'AUL differ from prior proposals which mask modified or unmodified (https://t.co/L5vswg3o17, https://t.co/23Dv8Jh4gZ) tokens in example sentences  in that it predicts all tokens in a sentence. This avoids freqency-related biases in psuedo log-likelihood computation. (2/n)', 'We evaluate social biases in BERT, RoBERTa and ALBERT in the paper. (3/n) https://t.co/QNeS3I0gub', 'We also propose a variant of AUL, which uses attention over tokens to emphasize salient tokens in a sentence (AULA). Both AUL and AULA report higher agreement with human bias ratings in the SeteroSet dataset.  (4/n) https://t.co/99JeswRIa9', 'Would like to thank @sleepinyourhat and @sivareddyg research groups for creating the CrowsPairs and StereoSet datasets, without which this work would not be possible! (5/n=5)']",https://arxiv.org/abs/2104.07496,"Masked Language Models (MLMs) have shown superior performances in numerous downstream NLP tasks when used as text encoders. Unfortunately, MLMs also demonstrate significantly worrying levels of social biases. We show that the previously proposed evaluation metrics for quantifying the social biases in MLMs are problematic due to following reasons: (1) prediction accuracy of the masked tokens itself tend to be low in some MLMs, which raises questions regarding the reliability of the evaluation metrics that use the (pseudo) likelihood of the predicted tokens, and (2) the correlation between the prediction accuracy of the mask and the performance in downstream NLP tasks is not taken into consideration, and (3) high frequency words in the training data are masked more often, introducing noise due to this selection bias in the test cases. To overcome the above-mentioned disfluencies, we propose All Unmasked Likelihood (AUL), a bias evaluation measure that predicts all tokens in a test case given the MLM embedding of the unmasked input. We find that AUL accurately detects different types of biases in MLMs. We also propose AUL with attention weights (AULA) to evaluate tokens based on their importance in a sentence. However, unlike AUL and AULA, previously proposed bias evaluation measures for MLMs systematically overestimate the measured biases, and are heavily influenced by the unmasked tokens in the context. ",Unmasking the Mask -- Evaluating Social Biases in Masked Language Models
183,1383037700270350341,154374002,Prasanna,"['üì¢Yes! Sometimes the translations have to assume the input word-order to be as is, despite being ungrammatical. We propose a suite of perturbations and metrics to verify if a NMT system is robust beyond necessary.  @koustuv Joelle @adinamwilliams.#NLProc <LINK> <LINK>', 'We observe that NMT systems are generally more robust than faithful. A positive sign for tasks where assuming the input is prone to language errors but imagine the adverse effects such systems can have on poetry ! üò®üò® https://t.co/jF6JCmXBiQ', 'The standard performance of NMT systems has a strong correlation with the proposed robustness metric üôÜüëç . But, has a weak correlation with the faithfulness metricüëé. https://t.co/BSTalISSGV', 'We observed the models to be able to stay robust more so on the perturbations that focus on perturbing Part-of-Speech Tags indicating that the systems find it easier to fix the sentence when the majority of the structure is preserved. üòéüòé https://t.co/Azp0MYxusJ', 'Also we observed that models were able to stay robust mostly when the source sentences are shorter in length.ü§î And, the models are able to stay faithful in longer sentences ! This could be largely due to the higher fraction of n-grams preserved in perturbed longer sentences.ü§î https://t.co/PFaV0iMpAZ', 'Overall, our analysis suggests that over-focusing on accuracy and robustness may limit richer development and broader usefulness of NMT systems.üåÖ https://t.co/WAnfLacd3P', 'Joint work with @koustuvsinha @adinamwilliams Joelle Pineau. Thanks to @facebookai @Mila_Quebec @ComputeCanada']",https://arxiv.org/abs/2104.07623,"Rapid progress in Neural Machine Translation (NMT) systems over the last few years has been driven primarily towards improving translation quality, and as a secondary focus, improved robustness to input perturbations (e.g. spelling and grammatical mistakes). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness and faithfulness, focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure the effects on the target side in terms of both robustness and faithfulness. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case. ",Sometimes We Want Translationese
184,1383026419098284035,1334866054573666304,Gregor Geigle,"['Check out our new publication:\n‚ÄúTWEAC: Transformer with Extendable QA Agent Classifiers‚Äù \n\nWe propose a meta-QA to find the most fitting QA systems within a pool for a given question.\n\nw/ @Nils_Reimers @arueckle IGurevych \n<LINK>\nCode &amp; data released as well <LINK>', 'A single QA system is unlikely to answer all possible questions users can ask. An ensemble of specialized QA agents can cover more questions but we need to select the relevant agents for each question because querying each model is far too expensive. https://t.co/kMEoAtJkhA', 'We evaluate both supervised and unsupervised approaches for the task and show that our supervised TWEAC performs best overall. TWEAC is easily extendable by adding additional classification heads.\nOur approaches are sample efficient (1k per agent) and scalable (&gt;100 agents). https://t.co/gLyAnQ5Gk2', 'You can find the code &amp; data here: https://t.co/BPSZVq8Aj0']",https://arxiv.org/abs/2104.07081,"Question answering systems should help users to access knowledge on a broad range of topics and to answer a wide array of different questions. Most systems fall short of this expectation as they are only specialized in one particular setting, e.g., answering factual questions with Wikipedia data. To overcome this limitation, we propose composing multiple QA agents within a meta-QA system. We argue that there exist a wide range of specialized QA agents in literature. Thus, we address the central research question of how to effectively and efficiently identify suitable QA agents for any given question. We study both supervised and unsupervised approaches to address this challenge, showing that TWEAC -- Transformer with Extendable Agent Classifiers -- achieves the best performance overall with 94% accuracy. We provide extensive insights on the scalability of TWEAC, demonstrating that it scales robustly to over 100 QA agents with each providing just 1000 examples of questions they can answer. Our code and data is available: this https URL ",TWEAC: Transformer with Extendable QA Agent Classifiers
185,1383009233403273216,2785337469,Sebastian Ruder,"['XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation\n\nWe examine the state of multilingual benchmarking and propose an improved benchmark covering more challenging tasks, including a diagnostic and evaluation suite to inform future work.\n\n<LINK> <LINK>', 'We observe that recent multilingual models have particularly improved on cross-lingual retrieval tasks (benefitting from parallel data, task-specific fine-tuning, and new pre-training objectives). Improvements on other tasks have been modest in comparison. https://t.co/dtyaQVYQOq', 'XTREME-R incorporates new tasks including language-agnostic retrieval, a more challenging version of retrieval where models need to retrieve sentences from a multilingual pool. https://t.co/DDqwIQ2QDq', 'As it‚Äôs less clear how to make progress on the other tasks like QA and structured prediction, we aim to develop resources to make the diagnosis and fine-grained evaluation of multilingual models easier.', 'To this end, we create a multilingual version of CheckList by translating English QA test templates to 49 languages. We encourage future effort to focus on participatory research and working with native speakers to create such template-based test cases in their own language. https://t.co/A8G4hvKmIN', 'We additionally extend ExplainaBoard to the languages and tasks in XTREME-R. ExplainaBoard allows us to break down a model‚Äôs performance based on task-specific attributes, shedding light on the settings where models struggle.\n\nhttps://t.co/lYp5oVAAB3', 'Multilingual models differ drastically in the nature and scale of their pre-training data. To make it easier to select an appropriate model, we aim to create a meta-data rich, interactive leaderboard that includes information about the scale and efficiency of such models.', 'In our evaluation, larger models such as mT5 and XLM-R perform better as expected but gains are again smaller on QA and structured prediction tasks. Particularly for language-agnostic retrieval, there is still a lot of room for improvement. https://t.co/1b44fTi0ad', 'On the multilingual CheckList tests, XLM-R demonstrates much more robust cross-lingual understanding capabilities compared to mBERT. Nevertheless, it performs worse on tests in low-resource languages and in languages with non-Latin scripts. https://t.co/fqKnkxQiFa', 'The Explainaboard analysis can shed light on more fine-grained model behaviour. For instance, ERNIE-M struggles to answer relatively frequent abstract question types (what and how) and XLM-R is better at dealing with longer answers and questions in English. https://t.co/vkbbSzgw3m', 'There are a lot of interesting directions and challenges ahead. In order to make progress on the hardest tasks, we believe that future models will not just need to leverage parallel data but develop cross-lingual inductive biases that are relevant for downstream tasks.', 'Overall, we hope that our improved benchmark and evaluation resources will be able to inform and support future work in this area and enable the community to gain a better understanding of multilingual models.', 'We are currently working on making the data, evaluation resources, and leaderboard available at https://t.co/sY3uknM4HX and https://t.co/wDocdGnz4t.', 'This is joint work with @noahconst, Jan Botha, @adisid01, @orf_bnw, @JinlanFu, @stefan_fee, @JunjieHu12, @gneubig, and @melvinjohnsonp.', ""@NirantK We're just in the process of cleaning up the code and getting necessary approval. Will follow-up here once I have an update.""]",https://arxiv.org/abs/2104.07412,"Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite (MultiCheckList) and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models. The leaderboard and code for XTREME-R will be made available at this https URL and this https URL respectively. ",XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation
186,1382970840535425024,977906884886827008,Marcos Mari√±o,"['Peacock patterns are everywhere! In my last paper with Jie Gu, we study these patterns in topological string theory. They can be used to define new integer invariants of the topological string.\n<LINK> <LINK>']",https://arxiv.org/abs/2104.07437,"Topological string theory near the conifold point of a Calabi-Yau threefold gives rise to factorially divergent power series which encode the all-genus enumerative information. These series lead to infinite towers of singularities in their Borel plane (also known as ""peacock patterns""), and we conjecture that the corresponding Stokes constants are integer invariants of the Calabi-Yau threefold. We calculate these Stokes constants in some toric examples, confirming our conjecture and providing in some cases explicit generating functions for the new integer invariants, in the form of q-series. Our calculations in the toric case rely on the TS/ST correspondence, which promotes the asymptotic series near the conifold point to spectral traces of operators, and makes it easier to identify the Stokes data. The resulting mathematical structure turns out to be very similar to the one of complex Chern-Simons theory. In particular, spectral traces correspond to state integral invariants and factorize in holomorphic/anti-holomorphic blocks. ",Peacock patterns and new integer invariants in topological string theory
187,1382960353148026883,301178769,Preslav Nakov,"['New #NLProc paper preprint üìù, in which we study the Role of Context in Detecting Previously Fact-Checked Claims (Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov) <LINK> #factchecking #fakenews @firojalam04 @giodsm <LINK>']",https://arxiv.org/abs/2104.07423,"Recent years have seen the proliferation of disinformation and fake news online. Traditional approaches to mitigate these issues is to use manual or automatic fact-checking. Recently, another approach has emerged: checking whether the input claim has previously been fact-checked, which can be done automatically, and thus fast, while also offering credibility and explainability, thanks to the human fact-checking and explanations in the associated fact-checking article. Here, we focus on claims made in a political debate and we study the impact of modeling the context of the claim: both on the source side, i.e., in the debate, as well as on the target side, i.e., in the fact-checking explanation document. We do this by modeling the local context, the global context, as well as by means of co-reference resolution, and multi-hop reasoning over the sentences of the document describing the fact-checked claim. The experimental results show that each of these represents a valuable information source, but that modeling the source-side context is most important, and can yield 10+ points of absolute improvement over a state-of-the-art model. ",The Role of Context in Detecting Previously Fact-Checked Claims
188,1381962474979684352,1240583910419050496,Pascal Mettes,"['Learning from examples is fun, but can you localise and classify actions without having seen a video before?\n\nWe find that with the right prior spatial and semantic knowledge from objects, more is possible than you think.\n\nIn IJCV w/ @twuilliam @cgmsnoek\n\n<LINK> <LINK>', '@ajaydiv @twuilliam @cgmsnoek Thanks Ajay!']",https://arxiv.org/abs/2104.04715,"This work strives for the classification and localization of human actions in videos, without the need for any labeled video training examples. Where existing work relies on transferring global attribute or object information from seen to unseen action videos, we seek to classify and spatio-temporally localize unseen actions in videos from image-based object information only. We propose three spatial object priors, which encode local person and object detectors along with their spatial relations. On top we introduce three semantic object priors, which extend semantic matching through word embeddings with three simple functions that tackle semantic ambiguity, object discrimination, and object naming. A video embedding combines the spatial and semantic object priors. It enables us to introduce a new video retrieval task that retrieves action tubes in video collections based on user-specified objects, spatial relations, and object size. Experimental evaluation on five action datasets shows the importance of spatial and semantic object priors for unseen actions. We find that persons and objects have preferred spatial relations that benefit unseen action localization, while using multiple languages and simple object filtering directly improves semantic matching, leading to state-of-the-art results for both unseen action classification and localization. ",Object Priors for Classifying and Localizing Unseen Actions
189,1381943167138795522,2679086784,Larry Han,['How can we leverage surrogate markers captured in real-world data to make inference on long-term treatment effects? Check out our paper to find out! <LINK>'],https://arxiv.org/abs/2104.05513,"Shortcomings of randomized clinical trials are pronounced in urgent health crises, when rapid identification of effective treatments is critical. Leveraging short-term surrogates in real-world data (RWD) can guide policymakers evaluating new treatments. In this paper, we develop novel estimators for the proportion of treatment effect (PTE) on the true outcome explained by a surrogate in RWD settings. We propose inverse probability weighted and doubly robust (DR) estimators of an optimal transformation of the surrogate and PTE by semi-nonparametrically modeling the relationship between the true outcome and surrogate given baseline covariates. We show that our estimators are consistent and asymptotically normal, and the DR estimator is consistent when either the propensity score model or outcome regression model is correctly specified. We compare our proposed estimators to existing estimators and show a reduction in bias and gains in efficiency through simulations. We illustrate the utility of our method in obtaining an interpretable PTE by conducting a cross-trial comparison of two biologic therapies for ulcerative colitis. ",On the Evaluation of Surrogate Markers in Real World Data Settings
190,1381692581290110978,1284311543648329728,Maitraiyee Tiwari,"[""Our new paper describing the first results of  @SOFIAtelescope's legacy program (FEEDBACK) was accepted by ApJ. We study the stellar feedback in RCW49 by characterizing its wind driven shell. \n@AstroRamsey \n\narXiv: <LINK> <LINK>""]",http://arxiv.org/abs/2104.04276,"We unveil the stellar wind driven shell of the luminous massive star-forming region of RCW 49 using SOFIA FEEDBACK observations of the [CII] 158 $\mu$m line. The complementary dataset of the $^{12}$CO and $^{13}$CO J = 3 - 2 transitions is observed by the APEX telescope and probes the dense gas toward RCW 49. Using the spatial and spectral resolution provided by the SOFIA and APEX telescopes, we disentangle the shell from a complex set of individual components of gas centered around RCW 49. We find that the shell of radius ~ 6 pc is expanding at a velocity of 13 km s$^{-1}$ toward the observer. Comparing our observed data with the ancillary data at X-Ray, infrared, sub-millimeter and radio wavelengths, we investigate the morphology of the region. The shell has a well defined eastern arc, while the western side is blown open and is venting plasma further into the west. Though the stellar cluster, which is ~ 2 Myr old gave rise to the shell, it only gained momentum relatively recently as we calculate the shell's expansion lifetime ~ 0.27 Myr, making the Wolf-Rayet star WR20a a likely candidate responsible for the shell's re-acceleration. ","SOFIA FEEDBACK survey: exploring the dynamics of the stellar wind driven
  shell of RCW 49"
191,1380469687230795780,78913886,parfait Atchad√©,"['New paper <LINK>! We propose a convolutional filter that takes advantage of the classic #ML experience, quantum effects and the variational principle to enhance #CNN. I like to thank @XanaduAI @pennylaneai @awscloud &amp; team for hosting #Qhack21 #QML #ai #Quantum <LINK>']",http://arxiv.org/abs/2104.03418,"Convolutional Neural Networks (CNN) are used mainly to treat problems with many images characteristic of Deep Learning. In this work, we propose a hybrid image classification model to take advantage of quantum and classical computing. The method will use the potential that convolutional networks have shown in artificial intelligence by replacing classical filters with variational quantum filters. Similarly, this work will compare with other classification methods and the system's execution on different servers. The algorithm's quantum feasibility is modelled and tested on Amazon Braket Notebook instances and experimented on the Pennylane's philosophy and framework. ",Quantum Enhanced Filter: QFilter
192,1380445635187466246,1359421250431516683,Dr. Eva Laplace,"[""It's new paper day! With 2D supernova explosion simulations, we find that stars that get stripped by a binary companion tend to explode more easily than stars of the same initial mass that remain single all their life.\nMore details at <LINK>""]",https://arxiv.org/abs/2104.03317,"Most massive stars experience binary interactions in their lifetimes that can alter both the surface and core structure of the stripped star with significant effects on their ultimate fate as core-collapse supernovae. However, core-collapse supernovae simulations to date have focused almost exclusively on the evolution of single stars. We present a systematic simulation study of single and binary-stripped stars with the same initial mass as candidates for core-collapse supernovae (11 - 21 M$_{\odot}$). Generally, we find that binary-stripped stars core tend to be less compact, with a more prominent, deeper silicon/oxygen interface, and explode preferentially to the corresponding single stars of the same initial mass. Such a dichotomy of behavior between these two modes of evolution would have important implications for supernovae statistics, including the final neutron star masses, explosion energies, and nucleosynthetic yields. Binary-stripped remnants are also well poised to populate the possible mass gap between the heaviest neutron stars and the lightest black holes. Our work presents an improvement along two fronts, as we self-consistently account for the pre-collapse stellar evolution and the subsequent explosion outcome. Even so, our results emphasize the need for more detailed stellar evolutionary models to capture the sensitive nature of explosion outcome. ",Binary-Stripped Stars as Core-Collapse Supernovae Progenitors
193,1380442667964850179,2739464881,Edward Bryant,"[""What's this? Large TTVs detected for an extremely low density, long period planet?! \n\nCheck out the paper - <LINK>  - to find out how we did this using data from @NextGenTransits and other telescopes, and what the future holds for the fascinating HIP41378 system <LINK>""]",https://arxiv.org/abs/2104.03159,"HIP 41378 f is a temperate $9.2\pm0.1 R_{\oplus}$ planet with period of 542.08 days and an extremely low density of $0.09\pm0.02$ g cm$^{-3}$. It transits the bright star HIP 41378 (V=8.93), making it an exciting target for atmospheric characterization including transmission spectroscopy. HIP 41378 was monitored photometrically between the dates of 2019 November 19 and November 28. We detected a transit of HIP 41378 f with NGTS, just the third transit ever detected for this planet, which confirms the orbital period. This is also the first ground-based detection of a transit of HIP 41378 f. Additional ground-based photometry was also obtained and used to constrain the time of the transit. The transit was measured to occur 1.50 hours earlier than predicted. We use an analytic transit timing variation (TTV) model to show the observed TTV can be explained by interactions between HIP 41378 e and HIP 41378 f. Using our TTV model, we predict the epochs of future transits of HIP 41378 f, with derived transit centres of T$_{C,4} = 2459355.087^{+0.031}_{-0.022}$ (May 2021) and T$_{C,5} = 2459897.078^{+0.114}_{-0.060}$ (Nov 2022). ","A transit timing variation observed for the long-period extremely low
  density exoplanet HIP 41378f"
194,1380138443276181509,19149703,Karina Voggel ‚ú®üî≠üèÉüèº‚Äç‚ôÄÔ∏è,"['A huge effort of our Student in Arizona Allie Hughes is out on the Arxiv today. \nWe find 1900 good GC candidates in Centaurus A in the outer Halo, using Data from Gaia and our own imaging survey.\n<LINK> <LINK>', 'This work is essentially extending the Gaia method I have developed in my 2020 paper (https://t.co/jWLbemNdL1) to fainter magnitudes and its really exciting to see how it is still efficient.', ""What Gaia can provide us that normal spectroscopic surveys can't is completeness in the sparse outskirts of CenAs Halo. Before this method 99% of all known GCs were located within 20kpc. Now we can detect them out to 150kpc. The box is where we had literature GCs! https://t.co/BlEzWMdDOG"", 'Another cool thing that Gaia provides is a confirmation of what literature measurements were not real GCs (blue). Removing these from the overall GCs of CenA shows that the velocity dispersion of the GCs is much smaller than previously thought. https://t.co/8yrMXtvKqu', 'Because these false positives the average velocity of the GC system used to be much lower then the one of the galaxy itself. This discrepancy is now gone. What we learned: If you GC systems velocity overlaps with the MW use Gaia to see if it can confirm some as foreground.', ""Also fun fact: This is ground based imaging of one such GC. They were there and visibly resolved into stars with semi-decent imaging. But Galaxy Halos are so big that we miss them if we don't know where to look. https://t.co/pQrtjF512l""]",https://arxiv.org/abs/2104.02719,"We present a new catalog of 40502 globular cluster (GC) candidates in NGC 5128 out to a projected radius of $\sim$150 kpc, based on data from the Panoramic Imaging Survey of Centaurus and Sculptor (PISCeS), Gaia Data Release 2, and the NOAO Source Catalog. Ranking these candidates based on the likelihood that they are true GCs, we find that approximately 1900 belong to our top two ranking categories and should be the highest priority for spectroscopic follow-up for confirmation. Taking into account our new data and a vetting of previous GC catalogs, we estimate a total GC population of $1450 \pm 160$ GCs. We show that a substantial number of sources previously argued to be low-velocity GCs are instead foreground stars, reducing the inferred GC velocity dispersion. This work showcases the power of Gaia to identify slightly extended sources at the $\sim 4$ Mpc distance of NGC 5128, enabling accurate identification of GCs throughout the entire extended halo, not just the inner regions that have been the focus of most previous work. ","NGC 5128 globular cluster candidates out to 150 kpc: a comprehensive
  catalog from Gaia and ground based data"
195,1380076559470628883,1242075170279493632,Matteo A. C. Rossi,"['Excited to share our new results on VQE obtained in a nice collaboration with @IBMResearch Zurich team, @quantum_of_me , @PBarkoutsos , @GuglielmoMazzo3  and Ivano Tavernelli. We propose a new variational approach: learning to measure on the fly! <LINK>']",https://arxiv.org/abs/2104.00569,"Many prominent quantum computing algorithms with applications in fields such as chemistry and materials science require a large number of measurements, which represents an important roadblock for future real-world use cases. We introduce a novel approach to tackle this problem through an adaptive measurement scheme. We present an algorithm that optimizes informationally complete positive operator-valued measurements (POVMs) on the fly in order to minimize the statistical fluctuations in the estimation of relevant cost functions. We show its advantage by improving the efficiency of the variational quantum eigensolver in calculating ground-state energies of molecular Hamiltonians with extensive numerical simulations. Our results indicate that the proposed method is competitive with state-of-the-art measurement-reduction approaches in terms of efficiency. In addition, the informational completeness of the approach offers a crucial advantage, as the measurement data can be reused to infer other quantities of interest. We demonstrate the feasibility of this prospect by reusing ground-state energy-estimation data to perform high-fidelity reduced state tomography. ","Learning to Measure: Adaptive Informationally Complete Generalized
  Measurements for Quantum Algorithms"
196,1380066781990109186,1215589379256786947,Ilja Behnke,"[""Connecting embedded devices with realtime requirements to IP networks is risky. The impact of network-generated interrupts and networking overhead quickly becomes a threat to realtime. @edge_sys '21 we propose and evaluate 4 mitigation techniques <LINK> #iot 1/2"", 'Our approaches adaptively mitigate deadline-misses caused by high network loads. Real-time and network receive metrics are monitored to react to high loads and keep the potentially critical real-time MCU running. 2/2']",https://arxiv.org/abs/2104.02393,"Manufacturing, automotive, and aerospace environments use embedded systems for control and automation and need to fulfill strict real-time guarantees. To facilitate more efficient business processes and remote control, such devices are being connected to IP networks. Due to the difficulty in predicting network packets and the interrelated workloads of interrupt handlers and drivers, devices controlling time critical processes stand under the risk of missing process deadlines when under high network loads. Additionally, devices at the edge of large networks and the internet are subject to a high risk of load spikes and network packet overloads. In this paper, we investigate strategies to detect network packet overloads in real-time and present four approaches to adaptively mitigate local deadline misses. In addition to two strategies mitigating network bursts with and without hysteresis, we present and discuss two novel mitigation algorithms, called Budget and Queue Mitigation. In an experimental evaluation, all algorithms showed mitigating effects, with the Queue Mitigation strategy enabling most packet processing while preventing lateness of critical tasks. ","Detecting and Mitigating Network Packet Overloads on Real-Time Devices
  in IoT Systems"
197,1379752169398800387,519483,Davide Fucci,['1/ We have extended our #RE20 paper to show how biofeedback can be used to predict user engagement during requirements interviews \n/cc @alessferra @PaolaSpo @NicoleNovielli @DanielaGirard91  and Thaide Huichapa).\nFind out more in the pre-print <LINK>'],https://arxiv.org/abs/2104.02410,"Capturing users' engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collect and analyze users' feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in bespoke software development, or in contexts in which one needs to gather information from specific users. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this paper, we propose to utilize biometric data, in terms of physiological and voice features, to complement interviews with information about the engagement of the user on the discussed product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users' engagement by training supervised machine learning algorithms on biometric data (F1=0.72), and that voice features alone are sufficiently effective (F1=0.71). Our work contributes with one the first studies in requirements engineering in which biometrics are used to identify emotions. This is also the first study in software engineering that considers voice analysis. The usage of voice features could be particularly helpful for emotion-aware requirements elicitation in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research. ","Using Voice and Biofeedback to Predict User Engagement during
  Requirements Interviews"
198,1379737640388472834,14327235,Jack Valmadre,"['New paper on arxiv!\nLocal metrics for multi-object tracking\n<LINK>\n\nWe propose metrics for MOT where tracks only need to be correct within a finite temporal horizon. Varying the horizon reveals the distance at which trackers make association errors\n\n1/n', 'A difficult question in MOT is how to measure accuracy when some degree of association error is tolerable. Various association metrics have been proposed over the years (MOTA, HOTA, mutual info, Rand index, ...) and each defines an implicit trade-off with detection\n\n2/n', 'Local metrics provide an alternative mechanism to explicitly specify the trade-off between detection and association. Instead of choosing an association metric and combining it with a detection metric, benchmarks can just choose one (or more) temporal horizon(s)\n\n3/n', 'We further propose a decomposition of tracker errors into detection (false-negative and false-positive) and association (split and merge). Together with the temporal horizon, these constitute valuable tools for analysing and comparing MOT algorithms\n\n4/n', 'Shown here: Average Local Tracking Accuracy (ALTA) as a function of temporal horizon for state-of-the-art trackers on MOT17. The metric ranges from pure detection (horizon &lt; 1 frame) to strict tracking (horizon &gt; seq len). Asterisks denote the use of a custom detector\n\n5/n https://t.co/aVWbRoqvjD']",https://arxiv.org/abs/2104.02631,"This paper introduces temporally local metrics for Multi-Object Tracking. These metrics are obtained by restricting existing metrics based on track matching to a finite temporal horizon, and provide new insight into the ability of trackers to maintain identity over time. Moreover, the horizon parameter offers a novel, meaningful mechanism by which to define the relative importance of detection and association, a common dilemma in applications where imperfect association is tolerable. It is shown that the historical Average Tracking Accuracy (ATA) metric exhibits superior sensitivity to association, enabling its proposed local variant, ALTA, to capture a wide range of characteristics. In particular, ALTA is better equipped to identify advances in association independent of detection. The paper further presents an error decomposition for ATA that reveals the impact of four distinct error types and is equally applicable to ALTA. The diagnostic capabilities of ALTA are demonstrated on the MOT 2017 and Waymo Open Dataset benchmarks. ",Local Metrics for Multi-Object Tracking
199,1379720154406346752,352048533,Sebasti√°n Marino,"['2 papers just out of the oven üòÄ \n-@Milkyway_73 (+me, Mark Booth, @lucaroundthewo1 @EricMamajek et al) studied HR8799 #debrisdisk with new @almaobs  <LINK>\n-I showed how we can use #debrisdisk edges to constrain stirring levels. <LINK>\ndetailsüëáüèæ <LINK>', ""@Milkyway_73 et al. showed that HR8799's disk is broad and has smooth edges as expected if the planets and disk had strongly interacted in the past and produce high eccentricity populations. Moreover, we showed that the disk is consistent with being aligned with the inner planets https://t.co/xEN0UKzXwj"", 'I showed that the higher e the smoother the edges will be.  We can parametrize the edges as tanh, which mimics very well the shape produced by Rayleigh distributions of e. This allows constraining e based on the sharpness. https://t.co/7DagFEbjso']",https://arxiv.org/abs/2104.02088,"The exoplanetary system of HR 8799 is one of the rare systems in which multiple planets have been directly imaged. Its architecture is strikingly similar to that of the Solar System, with the four imaged giant planets surrounding a warm dust belt analogous to the Asteroid Belt, and themselves being surrounded by a cold dust belt analogue to the Kuiper Belt. Previous observations of this cold belt with ALMA in Band 6 (1.3 mm) revealed its inner edge, but analyses of the data differ on its precise location. It was therefore unclear whether the outermost planet HR 8799 b was dynamically sculpting it or not. We present here new ALMA observations of this debris disk in Band 7 (340 GHz, 880 micron). These are the most detailed observations of this disk obtained so far, with a resolution of 1"" (40 au) and sensitivity of $9.8\,\mu\mathrm{Jy\,beam^{-1}}$, which allowed us to recover the disk structure with high confidence. In order to constrain the disk morphology, we fit its emission using radiative transfer models combined with a MCMC procedure. We find that this disk cannot be adequately represented by a single power law with sharp edges. It exhibits a smoothly rising inner edge and smoothly falling outer edge, with a peak in between, as expected from a disk that contains a high eccentricity component, hence confirming previous findings. Whether this excited population and inner edge shape stem from the presence of an additional planet remains, however, an open question. ",A detailed characterization of HR 8799's debris disk with ALMA in Band 7
200,1379445374339940353,860542699929313280,Dr. Rocio Kiman,"['Paper day! Today is in the arXiv the work I\'ve done with amazing collaborators: ""Calibration of the Halpha Age-Activity relation for M dwarfs"". In this paper we study the relation between Halpha equivalent width (line strength) and age for M dwarfs. <LINK>', '@jfaherty @kellecruz @jgagneastro @ruthangus @sjs917 @amannastro @astro_daniella @emilylurice', 'We compiled a calibration sample of 892 M dwarfs with Halpha measurements from the literature, and we identified that they were either co-moving with a white dwarf or in a known young association to obtain their ages. We found that the Halpha decreases with increasing age. https://t.co/qtiQ5tGIzU', 'We classified the age-calibrators as active and inactive according to Halpha equivalent width and their Gaia red color. We found that the active fraction changes with mass and with age. We note that the evolution of active fraction with time changes according to the color. https://t.co/bCLYXpjIb9', 'We fit the age-activity relation with a broken power-law. We did not have enough stars to precisely constrain the breaking point or the slope of the decrease after 1 Gyr. https://t.co/HGCGJyjoFT']",https://arxiv.org/abs/2104.01232,"In this work, we calibrate the relationship between Halpha emission and M dwarf ages. We compile a sample of 892 M dwarfs with Halpha equivalent width (HaEW) measurements from the literature that are either co-moving with a white dwarf of known age (21 stars) or in a known young association (871 stars). In this sample we identify 7 M dwarfs that are new candidate members of known associations. By dividing the stars into active and inactive categories according to their HaEW and spectral type (SpT), we find that the fraction of active dwarfs decreases with increasing age, and the form of the decline depends on SpT. Using the compiled sample of age-calibrators we find that HaEW and fractional Halpha luminosity (LHaLbol) decrease with increasing age. HaEW for SpT<M7 decreases gradually up until ~1Gyr. For older ages, we found only two early M dwarfs which are both inactive and seem to continue the gradual decrease. We also found 14 mid-type out of which 11 are inactive and present a significant decrease of HaEW, suggesting that the magnetic activity decreases rapidly after ~1Gyr. We fit LHaLbol versus age with a broken power-law and find an index of -0.11+0.02-0.01 for ages <~776Myr. The index becomes much steeper at older ages however a lack of field age-calibrators leaves this part of the relation far less constrained. Finally, from repeated independent measurements for the same stars we find that 94% of these has a level of HaEW variability <=5A at young ages (<1Gyr). ",Calibration of the Halpha Age-Activity relation for M dwarfs
201,1379327762020831232,731914182178787333,Steffen Herbold,"['Our exploratory study protocol was accepted at the @msrconf registered reports track and can be found online (<LINK>). ü•≥\n\nBut this is not really about the study, but why we pre-registered this *exploratory* study. (1/n)', 'There are *many* ways such a study could be executed. You could replace almost any part with something else. Pre-registrations means that in this case at least 5 (!) independent parties (authors+4 reviewers) agree that these choices are reasonable. (2/n)', 'This also means that we as authors could just switch to a different approach to ""find"" the results we want. This is not only an issue for confirmatory, but also exploratory research! (3/n)', 'This also means that the infamous Reviewer 2 could easily write drive-by rejections, because ""we did/did not do X"". Well, we agreed on X beforehand, so this is not an issue.', 'There is also the aspect that our protocol is now BETTER! We had some very insightful reviewer comments which would have been a pain in a journal revision, but that we can easily address prior to starting the study. (5/n)', 'Also, while I wrote this alone, my plan is to give this as project to a student as co-author. The student has a *detailed* project description. Should this be done with senior PhD students? Probably not. Students doing their first major project! Yes! (6/n)', 'I could go on about the advantages. But I rather also want to talk about the potential negative aspects. I see two issues. (7/n)', 'First, there are only few opportunities right now. This could be easily fixed by making this a ""normal"" aspect of journals, instead of backdooring this through conference special tracks. (8/n)', 'Second, this requires significant up-front investment by the PIs, while we already have our plates full. Not sure how to solve this, but there is also a positive spin to this: maybe we only do the studies we really care about, leading less studies but with higher quality. (n/n)']",https://arxiv.org/abs/2104.00566,"Performance metrics are a core component of the evaluation of any machine learning model and used to compare models and estimate their usefulness. Recent work started to question the validity of many performance metrics for this purpose in the context of software defect prediction. Within this study, we explore the relationship between performance metrics and the cost saving potential of defect prediction models. We study whether performance metrics are suitable proxies to evaluate the cost saving capabilities and derive a theory for the relationship between performance metrics and cost saving potential. ","Exploring the relationship between performance metrics and cost saving
  potential of defect prediction models"
202,1377982248108969984,1537469491,Andrew H. Song,"['Check out our submission ""Gaussian Process Convolutional Dictionary Learning""! \nWe propose a dictionary learning framework constrained by Gaussian process prior, yielding nice theoretical/experimental results. Joint work with @BTolooshams and @dunbar_ba\n \n<LINK>']",https://arxiv.org/abs/2104.00530,"Convolutional dictionary learning (CDL), the problem of estimating shift-invariant templates from data, is typically conducted in the absence of a prior/structure on the templates. In data-scarce or low signal-to-noise ratio (SNR) regimes, learned templates overfit the data and lack smoothness, which can affect the predictive performance of downstream tasks. To address this limitation, we propose GPCDL, a convolutional dictionary learning framework that enforces priors on templates using Gaussian Processes (GPs). With the focus on smoothness, we show theoretically that imposing a GP prior is equivalent to Wiener filtering the learned templates, thereby suppressing high-frequency components and promoting smoothness. We show that the algorithm is a simple extension of the classical iteratively reweighted least squares algorithm, independent of the choice of GP kernels. This property allows one to experiment flexibly with different smoothness assumptions. Through simulation, we show that GPCDL learns smooth dictionaries with better accuracy than the unregularized alternative across a range of SNRs. Through an application to neural spiking data, we show that GPCDL learns a more accurate and visually-interpretable smooth dictionary, leading to superior predictive performance compared to non-regularized CDL, as well as parametric alternatives. ",Gaussian Process Convolutional Dictionary Learning
203,1392109863636017155,942238055607435264,Luca Carlone,"['Traditional approaches for category-level object pose and shape estimation are sensitive to outliers and get stuck in local minima. We propose the first approach that avoids local minima and is robust to 70-90% outliers: <LINK> \n#computervision #mitsparklab', 'accepted at RSS 2021: https://t.co/zhC7MTjaCJ']",https://arxiv.org/abs/2104.08383,"We consider a category-level perception problem, where one is given 3D sensor data picturing an object of a given category (e.g. a car), and has to reconstruct the pose and shape of the object despite intra-class variability (i.e. different car models have different shapes). We consider an active shape model, where -- for an object category -- we are given a library of potential CAD models describing objects in that category, and we adopt a standard formulation where pose and shape estimation are formulated as a non-convex optimization. Our first contribution is to provide the first certifiably optimal solver for pose and shape estimation. In particular, we show that rotation estimation can be decoupled from the estimation of the object translation and shape, and we demonstrate that (i) the optimal object rotation can be computed via a tight (small-size) semidefinite relaxation, and (ii) the translation and shape parameters can be computed in closed-form given the rotation. Our second contribution is to add an outlier rejection layer to our solver, hence making it robust to a large number of misdetections. Towards this goal, we wrap our optimal solver in a robust estimation scheme based on graduated non-convexity. To further enhance robustness to outliers, we also develop the first graph-theoretic formulation to prune outliers in category-level perception, which removes outliers via convex hull and maximum clique computations; the resulting approach is robust to 70%-90% outliers. Our third contribution is an extensive experimental evaluation. Besides providing an ablation study on a simulated dataset and on the PASCAL3D+ dataset, we combine our solver with a deep-learned keypoint detector, and show that the resulting approach improves over the state of the art in vehicle pose estimation in the ApolloScape datasets. ","Optimal Pose and Shape Estimation for Category-level 3D Object
  Perception"
204,1389267701957230594,765341324279156736,Sinong Wang,"[""You don't need a 175B GPT-3 for few shot learning. All you need is entailment! Check out our new preprints: <LINK>\n\nIn short, we propose a new method turning small LM into better few shot learner.\n@Han_Fang_ @MadianKhabsa @hanna_mao @gabema <LINK>""]",https://arxiv.org/abs/2104.14690,"Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3. ",Entailment as Few-Shot Learner
205,1387780067212791819,51169895,Gianluca Stringhini,['ICYMI here is our study on the harassment that researchers receive as a reaction to their work. We provide recommendations on how academics and their institutions could better deal with the problem. \n\nWork lead by @PeriwinkleID \n\nüìú<LINK> <LINK>'],https://arxiv.org/abs/2104.11145,"While the Internet has dramatically increased the exposure that research can receive, it has also facilitated harassment against scholars. To understand the impact that these attacks can have on the work of researchers, we perform a series of systematic interviews with researchers including academics, journalists, and activists, who have experienced targeted, Internet-facilitated harassment. We provide a framework for understanding the types of harassers that target researchers, the harassment that ensues, and the personal and professional impact on individuals and academic freedom. We then study preventative and remedial strategies available, and the institutions that prevent some of these strategies from being more effective. Finally, we discuss the ethical structures that could facilitate more equitable access to participating in research without serious personal suffering. ","""I'm a Professor, which isn't usually a dangerous job"":
  Internet-Facilitated Harassment and its Impact on Researchers"
206,1385077064995270656,844013017830379522,Nayeon Lee,"['üö®Happy to share 2 papers accepted in #NAACL2021 to tackle misinformation üîçüë©\u200d‚úàÔ∏è‚ùå‚úÖ \n\nWe propose an UnifiedM2 model that unifies multiple domains of misinfo to learn a richer representation that aids effective few-shot learning of unseen misinfo tasks.1/n\n\n<LINK> <LINK> <LINK>', 'Many thanks to all the co-authors üòá @belindazli @sinongwang @pascalefung @gabema @scottyih @MadianKhabsa']",https://arxiv.org/abs/2104.05243,"In this paper, we introduce UnifiedM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news, and verifying rumors. By grouping these tasks together, UnifiedM2learns a richer representation of misinformation, which leads to state-of-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UnifiedM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and model's generalizability to unseen events. ",On Unifying Misinformation Detection
207,1384703387309010944,957685323198164992,Ziwei Liu,"['Our #CVPR2021 **oral** paper ""Variational Relational Point Completion Network"":\n\nPaper: <LINK>\nProject: <LINK>\n\n- We propose 1) a variational framework (VRCNet) and 2) a high-resolution multi-view point cloud dataset (MVP) for point completion. <LINK>']",https://arxiv.org/abs/2104.10154,"Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifically, we carefully design point self-attention kernel and point selective kernel module to exploit relational point features, which refines local shape details conditioned on the coarse completion. In addition, we contribute a multi-view partial point cloud dataset (MVP dataset) containing over 100,000 high-quality scans, which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model. Extensive experiments demonstrate that VRCNet outperforms state-of-theart methods on all standard point cloud completion benchmarks. Notably, VRCNet shows great generalizability and robustness on real-world point cloud scans. ",Variational Relational Point Completion Network
208,1384437169465430027,1236622778175909888,Jingtao Zhan,"['Our work ""Optimizing Dense Retrieval Model Training with Hard Negatives"" has been accepted by #SIGIR2021 Full Paper Track. \nWe provide theoretical analysis and propose DR training method using dynamic hard negatives. Very promising results! See <LINK>']",https://arxiv.org/abs/2104.08051,"Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance. ",Optimizing Dense Retrieval Model Training with Hard Negatives
209,1383066922997182471,1056626853652426752,Jonathan Herzig,"['What is the impact of intermediate representations on compositional generalization?\nWe find them to be surprisingly effective in improving semantic parsing generalization for pre-trained LMs!\n \nw/ @ptshaw2 @mchang21 @kelvin_guu @IcePasupat Yuan Zhang\n \n<LINK>\n1/4 <LINK>', 'We posit that low structural correspondence between utterances and programs hurts generalization.\n \nTo improve correspondence, we study reversible and lossy intermediate representations and identify key principles for designing effective representations for new formalisms.\n \n2/4 https://t.co/GUxH1vzklg', 'Our best intermediate representations substantially improve a T5 baseline fine-tuned from original programs, and achieve state-of-the-art results on CFQ and the template splits 3 text-to-SQL datasets\n\n3/4', 'This preprint describes my internship project at @GoogleAI  \n\nThe code for generating the data for all of our intermediate representations and compositional splits is available at:\n https://t.co/18gHt4tr99\n4/4']",https://arxiv.org/abs/2104.07478,"Sequence-to-sequence (seq2seq) models are prevalent in semantic parsing, but have been found to struggle at out-of-distribution compositional generalization. While specialized model architectures and pre-training of seq2seq models have been proposed to address this issue, the former often comes at the cost of generality and the latter only shows limited success. In this paper, we study the impact of intermediate representations on compositional generalization in pre-trained seq2seq models, without changing the model architecture at all, and identify key aspects for designing effective representations. Instead of training to directly map natural language to an executable form, we map to a reversible or lossy intermediate representation that has stronger structural correspondence with natural language. The combination of our proposed intermediate representations and pre-trained models is surprisingly effective, where the best combinations obtain a new state-of-the-art on CFQ (+14.8 accuracy points) and on the template-splits of three text-to-SQL datasets (+15.0 to +19.4 accuracy points). This work highlights that intermediate representations provide an important and potentially overlooked degree of freedom for improving the compositional generalization abilities of pre-trained seq2seq models. ","Unlocking Compositional Generalization in Pre-trained Models Using
  Intermediate Representations"
210,1382505599401869313,456933366,Alexandre Lacoste,"['Fairness in machine learning can take many hidden forms, but we found that active learning can tackle part of it without needing to know the sensitive variables.\nThanks @fbranchaud1 , @Parmida44913188 , @Gabuhamad and @prlz77 \n<LINK>']",https://arxiv.org/abs/2104.06879,"Dataset bias is one of the prevailing causes of unfairness in machine learning. Addressing fairness at the data collection and dataset preparation stages therefore becomes an essential part of training fairer algorithms. In particular, active learning (AL) algorithms show promise for the task by drawing importance to the most informative training samples. However, the effect and interaction between existing AL algorithms and algorithmic fairness remain under-explored. In this paper, we study whether models trained with uncertainty-based AL heuristics such as BALD are fairer in their decisions with respect to a protected class than those trained with identically independently distributed (i.i.d.) sampling. We found a significant improvement on predictive parity when using BALD, while also improving accuracy compared to i.i.d. sampling. We also explore the interaction of algorithmic fairness methods such as gradient reversal (GRAD) and BALD. We found that, while addressing different fairness issues, their interaction further improves the results on most benchmarks and metrics we explored. ",Can Active Learning Preemptively Mitigate Fairness Issues?
211,1382432437632790529,1242135548040548352,Xi Ye,"['Excited to share our (w/ Rohan Nair, @gregd_nlp ) new pre-print ""Evaluating Explanations for Reading Comprehension with Realistic Counterfactuals"":\n<LINK>\n\nWe propose a methodology to evaluate explanations <LINK>', ""An explanation should allow us to understand the reading comprehension model's high-level behavior with respect to a set of realistic counterfactual input scenarios.\xa0\nAs an example, we show explanations for a HotpotQA example (blue) generated by several methods."", ""At first glance, it seems token-level attributions are more plausible than pairwise interactions, as they highlight both movies being documentaries. But does that reflect the model's true reasoning?"", 'We profile the model behaviors with the predictions on realistic counterfactual input. The model always makes the same predictions on the counterfactuals \xa0-- and the pairwise explanation actually conveys this more accurately!', 'We annotate realistic counterfactuals on multiple settings (HotpotQA, SQuAD, and synthetic) and evaluate several explanation methods including token-level attribution techniques and pairwise interaction techniques\xa0in terms of whether they can give insights about model behavior.', 'Our analysis suggests that pairwise explanation techniques are better suited to\xa0analyzing the behavior\xa0of RC models, which fundamentally involves complex interaction between questions and contexts.\n\ncode and data: https://t.co/4uxg53GX6r']",https://arxiv.org/abs/2104.04515,"When a model attribution technique highlights a particular part of the input, a user might understand this highlight as making a statement about counterfactuals (Miller, 2019): if that part of the input were to change, the model's prediction might change as well. This paper investigates how well different attribution techniques align with this assumption on realistic counterfactuals in the case of reading comprehension (RC). RC is a particularly challenging test case, as token-level attributions that have been extensively studied in other NLP tasks such as sentiment analysis are less suitable to represent the reasoning that RC models perform. We construct counterfactual sets for three different RC settings, and through heuristics that can connect attribution methods' outputs to high-level model behavior, we can evaluate how useful different attribution methods and even different formats are for understanding counterfactuals. We find that pairwise attributions are better suited to RC than token-level attributions across these different RC settings, with our best performance coming from a modification that we propose to an existing pairwise attribution method. ","Connecting Attributions and QA Model Behavior on Realistic
  Counterfactuals"
212,1382286857044627457,769142140765167616,Siamak F. Shahandashti,"[""With George Kampanos, we looked at #cookie notices in 14.5k UK, 3k Greek websites, found widespread  violations: no notice, direct 'reject' option rare, 'reject' harder than 'accept', biased info given to users.\nPaper accepted to @IFIPSEC '21\nePrint: <LINK> <LINK>"", 'Thanks to @s_englehardt and @random_walker for developing OpenWPM of course and @mozilla for maintaining it.']",https://arxiv.org/abs/2104.05750,"Cookie banners are devices implemented by websites to allow users to manage their privacy settings with respect to the use of cookies. They are part of a user's daily web browsing experience since legislation in Europe requires websites to show such notices. In this paper, we carry out a large-scale study of more than 17,000 websites including more than 7,500 cookie banners in Greece and the UK to determine compliance and tracking transparency levels. Our analysis shows that although more than 60% of websites store third-party cookies in both countries, only less than 50% show a cookie notice and hence a substantial proportion do not comply with the law even at the very basic level. We find only a small proportion of the surveyed websites providing a direct opt-out option, with an overwhelming majority either nudging users towards privacy-intrusive choices or making cookie rejection much harder than consent. Our results differ significantly in some cases from previous smaller-scale studies and hence underline the importance of large-scale studies for a better understanding of the big picture in cookie practices. ",Accept All: The Landscape of Cookie Banners in Greece and the UK
213,1382000480281362432,938457074278846468,Greg Durrett,"['New work with @tanyaagoyal ""Annotating and Modeling Fine-grained Factuality in Summarization‚Äù in NAACL21:\n<LINK>\nWe study factuality evaluation across 2 different summarization domains to see where current data creation strategies and models succeed and fail.', '(1) Annotation: synthetic datasets for factuality (artificial data corruption/paraphrasing techniques) don‚Äôt capture the full range of generation errors, esp. on datasets like XSum. We manually annotated error types to see these distributions in synthetic and real data. https://t.co/nxhLZRY48H', '(2) Modeling: we compare different modeling frameworks for factuality, and conclude that models that both leverage and model fine-grained factuality annotations outperform summary-level models, giving both interpretability and accuracy benefits.', ""In particular, the dependency arc entailment framework from Tanya‚Äôs past work at EMNLP-Findings'20 ( https://t.co/oRLU0PyjW0 ) worked especially well (check it out!) https://t.co/i3MV34WXM1"", 'Finally, localizing factual errors is useful can actually help us train more factual summarization models. By filtering tokens in the training data that the model identifies as non-factual, our final summarization system makes fewer factual errors. https://t.co/MJhJQOArHS', ""We hope to see more data collection + annotation efforts (like the Maynez et al. dataset) surrounding actual errors encountered in generated text!\n\nTanya's website with links to code and other papers: https://t.co/QF7Ys8PPYO""]",https://arxiv.org/abs/2104.04302,"Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data. ",Annotating and Modeling Fine-grained Factuality in Summarization
214,1381898128618573829,1199686509328289793,Jean-Baptiste Cordonnier,"['I am happy to present our work at #CVPR2021 from my internship with Google Brain Berlin. \n\nWe study a CNN inefficiency: every part of the image is processed by same network at same resolution. Instead we extract only ""relevant"" patches at full res.\n\n<LINK> <LINK>', 'Our patch selection layer is a standard differentiable layer: the extracted patches can be processed by *any* downstream networks or aggregation scheme (like transformers).\n\nCode: https://t.co/GqzSAp6u37 https://t.co/ytWEYI4ShV', 'Our top-k patch extraction is end-to-end differentiable thanks to the great work on\xa0Differentiable Perturbed Optimizers by @qberthet et al. Their clean framework makes discrete problems (expressible as a LP) differentiable and opens so many opportunities!\n\nhttps://t.co/txH1CqJzM6 https://t.co/J9AShUIC5M', 'I was lucky to collaborate with such a great team :) I learned a lot from their expertise on attention, transformers, vision üçª']",https://arxiv.org/abs/2104.03059,"Neural Networks require large amounts of memory and compute to process high resolution images, even when only a small part of the image is actually informative for the task at hand. We propose a method based on a differentiable Top-K operator to select the most relevant parts of the input to efficiently process high resolution images. Our method may be interfaced with any downstream neural network, is able to aggregate information from different patches in a flexible way, and allows the whole model to be trained end-to-end using backpropagation. We show results for traffic sign recognition, inter-patch relationship reasoning, and fine-grained recognition without using object/part bounding box annotations during training. ",Differentiable Patch Selection for Image Recognition
215,1379981465451122688,1878909680,Amin Karbasi,"['In this paper, just accepted to the Mathematics of Operations Research, we study the Nyquist rate of subsampling for submodular maximization. A simple idea that provides the currently best-known approximation guarantee in offline and streaming settings. \n<LINK> <LINK>']",https://arxiv.org/abs/2104.02772,"We propose subsampling as a unified algorithmic technique for submodular maximization in centralized and online settings. The idea is simple: independently sample elements from the ground set, and use simple combinatorial techniques (such as greedy or local search) on these sampled elements. We show that this approach leads to optimal/state-of-the-art results despite being much simpler than existing methods. In the usual offline setting, we present SampleGreedy, which obtains a $(p + 2 + o(1))$-approximation for maximizing a submodular function subject to a $p$-extendible system using $O(n + nk/p)$ evaluation and feasibility queries, where $k$ is the size of the largest feasible set. The approximation ratio improves to $p+1$ and $p$ for monotone submodular and linear objectives, respectively. In the streaming setting, we present SampleStreaming, which obtains a $(4p +2 - o(1))$-approximation for maximizing a submodular function subject to a $p$-matchoid using $O(k)$ memory and $O(km/p)$ evaluation and feasibility queries per element, where $m$ is the number of matroids defining the $p$-matchoid. The approximation ratio improves to $4p$ for monotone submodular objectives. We empirically demonstrate the effectiveness of our algorithms on video summarization, location summarization, and movie recommendation tasks. ",The Power of Subsampling in Submodular Maximization
