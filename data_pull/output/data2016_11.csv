,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,809054153997774850,39209113,Pratap Tokekar,['. @zs_zhang will be presenting our new paper on active target tracking at #CDC2016 today <LINK>'],https://arxiv.org/abs/1611.02343,"We study the problem of devising a closed-loop strategy to control the position of a robot that is tracking a possibly moving target. The robot is capable of obtaining noisy measurements of the target's position. The key idea in active target tracking is to choose control laws that drive the robot to measurement locations that will reduce the uncertainty in the target's position. The challenge is that measurement uncertainty often is a function of the (unknown) relative positions of the target and the robot. Consequently, a closed-loop control policy is desired which can map the current estimate of the target's position to an optimal control law for the robot. Our main contribution is to devise a closed-loop control policy for target tracking that plans for a sequence of control actions, instead of acting greedily. We consider scenarios where the noise in measurement is a function of the state of the target. We seek to minimize the maximum uncertainty (trace of the posterior covariance matrix) over all possible measurements. We exploit the structural properties of a Kalman Filter to build a policy tree that is orders of magnitude smaller than naive enumeration while still preserving optimality guarantees. We show how to obtain even more computational savings by relaxing the optimality guarantees. The resulting algorithms are evaluated through simulations. ",Non-Myopic Target Tracking Strategies for State-Dependent Noise
1,806658074090029056,114508388,Vikas Sindhwani,['New paper on Geometric Reasoning in 3D Environments for Robotics using Sum-of-Squares Programming techniques:\n<LINK>'],https://arxiv.org/abs/1611.07369,"Motivated by applications in robotics and computer vision, we study problems related to spatial reasoning of a 3D environment using sublevel sets of polynomials. These include: tightly containing a cloud of points (e.g., representing an obstacle) with convex or nearly-convex basic semialgebraic sets, computation of Euclidean distances between two such sets, separation of two convex basic semalgebraic sets that overlap, and tight containment of the union of several basic semialgebraic sets with a single convex one. We use algebraic techniques from sum of squares optimization that reduce all these tasks to semidefinite programs of small size and present numerical experiments in realistic scenarios. ",Geometry of 3D Environments and Sum of Squares Polynomials
2,804371225665384448,184656143,Aseem Agarwala,['Our new paper on neural facial expression editing. <LINK>'],https://arxiv.org/abs/1611.09961,"High-level manipulation of facial expressions in images --- such as changing a smile to a neutral expression --- is challenging because facial expression changes are highly non-linear, and vary depending on the appearance of the face. We present a fully automatic approach to editing faces that combines the advantages of flow-based face manipulation with the more recent generative capabilities of Variational Autoencoders (VAEs). During training, our model learns to encode the flow from one expression to another over a low-dimensional latent space. At test time, expression editing can be done simply using latent vector arithmetic. We evaluate our methods on two applications: 1) single-image facial expression editing, and 2) facial expression interpolation between two images. We demonstrate that our method generates images of higher perceptual quality than previous VAE and flow-based methods. ",Semantic Facial Expression Editing using Autoencoded Flow
3,803700208282570756,333597222,Matt Lease,['New deep learning paper: Neural Information Retrieval: A Literature Review. w/ student Ye Zhang &amp; @byron_c_wallace. <LINK>'],https://arxiv.org/abs/1611.06792,"A recent ""third wave"" of Neural Network (NN) approaches now delivers state-of-the-art performance in many machine learning tasks, spanning speech recognition, computer vision, and natural language processing. Because these modern NNs often comprise multiple interconnected layers, this new NN research is often referred to as deep learning. Stemming from this tide of NN work, a number of researchers have recently begun to investigate NN approaches to Information Retrieval (IR). While deep NNs have yet to achieve the same level of success in IR as seen in other areas, the recent surge of interest and work in NNs for IR suggest that this state of affairs may be quickly changing. In this work, we survey the current landscape of Neural IR research, paying special attention to the use of learned representations of queries and documents (i.e., neural embeddings). We highlight the successes of neural IR thus far, catalog obstacles to its wider adoption, and suggest potentially promising directions for future research. ",Neural Information Retrieval: A Literature Review
4,803690394156408834,96629016,N√©stor Espinoza,"['New paper with @jjfplanet et al: giant planets have lots of metals -&gt; low C/O ratios on their envelopes -&gt; water! <LINK>', 'Cool result possible thanks to the amazing experience during the Kavli Summer Program in Astrophysics!']",https://arxiv.org/abs/1611.08616,"We predict the carbon-to-oxygen (C/O) ratios in the hydrogen-helium envelope and atmospheres of a sample of nearly 50 relatively cool ($T_{\mathrm eq}<$ 1000 K) transiting gas giant planets. The method involves planetary envelope metallicity estimates that use the structure models of Thorngren et al. (2016) and the disk and planetary accretion model of \""Oberg et al. (2011). We find that nearly all of these planets are strongly metal-enriched which, coupled with the fact that solid material is the main deliverer of metals in the protoplanetary disk, implies that the substellar C/O ratios of their accreted solid material dominate compared to the enhanced C/O ratio of their accreted gaseous component. We predict that these planets will have atmospheres that are typically reduced in their C/O compared to parent star values independent of the assessed formation locations, with C/O $<1$ a nearly universal outcome within the framework of the model. We expect water vapor absorption features to be ubiquitous in the atmospheres of these planets, and by extension, other gas giants. ","Metal enrichment leads to low atmospheric C/O ratios in transiting giant
  exoplanets"
5,803417982013612032,3854851632,Hamish Clark,"[""I've got a new paper up on the arXiv! The first taste of DM annihilation in N-body sims, with more to come! <LINK> <LINK>""]",http://arxiv.org/abs/1611.08619,"The existence of substructure in halos of annihilating dark matter would be expected to substantially boost the rate at which annihilation occurs. Ultracompact minihalos of dark matter (UCMHs) are one of the more extreme examples of this. The boosted annihilation can inject significant amounts of energy into the gas of a galaxy over its lifetime. Here we determine the impact of the boost factor from UCMH substructure on the heating of galactic gas in a Milky Way-type galaxy, by means of N-body simulation. If $1\%$ of the dark matter exists as UCMHs, the corresponding boost factor can be of order $10^5$. For reasonable values of the relevant parameters (annihilation cross section $3\times10^{-26} ~\textrm{cm}^3~ \textrm{s}^{-1}$, dark matter mass 100 GeV, 10% heating efficiency), we show that the presence of UCMHs at the 0.1% level would inject enough energy to eject significant amounts of gas from the halo, potentially preventing star formation within $\sim$1 kpc of the halo centre. ","Heating of galactic gas by dark matter annihilation in ultracompact
  minihalos"
6,803411290433601536,2337598033,Geraint F. Lewis,['New paper on the arXiv with @hamishquark <LINK> <LINK>'],https://arxiv.org/abs/1611.08619,"The existence of substructure in halos of annihilating dark matter would be expected to substantially boost the rate at which annihilation occurs. Ultracompact minihalos of dark matter (UCMHs) are one of the more extreme examples of this. The boosted annihilation can inject significant amounts of energy into the gas of a galaxy over its lifetime. Here we determine the impact of the boost factor from UCMH substructure on the heating of galactic gas in a Milky Way-type galaxy, by means of N-body simulation. If $1\%$ of the dark matter exists as UCMHs, the corresponding boost factor can be of order $10^5$. For reasonable values of the relevant parameters (annihilation cross section $3\times10^{-26} ~\textrm{cm}^3~ \textrm{s}^{-1}$, dark matter mass 100 GeV, 10% heating efficiency), we show that the presence of UCMHs at the 0.1% level would inject enough energy to eject significant amounts of gas from the halo, potentially preventing star formation within $\sim$1 kpc of the halo centre. ","Heating of galactic gas by dark matter annihilation in ultracompact
  minihalos"
7,801703247908052993,20117127,Leigh Fletcher,"[""NEW PAPER KLAXON! Our chapter on Saturn's great springtime storm of 2010-11, ready for the upcoming Saturn book: <LINK>""]",https://arxiv.org/abs/1611.07669,"In December 2010, a major storm erupted in Saturn's northern hemisphere near 37 degree planetographic latitude. This rather surprising event, occurring at an unexpected latitude and time, is the sixth ""Great White Spot"" (GWS) storm observed over the last century and a half. Such GWS events are planetary-scale atmospheric phenomena that dramatically change the typically bland appearance of the planet. Occurring while the Cassini mission was on-orbit at Saturn, the Great Storm of 2010-2011 was well-suited for intense scrutiny by the suite of sophisticated instruments onboard the Cassini spacecraft as well by modern instrumentation on ground-based telescopes and onboard the Hubble Space Telescope. This GWS erupted on December 5th and generated a major dynamical disturbance that affected the whole latitude band from 25 deg to 48 deg N. Lightning events were prominent and detected as outbursts and flashes at both optical and radio wavelengths. The activity of the head ceased after about seven months, leaving the cloud structure and ambient winds perturbed. The tops of the optically dense clouds of the storm's head reached the 300 mbar altitude level where a mixture of ices was detected. The energetics of the frequency and power of lightning, as well as the estimated power generated by the latent heat released in the water-based convection, both indicate that the power released for the storm was a significant fraction of Saturn's total radiated power. The effects of the storm propagated into the stratosphere forming two warm airmasses at the 0.5-5 mbar pressure level altitude. Related to the stratospheric disturbance, hydrocarbon composition excesses were found. The decades-long interval between storms is probably related to the insolation cycle and the long radiative time constant of Saturn's atmosphere, and several theories for temporarily storing energy have been proposed. ",The Great Saturn Storm of 2010-2011
8,801594469267152896,101980926,Masahito Yamazaki,"['Submitted a new paper ""Cluster-Enriched Yang-Baxter Equation from SUSY Gauge Theories"" <LINK>']",https://arxiv.org/abs/1611.07522,"We propose a new generalization of the Yang-Baxter equation, where the R-matrix depends on cluster $y$-variables in addition to the spectral parameters. We point out that we can construct solutions to this new equation from the recently-found correspondence between Yang-Baxter equations and supersymmetric gauge theories. The $S^2$ partition function of a certain 2d $\mathcal{N}=(2,2)$ quiver gauge theory gives an R-matrix, whereas its FI parameters can be identified with the cluster $y$-variables. ",Cluster-Enriched Yang-Baxter Equation from SUSY Gauge Theories
9,801545071522971648,305650666,Eva Infeld,['New paper: The Total Acquisition Number of Random Geometric Graphs\n<LINK>'],https://arxiv.org/abs/1611.07111,"Let $G$ be a graph in which each vertex initially has weight 1. In each step, the weight from a vertex $u$ to a neighbouring vertex $v$ can be moved, provided that the weight on $v$ is at least as large as the weight on $u$. The total acquisition number of $G$, denoted by $a_t(G)$, is the minimum cardinality of the set of vertices with positive weight at the end of the process. In this paper, we investigate random geometric graphs $G(n,r)$ with $n$ vertices distributed u.a.r. in $[0,\sqrt{n}]^2$ and two vertices being adjacent if and only if their distance is at most $r$. We show that asymptotically almost surely $a_t(G(n,r)) = \Theta( n / (r \lg r)^2)$ for the whole range of $r=r_n \ge 1$ such that $r \lg r \le \sqrt{n}$. By monotonicity, asymptotically almost surely $a_t(G(n,r)) = \Theta(n)$ if $r < 1$, and $a_t(G(n,r)) = \Theta(1)$ if $r \lg r > \sqrt{n}$. ",The Total Acquisition Number of Random Geometric Graphs
10,801340397884764160,797433864,Danilo J. Rezende,['Checkout our new paper on unsupervised learning of options that maximize control: Variational Intrinsic Control. <LINK> <LINK>'],https://arxiv.org/abs/1611.07507,"In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks. ",Variational Intrinsic Control
11,801319437630656512,4831156173,Ricardo Romero,['New paper posted to the arXiv\n<LINK>'],https://arxiv.org/abs/1611.07446,"An extended spin-space model in $7+1$ dimensions is presented that describes the standard-model electroweak quark sector. Up to four generations of massless and massive quarks and two-Higgs doublets derive from the associated representation space, in addition to the W- and Z-vector bosons. Other mass operators are obtained that put restrictions on additional non-Higgs scalars and their vacuum expectation value. After symmetry breaking, the scalar components give rise to a hierarchy effect vertically (within doublets) associated to the Higgs fields, and horizontally (within generations) associated to the non-Higgs elements. ","Quark horizontal flavor symmetry and two-Higgs doublet in
  (7+1)-dimensional extended spin space"
12,801262058515746818,2545244784,Joss Bland-Hawthorn,['Check out our exciting new monster SAMI paper - discovering new class of stellar systems anticipated by Naab:\n<LINK> <LINK>'],https://arxiv.org/abs/1611.07039,"Recent cosmological hydrodynamical simulations suggest that integral field spectroscopy can connect the high-order stellar kinematic moments h3 (~skewness) and h4 (~kurtosis) in galaxies to their cosmological assembly history. Here, we assess these results by measuring the stellar kinematics on a sample of 315 galaxies, without a morphological selection, using 2D integral field data from the SAMI Galaxy Survey. A proxy for the spin parameter ($\lambda_{R_e}$) and ellipticity ($\epsilon_e$) are used to separate fast and slow rotators; there exists a good correspondence to regular and non-regular rotators, respectively, as also seen in earlier studies. We confirm that regular rotators show a strong h3 versus $V/\sigma$ anti-correlation, whereas quasi-regular and non-regular rotators show a more vertical relation in h3 and $V/\sigma$. Motivated by recent cosmological simulations, we develop an alternative approach to kinematically classify galaxies from their individual h3 versus $V/\sigma$ signatures. We identify five classes of high-order stellar kinematic signatures using Gaussian mixture models. Class 1 corresponds to slow rotators, whereas Classes 2-5 correspond to fast rotators. We find that galaxies with similar $\lambda_{R_e}-\epsilon_e$ values can show distinctly different h3-$V/\sigma$ signatures. Class 5 objects are previously unidentified fast rotators that show a weak h3 versus $V/\sigma$ anti-correlation. These objects are predicted to be disk-less galaxies formed by gas-poor mergers. From morphological examination, however, there is evidence for large stellar disks. Instead, Class 5 objects are more likely disturbed galaxies, have counter-rotating bulges, or bars in edge-on galaxies. Finally, we interpret the strong anti-correlation in h3 versus $V/\sigma$ as evidence for disks in most fast rotators, suggesting a dearth of gas-poor mergers among fast rotators. ","The SAMI Galaxy Survey: Revisiting Galaxy Classification Through
  High-Order Stellar Kinematics"
13,801128105360392192,48856249,Alex Wozniakowski,['A new paper on the topological design of protocols with Jaffe and Liu <LINK> #quantum #topology #design <LINK>'],https://arxiv.org/abs/1611.06447,"We give a topological simulation for tensor networks that we call the two-string model. In this approach we give a new way to design protocols, and we discover a new multipartite quantum communication protocol. We introduce the notion of topologically-compressed transformations. Our new protocol can implement multiple, non-local compressed transformations among multi-parties using one multipartite resource state. ",Constructive Simulation and Topological Design of Protocols
14,801035141317394436,623649189,Giovanni S. Alberti,['New paper on critical points in elliptic equations <LINK> #maths #inverseproblems'],https://arxiv.org/abs/1611.06989,"This paper concerns the existence of critical points for solutions to second order elliptic equations of the form $\nabla\cdot \sigma(x)\nabla u=0$ posed on a bounded domain $X$ with prescribed boundary conditions. In spatial dimension $n=2$, it is known that the number of critical points (where $\nabla u=0$) is related to the number of oscillations of the boundary condition independently of the (positive) coefficient $\sigma$. We show that the situation is different in dimension $n\geq3$. More precisely, we obtain that for any fixed (Dirichlet or Neumann) boundary condition for $u$ on $\partial X$, there exists an open set of smooth coefficients $\sigma(x)$ such that $\nabla u$ vanishes at least at one point in $X$. By using estimates related to the Laplacian with mixed boundary conditions, the result is first obtained for a piecewise constant conductivity with infinite contrast, a problem of independent interest. A second step shows that the topology of the vector field $\nabla u$ on a subdomain is not modified for appropriate bounded, sufficiently high-contrast, smooth coefficients $\sigma(x)$. These results find applications in the class of hybrid inverse problems, where optimal stability estimates for parameter reconstruction are obtained in the absence of critical points. Our results show that for any (finite number of) prescribed boundary conditions, there are coefficients $\sigma(x)$ for which the stability of the reconstructions will inevitably degrade. ","Critical Points for Elliptic Equations with Prescribed Boundary
  Conditions"
15,801022290985160704,804483812,Matteo Fasiolo,['New importance sampling paper on arXiv: <LINK> (Badly written) Julia code for the sampler: <LINK>'],https://arxiv.org/abs/1611.06874,"This work proposes a novel method through which local information about the target density can be used to construct an efficient importance sampler. The backbone of the proposed method is the Incremental Mixture Importance Sampling (IMIS) algorithm of Raftery and Bao (2010), which builds a mixture importance distribution incrementally, by positioning new mixture components where the importance density lacks mass, relative to the target. The key innovation proposed here is that the mixture components used by IMIS are local approximations to the target density. In particular, their mean vectors and covariance matrices are constructed by numerically solving certain differential equations, whose solution depends on the gradient field of the target log-density. The new sampler has a number of advantages: a) it provides an extremely parsimonious parametrization of the mixture importance density, whose configuration effectively depends only on the shape of the target and on a single free parameter representing pseudo-time; b) it scales well with the dimensionality of the target; c) it can deal with targets that are not log- concave. The performance of the proposed approach is demonstrated on a synthetic non-Gaussian multimodal density, defined on up to eighty dimensions, and on a Bayesian logistic regression model, using the Sonar data-set. The Julia code implementing the importance sampler proposed here can be found at https:/github.com/mfasiolo/LIMIS. ",Langevin Incremental Mixture Importance Sampling
16,800893871353794560,96779364,Arnab Bhattacharyya,"[""New paper: <LINK>. First time (I'm aware of) the regularity lemma has come in useful for a question in coding theory!""]",http://arxiv.org/abs/1611.06980,"A locally correctable code (LCC) is an error correcting code that allows correction of any arbitrary coordinate of a corrupted codeword by querying only a few coordinates. We show that any {\em zero-error} $2$-query locally correctable code $\mathcal{C}: \{0,1\}^k \to \Sigma^n$ that can correct a constant fraction of corrupted symbols must have $n \geq \exp(k/\log|\Sigma|)$. We say that an LCC is zero-error if there exists a non-adaptive corrector algorithm that succeeds with probability $1$ when the input is an uncorrupted codeword. All known constructions of LCCs are zero-error. Our result is tight upto constant factors in the exponent. The only previous lower bound on the length of 2-query LCCs over large alphabet was $\Omega\left((k/\log|\Sigma|)^2\right)$ due to Katz and Trevisan (STOC 2000). Our bound implies that zero-error LCCs cannot yield $2$-server private information retrieval (PIR) schemes with sub-polynomial communication. Since there exists a $2$-server PIR scheme with sub-polynomial communication (STOC 2015) based on a zero-error $2$-query locally decodable code (LDC), we also obtain a separation between LDCs and LCCs over large alphabet. For our proof of the result, we need a new decomposition lemma for directed graphs that may be of independent interest. Given a dense directed graph $G$, our decomposition uses the directed version of Szemer\'edi regularity lemma due to Alon and Shapira (STOC 2003) to partition almost all of $G$ into a constant number of subgraphs which are either edge-expanding or empty. ",Lower bounds for 2-query LCCs over large alphabet
17,800151383064252417,2956121356,Russ Salakhutdinov,"['New paper on the Quantitative Analysis of Decoder-Based Generative Models: <LINK>\nwith Tony Wu, Yura Burda and Roger Grosse']",https://arxiv.org/abs/1611.04273,"The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at this https URL Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution. ",On the Quantitative Analysis of Decoder-Based Generative Models
18,799576212607799297,4249537197,Christian Wolf,"['Our new paper: object localization with Fully Convolutional Networks (even regression output), context with 2D-LSTMs <LINK> <LINK>']",https://arxiv.org/abs/1611.05664,"The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing. ",Learning to detect and localize many objects from few examples
19,799550103593910273,822867138,Bradley Kavanagh,"['New paper out today: ""Signatures of Earth-Scattering in the direct detection of #DarkMatter"" <LINK> <LINK>', 'Animations showing daily modulation of #DarkMatter direct detection rate available on @figshare: https://t.co/n3tu59S4VO https://t.co/d9JbZhhtAd', 'The #code for the calculations is also available at https://t.co/ommhGGKdWL (with more to be added soon!) #OpenScience']",https://arxiv.org/abs/1611.05453,"Direct detection experiments search for the interactions of Dark Matter (DM) particles with nuclei in terrestrial detectors. But if these interactions are sufficiently strong, DM particles may scatter in the Earth, affecting their distribution in the lab. We present a new analytic calculation of this `Earth-scattering' effect in the regime where DM particles scatter at most once before reaching the detector. We perform the calculation self-consistently, taking into account not only those particles which are scattered away from the detector, but also those particles which are deflected towards the detector. Taking into account a realistic model of the Earth and allowing for a range of DM-nucleon interactions, we present the EarthShadow code, which we make publicly available, for calculating the DM velocity distribution after Earth-scattering. Focusing on low-mass DM, we find that Earth-scattering reduces the direct detection rate at certain detector locations while increasing the rate in others. The Earth's rotation induces a daily modulation in the rate, which we find to be highly sensitive to the detector latitude and to the form of the DM-nucleon interaction. These distinctive signatures would allow us to unambiguously detect DM and perhaps even identify its interactions in regions of the parameter space within the reach of current and future experiments. ",Signatures of Earth-scattering in the direct detection of Dark Matter
20,799423126195044353,31961301,Alan Godoy,['New paper online: Cross-Domain Face Verification: Matching ID Document and Self-Portrait  Photographs <LINK>'],https://arxiv.org/abs/1611.05755,"Cross-domain biometrics has been emerging as a new necessity, which poses several additional challenges, including harsh illumination changes, noise, pose variation, among others. In this paper, we explore approaches to cross-domain face verification, comparing self-portrait photographs (""selfies"") to ID documents. We approach the problem with proper image photometric adjustment and data standardization techniques, along with deep learning methods to extract the most prominent features from the data, reducing the effects of domain shift in this problem. We validate the methods using a novel dataset comprising 50 individuals. The obtained results are promising and indicate that the adopted path is worth further investigation. ","Cross-Domain Face Verification: Matching ID Document and Self-Portrait
  Photographs"
21,799091598487416832,3199605543,Afonso S. Bandeira,['New 6-page paper in Random Matrix Theory! <LINK>'],https://arxiv.org/abs/1611.04505,"We prove that Kendall's Rank correlation matrix converges to the Mar\v{c}enko-Pastur law, under the assumption that the observations are i.i.d random vectors $X_1$, $\dots$, $X_n$ with components that are independent and absolutely continuous with respect to the Lebesgue measure. This is the first result on the empirical spectral distribution of a multivariate $U$-statistic. ",Mar\v{c}enko-Pastur Law for Kendall's Tau
22,798809652955746305,112462367,Dan Lucas,"['We just submitted our new paper on unstable periodic orbits in 3D turbulence to JFM, the arXiv preprint is here: <LINK>']",https://arxiv.org/abs/1611.04829,"By extracting unstable invariant solutions directly from body-forced three-dimensional turbulence, we study the dynamical processes at play when the forcing is large scale and either unidirectional in the momentum or the vorticity equations. In the former case, the dynamical processes familiar from recent work on linearly-stable shear flows - variously called the Self-Sustaining Process (Waleffe 1997) or Vortex-Wave Interaction (Hall & Smith 1991; Hall & Sherwin 2010) - are important even when the base flow is linearly unstable. In the latter case, where the forcing drives Taylor-Green vortices, a number of mechanisms are observed from the various types of periodic orbits isolated. In particular, two different transient growth mechanisms are discussed to explain the more complex states found. ",Sustaining processes from recurrent flows in body-forced turbulence
23,798787247889727489,40639812,Colin Cotter,['New paper submitted on compatible FEM for Eady frontogenesis model. <LINK>'],http://arxiv.org/abs/1611.04929,"A vertical slice model is developed for the Euler-Boussinesq equations with a constant temperature gradient in the direction normal to the slice (the Eady-Boussinesq model). The model is a solution of the full three-dimensional equations with no variation normal to the slice, which is an idealized problem used to study the formation and subsequent evolution of weather fronts. A compatible finite element method is used to discretise the governing equations. To extend the Charney-Phillips grid staggering in the compatible finite element framework, we use the same node locations for buoyancy as the vertical part of velocity and apply a transport scheme for a partially continuous finite element space. For the time discretisation, we solve the semi-implicit equations together with an explicit strong-stability-preserving Runge-Kutta scheme to all of the advection terms. The model reproduces several quasi-periodic lifecycles of fronts despite the presence of strong discontinuities. An asymptotic limit analysis based on the semi-geostrophic theory shows that the model solutions are converging to a solution in cross-front geostrophic balance. The results are consistent with the previous results using finite difference methods, indicating that the compatible finite element method is performing as well as finite difference methods for this test problem. We observe dissipation of kinetic energy of the cross-front velocity in the model due to the lack of resolution at the fronts, even though the energy loss is not likely to account for the large gap on the strength of the fronts between the model result and the semi-geostrophic limit solution. ","Vertical slice modelling of nonlinear Eady waves using a compatible
  finite element method"
24,798621534449377281,14680270,Ricky Egeland,['New paper accepted! It turns out our Sun is a little less active compared to the stars than was previously thought: <LINK>'],https://arxiv.org/abs/1611.04540,"The most commonly used index of stellar magnetic activity is the instrumental flux scale of singly-ionized calcium H & K line core emission, S, developed by the Mount Wilson Observatory (MWO) HK Project, or the derivative index R'_HK. Accurately placing the Sun on the S scale is important for comparing solar activity to that of the Sun-like stars. We present previously unpublished measurements of the reflected sunlight from the Moon using the second-generation MWO HK photometer during solar cycle 23 and determine cycle minimum S_min,23 = 0.1634 +/- 0.0008, amplitude Delta S_23 = 0.0143 +/- 0.0012, and mean <S_23> = 0.1701 +/- 0.0005. By establishing a proxy relationship with the closely related National Solar Observatory Sacramento Peak calcium K emission index, itself well-correlated with the Kodaikanal Observatory plage index, we extend the MWO S time series to cover cycles 15-24 and find on average <S_min> = 0.1621 +/- 0.0008, <Delta S_cyc> = 0.0145 +/- 0.0012, <S_cyc> = 0.1694 +/- 0.0005. Our measurements represent an improvement over previous estimates which relied on stellar measurements or solar proxies with non-overlapping time series. We find good agreement from these results with measurements by the Solar-Stellar Spectrograph at Lowell Observatory, an independently calibrated instrument, which gives us additional confidence that we have accurately placed the Sun on the S-index flux scale. ",The Mount Wilson Observatory S-index of the Sun
25,798442097003102208,1523174335,Kolja Kleineberg,"['Exited! New paper w @DirkHelbing: ""Collective navigation of complex networks: Participatory greedy routing"" <LINK> \n@cxdig <LINK>']",https://arxiv.org/abs/1611.04395,"Many networks are used to transfer information or goods, in other words, they are navigated. The larger the network, the more difficult it is to navigate efficiently. Indeed, information routing in the Internet faces serious scalability problems due to its rapid growth, recently accelerated by the rise of the Internet of Things. Large networks like the Internet can be navigated efficiently if nodes, or agents, actively forward information based on hidden maps underlying these systems. However, in reality most agents will deny to forward messages, which has a cost, and navigation is impossible. Can we design appropriate incentives that lead to participation and global navigability? Here, we present an evolutionary game where agents share the value generated by successful delivery of information or goods. We show that global navigability can emerge, but its complete breakdown is possible as well. Furthermore, we show that the system tends to self-organize into local clusters of agents who participate in the navigation. This organizational principle can be exploited to favor the emergence of global navigability in the system. ",Collective navigation of complex networks: Participatory greedy routing
26,798435005223596032,1710697381,Diego F. Torres,['New paper today: <LINK> MW study of a new asynchronous magnetic CV and a flaring X-ray source. <LINK>'],https://arxiv.org/abs/1611.04194,"In search for the counterpart to the Fermi-LAT source 3FGL J0838.8-2829, we performed a multi-wavelength campaign, in the X-ray band with Swift and XMM-Newton, performed infrared, optical (with OAGH, ESO-NTT and IAC80) and radio (ATCA) observations, as well as analysed archival hard X-ray data taken by INTEGRAL. We report on three X-ray sources consistent with the position of the Fermi-LAT source. We confirm the identification of the brightest object, RX J0838-2827, as a magnetic cataclysmic variable, that we recognize as an asynchronous system (not associated with the Fermi-LAT source). RX J0838-2827, is extremely variable in the X-ray and optical bands, and timing analysis reveals the presence of several periodicities modulating its X-ray and optical emission. The most evident modulations are interpreted as due to the binary system orbital period of ~1.64h and the white dwarf spin period of ~1.47h. A strong flux modulation at ~15h is observed at all energy bands, consistent with the beat frequency between spin and orbital periods. Optical spectra show prominent Hbeta, HeI and HeII emission lines Doppler-modulated at the orbital period and at the beat period. Therefore, RX J0838-2827, accretes through a diskless configuration and could be either a strongly asynchronous polar or a rare example of a pre-polar system in its way to reach synchronism. Among the other two X-ray sources, XMM J083850.4-282759 showed a variable X-ray emission, with a powerful flare lasting ~600s, similar to what is observed in transitional millisecond pulsars during the sub-luminous disc state: that would possibly associate this source with the Fermi-LAT source. ","Multi-band study of RX J0838-2827 and XMM J083850.4-282759: a new
  asynchronous magnetic cataclysmic variable and a candidate transitional
  millisecond pulsar"
27,798299476691734528,180240998,Franck Marchis,['Our new paper: Shape model of main-belt asteroid (130) Elektra from @ESO SPHERE &amp; @Keckobservatory data <LINK>'],https://arxiv.org/abs/1611.03632,"Asteroid (130) Elektra belongs to one of the six known triple asteroids in the main belt, so its mass has been reliably determined. We aim to use all available disk-resolved images of (130) Elektra obtained by the SPHERE instrument at VLT and by the Nirc2 of the Keck telescope together with the disk-integrated photometry to determine its shape model and its size. The volume can be then used in combination with the known mass to derive the bulk density of the primary. We apply the All-Data Asteroid Modeling (ADAM) algorithm to the optical disk-integrated data, 2 disk-resolved images obtained by the SPHERE instrument and 13 disk-resolved images from the Nirc2 of the Keck telescope, and derive the shape model and size of Elektra. We present the shape model, volume-equivalent diameter (199$\pm$7 km) and bulk density (1.60$\pm$0.13 g cm$^{-3}$) of the C-type asteroid Elektra. ","Shape model of asteroid (130) Elektra from optical photometry and
  disk-resolved images from VLT/SPHERE and Nirc2/Keck"
28,798257568145666049,29843511,Nando de Freitas üè≥Ô∏è‚Äçüåà,"['Our new paper on Learning to Learn, including a discussion of some good recent papers on this at #ICLR2017  <LINK>']",https://arxiv.org/abs/1611.03824,"We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning. ",Learning to Learn without Gradient Descent by Gradient Descent
29,798051071813746689,764458632679333889,Veelasha Moonsamy,['New to SCA on mobile devices? Our survey paper should bring you up to speed on the topic: <LINK> (CC:@rspreitzer_ )'],https://arxiv.org/abs/1611.03748,"Side-channel attacks on mobile devices have gained increasing attention since their introduction in 2007. While traditional side-channel attacks, such as power analysis attacks and electromagnetic analysis attacks, required physical presence of the attacker as well as expensive equipment, an (unprivileged) application is all it takes to exploit the leaking information on modern mobile devices. Given the vast amount of sensitive information that are stored on smartphones, the ramifications of side-channel attacks affect both the security and privacy of users and their devices. In this paper, we propose a new categorization system for side-channel attacks, which is necessary as side-channel attacks have evolved significantly since their scientific investigations during the smart card era in the 1990s. Our proposed classification system allows to analyze side-channel attacks systematically, and facilitates the development of novel countermeasures. Besides this new categorization system, the extensive survey of existing attacks and attack strategies provides valuable insights into the evolving field of side-channel attacks, especially when focusing on mobile devices. We conclude by discussing open issues and challenges in this context and outline possible future research directions. ","Systematic Classification of Side-Channel Attacks: A Case Study for
  Mobile Devices"
30,797019976481783808,4603400475,Alexander Novikov,"['[TensorNet] New paper on tensor compression, now for conv layers. <LINK>, <LINK>\nLets chat at #nips2016!']",https://arxiv.org/abs/1611.03214,"Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80x network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset. ",Ultimate tensorization: compressing convolutional and FC layers alike
31,796998541130559488,263265637,Dennis Prangle,['New paper! Rare event methods for ABC <LINK>'],http://arxiv.org/abs/1611.02492,"Approximate Bayesian computation (ABC) methods permit approximate inference for intractable likelihoods when it is possible to simulate from the model. However they perform poorly for high dimensional data, and in practice must usually be used in conjunction with dimension reduction methods, resulting in a loss of accuracy which is hard to quantify or control. We propose a new ABC method for high dimensional data based on rare event methods which we refer to as RE-ABC. This uses a latent variable representation of the model. For a given parameter value, we estimate the probability of the rare event that the latent variables correspond to data roughly consistent with the observations. This is performed using sequential Monte Carlo and slice sampling to systematically search the space of latent variables. In contrast standard ABC can be viewed as using a more naive Monte Carlo estimate. We use our rare event probability estimator as a likelihood estimate within the pseudo-marginal Metropolis-Hastings algorithm for parameter inference. We provide asymptotics showing that RE-ABC has a lower computational cost for high dimensional data than standard ABC methods. We also illustrate our approach empirically, on a Gaussian distribution and an application in infectious disease modelling. ","A rare event approach to high dimensional Approximate Bayesian
  computation"
32,796654031045357569,778228628601602048,BIGWaves,['New @LIGO paper on the search for unmodelled #GravitationalWaves  <LINK> No detections (aside from @iamgw150914) yet'],http://arxiv.org/abs/1611.02972,"We present the results from an all-sky search for short-duration gravitational waves in the data of the first run of the Advanced LIGO detectors between September 2015 and January 2016. The search algorithms use minimal assumptions on the signal morphology, so they are sensitive to a wide range of sources emitting gravitational waves. The analyses target transient signals with duration ranging from milliseconds to seconds over the frequency band of 32 to 4096 Hz. The first observed gravitational-wave event, GW150914, has been detected with high confidence in this search; other known gravitational-wave events fall below the search's sensitivity. Besides GW150914, all of the search results are consistent with the expected rate of accidental noise coincidences. Finally, we estimate rate-density limits for a broad range of non-BBH transient gravitational-wave sources as a function of their gravitational radiation emission energy and their characteristic frequency. These rate-density upper-limits are stricter than those previously published by an order-of-magnitude. ","All-sky search for short gravitational-wave bursts in the first Advanced
  LIGO run"
33,796406240867778560,778200301,vinko zlatiƒá ‚òÆ,['Our new paper on theory of color avoiding percolation - a technical follow up of previous paper <LINK>'],https://arxiv.org/abs/1611.02617,"Many real world networks have groups of similar nodes which are vulnerable to the same failure or adversary. Nodes can be colored in such a way that colors encode the shared vulnerabilities. Using multiple paths to avoid these vulnerabilities can greatly improve network robustness. Color-avoiding percolation provides a theoretical framework for analyzing this scenario, focusing on the maximal set of nodes which can be connected via multiple color-avoiding paths. In this paper we extend the basic theory of color-avoiding percolation that was published in [Krause et. al., Phys. Rev. X 6 (2016) 041022]. We explicitly account for the fact that the same particular link can be part of different paths avoiding different colors. This fact was previously accounted for with a heuristic approximation. We compare this approximation with a new, more exact theory and show that the new theory is substantially more accurate for many avoided colors. Further, we formulate our new theory with differentiated node functions, as senders/receivers or as transmitters. In both functions, nodes can be explicitly trusted or avoided. With only one avoided color we obtain standard percolation. With one by one avoiding additional colors, we can understand the critical behavior of color avoiding percolation. For heterogeneous color frequencies, we find that the colors with the largest frequencies control the critical threshold and exponent. Colors of small frequencies have only a minor influence on color avoiding connectivity, thus allowing for approximations. ",Color-avoiding percolation
34,796339920910610433,65528671,G.-A. Bilodeau,['New paper on multiple object tracking for urban scenes: <LINK>'],https://arxiv.org/abs/1611.02364,"Recently, the Kernelized Correlation Filters tracker (KCF) achieved competitive performance and robustness in visual object tracking. On the other hand, visual trackers are not typically used in multiple object tracking. In this paper, we investigate how a robust visual tracker like KCF can improve multiple object tracking. Since KCF is a fast tracker, many can be used in parallel and still result in fast tracking. We build a multiple object tracking system based on KCF and background subtraction. Background subtraction is applied to extract moving objects and get their scale and size in combination with KCF outputs, while KCF is used for data association and to handle fragmentation and occlusion problems. As a result, KCF and background subtraction help each other to take tracking decision at every frame. Sometimes KCF outputs are the most trustworthy (e.g. during occlusion), while in some other case, it is the background subtraction outputs. To validate the effectiveness of our system, the algorithm is demonstrated on four urban video recordings from a standard dataset. Results show that our method is competitive with state-of-the-art trackers even if we use a much simpler data association step. ","Multiple Object Tracking with Kernelized Correlation Filters in Urban
  Mixed Traffic"
35,796296649106661376,40639812,Colin Cotter,"['New paper submitted with my student Andrea, on  multiscale properties of energy conserving FE schemes. <LINK>']",https://arxiv.org/abs/1611.02623,We analyse the multiscale properties of energy-conserving upwind-stabilised finite element discretisations of the two-dimensional incompressible Euler equations. We focus our attention on two particular methods: the Lie derivative discretisation introduced in Natale and Cotter (2016a) and the Streamline Upwind/Petrov-Galerkin (SUPG) discretisation of the vorticity advection equation. Such discretisations provide control on enstrophy by modelling different types of scale interactions. We quantify the performance of the schemes in reproducing the non-local energy backscatter that characterises two-dimensional turbulent flows. ,"Scale-selective dissipation in energy-conserving finite element schemes
  for two-dimensional turbulence"
36,795807503548383232,44679505,Jaydeep Bardhan,"[""our new paper's up on the arxiv! predicting solvation thermodynamics using dielectric continuum solvent models\n<LINK>""]",https://arxiv.org/abs/1611.02150,"We demonstrate that with two small modifications, the popular dielectric continuum model is capable of predicting, with high accuracy, ion solvation thermodynamics in numerous polar solvents, and ion solvation free energies in water--co-solvent mixtures. The first modification involves perturbing the macroscopic dielectric-flux interface condition at the solute--solvent interface with a nonlinear function of the local electric field, giving what we have called a solvation-layer interface condition (SLIC). The second modification is a simple treatment of the microscopic interface potential (static potential). We show that the resulting model exhibits high accuracy without the need for fitting solute atom radii in a state-dependent fashion. Compared to experimental results in nine water--co-solvent mixtures, SLIC predicts transfer free energies to within 2.5 kJ/mol. The co-solvents include both protic and aprotic species, as well as biologically relevant denaturants such as urea and dimethylformamide. Furthermore, our results indicate that the interface potential is essential to reproduce entropies and heat capacities. The present work, together with previous studies of SLIC illustrating its accuracy for biomolecules in water, indicates it as a promising dielectric continuum model for accurate predictions of molecular solvation in a wide range of conditions. ","Predicting Solvation Free Energies and Thermodynamics in Polar Solvents
  and Mixtures Using a Solvation-Layer Interface Condition"
37,795804998030520321,4438354094,Tom Wong,"[""New paper showing a direct equivalence between coined quantum walks and Szegedy's quantum walk, including search. <LINK> <LINK>""]",https://arxiv.org/abs/1611.02238,"Szegedy's quantum walk is a quantization of a classical random walk or Markov chain, where the walk occurs on the edges of the bipartite double cover of the original graph. To search, one can simply quantize a Markov chain with absorbing vertices. Recently, Santos proposed two alternative search algorithms that instead utilize the sign-flip oracle in Grover's algorithm rather than absorbing vertices. In this paper, we show that these two algorithms are exactly equivalent to two algorithms involving coined quantum walks, which are walks on the vertices of the original graph with an internal degree of freedom. The first scheme is equivalent to a coined quantum walk with one walk-step per query of Grover's oracle, and the second is equivalent to a coined quantum walk with two walk-steps per query of Grover's oracle. These equivalences lie outside the previously known equivalence of Szegedy's quantum walk with absorbing vertices and the coined quantum walk with the negative identity operator as the coin for marked vertices, whose precise relationships we also investigate. ",Equivalence of Szegedy's and Coined Quantum Walks
38,795548307871461376,34376328,Tal Linzen,"['New paper on the syntactic abilities of contemporary recurrent neural networks (with Emmanuel Dupoux &amp; @yoavgo): <LINK>', ""Here's the part where Google's massive language model makes a lot of agreement attraction errors: https://t.co/j5Sf1ZZFIE"", ""And the LSTM unit that seems to track whether or not it's in the middle of an embedded clause right now: https://t.co/Q3ZOhZqf8y""]",https://arxiv.org/abs/1611.01368,"The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured. ",Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies
39,795542168291147777,3124305238,Anna Scaife,"[""Beautiful separation of ICM components in @cjriseley's new paper MACSJ0025: <LINK> @jodrellbank @CSIRO_ATNF #GMRTtelescope <LINK>""]",https://arxiv.org/abs/1611.01273,"Mergers of galaxy clusters are among the most energetic events in the Universe. These events have significant impact on the intra-cluster medium, depositing vast amounts of energy - often in the form of shocks - as well as heavily influencing the properties of the constituent galaxy population. Many clusters have been shown to host large-scale diffuse radio emission, known variously as radio haloes and relics. These sources arise as a result of electron (re-)acceleration in cluster-scale magnetic fields, although the processes by which this occurs are still poorly understood. We present new, deep radio observations of the high-redshift galaxy cluster MACS J0025.4$-$1222, taken with the GMRT at 325 MHz, as well as new analysis of all archival $Chandra$ X-ray observations. We aim to investigate the potential of diffuse radio emission and categorise the radio population of this cluster, which has only been covered previously by shallow radio surveys. We produce low-resolution maps of MACS J0025.4$-$1222 through a combination of uv-tapering and subtracting the compact source population. Radial surface brightness and mass profiles are derived from the $Chandra$ data. We also derive a 2D map of the ICM temperature. For the first time, two sources of diffuse radio emission are detected in MACS J0025.4$-$1222, on linear scales of several hundred kpc. Given the redshift of the cluster and the assumed cosmology, these sources appear to be consistent with established trends in power scaling relations for radio relics. The X-ray temperature map presents evidence of an asymmetric temperature profile and tentative identification of a temperature jump associated with one relic. We classify the pair of diffuse radio sources in this cluster as a pair of radio relics, given their consistency with scaling relations, location toward the cluster outskirts, and the available X-ray data. ","Diffuse radio emission in MACS J0025.4$-$1222: the effect of a major
  merger on bulk separation of ICM components"
40,795521530214154240,47059480,Daniela Huppenkothen,"['New paper on the arxiv: machine learning on black hole light curves! <LINK> with @purplefroglet @davidwhogg @amuellerml', '@profjsb @purplefroglet @davidwhogg @amuellerml @stefanvdwalt yes! Still looking for a convenient spot to upload 80GB of data. Suggestions?']",https://arxiv.org/abs/1611.01332,"Among the population of known galactic black hole X-ray binaries, GRS 1915+105 stands out in multiple ways. It has been in continuous outburst since 1992, and has shown a wide range of different states that can be distinguished by their timing and spectral properties. These states, also observed in IGR J17091-3624, have in the past been linked to accretion dynamics. Here, we present the first comprehensive study into the long-term evolution of GRS 1915+105, using the entire data set observed with RXTE over its sixteen-year lifetime. We develop a set of descriptive features allowing for automatic separation of states, and show that supervised machine learning in the form of logistic regression and random forests can be used to efficiently classify the entire data set. For the first time, we explore the duty cycle and time evolution of states over the entire sixteen-year time span, and find that the temporal distribution of states has significantly changed over the span of the observations. We connect the machine classification with physical interpretations of the phenomenology in terms of chaotic and stochastic processes. ",Exploring the Long-Term Evolution of GRS 1915+105
41,795439611409354752,460069521,Andrew Francis,"['Very excited about my new paper out with Mike Steel and Dave Bryant: ""Can we future-proof consensus trees?"". \n<LINK>', 'We show that generally speaking, the answer is ""no"".  If you add new taxa to a set of trees, your consensus tree is not safe. #phylogeny', 'It\'s an ""Arrow-type"" theorem for #phylogenetic trees.  (Arrow proved non-existence of consistent voting methods in the 1950s).']",https://arxiv.org/abs/1611.01225,"Consensus methods are widely used for combining phylogenetic trees into a single estimate of the evolutionary tree for a group of species. As more taxa are added, the new source trees may begin to tell a different evolutionary story when restricted to the original set of taxa. However, if the new trees, restricted to the original set of taxa, were to agree exactly with the earlier trees, then we might hope that their consensus would either agree with or resolve the original consensus tree. In this paper, we ask under what conditions consensus methods exist that are 'future proof' in this sense. While we show that some methods (e.g. Adams consensus) have this property for specific types of input, we also establish a rather surprising `no-go' theorem: there is no 'reasonable' consensus method that satisfies the future-proofing property in general. We then investigate a second notion of 'future proofing' for consensus methods, in which trees (rather than taxa) are added, and establish some positive and negative results. We end with some questions for future work. ",Can we 'future-proof' consensus trees?
42,794229273838006272,14138114,Casey Law üí•,"['Our work on a new (better) way to estimate FRB rates is out today: <LINK>.\n\nPaper 1 of 2 from LANL/realfast collaboration. <LINK>', 'Upshot:\n1) the FRB brightness distribution is not euclidean,\n2) that pushes the all-sky rate even lower than typically assumed.', '@basebandgeek This result supports the general idea of a latitude dependent rate, but favors an much flatter flux distribution.', ""@basebandgeek But I do need to reread that paper to be sure we're talking about the same thing...""]",https://arxiv.org/abs/1611.00458,"This paper presents the non-homogeneous Poisson process (NHPP) for modeling the rate of fast radio bursts (FRBs) and other infrequently observed astronomical events. The NHPP, well-known in statistics, can model changes in the rate as a function of both astronomical features and the details of an observing campaign. This is particularly helpful for rare events like FRBs because the NHPP can combine information across surveys, making the most of all available information. The goal of the paper is two-fold. First, it is intended to be a tutorial on the use of the NHPP. Second, we build an NHPP model that incorporates beam patterns and a power law flux distribution for the rate of FRBs. Using information from 12 surveys including 15 detections, we find an all-sky FRB rate of 586.88 events per sky per day above a flux of 1 Jy (95\% CI: 271.86, 923.72) and a flux power-law index of 0.91 (95\% CI: 0.57, 1.25). Our rate is lower than other published rates, but consistent with the rate given in Champion et al. 2016. ",The Non-homogeneous Poisson Process for Fast Radio Burst Rates
43,793800608650301440,2956121356,Russ Salakhutdinov,"['New paper on Stochastic Variational Deep Kernel Learning \nwith @andrewgwils, Zhiting Hu, Eric P. Xing\n<LINK>  \n#nips2016 <LINK>']",https://arxiv.org/abs/1611.00336,"Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet. ",Stochastic Variational Deep Kernel Learning
44,793734838457163776,40639812,Colin Cotter,"['New paper submitted on our seamless multilevel ensemble transform particle filter: <LINK>', '@jack_s_hale The term is just taken from Mike Giles.']",https://arxiv.org/abs/1611.00266,"This paper presents a seamless algorithm for the application of the multilevel Monte Carlo (MLMC) method to the ensemble transform particle filter (ETPF). The algorithm uses a combination of optimal coupling transformations between coarse and fine ensembles in difference estimators within a multilevel framework, to minimise estimator variance. It differs from that of Gregory et al. (2016) in that strong coupling between the coarse and fine ensembles is seamlessly maintained during all stages of the assimilation algorithm, instead of using independent transformations to equal weights followed by recoupling with an assignment problem. This modification is found to lead to an increased rate in variance decay between coarse and fine ensembles with level in the hierarchy, a key component of MLMC. This offers the potential for greater computational cost reductions. This is shown, alongside evidence of asymptotic consistency, in numerical examples. ",A Seamless Multilevel Ensemble Transform Particle Filter
45,793654368717406208,3199605543,Afonso S. Bandeira,['New paper on Synchronization over the Special Euclidean Group with applications in robotics <LINK>'],https://arxiv.org/abs/1611.00128,"Many geometric estimation problems take the form of synchronization over the special Euclidean group: estimate the values of a set of poses given noisy measurements of a subset of their pairwise relative transforms. This problem is typically formulated as a maximum-likelihood estimation that requires solving a nonconvex nonlinear program, which is computationally intractable in general. Nevertheless, in this paper we present an algorithm that is able to efficiently recover certifiably globally optimal solutions of this estimation problem in a non-adversarial noise regime. The crux of our approach is the development of a semidefinite relaxation of the maximum-likelihood estimation whose minimizer provides the exact MLE so long as the magnitude of the noise corrupting the available measurements falls below a certain critical threshold; furthermore, whenever exactness obtains, it is possible to verify this fact a posteriori, thereby certifying the optimality of the recovered estimate. We develop a specialized optimization scheme for solving large-scale instances of this semidefinite relaxation by exploiting its low-rank, geometric, and graph-theoretic structure to reduce it to an equivalent optimization problem on a low-dimensional Riemannian manifold, and then design a Riemannian truncated-Newton trust-region method to solve this reduction efficiently. We combine this fast optimization approach with a simple rounding procedure to produce our algorithm, SE-Sync. Experimental evaluation on a variety of simulated and real-world pose-graph SLAM datasets shows that SE-Sync is capable of recovering globally optimal solutions when the available measurements are corrupted by noise up to an order of magnitude greater than that typically encountered in robotics applications, and does so at a computational cost that scales comparably with that of direct Newton-type local search techniques. ","A Certifiably Correct Algorithm for Synchronization over the Special
  Euclidean Group"
46,803146406437548032,3426605115,Wojciech Mazurczyk,['Our new paper on #ransomware (#cryptowall and #locky) detection based on HTTP traffic characteristics: <LINK> #infosec'],https://arxiv.org/abs/1611.08294,"Ransomware is currently the key threat for individual as well as corporate Internet users. Especially dangerous is crypto ransomware that encrypts important user data and it is only possible to recover it once a ransom has been paid. Therefore devising efficient and effective countermeasures is a rising necessity. In this paper we present a novel Software-Defined Networking (SDN) based detection approach that utilizes characteristics of ransomware communication. Based on the observation of network communication of two crypto ransomware families, namely CryptoWall and Locky we conclude that analysis of the HTTP messages' sequences and their respective content sizes is enough to detect such threats. We show feasibility of our approach by designing and evaluating the proof-of-concept SDN-based detection system. Experimental results confirm that the proposed approach is feasible and efficient. ","Software-Defined Networking-based Crypto Ransomware Detection Using HTTP
  Traffic Characteristics"
47,801025701109436417,14699604,James Hensman,"['New paper ""Variational Fourier Features for Gaussian Processes"": <LINK>. 10^6 data in seconds. With @arnosolin &amp; N.Durrande', '@vallens @arnosolin limitation is input dimensionality, so will work very well for decoding, but perhaps need something else to encode.', '@geospacedman @arnosolin working on it. Code is here: https://t.co/mP0Jsl0JHD', '@geospacedman @arnosolin Here we go: https://t.co/zZLs8kp7Oo']",https://arxiv.org/abs/1611.06740,"This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the dataset, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features. ",Variational Fourier features for Gaussian processes
48,798601526893690880,1442906958,Richard Socher,['QRNNs: Faster sequence model for #nlproc \nThey take the best from LSTMs and CNNs\nBlog <LINK>\nPaper <LINK> <LINK>'],https://arxiv.org/abs/1611.01576,"Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks. ",Quasi-Recurrent Neural Networks
