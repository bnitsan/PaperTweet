,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,738741013875949568,4179822388,Martin Rajchl,['Our new paper on  segmentation from bounding boxes (DeepCut) is now available on arXiv: <LINK> @BioMedIAICL'],http://arxiv.org/abs/1605.07866,"In this paper, we propose DeepCut, a method to obtain pixelwise object segmentations given an image dataset labelled with bounding box annotations. It extends the approach of the well-known GrabCut method to include machine learning by training a neural network classifier from bounding box annotations. We formulate the problem as an energy minimisation problem over a densely-connected conditional random field and iteratively update the training targets to obtain pixelwise object segmentations. Additionally, we propose variants of the DeepCut method and compare those to a naive approach to CNN training under weak supervision. We test its applicability to solve brain and lung segmentation problems on a challenging fetal magnetic resonance dataset and obtain encouraging results in terms of accuracy. ","DeepCut: Object Segmentation from Bounding Box Annotations using
  Convolutional Neural Networks"
1,738120961929695232,2228689848,Xu Jia,"[""Check out our new paper on a novel and powerful architecture called 'dynamic filter networks'  <LINK>""]",http://arxiv.org/abs/1605.09673v1,"In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation. ",] Dynamic Filter Networks
2,738057240632926208,3245949691,Rebecca Leane,"[""New paper! We show adding a scalar to Z'+DM models as req'd for unitarity gives new s-wave process for indirect exps <LINK>""]",http://arxiv.org/abs/1605.09382,"We consider the indirect detection signals for a self-consistent hidden $U(1)$ model containing a Majorana dark matter candidate, $\chi$, a dark gauge boson, $Z'$, and a dark Higgs, $s$. Compared with a model containing only a dark matter candidate and $Z'$ mediator, the addition of the scalar provides a mass generation mechanism for the dark sector particles and is required in order to avoid unitarity violation at high energies. We find that the inclusion of the two mediators opens up a new two-body s-wave annihilation channel, $\chi\overline\chi\rightarrow sZ'$. This new process, which is missed in the usual single-mediator simplified model approach, can be the dominant annihilation channel. This provides rich phenomenology for indirect detection searches, allows indirect searches to explore regions of parameter space not accessible with other commonly considered s-wave annihilation processes, and enables both the $Z'$ and scalar couplings to be probed. We examine the phenomenology of the sector with a focus on this new process, and determine the limits on the model parameter space from Fermi data on dwarf spheriodal galaxies and other relevant experiments. ",Dark Forces in the Sky: Signals from Z' and the Dark Higgs
3,738029055522394112,212382437,Paul Smaldino,"['The Natural Selection of Bad Science. New paper on #arXiv from me and @rlmcelreath. <LINK> <LINK>', '@amesoudi @rlmcelreath Thanks Alex! We thought Popper ref would resonate with more readers, but Hull also an influence. May cite in revision']",http://arxiv.org/abs/1605.09511,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favor them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing---no deliberate cheating nor loafing---by scientists, only that publication is a principle factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modeling. We first present a 60-year meta-analysis of statistical power in the behavioral sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ""progeny"", such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level. ",The Natural Selection of Bad Science
4,738001479534039041,45861644,Ali Gregory,"['My new paper on the calibration of #multilevelmontecarlo ensemble forecasts has been posted on #arXiv! Check it out <LINK>!', '@attmcrae cheers! Ha, me, post to Facebook?! When do I ever do that...? ;)']",https://arxiv.org/abs/1605.08701,"Multilevel Monte Carlo can efficiently compute statistical estimates of discretized random variables, for a given error tolerance. Traditionally, only a certain statistic is computed from a particular implementation of multilevel Monte Carlo. This paper considers the multilevel case when one wants to verify and evaluate a single ensemble that forms an empirical approximation to many different statistics, namely an ensemble forecast. We propose a simple algorithm that, in the univariate case, allows one to derive a statistically consistent single ensemble forecast from the hierarchy of ensembles that are formed during an implementation of multilevel Monte Carlo. This ensemble forecast then allows the entire multilevel hierarchy of ensembles to be evaluated using standard ensemble forecast verification techniques. We demonstrate the case of evaluating the calibration of the forecast in this paper. ",On the Calibration of Multilevel Monte Carlo Ensemble Forecasts
5,737872343364685824,3186334207,L Glaser,"['And now for a little self advertisement, my new paper:\n<LINK>', '@emamiru Huh? Why ""invention""? I\'m confused!', '@emamiru I actually had not really read them :D Hooray for the language of bureaucracy!']",http://arxiv.org/abs/1605.09618,"Causal Dynamical Triangulations (CDT) is a non-perturbative quantisation of general relativity. Ho\v{r}ava-Lifshitz gravity on the other hand modifies general relativity to allow for perturbative quan- tisation. Past work has given rise to the speculation that Ho\v{r}ava-Lifshitz gravity might correspond to the continuum limit of CDT. In this paper we add another piece to this puzzle by applying the CDT quantisation prescription directly to Ho\v{r}ava-Lifshitz gravity in 2 dimensions. We derive the continuum Hamiltonian and we show that it matches exactly the Hamiltonian one derives from canonically quantising the Ho\v{r}ava-Lifshitz action. Unlike the standard CDT case, here the intro- duction of a foliated lattice does not impose further restriction on the configuration space and, as a result, lattice quantisation does not leave any imprint on continuum physics as expected. ",Extrinsic curvature in 2-dimensional Causal Dynamical Triangulation
6,737677921968492544,259226866,Ryan Nel,['New TensorFlow paper: [1605.08695] A system for large-scale machine learning #MachineLearning #Analytics\n\n<LINK>'],https://arxiv.org/abs/1605.08695,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications. ",TensorFlow: A system for large-scale machine learning
7,736213513484554241,70831441,Soumith Chintala,"['""Discovering Causal Signals in Images"". Given an image, can you tell which objects cause others? Our new paper. wdyt?<LINK>', '@RahelJhirad why use simulated images when you dont need to...', '@RahelJhirad i think using videos is an easier proposition. The arrow of time is a strong signal.', '@RahelJhirad Either ways, training on image-specific data is not needed, as shown in our paper.']",http://arxiv.org/abs/1605.08179,"This paper establishes the existence of observable footprints that reveal the ""causal dispositions"" of the object categories appearing in collections of images. We achieve this goal in two steps. First, we take a learning approach to observational causal discovery, and build a classifier that achieves state-of-the-art performance on finding the causal direction between pairs of random variables, given samples from their joint distribution. Second, we use our causal direction classifier to effectively distinguish between features of objects and features of their contexts in collections of static images. Our experiments demonstrate the existence of a relation between the direction of causality and the difference between objects and their contexts, and by the same token, the existence of observable signals that reveal the causal dispositions of objects. ",Discovering Causal Signals in Images
8,736122291591864321,134120175,akihiro suzuki,"['Ë´ñÊñá„ÅåÂèóÁêÜ„Åï„Çå„Åæ„Åó„Åü„ÄÇ\nÈáçÂäõÂ¥©Â£äÂûãË∂ÖÊñ∞Êòü„Åã„Çâ„ÅÆ""ÊúÄÂàù„ÅÆÂÖâ""„ÅØ„Å©„ÅÜË¶ã„Åà„Çã„ÅÆ„Åã„ÄÅ„Å®„ÅÑ„ÅÜ„Ç∑„Éü„É•„É¨„Éº„Ç∑„Éß„É≥\n\nOur new paper on the ""first light"" from a core-collapse supernova\n<LINK>', '@jasmine_marker „Éò„Ç§„Éò„Éº„Ç§']",http://arxiv.org/abs/1605.08250,"A two-dimensional special relativistic radiation-hydrodynamics code is developed and applied to numerical simulations of supernova shock breakout in bipolar explosions of a blue supergiant. Our calculations successfully simulate the dynamical evolution of a blast wave in the star and its emergence from the surface. Results of the model with spherical energy deposition show a good agreement with previous simulations. Furthermore, we calculate several models with bipolar energy deposition and compare their results with the spherically symmetric model. The bolometric light curves of the shock breakout emission are calculated by a ray-tracing method. Our radiation-hydrodynamic models indicate that the early part of the shock breakout emission can be used to probe the geometry of the blast wave produced as a result of the gravitational collapse of the iron core. ","2D radiaition-hydrodynamic simulations of supernova shock breakout in
  bipolar explosions of a blue supergiant progenitor"
9,736055744424906753,368330699,Christian Gagn√©,['New paper: Bayesian Hyperparameter Optimization for Ensemble Learning <LINK> with @levesquejc and R. Sabourin #UAI2016'],http://arxiv.org/abs/1605.06394,"In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization. ",Bayesian Hyperparameter Optimization for Ensemble Learning
10,735627549313765376,6222842,Nick Feamster,['New paper on routing detours through surveillance states:\n<LINK>'],http://arxiv.org/abs/1605.07685,"An increasing number of countries are passing laws that facilitate the mass surveillance of Internet traffic. In response, governments and citizens are increasingly paying attention to the countries that their Internet traffic traverses. In some cases, countries are taking extreme steps, such as building new Internet Exchange Points (IXPs), which allow networks to interconnect directly, and encouraging local interconnection to keep local traffic local. We find that although many of these efforts are extensive, they are often futile, due to the inherent lack of hosting and route diversity for many popular sites. By measuring the country-level paths to popular domains, we characterize transnational routing detours. We find that traffic is traversing known surveillance states, even when the traffic originates and ends in a country that does not conduct mass surveillance. Then, we investigate how clients can use overlay network relays and the open DNS resolver infrastructure to prevent their traffic from traversing certain jurisdictions. We find that 84\% of paths originating in Brazil traverse the United States, but when relays are used for country avoidance, only 37\% of Brazilian paths traverse the United States. Using the open DNS resolver infrastructure allows Kenyan clients to avoid the United States on 17\% more paths. Unfortunately, we find that some of the more prominent surveillance states (e.g., the U.S.) are also some of the least avoidable countries. ",Characterizing and Avoiding Routing Detours Through Surveillance States
11,735504459749429248,3008464880,Carnegie Astronomy,['New paper: Reconciling the Stellar and Nebular Spectra of High Redshift Galaxies. incl. @GwenCRudie @crosstrainor <LINK>'],http://arxiv.org/abs/1605.07186,"We present a combined analysis of rest-frame far-UV (1000-2000 A) and rest-frame optical (3600-7000 A) composite spectra formed from very deep observations of a sample of 30 star-forming galaxies with z=2.4+/-0.1, selected to be representative of the full KBSS-MOSFIRE spectroscopic survey. Since the same massive stars are responsible for the observed FUV continuum and the excitation of the observed nebular emission, a self-consistent stellar population synthesis model must simultaneously match the details of the far-UV stellar+nebular continuum and-- when inserted as the excitation source in photoionization models-- account for all observed nebular emission line ratios. We find that only models including massive star binaries, having low stellar metallicity (Z_*/Z_{sun} ~ 0.1) but relatively high ionized gas-phase oxygen abundances (Z_{neb}/Z_{sun} ~ 0.5), can successfully match all of the observational constraints. We argue that this apparent discrepancy is naturally explained by highly super-solar O/Fe [4-5 times (O/Fe)_{sun}], expected for gas whose enrichment is dominated by the products of core-collapse supernovae. Once the correct ionizing spectrum is identified, photoionization models reproduce all of the observed strong emission line ratios, the direct T_e measurement of O/H, and allow accurate measurement of the gas-phase abundance ratios of N/O and C/O -- both of which are significantly sub-solar but, as for O/Fe, are in remarkable agreement with abundance patterns observed in Galactic thick disk, bulge, and halo stars with similar O/H. High nebular excitation is the rule at high-z (and rare at low-z) because of systematically shorter enrichment timescales (<<1 Gyr): low Fe/O environments produce harder (and longer-lived) stellar EUV spectra at a given O/H, enhanced by dramatic effects on the evolution of massive star binaries. ",Reconciling the Stellar and Nebular Spectra of High Redshift Galaxies
12,735502863976280068,2956121356,Russ Salakhutdinov,"['New paper on Path-Normalized Optimization of RNNs with ReLU Activations.\n<LINK>\nwith B. Neyshabur, T. Wu, and N. Srebro']",http://arxiv.org/abs/1605.07154,"We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes. ","Path-Normalized Optimization of Recurrent Neural Networks with ReLU
  Activations"
13,735368116977446912,1728419371,Dr Joanna Barstow,"['New paper: if the TRAPPIST-1 planets are Earth-like, could we detect the signatures with @NASAWebbTelescp? <LINK>']",http://arxiv.org/abs/1605.07352,"The recent discovery of three Earth-sized, potentially habitable planets around a nearby cool star, TRAPPIST-1, has provided three key targets for the upcoming James Webb Space Telescope (JWST). Depending on their atmospheric characteristics and precise orbit configurations, it is possible that any of the three planets may be in the liquid water habitable zone, meaning that they may be capable of supporting life. We find that present-day Earth levels of ozone, if present, would be detectable if JWST observes 60 transits for innermost planet 1b and 30 transits for 1c and 1d. ","Habitable worlds with JWST: transit spectroscopy of the TRAPPIST-1
  system?"
14,734943258367791109,4313989761,Dr. Andreas Faisst,"['My new paper is out: Are #galaxies capable of reionizing the #Universe? #science @Caltech\n<LINK> <LINK>', '@bradpholden thanks!']",http://arxiv.org/abs/1605.06507,"The intrinsic escape fraction of ionizing Lyman continuum photons ($f_{esc}$) is crucial to understand whether galaxies are capable of reionizing the neutral hydrogen in the early universe at z>6. Unfortunately, it is not possible to access $f_{esc}$ at z>4 with direct observations and the handful of measurements from low redshift galaxies consistently find $f_{esc}$ < 10%, while at least $f_{esc}$ ~ 10% is necessary for galaxies dominate reionization. Here, we present the first empirical prediction of $f_{esc}$ at z>6 by combining the (sparsely populated) relation between [OIII]/[OII] and $f_{esc}$ with the redshift evolution of [OIII]/[OII] as predicted from local high-z analogs selected by their H$\alpha$ equivalent-width. We find $f_{esc}$ = $5.7_{-3.3}^{+8.3}$% at z=6 and $f_{esc}$ = $10.4_{-6.3}^{+15.5}$% at z=9 for galaxies with log(M/M$_{sun}$) ~ 9.0 (errors given as 1$\sigma$). However, there is a negative correlation with stellar mass and we find up to 50% larger $f_{esc}$ per 0.5 dex decrease in stellar mass. The population averaged escape fraction increases according to $f_{esc}$ = $f_{esc,0} ((1+z)/3)^a$, with $f_{esc,0} = 2.3 \pm 0.05$% and $a=1.17 \pm 0.02$ at z > 2 for log(M/M$_{sun}$) ~ 9.0. With our empirical prediction of $f_{esc}$ (thus fixing an important previously unknown variable) and further reasonable assumption on clumping factor and the production efficiency of Lyman continuum photons, we conclude that the average population of galaxies is just capable to reionize the universe by z ~ 6. ","Revisiting the Lyman Continuum Escape Crisis: Predictions for z &gt; 6 from
  Local Galaxies"
15,734919279296925696,141440459,Rod Van Meter üåª,['New paper (or at least preprint) dance!\n<LINK>'],https://arxiv.org/abs/1605.06951,"Experimental groups are now fabricating quantum processors powerful enough to execute small instances of quantum algorithms and definitively demonstrate quantum error correction that extends the lifetime of quantum data, adding urgency to architectural investigations. Although other options continue to be explored, effort is coalescing around topological coding models as the most practical implementation option for error correction on realizable microarchitectures. Scalability concerns have also motivated architects to propose distributed memory multicomputer architectures, with experimental efforts demonstrating some of the basic building blocks to make such designs possible. We compile the latest results from a variety of different systems aiming at the construction of a scalable quantum computer. ",Local and Distributed Quantum Computation
16,734766972760555520,1202760024,Stacy McGaugh,['New blog post <LINK> briefly describing the paper <LINK> <LINK>'],http://arxiv.org/abs/1605.05971,"Cosmological $N$-body simulations predict dark matter (DM) haloes with steep central cusps (e.g. NFW, Navarro et al. 1996). This contradicts observations of gas kinematics in low-mass galaxies that imply the existence of shallow DM cores. Baryonic processes such as adiabatic contraction and gas outflows can, in principle, alter the initial DM density profile, yet their relative contributions to the halo transformation remain uncertain. Recent high resolution, cosmological hydrodynamic simulations (Di Cintio et al. 2014, DC14) predict that inner density profiles depend systematically on the ratio of stellar to DM mass (M$_*$/M$_{\text{halo}}$). Using a Markov Chain Monte Carlo approach, we test the NFW and the M$_*$/M$_{\text{halo}}$-dependent DC14 halo models against a sample of 147 galaxy rotation curves from the new {\it Spitzer} Photometry and Accurate Rotation Curves (SPARC) data set. These galaxies all have extended H{\small I} rotation curves from radio interferometry as well as accurate stellar mass density profiles from near-infrared photometry. The DC14 halo profile provides markedly better fits to the data compared to the NFW profile. Unlike NFW, the DC14 halo parameters found in our rotation curve fits naturally fall within two standard deviations of the mass-concentration relation predicted by $\Lambda$CDM and the stellar mass-halo mass relation inferred from abundance matching with few outliers. Halo profiles modified by baryonic processes are therefore more consistent with expectations from $\Lambda$ cold dark matter ($\Lambda$CDM) cosmology and provide better fits to galaxy rotation curves across a wide range of galaxy properties than do halo models that neglect baryonic physics. Our results offer a solution to the decade long cusp-core discrepancy. ","Testing Feedback-Modified Dark Matter Haloes with Galaxy Rotation
  Curves: Estimation of Halo Parameters and Consistency with $\Lambda$CDM"
17,734638766581776384,10046412,fwilhelm,['New paper out: Go anywhere between mixed states with simple switchable noise <LINK>'],https://arxiv.org/abs/1605.06473,"We study reachable sets of open n-qubit quantum systems, whose coherent parts are under full unitary control, by adding as a further degree of incoherent control switchable Markovian noise on a single qubit. In particular, adding bang-bang control of amplitude damping noise (non-unital) allows the dynamic system to act transitively on the entire set of density operators. Thus one can transform any initial quantum state into any desired target state. Adding switchable bit-flip noise (unital) instead suffices to get all states majorised by the initial state. Our open-loop optimal control package DYNAMO is extended by incoherent control to exploit these unprecedented reachable sets in experiments. We propose implementation by a GMon, a superconducting device with fast tunable coupling to an open transmission line, and illustrate how open-loop control with noise switching achieves all state transfers without measurement-based closed-loop feedback and resettable ancilla. ","Arbitrary n-Qubit State Transfer Implemented by Coherent Control and
  Simplest Switchable Local Noise"
18,734151199738474496,139734220,Matteo Ceccarello,['New ArXiV paper out! Diversity maximization in #mapreduce and #streaming! <LINK>'],http://arxiv.org/abs/1605.05590,"Given a dataset of points in a metric space and an integer $k$, a diversity maximization problem requires determining a subset of $k$ points maximizing some diversity objective measure, e.g., the minimum or the average distance between two points in the subset. Diversity maximization is computationally hard, hence only approximate solutions can be hoped for. Although its applications are mainly in massive data analysis, most of the past research on diversity maximization focused on the sequential setting. In this work we present space and pass/round-efficient diversity maximization algorithms for the Streaming and MapReduce models and analyze their approximation guarantees for the relevant class of metric spaces of bounded doubling dimension. Like other approaches in the literature, our algorithms rely on the determination of high-quality core-sets, i.e., (much) smaller subsets of the input which contain good approximations to the optimal solution for the whole input. For a variety of diversity objective functions, our algorithms attain an $(\alpha+\epsilon)$-approximation ratio, for any constant $\epsilon>0$, where $\alpha$ is the best approximation ratio achieved by a polynomial-time, linear-space sequential algorithm for the same diversity objective. This improves substantially over the approximation ratios attainable in Streaming and MapReduce by state-of-the-art algorithms for general metric spaces. We provide extensive experimental evidence of the effectiveness of our algorithms on both real world and synthetic datasets, scaling up to over a billion points. ","MapReduce and Streaming Algorithms for Diversity Maximization in Metric
  Spaces of Bounded Doubling Dimension"
19,733756863376982016,733640801914343425,Adam Santoro,"['New account, new paper: one-shot learning with memory-augmented neural networks: <LINK>']",http://arxiv.org/abs/1605.06065,"Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ""one-shot learning."" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms. ",One-shot Learning with Memory-Augmented Neural Networks
20,733664760651190272,3122708111,Federico Lelli,['New paper from H. Katz: feedback-modified DM halos can fit rotation curves AND recover the LCDM scaling relations. <LINK>'],http://arxiv.org/abs/1605.05971,"Cosmological $N$-body simulations predict dark matter (DM) haloes with steep central cusps (e.g. NFW, Navarro et al. 1996). This contradicts observations of gas kinematics in low-mass galaxies that imply the existence of shallow DM cores. Baryonic processes such as adiabatic contraction and gas outflows can, in principle, alter the initial DM density profile, yet their relative contributions to the halo transformation remain uncertain. Recent high resolution, cosmological hydrodynamic simulations (Di Cintio et al. 2014, DC14) predict that inner density profiles depend systematically on the ratio of stellar to DM mass (M$_*$/M$_{\text{halo}}$). Using a Markov Chain Monte Carlo approach, we test the NFW and the M$_*$/M$_{\text{halo}}$-dependent DC14 halo models against a sample of 147 galaxy rotation curves from the new {\it Spitzer} Photometry and Accurate Rotation Curves (SPARC) data set. These galaxies all have extended H{\small I} rotation curves from radio interferometry as well as accurate stellar mass density profiles from near-infrared photometry. The DC14 halo profile provides markedly better fits to the data compared to the NFW profile. Unlike NFW, the DC14 halo parameters found in our rotation curve fits naturally fall within two standard deviations of the mass-concentration relation predicted by $\Lambda$CDM and the stellar mass-halo mass relation inferred from abundance matching with few outliers. Halo profiles modified by baryonic processes are therefore more consistent with expectations from $\Lambda$ cold dark matter ($\Lambda$CDM) cosmology and provide better fits to galaxy rotation curves across a wide range of galaxy properties than do halo models that neglect baryonic physics. Our results offer a solution to the decade long cusp-core discrepancy. ","Testing Feedback-Modified Dark Matter Haloes with Galaxy Rotation
  Curves: Estimation of Halo Parameters and Consistency with $\Lambda$CDM"
21,733608626154176512,88806960,Dr. Vivienne Baldassare,"['New paper up on arxiv! Broad H-alpha in star forming dwarf galaxies likely due to SNe, not AGN: <LINK>']",http://arxiv.org/abs/1605.05731,"We use time-domain optical spectroscopy to distinguish between broad emission lines powered by accreting black holes (BHs) or stellar processes (i.e., supernovae) for 16 galaxies identified as AGN candidates by Reines \etal (2013). Our study is primarily focused on those objects with narrow emission-line ratios dominated by star formation. Based on follow-up spectra taken with the Magellan Echellette Spectrograph (MagE), the Dual Imaging Spectrograph, and the Ohio State Multi-Object Spectrograph, we find that the broad H$\alpha$ emission has faded or was ambiguous for all of the star-forming objects (14/16) over baselines ranging from 5 to 14 years. For the two objects in our follow-up sample with narrow-line AGN signatures (RGG 9 and RGG 119), we find persistent broad H$\alpha$ emission consistent with an AGN origin. Additionally, we use our MagE observations to measure stellar velocity dispersions for 15 objects in the Reines et al. (2013) sample, all with narrow-line ratios indicating the presence of an AGN. Stellar masses range from $\sim5\times10^{8}$ to $3\times10^{9}$~\msun, and we measure $\sigma_{\ast}$ ranging from $28-71~{\rm km~s^{-1}}$. These $\sigma_{\ast}$ correspond to some of the lowest-mass galaxies with optical signatures of AGN activity. We show that RGG 119, the one object which has both a measured $\sigma_{\ast}$ and persistent broad H$\alpha$ emission, falls near the extrapolation of the $\rm M_{BH}-\sigma_{\star}$ relation to the low-mass end. ","Multi-epoch Spectroscopy of Dwarf Galaxies with AGN Signatures:
  Identifying Sources with Persistent Broad H-alpha Emission"
22,733466849191460865,88806960,Dr. Vivienne Baldassare,['I have a new paper appearing on arxiv tmrw on multi-epoch spectroscopy of dwarf Galaxy AGN candidates <LINK>'],http://arxiv.org/abs/1605.05731,"We use time-domain optical spectroscopy to distinguish between broad emission lines powered by accreting black holes (BHs) or stellar processes (i.e., supernovae) for 16 galaxies identified as AGN candidates by Reines \etal (2013). Our study is primarily focused on those objects with narrow emission-line ratios dominated by star formation. Based on follow-up spectra taken with the Magellan Echellette Spectrograph (MagE), the Dual Imaging Spectrograph, and the Ohio State Multi-Object Spectrograph, we find that the broad H$\alpha$ emission has faded or was ambiguous for all of the star-forming objects (14/16) over baselines ranging from 5 to 14 years. For the two objects in our follow-up sample with narrow-line AGN signatures (RGG 9 and RGG 119), we find persistent broad H$\alpha$ emission consistent with an AGN origin. Additionally, we use our MagE observations to measure stellar velocity dispersions for 15 objects in the Reines et al. (2013) sample, all with narrow-line ratios indicating the presence of an AGN. Stellar masses range from $\sim5\times10^{8}$ to $3\times10^{9}$~\msun, and we measure $\sigma_{\ast}$ ranging from $28-71~{\rm km~s^{-1}}$. These $\sigma_{\ast}$ correspond to some of the lowest-mass galaxies with optical signatures of AGN activity. We show that RGG 119, the one object which has both a measured $\sigma_{\ast}$ and persistent broad H$\alpha$ emission, falls near the extrapolation of the $\rm M_{BH}-\sigma_{\star}$ relation to the low-mass end. ","Multi-epoch Spectroscopy of Dwarf Galaxies with AGN Signatures:
  Identifying Sources with Persistent Broad H-alpha Emission"
23,733110407724830720,135065698,Shaun Hendy,"['New paper: ""Symmetry Breaking in Pedestrian Dynamics"" <LINK> @PunahaMatatini', '@AllisonGroup @PunahaMatatini yes, we finally built a proper model over the summer']",http://arxiv.org/abs/1605.05437,"When two pedestrians travelling in opposite directions approach one another, each must decide on which side (the left or the right) they will attempt to pass. If both make the same choice then passing can be completed with ease, while if they make opposite choices an embarrassing stand-off or collision can occur. Pedestrians who encounter each other frequently can establish ""social norms"" that bias this decision. In this study we investigate the effect of binary decision-making by pedestrians when passing on the dynamics of pedestrian flows in order to study the emergence of a social norm in crowds with a mixture of individual biases. Such a situation may arise, for instance, when individuals from different communities mix at a large sporting event or at transport hubs. We construct a phase diagram that shows that a social norm can still emerge provided pedestrians are sufficiently attentive to the choices of others in the crowd. We show that this collective behaviour has the potential to greatly influence the dynamics of pedestrians, including the breaking of symmetry by the formation of lanes. ",Symmetry Breaking in Pedestrian Dynamics
24,732815474812145664,2999702157,Anton Ilderton,['New paper relating strong field QED to Very Special Relativity: <LINK>'],http://arxiv.org/abs/1605.04967,"We consider violation of Lorentz invariance in QED induced by a very high frequency background wave. An effective theory is obtained by averaging observables over the rapid field oscillations. This preserves Ward identities and restores translation invariance below the high frequency scale, but only partial Lorentz invariance: we show that the effective theory is C-invariant SIM(2)-QED in Very Special Relativity. Averaging generates the nonlocal terms familiar from SIM(2) theories, while the short-distance behaviour of the background field fermion propagator generates the infinite number of higher-order vertices of SIM(2)-QED. ",Very Special Relativity as a background field theory
25,732749250551910400,1232964338,Shaikh saad,"[""Here is my paper on 'New Class of SO(10) Models for Flavor' that appeared today on arxiv  <LINK>""]",http://arxiv.org/abs/1605.05116,"We present a new class of unified models based on SO(10) symmetry which provides insights into the masses and mixings of quarks and leptons, including the neutrinos. The key feature of our proposal is the absence of Higgs boson 10_H belonging to the fundamental representation that is normally employed. Flavor mixing is induced via vector-like fermions in the 16 + 16-bar representation. A variety of scenarios, both supersymmetric and otherwise, are analyzed involving a 126_H along with either a 45_H or a 210_H of Higgs boson employed for symmetry breaking. It is shown that this framework, with only a limited number of parameters, provides an excellent fit to the full fermion spectrum, utilizing either type-I or type-II seesaw mechanism. These flavor models can be potentially tested and distinguished in their predictions for proton decay branching ratios, which are analyzed. ",New Class of SO(10) Models for Flavor
26,732740165169479681,479672092,Constantine Dovrolis,"['New paper: ""The Hourglass Effect in Hierarchical Dependency Networks"" <LINK>  Please email us your comments if you read it']",https://arxiv.org/abs/1605.05025,"Many hierarchically modular systems are structured in a way that resembles an hourglass. This ""hourglass effect"" means that the system generates many outputs from many inputs through a relatively small number of intermediate modules that are critical for the operation of the entire system, referred to as the waist of the hourglass. We investigate the hourglass effect in general, not necessarily layered, hierarchical dependency networks. Our analysis focuses on the number of source-to-target dependency paths that traverse each vertex, and it identifies the core of a dependency network as the smallest set of vertices that collectively cover almost all dependency paths. We then examine if a given network exhibits the hourglass property or not, comparing its core size with a ""flat"" (i.e., non-hierarchical) network that preserves the source dependencies of each target in the original network. As a possible explanation for the hourglass effect, we propose the Reuse Preference (RP) model that captures the bias of new modules to reuse intermediate modules of similar complexity instead of connecting directly to sources or low complexity modules. We have applied the proposed framework in a diverse set of dependency networks from technological, natural and information systems, showing that all these networks exhibit the general hourglass property but to a varying degree and with different waist characteristics. ",The Hourglass Effect in Hierarchical Dependency Networks
27,732562266743263232,49347058,Albert Zeyer,"[""<LINK> new Theano paper (I'm acknowledged :)). TensorFlow performance seems to be quite good now""]",https://arxiv.org/abs/1605.02688,"Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it. ","Theano: A Python framework for fast computation of mathematical
  expressions"
28,732465093682696193,4438354094,Tom Wong,['New paper with Philipp on engineering weights in networks to improve quantum search. <LINK> <LINK>'],http://arxiv.org/abs/1605.04862,"Continuous-time quantum walks are natural tools for spatial search, where one searches for a marked vertex in a graph. Sometimes, the structure of the graph causes the walker to get trapped, such that the probability of finding the marked vertex is limited. We give an example with two linked cliques, proving that the captive probability can be liberated by increasing the weights of the links. This allows the search to succeed with probability 1 without increasing the energy scaling of the algorithm. Further increasing the weights, however, slows the runtime, so the optimal search requires weights that are neither too weak nor too strong. ",Engineering the Success of Quantum Walk Search Using Weighted Graphs
29,732376626248323073,40108176,Zhangqi Yin,"['My new paper: ""Torsional optomechanics of a levitated nonspherical nanoparticle. (arXiv:1605.03990"" <LINK>']",http://arxiv.org/abs/1605.03990,"An optically levitated nanoparticle in vacuum is a paradigm optomechanical system for sensing and studying macroscopic quantum mechanics. While its center-of-mass motion has been investigated intensively, its torsional vibration has only been studied theoretically in limited cases. Here we report the first experimental observation of the torsional vibration of an optically levitated nonspherical nanoparticle in vacuum. We achieve this by utilizing the coupling between the spin angular momentum of photons and the torsional vibration of a nonspherical nanoparticle whose polarizability is a tensor. The torsional vibration frequency can be one order of magnitude higher than its center-of-mass motion frequency, which is promising for ground state cooling. We propose a simple yet novel scheme to achieve ground state cooling of its torsional vibration with a linearly-polarized Gaussian cavity mode. A levitated nonspherical nanoparticle in vacuum will also be an ultrasensitive nanoscale torsion balance with a torque detection sensitivity on the order of $10^{-29} ~\mathrm{N}\cdot \mathrm{m}/\sqrt{\mathrm{ Hz}}$ under realistic conditions. ",Torsional optomechanics of a levitated nonspherical nanoparticle
30,732239037277323266,65010804,Hugh Osborn,"['For the evening crowd: My paper on a new mini-Neptune from K2, EPIC-1166 b! <LINK> \nBlog post here: <LINK>']",http://arxiv.org/abs/1605.04291,"We report the discovery of the exoplanet K2-110 b (previously EPIC212521166b) from K2 photometry orbiting in a 13.8637d period around an old, metal-poor K3 dwarf star. With a V-band magnitude of 11.9, K2-110 is particularly amenable to RV follow-up. A joint analysis of K2 photometry and high-precision RVs from 28 HARPS and HARPS-N spectra reveal it to have a radius of 2.6$\pm 0.1 R_{\oplus}$ and a mass of 16.7$\pm 3.2$~M$_{\oplus}$, hence a density of $5.2\pm1.2$ g.cm$^{-3}$, making it one of the most massive planets yet to be found with a sub-Neptune radius. When accounting for compression, the resulting Earth-like density is best fitted by a 0.2 M$_{\oplus}$ hydrogen atmosphere over an 16.5 M$_{\oplus}$ Earth-like interior, although the planet could also have significant water content. At 0.1~AU, even taking into account the old stellar age of $8 \pm 3$ Gyr, the planet is unlikely to have been significantly affected by EUV evaporation. However the planet likely disc-migrated to its current position making the lack of a thick H$_2$ atmosphere puzzling. This analysis has made K2-110 b one of the best-characterised mini-Neptunes with density constrained to less than 30%. ",K2-110 b - a massive mini-Neptune exoplanet
31,732133438971641856,481539448,Richard Alexander,"['Our new paper: an analytic model for thermal disc winds. Some years in the making, but worked out nicely in the end.\n<LINK>', 'One-line summary is that assuming photoevaporative winds are scale-free works *much* better than we expected. https://t.co/vdC0m1IHMf']",http://arxiv.org/abs/1605.04089,"We derive a self-similar description for the 2D streamline topology and flow structure of an axi-symmetric, thermally driven wind originating from a disc in which the density is a power law function of radius. Our scale-free solution is strictly only valid in the absence of gravity or centrifugal support; comparison with 2D hydrodynamic simulations of winds from Keplerian discs however demonstrates that the scale-free solution is a good approximation also in the outer regions of such discs, and can provide a reasonable description even for launch radii well within the gravitational radius of the flow. Although other authors have considered the flow properties along streamlines whose geometry has been specified in advance, this is the first isothermal calculation in which the flow geometry and variation of flow variables along streamlines is determined self-consistently. It is found that the flow trajectory is very sensitive to the power-law index of radial density variation in the disc: the steeper the density gradient, the stronger is the curvature of streamlines close to the flow base that is required in order to maintain momentum balance perpendicular to the flow. Steeper disc density profiles are also associated with more rapid acceleration, and a faster fall-off of density, with height above the disc plane. The derivation of a set of simple governing equations for the flow structure of thermal winds from the outer regions of power law discs offers the possibility of deriving flow observables without having to resort to hydrodynamical simulation. ",A self-similar solution for thermal disc winds
32,732087351048962048,2569631268,Daniel Huber,['Our new asteroseismology paper on oscillating Kepler giants out to the edge of the Milky Way <LINK> <LINK>'],http://arxiv.org/abs/1605.03675,"Asteroseismology has proven to be an excellent tool to determine not only the global stellar properties with a good precision but also to infer stellar structure, dynamics, and evolution for a large sample of Kepler stars. Prior to the launch of the mission the properties of Kepler targets were inferred from broadband photometry, leading to the Input Catalog (KIC Brown et al. 2011). The KIC was later revised in the Kepler Star Properties Catalog (Huber et al. 2014), based on literature values and an asteroseismic analysis of stars which were unclassified in the KIC. Here we present an asteroseismic analysis of 45,400 stars which were classified as dwarfs in the Kepler Star Properties Catalog. We found that around 2% of the sample shows acoustic modes in the typical frequency range that put them in the red-giant category rather than cool dwarfs. We analyse the asteroseismic properties of these stars, derive their surface gravities, masses, and radii and present updated effective temperatures and distances. We show that the sample is significantly fainter than the previously known oscillating giants in the Kepler field, with the faintest stars reaching down to a Kepler magnitude, Kp~16. We demonstrate that 404 stars are at distances beyond 5 kpc and that the stars are significantly less massive than for the original Kepler red-giant sample, consistent with a population of distant halo giants. A comparison with a galactic population model shows that up to 40 stars might be genuine halo giants, which would increase the number of known asteroseismic halo stars by a factor of 4. The detections presented here will provide a valuable sample for galactic archeology studies. ","Probing the deep end of the Milky Way with \emph{Kepler}: Asteroseismic
  analysis of 854 faint Red Giants misclassified as Cool Dwarfs"
33,731077300079480833,4603400475,Alexander Novikov,"['Our new paper: like Factorization Machines, but models all interactions of all orders at once\n<LINK>\n#icml2016 #expMachines', '@SashaVNovikov Python implementation https://t.co/RrDBnJvWxG\n#icml2016 #expMachines']",https://arxiv.org/abs/1605.03795,"Modeling interactions between features improves the performance of machine learning solutions in many domains (e.g. recommender systems or sentiment analysis). In this paper, we introduce Exponential Machines (ExM), a predictor that models all interactions of every order. The key idea is to represent an exponentially large tensor of parameters in a factorized format called Tensor Train (TT). The Tensor Train format regularizes the model and lets you control the number of underlying parameters. To train the model, we develop a stochastic Riemannian optimization procedure, which allows us to fit tensors with 2^160 entries. We show that the model achieves state-of-the-art performance on synthetic data with high-order interactions and that it works on par with high-order factorization machines on a recommender system dataset MovieLens 100K. ",Exponential Machines
34,730704506640576512,715134939155578880,Francesco Maione,"['Our new paper on Binary Neutron Star merger simulations is out on @arXiv_grqc!\n\n<LINK> <LINK>', 'Thanks to co-authors R.De Pietri, A.Feo and F.L√∂ffler and to our institutions @UffComINFN, @unipr, @NSF!']",http://arxiv.org/abs/1605.03424,"We present results from three-dimensional general relativistic simulations of binary neutron star coalescences and mergers using public codes. We considered equal mass models where the baryon mass of the two Neutron Stars (NS) is $1.4M_{\odot}$, described by four different equations of state (EOS) for the cold nuclear matter (APR4, SLy, H4, and MS1; all parametrized as piecewise polytropes). We started the simulations from four different initial interbinary distances ($40, 44.3, 50$, and $60$ km), including up to the last 16 orbits before merger. That allows to show the effects on the gravitational wave phase evolution, radiated energy and angular momentum due to: the use of different EOSs, the orbital eccentricity present in the initial data and the initial separation (in the simulation) between the two stars. Our results show that eccentricity has a major role in the discrepancy between numerical and analytical waveforms until the very last few orbits, where ""tidal"" effects and missing high-order post-Newtonian coefficients also play a significant role. We test different methods for extrapolating the gravitational wave signal extracted at finite radii to null infinity. We show that an effective procedure for integrating the Newman-Penrose $\psi_4$ signal to obtain the gravitational wave strain $h$ is to apply a simple high-pass digital filter to $h$ after a time domain integration, where only the two physical motivated integration constants are introduced. That should be preferred to the more common procedures of introducing additional integration constants, integrating in the frequency domain or filtering $\psi_4$ before integration. ","Binary neutron star merger simulations with different initial orbital
  frequency and equation of state"
35,730336323383496706,432661159,Jeremy Sanders,['Our new paper showing how edge filtering examines physical processes in galaxy clusters and makes great images: <LINK>'],http://arxiv.org/abs/1605.02911,"The effects of many physical processes in the intracluster medium of galaxy clusters imprint themselves in X-ray surface brightness images. It is therefore important to choose optimal methods for extracting information from and enhancing the interpretability of such images. We describe in detail a gradient filtering edge detection method that we previously applied to images of the Centaurus cluster of galaxies. The Gaussian gradient filter measures the gradient in the surface brightness distribution on particular spatial scales. We apply this filter on different scales to Chandra X-ray observatory images of two clusters with AGN feedback, the Perseus cluster and M87, and a merging system, A3667. By combining filtered images on different scales using radial filters spectacular images of the edges in a cluster are produced. We describe how to assess the significance of features in filtered images. We find the gradient filtering technique to have significant advantages for detecting many kinds of features compared to other analysis techniques, such as unsharp-masking. Filtering cluster images in this way in a hard energy band allows shocks to be detected. ",Detecting edges in the X-ray surface brightness of galaxy clusters
36,729986179337195520,3333052551,Hugo Larochelle,['Our long paper on NADE (with @driainmurray @Cote_Marc and more): <LINK> . Includes a new convolutional NADE model.'],http://arxiv.org/abs/1605.02226,"We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE. ",Neural Autoregressive Distribution Estimation
37,729702225677783040,490592983,Luciano Pandola,['New paper from #GERDA at #LNGS: limits on ECEC decay of Ar36 improved by two orders of magnitude @LNS_INFN @LngsInfn <LINK>'],http://arxiv.org/abs/1605.01756,"Neutrinoless double electron capture is a process that, if detected, would give evidence of lepton number violation and the Majorana nature of neutrinos. A search for neutrinoless double electron capture of $^{36}$Ar has been performed with germanium detectors installed in liquid argon using data from Phase I of the GERmanium Detector Array (GERDA) experiment at the Gran Sasso Laboratory of INFN, Italy. No signal was observed and an experimental lower limit on the half-life of the radiative neutrinoless double electron capture of $^{36}$Ar was established: $T_{1/2} > $ 3.6 $\times$ 10$^{21}$ yr at 90 % C.I. ","Limit on the Radiative Neutrinoless Double Electron Capture of $^{36}$Ar
  from GERDA Phase I"
38,728200531823333376,14118089,Henry Segerman,"['New paper with Saul Schleimer, on transforming spherical images. <LINK> <LINK>', 'Our standard input image features @AndreaHawksley, @blinkpopshift, and @vihartvihart! See https://t.co/NAqw6m0CIg &amp; https://t.co/Qkn1TXnkra', 'Equirectangular still images from the paper can be viewed spherically at https://t.co/3mZrJURvcl']",http://arxiv.org/abs/1605.01396,"We propose M\""obius transformations as the natural rotation and scaling tools for editing spherical images. As an application we produce spherical Droste images. We obtain other self-similar visual effects using rational functions, elliptic functions, and Schwarz-Christoffel maps. ",Squares that Look Round: Transforming Spherical Images
39,728018427332284416,140587725,Raghu Mahajan,['Check out my new paper! Happy we figured out subtle issues in Kubo formula &amp; anomaly terms in 3D Bose-Fermi duality\n\n<LINK>'],https://arxiv.org/abs/1605.01122,"The frequency-dependent longitudinal and Hall conductivities --- $\sigma_{xx}$ and $\sigma_{xy}$ --- are dimensionless functions of $\omega/T$ in 2+1 dimensional CFTs at nonzero temperature. These functions characterize the spectrum of charged excitations of the theory and are basic experimental observables. We compute these conductivities for large $N$ Chern-Simons theory with fermion matter. The computation is exact in the 't Hooft coupling $\lambda$ at $N = \infty$. We describe various physical features of the conductivity, including an explicit relation between the weight of the delta function at $\omega = 0$ in $\sigma_{xx}$ and the existence of infinitely many higher spin conserved currents in the theory. We also compute the conductivities perturbatively in Chern-Simons theory with scalar matter and show that the resulting functions of $\omega/T$ agree with the strong coupling fermionic result. This provides a new test of the conjectured 3d bosonization duality. In matching the Hall conductivities we resolve an outstanding puzzle by carefully treating an extra anomaly that arises in the regularization scheme used. ",Transport in Chern-Simons-Matter Theories
40,727659965918236673,1271414598,Nathan Goldbaum,"['My new paper, including links to 6 TB of data and a nifty interactive visualization widget, is up on the arxiv. <LINK>', ""@adam_m_jcbs it's hosted on a server @NCSAatIllinois, served by nginx. Our group (@Xarthisius) are building a yt hub for this sort of thing."", '@astrocrash ugh.', '@astrocrash the arxiv is really frustrating to deal with sometimes.', '@astrocrash well at leas the link is right in the paper!', ""@astrocrash yes, it was confusing to people that there wasn't a way to cancel the movie mode. And ipywidget buttons don't have state.""]",http://arxiv.org/abs/1605.00646,"Self-gravity and stellar feedback are capable of driving turbulence and transporting mass and angular momentum in disk galaxies, but the balance between them is not well understood. In the previous paper in this series, we showed that gravity alone can drive turbulence in galactic disks, regulate their Toomre $Q$ parameters to $\sim$ 1, and transport mass inwards at a rate sufficient to fuel star formation in the centers of present-day galaxies. In this paper we extend our models to include the effects of star formation feedback. We show that feedback suppresses galaxies' star formation rates by a factor of $\sim$ 5 and leads to the formation of a multi-phase atomic and molecular ISM. Both the star formation rate and the phase balance produced in our simulations agree well with observations of nearby spirals. After our galaxies reach steady state, we find that the inclusion of feedback actually lowers the gas velocity dispersion slightly compared to the case of pure self-gravity, and also slightly reduces the rate of inward mass transport. Nevertheless, we find that, even with feedback included, our galactic disks self-regulate to $Q$ $\sim$ 1, and transport mass inwards at a rate sufficient to supply a substantial fraction of the inner disk star formation. We argue that gravitational instability is therefore likely to be the dominant source of turbulence and transport in galactic disks, and that it is responsible for fueling star formation in the inner parts of galactic disks over cosmological times. ","Mass Transport and Turbulence in Gravitationally Unstable Disk Galaxies
  II: The Effects of Star Formation Feedback"
41,727593070468857856,425555689,Dr. Cecilia Leung,"['New on ArXiv, our paper of the #Enceladus Mission Concept we designed during 2015 #NASA Planetary Sci Summer School. <LINK>', ""THEO (Testing the Habitability of Enceladus's Ocean) is designed based on a New-Frontiers class, solar-powered Enceladus orbiter. #NASA #JPL"", ""THEO would sample the plumes of water vapor &amp; ice in the south polar region to investigate Enceladus's hidden ocean environment. #NASA #JPL""]",http://arxiv.org/abs/1605.00579,"Saturn's moon Enceladus offers a unique opportunity in the search for life and habitable environments beyond Earth, a key theme of the National Research Council's 2013-2022 Decadal Survey. A plume of water vapor and ice spews from Enceladus's south polar region. Cassini data suggest that this plume, sourced by a liquid reservoir beneath the moon's icy crust, contain organics, salts, and water-rock interaction derivatives. Thus, the ingredients for life as we know it-- liquid water, chemistry, and energy sources-- are available in Enceladus's subsurface ocean. We have only to sample the plumes to investigate this hidden ocean environment. We present a New Frontiers class, solar-powered Enceladus orbiter that would take advantage of this opportunity, Testing the Habitability of Enceladus's Ocean (THEO). Developed by the 2015 Jet Propulsion Laboratory Planetary Science Summer School student participants under the guidance of TeamX, this mission concept includes remote sensing and in situ analyses with a mass spectrometer, a sub-mm radiometer-spectrometer, a camera, and two magnetometers. These instruments were selected to address four key questions for ascertaining the habitability of Enceladus's ocean within the context of the moon's geological activity: (1) How are the plumes and ocean connected? (2) Are the abiotic conditions of the ocean suitable for habitability? (3) How stable is the ocean environment? (4) Is there evidence of biological processes? By taking advantage of the opportunity Enceladus's plumes offer, THEO represents a viable, solar-powered option for exploring a potentially habitable ocean world of the outer solar system. ",THEO Concept Mission: Testing the Habitability of Enceladus's Ocean
42,727572641268273152,24784896,Lawrence,['New paper on multigrid for compatible FE for weather prediction with Eike M√ºller. <LINK>'],http://arxiv.org/abs/1605.00492,"The implementation of efficient multigrid preconditioners for elliptic partial differential equations (PDEs) is a challenge due to the complexity of the resulting algorithms and corresponding computer code. For sophisticated finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in-depth knowledge of the mathematical theory, parallel computing and optimisation techniques on manycore CPUs. In this paper we show how the development of bespoke multigrid preconditioners can be simplified significantly by using a framework which allows the expression of the each component of the algorithm at the correct abstraction level. Our approach (1) allows the expression of the finite element problem in a language which is close to the mathematical formulation of the problem, (2) guarantees the automatic generation and efficient execution of parallel optimised low-level computer code and (3) is flexible enough to support different abstraction levels and give the programmer control over details of the preconditioner. We use the composable abstractions of the Firedrake/PyOP2 package to demonstrate the efficiency of this approach for the solution of strongly anisotropic PDEs in atmospheric modelling. The weak formulation of the PDE is expressed in Unified Form Language (UFL) and the lower PyOP2 abstraction layer allows the manual design of computational kernels for a bespoke geometric multigrid preconditioner. We compare the performance of this preconditioner to a single-level method and hypre's BoomerAMG algorithm. The Firedrake/PyOP2 code is inherently parallel and we present a detailed performance analysis for a single node (24 cores) on the ARCHER supercomputer. Our implementation utilises a significant fraction of the available memory bandwidth and shows very good weak scaling on up to 6,144 compute cores. ","High level implementation of geometric multigrid solvers for finite
  element problems: applications in atmospheric modelling"
43,740504777285636096,2676457430,MAGIC telescopes üå¥üå∫,['A new MAGIC paper accepted in A&amp;A: a long term variability study of Mrk421. chek it out\n<LINK> <LINK>'],https://arxiv.org/abs/1605.09017,"We study the multi-band variability and correlations of the TeV blazar Mrk 421 on year time scales, which can bring additional insight on the processes responsible for its broadband emission. We observed Mrk 421 in the very high energy (VHE) gamma-ray range with the Cherenkov telescope MAGIC-I from March 2007 to June 2009 for a total of 96 hours of effective time after quality cuts. The VHE flux variability is quantified with several methods, including the Bayesian Block algorithm, which is applied to data from Cherenkov telescopes for the first time. The 2.3 year long MAGIC light curve is complemented with data from the Swift/BAT and RXTE/ASM satellites and the KVA, GASP-WEBT, OVRO, and Mets\""ahovi telescopes from February 2007 to July 2009, allowing for an excellent characterisation of the multi-band variability and correlations over year time scales. Mrk 421 was found in different gamma-ray emission states during the 2.3 year long observation period. Flares and different levels of variability in the gamma-ray light curve could be identified with the Bayesian Block algorithm. The same behaviour of a quiet and active emission was found in the X-ray light curves measured by Swift/BAT and the RXTE/ASM, with a direct correlation in time. The behaviour of the optical light curve of GASP-WEBT and the radio light curves by OVRO and Mets\""ahovi are different as they show no coincident features with the higher energetic light curves and a less variable emission. The fractional variability is overall increasing with energy. The comparable variability in the X-ray and VHE bands and their direct correlation during both high- and low-activity periods spanning many months show that the electron populations radiating the X-ray and gamma-ray photons are either the same, as expected in the Synchrotron-Self-Compton mechanism, or at least strongly correlated, as expected in electromagnetic cascades. ","Long-term multi-wavelength variability and correlation study of
  Markarian 421 from 2007 to 2009"
44,735680619183628288,259816325,Zubair Shafiq,['New paper on adblock detectors. Websites increasingly using 3rd-party @pagefair adblock detection scripts.  #adblock <LINK>'],https://arxiv.org/abs/1605.05841,"The rise of ad-blockers is viewed as an economic threat by online publishers, especially those who primarily rely on ad- vertising to support their services. To address this threat, publishers have started retaliating by employing ad-block detectors, which scout for ad-blocker users and react to them by restricting their content access and pushing them to whitelist the website or disabling ad-blockers altogether. The clash between ad-blockers and ad-block detectors has resulted in a new arms race on the web. In this paper, we present the first systematic measurement and analysis of ad-block detection on the web. We have designed and implemented a machine learning based tech- nique to automatically detect ad-block detection, and use it to study the deployment of ad-block detectors on Alexa top- 100K websites. The approach is promising with precision of 94.8% and recall of 93.1%. We characterize the spectrum of different strategies used by websites for ad-block detection. We find that most of publishers use fairly simple passive ap- proaches for ad-block detection. However, we also note that a few websites use third-party services, e.g. PageFair, for ad-block detection and response. The third-party services use active deception and other sophisticated tactics to de- tect ad-blockers. We also find that the third-party services can successfully circumvent ad-blockers and display ads on publisher websites. ",A First Look at Ad-block Detection: A New Arms Race on the Web
45,735182501346447361,620458645,Jennifer Rexford,"['Our new paper on ""Performance Characterization of a Commercial Video Streaming Service"" with #Yahoo <LINK>']",https://arxiv.org/abs/1605.04966,"Despite the growing popularity of video streaming over the Internet, problems such as re-buffering and high startup latency continue to plague users. In this paper, we present an end-to-end characterization of Yahoo's video streaming service, analyzing over 500 million video chunks downloaded over a two-week period. We gain unique visibility into the causes of performance degradation by instrumenting both the CDN server and the client player at the chunk level, while also collecting frequent snapshots of TCP variables from the server network stack. We uncover a range of performance issues, including an asynchronous disk-read timer and cache misses at the server, high latency and latency variability in the network, and buffering delays and dropped frames at the client. Looking across chunks in the same session, or destined to the same IP prefix, we see how some performance problems are relatively persistent, depending on the video's popularity, the distance between the client and server, and the client's operating system, browser, and Flash runtime. ",Performance Characterization of a Commercial Video Streaming Service
46,737397959965483008,58967234,Javier Bustos,"['in order to study the internet robustness, we model it as a supply network, and use @google as the supplier. +info:  <LINK>']",http://arxiv.org/abs/1605.08714,"Whether as telecommunications or power systems, networks are very important in everyday life. Maintaining these networks properly functional and connected, even under attacks or failures, is of special concern. This topic has been previously studied with a whole network robustness perspective,modeling networks as undirected graphs (such as roads or simply cables). This perspective measures the average behavior of the network after its last node has failed. In this article we propose two alternatives to well-known studies about the robustness of the backbone Internet: to use a supply network model and metrics for its representation (we called it the Go-Index), and to use robustness metrics that can be calculated while disconnections appear. Our research question is: if a smart adversary has a limited number of strikes to attack the Internet, how much will the damage be after each one in terms of network disconnection? Our findings suggest that in order to design robust networks it might be better to have a complete view of the robustness evolution of the network, from both the infrastructure and the users perspective. ",I Accidentally the Whole Internet
47,729845184759013376,725570845448495104,GWaves,"['RT @nnobuya ""We find no plausible gravitational-wave candidates."" <LINK> <LINK>']",https://arxiv.org/abs/1605.01785,"We present results from a search for gravitational-wave bursts coincident with a set of two core-collapse supernovae observed between 2007 and 2011. We employ data from the Laser Interferometer Gravitational-wave Observatory (LIGO), the Virgo gravitational-wave observatory, and the GEO 600 gravitational-wave observatory. The targeted core-collapse supernovae were selected on the basis of (1) proximity (within approximately 15 Mpc), (2) tightness of observational constraints on the time of core collapse that defines the gravitational-wave search window, and (3) coincident operation of at least two interferometers at the time of core collapse. We find no plausible gravitational-wave candidates. We present the probability of detecting signals from both astrophysically well-motivated and more speculative gravitational-wave emission mechanisms as a function of distance from Earth, and discuss the implications for the detection of gravitational waves from core-collapse supernovae by the upgraded Advanced LIGO and Virgo detectors. ","A First Targeted Search for Gravitational-Wave Bursts from Core-Collapse
  Supernovae in Data of First-Generation Laser Interferometer Detectors"
