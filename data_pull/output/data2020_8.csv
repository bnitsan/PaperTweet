,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1305505708780077057,48319844,Raghav Bali,['New paper presenting a scalable and efficient architecture(EASTER) for OCR and HTR tasks. A fully convolutional architecture with better or on par performance against more complex works.\n\narxiv: <LINK>\n#MachineLearning #DeepLearning #DataScience @machine_ml'],https://arxiv.org/abs/2008.07839,"Recent progress in deep learning has led to the development of Optical Character Recognition (OCR) systems which perform remarkably well. Most research has been around recurrent networks as well as complex gated layers which make the overall solution complex and difficult to scale. In this paper, we present an Efficient And Scalable TExt Recognizer (EASTER) to perform optical character recognition on both machine printed and handwritten text. Our model utilises 1-D convolutional layers without any recurrence which enables parallel training with considerably less volume of data. We experimented with multiple variations of our architecture and one of the smallest variant (depth and number of parameter wise) performs comparably to RNN based complex choices. Our 20-layered deepest variant outperforms RNN architectures with a good margin on benchmarking datasets like IIIT-5k and SVT. We also showcase improvements over the current best results on offline handwritten text recognition task. We also present data generation pipelines with augmentation setup to generate synthetic datasets for both handwritten and machine printed text. ",EASTER: Efficient and Scalable Text Recognizer
1,1303380731960397827,2751320737,Amir Barati,"['Our paper introducing new #MachineLearning #Algorithms for #material #discovery using #orbital #graphconv is out in  PRM(<LINK>), Arxiv: <LINK>']",https://arxiv.org/abs/2008.06415,"Material representations that are compatible with machine learning models play a key role in developing models that exhibit high accuracy for property prediction. Atomic orbital interactions are one of the important factors that govern the properties of crystalline materials, from which the local chemical environments of atoms is inferred. Therefore, to develop robust machine learningmodels for material properties prediction, it is imperative to include features representing such chemical attributes. Here, we propose the Orbital Graph Convolutional Neural Network (OGCNN), a crystal graph convolutional neural network framework that includes atomic orbital interaction features that learns material properties in a robust way. In addition, we embedded an encoder-decoder network into the OGCNN enabling it to learn important features among basic atomic (elemental features), orbital-orbital interactions, and topological features. We examined the performance of this model on a broad range of crystalline material data to predict different properties. We benchmarked the performance of the OGCNN model with that of: 1) the crystal graph convolutional neural network (CGCNN), 2) other state-of-the-art descriptors for material representations including Many-body Tensor Representation (MBTR) and the Smooth Overlap of Atomic Positions (SOAP), and 3) other conventional regression machine learning algorithms where different crystal featurization methods have been used. We find that OGCNN significantly outperforms them. The OGCNN model with high predictive accuracy can be used to discover new materials among the immense phase and compound spaces of materials ","Orbital Graph Convolutional Neural Network for Material Property
  Prediction"
2,1301800001799102467,734667419055230976,Lisa Matthias,['Update on our study on vanished OA journals. We re-validated our data&amp;extended our search for archives beyond the Keepers to remove journals that still exist. We also added a few new ones! New count: 176. Paper: <LINK> Data: <LINK> #openaccess <LINK>'],https://arxiv.org/abs/2008.11933,"The preservation of the scholarly record has been a point of concern since the beginning of knowledge production. With print publications, the responsibility rested primarily with librarians, but the shift toward digital publishing and, in particular, the introduction of open access (OA) have caused ambiguity and complexity. Consequently, the long-term accessibility of journals is not always guaranteed, and they can even disappear from the web completely. The focus of this exploratory study is on the phenomenon of vanished journals, something that has not been carried out before. For the analysis, we consulted several major bibliographic indexes, such as Scopus, Ulrichsweb, and the Directory of Open Access Journals, and traced the journals through the Internet Archive's Wayback Machine. We found 174 OA journals that, through lack of comprehensive and open archives, vanished from the web between 2000 and 2019, spanning all major research disciplines and geographic regions of the world. Our results raise vital concern for the integrity of the scholarly record and highlight the urgency to take collaborative action to ensure continued access and prevent the loss of more scholarly knowledge. We encourage those interested in the phenomenon of vanished journals to use the public dataset for their own research. ",Open is not forever: a study of vanished open access journals
3,1301475345636487168,28734416,Sebastian Risi,"['Can we use machine learning methods such as GANs to assess creativity? In a recent collaboration lead by @jrafner we try to find out by letting players ""blend"" existing images into new images under varying constraints. Paper: <LINK> <LINK>', 'Our study indicates that the system provides a playful experience, affords players a sense of control over the interface, and elicits different types of player behavior, supporting further study of the tool for use in a scalable, playful, creativity assessment. https://t.co/PvIyNv9pBA', 'w/ @jacobsherson, @sparvell, @Learnonomy, @jacksohne, @ACRold, @asmaalfadala, @dominicregester']",https://arxiv.org/abs/2008.05914,"We present a pilot study on crea.blender, a novel co-creative game designed for large-scale, systematic assessment of distinct constructs of human creativity. Co-creative systems are systems in which humans and computers (often with Machine Learning) collaborate on a creative task. This human-computer collaboration raises questions about the relevance and level of human creativity and involvement in the process. We expand on, and explore aspects of these questions in this pilot study. We observe participants play through three different play modes in crea.blender, each aligned with established creativity assessment methods. In these modes, players ""blend"" existing images into new images under varying constraints. Our study indicates that crea.blender provides a playful experience, affords players a sense of control over the interface, and elicits different types of player behavior, supporting further study of the tool for use in a scalable, playful, creativity assessment. ","crea.blender: A Neural Network-Based Image Generation Game to Assess
  Creativity"
4,1301293833116946432,1115192984449028096,Dr. Karl Remeis-Observatory,['Interested in the cyclotron line energy in Her X-1? Than you should read this new paper <LINK>\n#theobservatory #Astronomy'],https://arxiv.org/abs/2008.13434,"We summarize the results of a dedicated effort between 2012 and 2019 to follow the evolution of the cyclotron line in Her~X-1 through repeated NuSTAR observations. The previously observed nearly 20-year long decay of the cyclotron line energy has ended around 2012: from there onward the pulse phase averaged flux corrected cyclotron line energy has remained stable and constant at an average value of Ecyc= (37.44+/-0.07) keV (normalized to a flux level of 6.8 RXTE/ASM-cts/s). The flux dependence of Ecyc discovered in 2007 is now measured with high precision, giving a slope of (0.675+/-0.075) keV/(ASM-cts/s), corresponding to an increase of 6.5% of Ecyc for an increase in flux by a factor of two. We also find that all line parameters as well as the continuum parameters show a correlation with X-ray flux. While a correlation between Ecyc and X-ray flux (both positive and negative) is now known for several accreting binaries with various suggestions for the underlying physics, the phenomenon of a long-term decay has so far only been seen in Her~X-1 and Vela~X-1, with far less convincing explanations. ",The cyclotron line energy in Her X-1: stable after the decay
5,1301117235692539905,882883029689151488,Amir Vaxman,"['A cool and previously unpublished side-result of our Moebius-subdivision scheme from  #siggraphasia 2019 was that we found we could describe discrete curvature and torsion in a provably convergent and circle-reproducing manner. This our new paper:\n<LINK>', '@harrisonpartch @vm_braindumps ?']",https://arxiv.org/abs/2008.13236,"Motivated by a M\""obius invariant subdivision scheme for polygons, we study a curvature notion for discrete curves where the cross-ratio plays an important role in all our key definitions. Using a particular M\""obius invariant point-insertion-rule, comparable to the classical four-point-scheme, we construct circles along discrete curves. Asymptotic analysis shows that these circles defined on a sampled curve converge to the smooth curvature circles as the sampling density increases. We express our discrete torsion for space curves, which is not a M\""obius invariant notion, using the cross-ratio and show its asymptotic behavior in analogy to the curvature. ",Discrete Curvature and Torsion from Cross-Ratios
6,1300856923277983747,1548884580,Ryan Urbanowicz,"['Check out our new rigorous #MachineLearning analysis #pipeline for #biomedical binary #classification paper available on #arXiv  <LINK> #ArtificialIntelligence. Jupyter notebook at: <LINK> with @DrShannonMLynch @moorejh  @RobertZhang100 +others <LINK>', 'Other authors include Pranshu Suri, Yuchen Lu, Karen Ruth and Rachael Stolzenberg-Solomon']",https://arxiv.org/abs/2008.12829,"Machine learning (ML) offers a collection of powerful approaches for detecting and modeling associations, often applied to data having a large number of features and/or complex associations. Currently, there are many tools to facilitate implementing custom ML analyses (e.g. scikit-learn). Interest is also increasing in automated ML packages, which can make it easier for non-experts to apply ML and have the potential to improve model performance. ML permeates most subfields of biomedical research with varying levels of rigor and correct usage. Tremendous opportunities offered by ML are frequently offset by the challenge of assembling comprehensive analysis pipelines, and the ease of ML misuse. In this work we have laid out and assembled a complete, rigorous ML analysis pipeline focused on binary classification (i.e. case/control prediction), and applied this pipeline to both simulated and real world data. At a high level, this 'automated' but customizable pipeline includes a) exploratory analysis, b) data cleaning and transformation, c) feature selection, d) model training with 9 established ML algorithms, each with hyperparameter optimization, and e) thorough evaluation, including appropriate metrics, statistical analyses, and novel visualizations. This pipeline organizes the many subtle complexities of ML pipeline assembly to illustrate best practices to avoid bias and ensure reproducibility. Additionally, this pipeline is the first to compare established ML algorithms to 'ExSTraCS', a rule-based ML algorithm with the unique capability of interpretably modeling heterogeneous patterns of association. While designed to be widely applicable we apply this pipeline to an epidemiological investigation of established and newly identified risk factors for pancreatic cancer to evaluate how different sources of bias might be handled by ML algorithms. ","A Rigorous Machine Learning Analysis Pipeline for Biomedical Binary
  Classification: Application in Pancreatic Cancer Nested Case-control Studies
  with Implications for Bias Assessments"
7,1300832956701970434,3832040415,Ran Zmigrod,"['Check out our new TACL paper: Efficient Computation of Expectations under Spanning Tree Distributions [<LINK>]:\n🌲We save a factor of O(n) over existing algorithms\n✨Explain the backprop ⇔ expectation connection\n🔥Open source PyTorch implementation coming soon', 'The Matrix-Tree Theorem (MTT) provides an efficient algorithm to compute the normalization factor of an MST-based parser. In addition, the algorithm has been widely used to compute quantities such as risk, entropy as well as their gradients.', 'However, some oversights to MTT’s efficiency have been made resulting in suboptimal algorithms. We propose a generalization to computing expectations and their gradients using MTT using automatic differentiation.', 'As a concrete example, we provide a simple O(n^3) algorithm to compute the entropy of an MST-based parser, improving over the O(n^4) suggestion by Smith and Eisner (2007). Joint work with @xtimv and @ryandcotterell. Code at https://t.co/N5rZwhOfv7.', '@ryandcotterell @xtimv @hannawallach Who else is in the group besides us @ryandcotterell ?']",https://arxiv.org/abs/2008.12988,"We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement with or without automatic differentiation software. We motivate the development of our framework with several \emph{cautionary tales} of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 15 and 9 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively. ",Efficient Computation of Expectations under Spanning Tree Distributions
8,1300806821523984385,863828060600139776,Dr. Deep Anand,"['check out our new paper on the arXiv, in which we derive the mass of the M104 (Sombrero) group! #cosmicflows\n\n<LINK> <LINK>']",https://arxiv.org/abs/2008.13152v1,"Distances and radial velocities of galaxies in the vicinity of the luminous early-type galaxy M 104 (Sombrero) are used to derive its dark matter mass. Two dwarf galaxies: UGCA 307 and KKSG 30 situated near M 104 were observed with the Advanced Camera for Surveys on the Hubble Space Telescope. The distances $9.03^{+0.84}_{-0.51}$ Mpc (UGCA 307) and $9.72^{+0.44}_{-0.41}$ Mpc (KKSG 30) were determined using the tip of the red giant branch method. These distances are consistent with the dwarf galaxies being satellites of Sombrero. Using radial velocities and projected separations of UGCA 307, KKSG 30, and a third galaxy with an accurate distance (KKSG 29), as well as 12 other assumed companions with less accurate distances, the total mass of M 104 is estimated to be $(1.55\pm0.49) 10^{13} M_{\odot}$. At the K-band luminosity of the Sombrero galaxy of $2.4 10^{11} L_{\odot}$, its total mass-to-luminosity ratio is $M_T/L_K = (65\pm20)M_{\odot}/L_{\odot}$, which is about three times higher than that of luminous bulgeless galaxies. ",] Distance and mass of the M104 (Sombrero) group
9,1300796093194797058,3376954971,PaoloTozzi,"[""Gas-rich galaxies and a BCG wannabe lurking in the midst of a protocluster: enjoy the new paper by Quirino D'Amato\n<LINK> <LINK>""]",https://arxiv.org/abs/2008.13665,"Based on ALMA Band 3 observations of the CO(2-1) line transition, we report the discovery of three new gas-rich (M_H2 ~ 1.5-4.8 x 10^10 M_sun, SFRs in the range ~5-100 M_sun/yr) galaxies in an overdense region at z=1.7, that already contains eight spectroscopically confirmed members. This leads to a total of 11 confirmed overdensity members, within a projected distance of ~ 1.15 Mpc and in a redshift range of Dz = 0.012. Under simple assumptions, we estimate that the system has a total mass of >= 3-6 x 10^13 M_sun, and show that it will likely evolve into a >~ 10^14 M_sun cluster at z = 0. The overdensity includes a powerful Compton-thick Fanaroff-Riley type II (FRII) radio-galaxy, around which we discovered a large molecular gas reservoir (M_H2 ~ 2 x 10^11 M_sun). We fitted the FRII resolved CO emission with a 2-D Gaussian model with major (minor) axis of ~ 27 (~ 17) kpc, that is a factor of ~3 larger than the optical rest-frame emission. Under the assumption of a simple edge-on disk morphology, we find that the galaxy interstellar medium produces a column density towards the nucleus of ~ 5.5 x 10^23 cm^-2. Such a dense ISM may then contribute significantly to the total nuclear obscuration measured in the X-rays (N_(H,X) ~ 1.5 x 10^24 cm^-2) in addition to a small, pc-scale absorber around the central engine. The velocity map of this source unveils a rotational motion of the gas that is perpendicular to the radio-jets. The FRII is located at the center of the projected spatial distribution of the structure members, and its velocity offset from the peak of the redshift distribution is well within the structure's velocity dispersion. All this, coupled with the large amount of gas around the FRII, its stellar mass of ~ 3 x 10^11 M_sun, SFR of ~ 200-600 M_sun/yr, and powerful radio-to-X-ray emission, suggests that this source is the likely progenitor of the future brightest cluster galaxy. ","Discovery of molecular gas fueling galaxy growth in a protocluster at
  z=1.7"
10,1300713906575376384,944291984675614721,Tobias de Jong,"['New paper on arXiv: <LINK> 😁 We analyze the heterogeneity of twisted bilayer #graphene near the #magicangle, extracting the relative displacement of the layers from the #moire pattern as observed in #STM. <LINK>', 'We worked with the Milan Allan group (also @LeidenPhysics ) to extract these properties using geometric phase analysis from moire patterns as measured on devices made in Barcelona @ICFOnians by the Efetov group.', 'The relevant properties here are the relative strain between the two lattices, and the effective twist angle, which both influence the band structure of  #superconducting #magic angle #twisted bilayer #graphene significantly (at least according to theory).', 'The cool part to me here was that the moire pattern magnifies any shift of the two lattices by a large ( ~50 at magic angle) factor, but does so with a literal twist of about 90 degrees: horizontal shifts of moire pattern indicate vertical shifts of the lattices.', 'I should make an animation to visualize this.😅']",https://arxiv.org/abs/2008.13766,"We introduce a new method to continuously map inhomogeneities of a moir\'e lattice and apply it to large-area topographic images we measure on open-device twisted bilayer graphene (TBG). We show that the variation in the twist angle of a TBG device, which is frequently conjectured to be the reason for differences between devices with a supposed similar twist angle, is about 0.08{\deg} around the average of 2.02{\deg} over areas of several hundred nm, comparable to devices encapsulated between hBN slabs. We distinguish between an effective twist angle and local anisotropy and relate the latter to heterostrain. Our results imply that for our devices, twist angle heterogeneity has a roughly equal effect to the electronic structure as local strain. The method introduced here is applicable to results from different imaging techniques, and on different moir\'e materials. ","Measuring local moir\'e lattice heterogeneity of twisted bilayer
  graphene"
11,1300674685378744320,1248290263698718721,Peter Dueben,"['Our new paper on post-processing of precipitation predictions over the UK is out on the arxiv: <LINK>\nWe tackle scale interactions of the atmosphere in space and time using fused temporal cross attention in combination with ConvGrus and improve results. <LINK>', 'Great collaboration between @warwickuni with Rilwan Adewoyin, Ritabrata Dutta and @Yulanhe, @BristolUni with @PeterAGWatson and @ECMWF.']",https://arxiv.org/abs/2008.09090,"Climate models (CM) are used to evaluate the impact of climate change on the risk of floods and strong precipitation events. However, these numerical simulators have difficulties representing precipitation events accurately, mainly due to limited spatial resolution when simulating multi-scale dynamics in the atmosphere. To improve the prediction of high resolution precipitation we apply a Deep Learning (DL) approach using an input of CM simulations of the model fields (weather variables) that are more predictable than local precipitation. To this end, we present TRU-NET (Temporal Recurrent U-Net), an encoder-decoder model featuring a novel 2D cross attention mechanism between contiguous convolutional-recurrent layers to effectively model multi-scale spatio-temporal weather processes. We use a conditional-continuous loss function to capture the zero-skewed %extreme event patterns of rainfall. Experiments show that our model consistently attains lower RMSE and MAE scores than a DL model prevalent in short term precipitation prediction and improves upon the rainfall predictions of a state-of-the-art dynamical weather model. Moreover, by evaluating the performance of our model under various, training and testing, data formulation strategies, we show that there is enough data for our deep learning approach to output robust, high-quality results across seasons and varying regions. ","TRU-NET: A Deep Learning Approach to High Resolution Prediction of
  Rainfall"
12,1300652254307840000,479745680,Valeri,"[""New paper on arXiv with @adamsolo, Yashar Akrami and Misao Sasaki. Turns out we don't really need flat potentials for dark energy. <LINK>""]",https://arxiv.org/abs/2008.13660,"We argue that dark energy with multiple fields is theoretically well-motivated and predicts distinct observational signatures, in particular when cosmic acceleration takes place along a trajectory that is highly non-geodesic in field space. Such models provide novel physics compared to $\Lambda$CDM and quintessence by allowing cosmic acceleration on steep potentials. From the theoretical point of view, these theories can easily satisfy the conjectured swampland constraints and may in certain cases be technically natural, potential problems which are endemic to standard single-field dark energy. Observationally, we argue that while such multi-field models are likely to be largely indistinguishable from the concordance cosmology at the background level, dark energy perturbations can cluster, leading to an enhanced growth of large-scale structure that may be testable as early as the next generation of cosmological surveys. ",Multi-field dark energy: cosmic acceleration on a steep potential
13,1300512560341549061,999856806,Sergei V. Gleyzer 🇺🇸🇺🇦,"['A new paper on unsupervised deep learning techniques for identifying dark matter substructure using strong gravitational lensing <LINK>\nContinuing an exciting collaboration with @stephstem @emanuele_usai @ReddyPranath and others on this interdisciplinary project', 'This is a follow-up on our earlier supervised learning work in this area https://t.co/w7O6rGGZCR', 'The paper contains excellent contributions from @ReddyPranath who took part in the 2020 Google Summer of Code with us at CERN-HSF @gsoc @GoogleOSS']",https://arxiv.org/abs/2008.12731,"The identity of dark matter remains one of the most pressing questions in physics today. While many promising dark matter candidates have been put forth over the last half-century, to date the true identity of dark matter remains elusive. While it is possible that one of the many proposed candidates may turn out to be dark matter, it is at least equally likely that the correct physical description has yet to be proposed. To address this challenge, novel applications of machine learning can help physicists gain insight into the dark sector from a theory agnostic perspective. In this work we demonstrate the use of unsupervised machine learning techniques to infer the presence of substructure in dark matter halos using galaxy-galaxy strong lensing simulations. ",Decoding Dark Matter Substructure without Supervision
14,1300450571439144961,2303004390,Brandon Amos,"['In our new paper we scale model-based reinforcement learning to the gym humanoid by using short-horizon model rollouts followed by a learned model-free value estimate.\n\nPaper: <LINK>\nVideos: <LINK>\n\nWith @sam_d_stanton @denisyarats @andrewgwils <LINK>', ""We don't propose any extremely new approaches in here. We just show that the right combination of known techniques works well, mostly leveraging short-horizon model rollouts, the stochastic value gradient, and entropy-based policy regularization: https://t.co/ZWeimxjWIx"", 'This combination has competitive performance on the benchmarks used by two recent state-of-the-art model-based reinforcement learning methods (POPLIN and MBPO): https://t.co/hEQtYpUl2Q', 'We also found that a deterministic dynamics model works well in this short-horizon setting and ran an experiment to better-understand how this generalizes compared to an ensembled dynamics model: https://t.co/Xb8MBGiTvz', 'Another interesting question here is once you have a short-horizon dynamics model, do you use it to improve the policy update, value update, or both? We ablated this and find that in our setting using it only in the policy update is the most useful: https://t.co/AU0nDzHJE2', 'Finally, there are a lot of awesome ideas in model-based reinforcement learning. Here are a few key dimensions to think about them on: https://t.co/zKONx0TPE0']",https://arxiv.org/abs/2008.12775,"For over a decade, model-based reinforcement learning has been seen as a way to leverage control-based domain knowledge to improve the sample-efficiency of reinforcement learning agents. While model-based agents are conceptually appealing, their policies tend to lag behind those of model-free agents in terms of final reward, especially in non-trivial environments. In response, researchers have proposed model-based agents with increasingly complex components, from ensembles of probabilistic dynamics models, to heuristics for mitigating model error. In a reversal of this trend, we show that simple model-based agents can be derived from existing ideas that not only match, but outperform state-of-the-art model-free agents in terms of both sample-efficiency and final reward. We find that a model-free soft value estimate for policy evaluation and a model-based stochastic value gradient for policy improvement is an effective combination, achieving state-of-the-art results on a high-dimensional humanoid control task, which most model-based agents are unable to solve. Our findings suggest that model-based policy evaluation deserves closer attention. ","On the model-based stochastic value gradient for continuous
  reinforcement learning"
15,1300370689887735808,1298963141745930242,Javier Nistal,['New paper accepted for ISMIR2020! 🤠 DrumGAN: Synthesis of Drum Sounds With Timbral Feature Conditioning Using GANs\n\npaper: <LINK>\nblog: <LINK>\n\n@deeplearnmusic @SonyCSLParis @CSLMusicTeam @TelecomParis_ @MIP_frontiers \n\n#music #DL #AI #ML #GANs'],https://arxiv.org/abs/2008.12073,"Synthetic creation of drum sounds (e.g., in drum machines) is commonly performed using analog or digital synthesis, allowing a musician to sculpt the desired timbre modifying various parameters. Typically, such parameters control low-level features of the sound and often have no musical meaning or perceptual correspondence. With the rise of Deep Learning, data-driven processing of audio emerges as an alternative to traditional signal processing. This new paradigm allows controlling the synthesis process through learned high-level features or by conditioning a model on musically relevant information. In this paper, we apply a Generative Adversarial Network to the task of audio synthesis of drum sounds. By conditioning the model on perceptual features computed with a publicly available feature-extractor, intuitive control is gained over the generation process. The experiments are carried out on a large collection of kick, snare, and cymbal sounds. We show that, compared to a specific prior work based on a U-Net architecture, our approach considerably improves the quality of the generated drum samples, and that the conditional input indeed shapes the perceptual characteristics of the sounds. Also, we provide audio examples and release the code used in our experiments. ","DrumGAN: Synthesis of Drum Sounds With Timbral Feature Conditioning
  Using Generative Adversarial Networks"
16,1300252747086704642,141440459,Rod Van Meter 🌻,"[""New paper dance! First author @SaraMetwalli's first full quantum paper. Finding cliques in a graph, on a quantum computer.\n<LINK>""]",https://arxiv.org/abs/2008.12525,"Algorithms for triangle-finding, the smallest nontrivial instance of the k-clique problem, have been proposed for quantum computers. Still, those algorithms assume the use of fixed access time quantum RAM (QRAM). We present a practical gate-based approach to both the triangle-finding problem and its NP-hard k-clique generalization. We examine both constant factors for near-term implementation on a Noisy Intermediate Scale Quantum computer (NISQ) device, and the scaling of the problem to evaluate long-term use of quantum computers. We compare the time complexity and circuit practicality of the theoretical approach and actual implementation. We propose and apply two different strategies to the k-clique problem, examining the circuit size of Qiskit implementations. We analyze our implementations by simulating triangle finding with various error models, observing the effect on damping the amplitude of the correct answer, and compare to execution on six real IBMQ machines. Finally, we estimate the date when the methods proposed can run effectively on an actual device based on IBM's quantum volume exponential growth forecast and the results of our error analysis. ",Finding Small and Large k-Clique Instances on a Quantum Computer
17,1299422149804019714,735269593443274753,Dr. Jennifer Burt,"['We have a new paper on @Arxiv today!! It describes the discovery and characterization of TOI-824 b, a new sub-Neptune planet that whips around a nearby K star in just 1.4 days: <LINK>', ""The planet was first detected by @NASA_TESS. After ground based follow up via ExoFOP TESS confirmed that TOI-824 was singular and that the transits were actually happening on that star, we started a precision RV campaign to try and measure the planet's mass. https://t.co/1iucc4JI3l"", 'We observed the star from both @LCOAstro (using PFS) and @ESO_Chile (using HARPS) and got a beautiful RV phase curve where the period and phase curve matched up exactly with the TESS transits https://t.co/fgSYROxEQ0', ""And we thought that was that! We did a full system fit using @exofastupdates, got a pretty precise planet mass and radius and everything looked quite nice. This seemed like the most straightforward planet confirmation I'd ever worked on... BUT THEN!"", ""Then we folded the ground based follow up photometry into the EXOFASTv2 fit. Even though they're less precise than TESS, those later transits help to constrain the planet's transit ephemeris, which is important for planning any future transit observations."", ""The ground based photometry (which has a higher spatial resolution, also provides a good check on the transit depth measured in the TESS data. Since the TESS pixels are so large (21' across) they can often include light from more than one star which can dilute the transit signal https://t.co/2obVfnsUIB"", 'So we included five follow up transits from @LCO_Global and PEST and reran EXOFASTv2 including a dilution term for the TESS data and... suddenly the radius of the planet went from 3.4 Earth radii to 2.93 Earth radii. Which is a pretty big shift. https://t.co/lVr1bSw8rz', ""But it wasn't due to stellar contamination, which was our 1st assumption! Turns out TOI-824 is in such a crowded region of the sky that when the TESS pipeline tried to measure the background flux using nearby pixels w/o any stars in them there just *weren't* any star-free pixels https://t.co/jR987zXGpk"", ""This meant the background estimate was biased, altering the transit depth measurement. But the SPOC team was able to ID the issue and fix the background correction algorithm so this won't impact Cycle 3 onwards - it was a super quick/thorough response to a community-raised issue!"", ""(Note: The transit depth bias is &lt;2% for the vast majority of stars already observed by TESS because they're in less crowded regions of the sky, so there's likely no need to worry about your favorite TOIs)"", 'After sorting all of this out and including a dilution factor in our EXOFASTv2 fit to address the background bias, we now have a refinded planet mass and radius (2.93 R_earth, 18.47 M_Earth) that place TOI-824 b at the bottom of the hot Neptune desert https://t.co/lLAqqvQhbm', ""With a TSM of 85, TOI-824 b isn't the most obvious choice for atmospheric follow up. But if it follows the trend noted in Crossfield &amp; Kreidberg 2017 then it may have relatively large spectral features due to its high temperature"", 'Either way, this planet would make for a good test of the higher temperature -&gt; larger spectral features hypothesis!', ""Together, the final planet radius and the mass precision provided by the PFS and HARPS data also mean that TOI-824 b counts towards TESS' level one science goal of measuring the masses of fifty small exoplanets -- so now we're 2% closer to meeting that goal! https://t.co/wQs2ZNHIRV""]",https://arxiv.org/abs/2008.11732,"We report the detection of a transiting hot Neptune exoplanet orbiting TOI-824 (SCR J1448-5735), a nearby (d = 64 pc) K4V star, using data from the \textit{Transiting Exoplanet Survey Satellite} (TESS). The newly discovered planet has a radius, $R_{\rm{p}}$ = 2.93 $\pm$ 0.20 R$_{\oplus}$, and an orbital period of 1.393 days. Radial velocity measurements using the Planet Finder Spectrograph (PFS) and the High Accuracy Radial velocity Planet Searcher (HARPS) spectrograph confirm the existence of the planet and we estimate its mass to be $M_{\rm{p}}$ = 18.47 $\pm$ 1.84 M$_{\oplus}$. The planet's mean density is $\rho_{\rm{p}}$ = 4.03$^{+0.98}_{-0.78}$ g cm$^{-3}$ making it more than twice as dense as Neptune. TOI-824 b's high equilibrium temperature makes the planet likely to have a cloud free atmosphere, and thus an excellent candidate for follow up atmospheric studies. The detectability of TOI-824 b's atmosphere from both ground and space is promising and could lead to the detailed characterization of the most irradiated, small planet at the edge of the hot Neptune desert that has retained its atmosphere to date. ",TOI-824 b: A New Planet on the Lower Edge of the Hot Neptune Desert
18,1299336906497552385,866221331184050176,Maximilian N. Günther,"[""Yesterday we had our new paper on @arxiv, so it's time for a short summary. Young M dwarfs are weird! We previously found some crazy light curves in Kepler/K2 and TESS, and are now adding some observational pieces to the puzzle to try to understand them. <LINK> <LINK>"", 'A short history: binaries as well as spotted M dwarfs can make for beautiful light curves, no doubt, but they are still rather ""smooth"" rotators, meaning, they only leave us with a few peaks in the frequency spectrum. Like this example of GJ 1243 in @NASA_TESS : https://t.co/JEz7jwbl8E', 'Now, a few years ago, people started finding exceptions to these stars. For example, the dippers and bursters! Weird patterns, deep and sharp-peaked modulations. But no strict periodicity: https://t.co/vMMFLC7hq2', 'Their mystery was rather quickly solved (or so we think). Circumstellar material being accreted onto the star can, depending on the angle we look at the system, lead to these wild features: https://t.co/7WZx4i9mAp', 'So far so good, if it wasn\'t for old boi Kepler/K2, who helped us find even stranger things. Dubbed ""scallop shells"" by Stauffer et al., these young M dwarfs had rapid rotation periods of hours and showed extremely sharp-peaked and strictly periodic features: https://t.co/9H15RBSkDX', 'Neither spots alone nor accreting dust disks made sense for this. So Stauffer et al. shaped the idea that some patchy torus of clouds of material (maybe dust, maybe gas, maybe a mix) might orbit at the co-rotation radius and cause this phenomenon: https://t.co/i2GQHfClLN', 'Jump ahead two year, @TESSatMIT enters the playing field, and with its 2-min cadence and focus on nearby, bright M dwarfs gives us another chance to dig into this topic. And so we found ten ""complex rotators"" back in 2019 (https://t.co/8DJ3ttgqM7): https://t.co/db3U9J3biw', 'And, science being science, we thought of yet another hypothesis: maybe those are just some ""boring"" rapidly rotating stars with spots, but have a slightly inclined dust disk around them. Then the spots get occulted behind the dust, and lead to the sudden, sharp features: https://t.co/noUFSV0UpY', 'So, in the new paper (https://t.co/Ab8lEO8Mhu), we set out to probe all this. How about occurence rates? Do we even have enough young M dwarfs to explain either the co-rotating clouds or the spots and misaligned disks? Turns out, yes we do: https://t.co/6MkjCmAgrT', '(yay, simple math!) https://t.co/KBScJNX3TN', 'So both could work, but it means a large fraction of young M dwarfs must have either some co-rotating clouds or some misaligned disks around them! That might have quite interesting implications for any exoplanets in those systems...', 'Next, we used the SPECULOOS Southern Observatory (named after the delicious Belgian cookies and successor of its TRAPPIST-1 finding prototype) to obtain simultaneous multi-color observations of four of our ten targets. And boi oh boi do we see color dependencies! https://t.co/GFXvCYtx0L', ""(sorry for mixing red and green colors, it will be changed into red and blue soon, so it's easier to see for everyone!)"", ""Alright alright alright, you say, that can be a proof for spots and allow us to measure spot temperatures, but it could also mean the circumstellar material absorbs differently across wavelengths. You're right!"", 'Now, how stable and periodic are those ""stable and periodic"" patterns really? Five of our ten targets had observations spanning over one year, includen TIC 177#, which fell into the @NASA_TESS continuos viewing zone! And I can tell you: these fellas ARE stable and periodic. https://t.co/92ygp7kDP5', 'We also investigated their flaring, developed some toy models to mimic the patterns, dug into stellar-activity corrections for the stellar parameters, and much more. But this thread is already too long, so I encourage you to have a look at the paper for all details :) https://t.co/ylqfHoiPdb', 'And finally, after all these efforts, only one thing is save to say to our complex rotators: https://t.co/MlwMcZkufJ', ""(Just kidding. We will find out what you are... You might win this time, complex rotators, but don't celebrate for too long...) https://t.co/t33co9nHiK"", '@threader_app @arifsolmaz Wow, this is cool!']",https://arxiv.org/abs/2008.11681,"New sets of young M dwarfs with complex, sharp-peaked, and strictly periodic photometric modulations have recently been discovered with Kepler/K2 (scallop shells) and TESS (complex rotators). All are part of star-forming associations, are distinct from other variable stars, and likely belong to a unified class. Suggested hypotheses include star spots, accreting dust disks, co-rotating clouds of material, magnetically constrained material, spots and misaligned disks, and pulsations. Here, we provide a comprehensive overview and add new observational constraints with TESS and SPECULOOS Southern Observatory (SSO) photometry. We scrutinize all hypotheses from three new angles: (1) we investigate each scenario's occurrence rates via young star catalogs; (2) we study the features' longevity using over one year of combined data; and (3) we probe the expected color dependency with multi-color photometry. In this process, we also revisit the stellar parameters accounting for activity effects, study stellar flares as activity indicators over year-long time scales, and develop toy models to simulate typical morphologies. We rule out most hypotheses, and only (i) co-rotating material clouds and (ii) spots and misaligned disks remain feasible - with caveats. For (i), co-rotating dust might not be stable enough, while co-rotating gas alone likely cannot cause percentage-scale features; and (ii) would require misaligned disks around most young M dwarfs. We thus suggest a unified hypothesis, a superposition of large-amplitude spot modulations and sharp transits of co-rotating gas clouds. While the complex rotators' mystery remains, these new observations add valuable pieces to the puzzle going forward. ","Complex Modulation of Rapidly Rotating Young M Dwarfs: Adding Pieces to
  the Puzzle"
19,1299336291469930498,5935462,Marco De Nadai,"['Thread) ""Describe What to Change: A Text-guided Unsupervised Image-to-Image Translation Approach"" is our new paper trying to manipulate images from textual commands. <LINK>. This is not #ECCV2020 but a sneak peek of #ACMMM2020 <LINK>', '1) In literature, there are a lot of approaches trying to generate images from text, but no works try to modify images keeping the content (e.g. identity of people) while changing the style (e.g. the hair colour). Thus, we resort image-to-image translation approaches.', '2) First, we do not need a full textual description of the image we desire, thus we automatically generate textual commands. These commands generalize what human would say to change some parts of an image https://t.co/GFOnKw7F8L', '3) As commands are inherently ambiguous (what does it mean ""make her blonde""? Dark-blonde?!? Scandinavian-style blonde??!?) we generate multiple images per translation. Our model is thus multi-modal. Ofc, we learn to modify images in an unsupervised fashion. https://t.co/92QVdhcE8J', '4) Our results show that we can modify images using textual commands, which is very useful especially for impaired users, or for Alexa-type tools (ehm Google!). Ah do you want to edit other pictures? Birds? here we are https://t.co/sP2OvRRjyT', 'Our multi-modal and multi-domain unsupervised learned latent space is smooth and allows also to travel between transformations. Interpolations are very smooth! https://t.co/xM88QnbyX9', 'This said, please give us feedback on our paper! Sources and data are available! https://t.co/IGayRyvxmw.']",https://arxiv.org/abs/2008.04200,"Manipulating visual attributes of images through human-written text is a very challenging task. On the one hand, models have to learn the manipulation without the ground truth of the desired output. On the other hand, models have to deal with the inherent ambiguity of natural language. Previous research usually requires either the user to describe all the characteristics of the desired image or to use richly-annotated image captioning datasets. In this work, we propose a novel unsupervised approach, based on image-to-image translation, that alters the attributes of a given image through a command-like sentence such as ""change the hair color to black"". Contrarily to state-of-the-art approaches, our model does not require a human-annotated dataset nor a textual description of all the attributes of the desired image, but only those that have to be modified. Our proposed model disentangles the image content from the visual attributes, and it learns to modify the latter using the textual description, before generating a new image from the content and the modified attribute representation. Because text might be inherently ambiguous (blond hair may refer to different shadows of blond, e.g. golden, icy, sandy), our method generates multiple stochastic versions of the same translation. Experiments show that the proposed model achieves promising performances on two large-scale public datasets: CelebA and CUB. We believe our approach will pave the way to new avenues of research combining textual and speech commands with visual attributes. ","Describe What to Change: A Text-guided Unsupervised Image-to-Image
  Translation Approach"
20,1299306215491670020,933713808018862082,Cian Scannell,"['New paper online using domain-adversarial learning to train a U-Net to segment cardiac MR images that generalises well across different domains (MR scanners). Accepted at STACOM @MICCAI20 for the M&amp;Ms challenge. @achir76 @mitkoveta \n<LINK>', 'The idea (Ganin et al. JMLR 2016) is to simultaneously train the segmentation U-Net and a domain classifier to discriminate between input domains based on the internal representations of the U-Net. Then to update the weights of U-Net to maximise the loss of the domain classifier https://t.co/1b4oYmaPPv', ""This ensures that the domain of the input cannot be recovered from the features learned by the U-Net.\nIt learns features that are less dependent on domain information than conventional training (t-SNE embeddings shown) and performance doesn't drop off when applied to new domains https://t.co/HNOv31zCXO""]",https://arxiv.org/abs/2008.11776,"Cine cardiac magnetic resonance (CMR) has become the gold standard for the non-invasive evaluation of cardiac function. In particular, it allows the accurate quantification of functional parameters including the chamber volumes and ejection fraction. Deep learning has shown the potential to automate the requisite cardiac structure segmentation. However, the lack of robustness of deep learning models has hindered their widespread clinical adoption. Due to differences in the data characteristics, neural networks trained on data from a specific scanner are not guaranteed to generalise well to data acquired at a different centre or with a different scanner. In this work, we propose a principled solution to the problem of this domain shift. Domain-adversarial learning is used to train a domain-invariant 2D U-Net using labelled and unlabelled data. This approach is evaluated on both seen and unseen domains from the M\&Ms challenge dataset and the domain-adversarial approach shows improved performance as compared to standard training. Additionally, we show that the domain information cannot be recovered from the learned features. ","Domain-Adversarial Learning for Multi-Centre, Multi-Vendor, and
  Multi-Disease Cardiac MR Image Segmentation"
21,1299222634098376704,140287694,Anowar J Shajib,"['Our new paper is on arXiv today: <LINK>.\n\nBy analyzing 23 elliptical lens galaxies from SLACS, we find that the dark matter distribution is close to the NFW profile on average at z~0.2 without any contraction/expansion.\n\nA summary with some figures in the thread.', 'We perform state-of-the-art lens modeling for these 23 lens galaxies. This figure shows 5 of them. https://t.co/tP9NrGyH5S', 'We combine the strong lensing constraints with the stellar kinematics and weak lensing measurements to individually constrain the stellar and dark matter distributions. Our model allows for adiabatic contraction in the dark matter and a M/L gradient in the stellar distribution. https://t.co/XhE2OHahsz', 'We find that the NFW+stars profile deviate upwards by ~5% on average from the power-law model near the Einstein radius. https://t.co/IAILEn1rL4', 'On average, there is no significant contraction/expansion in the dark matter halos with M_200 ~ 10^13.1 M_sun at z~0.2. Furthermore, almost no gradient in the stellar M/L is favored around the effective or half-light radius. https://t.co/kk96jRfzxy', 'Comparing our inferred stellar masses with those from SPS-based measurements supports a heavy IMF like the Salpeter IMF in these elliptical galaxies. https://t.co/IglQXCUWcD', 'All of our results are consistent with a scenario where halos of  massive elliptical galaxies first contract up to z~2 due to baryonic cooling, but then the halos primarily grow through dissipationless mergers while AGN feedback counteracts the initial contraction.']",https://arxiv.org/abs/2008.11724,"We investigate the internal structure of elliptical galaxies at $z\sim 0.2$ from a joint lensing-dynamics analysis. We model Hubble Space Telescope images of a sample of 23 galaxy-galaxy lenses selected from the Sloan Lens ACS (SLACS) survey. Whereas the original SLACS analysis estimated the logarithmic slopes by combining the kinematics with the imaging data, we estimate the logarithmic slopes only from the imaging data. We find that the distribution of the lensing-only logarithmic slopes has a median $2.08\pm0.03$ and intrinsic scatter $0.13 \pm 0.02$, consistent with the original SLACS analysis. We combine the lensing constraints with the stellar kinematics and weak lensing measurements, and constrain the amount of adiabatic contraction in the dark matter (DM) halos. We find that the DM halos are well described by a standard Navarro-Frenk-White halo with no contraction on average for both of a constant stellar mass-to-light ratio ($M/L$) model and a stellar $M/L$ gradient model. For the $M/L$ gradient model, we find that most galaxies are consistent with no $M/L$ gradient. Comparison of our inferred stellar masses with those obtained from the stellar population synthesis method supports a heavy initial mass function (IMF) such as the Salpeter IMF. We discuss our results in the context of previous observations and simulations, and argue that our result is consistent with a scenario in which active galactic nucleus feedback counteracts the baryonic-cooling-driven contraction in the DM halos. ","Dark matter halos of massive elliptical galaxies at $z \sim 0.2$ are
  well described by the Navarro-Frenk-White profile"
22,1299171177449963520,393956778,Hang Li,"['Our new archive paper ""AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization"".  It works better than BERT,  Albert, XLNet, etc at CLUE and GLUE.  <LINK>']",https://arxiv.org/abs/2008.11869,"Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU). The tokens in the models are usually fine-grained in the sense that for languages like English they are words or sub-words and for languages like Chinese they are characters. In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable. In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models. In this paper, we propose a novel pre-trained language model, referred to as AMBERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations. For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases. Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE. The results show that AMBERT can outperform BERT in all cases, particularly the improvements are significant for Chinese. We also develop a method to improve the efficiency of AMBERT in inference, which still performs better than BERT with the same computational cost as BERT. ",AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization
23,1299165697285611522,114562472,Prof. Emily Levesque 🤓✨🔭📚,"[""Check out this snazzy new figure from the latest @UW Massive Stars group paper, led by @trevorzaylen! 🤩\n\nYou'll want to read the whole paper: it also shares the SUPER-cool discovery of a new class of pulsating yellow supergiants 🌟 #FYPS Check it out at <LINK>! <LINK>""]",https://arxiv.org/abs/2008.11723,"Massive stars briefly pass through the yellow supergiant (YSG) phase as they evolve redward across the HR diagram and expand into red supergiants (RSGs). Higher-mass stars pass through the YSG phase again as they evolve blueward after experiencing significant RSG mass loss. These post-RSG objects offer us a tantalizing glimpse into which stars end their lives as RSGs, and why. One telltale sign of a post-RSG object may be an instability to pulsations, depending on the star's interior structure. Here we report the discovery of five YSGs with pulsation periods faster than 1 day, found in a sample of 76 cool supergiants observed by \tess at two-minute cadence. These pulsating YSGs are concentrated in a HR diagram region not previously associated with pulsations; we conclude that this is a genuine new class of pulsating star, Fast Yellow Pulsating Supergiants (FYPS). For each FYPS, we extract frequencies via iterative prewhitening and conduct a time-frequency analysis. One FYPS has an extracted frequency that is split into a triplet, and the amplitude of that peak is modulated on the same timescale as the frequency spacing of the triplet; neither rotation nor binary effects are likely culprits. We discuss the evolutionary status of FYPS and conclude that they are candidate post-RSGs. All stars in our sample also show the same stochastic low-frequency variability (SLFV) found in hot OB stars and attributed to internal gravity waves. Finally, we find four $\alpha$ Cygni variables in our sample, of which three are newly discovered. ","Short Term Variability of Evolved Massive Stars with TESS II: A New
  Class of Cool, Pulsating Supergiants"
24,1299055725654802433,400104438,Max Laves,['Our new paper “Uncertainty Estimation in Medical Image Denoising with Bayesian Deep Image Prior” was accepted as long oral at UNSURE workshop at MICCAI 2020 🎉\nPaper: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/2008.08837,"Uncertainty quantification in inverse medical imaging tasks with deep learning has received little attention. However, deep models trained on large data sets tend to hallucinate and create artifacts in the reconstructed output that are not anatomically present. We use a randomly initialized convolutional network as parameterization of the reconstructed image and perform gradient descent to match the observation, which is known as deep image prior. In this case, the reconstruction does not suffer from hallucinations as no prior training is performed. We extend this to a Bayesian approach with Monte Carlo dropout to quantify both aleatoric and epistemic uncertainty. The presented method is evaluated on the task of denoising different medical imaging modalities. The experimental results show that our approach yields well-calibrated uncertainty. That is, the predictive uncertainty correlates with the predictive error. This allows for reliable uncertainty estimates and can tackle the problem of hallucinations and artifacts in inverse medical imaging tasks. ","Uncertainty Estimation in Medical Image Denoising with Bayesian Deep
  Image Prior"
25,1298983558787108864,1031565378,Evan Rich,"[""It's a paper day! Check out my new paper on Herbig Ae star and always different protoplanetary disk HD 163296 (MWC 275). Inside you will find observations in the UV, optical, near-IR, and radio! Check out the thread below to get the hi-lights. <LINK>"", 'First off is our new 2 epochs of Hubble STIS coronographic images, then compared to a previous STIS image taken in 1998. There is a dust ring at 3.25"" (330 au) and in none of the epochs does the dust ring look the same! The most recent epochs are only 3 months apart. https://t.co/ylnQ0D87lN', 'The dust ring at 3.25"" is the 4th ring of the system. Located at 330 au, it has a minor axis offset of 0.7"" giving the dust a height of 64 au. Interestingly, the proposed Pinte+2018 planet is located within the 3rd and 4th ring,  giving this planet candidate more evidence. https://t.co/yaBClUIFqG', 'What is also interesting in the HST/STIS images is no HH-KNOTS in the 2018 images! They can be seen in the 1998 images. These older knots could be experiencing less shock-heating and have cooled sufficiently to not be observable with HST/STIS. The jet was last seen in UV Aug 2017 https://t.co/JcZctjdltM', 'We did not detect a predicted HH-knot launch in 2018 by Ellerbroek+2014 in our optical, near-IR, or radio observations. We did potentially observe two optical dippers (red arrows), similar to what was seen by Ellerbroek+2014. https://t.co/ztSsVJhtGp', 'The disk illumination timescale &lt;3 months most likely from dust obscuring the outer disk. Dippers seen in the optical are thought to be due to dust obscuration. We can estimate the location and size of the dust and find that the values are very similar suggesting the same origin. https://t.co/lLxn879PrP', 'In summary, HD 163296 continues to be a fascinating and an ever changing system. Hopefully, future observations will identify what is going on with the Jet. Thanks to all of those who helped with this project (esp @wistwitski) and thanks to you for reading this work.']",http://arxiv.org/abs/2008.11606,"We present two new epochs of Hubble Space Telescope/Space Telescope Imaging Spectrograph coronagraphic imaging, along with multi-epoch optical, near-IR, and radio monitoring, of the HD 163296 system. We find ansae features identified in earlier epoch HST imagery are a 4th ring, that resides at a semi-major axis distance of 3.25"" (330 au). We determine the scale height of the dust is 64 au at a radial distance of 330 au. We observe surface brightness variations in the 4th ring on <3 month timescales, including large-scale, azimuthally asymmetric changes. This variability resembles earlier studies of the innermost disk ring (0.66"", 67 au), suggesting a common origin. We find no evidence for the ejection of new HH-knots predicted to occur in 2018. Moreover, our non-detection of older HH-knots indicate the knots could be experiencing less shock-heating. We also detect one clear dipper event in our optical light curve from 2018. Using the time-scale and spatial extent of disk illumination changes we observe, we estimate the source of this shadowing resides within 0.5 au from the star, must extend at least 0.08 au above the midplane of the disk, and has an azimuthal extent of 0.26 au. We estimate the source of the dipper event reaches a scale height of 0.37 au above the midplane at 0.41 au, and has an azimuthal extent of 0.3 au. We suggest these similarities could indicate the same (or similar) mechanisms are responsible for producing both dippers and variable ring illumination in the system. ","Disk Illumination and Jet Variability of the Herbig Ae Star HD 163296
  Using Multi-Epoch HST/STIS Optical, Near-IR, and Radio Imagery and
  Spectroscopy"
26,1298955387836788736,2613619922,byron wallace,"['In new work we set out to automatically generate abstractive brief narrative summaries of all randomized controlled trials relevant to a given clinical question (in the style of Cochrane evidence narrative syntheses). \n\npaper: <LINK> <LINK>', 'We find that generated summaries are relevant and fluent, but struggle with *factuality*; they often mischaracterize the evidence presented in trials. A few simple strategies, like ""decorating"" inputs with automatically extracted elements, seems to improve this a bit. https://t.co/8YWpCCOaue', 'w/Sayantan Saha @h21k and @ijmarshall']",https://arxiv.org/abs/2008.11293,"We consider the problem of automatically generating a narrative biomedical evidence summary from multiple trial reports. We evaluate modern neural models for abstractive summarization of relevant article abstracts from systematic reviews previously conducted by members of the Cochrane collaboration, using the authors conclusions section of the review abstract as our target. We enlist medical professionals to evaluate generated summaries, and we find that modern summarization systems yield consistently fluent and relevant synopses, but that they are not always factual. We propose new approaches that capitalize on domain-specific models to inform summarization, e.g., by explicitly demarcating snippets of inputs that convey key findings, and emphasizing the reports of large and high-quality trials. We find that these strategies modestly improve the factual accuracy of generated summaries. Finally, we propose a new method for automatically evaluating the factuality of generated narrative evidence syntheses using models that infer the directionality of reported findings. ","Generating (Factual?) Narrative Summaries of RCTs: Experiments with
  Neural Multi-Document Summarization"
27,1298806069767045121,1019760963569049601,Almog Yalinewich,"[""New paper on the archive, in which I try to explain how massive stars explode in core collapse supernovae.\n\n<LINK>\n\nLet's dive in\n\n1/9 <LINK>"", 'Background: Massive stars exhaust the available nuclear fuel at their cores. The core contracts to form a proto neutron star, releasing a large amount of energy. A shock wave emerges and tries to move outward, but the whole star collapses inward. Will the shock make it?\n\n2/9 https://t.co/tOy5MHt4kJ', ""In spherically symmetric, 1D computer simulations stars collapse under their own gravity rather than explode. In 2D and 3D they do explode. What's the difference? Turbulence.\n\n3/9 https://t.co/fkDTvpUgmB"", ""But why should turbulence help? Turbulence doesn't add more energy to the shock. If anything, it's taking up energy that could be used to push the material outward, and uses it to move the material in seemingly random directions. \n\n4/9\n\nhttps://t.co/9v9GwuiW1f"", ""Here's an idea: what if turbulence distributes the material in a more efficient way inside the shocked region? Without turbulence we'd get the Sedov Taylor solution, where all the material accumulates close to the shock front. \n\n5/9 https://t.co/XpwaKhccmn"", 'Turbulence acts like diffusion, and moves material from the shock front to the interior of the explosion. Thus, the shock requires less kinetic energy to expand at the same velocity, and the potential energy is also lower.\n\n6/9 https://t.co/DzzS93ErUz', 'Even if this were true, the shock velocity would only change by a factor of a few, not orders of magnitude. Could this be enough to revive the shock? This is where my new study comes in, and the answer is a resounding YES!\n\n7/9 https://t.co/rjE40YvrSl', ""I developed an analytic model for a shock propagating in a collapsing star, and show that there's a critical energy below which the shock stalls and recedes instead of expanding. Moreover, this threshold decreases as the material is distributed more evenly.\n\n8/9 https://t.co/i0F7mgRicW"", 'In this work I control the structure of the shock with the adiabatic index, which also changes the pressure. In a future work I plan to generalise the formalism so I could change the structure without modifying the equation of state, so stay tuned. \n\n9/9 https://t.co/mpAcT5FjTu']",https://arxiv.org/abs/2008.11236,"We develop a generalisation to the classical Sedov Taylor explosion where the medium free falls to a point mass at the centre of the explosion. To verify our analytic results, we compare them to a suite of numerical simulations. We find that there exists a critical energy below which, instead of propagating outward the shock stalls and collapses under gravity. Furthermore, we find that the value of the critical energy threshold decreases when the adiabatic index increases and material is more evenly distributed within the shocked region. We apply this model to the problem of a shock bounce in core collapse supernova, in which the proto neutron star serves as the point mass. The relation between the threshold energy and the distribution of mass in the shock might help explain how turbulence prevents shock stalling and recession in a core collapse supernova explosion. ","Self Similar Adiabatic Strong Explosion in a Medium Gravitationally Free
  Falling to a Point Mass"
28,1298659078173011968,2180565864,Robert Fisher,"['I opened the arXiv mailing this morning to be pleasantly amazed by a new paper by Lidia Oskinova and colleagues on a remarkable supernova remnant, known to its closest friends as IRAS 00500+6713. <LINK> 1/9 <LINK>', 'The very first figure (shown above) blows my mind. You can clearly see both the surrounding nebula as well as the central star itself. Now, there are several supernovae remnants powered by the beating heart of rapidly-rotating neutron star -- most famously the Crab nebula.  2/9 https://t.co/CJmcFOUoVp', ""However, the composition of the central star and nebula are both measured here - they're full of carbon, oxygen, neon, and some other elements like silicon and sulfur. The properties just don't match the Crab or other possible sources, such as rapidly-accreting white dwarfs. 3/9"", 'The authors remarkably conclude that they are seeing an entirely new class of supernova remnant. ""Both the central star and the nebula are clearly detected, heralding the [white dwarf] merger products as a new distinct type of strong X-ray sources."" 4/9 https://t.co/QgTStKKzaA', 'Flashback to a few years ago. @rahulkashyap411 and Tazkera Haque, and collaborators @PLorenAguilar and Enrique García-Berro and I simulated the merger of a massive white dwarf made of oxygen and neon, with a garden variety one of carbon and oxygen. https://t.co/KicnEs1j8O 5/9 https://t.co/skV3JZHqkg', 'Oxygen undergoes nuclear burning under extreme conditions only, so the outcome was a dud - a failed supernova. The predicted luminosity was so low that it seemed unlikely to ever be seen during the initial explosion. However, the white dwarf survives the blast. 6/9 https://t.co/KjnWpJTAzZ', 'The nuclear ash produced in our models includes the magnesium, silicon, and sulfur seen in IRAS 00500+6713. The initial optical transient hundreds of years ago would have been exceedingly faint, but the blast, with about 1% of a typical supernova energy, still packs a punch. 7/9', ""It was a dud in its infancy, but this oddball merger evolved into a bright X-ray source with powerful winds as a supernova remnant. That's something we hadn't considered, and it is an example of the beautiful serendipity that sometimes arises between observations and theory. 8/9"", 'A final coda: This was the last paper I wrote with my late collaborator Enrique García-Berro, who tragically passed away in 2017. We had great fun brainstorming about these oddball white dwarf mergers. He would have been thrilled to have seen them realized in nature. 9/9']",https://arxiv.org/abs/2008.10612,"The merger of two white dwarfs (WD) is a natural outcome from the evolution of many binary stars. Recently, a WD merger product, IRAS 00500+6713, was identified. IRAS 00500+6713 consists of a central star embedded in a circular nebula. The analysis of the optical spectrum of the central star revealed that it is hot, hydrogen and helium free, and drives an extremely fast wind with a record breaking speed. The nebula is visible in infrared and in the [O III] line images. No nebula spectroscopy was obtained prior to our observations. Here we report the first deep X-ray imaging spectroscopic observations of IRAS 00500+6713. Both the central star and the nebula are detected in X-rays, heralding the WD merger products as a new distinct type of strong X-ray sources. Low-resolution X-ray spectra reveal large neon, magnesium, silicon, and sulfur enrichment of the central star and the nebula. We conclude that IRAS 00500+6713 resulted from a merger of an ONe and a CO WD, which supports earlier suggestion for a super-Chandrasekhar mass of this object. X-ray analysis indicates that the merger was associated with an episode of carbon burning and possibly accompanied by a SN Iax. In X-rays, we observe the point source associated with the merger product while the surrounding diffuse nebula is a supernova remnant. IRAS 00500+6713 will likely terminate its evolution with another peculiar Type I supernova, where the final core collapse to a neutron star might be induced by electron captures. ","X-rays observations of a super-Chandrasekhar object reveal an ONeMg and
  a CO white dwarf merger product embedded in a putative SN Iax remnant"
29,1298589325203046400,561899047,Aki Vehtari,"['We (Tuomas Sivula, @MansMeg and I) have another new related paper <LINK> showing that although there is no generally unbiased estimator of CV error variance (as shown by Bengio and Grandvalet (2004)) there can be unbiased estimator for a specific model. <LINK> <LINK>', 'The unbiasedness is not a necessary property, but the example demonstrates that it is possible to derive model specific estimators that could have better calibration and smaller error than the usual naive estimator.']",https://arxiv.org/abs/2008.10859,"When evaluating and comparing models using leave-one-out cross-validation (LOO-CV), the uncertainty of the estimate is typically assessed using the variance of the sampling distribution. Considering the uncertainty is important, as the variability of the estimate can be high in some cases. An important result by Bengio and Grandvalet (2004) states that no general unbiased variance estimator can be constructed, that would apply for any utility or loss measure and any model. We show that it is possible to construct an unbiased estimator considering a specific predictive performance measure and model. We demonstrate an unbiased sampling distribution variance estimator for the Bayesian normal model with fixed model variance using the expected log pointwise predictive density (elpd) utility score. This example demonstrates that it is possible to obtain improved, problem-specific, unbiased estimators for assessing the uncertainty in LOO-CV estimation. ","Unbiased estimator for the variance of the leave-one-out
  cross-validation estimator for a Bayesian normal model with fixed variance"
30,1298565070235074561,387015847,Carlo Campajola,"['Out today on arXiv our new short paper ""On the equivalence between the Kinetic Ising Model and discrete autoregressive processes"", with Fabrizio Lillo, @piero_mazzarisi and Daniele Tantari. Check it out at <LINK> ! <LINK>', 'Our work puts in relation two streams of literature that have been evolving separately for a long time, with applications in machine learning, neuroscience, temporal network models, finance and many other fields.', 'We prove that the Kinetic Ising Model, developed by statistical physicists, and the Discrete AutoRegressive family models, developed in the time series analysis literature, are indeed the same Maximum Entropy model, with a 1-to-1 mapping linking the two.', 'Hopefully this opens up bridges between communities that can benefit from the exchange of ideas and methods to work with these models! Comments and retweets appreciated!']",https://arxiv.org/abs/2008.10666,"Binary random variables are the building blocks used to describe a large variety of systems, from magnetic spins to financial time series and neuron activity. In Statistical Physics the Kinetic Ising Model has been introduced to describe the dynamics of the magnetic moments of a spin lattice, while in time series analysis discrete autoregressive processes have been designed to capture the multivariate dependence structure across binary time series. In this article we provide a rigorous proof of the equivalence between the two models in the range of a unique and invertible map unambiguously linking one model parameters set to the other. Our result finds further justification acknowledging that both models provide maximum entropy distributions of binary time series with given means, auto-correlations, and lagged cross-correlations of order one. We further show that the equivalence between the two models permits to exploit the inference methods originally developed for one model in the inference of the other. ","On the equivalence between the Kinetic Ising Model and discrete
  autoregressive processes"
31,1298560567070916608,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper on arXiv (<LINK>) comparing the behaviour of reverse annealing on higher and lower noise @dwavesys QPUs, and finding that lower noise leads to a longer range character of the search, suggesting this as a useful direction for further improvement', ""Forgot to mention, work performed @jqcDurNew and @DurhamQlm done in collaboration with Viv Kendon (who isn't on twitter)""]",https://arxiv.org/abs/2008.11054,"We construct an Ising Hamiltonian with an engineered energy landscape such that it has a local energy minimum which is near to the true global minimum solution, and further away from a false minimum. Using a technique established in previous experiments, we design our experiment such that (at least on timescales relevant to our study) the false minimum is reached preferentially in forward annealing due to high levels of quantum fluctuations. This allows us to demonstrate the key principle of reverse annealing, that the solution space can be searched locally, preferentially finding nearby solutions, even in the presence of a false minimum. The techniques used here are, to the best of our knowledge, distinct from previously used experimental techniques, and allow us to probe the fundamental search range of the device in a way which has not been previously explored. We perform these experiments on two flux qubit quantum annealers, one with higher noise levels than the other. We find evidence that the lower noise device is more likely to find the more distant energy minimum (the false minimum in this case), suggesting that reducing noise fundamentally increases the range over which flux qubit quantum annealers are able to search. Our work explains why reducing the noise leads to improved performance on these quantum annealers. This supports the idea that these devices may be able to search over broad regions of the solution space quickly, one of the core reasons why quantum annealers are viewed as a potential avenue for a quantum computational advantage. ",Search range in experimental quantum annealing
32,1298539741168545792,197936466,Fuhito Kojima,"['New paper with Yeon-Koo Che, Jinwoo Kim, and Chris Ryan, <LINK>  This is a fun paper, you should check it out!']",https://arxiv.org/abs/2008.10819,We characterize Pareto optimality via sequential utilitarian welfare maximization: a utility vector u is Pareto optimal if and only if there exists a finite sequence of nonnegative (and eventually positive) welfare weights such that $u$ maximizes utilitarian welfare with each successive welfare weights among the previous set of maximizers. The characterization can be further related to maximization of a piecewise-linear concave social welfare function and sequential bargaining among agents a la generalized Nash bargaining. We provide conditions enabling simpler utilitarian characterizations and a version of the second welfare ,"Characterizing Pareto Optima: Sequential Utilitarian Welfare
  Maximization"
33,1298496809011163136,60282797,Dr. Theodosios Chatzistergos,"['Our new paper was accepted for publication in JSWSC and is now in Arxiv!\nWe used high quality CaIIK and continuum observations of the Sun to reconstruct the total solar irradiance (TSI) over the last 24 years! (1/n)\n<LINK>\n<LINK> <LINK>', 'The direct measurements of TSI over that period derive from multiple sources.\nIn contrast, the data we used are consistent and independent covering the entire period in question.\n (2/3)', 'Hence our reconstruction allows us to improve our understanding of the TSI variations over the last 2 solar cycles. \nOur results generally suggest a weak declining trend in TSI over the last 24 years. (3/3)']",https://arxiv.org/abs/2008.10735,"Total solar irradiance (TSI) has been monitored from space since 1978. The measurements show a prominent variability in phase with the solar cycle, as well as fluctuations on timescales shorter than a few days. However, the measurements were done by multiple and usually relatively short-lived missions making the possible long-term trend in the TSI highly uncertain. While the variability in the UV irradiance is clearly in-phase with the solar cycle, the phase of the variability in the visible range has been debated. In this paper, we aim at getting an insight into the long-term trend of TSI since 1996 and the phase of the solar irradiance variations in the visible part of the spectrum. We use independent ground-based full-disc photometric observations in Ca~II~K and continuum from the Rome and San Fernando observatories to compute the TSI since 1996. We follow the empirical San Fernando approach based on the photometric sum index. We find a weak declining trend in the TSI of -7.8$^{+4.9}_{-0.8}\times10^{-3}$ Wm$^{-2}$y$^{-1}$ between the 1996 and 2008 activity minima, while between 2008 and 2019 the reconstructed TSI shows no trend to a marginally decreasing (but statistically insignificant) trend of -0.1$^{+0.25}_{-0.02}\times10^{-3}$ Wm$^{-2}$y$^{-1}$. The reference TSI series used for the reconstruction does not significantly affect the determined trend. The variation in the blue continuum (409.2 nm) is rather flat, while the variation in the red continuum (607.1 nm) is marginally in anti-phase, although this result is extremely sensitive to the accurate assessment of the quiet Sun level in the images. These results provide further insights into the long-term variation of the total solar irradiance. The amplitude of the variations in the visible is below the uncertainties of the processing, which prevents an assessment of the phase of the variations. ",Modelling solar irradiance from ground-based photometric observations
34,1298440354459525121,938827816614944769,Nicolas Parra Avila,"[""New paper in #NaturalLanguageProcessing  on students perception abaut professors, I'm co-author with @vladimirphy, @leonelardilap and @mongosorongo.  <LINK>""]",https://arxiv.org/abs/2008.11183,"Students' perception of classes measured through their opinions on teaching surveys allows to identify deficiencies and problems, both in the environment and in the learning methodologies. The purpose of this paper is to study, through sentiment analysis using natural language processing (NLP) and machine learning (ML) techniques, those opinions in order to identify topics that are relevant for students, as well as predicting the associated sentiment via polarity analysis. As a result, it is implemented, trained and tested two algorithms to predict the associated sentiment as well as the relevant topics of such opinions. The combination of both approaches then becomes useful to identify specific properties of the students' opinions associated with each sentiment label (positive, negative or neutral opinions) and topic. Furthermore, we explore the possibility that students' perception surveys are carried out without closed questions, relying on the information that students can provide through open questions where they express their opinions about their classes. ",Learning from students' perception on professors through opinion mining
35,1298380576911773701,154374002,Prasanna,['Checkout our new paper on evaluating dialogue models with probe tasks! <LINK> !! #NLProc <LINK>'],https://arxiv.org/abs/2008.10427,"Though generative dialogue modeling is widely seen as a language modeling task, the task demands an agent to have a complex natural language understanding of its input text to carry a meaningful interaction with an user. The automatic metrics used evaluate the quality of the generated text as a proxy to the holistic interaction of the agent. Such metrics were earlier shown to not correlate with the human judgement. In this work, we observe that human evaluation of dialogue agents can be inconclusive due to the lack of sufficient information for appropriate evaluation. The automatic metrics are deterministic yet shallow and human evaluation can be relevant yet inconclusive. To bridge this gap in evaluation, we propose designing a set of probing tasks to evaluate dialogue models. The hand-crafted tasks are aimed at quantitatively evaluating a generative dialogue model's understanding beyond the token-level evaluation on the generated text. The probing tasks are deterministic like automatic metrics and requires human judgement in their designing; benefiting from the best of both worlds. With experiments on probe tasks we observe that, unlike RNN based architectures, transformer model may not be learning to comprehend the input text despite its generated text having higher overlap with the target text. ","How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for
  Token-level Evaluation Metrics"
36,1298380109603196928,617213120,Daniel Murfet,"['New paper with Will Troiani revisiting the relationship between sequent calculus, natural deduction and lambda calculus <LINK> <LINK>']",https://arxiv.org/abs/2008.10131,"The Curry-Howard correspondence is often described as relating proofs (in intutionistic natural deduction) to programs (terms in simply-typed lambda calculus). However this narrative is hardly a perfect fit, due to the computational content of cut-elimination and the logical origins of lambda calculus. We revisit Howard's work and interpret it as an isomorphism between a category of proofs in intuitionistic sequent calculus and a category of terms in simply-typed lambda calculus. In our telling of the story the fundamental duality is not between proofs and programs but between local (sequent calculus) and global (lambda calculus or natural deduction) points of view on a common logico-computational mathematical structure. ",Gentzen-Mints-Zucker duality
37,1298250595313184768,561899047,Aki Vehtari,"['Our new paper analyzes one of the limitations of cross-validation\n""Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison"" with Tuomas Sivula and @MansMeg  <LINK> <LINK>', 'Normal distribution has been used to present the uncertainty in CV for a single model (e.g. elpd_loo SE) and in model comparison (e.g. elpd_diff SE), and there have been couple papers discussing the limitations, but it was still unclear when we can trust the model comparison.', 'Tuomas did great work on deriving new finite case and asymptotic results (24p. paper and 64p. appendix with proofs). tl;dr Do model checking before model comparison, uncertainty calibration bad for small differences but then the differences are small anyway, small n is difficult.', 'Conclusions: cross-validation model comparison uncertainty estimates can perform badly when\n 1. the models make very similar predictions,\n 2. the models are misspecified with outliers in the data, and\n 3. the number of observations is small. https://t.co/Np4iF5yqsg', 'Corresponding consequences:\n 1. When the models make similar predictions there is not much difference in the predictive performance, but the bad calibration makes LOO-CV less useful for separating very small effect sizes from zero effect sizes.', '2. The model misspecification in model comparison should be avoided by proper model checking and expansion before using LOO-CV.\n 3. Small differences in the predictive performance can not reliably be detected by LOO-CV if the number of observations is small.', '@lei_zhang_lz If the predictions are very similar model averaging predictions will also be similar to individual model predictions. Model averaging is useful if the models have similar predictive performance, but make different kind of predictions. See https://t.co/KkoRy5wuO3 for more.', '@lei_zhang_lz Instead of ""less sense"" I\'d say for prediction in such cases it\'s less important to decide on one single model unless there is, e.g., costs for making the future measurements for the predictors (think e.g. predicting disease status and bunch of potentially useful medical tests).']",https://arxiv.org/abs/2008.10296,"Leave-one-out cross-validation (LOO-CV) is a popular method for comparing Bayesian models based on their estimated predictive performance on new, unseen, data. As leave-one-out cross-validation is based on finite observed data, there is uncertainty about the expected predictive performance on new data. By modeling this uncertainty when comparing two models, we can compute the probability that one model has a better predictive performance than the other. Modeling this uncertainty well is not trivial, and for example, it is known that the commonly used standard error estimate is often too small. We study the properties of the Bayesian LOO-CV estimator and the related uncertainty estimates when comparing two models. We provide new results of the properties both theoretically in the linear regression case and empirically for multiple different models and discuss the challenges of modeling the uncertainty. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection in the skewness of the individual leave-one-out terms and the distribution of the error of the Bayesian LOO-CV estimator. We show that it is possible that the problematic skewness of the error distribution, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide practical recommendations for the users of Bayesian LOO-CV for model comparison. ","Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model
  Comparison"
38,1298148114033934336,948995274961309697,Andrea Botteon,"['New accepted paper on arXiv today! We reported the firm detection of a giant (2 Mpc) radio bridge connecting the galaxy clusters A1758N and A1758S. The analysis is based on deep observations from 53 MHz to 1.5 GHz (@LOFAR, uGMRT, @TheNRAO JVLA) 1/7\n<LINK> <LINK>', 'This is a follow-up work of our paper of 2018 (https://t.co/2U6d36AXY9) where the bridge was reported only tentatively. A1758N and A1758S are two clusters that are in a ""pre-merger"" phase. They are massive and disturbed, and each one hosts a giant radio halo 2/7', 'With the new data, the bridge is clearly observed at 144 MHz, while only patches of emission are present at 53 and 383 MHz. This makes the determination of its spectral index uncertain. Depending on the procedure adopted, we estimated alpha &lt; 1.8 or alpha = (1.6\\pm0.3) 3/7 https://t.co/2kPt8hPYeO', 'The properties of the radio bridge in A1758 (eg size, radio emissivity, dynamical state of the cluster pair) are very similar to those of that in A399-A401 (Govoni+19, https://t.co/cw7BnLZH4z), the only other intra-cluster radio bridge observed to date 4/7', 'The radio and X-ray emissions of the bridge are correlated, suggesting that thermal and non-thermal components are connected and originate from similar volumes. The turbulence generated during the pre-merger phase may be responsible of the formation of the radio bridge 5/7 https://t.co/uA9ejLiJ5a', 'Intra-custer bridges are among the most giant radio structures observed in the Universe so far and open a new parameter space in the study of non-thermal phenomena in merging clusters, at the crossroads between the denser and hotter ICM and the colder and rarefied cosmic web 6/7', 'Of course this work was done in collaboration with many persons (mainly from @UniLeiden, @IRA_INAF, @mediainaf, @HambObs, @ASTRON_NL), that I would like to thank...some of them are on Twitter: @fradega, @AnnalisBonafede, @roxycas, @fabiogasta, @astro_jit, @mcrossetti_twit 7/7']",https://arxiv.org/abs/2008.09613,"Collisions between galaxy clusters dissipate enormous amounts of energy in the intra-cluster medium (ICM) through turbulence and shocks. In the process, Mpc-scale diffuse synchrotron emission in form of radio halos and relics can form. However, little is known about the very early phase of the collision. We used deep radio observations from 53 MHz to 1.5 GHz to study the pre-merging galaxy clusters A1758N and A1758S that are $\sim2$ Mpc apart. We confirm the presence of a giant bridge of radio emission connecting the two systems that was reported only tentatively in our earlier work. This is the second large-scale radio bridge observed to date in a cluster pair. The bridge is clearly visible in the LOFAR image at 144 MHz and tentatively detected at 53 MHz. Its mean radio emissivity is more than one order of magnitude lower than that of the radio halos in A1758N and A1758S. Interestingly, the radio and X-ray emissions of the bridge are correlated. Our results indicate that non-thermal phenomena in the ICM can be generated also in the region of compressed gas in-between infalling systems. ",A giant radio bridge connecting two clusters in Abell 1758
39,1298136113639497729,719928410814955520,Evgenii Zheltonozhskii,"['Self-supervised learning is really hot now. In our new paper (<LINK>) with @ChaimBaskin Alex Bronstein and Avi Mendelson we study self-supervised learning in unsupervised clustering settings. The code is available at <LINK> 1/n', 'We evaluate multiple self-supervised methods, including BigBiGAN (Jeff Donahue and Karen Simonyan), MoCo v2 (@endernewton  Haoqi Fan, @inkynumbers Kaiming He), InfoMin (@YonglongT et al.), SwAV (Mathilde Caron et al.)... 2/n', '...SimCLRv2 (@tingchenai @skornblith @kswersk @mo_norouzi @geoffreyhinton). We also compare to SCAN (@WGansbeke @svandenh1 @stam_g Marc Proesmans, Luc Van Gool) and Self-label (@y_m_asano @chrirupp Andrea Vedaldi) 3/n', 'We show that self-supervised learning is a strong baseline for unsupervised learning. We achieve 39% accuracy on ImageNet with 1000 clusters and 46% with overclustering (1500 clusters). We also evaluate on ObjectNet, and propose clustering as additional benchmarks for SSL. 4/n', 'Finally we raise a number of questions which we hope will be answered in future research. 5/n (n=5)']",https://arxiv.org/abs/2008.10312,"Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at this https URL ",Self-Supervised Learning for Large-Scale Unsupervised Image Clustering
40,1298117668000604160,1452223452,Kwabena Bediako,"['Our first paper. Excited about Nathanael and Maddie’s work, introducing a new method for mapping strain in twisted bilayer graphene (and other moiré materials). Thank you also to an outstanding team of collaborators at NCEM \u2066@molecularfndry\u2069  <LINK>']",https://arxiv.org/abs/2008.09761,"Van der Waals heteroepitaxy allows deterministic control over lattice mismatch or azimuthal orientation between atomic layers to produce long wavelength superlattices. The resulting electronic phases depend critically on the superlattice periodicity as well as localized structural deformations that introduce disorder and strain. Here, we introduce Bragg interferometry, based on four-dimensional scanning transmission electron microscopy, to capture atomic displacement fields in twisted bilayer graphene with twist angles < 2{\deg}. Nanoscale spatial fluctuations in twist angle and uniaxial heterostrain are statistically evaluated, revealing the prevalence of short-range disorder in this class of materials. By quantitatively mapping strain tensor fields we uncover two distinct regimes of structural relaxation -- in contrast to previous models depicting a single continuous process -- and we disentangle the electronic contributions of the rotation modes that comprise this relaxation. Further, we find that applied heterostrain accumulates anisotropically in saddle point regions to generate distinctive striped shear strain phases. Our results thus establish the reconstruction mechanics underpinning the twist angle dependent electronic behaviour of twisted bilayer graphene, and provide a new framework for directly visualizing structural relaxation, disorder, and strain in any moir\'e material. ",Strain fields in twisted bilayer graphene
41,1298112328529874944,814625838381617154,Rob Harris,"['Our new paper is out, in which we show that holographic codes have thresholds against depolarising errors, using our Gurobi based decoder.\n<LINK>']",http://arxiv.org/abs/2008.10206,"We develop a most likely error Pauli error decoding algorithm for stabiliser codes based on general purpose integer optimisation. Using this decoder we analyse the performance of holographic codes against Pauli errors and find numerical evidence for thresholds against Pauli errors for bulk qubits. We compare the performance of holographic code families of various code rates and find phenomenological Pauli error thresholds ranging from $7\%$ to $16\%$, depending on the code rate. Additionally we give numerical evidence that specific distance measures of the codes we consider scales polynomially with number of physical qubits. ",Decoding Holographic Codes with an Integer Optimisation Decoder
42,1298018355513659392,536866317,Hernan Garcia,"['New paper with Nick Lammers, @yangjoonkim @jiaxi_zhao94!\nTranscriptional bursts occur on times scales of minutes to hours, but transcription factor binding lasts &lt;5s. We review several molecular models that can reconcile these dissimilar timescales.\n<LINK> <LINK>']",http://arxiv.org/abs/2008.09225,"Eukaryotic transcription generally occurs in bursts of activity lasting minutes to hours; however, state-of-the-art measurements have revealed that many of the molecular processes that underlie bursting, such as transcription factor binding to DNA, unfold on timescales of seconds. This temporal disconnect lies at the heart of a broader challenge in physical biology of predicting transcriptional outcomes and cellular decision-making from the dynamics of underlying molecular processes. Here, we review how new dynamical information about the processes underlying transcriptional control can be combined with theoretical models that predict not only averaged transcriptional dynamics, but also their variability, to formulate testable hypotheses about the molecular mechanisms underlying transcriptional bursting and control. ","A matter of time: Using dynamics and theory to uncover mechanisms of
  transcriptional bursting"
43,1297881775318994944,1068545181576773632,Kenneth Brown,"['New paper on arXiv examining schemes for transferring information from alkaline-earth monoxide and monosulfide cations to atomic ions using a coupling between the molecular dipole and the shared motion <LINK>', 'Collaboration led by Eric Hudson and Wes Campbell groups at UCLA based off their proposal https://t.co/bhz7391BEe. \nTheory support for molecular levels from Michael Heaven @EmoryChem .', 'My student @onewiththequbit and postdoc Lu Qi looked at the CaO+ and Ca+ system in detail and the challenge of adiabatic transfer in the presence of heating.  They also co-trapped Ca+ and CaO+, so look forward to experimental results in the future. https://t.co/dwOLpYy1bW']",https://arxiv.org/abs/2008.09201,"Dipole-phonon quantum logic (DPQL) leverages the interaction between polar molecular ions and the motional modes of a trapped-ion Coulomb crystal to provide a potentially scalable route to quantum information science. Here, we study a class of candidate molecular ions for DPQL, the cationic alkaline-earth monoxides and monosulfides, which possess suitable structure for DPQL and can be produced in existing atomic ion experiments with little additional complexity. We present calculations of DPQL operations for one of these molecules, CaO$^+$, and discuss progress towards experimental realization. We also further develop the theory of DPQL to include state preparation and measurement and entanglement of multiple molecular ions. ","Dipole-phonon quantum logic with alkaline-earth monoxide and monosulfide
  cations"
44,1297796893460836352,632724692,Renaud Lambiotte,"['Our new paper exploring the notion of variance on networks is out. As a byproduct, we provide new understanding for what is the boundary of a network. <LINK>']",https://arxiv.org/abs/2008.09155,"We develop a theory to measure the variance and covariance of probability distributions defined on the nodes of a graph, which takes into account the distance between nodes. Our approach generalizes the usual (co)variance to the setting of weighted graphs and retains many of its intuitive and desired properties. Interestingly, we find that a number of famous concepts in graph theory and network science can be reinterpreted in this setting as variances and covariances of particular distributions. As a particular application, we define the maximum variance problem on graphs with respect to the effective resistance distance, and characterize the solutions to this problem both numerically and theoretically. We show how the maximum variance distribution is concentrated on the boundary of the graph, and illustrate this in the case of random geometric graphs. Our theoretical results are supported by a number of experiments on a network of mathematical concepts, where we use the variance and covariance as analytical tools to study the (co-)occurrence of concepts in scientific papers with respect to the (network) relations between these concepts. ",Variance and covariance of distributions on graphs
45,1297757506068860928,296161364,Chris Power,"['New paper on the @arxiv by @ICRAR @ARC_ASTRO3D postdoc, Lilian Garratt-Smithson, using the EAGLE Simulations to study properties of Damped Lyman-alpha Absorbers, with implications for ASKAP FLASH survey of HI absorption - with @cdplagos et al. - <LINK> <LINK>']",https://arxiv.org/abs/2008.09302,"Determining the spatial distribution and intrinsic physical properties of neutral hydrogen on cosmological scales is one of the key goals of next-generation radio surveys. We use the EAGLE galaxy formation simulations to assess the properties of damped Lyman-alpha absorbers (DLAs) that are associated with galaxies and their underlying dark matter haloes between 0 $\leq$ z $\leq$ 2. We find that the covering fraction of DLAs increases at higher redshift; a significant fraction of neutral atomic hydrogen (HI) resides in the outskirts of galaxies with stellar mass greater than or equal to 10$^{10}$ M$_\odot$; and the covering fraction of DLAs in the circumgalactic medium (CGM) is enhanced relative to that of the interstellar medium (ISM) with increasing halo mass. Moreover, we find that the mean density of the HI in galaxies increases with increasing stellar mass, while the DLAs in high- and low-halo-mass systems have higher column densities than those in galaxies with intermediate halo masses (~ 10$^{12}$ M$_\odot$ at z = 0). These high-impact CGM DLAs in high-stellar-mass systems tend to be metal-poor, likely tracing smooth accretion. Overall, our results point to the CGM playing an important role in DLA studies at high redshift (z $\geq$ 1). However, their properties are impacted both by numerical resolution and the detailed feedback prescriptions employed in cosmological simulations, particularly that of AGN. ","The distribution and properties of DLAs at z $\leq$ 2 in the EAGLE
  simulations"
46,1297697429639159808,304399622,Nobuyuki Yoshioka,['Our new paper is out in arXiv! Ground states and quasiparticle band structures in crystalline systems are  tackled by NISQ algorithms.\n<LINK>'],https://arxiv.org/abs/2008.09492,"We present a quantum-classical hybrid algorithm that simulates electronic structures of periodic systems such as ground states and quasiparticle band structures. By extending the unitary coupled cluster (UCC) theory to describe crystals in arbitrary dimensions, for a hydrogen chain, we numerically demonstrate that the UCC ansatz implemented on a quantum circuit can be successfully optimized with a small deviation from the exact diagonalization over the entire range of the potential energy curves. Furthermore, by using the quantum subspace expansion method, in which we truncate the Hilbert space within the linear response regime from the ground state, the quasiparticle band structure is computed as charged excited states. Our work establishes a powerful interface between the rapidly developing quantum technology and modern material science. ",Variational Quantum Simulation for Periodic Materials
47,1297692630097301505,1169068112177745922,Alexis Plascencia,"['New paper out 😀 @fileviez and I have studied electric dipole moments in gauge theories where a dark matter candidate is predicted by the cancellation of gauge anomalies\n<LINK>\n\nA tale of EDMs and dark matter 1/6', 'In BSM theories where baryon and/or lepton number are promoted to local gauge symmetries, we need to introduce new fields to cancel all gauge anomalies  2/6', 'One of these fermions is neutral and automatically stable which makes it a good DM candidate. In addition, these models can naturally accommodate new sources of CP violation!  3/6', 'The new charged states lead to EDMs of the electron and the neutron via two-loop Barr-Zee diagrams. Namely, the charged fermions run in the loop in this diagram:   4/6 https://t.co/B5cnzPqS7c', 'A crucial point is that not overproducing dark matter gives an upper bound on the symmetry breaking scale of the new U(1). Since the new charged fermions get their mass from this scale, this represents an upper bound on their masses.  5/6', 'We computed the EDMs and showed that, for large values of the CP-violating phase, future experiments that search for the EDM of the electron such as ACME will fully probe these theories 😀   6/6 https://t.co/7wLTnrmcON']",https://arxiv.org/abs/2008.09116,"New sources of CP violation beyond the Standard Model are crucial to explain the baryon asymmetry in the Universe. We discuss the impact of new CP violating interactions in theories where a dark matter candidate is predicted by the cancellation of gauge anomalies. In these theories, the constraint on the dark matter relic density implies an upper bound on the new symmetry breaking scale from which all new states acquire their masses. We investigate in detail the predictions for electric dipole moments and show that if the relevant CP-violating phase is large, experiments such as the ACME collaboration will be able to fully probe the theory. ","Electric Dipole Moments, New Forces and Dark Matter"
48,1296897463316393984,60801171,Andrey Kurenkov 🇺🇦,"['New paper alert!\n\nHumans find it easy to push around a heap of objects to find a target object, but it would typically take DRL a LONG time to learn this. We show how RL can be made to do this in just 10k env steps!\n\nArxiv: <LINK>\nWebsite: <LINK> <LINK>', 'We combine some previously known tricks in a ~novel~ way and find they combine well to make RL training super fast. Specifically, we combine:\na) task-relevant mid-level representation \nb) teacher guidance \nc) assymetric learning with privileged info \n\nMore explanation in paper! https://t.co/1kqjGeV7LU', 'With Josepth Taglic, Rohun Kulkarni, Marcus Dominguez-Kuhne, @animesh_garg, @RobobertoMM, @silviocinguetta  at @StanfordSVL @StanfordAILab']",https://arxiv.org/abs/2008.06073,"When searching for objects in cluttered environments, it is often necessary to perform complex interactions in order to move occluding objects out of the way and fully reveal the object of interest and make it graspable. Due to the complexity of the physics involved and the lack of accurate models of the clutter, planning and controlling precise predefined interactions with accurate outcome is extremely hard, when not impossible. In problems where accurate (forward) models are lacking, Deep Reinforcement Learning (RL) has shown to be a viable solution to map observations (e.g. images) to good interactions in the form of close-loop visuomotor policies. However, Deep RL is sample inefficient and fails when applied directly to the problem of unoccluding objects based on images. In this work we present a novel Deep RL procedure that combines i) teacher-aided exploration, ii) a critic with privileged information, and iii) mid-level representations, resulting in sample efficient and effective learning for the problem of uncovering a target object occluded by a heap of unknown objects. Our experiments show that our approach trains faster and converges to more efficient uncovering solutions than baselines and ablations, and that our uncovering policies lead to an average improvement in the graspability of the target object, facilitating downstream retrieval applications. ","Visuomotor Mechanical Search: Learning to Retrieve Target Objects in
  Clutter"
49,1296782494155575297,1541749356,Matt Landreman,['Paper just out with Rogerio Jorge: a new way to parameterize 3D magnetic geometries for turbulence studies. No need for a 3D equilibrium code like VMEC.\xa0<LINK>'],https://arxiv.org/abs/2008.09057,"The design of turbulence optimized stellarators has so far relied on three-dimensional equilibrium codes such as VMEC in order to find the minimum of a given objective function. In this work, we propose a complimentary approach based on the near-axis expansion to compute the geometry parameters of neoclassicaly optimized stellarators used in turbulence studies. As shown here, the near-axis expansion can be a reasonable approximation of the geometric parameters relevant for turbulence and stability simulations of the core of existing optimized stellarator designs. In particular, we examine the geometry coefficients that appear in the gyrokinetic equation, the drift-reduced fluid equations and the ideal ballooning equation. This approach may allow for the development of new stellarator optimization techniques significantly faster than conventional methods. ","The Use of Near-Axis Magnetic Fields for Stellarator Turbulence
  Simulations"
50,1296728099623772162,1064069189143601153,Stephan Rasp,"[""Can data-driven, medium-range weather forecasts beat physical models? In our new paper, we are trying to find out. \n\ntl;dr: Theoretically, yes. Practically, there isn't enough data.\n\nPreprint: <LINK>\nCo-author: @thuereyGroup\n\nThread 👇\n\n1/"", 'Traditionally, medium-range (2d-2w) weather forecasting is done with models based on physics. This works pretty well: https://t.co/l7N3qK96iL\n\nHowever, deep learning has surprised us many times with how good it can be compared to traditional approaches.\n\n2/', 'So naturally people asked the question, based purely on data, how good can neural networks predict weather. \n\nTo find out, we (@PDueben, @wx_jon and S.Scher) designed WeatherBench, a benchmark for data-driven, medium-range weather forecasting. \n\nhttps://t.co/RXrR4wLMVp\n\n3/', 'Here, we train a deep resnet on 40y of ERA5 data but still got overfitting. So we pretrained on 150y of climate model data, all at 5.625 degree resolution (600km).\n\nIn the end, we have similar skill as a physical model run at 210km (IFS T63). Details on comparison in paper.\n\n4/ https://t.co/NRcQCMGWGe', ""Direct models are trained for each forecast time specifically. Continuous models have time as an input (cf. MetNet). \n\nThe third possible approach is to train an iterative model but this is technically much harder, see @wx_jon's latest paper: https://t.co/P4qyBeotAa\n\n5/"", 'Of course, nobody cares about a 600km forecast but the scaling of skill with resolution and network size allows us to (cautiously) extrapolate our results to current operational resolutions (10km).\n\nFindings: Skill improves but more data is probably needed.\n\n6/ https://t.co/zxk2L69MMV', ""It would be very interesting to see how good data-driven models can get when they are run with all the available data at the highest possible resolution (huge technical challenge!!!) but current evidence suggests there won't be enough data to compete with operational models.\n\n7/"", 'As a bonus, we investigate how physical the data-driven models are by looking at saliency maps. \n\nOn average they learn physically plausible correlations but sometimes they also make rather unlikely connections (Hawaii-London in 3d).\n\n8/ https://t.co/mU3UzFjiMx', 'Finally, these results are specific to medium-range forecasting. The potential for AI needs to be assessed for each problem separately:\n\n1) How much data is there?\n2) How good are physical models?\n\nNowcasting with AI e.g. works really well: \n\nhttps://t.co/gSCbm84d6B\n\n9/', ""It seems like the challenge for AI in weather in climate at the moment is finding the right problems. Not every task is suited for a ML approach.\n\nFor a vision on how AI and global NWP can collaborate check out Tim Palmer's essay: https://t.co/tSqQnr2w2q\n\nThe End."", 'Oh yeah. Here is the updated WeatherBench leaderboard on GitHub: https://t.co/hKALQSHfDM https://t.co/9rhHAoUMok', ""@braaannigan Yes, using data more efficiently would certainly be very helpful. Techniques to make the model more physical (eg spherical convolutions) might help. \n\nIt's not as easy though as in MetNet where you can train on small patches. For global NWP you need a wide field of view.""]",http://arxiv.org/abs/2008.08626,"Numerical weather prediction has traditionally been based on physical models of the atmosphere. Recently, however, the rise of deep learning has created increased interest in purely data-driven medium-range weather forecasting with first studies exploring the feasibility of such an approach. To accelerate progress in this area, the WeatherBench benchmark challenge was defined. Here, we train a deep residual convolutional neural network (Resnet) to predict geopotential, temperature and precipitation at 5.625 degree resolution up to 5 days ahead. To avoid overfitting and improve forecast skill, we pretrain the model using historical climate model output before fine-tuning on reanalysis data. The resulting forecasts outperform previous submissions to WeatherBench and are comparable in skill to a physical baseline at similar resolution. We also analyze how the neural network creates its predictions and find that, with some exceptions, it is compatible with physical reasoning. Finally, we perform scaling experiments to estimate the potential skill of data-driven approaches at higher resolutions. ","Data-driven medium-range weather prediction with a Resnet pretrained on
  climate simulations: A new model for WeatherBench"
51,1296617501506207744,1338201043,Koichi Hamaguchi,"['New Paper!\n<LINK>\nWe constructed a new supersymmetric flipped SU(5) GUT model which solves the doublet-triplet splitting problem. It predicts characteristic proton decay modes p-&gt;pi0 mu+ and p-&gt;K0 mu+, which may be tested at Hyper-Kamiokande.']",https://arxiv.org/abs/2008.08940,"We construct a supersymmetric flipped SU(5) grand unified model that possesses an $R$ symmetry. This $R$ symmetry forbids dangerous non-renormalizable operators suppressed by a cut-off scale up to sufficiently large mass dimensions so that the SU(5)-breaking Higgs field develops a vacuum expectation value of the order of the unification scale along the $F$- and $D$-flat directions, with the help of the supersymmetry-breaking effect. The mass terms of the Higgs fields are also forbidden by the $R$ symmetry, with which the doublet-triplet splitting problem is solved with the missing partner mechanism. The masses of right-handed neutrinos are generated by non-renormalizable operators, which then yield a light neutrino mass spectrum and mixing through the seesaw mechanism that are consistent with neutrino oscillation data. This model predicts one of the color-triplet Higgs multiplets to lie at an intermediate scale, and its mass is found to be constrained by proton decay experiments to be $\gtrsim 5 \times 10^{11}$ GeV. If it is $\lesssim 10^{12}$ GeV, future proton decay experiments at Hyper-Kamiokande can test our model in the $p \to \pi^0 \mu^+$ and $p \to K^0 \mu^+$ decay modes, in contrast to ordinary grand unified models where $p \to \pi^0 e^+ $ or $p \to K^+ \bar{\nu}$ is the dominant decay mode. This characteristic prediction for the proton decay branches enables us to distinguish our model from other scenarios. ",R-Symmetric Flipped SU(5)
52,1296599324533628932,1001049754787368960,Dr. Yu-Dai Tsai,['New paper on arXiv now with Robert McGehee &amp; Hitoshi Murayama @sleptogenesis!\n<LINK>\nThanks for this wonderful collaboration!'],https://arxiv.org/abs/2008.08608,"We present models of resonant self-interacting dark matter in a dark sector with QCD, based on analogies to the meson spectra in Standard Model QCD. For dark mesons made of two light quarks, we present a simple model that realizes resonant self-interaction (analogous to the $\phi$-K-K system) and thermal freeze-out. We also consider asymmetric dark matter composed of heavy and light dark quarks to realize a resonant self-interaction (analogous to the $\Upsilon(4S)$-B-B system) and discuss the experimental probes of both setups. Finally, we comment on the possible resonant self-interactions already built into SIMP and ELDER mechanisms while making use of lattice results to determine feasibility. ",Resonant Self-Interacting Dark Matter from Dark QCD
53,1296506144022769666,29997510,Neel Dey,"['It never ceases to surprise just how versatile diffeomorphic registration is.\n\nIn a new #MICCAI OMIA workshop paper with Guillaume Gisbert, we denoise clinical-grade #OCT images where repeat scans with large nonlinear deformations are commonly acquired: <LINK> <LINK>', ""A hybrid registration+unsupervised deep denoising approach outperforms classic unsupervised denoising methods. Of course, the classic methods don't require repeats, so there's the catch. https://t.co/lI79IKlW8X""]",https://arxiv.org/abs/2008.08024,"Optical Coherence Tomography (OCT) is pervasive in both the research and clinical practice of Ophthalmology. However, OCT images are strongly corrupted by noise, limiting their interpretation. Current OCT denoisers leverage assumptions on noise distributions or generate targets for training deep supervised denoisers via averaging of repeat acquisitions. However, recent self-supervised advances allow the training of deep denoising networks using only repeat acquisitions without clean targets as ground truth, reducing the burden of supervised learning. Despite the clear advantages of self-supervised methods, their use is precluded as OCT shows strong structural deformations even between sequential scans of the same subject due to involuntary eye motion. Further, direct nonlinear alignment of repeats induces correlation of the noise between images. In this paper, we propose a joint diffeomorphic template estimation and denoising framework which enables the use of self-supervised denoising for motion deformed repeat acquisitions, without empirically registering their noise realizations. Strong qualitative and quantitative improvements are achieved in denoising OCT images, with generic utility in any imaging modality amenable to multiple exposures. ","Self-supervised Denoising via Diffeomorphic Template Estimation:
  Application to Optical Coherence Tomography"
54,1296501230605406209,14551614,Jason Weston,"['(1/3) New paper! \n\nInstead of a *static* train/valid/test setup, ML systems should become more useful as they interact with people &amp; the world.\n\nAs a step in that direction, we deploy dialogue as a game and show that models improve by talking to humans!\n\n<LINK>', '(2/3)\n\n...and the more models improve, the more humans want to talk to them! Virtuous circle!\n\nIntrinsically motivated players provide a better distbn &amp; collection is more efficient than crowdsourcing. \n\nWe collect ~461k utterances over ~13k players, and release the data.', '(3/3) \n\nwith @kurt_shuster* @JackUrbs*  @em_dinan Arthur Szlam @jaseweston  (* joint first)\n\nBuilt in ParlAI @parlai_parley.\nSee: https://t.co/xVAaHVEwmc']",https://arxiv.org/abs/2008.08076,"Much of NLP research has focused on crowdsourced static datasets and the supervised learning paradigm of training once and then evaluating test performance. As argued in de Vries et al. (2020), crowdsourced data has the issues of lack of naturalness and relevance to real-world use cases, while the static dataset paradigm does not allow for a model to learn from its experiences of using language (Silver et al., 2013). In contrast, one might hope for machine learning systems that become more useful as they interact with people. In this work, we build and deploy a role-playing game, whereby human players converse with learning agents situated in an open-domain fantasy world. We show that by training models on the conversations they have with humans in the game the models progressively improve, as measured by automatic metrics and online engagement scores. This learning is shown to be more efficient than crowdsourced data when applied to conversations with real users, as well as being far cheaper to collect. ",Deploying Lifelong Open-Domain Dialogue Learning
55,1296361039265759234,2778729792,Saquib Sarfraz,['Our new #BMVC2020 paper on multi-spectral pedestrian detection. Interesting practical insights to achieve significant gains over SoTA (5.68 log-avg miss rate on KAIST). Much better at detecting small-scale pedestrians (&lt; 40pix). #DaimlerTSS\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2008.08418,"Multispectral images consisting of aligned visual-optical (VIS) and thermal infrared (IR) image pairs are well-suited for practical applications like autonomous driving or visual surveillance. Such data can be used to increase the performance of pedestrian detection especially for weakly illuminated, small-scaled, or partially occluded instances. The current state-of-the-art is based on variants of Faster R-CNN and thus passes through two stages: a proposal generator network with handcrafted anchor boxes for object localization and a classification network for verifying the object category. In this paper we propose a method for effective and efficient multispectral fusion of the two modalities in an adapted single-stage anchor-free base architecture. We aim at learning pedestrian representations based on object center and scale rather than direct bounding box predictions. In this way, we can both simplify the network architecture and achieve higher detection performance, especially for pedestrians under occlusion or at low object resolution. In addition, we provide a study on well-suited multispectral data augmentation techniques that improve the commonly used augmentations. The results show our method's effectiveness in detecting small-scaled pedestrians. We achieve 5.68% log-average miss rate in comparison to the best current state-of-the-art of 7.49% (25% improvement) on the challenging KAIST Multispectral Pedestrian Detection Benchmark. Code: this https URL ",Anchor-free Small-scale Multispectral Pedestrian Detection
56,1296360577930076160,50901426,Rafael Alves Batista,['New paper by the Pierre Auger Collaboration to appear in PRL.\nWe estimate the energy density in cosmic rays at ultra-high energies.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2008.06488,"We report a measurement of the energy spectrum of cosmic rays above $2.5{\times} 10^{18}$ eV based on $215,030$ events. New results are presented: at about $1.3{\times} 10^{19}$ eV, the spectral index changes from $2.51 \pm 0.03 \textrm{ (stat.)} \pm 0.05 \textrm{ (sys.)}$ to $3.05 \pm 0.05 \textrm{ (stat.)}\pm 0.10\textrm{ (sys.)}$, evolving to $5.1\pm0.3\textrm{ (stat.)} \pm 0.1\textrm{ (sys.)}$ beyond $5{\times} 10^{19}$ eV, while no significant dependence of spectral features on the declination is seen in the accessible range. These features of the spectrum can be reproduced in models with energy-dependent mass composition. The energy density in cosmic rays above $5{\times} 10^{18}$ eV is $(5.66 \pm 0.03 \textrm{ (stat.)} \pm 1.40 \textrm{ (sys.)} ) {\times} 10^{53}~$erg Mpc$^{-3}$. ","Features of the energy spectrum of cosmic rays above $2.5{\times}
  10^{18}$ eV using the Pierre Auger Observatory"
57,1296058358114525185,5850692,Aaron Roth,"['I\'m excited about our new paper ""Moment Multicalibration for Uncertainty Estimation"" with @crispy_jung, @ChanghwaLee3, @malleshpai, and Ricky. <LINK> It gives a way to estimate the uncertainty of predictions that are simultaneously valid over many groups. 1/', 'Marginal prediction intervals (what the ""conformal prediction"" literature aims for) quantify uncertainty -on average- over everyone in the population. But a 95% conformal prediction interval might be completely wrong for people -like you- if your demographics are not typical. 2/', 'A similar problem arises for expectation estimation: the standard performance metric of calibration is an average over the whole population. Hebert-Johnson, @mikekimbackward, Reingold, and Rothblum proposed a way to do better called ""multicalibration"". https://t.co/9zzHfl4oSe 3/', 'Multicalibration asks for calibration not just overall, but over an enormous number of intersecting subgroups. It turns out this is achievable even from a small sample! We show how to do something similar not just for means, but for variances and other higher moments. 4/', 'You can use multicalibrated moment estimates to compute prediction intervals that are valid not just averaged over the whole population, but simultaneously over each subgroup. Its also technically interesting that you can do this, because higher moments are nonlinear. 5/', 'Short blog post here: https://t.co/mSmXDCEZVI 6/6']",https://arxiv.org/abs/2008.08037,"We show how to achieve the notion of ""multicalibration"" from H\'ebert-Johnson et al. [2018] not just for means, but also for variances and other higher moments. Informally, it means that we can find regression functions which, given a data point, can make point predictions not just for the expectation of its label, but for higher moments of its label distribution as well-and those predictions match the true distribution quantities when averaged not just over the population as a whole, but also when averaged over an enormous number of finely defined subgroups. It yields a principled way to estimate the uncertainty of predictions on many different subgroups-and to diagnose potential sources of unfairness in the predictive power of features across subgroups. As an application, we show that our moment estimates can be used to derive marginal prediction intervals that are simultaneously valid as averaged over all of the (sufficiently large) subgroups for which moment multicalibration has been obtained. ",Moment Multicalibration for Uncertainty Estimation
58,1296044140858191874,460069521,Andrew Francis,"['Very stoked with this new little paper, joint with Mike Steel (Canterbury) and Dan Huson (Tübingen): ""Normalising phylogenetic networks”.\n\nWe show how every phylogenetic network has an associated canonical normal network! \n\n<LINK>', '@robynaraujo Oh thanks, what a nice thing to say!']",https://arxiv.org/abs/2008.07797,"Rooted phylogenetic networks provide a way to describe species' relationships when evolution departs from the simple model of a tree. However, networks inferred from genomic data can be highly tangled, making it difficult to discern the main reticulation signals present. In this paper, we describe a natural way to transform any rooted phylogenetic network into a simpler canonical network, which has desirable mathematical and computational properties, and is based only on the 'visible' nodes in the original network. The method has been implemented and we demonstrate its application to some examples. ",Normalising phylogenetic networks
59,1296011300099837952,4599982343,Attila Geresdi,"['Our paper on supercurrent reversal and spin-blockaded supercurrent in double quantum dots is now on arXiv! This work, led by @DanielBouman, shows a new way to couple localized electron spins and superconducting quantum systems.\n<LINK> <LINK>']",https://arxiv.org/abs/2008.04375,"Serial double quantum dots created in semiconductor nanostructures provide a versatile platform for investigating two-electron spin quantum states, which can be tuned by electrostatic gating and an external magnetic field. In this work, we directly measure the supercurrent reversal between adjacent charge states of an InAs nanowire double quantum dot with superconducting leads, in good agreement with theoretical models. In the even charge parity sector, we observe a supercurrent blockade with increasing magnetic field, corresponding to the spin singlet to triplet transition. Our results demonstrate a direct spin to supercurrent conversion, the superconducting equivalent of the Pauli spin blockade. This effect can be exploited in hybrid quantum architectures coupling the quantum states of spin systems and superconducting circuits. ",Triplet-blockaded Josephson supercurrent in double quantum dots
60,1295887475290734592,2337598033,Geraint F. Lewis,"['Paper day! New on @arxiv, a new S5 led by @alexanderpji and with our usual cast including @sazabi_li @astrowizicist @dougalmackey @FadAstra @JossBlandHawtho @_sarahmartell_ @deniserkal @norashipp @Zen_W_ (and more!) <LINK> <LINK>']",https://arxiv.org/abs/2008.07568,"We present high-resolution Magellan/MIKE spectroscopy of 42 red giant stars in seven stellar streams confirmed by the Southern Stellar Stream Spectroscopic Survey (S5): ATLAS, Aliqa Uma, Chenab, Elqui, Indus, Jhelum, and Phoenix. Abundances of 30 elements have been derived from over 10,000 individual line measurements or upper limits using photometric stellar parameters and a standard LTE analysis. This is currently the most extensive set of element abundances for stars in stellar streams. Three streams (ATLAS, Aliqa Uma, and Phoenix) are disrupted metal-poor globular clusters, although only weak evidence is seen for the light element anticorrelations commonly observed in globular clusters. Four streams (Chenab, Elqui, Indus, and Jhelum) are disrupted dwarf galaxies, and their stars display abundance signatures that suggest progenitors with stellar masses ranging from $10^6-10^7 M_\odot$. Extensive description is provided for the analysis methods, including the derivation of a new method for including the effect of stellar parameter correlations on each star's abundance and uncertainty. This paper includes data gathered with the 6.5 meter Magellan Telescopes located at Las Campanas Observatory, Chile. ","The Southern Stellar Stream Spectroscopic Survey (S5): Chemical
  Abundances of Seven Stellar Streams"
61,1295753861936885764,827634277818966021,backyardworlds,"['New paper!! <LINK> Featuring @sasstronaut42 @dancaselden @karmeliet64 @HADL2015 @planetsam1 @GrmzLeo @telmahsel @ken_hinckley @space_r2 @NikolajStevnbak @vthakur303 @AstroMelina P. Beaulieu, F. Kiwy D. Martin, W. Pendrill A. Rothermich  J. Schumann et al.']",https://arxiv.org/abs/2008.06396,"We present Spitzer follow-up imaging of 95 candidate extremely cold brown dwarfs discovered by the Backyard Worlds: Planet 9 citizen science project, which uses visually perceived motion in multi-epoch WISE images to identify previously unrecognized substellar neighbors to the Sun. We measure Spitzer [3.6]-[4.5] color to phototype our brown dwarf candidates, with an emphasis on pinpointing the coldest and closest Y dwarfs within our sample. The combination of WISE and Spitzer astrometry provides quantitative confirmation of the transverse motion of 75 of our discoveries. Nine of our motion-confirmed objects have best-fit linear motions larger than 1""/yr; our fastest-moving discovery is WISEA J155349.96+693355.2 (total motion ~2.15""/yr), a possible T type subdwarf. We also report a newly discovered wide-separation (~400 AU) T8 comoving companion to the white dwarf LSPM J0055+5948 (the fourth such system to be found), plus a candidate late T companion to the white dwarf LSR J0002+6357 at 5.5' projected separation (~8,700 AU if associated). Among our motion-confirmed targets, five have Spitzer colors most consistent with spectral type Y. Four of these five have exceptionally red Spitzer colors suggesting types of Y1 or later, adding considerably to the small sample of known objects in this especially valuable low-temperature regime. Our Y dwarf candidates begin bridging the gap between the bulk of the Y dwarf population and the coldest known brown dwarf. ","Spitzer Follow-up of Extremely Cold Brown Dwarfs Discovered by the
  Backyard Worlds: Planet 9 Citizen Science Project"
62,1295741296133517313,1280516180965457923,John Hopfield,"['Dense Associative Memories, aka modern Hopfield networks, have a huge memory storage capacity. But are they biologically realistic? In our new paper with @DimaKrotov we argue that they can be written in terms of biological variables. <LINK> <LINK>']",https://arxiv.org/abs/2008.06996,"Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ""Hopfield Networks is All You Need"" paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class. ",Large Associative Memory Problem in Neurobiology and Machine Learning
63,1295740398871117825,435271016,Dmitry Krotov,"['New microscopic theory of Dense Associative Memory aka modern Hopfield network can be reduced to the model proposed in “Hopfield Networks is All You Need’’ paper, which is equivalent to self-attention mechanism of Transformers. Work with @HopfieldJohn <LINK> <LINK>', '@SpaceCyborgBoy @HopfieldJohn Thank you!']",https://arxiv.org/abs/2008.06996,"Dense Associative Memories or modern Hopfield networks permit storage and reliable retrieval of an exponentially large (in the dimension of feature space) number of memories. At the same time, their naive implementation is non-biological, since it seemingly requires the existence of many-body synaptic junctions between the neurons. We show that these models are effective descriptions of a more microscopic (written in terms of biological degrees of freedom) theory that has additional (hidden) neurons and only requires two-body interactions between them. For this reason our proposed microscopic theory is a valid model of large associative memory with a degree of biological plausibility. The dynamics of our network and its reduced dimensional equivalent both minimize energy (Lyapunov) functions. When certain dynamical variables (hidden neurons) are integrated out from our microscopic theory, one can recover many of the models that were previously discussed in the literature, e.g. the model presented in ""Hopfield Networks is All You Need"" paper. We also provide an alternative derivation of the energy function and the update rule proposed in the aforementioned paper and clarify the relationships between various models of this class. ",Large Associative Memory Problem in Neurobiology and Machine Learning
64,1295665741493133312,2992109189,Luca Demetrio,"['Welcome RAMEn, a new light-weight formalization for crafting adversarial EXEmples, shipped with two novel powerful attacks: Extend and Shift!\nHappy to share!\nPaper: <LINK>\nCode: <LINK>\n@biggiobattista @DrScottCoull @zxgio @AlessanArmando Roli <LINK>', '@EdwardRaffML @filar @biggiobattista @DrScottCoull @zxgio @AlessanArmando It would be very interesting, for sure! Everything is already coded and it is easy to add Malconv+ to the library.']",https://arxiv.org/abs/2008.07125,"Recent work has shown that adversarial Windows malware samples - referred to as adversarial EXEmples in this paper - can bypass machine learning-based detection relying on static code analysis by perturbing relatively few input bytes. To preserve malicious functionality, previous attacks either add bytes to existing non-functional areas of the file, potentially limiting their effectiveness, or require running computationally-demanding validation steps to discard malware variants that do not correctly execute in sandbox environments. In this work, we overcome these limitations by developing a unifying framework that does not only encompass and generalize previous attacks against machine-learning models, but also includes three novel attacks based on practical, functionality-preserving manipulations to the Windows Portable Executable (PE) file format. These attacks, named Full DOS, Extend and Shift, inject the adversarial payload by respectively manipulating the DOS header, extending it, and shifting the content of the first section. Our experimental results show that these attacks outperform existing ones in both white-box and black-box scenarios, achieving a better trade-off in terms of evasion rate and size of the injected payload, while also enabling evasion of models that have been shown to be robust to previous attacks. To facilitate reproducibility of our findings, we open source our framework and all the corresponding attack implementations as part of the secml-malware Python library. We conclude this work by discussing the limitations of current machine learning-based malware detectors, along with potential mitigation strategies based on embedding domain knowledge coming from subject-matter experts directly into the learning process. ","Adversarial EXEmples: A Survey and Experimental Evaluation of Practical
  Attacks on Machine Learning for Windows Malware Detection"
65,1295653270720061440,261865146,Dr Sofia Qvarfort,['New paper on the arXiv! <LINK> We compute the fundamental sensitivity for measurements of time-dependent gravitational fields with a nonlinear quantum optomechanical system. Applications include measurements of small oscillating masses and gravitational waves. 🌊 <LINK>'],https://arxiv.org/abs/2008.06507,"We study the fundamental sensitivity that can be achieved with an ideal optomechanical system in the nonlinear regime for measurements of time-dependent gravitational fields. Using recently developed methods to solve the dynamics of a nonlinear optomechanical system with a time-dependent Hamiltonian, we compute the quantum Fisher information for linear displacements of the mechanical element due to gravity. We demonstrate that the sensitivity can not only be further enhanced by injecting squeezed states of the cavity field, but also by modulating the light--matter coupling of the optomechanical system. We specifically apply our results to the measurement of gravitational fields from small oscillating masses, where we show that, in principle, the gravitational field of an oscillating nano-gram mass can be detected based on experimental parameters that will likely be accessible in the near-term future. Finally, we identify the experimental parameter regime necessary for gravitational wave detection with a quantum optomechanical sensor. ","Optimal estimation of time-dependent gravitational fields with quantum
  optomechanical systems"
66,1295636132013768704,129492292,Yudai Suwa / 諏訪雄大,"['A new paper has appeared on arXiv. \n<LINK>\nWe derive analytic formulae for the long-time evolution of supernova neutrinos. The most important ones are Eqs 47-49, which also reproduce the numerical models very well. <LINK>', ""Useful for data analysis are Eqs 54 and 55, which give how the neutrino detection rate and the positron's average energy are related to the NS mass and radius. Changing M_det applies to different detectors. https://t.co/cyO0kg5KuX""]",https://arxiv.org/abs/2008.07070,"Neutrinos are a guaranteed signal from supernova explosions in the Milky Way, and a most valuable messenger that can provide us with information about the deepest parts of supernovae. In particular, neutrinos will provide us with physical quantities, such as the radius and mass of protoneutron stars (PNS), which are the central engine of supernovae. This requires a theoretical model that connects observables such as neutrino luminosity and average energy with physical quantities. Here, we show analytic solutions for the neutrino-light curve derived from the neutrino radiation transport equation by employing the diffusion approximation and the analytic density solution of the hydrostatic equation for a PNS. The neutrino luminosity and the average energy as functions of time are explicitly presented, with dependence on PNS mass, radius, the total energy of neutrinos, surface density, and opacity. The analytic solutions provide good representations of the numerical models from a few seconds after the explosion and allow a rough estimate of these physical quantities to be made from observational data. ",Analytic solutions for neutrino-light curves of core-collapse supernovae
67,1295602691046088705,1376287471,Tharindu Jayasinghe,"[""There is a new paper on tonight's astro-ph describing the search for highly variable, cool, luminous stars in the LMC+SMC using @SuperASASSN light curves! Congratulations @AstrOGrady!\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2008.06563,"Stars with unusual properties can provide a wealth of information about rare stages of stellar evolution and exotic physics. However, determining the true nature of peculiar stars is often difficult. In this work, we conduct a systematic search for cool and luminous stars in the Magellanic Clouds with extreme variability, motivated by the properties of the unusual SMC star and Thorne-\.Zytkow Object (T\.ZO) candidate HV2112. Using light curves from ASAS-SN we identify 38 stars with surface temperatures T $<$ 4800K, luminosities $\log$(L/L$_\odot$) $>$ 4.3, variability periods $>$ 400 days, and variability amplitudes $\Delta$V $>$ 2.5 mag. Eleven of these stars possess the distinctive double-peaked light curve morphology of HV2112. We use the pulsation properties and derived occurrence rates for these 12 objects to constrain their nature. From comparisons to stellar populations and models, we find that one star may be a red supergiant with large amplitude pulsations. For the other 11 stars we derive current masses of $\sim$5-10 M$_{\odot}$, below the theoretical minimum mass of $\sim$15 M$_{\odot}$ for T\.ZOs to be stable, casting doubt on this interpretation. Instead, we find that the temperatures, luminosities, mass-loss rates, and periods of these stars are consistent with predictions for super-Asymptotic Giant Branch (s-AGB) stars that have begun carbon burning but have not reached the superwind phase. We infer lifetimes in this phase of $\sim($1$-$7) $\times$ 10$^{4}$ years, also consistent with an s-AGB interpretation. If confirmed, these objects would represent the first identified population of s-AGB stars, illuminating the transition between low- and high-mass stellar evolution. ","Cool, Luminous, and Highly Variable Stars in the Magellanic Clouds from
  ASAS-SN: Implications for Thorne-\.Zytkow Objects and Super-Asymptotic Giant
  Branch Stars"
68,1295304450136051713,1190175298106675200,Jonas Latz,"['Error analysis for probabilities of rare events with approximate models - a new paper in the arXiv (<LINK>) by Fabian Wagner, Iason Papaioannou, Elisabeth Ullmann, and myself. A #thread. #research #Mathematics #numerics (1/7)', 'An important task in, e.g. #structuralengineering and #environmentalengineering, is the estimation of the probability of a system failure, e.g., the probability of groundwater pollution in case a radioactive waste repository is damaged. We call this probability P_f. (2/7)', 'P_f will usually be in the range [1E-9, 1E-6]. Moreover, as in the example mentioned above, P_f often depends on mathematical models, like a #PDE or #ODE. When estimating P_f, e.g. using a sampling or optimisation method, this model needs to be approximated as well. (3/7)', 'In this paper, we are not interested in the accuracy of a sampling method, but in how accurate we need to approximate the underlying mathematical model (ODE or PDE) to get a reasonable approximation of P_f. We denote the probability of failure with approximate model by P_h. (4/7)', 'Past results indicate that the error |P_f - P_h| scales like the approximation error in the mathematical model (Elfverson et al. 2016; https://t.co/z8koIHlEOS). This implies that (up to an unknown constant) the model needs to be approximated super accurately. (5/7)', 'In our paper, we show that actually |P_f - P_h|/P_hFORM behaves like the model approximation error. Here, P_hFORM is the approximation of P_h using the First Order Reliability Method (FORM), an optimisation based approach towards the estimation of rare events. (6/7)', 'This shows that the model only needs to be approximated moderately precisely. Moreover, it gives an immediate algorithmic procedure to estimate the error in P_f. The paper contains a rigorous analysis alongside with numerical experiments. (7/7)']",https://arxiv.org/abs/2008.06368,"The estimation of the probability of rare events is an important task in reliability and risk assessment. We consider failure events that are expressed in terms of a limit-state function, which depends on the solution of a partial differential equation (PDE). In many applications, the PDE cannot be solved analytically. We can only evaluate an approximation of the exact PDE solution. Therefore, the probability of rare events is estimated with respect to an approximation of the limit-state function. This leads to an approximation error in the estimate of the probability of rare events. Indeed, we prove an error bound for the approximation error of the probability of failure, which behaves like the discretization accuracy of the PDE multiplied by an approximation of the probability of failure, the first order reliability method (FORM) estimate. This bound requires convexity of the failure domain. For non-convex failure domains, we prove an error bound for the relative error of the FORM estimate. Hence, we derive a relationship between the required accuracy of the probability of rare events estimate and the PDE discretization level. This relationship can be used to guide practicable reliability analyses and, for instance, multilevel methods. ",Error analysis for probabilities of rare events with approximate models
69,1295303780196651009,120325394,Aswin P Vijayan,"['The Malayalam New Year brings with it a new paper. The second of the FLARES project, presenting the photometric properties of high-redshift galaxies.\n\n<LINK>\n\n#Chingam1 <LINK>']",https://arxiv.org/abs/2008.06057,"We present the photometric properties of galaxies in the First Light and Reionisation Epoch Simulations (FLARES). The simulations trace the evolution of galaxies in a range of overdensities through the Epoch of Reionistion (EoR). With a novel weighting scheme we combine these overdensities, extending significantly the dynamic range of observed composite distribution functions compared to periodic simulation boxes. FLARES predicts a significantly larger number of intrinsically bright galaxies, which can be explained through a simple model linking dust-attenuation to the metal content of the interstellar medium, using a line-of-sight (LOS) extinction model. With this model we present the photometric properties of the FLARES galaxies for $z \in [5,10]$. We show that the ultraviolet (UV) luminosity function (LF) matches the observations at all redshifts. The function is fit by Schechter and double power-law forms, with the latter being favoured at these redshifts by the FLARES composite UV LF. We also present predictions for the UV continuum slope as well as the attenuation in the UV. The impact of environment on the UV LF is also explored, with the brightest galaxies forming in the densest environments. We then present the line luminosity and equivalent widths of some prominent nebular emission lines arising from the galaxies, finding rough agreement with available observations. We also look at the relative contribution of obscured and unobscured star formation, finding comparable contributions at these redshifts. ","First Light And Reionisation Epoch Simulations (FLARES) II: The
  Photometric Properties of High-Redshift Galaxies"
70,1295299001961545729,738769492122214400,Johannes Lischner,"['Twisted double bilayer #graphene consisting of two AB bilayers exhibits flat bands that are tunable by electric fields. In our new paper, we study the atomic and electronic structure of stacked AA bilayers and combinations of AA and AB bilayers: <LINK> <LINK>']",https://arxiv.org/abs/2008.05269,"Twisted double bilayer graphene has recently emerged as an interesting moir\'e material that exhibits strong correlation phenomena that are tunable by an applied electric field. Here we study the atomic and electronic properties of three different graphene double bilayers: double bilayers composed of two AB stacked bilayers (AB/AB), double bilayers composed of two AA stacked bilayers (AA/AA) as well as heterosystems composed of one AB and one AA bilayer (AB/AA). The atomic structure is determined using classical force fields. We find that the inner layers of the double bilayer exhibit significant in-plane and out-of-plane relaxations, similar to twisted bilayer graphene. The relaxations of the outer layers depend on the stacking: atoms in AB bilayers follow the relaxations of the inner layers, while atoms in AA bilayers attempt to avoid higher-energy AA stacking. For the relaxed structures, we calculate the electronic band structures using the tight-binding method. All double bilayers exhibit flat bands at small twist angles, but the shape of the bands depends sensitively on the stacking of the outer layers. To gain further insight, we study the evolution of the band structure as the outer layers are rigidly moved away from the inner layers, while preserving their atomic relaxations. This reveals that the hybridization with the outer layers results in an additional flattening of the inner-layer flat band manifold. Our results establish AA/AA and AB/AA twisted double bilayers as interesting moir\'e materials with different flat band physics compared to the widely studied AB/AB system. ","Effect of bilayer stacking on the atomic and electronic structure of
  twisted double bilayer graphene"
71,1295288097480081409,4032064210,Sesh Nadathur,"['Paper day: Did you see the @eBOSSurvey results and think “Nice, but I wish they had even more data”? Then this is for you.\n\nWe show how to effectively quadruple the data volume of the LRG survey for free – no new observations! – using info from voids.\n\n<LINK> <LINK>', 'One way you can get more cosmological information from the large-scale structure of the Universe is to measure more of it – build a better telescope, survey a larger area, observe more galaxies.\n\nThis is what @desisurvey will do over the next 5 years.', ""A second way is to improve your theory so you can use more small-scale data: this is what the EFT approach to large-scale structure achieves.\n\nA third way is to measure *differently* –\xa0use a new observable in addition to galaxy 2-pt statistics. That's what we do."", 'Voids are regions of space with low matter density, where galaxy motions can be modelled by linear perturbation theory alone.\n\nThese motions cause distortions to the apparent shapes of voids so they no longer appear symmetrical. https://t.co/1plChPFjhs', 'Because we can model this distortion very well, we can use voids as ""standard spheres"": the observed asymmetry in their shapes tells us about cosmological distances, and how fast structure is growing.\n\nThis is information we can\'t get as accurately from galaxy clustering alone! https://t.co/hKjEQQmhhQ', 'As a result, if you take the current state-of-the-art, top-drawer results from the combination of several different BAO and full-shape galaxy clustering techniques ...\n\n... adding voids on top *doubles* the precision we can achieve! https://t.co/oCHkTmr0dy', ""Technically, this is a 55% reduction in the allowed parameter space. As the uncertainty goes roughly as the square root of the survey volume, that's equivalent to analysing a survey 4x as large ... https://t.co/r9PNkdyCam"", ""Since I forgot to add it earlier in the thread, here's a simplified picture of the model we use and the data it is fit to. Have a look at the paper for more details! https://t.co/8ZVWdOaYA1 https://t.co/c2JAHwXF6r""]",https://arxiv.org/abs/2008.06060,"We present an analysis of the anisotropic redshift-space void-galaxy correlation in configuration space using the Sloan Digital Sky Survey extended Baryon Oscillation Spectroscopic Survey (eBOSS) Data Release 16 luminous red galaxy (LRG) sample. This sample consists of LRGs between redshifts 0.6 and 1.0, combined with the high redshift $z>0.6$ tail of the Baryon Oscillation Spectroscopic Survey Data Release 12 CMASS sample. We use a reconstruction method to undo redshift-space distortion (RSD) effects from the galaxy field before applying a watershed void-finding algorithm to remove bias from the void selection. We then perform a joint fit to the multipole moments of the correlation function for the growth rate $f\sigma_8$ and the geometrical distance ratio $D_M/D_H$, finding $f\sigma_8(z_\mathrm{eff})=0.356\pm0.079$ and $D_M/D_H(z_\mathrm{eff})=0.868\pm0.017$ at the effective redshift $z_\mathrm{eff}=0.69$ of the sample. The posterior parameter degeneracies are orthogonal to those from galaxy clustering analyses applied to the same data, and the constraint achieved on $D_M/D_H$ is significantly tighter. In combination with the consensus galaxy BAO and full-shape analyses of the same sample, we obtain $f\sigma_8=0.447\pm0.039$, $D_M/r_d=17.48\pm0.23$ and $D_H/r_d=20.10\pm0.34$. These values are in good agreement with the $\Lambda$CDM model predictions and represent reductions in the uncertainties of $13\%$, $23\%$ and $28\%$ respectively compared to the combined results from galaxy clustering, or an overall reduction of 55\% in the allowed volume of parameter space. ","The completed SDSS-IV extended Baryon Oscillation Spectroscopic Survey:
  geometry and growth from the anisotropic void-galaxy correlation function in
  the luminous red galaxy sample"
72,1295162361104539648,20703003,Peter B Denton,"[""New paper about carefully analyzing neutrino data from @COHERENT_NUS with Julia Gehrlein:\n\nA Statistical Analysis of the COHERENT Data and Applications to New Physics\n\n<LINK>\n\nWe've been looking into handling COHERENT data carefully for the last year... 1/5"", ""...and we realized that most analyses (mine included) take too many liberties with the treatment of the data and statistics. We decided to do things Right.\n\nWe performed the most serious background study that we're aware of and simulate everything that the exp gives us. 2/5"", ""The biggest result we came across was that Wilks' theorem isn't really satisfied (seriously neutrinoers, we need to do better on Wilks' theorem). As an example, we checked the easiest thing (NSIs). 3/5"", ""We found that, depending on binning, assuming Wilks' theorem can either over or under-estimate the size of the true confidence interval. That is, you can't even be sure if assuming Wilks' theorem is optimistic or conservative; it's just wrong (unless you numerically checked). 4/5"", 'We provide, in gory detail, how to simulate the experiment right for future applications to other physics scenarios and release our code and data files to somewhat lessen the burden on people trying to get into this exciting field. 5/5', ""Addendum: There's another paper about the shortcomings of Wilks' theorem in the context of neutrino experiments today by Pilar, Patrick, and Thomas X D.\n\nWe totally planned this. https://t.co/7hRIZsohYT"", '. @pahuberVT', ""@teppeikatori Maybe. The background rate is a factor of a few higher than the signal which doesn't help. There are also some issues with how one should/can combine the background and the signal. Perhaps unbinned would help in certain cases.""]",https://arxiv.org/abs/2008.06062,"The observation of coherent elastic neutrino nucleus scattering (CE$\nu$NS) by the COHERENT collaboration in 2017 has opened a new window to both test Standard Model predictions at relatively low energies and probe new physics scenarios. Our investigations show, however, that a careful treatment of the statistical methods used to analyze the data is essential to derive correct constraints and bounds on new physics parameters. In this manuscript we perform a detailed analysis of the publicly available COHERENT CsI data making use of all available background data. We point out that Wilks' theorem is not fulfilled in general and a calculation of the confidence regions via Monte Carlo simulations following a Feldman-Cousins procedure is necessary. As an example for the necessity of this approach to test new physics scenarios we quantify the allowed ranges for several scenarios with neutrino non-standard interactions. Furthermore, we provide accompanying code to enable an easy implementation of other new physics scenarios as well as data files of our results. ","A Statistical Analysis of the COHERENT Data and Applications to New
  Physics"
73,1294316411700092930,1068607927802621953,Kai Fabi,['New paper on the perceptual uncertainty of neural networks:\n\n<LINK> <LINK>'],https://arxiv.org/abs/2008.01468,"Understanding decisions made by neural networks is key for the deployment of intelligent systems in real world applications. However, the opaque decision making process of these systems is a disadvantage where interpretability is essential. Many feature-based explanation techniques have been introduced over the last few years in the field of machine learning to better understand decisions made by neural networks and have become an important component to verify their reasoning capabilities. However, existing methods do not allow statements to be made about the uncertainty regarding a feature's relevance for the prediction. In this paper, we introduce Monte Carlo Relevance Propagation (MCRP) for feature relevance uncertainty estimation. A simple but powerful method based on Monte Carlo estimation of the feature relevance distribution to compute feature relevance uncertainty scores that allow a deeper understanding of a neural network's perception and reasoning. ","On Feature Relevance Uncertainty: A Monte Carlo Dropout Sampling
  Approach"
74,1294311608227856385,56113666,Mengye Ren,"[""Towards more interactive #selfdriving, we propose a new motion forecasting network based on the transformer architecture to explicitly model interaction among actors. Check out our recent IROS'20 paper, available on arXiv:\n<LINK>\n\n#SelfDrivingCars @uber @uberatg <LINK>"", 'Joint work with Lingyun (Luke) Li, Bin Yang, Ming Liang @zengwenyuan1995  @seanseg  @RaquelUrtasun']",https://arxiv.org/abs/2008.05927,"In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: ATG4D and nuScenes. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors. ","End-to-end Contextual Perception and Prediction with Interaction
  Transformer"
75,1294100692525694983,2416760538,Peter Gao,"['New co-authored paper! Led by Yayaati Chachan, superstar grad student @Caltech, we add another super-puff to the Flat Transmission Spectrum Club [tm]. Along with the super-puffs in the Kepler-51 system, are we beginning to see a trend? <LINK>', 'As a reminder, Xi Zhang and I predicted that some super-puffs may actually be ""normal"" sub-Neptunes with high altitude hazes that make them appear bigger than they really are: https://t.co/H4vifgXB2V \n\nKep-51bd and now Kep-79d fit this idea. What about the others?']",https://arxiv.org/abs/2008.05480,"Extremely low density planets ('super-puffs') are a small but intriguing subset of the transiting planet population. With masses in the super-Earth range ($1-10$ M$_{\oplus}$) and radii akin to those of giant planets ($>4$ R$_{\oplus}$), their large envelopes may have been accreted beyond the water snow line and many appear to be susceptible to catastrophic mass loss. Both the presence of water and the importance of mass loss can be explored using transmission spectroscopy. Here, we present new HST WFC3 spectroscopy and updated Kepler transit depth measurements for the super-puff Kepler-79d. We do not detect any molecular absorption features in the $1.1-1.7$ $\mu$m WFC3 bandpass and the combination of Kepler and WFC3 data are consistent with a flat line model, indicating the presence of aerosols in the atmosphere. We compare the shape of Kepler-79d's transmission spectrum to predictions from a microphysical haze model that incorporates an outward particle flux due to ongoing mass loss. We find that photochemical hazes offer an attractive explanation for the observed properties of super-puffs like Kepler-79d, as they simultaneously render the near-infrared spectrum featureless and reduce the inferred envelope mass loss rate by moving the measured radius (optical depth unity surface during transit) to lower pressures. We revisit the broader question of mass loss rates for super-puffs and find that the age estimates and mass loss rates for the majority of super-puffs can be reconciled if hazes move the photosphere from the typically assumed pressure of $\sim 10$ mbar to $\sim 10 \; \mu$bar. ","A Featureless Infrared Transmission Spectrum for the Super-Puff Planet
  Kepler-79d"
76,1294078695594790912,296161364,Chris Power,"['New paper by our finishing @ICRAR @ARC_ASTRO3D PhD student, Lucie Bakels, on the orbits and interaction histories of a statistical sample of haloes and subhaloes in the LCDM model - well done Lucie! -  with Aaron Ludlow - see <LINK>. <LINK>']",https://arxiv.org/abs/2008.05475,"We use a high-resolution cosmological dark matter-only simulation to study the orbital trajectories of haloes and subhaloes in the environs of isolated hosts. We carefully tally all apsis points and use them to distinguish haloes that are infalling for the first time from those that occupy more evolved orbits. We find that roughly 21 per cent of subhaloes within a host's virial radius are currently on first infall, and have not yet reached their first orbital pericentre; roughly 44 per cent are still approaching their first apocentre after infall. For the range of host masses studied, roughly half of all accreted systems were pre-processed prior to infall, and about 20 per cent were accreted in groups. We confirm that the entire population of accreted subhaloes -- often referred to as ""associated"" subhaloes -- extend far beyond the virial radii of their hosts, with roughly half currently residing at distances that exceed $\approx 1.2\times r_{200}$. Many of these backsplash haloes have gained orbital energy since infall, and occupy extreme orbits that carry them well past their initial turnaround radii. Such extreme orbits are created during the initial accretion and dissolution of loosely bound groups, but also through penetrating encounters between subhaloes on subsequent orbits. The same processes may also give rise to unexpectedly abrupt losses of orbital energy. These effects combine, giving rise to a large variation in the ratio of sequent apocentres for accreted systems. We find that, within 2 virial radii from host centres, the concentrations of first-infall halos are remarkably similar those of isolated field halos, whereas backsplash haloes, as well as systems that were pre-processed, are considerably more concentrated. ","Pre-processing, group accretion and the orbital trajectories of
  associated subhaloes"
77,1293815146318639105,19813240,John Jowett,['Our new paper on overcoming the bound-free pair production limit on Pb-Pb luminosity at #LHC and experiment which used it to quench a superconducting magnet #heavyions #accelerators  \n<LINK>'],https://arxiv.org/abs/2008.05312,"During its Run 2 (2015-2018), the Large Hadron Collider (LHC) operated at almost twice higher energy, and provided Pb-Pb collisions with an order of magnitude higher luminosity, than in the previous Run 1. In consequence, the power of the secondary beams emitted from the interaction points by the bound-free pair production (BFPP) process increased by a factor ~20, while the propensity of the bending magnets to quench increased with the higher magnetic field. This beam power is about 35 times greater than that contained in the luminosity debris from hadronic interactions and is focused on specific locations that fall naturally inside superconducting magnets. The risk of quenching these magnets has long been recognized as severe and there are operational limitations due to the dynamic heat load that must be evacuated by the cryogenic system. High-luminosity operation was nevertheless possible thanks to orbit bumps that were introduced in the dispersion suppressors around the ATLAS and CMS experiments to prevent quenches by displacing and spreading out these beam losses. Further, in 2015, the BFPP beams were manipulated to induce a controlled quench, thus providing the first direct measurement of the steady-state quench level of an LHC dipole magnet. The same experiment demonstrated the need for new collimators that are being installed around the ALICE experiment to intercept the secondary beams in the future. This paper discusses the experience with BFPP at luminosities very close to the future High Luminosity LHC (HL-LHC) target, gives results on the risk reduction by orbit bumps and presents a detailed analysis of the controlled quench experiment. ","Bound-free pair production from nuclear collisions and the steady-state
  quench limit of the main dipole magnets of the CERN Large Hadron Collider"
78,1293744907169837057,1093387119148462081,Daniel Green,"['New paper today with UCSD postdoc Alec Ridgway \n\n<LINK>\n\nOur goal was to have a pen and paper understanding of the linear BAO at the level it is measured in galaxy surveys.  This includes a better understanding of neutrinos and baryon decoupling. <LINK>', 'This paper is the BAO analogue of a beautiful paper by Pan et al (including @lloydeknox)  that computes the locations of every peak in the CMB needed to agree with Planck\n\nhttps://t.co/RtLHLKlFYd']",https://arxiv.org/abs/2008.05026,"The baryon acoustic oscillations (BAO) provide an important bridge between the early universe and the expansion history at late times. While the BAO has primarily been used as a standard ruler, it also encodes recombination era physics, as demonstrated by a recent measurement of the neutrino-induced phase shift in the BAO feature. In principle, these measurements offer a novel window into physics at the time of baryon decoupling. However, our analytic understanding of the BAO feature is limited, particularly for the range of Fourier modes measured in surveys. As a result, it is unclear what the BAO phase teaches us about the early universe beyond what is already known from the cosmic microwave background (CMB). In this paper, we provide a more complete (semi-)analytic treatment of the BAO on observationally relevant scales. In particular, we compute corrections to the frequency and phase of the BAO feature that arise from higher order effects which occur in the tight coupling regime and during baryon decoupling. The total phase shift we find is comparable to a few percent shift in the BAO scale (frequency) and thus relevant in current data. Our results include an improved analytic calculation of the neutrino induced phase shift template that is in close agreement with the numerically determined template used in measurements of the CMB and BAO. ",The Phase of the BAO on Observable Scales
79,1293723577779253248,746440524052082688,Nicolas Delfosse,"['Three reasons to read our new paper with @breic: \n1. All the words of the title start with S.\n2. It contains 10 open questions.\n3. It is full of new techniques for quantum error correction. My favorite: single-shot logical measurements.\n\n#QuantumComputing\n<LINK>', 'This would have been impossible without my fantastic collaborator Ben Reichardt!', 'If you want to learn more about our results, login for my talk at the FTQT workshop tomorrow: https://t.co/7UMaeQU9xw', '@YingkaiOuyang @breic Thanks for the reference! I will have a look at it', ""@BenBrow31417089 Yes I like this result! It could be quite complementary. The advantage of our scheme is that we perform single-shot logical operations on block of multiple logical qubits whereas gauge color code give you only one logical qubit. But don't have non-Clifford yet""]",https://arxiv.org/abs/2008.05051,"We optimize fault-tolerant quantum error correction to reduce the number of syndrome bit measurements. Speeding up error correction will also speed up an encoded quantum computation, and should reduce its effective error rate. We give both code-specific and general methods, using a variety of techniques and in a variety of settings. We design new quantum error-correcting codes specifically for efficient error correction, e.g., allowing single-shot error correction. For codes with multiple logical qubits, we give methods for combining error correction with partial logical measurements. There are tradeoffs in choosing a code and error-correction technique. While to date most work has concentrated on optimizing the syndrome-extraction procedure, we show that there are also substantial benefits to optimizing how the measured syndromes are chosen and used. As an example, we design single-shot measurement sequences for fault-tolerant quantum error correction with the 16-qubit extended Hamming code. Our scheme uses 10 syndrome bit measurements, compared to 40 measurements with the Shor scheme. We design single-shot logical measurements as well: any logical Z measurement can be made together with fault-tolerant error correction using only 11 measurements. For comparison, using the Shor scheme a basic implementation of such a non-destructive logical measurement uses 63 measurements. We also offer ten open problems, the solutions of which could lead to substantial improvements of fault-tolerant error correction. ",Short Shor-style syndrome sequences
80,1293708096477437954,1015053310603284480,Stephen Kane,"['Venus currently has the smallest orbital eccentricity of all Solar System planets, but was that always true? In our new PSJ paper, we explore the effects on Venus of an early high eccentricity and how that could have been triggered by Jupiter. <LINK>', ""@ARedheadOfVenus Well ... you're not wrong!"", '@starsumner Because Jupiter is king, and in that kind of political system he is ultimately responsible for everything that occurs within his kingdom.']",https://arxiv.org/abs/2008.04927,"In the study of planetary habitability and terrestrial atmospheric evolution, the divergence of surface conditions for Venus and Earth remains an area of active research. Among the intrinsic and external influences on the Venusian climate history are orbital changes due to giant planet migration that have both variable incident flux and tidal heating consequences. Here, we present the results of a study that explores the effect of Jupiter's location on the orbital parameters of Venus and subsequent potential water loss scenarios. Our dynamical simulations show that various scenarios of Jovian migration could have resulted in orbital eccentricities for Venus as high as 0.31. We quantify the implications of the increased eccentricity, including tidal energy, surface energy flux, and the variable insolation flux expected from the faint young Sun. The tidal circularization timescale calculations demonstrate that a relatively high tidal dissipation factor is required to reduce the eccentricity of Venus to the present value, which implies a high initial water inventory. We further estimate the consequences of high orbital eccentricity on water loss, and estimate that the water loss rate may have increased by at least $\sim$5\% compared with the circular orbit case as a result of orbital forcing. We argue that these eccentricity variations for the young Venus may have accelerated the atmospheric evolution of Venus towards the inevitable collapse of the atmosphere into a runaway greenhouse state. The presence of giant planets in exoplanetary systems may likewise increase the expected rate of Venus analogs in those systems. ","Could the Migration of Jupiter have Accelerated the Atmospheric
  Evolution of Venus?"
81,1293657106516717568,157014702,Jessie Shelton,"['New paper! ""Cannibal domination and the matter power spectrum"", with Adrienne Erickcek and Pranjal Ralegankar: <LINK>', 'Some theories have number-changing self-interactions that remain efficient even after the particle becomes nonrelativistic.  These theories are called ""cannibals"" because these self-interactions convert rest mass into kinetic energy as the universe expands.', ""This sounds exotic, but that's just because we are used to thinking about the Standard Model, where the lightest particle in the thermal plasma is always relativistic."", 'In a dark sector, the lightest particle might easily be massive - some popular examples of cannibal theories are a confining gauge theory, or just a scalar field.', 'Note we are talking about ""dark sectors"", meaning, particles that don\'t interact directly with us, but the cannibals we\'re interested in here aren\'t dark matter. (They might be part of the dynamics that gave rise to dark matter.)', 'If our universe had a cannibal species, it could easily become the dominant form of matter. This would slow down the expansion of the universe until the cannibal decays, which has to happen before big bang nucleosynthesis.', 'In this paper we studied how this altered expansion history leaves fingerprints on the growth of dark matter perturbations, i.e., the ancestors of present-day dark matter haloes.  Slower expansion means that dark matter perturbations have a chance to grow.', 'But since the cannibal interactions keep the cannibal from cooling down efficiently as the universe expands, its thermal pressure will suppress perturbation growth (technically, modes inside the cannibal Jeans horizon will oscillate rather than grow)', 'The net result of this competition between cannibal thermal pressure and slow expansion is that a period of early cannibal domination gives rise to a characteristic peak in the matter power spectrum.', 'The location and size of this peak is something that we can directly compute from the mass, lifetime, and self-interaction strength of the cannibal species.', 'In turn, the location and size of this peak determine the characteristic clumpiness of dark matter on very small scales', 'So we have new places to look for the fingerprints of dark particle physics in the sky -- and new ideas for pushing our view of the very early universe back to before big bang nucleosynthesis!']",https://arxiv.org/abs/2008.04311,"Decoupled hidden sectors can easily and generically result in a period of cannibal domination, during which the dominant component of the Universe has an equation of state intermediate between radiation and matter due to self-heating by number-changing interactions. We present for the first time the consequences of a cannibal-dominated era prior to big bang nucleosynthesis for structure formation on small scales. We find that an early cannibal-dominated era imprints a characteristic peak on the dark matter power spectrum, with scale and amplitude directly determined by the mass, lifetime, and number-changing interaction strength of the cannibal field. This enhancement to the small-scale matter power spectrum will generate early-forming dark matter microhalos, and we provide a detailed and transparent map between the properties of the cannibal species and the characteristic mass and formation time of these structures. These relations demonstrate how the internal workings of a hidden sector leave a potentially observable imprint on the matter power spectrum even if dark matter has no direct couplings to the Standard Model. ",Cannibal domination and the matter power spectrum
82,1293546169545760769,416560047,Ken Duncan,"['New paper day! Culmination of a long running side project that i\'ve finally been able to finish: \n""The MOSDEF Survey: Calibrating the relationship between Hα star-formation rate and radio continuum luminosity at 1.4&lt;z&lt;2.6"" - <LINK>', 'Combining rest-frame optical spectra of z~2 galaxies from the MOSDEF survey on Keck with some of the deepest radio continuum data available, we explore the redshift evolution of the SFR - radio relation using dust-correct Hα, avoiding the use of confused far-IR measurements. https://t.co/NFLy4P0GXX', ""While the samples are still small, we crucially don't see strong evidence for evolution in the normalisation of SFR(Hα)-radio relation with redshift out to z &gt; 2. And certainly nowhere near the evolution implied by recent studies of the far-IR - radio correlation. https://t.co/yweIaAGpA9"", 'Theres still a lot to be done in linking all the observational studies together to understand the apparent discrepancies. But I think this offers a really nice new window into the problem and is v. encouraging for studying the cosmic star-formation history with LOFAR/MeerKAT.', 'P.S. This was also just a very nice collaboration to have. I approached the MOSDEF team with an idea. They were open, engaging and generous with their data/time. As a result we have what I think is a nice little paper out of it. \nI just wish more collaborations were like this...']",https://arxiv.org/abs/2008.04329,"The observed empirical relation between the star-formation rates (SFR) of low-redshift galaxies and their radio continuum luminosity offers a potential means of measuring SFR in high redshift galaxies that is unaffected by dust obscuration. In this study, we make the first test for redshift evolution in the SFR-radio continuum relation at high redshift using dust-corrected H$\alpha$ SFR. Our sample consists of 178 galaxies from the MOSFIRE Deep Evolution Field (MOSDEF) Survey at $1.4 < z < 2.6$ with rest-frame optical spectroscopy and deep 1.5 GHz radio continuum observations from the Karl G. Jansky Very Large Array (VLA) GOODS North field. Using a stacking analysis we compare the observed radio continuum luminosities with those predicted from the dust-corrected H$\alpha$ SFR assuming a range of $z\sim0$ relations. We find no evidence for a systematic evolution with redshift, when stacking the radio continuum as a function of dust-corrected H$\alpha$ SFR and when stacking both optical spectroscopy and radio continuum as a function of stellar mass. We conclude that locally calibrated relations between SFR and radio continuum luminosity remain valid out to $z\sim 2$. ","The MOSDEF Survey: Calibrating the relationship between H$\alpha$
  star-formation rate and radio continuum luminosity at $1.4 &lt; z &lt; 2.6$"
83,1293520587449602050,3187066938,"Ben K. D. Pearce, PhD","[""We have a brand new paper out on arxiv today, accepted to ApJ!\n\nIt's about hydrogen cyanide (HCN) production in Titan's atmosphere and involves some exciting new quantum chemistry. 😯\n\nAuthors: Pearce, @molaverdikhani, Pudritz, Henning, Hèbrard\n\n<LINK>\n\n1/13"", 'HCN is sort of an ironic molecule. It is TOXIC to life today, yet, it may have been critical to starting life in the first place.\n\n2/13', ""HCN is involved in producing the bases of RNA: the information polymer that many origins of life researchers think made up first life on Earth. \n\nIt's also involved in producing amino acids, which make up proteins.\n\n3/13 https://t.co/aGoo8tf8zW"", ""Titan has got a WHOLE BUNCH of HCN in its atmosphere. \n\n...okay, it's like 1 part per million in the lower atmosphere where it is dense, but that would be a VERY CONCERNING amount for a human to breath in every day.\n\nIn the upper atmosphere it's something like 0.1%!\n\n4/13 https://t.co/nMBhlGgxmf"", ""We are very lucky to have had the Cassini spacecraft measure HCN in Titan's atmosphere with FOUR different instruments. THANK YOU CASSINI! This is an atmospheric modelers dream come true.\n\n5/13 https://t.co/TO1Ia0SvXJ"", 'So what did we do? We modeled HCN production on Titan to try to nail down precisely how it got there! \n\n6/13', 'This was no easy feat, as there have been gaps in the experimental HCN chemical data, preventing simulations from putting together a complete picture.\n\n7/13', 'This is where quantum chemistry comes in (I told you it was coming!)\n\nWe used our pal Schrödinger\'s equation (famous for this and cats) to discover previously unknown chemical reactions. You may recall me tweeting things like ""I DISCOVERED 3 NEW REACTIONS TODAY! AHHH!!""\n\n8/13 https://t.co/Al9eBJcVil', 'We discovered a total of 48 brand new reactions in this work and our previous paper (https://t.co/p8IUVHj0px) (whoa!). With this data, and the available experimental data, we now felt confident that we could narrow in on the pathways to HCN on Titan.\n\n9/13', 'Science is collaborative, and this work is a perfect example of this. We teamed up with Karan Molaverdikhani (@molaverdikhani) and Thomas Henning at MPIA and Eric Hèbrard at Exetor to put the complete model together.\n\n10/13', ""Karan put our new chemisty through his atmospheric chemical kinetic model (ChemKM) using Eric's Titan parameters (such as temperature profile, UV radiation and photochemistry), and voila! We anxiously plotted the output.\n\n11/13"", 'Look how well our main (fiducial) model matches the Cassini data!! Look!\n\nThe purple data point at the top is an actual atmospheric sample put through a mass spectrometer: we were extremely happy and in awe that our model nailed that point.\n\nOUR HCN CHEMISTRY LOOKS GOOD!\n\n12/13 https://t.co/Nz4UAheWam', 'The cool thing about our approach is that we had enough reactions to accurately simulate HCN production on Titan, but few enough to be able to piece together a complete picture of which reactions are important to the HCN story.\n\n13/13', 'It turns out there are 4 main reaction paths to HCN on Titan, and the extremely exciting thing that made me jump in the air and scream at my desk but 6 months ago? ONE OF THE PATHWAYS WAS DISCOVERED BY US!! How cool is that!?!\n\n14/13 https://t.co/mNIiUcphXF', 'For more details on the story of HCN production on Titan, check out the arxiv link at the top! An stay tuned for more exciting work like this! \n\n15/13', ""@AstroEvert Good question, this is something we've thought about a lot.\n\nYou are bang on with your suggestion. \n\nThere is a large spread in upper atmospheric HCN observations and measurements, so we have to consider which upper atmospheric data points are more representative.\n\n1/2"", '@AstroEvert In our view, the mass spec measurement is most representative, given an actual sample was taken.\n\nIt is pretty hard to get a nearly 1% HCN mixing ratio in the upper atmosphere cosidering UV destruction and turbulent (downward mixing). \n\n2/3', '@AstroEvert However, this variation in observed mixing ratio could also be the case of some 3D effects.', '@kevinlacaille @molaverdikhani Thanks budday!']",http://arxiv.org/abs/2008.04312,"Hydrogen cyanide (HCN) is a critical reactive source of nitrogen for building key biomolecules relevant for the origin of life. Still, many HCN reactions remain uncharacterized by experiments and theory, and the complete picture of HCN production in planetary atmospheres is not fully understood. To improve this situation, we develop a novel technique making use of computational quantum chemistry, experimental data, and atmospheric numerical simulations. First, we use quantum chemistry simulations to explore the entire field of possible reactions for a list of primary species in N2-, CH4-, and H2-dominated atmospheres. In this process, we discover 33 new reactions with no previously known rate coefficients. From here, we develop a consistent reduced atmospheric hybrid chemical network (CRAHCN) containing experimental values when available, and our calculated rate coefficients otherwise. Next, we couple CRAHCN to a 1D chemical kinetic model (ChemKM) to compute the HCN abundance as a function of atmospheric depth on Titan. Our simulated atmospheric HCN profile agrees very well with the Cassini observations. CRAHCN contains 104 reactions however nearly all of the simulated atmospheric HCN profile can be obtained using a scaled down network of only 19 dominant reactions. From here, we form a complete picture of HCN chemistry in Titan's atmosphere, from the dissociation of the main atmospheric species, down to the direct production of HCN along 4 major channels. One of these channels was first discovered and characterized in Pearce et al. (2019) and this work. ","HCN production in Titan's Atmosphere: Coupling quantum chemistry and
  disequilibrium atmospheric modeling"
84,1293427131599421440,24628995,Sourabh Nampalliwar,"[""New paper alert! <LINK> with colleagues at @uni_tue. We started with the question: how weird can black holes get? and, can we tell them apart from our regular black holes? It's like a stress-test for observables!"", 'As candidates for black-holes-but-stranger, we derived solutions in a specific f(R) theory, studied in detail by Clifton &amp; Barrow (https://t.co/sK5dd6rXnP), that have toroidal horizons! Imagine regular black holes as beach balls, these stranger things are like lifebuoys! https://t.co/mqyjuqJN8N', '(In reality, the stranger thing possesses an extra shell-like singularity between the event horizon and the central singularity) Can something like this exist in our universe?', 'To see if we can tell them apart, we analyze a few different ways one ""sees"" black holes. We use the iron-line (relativistically broadened spectral emission line), the black hole shadow (apparent boundary) and the image (radiation from infalling gas).', 'In each case, we compare the observable for regular and toroidal black holes. The iron-line and the image emerged as excellent observables and were modified so much for the toroidal objects that there was no ambiguity left: such things are highly unlikely to exist in nature.', 'The shadow case was surprising: the horizon topology had absolutely NO effect on the shadow! Since there is some debate if the shadow was really observed with the Event Horizon Telescope, this study, though theoretical, made me rethink my intuition about shadows and images!', ""We later worked out the reasons why the shadow did not change, and we do not expect this to be a generic feature. But it's cool to accidentally discover interesting features!"", 'One of the aspects I had a lot of fun with during this project: pretty pictures! https://t.co/yqsjieFPEu']",https://arxiv.org/abs/2008.04066,"In general relativity without a cosmological constant, a classical theorem due to Hawking states that stationary black holes must be topologically spherical. This result is one of the several ingredients that collectively imply the uniqueness of the Kerr metric. If, however, general relativity describes gravity inexactly at high energies or over cosmological scales, Hawking's result may not apply, and black holes with non-trivial topology may be, at least mathematically, permissible. While tests involving electromagnetic and gravitational-wave data have been used to place tight constraints on various theoretical departures from a Kerr description of astrophysical black holes, relatively little attention has been paid to topological alternatives. In this paper, we derive a new exact solution in an $f(R)$ theory of gravity which admits topologically non-trivial black holes, and calculate observables like fluorescent K$\alpha$ iron-line profiles and black hole images from hypothetical astrophysical systems which house these objects, to provide a theoretical basis for new tests of black hole nature. On the basis of qualitative comparisons, we show that topologically non-trivial objects would leave a strong imprint on electromagnetic observables and can be easily distinguished from general-relativistic black holes in nearly all cases. ",Testing horizon topology with electromagnetic observations
85,1293367304336662529,3433220662,Anthony Bonato,"['New graph searching paper on the localization number and metric dimension of graphs of diameter 2. We found bounds for Kneser graphs, Moore graphs, and polarity graphs. Exact value for the metric dimension of Kneser graphs in infinitely many cases.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2008.04896,"We consider the localization number and metric dimension of certain graphs of diameter $2$, focusing on families of Kneser graphs and graphs without 4-cycles. For the Kneser graphs with diameter $2$, we find upper and lower bounds for the localization number and metric dimension, and in many cases these parameters differ only by an additive constant. Our results on the metric dimension of Kneser graphs improve on earlier ones, yielding exact values in infinitely many cases. We determine bounds on the localization number and metric dimension of Moore graphs of diameter $2$ and polarity graphs. ",The localization number and metric dimension of graphs of diameter 2
86,1293335487483006976,1032007830386012160,Paul Dalba,"['New long-period, transiting exoplanet paper announcement from this week:\xa0<LINK>. And this one involves a #Kepler system! #AlwaysMoreToFindFromKepler. Check it out:', ""Back in 2010, Kepler spotted a transit event for the star KIC 5951458. It only happened once in Kepler's entire (4 yr!!) primary mission. It looked like it could be planetary and a few papers validated the existence a transiting exoplanet (Kepler-456b). https://t.co/wEY8fb9dPz"", 'Based on the single transit, this was thought to be a super long-period giant transiting planet, with P&gt;1000 days (with understandably large error bars). These are exactly the kinds of planets I love, so I started getting RVs with HIRES at @keckobservatory.', ""My colleagues and I quickly noticed a HUGE linear trend in the RVs, which made us think this was actually a grazing EB. C'est la via, right? Maybe not! Subtracting the trend revealed another signal, possibly from a giant planet (note the RV units). https://t.co/Al9mZwSmJj"", 'We employed The Joker to investigate, which model RVs in cases with sparse data. Separate Joker runs exploring the separate signals suggested that either the possible giant planet or the stellar companion could have caused the event Kepler observed. #TransitWhodunnit', 'In either case though, Kepler data, Keck data, and even a Gaia RV data point limit the properties of whichever companion caused the single ""transit."" At the end, the guilty culprit is left as an unsolved mystery, but we can solve this efficiently if we just wait a few years.', 'This paper also includes the development of a new method of processing Keck-HIRES RVs using an already existing template. This is super useful for faint stars (#Kepler), which need &gt;=1 hr of time for a template. It yields RVs with precision 4-8 m/s for most types of stars!', 'Thanks so much to my colleagues who helped to make this work possible!! @ExoCytherean @awhoward BJ Fulton and Howard Isaacson!']",https://arxiv.org/abs/2008.02811,"Planetary systems that show single-transit events are a critical pathway to increasing the yield of long-period exoplanets from transit surveys. From the primary Kepler mission, KIC 5951458b (Kepler-456b) was thought to be a single-transit giant planet with an orbital period of 1310 days. However, radial velocity (RV) observations of KIC 5951458 from the HIRES instrument on the Keck telescope suggest that the system is far more complicated. To extract precise RVs for this $V\approx13$ star, we develop a novel matched-template technique that takes advantage of a broad library of template spectra acquired with HIRES. We validate this technique and measure its noise floor to be 4 - 8 m s$^{-1}$ (in addition to internal RV error) for most stars that would be targeted for precision RVs. For KIC 5951458, we detect a long-term RV trend that suggests the existence of a stellar companion with an orbital period greater than a few thousand days. We also detect an additional signal in the RVs that is possibly caused by a planetary or brown dwarf companion with mass in the range of 0.6 - 82 $M_{\rm J}$ and orbital period below a few thousand days. Curiously, from just the data on hand, it is not possible to determine which object caused the single ""transit"" event. We demonstrate how a modest set of RVs allows us to update the properties of this unusual system and predict the optimal timing for future observations. ","Multiple Explanations for the Single Transit of KIC 5951458 based on
  Radial Velocity Measurements Extracted with a Novel Matched-template
  Technique"
87,1293295565111881728,1008944276431036416,Boris Ivanovic,"['New paper up on arXiv!! In it, Karen Leung, Ed Schmerling, @MarcoPavoneSU, and I give an accessible intro to our CVAE-based approach for multimodal modeling of human trajectories. Perfect for those new to the field or looking for a refresher! Read it here: <LINK>']",https://arxiv.org/abs/2008.03880,"Human behavior prediction models enable robots to anticipate how humans may react to their actions, and hence are instrumental to devising safe and proactive robot planning algorithms. However, modeling complex interaction dynamics and capturing the possibility of many possible outcomes in such interactive settings is very challenging, which has recently prompted the study of several different approaches. In this work, we provide a self-contained tutorial on a conditional variational autoencoder (CVAE) approach to human behavior prediction which, at its core, can produce a multimodal probability distribution over future human trajectories conditioned on past interactions and candidate robot future actions. Specifically, the goals of this tutorial paper are to review and build a taxonomy of state-of-the-art methods in human behavior prediction, from physics-based to purely data-driven methods, provide a rigorous yet easily accessible description of a data-driven, CVAE-based approach, highlight important design characteristics that make this an attractive model to use in the context of model-based planning for human-robot interactions, and provide important design considerations when using this class of models. ","Multimodal Deep Generative Models for Trajectory Prediction: A
  Conditional Variational Autoencoder Approach"
88,1293220610370404352,3423739275,Felix Leditzky,"['New paper on ""Efficient and low-backaction quantum measurement using a chip-scale detector"" in a collaboration between @JILAscience @CUBoulder @NIST @iqoqi @uniinnsbruck @Yale. Fun collaboration and completely new territory for me! <LINK>', ""Also, this is the first time I'm on a paper with people I haven't actually met yet... Experimental physics is crazy! \n\nBut shout-out to the ones I do know: @quantum_graeme, @HarkNerdLento, Eric, Christian, Maxime, and Ziyi.""]",https://arxiv.org/abs/2008.03805,"Superconducting qubits are a leading platform for scalable quantum computing and quantum error correction. One feature of this platform is the ability to perform projective measurements orders of magnitude more quickly than qubit decoherence times. Such measurements are enabled by the use of quantum-limited parametric amplifiers in conjunction with ferrite circulators - magnetic devices which provide isolation from noise and decoherence due to amplifier backaction. Because these non-reciprocal elements have limited performance and are not easily integrated on-chip, it has been a longstanding goal to replace them with a scalable alternative. Here, we demonstrate a solution to this problem by using a superconducting switch to control the coupling between a qubit and amplifier. Doing so, we measure a transmon qubit using a single, chip-scale device to provide both parametric amplification and isolation from the bulk of amplifier backaction. This measurement is also fast, high fidelity, and has 70% efficiency, comparable to the best that has been reported in any superconducting qubit measurement. As such, this work constitutes a high-quality platform for the scalable measurement of superconducting qubits. ","Efficient and Low-Backaction Quantum Measurement Using a Chip-Scale
  Detector"
89,1293167682930593792,1140286863388893185,Egbert Rijke,"['A new paper by @dan2christensen and me is out on the ArXiv!\n\nCharacterizations of modalities and lex modalities\n\n<LINK>', '@SchreiberUrs @dan2christensen Ah thanks for pointing that out. We will correct it']",https://arxiv.org/abs/2008.03538,"A reflective subuniverse in homotopy type theory is an internal version of the notion of a localization in topology or in the theory of $\infty$-categories. Working in homotopy type theory, we give new characterizations of the following conditions on a reflective subuniverse $L$: (1) the associated subuniverse $L'$ of $L$-separated types is a modality; (2) $L$ is a modality; (3) $L$ is a lex modality; and (4) $L$ is a cotopological modality. In each case, we give several necessary and sufficient conditions. Our characterizations involve various families of maps associated to $L$, such as the $L$-\'etale maps, the $L$-equivalences, the $L$-local maps, the $L$-connected maps, the unit maps $\eta_X$, and their left and/or right orthogonal complements. More generally, our main theorem gives an overview of how all of these classes related to each other. We also give examples that show that all of the inclusions we describe between these classes of maps can be strict. ",Characterizations of modalities and lex modalities
90,1293087703517376512,929624636098273280,Max Maass,"['Today at #WOOT20, @_kleest will be presenting our new paper on NFCGate, our newly upgraded #NFC security auditing tool. \nPaper: <LINK>\nPresentation: <LINK>\nCode: <LINK>\nAs always, a short thread explaining the core contributions:', ""First of all: This was joint work with Alexandros Roussos, me and our PI, Matthias Hollick, from @seemoolab, but @_kleest and Alexandros did 90% of the work - I'm just relaying the results here :). So, on to the topic."", ""NFC is notoriously hard to audit. It's a short-range communication technology, in most everyday settings used with centimeters or less between the two communication endpoints. Your contactless credit card payments use NFC, for example."", 'This makes it really hard to ""get in the middle"" of the communication and eavesdrop. You can do it with big antennas or specialized hardware like the Chameleon (https://t.co/GpchY3F8kx), but that is hardly an easy setup, and costs a bunch of money for a special-purpose device.', 'A few years ago, Uwe Müller, Tom Schons, Daniel Wegemer and I developed the first prototype version of NFCGate. The idea was to have an Android app that allows you to use the device you probably already have in your pocket to intercept NFC communication.', 'The idea is simple: Get two Android phones, hold one to the card, one to the reader, and have them relay the communication. The practical considerations are a lot more complex, as you need to clone some information from the card, like the card ID, and emulate them. https://t.co/xOJlnYl3YV', 'Of course, this will introduce additional latency that can be detected, but a shocking number of NFC systems do not actually defend against such an attack (and an even more shocking number simply relies on the supposedly-unique identifier of a card, which can be easily cloned).', 'Back then, we wrote the results up as a demo paper for ACM WiSec (https://t.co/wEMGoyqNCE), if you are interested. However, the system was very much a research prototype, did not look good, and did not run on up-to-date Android versions.', 'This is where @_kleest and Alexandros come in. They extended and improved the codebase significantly, taking it from a prototype to a fairly polished, feature-rich security research application that can relay and replay communication, save pcap files, clone cards, etc.', 'We then used the new application to audit a commercial NFC-based door lock by a major vendor. This audit revealed flaws in their implementation of the DESFire protocol that allowed replay attacks (which should be prevented by the protocol, if it was implemented correctly).', 'We also found additional issues with the system, all of which were disclosed to the vendor long before publication, and are currently being adressed. (It was a really smooth process, really enjoyed working with this vendor).', 'So, if you want to know more, you can find the paper at https://t.co/HRwSQRbocW, watch the presentation at https://t.co/oChKSRAMfl, or simply download our App from the GitHub page: https://t.co/pG02AkgvEa']",https://arxiv.org/abs/2008.03913,"Near-Field Communication (NFC) is being used in a variety of security-critical applications, from access control to payment systems. However, NFC protocol analysis typically requires expensive or conspicuous dedicated hardware, or is severely limited on smartphones. In 2015, the NFCGate proof of concept aimed at solving this issue by providing capabilities for NFC analysis employing off-the-shelf Android smartphones. In this paper, we present an extended and improved NFC toolkit based on the functionally limited original open-source codebase. With in-flight traffic analysis and modification, relay, and replay features this toolkit turns an off-the-shelf smartphone into a powerful NFC research tool. To support the development of countermeasures against relay attacks, we investigate the latency incurred by NFCGate in different configurations. Our newly implemented features and improvements enable the case study of an award-winning, enterprise-level NFC lock from a well-known European lock vendor, which would otherwise require dedicated hardware. The analysis of the lock reveals several security issues, which were disclosed to the vendor. ","NFCGate: Opening the Door for NFC Security Research with a
  Smartphone-Based Toolkit"
91,1293007988068777984,1338201043,Koichi Hamaguchi,"['New Paper! \nSupernova-scope for the Direct Search of Supernova Axions\n<LINK>\nIf a nearby supernova (SN) occurs within a few 100 pc from the Earth, such as the case for the Betelgeuse, a huge number of axions (in addition to neutrinos) may arrive at the Earth. We', 'studied the prospect of detecting those SN axions by an axion helioscope (such as IAXO) equipped with a gamma-ray detector, with the help of pre-SN neutrino alert.']",http://arxiv.org/abs/2008.03924,"If a supernova explosion occurs within a few hundred parsecs from the Earth, a huge number of axions, in addition to neutrinos, may arrive at the Earth. In this paper, we discuss in detail the prospect of detecting those supernova axions by an axion helioscope. With the help of a pre-supernova neutrino alert system, it is possible to point a helioscope at an exploding supernova in advance. The supernova axions can then be detected by a gamma-ray detector installed at the end of the helioscope. We call such a detection system an axion supernova-scope (SNscope). We propose a conceptual design for an axion SNscope, where the gamma-ray detector is installed at the opposite end to the X-ray detector for the solar axion. It still functions as an axion helioscope during the normal operation time, and once a pre-SN neutrino alert is received, the scope is temporarily turned around and targeted to a SN candidate, waiting for the supernova axions. We estimate the sensitivity of supernova axion detection and find that SNscopes based on the next-generation axion helioscopes, such as IAXO, have potential to explore the invisible axions and to test the axion interpretation of stellar cooling anomalies. ",Supernova-scope for the Direct Search of Supernova Axions
92,1292823941963288576,742027674403672064,Johnny Greco,"['🚨PAPER ALERT🚨\n\nLed by the amazing @PU_Astro *undergrad* Jean Somalwar, our new paper is all about hunting globular clusters in #LowSurfaceBrightness galaxies from the #HyperSuprimeCam survey. Jean has become the lead of our ongoing HSC Glob Hunt (quack!) <LINK> <LINK>', 'With twitter friends @dr_guangtou, @rareflwr41, and @lachlancaster, we take advantage of the high resolution of @NASAHubble to search for point sources in ultra-diffuse galaxies in two galaxy groups. https://t.co/WTr4oQGsmR https://t.co/1VAWcj0TxU', 'Most of our targets appear to have ""normal"" globular cluster (GC) populations for their stellar mass, but two have more GCs than expected. Larger GC pops in diffuse galaxies seem to be associated with denser galaxy environments (S_N = # GC per galaxy luminosity in funny units): https://t.co/6x91Eep6QV']",https://arxiv.org/abs/2008.02806,"We increase the sample of ultra diffuse galaxies (UDGs) in lower density environments with characterized globular cluster (GC) populations using new Hubble Space Telescope observations of nine UDGs in group environments. While the bulk of our UDGs have GC abundances consistent with normal dwarf galaxies, two of these UDGs have excess GC populations. These two UDGs both have GC luminosity functions consistent with higher surface brightness galaxies and cluster UDGs. We then combine our nine objects with previous studies to create a catalog of UDGs with analyzed GC populations that spans a uniquely diverse range of environments. We use this catalog to examine broader trends in the GC populations of low stellar mass galaxies. The highest GC abundances are found in cluster UDGs, but whether cluster UDGs are actually more extreme requires study of many more UDGs in groups. We find a possible positive correlation between GC abundance and stellar mass, and between GC abundance and galaxy size at fixed stellar mass. However, we see no significant stellar-mass galaxy-size relation, over our limited stellar mass range. We consider possible origins of the correlation between GC abundance and galaxy size, including the possibility that these two galaxy properties are both dependent on the galaxy dark matter halo, or that they are related through baryonic processes like internal feedback. ","Hyper Suprime-Cam Low Surface Brightness Galaxies II: A Hubble Space
  Telescope Study of the Globular Cluster Systems of Ultra-Diffuse Galaxies in
  Groups"
93,1292794134881865728,211818431,nermin samet,['Our #BMVC2020 paper is now on arXiv!\nWe introduce PPDet as a new labeling strategy for anchor-free object detection based on pooling predictions. PPDet outperforms all major SOTA methods in small object detection.\nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2008.01167,"Current anchor-free object detectors label all the features that spatially fall inside a predefined central region of a ground-truth box as positive. This approach causes label noise during training, since some of these positively labeled features may be on the background or an occluder object, or they are simply not discriminative features. In this paper, we propose a new labeling strategy aimed to reduce the label noise in anchor-free detectors. We sum-pool predictions stemming from individual features into a single prediction. This allows the model to reduce the contributions of non-discriminatory features during training. We develop a new one-stage, anchor-free object detector, PPDet, to employ this labeling strategy during training and a similar prediction pooling method during inference. On the COCO dataset, PPDet achieves the best performance among anchor-free top-down detectors and performs on-par with the other state-of-the-art methods. It also outperforms all major one-stage and two-stage methods in small object detection (${AP}_{S}$ $31.4$). Code is available at this https URL ",Reducing Label Noise in Anchor-Free Object Detection
94,1292687263613231105,1113856096119197699,Lucas Lamata,"['New paper today! Extracting a magnetic field topology from time-averaged quantities, and a possible implementation with trapped ions! A rewarding collaboration with Jesús Casado-Pascual (Sevilla) and Andrés Reynoso (Bariloche)\n<LINK> @unisevilla @fisicaUS <LINK>']",https://arxiv.org/abs/2008.03078,"We focus on quantum systems that can be effectively described as a localized spin-$s$ particle subject to a static magnetic field coplanar to a coexisting elliptically rotating time-periodic field. Depending on the values taken on by the static and rotating components, the total magnetic field shows two regimes with different topological properties. Along the boundary that separates these two regimes, the total magnetic field vanishes periodically in time and the system dynamics becomes highly nonadiabatic. We derive a relation between two time-averaged quantities of the system which is linked to the topology of the applied magnetic field. Based on this finding, we propose a measurable quantity that has the ability to indicate the topology of the total magnetic field without knowing a priori the value of the static component. We also propose a possible implementation of our approach by a trapped-ion quantum system. The results presented here are independent of the initial state of the system. In particular, when the system is initialized in a Floquet state, we find some interesting properties of the quasienergy spectrum which are linked to the topological change of the total magnetic field. Throughout the paper, the theoretical results are illustrated with numerical simulations for the case of a two-level quantum system. ","Spin dynamics under the influence of elliptically rotating fields:
  Extracting the field topology from time-averaged quantities"
95,1292149536408907777,956622601,Piotr Piecuch,"['We had a great day yesterday. First, our paper about a new ReaxFF for the Li-O material was accepted by JCP. A few hrs later, @JanusEriksen submitted our work <LINK> aimed at determining the FCI ground-state energy of the benzene/cc-pVDZ system to @ACSCentSci.1/2', ""We hope that the results reported in https://t.co/oOOGOq7NEF will provide our community with valuable information. I'd like to thank our collaborators. Among them are 2 current members of my group Ilias Magoulas and Jun Shen and our recent alumnus @edeustuas. Thank you guys! 2/2""]",https://arxiv.org/abs/2008.02678,"We report on the findings of a blind challenge devoted to determining the frozen-core, full configuration interaction (FCI) ground state energy of the benzene molecule in a standard correlation-consistent basis set of double-$\zeta$ quality. As a broad international endeavour, our suite of wave function-based correlation methods collectively represents a diverse view of the high-accuracy repertoire offered by modern electronic structure theory. In our assessment, the evaluated high-level methods are all found to qualitatively agree on a final correlation energy, with most methods yielding an estimate of the FCI value around $-863$ m$E_{\text{H}}$. However, we find the root-mean-square deviation of the energies from the studied methods to be considerable (1.3 m$E_{\text{H}}$), which in light of the acclaimed performance of each of the methods for smaller molecular systems clearly displays the challenges faced in extending reliable, near-exact correlation methods to larger systems. While the discrepancies exposed by our study thus emphasize the fact that the current state-of-the-art approaches leave room for improvement, we still expect the present assessment to provide a valuable community resource for benchmark and calibration purposes going forward. ",The Ground State Electronic Energy of Benzene
96,1291931693276782592,234398193,Hal Tasaki,"['Hohenberg-Mermin-Wagner type theorems for equilibrium models of flocking / Hal Tasaki\nMy new paper, motivated by active matter physics, but on inactive matters.  The result clearly shows that you need something active to observe SSB for flocking.\n<LINK>', 'There is an accompanying 18.5 mins video (in fact a cheap webinar quality video) in which I discuss the background, motivation, and the main results of the paper.\nhttps://t.co/LmLQchmKCY']",https://arxiv.org/abs/2008.02698,"We study a class of two-dimensional models of classical hard-core particles with Vicsek-type ""exchange interaction"" that aligns the directions of motion of nearby particles. By extending the Hohenberg-Mermin-Wagner theorem for the absence of spontaneous magnetization and the McBryan-Spencer bound for correlation functions, we prove that the models do not spontaneously break the rotational symmetry in their equilibrium states at any nonzero temperature. We thus conclude that the mobility of particles alone does not account for the spontaneous symmetry breaking in Vicsek type models. The origin of the symmetry breaking must be sought in the absence of detailed balance condition, or, equivalently, in the nonequilibrium nature. ",Hohenberg-Mermin-Wagner type theorems for equilibrium models of flocking
97,1291830136975892481,1003652696723873792,Max Gaspari,"['New paper with a brilliant student, I had the pleasure to mentor in recent years, dissecting the hot halo properties of rotating #galaxies (e.g. #cca_rain/condensation): <LINK> \nHere her precursor work: <LINK>\nGreat works Anna!!\n#BlackHoleWeather <LINK>']",https://arxiv.org/abs/2008.01161,"X-ray emitting atmospheres of non-rotating early-type galaxies and their connection to central active galactic nuclei have been thoroughly studied over the years. However, in systems with significant angular momentum, processes of heating and cooling are likely to proceed differently. We present an analysis of the hot atmospheres of six lenticulars and a spiral galaxy to study the effects of angular momentum on the hot gas properties. We find an alignment between the hot gas and the stellar distribution, with the ellipticity of the X-ray emission generally lower than that of the optical stellar emission, consistent with theoretical predictions for rotationally-supported hot atmospheres. The entropy profiles of NGC 4382 and the massive spiral galaxy NGC 1961 are significantly shallower than the entropy distribution in other galaxies, suggesting the presence of strong heating (via outflows or compressional) in the central regions of these systems. Finally, we investigate the thermal (in)stability of the hot atmospheres via criteria such as the TI- and C-ratio, and discuss the possibility that the discs of cold gas present in these objects have condensed out of the hot atmospheres. ",Hot gaseous atmospheres of rotating galaxies observed with XMM-Newton
98,1291651231300169728,790548789685788672,Michael Spence,"['New paper calibrating fishing mortality and static parameters in multispecies models of intermediate complexity using novel Markov Chain Monte Carlo algorithms. With @BlanchardJulia, @drfinlayscott and @robertthorpe16\n<LINK>']",https://arxiv.org/abs/2008.02765,"In marine management, fish stocks are often managed on a stock-by-stock basis using single-species models. Many of these models are based upon statistical techniques and are good at assessing the current state and making short-term predictions; however, as they do not model interactions between stocks, they lack predictive power on longer timescales. Additionally, there are mechanistic multi-species models that represent key biological processes and consider interactions between stocks such as predation and competition for resources. Due to the complexity of these models, they are difficult to fit to data, and so many mechanistic multi-species models depend upon single-species models where they exist, or ad hoc assumptions when they don't, for parameters such as annual fishing mortality. In this paper we demonstrate that by taking a state-space approach, many of the uncertain parameters can be treated dynamically, allowing us to fit, with quantifiable uncertainty, mechanistic multi-species models directly to data. We demonstrate this by fitting uncertain parameters, including annual fishing mortality, of a size-based multi-species model of the Celtic Sea, for species with and without single-species stock-assessments. Consequently, errors in the single-species models no longer propagate through the multi-species model and underlying assumptions are more transparent. Building mechanistic multi-species models that are internally consistent, with quantifiable uncertainty, will improve their credibility and utility for management. This may lead to their uptake by being either used to corroborate single-species models; directly in the advice process to make predictions into the future; or used to provide a new way of managing data-limited stocks. ","Quantifying uncertainty and dynamical changes in multi-species fishing
  mortality rates, catches and biomass by combining state-space and mechanistic
  multi-species models"
99,1291626371081547776,159796963,Manuel Burghardt,"['Our (@poke8192) new OCR paper ""On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter Evaluation"" is out now! <LINK> We question the role of binarization and found that 10k lines for training are usually quite sufficient. Any feedback appreciated!']",https://arxiv.org/abs/2008.02777,"We investigate how to train a high quality optical character recognition (OCR) model for difficult historical typefaces on degraded paper. Through extensive grid searches, we obtain a neural network architecture and a set of optimal data augmentation settings. We discuss the influence of factors such as binarization, input line height, network width, network depth, and other network training parameters such as dropout. Implementing these findings into a practical model, we are able to obtain a 0.44% character error rate (CER) model from only 10,000 lines of training data, outperforming currently available pretrained models that were trained on more than 20 times the amount of data. We show ablations for all components of our training pipeline, which relies on the open source framework Calamari. ","On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter
  Evaluation"
100,1291610469401124866,1570476014,Yi-Hsuan Yang,"[""Our new MMSP'20 paper shows STOA model (we use `open-unmix`) can be adapted to separate the violin &amp; piano in a violin piano ensemble. We also open source the code &amp; model checkpoint\n* paper: <LINK>\n* demo: <LINK>\n* code: <LINK> <LINK>""]",https://arxiv.org/abs/2008.02480,"Blind music source separation has been a popular and active subject of research in both the music information retrieval and signal processing communities. To counter the lack of available multi-track data for supervised model training, a data augmentation method that creates artificial mixtures by combining tracks from different songs has been shown useful in recent works. Following this light, we examine further in this paper extended data augmentation methods that consider more sophisticated mixing settings employed in the modern music production routine, the relationship between the tracks to be combined, and factors of silence. As a case study, we consider the separation of violin and piano tracks in a violin piano ensemble, evaluating the performance in terms of common metrics, namely SDR, SIR, and SAR. In addition to examining the effectiveness of these new data augmentation methods, we also study the influence of the amount of training data. Our evaluation shows that the proposed mixing-specific data augmentation methods can help improve the performance of a deep learning-based model for source separation, especially in the case of small training data. ","Mixing-Specific Data Augmentation Techniques for Improved Blind
  Violin/Piano Source Separation"
101,1291539867890049026,1138255660750127108,Berthold Jaeck,['Check out our new paper on quantum spin liquids: we report that tunneling spectroscopy could be well suited to find evidence for this elusive phase of matter. Great collaboration @PrincetonPhys with Mallika @MIT and my high school friend @ElioKonig @mpifkf <LINK>'],https://arxiv.org/abs/2008.02278,"We examine the spectroscopic signatures of tunneling through a Kitaev quantum spin liquid (QSL) barrier in a number of experimentally relevant geometries. We combine contributions from elastic and inelastic tunneling processes and find that spin-flip scattering at the itinerant spinon modes gives rise to a gaped contribution to the tunneling conductance spectrum. We address the spectral modifications that arise in a magnetic field necessary to drive the candidate material $\alpha$-RuCl$_3$ into a QSL phase, and we propose a lateral 1D tunnel junction as a viable setup in this regime. The characteristic spin gap is an unambiguous signature of the fractionalized QSL excitations, distinguishing it from magnons or phonons. The results of our analysis are generically applicable to a wide variety of topological QSL systems. ",Tunneling spectroscopy of quantum spin liquids
102,1291523725431463937,2228815292,Lewis Mitchell,"['Thread 👇about our new paper on word shift graphs. The most important thing to say is that you can now\n\npip install shifterator\n\nand improve your word cloud game!!\n\nA true pleasure to work with @ryanjgallag, and get back with the @compstorylab band: <LINK> <LINK>']",https://arxiv.org/abs/2008.02250,"A common task in computational text analyses is to quantify how two corpora differ according to a measurement like word frequency, sentiment, or information content. However, collapsing the texts' rich stories into a single number is often conceptually perilous, and it is difficult to confidently interpret interesting or unexpected textual patterns without looming concerns about data artifacts or measurement validity. To better capture fine-grained differences between texts, we introduce generalized word shift graphs, visualizations which yield a meaningful and interpretable summary of how individual words contribute to the variation between two texts for any measure that can be formulated as a weighted average. We show that this framework naturally encompasses many of the most commonly used approaches for comparing texts, including relative frequencies, dictionary scores, and entropy-based measures like the Kullback-Leibler and Jensen-Shannon divergences. Through several case studies, we demonstrate how generalized word shift graphs can be flexibly applied across domains for diagnostic investigation, hypothesis generation, and substantive interpretation. By providing a detailed lens into textual shifts between corpora, generalized word shift graphs help computational social scientists, digital humanists, and other text analysis practitioners fashion more robust scientific narratives. ","Generalized Word Shift Graphs: A Method for Visualizing and Explaining
  Pairwise Comparisons Between Texts"
103,1291448658416738304,2818695390,Sasho Nikolov,"['New paper with PhD student Lily Li <LINK>. We prove some algorithmic and hardness results about linear discrepancy, a quantity related to rounding (e.g., LP rounding). We also leave lots of problems open! To appear in ESA 2020. Small teaser in thread 👇 (1/4)', 'Remember the NP-complete Subset Sum problem: given numbers a_1, ..., a_n, and a target number t, is there a subset of the numbers that add up to t? \n\nA more general problem asks for the minimum distance between t and a subset sum of the a_i. Clearly also NP-hard. (2/4)', ""What if I ask for the max distance between any t (between the min and max of the a_i) and the (exponentially many) subset sums of the a_i? Intuitively, seems pretty hard.\n\nBut it isn't! We show that this quantity is computable in time O(n log n). (3/4)"", 'Linear discrepancy generalizes this problem to higher dimensions, and is very useful for studying rounding fractional values to integers. We show that it is NP-hard in high dimensions, but there is little known about hardness of approximation, or even if it is in NP at all. (4/4)']",https://arxiv.org/abs/2008.00044,"Many problems in computer science and applied mathematics require rounding a vector $\mathbf{w}$ of fractional values lying in the interval $[0,1]$ to a binary vector $\mathbf{x}$ so that, for a given matrix $\mathbf{A}$, $\mathbf{A}\mathbf{x}$ is as close to $\mathbf{A}\mathbf{w}$ as possible. For example, this problem arises in LP rounding algorithms used to approximate $\mathsf{NP}$-hard optimization problems and in the design of uniformly distributed point sets for numerical integration. For a given matrix $\mathbf{A}$, the worst-case error over all choices of $\mathbf{w}$ incurred by the best possible rounding is measured by the linear discrepancy of $\mathbf{A}$, a quantity studied in discrepancy theory, and introduced by Lovasz, Spencer, and Vesztergombi (EJC, 1986). We initiate the study of the computational complexity of linear discrepancy. Our investigation proceeds in two directions: (1) proving hardness results and (2) finding both exact and approximate algorithms to evaluate the linear discrepancy of certain matrices. For (1), we show that linear discrepancy is $\mathsf{NP}$-hard. Thus we do not expect to find an efficient exact algorithm for the general case. Restricting our attention to matrices with a constant number of rows, we present a poly-time exact algorithm for matrices consisting of a single row and matrices with a constant number of rows and entries of bounded magnitude. We also present an exponential-time approximation algorithm for general matrices, and an algorithm that approximates linear discrepancy to within an exponential factor. ",On the Computational Complexity of Linear Discrepancy
104,1291403394092093441,772809603046334464,Jonathan Mackey,"['New paper on @arxiv from the @hesstelescopes collaboration, including current and former @DIASAstronomy researchers, searching for (and unfortunately not finding) dark matter signals from dwarf galaxies:\n<LINK>\n@davit_zargaryan #DIASdiscovers']",https://arxiv.org/abs/2008.00688,"Dwarf spheroidal galaxy satellites of the Milky Way are prime targets for indirect detection of dark matter with gamma rays due to their proximity, high dark matter content and absence of non-thermal emission processes. Recently, the Dark Energy Survey (DES) revealed the existence of new ultra-faint dwarf spheroidal galaxies in the southern-hemisphere sky, therefore ideally located for ground-based observations with the imaging atmospheric Cherenkov telescope array H.E.S.S. We present a search for very-high-energy ($E\gtrsim100$ GeV) gamma-ray emission using H.E.S.S. observations carried out recently towards Reticulum II, Tucana II, Tucana III, Tucana IV and Grus II satellites. No significant very-high-energy gamma-ray excess is found from the observations on any individual object nor in the combined analysis of all the datasets. Using the most recent modeling of the dark matter distribution in the dwarf galaxy halo, we compute for the first time on DES satellites individual and combined constraints from Cherenkov telescope observations on the annihilation cross section of dark matter particles in the form of Weakly Interacting Massive Particles. The combined 95% C.L. observed upper limits reach $\langle \sigma v \rangle \simeq 1 \times 10^{-23}$ cm$^3$s$^{-1}$ in the $W^+W^-$ channel and $4 \times 10^{-26}$ cm$^3$s$^{-1}$ in the $\gamma\gamma$ channels for a dark matter mass of 1.5 TeV. The H.E.S.S. constraints well complement the results from Fermi-LAT, HAWC, MAGIC and VERITAS and are currently the most stringent in the $\gamma\gamma$ channels in the multi-GeV/multi-TeV mass range. ","Search for dark matter signals towards a selection of recently-detected
  DES dwarf galaxy satellites of the Milky Way with H.E.S.S"
105,1291297655302361088,754321756559667201,Ondrej Dusek,"['New #Interspeech2020 #NLProc paper by @tomiinek\n &amp; myself is now out on arXiv – we train a single text-to-speech model to seamlessly switch among 5 languages (i.e. pronounce foreign names correctly and keep a consistent voice): <LINK>', ""The model is a convolutional #Tacotron using meta-learning for different languages' parameters. It's trained using data from monolingual speakers, with more languages and less data than any previous attempt 🤓. \n\nCode: https://t.co/u7u8rx4Hh2\nColab demo: https://t.co/SSw2eGTmlW""]",https://arxiv.org/abs/2008.00768,"We introduce an approach to multilingual speech synthesis which uses the meta-learning concept of contextual parameter generation and produces natural-sounding multilingual speech using more languages and less training data than previous approaches. Our model is based on Tacotron 2 with a fully convolutional input text encoder whose weights are predicted by a separate parameter generator network. To boost voice cloning, the model uses an adversarial speaker classifier with a gradient reversal layer that removes speaker-specific information from the encoder. We arranged two experiments to compare our model with baselines using various levels of cross-lingual parameter sharing, in order to evaluate: (1) stability and performance when training on low amounts of data, (2) pronunciation accuracy and voice quality of code-switching synthesis. For training, we used the CSS10 dataset and our new small dataset based on Common Voice recordings in five languages. Our model is shown to effectively share information across languages and according to a subjective evaluation test, it produces more natural and accurate code-switching speech than the baselines. ","One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech"
106,1290854587293933571,85962581,Adenilton Silva,"['My new paper A divide-and-conquer algorithm for quantum state preparation with Israel Ferraz, Daniel Park, and @petruccione is on arxiv. A polylog depth circuit to load an N-dimensional vector with entangled information in ancillary qubits. <LINK>']",http://arxiv.org/abs/2008.01511,"Advantages in several fields of research and industry are expected with the rise of quantum computers. However, the computational cost to load classical data in quantum computers can impose restrictions on possible quantum speedups. Known algorithms to create arbitrary quantum states require quantum circuits with depth O(N) to load an N-dimensional vector. Here, we show that it is possible to load an N-dimensional vector with a quantum circuit with polylogarithmic depth and entangled information in ancillary qubits. Results show that we can efficiently load data in quantum devices using a divide-and-conquer strategy to exchange computational time for space. We demonstrate a proof of concept on a real quantum device and present two applications for quantum machine learning. We expect that this new loading strategy allows the quantum speedup of tasks that require to load a significant volume of information to quantum devices. ",A divide-and-conquer algorithm for quantum state preparation
107,1290823263225085955,1077995761487568896,Jon Miller,"['New paper day!  \n@NicolasTrueba has found gravitationally redshifted absorption lines from the inner accretion disk around a neutron star.  Absorption from the outer disk serves as a standard of rest.  Very, very cool. All thanks to @chandraxray. \n<LINK> <LINK>']",https://arxiv.org/abs/2008.01083,"The very small accretion disks in ultra-compact X-ray binaries (UCXBs) are special laboratories in which to study disk accretion and outflows. We report on three sets of new (250 ks total) and archival (50 ks) Chandra/HETG observations of the ""dipping"" neutron-star X-ray binary 4U 1916$-$053, which has an orbital period of $P\simeq 50$~minutes. We find that the bulk of the absorption in all three spectra originates in a disk atmosphere that is redshifted by $v\simeq 220-290$ $\text{km}$ $\text{s}^{-1}$, corresponding to the gravitational redshift at radius of $R \sim 1200$ $GM/{c}^{2}$. This shift is present in the strongest, most highly ionized lines (Si XIV and Fe XXVI), with a significance of 5$\sigma$. Absorption lines observed during dipping events (typically associated with the outermost disk) instead display no velocity shifts and serve as a local standard of rest, suggesting that the redshift is intrinsic to an inner disk atmosphere and not due to radial motion in the galaxy or a kick. In two spectra, there is also evidence of a more strongly redshifted component that would correspond to a disk atmosphere at $R \sim 70$ $GM/{c}^{2}$; this component is significant at the 3$\sigma$ level. Finally, in one spectrum, we find evidence of disk wind with a blue shift of $v = {-1700}^{+1700}_{-1200}$ $\text{km}$ $\text{s}^{-1}$. If real, this wind would require magnetic driving. ","A Redshifted Inner Disk Atmosphere and Transient Absorbers in the
  Ultra-Compact Neutron Star X-ray Binary 4U 1916-053"
108,1290807436643901447,20703003,Peter B Denton,"[""Nu paper with the team at BNL, Julia and Rebekah:\n\nCP-Violating Neutrino Non-Standard Interactions in Long-Baseline-Accelerator Data\n\n<LINK>\n\nNOvA and T2K disagree in a weird way. The significance isn't yet high (~2sig), what if it was due to new physics? 1/7"", 'Long-baseline neutrinos are a great place to probe all the oscillation parameters as well as new physics. @novaexperiment and @Tokai2Kamioka just released updated data and it was weird. They both prefer the normal ordering, but combined they prefer the inverted. 2/7 https://t.co/ERO9C6bEgK', ""But even in the inverted ordering things aren't a great fit.\n\nNo one has asked what kind of new physics this data could be pointing to. So a few weeks after Neutrino, we dove in!\n\n(Figure borrowed from the FNAL team with every intention of returning: https://t.co/qolzWxUxbw) 3/7 https://t.co/ySxV4fbShL"", ""Since the matter effect is bigger at NOvA than at T2K, NSIs will do the trick. The experiments measure nus/nubars separately at different rates, so the NSI had better violate CP. \n\nIn fact, there's a simple relationship between what's measured and the phase of the new physics 4/7 https://t.co/BiOVun5fgS"", ""You can also estimate the size of the NSI the same way to be ~0.2 (orange is preferred relative to the SM, gray is disfavored, dark gray is disfavored at &gt;90%).\n\nThe NOvA and T2K data are pointing to a region that is right at the edge of IceCube's constraint (also COHERENT). 5/7 https://t.co/5CqPQpdbgA"", ""I'm not saying that this is new physics, but if it were, it would mean that not only is there (fairly large) CP violation in the lepton mass matrix, but also a new neutrino interaction that *maximally* violates CP. 6/7"", 'Given that the quark matrix violates CP only a tiny bit, the strong interaction seems to conserve CP while the weak interaction maximally violates CP: understanding CP violation in neutrinos is vital to guide our understanding of what the heck is going on with CP. 7/7']",https://arxiv.org/abs/2008.01110,"Neutrino oscillations in matter provide a unique probe of new physics. Leveraging the advent of neutrino appearance data from NOvA and T2K in recent years, we investigate the presence of CP-violating neutrino non-standard interactions in the oscillation data. We first show how to very simply approximate the expected NSI parameters to resolve differences between two long-baseline appearance experiments analytically. Then, by combining recent NOvA and T2K data, we find a tantalizing hint of CP-violating NSI preferring a new complex phase that is close to maximal: $\phi_{e\mu}$ or $\phi_{e\tau}\approx3\pi/2$ with $|\epsilon_{e\mu}|$ or $|\epsilon_{e\tau}|\sim0.2$. We then compare the results from long-baseline data to constraints from IceCube and COHERENT. ","CP-Violating Neutrino Non-Standard Interactions in
  Long-Baseline-Accelerator Data"
109,1290736253202313218,1290691324363120640,Kellen D Lawson,"['Below: a visualization of the optimization of a noisy 7-D toy model debris disk w/ Differential Evolution (DE). We use DE to explore models for SCExAO/CHARIS observations of the “blue-needle” debris disk around HD 15115 in a new paper: <LINK>\n\nSo why DE? [THREAD] <LINK>', 'Modeling gets intractable quickly for studies using optical/near-IR ground-based imagery, as each model evaluation is a lengthy procedure (~10 minutes each for our data). This rules out techniques like Markov-Chain Monte Carlo, which requires evaluating many models (~10⁶).', 'Studies like ours historically use simple parameter grid searches instead. The difficulty of using and implementing more sophisticated techniques is likely another factor here. However, DE is a strong answer for all of these problems, and easily out-performs a grid search too:', 'e.g. in the toy-model visualization, the background contours are drawn using a 12 point grid, meaning 12⁷ or ~36 million evals. The DE run beats the best solution from this massive grid after only 1330 evaluations. But how hard is it to implement?', 'In our paper, we provide a Python implementation of DE in only 18 lines of code. Better yet, you really don’t need to have any idea how DE works to use it; no tuning of algorithm settings needed. Just show up with bounds for each model parameter. So what about our results?', 'Our SCExAO/CHARIS NIR imagery reveals the nearly edge-on  debris-disk of HD 15115 to significantly smaller separations than any previous NIR/optical observations: ~0.2”, versus ~0.4” from HST/STIS imagery or ~1.0” from the previous best ground-based observations. https://t.co/D7hVNv3MUR', 'We\'re able to get comparable fits for disk models with both one and two distinct rings in the &lt; ~1"" CHARIS field of view. However, we aren\'t able to produce any strong models showing a significantly non-coplanar inner ring as recently hypothesized. https://t.co/QXxfRX4U3S', 'The disk shows the same brightness asymmetry typically reported in optical/NIR at larger separations. Given the narrow separations probed by CHARIS, it seems unlikely that our asymmetry can be explained by the ISM interaction favored to explain asymmetry at wider separations.', 'Combining our NIR broadband disk photometry with measures from prior HST/STIS observations, ""the blue needle"" manifests with a definitively red optical-NIR color, and mostly gray NIR colors. Admittedly, ""the bluish, reddish, grayish needle"" is a bit of a mouthful.', 'Like many things in astronomy, I find HD 15115 to be sort-of ""fractally interesting"". For every phenomenon studied in detail, more nuanced mysteries become evident. Despite over a decade of observations of the system, I expect that interesting revelations are still to come.', 'Thanks for reading along here, and a massive thanks to the long list of collaborators on this project. Be sure to check out the pre-print on arXiv! https://t.co/TlsxhRSIGz', ""@EvanAstro Absolutely! Shoot me an email with details if you'd like.""]",https://arxiv.org/abs/2008.00309,"We present new, near-infrared ($1.1 - 2.4$ $\mu m$) high-contrast imaging of the debris disk around HD 15115 with the Subaru Coronagraphic Extreme Adaptive Optics system (SCExAO) coupled with the Coronagraphic High Angular Resolution Imaging Spectrograph (CHARIS). SCExAO/CHARIS resolves the disk down to $\rho \sim 0.2''$ ($\rm{r_{proj}} \sim 10$ $\rm{au}$), a factor of $\sim 3-5$ smaller than previous recent studies. We derive a disk position angle of $\rm{PA}$ $\sim 279.4^\circ - 280.5^\circ$ and an inclination of $\rm{i}$ $\sim 85.3^\circ - 86.2^\circ$. While recent SPHERE/IRDIS imagery of the system could suggest a significantly misaligned two ring disk geometry, CHARIS imagery does not reveal conclusive evidence for this hypothesis. Moreover, optimizing models of both one and two ring geometries using differential evolution, we find that a single ring having a Hong-like scattering phase function matches the data equally well within the CHARIS field of view ($\rho \lesssim 1''$). The disk's asymmetry, well-evidenced at larger separations, is also recovered; the west side of the disk appears on average around 0.4 magnitudes brighter across the CHARIS bandpass between $0.25''$ and $1''$. Comparing STIS/50CCD optical photometry ($2000-10500$ $\r{A}$) with CHARIS NIR photometry, we find a red (STIS/50CCD$-$CHARIS broadband) color for both sides of the disk throughout the $0.4'' - 1''$ region of overlap, in contrast to the blue color reported at similar wavelengths for regions exterior to $\sim 2''$. Further, this color may suggest a smaller minimum grain size than previously estimated at larger separations. Finally, we provide constraints on planetary companions, and discuss possible mechanisms for the observed inner disk flux asymmetry and color. ","SCExAO/CHARIS Near-IR Integral Field Spectroscopy of the HD 15115 Debris
  Disk"
110,1290638514829262854,39341168,Dan Sacks,"['🚨🚨🚨New paper!🚨🚨🚨\n\nWhat can we learn about SARS-CoV-2 prevalence from hospital and test data? \n\nurl: <LINK>\njoint with Nir Menachemi, @embimd , @coady_wing <LINK>', ""2/ The starting point for this paper is frustration that we still don't have a great measure of how many people have COVID, because testing is mostly limited to symptomatic people"", ""3/ the insight of the paper is that hospitalized people are tested at a very high rate, even if they're hospitalized for COVID unrelated reasons, like labor and delivery or cancer"", '4/ We develop bounds on population prevalence, under assumption that COVID-unrelated hospitalizations have COVID prevalence either (i) equal to population prevalence, or (ii) at least as high as population prevalence', '5/ We find people hospitalized for COVID unrelated reasons (non flu/non covid illness) are tested 50x more often than the general population https://t.co/mpboQ563XR', '6/ so we get tighter bounds on COVID prevalence among COVID-unrelated hospitalizations https://t.co/C1LSN6ko7B', '7/ Should we believe that COVID-unrelated hospitalizations are representative of population prevalence? We show the hospital-based bounds contain (random sample) estimates of prevalence (but see paper for nuance)', '8/  Overall we think test positivity among COVID-unrelated hospitalizations is a valuable metric for tracking COVID prevalence in general. And we think states could report this without much additional infrastructure.', '@MPKalina @onceuponA thanks, comments are welcome', ""@chrisjoondeph I would love to see systematic data on test rates and positive rates among professional sports (including staff). My casual impression is that there are lots of positive cases, but I'm not sure how many people are getting tested"", ""@chrisjoondeph more generally, yes this idea could work with any subpopulation that's tested at a high rate but has prevalence the same as the general pop (or if you think prevalence is higher in the tested subpop, you get a bound on pop prevalence)""]",https://arxiv.org/abs/2008.00298,"Measuring the prevalence of active SARS-CoV-2 infections in the general population is difficult because tests are conducted on a small and non-random segment of the population. However, people admitted to the hospital for non-COVID reasons are tested at very high rates, even though they do not appear to be at elevated risk of infection. This sub-population may provide valuable evidence on prevalence in the general population. We estimate upper and lower bounds on the prevalence of the virus in the general population and the population of non-COVID hospital patients under weak assumptions on who gets tested, using Indiana data on hospital inpatient records linked to SARS-CoV-2 virological tests. The non-COVID hospital population is tested fifty times as often as the general population, yielding much tighter bounds on prevalence. We provide and test conditions under which this non-COVID hospitalization bound is valid for the general population. The combination of clinical testing data and hospital records may contain much more information about the state of the epidemic than has been previously appreciated. The bounds we calculate for Indiana could be constructed at relatively low cost in many other states. ","What can we learn about SARS-CoV-2 prevalence from testing and hospital
  data?"
111,1290614120446918658,245517447,John Wisniewski,"['Congrats to OU grad student Kellen Lawson on his new paper <LINK> reporting SCExAO/CHARIS imaging of the inner (10-50 au) region of the HD 15115 debris disk! Kudos as well to @AstroThayne for a great job helping to co-mentor Kellen, and our broader science team.', 'Tagging @kellen_lawson who is making his twitter debut']",https://arxiv.org/abs/2008.00309,"We present new, near-infrared ($1.1 - 2.4$ $\mu m$) high-contrast imaging of the debris disk around HD 15115 with the Subaru Coronagraphic Extreme Adaptive Optics system (SCExAO) coupled with the Coronagraphic High Angular Resolution Imaging Spectrograph (CHARIS). SCExAO/CHARIS resolves the disk down to $\rho \sim 0.2''$ ($\rm{r_{proj}} \sim 10$ $\rm{au}$), a factor of $\sim 3-5$ smaller than previous recent studies. We derive a disk position angle of $\rm{PA}$ $\sim 279.4^\circ - 280.5^\circ$ and an inclination of $\rm{i}$ $\sim 85.3^\circ - 86.2^\circ$. While recent SPHERE/IRDIS imagery of the system could suggest a significantly misaligned two ring disk geometry, CHARIS imagery does not reveal conclusive evidence for this hypothesis. Moreover, optimizing models of both one and two ring geometries using differential evolution, we find that a single ring having a Hong-like scattering phase function matches the data equally well within the CHARIS field of view ($\rho \lesssim 1''$). The disk's asymmetry, well-evidenced at larger separations, is also recovered; the west side of the disk appears on average around 0.4 magnitudes brighter across the CHARIS bandpass between $0.25''$ and $1''$. Comparing STIS/50CCD optical photometry ($2000-10500$ $\r{A}$) with CHARIS NIR photometry, we find a red (STIS/50CCD$-$CHARIS broadband) color for both sides of the disk throughout the $0.4'' - 1''$ region of overlap, in contrast to the blue color reported at similar wavelengths for regions exterior to $\sim 2''$. Further, this color may suggest a smaller minimum grain size than previously estimated at larger separations. Finally, we provide constraints on planetary companions, and discuss possible mechanisms for the observed inner disk flux asymmetry and color. ","SCExAO/CHARIS Near-IR Integral Field Spectroscopy of the HD 15115 Debris
  Disk"
112,1302868135633211392,904084910163537920,Stefan Szeider (Hiring PhD students),"['Two teams compete. \n\nTeam A uses modern SAT solvers on a 20-year-old computer.\n\nTeam B uses 20-year-old SAT solvers on a modern computer.\n \nWho solves more problem instances? \n\nNew @cp2020conf paper ➡️<LINK>\n➡️<LINK> \n\nFunded by @FWF_at @WWTF <LINK>', ""@DvH24375691 @cp2020conf @FWF_at @WWTF Don't forget that we are dealing here with an NP-complete problem (SAT). The search space managed by new hw/sw is of orders of magnitude larger.""]",https://arxiv.org/abs/2008.02215,"We compare the impact of hardware advancement and algorithm advancement for SAT solving over the last two decades. In particular, we compare 20-year-old SAT-solvers on new computer hardware with modern SAT-solvers on 20-year-old hardware. Our findings show that the progress on the algorithmic side has at least as much impact as the progress on the hardware side. ",A Time Leap Challenge for SAT Solving
113,1301634360878067712,353206839,Peter Bailis,"['New paper from Stanford DAWN, joint with Google Ads AI, ""Leveraging Organizational Resources to Adapt Models to New Data Modalities"" appearing in #VLDB2020 <LINK>']",https://arxiv.org/abs/2008.09983,"As applications in large organizations evolve, the machine learning (ML) models that power them must adapt the same predictive tasks to newly arising data modalities (e.g., a new video content launch in a social media application requires existing text or image models to extend to video). To solve this problem, organizations typically create ML pipelines from scratch. However, this fails to utilize the domain expertise and data they have cultivated from developing tasks for existing modalities. We demonstrate how organizational resources, in the form of aggregate statistics, knowledge bases, and existing services that operate over related tasks, enable teams to construct a common feature space that connects new and existing data modalities. This allows teams to apply methods for training data curation (e.g., weak supervision and label propagation) and model training (e.g., forms of multi-modal learning) across these different data modalities. We study how this use of organizational resources composes at production scale in over 5 classification tasks at Google, and demonstrate how it reduces the time needed to develop models for new modalities from months to weeks to days. ","Leveraging Organizational Resources to Adapt Models to New Data
  Modalities"
114,1300811433471598597,891489280861904896,Sir Panda (Zad Rafi),"['New paper by @Lester_Domes and me. We discuss why uniformity is central to the validity of P-values and why some Bayesian variants don’t meet this, other units for S-values besides base-2 logs, and relation of S-values to other stat measures of information <LINK>', 'It’s an extension to our longer paper where we attempt to operationalize surprisals, P-values/S-values for alternative hypotheses, and confidence/surprisal distributions, which is now in press at BMC Medical Research Methodology https://t.co/ki7QUGQRyF', 'As always, we welcome all feedback!', '@ADAlthousePhD @Lester_Domes 👀👀👀👀', '@ashtroid22 @Lester_Domes Never heard of em']",https://arxiv.org/abs/2008.12991,"An extended technical discussion of $S$-values and unconditional information can be found in Greenland, 2019. Here we briefly cover several technical topics mentioned in our main paper, Rafi & Greenland, 2020: Different units for (scaling of) the $S$-value besides base-2 logs (bits); the importance of uniformity (validity) of the $P$-value for interpretation of the $S$-value; and the relation of the $S$-value to other measures of statistical information about a test hypothesis or model. ","Technical Issues in the Interpretation of S-values and Their Relation to
  Other Information Measures"
115,1300680036866043909,345129453,Nan Rosemary Ke,"['Our new paper out on amortized learning of neural representations: learning a fully continuous representation of causal models using neural networks! Thanks to my awesome collaborators @DaniloJRezende @janexwang @jovana_mitr, Martin Zummer  <LINK> <LINK>']",https://arxiv.org/abs/2008.09301,"Causal models can compactly and efficiently encode the data-generating process under all interventions and hence may generalize better under changes in distribution. These models are often represented as Bayesian networks and learning them scales poorly with the number of variables. Moreover, these approaches cannot leverage previously learned knowledge to help with learning new causal models. In order to tackle these challenges, we represent a novel algorithm called \textit{causal relational networks} (CRN) for learning causal models using neural networks. The CRN represent causal models using continuous representations and hence could scale much better with the number of variables. These models also take in previously learned information to facilitate learning of new causal models. Finally, we propose a decoding-based metric to evaluate causal models with continuous representations. We test our method on synthetic data achieving high accuracy and quick adaptation to previously unseen causal models. ",Amortized learning of neural causal representations
116,1300361107371184128,1218157200973139969,Dr. Rhaana Starling,"['New paper submitted with @Antonia_R: @LOFAR radio observations of a short gamma-ray burst just 4.4 minutes after trigger, putting constraints on the prompt coherent radio models. <LINK> <LINK>']",https://arxiv.org/abs/2008.12657,"The mergers of two neutron stars are typically accompanied by broad-band electromagnetic emission from either a relativistic jet or a kilonova. It has also been long predicted that coherent radio emission will occur during the merger phase or from a newly formed neutron star remnant, however this emission has not been seen to date. This paper presents the deepest limits for this emission from a neutron star merger folowing triggered LOFAR observations of the short gamma-ray burst (SGRB) 181123B, starting 4.4 minutes after the GRB occurred. During the X-ray plateau phase, a signature of ongoing energy injection, we detect no radio emission to a 3$\sigma$ limit of 153 mJy at 144 MHz (image integration time of 136 seconds), which is significantly fainter than the predicted emission from a standard neutron star. At a redshift of 1.8, this corresponds to a luminosity of $2.5 \times 10^{44}$ erg s$^{-1}$. Snapshot images were made of the radio observation on a range of timescales, targeting short duration radio flashes similar to fast radio bursts (FRBs). No emission was detected in the snapshot images at the location of GRB 181123B enabling constraints to be placed on the prompt coherent radio emission model and emission predicted to occur when a neutron star collapses to form a black hole. At the putative host redshift of 1.8 for GRB 181123B, the non detection of the prompt radio emission is two orders of magnitude lower than expected for magnetic reconnection models for prompt GRB emission and no magnetar emission is expected. ","LOFAR early-time search for coherent radio emission from Short GRB
  181123B"
117,1298784002619453442,4666231375,Konstantin Batygin,"['Jupiter has many Trojan asteroids, but one of them is retrograde. In a new paper led by Tobias Kohne, we argue that this body could be a temporarily captured high-inclination Centaur. Interestingly, high-inc Centaurs are readily created in Planet 9 sims.. <LINK>']",https://arxiv.org/abs/2008.11242,"Over the course of the last decade, observations of highly-inclined (orbital inclination i > 60{\deg}) Trans-Neptunian Objects (TNOs) have posed an important challenge to current models of solar system formation (Levison et al. 2008; Nesvorn\'y 2015). These remarkable minor planets necessitate the presence of a distant reservoir of strongly-out-of-plane TNOs, which itself requires some dynamical production mechanism (Gladman et al. 2009; Gomes et al. 2015; Batygin and Brown 2016). A notable recent addition to the census of high-i minor bodies in the solar system is the retrograde asteroid 514107 Ka'epaoka'awela, which currently occupies a 1:-1 mean motion resonance with Jupiter at i = 163{\deg} (Wiegert et al. 2017). In this work, we delineate a direct connection between retrograde Jupiter Trojans and high-i Centaurs. First, we back-propagate a large sample of clones of Ka'epaoka'awela for 100 Ma numerically, and demonstrate that long-term stable clones tend to decrease their inclination steadily until it concentrates between 90{\deg} and 135{\deg}, while their eccentricity and semi-major axis increase, placing many of them firmly into the trans-Neptunian domain. Importantly, the clones show significant overlap with the synthetic high-i Centaurs generated in Planet 9 studies (Batygin et al. 2019), and hint at the existence of a relatively prominent, steady-state population of minor bodies occupying polar trans-Saturnian orbits. Second, through direct numerical forward-modeling, we delineate the dynamical pathway through which conventional members of the Kuiper Belt's scattered disk population can become retrograde Jovian Trojan resonators in presence of Planet 9. ","On the Dynamical Origins of Retrograde Jupiter Trojans and their
  Connection to High-Inclination TNOs"
118,1295989405665067009,1021360423,Mubrak A Alqahtani,"['We have a new paper out where we did comparisons with Pb-Pb collisions at 5.02 TeV. Comparisons for the spectra, particles ratios, integrated elliptic flow...are shown where the agreement is quite good, and in some cases excellent!\n<LINK> <LINK>', 'Another observable! https://t.co/JPQdeq0rkO', 'The above figure shows the kaon-to-pion (K/π) and proton-to-pion (p/π) ratios as a function of centrality.']",https://arxiv.org/abs/2008.07657,"We present comparisons between 3+1D quasiparticle anisotropic hydrodynamics (aHydroQP) predictions for a large set of bulk observables and experimental data collected in 5.02 TeV Pb-Pb collisions. We make aHydroQP predictions for identified hadron spectra, identified hadron average transverse momentum, charged particle multiplicity as a function of pseudorapidity, the kaon-to-pion ($K/\pi$) and proton-to-pion ($p/\pi$) ratios, and integrated elliptic flow. We compare to data collected by the ALICE collaboration in 5.02 TeV Pb-Pb collisions. We find that these bulk observables are quite well described by aHydroQP with an assumed initial central temperature of $T_0=630$ MeV at $\tau_0 = 0.25$ fm/c and a constant specific shear viscosity of $\eta/s=0.159$ and a peak specific bulk viscosity of $\zeta/s = 0.048$. In particular, we find that the momentum dependence of the kaon-to-pion ($K/\pi$) and proton-to-pion ($p/\pi$) ratios reported recently by the ALICE collaboration are extremely well described by aHydroQP in the most central collisions. ","Bulk observables at 5.02 TeV using quasiparticle anisotropic
  hydrodynamics"
119,1295901753691578371,1238890699111751680,Anirudha Majumdar,"['How can we learn control policies that accomplish a given task using as little memory as possible? Check out our new preprint ""Learning to Actively Reduce Memory Requirements for Robot Control Tasks"".\nw/ Meghan Booker.\nPaper: <LINK>\nVideo: <LINK> <LINK>']",https://arxiv.org/abs/2008.07451,"Robots equipped with rich sensing modalities (e.g., RGB-D cameras) performing long-horizon tasks motivate the need for policies that are highly memory-efficient. State-of-the-art approaches for controlling robots often use memory representations that are excessively rich for the task or rely on hand-crafted tricks for memory efficiency. Instead, this work provides a general approach for jointly synthesizing memory representations and policies; the resulting policies actively seek to reduce memory requirements. Specifically, we present a reinforcement learning framework that leverages an implementation of the group LASSO regularization to synthesize policies that employ low-dimensional and task-centric memory representations. We demonstrate the efficacy of our approach with simulated examples including navigation in discrete and continuous spaces as well as vision-based indoor navigation set in a photo-realistic simulator. The results on these examples indicate that our method is capable of finding policies that rely only on low-dimensional memory representations, improving generalization, and actively reducing memory requirements. ",Learning to Actively Reduce Memory Requirements for Robot Control Tasks
120,1295736765408522242,52036434,Fabio Petrillo,"[""Is open source development ad-hoc? Our results show that OS development is less ad-hoc we could imagine. In a new paper accepted to present at IEEE EDOC'20, @biancanapolean compiles 33 studies to understand and learn with OSS process. Preprint at <LINK> ."", 'And there is a great OSS BPM meta-process for you... :-) https://t.co/92fgjhiZr9']",https://arxiv.org/abs/2008.05015,"Open Source Software (OSS) has been recognized by the software development community as an effective way to deliver software. Unlike traditional software development, OSS development is driven by collaboration among developers spread geographically and motivated by common goals and interests. Besides this fact, it is recognized by OSS community the need of understand OSS development process and its activities. Our goal is to investigate the state-of-art about OSS process through conducting a systematic literature review providing an overview of how the OSS community has been investigating OSS process over past years identifying and summarizing OSS process activities and their characteristics as well as translating OSS process in a macro process through BPMN notation. As a result, we systematically analysed 33 studies presenting an overview of the state-of-art of researches regarding OSS process, a generalized OSS development macro process represented by BPMN notation with a detailed description of each OSS process activity and roles in OSS environment. We conclude that OSS process can be in practice further investigated by researchers. In addition, the presented OSS process can be used as a guide for OSS projects and being adapted according to each OSS project reality. It provides insights to managers and developers who want to improve their development process even in OSS and traditional environments. Finally, recommendations for OSS community regarding OSS process activities are provided. ",Open Source Software Development Process: A Systematic Review
121,1293305776606330880,349172730,Ranjay Krishna,"[""If you're releasing a new user-facing AI project/product, you might want to read our new #CSCW2020 paper. We find that words or metaphors used to describe AI agents have a causal effect on users' intention to adopt your agent. <LINK> Thread👇"", 'Conceptual metaphors are one of the most common and powerful means that a designer has to influence user expectations. They have been traditionally used by designers to convey functionality. Ex, ""recycling bin"" is for unwanted files, and ""notepad"" is for taking notes. https://t.co/9V3BuWQUuI', 'Metaphors have also been used for sense-making: understanding how existing AI systems work using informal, intuitive folk theories. For example, people explain Google Search as a ""robotic nose"" and YouTube\'s recommendations as a ""drug dealer"". https://t.co/PSi1o2Bvvl', 'In our study, participants do a task with an agent described using various metaphors. Following Psych literature, we categorize metaphors along dimensions of competence and warmth: ""toddler"" projects low competence, high warmth, and ""inexperienced teen”-low competence, low warmth https://t.co/ut5frOCaUh', ""Contrary to how today's AI products are advertised, people are more likely to adopt an agent that they originally expected to have low competence but outperforms that expectation. They are less forgiving of mistakes made by agents they expect to have high competence. https://t.co/RHTSBk8IF4"", 'Meanwhile, the opposite is true for warmth. People are more likely to cooperate with agents that project high warmth. Also, people spend significantly more time interacting with high warmth agents. https://t.co/lLoFFcRAr3', 'Our work provides another lens explaining why some functionally-similar agents get adopted (Xiaoice ""sympathetic ear"") while others with high competence (Mitsuku ""record-breaking Turing Test winner"") or low warmth (Tay ""AI fam that\'s got no chill"") elicit anti-social behavior. https://t.co/SdWbNW2drA', 'This is work done with my amazing collaborators: @pranavkhadpe @drfeifei  Jeff Hancock and @msbernst']",https://arxiv.org/abs/2008.02311,"With the emergence of conversational artificial intelligence (AI) agents, it is important to understand the mechanisms that influence users' experiences of these agents. We study a common tool in the designer's toolkit: conceptual metaphors. Metaphors can present an agent as akin to a wry teenager, a toddler, or an experienced butler. How might a choice of metaphor influence our experience of the AI agent? Sampling metaphors along the dimensions of warmth and competence---defined by psychological theories as the primary axes of variation for human social perception---we perform a study (N=260) where we manipulate the metaphor, but not the behavior, of a Wizard-of-Oz conversational agent. Following the experience, participants are surveyed about their intention to use the agent, their desire to cooperate with the agent, and the agent's usability. Contrary to the current tendency of designers to use high competence metaphors to describe AI products, we find that metaphors that signal low competence lead to better evaluations of the agent than metaphors that signal high competence. This effect persists despite both high and low competence agents featuring human-level performance and the wizards being blind to condition. A second study confirms that intention to adopt decreases rapidly as competence projected by the metaphor increases. In a third study, we assess effects of metaphor choices on potential users' desire to try out the system and find that users are drawn to systems that project higher competence and warmth. These results suggest that projecting competence may help attract new users, but those users may discard the agent unless it can quickly correct with a lower competence metaphor. We close with a retrospective analysis that finds similar patterns between metaphors and user attitudes towards past conversational agents such as Xiaoice, Replika, Woebot, Mitsuku, and Tay. ",Conceptual Metaphors Impact Perceptions of Human-AI Collaboration
122,1292854888968474625,373525906,Weijie Su,"['New interpretation of the *double descent* phenomenon: noise in features is ubiquitous, and we show using a random feature model that noise can lead to benign overfitting. Paper: <LINK>. w/ Zhu Li and Dino Sejdinovic. <LINK>', '@2prime_PKU Thanks for the reference! Will look into it.', '@roydanroy @jeffNegrea @KDziugaite Thanks for the reference. Very related']",https://arxiv.org/abs/2008.02901,"Modern machine learning often operates in the regime where the number of parameters is much higher than the number of data points, with zero training loss and yet good generalization, thereby contradicting the classical bias-variance trade-off. This \textit{benign overfitting} phenomenon has recently been characterized using so called \textit{double descent} curves where the risk undergoes another descent (in addition to the classical U-shaped learning curve when the number of parameters is small) as we increase the number of parameters beyond a certain threshold. In this paper, we examine the conditions under which \textit{Benign Overfitting} occurs in the random feature (RF) models, i.e. in a two-layer neural network with fixed first layer weights. We adopt a new view of random feature and show that \textit{benign overfitting} arises due to the noise which resides in such features (the noise may already be present in the data and propagate to the features or it may be added by the user to the features directly) and plays an important implicit regularization role in the phenomenon. ",Benign Overfitting and Noisy Features
123,1291839861687832576,773069606,Federico Ardila,"['New paper!\n\nThe bipermutahedron\n<LINK>\n\nThis polytope arose in my work with Graham Denham and June Huh on the Lagrangian geometry of matroids. This paper studies its combinatorics and discrete geometry. \n\n(This picture proves that the bipermutahedron exists.) <LINK>', '@spsaaibi Gracias @spsaaibi, ha estado muy divertido este proyecto!', 'Credit to @scribbletogethr, my new favorite tool:\nhttps://t.co/f7uaqXYXDN', '@eramirem @spsaaibi A mí me parece mentira que este tipo de pruebas aparezca así ""en la naturaleza"". La matemática muchas veces es bonita, pero a veces exagera!']",https://arxiv.org/abs/2008.02295,"The harmonic polytope and the bipermutahedron are two related polytopes which arose in Ardila, Denham, and Huh's work on the Lagrangian geometry of matroids. We study the bipermutahedron. We show that its faces are in bijection with the vertex-labeled and edge-labeled multigraphs with no isolated vertices; the generating function for its f-vector is a simple evaluation of the three variable Rogers--Ramanujan function. We show that the h-polynomial of the bipermutahedral fan is the biEulerian polynomial, which counts bipermutations according to their number of descents. We construct a unimodular triangulation of the product of n triangles that is combinatorially equivalent to (the triple cone over) the nth bipermutahedral fan. Ehrhart theory then gives us a formula for the biEulerian polynomial, which we use to show that this polynomial is real-rooted and that the h-vector of the bipermutahedral fan is log-concave and unimodal. We describe all the deformations of the bipermutahedron; that is, the ample cone of the bipermutahedral toric variety. We prove that among all polytopes in this family, the bipermutahedron has the largest possible symmetry group. Finally, we show that the Minkowski quotient of the bipermutahedron and the harmonic polytope equals 2. ",The bipermutahedron
124,1303329381587222528,752184524121993216,Menelaos Kanakis,"['Check out our #BMVC2020 paper ""Automated Search for Resource-Efficient Branched Multi-Task Networks"". We propose an approach to automatically define branched multi-task networks, while using a resource-aware loss to control the model size\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2008.10292,"The multi-modal nature of many vision problems calls for neural network architectures that can perform multiple tasks concurrently. Typically, such architectures have been handcrafted in the literature. However, given the size and complexity of the problem, this manual architecture exploration likely exceeds human design abilities. In this paper, we propose a principled approach, rooted in differentiable neural architecture search, to automatically define branching (tree-like) structures in the encoding stage of a multi-task neural network. To allow flexibility within resource-constrained environments, we introduce a proxyless, resource-aware loss that dynamically controls the model size. Evaluations across a variety of dense prediction tasks show that our approach consistently finds high-performing branching structures within limited resource budgets. ",Automated Search for Resource-Efficient Branched Multi-Task Networks
125,1301800001799102467,734667419055230976,Lisa Matthias,['Update on our study on vanished OA journals. We re-validated our data&amp;extended our search for archives beyond the Keepers to remove journals that still exist. We also added a few new ones! New count: 176. Paper: <LINK> Data: <LINK> #openaccess <LINK>'],https://arxiv.org/abs/2008.11933,"The preservation of the scholarly record has been a point of concern since the beginning of knowledge production. With print publications, the responsibility rested primarily with librarians, but the shift toward digital publishing and, in particular, the introduction of open access (OA) have caused ambiguity and complexity. Consequently, the long-term accessibility of journals is not always guaranteed, and they can even disappear from the web completely. The focus of this exploratory study is on the phenomenon of vanished journals, something that has not been carried out before. For the analysis, we consulted several major bibliographic indexes, such as Scopus, Ulrichsweb, and the Directory of Open Access Journals, and traced the journals through the Internet Archive's Wayback Machine. We found 174 OA journals that, through lack of comprehensive and open archives, vanished from the web between 2000 and 2019, spanning all major research disciplines and geographic regions of the world. Our results raise vital concern for the integrity of the scholarly record and highlight the urgency to take collaborative action to ensure continued access and prevent the loss of more scholarly knowledge. We encourage those interested in the phenomenon of vanished journals to use the public dataset for their own research. ",Open is not forever: a study of vanished open access journals
126,1301475345636487168,28734416,Sebastian Risi,"['Can we use machine learning methods such as GANs to assess creativity? In a recent collaboration lead by @jrafner we try to find out by letting players ""blend"" existing images into new images under varying constraints. Paper: <LINK> <LINK>', 'Our study indicates that the system provides a playful experience, affords players a sense of control over the interface, and elicits different types of player behavior, supporting further study of the tool for use in a scalable, playful, creativity assessment. https://t.co/PvIyNv9pBA', 'w/ @jacobsherson, @sparvell, @Learnonomy, @jacksohne, @ACRold, @asmaalfadala, @dominicregester']",https://arxiv.org/abs/2008.05914,"We present a pilot study on crea.blender, a novel co-creative game designed for large-scale, systematic assessment of distinct constructs of human creativity. Co-creative systems are systems in which humans and computers (often with Machine Learning) collaborate on a creative task. This human-computer collaboration raises questions about the relevance and level of human creativity and involvement in the process. We expand on, and explore aspects of these questions in this pilot study. We observe participants play through three different play modes in crea.blender, each aligned with established creativity assessment methods. In these modes, players ""blend"" existing images into new images under varying constraints. Our study indicates that crea.blender provides a playful experience, affords players a sense of control over the interface, and elicits different types of player behavior, supporting further study of the tool for use in a scalable, playful, creativity assessment. ","crea.blender: A Neural Network-Based Image Generation Game to Assess
  Creativity"
127,1301077939501568001,454838126,Jos de Bruijne,"['I am very proud of former @ESA intern Jurjen: ""Benfords law in the #GaiaMission Universe"" <LINK> ""We investigate whether Benford\'s law holds for the 1.3 billion parallaxes contained in #GaiaDR2 [and study its use as] validation tool for the parallax zero point"" <LINK>']",https://arxiv.org/abs/2008.12271,"Benfords law states that for scale- and base-invariant data sets covering a wide dynamic range, the distribution of the first significant digit is biased towards low values. This has been shown to be true for wildly different datasets, including financial, geographical, and atomic data. In astronomy, earlier work showed that Benfords law also holds for distances estimated as the inverse of parallaxes from the ESA Hipparcos mission. We investigate whether Benfords law still holds for the 1.3 billion parallaxes contained in the second data release of Gaia (Gaia DR2). In contrast to previous work, we also include negative parallaxes. We examine whether distance estimates computed using a Bayesian approach instead of parallax inversion still follow Benfords law. Lastly, we investigate the use of Benfords law as a validation tool for the zero-point of the Gaia parallaxes. ",Benfords law in the Gaia universe
128,1300972587615285248,293552287,Hoan Tran,['We formulate and propose the Universal Approximation Property of Quantum Feature Map.\n<LINK>\nOur research is independent but in the same line with the recent work by Xanadu to understand the expressive power of the quantum model.\n<LINK> <LINK>'],https://arxiv.org/abs/2009.00298,"Encoding classical data into quantum states is considered a quantum feature map to map classical data into a quantum Hilbert space. This feature map provides opportunities to incorporate quantum advantages into machine learning algorithms to be performed on near-term intermediate-scale quantum computers. The crucial idea is using the quantum Hilbert space as a quantum-enhanced feature space in machine learning models. While the quantum feature map has demonstrated its capability when combined with linear classification models in some specific applications, its expressive power from the theoretical perspective remains unknown. We prove that the machine learning models induced from the quantum-enhanced feature space are universal approximators of continuous functions under typical quantum feature maps. We also study the capability of quantum feature maps in the classification of disjoint regions. Our work enables an important theoretical analysis to ensure that machine learning algorithms based on quantum feature maps can handle a broad class of machine learning tasks. In light of this, one can design a quantum machine learning model with more powerful expressivity. ","Universal Approximation Property of Quantum Machine Learning Models in
  Quantum-Enhanced Feature Spaces"
129,1300831011878572032,37838307,Justin Caram,"['More forays into theory from the Caram Group!  Here with the Neuhauser group we show that you can use stochastic methods to rapidly study the excitonic properties of molecular aggregates.  Congrats Nadine and  @arundhati175 and others not on twitter :)\n<LINK>', '@arundhati175 Excitonic aggregates have two features that make them hard compared to say...semiconductors.  Lots of disorder, dipolar coupling that falls of slowly (meaning tight binding misses the details).  The frenkel exciton hamiltonian works ok, but requires diagonalization.', '@arundhati175 Since diagonalization is N^3 with system size, Its really slow for large systems, particularly 2D systems for which the system size grows with edge^2.  Stochastic methods scale with NlogN (basically linearly).  So much easier to screen disorder, lineshapes etc.', '@arundhati175 This lets us look at other cool properties...like if energy levels are correlated over any distance range, what does that do (a lot it turns out!).  This is a cool application of stochastic methods that are widely used to improve DFT scaling.']",https://arxiv.org/abs/2008.13228,"We show that a stochastic approach enables calculations of the optical properties of large 2-dimensional and nanotubular excitonic molecular aggregates. Previous studies of such systems relied on numerically diagonalizing the dense and disordered Frenkel Hamiltonian, which scales approximately as $\mathcal{O}(N^3)$ for $N$ dye molecules. Our approach scales much more efficiently as $\mathcal{O}(N\log(N))$, enabling quick study of systems with a million of coupled molecules on the micron size scale. We calculate several important experimental observable including the optical absorption spectrum and density of states, and develop a stochastic formalism for the participation ratio. Quantitative agreement with traditional matrix diagonalization methods is demonstrated for both small- and intermediate-size systems. The stochastic methodology enables the study of the effects of spatial-correlation in site energies on the optical signatures of large 2D aggregates. Our results demonstrate that stochastic methods present a path forward for screening structural parameters and validating experiments and theoretical predictions in large excitonic aggregates. ",Stochastically Realized Observables for Excitonic Molecular Aggregates
130,1299304751457357825,1720813753,yappie,['Fast Bayesian Force Fields from Active Learning: Study of Inter-Dimensional Transformation of Stanene. (arXiv:2008.11796v1 [physics.comp-ph]) <LINK>\n\nWe present a way to dramatically accelerate Gaussian process models for interatomic force fields based on many-…'],http://arxiv.org/abs/2008.11796,"We present a way to dramatically accelerate Gaussian process models for interatomic force fields based on many-body kernels by mapping both forces and uncertainties onto functions of low-dimensional features. This allows for automated active learning of models combining near-quantum accuracy, built-in uncertainty, and constant cost of evaluation that is comparable to classical analytical models, capable of simulating millions of atoms. Using this approach, we perform large scale molecular dynamics simulations of the stability of the stanene monolayer. We discover an unusual phase transformation mechanism of 2D stanene, where ripples lead to nucleation of bilayer defects, densification into a disordered multilayer structure, followed by formation of bulk liquid at high temperature or nucleation and growth of the 3D bcc crystal at low temperature. The presented method opens possibilities for rapid development of fast accurate uncertainty-aware models for simulating long-time large-scale dynamics of complex materials. ","Bayesian Force Fields from Active Learning for Simulation of
  Inter-Dimensional Transformation of Stanene"
131,1299260107486834688,786855300322172928,Alkistis Pourtsidou,"['Paper alert! In <LINK> led by Paula Soares @psahds we study unbiased parameter estimation for HI intensity mapping experiments [thread].', 'Using state-of-the-art simulations by @CunningtonSD and MCMC, we provide forecasts for a range of parameters including RSDs and the Alcock-Paczynski effect. Using multipole expansion and appropriate scale cuts we ensure we are not biased due to nonlinearities.', 'Importantly, @psahds devised a 2-parameter function that can successfully model the effects of foreground removal in the parallel and perpendicular to the LOS directions; this ensures we get both the precision *and* the accuracy right! https://t.co/nKjdOV0vRt', ""Important asides: We also study the effects of the instrument's beam and foreground removal on the covariance matrix, and we comment on the usefulness of higher order multipoles like the beloved hexacontatetrapole 😄 Worth a read!!"", 'Many thanks also to our wonderful collaborator Chris Blake!', '@SeshNadathur @psahds @cosmofromhome Yeah, that\'s a good point to ponder about! The ~2x intuition from galaxy surveys comes from varying only the params we vary or + all the ""shape"" parameters we have assumed fixed? Would be easy to do a test without the beam and see what we get to have a more direct comparison.', '@SeshNadathur @psahds @cosmofromhome The size of the beam vs the redshift of the measurement also matters -- I believe we had initially tested that and we got reasonable answers. Again, we should probably comment on v2!', '@SeshNadathur @psahds @cosmofromhome Cheers! I think a test without the beam and with varying beam sizes would help pin this down. Especially comparing the degeneracies between params.']",https://arxiv.org/abs/2008.12102,"We assess the performance of the multipole expansion formalism in the case of single-dish HI intensity mapping, including instrumental and foreground removal effects. This formalism is used to provide MCMC forecasts for a range of HI and cosmological parameters, including redshift space distortions and the Alcock-Paczynski effect. We first determine the range of validity of our power spectrum modelling by fitting to simulation data, concentrating on the monopole, quadrupole, and hexadecapole contributions. We then show that foreground subtraction effects can lead to severe biases in the determination of cosmological parameters, in particular the parameters relating to the transverse BAO rescaling, the growth rate and the HI bias ($\alpha_\perp$, $\overline{T}_\text{HI} f\sigma_8$, and $\overline{T}_\text{HI} b_\text{HI} \sigma_8$, respectively). We attempt to account for these biases by constructing a 2-parameter foreground modelling prescription, and find that our prescription leads to unbiased parameter estimation at the expense of increasing the estimated uncertainties on cosmological parameters. In addition, we confirm that instrumental and foreground removal effects significantly impact the theoretical covariance matrix, and cause the covariance between different multipoles to become non-negligible. Finally, we show the effect of including higher-order multipoles in our analysis, and how these can be used to investigate the presence of instrumental and systematic effects in HI intensity mapping data. ","Power spectrum multipole expansion for HI intensity mapping experiments:
  unbiased parameter estimation"
132,1299222634098376704,140287694,Anowar J Shajib,"['Our new paper is on arXiv today: <LINK>.\n\nBy analyzing 23 elliptical lens galaxies from SLACS, we find that the dark matter distribution is close to the NFW profile on average at z~0.2 without any contraction/expansion.\n\nA summary with some figures in the thread.', 'We perform state-of-the-art lens modeling for these 23 lens galaxies. This figure shows 5 of them. https://t.co/tP9NrGyH5S', 'We combine the strong lensing constraints with the stellar kinematics and weak lensing measurements to individually constrain the stellar and dark matter distributions. Our model allows for adiabatic contraction in the dark matter and a M/L gradient in the stellar distribution. https://t.co/XhE2OHahsz', 'We find that the NFW+stars profile deviate upwards by ~5% on average from the power-law model near the Einstein radius. https://t.co/IAILEn1rL4', 'On average, there is no significant contraction/expansion in the dark matter halos with M_200 ~ 10^13.1 M_sun at z~0.2. Furthermore, almost no gradient in the stellar M/L is favored around the effective or half-light radius. https://t.co/kk96jRfzxy', 'Comparing our inferred stellar masses with those from SPS-based measurements supports a heavy IMF like the Salpeter IMF in these elliptical galaxies. https://t.co/IglQXCUWcD', 'All of our results are consistent with a scenario where halos of  massive elliptical galaxies first contract up to z~2 due to baryonic cooling, but then the halos primarily grow through dissipationless mergers while AGN feedback counteracts the initial contraction.']",https://arxiv.org/abs/2008.11724,"We investigate the internal structure of elliptical galaxies at $z\sim 0.2$ from a joint lensing-dynamics analysis. We model Hubble Space Telescope images of a sample of 23 galaxy-galaxy lenses selected from the Sloan Lens ACS (SLACS) survey. Whereas the original SLACS analysis estimated the logarithmic slopes by combining the kinematics with the imaging data, we estimate the logarithmic slopes only from the imaging data. We find that the distribution of the lensing-only logarithmic slopes has a median $2.08\pm0.03$ and intrinsic scatter $0.13 \pm 0.02$, consistent with the original SLACS analysis. We combine the lensing constraints with the stellar kinematics and weak lensing measurements, and constrain the amount of adiabatic contraction in the dark matter (DM) halos. We find that the DM halos are well described by a standard Navarro-Frenk-White halo with no contraction on average for both of a constant stellar mass-to-light ratio ($M/L$) model and a stellar $M/L$ gradient model. For the $M/L$ gradient model, we find that most galaxies are consistent with no $M/L$ gradient. Comparison of our inferred stellar masses with those obtained from the stellar population synthesis method supports a heavy initial mass function (IMF) such as the Salpeter IMF. We discuss our results in the context of previous observations and simulations, and argue that our result is consistent with a scenario in which active galactic nucleus feedback counteracts the baryonic-cooling-driven contraction in the DM halos. ","Dark matter halos of massive elliptical galaxies at $z \sim 0.2$ are
  well described by the Navarro-Frenk-White profile"
133,1298823818509774851,422672164,Dr Michael Reidinger,"['Deep learning the astrometric signature of dark matter substructure\n\n""We study the application of machine learning techniques for the detection of the astrometric signature of dark matter substructure.""\n<LINK>']",https://arxiv.org/abs/2008.11577,"We study the application of machine learning techniques for the detection of the astrometric signature of dark matter substructure. In this proof of principle a population of dark matter subhalos in the Milky Way will act as lenses for sources of extragalactic origin such as quasars. We train {\it ResNet-18}, a state-of-the-art convolutional neural network to classify angular velocity maps of a population of quasars into lensed and no lensed classes. We show that an SKA -like survey with extended operational baseline can be used to probe the substructure content of the Milky Way, and demonstrate how axiomatic attribution can be used to localize substructures in lensing maps. ",Deep learning the astrometric signature of dark matter substructure
134,1298714508672684036,472628395,Peter Melchior,"['After requests from several parties, the final report for the Joint Processing Study Group is now public: <LINK>\nIf you want to know what we plan for joint pixel level analyses of @VRubinObs, @ESA_Euclid &amp; @NASARoman, this is the most detailed description (yet).']",https://arxiv.org/abs/2008.10663,"The Euclid, Rubin/LSST and Roman (WFIRST) projects will undertake flagship optical/near-infrared surveys in the next decade. By mapping thousands of square degrees of sky and covering the electromagnetic spectrum between 0.3 and 2 microns with sub-arcsec resolution, these projects will detect several tens of billions of sources, enable a wide range of astrophysical investigations by the astronomical community and provide unprecedented constraints on the nature of dark energy and dark matter. The ultimate cosmological, astrophysical and time-domain science yield from these missions will require joint survey processing (JSP) functionality at the pixel level that is outside the scope of the individual survey projects. The JSP effort scoped here serves two high-level objectives: 1) provide precise concordance multi-wavelength images and catalogs over the entire sky area where these surveys overlap, which accounts for source confusion and mismatched isophotes, and 2) provide a science platform to analyze concordance images and catalogs to enable a wide range of astrophysical science goals to be formulated and addressed by the research community. For the cost of about 200WY, JSP will allow the U.S. (and international) astronomical community to manipulate the flagship data sets and undertake innovative science investigations ranging from solar system object characterization, exoplanet detections, nearby galaxy rotation rates and dark matter properties, to epoch of reionization studies. It will also allow for the ultimate constraints on cosmological parameters and the nature of dark energy, with far smaller uncertainties and a better handle on systematics than by any one survey alone. ","Joint Survey Processing of Euclid, Rubin and Roman: Final Report"
135,1298685732966604800,1217638713259438080,John Beacom,"[""We don't know what dark matter is. Usually, we assume this is because it interacts too weakly. But it could also be because it interacts too strongly. In <LINK>, we search for strongly interacting dark matter. We didn't find it, but it could still be out there.""]",https://arxiv.org/abs/2008.10646,"Certain strongly interacting dark matter candidates could have evaded detection, and much work has been done on constraining their parameter space. Recently, it was shown theoretically that the scattering cross section for $m_\chi \gtrsim 1$ GeV pointlike dark matter with a nucleus cannot be significantly larger than the geometric cross section of the nucleus. This realization closes the parameter space for pointlike strongly interacting dark matter. However, strongly interacting dark matter is still theoretically possible for composite particles, with much parameter space open. We set new, wide-ranging limits based on data from a novel detector at the University of Chicago. Backgrounds are greatly suppressed by requiring coincidence detection between two spatially separated liquid-scintillator modules. For dark matter ($v \sim 10^{-3}$c), the time of flight would be $\sim 2~\mu{\rm s}$, whereas for cosmic rays, it would be $\sim 2~{\rm ns}$. We outline ways to greatly increase sensitivity at modest costs. ","New Experimental Constraints in a New Landscape for Composite Dark
  Matter"
136,1298639039147446273,1011816374379929600,Nick Young,"['And the preprint is out!\n\nThe physics GRE is supposed to help applicants who might otherwise be missed stand out if they score well. We find that isn’t true. <LINK> \n\n🧵1/5\n#EndTheGRE #academicchatter #phdchat <LINK>', 'We looked at the admissions decisions of over 2500 applicants to 5 physics graduate programs over a 2 year period. If doing well on the physics GRE helped applicants stand out, we would expect applicants with low GPAs or from smaller or less prestigious schools to benefit. 2/5', 'Yet, this wasn’t the case. Having a high physics GRE score but a low GPA did not offer a higher chance of admission than having a low physics GRE score but a high GPA offered. More students fell into the latter group as opposed to the former. 3/5', 'Further, applicants who scored highly on the physics GRE but attended a small or less prestigious university were admitted at similar rates as applicants who scored moderately on the physics GRE but attended larger or more prestigious universities. 4/5', 'So, if you are using the physics GRE to identify dark horse applicants, don’t do that! You’re more likely to exclude applicants who could be successful in grad school than find someone you would have otherwise rejected. 5/5', '@SciWithChelsie Thanks, Chelsie!', '@SJDJ The colors are related to whether an applicant would stand out in the admissions process. Red and lighter shades of gray would be the ""stand outs."" These are just examples based on the results and not specific applicants though.', ""@cmbennett01 Many departments use a cutoff GRE score so even if the applicant has a 4.0, they might not be admitted if they don't reach that cutoff. However, for low GPA applicants, if they score just above the cutoff or highly (&gt;80th percentile), they are admitted at similar rates."", '@cmbennett01 You are correct. There are definitely applicants with lower GPAs and meet the GRE requirement who are admitted. However, they are relatively rare (&lt;10% of applicants). Also, for every 1 of them, there are ~9 applicants with high GPAs but low GREs who are not admitted.', ""@cmbennett01 Yes, it's an advantage in that the applicant makes it past the first cut, but in terms of admission, it's not providing an additional benefit. So it's more of an advantage based on meeting the minimum requirements compared to someone who doesn't."", ""@cmbennett01 To some degree, yes. I would agree with you that there is an advantage, but that's not the type advantage we were interested in."", ""@cmbennett01 Yes, as long as you do well on one of them, you have a good chance of admission. Unfortunately it's impossible fit all the details into the graphic. One of our analyses did treat them as categorical and the other as continuous. Either way, the conclusions were similar.""]",https://arxiv.org/abs/2008.10712,"One argument for keeping the physics GRE is that it can help applicants who might otherwise be missed in the admissions process stand out. In this work, we evaluate whether this claim is supported by physics graduate school admissions decisions. We used admissions data from five PhD-granting physics departments over a 2-year period (N=2537) to see how the fraction of applicants admitted varied based on their physics GRE scores. We compared applicants with low GPAs to applicants with higher GPAs, applicants from large undergraduate universities to applicants from smaller undergraduate universities, and applicants from selective undergraduate institutions to applicants from less selective undergraduate institutions. We also performed a mediation and moderation analysis to provide statistical rigor and to better understand the previous relationships. We find that for applicants who might otherwise have been missed (e.g. have a low GPA or attended a small or less selective school) having a high physics GRE score did not seem to increase the applicant's chances of being admitted to the schools. However, having a low physics GRE score seemed to penalize otherwise competitive applicants. Thus, our work suggests that the physics GRE does not, in fact, help applicants who might otherwise be missed stand out. ","The Physics GRE does not help applicants ""stand out"""
137,1298136113639497729,719928410814955520,Evgenii Zheltonozhskii,"['Self-supervised learning is really hot now. In our new paper (<LINK>) with @ChaimBaskin Alex Bronstein and Avi Mendelson we study self-supervised learning in unsupervised clustering settings. The code is available at <LINK> 1/n', 'We evaluate multiple self-supervised methods, including BigBiGAN (Jeff Donahue and Karen Simonyan), MoCo v2 (@endernewton  Haoqi Fan, @inkynumbers Kaiming He), InfoMin (@YonglongT et al.), SwAV (Mathilde Caron et al.)... 2/n', '...SimCLRv2 (@tingchenai @skornblith @kswersk @mo_norouzi @geoffreyhinton). We also compare to SCAN (@WGansbeke @svandenh1 @stam_g Marc Proesmans, Luc Van Gool) and Self-label (@y_m_asano @chrirupp Andrea Vedaldi) 3/n', 'We show that self-supervised learning is a strong baseline for unsupervised learning. We achieve 39% accuracy on ImageNet with 1000 clusters and 46% with overclustering (1500 clusters). We also evaluate on ObjectNet, and propose clustering as additional benchmarks for SSL. 4/n', 'Finally we raise a number of questions which we hope will be answered in future research. 5/n (n=5)']",https://arxiv.org/abs/2008.10312,"Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available at this https URL ",Self-Supervised Learning for Large-Scale Unsupervised Image Clustering
138,1298103182569209857,630587365,Bill Peebles,"[""We propose a simple approach to disentanglement: make your generative model's Hessian diagonal in its input. Can implement in &lt;10 lines of code.\n\n<LINK>\nProject Page: <LINK>\nECCV 2020 Spotlight\nw/ J Peebles, @junyanz89, Efros and Torralba <LINK>"", ""Basic idea: When we perturb one input z component, we want the *change* in the output to be invariant to other components. This is equivalent to saying we want a diagonal Hessian in z. So we can just add a regularizer that penalizes a generator's off-diagonal Hessian term! 2/7 https://t.co/aonmrvNe7G"", 'This model-agnostic regularizer can get reasonable axis-aligned disentanglement results when applied to GANs, such as ProGAN trained on CLEVR. In comparison, InfoGAN seems to struggle. 3/7 https://t.co/WZd0Abpqm3', 'The Hessian Penalty displays a tendency to *turn-off* extra z components when the latent space is overparameterized. For example, if |z|=12 but your dataset has 1 factor of variation, 11 components get disabled. In contrast, vanilla ProGAN uses all 12 components. 4/7 https://t.co/dusf0oU1ti', 'Finally, it can also be used for unsupervised direction discovery. Can identify BigGAN directions that, e.g., perform object rotation cleaner than past methods. 5/7 https://t.co/g0YBTVAG0y', ""The Hessian Penalty can be efficiently computed by minimizing the variance of Hutchinson's estimator. We have @PyTorch and TensorFlow implementations ready for use at https://t.co/QLJm8D9d2r. Example implementation in six lines of code below. 6/7 https://t.co/2odGzKoWpQ"", 'Big thanks to @pabbeel, Taesung Park and @rzhang88 for very helpful conversations and feedback! 7/7', ""@unsorsodicorda @pabbeel @rzhang88 We've only tried it with GANs so far, but I don't think there's anything preventing it from being applied to arbitrary latent variable generative models (or even discriminative networks).""]",https://arxiv.org/abs/2008.10599,"Existing disentanglement methods for deep generative models rely on hand-picked priors and complex encoder-based architectures. In this paper, we propose the Hessian Penalty, a simple regularization term that encourages the Hessian of a generative model with respect to its input to be diagonal. We introduce a model-agnostic, unbiased stochastic approximation of this term based on Hutchinson's estimator to compute it efficiently during training. Our method can be applied to a wide range of deep generators with just a few lines of code. We show that training with the Hessian Penalty often causes axis-aligned disentanglement to emerge in latent space when applied to ProGAN on several datasets. Additionally, we use our regularization term to identify interpretable directions in BigGAN's latent space in an unsupervised fashion. Finally, we provide empirical evidence that the Hessian Penalty encourages substantial shrinkage when applied to over-parameterized latent spaces. ",The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement
139,1296897463316393984,60801171,Andrey Kurenkov 🇺🇦,"['New paper alert!\n\nHumans find it easy to push around a heap of objects to find a target object, but it would typically take DRL a LONG time to learn this. We show how RL can be made to do this in just 10k env steps!\n\nArxiv: <LINK>\nWebsite: <LINK> <LINK>', 'We combine some previously known tricks in a ~novel~ way and find they combine well to make RL training super fast. Specifically, we combine:\na) task-relevant mid-level representation \nb) teacher guidance \nc) assymetric learning with privileged info \n\nMore explanation in paper! https://t.co/1kqjGeV7LU', 'With Josepth Taglic, Rohun Kulkarni, Marcus Dominguez-Kuhne, @animesh_garg, @RobobertoMM, @silviocinguetta  at @StanfordSVL @StanfordAILab']",https://arxiv.org/abs/2008.06073,"When searching for objects in cluttered environments, it is often necessary to perform complex interactions in order to move occluding objects out of the way and fully reveal the object of interest and make it graspable. Due to the complexity of the physics involved and the lack of accurate models of the clutter, planning and controlling precise predefined interactions with accurate outcome is extremely hard, when not impossible. In problems where accurate (forward) models are lacking, Deep Reinforcement Learning (RL) has shown to be a viable solution to map observations (e.g. images) to good interactions in the form of close-loop visuomotor policies. However, Deep RL is sample inefficient and fails when applied directly to the problem of unoccluding objects based on images. In this work we present a novel Deep RL procedure that combines i) teacher-aided exploration, ii) a critic with privileged information, and iii) mid-level representations, resulting in sample efficient and effective learning for the problem of uncovering a target object occluded by a heap of unknown objects. Our experiments show that our approach trains faster and converges to more efficient uncovering solutions than baselines and ablations, and that our uncovering policies lead to an average improvement in the graspability of the target object, facilitating downstream retrieval applications. ","Visuomotor Mechanical Search: Learning to Retrieve Target Objects in
  Clutter"
140,1296826314712707073,1261860960895004672,Jorge Moreno,"[""It's paper day! We use FIRE to investigate the timescales probed by H-alpha and FUV. We find that whilst the latter is sensitive to recent bursty periods, the former is not. An honour to publish with @astro__jose , such a kind and outstanding scientist!♥️<LINK> <LINK>"", 'Especially thanks to @alexbgurvich  for bringing this work to (almost) the finishing line, and for being such a superb mentor to José. Same goes to @jbprime  and CAFG for believing in him. And many thanks to all the co-authors, including @AstronoMerc_  and @cchayward82.', 'May you Rest In Peace, my dearest José. ♥️', 'And @DarthLazar, @AndrewWetzel, and @martinsparre!', 'https://t.co/DJDM7Fbzjt']",https://arxiv.org/abs/2008.08582,"Understanding the rate at which stars form is central to studies of galaxy formation. Observationally, the star formation rates (SFRs) of galaxies are measured using the luminosity in different frequency bands, often under the assumption of a time-steady SFR in the recent past. We use star formation histories (SFHs) extracted from cosmological simulations of star-forming galaxies from the FIRE project to analyze the time-scales to which the H${\alpha}$ and far-ultraviolet (FUV) continuum SFR indicators are sensitive. In these simulations, the SFRs are highly time variable for all galaxies at high redshift, and continue to be bursty to z=0 in dwarf galaxies. When FIRE SFHs are partitioned into their bursty and time-steady phases, the best-fitting FUV time-scale fluctuates from its ~10 Myr value when the SFR is time-steady to >~100 Myr immediately following particularly extreme bursts of star formation during the bursty phase. On the other hand, the best-fitting averaging time-scale for H${\alpha}$ is generally insensitive to the SFR variability in the FIRE simulations and remains ~5 Myr at all times. These time-scales are shorter than the 100 Myr and 10 Myr time-scales sometimes assumed in the literature for FUV and H${\alpha}$, respectively, because while the FUV emission persists for stellar populations older than 100 Myr, the time-dependent luminosities are strongly dominated by younger stars. Our results confirm that the ratio of SFRs inferred using H${\alpha}$ vs. FUV can be used to probe the burstiness of star formation in galaxies. ","The time-scales probed by star formation rate indicators for realistic,
  bursty star formation histories from the FIRE simulations"
141,1296728099623772162,1064069189143601153,Stephan Rasp,"[""Can data-driven, medium-range weather forecasts beat physical models? In our new paper, we are trying to find out. \n\ntl;dr: Theoretically, yes. Practically, there isn't enough data.\n\nPreprint: <LINK>\nCo-author: @thuereyGroup\n\nThread 👇\n\n1/"", 'Traditionally, medium-range (2d-2w) weather forecasting is done with models based on physics. This works pretty well: https://t.co/l7N3qK96iL\n\nHowever, deep learning has surprised us many times with how good it can be compared to traditional approaches.\n\n2/', 'So naturally people asked the question, based purely on data, how good can neural networks predict weather. \n\nTo find out, we (@PDueben, @wx_jon and S.Scher) designed WeatherBench, a benchmark for data-driven, medium-range weather forecasting. \n\nhttps://t.co/RXrR4wLMVp\n\n3/', 'Here, we train a deep resnet on 40y of ERA5 data but still got overfitting. So we pretrained on 150y of climate model data, all at 5.625 degree resolution (600km).\n\nIn the end, we have similar skill as a physical model run at 210km (IFS T63). Details on comparison in paper.\n\n4/ https://t.co/NRcQCMGWGe', ""Direct models are trained for each forecast time specifically. Continuous models have time as an input (cf. MetNet). \n\nThe third possible approach is to train an iterative model but this is technically much harder, see @wx_jon's latest paper: https://t.co/P4qyBeotAa\n\n5/"", 'Of course, nobody cares about a 600km forecast but the scaling of skill with resolution and network size allows us to (cautiously) extrapolate our results to current operational resolutions (10km).\n\nFindings: Skill improves but more data is probably needed.\n\n6/ https://t.co/zxk2L69MMV', ""It would be very interesting to see how good data-driven models can get when they are run with all the available data at the highest possible resolution (huge technical challenge!!!) but current evidence suggests there won't be enough data to compete with operational models.\n\n7/"", 'As a bonus, we investigate how physical the data-driven models are by looking at saliency maps. \n\nOn average they learn physically plausible correlations but sometimes they also make rather unlikely connections (Hawaii-London in 3d).\n\n8/ https://t.co/mU3UzFjiMx', 'Finally, these results are specific to medium-range forecasting. The potential for AI needs to be assessed for each problem separately:\n\n1) How much data is there?\n2) How good are physical models?\n\nNowcasting with AI e.g. works really well: \n\nhttps://t.co/gSCbm84d6B\n\n9/', ""It seems like the challenge for AI in weather in climate at the moment is finding the right problems. Not every task is suited for a ML approach.\n\nFor a vision on how AI and global NWP can collaborate check out Tim Palmer's essay: https://t.co/tSqQnr2w2q\n\nThe End."", 'Oh yeah. Here is the updated WeatherBench leaderboard on GitHub: https://t.co/hKALQSHfDM https://t.co/9rhHAoUMok', ""@braaannigan Yes, using data more efficiently would certainly be very helpful. Techniques to make the model more physical (eg spherical convolutions) might help. \n\nIt's not as easy though as in MetNet where you can train on small patches. For global NWP you need a wide field of view.""]",http://arxiv.org/abs/2008.08626,"Numerical weather prediction has traditionally been based on physical models of the atmosphere. Recently, however, the rise of deep learning has created increased interest in purely data-driven medium-range weather forecasting with first studies exploring the feasibility of such an approach. To accelerate progress in this area, the WeatherBench benchmark challenge was defined. Here, we train a deep residual convolutional neural network (Resnet) to predict geopotential, temperature and precipitation at 5.625 degree resolution up to 5 days ahead. To avoid overfitting and improve forecast skill, we pretrain the model using historical climate model output before fine-tuning on reanalysis data. The resulting forecasts outperform previous submissions to WeatherBench and are comparable in skill to a physical baseline at similar resolution. We also analyze how the neural network creates its predictions and find that, with some exceptions, it is compatible with physical reasoning. Finally, we perform scaling experiments to estimate the potential skill of data-driven approaches at higher resolutions. ","Data-driven medium-range weather prediction with a Resnet pretrained on
  climate simulations: A new model for WeatherBench"
142,1296004210178363398,880514663959715840,Kirsty Wan 🔬,"['How does a #Chlamydomonas cell control the synchrony of its two #cilia? We propose one such mechanism <LINK> \n- by controlling the strength of basal elastic coupling relative to filament activity! #algae\n\nA really fun collaboration with the Kanso lab (USC)! 😎 <LINK>', 'p.s. we kept disagreeing about which direction is in-phase which is anti-phase so in the end went for the safe swimming analogy instead 🤷🏊']",https://arxiv.org/abs/2008.07626,"Beating flagella exhibit a variety of synchronization modes. This synchrony has long been attributed to hydrodynamic coupling between the flagella. However, recent work with flagellated algae indicates that a mechanism internal to the cell, through the contractile fibres connecting the flagella basal bodies, must be at play to actively modulate flagellar synchrony. Exactly how basal coupling mediates flagellar coordination remains unclear. Here, we examine the role of basal coupling in the synchronization of the model biflagellate \textit{Chlamydomonas reinhardtii} using a series of mathematical models of decreasing level of complexity. We report that basal coupling is sufficient to achieve inphase, antiphase, and bistable synchrony, even in the absence of hydrodynamic coupling and flagellar compliance. These modes can be reached by modulating the activity level of the individual flagella or the strength of the basal coupling. We observe a slip mode when allowing for differential flagellar activity, just as in experiments with live cells. We introduce a dimensionless ratio of flagellar activity to basal coupling that is predictive of the mode of synchrony. This ratio allows us to query biological parameters which are not yet directly measurable experimentally. Our work shows a concrete route for cells to actively control the synchronization of their flagella. ",Intracellular coupling modulates biflagellar synchrony
143,1295704844616040448,1038513938600796160,Thomas Feuillen,"['Here is a new short preprint: One Bit to Rule Them All (<LINK>, #ITWIST) where we study the reconstruction of 1-bit quantized data using a 1-bit version of the Back-projection operator (hence the title :) ). Thanks to @jacquesdurden, Mike Davies and @vdd_ucl_1 <LINK>']",http://arxiv.org/abs/2008.07264,"This work focuses on the reconstruction of sparse signals from their 1-bit measurements. The context is the one of 1-bit compressive sensing where the measurements amount to quantizing (dithered) random projections. Our main contribution shows that, in addition to the measurement process, we can additionally reconstruct the signal with a binarization of the sensing matrix. This binary representation of both the measurements and sensing matrix can dramatically simplify the hardware architecture on embedded systems, enabling cheaper and more power efficient alternatives. Within this framework, given a sensing matrix respecting the restricted isometry property (RIP), we prove that for any sparse signal the quantized projected back-projection (QPBP) algorithm achieves a reconstruction error decaying like O(m-1/2)when the number of measurements m increases. Simulations highlight the practicality of the developed scheme for different sensing scenarios, including random partial Fourier sensing. ","One Bit to Rule Them All : Binarizing the Reconstruction in 1-bit
  Compressive Sensing"
144,1295586948019417090,712851889793183745,Yun S. Song,"['Interested in non-parametric two-sample tests that can detect more general distributional shifts than just changes in the mean? In this paper we propose a family of such tests.  Sec 5 discusses several applications, including detecting changes in variance. <LINK>', 'A Mathematica implementation can be found here: \nhttps://t.co/7tFQsptJJT\nWe hope to implement a Python or an R version soon.']",https://arxiv.org/abs/2008.06664,"Random divisions of an interval arise in various context, including statistics, physics, and geometric analysis. For testing the uniformity of a random partition of the unit interval $[0,1]$ into $k$ disjoint subintervals of size $(S_k[1],\ldots,S_k[k])$, Greenwood (1946) suggested using the squared $\ell_2$-norm of this size vector as a test statistic, prompting a number of subsequent studies. Despite much progress on understanding its power and asymptotic properties, attempts to find its exact distribution have succeeded so far for only small values of $k$. Here, we develop an efficient method to compute the distribution of the Greenwood statistic and more general spacing-statistics for an arbitrary value of $k$. Specifically, we consider random divisions of $\{1,2,\dots,n\}$ into $k$ subsets of consecutive integers and study $\|S_{n,k}\|^p_{p,w}$, the $p$th power of the weighted $\ell_p$-norm of the subset size vector $S_{n,k}=(S_{n,k}[1],\ldots,S_{n,k}[k])$ for arbitrary weights $w=(w_1,\ldots,w_k)$. We present an exact and quickly computable formula for its moments, as well as a simple algorithm to accurately reconstruct a probability distribution using the moment sequence. We also study various scaling limits, one of which corresponds to the Greenwood statistic in the case of $p=2$ and $w=(1,\ldots,1)$, and this connection allows us to obtain information about regularity, monotonicity and local behavior of its distribution. Lastly, we devise a new family of non-parametric tests using $\|S_{n,k}\|^p_{p,w}$ and demonstrate that they exhibit substantially improved power for a large class of alternatives, compared to existing popular methods such as the Kolmogorov-Smirnov, Cramer-von Mises, and Mann-Whitney/Wilcoxon rank-sum tests. ",Generalized Spacing-Statistics and a New Family of Non-Parametric Tests
145,1295299001961545729,738769492122214400,Johannes Lischner,"['Twisted double bilayer #graphene consisting of two AB bilayers exhibits flat bands that are tunable by electric fields. In our new paper, we study the atomic and electronic structure of stacked AA bilayers and combinations of AA and AB bilayers: <LINK> <LINK>']",https://arxiv.org/abs/2008.05269,"Twisted double bilayer graphene has recently emerged as an interesting moir\'e material that exhibits strong correlation phenomena that are tunable by an applied electric field. Here we study the atomic and electronic properties of three different graphene double bilayers: double bilayers composed of two AB stacked bilayers (AB/AB), double bilayers composed of two AA stacked bilayers (AA/AA) as well as heterosystems composed of one AB and one AA bilayer (AB/AA). The atomic structure is determined using classical force fields. We find that the inner layers of the double bilayer exhibit significant in-plane and out-of-plane relaxations, similar to twisted bilayer graphene. The relaxations of the outer layers depend on the stacking: atoms in AB bilayers follow the relaxations of the inner layers, while atoms in AA bilayers attempt to avoid higher-energy AA stacking. For the relaxed structures, we calculate the electronic band structures using the tight-binding method. All double bilayers exhibit flat bands at small twist angles, but the shape of the bands depends sensitively on the stacking of the outer layers. To gain further insight, we study the evolution of the band structure as the outer layers are rigidly moved away from the inner layers, while preserving their atomic relaxations. This reveals that the hybridization with the outer layers results in an additional flattening of the inner-layer flat band manifold. Our results establish AA/AA and AB/AA twisted double bilayers as interesting moir\'e materials with different flat band physics compared to the widely studied AB/AB system. ","Effect of bilayer stacking on the atomic and electronic structure of
  twisted double bilayer graphene"
146,1294311608227856385,56113666,Mengye Ren,"[""Towards more interactive #selfdriving, we propose a new motion forecasting network based on the transformer architecture to explicitly model interaction among actors. Check out our recent IROS'20 paper, available on arXiv:\n<LINK>\n\n#SelfDrivingCars @uber @uberatg <LINK>"", 'Joint work with Lingyun (Luke) Li, Bin Yang, Ming Liang @zengwenyuan1995  @seanseg  @RaquelUrtasun']",https://arxiv.org/abs/2008.05927,"In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: ATG4D and nuScenes. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors. ","End-to-end Contextual Perception and Prediction with Interaction
  Transformer"
147,1293898416049848325,1102500807679729664,Gaurav Tomar,"['Now for the first time, you can study the dark matter of any spin in dark matter direct detection experiments. We publish this work today on the arXiv. So for what spin of dark matter you want to study? 😀<LINK> @quirkyquark @soumyasa89']",https://arxiv.org/abs/2008.05120,"We introduce a systematic approach to characterize the most general non-relativistic WIMP-nucleus interaction allowed by Galilean invariance for a WIMP of arbitrary spin $j_\chi$ in the approximation of one-nucleon currents. Five nucleon currents arise from the nonrelativistic limit of the free nucleon Dirac bilinears. Our procedure consists in (1) organizing the WIMP currents according to the rank of the $2 j_\chi+1$ irreducible operator products of up to $2 j_\chi$ WIMP spin vectors, and (2) coupling each of the WIMP currents to each of the five nucleon currents. The transferred momentum $q$ appears to a power fixed by rotational invariance. For a WIMP of spin $j_\chi$ we find a basis of 4+20$j_\chi$ independent operators that exhaust all the possible operators that drive elastic WIMP-nucleus scattering in the approximation of one-nucleon currents. By comparing our operator basis, which is complete, to the operators already introduced in the literature we show that some of the latter for $j_\chi=1$ were not independent and some were missing. We provide explicit formulas for the squared scattering amplitudes in terms of the nuclear response functions, which are available in the literature for most of the targets used in WIMP direct detection experiments. ",The effective theory of nuclear scattering for a WIMP of arbitrary spin
148,1293707130126704641,1556664198,Kyle Cranmer,"[""Very excited to share our newest paper \ncombining machine learning &amp; physics. We develop normalizing flows that impose the elaborate of symmetry groups you find in fundamental particle physics. \nIt's a beautiful mix of math, ML, and physics\n<LINK> <LINK>"", 'It took several months to work through a series of challenges. Shout out to the team: \nDenis Boyda, Gurtej Kanwar, @sracaniere, @DaniloJRezende,\n@msalbergo, @kylecranmer, Daniel Hackett, Phiala Shanahan\nhttps://t.co/1keqEEEYM3 https://t.co/7O1dqsdRYs', 'This extends our previous work &amp; was much more involved. The goal as before is to dramatically speed up ""lattice techniques"", which is a powerful computational approach to studying fundamental physics. Here’s a twitter thread on our last paper:\nhttps://t.co/oJSKVHy2TQ', ""In this setting we are trying to model a quantum mechanical field in a discretized space-time lattice. Here's an example from Lattice Quantum Chromodynamics. This animation is just 1 sample from the 4D distribution (where time is used for 1 dimension). Think of a 4-d image. https://t.co/WTq1QOBQY2"", 'We don\'t want just a single ""image"" (lattice configuration), we want to sample the high-dimensional distribution of configurations predicted by a fundamental theory. That distribution is intractable. Typically people use Hamiltonian Monte Carlo for this, but it has limitations.', 'Part of the trick is that the high dimensional configurations have many redundant directions related to what is called a Gauge Symmetry. Gauge symmetries are at the heart of quantum field theory. Thanks Emmy Noether!\nhttps://t.co/IQd9MjCPQx\nhttps://t.co/w8li7B7nZx', 'We can call those redundant degrees of freedom ""pure gauge"" degrees of freedom xΩ and the physically meaningful ""gauge invariant"" degrees of freedom x/Ω. Then we know the true distribution should factorize like this:\np(x/Ω, xΩ) = p(x/Ω)p(xΩ) = p(x/Ω) x const. https://t.co/IXmpRFNRGD', 'However, standard normalizing flows don\'t work well for this problem. With local gauge invariance, one can ""rotate"" at every site on the lattice. The symmetry group is huge!\nSo we want to build this into the network.', 'In our last paper we figured out how to do this with a simpler group called U(1) — the group of complex numbers of the form exp(iθ) — which is equivalent to a circle.\nhttps://t.co/FmHOuqmIqT', 'But to describe the weak and strong fundamental forces we need to be able to extend the idea to the Lie groups SU(2) and SU(3).  This takes us into some beautiful group theory, which we have to realize in concrete numerical form that plays well with automatic differentiation https://t.co/drvPdgkOHt', 'SU(N) is the name for the special unitary group of degree N — it is the group of NxN unitary matrices with determinant 1. @johncarlosbaez recently had a nice thread about its geometric manifestation\nhttps://t.co/ZBkwiOAAch', 'We do this by modeling elements of SU(N) in terms of their eigenvalues &amp; location on the maximal torus. In addition to the nice smooth structure, there is also a discrete Weyl group, which is related to permutations of the eigenvalues. Respecting this discrete symmetry is harder https://t.co/WeWm94sxoD', 'We developed an algorithm that works for any SU(N). Below is an image of a slice through the 8 dimensional torus that describes elements of SU(9). Lines of zero density correspond to locations where the slice crosses through walls of the cells (where density goes to zero). https://t.co/uUdTP0qRRz', 'I’ve learned a ton and really enjoyed this project. I’ve been blown away repeatedly by my collaborators intuition and ingenuity. The amazing Gurtej Kanwar will be talking about this today/tomorrow (Thursday, Aug 13) at 10AM Eastern Time: \nhttps://t.co/ohoNKngzeq https://t.co/ewjj5ystNg', 'thank you for coming to my Ted talk.', 'And see the fantastic thread from @DaniloJRezende on the same paper \nhttps://t.co/DWI97K97Sj https://t.co/sGseYVy5FS']",https://arxiv.org/abs/2008.05456,We develop a flow-based sampling algorithm for $SU(N)$ lattice gauge theories that is gauge-invariant by construction. Our key contribution is constructing a class of flows on an $SU(N)$ variable (or on a $U(N)$ variable by a simple alternative) that respect matrix conjugation symmetry. We apply this technique to sample distributions of single $SU(N)$ variables and to construct flow-based samplers for $SU(2)$ and $SU(3)$ lattice gauge theory in two dimensions. ,Sampling using $SU(N)$ gauge equivariant flows
149,1293619124472430592,2715793145,Tirtha Banerjee,"['In meteorology &amp; engineering, we often measure turbulent fluxes. But how do they originate? What causes the different transfer efficiencies between heat &amp; momentum fluxes? In our new preprint led by @Subharthi10 we study this fundamental question-read here\n<LINK> <LINK>']",https://arxiv.org/abs/2008.04420,"The characterization of heat and momentum fluxes in wall-bounded turbulence is of paramount importance for a plethora of applications, ranging from engineering to Earth sciences. However, how the turbulent structures associated with velocity and temperature fluctuations interact to produce the emergent flux signatures, is not evident till date. In this work, we investigate this fundamental issue by studying the switching patterns of intermittently occurring turbulent fluctuations from one state to another, a phenomenon called persistence. We discover that the persistence patterns for heat and momentum fluxes are widely different. Moreover, we uncover power-law scaling and length scales of turbulent motions that cause this behavior. Furthermore, by separating the phases and amplitudes of flux events, we explain the origin and differences between heat and momentum transfer efficiencies in convective turbulence. Our findings provide new understanding on the connection between flow organization and flux generation mechanisms, two cornerstones of turbulence research. ","Persistence behaviour of heat and momentum fluxes in convective surface
  layer turbulence"
150,1293222604891983873,46032384,Yea Seul Kim,"['We’re excited to share our preprint: Bayesian-Assisted Inference from Visualized Data! (w/ @JessicaHullman, Paula Kayongo, Madeleine Grunde-McLaughlin) Our large preregistered study explored using ppl’s prior beliefs to put data in perspective <LINK> Thread!(1/N)', 'We design and evaluate two techniques driven by Bayesian inference to guide data interpretation. Assume a person w/ some beliefs about a parameter, like what % of ppl in a population have a disease or believe a certain thing. First, we elicit their prior graphically. (2/N) https://t.co/vHUCqILVAX', 'One approach we call an ‘uncertainty analogy’ numerically relates uncertainty about the parameter implied by new data to one’s subjective uncertainty as described by their prior beliefs. The user sees the analogy to help them understand the new data. (3/N) https://t.co/VBNZGdgYKE', 'The second is a ‘posterior visualization’ which prescribes how a user should update their beliefs given their prior beliefs &amp; the data. The user views the new data alongside a visualization of the predicted posterior distribution We wondered, will ppl follow this advice? (4/N) https://t.co/OYxdJMHoTT', 'We evaluate by eliciting posterior beliefs &amp; comparing to normative Bayesian posterior (how they should update if they take data at face value &amp; use Bayes’ rule). We compared to showing data as a point estimate and a shaded probability interval (uncertainty visualization). (5/N) https://t.co/xowPdotcTm', 'Of course, sometimes it’s not rational to take data at face value. So we varied how ‘trustworthy’ the data seemed - national health statistics vs coverage on late-term abortion views by a media company Both inspired by real online data scenarios. (6/N)', 'We also varied sample size, wondering if the techniques help more with small (N=~150) vs large (N=~5200) samples. Our prior work in bayesian cognition finds differences in how ‘Bayesian’ ppl appear to update from vis based on sample size https://t.co/pGyusGo4Kv (7/N)', ""In a pre-registered experiment (N=4800), we find that when a newly observed data sample is small (N=158), both techniques slightly improved people's updating on average compared to point estimates or current `best practice’, visualizing uncertainty in the new data. (8/N)"", 'For large samples, where updating deviates more from normative Bayesian theory, how effective the two techniques are depends on how trustworthy ppl think the data is. For health statistics still helpful. For abortion views, no longer beneficial over visualizing uncertainty. (9/N)', 'When it comes to understanding updating, our data suggests people are not bad at figuring out the right location (eg mean) their posterior beliefs should have but much worse at calibrating their remaining uncertainty about the parameter like a rational Bayesian would. (10/N)', 'We also wondered about the effect of prior elicitation itself. Could having to state a prior be helpful for guiding ppl to be more sensitive to uncertainty? We systematically varied prior elicitation &amp; find in an aggregate level analysis evidence that it alone may help. (11/N)', 'Plenty of follow up work to do! We’re excited to better model ‘pseudo-Bayesian’ updating among non-expert vis users, &amp; to identify other theoretically-driven ways that visualization systems could guide users toward better grasping how informative new data samples are. (12/12)']",https://arxiv.org/abs/2008.00142,"A Bayesian view of data interpretation suggests that a visualization user should update their existing beliefs about a parameter's value in accordance with the amount of information about the parameter value captured by the new observations. Extending recent work applying Bayesian models to understand and evaluate belief updating from visualizations, we show how the predictions of Bayesian inference can be used to guide more rational belief updating. We design a Bayesian inference-assisted uncertainty analogy that numerically relates uncertainty in observed data to the user's subjective uncertainty, and a posterior visualization that prescribes how a user should update their beliefs given their prior beliefs and the observed data. In a pre-registered experiment on 4,800 people, we find that when a newly observed data sample is relatively small (N=158), both techniques reliably improve people's Bayesian updating on average compared to the current best practice of visualizing uncertainty in the observed data. For large data samples (N=5208), where people's updated beliefs tend to deviate more strongly from the prescriptions of a Bayesian model, we find evidence that the effectiveness of the two forms of Bayesian assistance may depend on people's proclivity toward trusting the source of the data. We discuss how our results provide insight into individual processes of belief updating and subjective uncertainty, and how understanding these aspects of interpretation paves the way for more sophisticated interactive visualizations for analysis and communication. ",Bayesian-Assisted Inference from Visualized Data
151,1292869310529101830,787741117538267137,Shahnawaz Ahmed,"['Quantum state tomography (QST) with conditional generative adversarial networks (CGAN): \n<LINK>\n<LINK>\n\n(1/6) We show how two duelling neural networks conditioned on data reconstruct measurement statistics and thereby find the density matrix. <LINK>', '(2/6) Standard neural network output is converted to a valid density matrix using custom neural network layers. Moving away from a sampling-based ansatz such as Restricted Boltzmann Machine, we show that the direct map from data to density matrix can be learned.', '(3/6) G and D networks are optimised using gradient descent till G reproduces statistics indistinguishable by D from data. Learning a better loss function via D, in addition to simple metrics such as L1 gives excellent results for reconstruction of optical quantum states.', '(4/6) We show how simple cat states can be reconstructed with 100x fewer updates and 10x less data using Husimi Q sample. We also apply the method on a real experiment and show that even single-shot learning (with pre-training) is possible.', '(5/6) More results and comparisons in a follow up paper. Hoping that this leads to interesting ways how standard deep neural networks can be applied in quantum info. I will add all the code to reproduce the paper soon - starting with the benchmark iMLE method.', '(6/6) Adding the following two benchmarks now:\n\nReconstruction of a cat state from 1024 Husimi Q function samples (32 x 32 grid):\nhttps://t.co/qQeF44utV3\n\nHow photon counting can get fidelity ~0.99 faster using less displacements:\nhttps://t.co/2M2T0K4vdO', '@threadreaderapp unroll']",https://arxiv.org/abs/2008.03240,"Quantum state tomography (QST) is a challenging task in intermediate-scale quantum devices. Here, we apply conditional generative adversarial networks (CGANs) to QST. In the CGAN framework, two duelling neural networks, a generator and a discriminator, learn multi-modal models from data. We augment a CGAN with custom neural-network layers that enable conversion of output from any standard neural network into a physical density matrix. To reconstruct the density matrix, the generator and discriminator networks train each other on data using standard gradient-based methods. We demonstrate that our QST-CGAN reconstructs optical quantum states with high fidelity orders of magnitude faster, and from less data, than a standard maximum-likelihood method. We also show that the QST-CGAN can reconstruct a quantum state in a single evaluation of the generator network if it has been pre-trained on similar quantum states. ","Quantum State Tomography with Conditional Generative Adversarial
  Networks"
152,1292619564686155776,809197519288803330,hollis 🪐,"['super happy to announce that my first first-authored science paper is now up on the arXiv! in it, we study star formation in dwarf galaxies that orbit MW-like galaxies. i’m super proud of this work—summary below 👇\n<LINK>', 'galaxies come in all shapes and sizes, but those nearest to us are all dwarf galaxies (&lt; 1/100th the MW mass) orbiting the MW as satellites. these galaxies are distinct from isolated dwarfs—in particular, few are actively forming stars and few have any gas', 'we call these old, passive galaxies “quenched” since some process has seemingly removed their reservoir of gas (the fuel for star formation!) https://t.co/QNtxAuzMSu', 'our goal in this project was to figure out what process(es) quench dwarf satellites galaxies. to do that, we simulate 4 MW-like galaxies and their surrounding environment. together, these 4 simulations provide a sample in good agreement with the MW and other nearby galaxies https://t.co/rdzLQD4Wva', 'in particular, we find that a lot of our simulated dwarf galaxies are quenched by z=0! the lowest mass ones (&lt; 10^8 suns) are almost all quenched, while higher mas ones (&gt; 10^8 suns) are not, likely because the process that quenched them is more efficient at low masses', 'to figure out what happened to our simulated dwarf galaxies, we look back in time and study *when* these galaxies quenched! we find that all but the least massive satellites quenched right around their time of “infall,” i.e. when they became satellites of the larger host galaxy https://t.co/fTlwybGTKm', 'several of these galaxies show signatures of a gas-removal process called ram pressure stripping, in which the hot gas in the “halo” surrounding the host galaxy pushes out the cold gas from the infalling satellite, creating “trails” of stripped gas https://t.co/M5oFnMHKmv', 'overall, this work reaffirms past work on the quenching of dwarf satellite galaxies, provides context for upcoming observations of dwarf satellites, and lays the groundwork for future work examining this sample of simulated satellites and their histories in detail!', 'huge thanks to Prof. Charlotte Christensen at Grinnell and the @theNbodyShop team for making this work possible!! ive had a great time working on this project and learned a ton along the way 😊']",https://arxiv.org/abs/2008.02805,"Observations of the low-mass satellites in the Local Group have shown high fractions of gas-poor, quiescent galaxies relative to isolated dwarfs, implying that the host halo environment plays an important role in the quenching of dwarf galaxies. In this work, we present measurements of the quenched fractions and quenching timescales of dwarf satellite galaxies in the DC Justice League suite of 4 high-resolution cosmological zoom-in simulations of Milky Way-mass halos. We show that these simulations accurately reproduce the satellite luminosity functions of observed nearby galaxies, as well as the variation in satellite quenched fractions from $M_* \sim 10^5$ solar masses to $10^{10}$ solar masses. We then trace the histories of satellite galaxies back to $z \sim 15$, and find that many satellites with $M_* \sim 10^{6-8}$ solar masses quench within 2 Gyr of infall into the host halo, while others in the same mass range remain star-forming for as long as 5 Gyr. We show that this scatter can be explained by the satellite's gas mass and the ram pressure it feels at infall. Finally, we identify a characteristic stellar mass scale of $10^8$ solar masses above which infalling satellites are largely resistant to rapid environmental quenching. ",Quenching timescales of dwarf satellites around Milky Way-mass hosts
153,1292036642237296641,109571291,Daniel González,"['Our last paper on quantum simulations of high-energy physics is now on arxiv: \n\n<LINK>\n\nIn this work, we propose how certain open questions in particle physics could be adressed using cold-atom experiments. <LINK>', 'First of all, I would like to thank the Madriz-Barna-Munich collaboration. I really enjoyed working with them on such an interdisciplinary topic, where I even got to learn a bit about QCD. \n\n@adauphin4 @ICFOnians @unicomplutense @LMU_Muenchen  @MCQST_cluster https://t.co/RKgthXJcKS', ""But let's get down to business. It is a known fact that quarks, the building blocks of matter, do not appear as independent particles in normal conditions. They are usually confined together with other quarks forming composite objects, such as protons or neutrons. https://t.co/nVPPPz4OtO"", 'I say in normal conditions because in extreme situations, such as inside the dense core of neutron stars, or just a few moments after the Big Bang, when the temperature of the universe was huge, quarks can deconfined and form a new phase of matter called the quark-gluon plasma. https://t.co/Gl4L0L06VF', 'The existance of this exotic phase has been confirmed experimentally in particle accelerators involving the collision of heavy ions, which reproduce the temperature and density conditions of the early universe. However, there are still many unsolved questions to this problem. https://t.co/COEs8qdSHn', 'This is because, despite the huge success of the Standard Model explaining the properties of fundamental particles (think about the Higgs boson, predicted almost 50 years before its descovery), solving the corresponding equations is actually hard. Very hard. https://t.co/Xqm701iwnH', 'It is so hard that even the best supercomputers are unable to answer the following question: should we think about the change between confined and deconfined quarks as a process where the protons and neutron melt, similar to a solid to liquid transition? https://t.co/4d30JI51aC', 'And this is where cold atoms enter the story. In order to understand processes at the highest possible temperatures we want to use one of the coldest objects in the universe. https://t.co/Hug51XoWqX', 'Cold atoms are gases that can be trapped and cooled dawn in the lab to almost zero degrees using lasers and magnetic fields. In that regime, one can engineer these atomic systems to mimic the properties of any other quantum system, including fundamental particles such as quarks. https://t.co/IMXAjzxGXS', 'This is the essence of a quantum simulator, that can be thought of as a specific purpose quantum computer: using a controllable quantum setup you can obtain information about more complicated quantum systems, a more efficient approach than ussing a classical computer. https://t.co/WONW8WuEqt', 'As you can imagine, these experiments are not simple. Despite the huge technical progress in the recent years, we are still far from being able to quantum simulate the full Standard Model. But again, there is no fundamental reason that prevents it, so we remain optimistic. https://t.co/TYNp4Eqt3m', 'In the meantime, we propose an alternative approach: instead of simulating the full problem, we consider simpler models that capture some of the most interesting properties of quarks, simple enough so they can be investigated with current quantum simulators. https://t.co/4Ijmh2ndcV', 'In particular, we study a one-dimensional model where the interactions between particles describing (fermionic) matter are mediated by other (bosonic) particles. https://t.co/HSZuQ4uCvZ', 'The situation is similar in particle physics, where the quarks interact by interchanging gluons. This interaction is simpler in our model, allowing for its quantum simulation using current technology. We propose how to do it with atomic mixtures 👇🏽 https://t.co/YvZyeGEz30', 'Despite the differences, we observe many similarities with the physics of quarks: \n\n(1) We find particles whose charge is a fraction of the proton charge, similar to quarks.\n\n(2) In a certain regime, these quark-like particles are confined forming composite objects. https://t.co/3hcC9Mekjj', '(3) There is a phase transition to a deconfined regime where these particles are now independent and free.\n\n(4) This transition is associated to the restoration of a so-called chiral symmetry. https://t.co/CiNjXPDyJm', 'The last point is relevant since the breaking of chiral symmetry contributes to the mass of particles, similar to the Higgs mechanism, and it is believed to play a role in confinement. However, whether these two occur simultaneously or not is still debated. https://t.co/XIdP2VoVBA', 'In summary, we hope to learn more about the physics of fundamental particles by performing quantum simulations on simpler models, where qualitatively similar phenomena can be studied to a much greater detail. https://t.co/PbuxqOGo4i']",https://arxiv.org/abs/2008.02045,"Understanding the nature of confinement, as well as its relation with the spontaneous breaking of chiral symmetry, remains one of the long-standing questions in high-energy physics. The difficulty of this task stems from the limitations of current analytical and numerical techniques to address non-perturbative phenomena in non-Abelian gauge theories. In this work, we show how similar phenomena emerge in simpler models, and how these can be further investigated using state-of-the-art cold-atom quantum simulators. More specifically, we introduce the rotor Jackiw-Rebbi model, a (1+1)-dimensional quantum field theory where interactions between Dirac fermions are mediated by quantum rotors. Starting from a mixture of ultracold atoms in an optical lattice, we show how this quantum field theory emerges in the long-wavelength limit. For a wide and experimentally-relevant parameter regime, the Dirac fermions acquire a dynamical mass via the spontaneous breakdown of chiral symmetry. Moreover, we study the effect of both quantum and thermal fluctuations, which lead to the phenomenon of chiral symmetry restoration. Finally, we uncover a confinement-deconfinement quantum phase transition, where meson-like fermions fractionalise into quark-like quasi-particles bound to topological solitons of the rotor field. The proliferation of these solitons at finite chemical potentials again serves to restore the chiral symmetry, yielding a clear analogy with the quark-gluon plasma in quantum chromodynamics, where this symmetry coexists with the deconfined fractional charges. Our results show how the interplay between these phenomena could be analyse in realistic atomic experiments. ","The rotor Jackiw-Rebbi model: a cold-atom approach to chiral symmetry
  restoration and quark confinement"
154,1291565580827336705,2577596593,Chelsea Finn,"['Want your robot to explore intelligently? We study how to learn to explore &amp; introduce a *efficient* meta-learning method that can lead to optimal exploration.\n\nPaper: <LINK>\nw Evan Liu, Raghunathan, Liang @StanfordAILab\n\nThread👇🏼(1/5)\n<LINK>', 'Prior meta-RL methods either (a) optimize exploration &amp; execution end-to-end w.r.t. reward (e.g. RL^2, VariBAD), or (b) leverage principled but suboptimal strategies (e.g. PEARL).\n\nThe former is particularly hard, as it leads to a chicken-and-egg optimization problem.\n(2/5) https://t.co/0fge7AGySO', 'Turns out you can break this coupling by training a task-conditioned execution policy, and training the exploration policy to recover task-relevant information. \n\nThis is consistent with the end-to-end objective *and* substantially more efficient!\n(3/5) https://t.co/b48gB1VgPW', 'With this approach, DREAM can learn an exploration strategy that navigates a 3D environment from pixels to go “read” a sign that carries info about the task. (and then execute the task using that info)\n(4/5) https://t.co/4teggu5vXP', 'In comparison with state-of-the-art meta-RL methods, this approach can better scale to challenging meta-RL problems such as 3D visual object navigation.\n\nSee the paper for more experiments &amp; theoretical analysis.\n(5/5) https://t.co/HeC6C3JSz2']",https://arxiv.org/abs/2008.02790,"The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM: this https URL ","Decoupling Exploration and Exploitation for Meta-Reinforcement Learning
  without Sacrifices"
155,1291539867890049026,1138255660750127108,Berthold Jaeck,['Check out our new paper on quantum spin liquids: we report that tunneling spectroscopy could be well suited to find evidence for this elusive phase of matter. Great collaboration @PrincetonPhys with Mallika @MIT and my high school friend @ElioKonig @mpifkf <LINK>'],https://arxiv.org/abs/2008.02278,"We examine the spectroscopic signatures of tunneling through a Kitaev quantum spin liquid (QSL) barrier in a number of experimentally relevant geometries. We combine contributions from elastic and inelastic tunneling processes and find that spin-flip scattering at the itinerant spinon modes gives rise to a gaped contribution to the tunneling conductance spectrum. We address the spectral modifications that arise in a magnetic field necessary to drive the candidate material $\alpha$-RuCl$_3$ into a QSL phase, and we propose a lateral 1D tunnel junction as a viable setup in this regime. The characteristic spin gap is an unambiguous signature of the fractionalized QSL excitations, distinguishing it from magnons or phonons. The results of our analysis are generically applicable to a wide variety of topological QSL systems. ",Tunneling spectroscopy of quantum spin liquids
156,1291354490965233665,356645746,Blas Kolic,"['How have our emotions changed throughout the Covid-19 pandemic?\n\nIn our preprint, we find that Twitter users were highly emotional early on and then transitioned to a death-centered analytical discourse.  We model such discourse using psychology and data. <LINK> <LINK>']",https://arxiv.org/abs/2008.00854,"Successful navigation of the Covid-19 pandemic is predicated on public cooperation with safety measures and appropriate perception of risk, in which emotion and attention play important roles. Signatures of public emotion and attention are present in social media data, thus natural language analysis of this text enables near-to-real-time monitoring of indicators of public risk perception. We compare key epidemiological indicators of the progression of the pandemic with indicators of the public perception of the pandemic constructed from ~20 million unique Covid-19-related tweets from 12 countries posted between 10th March -- 14th June 2020. We find evidence of psychophysical numbing: Twitter users increasingly fixate on mortality, but in a decreasingly emotional and increasingly analytic tone. Semantic network analysis based on word co-occurrences reveals changes in the emotional framing of Covid-19 casualties that are consistent with this hypothesis. We also find that the average attention afforded to national Covid-19 mortality rates is modelled accurately with the Weber-Fechner and power law functions of sensory perception. Our parameter estimates for these models are consistent with estimates from psychological experiments, and indicate that users in this dataset exhibit differential sensitivity by country to the national Covid-19 death rates. Our work illustrates the potential utility of social media for monitoring public risk perception and guiding public communication during crisis scenarios. ","Public risk perception and emotion on Twitter during the Covid-19
  pandemic"
157,1291354461701582848,795909263751282688,Jorryt Matthee,"[""Paper day!\n<LINK>\nHere we used the amazing MUSE Integral Field Spectrograph on the VLT to study the Lyman-alpha emission from the galaxy `CR7', which is among the brightest known galaxies in the early Universe.\n1/10"", 'Together with @d_sobral_ and @ssantosPT we discovered CR7 in 2014 in old archival data. It turned out to be quite a spectacular galaxy...\nhttps://t.co/mYDdS277eM\n2/10', 'Back in 2015, it looked like very hot and metal poor stars were dominating the light in the brightest of the three star-forming complexes that together constitute CR7. Others speculated that the intense light may have been powered by a direct collapse black hole.\n3/10 https://t.co/LiQd8lILcc', 'However, thanks to the capabilities of ALMA we detected line emission from carbon atoms throughout the interstellar medium of CR7, suggesting metal enrichment from even earlier generations of stars was already well underway. https://t.co/ScEWN1Y7EN\n\n4/10', 'With these new MUSE observations we focussed on uncovering the origin and extent of the bright Lyman-alpha emission. We knew that extended Lya emission (which traces hydrogen gas around the stars) was present, but now we revealed the shape and the extent in much more detail. https://t.co/IxfII9dKKL', ""It's quite exciting to realise that this means that we are actually seeing the diffuse gas that nurtures the intense bursts of star formation that we witness at a lookback time of more than 13 billion years...\n6/10"", ""It turns out that the Lyman-alpha line profile and `halo' are very similar to other slightly less distant galaxies, except for its exceptional luminosity. This is in agreement with the idea that bright galaxies in the epoch of reionisation reside in large ionised bubbles. https://t.co/nXrEF2EI4v"", 'Despite the exceptionally high Lyman-alpha line luminosity, we argue in the paper that this luminosity can be powered by large amounts of very young (but not necessarily primordial) stars, and no additional ionising mechanism is required.  8/10', 'Bonus: we actually combined all the HST imaging obtained over the years and identified another UV emitting clump right in the middle of the galaxy. In addition, we showed that there is another HII region emitting some very faint Lyman-alpha emission at the edge of the galaxy. https://t.co/ugQuL3eCfH', 'To me, this illustrates that we are currently only seeing the tips of the icebergs of the complex structures that are these distant galaxies. It will be very exciting to zoom in even more with the ELT and to uncover the emission from gas and possibly older stars with JWST! (end)']",https://arxiv.org/abs/2008.01731,"CR7 is among the most luminous Lyman-$\alpha$ emitters (LAEs) known at $z = 6.6$ and consists of at least three UV components that are surrounded by Lyman-$\alpha$ (Ly$\alpha$) emission. Previous studies have suggested that it may host an extreme ionising source. Here, we present deep integral field spectroscopy of CR7 with VLT/MUSE. We measure extended emission with a similar halo scale length as typical LAEs at $z\approx5$. CR7's Ly$\alpha$ halo is clearly elongated along the direction connecting the multiple components, likely tracing the underlying gas distribution. The Ly$\alpha$ emission originates almost exclusively from the brightest UV component, but we also identify a faint kinematically distinct Ly$\alpha$ emitting region nearby a fainter component. Combined with new near-infrared data, the MUSE data show that the rest-frame Ly$\alpha$ equivalent width (EW) is $\approx100$ {\AA}. This is a factor four higher than the EW measured in low-redshift analogues with carefully matched Ly$\alpha$ profiles (and thus arguably HI column density), but this EW can plausibly be explained by star formation. Alternative scenarios requiring AGN powering are also disfavoured by the narrower and steeper Ly$\alpha$ spectrum and much smaller IR to UV ratio compared to obscured AGN in other Ly$\alpha$ blobs. CR7's Ly$\alpha$ emission, while extremely luminous, resembles the emission in more common LAEs at lower redshifts very well and is likely powered by a young metal poor starburst. ","The nature of CR7 revealed with MUSE: a young starburst powering
  extended Lyman-$\alpha$ emission at z=6.6"
158,1291033465975185411,616937421,Yuwen Xiong,"['Can we make unsupervised learning not backprop end-to-end? In our paper <LINK>, we propose LoCo that matches end-to-end BP using SIMCLR, unlocking the potential for model parallelism + memory savings + bio plausibility. @mengyer @RaquelUrtasun  @UberATG  @Uber <LINK>']",https://arxiv.org/abs/2008.01342,"Deep neural nets typically perform end-to-end backpropagation to learn the weights, a procedure that creates synchronization constraints in the weight update step across layers and is not biologically plausible. Recent advances in unsupervised contrastive representation learning point to the question of whether a learning algorithm can also be made local, that is, the updates of lower layers do not directly depend on the computation of upper layers. While Greedy InfoMax separately learns each block with a local objective, we found that it consistently hurts readout accuracy in state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective as well as gradient isolation. In this work, we discover that by overlapping local blocks stacking on top of each other, we effectively increase the decoder depth and allow upper blocks to implicitly send feedbacks to lower blocks. This simple design closes the performance gap between local learning and end-to-end contrastive learning algorithms for the first time. Aside from standard ImageNet experiments, we also show results on complex downstream tasks such as object detection and instance segmentation directly using readout features. ",LoCo: Local Contrastive Representation Learning
159,1290890767523811329,1055358835,M.Bülent Sarıyıldız,"['📢 Delighted to share our ECCV2020 paper: <LINK>.\nWe propose image-conditioned masked language modeling (ICMLM) to learn visual representations using text (no bounding boxes, just image-caption pairs!)\n@dlarlus @perezjln @naverlabseurope <LINK>']",https://arxiv.org/abs/2008.01392,"Pretraining general-purpose visual features has become a crucial part of tackling many computer vision tasks. While one can learn such features on the extensively-annotated ImageNet dataset, recent approaches have looked at ways to allow for noisy, fewer, or even no annotations to perform such pretraining. Starting from the observation that captioned images are easily crawlable, we argue that this overlooked source of information can be exploited to supervise the training of visual representations. To do so, motivated by the recent progresses in language models, we introduce {\em image-conditioned masked language modeling} (ICMLM) -- a proxy task to learn visual representations over image-caption pairs. ICMLM consists in predicting masked words in captions by relying on visual cues. To tackle this task, we propose hybrid models, with dedicated visual and textual encoders, and we show that the visual representations learned as a by-product of solving this task transfer well to a variety of target tasks. Our experiments confirm that image captions can be leveraged to inject global and localized semantic information into visual representations. Project website: this https URL ",Learning Visual Representations with Caption Annotations
160,1290824171698524161,1570476014,Yi-Hsuan Yang,"[""Jazz Transformer paper &amp; code (ISMIR'20) out!\n```we deploy Transformers to  compose Jazz melody+chord+structure all at once, but it does not work very well; so we propose objective metrics to examine why```\n<LINK>\n<LINK> \n<LINK> <LINK>""]",https://arxiv.org/abs/2008.01307,"This paper presents the Jazz Transformer, a generative model that utilizes a neural sequence model called the Transformer-XL for modeling lead sheets of Jazz music. Moreover, the model endeavors to incorporate structural events present in the Weimar Jazz Database (WJazzD) for inducing structures in the generated music. While we are able to reduce the training loss to a low value, our listening test suggests however a clear gap between the average ratings of the generated and real compositions. We therefore go one step further and conduct a series of computational analysis of the generated compositions from different perspectives. This includes analyzing the statistics of the pitch class, grooving, and chord progression, assessing the structureness of the music with the help of the fitness scape plot, and evaluating the model's understanding of Jazz music through a MIREX-like continuation prediction task. Our work presents in an analytical manner why machine-generated music to date still falls short of the artwork of humanity, and sets some goals for future work on automatic composition to further pursue. ","The Jazz Transformer on the Front Line: Exploring the Shortcomings of
  AI-composed Music through Quantitative Measures"
161,1290541721676214272,2324423269,Peyman 𝕄𝕀𝕃𝔸ℕ𝔽𝔸ℝ,"['We study a compression framework that considers the interplay between rate, distortion and classification accuracy. Optimizing the quantization tables in JPEG  yields a nice boost in performance using an easily-implemented modification of these tables. \n\n<LINK> <LINK>', '@HadiAmirpour For this paper, we used PSNR only since it is the standard and a full-reference distortion that is easy to compute. Other perceptual measures are very appropriate indeed, but in practice rather more difficult to optimize against, especially since they are typically ""no-reference""']",https://arxiv.org/abs/2008.00605,"Handling digital images is almost always accompanied by a lossy compression in order to facilitate efficient transmission and storage. This introduces an unavoidable tension between the allocated bit-budget (rate) and the faithfulness of the resulting image to the original one (distortion). An additional complicating consideration is the effect of the compression on recognition performance by given classifiers (accuracy). This work aims to explore this rate-distortion-accuracy tradeoff. As a case study, we focus on the design of the quantization tables in the JPEG compression standard. We offer a novel optimal tuning of these tables via continuous optimization, leveraging a differential implementation of both the JPEG encoder-decoder and an entropy estimator. This enables us to offer a unified framework that considers the interplay between rate, distortion and classification accuracy. In all these fronts, we report a substantial boost in performance by a simple and easily implemented modification of these tables. ",The Rate-Distortion-Accuracy Tradeoff: JPEG Case Study
162,1299765511660724224,1014263782493818880,Pierre Ablin,"['[Preprint]\n\nWe propose SMICA, an ICA algorithm based on frequential diversity with a noise model for M/EEG processing.\n\nIt is unusual to have a noise model in ICA, but it brings many benefits 👇👇👇\n\nWith @agramfort and JF Cardoso\n\nPaper: <LINK>\n\n1/7 <LINK>', 'For SMICA, data x is modeled as a linear combination of sources s and some noise n:\n\nx = As + n,\n\nA is the mixing matrix.\n\nFirst benefit of the noise model: the likelihood of the model is not degenerate even when # sources &lt; # sensors !\n\n2/7', 'In usual ICA, if you have 100 sensors and want  10 sources, you have to do PCA to reduce the data dimension to 10, and then fit ICA. This destroys signals of low power which might still be useful.\n\nWith SMICA, you can estimate 10 sources straight from your 100 sensors  😃\n\n3/7', 'Another benefit of the noise model is the ability to have fine source estimation.\n\nIn standard ICA, sources are estimated as x = A^-1 s. \n\n4/7', 'With SMICA, if  there is a lot of noise on one sensor, the contribution of that sensor will be shrunk when estimating sources thanks to Wiener filtering:\n\nhttps://t.co/9eaVniKmKF\n\n5/7', ""But if noise modelling is so great in ICA, why isn't it used everywhere?\n\nThe problem is that it is much harder to fit than noiseless ICA, because the non-Gaussian ICA model with noise does not have a tractable likelihood. There is no practical algorithm in this case.\n\n6/7"", 'SMICA solves this by working in the spectral domain, offering a closed form likelihood, and simple parameter estimation with the EM algorithm 😃\n\n7/7', '@CaballeroGaudes @agramfort I guess that in this case instead of spectral ICA you can use non-stationnary ICA, where the sources are assumed non-stationnary instead of spectrally diverse. It leads to the same algorithm, without going in the Fourier domain.']",https://arxiv.org/abs/2008.09693,"Background: Independent Component Analysis (ICA) is a widespread tool for exploration and denoising of electroencephalography (EEG) or magnetoencephalography (MEG) signals. In its most common formulation, ICA assumes that the signal matrix is a noiseless linear mixture of independent sources that are assumed non-Gaussian. A limitation is that it enforces to estimate as many sources as sensors or to rely on a detrimental PCA step. Methods: We present the Spectral Matching ICA (SMICA) model. Signals are modelled as a linear mixing of independent sources corrupted by additive noise, where sources and the noise are stationary Gaussian time series. Thanks to the Gaussian assumption, the negative log-likelihood has a simple expression as a sum of divergences between the empirical spectral covariance matrices of the signals and those predicted by the model. The model parameters can then be estimated by the expectation-maximization (EM) algorithm. Results: Experiments on phantom MEG datasets show that SMICA can recover dipole locations more precisely than usual ICA algorithms or Maxwell filtering when the dipole amplitude is low. Experiments on EEG datasets show that SMICA identifies a source subspace which contains sources that have less pairwise mutual information, and are better explained by the projection of a single dipole on the scalp. Comparison with existing methods: Noiseless ICA models lead to degenerate likelihood when there are fewer sources than sensors, while SMICA succeeds without resorting to prior dimension reduction. Conclusions: SMICA is a promising alternative to other noiseless ICA models based on non-Gaussian assumptions. ","Spectral independent component analysis with noise modeling for M/EEG
  source separation"
163,1297957447294947329,850415526602059777,Vikram Dwarkadas,"[""No mid-life crisis! Contrary to  Dittmann et al.(2014, ApJ, 788, 38), we (<LINK>) do not find 3x increase in X-ray flux from SN 1970G at 40+ years - no newly forming PWN. Thanks to the excellent efforts of my Master's student V. Ramakrishnan, now PhD at Purdue. <LINK>""]",https://arxiv.org/abs/2008.09137,"Core-collapse supernovae (SNe) expand into a medium created by winds from the pre-SN progenitor. The SN explosion and resulting shock wave(s) heat up the surrounding plasma, giving rise to thermal X-ray emission, which depends on the density of the emitting material. Tracking the variation of the X-ray luminosity over long periods of time thus allows for investigation of the kinematics of the SN shock waves, the structure of the surrounding medium, and the nature of the progenitor star. In this paper X-ray observations of five of the oldest known X-ray supernovae - SN 1970G, SN 1968D, SN 1959D, SN 1957D and SN 1941C - are analyzed, with the aim of reconstructing their light curves over several decades. For those supernovae for which we can extract multi-epoch data, the X-ray luminosity appears to decline with time, although with large error bars. No increase in the X-ray emission from SN 1970G is found at later epochs, contrary to previous reports. All five SNe show X-ray luminosities that are of comparable magnitude. We compare the late-time X-ray luminosities of these SNe to those of supernova remnants (SNRs) in the Galaxy which are a few hundred years old, and find that when the tentative decline is taken into account, the luminosity of the old SNe studied herein could fall below the luminosity of some of the younger SNRs within a few hundred years. However, the X-ray luminosity should begin to increase as the SNe expand in the Sedov phase, thus reaching that of the observed SNRs. ","From Supernova to Remnant: Tracking the Evolution of the Oldest Known
  X-ray Supernovae"
164,1296467440340271106,998503348369272832,Monika Schnitzer🇺🇦🇪🇺,"['1/ Does Germany’s temporary VAT reduction as part of 2020 stimulus lead to lower prices for consumers? In a new study, we estimate pass-through rates in the fuel market. \n<LINK>\nWe find pass-through is fast, but incomplete. Diesel drivers benefit most.\nIn graphs:', '2/ We show the actual evolution of gasoline and diesel prices, along with counterfactuals under zero and full pass-through.\nPass-through for drivers with gasoline engines is only around 40% https://t.co/rsvlu9ffsU', '3/ while pass-through for diesel drivers is more than 80% https://t.co/hnMrvx5IkO', '4/ Key advantage of fuel market to learn about effects of VAT changes: \nPrice changes are costless and happen frequently. So menu costs should not prohibit pass-through.\nYet, despite costless price changes in the fuel market, VAT pass-through is incomplete.', '5/ We currently investigate the determinants for VAT pass-through: @f_montag, Alina Sagimuldina, @MonikaSchnitzer \nPreliminary results indicate that pass-through is higher when consumers are more inclined to shop for lower prices. So stay tuned!', '6/ We use data on the universe of price changes at fuel stations in Germany and France in June and July 2020 and estimate a difference-in-differences model. \nWe contribute evidence to different topics in the literature:', '7/ Most importantly, we contribute to\n- How effective is 2020 temporary VAT reduction as unconventional fiscal policy - D’Acunto, Hoang, Weber via @voxeu https://t.co/VZYjoJoNVM\n- Understanding when firms pass on VAT changes – Benzarti, Carloni, @TuomasKoson   @jarkko_harju', '@marcusroller @voxeu @TuomasKoson @jarkko_harju We are working on it, more to follow']",https://arxiv.org/abs/2008.08511,"This paper provides the first estimates of the pass-through rate of the ongoing temporary value-added tax (VAT) reduction, which is part of the German fiscal response to COVID-19. Using a unique dataset containing the universe of price changes at fuel stations in Germany and France in June and July 2020, we employ a difference-in-differences strategy and find that pass-through is fast and substantial but remains incomplete for all fuel types. Furthermore, we find a high degree of heterogeneity between the pass-through estimates for different fuel types. Our results are consistent with the interpretation that pass-through rates are higher for customer groups who are more likely to exert competitive pressure by shopping for lower prices. Our results have important implications for the effectiveness of the stimulus measure and the cost-effective design of unconventional fiscal policy. ","Are temporary value-added tax reductions passed on to consumers?
  Evidence from Germany's stimulus"
165,1295997372204548098,1311352436,Andrew Sellek,"['Excited to have my latest paper out on arXiv today: <LINK>\nWe use dust evolution models to reinterpret recent observations of protoplanetary disc accretion rates and disc masses in Lupus. We find that radial drift lowers masses rapidly (top right) and helps.. 1/3 <LINK>', 'Adding internal photoevaporation (EUV pictured) helps lower the accretion rates as discs disperse, ensuring that the full observed range of accretion rates at any mass is covered. 3/3 https://t.co/cjk0WB8uoW']",https://arxiv.org/abs/2008.07530,"Recent observations have uncovered a correlation between the accretion rates (measured from the UV continuum excess) of protoplanetary discs and their masses inferred from observations of the sub-mm continuum. While viscous evolution models predict such a correlation, the predicted values are in tension with data obtained from the Lupus and Upper Scorpius star forming regions; for example, they underpredict the scatter in accretion rates, particularly in older regions. Here we argue that since the sub-mm observations trace the discs' dust, by explicitly modelling the dust grain growth, evolution, and emission, we can better understand the correlation. We show that for turbulent viscosities with $\alpha \lesssim 10^{-3}$, the depletion of dust from the disc due to radial drift means we can reproduce the range of masses and accretion rates seen in the Lupus and Upper Sco datasets. One consequence of this model is that the upper locus of accretion rates at a given dust mass does not evolve with the age of the region. Moreover, we find that internal photoevaporation is necessary to produce the lowest accretion rates observed. In order to replicate the correct dust masses at the time of disc dispersal, we favour relatively low photoevaporation rates $\lesssim 10^{-9}~M_{\odot}~\mathrm{yr^{-1}}$ for most sources but cannot discriminate between EUV or X-ray driven winds. A limited number of sources, particularly in Lupus, are shown to have higher masses than predicted by our models which may be evidence for variations in the properties of the dust or dust trapping induced in substructures. ","A dusty origin for the correlation between protoplanetary disc accretion
  rates and dust masses"
166,1293305776606330880,349172730,Ranjay Krishna,"[""If you're releasing a new user-facing AI project/product, you might want to read our new #CSCW2020 paper. We find that words or metaphors used to describe AI agents have a causal effect on users' intention to adopt your agent. <LINK> Thread👇"", 'Conceptual metaphors are one of the most common and powerful means that a designer has to influence user expectations. They have been traditionally used by designers to convey functionality. Ex, ""recycling bin"" is for unwanted files, and ""notepad"" is for taking notes. https://t.co/9V3BuWQUuI', 'Metaphors have also been used for sense-making: understanding how existing AI systems work using informal, intuitive folk theories. For example, people explain Google Search as a ""robotic nose"" and YouTube\'s recommendations as a ""drug dealer"". https://t.co/PSi1o2Bvvl', 'In our study, participants do a task with an agent described using various metaphors. Following Psych literature, we categorize metaphors along dimensions of competence and warmth: ""toddler"" projects low competence, high warmth, and ""inexperienced teen”-low competence, low warmth https://t.co/ut5frOCaUh', ""Contrary to how today's AI products are advertised, people are more likely to adopt an agent that they originally expected to have low competence but outperforms that expectation. They are less forgiving of mistakes made by agents they expect to have high competence. https://t.co/RHTSBk8IF4"", 'Meanwhile, the opposite is true for warmth. People are more likely to cooperate with agents that project high warmth. Also, people spend significantly more time interacting with high warmth agents. https://t.co/lLoFFcRAr3', 'Our work provides another lens explaining why some functionally-similar agents get adopted (Xiaoice ""sympathetic ear"") while others with high competence (Mitsuku ""record-breaking Turing Test winner"") or low warmth (Tay ""AI fam that\'s got no chill"") elicit anti-social behavior. https://t.co/SdWbNW2drA', 'This is work done with my amazing collaborators: @pranavkhadpe @drfeifei  Jeff Hancock and @msbernst']",https://arxiv.org/abs/2008.02311,"With the emergence of conversational artificial intelligence (AI) agents, it is important to understand the mechanisms that influence users' experiences of these agents. We study a common tool in the designer's toolkit: conceptual metaphors. Metaphors can present an agent as akin to a wry teenager, a toddler, or an experienced butler. How might a choice of metaphor influence our experience of the AI agent? Sampling metaphors along the dimensions of warmth and competence---defined by psychological theories as the primary axes of variation for human social perception---we perform a study (N=260) where we manipulate the metaphor, but not the behavior, of a Wizard-of-Oz conversational agent. Following the experience, participants are surveyed about their intention to use the agent, their desire to cooperate with the agent, and the agent's usability. Contrary to the current tendency of designers to use high competence metaphors to describe AI products, we find that metaphors that signal low competence lead to better evaluations of the agent than metaphors that signal high competence. This effect persists despite both high and low competence agents featuring human-level performance and the wizards being blind to condition. A second study confirms that intention to adopt decreases rapidly as competence projected by the metaphor increases. In a third study, we assess effects of metaphor choices on potential users' desire to try out the system and find that users are drawn to systems that project higher competence and warmth. These results suggest that projecting competence may help attract new users, but those users may discard the agent unless it can quickly correct with a lower competence metaphor. We close with a retrospective analysis that finds similar patterns between metaphors and user attitudes towards past conversational agents such as Xiaoice, Replika, Woebot, Mitsuku, and Tay. ",Conceptual Metaphors Impact Perceptions of Human-AI Collaboration
