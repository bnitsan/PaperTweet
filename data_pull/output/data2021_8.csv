,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1435962936527364097,4483190122,Payel Das,"['Happy to share new EMNLP 2021 paper on RL for generating text and knowledge base using pretrained T5 language model (<LINK>), by  Pierre Dognin, Inkit Padhi, Igor Melnyk and myself @IBMResearch. And, here is the review from @Synced_Global <LINK>']",https://arxiv.org/abs/2108.12472,"Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. ","ReGen: Reinforcement Learning for Text and Knowledge Base Generation
  using Pretrained Language Models"
1,1435588732061724673,2226279236,Weiyan Shi,"['Have you wondered if your name\'s been used to train GPT2? How to train high-utility privacy-preserving language models üßê? Check out our new paper on ""Selective Differential Privacy""! @Zhou_Yu_AI @ruoxijia \n\n#NLProc  \nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2108.12944,"With the increasing adoption of language models in applications involving sensitive data, it has become crucial to protect these models from leaking private information. Previous work has attempted to tackle this challenge by training RNN-based language models with differential privacy guarantees. However, applying classical differential privacy to language models leads to poor model performance as the underlying privacy notion is over-pessimistic and provides undifferentiated protection for all tokens of the data. Given that the private information in natural language is sparse (for example, the bulk of an email might not carry personally identifiable information), we propose a new privacy notion, selective differential privacy, to provide rigorous privacy guarantees on the sensitive portion of the data to improve model utility. To realize such a new notion, we develop a corresponding privacy mechanism, Selective-DPSGD, for RNN-based language models. Besides language modeling, we also apply the method to a more concrete application -- dialog systems. Experiments on both language modeling and dialog system building show that the proposed privacy-preserving mechanism achieves better utilities while remaining safe under various privacy attacks compared to the baselines. The data, code and models are available at this https URL ",Selective Differential Privacy for Language Modeling
2,1435417799665537026,16614440,Jeremy Foote,"['üö® New paperüö®\nThis was fun to work on. @sohwng and I argue that small communities deserve more attention,  so we interviewed users in small subreddits.\n\nSee the thread from @sohwng (and read the whole paper at <LINK>) but I wanted to highlight two findings: <LINK>', '1. We found that small communities help to partition the information space - giving people control of what they see and _who_ they interact with.', ""2. This one is what we *didn't* find. I thought we'd find that people participated in small communities because it was easier to make friends. But that was really rare - even in communities of a few hundred people, people didn't really know each other personally (or seek to)."", 'There was still a sense of belonging, of being part of a tribe and of sharing personal experiences; these just didn\'t often lead to one-to-one-relationships. @sohwng called this being interested in ""the personal but not the person""', ""There's a lot more to the paper, and @sohwng or I would love to talk more about it."", ""Finally, @sohwng was an amazing first author and the driver of this project. She is doing lots of other amazing work that you'll be hearing about soon. You should definitely follow her here and on Google Scholar, and you should read and cite our paper! :)""]",https://arxiv.org/abs/2108.04282,"Many benefits of online communities---such as obtaining new information, opportunities, and social connections---increase with size. Thus, a ``successful'' online community often evokes an image of hundreds of thousands of users, and practitioners and researchers alike have sought to devise methods to achieve growth and thereby, success. On the other hand, small online communities exist in droves and many persist in their smallness over time. Turning to the highly popular discussion website Reddit, which is made up of hundreds of thousands of communities, we conducted a qualitative interview study examining how and why people participate in these persistently small communities, in order to understand why these communities exist when popular approaches would assume them to be failures. Drawing from twenty interviews, this paper makes several contributions: we describe how small communities provide unique informational and interactional spaces for participants, who are drawn by the hyperspecific aspects of the community; we find that small communities do not promote strong dyadic interpersonal relationships but rather promote group-based identity; and we highlight how participation in small communities is part of a broader, ongoing strategy to curate participants' online experience. We argue that online communities can be seen as nested niches: parts of an embedded, complex, symbiotic socio-informational ecosystem. We suggest ways that social computing research could benefit from more deliberate considerations of interdependence between diverse scales of online community sizes. ",Why do people participate in small online communities?
3,1435290715186159622,790289877388505089,Jeff Hyde,"['New paper* w/ undergrad Matthew Saveliev, who stuck with this project admirably throughout the past crazy year. <LINK>\n*actually ancient history from last week, but I was preoccupied w/ start of classes. (1/N)', 'We took a fresh look at the possibility of high-energy neutrino signals from decay or annihilation of dark matter particles captured within Earth. Some older papers look at this, and there are newer ones addressing things like the anomalous ANITA events, but...', '...we wanted a straightforward way to connect IceCube event rates to assumptions about DM properties &amp; existing constraints on that. e.g. how do limits on the DM-nucleon cross section relate to expected rate of neutrinos from annihilation?', 'First, some cool physics that happens here: tau neutrino regeneration. For ~PeV tau neutrinos, the mean free path is much shorter than Earth‚Äôs radius. But CC interactions produce a tau lepton, which quickly decays and gives us another (secondary) tau neutrino.', 'Above 10 PeV or so, the dominant energy loss mechanism becomes EM interactions of these tau leptons w/ matter. In fact, even taus that are orders of magnitude more energetic are quickly reduced down to ~PeV, which IceCube can detect.', 'We used a numerical simulation of tau neutrino propagation / regeneration to find a convenient approx distribution for emerging nu-taus. Along with other estimates of DM capture and decay/annihilation rates, we related model parameters to event rates.', 'Decays: The dotted line shows constraints on the DM-nucleon cross section, in comparison w/ the regions of parameter space (above each curve) that would have seen &gt; 10 events from Earth‚Äôs core in the IceCube HESE data set. https://t.co/sKnQa80QbN', 'Annihilations: The dotted line shows constraints on the thermally-averaged annihilation cross section. As w/ the decay plot, above each curve would give &gt; 10 events. https://t.co/Sw5Xt98rVF', 'IceCube would be sensitive to quite small &lt;sigma v&gt; ‚Ä¶ but only if the DM-nucleon cross section is pretty big by the standards of direct detection limits. (The cross section is relevant here because of the capture rate.)', 'So it would be tough to explain any HE neutrino events (not just ANITA) in terms of terrestrial DM annihilation or decay. Unless, of course, DM model-specific details manage to evade the assumptions and/or constraints used here...', '... in that case, the semianalytic estimate we give for event rate would still be useful, probably just with a change to the capture rate.']",https://arxiv.org/abs/2108.13412,"Dark matter particles can be gravitationally trapped by celestial bodies, motivating searches for localized annihilation or decay. If neutrinos are among the decay products, then IceCube and other neutrino observatories could detect them. We investigate this scenario for dark matter particles above $m_{\chi} \gtrsim$ PeV producing tau neutrino signals, using updated modeling of dark matter capture and thermalization. At these energies, tau neutrino regeneration is an important effect during propagation through Earth, allowing detection at distances far longer than one interaction length. We show how large energy loss of tau leptons above $\sim$ PeV drives a wide range of initial energies to the same final energy spectrum of ""secondary"" tau neutrinos at the detector, and we provide an analytic approximation to the numerical results. This effect enables an experiment to constrain decays that occur at very high energies, and we examine the reach of the IceCube high-energy starting event (HESE) sample in the parameter space of trapped dark matter annihilations and decays above PeV. We find that the parameter space probed by IceCube searches would require dark matter cross sections in tension with existing direct-detection bounds. ",Using Secondary Tau Neutrinos to Probe Heavy Dark Matter Decays in Earth
4,1434793954252369921,76887151,Dany Doerr,"['Hey Bioinformatics/Science Twitter! @lescureforever and I have a new paper out on solving the small parsimony problem under a pretty nifty rearrangement distance: <LINK>. Will be presented at #recombcg Sep. 21-24, register 4free here <LINK>']",https://arxiv.org/abs/2108.04297,"Reconstructing ancestral gene orders is an important step towards understanding genome evolution. The Small Parsimony Problem (SPP) has been extensively studied in this regard. The problem aims at finding the gene orders at internal nodes of a given phylogenetic tree such that the overall genome rearrangement distance along the tree branches is minimized. However, this problem is intractable in most genome rearrangement models, especially when gene duplication and loss are considered. In this work, we describe an Integer Linear Program algorithm to solve the SPP for natural genomes, i.e., genomes that contain conserved, unique, and duplicated markers. The evolutionary model that we consider is the DCJ-indel model that includes the Double-Cut and Join rearrangement operation and the insertion and deletion of genome segments. We evaluate our algorithm on simulated data and show that it is able to reconstruct very efficiently and accurately ancestral gene orders in a very comprehensive evolutionary model. ",Small Parsimony for Natural Genomes in the DCJ-Indel Model
5,1433825799929540619,2698179823,Yaron Lipman,"['New paper: Introducing Moser Flows (MFs), a new class of continuous normalizing flows (CNFs) on manifolds based on divergences of neural nets. First generative modeling results on general curved surfaces! \n\nwith Noam Rozen @adityagrover_  @mnick \n\n<LINK>\n\n1/7 <LINK>', '@adityagrover_ @mnick Given two probability densities on a manifold, J. Moser (1965) constructed a flow pushing the first to the second. The flow is defined by a vector field, the divergence of which equals the difference between densities. Here is a 1D example:\n\n2/7 https://t.co/7Rnxmvsuel', 'This motivates MF, a universal approximator, where the difference in the model and prior densities is expressed using the (local, easy to approximate) divergence operator applied directly to a NN. Unlike prior CNF methods, it doesn‚Äôt require ODE solvers during training! \n\n3/7', 'MFs are significantly faster and more accurate at density estimation compared to FFJORD:\n\n4/7 https://t.co/BuXp0czbOP', 'MFs achieve SOTA likelihoods by a large margin on earth science benchmarks with an underlying spherical geometry.\n\n5/7 https://t.co/tq1MLN8sN9', 'MF density estimation and generation over freeform 2D surface.\n\n6/7 https://t.co/RAeKxNWjel', 'We are excited with the application of MFs to new scientific domains with geometric data. Scaling Moser Flow to high dimensional manifolds is an open and interesting future work challenge!\n\n7/7']",https://arxiv.org/abs/2108.08052,"We are interested in learning generative models for complex geometries described via manifolds, such as spheres, tori, and other implicit surfaces. Current extensions of existing (Euclidean) generative models are restricted to specific geometries and typically suffer from high computational costs. We introduce Moser Flow (MF), a new class of generative models within the family of continuous normalizing flows (CNF). MF also produces a CNF via a solution to the change-of-variable formula, however differently from other CNF methods, its model (learned) density is parameterized as the source (prior) density minus the divergence of a neural network (NN). The divergence is a local, linear differential operator, easy to approximate and calculate on manifolds. Therefore, unlike other CNFs, MF does not require invoking or backpropagating through an ODE solver during training. Furthermore, representing the model density explicitly as the divergence of a NN rather than as a solution of an ODE facilitates learning high fidelity densities. Theoretically, we prove that MF constitutes a universal density approximator under suitable assumptions. Empirically, we demonstrate for the first time the use of flow models for sampling from general curved surfaces and achieve significant improvements in density estimation, sample quality, and training complexity over existing CNFs on challenging synthetic geometries and real-world benchmarks from the earth and climate sciences. ",Moser Flow: Divergence-based Generative Modeling on Manifolds
6,1433789954375753731,72781449,Nikos Aletras,['New #EMNLP2021 paper w/ @soon1otis: <LINK>\n\nSimple and neat method for improving explanation faithfulness of transformer models for text clf. The idea is to bring close the attention distribution to salient information (computed w/ TextRank) during training üëá <LINK>'],https://arxiv.org/abs/2108.13759,"Pretrained transformer-based models such as BERT have demonstrated state-of-the-art predictive performance when adapted into a range of natural language processing tasks. An open problem is how to improve the faithfulness of explanations (rationales) for the predictions of these models. In this paper, we hypothesize that salient information extracted a priori from the training data can complement the task-specific information learned by the model during fine-tuning on a downstream task. In this way, we aim to help BERT not to forget assigning importance to informative input tokens when making predictions by proposing SaLoss; an auxiliary loss function for guiding the multi-head attention mechanism during training to be close to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with SaLoss consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and SaLoss models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks. ","Enjoy the Salience: Towards Better Transformer-based Faithful
  Explanations with Word Salience"
7,1433455998086352899,1317208236257312770,Alyx Burns,"['Providing metadata about a visualization might build trust or help readers understand what is shown. But how do you decide what to say without overwhelming people?\n\nCheck out the pre-print of our new @vis4good paper: <LINK>\n\n@thaiqOn @thecindyxiong @nargesmahyar', 'We discuss potential pros and cons of providing different kinds of metadata (very broadly defined) including info on where the data came from, the methods used to clean and analyze data, the visual encoding used, the people who were involved, and the people the vis was made for.']",https://arxiv.org/abs/2108.13270,"Accompanying a data visualization with metadata may benefit readers by facilitating content understanding, strengthening trust, and providing accountability. However, providing this kind of information may also have negative, unintended consequences, such as biasing readers' interpretations, a loss of trust as a result of too much transparency, and the possibility of opening visualization creators with minoritized identities up to undeserved critique. To help future visualization researchers and practitioners decide what kinds of metadata to include, we discuss some of the potential benefits and risks of disclosing five kinds of metadata: metadata about the source of the underlying data; the cleaning and processing conducted; the marks, channels, and other design elements used; the people who directly created the visualization; and the people for whom the visualization was created. We conclude by proposing a few open research questions related to how to communicate metadata about visualizations. ","Making the Invisible Visible: Risks and Benefits of Disclosing Metadata
  in Visualization"
8,1433455787137921028,2391943519,Mycal Tucker,"['New paper! Can we leverage insights from NLP for better emergent communication? In this first step, we build agents that communicate via simplified word embeddings, supporting human-agent interaction and some zero-shot understanding! <LINK>', 'I had lots of fun working on this with Huao Li, Siddarth Agrawal, Dana Hughes, Michal Lewis, Katia Sycara, and @julie_a_shah.', ""And of course I took inspiration from a range of people like @jacobandreas and @rogerlevy and the broader Berkeley MARL group (@EugeneVinitsky, @natashajaques , and more), and even some who don't know me like @aggielaz""]",https://arxiv.org/abs/2108.01828,"Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication. ",Emergent Discrete Communication in Semantic Spaces
9,1433033906559852547,1211580702480748544,Andrea Idini,"['New paper out. Bit controversial, don‚Äôt know how to make a thread for it :P\n<LINK>']",https://arxiv.org/abs/2108.12932,"It is often stated that heavy-ion nucleon knockout reactions are mostly sensitive to the tails of the bound-state wavefunctions. In contrast, (p,2p) and (p,pn) reactions are known to access information on the full overlap functions within the nucleus. We analyze the oxygen isotopic chain and explore the differences between single-particle wave functions generated with potential models, used in the experimental analysis of knockout reactions, and ab initio computations from self-consistent Green's function theory. Contrary to the common belief, we find that not only the tail of the overlap functions, but also their internal part are assessed in both reaction mechanisms, which are crucial to yield accurately determined spectroscopic information. ","Sensitivity of quasi-free reactions on details of the bound-state
  overlap functions"
10,1432990270019227654,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper showing that domain wall encoding not only leads to nicer dynamics for computation, it is also the ‚Äúbest‚Äù you can do in terms of number of qubits under a set of realistic conditions <LINK> written with @QciQuantum <LINK>']",https://arxiv.org/abs/2108.12004,"We analyze the performance of encoding pairwise interactions of higher-than-binary discrete variables (these models are sometimes referred to as discrete quadratic models) into binary variables based on domain walls on one dimensional Ising chains. We discuss how this is relevant to quantum annealing, but also many gate model algorithms such as VQE and QAOA. We theoretically show that for problems of practical interest for quantum computing and assuming only quadratic interactions are available between the binary variables, it is not possible to have a more efficient general encoding in terms of number of binary variables per discrete variable. We furthermore use a D-Wave Advantage 1.1 flux qubit quantum annealing computer to show that the dynamics effectively freeze later for a domain-wall encoding compared to a traditional one-hot encoding. This second result could help explain the dramatic performance improvement of domain wall over one hot which has been seen in a recent experiment on D-Wave hardware. This is an important result because usually problem encoding and the underlying physics are considered separately, our work suggests that considering them together may be a more useful paradigm. We argue that this experimental result is also likely to carry over to a number of other settings, we discuss how this has implications for gate-model and quantum-inspired algorithms. ",Understanding domain-wall encoding theoretically and experimentally
11,1432983298083594243,225753681,David Bartram,"['New paper, debunking the idea that age‚Üíhappiness is ""u-shaped everywhere"" (a result that comes only via misuse of control variables and removal of people 70+).\n\n""Is happiness u-shaped in age everywhere? A methodological reconsideration for Europe""\n\n<LINK>']",https://arxiv.org/abs/2108.13671,"A recent contribution to research on age and well-being (Blanchflower 2021) found that the impact of age on happiness is ""u-shaped"" virtually everywhere: happiness declines towards middle age and subsequently rises, in almost all countries. This paper evaluates that finding for European countries, considering whether it is robust to alternative methodological approaches. The analysis here excludes control variables that are affected by age (noting that those variable are not themselves antecedents of age) and uses data from the entire adult age range (rather than using data only from respondents younger than 70). I also explore the relationship via models that do not impose a quadratic functional form. The paper shows that these alternate approaches do not lead us to perceive a u-shape ""everywhere"": u-shapes are evident for some countries, but for others the pattern is quite different. ","Is happiness u-shaped in age everywhere? A methodological
  reconsideration for Europe"
12,1432969657871384581,2999702157,Anton Ilderton,"[""My new paper on the #Schwinger effect is out on the #arXiv today. I *think* it's quite a nice result. Not sure what the community will think... #lasers #quantum #physics #AcademicTwitter \n\n <LINK>"", 'First responses are in!\nPartner: ""I didn\'t know particles could be diabetic!""', '@DrKate_L TOTES!']",https://arxiv.org/abs/2108.13885,"The production of electron-positron pairs from light is a famous prediction of quantum electrodynamics. Yet it is often emphasised that the number of produced pairs has no physical meaning until the driving electromagnetic fields are switched off, as otherwise its definition is basis-dependent. The common adiabatic definition, in particular, can predict the `creation' of a number of pairs orders of magnitude larger than the final yield. We show here, by clarifying exactly what is being counted, that the adiabatic number of pairs has an unambiguous and physical interpretation. As a result, and perhaps contrary to expectation, the large numbers of pairs seen at non-asymptotic times become, in principle, physically accessible. ",The physics of adiabatic particle number in the Schwinger effect
13,1432967558374772737,1216114532,Adam Carnall,"['New paper this morning on the metal contents of massive galaxies 8 billion years ago. Many thanks to @ESO for the use of their telescopes, and @LeverhulmeTrust for funding my research! <LINK>']",https://arxiv.org/abs/2108.13430,"We present a rest-frame UV-optical stacked spectrum representative of massive quiescent galaxies at $1.0<z<1.3$ with log$(M_*/\rm{M_\odot})>10.8$. The stack is constructed using VANDELS survey data, combined with new KMOS observations. We apply two independent full-spectral-fitting approaches, measuring a total metallicity, [Z/H]=$-0.13\pm0.08$ with Bagpipes, and [Z/H]=$0.04\pm0.14$ with Alf, a fall of $\sim0.2-0.3$ dex compared with the local Universe. We also measure an iron abundance, [Fe/H] =$-0.18\pm0.08$, a fall of $\sim0.15$ dex compared with the the local Universe. We measure the alpha enhancement via the magnesium abundance, obtaining [Mg/Fe]=$0.23\pm$0.12, consistent with similar-mass galaxies in the local Universe, indicating no evolution in the average alpha enhancement of log$(M_*/\rm{M_\odot})=11$ quiescent galaxies over the last $\sim8$ Gyr. This suggests the very high alpha enhancements recently reported for several bright $z\sim1-2$ quiescent galaxies are due to their extreme masses, log$(M_*/\rm{M_\odot})\gtrsim11.5$, rather than being typical of the $z\gtrsim1$ population. The metallicity evolution we observe with redshift (falling [Z/H], [Fe/H], constant [Mg/Fe]) is consistent with recent studies. We recover a mean stellar age of $2.5^{+0.6}_{-0.4}$ Gyr, corresponding to a formation redshift, $z_\rm{form}=2.4^{+0.6}_{-0.3}$. Recent studies have obtained varying average formation redshifts for $z\gtrsim1$ massive quiescent galaxies, and, as these studies report consistent metallicities, we identify different star-formation-history models as the most likely cause. Larger spectroscopic samples from upcoming ground-based instruments will provide precise constraints on ages and metallicities at $z\gtrsim1$. Combining these with precise JWST $z>2$ quiescent-galaxy stellar-mass functions will provide an independent test of formation redshifts derived from spectral fitting. ","The stellar metallicities of massive quiescent galaxies at 1.0 &lt; z &lt; 1.3
  from KMOS+VANDELS"
14,1432925539304714243,873043502791221248,Dustin Wright,"['üì∞ Interested in few-shot learning? #NLProc for misinfo in science? Read our new paper (w/ @IAugenstein) which is accepted to #EMNLP2021 üòÑ\n""Semi-Supervised Exaggeration Detection of Health Science Press Releases""\nüìù: <LINK>\nRead about it below üëá <LINK>', 'Popular journalism has a tendency to misrepresent scientific research. It turns out, the source of this in many cases is press releases written about scientific papers from e.g. university press offices! \nhttps://t.co/Uv5eEXm31K\n#EMNLP2021 #NLProc', 'How can #NLProc help? Data collection is hard, requiring domain expertise. Some data exists, and has been used to train models to predict the causal strength of conclusion sentences e.g. if the conclusion is correlational vs direct causal. \n#EMNLP2021', 'However, no standard benchmark of paired data labeled for exaggeration has been used. Given this, we first curate a test set of paired press releases and abstracts labeled for exaggeration from expert annotated data.\n#EMNLP2021 #NLProc', 'But we have limited training data! For this, we develop a multi-task extension to Pattern Exploiting Training (PET) call MT-PET, which uses complementary cloze-style QA tasks to improve few-shot learning over vanilla PET on exaggeration detection. \n#EMNLP2021 #NLProc https://t.co/rPMdwHAe1u', 'Our experiments demonstrate the effectiveness of MT-PET for scientific exaggeration detection and provide a solid baseline for work on this task with very limited labeled data. We also highlight difficult aspects of the task for our systems.\n#EMNLP2021 #NLProc', 'Paper can be read here: https://t.co/GFZqXo4tPI\nAnd code/data will be made available here: https://t.co/oZMGcncw52\n\nLooking forward to discussing this work at #EMNLP2021 üòÑ']",https://arxiv.org/abs/2108.13493,"Public trust in science depends on honest and factual communication of scientific papers. However, recent studies have demonstrated a tendency of news media to misrepresent scientific papers by exaggerating their findings. Given this, we present a formalization of and study into the problem of exaggeration detection in science communication. While there are an abundance of scientific papers and popular media articles written about them, very rarely do the articles include a direct link to the original paper, making data collection challenging. We address this by curating a set of labeled press release/abstract pairs from existing expert annotated studies on exaggeration in press releases of scientific papers suitable for benchmarking the performance of machine learning models on the task. Using limited data from this and previous studies on exaggeration detection in science, we introduce MT-PET, a multi-task version of Pattern Exploiting Training (PET), which leverages knowledge from complementary cloze-style QA tasks to improve few-shot learning. We demonstrate that MT-PET outperforms PET and supervised learning both when data is limited, as well as when there is an abundance of data for the main task. ",Semi-Supervised Exaggeration Detection of Health Science Press Releases
15,1432880589963763712,4438354094,Tom Wong,"['New paper with @Creighton undergraduate Jacob Rapoza! ""Search by Lackadaisical Quantum Walk with Symmetry Breaking."" Jacob started doing research with me the summer before his freshman year, and he is now a junior. <LINK> <LINK>', 'There\'s something incredibly rewarding about mentoring students in their first research project, to say, ""You\'ve contributed to the body of scientific knowledge!"" Or, ""You\'ve solved a problem that no one else has solved before!"" Or, ""You\'re the world expert on this topic!""']",https://arxiv.org/abs/2108.13856,"The lackadaisical quantum walk is a lazy version of a discrete-time, coined quantum walk, where each vertex has a weighted self-loop that permits the walker to stay put. They have been used to speed up spatial search on a variety of graphs, including periodic lattices, strongly regular graphs, Johnson graphs, and the hypercube. In these prior works, the weights of the self-loops preserved the symmetries of the graphs. In this paper, we show that the self-loops can break all the symmetries of vertex-transitive graphs while providing the same computational speedups. Only the weight of the self-loop at the marked vertex matters, and the remaining self-loop weights can be chosen randomly, as long as they are small compared to the degree of the graph. ",Search by Lackadaisical Quantum Walk with Symmetry Breaking
16,1432868981354864642,1075649842955866114,Luca Cortese,"['New paper by former @ICRAR @UWAresearch @ARC_ASTRO3D PhD student (now @LAM_Marseille) Wenkai Hu, looking at the effect of group environment on atomic hydrogen in galaxies. Decrease in gas content starts at the outskirts of groups. <LINK> <LINK>']",https://arxiv.org/abs/2108.12712,"We apply a spectral stacking technique to Westerbork Synthesis Radio Telescope observations to measure the neutral atomic hydrogen content (HI) of nearby galaxies in and around galaxy groups at $z < 0.11$. Our sample includes 577 optically-selected galaxies (120 isolated galaxies and 457 satellites) covering stellar masses between 10$^{10}$ and 10$^{11.5}$ M$_{\odot}$, cross-matched with Yang's group catalogue, with angular and redshift positions from the Sloan Digital Sky Survey. We find that the satellites in the centres of groups have lower HI masses at fixed stellar mass and morphology (characterised by the inverse concentration index) relative to those at larger radii. These trends persist for satellites in both high-mass ($M_{\rm halo} > 10^{13.5}h^{-1}$M$_{\odot}$) and low-mass ($M_{\rm halo} \leqslant 10^{13.5}h^{-1}$M$_{\odot}$) groups, but disappear if we only consider group members in low local density ($\Sigma <$ 5 gal/Mpc$^{-2}$) environments. Similar trends are found for the specific star formation rate. Interestingly, we find that the radial trends of decreasing HI mass with decreasing group-centric radius extend beyond the group virial radius, as isolated galaxies close to larger groups lack HI compared with those located more than $\sim$3.0 $R_{180}$ away from the center of their nearest group. We also measure these trends in the late-type subsample and obtain similar results. Our results suggest that the HI reservoir of galaxies can be affected before galaxies become group satellites, indicating the existence of pre-processing in the infalling isolated galaxies. ","The Atomic Hydrogen Content of Galaxies as a function of Group-Centric
  Radius"
17,1432720391483572233,972878356319473665,Sophia Economou,['Entangled photon factories! Very excited about our new paper where we design general resource-efficient protocols to produce any photonic graph/cluster state by controlling and pumping quantum emitters <LINK>'],https://arxiv.org/abs/2108.12466,"Multi-photon graph states are a fundamental resource in quantum communication networks, distributed quantum computing, and sensing. These states can in principle be created deterministically from quantum emitters such as optically active quantum dots or defects, atomic systems, or superconducting qubits. However, finding efficient schemes to produce such states has been a long-standing challenge. Here, we present an algorithm that, given a desired multi-photon graph state, determines the minimum number of quantum emitters and precise operation sequences that can produce it. The algorithm itself and the resulting operation sequence both scale polynomially in the size of the photonic graph state, allowing one to obtain efficient schemes to generate graph states containing hundreds or thousands of photons. ","Entangled photon factory: How to generate quantum resource states from a
  minimal number of quantum emitters"
18,1432713466532540419,3433220662,Anthony Bonato,"['Our new paper, Small Number of Communities in Twitter Keyword Networks, is up on arXiv <LINK> We found an emergent property of communities in word co-occurrence networks in tweets. Data from 560K tweets was taken from 700+ Twitter feeds of politicians! <LINK>']",https://arxiv.org/abs/2108.13259,"We investigate networks formed by keywords in tweets and study their community structure. Based on datasets of tweets mined from over seven hundred political figures in the U.S. and Canada, we hypothesize that such Twitter keyword networks exhibit a small number of communities. Our results are further reinforced by considering via so-called pseudo-tweets generated randomly and using AI-based language generation software. We speculate as to the possible origins of the small community hypothesis and further attempts at validating it. ",Small Number of Communities in Twitter Keyword Networks
19,1432631581525614595,735386827578875904,siegfried Vanaverbek,['Today our new paper on turbulence and its impact on episodic accretion in YSO binaries is on archiv: <LINK>'],https://arxiv.org/abs/2108.12328,"We report signatures of episodic accretion in young stellar objects (YSOs) that emerge in protobinary configurations in a gravoturbulent gas collapse. We find in most of these protobinary systems strong accretion bursts between the two companions with a recurrence time-scale of about 1 kyr. The accretion rate onto the secondary star typically exceeds that onto the primary with a peak value of 2 $\times 10^{-2}$ M$_{\odot}$ yr$^{-1}$ for the former and 6 $\times 10^{-3}$ M$_{\odot}$ yr$^{-1}$ for the latter. We propose that the secondary companion which remains more active in its episodes of accretion bursts, especially for the gas cores with subsonic velocity dispersion, may provide observational opportunities to find traces of episodic accretion in the surrounding gas of the embedded YSOs that are in a binary configuration. Also, protostars evolving as single objects in the same environment show fewer accretion bursts and all together a more steady mass growth history. The prestellar cores with subsonic velocity dispersion exhibit an order of magnitude more intense accretion bursts than in the case of cores with supersonic velocity dispersions. The latter shows the formation of some of the protobinaries in which the primary acts as a more actively accreting companion. This can support these binaries to become systems of extreme mass ratio. Moreover, the YSOs in binary configurations with small semi-major axis $a$ $\approx$ 50 au and high mass ratio $q$ > 0.7 support phases of intense episodic accretion. The eccentricity, however, seems to play no significant role in the occurrence of accretion bursts. ",Turbulence and its connection to episodic accretion in binary YSOs
20,1432602596154433542,2322575761,Prof Roberto Trotta,"[""If you were to bet, what odds would you get: \n- against the accelerated expansion of the universe? (A: 1100:1) \n- against the existence of a preferred direction in the expansion? (A: 900:1) \nOur new constraints from supernovae type Ia in today's paper: \n<LINK>"", 'Huge congrats to lead author and @ImperialAstro PhD student Wahid Rahman, and thanks to our peculiar velocity collaborators and experts @SuprantaSB &amp; Mike Hudson!', '@pietro_berkes Consider that the highest level (""decisive"") in the Jeffreys\' scale of evidence is ln(BF)=5, or odds of ~ 150:1. Evidence accumulates linearly in the posterior/prior width for the simpler model (as opposed to exponentially against a it), so 1000:1 is pretty strong IMHO.', ""@pietro_berkes Also, this paper uses a model that's an evolution of the Bayesian hierarchical model we wrote down together when I visited you in Boston, sometimes in the Late Middle Age ü§£"", '@pietro_berkes One problem?! I got half a dozen! Expect an email soon üòÉ', '@pietro_berkes #DreamTeam ü§ì']",https://arxiv.org/abs/2108.12497,"We re-examine the contentious question of constraints on anisotropic expansion from Type Ia supernovae (SNIa) in the light of a novel determination of peculiar velocities, which are crucial to test isotropy with supernovae out to distances $\lesssim 200/h$ Mpc. We re-analyze the Joint Light-Curve Analysis (JLA) Supernovae (SNe) data, improving on previous treatments of peculiar velocity corrections and their uncertainties (both statistical and systematic) by adopting state-of-the-art flow models constrained independently via the 2M$++$ galaxy redshift compilation. We also introduce a novel procedure to account for colour-based selection effects, and adjust the redshift of low-$z$ SNe self-consistently in the light of our improved peculiar velocity model. We adopt the Bayesian hierarchical model \texttt{BAHAMAS} to constrain a dipole in the distance modulus in the context of the $\Lambda$CDM model and the deceleration parameter in a phenomenological Cosmographic expansion. We do not find any evidence for anisotropic expansion, and place a tight upper bound on the amplitude of a dipole, $|D_\mu| < 5.93 \times 10^{-4}$ (95\% credible interval) in a $\Lambda$CDM setting, and $|D_{q_0}| < 6.29 \times 10^{-2}$ in the Cosmographic expansion approach. Using Bayesian model comparison, we obtain posterior odds in excess of 900:1 (640:1) against a constant-in-redshift dipole for $\Lambda$CDM (the Cosmographic expansion). In the isotropic case, an accelerating universe is favoured with odds of $\sim 1100:1$ with respect to a decelerating one. ",New Constraints on Anisotropic Expansion from Supernovae Type Ia
21,1432570469715087360,95872961,Praneet Dutta,"['New paper(from my alma mater in India ). Initial study on oscillating activation functions  that perform competitively on Vision benchmarks against popular activations(Swish, Mish etc). \n<LINK>\n\nGreat feeling to mentor younger students in AI, help them grow :)']",https://arxiv.org/abs/2108.12943,"Convolution neural networks have been successful in solving many socially important and economically significant problems. Their ability to learn complex high-dimensional functions hierarchically can be attributed to the use of nonlinear activation functions. A key discovery that made training deep networks feasible was the adoption of the Rectified Linear Unit (ReLU) activation function to alleviate the vanishing gradient problem caused by using saturating activation functions. Since then many improved variants of the ReLU activation have been proposed. However a majority of activation functions used today are non-oscillatory and monotonically increasing due to their biological plausibility. This paper demonstrates that oscillatory activation functions can improve gradient flow and reduce network size. It is shown that oscillatory activation functions allow neurons to switch classification (sign of output) within the interior of neuronal hyperplane positive and negative half-spaces allowing complex decisions with fewer neurons. A new oscillatory activation function C(z) = z cos z that outperforms Sigmoids, Swish, Mish and ReLU on a variety of architectures and benchmarks is presented. This new activation function allows even single neurons to exhibit nonlinear decision boundaries. This paper presents a single neuron solution to the famous XOR problem. Experimental results indicate that replacing the activation function in the convolutional layers with C(z) significantly improves performance on CIFAR-10, CIFAR-100 and Imagenette. ","Growing Cosine Unit: A Novel Oscillatory Activation Function That Can
  Speedup Training and Reduce Parameters in Convolutional Neural Networks"
22,1432526335193272322,805876560,Jordan Van Nest,"['New paper day! Now on arXiv, my first lead-author paper in collaboration with @fdmtweets ,@anchwr,@MichaelTremmel, Alyson Brooks, Daisuke Nagai, and Thomas Quinn: <LINK>', 'We explore populations of ultra-diffuse galaxies (UDGs) in the Romulus simulations, including how the populations vary with\nUDG definition and viewing orientation. Using a fiducial UDG definition, we see isolated UDGs have a more oblate morphology than their non-UDG counterparts https://t.co/ldVafH1KcH', 'This disparity suggests that the isolated UDGs underwent a unique formation channel. We see that this is indeed the case when looking at their shape histories and spin parameter histories scaled to the time of the galaxy‚Äôs last major merger.', 'This result is in agreement with our previous work  (https://t.co/cGVFoyAMIY) which showed how isolated UDGs form via early, high spin mergers. Since these isolated UDGs are more disky, it follows that their surface brightness profiles (and UDG status) would be rotation dependent', 'For each galaxy, we determine whether or not it is a UDG at 288 positions, approximating the entire range of possibilities in 3D space. This allows us to determine what percent of the time a galaxy would be seen as a UDG if oriented randomly https://t.co/3pzg5x4AEM', 'Using these percentages, we can also create mass functions for UDGs assuming random orientations. In comparing this to a ‚Äòmaximum‚Äô mass function (which includes any galaxy that could be a UDG), we see that in the field we would expect to only observe ~25% of possible UDGs https://t.co/wkngLHvXuy', 'To further complicate things, there is no standard definition for UDG. Different research groups use various criteria on what constitutes ‚Äòlarge enough‚Äô and ‚Äòdim enough‚Äô to be considered ‚Äòultra-diffuse‚Äô, 5 of which we consider in this work.', 'We find that by simply changing the definition, the size of the UDG population in Romulus can change by as much as 45% of the environments population.', 'So why does any of this matter? It turns out, changing how you define your UDG population can remove indications of the underlying formation mechanics. If a definition is too permissive, the resultant UDG population is so large that it is indistinguishable from non-UDGs.', 'For example, if we look at our shape plot using a much less restrictive definition, we see that the isolated UDGs and non-UDGs exhibit the same morphology, erasing the indication of a unique formation channel. https://t.co/i08ovos85E', 'There are a lot of open questions in the field of UDGs, specifically in terms of their formation and evolution. As long as different groups are identifying UDGs in different ways, reaching a cohesive understanding may not be possible.']",http://arxiv.org/abs/2108.12985,"We explore populations of ultra-diffuse galaxies (UDGs) in isolated, satellite, and cluster environments using the Romulus25 and RomulusC simulations, including how the populations vary with UDG definition and viewing orientation. Using a fiducial definition of UDGs, we find that isolated UDGs have notably larger semi-major (b/a) and smaller semi-minor (c/a) axis ratios than their non-UDG counterparts, i.e., they are more oblate, or diskier. This is in line with previous results that adopted the same UDG definition and showed that isolated UDGs form via early, high spin mergers. However, the choice of UDG definition can drastically affect what subset of a dwarf population are classified as UDGs, changing the number of UDGs by up to ~45% of the dwarf population. We also find that a galaxy's classification as a UDG is dependent on its viewing orientation, and this dependence decreases as environmental density increases. Overall, we conclude that some definitions for UDG used in the literature manage to isolate a specific formation mechanism for isolated dwarfs, while less restrictive definitions erase a link to formation mechanism. Thus, how we define UDG populations must be considered if we want to understand the formation and evolution of UDGs. ","What's in a name? Quantifying the interplay between definition,
  orientation and shape of UDGs using the ROMULUS Simulations"
23,1432514860542091268,848540707077865472,Hiroyuki Nakano,"['A new paper from B.H.P.C., based on M. Giesler et al., Phys. Rev. X, 9, 041060 (2019) and P. Mourier et al., Phys. Rev. D, 103, 044054 (2021). <LINK>']",https://arxiv.org/abs/2108.13017,"Ringdown gravitational waves of compact object binaries observed by ground-based gravitational-wave detectors encapsulate rich information to understand remnant objects after the merger and to test general relativity in the strong field. In this work, we investigate the ringdown gravitational waves in detail to better understand their property, assuming that the remnant objects are black holes. For this purpose, we perform numerical simulations of post-merger phase of binary black holes by using the black hole perturbation scheme with the initial data given under the close-limit approximation, and generate data of ringdown gravitational waves with smaller numerical errors than that associated with currently available numerical relativity simulations. Based on the analysis of the data, we propose an orthonormalization of the quasinormal mode functions describing the fundamental tone and overtones to model ringdown gravitational waves. Finally, through some demonstrations of the proposed model, we briefly discuss the prospects for ringdown gravitational-wave data analysis including the overtones of quasinormal modes. ","Fundamental Tone and Overtones of Quasinormal Modes in Ringdown
  Gravitational Waves: A Detailed Study in Black Hole Perturbation"
24,1432451504569528320,19333650,Vedant Chandra,"[""I'm pleased to announce our new paper, one of the first scientific results from the fifth-generation Sloan Digital Sky Survey: A 99-minute Double-lined White Dwarf Binary from SDSS-V <LINK>"", ""Searching for binary white dwarfs has been a central theme of my undergraduate research, and we've developed several tools to help us find these systems. Our pipeline flagged this candidate due to variations in the absorption lines across SDSS-V sub-exposures https://t.co/U6TRXWFw7p"", 'We quickly obtained time-resolved @GeminiObs spectra under the Fast-Turnaround program (shown here), and also got UV fluxes from the @NASAUniverse Swift space observatory. These helped us solve the orbital and stellar parameters of the system https://t.co/sGq0tbW2ZX', ""The upshot: this is a 99-minute WD+WD binary in which both stars are visible on the spectrum. This 'double-lined' or 'SB2' nature is relatively rare (only ~ 20 such WD+WD systems are known), and it allows us to precisely estimate the masses of both WDs https://t.co/nC1xPScWEI"", 'The short period and close distance of 113 pc (from @ESAGaia) imply that this system is a powerful source of millihertz gravitational waves, detectable by future space-based observatories. Due to the precisely determined system parameters, it could even be a verification source. https://t.co/CLPKJMhkwL', ""Gravitational wave emission will cause the system's orbit to shrink over time, and we estimate that the two stars will get close enough to interact and merge within ~ 220 million years ('soon' in astronomical terms...)"", ""Once the stars merge, they will probably create a 'reborn' helium star that will eventually evolve into a single helium WD. There might be a few thermonuclear explosions along the way, but the system is probably not massive enough to produce a Type Ia supernova."", 'This paper went from discovery to publication in exactly four months, which would not have been possible without the fantastic resources @GeminiObs and @NASAUniverse Swift provide for fast-turnaround proposals.', 'Thanks to the entire @sdssurveys @MilkyWayMapper collaboration for supporting this work, and especially my co-authors, some of whom are on Twitter: @hc_hwang @jotajotahermes @evbauer_astro. We look forward to finding more interesting systems in SDSS-V!', '@SuperASASSN the thread you requested!', ""@StellarTayar We tried sketching a rough picture in Section 5, but it's quite uncertain. I think it's plausible the original progenitors were ~ 1-1.5 Msun, and also that the 0.32 Msun WD formed first. Let me know if there's anything else I can clarify!""]",https://arxiv.org/abs/2108.11968,"We report the discovery of SDSS J133725.26+395237.7 (hereafter SDSS J1337+3952), a double-lined white dwarf (WD+WD) binary identified in early data from the fifth generation Sloan Digital Sky Survey (SDSS-V). The double-lined nature of the system enables us to fully determine its orbital and stellar parameters with follow-up Gemini spectroscopy and Swift UVOT ultraviolet fluxes. The system is nearby ($d = 113$ pc), and consists of a $0.51\, M_\odot$ primary and a $0.32\, M_\odot$ secondary. SDSS J1337+3952 is a powerful source of gravitational waves in the millihertz regime, and will be detectable by future space-based interferometers. Due to this gravitational wave emission, the binary orbit will shrink down to the point of interaction in $\approx 220$ Myr. The inferred stellar masses indicate that SDSS J1337+3952 will likely not explode as a Type Ia supernova (SN Ia). Instead, the system will probably merge and evolve into a rapidly rotating helium star, and could produce an under-luminous thermonuclear supernova along the way. The continuing search for similar systems in SDSS-V will grow the statistical sample of double-degenerate binaries across parameter space, constraining models of binary evolution and SNe Ia. ",A 99-minute Double-lined White Dwarf Binary from SDSS-V
25,1432418868224430081,2411222281,Dr./Prof. Meredith MacGregor,['Check out our awesome new paper on the Eta Tel debris disk led by @allisonyyyyy! <LINK>'],https://arxiv.org/abs/2108.11965,"We present far- and near-ultraviolet absorption spectroscopy of the $\sim$23 Myr edge-on debris disk surrounding the A0V star $\eta$ Telescopii, obtained with the Hubble Space Telescope Space Telescope Imaging Spectrograph. We detect absorption lines from C I, C II, O I, Mg II, Al II, Si II, S II, Mn II, Fe II, and marginally N I. The lines show two clear absorption components at $-22.7\pm0.5$ km s$^{-1}$ and $-17.8\pm0.7$ km s$^{-1}$, which we attribute to circumstellar (CS) and interstellar (IS) gas, respectively. CO absorption is not detected, and we find no evidence for star-grazing exocomets. The CS absorption components are blueshifted by $-16.9\pm2.6$ km s$^{-1}$ in the star's reference frame, indicating that they are outflowing in a radiatively driven disk wind. We find that the C/Fe ratio in the $\eta$ Tel CS gas is significantly higher than the solar ratio, as is the case in the $\beta$ Pic and 49 Cet debris disks. Unlike those disks, however, the measured C/O ratio in the $\eta$ Tel CS gas is consistent with the solar value. Our analysis shows that because $\eta$ Tel is an earlier type star than $\beta$ Pic and 49 Cet, with more substantial radiation pressure at the dominant C II transitions, this species cannot bind the CS gas disk to the star as it does for $\beta$ Pic and 49 Cet, resulting in the disk wind. ",A Radiatively Driven Wind from the eta Tel Debris Disk
26,1432397575672508423,1432394050196361220,Bahramy Lab,"['A new paper on ""Pressure-induced collapse of ferromagnetism in nickel"" was posted on arXiv:\n<LINK>']",https://arxiv.org/abs/2108.11977,"Transition metals, Fe, Co and Ni, are the canonical systems for studying the effect of external perturbations on ferromagnetism. Among these, Ni stands out as it undergoes no structural phase transition under pressure. Here we have investigated the long-debated issue of pressure-induced magnetisation drop in Ni from first-principles. Our calculations confirm an abrupt quenching of magnetisation at high pressures, not associated with any structural phase transition. We find that the pressure substantially enhances the crystal field splitting of Ni-$3d$ orbitals, driving the system towards a new metallic phase violating the Stoner Criterion for ferromagnetic ordering. Analysing the charge populations in each spin channel, we show that the next nearest neighbour interactions play a crucial role in quenching ferromagnetic ordering in Ni and materials alike. ",Pressure-induced collapse of ferromagnetism in Nickel
27,1432344702943211521,127070843,Michael Sentef,['New paper (experiment+theory) in which we investigate how real-time oscillations of charge order in a laser-excited charge-density wave material affect the quasiparticle relaxation dynamics. <LINK> @fhi_mpg_de @MPSDHamburg @Stanford <LINK>'],https://arxiv.org/abs/2108.12323,"We present a complementary experimental and theoretical investigation of relaxation dynamics in the charge-density-wave (CDW) system TbTe$_3$ after ultrafast optical excitation. Using time- and angle-resolved photoemission spectroscopy, we observe an unusual transient modulation of the relaxation rates of excited photocarriers. A detailed analysis of the electron self-energy based on a nonequilibrium Green's function formalism reveals that the phase space of electron-electron scattering is critically modulated by the photoinduced collective CDW excitation, providing an intuitive microscopic understanding of the observed dynamics and revealing the impact of the electronic band structure on the self-energy. ","Coherent Modulation of Quasiparticle Scattering Rates in a Photoexcited
  Charge-Density-Wave System"
28,1432340162806116354,889824297409277952,Nikos Kolotouros,"['Check out our new work ""Probablistic Modeling for Human Mesh Recovery"" with @geopavlakos @dineshjayaraman  and @KostasPenn  that will be presented at #ICCV21 !\nPaper: <LINK>\nProject page: <LINK>\nCode:\n<LINK> <LINK>', '3D reconstruction from 2D evidence is inherently ambiguous. In this work we embrace the reconstruction ambiguity and recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. https://t.co/TI0Pe66Zh3', 'Our method is not limited to learning a conditional generative model of 3D poses. We show that our learned distribution is useful for a variety of **downstream tasks**, such as body model fitting or reconstruction from multiple views. https://t.co/cHfkyJZVjC', 'If you want to test our method on random videos from YouTube please check the Colab notebook that we prepared:\nhttps://t.co/l7hHvJeoi6']",http://arxiv.org/abs/2108.11944,"This paper focuses on the problem of 3D human reconstruction from 2D evidence. Although this is an inherently ambiguous problem, the majority of recent works avoid the uncertainty modeling and typically regress a single estimate for a given input. In contrast to that, in this work, we propose to embrace the reconstruction ambiguity and we recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. Our approach is based on the normalizing flows model and offers a series of advantages. For conventional applications, where a single 3D estimate is required, our formulation allows for efficient mode computation. Using the mode leads to performance that is comparable with the state of the art among deterministic unimodal regression models. Simultaneously, since we have access to the likelihood of each sample, we demonstrate that our model is useful in a series of downstream tasks, where we leverage the probabilistic nature of the prediction as a tool for more accurate estimation. These tasks include reconstruction from multiple uncalibrated views, as well as human model fitting, where our model acts as a powerful image-based prior for mesh recovery. Our results validate the importance of probabilistic modeling, and indicate state-of-the-art performance across a variety of settings. Code and models are available at: this https URL ",Probabilistic Modeling for Human Mesh Recovery
29,1432282921914683400,1429088462972411910,Aswin Paul,"[""Take a look at our new paper that studies the utility of 'Active Inference' approach in solving stochastic control problems like the windy grid world task.\narXiv link: <LINK>"", 'Thanks to my supervisors @adeelrazi , @manoj_333 and co-author @nsajidt']",http://arxiv.org/abs/2108.12245,"Active inference has emerged as an alternative approach to control problems given its intuitive (probabilistic) formalism. However, despite its theoretical utility, computational implementations have largely been restricted to low-dimensional, deterministic settings. This paper highlights that this is a consequence of the inability to adequately model stochastic transition dynamics, particularly when an extensive policy (i.e., action trajectory) space must be evaluated during planning. Fortunately, recent advancements propose a modified planning algorithm for finite temporal horizons. We build upon this work to assess the utility of active inference for a stochastic control setting. For this, we simulate the classic windy grid-world task with additional complexities, namely: 1) environment stochasticity; 2) learning of transition dynamics; and 3) partial observability. Our results demonstrate the advantage of using active inference, compared to reinforcement learning, in both deterministic and stochastic settings. ",Active Inference for Stochastic Control
30,1432260914095460352,1326269340493246467,Rico Landman,"['Happy to announce the publication of my first master project in @SPIEtweets JATIS on a new approach towards data-driven predictive control for high-contrast imaging. Find the paper on <LINK> or <LINK>. <LINK>', 'Our approach for predictive control is based on Reinforcement Learning, a subfield of Machine Learning that takes inspiration from the way humans and animals learn. The general idea is that the adaptive optics system learns to optimize a ‚Äúreward‚Äù through trial and error.', 'This can be framed into a mathematical optimization problem. We can then try to estimate the solution using numerical optimization methods, such as gradient descent. https://t.co/PpTQbD4jH4', 'We show that our approach can lead to &gt;order of magnitude improvement close to the star in a simulated adaptive optics system, even under non-stationary wind speeds and directions. https://t.co/rtMXovY5T8', 'We also validate our control algorithm in the lab for tip-tilt control and effectively show that this can remove vibrations from the system. https://t.co/uAAX9s6euC', 'Our approach allows for a flexible framework to optimize nonlinear data-driven controllers. In principle, it is also possible to feed it with information from multiple sensors and directly optimize for nonlinear objectives, e.g. the contrast in the focal plane.']",https://arxiv.org/abs/2108.11332,"Current and future high-contrast imaging instruments require extreme adaptive optics (XAO) systems to reach contrasts necessary to directly image exoplanets. Telescope vibrations and the temporal error induced by the latency of the control loop limit the performance of these systems. One way to reduce these effects is to use predictive control. We describe how model-free Reinforcement Learning can be used to optimize a Recurrent Neural Network controller for closed-loop predictive control. First, we verify our proposed approach for tip-tilt control in simulations and a lab setup. The results show that this algorithm can effectively learn to mitigate vibrations and reduce the residuals for power-law input turbulence as compared to an optimal gain integrator. We also show that the controller can learn to minimize random vibrations without requiring online updating of the control law. Next, we show in simulations that our algorithm can also be applied to the control of a high-order deformable mirror. We demonstrate that our controller can provide two orders of magnitude improvement in contrast at small separations under stationary turbulence. Furthermore, we show more than an order of magnitude improvement in contrast for different wind velocities and directions without requiring online updating of the control law. ","Self-optimizing adaptive optics control with Reinforcement Learning for
  high-contrast imaging"
31,1432244519077126144,961960226013548544,Valentina Lenarduzzi,['Participant selection in experiments is a crucial activity. We highlighted some issues and draw some direction to a new methodology. Please have a look to our paper accepted at @ESEM_conf vision track. @dfucci_ @oscardieste. @LUTsoftware @UniLUT @La_UPM <LINK>'],https://arxiv.org/abs/2108.12411,"Background. Software Engineering (SE) researchers extensively perform experiments with human subjects. Well-defined samples are required to ensure external validity. Samples are selected \textit{purposely} or by \textit{convenience}, limiting the generalizability of results. Objective. We aim to depict the current status of participants selection in empirical SE, identifying the main threats and how they are mitigated. We draft a robust approach to participants' selection. Method. We reviewed existing participants' selection guidelines in SE, and performed a preliminary literature review to find out how participants' selection is conducted in SE in practice. % and 3) we summarized the main issues identified. Results. We outline a new selection methodology, by 1) defining the characteristics of the desired population, 2) locating possible sources of sampling available for researchers, and 3) identifying and reducing the ""distance"" between the selected sample and its corresponding population. Conclusion. We propose a roadmap to develop and empirically validate the selection methodology. ","Towards a Methodology for Participant Selection in Software Engineering
  Experiments. A Vision of the Future"
32,1431276942729392138,3083528599,Ashutosh Baheti,"['New paper in EMNLP 2021: <LINK>\nChatbots can insult anyone by agreeing with an offensive statement. To understand this contextually offensive behavior we create ToxiChat - A crowd-annotated dataset of 2000 Reddit Conversations.', 'We augment each conversation with responses generated by DialoGPT and GPT-3 models and label each utterance with offensive language and stance. https://t.co/hpL1TTBlnw', 'Key finding: Chatbots are 2x more likely to agree with statements that are offensive! https://t.co/QfspxkXR2R', 'We experiment with controllable text generation (CTG) methods to mitigate the contextual offensive behavior of dialogue models. Although our CTG model obtains a 19% reduction in agreement with offensive statement and 29% fewer offensive responses, there is scope for improvements. https://t.co/jfN9ee9YfC', ""Our experiments and results illustrate the need for future work in controllable text generation methods to mitigate chatbots' tendency to agree with offensive statements.""]",https://arxiv.org/abs/2108.11830,"Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer. Our code and corpus are available at this https URL . ","Just Say No: Analyzing the Stance of Neural Dialogue Generation in
  Offensive Contexts"
33,1431276627413983237,28840722,Phil Long,"['New paper with @niladrichat and Peter Bartlett called ""The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer Linear Networks"": <LINK>.']",https://arxiv.org/abs/2108.11489,"The recent success of neural network models has shone light on a rather surprising statistical phenomenon: statistical models that perfectly fit noisy data can generalize well to unseen test data. Understanding this phenomenon of $\textit{benign overfitting}$ has attracted intense theoretical and empirical study. In this paper, we consider interpolating two-layer linear neural networks trained with gradient flow on the squared loss and derive bounds on the excess risk when the covariates satisfy sub-Gaussianity and anti-concentration properties, and the noise is independent and sub-Gaussian. By leveraging recent results that characterize the implicit bias of this estimator, our bounds emphasize the role of both the quality of the initialization as well as the properties of the data covariance matrix in achieving low excess risk. ","The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer
  Linear Networks"
34,1431270923907252231,14104309,Darren Price,"['New @turinginst-funded paper driven by the excellent work of @AgentSte looks at how Gaussian mixture models can be used to model complex multi-dimensional datasets for machine learning enabled inference and experimental design <LINK> #ML <LINK>', 'üíªüíæ\nPublic code and data release coming very soon! ü•≥\nüìäüìö']",https://arxiv.org/abs/2108.11481,"We show that density models describing multiple observables with (i) hard boundaries and (ii) dependence on external parameters may be created using an auto-regressive Gaussian mixture model. The model is designed to capture how observable spectra are deformed by hypothesis variations, and is made more expressive by projecting data onto a configurable latent space. It may be used as a statistical model for scientific discovery in interpreting experimental observations, for example when constraining the parameters of a physical model or tuning simulation parameters according to calibration data. The model may also be sampled for use within a Monte Carlo simulation chain, or used to estimate likelihood ratios for event classification. The method is demonstrated on simulated high-energy particle physics data considering the anomalous electroweak production of a $Z$ boson in association with a dijet system at the Large Hadron Collider, and the accuracy of inference is tested using a realistic toy example. The developed methods are domain agnostic; they may be used within any field to perform simulation or inference where a dataset consisting of many real-valued observables has conditional dependence on external parameters. ","Learning to discover: expressive Gaussian mixture models for
  multi-dimensional simulation and parameter inference in the physical sciences"
35,1431235197278859266,1148106258,Andreas Wortmann,"['Does the ""new normal"" affect design thinking and creativity in software development? To find out, check our Journal of Software: Evolution and Process paper preprint at <LINK> CC @RodiJolak @GrischaLi @mrvchaudron <LINK>']",https://arxiv.org/abs/2108.11903,"Context: Designing software is an activity in which software developers think and make design decisions that shape the structure and behavior of software products. Designing software is one of the least understood software engineering activities. In a collaborative design setting, various types of distances can lead to challenges and effects that potentially affect how software is designed. Objective: To contribute to a better understanding of collaborative software design, we investigate how geographic distance affects its design thinking and the creativity of its discussions. Method: To this end, we conducted a multiple-case study exploring the design thinking and creativity of co-located and distributed software developers in a collaborative design setting. Results: Compared to co-located developers, distributed developers spend less time on exploring the problem space, which could be related to different socio-technical challenges, such as lack of awareness and common understanding. Distributed development does not seem to affect the creativity of their activities. Conclusion: Developers engaging in collaborative design need to be aware that problem space exploration is reduced in a distributed setting. Unless distributed teams take compensatory measures, this could adversely affect the development. Regarding the effect distance has on creativity, our results are inconclusive and further studies are needed. ","Design Thinking and Creativity of Co-located vs. Globally Distributed
  Software Developers"
36,1431058544715964420,1212029940033409024,Hiroyuki Kurokawa ÈªíÂ∑ùÂÆè‰πã,"[""Here is our new paper accepted for publication in Icarus. Mars' atmospheric neon suggests volatile-rich primitive mantle <LINK>"", '@RamsesSpaceman Thanks!']",https://arxiv.org/abs/2108.11537,"Martian atmospheric neon (Ne) has been detected by Viking and also found as trapped gas in Martian meteorites, though its abundance and isotopic composition have not been well determined. Because the timescale of Ne loss via atmospheric escape estimated from recent measurements with MAVEN is short (0.6--1 $\times$ 10$^8$ years), the abundance and isotope composition of Martian atmospheric Ne reflect recent atmospheric gas supply mostly from volcanic degassing. Thus, it can serve as a probe for the volatile content of the interior. Here we show that the tentatively-informed atmospheric Ne abundance suggests recent active volcanism and the mantle being richer in Ne than Earth's mantle today by more than a factor of 5--80. The estimated mantle Ne abundance requires efficient solar nebular gas capture or accretion of Ne-rich materials such as solar-wind-implanted dust in the planet formation stage, both of which provide important constraints on the abundance of other volatile elements in the interior and the accretion history of Mars. More precise determination of atmospheric Ne abundance and isotopic composition by in situ analysis or Mars sample return is crucial for distinguishing the possible origins of Ne. ",Mars' atmospheric neon suggests volatile-rich primitive mantle
37,1430961973718667272,1373472847750893569,Stanley H. Chan,"['New paper!\n\nDetecting and Segmenting Adversarial Graphics Patterns from Images\n\n<LINK>\n\n@ICCV_2021 workshop\n\n@PurdueEngineers @PurdueECE \n#ComputerForensics \n#ArtificialIntelligence <LINK>', 'In one of the visits at #Facebook, I was told that the majority of the attacks were not based on adversarial attacks published in @NeurIPSConf  and @icmlconf.\n\nA layperson just uses photoshop to add simple patterns to alter the image.', 'It turns out that defending these attacks is nontrivial because people are just very creative. Adversarial training fails miserably against the huge variety of patterns.\n\nSo we came up with this simple solution to identifying the altered parts.', 'And it becomes another (unfunded) side project that we had a lot of fun with!']",https://arxiv.org/abs/2108.09383,"Adversarial attacks pose a substantial threat to computer vision system security, but the social media industry constantly faces another form of ""adversarial attack"" in which the hackers attempt to upload inappropriate images and fool the automated screening systems by adding artificial graphics patterns. In this paper, we formulate the defense against such attacks as an artificial graphics pattern segmentation problem. We evaluate the efficacy of several segmentation algorithms and, based on observation of their performance, propose a new method tailored to this specific problem. Extensive experiments show that the proposed method outperforms the baselines and has a promising generalization capability, which is the most crucial aspect in segmenting artificial graphics patterns. ",Detecting and Segmenting Adversarial Graphics Patterns from Images
38,1430928575188770821,50573559,Adams Wei Yu,"['Happy to introduce SimVLM, a simplified vision-language pretraining framework trained with a single loss on weak supervision.\n\nTLDR: Our model not only achieves SoTA on 6 VL tasks including VQA, but also shows new zero-shot paradigms.\n\nPaper: <LINK>', 'Joint work with @MrZiruiWang, @JHYUXM, @ZihangDai, Yulia Tsvetkov and @caoyuan33.', 'Please *DO* take a look at our cool results! https://t.co/DdcWaYWVmt', 'And the summary: https://t.co/52fvkKpXxK']",http://arxiv.org/abs/2108.10904,"With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer. ",SimVLM: Simple Visual Language Model Pretraining with Weak Supervision
39,1430917784033271808,705390315,Paul H√ºnermund üá∫üá¶üá™üá∫,"['We have a new working paper up on @arxiv titled:\n\n""Double Machine Learning and Bad Controls ‚Äî A Cautionary Tale"" (with @beyers_louw &amp; @itamarcaspi)\n\nLink: <LINK> \n#CausalInference #Causality #DataScience #Econometrics #MachineLearning 1/ <LINK>', 'Double machine learning (DML) is getting more and more traction in econ. One important application ‚Äì among several others ‚Äì is in automatic model selection in high-dimensional settings. 2/', ""Suppose you're interested in estimating theta in the following regression:\n\nY = theta * D + X * beta + u\n\nThe covariate vector X possibly contains many variables, but only a few of them have non-zero coefficients. DML then allows you to automatically select relevant controls. 3/"", 'This is extremely useful, because it lets you detect relevant confounders among a large set of candidate variables, possibly even larger than the sample size. This ability makes selection-on-observables assumptions more plausible and the research process more transparent. 4/', ""You couldn't just use standard LASSO for this purpose. As the work by @VC31415 and coauthors demonstrates, by not taking the correlation between X and the treatment D into account, LASSO can lead to severe approximation error and bias. DML fixes this problem. 5/"", 'However, DML relies on unconfoundedness, i.e., exogeneity of all covariates, which is probably unrealistic if a large set of potential control variables is considered with only little economic theory to guide the choice which variables to include. 6/', 'So what happens if you apply DML when unconfoundedness is violated? E.g., because you encounter a situation like in the causal diagrams depicted below. Well, we demonstrate that things can turn ugly pretty quickly. 7/ https://t.co/kAqMWVNfVG', 'This figure shows our main results. In the first row, you can see the familiar picture with only good controls (i.e., unconfoundedness holds). DML is able to estimate the causal effect with correct coverage, while naive LASSO is far off. 8/ https://t.co/Nx3FheuOaV', 'If there are endogenous variables in the set of potential controls, however, DML leads to severe bias. It also loses its edge over naive LASSO, which was one of the main motivations for developing DML in the first place. 9/', ""For the simple mediator it kind of works, but you have to keep in mind that you're estimating a different parameter in this case (i.e., the direct effect), which is difficult if you have no good theory about the variables you have included in the estimation. 10/"", 'We show that this happens already if there are only a few bad controls in the covariate set. The intuition here is that bad controls are often themselves highly correlated with the treatment or outcome (or both) and thus likely get picked by the DML. 11/ https://t.co/GNJVTgqvom', ""We show this in a real-world application by using the gender wage gap data by Blau and Kahn (2017, JEP). If we include a woman's marital status, which the literature characterizes as likely endogenous, in a regression with 50+ other covariates, results change substantially. 12/ https://t.co/yKUG9sErW3"", 'So the bottom line is: DML is a very flexible tool and has many important applications (IV models, estimate arbitrary do-calculus objects, data splitting, etc), but if you want use it for automatically detecting controls without much theory behind, you might get into trouble. 13/', 'In our view, this is an important caveat to keep in mind for what has sometimes been coined ""causal machine learning"" in the economics literature. Purely data-driven confounder selection, in order to justify selection-on-observables assumptions, is a risky business. 14/', 'In particular, if the covariate space is large, the likelihood of having bad controls in the conditioning set could be quite high. So smaller models, based on sound economic theory might actually be preferable to ML for causal inference purposes. 15/', 'Thanks for reading. We hope you find the paper useful. And if you have any comments or suggestions, please let us know! üôè 16/16', '@JorgeGuzmanCBS @arxiv @beyers_louw @itamarcaspi Thanks! üôÇ', ""@quantadan It's single-spaced and small font though üòÅ"", '@joe_sill The cross validation question is not really related to this project. The argument is more about identification rather than the specific estimation procedure. The same goes for l2-boosting, for example.', '@kbkarlson @arxiv @beyers_louw @itamarcaspi Haha, I like it! üòÑ', '@economeager Thank you, Rachael! üôÇ']",https://arxiv.org/abs/2108.11294,"Double machine learning (DML) is becoming an increasingly popular tool for automatic model selection in high-dimensional settings. These approaches rely on the assumption of conditional independence, which may not hold in big-data settings where the covariate space is large. This paper shows that DML is very sensitive to the inclusion of even a few ""bad controls"" in the covariate space. The resulting bias varies with the nature of the causal model, which raises concerns about the feasibility of selecting control variables in a data-driven way. ","Double Machine Learning and Automated Confounder Selection -- A
  Cautionary Tale"
40,1430903052286844934,1196243170188824576,Javier F Acevedo,['New paper with @josephbramante and Alan Goodman on composite DM detection with the Migdal effect!\n<LINK>'],https://arxiv.org/abs/2108.10889,"Large composite dark matter states source a scalar binding field that, when coupled to Standard Model nucleons, provides a potential under which nuclei recoil and accelerate to energies capable of ionization, radiation, and thermonuclear reactions. We show that these dynamics are detectable for nucleon couplings as small as $g_n \sim 10^{-17}$ at dark matter experiments, where the greatest sensitivity is attained by considering the Migdal effect. We also explore Type-Ia supernovae and planetary heating as possible means to discover this type of dark matter. ","Accelerating Composite Dark Matter Discovery with Nuclear Recoils and
  the Migdal Effect"
41,1430803949439930371,1134375290581524480,Kai Schmitz,"['New paper on the arXiv, together with Valerie Domcke and @Tevong You: We show how a rolling ""relaxion"" can be trapped by gauge field friction, highlight the importance of Schwinger pair production, and provide a theory motivation for the dark axion portal. <LINK> <LINK>']",https://arxiv.org/abs/2108.11295,"The dark axion portal is a coupling of an axion-like particle to a dark photon kinetically mixed with the visible photon. We show how this portal, when applied to the relaxion, can lead to cosmological relaxation of the weak scale using dark photon production. The key backreaction mechanism involves the Schwinger effect: As long as electroweak symmetry is unbroken, Schwinger production of massless Standard Model fermions, which carry dark millicharges, suppresses the dark photon production. Once the electroweak symmetry is broken, the fermions acquire mass and the suppression is lifted. An enhanced dark photon dissipation then traps the relaxion at a naturally small weak scale. Our model thus provides a novel link between the phenomenological dark axion portal, dark photons, and the hierarchy problem of the Higgs mass. ",Cosmological Relaxation through the Dark Axion Portal
42,1430577004018012161,704533062860681216,Patrick Coles,"[""New paper from our Summer School üî•\n\n<LINK>\n\nWe propose a new optimizer that can save you time ‚åöÔ∏è and money üíµ when implementing a variational algorithm. We analytically prove fast convergence! See Andrew's thread below... <LINK> <LINK>""]",https://arxiv.org/abs/2108.10434,"Variational Quantum Algorithms (VQAs) are a promising approach for practical applications like chemistry and materials science on near-term quantum computers as they typically reduce quantum resource requirements. However, in order to implement VQAs, an efficient classical optimization strategy is required. Here we present a new stochastic gradient descent method using an adaptive number of shots at each step, called the global Coupled Adaptive Number of Shots (gCANS) method, which improves on prior art in both the number of iterations as well as the number of shots required. These improvements reduce both the time and money required to run VQAs on current cloud platforms. We analytically prove that in a convex setting gCANS achieves geometric convergence to the optimum. Further, we numerically investigate the performance of gCANS on some chemical configuration problems. We also consider finding the ground state for an Ising model with different numbers of spins to examine the scaling of the method. We find that for these problems, gCANS compares favorably to all of the other optimizers we consider. ","Adaptive shot allocation for fast convergence in variational quantum
  algorithms"
43,1430526416081334276,583637312,Srijan Kumar,"['Congrats to Sejoon on his first PhD paper accepted at @CIKM2021 üéâüéâ\n\n""Influence-guided Data Augmentation for Neural Tensor Completion""\nLink: <LINK>\nSejoon has created a new data augmentation technique for recommender systems\n\nCode + data: <LINK> <LINK>', '@mlatgt @GTCSE @gtcomputing @IDEaSatGT']",https://arxiv.org/abs/2108.10248,"How can we predict missing values in multi-dimensional data (or tensors) more accurately? The task of tensor completion is crucial in many applications such as personalized recommendation, image and video restoration, and link prediction in social networks. Many tensor factorization and neural network-based tensor completion algorithms have been developed to predict missing entries in partially observed tensors. However, they can produce inaccurate estimations as real-world tensors are very sparse, and these methods tend to overfit on the small amount of data. Here, we overcome these shortcomings by presenting a data augmentation technique for tensors. In this paper, we propose DAIN, a general data augmentation framework that enhances the prediction accuracy of neural tensor completion methods. Specifically, DAIN first trains a neural model and finds tensor cell importances with influence functions. After that, DAIN aggregates the cell importance to calculate the importance of each entity (i.e., an index of a dimension). Finally, DAIN augments the tensor by weighted sampling of entity importances and a value predictor. Extensive experimental results show that DAIN outperforms all data augmentation baselines in terms of enhancing imputation accuracy of neural tensor completion on four diverse real-world tensors. Ablation studies of DAIN substantiate the effectiveness of each component of DAIN. Furthermore, we show that DAIN scales near linearly to large datasets. ",Influence-guided Data Augmentation for Neural Tensor Completion
44,1430430322710155265,2872569532,Alejandro S. Borlaff,"['New publication! Today I am happy to present ""Euclid preparation XVI Exploring the ultra low-surface brightness Universe with Euclid/VIS""\n\nIn this paper we explore new in-flight callibration methods for the future @ESA_Euclid telescope optical cameraüöÄüõ∞üåå\n<LINK> <LINK>', 'The main idea of this paper is to simulate how well can we calibrate Euclid using nothing but the Science Exposures (sky-flat fielding). This is, without calibration lamps on-board, trusting the Zodiacal Light as if it was our telescope dome. https://t.co/5mi4vnnLLx', 'This is kinda usual on the ground, but not in space. So we included all the effects that might contaminate our process. We used the stellar catalogs from @ESAGaia to study how much straylight from Milky Way stars affect our images in the @ESA_Euclid mission. https://t.co/Oa3KPx7M7B', 'We also simulated the effects of the Solar System planets, which are very bright and move very rapidly on the sky! Their trayectories are the red lines on the previous plot, but here is an additional animation to visualize how the planets dance from L2. Just pure Galilean fun: https://t.co/YNZmnqX6Ln', 'Moreover, thanks to the detailed photometry of @ESAGaia, and the amazing engineering models from @ESA_Euclid, we could simulate the  background gradients that the stars **from all positions of the sky** might introduce on our images, a harmful potential contaminant.', ""Here we simulate an observation in the environment of Orion's belt. The size of the circlles represent the brightness of the stars, the red square is Euclid's field of view, and the purple circles are the contamination level. \n\nWe can predict this before we point the telescope! https://t.co/VVjw52tvme"", 'Taking all this into account, we simulated 4 months of observations with @ESA_Euclid, we generated sky flats and used them to callibrate the exposures themselves. \n\nWe found that @ESA_Euclid can self-calibrate its VIS detector using the Zodiacal light every 10 days! https://t.co/b5cqSH3IsA', 'Moreover, we found that Euclid\'s optimal limit in surface brightness will be close to 29.5 mag/arcsec¬≤ (3œÉ, 10""x10"") for 15.000 deg¬≤. That is two magnitudes deeper than SDSS for half of the sky, at 10 times the spatial resolution! https://t.co/VuauQKvxun', ""Thanks to these techniques, @ESA_Euclid will be able to explore the outskirts of galaxies to an unprecedented detail. For 24.000 galaxies with sizes&gt;1', we will have surface brightness profiles down to 30.5 mag/arcsec¬≤ (3œÉ)\n\nThe low surface brightness Universe wait for us! https://t.co/7OoVJBFxSN""]",https://arxiv.org/abs/2108.10321,"While Euclid is an ESA mission specifically designed to investigate the nature of Dark Energy and Dark Matter, the planned unprecedented combination of survey area ($\sim15\,000$ deg$^2$), spatial resolution, low sky-background, and depth also make Euclid an excellent space observatory for the study of the low surface brightness Universe. Scientific exploitation of the extended low surface brightness structures requires dedicated calibration procedures yet to be tested. We investigate the capabilities of Euclid to detect extended low surface brightness structure by identifying and quantifying sky background sources and stray-light contamination. We test the feasibility of generating sky flat-fields to reduce large-scale residual gradients in order to reveal the extended emission of galaxies observed in the Euclid Survey. We simulate a realistic set of Euclid/VIS observations, taking into account both instrumental and astronomical sources of contamination, including cosmic rays, stray-light, zodiacal light, ISM, and the CIB, while simulating the effects of the presence of background sources in the FOV. We demonstrate that a combination of calibration lamps, sky flats and self-calibration would enable recovery of emission at a limiting surface brightness magnitude of $\mu=29.5^{+0.08}_{-0.27} $ mag arcsec$^{-2}$ ($3\sigma$, $10\times10$ arcsec$^2$) in the Wide Survey, reaching regions 2 magnitudes deeper in the Deep Surveys. Euclid/VIS has the potential to be an excellent low surface brightness observatory. Covering the gap between pixel-to-pixel calibration lamp flats and self-calibration observations for large scales, the application of sky flat-fielding will enhance the sensitivity of the VIS detector at scales of larger than 1 degree, up to the size of the FOV, enabling Euclid to detect extended surface brightness structures below $\mu=31$ mag arcsec$^{-2}$ and beyond. ","Euclid preparation: XVI. Exploring the ultra low-surface brightness
  Universe with Euclid/VIS"
45,1430410364987052036,809472,Janet Yi-Ching Huang,"['üö®New Preprint Alert üö®\nSuper excited to share our incoming #CSCW2021 paper: ""Thing Constellation Visualizer: Exploring Emergent Relationships of Everyday Objects"" by me, Yu-Ting Cheng, Rung-Huei Liang, @yjhsu , Lin-Lin Chen. @ACM_CSCW #CSCW\n\npreprint: <LINK> <LINK>', 'This work presents a novel tool that empowers designers to change their original perspectives to perceive the world and gain new insights by playing with data and AI. The work contributes a new approach and tool to support More-Than Human-Centred Design of IoT ecosystems.']",https://arxiv.org/abs/2108.09448,"Designing future IoT ecosystems requires new approaches and perspectives to understand everyday practices. While researchers recognize the importance of understanding social aspects of everyday objects, limited studies have explored the possibilities of combining data-driven patterns with human interpretations to investigate emergent relationships among objects. This work presents Thing Constellation Visualizer (thingCV), a novel interactive tool for visualizing the social network of objects based on their co-occurrence as computed from a large collection of photos. ThingCV enables perspective-changing design explorations over the network of objects with scalable links. Two exploratory workshops were conducted to investigate how designers navigate and make sense of a network of objects through thingCV. The results of eight participants showed that designers were actively engaged in identifying interesting objects and their associated clusters of related objects. The designers projected social qualities onto the identified objects and their communities. Furthermore, the designers changed their perspectives to revisit familiar contexts and to generate new insights through the exploration process. This work contributes a novel approach to combining data-driven models with designerly interpretations of thing constellation towards More-Than Human-Centred Design of IoT ecosystems. ","Thing Constellation Visualizer: Exploring Emergent Relationships of
  Everyday Objects"
46,1430346656113774598,335051948,Anshuman Srivastava,"['Happy to inform about the new arXiv preprint from our LOQM student Lekshmi Eswaramoorthy on selective coupling to dark excitons using engineered Purcell anisotropy. Thanks to @IndiaDST @iitbombay and @iitbmonash \n\nLink to paper: <LINK> <LINK>', 'This is the culmination of her work during the lockdown and reflects her grit where as an experimentalist, she quickly caught up on simulation and theory work to positively use the lockdown period.']",http://arxiv.org/abs/2108.10680,"Tightly bound dark excitons in atomically thin semiconductors can be used for various optoelectronic applications including light storage and quantum communication. Their optical accessibility is however limited due to their out-of-plane transition dipole moment. We thus propose to strengthen the coupling of dark excitons in two dimensional materials with out-of-plane resonant modes of a cavity at room temperature, by engineering the anisotropy in the Purcell factor. A silica micro-disk characterised by high confinement of light in small modal volume, high Q-factor and free spectral range is used to couple to the excitons in monolayer transition metal dichalcogenides. We show numerically that the tapering of sidewalls of the micro-disk is an extremely versatile route for achieving the selective coupling of whispering gallery modes to light emitted from out-of-plane dipoles to the detriment of that from in-plane ones for four representative monolayer transition metal dichalcogenides. ","Engineering Purcell factor anisotropy for dark and bright excitons in
  two dimensional semiconductors"
47,1430195746805067789,23462367,Jeffrey West,"['üö®  üö®  üö®\nNew preprint pioneered by @GregoryJKimmel, @EvolSci \n\nThe question:\n\nCan the growth characteristics of cell lines be condensed into a single parameter?\n\nIn this paper, we propose that local neighborhood size is a key parameter.\n\n<LINK> <LINK>', '#mathonco folks will study growth laws until the end of time... but still, a mechanistic basis of Gompertzian growth remains elusive\n\nStarting from 2.7 (derived from simple assumptions about contact inhibition), a wide variety of growth laws can be found (including Gompertz!) https://t.co/Mirvef81gB', 'Gompertz fits better at high confluency (A) yes, \n\n...but cell lines can also be mapped onto a growth rate - neighborhood size space (B).\n\nWe confirmed this relationship using agent-based models with different neighborhood sizes (bottom panels) https://t.co/P5sHdA3AxE', 'We build in various neighborhood constraints into an agent-based model and determine which growth law they follow. https://t.co/RHWMgNihCu', 'Contact inhibition can provide the roadmap which dictates which growth law any particular cell line will follow. Migration is important, but under high migration assumption the size of the local neighborhood is important.\n\nhttps://t.co/l3Yg1PyYqz']",https://arxiv.org/abs/2108.10000,"Cancer cell population dynamics often exhibit remarkably replicable, universal laws despite their underlying heterogeneity. Mechanistic explanations of universal cell population growth remain partly unresolved to this day, whereby population feedback between the microscopic and mesoscopic configurations can lead to macroscopic growth laws. We here present a unification under density-dependent birth events via contact inhibition. We consider five classical tumor growth laws: exponential, generalized logistic, Gompertz, radial growth, and fractal growth, which can be seen as manifestations of a single microscopic model. Our theory is substantiated by agent based simulations and can explain growth curve differences in experimental data from in vitro cancer cell population growth. Thus, our framework offers a possible explanation for the large number of mean-field laws that can adequately capture seemingly unrelated cancer or microbial growth dynamics. ","Local contact inhibition leads to universal principles of cell
  population growth"
48,1430191450222841859,1181013626418798592,Leo Duan,['Our research group has been playing a fun game of getting cool optimization tricks (e.g. ADMM) into the Bayesian framework. Advertising our new paper: <LINK>'],https://arxiv.org/abs/2108.04851,"In statistical applications, it is common to encounter parameters supported on a varying or unknown dimensional space. Examples include the fused lasso regression, the matrix recovery under an unknown low rank, etc. Despite the ease of obtaining a point estimate via the optimization, it is much more challenging to quantify their uncertainty -- in the Bayesian framework, a major difficulty is that if assigning the prior associated with a $p$-dimensional measure, then there is zero posterior probability on any lower-dimensional subset with dimension $d<p$; to avoid this caveat, one needs to choose another dimension-selection prior on $d$, which often involves a highly combinatorial problem. To significantly reduce the modeling burden, we propose a new generative process for the prior: starting from a continuous random variable such as multivariate Gaussian, we transform it into a varying-dimensional space using the proximal mapping. This leads to a large class of new Bayesian models that can directly exploit the popular frequentist regularizations and their algorithms, such as the nuclear norm penalty and the alternating direction method of multipliers, while providing a principled and probabilistic uncertainty estimation. We show that this framework is well justified in the geometric measure theory, and enjoys a convenient posterior computation via the standard Hamiltonian Monte Carlo. We demonstrate its use in the analysis of the dynamic flow network data. ","Bayesian Inference using the Proximal Mapping: Uncertainty
  Quantification under Varying Dimensionality"
49,1430163593538936832,732342097458679808,Mallory Molina,"['Check out my new paper where I identify a new sample of black hole candidates in dwarf galaxies using [Fe X]. They are in bluer, lower-mass galaxies often missed by other detection techniques, and some have additional signatures of AGN activity.\n<LINK>']",https://arxiv.org/abs/2108.09307,"The massive black hole (BH) population in dwarf galaxies ($M_{\rm BH} \lesssim 10^5~M_\odot$) can provide strong constraints on the origin of BH seeds. However, traditional optical searches for active galactic nuclei (AGNs) only reliably detect high-accretion, relatively high-mass BHs in dwarf galaxies with low amounts of star formation, leaving a large portion of the overall BH population in dwarf galaxies relatively unexplored. Here, we present a sample of 81 dwarf galaxies ($M_\star \le 3 \times 10^9~M_\odot$) with detectable [Fe X]$\lambda$6374 coronal line emission indicative of accretion onto massive BHs, only two of which were previously identified as optical AGNs. We analyze optical spectroscopy from the Sloan Digital Sky Survey and find [Fe X]$\lambda$6374 luminosities in the range $L_{\rm [Fe\,X]}\approx10^{36}$-$10^{39}$ erg s$^{-1}$, with a median value of $1.6 \times 10^{38}$ erg s$^{-1}$. The [Fe X]$\lambda$6374 luminosities are generally much too high to be produced by stellar sources, including luminous Type IIn supernovae (SNe). Moreover, based on known SNe rates, we expect at most 8 Type IIn SNe in our sample. On the other hand, the [Fe X]$\lambda$6374 luminosities are consistent with accretion onto massive BHs from AGNs or tidal disruption events (TDEs). We find additional indicators of BH accretion in some cases using other emission line diagnostics, optical variability, X-ray and radio emission (or some combination of these). However, many of the galaxies in our sample only have evidence for a massive BH based on their [Fe X]$\lambda$6374 luminosities. This work highlights the power of coronal line emission to find BHs in dwarf galaxies missed by other selection techniques and to probe the BH population in bluer, lower mass dwarf galaxies. ","A Sample of Massive Black Holes in Dwarf Galaxies Detected via [Fe X]
  Coronal Line Emission: Active Galactic Nuclei and/or Tidal Disruption Events"
50,1430124498561404938,64710103,Lasse Rempe (he/him),"['New paper today with @davidmartipete and James Waterman. We show that Fatou components of a transcendental entire function can form ""Lakes of Wada"". In the context of rational functions, this was asked by Fatou in 1920. #Maths <LINK>', '""Lakes of Wada"" refer to a situation where more than two domains in the plane have the same boundary - this is possible, although hard to imagine!', 'This is a picture of a ""Lakes of Wada"" continuum - the white, black, light grey and dark grey regions all have the same boundary. (Picture by James Waterman) https://t.co/1ZPRYslQ9f']",https://arxiv.org/abs/2108.10256,"We develop a general technique for realising full closed subsets of the complex plane as wandering sets of entire functions. Using this construction, we solve a number of open problems. (1) We construct a counterexample to Eremenko's conjecture, a central problem in transcendental dynamics that asks whether every connected component of the set of escaping points of a transcendental entire function is unbounded. (2) We prove that there is a transcendental entire function for which infinitely many Fatou components share the same boundary. This resolves the long-standing problem whether ""Lakes of Wada continua"" can arise in complex dynamics, and answers the analogue of a question of Fatou from 1920 concerning Fatou components of rational functions. (3) We answer a question of Rippon concerning the existence of non-escaping points on the boundary of a bounded escaping wandering domain, that is, a wandering Fatou component contained in the escaping set. In fact we show that the set ofsuch points can have positive Lebesgue measure. (4) We give the first example of an entire function having a simply connected Fatou component whose closure has a disconnected complement, answering a question of Boc Thaler. In view of (3), we introduce the concept of ""maverick points"": points on the boundary of a wandering domain whose accumulation behaviour differs from that of internal points. We prove that the set of such points has harmonic measure zero, but that both escaping and oscillating wandering domains can contain large sets of maverick points. ","Eremenko's conjecture, Wandering Lakes of Wada, and Maverick Points"
51,1429994231171534854,837412258150035457,Dr. Burcin Mutlu-Pakdil,"['Check out our new paper on the arXiv today: <LINK>. We report the discovery of three ultra-faint dwarf galaxies around NGC253. They are among the faintest systems discovered beyond the Local Group ü§© (1/4) <LINK>', 'This brings the total number of NGC253 satellites uncovered by PISCeS (our @LCOAstro Magellan/Megacam Survey) to five. In this paper, we present @NASAHubble follow-up of these five dwarfs, confirm their nature, and firmly establish their membership with NGC253. (2/4)', 'All five systems contain predominantly old, metal-poor stellar populations and have sizes and luminosities largely consistent with Local Group dwarfs. (3/4)', 'Deep imaging surveys like PISCeS promise to elucidate the faint end of the satellite luminosity function and its scatter across a range of galaxy masses, morphologies, and environments in the decade to come (just as we showed in our previous paper üòâ https://t.co/X3d9tnaTHh)(4/4)', '@8minutesold @joshuadsimon @denija83 @puragragt @anilcseth @sand_dave Yes, you are correct. It is based on the projected positions, and the outlier is Scl-MM-dw4. We indeed need to get the velocities of these dwarfs to explore the spatially flattened structure of dwarfs that you suggested.']",https://arxiv.org/abs/2108.09312,"We present deep Hubble Space Telescope imaging of five faint dwarf galaxies associated with the nearby spiral NGC 253 (D$\approx$3.5 Mpc). Three of these are newly discovered ultra-faint dwarf galaxies, while all five were found in the Panoramic Imaging Survey of Centaurus and Sculptor (PISCeS), a Magellan$+$Megacam survey to identify faint dwarfs and other substructures in resolved stellar light around massive galaxies outside of the Local Group. Our HST data reach $\gtrsim$3 magnitudes below the tip of the red giant branch for each dwarf, allowing us to derive their distances, structural parameters, and luminosities. All five systems contain predominantly old, metal-poor stellar populations (age$\sim$12 Gyr, [M/H]$\lesssim$$-$1.5) and have sizes ($r_{h}$$\sim$110-3000 pc) and luminosities ($M_V$$\sim$$-7$ to $-12$ mag) largely consistent with Local Group dwarfs. The three new NGC 253 satellites are among the faintest systems discovered beyond the Local Group. We also use archival HI data to place limits on the gas content of our discoveries. Deep imaging surveys such as our program around NGC 253 promise to elucidate the faint end of the satellite luminosity function and its scatter across a range of galaxy masses, morphologies, and environments in the decade to come. ","Hubble Space Telescope Observations of NGC 253 Dwarf Satellites:
  Discovery of Three Ultra-faint Dwarf Galaxies"
52,1429916399594459137,2569631268,Daniel Huber,"['At long last, our paper on the new @NASA_TESS 20-second cadence data is out! <LINK> TL;DR: 20-sec data will improve your science *irrespective of variability timescale* for bright stars. A thread (see also <LINK> for a ~12 min #TESSCon2 talk) 1/9', 'Comparing 20-sec to 2-min data, we find that 20-sec shows a strong magnitude dependent precision improvement, with ~&gt;20% better precision for bright stars (T~8 mag). For fainter stars (T~&gt;13) the precision is about equal. 2/9 https://t.co/Us2FBCv6pc', 'The improvement is due to differences in cosmic ray mitigation, done onboard for 2-min data but in post-processing for 20-sec data. Onboard mitigation becomes less efficient for bright stars for orbits with high pointing jitter, as predicted in pre-flight simulations. 3/9 https://t.co/KGEaHImggT', 'We show a couple of science examples that the new @NASA_TESS 20-sec data enables. First, we detect oscillations in three Sun-like stars. Note the huge increase in S/N due to the improved precision and higher Nyquist frequency of 20-sec data! 4/9 https://t.co/2FedlrBU0R', 'Asteroseismology allows us to measure their radii, masses, ages and densities to ~1%, ~3%, ~20% and ~1%, *including systematic errors*. This is important to understand the interplay between rotation, age and activity, especially for bright stars with measured activity cycles. 5/9 https://t.co/Cw75UdAwD2', 'Second, we re-characterize of pi Men c, which is now the closest transiting exoplanet for which detailed asteroseismology of the host star is possible. Hopefully this plot by @ashleychontos will be populated even more in the TESS extended mission! 6/9 https://t.co/j4SRgoLzK4', 'Combining 20-sec @NASA_TESS asteroseismology+transits with public @espresso_astro RVs we find that pi Men c is on a low-eccentricity orbit. This suggests efficient tidal circularization if the planet formed via high-eccentricity migration (implied by observed misalignments). 7/9 https://t.co/7nMMmP3C6Z', 'We place pi Men c at the upper edge of the radius valley, suggesting that it has held on to an atmosphere. Using only planets around stars for which asteroseismology is possible, the valley remains devoid of planets. This likely tells us something about how it has formed! 8/9 https://t.co/hxZeprnIB9', 'Many thanks to all collaborators who helped with this paper (@hviddie, @travis_metcalfe, @ashleychontos, @vaneylenv, @warrickball &amp; many others) + the @TESS team for making 20-sec data possible. Looking forward to more exciting 20-sec results in the TESS extended mission(s)! 9/9']",https://arxiv.org/abs/2108.09109,"We present an analysis of the first 20-second cadence light curves obtained by the TESS space telescope during its extended mission. We find a precision improvement of 20-second data compared to 2-minute data for bright stars when binned to the same cadence (~10-25% better for T<~8 mag, reaching equal precision at T~13 mag), consistent with pre-flight expectations based on differences in cosmic ray mitigation algorithms. We present two results enabled by this improvement. First, we use 20-second data to detect oscillations in three solar analogs (gamma Pav, zeta Tuc and pi Men) and use asteroseismology to measure their radii, masses, densities and ages to ~1%, ~3%, ~1% and ~20% respectively, including systematic errors. Combining our asteroseismic ages with chromospheric activity measurements we find evidence that the spread in the activity-age relation is linked to stellar mass and thus convection-zone depth. Second, we combine 20-second data and published radial velocities to re-characterize pi Men c, which is now the closest transiting exoplanet for which detailed asteroseismology of the host star is possible. We show that pi Men c is located at the upper edge of the planet radius valley for its orbital period, confirming that it has likely retained a volatile atmosphere and that the ""asteroseismic radius valley"" remains devoid of planets. Our analysis favors a low eccentricity for pi Men c (<0.1 at 68% confidence), suggesting efficient tidal dissipation (Q/k <~ 2400) if it formed via high-eccentricity migration. Combined, these early results demonstrate the strong potential of TESS 20-second cadence data for stellar astrophysics and exoplanet science. ","A 20-Second Cadence View of Solar-Type Stars and Their Planets with
  TESS: Asteroseismology of Solar Analogs and a Re-characterization of pi Men c"
53,1429715836965687299,1142415620270690304,Gitta Kutyniok,"['Interested in how to optimally combine #DeepLearning with (pure) #Math to solve inverse problems in #imaging? Check out our new paper on ""Deep Microlocal Reconstruction for Limited-Angle Tomography"" <LINK>, joint with @hectorandradel, O. √ñktem, and P. Petersen.']",https://arxiv.org/abs/2108.05732,"We present a deep learning-based algorithm to jointly solve a reconstruction problem and a wavefront set extraction problem in tomographic imaging. The algorithm is based on a recently developed digital wavefront set extractor as well as the well-known microlocal canonical relation for the Radon transform. We use the wavefront set information about x-ray data to improve the reconstruction by requiring that the underlying neural networks simultaneously extract the correct ground truth wavefront set and ground truth image. As a necessary theoretical step, we identify the digital microlocal canonical relations for deep convolutional residual neural networks. We find strong numerical evidence for the effectiveness of this approach. ",Deep Microlocal Reconstruction for Limited-Angle Tomography
54,1429482848495546372,1291064871510069249,Calvin McPhail-Snyder,"[""New paper up on the arXiv! I'm a bit belated tweeting about it because I was at the lake last week when it went up <LINK>"", 'This paper works out the details of a construction of Kashaev and Reshetikhin. The idea is to improve ordinary quantum invariants of topological objects by ""twisting"" them with geometric data, in this case, reps of œÄ_1 into SL_2(C)', 'My thesis was on a similar construction, which has considerably more technical issues. This paper is nice because we can avoid them and do some explicit, algebraic computations.', ""I am hopeful that this sort of thing will provide useful for geometric topology, especially hyperbolic knot theory. If you're interested in doing this sort of thing, please let me know!""]",https://arxiv.org/abs/2108.06561,"Kashaev and Reshetikhin previously described a way to define holonomy invariants of knots using quantum $\mathfrak{sl}_2$ at a root of unity. These are generalized quantum invariants depend both on a knot $K$ and a representation of the fundamental group of its complement into $\mathrm{SL}_2(\mathbb{C})$; equivalently, we can think of $\mathrm{KR}(K)$ as associating to each knot a function on (a slight generalization of) its character variety. In this paper we clarify some details of their construction. In particular, we show that for $K$ a hyperbolic knot $\mathrm{KaRe}(K)$ can be viewed as a function on the geometric component of the $A$-polynomial curve of $K$. We compute some examples at a third root of unity. ",Kashaev--Reshetikhin Invariants of Links
55,1428740724074291208,888216099757490176,Maithra Raghu,"['Do Vision Transformers See Like Convolutional Neural Networks?\n\nNew paper <LINK>\n\nThe successes of Transformers in computer vision prompts a fundamental question: how are they solving these tasks? Do Transformers act like CNNs, or learn very different features? <LINK>', 'We explore this question in our paper, finding key differences between internal representations of the architectures, crucial roles played by attention and residual connections, and ramifications for localization and transfer learning.', 'Using representational similarity measures, we investigate the internal structure of the two architectures, finding striking differences, with ViT to having a much more uniform representation across all layers https://t.co/fxLQW94eA2', 'An analysis of self-attention reveals some reasons for this difference: very early ViT layers learn to incorporate local and *global* spatial information, unlike CNN early layers with their smaller receptive field size. https://t.co/ZhiuARqnsu', 'But attending locally is also very important! It is automatically encoded in CNNs, but larger ViTs only learn to do this with enough data (which is needed for their strong performance also.) https://t.co/7ilb9E7jo7', 'Using local and global info allows ViT earlier layers to learn better representations, which are strongly propagated through residual connections. Surprisingly ViT has stronger residual connections than ResNet! These help explain the uniform structure of ViT representations https://t.co/VLogLUCGPz', 'We study effects of classification and pretraining dataset size, finding data &amp; models should scale together -- larger pretraining data is very important for bigger ViT models and higher layer representations. Lower ViT layers also have higher classification success than CNNs https://t.co/jtCG0Jhaw6', 'Motivated by future applications to object detection, we study spatial localization across ViT and CNNs, finding that the amount of spatial information preserved in higher layers is sensitive to using a CLS token (well preserved) vs global average pooling (less preserved). https://t.co/hp7egno4xq', 'We also perform a preliminary representational analysis on the recently proposed MLP-mixer, finding its structure to be more similar to ViT. For future exploration! https://t.co/Ygd4asK4JQ', 'This work was done with a great team @TomUnterthiner  @skornblith  Chiyuan Zhang, Alexey Dosovitskiy! Please check out the paper for many more results, from finetuning effects to receptive field computations. https://t.co/mxLCIRBRLy https://t.co/x6Jtpz1mV5', 'PS: @OriolVinyalsML This time we think the abstract should sufficiently answer the title question üòÅ']",https://arxiv.org/abs/2108.08810,"Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer. ",Do Vision Transformers See Like Convolutional Neural Networks?
56,1428540539629047810,335051948,Anshuman Srivastava,"['Happy to inform about our new collaborative work posted now on arXiv. We develop high temperature mid infrared polarizers based on a lithography free approach, using a hyperbolic van der Waals crystal as the workhorse. Link to paper: <LINK> <LINK>', 'This work was led by our smart grad student Nihar and hardworking postdoc @Saurabhdixit_22, with support from our Raman expert grad student @AnujSin33973028. We carried out complementary measurements in the laboratory of our wonderful collaborator (@NicholasFang10) at MIT.', 'Thank you @iitbombay and @IndiaDST for support.']",http://arxiv.org/abs/2108.08510,"Integration of conventional mid to long-wavelength infrared polarizers with chip-scale platforms is restricted by their bulky size and complex fabrication. Van der Waals materials based polarizer can address these challenges due to its non-lithographic fabrication, ease of integration with chip-scale platforms, and room temperature operation. In the present work, mid-IR optical response of the sub-wavelength thin films of $\alpha$-MoO$_3$ is investigated for application towards high temperature mid-IR transmission and reflection type thin film polarizer. To our knowledge, this is the first report of above room temperature mid-IR optical response of $\alpha$-MoO$_3$ to determine the thermal stability of the proposed device. We find that our $\alpha$-MoO$_3$ based polarizer retains high extinction ratio with peak value exceeding 10 dB, up to a temperature of 140$^{\circ}$C. We explain our experimental findings by natural in-plane hyperbolic anisotropy of $\alpha$-MoO$_3$ in the mid-IR, high temperature X-ray diffraction and Raman spectroscopic measurements. This work opens up new avenues for naturally in-plane hyperbolic van der Waals thin-films to realize sub-wavelength IR optical components without lithographic constraints. ","High temperature mid-IR polarizer via natural in-plane hyperbolic Van
  der Waals crystals"
57,1428517710837821459,1558538456,Rodrigo Fern√°ndez,"['New paper led by #UAlberta student Coleman Dean and in collaboration with @bluekilonova \n\nWe looked into the production of fast (&gt;0.6c) ejecta in binary neutron star mergers and its sensitivity to spatial resolution in grid-based codes\n\n<LINK> <LINK>', 'Grid-based &amp; SPH simulations have predicted very different amounts of this material. \n\nWe designed a numerical experiment to test whether grid-codes have been under-producing this component due to insufficient resolution: a neutron star merger in a 2D corotating frame https://t.co/sYc8maLfgn', 'While the absolute amount of ejecta from this experiment is expected to be off due to the approximations, the resolution dependence should be more robust and applicable to 3D simulations.\n\nWe converge to within 10% in fast ejecta mass for a resolution of 20m (we go up to 4m!) https://t.co/A9AH42k5wf', 'The implication is that existing 3D (grid-based) simulations are not too far off in their fast ejecta predictions. Hence neutron-powered precursors should be closer to the faint end of the Metzger+15 predictions, while still detectable with upcoming UV facilities like Ultrasat']",https://arxiv.org/abs/2108.08311,"We examine the effect of spatial resolution on initial mass ejection in grid-based hydrodynamic simulations of binary neutron star mergers. The subset of the dynamical ejecta with velocities greater than $\sim 0.6$c can generate an ultraviolet precursor to the kilonova on $\sim$hr timescales and contribute to a years-long non-thermal afterglow. Previous work has found differing amounts of this fast ejecta, by one- to two orders of magnitude, when using particle-based or grid-based hydrodynamic methods. Here we carry out a numerical experiment that models the merger as an axisymmetric collision in a co-rotating frame, accounting for Newtonian self-gravity, inertial forces, and gravitational wave losses. The lower computational cost allows us to reach spatial resolutions as high as $4$m, or $\sim 3\times 10^{-4}$ of the stellar radius. We find that fast ejecta production converges to within $10\%$ for a cell size of $20$m. This suggests that fast ejecta quantities found in existing grid-based merger simulations are unlikely to increase to the level needed to match particle-based results upon further resolution increases. The resulting neutron-powered precursors are in principle detectable out to distances $\lesssim 200$Mpc with upcoming facilities. We also find that head-on collisions at the free-fall speed, relevant for eccentric mergers, yield fast and slow ejecta quantities of order $10^{-2}M_\odot$, with a kilonova signature distinct from that of quasi-circular mergers. ","Resolving the fastest ejecta from binary Neutron Star mergers:
  implications for electromagnetic counterparts"
58,1428418174672797697,36714410,Yang Li,"[""My group has recently released the Screen2words dataset (<LINK>) along the paper (<LINK>, UIST'21). We have also applied our language grounding model to a new task for visual tutorial generation in HelpViz (<LINK>, UIST'21).""]",https://arxiv.org/abs/2108.03353,"Mobile User Interface Summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen, which can be useful for many language-based application scenarios. We present Screen2Words, a novel screen summarization approach that automatically encapsulates essential information of a UI screen into a coherent language phrase. Summarizing mobile screens requires a holistic understanding of the multi-modal data of mobile UIs, including text, image, structures as well as UI semantics, motivating our multi-modal learning approach. We collected and analyzed a large-scale screen summarization dataset annotated by human workers. Our dataset contains more than 112k language summarization across $\sim$22k unique UI screens. We then experimented with a set of deep models with different configurations. Our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate high-quality summaries for mobile screens. We demonstrate potential use cases of Screen2Words and open-source our dataset and model to lay the foundations for further bridging language and user interfaces. ",Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning
59,1428383830055006211,29931309,Gillian Hadfield,"['Critical new paper from @StanfordHAI on the massive AI models emerging largely inside private tech and beyond the current reach of oversight and regulation--a core focus also for @TorontoSRI working with @RockefellerFdn, @jackclarkSF  and @MarietjeSchaake  <LINK>']",https://arxiv.org/abs/2108.07258,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature. ",On the Opportunities and Risks of Foundation Models
60,1428347781618577415,254034757,Elias Aydi,"['Check out our new paper on the peculiar 2019 outburst of the 2005 classical nova V1047 Cen. The event is likely a new phenomenon, first time observed in such systems!!\narXiv link: \n<LINK>\n\n#astro_twitter #astronomy #novae <LINK>']",https://arxiv.org/abs/2108.07868?fbclid=IwAR0PNCSfBNGXo6EJcACvy9zbbXJrpKOrhvwRfc4A1HAflI3ksj64sq_wXlU,"We present a detailed study of the 2019 outburst of the cataclysmic variable V1047 Cen, which hosted a classical nova eruption in 2005. The peculiar outburst occurred 14 years after the classical nova event, lasted for more than 400 days, and reached an amplitude of around 6 magnitudes in the optical. Early spectral follow-up revealed what could be a dwarf nova (accretion disk instability) outburst in a classical nova system. However, the outburst duration, high velocity ($>$2000 km s$^{-1}$) features in the optical line profiles, luminous optical emission, and the presence of prominent long-lasting radio emission, together suggest a phenomenon more exotic and energetic than a dwarf nova outburst. There are striking similarities between this V1047 Cen outburst and those of ""combination novae"" in classical symbiotic stars. We suggest that the outburst may have started as a dwarf nova that led to the accretion of a massive disk, which in turn triggered enhanced nuclear shell burning on the white dwarf and eventually led to generation of a wind/outflow. From optical photometry we find a \bf{possible} orbital period of 8.36 days, which supports the combination nova scenario and makes the system an intermediate case between typical cataclysmic variables and classical symbiotic binaries. If true, such a phenomenon would be the first of its kind to occur in a system that has undergone a classical nova eruption and is intermediate between cataclysmic variables and symbiotic binaries. ","The 2019 outburst of the 2005 classical nova V1047 Cen: a record
  breaking dwarf nova outburst or a new phenomenon?"
61,1428193104528478212,1155359501337288706,Benjamin Stone,"['New pre-print time! In this paper we calculate three-point functions involving a superspin-2 current multiplet in 3D.\n\nWe observe a novel parity-violating tensor structure, which is rather interesting as such structures do not appear in lower spin cases.\n\n<LINK>', 'One of the central ideas is regarding the uniqueness of the energy momentum tensor. In particular it was shown by Maldacena and Zhiboedov that parity violating structures cannot appear in correlation functions of higher-spin currents in 3D provided the EM tensor is unique.', 'However due to supersymmetry, if we have a conserved tensor superfield for a given superspin (say s), then it contains component fields with spin-s and (s + 1/2). Hence if a superspin 3/2 and superspin 2 field both exist then the spin-2 component field is not unique.', 'The superspin-3/2 multiplet contains the energy momentum tensor that we know and love. However the superspin-2 multiplet contains a new spin-2 component field with similar properties.', 'The idea of this paper is as follows: if such a field were to exist, hypothetically speaking, how do the constraints of superconformal symmetry (i.e group theory) fix the structure of the correlation functions?']",https://arxiv.org/abs/2108.01865,"We consider $\mathcal{N=1}$ superconformal field theories in three-dimensions possessing a conserved current multiplet $\mathcal{F}_{ (\alpha_{1} \alpha_{2} \alpha_{3} \alpha_{4}) }$ which we refer to as the superspin-2 current multiplet. At the component level it contains a conserved spin-2 current different from the energy-momentum tensor and a conserved fermionic higher spin current of spin 5/2. Using a superspace formulation, we calculate correlation functions involving $\mathcal{F}$, focusing particularly on the three-point function $ \langle \mathcal{F} \mathcal{F} \mathcal{F} \rangle $. After imposing the constraints arising from conservation equations and invariance under permutation of superspace points, we find that the parity-even and parity-odd sectors of this three-point function are each fixed up to a single coefficient. The presence of the parity-odd contribution is rather non-trivial, as there is an apparent tension between supersymmetry and the existence of parity-odd structures. ","Three-point functions of a superspin-2 current multiplet in 3D, N=1
  superconformal theory"
62,1428155009057837059,3409898008,„Äà Berger | Dillon „Äâ,"['New paper is live. My first ever single-author paper!\n\n<LINK>', '@choice_fielder thanks!', ""@0Simalex2 that's okay neither do i"", '@revprez it keeps us from sounded self centered.. also we believe it just sounds better', '@7othayfah @revprez lol no.. it\'s just called the ""royal we"" and it just sounds better', ""@gravity_levity it just sounds weird to me lol.. idk i didn't even know it was like cool to do that haha"", ""@gravity_levity i feel like there's this very incorrect subconscious  belief that if i say 'we' then if something i said in the paper was dumb then it all doesn't fall solely on me""]",https://arxiv.org/abs/2108.07849,"We consider a model in which dark matter is a light (350 MeV) Dirac fermion which couples to quarks as an axial-vector current. At center-of-mass (CM) energies below the confinement scale, such a DM interaction structure may be described through its low energy effective couplings to mesons. In this paper we focus on dark matter annihilation into these final state mesons, and its resulting photon spectrum. We present a coarse-grained approach for analyzing our model in which we argue the meson spectrum solely from kinematics and symmetries and thereby estimate the total DM annihilation cross section that corresponds to a given expected photon signal. We then corroborate and refine these findings with the more systematic approach of the chiral Lagrangian description of the model, since the CM energies we consider admit such a perturbative treatment. In both cases, we obtain constraints on the model by comparing the estimated photon signals to current and future observations, and show that the axial-vector DM portal is significantly more conducive to photon production for lighter forms of dark matter than a pure vector-like portal; establishing itself as a prime candidate for indirect detection probes with significant discovery reach. ",Light Dark Matter Through the Quark Axial-Vector Current Portal
63,1428067320593719296,1256741993075503106,Camilo Ruiz,"['Excited to be part of a new paper with @StanfordHAI! Single AI models with state-of-the-art performance for a range of applications (i.e. #GPT3 ) are rapidly rising! Here, we explore the promises and risks of these ""Foundation Models""!\n<LINK> <LINK>', 'I coauthored the healthcare section along with @michiyasunaga, Jing Huang, @Zhang_Yu_hui, Giray Ogut, @saahil9jain, @williamwang28, @yusufroohani, @ren_hongyu, @ABosselut, @eadeli, @jure, @Rbaltman https://t.co/NhwFRS7K3D', 'Huge thanks to @percyliang and @RishiBommasani for organizing such a large scale collaboration across &gt;100 researchers!', 'Some early press coverage:\nhttps://t.co/idacV8IWku', 'https://t.co/8x2q0G6zIr', 'Workshop next week! https://t.co/cPWWm48tWt']",https://arxiv.org/abs/2108.07258,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature. ",On the Opportunities and Risks of Foundation Models
64,1427941644800823304,1446792746,Andrew Davison,"['Definitely worth reading at least the introduction to the new position paper from Stanford on Foundation Models: giving a name (well chosen I think) to the concept of extremely large, general purpose ML models which can be adapted for specific tasks.  <LINK> <LINK>']",https://arxiv.org/abs/2108.07258,"AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature. ",On the Opportunities and Risks of Foundation Models
65,1427862988485238791,10666172,Sabine Hossenfelder,['We have a new paper about superdeterminism on the arXiv today  <LINK>'],https://arxiv.org/abs/2108.07292,"Bell's theorem is often said to imply that quantum mechanics violates local causality, and that local causality cannot be restored with a hidden-variables theory. This however is only correct if the hidden-variables theory fulfils an assumption called Statistical Independence. Violations of Statistical Independence are commonly interpreted as correlations between the measurement settings and the hidden variables (which determine the measurement outcomes). Such correlations have been discarded as ""finetuning"" or a ""conspiracy"". We here point out that the common interpretation is at best physically ambiguous and at worst incorrect. The problem with the common interpretation is that Statistical Independence might be violated because of a non-trivial measure in state space, a possibility we propose to call ""supermeasured"". We use Invariant Set Theory as an example of a supermeasured theory that violates the Statistical Independence assumption in Bell's theorem without requiring correlations between hidden variables and measurement settings. ","Supermeasured: Violating Statistical Independence without violating
  statistical independence"
66,1427794393373626368,3874714693,augustus odena,"['New paper! <LINK>\nWe use big language models to synthesize computer programs, execute programs, solve math problems, and dialog with humans to iteratively refine code.\nThe models can solve 60% and 81% of the programming and math problems, respectively. A thread: <LINK>', 'First, we evaluate models from 244M to 137B params on a new data-set we created https://t.co/UvnLquoENP.\nNumber of problems solved scales pretty cleanly with model-size. https://t.co/AnZQd47e4L', ""Larger models not only solve problems that smaller models can't solve, they also more reliably solve easier problems that smaller models solve less frequently. https://t.co/Tflgg6WTC3"", 'We have a pretty thorough error analysis in the paper, but one thing I thought was especially fun is that the model sometimes ‚Äúcheats‚Äù and hard-codes an answer that passes the tests but does not solve the problem. https://t.co/eMOApB0L1W', 'Second, we evaluate whether these models can interact with a human to iteratively refine their outputs. We find that 4 turns of dialog with a human can double the number of problems solved by the model. https://t.co/GNFTEGJ0zB', 'Third, we try (and largely fail) to get language models to ‚Äòexecute‚Äô programs. This casts some doubt on the extent to which the models ‚Äòunderstand‚Äô the code they‚Äôre emitting. https://t.co/Nw81suyC9y', 'Fourth, we convert a dataset (MathQA) of math word problems into synthesis problems and show that this allows language models to solve a large majority of the problems. https://t.co/gVtfUEWPw7', 'There‚Äôs a bunch of other fun stuff in there, but these are the main things. Thanks to the other lead author @jacobaustin132, and all other authors: @Maxwell_Nye, Maarten Bosma, Henryk Michalewski, @dmdohan, Ellen Jiang, Carrie Cai, Michael Terry, @quocleix, and @RandomlyWalking']",https://arxiv.org/abs/2108.07732,"This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input. ",Program Synthesis with Large Language Models
67,1427703393573736449,859868975244693508,Luca Rigotti,"['Happy to announce a new paper with the very talented @RunnerVgn. My first foray into econometric theory, with a sprinkle of economic theory: Identification of Incomplete Preferences\n<LINK> \n1/N', 'We find the sharp identification region for discrete choice models in which individuals may have non complete preferences and only aggregate data is available (no individual data). 2/N', 'Intuitively, the fraction of consumers that choose a product places bounds on the fraction of individuals who strictly prefer that product, but not on those who cannot compare. 3/N', 'We show that the presence of an ""instrumental variable"" can help rule out the hypothesis that all consumers have complete preference. Attention sets can also shrink the identification region. 4/N', 'We apply our methods to the 2018 Supreme Court election in Ohio, where a substantial fraction of voters abstain even though they vote in other races, using the order of candidates on the ballot as our instrumental variable. 5/N', 'Obviously, I think the paper is very cool... but in Italy we say ‚ÄúOgni sacarrafone √® bello a mamma sua‚Äù (every cockroach is beautiful to its mom). So you will have to judge for yourself. 6/6', '@itaisher @RunnerVgn Thank you!', '@adikuva Thank you!']",https://arxiv.org/abs/2108.06282,"We provide a sharp identification region for discrete choice models in which consumers' preferences are not necessarily complete and only aggregate choice data is available to the analysts. Behavior with non complete preferences is modeled using an upper and a lower utility for each alternative so that non-comparability can arise. The identification region places intuitive bounds on the probability distribution of upper and lower utilities. We show that the existence of an instrumental variable can be used to reject the hypothesis that all consumers' preferences are complete, while attention sets can be used to rule out the hypothesis that all individuals cannot compare any two alternatives. We apply our methods to data from the 2018 mid-term elections in Ohio. ",Identification of Incomplete Preferences
68,1427662809563860994,942238055607435264,Luca Carlone,"['for a team of robots gathering information in a dynamic environment, is it better to transmit frequently or to let the robots spend more time collecting/processing data before transmitting? we address this #codesign problem in a new paper <LINK> #mitsparklab']",http://arxiv.org/abs/2108.03122,"We investigate the problem of co-designing computation and communication in a multi-agent system (e.g. a sensor network or a multi-robot team). We consider the realistic setting where each agent acquires sensor data and is capable of local processing before sending updates to a base station, which is in charge of making decisions or monitoring phenomena of interest in real time. Longer processing at an agent leads to more informative updates but also larger delays, giving rise to a delay-accuracy-tradeoff in choosing the right amount of local processing at each agent. We assume that the available communication resources are limited due to interference, bandwidth, and power constraints. Thus, a scheduling policy needs to be designed to suitably share the communication channel among the agents. To that end, we develop a general formulation to jointly optimize the local processing at the agents and the scheduling of transmissions. Our novel formulation leverages the notion of Age of Information to quantify the freshness of data and capture the delays caused by computation and communication. We develop efficient resource allocation algorithms using the Whittle index approach and demonstrate our proposed algorithms in two practical applications: multi-agent occupancy grid mapping in time-varying environments, and ride sharing in autonomous vehicle networks. Our experiments show that the proposed co-design approach leads to a substantial performance improvement (18-82% in our tests). ","Computation and Communication Co-Design for Real-Time Monitoring and
  Control in Multi-Agent Systems"
69,1427446085631451160,1093387119148462081,Daniel Green,['New paper: <LINK>\n\nThis is the outcome of (a) learning a lot of neutrino cosmology for the CMB-S4 Science Book and (b) needing an excuse to collaborate with some old friends\n\nKey idea: CMB lensing already constraints sub-percent of DM interacting with neutrinos <LINK>'],https://arxiv.org/abs/2108.06928,"The cosmic neutrino background is both a dramatic prediction of the hot Big Bang and a compelling target for current and future observations. The impact of relativistic neutrinos in the early universe has been observed at high significance in a number of cosmological probes. In addition, the non-zero mass of neutrinos alters the growth of structure at late times, and this signature is a target for a number of upcoming surveys. These measurements are sensitive to the physics of the neutrino and could be used to probe physics beyond the standard model in the neutrino sector. We explore an intriguing possibility where light right-handed neutrinos are coupled to all, or a fraction of, the dark matter through a mediator. In a wide range of parameter space, this interaction only becomes important at late times and is uniquely probed by late-time cosmological observables. Due to this coupling, the dark matter and neutrinos behave as a single fluid with a non-trivial sound speed, leading to a suppression of power on small scales. In current and near-term cosmological surveys, this signature is equivalent to an increase in the sum of the neutrino masses. Given current limits, we show that at most 0.5% of the dark matter could be coupled to neutrinos in this way. ",Neutrino Interactions in the Late Universe
70,1427283460893274114,838292815,Ofir Nachum,"['New paper: We apply ideas of VAE (or, info bottleneck) to RL. I.e., take a VAE and replace the bottlenecked input with future trajectory and replace prediction with agent\'s policy/value fn. This allows agent to ""look into the future"" without predicting it  <LINK>', ""The VAE/bottleneck's learned prior then gives you the policy/value fn during inference or data collection (when you can't look into the future). https://t.co/I2G0xiP35T"", 'The full algo ""PGIF"" seems pretty versatile and works across a number of domains: offline/online RL, sparse/delayed rewards, etc.\n\nThis paper is a collab w/ @DavidAVenuto, Lau, Precup; also follows a lot of great existing work on ""hindsight"" from Guez, Harutyunyan, Mesnard, etc. https://t.co/mWB1EkEyLi']",https://arxiv.org/abs/2108.02096,"Reasoning about the future -- understanding how decisions in the present time affect outcomes in the future -- is one of the central challenges for reinforcement learning (RL), especially in highly-stochastic or partially observable environments. While predicting the future directly is hard, in this work we introduce a method that allows an agent to ""look into the future"" without explicitly predicting it. Namely, we propose to allow an agent, during its training on past experience, to observe what \emph{actually} happened in the future at that time, while enforcing an information bottleneck to avoid the agent overly relying on this privileged information. This gives our agent the opportunity to utilize rich and useful information about the future trajectory dynamics in addition to the present. Our method, Policy Gradients Incorporating the Future (PGIF), is easy to implement and versatile, being applicable to virtually any policy gradient algorithm. We apply our proposed method to a number of off-the-shelf RL algorithms and show that PGIF is able to achieve higher reward faster in a variety of online and offline RL domains, as well as sparse-reward and partially observable environments. ",Policy Gradients Incorporating the Future
71,1427210696685756419,1425340957,Alex Haber,"['We (Mark Alford, Ziyuan Yhang, Steven Harris and me) recently published a new paper on beta equilibration in neutron star mergers: <LINK> . What is it about? 1/n', 'In a neutron star, weak interactions lead to various nucleonic reactions like neutron decay, electron capture and so on. These reactions change the chemical composition of the star, like the proton fraction (yes neutron stars do have some other particles, not only neutrons!) 2/n', 'If you wait long enough, these reactions will balance and the chemical composition stays constant. Traditionally, we have always studied that at T=0. Although neutron stars are quite hot by traditional standards (10^6 K), in a particle physics context they are actually very cold.', 'At T=0, the reactions balance each other when the chemical potential of the neutron equals the sum of the proton and electron chemical potential. But neutron star mergers require us to reexamine a lot of our prior assumptions and findings. 4/n', 'Mergers are much hotter than isolated, old neutron stars. We find that in these conditions, the traditional beta equilibrium condition needs to be modified. The correction term reach magnitudes of more than 10 MeV. 5/n', 'The corrected nucleonic rates change by up to an order of magnitude. How our results influence cooling or the outcome of neutron star mergers remains to be seen, but there is interesting work from @ProfNilsAnd and collaborators coming  6/6']",https://arxiv.org/abs/2108.03324,"We calculate the nonzero-temperature correction to the beta equilibrium condition in nuclear matter under neutron star merger conditions, in the temperature range $1\,$MeV$ < T \lesssim 5\,$MeV. We improve on previous work by using a consistent description of nuclear matter based on the IUF and SFHo relativistic mean field models. This includes using relativistic dispersion relations for the nucleons, which we show is essential in these models. We find that the nonzero-temperature correction can be of order $10$ to $20\,$MeV, and plays an important role in the correct calculation of Urca rates, which can be wrong by factors of $10$ or more if it is neglected. ",Beta equilibrium under neutron star merger conditions
72,1427210038347714561,1214477029053214720,Jan Kukaƒçka üá∫üá¶,"['Self-supervised learning has gained lots of attention recently. But what can a network really learn from unlabeled images of the retina? And is it useful for image segmentation? Good thing our new paper is out, with all the answers <LINK> 1/6 <LINK>', 'We used contrastive self-supervised learning to train a small convnet on Kaggle-DR dataset. Fascinatingly, without being provided any labels at all, the network learned to recognize various anatomical/pathological structures in the fundus images. 2/6 https://t.co/LNJpBotlax', 'Using this pre-trained network as the encoder of a U-Net led to improvements in image segmentation performance, compared to a U-Net trained from scratch. 3/6 https://t.co/yFjICKcS6Y', 'The improvement was greater in few-shot scenarios. Moreover, the pre-trained networks converged significantly faster. 4/6 https://t.co/rU9KAF3NhO', 'What is it good for? Glad you ask! It is impossible to have a large annotated dataset for every camera, pathology, and population. Self-supervised learning seems as an approach that could scale well with abundant unlabeled data and produce representations 5/6', ""which are robust and can be adapted with few annotated samples to new devices etc. Finally, big shout-out to üë©\u200düî¨ Anja whose master's thesis is behind large portion of this paper! 6/6""]",https://arxiv.org/abs/2108.02798,"Fundus photography is the primary method for retinal imaging and essential for diabetic retinopathy prevention. Automated segmentation of fundus photographs would improve the quality, capacity, and cost-effectiveness of eye care screening programs. However, current segmentation methods are not robust towards the diversity in imaging conditions and pathologies typical for real-world clinical applications. To overcome these limitations, we utilized contrastive self-supervised learning to exploit the large variety of unlabeled fundus images in the publicly available EyePACS dataset. We pre-trained an encoder of a U-Net, which we later fine-tuned on several retinal vessel and lesion segmentation datasets. We demonstrate for the first time that by using contrastive self-supervised learning, the pre-trained network can recognize blood vessels, optic disc, fovea, and various lesions without being provided any labels. Furthermore, when fine-tuned on a downstream blood vessel segmentation task, such pre-trained networks achieve state-of-the-art performance on images from different datasets. Additionally, the pre-training also leads to shorter training times and an improved few-shot performance on both blood vessel and lesion segmentation tasks. Altogether, our results showcase the benefits of contrastive self-supervised pre-training which can play a crucial role in real-world clinical applications requiring robust models able to adapt to new devices with only a few annotated samples. ","Self-Supervised Learning from Unlabeled Fundus Photographs Improves
  Segmentation of the Retina"
73,1426932455806849034,33522912,Elliott Ash,"['A short thread introducing new work with @PinchOfData and @phinifa:\n\n‚ÄúText Semantics Capture Political and Economic Narratives‚Äù\n\nPaper: <LINK>\nRepo: <LINK>\nDemo: <LINK> <LINK>', 'Human beings are storytellers. \n\nIt‚Äôs no wonder then that social scientists are increasingly interested in narratives -- the stories we tell in fiction, politics, and life -- and how they shape beliefs, behavior, and government policies.\n\ne.g. @RobertJShiller https://t.co/2MpQ5G9EBf', 'Narratives are obscure to social scientists because they consist of information, so the physical manifestations are spoken or written language. https://t.co/4qehFArvU3', 'More specifically, a narrative is an ‚Äúaccount of a series of events, facts, etc., given in order and with the establishing of connections between them‚Äù (@OED).\n\nYet existing text-as-data approaches do not account for ""who"" does ""what"" to ""whom"". https://t.co/cnjlNeYu5P', 'We provide an approach for extracting narratives from text. \n\nFirst, we use semantic role labeling (@ai2_allennlp) to extract the semantic roles of agent, verb, and patient. The agent is the entity that performs an action, while the patient is the entity acted upon. https://t.co/j0ito1gG9M', 'The set of agents and patients is high-dimensional (typically millions of plain-text phrases).\n\nWe use named entity recognition (@spacy_io) to identify specific individuals and organizations. The remaining phrases are embedded (@gensim_py) and then clustered (@scikit_learn). https://t.co/ZQGdvkx5VR', 'The resulting unsupervised pipeline takes in a plain-text corpus and outputs interpretable narratives representing the core claims.\n\nIn the paper, we construct narratives from floor speeches in U.S. Congress. https://t.co/dQHBXClhNc', 'Some narratives are simple (e.g. ‚Äúimmigrants steal jobs‚Äù), but others are complex and interconnected. \n\nWe use a graph-based approach to build networks of connected entities, representing the larger narrative structures ‚Äî or worldviews ‚Äî expressed in a corpus. https://t.co/iAA3U2fBUZ', 'Check out this interactive worldview graph constructed from  Trump‚Äôs tweet archive: https://t.co/zYa7iNKM1p\n\n#networkx #pyvis', 'The pipeline has a lot going on under the hood. We provide a python package  Ä·¥á ü·¥Ä·¥õ…™·¥è that makes it easy to use.\n\nRepo: https://t.co/SqT48s9o0O\nDemo Notebook: https://t.co/SL9qzZdH8z\n\nSpecial thanks to @AndreiPlamada  and @ETH_SIS for indispensable contributions to the package! https://t.co/ajLcjLTtvm', 'In the paper, we apply the method to over a million speeches given in U.S. Congress for the period 1994-2015. We show dynamics, sentiment, and partisanship in the narratives. https://t.co/kftUEhta39', 'In particular, we show the most divisive policy narratives.\n\nFor example, ‚ÄúOil‚Äù: Democrats say ‚Äúoil makes profit‚Äù while Republicans say ‚Äúoil creates jobs‚Äù.\n\nOr ‚ÄúJobs‚Äù: Democrats say ‚Äúcompanies ship jobs‚Äù while Republicans say ‚Äútaxes kill jobs‚Äù. https://t.co/SiJvQwctUF', 'Section 4 discusses the potential and limitations of the approach. One thing we are excited about is how  Ä·¥á ü·¥Ä·¥õ…™·¥è could be used to support qualitative analysis of narratives, not just in social science but also in history and the humanities.\n\nFeedback welcome!', 'A special shout-out to teammates @phinifa and @PinchOfData, talented upcoming economists, grand co-authors, and a delight to work with.\n\nThe project originated at #SICSS Zurich 2019! \n@msalganik @chris_bail \n\nAnd thanks to @snsf_ch for Spark funding.', 'cc @jurafsky @nlpnoah @KamiarMohaddes @dbamman @Noahpinion @rodrikdani @mollyeroberts @justingrimmer @b_m_stewart @MattGrossmann']",https://arxiv.org/abs/2108.01720,"Social scientists have become increasingly interested in how narratives -- the stories in fiction, politics, and life -- shape beliefs, behavior, and government policies. This paper provides an unsupervised method to quantify latent narrative structures in text documents. Our new software package RELATIO identifies coherent entity groups and maps explicit relations between them in the text. We provide an application to the United States Congressional Record to analyze political and economic narratives in recent decades. Our analysis highlights the dynamics, sentiment, polarization, and interconnectedness of narratives in political discourse. ",RELATIO: Text Semantics Capture Political and Economic Narratives
74,1426272540033437697,1240369991888896003,Riley Simmons-Edler,"['Time for another new paper thread: Introducing ""AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection"" to appear at IROS next month! <LINK> <LINK>', 'Proximity sensing is important for robots and other mobile (and static) agents (for collision avoidance, navigation in clutter, etc), but a big challenge is sensor coverage- complex agents (like robot arms) can easily have sensor blind spots.', 'This can come up surprisingly often in practice- Consider what happens when you reach into a drawer or bin to grab something that is partially occluded. Dense-coverage proximity sensing (navigating by touch, for humans) is needed.', 'To get dense coverage for robots, we developed a hardware+ML system that detects proximity across the entire surface of the robot (no dead spots) using a single pair of (cheap and small) ultrasonic transducers. https://t.co/g7SSPAyHDL', 'The key hardware trick is that we couple the transducers to the robot, so they make the robot vibrate like a speaker into the air. Objects close to the robot will interfere with the waves on the robot, and can be detected by a second sensor. https://t.co/EkkTpQ3YaD', 'This is particularly useful because the interference is not position-sensitive: We can detect it anywhere along the surface of the robot, including the backside of the object and far from the receiver (tested out to 20 cm) https://t.co/XA9koiUpQW', ""We also tested this on a number of objects (dummy robots) of differing shape/size/composition, and found that it works almost everywhere (metal foils and thin wood didn't work, thin PVC works but not as well). https://t.co/Gxo5CkpnHs"", ""It's also pretty good at detecting diverse objects- All the objects in our test set (not seen during training) were detected with &gt;90% accuracy on a moving robot. Since submission the only thing we've found that doesn't work is very open/fine mesh structures. https://t.co/dE09Yjmf1a"", 'One key detail in making this work (where I came in) is that a moving robot will detect itself as it moves, so we need to identify which signals are self-detection and ignore those. In addition, the signals can be small and noisy. Enter machine learning.', 'Surprisingly, my first pass at the problem using binary classification (proximity/no proximity) with a small 1D CNN worked really well- I wish I could tell you it was a big ML challenge, but it seems that self-proximity is pretty easy to identify with a modest dataset.üòÖ https://t.co/wXtRaFpBt0', ""This also suggests that there's a lot of information about the object being detected in the signal, so some interesting applications like identifying or reconstructing objects might be possible (though data collection is tricky here)."", ""This project was a bit different from most of my work, since it didn't involve RL and is hardware-focused, but I think there's a ton of potential here. There's a lot of unexplored ways these sensors could be used, and RL could help get around the lack of ground truth data."", ""What's more, this setup is extremely practical: We basically hot glued some $3 transducers to a robot and connected them to a commodity DAC, then piped the signals through a modest CNN. It can be cheaply implemented on any robot, and it's easy to imagine use in production. https://t.co/Imh8VacwO3"", 'Thanks for reading! This was work I did at SAIC-NY with my fantastic collaborators Xiaoran (Van) Fan, Daewon Lee, Larry Jackel, Rich Howard, and Dan Lee (none of whom are on twitter, SMH). Check out our paper for more! https://t.co/erlQINfevp']",https://arxiv.org/abs/2108.04867,"Perceiving obstacles and avoiding collisions is fundamental to the safe operation of a robot system, particularly when the robot must operate in highly dynamic human environments. Proximity detection using on-robot sensors can be used to avoid or mitigate impending collisions. However, existing proximity sensing methods are orientation and placement dependent, resulting in blind spots even with large numbers of sensors. In this paper, we introduce the phenomenon of the Leaky Surface Wave (LSW), a novel sensing modality, and present AuraSense, a proximity detection system using the LSW. AuraSense is the first system to realize no-dead-spot proximity sensing for robot arms. It requires only a single pair of piezoelectric transducers, and can easily be applied to off-the-shelf robots with minimal modifications. We further introduce a set of signal processing techniques and a lightweight neural network to address the unique challenges in using the LSW for proximity sensing. Finally, we demonstrate a prototype system consisting of a single piezoelectric element pair on a robot manipulator, which validates our design. We conducted several micro benchmark experiments and performed more than 2000 on-robot proximity detection trials with various potential robot arm materials, colliding objects, approach patterns, and robot movement patterns. AuraSense achieves 100% and 95.3% true positive proximity detection rates when the arm approaches static and mobile obstacles respectively, with a true negative rate over 99%, showing the real-world viability of this system. ",AuraSense: Robot Collision Avoidance by Full Surface Proximity Detection
75,1426259065202692103,1425872270627639303,or castel,"['*New #NLProc paper alert!*\n \nWe examine greedy decoding for extractive QA, by comparing it with our optimal algorithm: exact-extract. We found out that given just a few examples - greedy decoding does a great job!\nPaper: <LINK>\n \n1/N', 'Greedy decoding is used with great success for extractive QA. But it is not necessarily extractive (=generates spans from the passage) nor exact (=produces the max prob span). Can we do better?\n\nWith @ori__ram  @AviaEfrat  and @omerlevy_ \n\nSummary Thread:\n\n2/N', 'We present a novel decoding algorithm: *exact-extract*, a dynamic programming algorithm that efficiently calculates the probability of all possible spans from the input passage, enabling us to find the most probable one.\n\n3/N', 'We first examine T5‚Äôs performance in a few-shot setting. Given only 16 examples, the model reaches ~82% F1 on SQuAD. 1024 examples are enough to reach human performance (91% F1); all this while using the naive greedy decoding, matching our optimal algorithm quite quickly. \n\n4/N https://t.co/0msfiP5UCk', 'On some datasets the gap is more substantial - but it shrinks down at 1024 examples too.\n\n5/N https://t.co/EBQP9pj8vn', 'This is not the case for the zero-shot setting, where exact-extract obtains 60% F1 on SQuAD, without any fine-tuning (10 points over greedy decoding).\n\n6/N', 'We have plenty more analyses, results, and even a lightweight pretraining method for improving greedy decoding in zero-shot.\n\n7/N', 'Overall, our results show that the naive greedy decoding is nearly as good as the optimal strategy even when only a handful of labeled examples are available; and that pretrained language models can be easily adapted to extractive QA and with great results. \n\n8/8', 'and the paper, since tweeter is hiding the first tweetü§® : https://t.co/Rfo8eIZs65', 'and the paper, since tweeter is hiding the first tweet ü§® : https://t.co/Rfo8eIZs65']",https://arxiv.org/abs/2108.05857,"Fine-tuned language models use greedy decoding to answer reading comprehension questions with relative success. However, this approach does not ensure that the answer is a span in the given passage, nor does it guarantee that it is the most probable one. Does greedy decoding actually perform worse than an algorithm that does adhere to these properties? To study the performance and optimality of greedy decoding, we present exact-extract, a decoding algorithm that efficiently finds the most probable answer span in the context. We compare the performance of T5 with both decoding algorithms on zero-shot and few-shot extractive question answering. When no training examples are available, exact-extract significantly outperforms greedy decoding. However, greedy decoding quickly converges towards the performance of exact-extract with the introduction of a few training examples, becoming more extractive and increasingly likelier to generate the most probable span as the training set grows. We also show that self-supervised training can bias the model towards extractive behavior, increasing performance in the zero-shot setting without resorting to annotated examples. Overall, our results suggest that pretrained language models are so good at adapting to extractive question answering, that it is often enough to fine-tune on a small training set for the greedy algorithm to emulate the optimal decoding strategy. ",How Optimal is Greedy Decoding for Extractive Question Answering?
76,1426179742957543426,1003652696723873792,Max Gaspari,"['New paper on AGN feeding &amp; feedback, done with a brilliant young PD (F. Maccagni)! \nOne of the best probes of chaotic cold accretion (raining on galaxies/BHs), with many multiphase thermo-kinematical diagnostics (e.g. k-plot). \n#astronomy #BlackHoleWeather\n<LINK>']",https://arxiv.org/abs/2108.05247,"We present a multi-wavelength study of the gaseous medium surrounding the nearby active galactic nucleus (AGN) Fornax A. Using MeerKAT, ALMA and MUSE observations we reveal a complex distribution of the atomic (HI), molecular (CO), and ionised gas in its centre and along the radio jets. By studying the multi-scale kinematics of the multi-phase gas, we reveal the presence of concurrent AGN feeding and feedback phenomena. Several clouds and an extended 3 kpc filament -- perpendicular to the radio jets and the inner disk ($r\lesssim 4.5$ kpc) -- show highly-turbulent kinematics, which likely induces nonlinear condensation and subsequent Chaotic Cold Accretion (CCA) onto the AGN. In the wake of the radio jets and in an external ($r\gtrsim 4.5$ kpc) ring, we identify an entrained massive ($\sim$ $10^7$ M$_\odot$) multi-phase outflow ($v_{\rm OUT}\sim 2000$ km s$^{-1}$). The rapid flickering of the nuclear activity of Fornax A ($\sim$ 3 Myr) and the gas experiencing turbulent condensation raining onto the AGN provide quantitative evidence that a recurrent, tight feeding and feedback cycle may be self-regulating the activity of Fornax A, in agreement with CCA simulations. To date, this is one of the most in-depth probes of such a mechanism, paving the way to apply these precise diagnostics to a larger sample of nearby AGN hosts and their multi-phase ISM. ","AGN feeding and feedback in Fornax A: kinematical analysis of the
  multi-phase ISM"
77,1426173293791436800,1191642560,Karl Rohe,"['new paper!\n\nPCA, k-means, and related techniques all need you to provide k. How do you choose it?\n\nWe have a new solution for social networks. we call it ‚Äúcross-validated eigenvalues‚Äù\n\nThis thread explains why this problem is hard and our solutionüßµ\n\n<LINK> <LINK>', 'The first thing to know:\n\nEigenvectors in social networks find latent social dimensions\n\nFor example, in the eigenvectors of realdonald followers during the 2016 election, some vectors were left-right politics, others were about NASCAR or other stuff üöó \n\nhttps://t.co/C7NP9J4Z2u', 'But how many dimensions/eigenvectors should we look for??? \n\nThat‚Äôs k.\n\nThis is hard because some dimensions are easy to estimate, some dimensions are impossible to estimate, and other dimensions can sorta-kinda be estimated. \n\nWe need a methodology that adapts to this spectrum.', 'There is rich theory when k=2 saying exactly when a dimension is estimable. In practice, k is much bigger and there are so many dimensions on the cusp of detectability. \n\nCan we find this cusp in data?\n\nYes.  Now you can.', 'Our procedure provides a p-value for each eigenvector, for the null hypothesis:\n\nH_0:  this eigenvector is just noise. \n\nHere is how‚Ä¶', 'We study the adjacency matrix A\n\nElement ij is 1 if {i is friends with j}. Otherwise, element ij is zero.\n\nConsider this statistical model for A:\n1) everyone is assigned a latent position Z_i in R^k\n2) P( A_ij = 1) = &lt;Z_i, Z_j&gt; (inner product)\n3) friendships are then independent https://t.co/WuVTRBdB7p', 'Suppose we see the expected matrix E(A), the matrix of edge probabilities.  \n\nWe could compute k by computing the rank of E(A) (i.e. the number of non-zero eigenvalues).', 'Unfortunately , we only get A. It has noise.  So, instead a common diagnostic looks for a ‚Äúgap‚Äù or an ‚Äúelbow‚Äù in the eigenvalues of A, plotted in decreasing order: https://t.co/mS3LfdWmDk', 'Because of *overfitting*‚Ä¶ the gap/elbow is often not visible in the scree plot.\n\nOverfitting??  Yes, overfitting.', 'To see why, recall that eigenvectors are the solution to an optimization problem\n\nThe eigenvalue is the value of that function *at its maximum* and we maximize over a large space of possible vectors. \n\nThis is a recipe for overfitting https://t.co/IyPKOZQhFa', ""The hidden key to our paper is that x'E(A)x reveals whether x is important.  In particular, if it is zero, then x is just noise\n\nUnfortunately, overfitting makes the eigenvalues x‚ÄôAx much larger than x‚ÄôE(A)x. This bias makes the eigenvalues less useful... blurring the gap/elbow"", 'Analogously, in regression, the *training* MSE is a poor estimate of prediction MSE.  This is due to overfitting.  To fix that, we cross validate.\n\nIn our setting, to form a better estimate of x‚ÄôE(A)x, we also cross validate.', 'But how do you split a social network into a ‚Äútraining‚Äù and ‚Äútesting‚Äù network??\n\nIt turns out that for our purposes, it is very easy. \n\nTo see why, a little bit of math goes a long way.', 'We observe the adjacency matrix A.  \n\nFor every friendship in A, send the friendship to either the ‚Äútraining adjacency matrix‚Äù B or ‚Äútesting adjacency matrix‚Äù C by flipping a fair coin.\n\nBoth B and C have same dimension as A, just fewer edges.  \n\nB+C=A', 'Here is the little bit of math:\n\nE(A) = 2 E(B) = 2 E(C)\n\nThe 2 comes from the fair coin flip.\n\nThis means that in expectation, they have the same eigenvectors *AND* the same number of non-zero eigenvalues k. https://t.co/n8kxIrYr2x', 'Moreover, under certain conditions\n\nB and C are independent \n\n(or very very close to independent. see paper for more) https://t.co/Cj2qJPFoSr', 'Computing cross-validated eigenvalues is simple.\n\n1. Split the graph\n2. compute a bunch of eigenvectors x of B. \n3. For each one, compute the ‚Äúcross-validated eigenvalue‚Äù:  \n\nx‚Äô C x', 'When B and C are independent, \n\nE( x‚Äô C x | B ) = x‚ÄôE(A)x /2\n\nThis means that 2 times the cross validated eigenvalue is an unbiased estimator of x‚Äô E(A) x (the thing we want)', 'Moreover, when they are independent, we have a CLT for this CV eigenvalue. So, we provide a p-value for the null hypothesis \n\nNull: x‚ÄôE(A)x = 0\n\nThis is a wild null hypothesis because it is specific to the sample eigenvector x computed on B.  So specific!', 'In the simulation screeplot above, k=128; the orange line gives the eigenvalues of E(A)\n\nUnfortunately, only about 50 of those eigenvectors are detectible\n\nThe red line gives x‚Äô E(C) x and the blue line gives x‚Äô C x\n\nSo, CV eigenvalues adapt to the difficulty of the problem! https://t.co/aKrTgnT5Za', 'Something we find in large empirical social networks is that so so many dimensions are statistically significant\n\nOn a citation graph among 22k academic journals, over 150 dimensions are highly statistically significant: https://t.co/9FKl0vCFYQ', 'In summary, eigenvectors overfit\n\nCross-validate your eigenvalues to get better summaries\n\nIf you want, you can get p-value for each eigenvector!\n\nhttps://t.co/MgWqhae6lf', 'This was a super fun collaboration with @fchen365 , Sebastien Roch, and Shuqi Yu', 'If you want to know more about using eigenvectors to explore data, check out this thread of my threads...\n\nhttps://t.co/MC6do1oaVM', '@bmwiernik Thanks!']",https://arxiv.org/abs/2108.03336,"In applied multivariate statistics, estimating the number of latent dimensions or the number of clusters is a fundamental and recurring problem. One common diagnostic is the scree plot, which shows the largest eigenvalues of the data matrix; the user searches for a ""gap"" or ""elbow"" in the decreasing eigenvalues; unfortunately, these patterns can hide beneath the bias of the sample eigenvalues. This methodological problem is conceptually difficult because, in many situations, there is only enough signal to detect a subset of the $k$ population dimensions/eigenvectors. In this situation, one could argue that the correct choice of $k$ is the number of detectable dimensions. We alleviate these problems with cross-validated eigenvalues. Under a large class of random graph models, without any parametric assumptions, we provide a p-value for each sample eigenvector. It tests the null hypothesis that this sample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent dimensions. This approach naturally adapts to problems where some dimensions are not statistically detectable. In scenarios where all $k$ dimensions can be estimated, we prove that our procedure consistently estimates $k$. In simulations and a data example, the proposed estimator compares favorably to alternative approaches in both computational and statistical performance. ",Estimating Graph Dimension with Cross-validated Eigenvalues
78,1426129957885992964,1575687696,Dr. Darko Donevski,"[""If you're interested in massive galaxies, there is a new paper from our group led by @Sissaschool PhD student Lara Pantoni. The study is on today's @arxiv as <LINK>. It explores CO gas properties to understand the evolution of some fascinating, distant objects.""]",https://arxiv.org/abs/2108.05596,"We present the ALMA view of 11 main-sequence DSFGs, (sub-)millimeter selected in the GOODS-S field, and spectroscopically confirmed to be at the peak of Cosmic SFH (z = 2-3). Our study combines the analysis of galaxy SED with ALMA continuum and CO spectral emission, by using ALMA Science Archive products at the highest spatial resolution currently available for our sample (< 1 arcsec). We include galaxy multi-band images and photometry (in the optical, radio and X-rays) to investigate the interlink between dusty, gaseous and stellar components and the eventual presence of AGN. We use multi-band sizes and morphologies to gain an insight on the processes that lead galaxy evolution, e.g. gas condensation, star formation, AGN feedback. The 11 DSFGs are very compact in the (sub-)millimeter (median r(ALMA) = 1.15 kpc), while the optical emission extends tolarger radii (median r(H)/r(ALMA) = 2.05). CO lines reveal the presence of a rotating disc of molecular gas, but we can not exclude either the presence of interactions and/or molecular outflows. Images at higher (spectral and spatial) resolution are needed to disentangle from the possible scenarios. Most of the galaxies are caught in the compaction phase, when gas cools and falls into galaxy centre, fuelling the dusty burst of star formation and the growing nucleus. We expect these DSFGs to be the high-zstar-forming counterparts of massive quiescent galaxies. Some features of CO emission in three galaxies are suggestive of forthcoming/ongoing AGN feedback, that is thought to trigger the morphological transition from star-forming disks to ETGs. ","An ALMA view of 11 Dusty Star Forming Galaxies at the peak of Cosmic
  Star Formation History"
79,1426119061671276549,15068044,Daniel Beck,"[""So we have a new paper, spearheaded by Joe Han, who just finished his Masters under mine and @trevorcohn's supervision.\n\nIf you're looking into new ways of evaluating diversity in generation you should definitely take a look on what we propose here. (1/n)\n<LINK>"", ""@trevorcohn This will appear at INLG 2021.\n\nWe propose a new way to evaluate diversity by grounding it on quality. Our rationale is that a diverse set of generated sentences is only good if it's also of good quality.\n\nHow to evaluate both jointly? Use multiple references. (2/n)"", '@trevorcohn We assume the ""gold standard"" of diversity is reflected in multiple references. So the goal of diversity is to ""cover"" the reference set. Assuming a sentence-level metric, we turn this into a maximum matching problem. (3/n)', '@trevorcohn This approach has a number of perks. For instance, a perfect model that generates the reference set exactly will achieve maximum score. But a model that generates N good sentences that are just slight variations of each other will be penalised. (4/n)', 'It is also completely agnostic of whatever sentence-level metric is used, as long as it is bounded. So this can be used for a range of generation tasks. (5/n)', ""One limitation is the reliance on the reference set: if it's not diverse enough then the method will not give preference to more diverse models, even if they give good quality sentences. (6/n)"", ""We also don't really test this against an implicit human evaluation of diversity (as quality metrics do). This is something we certainly would love to do in the future. However, it's not clear to us how to even define diversity intrinsically... (7/n)"", 'This is why we focus on the reference set. Our intuition is that it gives an ""extrinsic"" measure of diversity. It is not without its drawbacks (as I mention above) but it give us more information compared to other diversity metrics we know about. (8/n)', 'Anyways, check our paper for more info. Happy to discuss more about diversity in generation if you have ideas =). (9/9)', 'PS: yes, we know we messed up the references in the current arXiv version... we will update it shortly... =S']",https://arxiv.org/abs/2108.05659,"Text generation from semantic graphs is traditionally performed with deterministic methods, which generate a unique description given an input graph. However, the generation problem admits a range of acceptable textual outputs, exhibiting lexical, syntactic and semantic variation. To address this disconnect, we present two main contributions. First, we propose a stochastic graph-to-text model, incorporating a latent variable in an encoder-decoder model, and its use in an ensemble. Second, to assess the diversity of the generated sentences, we propose a new automatic evaluation metric which jointly evaluates output diversity and quality in a multi-reference setting. We evaluate the models on WebNLG datasets in English and Russian, and show an ensemble of stochastic models produces diverse sets of generated sentences, while retaining similar quality to state-of-the-art models. ",Generating Diverse Descriptions from Semantic Graphs
80,1426101099971284995,42604759,Oliver Obst,"[""Machine learning is already used to improve performance of athletes. Modelling energy and recovery involves exhausting tests, so established models are simple - but it doesn't mean they can't be improved. Fabian Weigend has just done that in our new paper: <LINK> <LINK>""]",https://arxiv.org/abs/2108.04510,"Data Science advances in sports commonly involve ""big data"", i.e., large sport-related data sets. However, such big data sets are not always available, necessitating specialized models that apply to relatively few observations. One important area of sport-science research that features small data sets is the study of energy recovery from exercise. In this area, models are typically fitted to data collected from exhaustive exercise test protocols, which athletes can perform only a few times. Recent findings highlight that established recovery models like W' balance (W'bal) models are too simple to adequately fit observed trends in the data. Therefore, we investigated a hydraulic model that requires the same few data points as W'bal models to be applied, but promises to predict recovery dynamics more accurately. To compare the hydraulic model to established W'bal models, we retrospectively applied them to a compilation of data from published studies. In total, one hydraulic model and three W'bal models were compared on data extracted from five studies. The hydraulic model outperformed established W'bal models on all defined metrics, even those that penalize models featuring higher numbers of parameters. These results incentivize further investigation of the hydraulic model as a new alternative to established performance models of energy recovery. ","A hydraulic model outperforms work-balance models for predicting
  recovery kinetics from intermittent exercise"
81,1425997725485518851,38824024,Aman Gupta,"['Introducing LAWN, a new family of optimizers for deep learning. LAWN can be added to most base optimizers like Adam and SGD to improve generalization performance, even at extremely large batch sizes! \n\nLink to paper - <LINK> <LINK>', '(2/n) For recommendation systems, we were able to scale to almost full batch training! For MovieLens-1M, we scaled to a batch size of 1 million with strong generalization performance.']",https://arxiv.org/abs/2108.05839,"Over-parameterized deep networks trained using gradient-based optimizers are a popular choice for solving classification and ranking problems. Without appropriately tuned $\ell_2$ regularization or weight decay, such networks have the tendency to make output scores (logits) and network weights large, causing training loss to become too small and the network to lose its adaptivity (ability to move around) in the parameter space. Although regularization is typically understood from an overfitting perspective, we highlight its role in making the network more adaptive and enabling it to escape more easily from weights that generalize poorly. To provide such a capability, we propose a method called Logit Attenuating Weight Normalization (LAWN), that can be stacked onto any gradient-based optimizer. LAWN controls the logits by constraining the weight norms of layers in the final homogeneous sub-network. Empirically, we show that the resulting LAWN variant of the optimizer makes a deep network more adaptive to finding minimas with superior generalization performance on large-scale image classification and recommender systems. While LAWN is particularly impressive in improving Adam, it greatly improves all optimizers when used with large batch sizes ",Logit Attenuating Weight Normalization
82,1425980940279554050,824425573330067458,M.-M. Mac Low,"['New paper (<LINK>): we simulated magnetic fields in these clouds of dense interstellar gas to see if the fields could get strong enough to prevent gravity from causing cloud collapse. (zoom boxes below are each 100 pc = 320 light years on a side). 1/2 <LINK>', 'We found that magnetic (green) and thermal (yellow) pressures were close to equal and far stronger than gravity (gravity) in low-density gas, but in the dense gas in the centers of the clouds gravity always won (time from top to bottom for 3 different clouds) 2/2 https://t.co/8RI0qaBMBy', 'PS Full simulation data is available in the @AMNH Library repository at https://t.co/L8GeGDTMeh  3/2', 'I should have said ""magnetic and thermal pressure *gradients*"", as this is a plot of acceleration terms (forces), not energies.']",https://arxiv.org/abs/2108.04967,"Magnetic fields are dynamically important in the diffuse interstellar medium. Understanding how gravitationally bound, star-forming clouds form requires modeling of the fields in a self-consistent, supernova-driven, turbulent, magnetized, stratified disk. We employ the FLASH magnetohydrodynamics code to follow the formation and early evolution of clouds with final masses of 3-8 $\times 10^3 M_{\odot}$ within such a simulation. We use the code's adaptive mesh refinement capabilities to concentrate numerical resolution in zoom-in regions covering single clouds, allowing us to investigate the detailed dynamics and field structure of individual self-gravitating clouds in a consistent background medium. Our goal is to test the hypothesis that dense clouds are dynamically evolving objects far from magnetohydrostatic equilibrium. We find that the cloud envelopes are magnetically supported with field lines parallel to density gradients and flow velocity, as indicated by the histogram of relative orientations and other statistical measures. In contrast, the dense cores of the clouds are gravitationally dominated, with gravitational energy exceeding internal, kinetic, or magnetic energy and accelerations due to gravity exceeding those due to magnetic or thermal pressure gradients. In these regions field directions vary strongly, with a slight preference towards being perpendicular to density gradients, as shown by three-dimensional histograms of relative orientation. ",Gravity Versus Magnetic Fields in Forming Molecular Clouds
83,1425837764545757186,204599207,Forrester Cole,"['We have a new paper on differentiable rendering! The core idea is to produce differentiable occlusions by using non-diff. rasterization to point-sample the geometry, then differentiable splatting to draw the points to the screen. 1/6 <LINK>', ""This approach uses autodiff for all derivatives, so it's simple to implement and needs no custom CUDA code. Even with autodiff, it renders complex geometry as fast or faster than any other diff. rasterizer and easily handles millions of faces. 2/6 https://t.co/9y4XavLNfF"", 'But the real payoff is that it can handle any kind of surface you can rasterize, without needing to push derivatives through the rasterization step. So we can optimize B-Spline surfaces without worrying about differentiating through tessellation... 3/6 https://t.co/cnzAnvUqy3', '... and extract implicit surfaces using Marching Cubes without worrying about derivatives, allowing us to optimize SDF surfaces with weird topology, like Metaballs. 4/6 https://t.co/BqrqbBXBtm', ""Finally, we can render (and optimize!) everyone's favorite implicit surface representation, NeRF, by rendering an isosurface instead of ray marching. Surface-based rendering reduces the number of samples needed per ray to 1, down from 128 or more. 5/6 https://t.co/malBSMzuAX"", 'The mesh-based version is implemented in TensorFlow Graphics and you can try it out right now in this Colab: https://t.co/wGZ0dz1LCV\n\nA JAX version supporting implicit surface rasterization is coming soon. 6/6', 'I forgot to add: this is joint work with Kyle Genova, @AvneeshSud, Daniel Vlasic, and Zhoutong Zhang.']",https://arxiv.org/abs/2108.04886,"We present a method for differentiable rendering of 3D surfaces that supports both explicit and implicit representations, provides derivatives at occlusion boundaries, and is fast and simple to implement. The method first samples the surface using non-differentiable rasterization, then applies differentiable, depth-aware point splatting to produce the final image. Our approach requires no differentiable meshing or rasterization steps, making it efficient for large 3D models and applicable to isosurfaces extracted from implicit surface definitions. We demonstrate the effectiveness of our method for implicit-, mesh-, and parametric-surface-based inverse rendering and neural-network training applications. In particular, we show for the first time efficient, differentiable rendering of an isosurface extracted from a neural radiance field (NeRF), and demonstrate surface-based, rather than volume-based, rendering of a NeRF. ",Differentiable Surface Rendering via Non-Differentiable Sampling
84,1425784358154158082,864555701783474179,julesh,"['New preprint!\n\n‚ÄúComposing games into complex institutions‚Äù with Seth Frey, Josh Tan and Philipp Zahn\n\nThis is a general-audience social science paper, summarising our explorations of using open games to think about institutions and governance\n<LINK> <LINK>', 'Writing a research paper on Google Docs has been a new one for me']",https://arxiv.org/abs/2108.05318,"Game theory is used by all behavioral sciences, but its development has long centered around tools for relatively simple games and toy systems, such as the economic interpretation of equilibrium outcomes. Our contribution, compositional game theory, permits another approach of equally general appeal: the high-level design of large games for expressing complex architectures and representing real-world institutions faithfully. Compositional game theory, grounded in the mathematics underlying programming languages, and introduced here as a general computational framework, increases the parsimony of game representations with abstraction and modularity, accelerates search and design, and helps theorists across disciplines express real-world institutional complexity in well-defined ways. ",Composing games into complex institutions
85,1425779157443747840,45872583,Fran Chadha-Day,['New paper today! Using machine learning to search for axion-like particles.\n<LINK>'],https://arxiv.org/abs/2108.04827,"In this work we revisit five different point sources within or behind galaxy clusters in order to constrain the coupling constant between axion-like particles (ALPs) and photons. We use three distinct machine learning (ML) techniques and compare our results with a standard $\chi^2$ analysis. For the first time we apply approximate Bayesian computation to searches for ALPs and find consistently good performance across ML classifiers. Further, we apply more realistic 3D magnetic field simulations of galaxy clusters and compare our results with previously used 1D simulations. We find constraints on the ALP-photon coupling at the level of state-of-the-art bounds with $g_{a\gamma\gamma} \lesssim 0.6 \times 10^{-12}$ GeV${}^{-1}$, hence improving on previous constraints obtained from the same observations. ",Updated Bounds on Axion-Like Particles from X-ray Observations
86,1425649945026076673,2467767397,Howard Cohl,"['In this paper we exploit this relationship to highlight potentially new properties of Ferrers functions which are elucidated by properties of Gegenbauer/ultraspherical polynomial such as e.g., related to orthogonality, Poisson kernel and Poisson-Darboux. <LINK>']",https://arxiv.org/abs/2108.03276,"Using the direct relation between the Gegenbauer polynomials and the Ferrers function of the first kind, we compute interrelations between certain Jacobi polynomials, Meixner polynomials, and the Ferrers function of the first kind. We then compute Rodrigues-type and orthogonality relations for Ferrers functions of the first and second kinds. In the remainder of the paper using the relation between Gegenbauer polynomials and the Ferrers function of the first kind we derive connection and linearization relations, some definite integral and series expansions, some asymptotic expansions of Mehler-Heine type, Christoffel-Darboux summation formulas, and infinite series closure relations (Dirac delta distribution). ","On the relation between Gegenbauer polynomials and the Ferrers function
  of the first kind"
87,1425476175372210177,322460769,Yoav Artzi,"[""New paper: can observational behavioral signal facilitate continual instruction generation learning? Yes! Observe what people do -&gt; they don't do what you want? -&gt; maybe you said it wrong\n\nby @noriyuki_kojima in collaboration w/@alsuhr and myself.\n<LINK>\n\nüßµ..."", 'We use CerealBar as a collaborative platform to study continual instruction generation learning with our system as the leader and human followers (this is a super simple example) https://t.co/eKZ91B5hbb', 'Comparing the system intent (its plan for the follower) to what the user did (their actions in the world) can tell us about the effectiveness of the communication channel (language generation!) https://t.co/KiV5vxDbty', 'We convert this signal to a numerical contextual bandit reward, and put it all together: over rounds, system collaborates and gives instructions, observes behavior, learns to generate language better, and repeats ... bandits are great because ~supervised learning https://t.co/GXo2y6mVSm', 'Multiple live deployments with continual learning. Task completion üöÄ 44-&gt;79% with a nifty üòé bootstrapping for more complex instructions. System improves, executions become similar to plans, interesting language change (outlines future work), data distribution -&gt; more positive https://t.co/wml1a1LxXT', ""This data arises naturally from interaction, so not active learning (no oracles! no instruction writing!) and no explicit feedback. Interestingly, it's overall better than supervised data! Interactive systems üíî supervised data? something to think about https://t.co/X45fN1wlWj"", 'Much much more in the paper\nhttps://t.co/Vdczwpvt5B\nIncluding small treat: this signal relates to signals humans use for learning... so something human users are likely to expect and facilitate ... needs further study', ""There's a video of a very recent talk too:\nhttps://t.co/NXXSv3oID3\n\nThis was a giant and complex project\nüëèüëèüëè @noriyuki_kojima @alsuhr üëèüëèüëè""]",https://arxiv.org/abs/2108.04812,"We study continual learning for natural language instruction generation, by observing human users' instruction execution. We focus on a collaborative scenario, where the system both acts and delegates tasks to human users using natural language. We compare user execution of generated instructions to the original system intent as an indication to the system's success communicating its intent. We show how to use this signal to improve the system's ability to generate instructions via contextual bandit learning. In interaction with real users, our system demonstrates dramatic improvements in its ability to generate language over time. ","Continual Learning for Grounded Instruction Generation by Observing
  Human Following Behavior"
88,1425437801785536513,748850869366468609,"Rishi Paudel, PhD","['Our new paper ‚ÄúSimultaneous Multiwavelength Flare Observations of EV Lacertae‚Äô is now available in <LINK>. It has been accepted for publication in the Astrophysical Journal. I am very grateful to all my collaborators for their great support, guidance and help.', 'In this paper, we studied flares on EV Lac using simultaneous multiwavelength data obtained by TESS, Swift, NICER, LCOGT and UH88.']",https://arxiv.org/abs/2108.04753,"We present the first results of our ongoing project conducting simultaneous multiwavelength observations of flares on nearby active M dwarfs. We acquired data of the nearby dM3.5e star EV Lac using 5 different observatories: NASA's Transiting Exoplanet Survey Satellite (TESS), NASA's Neil Gehrels Swift Observatory (\textit{Swift}), NASA's Neutron Interior Composition Explorer (NICER), the University of Hawaii 2.2-m telescope (UH88) and the Las Cumbres Observatory Global Telescope (LCOGT) Network. During the $\sim$25 days of TESS observations, we acquired three simultaneous UV/X-ray observations using \textit{Swift} that total $\sim$18 ks, 21 simultaneous epochs totaling $\sim$98 ks of X-ray data using NICER, one observation ($\sim$ 3 hours) with UH88, and one observation ($\sim$ 3 hours) with LCOGT. We identified 56 flares in the TESS light curve with estimated energies in the range log $E_{\rm T}$ (erg) = (30.5 - 33.2), nine flares in the \textit{Swift} UVM2 light curve with estimated energies in the range log $E_{UV}$ (erg) = (29.3 - 31.1), 14 flares in the NICER light curve with estimated minimum energies in the range log $E_{N}$ (erg) = (30.5 - 32.3), and 1 flare in the LCOGT light curve with log $E_{L}$ (erg) = 31.6. We find that the flare frequency distributions (FFDs) of TESS and NICER flares have comparable slopes, $\beta_{T}$ = -0.67$\pm$0.09 and $\beta_{N}$ = -0.65$\pm$0.19, and the FFD of UVOT flares has a shallower slope ($\beta_{U}$ = -0.38$\pm$0.13). Furthermore, we do not find conclusive evidence for either the first ionization potential (FIP) or the inverse FIP effect during coronal flares on EV Lac. ",Simultaneous Multiwavelength Flare Observations of EV Lacertae
89,1425432307981295619,56081214,Jan Witowski,"['Today, we release an open-source *meta-repository* for breast cancer mammography classifiers! In a new paper, we use it to evaluate 5 SOTA models on 5 various datasets from around the world. Preprint is now live at: <LINK>, and a thread below: <LINK>', ""If you own a mammography dataset, meta-repository makes it trivial to evaluate multiple SOTA models on your data.\nOn the other hand, if you have a great breast cancer classifier and you want to prove that it's the best, simply follow the instructions in our repo and make a PR!"", 'As of now, there are 5 models ready to use on any mammography dataset.\nWith the use of our meta-repository, we evaluated those models on 5 datasets from around the world. 3 are public (DDSM, INBreast, CCMD) and the remaining 2 (NYU) are available for evaluation upon request. https://t.co/rSL7sLjT4U', 'As expected, models perform better for test sets drawn from the same distribution as the training data.\nAlso, did we mention that you can compare your model performance to human experts? We use our 2020 reader study to allow *anyone* compare their model to radiologists! https://t.co/LjgHhYWENu', 'Meta-repository is now live at https://t.co/Cytm21Kq3o. If you use it to evaluate your data or want to add a model, please send us a message! We‚Äôre happy to help and committed to making science more transparent.', 'This was a collaborative effort of many people brought together by @NYUImaging, @cai2r, @NYUDataScience, @NYUAbuDhabi, @JagiellonskiUni. S/o to @kjgeras, @kchonyc, @jchledowski, @farahshamout.', 'Cc: @DrLukeOR, @NAWA_Poland &amp; @gra_ze, @ngsinformatics, @yindalon, @StanfordHAI, @MazurowskiPhD']",https://arxiv.org/abs/2108.04800,"Artificial intelligence (AI) is showing promise in improving clinical diagnosis. In breast cancer screening, recent studies show that AI has the potential to improve early cancer diagnosis and reduce unnecessary workup. As the number of proposed models and their complexity grows, it is becoming increasingly difficult to re-implement them. To enable reproducibility of research and to enable comparison between different methods, we release a meta-repository containing models for classification of screening mammograms. This meta-repository creates a framework that enables the evaluation of AI models on any screening mammography data set. At its inception, our meta-repository contains five state-of-the-art models with open-source implementations and cross-platform compatibility. We compare their performance on seven international data sets. Our framework has a flexible design that can be generalized to other medical image analysis tasks. The meta-repository is available at this https URL ",Meta-repository of screening mammography classifiers
90,1425394215958241285,952949678533849088,Kareem El-Badry,"['New paper! We present first results from a survey of compact binary stars with ongoing and just-terminated mass transfer. 1/\n<LINK> <LINK>', 'We select targets from below the main-sequence in the #GaiaMission color-magnitude diagram that have @ztfsurvey light curves dominated by ellipsoidal variability (due to tidal deformation). 2/ https://t.co/HVLOtbWmob', 'This selects objects that (a) are hotter and smaller than normal main-sequence stars, and (b) are dominated a star (the ""donor""), not an accretion disk. We vet targets with spectroscopic follow-up. 3/ https://t.co/zuRSxwUMgC', 'Our final sample of objects lives in a previously (almost) empty region of the HR diagram, between extremely low-mass white dwarfs and main-sequence stars (and ""normal"" cataclysmic variable donors). 4/ https://t.co/mOp0Cft2kt', 'Most of the objects we find are hotter than any previously identified similar objects. We think this is because they have undergone more nuclear evolution. That is,... 5/ https://t.co/WGMSybcN3Z', '... these objects form when mass is stripped off the outside of a star by a white dwarf companion. We think mass transfer began late in these objects, after a helium core had started to form. Today, just the helium core and a thin envelope are left. 6/', 'All the hottest (Teff &gt;~ 7000 K) objects appear to have just ended mass transfer, while the cooler ones still have ongoing mass transfer. We think this means magnetic braking becomes inefficient at Teff &gt;~ 7000 K, when stars lose their convective envelopes, slowing inspiral. 7/', 'This systematic survey allows us to derive a space density and birth rate for these evolved-CVs-turning-into-extremely-low-mass-white-dwarfs. Our inferred birth rate is about half that of the birth rate of ultracompact mass-transferring ""AM-CVn"" binaries. 8/ https://t.co/tsq5vhKKan', 'We think many of these objects will turn into AM-CVn binaries within a few Gyr. The birth rate suggests this channel may be (is likely to be?) one of the dominant formation channels for AM-CVn binaries. \nWe have more observations and analysis of these objects in the works! 9/9.']",https://arxiv.org/abs/2108.04255,"We present a systematic survey for mass-transferring and recently-detached cataclysmic variables (CVs) with evolved secondaries, which are progenitors of extremely low-mass white dwarfs (ELM WDs), AM CVn systems, and detached ultracompact binaries. We select targets below the main sequence in the Gaia color-magnitude diagram with ZTF light curves showing large-amplitude ellipsoidal variability and orbital period $P_{\rm orb} < 6$ hr. This yields 51 candidates brighter than G=18, of which we have obtained many-epoch spectra for 21. We confirm all 21 to be completely -- or nearly -- Roche lobe filling close binaries. 13 show evidence of ongoing mass transfer, which has likely just ceased in the other 8. Most of the secondaries are hotter than any previously known CV donors, with temperatures $4700<T_{{\rm eff}}/{\rm K}<8000$. Remarkably, all secondaries with $T_{\rm eff} \gtrsim 7000\,\rm K$ appear to be detached, while all cooler secondaries are still mass-transferring. This transition likely marks the temperature where magnetic braking becomes inefficient due to loss of the donor's convective envelope. Most of the proto-WD secondaries have masses near $0.15\,M_{\odot}$; their companions have masses near $0.8\,M_{\odot}$. We infer a space density of $\sim 60\,\rm kpc^{-3}$, roughly 80 times lower than that of normal CVs and three times lower than that of ELM WDs. The implied Galactic birth rate, $\mathcal{R}\sim 60\,\rm Myr^{-1}$, is half that of AM CVn binaries. Most systems are well-described by MESA models for CVs in which mass transfer begins only as the donor leaves the main sequence. All are predicted to reach minimum periods $5\lesssim P_{\rm orb}/{\rm min}\lesssim30$ within a Hubble time, where they will become AM CVn binaries or merge. This sample triples the known evolved CV population and offers broad opportunities for improving understanding of the compact binary population. ","Birth of the ELMs: a ZTF survey for evolved cataclysmic variables
  turning into extremely low-mass white dwarfs"
91,1425377712806539268,1310552063999438849,Hauke Group,['üéØüìöA new paper is published on @_arXiv_cond_mat\nüôåüèªCongrats on the work done to all authors contributing:\nüî∏@JCHalimeh @PhilippHauke of the @HaukeGroup\nüî∏Monika Aidelsburger\nüîπLukas Homeier\nüîπChristian Schweizer\nüîπFabian Grusdt\nüëÄRead the full article üëâ<LINK> <LINK>'],https://arxiv.org/abs/2108.02203,"The postulate of gauge invariance in nature does not lend itself directly to implementations of lattice gauge theories in modern setups of quantum synthetic matter. Unavoidable gauge-breaking errors in such devices require gauge invariance to be enforced for faithful quantum simulation of gauge-theory physics. This poses major experimental challenges, in large part due to the complexity of the gauge-symmetry generators. Here, we show that gauge invariance can be reliably stabilized by employing simplified \textit{local pseudo generators} designed such that within the physical sector they act identically to the actual local generator. Dynamically, they give rise to emergent exact gauge theories up to timescales polynomial and even exponential in the protection strength. This obviates the need for implementing often complex multi-body full gauge symmetries, thereby further reducing experimental overhead in physical realizations. We showcase our method in the $\mathbb{Z}_2$ lattice gauge theory, and discuss experimental considerations for its realization in modern ultracold-atom setups. ","Stabilizing Lattice Gauge Theories Through Simplified Local Pseudo
  Generators"
92,1425366685435973636,1339508444764786694,Gregor Kasieczka,"['New paper today: ""Symmetries, Safety, and Self-Supervision"" (arXiv: <LINK>). We - driven by excellent Heidelberg people including @LorenzVogel - look at how known physical symmetries can be used to learn better representations. 1/3', ""We use contrastive learning a la #SimCLR and include translation, rotations, and soft+collinear emissions. This figure shows how well rotations are learned. Left is without including rotations, right is with. s(z,z')=1 &lt;-&gt; identical representations 2/3 https://t.co/zFaYjFEiEk"", 'The goal is to have a better input for #unsupervised learning (coming next..) but we can already test how well the learned representation does as input to a linear classifier. Spoiler: Pretty well (curve is for top tagging w/ a linear network) 3/3 https://t.co/YyzlA9srTG']",https://arxiv.org/abs/2108.04253,"Collider searches face the challenge of defining a representation of high-dimensional data such that physical symmetries are manifest, the discriminating features are retained, and the choice of representation is new-physics agnostic. We introduce JetCLR to solve the mapping from low-level data to optimized observables though self-supervised contrastive learning. As an example, we construct a data representation for top and QCD jets using a permutation-invariant transformer-encoder network and visualize its symmetry properties. We compare the JetCLR representation with alternative representations using linear classifier tests and find it to work quite well. ","Symmetries, Safety, and Self-Supervision"
93,1425338028546043904,1119514956708438017,Stanley E. Lazic,"['New paper is online arguing that multiple corrections don\'t work in practice. \n\n""...p-values are not the problem, it‚Äôs the effect sizes. Adjusting p-values is like treating the symptoms of a disease instead of the cause.""\n\n<LINK>']",https://arxiv.org/abs/2108.04752,"Most scientific disciplines use significance testing to draw conclusions from experimental or observational data. This classical approach provides theoretical guarantees for controlling the number of false positives across a set of hypothesis tests, making it an appealing framework for scientists who wish to limit the number of false effects or associations that they claim exist. Unfortunately, these theoretical guarantees apply to few experiments and the actual false positive rate (FPR) is much higher than the theoretical rate. In real experiments, hypotheses are often tested after finding unexpected relationships or patterns, the data are analysed in several ways, analyses may be run repeatedly as data accumulate from new experimental runs, and publicly available data are analysed by many groups. In addition, the freedom scientists have to choose the error rate to control, the collection of tests to include in the adjustment, and the method of correction provides too much flexibility for strong error control. Even worse, methods known to provide poor control of the FPR such as Newman-Keuls and Fisher's Least Significant Difference are popular with researchers. As a result, adjusted p-values are too small, the incorrect conclusion is often reached, and reported results are less reproducible. Here, I show why the FPR is rarely controlled in any meaningful way and argue that a single well-defined FPR does not even exist. ","Why multiple hypothesis test corrections provide poor control of false
  positives in the real world"
94,1425278532951842821,2377407248,Daniel Whiteson,"['New paper!\n\nData Acquisition System for a Distributed Smartphone Cosmic Ray Observatory\n\n<LINK>\n\nled by Jeff Swaney, with Chase Shimmin.\n\nHow to turn smartphones into  detectors, calibrate them , keep them from overheating and tie them together into a network!', 'First, did you know that your phone camera doesn‚Äôt treat all pixels equally? It boosts ones far from the center because they get less light. We had to correct that because we aren‚Äôt interested in LIGHT. https://t.co/zIxNWikF79', 'Second, most of the pixels that fire are garbage hot pixels. Find ‚Äòem and mask ‚Äòem for a nice clean image. https://t.co/pT48O0eIin', 'Third, throttle the frame rate to keep the phone from cooking! https://t.co/qTxM3McAlb', 'Fourth, build a cloud-based DAQ that monitors the phones and gives real-time feedback to improve the calibration. https://t.co/5Cxup4rjr6', 'Last, be grateful to our thousands of beta testers! https://t.co/CyHn7WIlZo', '@BryanJField https://t.co/OUcx4MyUNk']",http://arxiv.org/abs/2108.04803,"A scientific instrument comprised of a global network of millions of independent, connected, remote devices presents unique data acquisition challenges. We describe the software design of a mobile application which collects data from smartphone cameras without overburdening the phone's CPU or battery. The deployed software automatically calibrates to heterogeneous hardware targets to improve the quality and manage the rate of data transfer, and connects to a cloud-based data acquisition system which can manage and refine the operation of the network. ","Data Acquisition System for a Distributed Smartphone Cosmic Ray
  Observatory"
95,1425208093953114112,109394186,Rodrigo Canaan,"['New paper on arXiv - a post mortem of the 2020 @GenDesignMC challenge including the thoughts of organizers, participants and judges on the achievements so far and challenges ahead!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2108.02955,"The GDMC AI settlement generation challenge is a PCG competition about producing an algorithm that can create an ""interesting"" Minecraft settlement for a given map. This paper contains a collection of written experiences with this competition, by participants, judges, organizers and advisors. We asked people to reflect both on the artifacts themselves, and on the competition in general. The aim of this paper is to offer a shareable and edited collection of experiences and qualitative feedback - which seem to contain a lot of insights on PCG and computational creativity, but would otherwise be lost once the output of the competition is reduced to scalar performance values. We reflect upon some organizational issues for AI competitions, and discuss the future of the GDMC competition. ",Impressions of the GDMC AI Settlement Generation Challenge in Minecraft
96,1425165149904809987,318652707,Fangzhou Jiang (Arthur),"['Check out our new paper on SIDM subhalo s. We compare the effects of dark ram-pressure and tides on satellite evolution, and show that the central density of compact satellites can be used to constrain cross-section parameters, in a proof-of-concept way. <LINK>']",https://arxiv.org/abs/2108.03243,"Dark matter self interactions can leave distinctive signatures on the properties of satellite galaxies around Milky Way--like hosts through their impact on tidal stripping, ram pressure, and gravothermal collapse. We delineate the regions of self-interacting dark matter parameter space---specified by interaction cross section and a velocity scale---where each of these effects dominates, and show how the relative mass loss depends on the satellite's initial mass, density profile and orbit. We obtain novel, conservative constraints in this parameter space using Milky Way satellite galaxies with notably high central densities and small pericenter distances. Our results for self-interacting dark matter models, in combination with constraints from clusters of galaxies, favor velocity-dependent cross sections that lead to gravothermal core collapse in the densest satellites. ","Orbital Evolution of Satellite Galaxies in Self-Interacting Dark Matter
  Models"
97,1425099713997537280,928289778960707584,Sewon Min,"['New paper!‚ú®We introduce a noisy channel approach for LM prompting in few-shot text classification. Channel models are more stable (much lower variance), and better with limited data / imbalanced labels.\n\n<LINK>\nw/ @ml_perception @HannaHajishirzi @LukeZettlemoyer', '@ml_perception @HannaHajishirzi @LukeZettlemoyer Instead of P(y|x), channel models compute P(x|y)P(y) ~ P(x|y). We use this approach for in-context demonstration and for prompt tuning. We find channel models have lower variance &amp; better worst-case accuracy, are more robust to imbalance in training data, and generalize better.', '@ml_perception @HannaHajishirzi @LukeZettlemoyer We also compare with some of surprisingly strong baselines that are often ignored (e.g. direct head tuning) ---  see our extensive ablations for when to use channel prompt tuning vs. other competitive models. As a bonus, find a new way of doing in-context demonstration. #NLProc']",http://arxiv.org/abs/2108.04106,"We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive methods (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required. ",Noisy Channel Language Model Prompting for Few-Shot Text Classification
98,1425078498193027073,445739626,Gozde Unal,"['Can a dedicated learning model recognize visual design elements used by artists, designers &amp; architects? Read our new paper <LINK> published @ J Automation in Construction. A truly interdisciplinary work! Thanks to coauthors @asLIckms  @VYesilkaynak &amp; gdemir üëèüòÄ']",https://arxiv.org/abs/2108.04048,"Visual design is associated with the use of some basic design elements and principles. Those are applied by the designers in the various disciplines for aesthetic purposes, relying on an intuitive and subjective process. Thus, numerical analysis of design visuals and disclosure of the aesthetic value embedded in them are considered as hard. However, it has become possible with emerging artificial intelligence technologies. This research aims at a neural network model, which recognizes and classifies the design principles over different domains. The domains include artwork produced since the late 20th century; professional photos; and facade pictures of contemporary buildings. The data collection and curation processes, including the production of computationally-based synthetic dataset, is genuine. The proposed model learns from the knowledge of myriads of original designs, by capturing the underlying shared patterns. It is expected to consolidate design processes by providing an aesthetic evaluation of the visual compositions with objectivity. ","Detecting Visual Design Principles in Art and Architecture through Deep
  Convolutional Neural Networks"
99,1425057742583181314,7773042,Yasser Souri,"['Happy to announce our new paper: ""FIFA: Fast Inference Approximation for Action Segmentation"".\n\nPaper: <LINK>\nAnimation: <LINK>', 'We propose an approximate approach to perform inference for action segmentation using gradient-descent instead of dynamic programming.\nUsing gradient based optimization results in a fast and ""any-time"" algorithm where one can stop the optimization after any number of steps. https://t.co/VZev9DZmh3', 'FIFA can be used in combination with any action segmentation approach that estimates framewise probabilities.\nWe use FIFA in combination with MuCon, CDFL, MS-TCN and MS-TCN++ in both fully supervised and weakly supervised settings.', 'We report state-of-the-art or comparable results on the Breakfast and Hollywood extended dataset.\nFIFA achieves this accuracy while being 5-12 times faster than dynamic programming based exact inference algorithms.']",https://arxiv.org/abs/2108.03894,"We introduce FIFA, a fast approximate inference method for action segmentation and alignment. Unlike previous approaches, FIFA does not rely on expensive dynamic programming for inference. Instead, it uses an approximate differentiable energy function that can be minimized using gradient-descent. FIFA is a general approach that can replace exact inference improving its speed by more than 5 times while maintaining its performance. FIFA is an anytime inference algorithm that provides a better speed vs. accuracy trade-off compared to exact inference. We apply FIFA on top of state-of-the-art approaches for weakly supervised action segmentation and alignment as well as fully supervised action segmentation. FIFA achieves state-of-the-art results on most metrics on two action segmentation datasets. ",FIFA: Fast Inference Approximation for Action Segmentation
100,1425011927076462604,137823497,Paul Goldberg,['New paper on cake-cutting protocols &amp; how to represent them:\n<LINK>\nWe show that some alternative representation formats have the same expressive power (analogous to DFAs/regexps in context of formal languages)'],https://arxiv.org/abs/2108.03641,"The cake-cutting problem involves dividing a heterogeneous, divisible resource fairly between $n$ agents. Br\^{a}nzei et al. [6] introduced {\em generalised cut and choose} (GCC) protocols, a formal model for representing cake-cutting protocols as trees with ""cut"" and ""choose"" nodes corresponding to the agents' actions, and if-else statements. In this paper, we identify an alternative and simpler extensive-form game model for cake-cutting protocols, that we call {\em branch choice} (BC) protocols. We show that the class of protocols we can represent using this model is invariant under certain modifications to its definition. We further prove that any such protocol can be converted to a restricted form in which the agents first cut the cake and then get to choose between various branches leading to different allocations. Finally, we show that this model has the same expressive power as GCC protocols, i.e. they represent the same class of protocols up to a notion of equivalence involving the bounds on envy that each agent can guarantee for themselves. For this purpose, we introduce a new notion of envy-equivalence of protocols. ",Equivalence of Models of Cake-Cutting Protocols
101,1425008776709562397,561899047,Aki Vehtari,"['New paper ""Pathfinder: Parallel quasi-Newton variational inference"" with Lu Zhang, Bob Carpenter, and Andrew Gelman <LINK>. We combine deterministic quasi-Newton optimization with variational KL-divergence minimization. <LINK>', 'Stochastic gradient descent optimization in black box variational inference (BBVI) has theoretically nice asymptotic properties, but in limited computation time, stochastic optimization is slower than people commonly think.', ""Deterministic quasi-Newton optimization (which exploits also the curvature information) is computationally very efficient, but can't, in general, be used directly in BBVI. Optimizing the log posterior density is easy, but the mode is often far from being representative."", 'We use efficient L-BFGS to optimize the log posterior density, construct normal approximations as a by product of L-BFGS, and choose the best approximation minimizing the KL-divergence (maximizing the evidence lower bound (ELBO))', 'In the case of a normal posterior, the best approximation is the same as the normal approximation at the mode. In the case of a non-normal posterior, the best approximation is somewhere along the way. https://t.co/b1HpbKT8A3', 'The result is made less sensitive to random initial values and stochasticity of the KL-divergence estimate by running multiple optimizations and using importance resampling. https://t.co/GWR08I84jv', ""Pathfinder doesn't have an asymptotic guarantee to find the best possible approximation with unlimited computation time, but it finds approximations better or comparable to BBVI in orders of magnitude less function and gradient evaluations than BBVI."", 'Pathfinder is especially well suited for quick prototyping and for providing initial values for more elaborate methods like BBVI and dynamic HMC. Pathfinder is trivially parallelizable.', 'Lu made a huge amount of work for this paper, and the illustrative figures she made are just beautiful!', '@vthorrf @TimSalimans Their method is using stochastic optimization (in general), is more elaborate/complex, and with unlimited computation time probably finds better approximations. Our method uses deterministic optimization for the path, is simpler, and finds somewhat useful approximations fast.']",https://arxiv.org/abs/2108.03782,"We introduce Pathfinder, a variational method for approximately sampling from differentiable log densities. Starting from a random initialization, Pathfinder locates normal approximations to the target density along a quasi-Newton optimization path, with local covariance estimated using the inverse Hessian estimates produced by the optimizer. Pathfinder returns draws from the approximation with the lowest estimated Kullback-Leibler (KL) divergence to the true posterior. We evaluate Pathfinder on a wide range of posterior distributions, demonstrating that its approximate draws are better than those from automatic differentiation variational inference (ADVI) and comparable to those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC runs, Pathfinder requires one to two orders of magnitude fewer log density and gradient evaluations, with greater reductions for more challenging posteriors. Importance resampling over multiple runs of Pathfinder improves the diversity of approximate draws, reducing 1-Wasserstein distance further and providing a measure of robustness to optimization failures on plateaus, saddle points, or in minor modes. The Monte Carlo KL-divergence estimates are embarrassingly parallelizable in the core Pathfinder algorithm, as are multiple runs in the resampling version, further increasing Pathfinder's speed advantage with multiple cores. ",Pathfinder: Parallel quasi-Newton variational inference
102,1425006628231864331,945445796098473984,Patrick Schnider,"['A new paper on the @arxiv . In ‚ÄûTopological Art in Simple Galleries‚Äú, together with Daniel Bertschinger, Nicolas El Maalouly, Till Miltzow and Simon Weber, we study the space of optimal guard placements in the Art Gallery Problem.\n\n<LINK>\n\n1/6', 'See this link for more info on the Art Gallery Problem:\n\nhttps://t.co/TRE5OOhiGo\n\n2/6', 'We show a universality theorem similar to Mn√´vs universality theorem for order types. Specifically we show that for every semi-algebraic set S, there exists a polygon for which the space of optimal guard placements is homotopy-equivalent to S.\n\n3/6', 'We further give some small polygons for which the space of optimal guard placements are homeomorphic to non-trivial spaces such as spheres, torus or double torus.\n\n4/6', 'The paper is accompanied by a video with some animations, made by my coauthor Simon Weber.\n\nhttps://t.co/J1FWh66Qhh\n\n5/6', 'The animations were made using @geogebra , and Simon has also made an applet where you can play around with guard placements yourself.\n\nSphere:\n\nhttps://t.co/7CC8KOTeSU\n\nDouble Torus:\n\nhttps://t.co/OXqdcUCth3\n\n6/6']",https://arxiv.org/abs/2108.04007,"Let $P$ be a simple polygon, then the art gallery problem is looking for a minimum set of points (guards) that can see every point in $P$. We say two points $a,b\in P$ can see each other if the line segment $seg(a,b)$ is contained in $P$. We denote by $V(P)$ the family of all minimum guard placements. The Hausdorff distance makes $V(P)$ a metric space and thus a topological space. We show homotopy-universality, that is for every semi-algebraic set $S$ there is a polygon $P$ such that $V(P)$ is homotopy equivalent to $S$. Furthermore, for various concrete topological spaces $T$, we describe instances $I$ of the art gallery problem such that $V(I)$ is homeomorphic to $T$. ",Topological Art in Simple Galleries
103,1425006045060030471,989510983,Antonio Fern√°ndez,"['""Estimating Active Cases of COVID-19,"" new @coronasurveys paper, to be presented at KDD Workshop on Data-driven Humanitarian Mapping @HumanitarianSys <LINK>']",https://arxiv.org/abs/2108.03284,"Having accurate and timely data on confirmed active COVID-19 cases is challenging, since it depends on testing capacity and the availability of an appropriate infrastructure to perform tests and aggregate their results. In this paper, we propose methods to estimate the number of active cases of COVID-19 from the official data (of confirmed cases and fatalities) and from survey data. We show that the latter is a viable option in countries with reduced testing capacity or suboptimal infrastructures. ",Estimating Active Cases of COVID-19
104,1424986072895926283,2416610796,Gorka Mu√±oz-Gil,"['If you want a nice read for your holidays, our new paper on machine learning for anomalous diffusion is out!\n\nWe asked ourselves if unsupervised learning could be used to learn features of stochastic processes. Answer: Yes!!\n\nCheck the details here: <LINK>']",https://arxiv.org/abs/2108.03411,"The characterization of diffusion processes is a keystone in our understanding of a variety of physical phenomena. Many of these deviate from Brownian motion, giving rise to anomalous diffusion. Various theoretical models exists nowadays to describe such processes, but their application to experimental setups is often challenging, due to the stochastic nature of the phenomena and the difficulty to harness reliable data. The latter often consists on short and noisy trajectories, which are hard to characterize with usual statistical approaches. In recent years, we have witnessed an impressive effort to bridge theory and experiments by means of supervised machine learning techniques, with astonishing results. In this work, we explore the use of unsupervised methods in anomalous diffusion data. We show that the main diffusion characteristics can be learnt without the need of any labelling of the data. We use such method to discriminate between anomalous diffusion models and extract their physical parameters. Moreover, we explore the feasibility of finding novel types of diffusion, in this case represented by compositions of existing diffusion models. At last, we showcase the use of the method in experimental data and demonstrate its advantages for cases where supervised learning is not applicable. ",Unsupervised learning of anomalous diffusion data
105,1424948522621685762,1373003924005818369,BICEP/Keck,"[""New paper alert! The CMB isn't just useful for detecting primordial gravitational waves - it can also be used to search for axion-like dark matter! Axions cause CMB polarization to rotate over time, as shown in the gif below. Read all about it here: <LINK> <LINK>""]",http://arxiv.org/abs/2108.03316,"We present an improved search for axion-like polarization oscillations in the cosmic microwave background (CMB) with observations from the Keck Array. An all-sky, temporally sinusoidal rotation of CMB polarization, equivalent to a time-variable cosmic birefringence, is an observable manifestation of a local axion field and potentially allows a CMB polarimeter to detect axion-like dark matter directly. We describe improvements to the method presented in previous work, and we demonstrate the updated method with an expanded dataset consisting of the 2012-2015 observing seasons. We set limits on the axion-photon coupling constant for mass $m$ in the range $10^{-23}$-$10^{-18}~\mathrm{eV}$, which corresponds to oscillation periods on the order of hours to years. Our results are consistent with the background model. For periods between $1$ and $30~\mathrm{d}$ ($1.6 \times 10^{-21} \leq m \leq 4.8 \times 10^{-20}~\mathrm{eV}$), the $95\%$-confidence upper limits on rotation amplitude are approximately constant with a median of $0.27^\circ$, which constrains the axion-photon coupling constant to $g_{\phi\gamma} < (4.5 \times 10^{-12}~\mathrm{GeV}^{-1}) m/(10^{-21}~\mathrm{eV}$), if axion-like particles constitute all of the dark matter. More than half of the collected BICEP dataset has yet to be analyzed, and several current and future CMB polarimetry experiments can apply the methods presented here to achieve comparable or superior constraints. In the coming years, oscillation measurements can achieve the sensitivity to rule out unexplored regions of the axion parameter space. ","BICEP / Keck XIV: Improved constraints on axion-like polarization
  oscillations in the cosmic microwave background"
106,1424922599675494401,2297684784,Yang You,['Our new paper: ONES automatically manages the elasticity of each AI job based on the workload to maximize GPU utilization and improve scheduling efficiency. Experiments on 64 GPUs show great results. This paper will appear on @Supercomputing #SC21 (<LINK>) <LINK>'],https://arxiv.org/abs/2108.03645,"Efficient GPU resource scheduling is essential to maximize resource utilization and save training costs for the increasing amount of deep learning workloads in shared GPU clusters. Existing GPU schedulers largely rely on static policies to leverage the performance characteristics of deep learning jobs. However, they can hardly reach optimal efficiency due to the lack of elasticity. To address the problem, we propose ONES, an ONline Evolutionary Scheduler for elastic batch size orchestration. ONES automatically manages the elasticity of each job based on the training batch size, so as to maximize GPU utilization and improve scheduling efficiency. It determines the batch size for each job through an online evolutionary search that can continuously optimize the scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on TACC's Longhorn supercomputers. The results show that ONES can outperform the prior deep learning schedulers with a significantly shorter average job completion time. ","Online Evolutionary Batch Size Orchestration for Scheduling Deep
  Learning Workloads in GPU Clusters"
107,1424907889261514767,1041578714,Benjamin Pope,"[""Excited to see Dan Hey's awesome new paper on arXiv, about searching for transiting planets around Œ¥ Scuti stars in Kepler! Expected to be the last of his PhD at @sifa_astro before he moves to @UHIfA as a postdoc.\n\n<LINK> <LINK>"", 'Œ¥ Scuti stars are a kind of hot star that pulsates with short periods (&lt;&lt; 1 day) with acoustic oscillations. It is hard to see their planets, even though we really would like to know more about planet populations around hot stars, because they get swamped with pulsation noise.', 'Dan and co (inc @benmontet, Tim Bedding, Simon Murphy, and myself) iteratively subtracted sinusoids from Kepler Œ¥ Scuti stars to get precise prewhitened light curves to search for planets - finding just a handful of new marginal candidates, despite sensitive searches.', 'The method is powerful and we hope to apply it to TESS and PLATO next - shallow wide surveys are biased toward hot stars so there should be plenty to look at!', 'Meanwhile as a by-product, we have I think the largest sample of Œ¥ Scuti and Œ≥ Dor pulsation periods. Been playing around with embeddings for these - @davidwhogg? https://t.co/oVv4hc5hwb']",https://arxiv.org/abs/2108.03785,"We search for transits around all known pulsating {\delta} Sct variables (6500 K < Teff < 10 000 K) in the long-cadence Kepler data after subtracting the pulsation signal through an automated routine. To achieve this, we devise a simple and computationally inexpensive method for distinguishing between low-frequency pulsations and transits in light curves. We find 3 new candidate transit events that were previously hidden behind the pulsations, but caution that they are likely to be false positive events. We also examined the Kepler Objects of Interest catalog and identify 13 additional host stars which show {\delta} Sct pulsations. For each star in our sample, we use the non-detection of pulsation timing variations for a planet that is known to be transiting a {\delta} Sct variable to obtain both an upper limit on the mass of the planet and the expected radial velocity semi-amplitude of the host star. Simple injection tests of our pipeline imply 100% recovery for planets of 0.5 RJup or greater. Extrapolating our number of Kepler {\delta} Sct stars, we expect 12 detectable planets above 0.5 RJup in TESS. Our sample contains some of the hottest known transiting planets around evolved stars, and is the first complete sample of transits around {\delta} Sct variables. We make available our code and pulsation-subtracted light curves to facilitate further analysis. ",A search for transits among the {\delta} Scuti variables in Kepler
108,1424903087735595008,85051333,Pietro Giampa,"['New paper alert #DarkMatter #NeutrinoFloor\n<LINK>\n\nI had a blast working on this Theory/Experimental physics mix, with Andrea Gaspert and David Morrissey.\n\n@SNOLABscience-@TRIUMFLab Connection üöÄ <LINK>']",https://arxiv.org/abs/2108.03248,"Experiments that use liquid noble gasses as target materials, such as argon and xenon, play a significant role in direct detection searches for WIMP(-like) dark matter. As these experiments grow in size, they will soon encounter a new background to their dark matter discovery potential from neutrino scattering off nuclei and electrons in their targets. Therefore, a better understanding of this new source of background is crucial for future large-scale experiments such as ARGO and DARWIN. In this work, we study the impact of atmospheric neutrino flux uncertainties, electron recoil rejection efficiency, recoil energy sensitivity, and other related factors on the dark matter discovery reach. We also show that a significant improvement in sensitivity can potentially be obtained, at large exposures, by combining data from independent argon and xenon experiments. ","Neutrino Backgrounds in Future Liquid Noble Element Dark Matter Direct
  Detection Experiments"
109,1424716283006885892,2869863482,Jason Aufdenberg,['New paper day!  Spica and its faint nebula (20 full moons wide on the sky). My first time modeling both the star and the interstellar medium. <LINK> <LINK>'],https://arxiv.org/abs/2108.02820,"The large, faint H$\alpha$ emission surrounding the early B-star binary Spica has been used to constrain the total hydrogen recombination rate of the nebula and indirectly probe the Lyman continuum luminosity of the primary star. Early analysis suggested that a stellar atmosphere model, consistent with Spica A's spectral type, has a Lyman continuum luminosity about two times lower than required to account for the measured H$\alpha$ surface brightness within the nebula. To more consistently model both the stellar and nebular emission, we have used a model atmosphere for Spica A which includes the effects of gravity darkening as input to photoionization models to produce synthetic H$\alpha$ surface brightness distributions for comparison to data from the Southern $H\alpha$ Sky Survey Atlas (SHASSA). This paper presents a method for the computation of projected surface brightness profiles from 1D volume emissivity models and constrains both stellar and nebular parameters. A mean effective temperature for Spica A of $\simeq$ 24,800 K is sufficient to match both the observed absolute spectrophotometry, from the far-UV to the near-IR, and radial H$\alpha$ surface brightness distributions. Model hydrogen densities increase with the distance from the star, more steeply and linearly towards the southeast. The northwest matter-bounded portion of the nebula is predicted to leak $\sim$17% of Lyman continuum photons. Model H II region column densities are consistent with archival observations along the line of sight. ","Modeling the H$\alpha$ Emission Surrounding Spica using the Lyman
  Continuum from a Gravity-darkened Central Star"
110,1423713223736442880,3314842991,Andre Martins,"['New arXiv paper on ""Sparse Continuous Distributions and Fenchel-Young Losses"" with @MarcosTreviso @tozefarinhas Pedro Aguiar @mariotelfig @mblondel_ml @vnfrombucharest: <LINK> (long paper and long thread...) 1/N', 'We generalize sparse transformations (sparsemax, entmax, fusedmax) and Fenchel-Young losses to infinite domains, both countably infinite and continuous. 2/N', 'We build upon the idea of Œ©-regularized prediction maps (https://t.co/t629mm4k3u) which use a convex function Œ© (a generalized entropy) as a regularizer to induce a mapping from scoring functions to probability densities. 3/N https://t.co/LwRh6mrmZy', 'When Œ© is the Shannon-Boltzmann-Gibbs negentropy, this is the free energy variational principle and we obtain Gibbs distributions, including softmax and exponential families (Gaussian, Laplace, Poisson, ‚Ä¶) These distributions have fixed support. 4/N', 'But there are other interesting Œ©, such as Tsallis negentropies, parametrized by a scalar Œ±, which extends Shannon-Boltzmann-Gibbs (recovered when Œ± = 1). We focus on the case Œ± &gt; 1, where the resulting densities are ""sparse"" ‚Äî their support may vary inside the family. 5/N https://t.co/qa1smFuEC9', 'For various choices of scoring functions, Tsallis regularizers recover as particular cases Œ±-entmax and sparsemax (finite domains), sparse Poisson distributions (countably infinite), triangular and truncated paraboloid distributions (continuous), etc. 6/N https://t.co/Mx3ifpLE3Y', 'Fenchel-Young losses generalize KL divergences and cross-entropy losses. For linear scoring functions, the minimization of Fenchel-Young losses (for any Œ©) is equivalent to moment matching of the statistics, generalizing a fundamental property of exponential families. 7/N', 'The combination of quadratic scoring functions with Tsallis regularizers is particularly interesting: it generates a class of elliptical densities (Œ≤-Gaussians) that contain as particular cases the Gaussian, biweight, triweight and Epanechnikov densities. 8/N https://t.co/5npzOF5eta', 'Œ≤-Gaussians have very interesting properties. For example,  we can sample from multivariate Œ≤-Gaussians by sampling a vector uniformly from a sphere and scaling it by a radius distributed as a Beta. 9/N https://t.co/CgW6rlskJ0', 'And we also have a closed form expression for the Fenchel-Young loss for Œ≤-Gaussians that extends the formula for the KL divergence between Gaussians. We can use it to fit Œ≤-Gaussians to data. 10/N https://t.co/8p1EIA15tv', 'Other interesting Œ© are the total variation and Sobolev regularizers, which penalize the absolute value of the density derivatives or its square ‚Äî this extends fusedmax to the continuous domain. 11/N https://t.co/WXQFwiIwQ5', 'We use this framework to extend attention mechanisms beyond finite sets (words, image regions) to the continuous domain (e.g. a 1-d temporal or spatial dimension, a 2-d image).  We derive  efficient forward and gradient backpropagation algorithms for Œ± ‚àà {1, 4/3, 3/2, 2}. 12/N https://t.co/gsn3LZs3h3', 'Using them, we demonstrate our sparse continuous distributions for attention-based audio classification and visual question answering, showing that they allow attending to time intervals and compact regions. 13/N https://t.co/FDA7czRJ4d', 'This paper generalizes https://t.co/t629mm4k3u (which proposed Fenchel-Young losses for finite domains) and extends https://t.co/k2LG7YJWur (which proposed continuous attention mechanisms). 14/N', 'It took a while to put everything together, but we found it well worth the effort to present all this in a unified manner! 15/N https://t.co/CaE6M5eczy', 'We are building a comprehensive Python package implementing all the densities, samplers, losses, and attention mechanisms mentioned above: https://t.co/NL5zGSY4p5. 16/16']",https://arxiv.org/abs/2108.01988,"Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent works on sparse alternatives to softmax (e.g. sparsemax, $\alpha$-entmax, and fusedmax) and corresponding losses, which have varying support. This paper expands that line of work in several directions: first, it extends $\Omega$-regularized prediction maps and Fenchel-Young losses to arbitrary domains (possibly countably infinite or continuous). For linearly parametrized families, we show that minimization of Fenchel-Young losses is equivalent to moment matching of the statistics, generalizing a fundamental property of exponential families. When $\Omega$ is a Tsallis negentropy with parameter $\alpha$, we obtain ""deformed exponential families,"" which include $\alpha$-entmax and sparsemax ($\alpha$ = 2) as particular cases. For quadratic energy functions in continuous domains, the resulting densities are $\beta$-Gaussians, an instance of elliptical distributions that contain as particular cases the Gaussian, biweight, triweight and Epanechnikov densities, and for which we derive closed-form expressions for the variance, Tsallis entropy, and Fenchel-Young loss. When $\Omega$ is a total variation or Sobolev regularizer, we obtain a continuous version of the fusedmax. Finally, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for $\alpha \in \{1, 4/3, 3/2, 2\}$. Using them, we demonstrate our sparse continuous distributions for attention-based audio classification and visual question answering, showing that they allow attending to time intervals and compact regions. ",Sparse Continuous Distributions and Fenchel-Young Losses
111,1423649309921263619,1263583669618307079,Ryan Levy,"['New Paper out this morning! Bryan and I explore entanglement transitions in RTNs <LINK>', '@ShubhangGoswam1 Thanks Shubhang!! üòÄ']",https://arxiv.org/abs/2108.02225,"Entanglement is a key quantum phenomena and understanding transitions between phases of matter with different entanglement properties are an interesting probe of quantum mechanics. We numerically study a model of a 2D tensor network proposed to have an entanglement entropy transition first considered by Vasseur et al.[Phys. Rev. B 100, 134203 (2019)]. We find that by varying the bond dimension of the tensors in the network we can observe a transition between an area and volume phase with a logarithmic critical point around $D\approx 2$. We further characterize the critical behavior measuring a critical exponent using entanglement entropy and the tripartite quantum mutual information, observe a crossover from a `nearly pure' to entangled area law phase using the the distributions of the entanglement entropy and find a cubic decay of the pairwise mutual information at the transition. We further consider the dependence of these observables for different R\'enyi entropy. This work helps further validate and characterize random tensor networks as a paradigmatic examples of an entanglement transition. ",Entanglement Entropy Transitions with Random Tensor Networks
112,1423579098455158785,2215966376,Bose üáÆüá≥,"['Are type-II spicules responsible for the persistent downflows observed in the transition region and lower corona? Here we revisit this idea with high-resolution observations aided with an MHD simulation. Check out our new paper: <LINK> <LINK>', 'Maybe interesting for you @swmcintosh :)', '@swmcintosh Of course! We largely support your speculations and interpretations from McIntosh et al. (2012).', '@swmcintosh And connect them unambiguously with chromospheric spicules from high-resolution SST observations.', '@swmcintosh Well ""intuitive reasoning"" is indeed perhaps the better word.']",https://arxiv.org/abs/2108.02153,"Spectroscopic observations of the emission lines formed in the solar transition region (TR) commonly show persistent downflows of the order of 10--15 km/s. The cause of such downflows, however, is still not fully clear and has remained a matter of debate. We aim to understand the cause of such downflows by studying the coronal and TR responses to the recently reported chromospheric downflowing rapid red shifted excursions (RREs), and their impact on heating the solar atmosphere. We have used two sets of coordinated data from SST, IRIS, and SDO for analyzing the response of the downflowing RREs in the TR and corona. To provide theoretical support, we use an already existing 2.5D MHD simulation of spicules performed with the Bifrost code. We find ample occurrences of downflowing RREs and show several examples of their spatio-temporal evolution, sampling multiple wavelength channels ranging from the cooler chromospheric to hotter coronal channels. These downflowing features are thought to be likely associated with the returning components of the previously heated spicular plasma. Furthermore, the TR Doppler shifts associated with them are close to the average red shifts observed in this region which further implies that these flows could (partly) be responsible for the persistent downflows observed in the TR. We also propose two mechanisms (a typical upflow followed by a downflow and downflows along a loop), from the perspective of numerical simulation, that could explain the ubiquitous occurrence of such downflows. A detailed comparison between the synthetic and observed spectral characteristics, reveals a distinctive match, and further suggests an impact on the heating of the solar atmosphere. We present evidence that suggests that at least some of the downflowing RREs are the chromospheric counterparts of the TR and lower coronal downflows. ","Evidence of multithermal nature of spicular downflows. Impact on solar
  atmospheric heating"
113,1423562298719936513,3341859483,Dr Justyn Campbell-White,['STAR-MELT paper accepted for publication in MNRAS! <LINK>\nWe give an overview of the package features for emission line analysis and show its application and discussion for three YSOs (archival and new data)'],https://arxiv.org/abs/2108.02552,"We introduce the STAR-MELT Python package that we developed to facilitate the analysis of time-resolved emission line spectroscopy of young stellar objects. STAR-MELT automatically extracts, identifies and fits emission lines. We summarise our analysis methods that utilises the time domain of high-resolution stellar spectra to investigate variability in the line profiles and corresponding emitting regions. This allows us to probe the innermost disc and accretion structures of YSOs. Local temperatures and densities can be determined using Boltzmann statistics, the Saha equation, and the Sobolev large velocity gradient approximation. STAR-MELT allows for new results to be obtained from archival data, as well as facilitating timely analysis of new data as it is obtained. We present the results of applying STAR-MELT to three YSOs, using spectra from UVES, XSHOOTER, FEROS, HARPS, and ESPaDOnS. We demonstrate what can be achieved for data with disparate time sampling, for stars with different inclinations and variability types. For EX Lupi, we confirm the presence of a localised and stable stellar-surface hot spot associated with the footprint of the accretion column. For GQ Lupi A, we find that the maximum infall rate from an accretion column is correlated with lines produced in the lowest temperatures. For CVSO109 we investigate the rapid temporal variability of a redshifted emission wing, indicative of rotating and infalling material in the inner disc. Our results show that STAR-MELT is a useful tool for such analysis, as well as other applications for emission lines. ",The STAR-MELT Python package for emission line analysis of YSOs
114,1423557886983168004,328430286,Jad C. Halimeh,"['New paper <LINK>: Instead of constructing complex gauge-symmetry generators, one can just build much simpler local pseudo generators identical to them in the physical sector in implementations of gauge theories.\n@HaukeGroup \n@MCQST_cluster \n@ERC_Research <LINK>']",https://arxiv.org/abs/2108.02203,"The postulate of gauge invariance in nature does not lend itself directly to implementations of lattice gauge theories in modern setups of quantum synthetic matter. Unavoidable gauge-breaking errors in such devices require gauge invariance to be enforced for faithful quantum simulation of gauge-theory physics. This poses major experimental challenges, in large part due to the complexity of the gauge-symmetry generators. Here, we show that gauge invariance can be reliably stabilized by employing simplified \textit{local pseudo generators} designed such that within the physical sector they act identically to the actual local generator. Dynamically, they give rise to emergent exact gauge theories up to timescales polynomial and even exponential in the protection strength. This obviates the need for implementing often complex multi-body full gauge symmetries, thereby further reducing experimental overhead in physical realizations. We showcase our method in the $\mathbb{Z}_2$ lattice gauge theory, and discuss experimental considerations for its realization in modern ultracold-atom setups. ","Stabilizing Lattice Gauge Theories Through Simplified Local Pseudo
  Generators"
115,1423486044457025536,917589174,Harold Erbin,"['New paper with @thesfinox, Robin Schneider and Mohamed Tamaazousti: we compute Hodge numbers for complete intersection #CalabiYau using #DeepLearning.\nAccuracy for 80% training ratio: 100% for h(1,1) and h(2,1), 96% for h(3,1), 83% for h(2,2)\n<LINK>', ""@thesfinox And it's a new milestone since it's my 30th paper!""]",https://arxiv.org/abs/2108.02221,"We continue earlier efforts in computing the dimensions of tangent space cohomologies of Calabi-Yau manifolds using deep learning. In this paper, we consider the dataset of all Calabi-Yau four-folds constructed as complete intersections in products of projective spaces. Employing neural networks inspired by state-of-the-art computer vision architectures, we improve earlier benchmarks and demonstrate that all four non-trivial Hodge numbers can be learned at the same time using a multi-task architecture. With 30% (80%) training ratio, we reach an accuracy of 100% for $h^{(1,1)}$ and 97% for $h^{(2,1)}$ (100% for both), 81% (96%) for $h^{(3,1)}$, and 49% (83%) for $h^{(2,2)}$. Assuming that the Euler number is known, as it is easy to compute, and taking into account the linear constraint arising from index computations, we get 100% total accuracy. ",Deep multi-task mining Calabi-Yau four-folds
116,1423109540468252676,755924666,Brant Robertson,['New paper tonight led by Takashi Moriya on searching for distant exotic supernovae with \u2066@NASARoman\u2069! Amazing what Roman can do in this unique discovery space. <LINK>'],https://arxiv.org/abs/2108.01801,"Massive stars play critical roles for the reionization of the Universe. Individual massive stars at the reionization epoch (z > 6) are too faint to observe and quantify their contributions to reionization. Some massive stars, however, explode as superluminous supernovae (SLSNe) or pair-instability supernovae (PISNe) that are luminous enough to observe even at z > 6 and allow for the direct characterization of massive star properties at the reionization epoch. In addition, hypothetical long-sought-after PISNe are expected to be present preferentially at high redshifts, and their discovery will have a tremendous impact on our understanding of massive star evolution and the formation of stellar mass black holes. The near-infrared Wide Field Instrument on Nancy Grace Roman Space Telescope will excel at discovering such rare high-redshift supernovae. In this work, we investigate the best survey strategy to discover and identify SLSNe and PISNe at z > 6 with Roman. We show that the combination of the F158 and F213 filters can clearly separate both SLSNe and PISNe at z > 6 from nearby supernovae through their colors and magnitudes. The limiting magnitudes are required to be 27.0 mag and 26.5 mag in the F158 and F213 filters, respectively, to identify supernovae at z > 6. If we conduct a 10 deg2 transient survey with these limiting magnitudes for 5 years with a cadence of one year, we expect to discover 22.5 +- 2.8 PISNe and 3.1 +- 0.3 SLSNe at z > 6, depending on the cosmic star-formation history. The same survey is estimated to discover 76.1 +- 8.2 PISNe and 9.1 +- 0.9 SLSNe at 5 < z < 6. Such a supernova survey requires the total observational time of approximately 525 hours in 5 years. The legacy data acquired with the survey will also be beneficial for many different science cases including the study of high-redshift galaxies. ","Discovering Supernovae at Epoch of Reionization with Nancy Grace Roman
  Space Telescope"
117,1423096848730914821,203254308,Feng Li,"['New paper with Li Li &amp; @YanfeiKang.\nWe estimate weights in the forecast combination via Bayesian log predictive scores, in which the optimal forecasting combination is determined by time series features from historical information. Feedbacks are welcome! <LINK>']",https://arxiv.org/abs/2108.02082,"In this work, we propose a novel framework for density forecast combination by constructing time-varying weights based on time series features, which is called Feature-based Bayesian Forecasting Model Averaging (FEBAMA). Our framework estimates weights in the forecast combination via Bayesian log predictive scores, in which the optimal forecasting combination is determined by time series features from historical information. In particular, we use an automatic Bayesian variable selection method to add weight to the importance of different features. To this end, our approach has better interpretability compared to other black-box forecasting combination schemes. We apply our framework to stock market data and M3 competition data. Based on our structure, a simple maximum-a-posteriori scheme outperforms benchmark methods, and Bayesian variable selection can further enhance the accuracy for both point and density forecasts. ",Bayesian forecast combination using time-varying features
118,1422908502297780225,917589174,Harold Erbin,['New paper on #QFT and #renormalization for #NeuralNetworks is online! Inspired by great work from @jhhalverson.\nMain practical result: networks with weights initialized with different std are related by a renormalization flow.\n<LINK>'],https://arxiv.org/abs/2108.01403,"In a recent work arXiv:2008.08601, Halverson, Maiti and Stoner proposed a description of neural networks in terms of a Wilsonian effective field theory. The infinite-width limit is mapped to a free field theory, while finite $N$ corrections are taken into account by interactions (non-Gaussian terms in the action). In this paper, we study two related aspects of this correspondence. First, we comment on the concepts of locality and power-counting in this context. Indeed, these usual space-time notions may not hold for neural networks (since inputs can be arbitrary), however, the renormalization group provides natural notions of locality and scaling. Moreover, we comment on several subtleties, for example, that data components may not have a permutation symmetry: in that case, we argue that random tensor field theories could provide a natural generalization. Second, we improve the perturbative Wilsonian renormalization from arXiv:2008.08601 by providing an analysis in terms of the nonperturbative renormalization group using the Wetterich-Morris equation. An important difference with usual nonperturbative RG analysis is that only the effective (IR) 2-point function is known, which requires setting the problem with care. Our aim is to provide a useful formalism to investigate neural networks behavior beyond the large-width limit (i.e.~far from Gaussian limit) in a nonperturbative fashion. A major result of our analysis is that changing the standard deviation of the neural network weight distribution can be interpreted as a renormalization flow in the space of networks. We focus on translations invariant kernels and provide preliminary numerical results. ","Nonperturbative renormalization for the neural network-QFT
  correspondence"
119,1422690450905780227,88627644,Ray Norris,"[""After 13 years of hard work, we've  published the first deep survey from Evolutionary Map of the Universe. Lots of new science!  Many thanks to all the team who contributed so much to this paper, and to all the amazing CSIRO engineers who built ASKAP. <LINK>"", '@astroduff Many thanks Alan! Yes it‚Äôs all paying off now!']",http://arxiv.org/abs/2108.00569,"We present the data and initial results from the first Pilot Survey of the Evolutionary Map of the Universe (EMU), observed at 944 MHz with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope. The survey covers 270 \sqdeg of an area covered by the Dark Energy Survey, reaching a depth of 25--30 \ujybm\ rms at a spatial resolution of $\sim$ 11--18 arcsec, resulting in a catalogue of $\sim$ 220,000 sources, of which $\sim$ 180,000 are single-component sources. Here we present the catalogue of single-component sources, together with (where available) optical and infrared cross-identifications, classifications, and redshifts. This survey explores a new region of parameter space compared to previous surveys. Specifically, the EMU Pilot Survey has a high density of sources, and also a high sensitivity to low surface-brightness emission. These properties result in the detection of types of sources that were rarely seen in or absent from previous surveys. We present some of these new results here. ",The Evolutionary Map of the Universe Pilot Survey
120,1422663599181361152,333826633,Evan Nunez,"['Check out my new paper <LINK> w/ Evan Kirby and Chuck Steidel! (For the thread: DT=detailed-ish explanation) 1/8', 'Using very metal poor damped Lyman alpha absorbers (VMP DLAs) we constrain the amount of metals ejected from the first stars! DT: place empirical constraints on the core collapse supernova (CCSN) yields of zero- and low-metallicity stars 2/8', 'VMP DLAs are distant, dense, almost metal-free gas clouds that have been exclusively enriched by the first stars; meaning the metals we measure in them are reflective of the metals from the first stars. DT: we use the median of the abundance ratios from ~80 VMP DLAs) 3/8', 'We show that for some elements, using VMP DLAs is superior to using the abundances from VMP stars for this work. DT: measuring abundances from VMP DLAs cool, dense and neutral gas avoids stellar atmosphere and evolution corrections needed to infer abundances in VMP stars 4/8', 'We compare our yields to widely used  theoretical yields and find that all models do a good job on reproducing Si and S, but vary otherwise for C, N, Al, O, and Fe.  (DT: Woosley&amp;Weaver1995, Nomoto+2006, Heger&amp;Woosley2010, Limongi&amp;Chieffi2018, PUSH collaboration) 5/8', 'DT: We adopt a SN explosion landscape and Initial Distribution of Rotation Velocities onto HW10 and LC18 respectively, and find no change for HW10 and a slight improvement for LC18. When we impose and explosion energy landscape HW10 is unable to reproduce our yields. 6/8', 'Excitingly, our constraints can be used, right now, in models concerned with the chemical evolution of galaxies i.e., galactic chemical evolution models! 7/8', 'Future work would benefit from discovering more VMP DLAs, having more high resolution VMP DLA measurements, and measuring more elemental abundances in existing VMP DLA spectra. 8/8']",https://arxiv.org/abs/2108.00659,"We place empirical constraints on the yields from zero- and low-metallicity core collapse supernovae (CCSNe) using abundances measured in very metal-poor (VMP; [Fe/H] $\leq$ $-2$) Damped Lyman Alpha Absorbers (DLAs). For some abundance ratios ([N,Al,S/Fe]), VMP DLAs constrain the metal yields of the first SNe more reliably than VMP stars. We compile a large sample of high-S/N VMP DLAs from over 30 years of literature, most with high resolution spectral measurements. We infer the IMF-averaged CCSNe yield from the median values from the DLA abundance ratios of C, N, O, Al, Si, S, and Fe (over Fe and O). We assume that the DLAs are metal-poor enough that they represent galaxies in their earliest stages of evolution, when CCSNe are the only nucleosynthetic sources of the metals we analyze. We compare five sets of zero- and low-metallicity theoretical yields to the empirical yields derived in this work. We find that the five models agree with the DLA yields for ratios containing Si and S. Only one model, Heger & Woosley (2010, hereafter HW10), reproduced the DLA values for N, and one other model, Limongi & Chieffi (2018, hereafter LC18), reproduced [N/O]. We found little change in the theoretical yields with the adoption of a SN explosion landscape (where certain progenitor masses collapse into black holes, contributing no yields) onto HW10, but fixing explosion energy to progenitor mass results in wide disagreements between the predictions and DLA abundances. We investigate the adoption of a simple, observationally motivated Initial Distribution of Rotational Velocities for LC18 and find a slight improvement. ","Empirical Constraints on Core Collapse Supernova Yields using Very Metal
  Poor Damped Lyman Alpha Absorbers"
121,1422613885144686596,1140222123006472194,Kasper Elm Heintz,"['Do you know that scenario where you have an awesome idea, that turns out to not work at all? Well this new paper: <LINK> that just hit ArXiv is exactly that! üòÖ\nHere we tried to detect the CO emission of a sample of GRB-selected galaxies with high H2 abundances', 'Based on the premise that ‚Äúabundant in molecules = easy to detect in CO‚Äù ‚Äî however, it turns out that at these high redshifts and the low-metallicity galaxies probed by GRBs, the CO-to-H2 factor becomes *extremely* significant, something we then set up to explore quantatively. https://t.co/uIKj6O7Csd', 'This pilot study was done in collab with many excellent people, incl. @gullibjoss, @jfynbo, @tanmoylaskar, @darach + many non-tweeps. Looking forward to explore it further in the future! üí• https://t.co/IYxM0KxHgf']",https://arxiv.org/abs/2108.00714,"We present a pilot search of CO emission in three H$_2$-absorbing, long-duration gamma-ray burst (GRB) host galaxies at z~2-3. We used the Atacama Large Millimeter/sub-millimeter Array (ALMA) to target the CO(3-2) emission line and report non-detections for all three hosts. These are used to place limits on the host molecular gas masses, assuming a metallicity-dependent CO-to-H$_2$ conversion factor ($\alpha_{\rm CO}$). We find, $M_{\rm mol} < 3.5\times 10^{10}\,M_{\odot}$ (GRB\,080607), $M_{\rm mol} < 4.7\times 10^{11}\,M_{\odot}$ (GRB\,120815A), and $M_{\rm mol} < 8.9\times 10^{11}\,M_{\odot}$ (GRB\,181020A). The high limits on the molecular gas mass for the latter two cases are a consequence of their low stellar masses $M_\star$ ($M_\star \lesssim 10^{8}\,M_{\odot}$) and low gas-phase metallicities ($Z\sim 0.03\,Z_{\odot}$). The limit on the $M_{\rm mol}/M_\star$ ratio derived for GRB\,080607, however, is consistent with the average population of star-forming galaxies at similar redshifts and stellar masses. We discuss the broader implications for a metallicity-dependent CO-to-H$_2$ conversion factor, and demonstrate that the canonical Galactic $\alpha_{\rm CO}$, will severely underestimate the actual molecular gas mass for all galaxies at $z>1$ with $M_\star < 10^{10}\,M_\odot$. To better quantify this we develop a simple approach to estimate the relevant $\alpha_{\rm CO}$ factor based only on the redshift and stellar mass of individual galaxies. The elevated conversion factors will make these galaxies appear CO-""dark"" and difficult to detect in emission, as is the case for the majority of GRB hosts. GRB spectroscopy thus offers a complementary approach to identify low-metallicity, star-forming galaxies with abundant molecular gas reservoirs at high redshifts that are otherwise missed by current ALMA surveys. ","GRB host galaxies with strong H$_2$ absorption: CO-dark molecular gas at
  the peak of cosmic star formation"
122,1422508607271317507,120325394,Aswin P Vijayan,"['Paper day! Our new paper is on arxiv today, FLARES III: The properties of massive galaxies at cosmic dawn. It was really fun to write this paper, hope some of you do find it interesting. Also last chapter in my PhD thesis.\n<LINK>', 'Here is a short summary of the work:\nWe post-process the FLARES (https://t.co/hqNLzsaR62) galaxies with the radiative transfer code SKIRT to produce full SEDs. We look at the LFs, IRX-beta relation and luminosity-weighted dust temperatures in the EoR.', 'We find reasonable agreement of the IR LF of the galaxies, but underpredict the number densities of bright IR galaxies at z=5. https://t.co/5dA9yXs70i', 'Most galaxies in FLARES, follow the local starburst IRX-beta relation. https://t.co/NPpbxF39eW', 'Peak dust temperature (Tpeak) correlates strongly with sSFR. All luminosity-weighted dust temperatures increases towards high-z, with the slope of the Tpeak-z relation showing higher slope than previous observational and theoretical fits. https://t.co/xU5WEjMtjz', 'Thanks to all the co-authors for their help. @stewilkins @chrisclovell \nMore soon to come from FLARES. Stay tuned!']",https://arxiv.org/abs/2108.00830,"Using the First Light And Reionisation Epoch Simulations (\textsc{Flares}) we explore the dust driven properties of massive high-redshift galaxies at $z\in[5,10]$. By post-processing the galaxy sample using the radiative transfer code \textsc{skirt} we obtain the full spectral energy distribution. We explore the resultant luminosity functions, IRX-$\beta$ relations as well as the luminosity-weighted dust temperatures in the Epoch of Reionisation (EoR). We find that most of our results are in agreement with the current set of observations, but under-predict the number densities of bright IR galaxies, which are extremely biased towards the most overdense regions. We see that the \textsc{Flares} IRX-$\beta$ relation (for $5\le z\le8$) predominantly follows the local starburst relation. The IRX shows an increase with stellar mass, plateauing at the high-mass end ($\sim10^{10}$M$_{\odot}$) and shows no evolution in the median normalisation with redshift. We also look at the dependence of the peak dust temperature ($T_{\mathrm{peak}}$) on various galaxy properties including the stellar mass, IR luminosity and sSFR, finding the correlation to be strongest with sSFR. The luminosity-weighted dust temperatures increase towards higher redshifts, with the slope of the $T_{\mathrm{peak}}$ - redshift relation showing a higher slope than the lower redshift relations obtained from previous observational and theoretical works. The results from \textsc{Flares}, which is able to provide a better statistical sample of high-redshift galaxies compared to other simulations, provides a distinct vantage point for the high-redshift Universe. ","First Light And Reionisation Epoch Simulations (FLARES) III: The
  properties of massive dusty galaxies at cosmic dawn"
123,1422463360839294980,1097589146858663936,Bryce Clarke,"['Today I have a new preprint out titled ""Delta lenses as coalgebras for a monad"" (<LINK>). \nThis was my first time writing a paper under 10 pages, so I wanted to write a thread informally outlining the main ideas in a bit more detail. (1 / 11) <LINK>', 'Most people who have heard of (very well behaved) lenses probably know that they are coalgebras for a comonad on Set. I believe this result was first attributed to Russell O‚ÄôConnor in 2010, and a paper by Gibbons &amp; Johnson has more details (https://t.co/vsK3DIzKmO ). (2 / 11)', 'Delta lenses are a categorification of these classical lenses; they are morphisms between categories rather than sets, and still satisfy suitable lens laws. One of the key motivating questions of the paper: could delta lenses also be coalgebras for a comonad? (3 / 11)', 'For a long time I didn\'t believe this question had a suitable answer, and in the end I stumbled upon the result by accident! The crucial concept that was needed was that of a ""cofunctor"", understood as a generalised version of a discrete opfibration. (4 / 11)', 'For a fixed category B, the category of elements construction describes an equivalence between the functor category [B, Set] and the category DOpf(B) of discrete opfibrations over B. Importantly, discrete opfibrations are special kinds of cofunctors. (5 / 11)', 'A cofunctor is a span of functors, whose left leg is a bijective-on-objects functor, and whose right leg is a discrete opfibration. For a fixed category B, there is a category Cof(B) of cofunctors over a base, and DOpf(B) is a coreflective subcategory of it. (6 / 11)', ""However the subcategory inclusion DOpf(B) -&gt; Cof(B) doesn't preserve the terminal object, given by the identity functor into B. So we could consider the factorisation of the subcategory inclusion through the slice category Cof(B) / id_B. (7 / 11)"", 'Then a miracle happens: a delta lens is precisely an object in the slice category Cof(B) / id_B ! It has been known for a while that delta lenses are described by functor and cofunctors, but this was the first time they had been understood in this way. (8 / 11)', 'Moreover, since the category Cof(B) has all products, the forgetful functor Cof(B) / 1_B -&gt; Cof(B) has a right adjoint and is automatically comonadic! So in the end we find that delta lenses are coalgebras for a comonad with almost no work at all. (9 / 11)', ""There are a bunch of interested consequences which I outline in the paper, and there is also further generalisations which can be made which aren't included in the paper (but see my talk slides here https://t.co/tvICZEReHW). (10 / 11)"", 'Perhaps one of the most interesting questions this work motivates is whether there are other kinds of lenses which are coalgebras for a comonad? Can we unify this result to the one for classical lenses? And is there a connection to lawful optics? Time will tell! (11 / 11)', 'Just realised the typo! Title should read ""Delta lenses as coalgebras for a comonad"".']",https://arxiv.org/abs/2108.00390,"Delta lenses are a kind of morphism between categories which are used to model bidirectional transformations between systems. Classical state-based lenses, also known as very well-behaved lenses, are both algebras for a monad and coalgebras for a comonad. Delta lenses generalise state-based lenses, and while delta lenses have been characterised as certain algebras for a semi-monad, it is natural to ask if they also arise as coalgebras. This short paper establishes that delta lenses are coalgebras for a comonad, through showing that the forgetful functor from the category of delta lenses over a base, to the category of cofunctors over a base, is comonadic. The proof utilises a diagrammatic approach to delta lenses, and clarifies several results in the literature concerning the relationship between delta lenses and cofunctors. Interestingly, while this work does not generalise the corresponding result for state-based lenses, it does provide new avenues for exploring lenses as coalgebras. ",Delta lenses as coalgebras for a comonad
124,1422448676455264272,2816968963,Miguel A.F. Sanju√°n,['Our new paper: Ergodic decay laws in Newtonian and relativistic chaotic scattering\n\n<LINK> <LINK>'],https://arxiv.org/abs/2108.00186,"In open Hamiltonian systems, the escape from a bounded region of phase space according to an exponential decay law is frequently associated with the existence of hyperbolic dynamics in such a region. Furthermore, exponential decay laws based on the ergodic hypothesis are used to describe escapes in these systems. However, we uncover that the presence of the set that governs the hyperbolic dynamics, commonly known as the chaotic saddle, invalidates the assumption of ergodicity. For the paradigmatic H\'enon-Heiles system, we use both theoretical and numerical arguments to show that the escaping dynamics is non-ergodic independently of the existence of KAM tori, since the chaotic saddle, in whose vicinity trajectories are more likely to spend a finite amount of time evolving before escaping forever, is not utterly spread over the energy shell. Taking this into consideration, we provide a clarifying discussion about ergodicity in open Hamiltonian systems and explore the limitations of ergodic decay laws when describing escapes in this kind of systems. Finally, we generalize our claims by deriving a new decay law in the relativistic regime for an inertial and a non-inertial reference frames under the assumption of ergodicity, and suggest another approach to the description of escape laws in open Hamiltonian systems. ",Ergodic decay laws in Newtonian and relativistic chaotic scattering
125,1433063106037436424,816751476571799553,Mason Earles,['New paper out! How can we use 3D simulated crops to train deep learning models for fruit detection across the diversity of sensing conditions? <LINK> @ucdplantsimlab <LINK>'],https://arxiv.org/abs/2108.13344,"Training real-world neural network models to achieve high performance and generalizability typically requires a substantial amount of labeled data, spanning a broad range of variation. This data-labeling process can be both labor and cost intensive. To achieve desirable predictive performance, a trained model is typically applied into a domain where the data distribution is similar to the training dataset. However, for many agricultural machine learning problems, training datasets are collected at a specific location, during a specific period in time of the growing season. Since agricultural systems exhibit substantial variability in terms of crop type, cultivar, management, seasonal growth dynamics, lighting condition, sensor type, etc, a model trained from one dataset often does not generalize well across domains. To enable more data efficient and generalizable neural network models in agriculture, we propose a method that generates photorealistic agricultural images from a synthetic 3D crop model domain into real world crop domains. The method uses a semantically constrained GAN (generative adversarial network) to preserve the fruit position and geometry. We observe that a baseline CycleGAN method generates visually realistic target domain images but does not preserve fruit position information while our method maintains fruit positions well. Image generation results in vineyard grape day and night images show the visual outputs of our network are much better compared to a baseline network. Incremental training experiments in vineyard grape detection tasks show that the images generated from our method can significantly speed the domain adaption process, increase performance for a given number of labeled images (i.e. data efficiency), and decrease labeling requirements. ","Enlisting 3D Crop Models and GANs for More Data Efficient and
  Generalizable Fruit Detection"
126,1432881629492486152,821014804232097793,Esin Durmus,"['Checkout our new paper: ‚ÄúFaithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization‚Äù. <LINK> #NLProc 1/n', 'While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs. 2/n', 'We present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulness- abstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum. 3/n', 'We show that the MLE baseline as well as a recently proposed method for improving faithfulness (loss truncation) are both worse than the control at the same level of abstractiveness. 4/n', 'We further learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets. 5/n', 'With great collaborators: @faisalladhak, @hhexiy, Kathleen Mckeown and Claire Cardie.']",https://arxiv.org/abs/2108.13684,"Despite recent progress in abstractive summarization, systems still suffer from faithfulness errors. While prior work has proposed models that improve faithfulness, it is unclear whether the improvement comes from an increased level of extractiveness of the model outputs as one naive way to improve faithfulness is to make summarization models more extractive. In this work, we present a framework for evaluating the effective faithfulness of summarization systems, by generating a faithfulnessabstractiveness trade-off curve that serves as a control at different operating points on the abstractiveness spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as well as a recently proposed method for improving faithfulness, are both worse than the control at the same level of abstractiveness. Finally, we learn a selector to identify the most faithful and abstractive summary for a given document, and show that this system can attain higher faithfulness scores in human evaluations while being more abstractive than the baseline system on two datasets. Moreover, we show that our system is able to achieve a better faithfulness-abstractiveness trade-off than the control at the same level of abstractiveness. ","Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness
  Trade-off in Abstractive Summarization"
127,1431284942605066248,366380609,Evan Rosenman,"['New paper from me, @rina_friedberg, and @BaiocchiMike: ""Robust Designs for Prospective Randomized Trials Surveying Sensitive Topics"". <LINK>', 'This work emerged out of our experience analyzing a cluster-randomized trial of an empowerment training program deployed to adolescent girls in Nairobi, Kenya. The treatment was intended to reduce the incidence of gender-based violence.', 'When surveying sensitive topics, reporting biases -- e.g. the possibility of underreporting troubling outcomes -- pose a threat to causal inference. We approach the problem under the potential outcomes framework, assuming a binary outcome.', 'We suppose reporting behavior is fixed given the choice of survey and show the joint distribution of ""reporting classes"" (e.g. underreporter, overreporter, truth-teller) and ""response classes"" (e.g. outcome increases, decreases, stays the same) determines the bias exactly. https://t.co/saUrrRg92u', 'Then, we propose a sensitivity model and an optimization procedure to determine the required sample size for achieving a desired power level, given the worst-case configuration of misreporters. https://t.co/mCsRmEWxcK', 'This is a challenging area! Insights from social scientists + local stakeholders are crucial to design the best survey instruments. Rigorous practices must be followed to preserve comfort and anonymity. Then, statisticians can design procedures to help address residual biases.', 'We hope folks find these results interesting and welcome any feedback!']",https://arxiv.org/abs/2108.08944,"We consider the problem of designing a prospective randomized trial in which the outcome data will be self-reported, and will involve sensitive topics. Our interest is in misreporting behavior, and how respondents' tendency to under- or overreport a binary outcome might affect the power of the experiment. We model the problem by assuming each individual in our study is a member of one ""reporting class"": a truth-teller, underreporter, overreporter, or false-teller. We show that the joint distribution of reporting classes and ""response classes"" (characterizing individuals' response to the treatment) will exactly define the bias and variance of the causal estimate in our experiment. Then, we propose a novel procedure for deriving sample sizes under the worst-case power corresponding to a given level of misreporting. Our problem is motivated by prior experience implementing a randomized controlled trial of a sexual violence prevention program among adolescent girls in Nairobi, Kenya. ","Robust Designs for Prospective Randomized Trials Surveying Sensitive
  Topics"
128,1428739477770489863,630560519,Dr Johanna Vos,"['Delighted that our new paper, led by grad student Mary Anne Limbach, has been accepted for publication in ApJL! We explore the prospects of detecting exomoons orbiting isolated, planetary-mass companions <LINK> üßµ', 'Based on gas giant moons in our own Solar System as well as a variety of moon formation simulations, we expect that moons around gas giants are common. But the question we asked is - could we actually detect them using the transit method?', 'We calculate the probability that at least one transiting companion exists in a sample. For a sample of 10 targets, the probability that at least one companion transits is 10% for Mercury-Sun analogs, 68% for Europa-Jupiter analogs, and 85% for Io-Jupiter analogs. Not bad! https://t.co/4SAGa6rmVS', 'Furthermore, we find that the transit probability of a habitable zone companion is at a maximum for isolated planetary-mass objects and brown dwarfs. Brown dwarfs and planetary-mass objects are in the sweet spot to host habitable, transiting companions. https://t.co/dxH4uOZ8uk', 'We also looked at the photometric precision required to detect an exomoon using current facilities. More than 50% of the currently known isolated planetary-mass objects are bright enough to detect Titan or Ganymede-sized moons during one transit with JWST. https://t.co/JcaYntF2wi', 'Finally, we note that cloud-driven variability is going to make it hard to draw conclusions on transit signals! As an example we explore a mysterious dimming event in the light curve of 2M1119. This signal may be due to a 1.7 R_earth exomoon or evolving variability. https://t.co/UFrxYyoZXa', 'There is a LOT more in the paper so please take a look! https://t.co/PgC4IvLFQa']",https://arxiv.org/abs/2108.08323,"All-sky imaging surveys have identified several dozen isolated planetary-mass objects (IPMOs), far away from any star. Here, we examine the prospects for detecting transiting moons around these objects. We expect transiting moons to be common, occurring around 10-15% of IPMOs, given that close-orbiting moons have a high geometric transit probability and are expected to be a common outcome of giant planet formation. IPMOs offer an advantage over other directly imaged planets in that high-contrast imaging is not necessary to detect the photometric transit signal. For at least 30 (>50%) of the currently known IPMOs, observations of a single transit with the James Webb Space Telescope would have low enough forecasted noise levels to allow for the detection of an Io-like or Titan-like moon. Intrinsic variability of the IPMOs will be an obstacle. Using archival time-series photometry of IPMOs with the Spitzer Space Telescope as a proof-of-concept, we found evidence for a fading event of 2MASS J1119-1137 AB that might have been caused by intrinsic variability, but is also consistent with a single transit of a habitable-zone 1.7$R_\oplus$ exomoon. Although the interpretation of this particular event is inconclusive, the characteristics of the data and the candidate signal suggest that Earth-sized habitable-zone exomoons around IPMOs are detectable with existing instrumentation. ",On the Detection of Exomoons Transiting Isolated Planetary-Mass Objects
129,1428074387123544068,1138838376,Baishakhi Ray,['Another step forward to automated code generation...out new paper on multimodal code edit will appear in @ASE_conf (<LINK>)'],https://arxiv.org/abs/2108.06645,"In recent years, Neural Machine Translator (NMT) has shown promise in automatically editing source code. Typical NMT based code editor only considers the code that needs to be changed as input and suggests developers with a ranked list of patched code to choose from - where the correct one may not always be at the top of the list. While NMT based code editing systems generate a broad spectrum of plausible patches, the correct one depends on the developers' requirement and often on the context where the patch is applied. Thus, if developers provide some hints, using natural language, or providing patch context, NMT models can benefit from them. As a proof of concept, in this research, we leverage three modalities of information: edit location, edit code context, commit messages (as a proxy of developers' hint in natural language) to automatically generate edits with NMT models. To that end, we build MODIT, a multi-modal NMT based code editing engine. With in-depth investigation and analysis, we show that developers' hint as an input modality can narrow the search space for patches and outperform state-of-the-art models to generate correctly patched code in top-1 position. ",On Multi-Modal Learning of Editing Source Code
130,1427750806137233414,1263676931112947712,Abdullah Abuolaim,"['In addition to being useful for defocus deblurring, what else Dual-Pixel (DP) can offer?\n.\nCheck our DP-based multi-view synthesis and a New Image Motion Attribute (NIMAT).\n.\nPaper: <LINK>\nGitHub: <LINK>\n.\nWith M. Afifi @mahmoudnafifi &amp; M. Brown <LINK>']",https://arxiv.org/abs/2108.05251,"Many camera sensors use a dual-pixel (DP) design that operates as a rudimentary light field providing two sub-aperture views of a scene in a single capture. The DP sensor was developed to improve how cameras perform autofocus. Since the DP sensor's introduction, researchers have found additional uses for the DP data, such as depth estimation, reflection removal, and defocus deblurring. We are interested in the latter task of defocus deblurring. In particular, we propose a single-image deblurring network that incorporates the two sub-aperture views into a multi-task framework. Specifically, we show that jointly learning to predict the two DP views from a single blurry input image improves the network's ability to learn to deblur the image. Our experiments show this multi-task strategy achieves +1dB PSNR improvement over state-of-the-art defocus deblurring methods. In addition, our multi-task framework allows accurate DP-view synthesis (e.g., ~39dB PSNR) from the single input image. These high-quality DP views can be used for other DP-based applications, such as reflection removal. As part of this effort, we have captured a new dataset of 7,059 high-quality images to support our training for the DP-view synthesis task. Our dataset, code, and trained models are publicly available at this https URL ","Improving Single-Image Defocus Deblurring: How Dual-Pixel Images Help
  Through Multi-Task Learning"
131,1426108283950075906,1044620333783896065,Pieter Wolfert,['New paper out! To Rate or Not To Rate: Investigating Evaluation Methods for Generated Co-Speech Gestures (<LINK>). Work with @jeffreymgirard @SvitozarTaras @TonyBelpaeme'],https://arxiv.org/abs/2108.05709,"While automatic performance metrics are crucial for machine learning of artificial human-like behaviour, the gold standard for evaluation remains human judgement. The subjective evaluation of artificial human-like behaviour in embodied conversational agents is however expensive and little is known about the quality of the data it returns. Two approaches to subjective evaluation can be largely distinguished, one relying on ratings, the other on pairwise comparisons. In this study we use co-speech gestures to compare the two against each other and answer questions about their appropriateness for evaluation of artificial behaviour. We consider their ability to rate quality, but also aspects pertaining to the effort of use and the time required to collect subjective data. We use crowd sourcing to rate the quality of co-speech gestures in avatars, assessing which method picks up more detail in subjective assessments. We compared gestures generated by three different machine learning models with various level of behavioural quality. We found that both approaches were able to rank the videos according to quality and that the ranking significantly correlated, showing that in terms of quality there is no preference of one method over the other. We also found that pairwise comparisons were slightly faster and came with improved inter-rater reliability, suggesting that for small-scale studies pairwise comparisons are to be favoured over ratings. ","To Rate or Not To Rate: Investigating Evaluation Methods for Generated
  Co-Speech Gestures"
132,1426044150592331783,890798333094019072,Paul Thompson,['Can #DeepLearning detect patterns in brain scans WITHOUT sharing the scans? Fun new #cryptography paper with super-smart scientists in our building in Marina del Rey !!  <LINK> <LINK>'],https://arxiv.org/abs/2108.03437,"Federated learning (FL) enables distributed computation of machine learning models over various disparate, remote data sources, without requiring to transfer any individual data to a centralized location. This results in an improved generalizability of models and efficient scaling of computation as more sources and larger datasets are added to the federation. Nevertheless, recent membership attacks show that private or sensitive personal data can sometimes be leaked or inferred when model parameters or summary statistics are shared with a central site, requiring improved security solutions. In this work, we propose a framework for secure FL using fully-homomorphic encryption (FHE). Specifically, we use the CKKS construction, an approximate, floating point compatible scheme that benefits from ciphertext packing and rescaling. In our evaluation on large-scale brain MRI datasets, we use our proposed secure FL framework to train a deep learning model to predict a person's age from distributed MRI scans, a common benchmarking task, and demonstrate that there is no degradation in the learning performance between the encrypted and non-encrypted federated models. ","Secure Neuroimaging Analysis using Federated Learning with Homomorphic
  Encryption"
133,1425216411505364994,843929270,Dr. Charan Ranganath,"[""New preprint &amp; my first paper w/Randy O'Reilly. We discuss the computational benefits of differentiating b/w  content and structure in a variety of domains. We propose that event structure may be represented by the PM Network via predictive learning  <LINK>""]",https://arxiv.org/abs/2108.03387,"A hallmark of human intelligence is the ability to adapt to new situations, by applying learned rules to new content (systematicity) and thereby enabling an open-ended number of inferences and actions (generativity). Here, we propose that the human brain accomplishes these feats through pathways in the parietal cortex that encode the abstract structure of space, events, and tasks, and pathways in the temporal cortex that encode information about specific people, places, and things (content). Recent neural network models show how the separation of structure and content might emerge through a combination of architectural biases and learning, and these networks show dramatic improvements in the ability to capture systematic, generative behavior. We close by considering how the hippocampal formation may form integrative memories that enable rapid learning of new structure and content representations. ",The Structure of Systematicity in the Brain
134,1432969414912131076,284468794,Joe Zuntz,"['Paper out today! We want to know if we can use a limited set of wavelengths (riz) to classify galaxies into broad redshift bins, so launched a mini-challenge to find  methods. The answer seems to be ""yes"", at least if the training data is good enough.<LINK>']",https://arxiv.org/abs/2108.13418,"This paper presents the results of the Rubin Observatory Dark Energy Science Collaboration (DESC) 3x2pt tomography challenge, which served as a first step toward optimizing the tomographic binning strategy for the main DESC analysis. The task of choosing an optimal tomographic binning scheme for a photometric survey is made particularly delicate in the context of a metacalibrated lensing catalogue, as only the photometry from the bands included in the metacalibration process (usually riz and potentially g) can be used in sample definition. The goal of the challenge was to collect and compare bin assignment strategies under various metrics of a standard 3x2pt cosmology analysis in a highly idealized setting to establish a baseline for realistically complex follow-up studies; in this preliminary study, we used two sets of cosmological simulations of galaxy redshifts and photometry under a simple noise model neglecting photometric outliers and variation in observing conditions, and contributed algorithms were provided with a representative and complete training set. We review and evaluate the entries to the challenge, finding that even from this limited photometry information, multiple algorithms can separate tomographic bins reasonably well, reaching figures-of-merit scores close to the attainable maximum. We further find that adding the g band to riz photometry improves metric performance by ~15% and that the optimal bin assignment strategy depends strongly on the science case: which figure-of-merit is to be optimized, and which observables (clustering, lensing, or both) are included. ",The LSST-DESC 3x2pt Tomography Optimization Challenge
135,1430577004018012161,704533062860681216,Patrick Coles,"[""New paper from our Summer School üî•\n\n<LINK>\n\nWe propose a new optimizer that can save you time ‚åöÔ∏è and money üíµ when implementing a variational algorithm. We analytically prove fast convergence! See Andrew's thread below... <LINK> <LINK>""]",https://arxiv.org/abs/2108.10434,"Variational Quantum Algorithms (VQAs) are a promising approach for practical applications like chemistry and materials science on near-term quantum computers as they typically reduce quantum resource requirements. However, in order to implement VQAs, an efficient classical optimization strategy is required. Here we present a new stochastic gradient descent method using an adaptive number of shots at each step, called the global Coupled Adaptive Number of Shots (gCANS) method, which improves on prior art in both the number of iterations as well as the number of shots required. These improvements reduce both the time and money required to run VQAs on current cloud platforms. We analytically prove that in a convex setting gCANS achieves geometric convergence to the optimum. Further, we numerically investigate the performance of gCANS on some chemical configuration problems. We also consider finding the ground state for an Ising model with different numbers of spins to examine the scaling of the method. We find that for these problems, gCANS compares favorably to all of the other optimizers we consider. ","Adaptive shot allocation for fast convergence in variational quantum
  algorithms"
136,1430195746805067789,23462367,Jeffrey West,"['üö®  üö®  üö®\nNew preprint pioneered by @GregoryJKimmel, @EvolSci \n\nThe question:\n\nCan the growth characteristics of cell lines be condensed into a single parameter?\n\nIn this paper, we propose that local neighborhood size is a key parameter.\n\n<LINK> <LINK>', '#mathonco folks will study growth laws until the end of time... but still, a mechanistic basis of Gompertzian growth remains elusive\n\nStarting from 2.7 (derived from simple assumptions about contact inhibition), a wide variety of growth laws can be found (including Gompertz!) https://t.co/Mirvef81gB', 'Gompertz fits better at high confluency (A) yes, \n\n...but cell lines can also be mapped onto a growth rate - neighborhood size space (B).\n\nWe confirmed this relationship using agent-based models with different neighborhood sizes (bottom panels) https://t.co/P5sHdA3AxE', 'We build in various neighborhood constraints into an agent-based model and determine which growth law they follow. https://t.co/RHWMgNihCu', 'Contact inhibition can provide the roadmap which dictates which growth law any particular cell line will follow. Migration is important, but under high migration assumption the size of the local neighborhood is important.\n\nhttps://t.co/l3Yg1PyYqz']",https://arxiv.org/abs/2108.10000,"Cancer cell population dynamics often exhibit remarkably replicable, universal laws despite their underlying heterogeneity. Mechanistic explanations of universal cell population growth remain partly unresolved to this day, whereby population feedback between the microscopic and mesoscopic configurations can lead to macroscopic growth laws. We here present a unification under density-dependent birth events via contact inhibition. We consider five classical tumor growth laws: exponential, generalized logistic, Gompertz, radial growth, and fractal growth, which can be seen as manifestations of a single microscopic model. Our theory is substantiated by agent based simulations and can explain growth curve differences in experimental data from in vitro cancer cell population growth. Thus, our framework offers a possible explanation for the large number of mean-field laws that can adequately capture seemingly unrelated cancer or microbial growth dynamics. ","Local contact inhibition leads to universal principles of cell
  population growth"
137,1429929055898542086,767037047668371456,Valentin Sulzer,"['New preprint on arxiv led by @PMohtat \n\nWe propose a fast charging algorithm that automatically minimizes charge time while satisfying voltage, overpotential, stress, and temp constraints. This is achieved simply by adding a single ODE to any model.\n\n<LINK>', 'The main result: the algorithm rides each constraint in turn as it is reached. Just one extra ODE, no optimal control algorithm required! \n\nHere comparing standard CC-CV (left) with CC-CVEST (right) https://t.co/KGraNXlU9s', '@ICBillyWu @PMohtat A homage to our Greek PI @anna_energy üòÅ']",https://arxiv.org/abs/2108.07833,"Fast charging of lithium-ion batteries is crucial to increase desirability for consumers and hence accelerate the adoption of electric vehicles. A major barrier to shorter charge times is the accelerated aging of the battery at higher charging rates, which can be driven by lithium plating, increased solid electrolyte interphase growth due to elevated temperatures, and particle cracking due to mechanical stress. Lithium plating depends on the overpotential of the negative electrode, and mechanical stress depends on the concentration gradient, both of which cannot be measured directly. Techniques based on physics-based models of the battery and optimal control algorithms have been developed to this end. While these methods show promise in reducing degradation, their optimization algorithms' complexity can limit their implementation. In this paper, we present a method based on the constant current constant voltage (CC-CV) charging scheme, called CC-CV$\eta \sigma$T (VEST). The new approach is simpler to implement and can be used with any model to impose varying levels of constraints on variables pertinent to degradation, such as plating potential and mechanical stress. We demonstrate the new CC-CV$\eta \sigma$T charging using an electrochemical model with mechanical and thermal effects included. Furthermore, we discuss how uncertainties can be accounted for by considering safety margins for the plating and stress constraints. ",An Algorithmic Safety VEST For Li-ion Batteries During Fast Charging
138,1428336031078756353,1164202716,Magnus Jonsson,"['Thank you @KAWstiftelsen for the nice description of our project, and for enabling our research. Our latest study just got available online, where we demonstrate electrical tuning of the nanoantennas. <LINK> <LINK>', '...and here is the previous publication that is mentioned in the text. https://t.co/0MVp22eSE6']",https://arxiv.org/abs/2108.04045,"Nanostructures of conventional metals offer manipulation of light at the nanoscale but are limited to static behavior due to their fixed material properties. To develop the next frontier of dynamic nanooptics and metasurfaces, we utilize the redox-tunable optical properties of conducting polymers, which were recently shown to be capable of sustaining plasmons in their most conducting oxidized state. Using nanodisks of poly(3,4-ethylenedioxythiophene:sulfate) (PEDOT:Sulf) as a model system, we present the first electrically tunable conducting polymer nanooptical antennas. In addition to repeated on/off switching of the polymeric nanoantennas, we demonstrate the possibility for gradual electrical tuning of their nanooptical response, which was found to be related to the modulation of both density and mobility of the mobile polaronic charge carriers in the polymer. The presented concept takes important steps towards electrically tunable metasurfaces with truly dynamic optical nanoantenna pixels, with not only varying farfield but also tunable nearfield. The work paves the way for applications ranging from tunable flat metaoptics to adaptable smart windows. ",Electrical Tuning of Plasmonic Conducting Polymer Nanoantennas
139,1427851262523940864,826222576754057216,Aaron Parsons,"['Our next big HERA paper is out! Interpreting our groundbreaking limits, we find evidence for early heating in the universe, most likely from galaxies that are brighter in X-ray than they are today. <LINK>', 'Because our measurements come from a later epoch in the evolution of the universe, we cannot confirm or refute a cosmological interpretation of the EDGES feature seen at z~18, but with HERA‚Äôs upgraded receivers, we may soon be able to address this directly.', 'This work is the culmination of an immense amount of effort by many dedicated scientists. As a collaboration, we have begun listing author contributions to recognize the efforts of our junior colleagues who have poured their careers into this. More groups should do this.']",http://arxiv.org/abs/2108.07282,"Recently, the Hydrogen Epoch of Reionization Array (HERA) collaboration has produced the experiment's first upper limits on the power spectrum of 21-cm fluctuations at z~8 and 10. Here, we use several independent theoretical models to infer constraints on the intergalactic medium (IGM) and galaxies during the epoch of reionization (EoR) from these limits. We find that the IGM must have been heated above the adiabatic cooling threshold by z~8, independent of uncertainties about the IGM ionization state and the nature of the radio background. Combining HERA limits with galaxy and EoR observations constrains the spin temperature of the z~8 neutral IGM to 27 K < T_S < 630 K (2.3 K < T_S < 640 K) at 68% (95%) confidence. They therefore also place a lower bound on X-ray heating, a previously unconstrained aspects of early galaxies. For example, if the CMB dominates the z~8 radio background, the new HERA limits imply that the first galaxies produced X-rays more efficiently than local ones (with soft band X-ray luminosities per star formation rate constrained to L_X/SFR = { 10^40.2, 10^41.9 } erg/s/(M_sun/yr) at 68% confidence), consistent with expectations of X-ray binaries in low-metallicity environments. The z~10 limits require even earlier heating if dark-matter interactions (e.g., through millicharges) cool down the hydrogen gas. Using a model in which an extra radio background is produced by galaxies, we rule out (at 95% confidence) the combination of high radio and low X-ray luminosities of L_{r,\nu}/SFR > 3.9 x 10^24 W/Hz/(M_sun/yr) and L_X/SFR<10^40 erg/s/(M_sun/yr). The new HERA upper limits neither support nor disfavor a cosmological interpretation of the recent EDGES detection. The analysis framework described here provides a foundation for the interpretation of future HERA results. ","HERA Phase I Limits on the Cosmic 21-cm Signal: Constraints on
  Astrophysics and Cosmology During the Epoch of Reionization"
140,1427842534626119681,2909395381,Andrey üî¨üß†üê†üí§,"['Physical data storage is still a major problem for microscopists - and biologist in general.\n\nWe propose how it can be improved: modern data infrastructure supported by funding:\n\n<LINK>\n\n@KristinBriney @temorrell @sandragesing @manorlaboratory \n1/5', 'We reviewed common ""solutions"" for data storage: cloud, USB drives, dedicated servers, university-supported clusters.\nData plumbing needs major improvement!\nWe also need your feedback and input. Let\'s solve this together.\n\n2/5', 'Physical data storage is a common problem. Astronomers face similar issues, and have come up with some good ideas! We present short section about their problems and successes.\n\n3/5', 'Our proposal: admit that we have data plumbing problem and that it is not too expensive to solve. Then, convince funding agencies and universities to work together to establish shared funds to support upgrades to data infra. It must support new projects and collaborations.\n\n4/5', 'This position statement was the result of more than a year of talking to wonderful, helpful people.\nSpecial thanks to @damiandn for major contribution.\n\nThanks to @valonychus, Dan Koo, @fracutrale, Jeremy Weimer, @Campbell_JD_PhD, Sarah Nusser, @JamesJonkman ...', ""... [cont'] thanks to Eric Wait and Blair Rossetti of @AICjanelia, @BenSteventon2, @Daniel_Wa19, @viktri08, Henry Neeman, @mjuric, and Alexander Szalay for talking with us and sharing your experience!\n\n5/5"", 'The most important part:\nwe need your input. Please email us with your experience and opinion, so we can improve the plan and make it happen.']",https://arxiv.org/abs/2108.07631,"Modern tools for biological research, especially microscopy, have rapidly advanced in recent years, which has led to the generation of increasingly large amounts of data on a regular basis. The result is that scientists desperately need state-of-the-art technical infrastructure for raw data storage, transfer, and processing. These scientists currently rely on outdated ways to move and store data, costing valuable time and risking loss of valuable data. While the community is aware of modern approaches to data management and high-level principles (including FAIR), highly-trained and highly-paid scientists are forced to spend time dealing with technical problems, which can ultimately costs more than providing storage and a fast network on campus. Here we provide concise arguments for better infrastructure, blueprints of possible solutions, and advice in navigating the political process of solving this issue. We suggest, as a broad solution, separate NIH-managed fund for supporting universities and institutes in deployment of data storage and long-term data sharing for all funded projects. This position statement is open for more contributors from imaging, life sciences, and other disciplines. Please contact us with your experience and perspective. ",Biologists need modern data infrastructure on campus
141,1427678621058027529,1193288437052260352,Laurel Orr,"['Ever woke up thinking ‚Äúwhat will ML pipelines look like in the next few years?‚Äù. Come to our tutorial ‚ÄúML Pipelines: Feature Stores and the Coming Wave of Embedding Ecosystems‚Äù 8/18/21 7:15am PT at VLDB to find out what we think the future will hold.\n\n<LINK>', 'Our take: self-supervised ecosystems, where embedding representations are learned over massive corpora and integrated into hundreds of downstream systems, are shifting the ML pipeline from the manual feature curation and data labeling of Feature Stores to hands-free training.', 'In this new self-supervised paradigm, engineers face challenges with respect to overcoming potential biases (e.g., popularity bias) in the uncurated training data, managing embedding stability, and continually monitoring and maintaining models.', 'We will discuss these challenges and exciting open problems tomorrow! This tutorial would not have been possible without my amazing collaborators @atinsanyal @lingxiao @m_leszczy @krandiash']",https://arxiv.org/abs/2108.05053,"The industrial machine learning pipeline requires iterating on model features, training and deploying models, and monitoring deployed models at scale. Feature stores were developed to manage and standardize the engineer's workflow in this end-to-end pipeline, focusing on traditional tabular feature data. In recent years, however, model development has shifted towards using self-supervised pretrained embeddings as model features. Managing these embeddings and the downstream systems that use them introduces new challenges with respect to managing embedding training data, measuring embedding quality, and monitoring downstream models that use embeddings. These challenges are largely unaddressed in standard feature stores. Our goal in this tutorial is to introduce the feature store system and discuss the challenges and current solutions to managing these new embedding-centric pipelines. ","Managing ML Pipelines: Feature Stores and the Coming Wave of Embedding
  Ecosystems"
142,1427437005558882317,937926012578549760,Andrew S. Rosen,"['Now out on @arxiv, find our recent perspective on data-driven MOF catalysis. In addition to a brief review of #compchem work in this space, we share ideas for how to address roadblocks currently facing the widespread use of #ML for MOF catalyst discovery.\n\n<LINK>', ""Feedback and suggestions are highly encouraged! Please don't hesitate to reach out either here or via email."", '@mepgg Thank you!! üòÅ https://t.co/m9t72ZaoEN']",https://arxiv.org/abs/2108.06667,"Metal-organic frameworks (MOFs) have been widely investigated for challenging catalytic transformations due to their well-defined structures and high degree of synthetic tunability. These features, at least in principle, make MOFs ideally suited for a computational approach towards catalyst design and discovery. Nonetheless, the widespread use of data science and machine learning to accelerate the discovery of MOF catalysts has yet to be substantially realized. In this review, we provide an overview of recent work that sets the stage for future high-throughput computational screening and machine learning studies involving MOF catalysts. This is followed by a discussion of several challenges currently facing the broad adoption of data-centric approaches in MOF computational catalysis, and we share possible solutions that can help propel the field forward. ","Realizing the Data-Driven, Computational Discovery of Metal-Organic
  Framework Catalysts"
143,1427436677698670592,1219708000790876161,Datta Lab,"['Interested in self-propelled living &amp; active systems? In <LINK>, we describe recent progress in the study of active transport in complex environments, focusing on two key biological systems‚Äîbacteria &amp; eukaryotic cells‚Äîas archetypes of active matter. (1/6)', 'Active transport is fundamentally interesting in biology, physics, &amp; engineering, and is important to biomedical, environmental, &amp; industrial processes. How do complexities such as geometric constraints, mechanical cues, and external stimuli influence transport? (2/6)', 'In this chapter to be published in a book by @RoySocChem press, we review research highlighting how such environmental factors can fundamentally alter cellular motility, hindering or promoting active transport in unexpected ways, &amp; giving rise to fascinating new behaviors. (3/6)', 'In parallel, we describe open questions and promising avenues for future research, and describe connections to other active systems &amp; more general theoretical/computational models of transport processes in complex environments. (4/6)', 'Our goal in writing this chapter was not to present a comprehensive overview of all the literature in the field, but rather, to highlight some active (pun intended) areas of research whose growth has been particularly rapid recently.  (5/6)', 'It was a lot of fun to work on this with postdocs Alejandro Mart√≠nez-Calvo and Carolina Trenado-Yuste. Please RT/share with anyone who might be interested interested. As always, any and all feedback is welcome! (6/6)']",http://arxiv.org/abs/2108.07011,"The ability of many living systems to actively self-propel underlies critical biomedical, environmental, and industrial processes. While such active transport is well-studied in uniform settings, environmental complexities such as geometric constraints, mechanical cues, and external stimuli such as chemical gradients and fluid flow can strongly influence transport. In this chapter, we describe recent progress in the study of active transport in such complex environments, focusing on two prominent biological systems -- bacteria and eukaryotic cells -- as archetypes of active matter. We review research findings highlighting how environmental factors can fundamentally alter cellular motility, hindering or promoting active transport in unexpected ways, and giving rise to fascinating behaviors such as directed migration and large-scale clustering. In parallel, we describe specific open questions and promising avenues for future research. Furthermore, given the diverse forms of active matter -- ranging from enzymes and driven biopolymer assemblies, to microorganisms and synthetic microswimmers, to larger animals and even robots -- we also describe connections to other active systems as well as more general theoretical/computational models of transport processes in complex environments. ",Active transport in complex environments
144,1427065722937749506,93189119,Nick Krichevsky,"[""Now that it's out there a bit more publically: \n\nI'm excited to share that my team's senior project at WPI has been accepted for publication in IC2E 2021! We studied how to efficiently load training data from S3-like buckets for distributed deep learning!\n\n<LINK>"", 'Thank you for everyone who made this possible, including my partner in crime, Matt; our advisor, @gtbelinda; and everyone at @wpicakelab for their help and support along the way :)']",https://arxiv.org/abs/2108.06322,"Cloud computing provides a powerful yet low-cost environment for distributed deep learning workloads. However, training complex deep learning models often requires accessing large amounts of data, which can easily exceed the capacity of local disks. Prior research often overlooks this training data problem by implicitly assuming that data is available locally or via low latency network-based data storage. Such implicit assumptions often do not hold in a cloud-based training environment, where deep learning practitioners create and tear down dedicated GPU clusters on demand, or do not have the luxury of local storage, such as in serverless workloads. In this work, we investigate the performance of distributed training that leverages training data residing entirely inside cloud storage buckets. These buckets promise low storage costs, but come with inherent bandwidth limitations that make them seem unsuitable for an efficient training solution. To account for these bandwidth limitations, we propose the use of two classical techniques, namely caching and pre-fetching, to mitigate the training performance degradation. We implement a prototype, DELI, based on the popular deep learning framework PyTorch by building on its data loading abstractions. We then evaluate the training performance of two deep learning workloads using Google Cloud's NVIDIA K80 GPU servers and show that we can reduce the time that the training loop is waiting for data by 85.6%-93.5% compared to loading directly from a storage bucket - thus achieving comparable performance to loading data directly from disk - while only storing a fraction of the data locally at a time. In addition, DELI has the potential of lowering the cost of running a training workload, especially on models with long per-epoch training times. ","Quantifying and Improving Performance of Distributed Deep Learning with
  Cloud Storage"
145,1426795987029397507,899323808494039040,Yochay Jerby,"[""The Lambert W-function named in 1990's. Used by Franca-LeClair in 2013 to give approximation t^1_n for zeros of zeta. We generalize to t^N_n and study dynamics w.r.t N. \n<LINK> <LINK>""]",https://arxiv.org/abs/2108.03716,"For $N \in \mathbb{N}$ consider the $N$-th section of the approximate functional equation $$ \zeta_N(s)= \sum_{n =1 }^N B_n(s),$$ where $$ B_n(s)= \frac{1}{2} \left [ n^{-s} + \chi(s) \cdot n^{s-1} \right ].$$ Our aim in this work is to introduce a new approach for the Riemann hypothesis by studying the way pairs of consecutive zeros of $\zeta_N(s)$ change with respect to $N$. For the initial stage, it is known that the non-trivial zeros of $\zeta_1(s)$ all lie on the critical line $Re(s)=\frac{1}{2}$. In the region $2N \leq Im(s) \leq 2 \pi (N+1)$ the function $\zeta_N(s)$ serves as an approximation of $\zeta(s)$ itself, and it was conjectured by Spira that in this region $\zeta_N(s)$ also admits zeros only on the critical line. We show that the appearance of zeros of a section off the critical line can be realized as the result of two consecutive zeros meeting and pushing each other off the critical line as $N$ changes, a process to which we refer to as a collision of zeros. Based on a study of the properties of $\zeta_N(s)$, we suggest a way of re-arranging the order of summation of the elements $B_n(s)$ in $\zeta_{N}(s)$ with $N=\left [ \frac{Im(s)}{2} \right ]$ that is expected to avoid collisions altogether, we refer to such a re-arrangement as a repelling re-arrangement. In particular, establishing that the suggested repelling re-arrangement indeed avoids collisions for any pair of zeros would imply RH. ","A dynamic approach for the zeros of the Riemann zeta function -
  collision and repulsion"
146,1426453540709355520,1310552063999438849,Hauke Group,"['üë®\u200düè´In our study, we reveal a universality in the equilibration dynamics of the Sachdev-Ye-Kitaev model by employing state-of-the-art numerical methods for disorder averaged evolution.\nRead the full article <LINK>\n@ERC_Research \n@HaukeGroup and Alessio Paviglianiti <LINK>']",https://arxiv.org/abs/2108.01718,"Equilibrium quantum many-body systems in the vicinity of phase transitions generically manifest universality. In contrast, limited knowledge has been gained on possible universal characteristics in the non-equilibrium evolution of systems in quantum critical phases. In this context, universality is generically attributed to the insensitivity of observables to the microscopic system parameters and initial conditions. Here, we present such a universal feature in the equilibration dynamics of the Sachdev-Ye-Kitaev (SYK) Hamiltonian -- a paradigmatic system of disordered, all-to-all interacting fermions that has been designed as a phenomenological description of quantum critical regions. We drive the system far away from equilibrium by performing a global quench, and track how its ensemble average relaxes to a steady state. Employing state-of-the-art numerical simulations for the exact evolution, we reveal that the disorder-averaged evolution of few-body observables, including the quantum Fisher information and low-order moments of local operators, exhibit within numerical resolution a universal equilibration process. Under a straightforward rescaling, data that correspond to different initial states collapse onto a universal curve, which can be well approximated by a Gaussian throughout large parts of the evolution. To reveal the physics behind this process, we formulate a general theoretical framework based on the Novikov--Furutsu theorem. This framework extracts the disorder-averaged dynamics of a many-body system as an effective dissipative evolution, and can have applications beyond this work. The exact non-Markovian evolution of the SYK ensemble is very well captured by Bourret--Markov approximations, which contrary to common lore become justified thanks to the extreme chaoticity of the system, and universality is revealed in a spectral analysis of the corresponding Liouvillian. ",Universal equilibration dynamics of the Sachdev-Ye-Kitaev model
147,1426155381617799168,1066288106,Fabian Dablander,"['Are you into puzzles? Then you might enjoy our new preprint!\n\nWe compare two Bayesian tests for the equality of two proportions and find that they can yield dramatically different conclusions üßµ\n\nLink: <LINK>\n@khuth6 @AlxEtz @EJWagenmakers <LINK>', 'Before we compare the two approaches, take a moment to test your intuitions!\n\nSuppose you observe two data sets, d‚ÇÅ = (y‚ÇÅ, n‚ÇÅ) &amp; d‚ÇÇ = (y‚ÇÇ, n‚ÇÇ). Is observing d‚ÇÅ = (0, 100) and d‚ÇÇ = (0, 100) stronger or weaker evidence for equality than d‚ÇÅ = (50, 100) and d‚ÇÇ = (50, 100)?', 'The ""Independent Beta"" (IB) approach assigns --- you guessed it! --- independent beta priors to ùúÉ‚ÇÅ and ùúÉ‚ÇÇ (left).\n\nThe ""Logit Transformation"" (LT) approach reparameterizes the problem and assigns Gaussian priors to the average log odds Œ≤ and the log odds ratio ‚≤Ø (right). https://t.co/JYC12KQqIg', ""The figures above show that the two tests make quite different assumptions. If you look at the marginal priors*, you see that you can play with the prior parameters to make them more similar to each other. (But note that it's the joint distribution that counts!)\n\n*Œ∑ = ùúÉ‚ÇÇ - ùúÉ‚ÇÅ https://t.co/cbrgquQ2kz"", 'Returning to our (0, 100) &amp; (0, 100) vs (50, 100) &amp; (50, 100) puzzle, we find that the IB approach yields Bayes factors in favour of H‚ÇÄ of 50.80 and 5.70, respectively.\n\nThe LT approach, on the other hand, yields Bayes factors of 1.40 and 3.70, respectively!', 'We also reanalyze 39 null results from the New England Journal of Medicine (top left) and find that the two approaches can differ strongly in practice. @RinkHoekstra @MondenRei @DonVanRaven https://t.co/Nuk09LscZW', 'What explains these big differences at the extremes?\n\nBecause the IB test assigns priors directly to ùúÉ‚ÇÅ and ùúÉ‚ÇÇ, it fumbles at the boundaries: it expects ùëôùëéùëüùëîùëíùëü differences compared to the center. The reverse holds for the LT test. See https://t.co/OMqXc9Lnnt for details https://t.co/M0cWTCAaqv', 'The IB test is widely used, also in the context of contingency tables. The LT test is less widely used, but it is arguably more suited for testing.\n\nThe expectation of increasingly larger differences towards the extremes is problematic, as is the assumption of prior independence.', 'One takeaway is to prefer the LT test over the IB test when testing the equality of two proportions. Luckily, easy to use software exists, both in R and @JASPStats.\n\nLink 1: https://t.co/OIm58TTOkE\nLink 2: https://t.co/cErWDOtpoG @hoffmann_tabea', ""This also stresses the importance of assessing the predictions of one's models and that one should never rely on just a single quantity.\n\nIf puzzles can happen in such a simple case, they are bound to occur in more complex scenarios. Best to come prepared. https://t.co/OMqXc9Lnnt"", '@rabaath Nope! We talk about this a bit in the paper, see Appendix C attached. https://t.co/MY1MI7rRZS']",https://arxiv.org/abs/2108.04909,"Testing the equality of two proportions is a common procedure in science, especially in medicine and public health. In these domains it is crucial to be able to quantify evidence for the absence of a treatment effect. Bayesian hypothesis testing by means of the Bayes factor provides one avenue to do so, requiring the specification of prior distributions for parameters. The most popular analysis approach views the comparison of proportions from a contingency table perspective, assigning prior distributions directly to the two proportions. Another, less popular approach views the problem from a logistic regression perspective, assigning prior distributions to logit-transformed parameters. Reanalyzing 39 null results from the New England Journal of Medicine with both approaches, we find that they can lead to markedly different conclusions, especially when the observed proportions are at the extremes (i.e., very low or very high). We explain these stark differences and provide recommendations for researchers interested in testing the equality of two proportions and users of Bayes factors more generally. The test that assigns prior distributions to logit-transformed parameters creates prior dependence between the two proportions and yields weaker evidence when the observations are at the extremes. When comparing two proportions, we argue that this test should become the new default. ","A Puzzle of Proportions: Two Popular Bayesian Tests Can Yield
  Dramatically Different Conclusions"
148,1426119061671276549,15068044,Daniel Beck,"[""So we have a new paper, spearheaded by Joe Han, who just finished his Masters under mine and @trevorcohn's supervision.\n\nIf you're looking into new ways of evaluating diversity in generation you should definitely take a look on what we propose here. (1/n)\n<LINK>"", ""@trevorcohn This will appear at INLG 2021.\n\nWe propose a new way to evaluate diversity by grounding it on quality. Our rationale is that a diverse set of generated sentences is only good if it's also of good quality.\n\nHow to evaluate both jointly? Use multiple references. (2/n)"", '@trevorcohn We assume the ""gold standard"" of diversity is reflected in multiple references. So the goal of diversity is to ""cover"" the reference set. Assuming a sentence-level metric, we turn this into a maximum matching problem. (3/n)', '@trevorcohn This approach has a number of perks. For instance, a perfect model that generates the reference set exactly will achieve maximum score. But a model that generates N good sentences that are just slight variations of each other will be penalised. (4/n)', 'It is also completely agnostic of whatever sentence-level metric is used, as long as it is bounded. So this can be used for a range of generation tasks. (5/n)', ""One limitation is the reliance on the reference set: if it's not diverse enough then the method will not give preference to more diverse models, even if they give good quality sentences. (6/n)"", ""We also don't really test this against an implicit human evaluation of diversity (as quality metrics do). This is something we certainly would love to do in the future. However, it's not clear to us how to even define diversity intrinsically... (7/n)"", 'This is why we focus on the reference set. Our intuition is that it gives an ""extrinsic"" measure of diversity. It is not without its drawbacks (as I mention above) but it give us more information compared to other diversity metrics we know about. (8/n)', 'Anyways, check our paper for more info. Happy to discuss more about diversity in generation if you have ideas =). (9/9)', 'PS: yes, we know we messed up the references in the current arXiv version... we will update it shortly... =S']",https://arxiv.org/abs/2108.05659,"Text generation from semantic graphs is traditionally performed with deterministic methods, which generate a unique description given an input graph. However, the generation problem admits a range of acceptable textual outputs, exhibiting lexical, syntactic and semantic variation. To address this disconnect, we present two main contributions. First, we propose a stochastic graph-to-text model, incorporating a latent variable in an encoder-decoder model, and its use in an ensemble. Second, to assess the diversity of the generated sentences, we propose a new automatic evaluation metric which jointly evaluates output diversity and quality in a multi-reference setting. We evaluate the models on WebNLG datasets in English and Russian, and show an ensemble of stochastic models produces diverse sets of generated sentences, while retaining similar quality to state-of-the-art models. ",Generating Diverse Descriptions from Semantic Graphs
149,1426012707338145797,3948752537,‚Ñôierre ùî∏lquier üá∫üá¶,"['With The Tien Mai, we derive elementary properties of ""Population Structure Correction"" Regression, and propose a simple trick to reduce its variance.\n\n<LINK>']",https://arxiv.org/abs/2108.05655,"Although genome-wide association studies (GWAS) on complex traits have achieved great successes, the current leading GWAS approaches simply perform to test each genotype-phenotype association separately for each genetic variant. Curiously, the statistical properties for using these approaches is not known when a joint model for the whole genetic variants is considered. Here we advance in GWAS in understanding the statistical properties of the ""population structure correction"" (PSC) approach, a standard univariate approach in GWAS. We further propose and analyse a correction to the PSC approach, termed as ""corrected population correction"" (CPC). Together with the theoretical results, numerical simulations show that CPC is always comparable or better than PSC, with a dramatic improvement in some special cases. ",Understanding the population structure correction regression
150,1425355170901266434,578977577,Javier Arg√ºello Luengo,"['Now online, the latest work with Darrick!\n\nWe show that a photon scattered by a single-atom in a cavity can highly entangle with the subsequent atomic motion We observe a much larger heating than expected in free space and propose experimental signaturesüëá\n<LINK>', 'Focusing on current platforms, we derive an optimal strong-coupling configuration where the probability that a photon entering the cavity gets reflected highly depends on the position of the atom inside the cavity. https://t.co/5QCEZgOTAJ', 'Therefore, if we see that a photon gets reflected, we gain information about where the atom is (or is not) in that moment. \n\nThe ‚Äúconditioned‚Äù atomic wavefunction changes accordingly: a ‚Äúhole‚Äù appears in the positions where it was unlikely to have been (shaded in blue). https://t.co/0uYOKjLWm2', 'A similar change also occurs when the photon is not reflected. As a consequence, even if we were not actively monitoring where the photon ends up, a trapped atom inside the cavity heats up as its wavefunction entangles with the scattered photons. https://t.co/5a8YXqW2a1', 'Interestingly, we see that this heating mechanism can be much larger than the one caused by a single photon in free-space. \n\nIn particular, heating gets enhanced by the number of photon round-trips inside the cavity (the cooperativity). https://t.co/STLf0oud1Q', 'But what if we were actually monitoring the cavity and detected a reflected photon? \n\nFor weak driving, the resulting ‚Äúholed‚Äù atomic wavefunction would start oscillating in the trap before a next photon arrives. https://t.co/ZOST7xWEDU', 'As the atom explores different regions of the cavity, the probability that a second photon reflects will change accordingly: https://t.co/VXkNX1BX47', 'This can then be captured by oscillations of second-order correlations in time g(t), showing a periodicity dictated by the trap.\n\nWhen the atom populates positions compatible with reflection (white) one measures g(t)&gt;1 (bunching), in the opposite case (shaded region), g(t)&lt;1. https://t.co/PmLVCjx0IH', 'For this analysis we have developed a minimal scattering matrix formalism that capture the leading features of the problem, derived an optimal configuration and showed that these effects can be observed in realistic systems, even for a non-zero initial motional temperature. https://t.co/6TaQInmEKR', 'As always, feel very welcome to send us any comment or feedback, and special thanks to our colleagues and funding agencies \n@ERC_Research @CienciaGob @CaixaResearch @ICFOnians']",https://arxiv.org/abs/2108.03526,"Single atoms coupled to a cavity offer unique opportunities as quantum optomechanical devices because of their small mass and strong interaction with light. A particular regime of interest in optomechanics is that of ""single-photon strong coupling,"" where motional displacements on the order of the zero-point uncertainty are sufficient to shift the cavity resonance frequency by more than its linewidth. In many cavity QED platforms, however, this is unfeasible due to the large cavity linewidth. Here, we propose an alternative route in such systems, which instead relies on the coupling of atomic motion to the much narrower cavity-dressed atomic resonance frequency. We discuss and optimize the conditions in which the scattering properties of single photons from the atom-cavity system become highly entangled with the atomic motional wave function. We also analyze the prominent observable features of this optomechanical strong coupling, which include a per-photon motional heating that is significantly larger than the single-photon recoil energy, as well as mechanically-induced oscillations in time of the second-order correlation function of the emitted light. This physics should be realizable in current experimental setups, such as trapped atoms coupled to photonic crystal cavities, and more broadly opens the door to realizing qualitatively different phenomena beyond what has been observed in optomechanical systems thus far. ","Optomechanical strong coupling between a single cavity photon and a
  single atom"
151,1425143860196515846,2891714420,Eduardo Ibarra-Garcia-Padilla,"['What do systems with a wide variety of ground states have in common at finite temperature? Come read our preprint <LINK>! We find a universal scaling in the SU(N) Fermi Hubbard Model. In collaboration with @sohaildasgupta,  @QuantumHazzard and the Kyoto group! <LINK>']",https://arxiv.org/abs/2108.04153,"The SU(2) symmetric Fermi-Hubbard model (FHM) plays an essential role in strongly correlated fermionic many-body systems. In the one particle per site and strongly interacting limit ${U/t \gg 1}$, it is effectively described by the Heisenberg Hamiltonian. In this limit, enlarging the spin and extending the typical SU(2) symmetry to SU($N$) has been predicted to give exotic phases of matter in the ground state, with a complicated dependence on $N$. This raises the question of what -- if any -- are the finite-temperature signatures of these phases, especially in the currently experimentally relevant regime near or above the superexchange energy. We explore this question for thermodynamic observables by numerically calculating the thermodynamics of the SU($N$) FHM in the two-dimensional square lattice near densities of one particle per site, using determinant Quantum Monte Carlo and Numerical Linked Cluster Expansion. Interestingly, we find that for temperatures above the superexchange energy, where the correlation length is short, the energy, number of on-site pairs, and kinetic energy are universal functions of $N$. Although the physics in the regime studied is well beyond what can be captured by low-order high-temperature series, we show that an analytic description of the scaling is possible in terms of only one- and two-site calculations. ",Universal thermodynamics of an SU($N$) Fermi-Hubbard Model
152,1425054995334971394,1345856121660178433,Annabelle Bohrdt,"['In my PhD, I spent a lot of time figuring out what a single hole üï≥ in a quantum antiferromagnet ‚¨áÔ∏è‚¨ÜÔ∏è does. Now, as a postdoc, I‚Äôve moved on to 2 holes üï≥ üï≥. We introduce a general pairing mechanism and find super high binding energies ‚Äî check it out: <LINK> <LINK>']",https://arxiv.org/abs/2108.04118,"Interacting many-body systems combining confined and extended dimensions, such as ladders and few layer systems are characterized by enhanced quantum fluctuations, which often result in interesting collective properties. Recently two-dimensional bilayer systems, such as twisted bilayer graphene or ultracold atoms, have sparked a lot of interest because they can host rich phase diagrams, including unconventional superconductivity. Here we present a theoretical proposal for realizing high temperature pairing of fermions in a class of bilayer Hubbard models. We introduce a general, highly efficient pairing mechanism for mobile dopants in antiferromagnetic Mott insulators, which leads to binding energies proportional to $t^{1/3}$, where $t$ is the hopping amplitude of the charge carriers. The pairing is caused by the energy that one charge gains when retracing a string of frustrated bonds created by another charge. Concretely, we show that this mechanism leads to the formation of highly mobile, but tightly bound pairs in the case of mixed-dimensional Fermi-Hubbard bilayer systems. This setting is closely related to the Fermi-Hubbard model believed to capture the physics of copper oxides, and can be realized by currently available ultracold atom experiments. ","Strong pairing in mixed dimensional bilayer antiferromagnetic Mott
  insulators"
153,1425006628231864331,945445796098473984,Patrick Schnider,"['A new paper on the @arxiv . In ‚ÄûTopological Art in Simple Galleries‚Äú, together with Daniel Bertschinger, Nicolas El Maalouly, Till Miltzow and Simon Weber, we study the space of optimal guard placements in the Art Gallery Problem.\n\n<LINK>\n\n1/6', 'See this link for more info on the Art Gallery Problem:\n\nhttps://t.co/TRE5OOhiGo\n\n2/6', 'We show a universality theorem similar to Mn√´vs universality theorem for order types. Specifically we show that for every semi-algebraic set S, there exists a polygon for which the space of optimal guard placements is homotopy-equivalent to S.\n\n3/6', 'We further give some small polygons for which the space of optimal guard placements are homeomorphic to non-trivial spaces such as spheres, torus or double torus.\n\n4/6', 'The paper is accompanied by a video with some animations, made by my coauthor Simon Weber.\n\nhttps://t.co/J1FWh66Qhh\n\n5/6', 'The animations were made using @geogebra , and Simon has also made an applet where you can play around with guard placements yourself.\n\nSphere:\n\nhttps://t.co/7CC8KOTeSU\n\nDouble Torus:\n\nhttps://t.co/OXqdcUCth3\n\n6/6']",https://arxiv.org/abs/2108.04007,"Let $P$ be a simple polygon, then the art gallery problem is looking for a minimum set of points (guards) that can see every point in $P$. We say two points $a,b\in P$ can see each other if the line segment $seg(a,b)$ is contained in $P$. We denote by $V(P)$ the family of all minimum guard placements. The Hausdorff distance makes $V(P)$ a metric space and thus a topological space. We show homotopy-universality, that is for every semi-algebraic set $S$ there is a polygon $P$ such that $V(P)$ is homotopy equivalent to $S$. Furthermore, for various concrete topological spaces $T$, we describe instances $I$ of the art gallery problem such that $V(I)$ is homeomorphic to $T$. ",Topological Art in Simple Galleries
154,1424992052199186448,1164202716,Magnus Jonsson,"['Happy to present our study by Karki et al. in which we demonstrate electrical tuning of plasmonic conducting polymer nanoantennas. The polymeric nanoantennas provide complete and reversible on/off switching as well as possibility for gradual tuning. <LINK>', 'We utilized the possibility to tune the charge carrier density of conducting polymers by orders of magnitude via their redox state, and could thereby switch the material of the nanoantennas between metallic/plasmonic and dielectric.', 'The system provides convenient electrical control of the nanoantennas, building on our previous demonstration of chemical switching of conducting polymer nanoantennas. #PEDOT #Plasmonics #Metasurfaces\nhttps://t.co/0MVp22eSE6']",https://arxiv.org/abs/2108.04045,"Nanostructures of conventional metals offer manipulation of light at the nanoscale but are limited to static behavior due to their fixed material properties. To develop the next frontier of dynamic nanooptics and metasurfaces, we utilize the redox-tunable optical properties of conducting polymers, which were recently shown to be capable of sustaining plasmons in their most conducting oxidized state. Using nanodisks of poly(3,4-ethylenedioxythiophene:sulfate) (PEDOT:Sulf) as a model system, we present the first electrically tunable conducting polymer nanooptical antennas. In addition to repeated on/off switching of the polymeric nanoantennas, we demonstrate the possibility for gradual electrical tuning of their nanooptical response, which was found to be related to the modulation of both density and mobility of the mobile polaronic charge carriers in the polymer. The presented concept takes important steps towards electrically tunable metasurfaces with truly dynamic optical nanoantenna pixels, with not only varying farfield but also tunable nearfield. The work paves the way for applications ranging from tunable flat metaoptics to adaptable smart windows. ",Electrical Tuning of Plasmonic Conducting Polymer Nanoantennas
155,1424914026203549699,379818529,Enrico Rinaldi,"['We benchmarked emerging scientific tools, such as machine learning and quantum computing, applied to the study of matrix models, which are relevant for understanding quantum black holes and a quantum theory of gravity <LINK>']",https://arxiv.org/abs/2108.02942,"Matrix quantum mechanics plays various important roles in theoretical physics, such as a holographic description of quantum black holes. Understanding quantum black holes and the role of entanglement in a holographic setup is of paramount importance for the development of better quantum algorithms (quantum error correction codes) and for the realization of a quantum theory of gravity. Quantum computing and deep learning offer us potentially useful approaches to study the dynamics of matrix quantum mechanics. In this paper we perform a systematic survey for quantum computing and deep learning approaches to matrix quantum mechanics, comparing them to Lattice Monte Carlo simulations. In particular, we test the performance of each method by calculating the low-energy spectrum. ","Matrix Model simulations using Quantum Computing, Deep Learning, and
  Lattice Monte Carlo"
156,1424525527059107842,2583457938,Hao Wang,"['Here, we propose a network compression and whole graph embedding by integrating a generalized DHC theorem and the Shannon entropy (E), abbreviated as DHC-E. \n<LINK>', 'The new methodology is overall simple, hyperparameter-free, extensible, and explainable for whole graph embedding with promising potential for exploring graph classification, prediction, and lower-dimensional graph visualization.', 'DHC iterations gradually and automatically converge to coreness, the information they correspond to is gradually reduced. Our DHC-E can be regarded as an adaptive and self-converging system that depicts the information of a complex system from a fine-grained to the coarse scale.']",https://arxiv.org/abs/2108.02113,"Graphs can be used to describe complex systems. Recently, whole graph embedding (graph representation learning) can compress a graph into a compact lower-dimension vector while preserving intrinsic properties, earning much attention. However, most graph embedding methods have problems such as tedious parameter tuning or poor explanation. This paper presents a simple and hyperparameter-free whole graph embedding method based on the DHC (Degree, H-index, and Coreness) theorem and Shannon Entropy (E), abbreviated as DHC-E. The DHC-E can provide a trade-off between simplicity and quality for supervised classification learning tasks involving molecular, social, and brain networks. Moreover, it performs well in lower-dimensional graph visualization. Overall, the DHC-E is simple, hyperparameter-free, and explainable for whole graph embedding with promising potential for exploring graph classification and lower-dimensional graph visualization. ",Hyperparameter-free and Explainable Whole Graph Embedding
157,1424325432028155909,2794056066,Matthias Grundmann,"['Since about a month, many invalid IP addresses are distributed in the #Bitcoin P2P network. We found that this can be used to estimate the number of neighbors of public Bitcoin Core peers and to match multiple addresses to the same peer. Find our report at <LINK>']",https://arxiv.org/abs/2108.00815,"A recent spam wave of IP addresses in the Bitcoin P2P network allowed us to estimate the degree distribution of reachable peers in the network. The resulting distribution shows that about every second reachable peer runs with Bitcoin Core's default setting of a maximum of 125 concurrent connections and nearly all connection slots are taken. We validate this result and, in addition, use our observations of the spam wave to group addresses that belong to the same peer. By doing this grouping, we improve on previous measurements and show that simply counting addresses overestimates the number of reachable peers by 13 %. ",Estimating the Peer Degree of Reachable Peers in the Bitcoin P2P Network
158,1423670629383933959,977906884886827008,Marcos Mari√±o,"['In my last paper with friends at @Sissaschool and @UniTrieste (<LINK>) we study the interplay between resurgence and the 1/N expansion in integrable quantum field theories in 2d. We find that, in certain cases, resurgence is undermined by the large N limit!', 'This might be an indication that the strong version of resurgence is not valid in theories with renormalons, or it might be an effect of truncating the perturbative expansion to a fixed order in 1/N', 'On the positive side, we find a new, explicit example of resurgence in the large N limit of the principal chiral field, which involves an infinite sequence of IR renormalons whose trans-series can be computed analytically. Quite beautiful.', 'In retrospect, this tension between resurgence and the 1/N expansion can be seen to be present in older calculations in solvable models, but it was not pointed out explicitly']",https://arxiv.org/abs/2108.02647,"In theories with renormalons the perturbative series is factorially divergent even after restricting to a given order in $1/N$, making the $1/N$ expansion a natural testing ground for the theory of resurgence. We study in detail the interplay between resurgent properties and the $1/N$ expansion in various integrable field theories with renormalons. We focus on the free energy in the presence of a chemical potential coupled to a conserved charge, which can be computed exactly with the thermodynamic Bethe ansatz (TBA). In some examples, like the first $1/N$ correction to the free energy in the non-linear sigma model, the terms in the $1/N$ expansion can be fully decoded in terms of a resurgent trans-series in the coupling constant. In the principal chiral field we find a new, explicit solution for the large $N$ free energy which can be written as the median resummation of a trans-series with infinitely many, analytically computable IR renormalon corrections. However, in other examples, like the Gross-Neveu model, each term in the $1/N$ expansion includes non-perturbative corrections which can not be predicted by a resurgent analysis of the corresponding perturbative series. We also study the properties of the series in $1/N$. In the Gross-Neveu model, where this is convergent, we analytically continue the series beyond its radius of convergence and show how the continuation matches with known dualities with sine-Gordon theories. ",Resurgence and $1/N$ Expansion in Integrable Field Theories
159,1423515535506083843,1233836016399679488,Said R. K. Rodriguez,"['Check out our new preprint ""Limit Cycles and Chaos Induced by a Nonlinearity with Memory"" , with  @KJH_Peters\n\nWe also consider implications for nonlinear energy harvesting. Surprisingly, we find a strong freq. peak can be better than broadband responseüòâ\n\n<LINK>']",https://arxiv.org/abs/2108.02680,"Inspired by the observation of a distributed time delay in the nonlinear response of an optical resonator, we investigate the effects of a similar delay on a noise-driven mechanical oscillator. For a delay time that is commensurate with the inverse dissipation rate, we find stable limit cycles. For longer delays, we discover a regime of chaotic dynamics associated with a double scroll attractor. We also analyze the effects of time delay on the spectrum and oscillation amplitude of the oscillator. Our results point to new opportunities for nonlinear energy harvesting, provided that a nonlinearity with distributed time delay can be implemented in mechanical systems. ",Limit Cycles and Chaos Induced by a Nonlinearity with Memory
160,1423120273746763781,807327556072402945,Machel Reid,"['Welcome to PARADISE! We propose a new, more efficient method for multilingual sequence to sequence pre-training by leveraging smaller corpora of word/sentence parallel data for improved cross-lingual Seq2Seq pre-training.\n\nw/ @artetxem \n\nüìÉ <LINK>\nüßµ (1/) <LINK>', 'Recent pre-trained seq2seq models papers (e.g. mT5) have shown that scaling up can be effective for cross-lingual transfer. However, can we leverage smaller monolingual corpora + both word and sentence parallel data to improve this in a more data/compute efficient way?\n\n2/', 'We introduce two noising objectives: dictionary noising (cross-lingual word-level denoising) and bitext noising to integrate multilingual dictionaries (constructed by combining multiple XX-En bilingual dictionaries) and sentence-level parallel data: \n\n3/ https://t.co/wnZGkosvha', 'We compare with an mBART model trained in the same settings/model size/monolingual data and find that our objectives (with dictionary noising being pertinent) help on both machine translation and cross-lingual classification experiments:\n\n4/ https://t.co/laEq8Wp1c3', 'We also test how our model compares to previous work on multilingual pre-training on cross-lingual classification experiments and find that our model is much more data and compute efficient than many baselines:\n\n5/ https://t.co/wDeFfi6TtV', 'Personally, I think dictionary noising is really promising because it shows that this technique may hold promise for extremely low resource languages and some small amount of parallel data and word-level alignments.\n\n5/', 'The code and models will be released at https://t.co/wypZ8PHHsM soon üòâ\n\n6/', 'Finally, thank you to @JunjieHu12, @wittgen_ball, and @hllo_wrld for their useful feedback and comments!\n\nIf anyone has any feedback, comments or ideas feel free to reach out!\n\n7/7']",https://arxiv.org/abs/2108.01887,"Despite the success of multilingual sequence-to-sequence pretraining, most existing approaches rely on monolingual corpora, and do not make use of the strong cross-lingual signal contained in parallel data. In this paper, we present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence models), which extends the conventional denoising objective used to train these models by (i) replacing words in the noised sequence according to a multilingual dictionary, and (ii) predicting the reference translation according to a parallel corpus instead of recovering the original sequence. Our experiments on machine translation and cross-lingual natural language inference show an average improvement of 2.0 BLEU points and 6.7 accuracy points from integrating parallel data into pretraining, respectively, obtaining results that are competitive with several popular models at a fraction of their computational cost. ","PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence
  Pretraining"
161,1431182446645714947,1195653141934542848,Itay Itzhak,"[""If LMs treat subword tokens as atomic units, they must be oblivious to each token's spelling, right?\n\nWe probe the embedding layer of pretrained LMs, and find that they do, in fact, know quite a bit about each token's spelling.\n\n<LINK>\n\nwith @omerlevy_""]",https://arxiv.org/abs/2108.11193,"Standard pretrained language models operate on sequences of subword tokens without direct access to the characters that compose each token's string representation. We probe the embedding layer of pretrained language models and show that models learn the internal character composition of whole word and subword tokens to a surprising extent, without ever seeing the characters coupled with the tokens. Our results show that the embedding layer of RoBERTa holds enough information to accurately spell up to a third of the vocabulary and reach high average character ngram overlap on all token types. We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment. Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not enhance its performance on such tasks. ","Models In a Spelling Bee: Language Models Implicitly Learn the Character
  Composition of Tokens"
162,1428016832997429252,560473379,nick frosst,"['Excited to share a @CohereAI  preprint on detoxifying language models!\n\nWe use a language model to find hateful text by calculating the conditional likelihood of curated trigger phrases and filtering the training set! This results in nicer models :)\n\n<LINK> <LINK>', 'Specifically, we calculate p(trigger phrase | document) for each trigger phrase and each document in our corpus. And we filter the data set based on this metric. \n\nWe show that this metric is able to remove hateful text while minimizing the removal of counterspeech. https://t.co/azTI5O4Y3b', 'This method allows us to write trigger phrases that are representative of aspects of language that we don‚Äôt think are useful for generative language modelling, such as racism or conspiratorial thinking and remove related documents. https://t.co/BEfxlQJUdk', 'This method is highly configurable and can be updated by simply writing new trigger phrases and refiltering. This makes it a practical tool for improving text corpora collected from the open web', 'We think this is a step towards solving a few issues with blocklist filtration which can erroneously flag non-hateful webpages that use blocklisted words in academic or expository contexts and disproportionately filter out text associated with minority identities.', 'We show training and finetuning a model on this filtered dataset results in models that are less likely to generate toxic text (as measured by humans and the Prospective API) https://t.co/NKtULU7YZe', 'We also show although unfiltered models perform best on standard NLP benchmarks, conditional likelihood-filtering outperforms blocklist filtering, resulting in a smaller decrease in performance on LM1B and LAMBADA https://t.co/ZM3Amg0WMl', 'This prompted us to look into the toxicity of these benchmarks and we find that they contain examples of hate speech. Such text is often used in an explanatory context, but including it in the dataset inadvertently incentives language models to generate harmful content.', 'We are really excited about this work as we believe it is a practical step towards solving some of the issues in large scale-language models trained on the open web. Read the paper for the full details! :)  \n\nhttps://t.co/Jl58TUoOi9', 'If you are interested in this kind of stuff and want to make large language models useful and accessible, join our team!  \n\nhttps://t.co/3xTjWmQVMO', ""big shoutout to @mathemakitten @cooper_raterink @_joaogui1 @1vnzh @kipperrii @kaledivergence  for their hard work on this and @timhwang @alienelf @metaviv @AidanNGomez for their help! \n\nI am really proud of the team here at @CohereAI , it's been an honor to work with them on this"", '@vote_no_body @mathemakitten @cooper_raterink @_joaogui1 @1vnzh @kipperrii @kaledivergence @timhwang @alienelf @metaviv @AidanNGomez @CohereAI They are in the appendix! But the method can be extended by writing new trigger phrases. It‚Äôs one of the things that we think makes it a practical approach to detoxifying models!']",https://arxiv.org/abs/2108.07790,"Language models trained on large-scale unfiltered datasets curated from the open web acquire systemic biases, prejudices, and harmful views from their training data. We present a methodology for programmatically identifying and removing harmful text from web-scale datasets. A pretrained language model is used to calculate the log-likelihood of researcher-written trigger phrases conditioned on a specific document, which is used to identify and filter documents from the dataset. We demonstrate that models trained on this filtered dataset exhibit lower propensity to generate harmful text, with a marginal decrease in performance on standard language modeling benchmarks compared to unfiltered baselines. We provide a partial explanation for this performance gap by surfacing examples of hate speech and other undesirable content from standard language modeling benchmarks. Finally, we discuss the generalization of this method and how trigger phrases which reflect specific values can be used by researchers to build language models which are more closely aligned with their values. ","Mitigating harm in language models with conditional-likelihood
  filtration"
163,1425216411505364994,843929270,Dr. Charan Ranganath,"[""New preprint &amp; my first paper w/Randy O'Reilly. We discuss the computational benefits of differentiating b/w  content and structure in a variety of domains. We propose that event structure may be represented by the PM Network via predictive learning  <LINK>""]",https://arxiv.org/abs/2108.03387,"A hallmark of human intelligence is the ability to adapt to new situations, by applying learned rules to new content (systematicity) and thereby enabling an open-ended number of inferences and actions (generativity). Here, we propose that the human brain accomplishes these feats through pathways in the parietal cortex that encode the abstract structure of space, events, and tasks, and pathways in the temporal cortex that encode information about specific people, places, and things (content). Recent neural network models show how the separation of structure and content might emerge through a combination of architectural biases and learning, and these networks show dramatic improvements in the ability to capture systematic, generative behavior. We close by considering how the hippocampal formation may form integrative memories that enable rapid learning of new structure and content representations. ",The Structure of Systematicity in the Brain
164,1422958089184190465,141822756,Enzo Ferrante,"['üö®‚ÄúDomain generalization via gradient surgery‚Äù‚úÇÔ∏èwas accepted at @iccv2021 üéâ\n\nWe study conflicting gradients emerging in domain shift\nscenarios and devise novel gradient agreement strategies to improve generalization performance on unseen domains\n\nPaper‚û°Ô∏è <LINK> <LINK>', 'We first show some experimental results supporting our hypothesis that gradients corresponding to batches coming from the same domain (intra-domain) tend to agree more than inter-domain gradients. https://t.co/tcmhQU9K8M', 'Then, we explore different surgery strategies (Agr-Sum, Agr-Rand and PCGrad) to encourage agreement between the gradients before updating the NN weights during SGD in the context of image classification.\n\nCheck the details in the paper!\n\nhttps://t.co/wDuM5fugJt', 'Kudos to @lmansilla09 for leading this work!\n\nIn collaboration with @d1001 @RSEcheveste from @sinc_i \n\n100% made in üá¶üá∑üòÑ --&gt; Gracias @CONICETDialoga , @NVIDIAAI , @UNLitoral y @agenciaidiar por el apoyo!\n\nhttps://t.co/fHKJaEZ0Q2']",https://arxiv.org/abs/2108.01621,"In real-life applications, machine learning models often face scenarios where there is a change in data distribution between training and test domains. When the aim is to make predictions on distributions different from those seen at training, we incur in a domain generalization problem. Methods to address this issue learn a model using data from multiple source domains, and then apply this model to the unseen target domain. Our hypothesis is that when training with multiple domains, conflicting gradients within each mini-batch contain information specific to the individual domains which is irrelevant to the others, including the test domain. If left untouched, such disagreement may degrade generalization performance. In this work, we characterize the conflicting gradients emerging in domain shift scenarios and devise novel gradient agreement strategies based on gradient surgery to alleviate their effect. We validate our approach in image classification tasks with three multi-domain datasets, showing the value of the proposed agreement strategy in enhancing the generalization capability of deep learning models in domain shift scenarios. ",Domain Generalization via Gradient Surgery
