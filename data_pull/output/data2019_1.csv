,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1093944976252248064,2675703570,Jorge I. Zuluaga,['A new crater discovered on the Moon by @LRO_NASA! It has a similar size that the crater possibly left by the impact during the lunar eclipse (our paper here <LINK>) <LINK> @coreyspowell @elakdawalla <LINK>'],http://arxiv.org/abs/1901.09573,"During lunar eclipse of January 21, 2019 a meteoroid impacted the Moon producing a visible light flash. The impact was witnessed by casual observers offering an opportunity to study the phenomenon from multiple geographical locations. We use images and videos collected by observers in 7 countries to estimate the location, impact parameters (speed and incoming direction) and energy of the meteoroid. Using parallax, we achieve determining the impact location at lat. $-29.43^{+0.30}_{-0.21}$, lon. $-67.89^{+0.07}_{-0.09}$ and geocentric distance as 356553 km. After devising and applying a photo-metric procedure for measuring flash standard magnitudes in multiple RGB images having different exposure times, we found that the flash, had an average G-magnitude $\langle G\rangle = 6.7\pm0.3$. We use gravitational ray tracing (GRT) to estimate the orbital properties and likely radiant of the impactor. We find that the meteoroid impacted the moon with a speed of $14^{+7}_{-6}$ km/s (70% C.L.) and at a shallow angle, $\theta < 38.2$ degrees. Assuming a normal error for our estimated flash brightness, educated priors for the luminous efficiency and object density, and using the GRT-computed probability distributions of impact speed and incoming directions, we calculate posterior probability distributions for the kinetic energy (median $K_{\rm med}$ = 0.8 kton), body mass ($M_{\rm med}$ = 27 kg) and diameter ($d_{\rm med}$ = 29 cm), and crater size ($D_{\rm med}$ = 9 m). If our assumptions are correct, the crater left by the impact could be detectable by prospecting lunar probes. These results arose from a timely collaboration between professional and amateur astronomers which highlight the potential importance of citizen science in astronomy. ","Location, orbit and energy of a meteoroid impacting the moon during the
  Lunar Eclipse of January 21, 2019"
1,1092818472567009281,228436420,Karlson Pfannschmidt,"['We asked ourselves, if it’s possible to generalize discrete choice to arbitrary choice functions while still beating current discrete choice models as a special case.\nThe answer is: Yes!\n\nWe are proud to announce our new paper: ""Learning Choice Functions""\n<LINK> <LINK>', 'The code of our approaches and all of the benchmark algorithms is implemented in #Python and #TensorFlow and available on #GitHub (incl. wrappers for hyperparameter optimization):\nhttps://t.co/iAVV36CcGj']",https://arxiv.org/abs/1901.10860,"Choice functions accept a set of alternatives as input and produce a preferred subset of these alternatives as output. We study the problem of learning such functions under conditions of context-dependence of preferences, which means that the preference in favor of a certain choice alternative may depend on what other options are also available. In spite of its practical relevance, this kind of context-dependence has received little attention in preference learning so far. We propose a suitable model based on context-dependent (latent) utility functions, thereby reducing the problem to the task of learning such utility functions. Practically, this comes with a number of challenges. For example, the set of alternatives provided as input to a choice function can be of any size, and the output of the function should not depend on the order in which the alternatives are presented. To meet these requirements, we propose two general approaches based on two representations of context-dependent utility functions, as well as instantiations in the form of appropriate end-to-end trainable neural network architectures. Moreover, to demonstrate the performance of both networks, we present extensive empirical evaluations on both synthetic and real-world datasets. ",Learning Context-Dependent Choice Functions
2,1092605829512089601,838848512766902272,Medford Group,['New paper from the group posted on arXiv: <LINK> We use computer vision and machine learning to examine the connection between electron density and exchange-correlation energy.'],https://arxiv.org/abs/1901.10822,"In this work we explore the potential of a new data-driven approach to the design of exchange-correlation (XC) functionals. The approach, inspired by convolutional filters in computer vision and surrogate functions from optimization, utilizes convolutions of the electron density to form a feature space to represent local electronic environments and neural networks to map the features to the exchange-correlation energy density. These features are orbital free, and provide a systematic route to including information at various length scales. This work shows that convolutional descriptors are theoretically capable of an exact representation of the electron density, and proposes Maxwell-Cartesian spherical harmonic kernels as a class of rotationally invariant descriptors for the construction of machine-learned functionals. The approach is demonstrated using data from the B3LYP functional on a number of small-molecules containing C, H, O, and N along with a neural network regression model. The machine-learned functionals are compared to standard physical approximations and the accuracy is assessed for the absolute energy of each molecular system as well as formation energies. The results indicate that it is possible to reproduce B3LYP formation energies to within chemical accuracy using orbital-free descriptors with a spatial extent of 0.2 A. The findings provide empirical insight into the spatial range of electron exchange, and suggest that the combination of convolutional descriptors and machine-learning regression models is a promising new framework for XC functional design, although challenges remain in obtaining training data and generating models consistent with pseudopotentials. ","Design and Analysis of Machine Learning Exchange-Correlation Functionals
  via Rotationally Invariant Convolutional Descriptors"
3,1091717920621821953,3098870333,Michael J. Baker,"[""New paper out this week, with @UZH_Science friends Javier Fuentes-Martin, Gino Isidori and Matthias König. \n\nWhy a B anomaly motivated leptoquark will come with a Z' and a coloron, and their associated LHC bounds.\n\n<LINK> <LINK>""]",https://arxiv.org/abs/1901.10480,"We present a detailed analysis of the collider signatures of TeV-scale massive vector bosons motivated by the hints of lepton flavour non-universality observed in $B$-meson decays. We analyse three representations that necessarily appear together in a large class of ultraviolet-complete models: a colour-singlet ($Z'$), a colour-triplet (the $U_1$ leptoquark), and a colour octet ($G'$). Under general assumptions for the interactions of these exotic states with Standard Model fields, including in particular possible right-handed and flavour off-diagonal couplings for the $U_1$, we derive a series of stringent bounds on masses and couplings that constrain a wide range of explicit new-physics models. ",High-pT Signatures in Vector-Leptoquark Models
4,1091272386098487302,1020088099,Umberto Picchini,"['new neural network architecture to automatically learn summaries for approximate Bayesian computation (ABC). Specifically designed for Markov data, works very well! See thread by @samuel_wiqvist , with @pamattei and @jesfrellsen. Paper at <LINK> Love this team! <LINK>']",https://arxiv.org/abs/1901.10230,"We present a novel family of deep neural architectures, named partially exchangeable networks (PENs) that leverage probabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the partial exchangeability properties of conditionally Markovian processes. Moreover, we show that any block-switch invariant function has a PEN-like representation. The DeepSets architecture is a special case of PEN and we can therefore also target fully exchangeable data. We employ PENs to learn summary statistics in approximate Bayesian computation (ABC). When comparing PENs to previous deep learning methods for learning summary statistics, our results are highly competitive, both considering time series and static models. Indeed, PENs provide more reliable posterior samples even when using less training data. ","Partially Exchangeable Networks and Architectures for Learning Summary
  Statistics in Approximate Bayesian Computation"
5,1091159857225330688,1576235694,Michael Brown,"['James Upjohn has a new paper on radio measurements of the cosmic star formation history, which riffs off his @Monash_Science honours research. \n\nOn @arxiv today and in PASA soon. <LINK> <LINK>']",https://arxiv.org/abs/1901.11222,"We measure the cosmic star formation history out to z = 1.3 using a sample of 918 radio-selected star forming galaxies within the 2 square degree COSMOS field. To increase our sample size, we combine 1.4 GHz flux densities from the VLA-COSMOS catalogue with flux densities measured from the VLA-COSMOS radio continuum image at the positions of I < 26.5 galaxies, enabling us to detect 1.4 GHz sources as faint as 40 uJy. We find radio measurements of the cosmic star formation history are highly dependent on sample completeness and models used to extrapolate the faint end of the radio luminosity function. For our preferred model of the luminosity function, we find the star formation rate density increases from 0.019 Solar masses per year per cubic Mpc at z = 0.225 to 0.104 Solar masses per year per cubic Mpc, which agrees to within 33% of recent UV, IR and 3 GHz measurements of the cosmic star formation history. ",The 1.4 GHz Cosmic Star Formation History at z &lt; 1.3
6,1091034695457034240,931179613178515459,Adilson E. Motter,"['Our new PNAS paper “Predicting Growth Rate from Gene Expression” (<LINK>) is now available on the arXiv: <LINK>. \n\nWe are also #SharingOurCode on the method, which includes the relevant metadata &amp; MI-POGUE’s source code: <LINK>. <LINK>']",https://arxiv.org/abs/1901.05010,"Growth rate is one of the most important and most complex phenotypic characteristics of unicellular microorganisms, which determines the genetic mutations that dominate at the population level, and ultimately whether the population will survive. Translating changes at the genetic level to their growth rate consequences remains a subject of intense interest, since such a mapping could rationally direct experiments to optimize antibiotic efficacy or bioreactor productivity. In this paper, we directly map transcriptional profiles to growth rates by gathering published gene-expression data from Escherichia coli and Saccharomyces cerevisiae with corresponding growth-rate measurements. Using a machine-learning technique called k-nearest-neighbors regression, we build a model which predicts growth rate from gene expression. By exploiting the correlated nature of gene expression and sparsifying the model, we capture 81% of the variance in growth rate of the E. coli dataset while reducing the number of features from over 4,000 to nine. In S. cerevisiae, we account for 89% of the variance in growth rate while reducing from over 5,500 dimensions to 18. Such a model provides a basis for selecting successful strategies from among the combinatorial number of experimental possibilities when attempting to optimize complex phenotypic traits like growth rate. ",Predicting Growth Rate from Gene Expression
7,1091020609071534082,35724743,Adrien Ecoffet,"[""Go-Explore paper out. New high/average scores: 18 million/650k on Montezuma's Revenge (44k w/o domain knowledge) &amp; 100k+/~60k on Pitfall, all tested w/ sticky actions! Huge thanks to my co-authors @Joost_Huizinga @joelbot3000 @jeffclune @kenneth0stanley. <LINK> <LINK>""]",https://arxiv.org/abs/1901.10995,"A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of ""superhuman"" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics). ",Go-Explore: a New Approach for Hard-Exploration Problems
8,1090965042416287744,20865039,Tristan Deleu,"['Our new paper ""A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms"" is now out <LINK> - together with Yoshua Bengio, Nasim Rahaman, @rosemary_ke, Sebastien Lachapelle, Olexa Bilaniuk, @anirudhg9119 &amp; @chrisjpal @MILAMontreal']",https://arxiv.org/abs/1901.10912,"We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities. ",A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms
9,1090870523813351424,760574568490635264,Eyal Ronen,"['Our new paper ""A Simple Explanation for the Existence of Adversarial Examples with Small Hamming Distance"" is now available online! This is a joint work together with Adi Shamir, Itay Safran and @mkilmo <LINK>']",https://arxiv.org/abs/1901.10861,"The existence of adversarial examples in which an imperceptible change in the input can fool well trained neural networks was experimentally discovered by Szegedy et al in 2013, who called them ""Intriguing properties of neural networks"". Since then, this topic had become one of the hottest research areas within machine learning, but the ease with which we can switch between any two decisions in targeted attacks is still far from being understood, and in particular it is not clear which parameters determine the number of input coordinates we have to change in order to mislead the network. In this paper we develop a simple mathematical framework which enables us to think about this baffling phenomenon from a fresh perspective, turning it into a natural consequence of the geometry of $\mathbb{R}^n$ with the $L_0$ (Hamming) metric, which can be quantitatively analyzed. In particular, we explain why we should expect to find targeted adversarial examples with Hamming distance of roughly $m$ in arbitrarily deep neural networks which are designed to distinguish between $m$ input classes. ","A Simple Explanation for the Existence of Adversarial Examples with
  Small Hamming Distance"
10,1090830082082521088,2816636344,Anirudh Goyal,"['New Paper Out: Transfer and Exploration via the Information Bottleneck. Thanks to my co-authors for all the help! \n@riashatislam , @djstrouse , Zaf Ahmed, Matthew Botvinick, @hugo_larochelle , Yoshua Bengio,  @svlevine <LINK> . Accepted at @iclr2019!']",https://arxiv.org/abs/1901.10902,"A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out {\it decision states}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space. ",InfoBot: Transfer and Exploration via the Information Bottleneck
11,1090826536775761922,2816636344,Anirudh Goyal,"['New Paper out: A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms <LINK>. Work done by amazing co-authors. Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Chris Pal\n\nFeedback is appreciated! :)']",https://arxiv.org/abs/1901.10912,"We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities. ",A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms
12,1090780097198637061,15163166,Sherri Rose,"['Our new paper ""Fair Regression for Health Care Spending"" is out: <LINK>\n\nWe build fairness into the objective function for continuous outcomes &amp; see large improvements in group undercompensation\n\nCoauthored w/PhD student Anna Zink\n\nCode: <LINK> <LINK>']",http://arxiv.org/abs/1901.10566,"The distribution of health care payments to insurance plans has substantial consequences for social policy. Risk adjustment formulas predict spending in health insurance markets in order to provide fair benefits and health care coverage for all enrollees, regardless of their health status. Unfortunately, current risk adjustment formulas are known to underpredict spending for specific groups of enrollees leading to undercompensated payments to health insurers. This incentivizes insurers to design their plans such that individuals in undercompensated groups will be less likely to enroll, impacting access to health care for these groups. To improve risk adjustment formulas for undercompensated groups, we expand on concepts from the statistics, computer science, and health economics literature to develop new fair regression methods for continuous outcomes by building fairness considerations directly into the objective function. We additionally propose a novel measure of fairness while asserting that a suite of metrics is necessary in order to evaluate risk adjustment formulas more fully. Our data application using the IBM MarketScan Research Databases and simulation studies demonstrate that these new fair regression methods may lead to massive improvements in group fairness (e.g., 98%) with only small reductions in overall fit (e.g., 4%). ",Fair Regression for Health Care Spending
13,1090551725902180357,1339162976,Jakob Wasserthal,['Excited to share preprint of our new paper on bundle-specific tractography. Contains several improvements and extended evaluation on 17 datasets.\npaper: <LINK>\ncode: <LINK>\n@mic_dkfz <LINK>'],https://arxiv.org/abs/1901.10271,"While the major white matter tracts are of great interest to numerous studies in neuroscience and medicine, their manual dissection in larger cohorts from diffusion MRI tractograms is time-consuming, requires expert knowledge and is hard to reproduce. In previous work we presented tract orientation mapping (TOM) as a novel concept for bundle-specific tractography. It is based on a learned mapping from the original fiber orientation distribution function (FOD) peaks to tract specific peaks, called tract orientation maps. Each tract orientation map represents the voxel-wise principal orientation of one tract. Here, we present an extension of this approach that combines TOM with accurate segmentations of the tract outline and its start and end region. We also introduce a custom probabilistic tracking algorithm that samples from a Gaussian distribution with fixed standard deviation centered on each peak thus enabling more complete trackings on the tract orientation maps than deterministic tracking. These extensions enable the automatic creation of bundle-specific tractograms with previously unseen accuracy. We show for 72 different bundles on high quality, low quality and phantom data that our approach runs faster and produces more accurate bundle-specific tractograms than 7 state of the art benchmark methods while avoiding cumbersome processing steps like whole brain tractography, non-linear registration, clustering or manual dissection. Moreover, we show on 17 datasets that our approach generalizes well to datasets acquired with different scanners and settings as well as with pathologies. The code of our method is openly available at this https URL ","Combined tract segmentation and orientation mapping for bundle-specific
  tractography"
14,1090431017184358400,3276698761,Albert S. Berahas,"['New paper with M. Jahani and M. Takáč ""Quasi-Newton Methods for Deep Learning: Forget the Past, Just Sample"". Quasi-Newton methods do work for Deep Learning!! <LINK>']",http://arxiv.org/abs/1901.09997,"We present two sampled quasi-Newton methods (sampled LBFGS and sampled LSR1) for solving empirical risk minimization problems that arise in machine learning. Contrary to the classical variants of these methods that sequentially build Hessian or inverse Hessian approximations as the optimization progresses, our proposed methods sample points randomly around the current iterate at every iteration to produce these approximations. As a result, the approximations constructed make use of more reliable (recent and local) information, and do not depend on past iterate information that could be significantly stale. Our proposed algorithms are efficient in terms of accessed data points (epochs) and have enough concurrency to take advantage of parallel/distributed computing environments. We provide convergence guarantees for our proposed methods. Numerical tests on a toy classification problem as well as on popular benchmarking binary classification and neural network training tasks reveal that the methods outperform their classical variants. ","Quasi-Newton Methods for Machine Learning: Forget the Past, Just Sample"
15,1090366719531466752,865627769631264768,Jamie Tayar,"['New paper out today with Matt Shetrone  <LINK> using the alpha rich stars in @APOGEEsurvey to map metallicity-dependent mixing and extra mixing on the giant branch.', 'At solar metallicity, what we see matches expectations- solar [C/N] before the first dredge up, then the dredge up happens (pink arrow), then the [C/N] stays the same for the rest of the giant branch. https://t.co/lvR1Y9xDOo', ""At lower metallicities, things get more complicated. At [Fe/H]~-1 the pre-dredge up [C/N] is no longer solar (blue region), the dredge up still looks okay (pink), but there's extra mixing (green arrow) above the luminosity bump and even more mixing on the upper GB (green vs red). https://t.co/gCwNZ9vo1F"", 'We can map this down to [Fe/H]~-1.7, and it looks like the extra mixing keeps getting stronger as low as we can go. Standard models can take into account the metallicity dependent [C/N] and get the first dredge up ~right, but the extra mixing requires something more sophisticated', ""So we're hoping the data can be used to carefully test those sophisticated models (e.g. thermohaline mixing)."", 'Also, if you use [C/N] as a mass or age indicator, keep in mind that this mixing complicates the mapping between [C/N] and age for low metallicity giants above the bump.', ""Finally, there's something weird going on with [C/N] as a function of metallicity for red clump stars, and if anyone figures out what that is please let me know."", ""Also includes @jajohnson51 @garrett_somers @Galactichawk @oneillleo @omzamora and some people whose don't have twitter/I don't know their twitter""]",https://arxiv.org/abs/1901.09592,"Internal mixing on the giant branch is an important process which affects the evolution of stars and the chemical evolution of the galaxy. While several mechanisms have been proposed to explain this mixing, better empirical constraints are necessary. Here, we use [C/N] abundances in 26097 evolved stars from the SDSS-IV/APOGEE-2 Data Release 14 to trace mixing and extra mixing in old field giants with -1.7< [Fe/H] < 0.1. We show that the APOGEE [C/N] ratios before any dredge-up occurs are metallicity dependent, but that the change in [C/N] at the first dredge-up is metallicity independent for stars above [Fe/H] ~ -1. We identify the position of the red giant branch (RGB) bump as a function of metallicity, note that a metallicity-dependent extra mixing episode takes place for low-metallicity stars ([Fe/H] <-0.4) 0.14 dex in log g above the bump, and confirm that this extra mixing is stronger at low metallicity, reaching $\Delta$ [C/N] = 0.58 dex at [Fe/H] = -1.4. We show evidence for further extra mixing on the upper giant branch, well above the bump, among the stars with [Fe/H] < -1.0. This upper giant branch mixing is stronger in the more metal-poor stars, reaching 0.38 dex in [C/N] for each 1.0 dex in log g. The APOGEE [C/N] ratios for red clump (RC) stars are significantly higher than for stars at the tip of the RGB, suggesting additional mixing processes occur during the helium flash or that unknown abundance zero points for C and N may exist among the red clump RC sample. Finally, because of extra mixing, we note that current empirical calibrations between [C/N] ratios and ages cannot be naively extrapolated for use in low-metallicity stars specifically for those above the bump in the luminosity function. ","Constraining Metallicity-dependent Mixing and Extra Mixing using [C/N]
  in Alpha-Rich Field Giants"
16,1090328161433210880,948044683673923584,Yiping Lu 💙💛,['our new paper CURE using a curvature based regularization on manifold assumption to cure missing data！ please check it <LINK> <LINK>'],https://arxiv.org/abs/1901.09548,"Missing data recovery is an important and yet challenging problem in imaging and data science. Successful models often adopt certain carefully chosen regularization. Recently, the low dimension manifold model (LDMM) was introduced by S.Osher et al. and shown effective in image inpainting. They observed that enforcing low dimensionality on image patch manifold serves as a good image regularizer. In this paper, we observe that having only the low dimension manifold regularization is not enough sometimes, and we need smoothness as well. For that, we introduce a new regularization by combining the low dimension manifold regularization with a higher order Curvature Regularization, and we call this new regularization CURE for short. The key step of solving CURE is to solve a biharmonic equation on a manifold. We further introduce a weighted version of CURE, called WeCURE, in a similar manner as the weighted nonlocal Laplacian (WNLL) method. Numerical experiments for image inpainting and semi-supervised learning show that the proposed CURE and WeCURE significantly outperform LDMM and WNLL respectively. ",CURE: Curvature Regularization For Missing Data Recovery
17,1090233215149690880,1424001956,Paul schrater,['New paper with Tom Gebhardt using topological analysis to show that deep networks create holes in input space exploited byadversarial examples:  <LINK>'],http://arxiv.org/abs/1901.09496,"The representations learned by deep neural networks are difficult to interpret in part due to their large parameter space and the complexities introduced by their multi-layer structure. We introduce a method for computing persistent homology over the graphical activation structure of neural networks, which provides access to the task-relevant substructures activated throughout the network for a given input. This topological perspective provides unique insights into the distributed representations encoded by neural networks in terms of the shape of their activation structures. We demonstrate the value of this approach by showing an alternative explanation for the existence of adversarial examples. By studying the topology of network activations across multiple architectures and datasets, we find that adversarial perturbations do not add activations that target the semantic structure of the adversarial class as previously hypothesized. Rather, adversarial examples are explainable as alterations to the dominant activation structures induced by the original image, suggesting the class representations learned by deep networks are problematically sparse on the input space. ",Characterizing the Shape of Activation Space in Deep Neural Networks
18,1090164092902481920,2999702157,Anton Ilderton,"['I have a new paper out with my collaborator @eddyedwards511 on #QED effects in strong #electromagnetic fields! Building on work by @GiesJena! This is @royalsociety funded research at @PlymUni @plym_math ! Check out <LINK> <LINK>', '@SciEngPlymUni too! ;-)']",https://arxiv.org/abs/1901.09416,"We consider one-particle reducible (1PR) contributions to QED and scalar QED processes in external fields, at one-loop and two-loop order. We investigate three cases in detail: constant crossed fields, constant magnetic fields, and plane waves. We find that 1PR tadpole contributions in plane waves and constant crossed fields are non-zero, but contribute only divergences to be renormalised away. In constant magnetic fields, on the other hand, tadpole contributions give physical corrections to processes at one-loop and beyond. Our calculations are exact in the external fields and we give strong and weak field expansions in the magnetic case. ",Reducible contributions to quantum electrodynamics in external fields
19,1090160880828989446,523241142,Juste Raimbault,"['New paper: \n""Space and complexities of territorial systems"" =&gt; Estimating Lyapounov exponents in a reaction-diffusion model of urban growth; Detecting spatial co-evolution niches in a city-network morphogenesis model <LINK>']",https://arxiv.org/abs/1901.09869,"The spatial character of territorial systems plays a crucial role in the emergence of their complexities. This contribution aims at illustrating to what extent different types of complexities can be exhibited in models of such systems. We develop from a theoretical viewpoint some arguments illustrating ontological complexity, in the sense of the diversity and multidimensionality of possible representations, and then complexity in the sense of emergence, i.e. the necessity of the existence of several autonomous levels. We then propose numerical experiments to explore properties of complexity (dynamical complexity and co-evolution) within two simple models of urban morphogenesis. We finally suggest other dimensions of complexity which could be typical of territorial systems. ",Space and complexities of territorial systems
20,1090121707170471937,3439194748,Alexander Jung,['check out our new paper on network flows in semi-supervised learning via total variation minimization: \n<LINK>\n#MachineLearning #networks #ArtificialIntelligence'],https://arxiv.org/abs/1901.09838,"We propose and analyze a method for semi-supervised learning from partially-labeled network-structured data. Our approach is based on a graph signal recovery interpretation under a clustering hypothesis that labels of data points belonging to the same well-connected subset (cluster) are similar valued. This lends naturally to learning the labels by total variation (TV) minimization, which we solve by applying a recently proposed primal-dual method for non-smooth convex optimization. The resulting algorithm allows for a highly scalable implementation using message passing over the underlying empirical graph, which renders the algorithm suitable for big data applications. By applying tools of compressed sensing, we derive a sufficient condition on the underlying network structure such that TV minimization recovers clusters in the empirical graph of the data. In particular, we show that the proposed primal-dual method amounts to maximizing network flows over the empirical graph of the dataset. Moreover, the learning accuracy of the proposed algorithm is linked to the set of network flows between data points having known labels. The effectiveness and scalability of our approach is verified by numerical experiments. ","Semi-supervised Learning in Network-Structured Data via Total Variation
  Minimization"
21,1090108166174580737,116657197,Robert Manduca,['New working paper with Benedikt Fritz is up! Exploring the economic complexity of US metros. <LINK> <LINK>'],https://arxiv.org/abs/1901.08112,"We calculate measures of economic complexity for US metropolitan areas for the years 2007-2015 based on industry employment data. We show that the concept of economic complexity translates well from the cross-country to the regional setting, and is able to incorporate local as well as traded industries. The largest cities and the Northeast of the US have the highest average complexity, while traded industries are more complex than local-serving ones on average, but with some exceptions. On average, regions with higher complexity have a higher income per capita, but those regions also were more affected by the financial crisis. Finally, economic complexity is a significant predictor of within-decreases in income per capita and population. Our findings highlight the importance of subnational regions, and particularly metropolitan areas, as units of economic geography. ",The Economic Complexity of US Metropolitan Areas
22,1089694423531741185,2816527975,Hiroshi Kera,['My new paper with @unlimitcycle has appeared in arXiv <LINK>.'],https://arxiv.org/abs/1901.08798,"Approximate vanishing ideal is a concept from computer algebra that studies the algebraic varieties behind perturbed data points. To capture the nonlinear structure of perturbed points, the introduction of approximation to exact vanishing ideals plays a critical role. However, such an approximation also gives rise to a theoretical problem---the spurious vanishing problem---in the basis construction of approximate vanishing ideals; namely, obtained basis polynomials can be approximately vanishing simply because of the small coefficients. In this paper, we propose a first general method that enables various basis construction algorithms to overcome the spurious vanishing problem. In particular, we integrate coefficient normalization with polynomial-based basis constructions, which do not need the proper ordering of monomials to process for basis constructions. We further propose a method that takes advantage of the iterative nature of basis construction so that computationally costly operations for coefficient normalization can be circumvented. Moreover, a coefficient truncation method is proposed for further accelerations. From the experiments, it can be shown that the proposed method overcomes the spurious vanishing problem, resulting in shorter feature vectors while sustaining comparable or even lower classification error. ",Spurious Vanishing Problem in Approximate Vanishing Ideal
23,1089511401587200001,15989532,dbalduzzi,"['Excited to share some new work on learning in games: <LINK>. The paper is about formulating useful objectives in nontransitive games (e.g. poker or StarCraft), which turns out to be a surprisingly subtle problem.', 'Usually, the learning objective is *given*: minimize a loss or maximize rewards. In nontransitive games, the objective is unclear. Yes, to win, but against whom? Beating paper and beating scissors in rock-paper-scissors are different objectives, that pull in different directions.', 'Blizzard has painstakingly embedded many rock-paper-scissor cycles into SC2. For example, ground units, void rays and phoenixes have this kind of dynamic. These endless exploits are a large part of why humans find the game is so rich and interesting.', 'Nontransitivity has also been linked to biodiversity: https://t.co/EVbj6ejMXm. Which makes sense! If there are lots of ways of “winning” in an ecosystem, then there’ll be lots of niches for organisms to evolve into.', 'The problem is that there’s no clear way to define the fitness of individuals in nontransitive games — which is better, rock or paper? And if you don’t have a clear objective, then all the compute in the world won’t save you.', 'Our solution is to formulate population-level objectives, using tools like Nash equilibria. Rather than trying to find a single dominant agent, which may not exist, the goal is to find all the underlying strategic dimensions of the game, and the best ways of executing them.', 'Doing this right requires some cool geometry: we extend the idea of a 1-dim fitness landscape to multi-dim gamescapes that represent the latent objectives in a game. The actual algorithms are generic and simple, building on the seminal https://t.co/M85EoYD3vk by @sharky6000.', 'The last six months have been quite an eye-opener for me. Symmetric two-player zero-sum games (that is, the *simplest* class of games!) are already an incredibly rich and challenging test-bed. There’s a wealth of new ideas waiting to be discovered in multi-agent learning.']",https://arxiv.org/abs/1901.08106,"Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them `winner' and `loser'. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSRO_rN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSRO_rN to two highly nontransitive resource allocation games and find that PSRO_rN consistently outperforms the existing alternatives. ",Open-ended Learning in Symmetric Zero-sum Games
24,1089498026408841216,959491287413149696,Eran Vos,"['My new paper on dynamic and isotopic evolution of the martian NPLD is now online at arXiv!!!  \n\n<LINK>\n\nPicture credit: @jtuttlekeane <LINK>', '@CJHandmer @jtuttlekeane @terraformedmars Hi Casey, with polar caps, ice sheet and so on I would guess 25-50 GEL.\nI would also add that in order for an atmosphere to be stable it ussaly need non reactive gas like nitrogen molecule.\nCheck this paper about terraformig Mars by Bruce Jakosky :\nhttps://t.co/25EQEyWiNj', ""@LoriKFenton @jtuttlekeane Definitely it's super cool and fun. I was lucky @jtuttlekeane came to my talk @LPSC2018.""]",https://arxiv.org/abs/1901.08401,"The layered polar caps of Mars have long been thought to be related to variations in orbit and axial tilt. We dynamically link Mars's past climate variations with the stratigraphy and isotopic composition of its ice by modeling the exchange of H2O and HDO among three reservoirs. The model shows that the interplay among equatorial, mid-latitude, and north-polar layered deposits (NPLD) induces significant isotopic changes in the cap. The diffusive properties of the sublimation lags and dust content in our model result in a cap size consistent with current Mars. The layer thicknesses are mostly controlled by obliquity variations, but the precession period of 50 kyr dominates the variations in the isotopic composition during epochs of relatively low and nearly constant obliquity such as at present. Isotopic sampling of the top 100 meters may reveal climate oscillations unseen in the layer thicknesses and would thus probe recent precession-driven climate cycles. ",Dynamic and Isotopic Evolution of Ice Reservoirs on Mars
25,1088891602880217088,1434859524,Jeff Coughlin,"['Check out the new paper led by Veselin Kostov for the DAVE team on vetting of 772 K2 planet candidates! <LINK>\n\nA searchable, sortable table for all 772 is on the DAVE website, along with comments and links to diagnostic plots for each: <LINK> <LINK>']",https://arxiv.org/abs/1901.07459,"We have adapted the algorithmic tools developed during the Kepler mission to vet the quality of transit-like signals for use on the K2 mission data. Using the four sets of publicly-available lightcurves on MAST, we produced a uniformly-vetted catalog of 772 transiting planet candidates from K2 as listed at the NASA Exoplanet archive in the K2 Table of Candidates. Our analysis marks 676 of these as planet candidates and 96 as false positives. All confirmed planets pass our vetting tests. 60 of our false positives are new identifications -- effectively doubling the overall number of astrophysical signals mimicking planetary transits in K2 data. Most of the targets listed as false positives in our catalog either show prominent secondary eclipses, transit depths suggesting a stellar companion instead of a planet, or significant photocenter shifts during transit. We packaged our tools into the open-source, automated vetting pipeline DAVE (Discovery and Vetting of Exoplanets) designed to streamline follow-up efforts by reducing the time and resources wasted observing targets that are likely false positives. DAVE will also be a valuable tool for analyzing planet candidates from NASA's TESS mission, where several guest-investigator programs will provide independent lightcurve sets -- and likely many more from the community. We are currently testing DAVE on recently-released TESS planet candidates and will present our results in a follow-up paper. ",Discovery and Vetting of Exoplanets I: Benchmarking K2 Vetting Tools
26,1088854117093728256,2352463609,Carlos Blanco,['What can coherent neutrino scattering tell us about sterile neutrinos? Find out in our new paper!  <LINK>\n\n#CEvNS #neutrinos #MiniBooNE #COHERENT'],https://arxiv.org/abs/1901.08094,"Results from the LSND and MiniBooNE experiments have been interpreted as evidence for a sterile neutrino with a mass near the electronvolt scale. Here we propose to test such a scenario by measuring the coherent elastic scattering rate of neutrinos from a pulsed spallation source. Coherent scattering is universal across all active neutrino flavors, and thus can provide a measurement of the total Standard Model neutrino flux. By performing measurements over different baselines and making use of timing information, it is possible to significantly reduce the systematic uncertainties and to independently measure the fluxes of neutrinos that originate as $\nu_{\mu}$ or as either $\nu_e$ or $\bar{\nu}_{\mu}$. We find that a 100 kg CsI detector would be sensitive to the large fraction of the sterile neutrino parameter space that could potentially account for the LSND and MiniBooNE anomalies. ","Constraining Sterile Neutrino Interpretations of the LSND and MiniBooNE
  Anomalies with Coherent Neutrino Scattering Experiments"
27,1088797431934922753,3319833287,Nicola Spaldin,['Ultrafast excitation INCREASES the order parameter in EuTiO3 -- how weird is that! You can read about why (or at least why we think so) in the new arXiv paper here: <LINK> <LINK>'],https://arxiv.org/abs/1901.08388,"The ability to control the structure of a crystalline solid on ultrafast timescales bears enormous potential for information storage and manipulation or generating new functional states of matter [1]. In many materials where the ultrafast control of crystalline structures has been explored, optical excitation pushes materials towards their less ordered high temperature phase [2{9] as electronically driven ordered phases melt and possible concomitant structural modifications relax. Nonetheless, for a few select materials it has been shown that photoexcitation can slightly enhance the amplitude of an electronic ordering phenomenon (i.e. its electronic order parameter) [9{13]. Here we show via femtosecond hard X-ray diffraction that photodoping of the perovskite EuTiO3 transiently increases the order parameter associated with a purely structural [14] phase transition represented by the antiferrodistortive rotation of the oxygen octahedra. This can be understood from an ultrafast charge-transfer induced reduction of the Goldschmidt tolerance factor [15], which is a fundamental control parameter for the properties of perovskites ","Ultrafast transient increase of oxygen octahedral rotations in a
  perovskite"
28,1088729599658336256,951503496867655680,Sanjeevan Ahilan,['Our new paper on how teams of AI agents can solve tasks more effectively when organised into a hierarchy.\n<LINK>'],https://arxiv.org/abs/1901.08492,"We investigate how reinforcement learning agents can learn to cooperate. Drawing inspiration from human societies, in which successful coordination of many individuals is often facilitated by hierarchical organisation, we introduce Feudal Multi-agent Hierarchies (FMH). In this framework, a 'manager' agent, which is tasked with maximising the environmentally-determined reward function, learns to communicate subgoals to multiple, simultaneously-operating, 'worker' agents. Workers, which are rewarded for achieving managerial subgoals, take concurrent actions in the world. We outline the structure of FMH and demonstrate its potential for decentralised learning and control. We find that, given an adequate set of subgoals from which to choose, FMH performs, and particularly scales, substantially better than cooperative approaches that use a shared reward function. ",Feudal Multi-Agent Hierarchies for Cooperative Reinforcement Learning
29,1088724601478234112,926551285788233728,Vincent Fortuin,['Our new paper on deep mean functions for meta-learning in Gaussian Processes is online: <LINK>'],http://arxiv.org/abs/1901.08098,"When fitting Bayesian machine learning models on scarce data, the main challenge is to obtain suitable prior knowledge and encode it into the model. Recent advances in meta-learning offer powerful methods for extracting such prior knowledge from data acquired in related tasks. When it comes to meta-learning in Gaussian process models, approaches in this setting have mostly focused on learning the kernel function of the prior, but not on learning its mean function. In this work, we explore meta-learning the mean function of a Gaussian process prior. We present analytical and empirical evidence that mean function learning can be useful in the meta-learning setting, discuss the risk of overfitting, and draw connections to other meta-learning approaches, such as model agnostic meta-learning and functional PCA. ",Meta-Learning Mean Functions for Gaussian Processes
30,1088658075744157696,1087183776,Jordan Ellenberg,['New paper w Wanlin Li and Mark Shusterman: many hyperelliptic curves / F_q have L-fctn nonzero at central point <LINK>'],https://arxiv.org/abs/1901.08202,"Fixing $t \in \mathbb{R}$ and a finite field $\mathbb{F}_q$ of odd characteristic, we give an explicit upper bound on the proportion of genus $g$ hyperelliptic curves over $\mathbb{F}_q$ whose zeta function vanishes at $\frac{1}{2} + it$. Our upper bound is independent of $g$ and tends to $0$ as $q$ grows. ",Nonvanishing of hyperelliptic zeta functions over finite fields
31,1088631370858065920,2816636344,Anirudh Goyal,['New paper out! Maximum Entropy Generators for Energy-Based Models\n<LINK> Great work by @ritheshkumar_ ! Other authors are Aaron Courvllle and Yoshua Bengio!'],https://arxiv.org/abs/1901.08508,"Maximum likelihood estimation of energy-based models is a challenging problem due to the intractability of the log-likelihood gradient. In this work, we propose learning both the energy function and an amortized approximate sampling mechanism using a neural generator network, which provides an efficient approximation of the log-likelihood gradient. The resulting objective requires maximizing entropy of the generated samples, which we perform using recently proposed nonparametric mutual information estimators. Finally, to stabilize the resulting adversarial game, we use a zero-centered gradient penalty derived as a necessary condition from the score matching literature. The proposed technique can generate sharp images with Inception and FID scores competitive with recent GAN techniques, does not suffer from mode collapse, and is competitive with state-of-the-art anomaly detection techniques. ",Maximum Entropy Generators for Energy-Based Models
32,1088610830701211648,228937822,Yongjin Park,"[""New preprint on causal inference is out.  Wonderful collaboration with @manoliskellis <LINK> \nI only discuss ML challenges, no biology here. But we'll soon have a separate paper on that.""]",http://arxiv.org/abs/1901.08540,"Summary statistics of genome-wide association studies (GWAS) teach causal relationship between millions of genetic markers and tens and thousands of phenotypes. However, underlying biological mechanisms are yet to be elucidated. We can achieve necessary interpretation of GWAS in a causal mediation framework, looking to establish a sparse set of mediators between genetic and downstream variables, but there are several challenges. Unlike existing methods rely on strong and unrealistic assumptions, we tackle practical challenges within a principled summary-based causal inference framework. We analyzed the proposed methods in extensive simulations generated from real-world genetic data. We demonstrated only our approach can accurately redeem causal genes, even without knowing actual individual-level data, despite the presence of competing non-causal trails. ","Causal Mediation Analysis Leveraging Multiple Types of Summary
  Statistics Data"
33,1088608960804925441,847801154473996288,Ilya Razenshteyn,"['In a new paper (joint with Y. Dong, P. Indyk, and T. Wagner) <LINK>, we show a new practical ML-based method for finding high-quality partitions of R^d that can be used for nearest neighbor data structures. (Thread, 1/9) <LINK>', 'The new method is heavily inspired by our theoretical work on NNS for general metrics: https://t.co/F1I0o82pnl (2/9)', 'The basic idea is: build the k-NN graph for the dataset, find a sparse balanced partition of the graph using a combinatorial solver, and then generalize the partition to the whole R^d using supervised learning. (3/9)', 'This is nice, since we reduce the question (geometric partitioning) to the two problems (graph partitioning and supervised learning) which people spent a lot of time thinking about, and the reduction is black-box. (4/9)', 'We instantiate the framework with linear models and neural networks and the resulting partitions perform consistently better than k-means or spectral heuristics. (5/9)', 'Combinatorial graph partitioning is very helpful for us in two ways: (6/9)', '1. It reduces an inherently unsupervised problem to a supervised one, which is much easier and better-understood. (7/9)', '2. It allows to enforce the balance between the parts easily, which is much harder to achieve via purely continuous methods. (8/9)', 'The end! (But there are lots of cool open problems, check out the paper.) (9/9)']",https://arxiv.org/abs/1901.08544,"Space partitions of $\mathbb{R}^d$ underlie a vast and important class of fast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical work on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn, Waingarten STOC 2018, FOCS 2018], we develop a new framework for building space partitions reducing the problem to balanced graph partitioning followed by supervised classification. We instantiate this general approach with the KaHIP graph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively, to obtain a new partitioning procedure called Neural Locality-Sensitive Hashing (Neural LSH). On several standard benchmarks for NNS, our experiments show that the partitions obtained by Neural LSH consistently outperform partitions found by quantization-based and tree-based methods as well as classic, data-oblivious LSH. ",Learning Space Partitions for Nearest Neighbor Search
34,1088431239755952128,19989030,Luis Lamb,"['Typed Graph Networks: our new paper on arxiv <LINK> with @marceloprates_  @Melleo54Sis Pedro Avelar and Henrique Lemos: In this paper, we revisit the original GNN model and show that it generalises many of the recent models...']",https://arxiv.org/abs/1901.07984,"Recently, the deep learning community has given growing attention to neural architectures engineered to learn problems in relational domains. Convolutional Neural Networks employ parameter sharing over the image domain, tying the weights of neural connections on a grid topology and thus enforcing the learning of a number of convolutional kernels. By instantiating trainable neural modules and assembling them in varied configurations (apart from grids), one can enforce parameter sharing over graphs, yielding models which can effectively be fed with relational data. In this context, vertices in a graph can be projected into a hyperdimensional real space and iteratively refined over many message-passing iterations in an end-to-end differentiable architecture. Architectures of this family have been referred to with several definitions in the literature, such as Graph Neural Networks, Message-passing Neural Networks, Relational Networks and Graph Networks. In this paper, we revisit the original Graph Neural Network model and show that it generalises many of the recent models, which in turn benefit from the insight of thinking about vertex \textbf{types}. To illustrate the generality of the original model, we present a Graph Neural Network formalisation, which partitions the vertices of a graph into a number of types. Each type represents an entity in the ontology of the problem one wants to learn. This allows - for instance - one to assign embeddings to edges, hyperedges, and any number of global attributes of the graph. As a companion to this paper we provide a Python/Tensorflow library to facilitate the development of such architectures, with which we instantiate the formalisation to reproduce a number of models proposed in the current literature. ",Typed Graph Networks
35,1088376046687318016,766065198721564672,Tim Langen,['New paper: Transient supersolid properties in an array of dipolar quantum droplets\n\n<LINK>\n\n #quantum @IQSTpress <LINK>'],https://arxiv.org/abs/1901.07982,"We study theoretically and experimentally the emergence of supersolid properties in a dipolar Bose-Einstein condensate. The theory reveals a ground state phase diagram with three distinct regimes - a regular Bose-Einstein condensate, incoherent and coherent arrays of quantum droplets. In the latter the droplets are connected by a finite superfluid density, which leads - in addition to the periodic density modulation - to a robust phase coherence throughout the whole system. We further theoretically demonstrate that we are able to dynamically approach the ground state in our experiment and that its lifetime is only limited by three-body losses. Experimentally we probe and confirm the signatures of the phase diagram by observing the in-situ density modulation as well as the phase coherence using matter wave interference. ",Transient supersolid properties in an array of dipolar quantum droplets
36,1088199286192521217,1132031455,James O' Neill,['New paper out (w/ @Bollegala) on Error-Correcting Output Coded Neural Sequence Prediction which reduces decoder size while maintaining performance on language modelling. Latent Mixture Sampling is also introduced to mitigate exposure bias in these models\n<LINK>'],https://arxiv.org/abs/1901.07002,"We propose a novel neural sequence prediction method based on \textit{error-correcting output codes} that avoids exact softmax normalization and allows for a tradeoff between speed and performance. Instead of minimizing measures between the predicted probability distribution and true distribution, we use error-correcting codes to represent both predictions and outputs. Secondly, we propose multiple ways to improve accuracy and convergence rates by maximizing the separability between codes that correspond to classes proportional to word embedding similarities. Lastly, we introduce our main contribution called \textit{Latent Variable Mixture Sampling}, a technique that is used to mitigate exposure bias, which can be integrated into training latent variable-based neural sequence predictors such as ECOC. This involves mixing the latent codes of past predictions and past targets in one of two ways: (1) according to a predefined sampling schedule or (2) a differentiable sampling procedure whereby the mixing probability is learned throughout training by replacing the greedy argmax operation with a smooth approximation. ECOC-NSP leads to consistent improvements on language modelling datasets and the proposed Latent Variable mixture sampling methods are found to perform well for text generation tasks such as image captioning. ",Error-Correcting Neural Sequence Prediction
37,1088106875617464320,192826908,Jorge Lillo-Box,"['Is TOI-178.01-02 the first pair of #coorbital exoplanets? In this new TROY paper by Adrien Leleu, we analyze this intriguing system detected by @NASA_TESS with a potential 3:2:2 resonance planetary system. <LINK> Thread 👇', '(1/4) Three planets were detected by @NASA_TESS around the star TOI-178. The outer two planet candidates displayed very similar periods of 9.95d and 10.35d which transit 60º apart.... https://t.co/Y1rPV7OzRj', '(2/4) The inner planet is located in a 3:2 resonance with other two.  We demonstrate that if the planets are confirmed, only the co-orbital scenario for the outer planets can maintain this system long-term stable (Credit image: Helena Morais &amp; Fathi Namouni) https://t.co/jqeeMIATJ7', '(3/4) Depending on the mass ratio of the two coorbital planets (m1 and m2) and the star (m0), they would be configured in tadpole (orange region) or horshoe (purple region) orbits. https://t.co/CLNMd8DyeG', '(4/4) And this is not the only system with these characteristics! Other systems from the Kepler mission were discarded because of this weird similar-period pair. In this paper we demonstrate that those system can exist and provide the tool to confirm their nature through TTVs! https://t.co/f6jI6BxzJg']",https://arxiv.org/abs/1901.07250,"Despite the existence of co-orbital bodies in the solar system, and the prediction of the formation of co-orbital planets by planetary system formation models, no co-orbital exoplanets (also called trojans) have been detected thus far. Here we study the signature of co-orbital exoplanets in transit surveys when two planet candidates in the system orbit the star with similar periods. Such pair of candidates could be discarded as false positives because they are not Hill-stable. However, horseshoe or long libration period tadpole co-orbital configurations can explain such period similarity. This degeneracy can be solved by considering the Transit Timing Variations (TTVs) of each planet. We then focus on the three planet candidates system TOI-178: the two outer candidates of that system have similar orbital period and had an angular separation near $\pi/3$ during the TESS observation of sector 2. Based on the announced orbits, the long-term stability of the system requires the two close-period planets to be co-orbitals. Our independent detrending and transit search recover and slightly favour the three orbits close to a 3:2:2 resonant chain found by the TESS pipeline, although we cannot exclude an alias that would put the system close to a 4:3:2 configuration. We then analyse in more detail the co-orbital scenario. We show that despite the influence of an inner planet just outside the 2:3 mean-motion resonance, this potential co-orbital system can be stable on the Giga-year time-scale for a variety of planetary masses, either on a trojan or a horseshoe orbit. We predict that large TTVs should arise in such configuration with a period of several hundred days. We then show how the mass of each planet can be retrieved from these TTVs. ",Co-orbital exoplanets from close period candidates: The TOI-178 case
38,1088083776532561920,1579142598,Dimitri Gadotti,"['Inner bars in galaxies as never seen before! Check our new #VLT #MUSE #TIMERsurvey paper by Adri Lorenzo-Cáceres, out today!\n<LINK> <LINK>']",https://arxiv.org/abs/1901.06394,"The formation of two stellar bars within a galaxy has proved challenging for numerical studies. It is yet not clear whether the inner bar is born via a star formation process promoted by gas inflow along the outer bar, or whether it is dynamically assembled from instabilities in a small-scale stellar disc. Observational constraints to these scenarios are scarce. We present a thorough study of the stellar content of two double-barred galaxies observed by the MUSE TIMER project, NGC 1291 and NGC 5850, combined with a two-dimensional multi-component photometric decomposition performed on the 3.6{\mu}m images from S4G. Our analysis confirms the presence of {\sigma}-hollows appearing in the stellar velocity dispersion distribution at the ends of the inner bars. Both galaxies host inner discs matching in size with the inner bars, suggestive of a dynamical formation for the inner bars from small-scale discs. The analysis of the star formation histories for the structural components shaping the galaxies provides constraints on the epoch of dynamical assembly of the inner bars, which took place >6.5 Gyr ago for NGC 1291 and >4.5 Gyr ago for NGC 5850. This implies that inner bars are long-lived structures. ","Clocking the assembly of double-barred galaxies with the MUSE TIMER
  project"
39,1088078623410737152,806058672619212800,Guillaume Lample,"['Check out our new paper on cross-lingual language model pretraining! We extend BERT to the cross-lingual setting. Huge improvements on XNLI, Supervised MT, Unsupervised MT.\n<LINK>\nWith @alex_conneau <LINK>', '@yoavgo @alex_conneau Thanks :) The data used for supervised/unsupervised MT pretraining is monolingual only, so the unsupervised MT remains without parallel data. The only case where we use parallel data for pretraining is for the TLM loss, and we only use it on the XNLI task. We will clarify this.', '@GregFrench26 @alex_conneau Thank you !', '@arankomatsuzaki @alex_conneau By ""unconditional LM"" you mean regular causal LM? We haven\'t tried, I think it could help but as you say, probably not as much as in MT. Also note that in MT pretraining the encoder is the important part, pretraining the decoder doesn\'t help much.', '@arankomatsuzaki @alex_conneau Which basically means that learning a good encoder / good sentence representations is harder than learning a good decoder.']",https://arxiv.org/abs/1901.07291,"Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available. ",Cross-lingual Language Model Pretraining
40,1088064124272234496,341126513,Francesca Fragkoudi,['New paper on arXiv today by Adri Lorenzo-Cáceres and the amazing TIMER team @DimitriGadotti @AstroJairo @psanchezb et al. 🤟<LINK> <LINK>'],https://arxiv.org/abs/1901.06394,"The formation of two stellar bars within a galaxy has proved challenging for numerical studies. It is yet not clear whether the inner bar is born via a star formation process promoted by gas inflow along the outer bar, or whether it is dynamically assembled from instabilities in a small-scale stellar disc. Observational constraints to these scenarios are scarce. We present a thorough study of the stellar content of two double-barred galaxies observed by the MUSE TIMER project, NGC 1291 and NGC 5850, combined with a two-dimensional multi-component photometric decomposition performed on the 3.6{\mu}m images from S4G. Our analysis confirms the presence of {\sigma}-hollows appearing in the stellar velocity dispersion distribution at the ends of the inner bars. Both galaxies host inner discs matching in size with the inner bars, suggestive of a dynamical formation for the inner bars from small-scale discs. The analysis of the star formation histories for the structural components shaping the galaxies provides constraints on the epoch of dynamical assembly of the inner bars, which took place >6.5 Gyr ago for NGC 1291 and >4.5 Gyr ago for NGC 5850. This implies that inner bars are long-lived structures. ","Clocking the assembly of double-barred galaxies with the MUSE TIMER
  project"
41,1088028458612113408,941929573742235649,Venu Kalari,"['Our new paper synergizing VPHAS, Gaia, and near-mid IR surveys to identify PMS stars in NGC6383 is out- <LINK>']",https://arxiv.org/abs/1901.07511,"This paper presents optical ($ugri$H$\alpha$)-infrared ($JHK$s,3.6--8.0$\mu$m) photometry, and $Gaia$ astrometry of 55 Classical T-Tauri stars (CTTS) in the star-forming region Sh 2-012, and it's central cluster NGC 6383. The sample was identified based on photometric H$\alpha$ emission line widths, and has a median age of 2.8$\pm$1.6 Myr, with a mass range between 0.3-1 $M_{\odot}$. 94% of CTTS with near-infrared cross-matches fall on the near-infrared T-Tauri locus, with all stars having mid-infrared photometry exhibiting evidence for accreting circumstellar discs. CTTS are found concentrated around the central cluster NGC 6383, and towards the bright rims located at the edges of Sh 2-012. Stars across the region have similar ages, suggestive of a single burst of star formation. Mass accretion rates ($\dot{M}_{\textrm{acc}}$) estimated via H$\alpha$ and $u$-band line intensities show a scatter (0.3 dex) similar to spectroscopic studies, indicating the suitability of H$\alpha$ photometry to estimate $\dot{M}_{\textrm{acc}}$. Examining the variation of $\dot{M}_{\textrm{acc}}$ with stellar mass ($M_{\ast}$), we find a smaller intercept in the $\dot{M}_{\textrm{acc}}$-$M_{\ast}$ relation than oft-quoted in the literature, providing evidence to discriminate between competing theories of protoplanetary disc evolution. ",Classical T-Tauri stars with VPHAS+: II: NGC 6383 in Sh 2-012
42,1087859646176477185,1475984894,Una Schneck,['New paper out on arXiv looking for extraterrestrial signals in a dozen planetary systems using the Green Bank Telescope 👽\n\nLooks like the Borg may have beaten us this time—BUT THE SEARCH CONTINUES! 👾\n\n<LINK>'],https://arxiv.org/abs/1901.04057,"As part of our ongoing search for technosignatures, we collected over three terabytes of data in May 2017 with the L-band receiver (1.15-1.73 GHz) of the 100 m diameter Green Bank Telescope. These observations focused primarily on planetary systems in the Kepler field, but also included scans of the recently discovered TRAPPIST-1 and LHS 1140 systems. We present the results of our search for narrowband signals in this data set with techniques that are generally similar to those described by Margot et al. (2018). Our improved data processing pipeline classified over $98\%$ of the $\sim$ 6 million detected signals as anthropogenic Radio Frequency Interference (RFI). Of the remaining candidates, 30 were detected outside of densely populated frequency regions attributable to RFI. These candidates were carefully examined and determined to be of terrestrial origin. We discuss the problems associated with the common practice of ignoring frequency space around candidate detections in radio technosignature detection pipelines. These problems include inaccurate estimates of figures of merit and unreliable upper limits on the prevalence of technosignatures. We present an algorithm that mitigates these problems and improves the efficiency of the search. Specifically, our new algorithm increases the number of candidate detections by a factor of more than four compared to Margot et al. (2018). ","A search for technosignatures from TRAPPIST-1, LHS 1140, and 10
  planetary systems in the Kepler field with the Green Bank Telescope at
  1.15-1.73 GHz"
43,1087723397650501632,831822567640006657,Dmitry Kangin,"['In our new paper with @pugeault, we investigate the possibility of improvement of trust region based reinforcement learning by reusing data from several consecutive policies. The paper is available here: <LINK>  Code repository: <LINK>']",https://arxiv.org/abs/1901.06212,"Building upon the recent success of deep reinforcement learning methods, we investigate the possibility of on-policy reinforcement learning improvement by reusing the data from several consecutive policies. On-policy methods bring many benefits, such as ability to evaluate each resulting policy. However, they usually discard all the information about the policies which existed before. In this work, we propose adaptation of the replay buffer concept, borrowed from the off-policy learning setting, to create the method, combining advantages of on- and off-policy learning. To achieve this, the proposed algorithm generalises the $Q$-, value and advantage functions for data from multiple policies. The method uses trust region optimisation, while avoiding some of the common problems of the algorithms such as TRPO or ACKTR: it uses hyperparameters to replace the trust region selection heuristics, as well as the trainable covariance matrix instead of the fixed one. In many cases, the method not only improves the results comparing to the state-of-the-art trust region on-policy learning algorithms such as PPO, ACKTR and TRPO, but also with respect to their off-policy counterpart DDPG. ",On-Policy Trust Region Policy Optimisation with Replay Buffers
44,1087619241300369409,2695714700,René Heller,"['Join our live stream about the new #exoplanet transit detection algorithm ""Transit Least Squares"" (TLS) by Michael Hippke. Tue, 22. Jan, 11:00am CET / 5:00am EST, i.e., in 2 hrs from now: <LINK>. The paper is at <LINK> <LINK>']",https://arxiv.org/abs/1901.02015,"We present a new method to detect planetary transits from time-series photometry, the Transit Least Squares (TLS) algorithm. TLS searches for transit-like features while taking the stellar limb darkening and planetary ingress and egress into account. We have optimized TLS for both signal detection efficiency (SDE) of small planets and computational speed. TLS analyses the entire, unbinned phase-folded light curve. We compensate for the higher computational load by (i.) using algorithms like ""Mergesort"" (for the trial orbital phases) and by (ii.) restricting the trial transit durations to a smaller range that encompasses all known planets, and using stellar density priors where available. A typical K2 light curve, including 80d of observations at a cadence of 30min, can be searched with TLS in ~10s real time on a standard laptop computer, as fast as the widely used Box Least Squares (BLS) algorithm. We perform a transit injection-retrieval experiment of Earth-sized planets around sun-like stars using synthetic light curves with 110ppm white noise per 30min cadence, corresponding to a photometrically quiet KP=12 star observed with Kepler. We determine the SDE thresholds for both BLS and TLS to reach a false positive rate of 1% to be SDE~7 in both cases. The resulting true positive (or recovery) rates are ~93% for TLS and ~76% for BLS, implying more reliable detections with TLS. We also test TLS with the K2 light curve of the TRAPPIST-1 system and find six of seven Earth-sized planets using an iterative search for increasingly lower signal detection efficiency, the phase-folded transit of the seventh planet being affected by a stellar flare. TLS is more reliable than BLS in finding any kind of transiting planet but it is particularly suited for the detection of small planets in long time series from Kepler, TESS, and PLATO. We make our Python implementation of TLS publicly available. ","Transit Least Squares: Optimized transit detection algorithm to search
  for periodic transits of small planets"
45,1087420488647929857,288194586,Max Kleiman-Weiner,"['Our #AAAI19 paper and oral with @michaelmshum @mlittmancs and Josh Tenenbaum on ""Theory of Minds"": <LINK>. A new composable hierarchy for representing the plans of groups. Can infer social intentions like cooperate &amp; compete from sparse data in human-like ways. <LINK>']",http://arxiv.org/abs/1901.06085,"Human social behavior is structured by relationships. We form teams, groups, tribes, and alliances at all scales of human life. These structures guide multi-agent cooperation and competition, but when we observe others these underlying relationships are typically unobservable and hence must be inferred. Humans make these inferences intuitively and flexibly, often making rapid generalizations about the latent relationships that underlie behavior from just sparse and noisy observations. Rapid and accurate inferences are important for determining who to cooperate with, who to compete with, and how to cooperate in order to compete. Towards the goal of building machine-learning algorithms with human-like social intelligence, we develop a generative model of multi-agent action understanding based on a novel representation for these latent relationships called Composable Team Hierarchies (CTH). This representation is grounded in the formalism of stochastic games and multi-agent reinforcement learning. We use CTH as a target for Bayesian inference yielding a new algorithm for understanding behavior in groups that can both infer hidden relationships as well as predict future actions for multiple agents interacting together. Our algorithm rapidly recovers an underlying causal model of how agents relate in spatial stochastic games from just a few observations. The patterns of inference made by this algorithm closely correspond with human judgments and the algorithm makes the same rapid generalizations that people do. ","Theory of Minds: Understanding Behavior in Groups Through Inverse
  Planning"
46,1086283068485201920,1545756036,Mike Boylan-Kolchin,['New paper from A. Graus et al. on the arXiv today: <LINK>\n\nSome take-aways: age gradients in simulated galaxies are common; feedback reduces age gradients in galaxies with density cores; mergers can complicate things a lot. <LINK>'],https://arxiv.org/abs/1901.05487,"We explore the radial variation of star formation histories in dwarf galaxies simulated with Feedback In Realistic Environments (FIRE) physics. The sample contains 9 low-mass field dwarfs with M_ star = 10^5 - 10^7 M_sun from previous FIRE results, and a new suite of 17 higher mass field dwarfs with M_star = 10^7 - 10^9 M_sun introduced here. We find that age gradients are common in our dwarfs, with older stars dominant at large radii. The strength of the gradient correlates with overall galaxy age such that earlier star formation produces a more pronounced gradient. The relation between formation time and strength of the gradient is driven by both mergers and star-formation feedback. Mergers can both steepen and flatten the age gradient depending on the timing of the merger and star formation history of the merging galaxy. In galaxies without significant mergers, early feedback pushes stars to the outskirts at early times. Interestingly, among galaxies without mergers, those with large dark matter cores have flatter age gradients because these galaxies have more late-time feedback. If real galaxies have age gradients as we predict, stellar population studies that rely on sampling a limited fraction of a galaxy can give a biased view of its global star formation history. We show that central fields can be biased young by a few Gyrs while outer fields are biased old. Fields positioned near the 2D half-light radius will provide the least biased measure of a dwarf galaxy's global star formation history. ","A Predicted Correlation Between Age Gradient and Star Formation History
  in FIRE Dwarf Galaxies"
47,1086178556311494656,68496043,Bob Nichol,"[""Fun new paper from my student Andrius <LINK> Emergent gravity isn't an obvious solution to #DarkMatter (unfortunately)""]",https://arxiv.org/abs/1901.05505,"Verlinde's theory of Emergent Gravity (EG) describes gravity as an emergent phenomenon rather than a fundamental force. Applying this reasoning in de Sitter space leads to gravity behaving differently on galaxy and galaxy cluster scales; this excess gravity might offer an alternative to dark matter. Here we test these ideas using the data from the Coma cluster and from 58 stacked galaxy clusters. The X-ray surface brightness measurements of the clusters at $0.1 < z < 1.2$ along with the weak lensing data are used to test the theory. We find that the simultaneous EG fits of the X-ray and weak lensing datasets are significantly worse than those provided by General Relativity (with cold dark matter). For the Coma cluster, the predictions from Emergent Gravity and General Relativity agree in the range of 250 - 700 kpc, while at around 1 Mpc scales, EG total mass predictions are larger by a factor of 2. For the cluster stack the predictions are only in good agreement at around the 1 - 2 Mpc scales, while for $r \gtrsim 10$ Mpc EG is in strong tension with the data. According to the Bayesian information criterion analysis, GR is preferred in all tested datasets; however, we also discuss possible modifications of EG that greatly relax the tension with the data. ",Testing Emergent Gravity on Galaxy Cluster Scales
48,1086088587790168064,352784707,Cody Dirks,"['New paper up on the arXiv today! \n\nI used the Hubble telescope to do some follow-up studies on cold, dusty interstellar gas first identified by the Planck satellite: <LINK>']",https://arxiv.org/abs/1901.05012,"We report results of the first study utilizing the ultraviolet capabilities of the Hubble Space Telescope to investigate a sample of Planck Galactic Cold Clump (PGCC) sources. We have selected high-resolution spectra toward 25 stars that contain a multitude of interstellar absorption lines associated with the interstellar medium (ISM) gas within these PGCC sources, including carbon monoxide (CO), C I and O I. By building cloud-component models of the individual absorption components present in these spectra, we can identify and isolate components associated with the PGCC sources, allowing for a more accurate investigation of the ISM behavior within these sources. Despite probing a broad range of overall sightline properties, we detect CO along each sightline. Sightlines with CO column density N(CO)$~>~$10$^{15}~$cm$^{-2}$ exhibit spatial dependence in N(CO) and CO/C I, while sightlines with N(CO)$~<~$10$^{15}~$cm$^{-2}$ show no such spatial dependence. Differences between N(H$_2$) values derived from UV absorption and dust emission suggest structure in the spatial distribution of N(H$_2$), where ""CO-bright"" sightlines are associated with PGCC sources embedded within smooth translucent envelopes, and ""CO-dark"" sightlines are associated with PGCC sources embedded in patchier environments containing more diffuse gas. ",Ultraviolet HST Spectroscopy of Planck Cold Clumps
49,1085821700573282304,2384314538,P-P Dechant 🇺🇦 🇩🇪🇬🇧🇪🇺,"[""New paper with Y-H He out on the arxiv today: #MachineLearning a #virus assembly #fitness #landscape <LINK>. Following @Virosphere2012 's example we welcome any feedback and suggestions for journal submission! #interdisciplinarity <LINK>""]",https://arxiv.org/abs/1901.05051,"Realistic evolutionary fitness landscapes are notoriously difficult to construct. A recent cutting-edge model of virus assembly consists of a dodecahedral capsid with $12$ corresponding packaging signals in three affinity bands. This whole genome/phenotype space consisting of $3^{12}$ genomes has been explored via computationally expensive stochastic assembly models, giving a fitness landscape in terms of the assembly efficiency. Using latest machine-learning techniques by establishing a neural network, we show that the intensive computation can be short-circuited in a matter of minutes to astounding accuracy. ",Machine-learning a virus assembly fitness landscape
50,1085807180320784384,595554321,Jason D Lotay,"['My new paper on Lagrangian mean curvature flow, with wonderful collaborators Ben Lambert and Felix Schulze @MathematicsUCL, is the first with my new @OxUniMaths affiliation: <LINK>']",https://arxiv.org/abs/1901.05383,"Ancient solutions of Lagrangian mean curvature flow in C^n naturally arise as Type II blow-ups. In this extended note we give structural and classification results for such ancient solutions in terms of their blow-down and, motivated by the Thomas-Yau Conjecture, focus on the almost calibrated case. In particular, we classify Type II blow-ups of almost calibrated Lagrangian mean curvature flow when the blow-down is a pair of transverse planes or, when n=2, a multiplicity two plane. We also show that the Harvey-Lawson Clifford torus cone in C^3 cannot arise as the blow-down of an almost calibrated Type II blow-up. ",Ancient solutions in Lagrangian mean curvature flow
51,1085573198316429313,838292815,Ofir Nachum,['My new paper on learning fair machine learning classifiers: <LINK>\nWe frame the problem as trying to learn with respect to unknown (and true) labels despite only having access to observed (and biased) labels.  We find a surprisingly simple solution for doing so!'],https://arxiv.org/abs/1901.04966,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification. ",Identifying and Correcting Label Bias in Machine Learning
52,1085572883815038978,714654290426667008,Fredrik Johansson,['New paper: Faster arbitrary-precision dot product and matrix multiplication <LINK>'],https://arxiv.org/abs/1901.04289,"We present algorithms for real and complex dot product and matrix multiplication in arbitrary-precision floating-point and ball arithmetic. A low-overhead dot product is implemented on the level of GMP limb arrays; it is about twice as fast as previous code in MPFR and Arb at precision up to several hundred bits. Up to 128 bits, it is 3-4 times as fast, costing 20-30 cycles per term for floating-point evaluation and 40-50 cycles per term for balls. We handle large matrix multiplications even more efficiently via blocks of scaled integer matrices. The new methods are implemented in Arb and significantly speed up polynomial operations and linear algebra. ",Faster arbitrary-precision dot product and matrix multiplication
53,1085552193397116928,1053730346930384897,Jeff Carlin,"['Check out the new paper I worked on with Sten Hasselquist - we show that Sagittarius stream stars can be (fairly efficiently) selected based on APOGEE abundances alone. The non-Sagittarius stars contain some interesting surprises, too! \n<LINK>']",https://arxiv.org/abs/1901.04559,"The SDSS-IV Apache Point Observatory Galactic Evolution Experiment (APOGEE) survey provides precise chemical abundances of 18 chemical elements for $\sim$ 176,000 red giant stars distributed over much of the Milky Way Galaxy (MW), and includes observations of the core of the Sagittarius dwarf spheroidal galaxy (Sgr). The APOGEE chemical abundance patterns of Sgr have revealed that it is chemically distinct from the MW in most chemical elements. We employ a \emph{k}-means clustering algorithm to 6-dimensional chemical space defined by [(C+N)/Fe], [O/Fe], [Mg/Fe], [Al/Fe], [Mn/Fe], and [Ni/Fe] to identify 62 MW stars in the APOGEE sample that have Sgr-like chemical abundances. Of the 62 stars, 35 have \emph{Gaia} kinematics and positions consistent with those predicted by \emph{N}-body simulations of the Sgr stream, and are likely stars that have been stripped from Sgr during the last two pericenter passages ($<$ 2 Gyr ago). Another 20 of the 62 stars exhibit chemical abundances indistinguishable from the Sgr stream stars, but are on highly eccentric orbits with median $r_{\rm apo} \sim $ 25 kpc. These stars are likely the `accreted' halo population thought to be the result of a separate merger with the MW 8-11 Gyr ago. We also find one hypervelocity star candidate. We conclude that Sgr was enriched to [Fe/H] $\sim$ -0.2 before its most recent pericenter passage. If the `accreted halo' population is from one major accretion event, then this progenitor galaxy was enriched to at least [Fe/H] $\sim$ -0.6, and had a similar star formation history to Sgr before merging. ","Identifying Sagittarius Stream Stars By Their APOGEE Chemical Abundance
  Signatures"
54,1085482957693992960,3124305238,Anna Scaife,['Our new paper on fast source finding for @SKA_telescope scale image sizes is now on @arxiv - may be useful for anyone attempting #skadatachallenge1! Code available on @github #bigdata @YossariansLife <LINK> <LINK>'],https://arxiv.org/abs/1901.04956,"Object detection in astronomical images, generically referred to as source finding, is often performed before the object characterisation stage in astrophysical processing work flows. In radio astronomy, source finding has historically been performed by bespoke off-line systems; however, modern data acquisition systems as well as those proposed for upcoming observatories such as the Square Kilometre Array (SKA), will make this approach unfeasible. One area where a change of approach is particularly necessary is in the design of fast imaging systems for transient studies. This paper presents a number of advances in accelerating and automating the source finding in such systems. ",Efficient Source Finding for Radio Interferometric Images
55,1085474755774418944,321794593,José G. Fernández-Trincado,"['Check-out our new APOGEE paper about Sgr "" Identifying Sagittarius Stream Stars By Their APOGEE Chemical Abundance Signatures"" <LINK> <LINK>']",https://arxiv.org/abs/1901.04559,"The SDSS-IV Apache Point Observatory Galactic Evolution Experiment (APOGEE) survey provides precise chemical abundances of 18 chemical elements for $\sim$ 176,000 red giant stars distributed over much of the Milky Way Galaxy (MW), and includes observations of the core of the Sagittarius dwarf spheroidal galaxy (Sgr). The APOGEE chemical abundance patterns of Sgr have revealed that it is chemically distinct from the MW in most chemical elements. We employ a \emph{k}-means clustering algorithm to 6-dimensional chemical space defined by [(C+N)/Fe], [O/Fe], [Mg/Fe], [Al/Fe], [Mn/Fe], and [Ni/Fe] to identify 62 MW stars in the APOGEE sample that have Sgr-like chemical abundances. Of the 62 stars, 35 have \emph{Gaia} kinematics and positions consistent with those predicted by \emph{N}-body simulations of the Sgr stream, and are likely stars that have been stripped from Sgr during the last two pericenter passages ($<$ 2 Gyr ago). Another 20 of the 62 stars exhibit chemical abundances indistinguishable from the Sgr stream stars, but are on highly eccentric orbits with median $r_{\rm apo} \sim $ 25 kpc. These stars are likely the `accreted' halo population thought to be the result of a separate merger with the MW 8-11 Gyr ago. We also find one hypervelocity star candidate. We conclude that Sgr was enriched to [Fe/H] $\sim$ -0.2 before its most recent pericenter passage. If the `accreted halo' population is from one major accretion event, then this progenitor galaxy was enriched to at least [Fe/H] $\sim$ -0.6, and had a similar star formation history to Sgr before merging. ","Identifying Sagittarius Stream Stars By Their APOGEE Chemical Abundance
  Signatures"
56,1085427917398523904,15751831,Yuke Zhu,['We have just released our new work on 6D pose estimation from RGB-D data -- real-time inference with end-to-end deep models for real-world robot grasping and manipulation! Paper: <LINK> Code: <LINK> w/ @danfei_xu @drfeifei @silviocinguetta <LINK>'],https://arxiv.org/abs/1901.04780,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose. ",DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion
57,1085414000182083584,56395761,Reza Abbasi-Asl,"['Check out our new review paper on interpretable machine learning, definitions, methods, and applications: <LINK>']",http://arxiv.org/abs/1901.04592,"Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant (PDR) framework for discussing interpretations. The PDR framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods. ","Interpretable machine learning: definitions, methods, and applications"
58,1085133191936200709,3228486315,Daniele Grattarola,"['On Arxiv, new paper in collab with @UiTromso!\nWe introduce a new type of spectral convolution on graphs, based on ARMA filters (better response w.r.t. poly ones). \nGreat results on node/graph signals/whole graphs classification. These things rock!\n\n<LINK> <LINK>', '@UiTromso We also formulate and explore the application of a principled pooling strategy for graphs, improving over previous solutions. The combo of the two results in a slim model and fast training times. https://t.co/5l4Nl1DLwF', '@UiTromso Results on node classification (citation nets), graph signal classification (20 news), and graph classification (graph kernel database). https://t.co/47MplxHoW7', 'Code will be released as #Keras layers + utils + experiments, as soon the paper is published. \nWill also be integrated as part of Spektral, our (yet to be released) framework for GNNs.', 'HUGE shoutout to @Slackericida for coming up with the idea, it was incredibly fun to work together on this paper!', '@Slackericida CC @m_deff @thomaskipf @PetarV_93 @PeterWBattaglia @KleineBottleM @mys_007 \nall cited in the paper']",http://arxiv.org/abs/1901.01343,"Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters. ",Graph Neural Networks with convolutional ARMA filters
59,1084215875836665856,2848867218,Vijini Mallawaarachchi,['The preprint of our new journal paper is now available at arXiv.\n<LINK> #research #WebChangeDetection <LINK>'],https://arxiv.org/abs/1901.02660v1,"Majority of the currently available webpages are dynamic in nature and are changing frequently. New content gets added to webpages and existing content gets updated or deleted. Hence, people find it useful to be alert for changes in webpages which contain information valuable to them. In the current context, keeping track of these webpages and getting alerts about different changes have become significantly challenging. Change Detection and Notification (CDN) systems were introduced to automate this monitoring process and notify users when changes occur in webpages. This survey classifies and analyzes different aspects of CDN systems and different techniques used for each aspect. Furthermore, the survey highlights current challenges and areas of improvement present within the field of research. ",] Change Detection and Notification of Webpages: A Survey
60,1083731258541170689,1420409148,Paul Hines,"['New paper: ""Risk of Cascading Blackouts Given Correlated Component Outages"" shows that it is possible to assess the risk of cascades in very large networks with correlated initiating failure probabilities. <LINK> <LINK>']",https://arxiv.org/abs/1901.03304,"Cascading blackouts typically occur when nearly simultaneous outages occur in k out of N components in a power system, triggering subsequent failures that propagate through the network and cause significant load shedding. While large cascades are rare, their impact can be catastrophic, so quantifying their risk is important for grid planning and operation. A common assumption in previous approaches to quantifying such risk is that the $k$ initiating component outages are statistically independent events. However, when triggered by a common exogenous cause, initiating outages may actually be correlated. Here, copula analysis is used to quantify the impact of correlation of initiating outages on the risk of cascading failure. The method is demonstrated on two test cases; a 2383-bus model of the Polish grid under varying load conditions and a synthetic 10,000-bus model based on the geography of the Western US. The large size of the Western US test case required development of new approaches for bounding an estimate of the total number of N-3 blackout-causing contingencies. The results suggest that both risk of cascading failure, and the relative contribution of higher order contingencies, increase as a function of spatial correlation in component failures. ",Risk of Cascading Blackouts Given Correlated Component Outages
61,1083717388741287938,3117596931,Roman Blum DipWSET,"['New paper online ""Superlight - A Permissionless, Light-client Only Blockchain with Self-Contained Proofs and BLS Signatures"" <LINK> #hsr #blockchain']",https://arxiv.org/abs/1901.02213,"Blockchain protocols are based on a distributed database where stored data is guaranteed to be immutable. The requirement that all nodes have to maintain their own local copy of the database ensures security while consensus mechanisms help deciding which data gets added to the database and keep powerful adversaries from derailing the system. However, since the database that forms the foundation of a blockchain is a continuously growing list of blocks, scalability is an inherent problem of this technology. Some public blockchains need a few 100 GB to Terabytes of storage. In this work, we present the concept Superlight with self-contained proofs, which is designed to improve scalability of a public blockchain, while preserving security and decentralization. Instead of all nodes having a local copy of the whole blockchain to verify a transaction, nodes can derive the validity of a transaction by only using block headers. To keep the block headers compact, BLS signatures are used to combine signatures. We provide a definition of SCPs and show the required steps of a client to create a proof that is accepted by other nodes for transferring funds. The advantage of such a light-client-only blockchain is the lower storage requirement, while the drawback is an increased computational complexity due to BLS signatures, limited use-cases due to lack of a global state, and the requirement for an interactive protocol between sender, receiver, and miner to create a transaction. ","Superlight -- A Permissionless, Light-client Only Blockchain with
  Self-Contained Proofs and BLS Signatures"
62,1083672377861591042,74163970,Tim Harries,['Beautiful spiral arms discovered around binary HD34700A using @PlanetImager described in a new paper by @AstroMonnier (and others inc me @AliciaAarnio @DrCEspaillat  @astrokraus). Available at <LINK> <LINK>'],https://arxiv.org/abs/1901.02467,"We present the first images of the transition disk around the close binary system HD 34700A in polarized scattered light using the Gemini Planet Imager instrument on Gemini South. The J and H band images reveal multiple spiral-arm structures outside a large (R = 0.49"" = 175 au) cavity along with a bluish spiral structure inside the cavity. The cavity wall shows a strong discontinuity and we clearly see significant non-azimuthal polarization Uphi consistent with multiple scattering within a disk at an inferred inclination ~42deg. Radiative transfer modeling along with a new Gaia distance suggest HD 37400A is a young (~5 Myr) system consisting of two intermediate-mass (~2Msun) stars surrounded by a transitional disk and not a solar-mass binary with a debris disk as previously classified. Conventional assumptions of the dust-to-gas ratio would rule out a gravitational instability origin to the spirals while hydrodynamical models using the known external companion or a hypothetical massive protoplanet in the cavity both have trouble reproducing the relatively large spiral arm pitch angles (~30deg) without fine tuning of gas temperature. We explore the possibility that material surrounding a massive protoplanet could explain the rim discontinuity after also considering effects of shadowing by an inner disk. Analysis of archival Hubble Space Telescope data suggests the disk is rotating counterclockwise as expected from the spiral arm structure and revealed a new low-mass companion at 6.45"" separation. We include an appendix which sets out clear definitions of Q, U, Qphi, Uphi, correcting some confusion and errors in the literature. ","Multiple spiral arms in the disk around intermediate-mass binary HD
  34700A"
63,1083410795407372293,70874545,Josh Lothringer,"['Check out a new paper (<LINK>) led by Ian Crossfield, where we detect 3(!) isotopologues of CO in both stars of an M-dwarf binary. This gives us C12/C13 and O16/O18 ratios, telling us this binary was enriched by a massive core-collapse SN.', ""You can hear Ian talk about this in Session 420 this morning at 11:20 at #aas233, 10 minutes after I'm done giving my dissertation talk in Session 404!"", ""@StellarTayar That's a good question- I don't have a good intuition when it comes to stellar abundances so I would probably chalk it up to uncertainty. Maybe that is good reason to get add'l observations though.""]",https://arxiv.org/abs/1901.02607,"Low-mass M dwarfs represent the most common outcome of star formation, but their complex emergent spectra hinder detailed studies of their composition and initial formation. The measurement of isotopic ratios is a key tool that has been used to unlock the formation of our Solar System, the Sun, and the nuclear processes within more massive stars. We observed GJ 745AB, two M dwarfs orbiting in a wide binary, with the IRTF/iSHELL spectrograph. Our spectroscopy of CO in these stars at the 4.7 micron fundamental and 2.3 micron first-overtone rovibrational bandheads reveals 12C16O, 13C16O, and 12C18O in their photospheres. Since the stars are fully convective, the atomic constituents of these isotopologues should be uniformly mixed throughout the stars' interiors. We find that in these M dwarfs, both 12C/13C and 16O/18O greatly exceed the Solar values. These measurements cannot be explained solely by models of Galactic chemical evolution, but require that the stars formed from an ISM significantly enriched by material ejected from an exploding core-collape supernova. These isotopic measurements complement the elemental abundances provided by large-scale spectroscopic surveys, and open a new window onto studies of Galactic evolution, stellar populations, and individual systems. ",Unusual Isotopic Abundances in a Fully-Convective Stellar Binary
64,1083408430579286017,2956121356,Russ Salakhutdinov,"['New paper: Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context: Learning long-term dependency without disrupting temporal coherence, SOTA on 5 datasets w/t Zihang Dai, Zhilin Yang et al. <LINK> \nCode, pretrained models: <LINK> <LINK>']",https://arxiv.org/abs/1901.02860,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch. ",Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
65,1083356388024348673,19446567,Tom Hands,"['My new paper ""The fate of planetesimal discs in young open clusters: implications for 1I/\'Oumuamua, the Kuiper belt, the Oort cloud and more"" is available on the arXiv today - <LINK> - featuring a brand new, completely reversible time-stepping scheme!', '@nick_attree Indeed! Well done man! Yours looks great!', ""@nick_attree Yeah it's my second paper with him. Good guy to work with - really knows his stuff."", '@astrokiwi Of course - would be very happy to hear from you.', '@nick_attree He sure does! Good guy to work with if you ever get chance :)', ""@astrokiwi You're very welcome - and thanks for the citation! Please do let me know if you have any more questions or ideas, would be great to chat about it.""]",https://arxiv.org/abs/1901.02465,"We perform $N$-body simulations of the early phases of open cluster evolution including a large population of planetesimals, initially arranged in Kuiper-belt like discs around each star. Using a new, 4th-order and time-reversible $N$-body code on Graphics Processing Units (GPUs), we evolve the whole system under the stellar gravity, i.e. treating planetesimals as test particles, and consider two types of initial cluster models, similar to IC348 and the Hyades, respectively. In both cases, planetesimals can be dynamically excited, transferred between stars or liberated to become free-floating (such as A/2017 U1 or 'Oumuamua) during the early cluster evolution. We find that planetesimals captured from another star are not necessarily dynamically distinct from those native to a star. After an encounter both native and captured planetesimals can exhibit aligned periastrons, qualitatively similar to that seen in the Solar system and commonly thought to be the signature of Planet 9. We discuss the implications of our results for both our Solar system and exoplanetary systems. ","The fate of planetesimal discs in young open clusters: implications for
  1I/'Oumuamua, the Kuiper belt, the Oort cloud and more"
66,1083080225410465792,923231130383536128,Eduardo Fonseca,['New preprint in collaboration with @GoogleAI Sound Understanding: a dataset to research label noise in sound event recognition &amp; an evaluation of noise-robust loss functions\npaper: <LINK>\nFSDnoisy18k: <LINK>\ncode: <LINK>\n@mtg_upf'],https://arxiv.org/abs/1901.01189,"As sound event classification moves towards larger datasets, issues of label noise become inevitable. Web sites can supply large volumes of user-contributed audio and metadata, but inferring labels from this metadata introduces errors due to unreliable inputs, and limitations in the mapping. There is, however, little research into the impact of these errors. To foster the investigation of label noise in sound event classification we present FSDnoisy18k, a dataset containing 42.5 hours of audio across 20 sound classes, including a small amount of manually-labeled data and a larger quantity of real-world noisy data. We characterize the label noise empirically, and provide a CNN baseline system. Experiments suggest that training with large amounts of noisy data can outperform training with smaller amounts of carefully-labeled data. We also show that noise-robust loss functions can be effective in improving performance in presence of corrupted labels. ",Learning Sound Event Classifiers from Web Audio with Noisy Labels
67,1082674274979086336,2235411914,Surya Ganguli,['New #deeplearning &amp; #neuroscience oral presentation at  #ICLR2019 w/ @Jack_W_Lindsey @SamOcko @StphTphsn; paper: A Unified Theory of Early Visual Representations from Retina to Cortex through Anatomically Constrained Deep CNNs <LINK> <LINK>'],https://arxiv.org/abs/1901.00945,"The visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex, typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortices. This result predicts that the retinas of small vertebrates should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system. ","A Unified Theory of Early Visual Representations from Retina to Cortex
  through Anatomically Constrained Deep CNNs"
68,1082602453672124416,226615056,Héctor Andrade,['New paper: <LINK>'],https://arxiv.org/abs/1901.01388,"Microlocal analysis provides deep insight into singularity structures and is often crucial for solving inverse problems, predominately, in imaging sciences. Of particular importance is the analysis of wavefront sets and the correct extraction of those. In this paper, we introduce the first algorithmic approach to extract the wavefront set of images, which combines data-based and model-based methods. Based on a celebrated property of the shearlet transform to unravel information on the wavefront set, we extract the wavefront set of an image by first applying a discrete shearlet transform and then feeding local patches of this transform to a deep convolutional neural network trained on labeled data. The resulting algorithm outperforms all competing algorithms in edge-orientation and ramp-orientation detection. ","Extraction of digital wavefront sets using applied harmonic analysis and
  deep neural networks"
69,1082561720504090624,1367912887,Stéphane Deny,"['Why are receptive fields circular in the retina but sharply oriented in primary visual cortex (V1)? Answers in our new #ICLR2019 paper using a deep convolutional model of the visual system: <LINK>. With @Jack_W_Lindsey, @SamOcko and @SuryaGanguli.\n⬇ THREAD ⬇ <LINK>', '2/ In a vanilla network trained on image classification (CIFAR-10), receptive fields (RFs) are oriented in the very first layers. But as we add a dimensionality bottleneck at the retinal output, mimicking the optic nerve anatomy, realistic RFs emerge in both the retina and V1. https://t.co/aFX2kgOSd6', '3/ Another outstanding question in neuroscience is the role of the retina: is it to efficiently encode *all* visual information, or to extract features relevant to behavior? Previous studies diverge on the question, depending on their perspective and the cell-type/species studied https://t.co/W2a36urMtP', '4/ In our model, retinal representations depend on the sophistication of the downstream visual cortices.  For a deep cortex, the retina is more linear and information-preserving.  For a shallow cortex, the retina is nonlinear, lossy, and extracts task-relevant features. https://t.co/MpI10WD7L5', '5/ These results predict that the retinas of different species should lie on a spectrum between the two objectives of efficient coding and feature extraction, depending on the sophistication of their visual cortices. More experimental work is needed to test these predictions!', 'All code available at: https://t.co/6L3RKsOlvr (using Keras and Tensorflow)']",https://arxiv.org/abs/1901.00945,"The visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex, typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortices. This result predicts that the retinas of small vertebrates should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system. ","A Unified Theory of Early Visual Representations from Retina to Cortex
  through Anatomically Constrained Deep CNNs"
70,1082457822091108353,907232486735958018,Jaki Noronha-Hostler,"['New article out on shrinking the Quark Gluon Plasma! Also my first paper out with my postdoc Matt Sievert. We compare the QGP produced in PbPb, XeXe, ArAr, and OO collisions at the LHC.\n\n<LINK>', 'So some of the main consequences that we find:\n1. Collisions with the same multiplicity have a different radius of the impact region (Pb the largest, OO the smallest), which leads to a hotter droplet of the QGP in OO collisions than PbPb.', ""2. The radii fluctuate significantly for small Npart so you CAN find Pb collisions with the same radius as O collisions for the same Npart.  It's just that statistically, PbPb collisions produce a larger impart region than OO collisions."", ""3. Remember my recent rant about Npart? Here's why, multiplicity shows an uptick in central collisions that is missed plotting versus Npart.  We have ideas on why this occurs but you'll have to be patient for future papers so we can flesh them out more thoroughly. https://t.co/f0uWxQbezN"", '4. This is one of my favorite plots.  We chose 4 events with the same initial entropy (i.e. multiplicity) with a radius close to the &lt;R&gt; of that system. One can see how ArAr and OO collisions are hotter and rounder whereas PbPb and XeXe collisions are cooler and elliptical. https://t.co/HfNXAxpSEB', '5. Much of our results are predictions that need to be validated or disproved by experimental data.  Certainly, any discrepancies would be interesting either to show our model breaking down (differences in transport coefficients? effects of large gradients? or signs of CGC?).', ""6. I'm always curious how initial conditions (right after two heavy ions collide) correlates to the final state.  Here we find a very non-trivial results in the system size scan.  When scaling by Npart, small systems are better reproduced by linear response than large systems!"", '7. We then checked the predictive power of linear+cubic response and found that is was quite non-trivial for v2{4}/v2{2}.  Large systems can predict the final results well just from the eccentricities.  Small systems have predictive power in more central collisions only.']",https://arxiv.org/abs/1901.01319,"In recent years the understanding on the limits of the smallest possible droplet of the Quark Gluon Plasma has been called into question. Experimental results from both the Large Hadron Collider and the Relativistic Heavy Ion Collider have provided hints that the Quark Gluon Plasma may be produced in systems as small as that formed in pPb or dAu collisions. Yet alternative explanations still exist from correlations arising from quarks and gluons in a color glass condensate picture. In order to resolve these two scenarios, a system size scan has been proposed at the Large Hadron Collider for collisions of ArAr and OO. Here we make predictions for a possible future run of ArAr and OO collisions at the Large Hadron Collider and study the system size dependence of a variety of flow observables. We find that linear response (from the initial conditions to the final flow harmonics) becomes more dominant in smaller systems whereas linear+cubic response can accurately predict multi-particle cumulants for a wide range of centralities in large systems. ",Shrinking the Quark Gluon Plasma
71,1082286363196751872,291589292,Marcella Wijngaarden,"[""My FIRST first-author paper with supervisor Wynn Ho and collaborators was accepted and is on arXiv today! 🎉 It's on diffusive nuclear burning in neutron star envelopes &amp; new temperature data of the Cas A neutron star, which may be cooling in real time! <LINK> <LINK>"", 'I am planning to write a short blog post, but was too excited to share already. In short: We studied the effect of diffusive nuclear burning in the envelope (very low density region of the star), which is typically assumed to be static in temperature evolution (cooling) studies.', 'But this low density region is important because it sets the relation between the observed surface temperature and the inferred interior (where all the funky fun dense matter is) temperature.', 'Diffusive nuclear burning can be important in this regard in two ways: 1) It can alter the static temperature relations between surface and interior. 2) It can change the envelope composition (which sets this relation) over time as light elements are consumed by nuclear burning.', 'We calculated new temperature relations that take diffusive nuclear burning into account and use the burning rates to calculate possible time-evolving envelope compositions for different accretion scenarios.', 'We also apply this to new temperature data of the Cassiopeia A neutron star, for which it was previously found that it may be cooling in real time! But data from different detectors resulted in inconsistent cooling rates, indicating contamination issues.', 'The new data analysis indicates a cooling rate of 2% over 10 years, which is less steep than previously found but more in line with observations using different detectors. The fast temperature decline can still be modelled with similar neutron superfluid properties as before.', ""Lot's more detailled work to do on all this, but this was a fun project! If anything, to show that inferring neutron star interior properties from surface temperature observations is exciting, but challenging. Can't get away with ignoring the low density stuff. 🙂"", '@samayanissanke @DrDa5id Thank you! 😄', '@astro_Liz 😁😃 Thank you!!', '@IvovanVulpen Dankjewel! 🥂', '@slavkobogdanov Thank you!']",https://arxiv.org/abs/1901.01012,"A critical relation in the study of neutron star cooling is the one between surface temperature and interior temperature. This relation is determined by the composition of the neutron star envelope and can be affected by the process of diffusive nuclear burning (DNB), which occurs when elements diffuse to depths where the density and temperature are sufficiently high to ignite nuclear burning. We calculate models of H-He and He-C envelopes that include DNB and obtain analytic temperature relations that can be used in neutron star cooling simulations. We find that DNB can lead to a rapidly changing envelope composition and prevents the build-up of thermally stable hydrogen columns y$_H$ > 10$^{7}$ g cm$^{-2}$, while DNB can make helium envelopes more transparent to heat flux for surface temperatures $T_s$ > 2 $\times 10^6$ K. We perform neutron star cooling simulations in which we evolve temperature and envelope composition, with the latter due to DNB and accretion from the interstellar medium. We find that a time-dependent envelope composition can be relevant for understanding the long-term cooling behaviour of isolated neutron stars. We also report on the latest Chandra observations of the young neutron star in the Cassiopeia A supernova remnant; the resulting 13 temperature measurements over more than 18 years yield a ten-year cooling rate of $\approx$ 2%. Finally, we fit the observed cooling trend of the Cassiopeia A neutron star with a model that includes DNB in the envelope. ","Diffusive nuclear burning in cooling simulations and application to new
  temperature data of the Cassiopeia A neutron star"
72,1082107145800298496,50901426,Rafael Alves Batista,"['In this new paper we compare the CRPropa and SimProp codes for cosmic-ray propagation. We study the cosmogenic neutrino, photon, and electron fluxes.\n<LINK>']",https://arxiv.org/abs/1901.01244,"The interactions of ultra-high-energy cosmic rays (UHECRs) in extragalactic space with photons of the cosmic microwave background (CMB) and extragalactic background light (EBL) can generate high-energy neutrinos and photons. Simulations of UHECR propagation require knowledge about physical quantities such as the spectrum of the EBL and photodisintegration cross sections. These assumptions, as well as the approximations used in the codes, may influence the computed UHECR spectrum and composition, and the associated cosmogenic neutrino and photon fluxes. Following up on our previous work where we studied the effects of these uncertainties on the UHECR spectrum and composition, here we quantify those on neutrino fluxes and production rates of photons, electrons, and positrons, using the Monte Carlo codes CRPropa and SimProp, in various astrophysical scenarios. We show that cosmogenic neutrinos are more sensitive to the choice of EBL model than UHECRs, whereas the overall cosmogenic gamma-ray production rates are relatively independent of propagation details. We also find significant differences between neutrino fluxes predicted by the latest released versions of CRPropa and SimProp, and discuss their causes and possible improvements in future versions of the codes. ",Secondary neutrino and gamma-ray fluxes from SimProp and CRPropa
73,1081452389180166144,372056477,Shirui Pan,"['Our review paper provides a new taxonomy, a comprehensive review,  open-source codes, practical applications, and future directions on this topic. <LINK> <LINK>']",https://arxiv.org/abs/1901.00596,"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field. ",A Comprehensive Survey on Graph Neural Networks
74,1081246452825972736,2766925212,Andrew Childs,['New paper from @yuansu_umd gives tight analysis of the performance of product formulas for simulating spatially local Hamiltonians <LINK>'],http://arxiv.org/abs/1901.00564,"We consider simulating an $n$-qubit Hamiltonian with nearest-neighbor interactions evolving for time $t$ on a quantum computer. We show that this simulation has gate complexity $(nt)^{1+o(1)}$ using product formulas, a straightforward approach that has been demonstrated by several experimental groups. While it is reasonable to expect this complexity---in particular, this was claimed without rigorous justification by Jordan, Lee, and Preskill---we are not aware of a straightforward proof. Our approach is based on an analysis of the local error structure of product formulas, as introduced by Descombes and Thalhammer and further simplified here. We prove error bounds for canonical product formulas, which include well-known constructions such as the Lie-Trotter-Suzuki formulas. We also develop a local error representation for time-dependent Hamiltonian simulation, and we discuss generalizations to periodic boundary conditions, constant-range interactions, and higher dimensions. Combined with a previous lower bound, our result implies that product formulas can simulate lattice Hamiltonians with nearly optimal gate complexity. ",Nearly optimal lattice simulation by product formulas
75,1080962165706641408,2285825876,Johanna,"['&lt;AHEM&gt; New paper out today! Led by \u2066@cabridelle\u2069, \u2066@M_N_Guenther\u2069 of \u2066@MIT\u2069, and friends (incl. me)!\u2069 [1901.00051] The Longest Period TESS Planet Yet: A Sub-Neptune Transiting A Bright, Nearby K Dwarf Star (1/2) <LINK>', 'Exciting b/c (1) 1st transiting planet system found amongst our @LCOAstro Magellan/PFS long-term RV survey for planets (we’ve been watching this star years!), (2) one planet is fairly long-period, was 1st only a single-transit detection, (3) smaller candidate planet is ~**1 R🌍**']",https://arxiv.org/abs/1901.00051,"The future of exoplanet science is bright, as TESS once again demonstrates with the discovery of its longest-period confirmed planet to date. We hereby present HD 21749b (TOI 186.01), a sub-Neptune in a 36-day orbit around a bright (V = 8.1) nearby (16 pc) K4.5 dwarf. TESS measures HD21749b to be 2.61$^{+0.17}_{-0.16}$ $R_{\oplus}$, and combined archival and follow-up precision radial velocity data put the mass of the planet at $22.7^{+2.2}_{-1.9}$ $M_{\oplus}$. HD 21749b contributes to the TESS Level 1 Science Requirement of providing 50 transiting planets smaller than 4 $R_{\oplus}$ with measured masses. Furthermore, we report the discovery of HD 21749c (TOI 186.02), the first Earth-sized ($R_p = 0.892^{+0.064}_{-0.058} R_{\oplus}$) planet from TESS. The HD21749 system is a prime target for comparative studies of planetary composition and architecture in multi-planet systems. ",TESS delivers its first Earth-sized planet and a warm sub-Neptune
76,1080830457112014853,702241209276829697,Cecilia Garraffo 💚,"['Cold and breezy but much milder than Proxima b and TRAPPIST-1 planets. Check our new paper on the space weather conditions of @BranardsStar_b. Orbital distance matters more than magnetic activity level! <LINK> @AstroRaikoh @cosmodrake @SofiaMoschou <LINK>', '@predictionmonk @AstroRaikoh @cosmodrake @SofiaMoschou If you like snow... ;)']",https://arxiv.org/abs/1901.00219,"A physically realistic stellar wind model based on Alfv\'en wave dissipation has been used to simulate the wind from Barnard's Star and to estimate the conditions at the location of its recently discovered planetary companion. Such models require knowledge of the stellar surface magnetic field that is currently unknown for Barnard's Star. We circumvent this by considering the observed field distributions of three different stars that constitute admissible magnetic proxies of this object. Under these considerations, Barnard's Star b experiences less intense wind pressure than the much more close-in planet Proxima~b and the planets of the TRAPPIST-1 system. The milder wind conditions are more a result of its much greater orbital distance rather than in differences in the surface magnetic field strengths of the host stars. The dynamic pressure experienced by the planet is comparable to present-day Earth values, but it can undergo variations by factors of several during current sheet crossings in each orbit. The magnetospause standoff distance would be $\sim$\,$20 - 40$\,\% smaller than that of the Earth for an equivalent planetary magnetic field strength. ",Breezing through the space environment of Barnard's Star b
77,1080799210239414272,2999702157,Anton Ilderton,"['New year, new #research projects, new paper on the conjectured breakdown of #QED in strong #laser fields! <LINK> @plym_math @PlymUni @PlymUniNews', ""Everyone's doing it :-) https://t.co/VufImQB12m""]",https://arxiv.org/abs/1901.00317,"Strong background fields require a non-perturbative treatment, which is afforded in QED by the Furry expansion of scattering amplitudes. It has been conjectured that this expansion breaks down for sufficiently strong fields, based on the asymptotic growth of loop corrections with increasing ""quantum nonlinearity"", essentially the product of field strength and particle energy. However, calculations to date have assumed that the background is constant. We show here, using general plane waves of finite duration, that observables at high quantum nonlinearity scale differently depending on whether intensity or energy is large. We find that, at high energy, loop contributions to observables tend to fall with increasing quantum nonlinearity, rather than grow. ","Note on the conjectured breakdown of QED perturbation theory in strong
  fields"
78,1091405766093955073,19819769,Eric Gilbert,"['New paper led by @im__jane showing that it\'s possible to computationally identify IRA-controlled accounts within a large set of ""typical"" Twitter accounts. Also, the model flags a number of currently active accounts likely under IRA control. <LINK>', '... this is work in collaboration with @eshwar_chan, @libbyh, @david__jurgens.', 'Another message of the paper: if we can do this well, the platforms can definitely do better.']",https://arxiv.org/abs/1901.11162,"There is evidence that Russia's Internet Research Agency attempted to interfere with the 2016 U.S. election by running fake accounts on Twitter - often referred to as ""Russian trolls"". In this work, we: 1) develop machine learning models that predict whether a Twitter account is a Russian troll within a set of 170K control accounts; and, 2) demonstrate that it is possible to use this model to find active accounts on Twitter still likely acting on behalf of the Russian state. Using both behavioral and linguistic features, we show that it is possible to distinguish between a troll and a non-troll with a precision of 78.5% and an AUC of 98.9%, under cross-validation. Applying the model to out-of-sample accounts still active today, we find that up to 2.6% of top journalists' mentions are occupied by Russian trolls. These findings imply that the Russian trolls are very likely still active today. Additional analysis shows that they are not merely software-controlled bots, and manage their online identities in various complex ways. Finally, we argue that if it is possible to discover these accounts using externally - accessible data, then the platforms - with access to a variety of private internal signals - should succeed at similar or better rates. ","Still out there: Modeling and Identifying Russian Troll Accounts on
  Twitter"
79,1090955796991696896,1070298087673995264,Lenz Belzner,"[""Here's our new paper on cooperative Multi-Agent Expert Iteration using open loop tree search, with potential applications in #industry40 and #logistics: <LINK>\n\nLet us know what you think! #ai #reinforcementlearning @MaibornWolff <LINK>""]",https://arxiv.org/abs/1901.08761,"Decision making in multi-agent systems (MAS) is a great challenge due to enormous state and joint action spaces as well as uncertainty, making centralized control generally infeasible. Decentralized control offers better scalability and robustness but requires mechanisms to coordinate on joint tasks and to avoid conflicts. Common approaches to learn decentralized policies for cooperative MAS suffer from non-stationarity and lacking credit assignment, which can lead to unstable and uncoordinated behavior in complex environments. In this paper, we propose Strong Emergent Policy approximation (STEP), a scalable approach to learn strong decentralized policies for cooperative MAS with a distributed variant of policy iteration. For that, we use function approximation to learn from action recommendations of a decentralized multi-agent planning algorithm. STEP combines decentralized multi-agent planning with centralized learning, only requiring a generative model for distributed black box optimization. We experimentally evaluate STEP in two challenging and stochastic domains with large state and joint action spaces and show that STEP is able to learn stronger policies than standard multi-agent reinforcement learning algorithms, when combining multi-agent open-loop planning with centralized function approximation. The learned policies can be reintegrated into the multi-agent planning process to further improve performance. ","Distributed Policy Iteration for Scalable Approximation of Cooperative
  Multi-Agent Policies"
80,1090513499497721856,874409930,Jiwei Li,"[""Excited to present <LINK>'s Glyce model, the Glyph-vectors for Chinese Character Representations. Glyce treats Chinese characters as images, and sets the new standards for 13 (almost all) NLP tasks in Chinese! \n\nPaper Link: <LINK>\n\n@stanfordnlp <LINK>""]",https://arxiv.org/abs/1901.10125,"It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\% on the Fudan corpus for text classification. Code found at this https URL ",Glyce: Glyph-vectors for Chinese Character Representations
81,1090234814379294720,1088407134461665282,Wojciech Czarnecki,['Excited to share the new paper on open-ended learning in zero-sum non-transitive games (e.g. poker or SC) <LINK> <LINK>'],https://arxiv.org/abs/1901.08106,"Zero-sum games such as chess and poker are, abstractly, functions that evaluate pairs of agents, for example labeling them `winner' and `loser'. If the game is approximately transitive, then self-play generates sequences of agents of increasing strength. However, nontransitive games, such as rock-paper-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this paper, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adaptive sequences of objectives that yield open-ended learning. The framework allows us to reason about population performance in nontransitive games, and enables the development of a new algorithm (rectified Nash response, PSRO_rN) that uses game-theoretic niching to construct diverse populations of effective agents, producing a stronger set of agents than existing algorithms. We apply PSRO_rN to two highly nontransitive resource allocation games and find that PSRO_rN consistently outperforms the existing alternatives. ",Open-ended Learning in Symmetric Zero-sum Games
82,1088911380101689344,933084565895286786,Dan Hooper,"[""I'm excited about our new paper, lead by my graduate student and collaborator Carlos Blanco (@yoMrBlanco). It turns out that coherent neutrino scattering experiments are very well suited to testing the LSND and MiniBooNE anomalies.\n<LINK>\n#neutrinos <LINK>""]",https://arxiv.org/abs/1901.08094,"Results from the LSND and MiniBooNE experiments have been interpreted as evidence for a sterile neutrino with a mass near the electronvolt scale. Here we propose to test such a scenario by measuring the coherent elastic scattering rate of neutrinos from a pulsed spallation source. Coherent scattering is universal across all active neutrino flavors, and thus can provide a measurement of the total Standard Model neutrino flux. By performing measurements over different baselines and making use of timing information, it is possible to significantly reduce the systematic uncertainties and to independently measure the fluxes of neutrinos that originate as $\nu_{\mu}$ or as either $\nu_e$ or $\bar{\nu}_{\mu}$. We find that a 100 kg CsI detector would be sensitive to the large fraction of the sterile neutrino parameter space that could potentially account for the LSND and MiniBooNE anomalies. ","Constraining Sterile Neutrino Interpretations of the LSND and MiniBooNE
  Anomalies with Coherent Neutrino Scattering Experiments"
83,1087719469542240256,338526004,Sam Bowman,"[""New paper with Alex Warstadt that I'm excited about: Using acceptability judgments from published linguistics literature to evaluate sentence encoders. (<LINK>) <LINK>""]",https://arxiv.org/abs/1901.03438,"Recent work on evaluating grammatical knowledge in pretrained sentence encoders gives a fine-grained view of a small number of phenomena. We introduce a new analysis dataset that also has broad coverage of linguistic phenomena. We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al. We find that these models have a strong command of complex or non-canonical argument structures like ditransitives (Sue gave Dan a book) and passives (The book was read). Sentences with long distance dependencies like questions (What do you think I ate?) challenge all models, but for these, BERT and GPT have a distinct advantage over the baseline. We conclude that recent sentence encoders, despite showing near-human performance on acceptability classification overall, still fail to make fine-grained grammaticality distinctions for many complex syntactic structures. ","Linguistic Analysis of Pretrained Sentence Encoders with Acceptability
  Judgments"
84,1087289279011848194,2374143056,Achim Ahrens,"['New working paper with @markeschaffer and Christian Hansen introducing our @Stata package for lasso, ridge and elastic net regression. Available here: <LINK>']",https://arxiv.org/abs/1901.05397,"This article introduces lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso and post-estimation OLS. The methods are suitable for the high-dimensional setting where the number of predictors $p$ may be large and possibly greater than the number of observations, $n$. We offer three different approaches for selecting the penalization (`tuning') parameters: information criteria (implemented in lasso2), $K$-fold cross-validation and $h$-step ahead rolling cross-validation for cross-section, panel and time-series data (cvlasso), and theory-driven (`rigorous') penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performance of the penalization approaches. ","lassopack: Model selection and prediction with regularized regression in
  Stata"
85,1083409548646338561,289461078,John B. Holbein,"['Use difference-in-difference?\n\nWant to know how susceptible your diff-in-diff estimates are to bias from hidden confounders?\n\nCheck out this new working paper: ""Patterns of Effects and Sensitivity Analysis for Differences-in-Differences""\n\n#SocSciResearch\n\n<LINK> <LINK>']",https://arxiv.org/abs/1901.01869,"Applied analysts often use the differences-in-differences (DID) method to estimate the causal effect of policy interventions with observational data. The method is widely used, as the required before and after comparison of a treated and control group is commonly encountered in practice. DID removes bias from unobserved time-invariant confounders. While DID removes bias from time-invariant confounders, bias from time-varying confounders may be present. Hence, like any observational comparison, DID studies remain susceptible to bias from hidden confounders. Here, we develop a method of sensitivity analysis that allows investigators to quantify the amount of bias necessary to change a study's conclusions. Our method operates within a matched design that removes bias from observed baseline covariates. We develop methods for both binary and continuous outcomes. We then apply our methods to two different empirical examples from the social sciences. In the first application, we study the effect of changes to disability payments in Germany. In the second, we re-examine whether election day registration increased turnout in Wisconsin. ","Patterns of Effects and Sensitivity Analysis for
  Differences-in-Differences"
86,1083009063972163584,933084565895286786,Dan Hooper,"['(1/6) My Collaborators (Miguel Escudero, @GordanKrnjaic and Mathias Pierre) and I just posted a new paper that I am excited about, ""Cosmology With a Very Light L_mu - L_tau Gauge Boson"", <LINK>\n\nLet me take a minute to walk you through the main points.\n#cosmology', ""(2/6) We considered a new light particle, a Z' associated with a broken U(1)_mu-tau symmetry, and solved the full set of Boltmann equations to determine how such a particle would impact the energy density in neutrinos and other radiation in the early universe. \n#cosmology"", ""(3/6) We found two regions of parameter space that change the expansion history in a way that would help to reconcile the discrepancy between local and cosmological measurements of the Hubble constant. The first of these is the well known region with m_Z' ~ 10-30 MeV."", ""(4/6) But to my surprise, we also found a second region that can address this issue. This region is a huge PLATEAU of parameter space with a light Z' and a small coupling of g_mu-tau ~10^-9 to 10^-13. Across this region, we find Delta N_eff ~ 0.21."", '(5/6)  In other words, this model predicts a value of N_eff that can  address the observed Hubble tension across a wide range of parameter  space. Although plenty of other models can address this problem, this one can do it without any careful tuning of the parameters.', ""(6/6) The reason that this plateau exists is that for this range of parameters the Z' does not reach equilibrium at early times, but undergoes freeze-in, ultimately reaching equilibrium with the neutrinos, but only AFTER the neutrinos have decoupled from thermal bath."", '@ecopenhaver To clarify, the plateau is not in a potential, but in the parameter space of the model.', '@Mitokochan It’s very weakly interacting, so it would be produced very rarely in accelerators.', '@Mitokochan The searches are underway. But there is more than mass that determines how easy or difficult it is to create and observe a particle at an accelerator. This particle could be very difficult indeed.']",https://arxiv.org/abs/1901.02010,"In this paper, we explore in detail the cosmological implications of an abelian $L_\mu-L_\tau$ gauge extension of the Standard Model featuring a light and weakly coupled $Z'$. Such a scenario is motivated by the longstanding $\sim \, 4 \sigma$ discrepancy between the measured and predicted values of the muon's anomalous magnetic moment, $(g-2)_\mu$, as well as the tension between late and early time determinations of the Hubble constant. If sufficiently light, the $Z'$ population will decay to neutrinos, increasing the overall energy density of radiation and altering the expansion history of the early universe. We identify two distinct regions of parameter space in this model in which the Hubble tension can be significantly relaxed. The first of these is the previously identified region in which a $\sim \, 10-20$ MeV $Z'$ reaches equilibrium in the early universe and then decays, heating the neutrino population and delaying the process of neutrino decoupling. For a coupling of $g_{\mu-\tau} \simeq (3-8) \times 10^{-4}$, such a particle can also explain the observed $(g-2)_{\mu}$ anomaly. In the second region, the $Z'$ is very light ($m_{Z'} \sim 1\,\text{eV}$ to $\text{MeV}$) and very weakly coupled ($g_{\mu-\tau} \sim 10^{-13}$ to $10^{-9}$). In this case, the $Z'$ population is produced through freeze-in, and decays to neutrinos after neutrino decoupling. Across large regions of parameter space, we predict a contribution to the energy density of radiation that can appreciably relax the reported Hubble tension, $\Delta N_{\rm eff} \simeq 0.2$. ",Cosmology With a Very Light $L_\mu - L_\tau$ Gauge Boson
87,1080794199136587777,65137727,Jonathan Mboyo Esole,"[""Our new paper on the geometry of the E7 model is out in collaboration with Sabrina Pasterski (@Harvard).  It is also my first New Year's Eve paper! It was posted a few minutes before midnight on New Year's Eve. \n<LINK>\n\n@NextEinsteinFor""]",https://arxiv.org/abs/1901.00093,We study the geography of crepant resolutions of E$_7$-models. An E$_7$-model is a Weierstrass model corresponding to the output of Step 9 of Tate's algorithm characterizing the Kodaira fiber of type III$^*$ over the generic point of a smooth prime divisor. The dual graph of the Kodaira fiber of type III$^*$ is the affine Dynkin diagram of type E$_7$. A Weierstrass model of type E$_7$ is conjectured to have eight distinct crepant resolutions whose flop diagram is a Dynkin diagram of type E$_8$. We construct explicitly four of these eight crepant resolutions forming a sub-diagram of type D$_4$. We explain how the flops between these four crepant resolutions can be understood using the flops between the crepant resolutions of two well-chosen suspended pinch points. ,D$_4$-flops of the E$_7$-model
88,1092450739035738112,2237355018,Elise van der Pol,"['Our recent paper, Hyperspherical Prototype Networks is available on Arxiv (<LINK>). We propose a class of deep networks that employ hyperspheres as output spaces with a priori defined structures.  Joint work with Pascal Mettes and @cgmsnoek.']",https://arxiv.org/abs/1901.10514,"This paper introduces hyperspherical prototype networks, which unify classification and regression with prototypes on hyperspherical output spaces. For classification, a common approach is to define prototypes as the mean output vector over training examples per class. Here, we propose to use hyperspheres as output spaces, with class prototypes defined a priori with large margin separation. We position prototypes through data-independent optimization, with an extension to incorporate priors from class semantics. By doing so, we do not require any prototype updating, we can handle any training size, and the output dimensionality is no longer constrained to the number of classes. Furthermore, we generalize to regression, by optimizing outputs as an interpolation between two prototypes on the hypersphere. Since both tasks are now defined by the same loss function, they can be jointly trained for multi-task problems. Experimentally, we show the benefit of hyperspherical prototype networks for classification, regression, and their combination over other prototype methods, softmax cross-entropy, and mean squared error approaches. ",Hyperspherical Prototype Networks
89,1091151913788133376,481288361,Joshua Batson,"[""I'm happy to share Noise2Self, a framework for blind-denoising high dimensional measurements. We calibrate classical image denoisers and train deep neural nets; the same idea works on matrices of single-cell gene expression.\n\nFind out how: <LINK>\n\nw/ @loicaroyer <LINK>"", 'There is always a tradeoff with denoising: too much and you blur out the signal, too little and you keep the noise. How do you find the right amount? https://t.co/x1PWo6Ihol', 'With self-supervision! https://t.co/8n98F1IvaB', 'This extends the cool work of @jaakkolehtinen et al on using pairs of noisy images to train neural nets to any measurement exhibiting some statistical independence in the noise.', 'Because of conf paper limits, the manuscript is a bit terse in places. Planning to unpack the single-cell story a bit for the bio community, but in the meanwhile feel free to ask questions by email or DM. Happy to brainstorm applications to other domains too!', '@jbloom22 Somehow I liked J for a partition. Could have been P? But my subconscious must have been playing the throwback game.', '@313V Thanks! Let me know how it goes.', 'Oh yeah, if you want to try this yourself, I have some demo code with a simple dataset (MNIST), noise (Gaussian), and model (small UNet). Runs in &lt; 1 min on a laptop. Also useful for those who find code easier to read than equations.\n\nhttps://t.co/dtFh94BcKj https://t.co/Q5nKZdH82h']",https://arxiv.org/abs/1901.11365,"We propose a general framework for denoising high-dimensional measurements which requires no prior on the signal, no estimate of the noise, and no clean training data. The only assumption is that the noise exhibits statistical independence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (""$\mathcal{J}$-invariant""), it is then possible to estimate the performance of a denoiser from noisy data alone. This allows us to calibrate $\mathcal{J}$-invariant versions of any parameterised denoising algorithm, from the single hyperparameter of a median filter to the millions of weights of a deep neural network. We demonstrate this on natural image and microscopy data, where we exploit noise independence between pixels, and on single-cell gene expression data, where we exploit independence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization. ",Noise2Self: Blind Denoising by Self-Supervision
90,1090629180855853056,835623319097454598,Krishnaswamy Lab,"['Another preprint from our lab by @david_van_dijk and @DBBurkhardt on using neural networks to find a latent space that has natural archetypes (AAnet). We apply AAnet to TILs and microbiota, inspired by the work of @UriAlonWeizmann . Arxiv link here <LINK> <LINK>', ""@SMukherjee89 @david_van_dijk @DBBurkhardt @UriAlonWeizmann Thanks. We'll take a look.""]",https://arxiv.org/abs/1901.09078,"Archetypal analysis is a data decomposition method that describes each observation in a dataset as a convex combination of ""pure types"" or archetypes. These archetypes represent extrema of a data space in which there is a trade-off between features, such as in biology where different combinations of traits provide optimal fitness for different environments. Existing methods for archetypal analysis work well when a linear relationship exists between the feature space and the archetypal space. However, such methods are not applicable to systems where the feature space is generated non-linearly from the combination of archetypes, such as in biological systems or image transformations. Here, we propose a reformulation of the problem such that the goal is to learn a non-linear transformation of the data into a latent archetypal space. To solve this problem, we introduce Archetypal Analysis network (AAnet), which is a deep neural network framework for learning and generating from a latent archetypal representation of data. We demonstrate state-of-the-art recovery of ground-truth archetypes in non-linear data domains, show AAnet can generate from data geometry rather than from data density, and use AAnet to identify biologically meaningful archetypes in single-cell gene expression data. ",Finding Archetypal Spaces Using Neural Networks
91,1087350548817461250,185910194,Graham Neubig,"['#ICLR2019 paper ""Lagging Inference Networks and Posterior Collapse in VAEs"". VAEs collapse to trivial solutions; we find this is because the inference network is poor at the beginning of training, then propose a simple solution of ""aggressive update"": <LINK> <LINK>', 'Nice work by @junxian_he, along with @dspoka, me, and @BergKirkpatrick!', '@sam_havens @poolio I think Section 3.1 of this paper is a reasonably clear explanation: https://t.co/2sHCU1atD8']",https://arxiv.org/abs/1901.05534,"The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as ""posterior collapse"" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster. ","Lagging Inference Networks and Posterior Collapse in Variational
  Autoencoders"
92,1087295802421194752,71332740,Dr Gwenllian Williams,['A paper I worked on is out today!🌟 We studied how a number of different techniques used to study the break-up of real filaments into cores behave when applied to fake filaments with a known fragmentation scale. We found some methods better than others! &gt; <LINK> <LINK>'],https://arxiv.org/abs/1901.06205,"Theories suggest that filament fragmentation should occur on a characteristic fragmentation length-scale. This fragmentation length-scale can be related to filament properties, such as the width and the dynamical state of the filament. Here we present a study of a number of fragmentation analysis techniques applied to filaments, and their sensitivity to characteristic fragmentation length-scales. We test the sensitivity to both single-tier and two-tier fragmentation, i.e. when the fragmentation can be characterised with one or two fragmentation length-scales respectively. The nearest neighbour separation, minimum spanning tree separation and two-point correlation function are all able to robustly detect characteristic fragmentation length-scales. The Fourier power spectrum and the Nth nearest neighbour technique are both poor techniques, and require very little scatter in the core spacings for the characteristic length-scale to be successfully determined. We develop a null hypothesis test to compare the results of the nearest neighbour and minimum spanning tree separation distribution with randomly placed cores. We show that a larger number of cores is necessary to successfully reject the null hypothesis if the underlying fragmentation is two-tier, N>20. Once the null is rejected we show how one may decide if the observed fragmentation is best described by single-tier or two-tier fragmentation, using either Akaike's information criterion or the Bayes factor. The analysis techniques, null hypothesis tests, and model selection approaches are all included in a new open-source Python/C library called FragMent. ","Determining the presence of characteristic fragmentation length-scales
  in filaments"
93,1085900823073312768,749337334303383552,Mark Humphries,"['We\'ve a fun new pre-print on how to find out if there is ""interesting"" structure in your network\n\nAKA finding out if and how your data network departs from your specified null model\n\n(With @jacaballeroo @mathewe @simaggi1 Abhinav Singh)\n\nThread...\n<LINK>', '1/ We use a spectral approach: we estimate the distribution of eigenvalues expected under the null model (using generative models), and compare that to the distribution in the actual data network\n\nData eigenvalues exceeding the predicted null model bounds = ""interesting"" https://t.co/GO4WZDdLke', '2/ The corresponding bound-exceeding eigenvectors give us a handy low-dimensional representation of the data network, allowing us to do two things:\n(i) find the ""interesting"" structure, by clustering (colours)\n(ii) reject nodes not contributing to the interesting structure (grey) https://t.co/qOmeZsqXoS', '3/ We show this works well on a range of synthetic networks with planted modules:\n* correctly rejects synthetic networks without modules\n* correctly finds the modules\n* even does well at pulling hidden modules out of noise', '4/ Applied to real data, we show conclusions about the structure present in a real network can dramatically depend on:\n(i) whether or not we apply our spectral rejection approach before further analysis \n(ii) the choice of null model itself', '5/ On the Allen Mouse Brain atlas of gene expression, our spectral rejection approach does a lovely job of clustering 625 voxels of gene expression into just 5 meaningful divisions of the entire brain https://t.co/kgVhuou5ym', '6/ and when we analyse the dialogue networks of Star Wars Episodes 1-6, we find the prequels have a strongly modular narrative structure, almost as though they were written by a robot... /Fin https://t.co/gtb5BeD4r3']",https://arxiv.org/abs/1901.04747,"Discovering low-dimensional structure in real-world networks requires a suitable null model that defines the absence of meaningful structure. Here we introduce a spectral approach for detecting a network's low-dimensional structure, and the nodes that participate in it, using any null model. We use generative models to estimate the expected eigenvalue distribution under a specified null model, and then detect where the data network's eigenspectra exceed the estimated bounds. On synthetic networks, this spectral estimation approach cleanly detects transitions between random and community structure, recovers the number and membership of communities, and removes noise nodes. On real networks spectral estimation finds either a significant fraction of noise nodes or no departure from a null model, in stark contrast to traditional community detection methods. Across all analyses, we find the choice of null model can strongly alter conclusions about the presence of network structure. Our spectral estimation approach is therefore a promising basis for detecting low-dimensional structure in real-world networks, or lack thereof. ","Spectral estimation for detecting low-dimensional structure in networks
  using arbitrary null models"
94,1085816195570417664,1910301474,MartinWeides,"['How stable are superconducting quantum circuits? We quantify and correlate their drifts in coherence and frequencies, and relate to the microscopic origin of the intrinsic decoherence mechanisms. Find out more: <LINK>']",https://arxiv.org/abs/1901.05352,"We report on long-term measurements of a highly coherent, non-tunable superconducting transmon qubit, revealing low-frequency burst noise in coherence times and qubit transition frequency. We achieve this through a simultaneous measurement of the qubit's relaxation and dephasing rate as well as its resonance frequency. The analysis of correlations between these parameters yields information about the microscopic origin of the intrinsic decoherence mechanisms in Josephson qubits. Our results are consistent with a small number of microscopic two-level systems located at the edges of the superconducting film, which is further confirmed by a spectral noise analysis. ","Correlating decoherence in transmon qubits: Low frequency noise by
  single fluctuators"
95,1085573198316429313,838292815,Ofir Nachum,['My new paper on learning fair machine learning classifiers: <LINK>\nWe frame the problem as trying to learn with respect to unknown (and true) labels despite only having access to observed (and biased) labels.  We find a surprisingly simple solution for doing so!'],https://arxiv.org/abs/1901.04966,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification. ",Identifying and Correcting Label Bias in Machine Learning
96,1083877101395214337,264501255,Eric Horvitz,"['While there are many theories, #humor and its link to human #cognition remains a mystery. What might we learn about #satire--and what people find funny--by transforming humor back to serious? <LINK> @cervisiarius @EPFL_en @MSFTResearch @RealAAAI @TheOfficialACM']",https://arxiv.org/abs/1901.03253,"Humor is an essential human trait. Efforts to understand humor have called out links between humor and the foundations of cognition, as well as the importance of humor in social engagement. As such, it is a promising and important subject of study, with relevance for artificial intelligence and human-computer interaction. Previous computational work on humor has mostly operated at a coarse level of granularity, e.g., predicting whether an entire sentence, paragraph, document, etc., is humorous. As a step toward deep understanding of humor, we seek fine-grained models of attributes that make a given text humorous. Starting from the observation that satirical news headlines tend to resemble serious news headlines, we build and analyze a corpus of satirical headlines paired with nearly identical but serious headlines. The corpus is constructed via Unfun.me, an online game that incentivizes players to make minimal edits to satirical headlines with the goal of making other players believe the results are serious headlines. The edit operations used to successfully remove humor pinpoint the words and concepts that play a key role in making the original, satirical headline funny. Our analysis reveals that the humor tends to reside toward the end of headlines, and primarily in noun phrases, and that most satirical headlines follow a certain logical pattern, which we term false analogy. Overall, this paper deepens our understanding of the syntactic and semantic structure of satirical news headlines and provides insights for building humor-producing systems. ","Reverse-Engineering Satire, or ""Paper on Computational Humor Accepted
  Despite Making Serious Advances"""
97,1083375276149690368,151193108,Mert R. Sabuncu 🇺🇦,['We propose a learning-based approach to optimize the sub-sampling pattern in compressed sensing and demonstrate its utility for MRI. Check it out: <LINK>'],https://arxiv.org/abs/1901.01960,"Acquisition of Magnetic Resonance Imaging (MRI) scans can be accelerated by under-sampling in k-space (i.e., the Fourier domain). In this paper, we consider the problem of optimizing the sub-sampling pattern in a data-driven fashion. Since the reconstruction model's performance depends on the sub-sampling pattern, we combine the two problems. For a given sparsity constraint, our method optimizes the sub-sampling pattern and reconstruction model, using an end-to-end learning strategy. Our algorithm learns from full-resolution data that are under-sampled retrospectively, yielding a sub-sampling pattern and reconstruction model that are customized to the type of images represented in the training data. The proposed method, which we call LOUPE (Learning-based Optimization of the Under-sampling PattErn), was implemented by modifying a U-Net, a widely-used convolutional neural network architecture, that we append with the forward model that encodes the under-sampling process. Our experiments with T1-weighted structural brain MRI scans show that the optimized sub-sampling pattern can yield significantly more accurate reconstructions compared to standard random uniform, variable density or equispaced under-sampling schemes. The code is made available at: this https URL . ",Learning-based Optimization of the Under-sampling Pattern in MRI
98,1082107145800298496,50901426,Rafael Alves Batista,"['In this new paper we compare the CRPropa and SimProp codes for cosmic-ray propagation. We study the cosmogenic neutrino, photon, and electron fluxes.\n<LINK>']",https://arxiv.org/abs/1901.01244,"The interactions of ultra-high-energy cosmic rays (UHECRs) in extragalactic space with photons of the cosmic microwave background (CMB) and extragalactic background light (EBL) can generate high-energy neutrinos and photons. Simulations of UHECR propagation require knowledge about physical quantities such as the spectrum of the EBL and photodisintegration cross sections. These assumptions, as well as the approximations used in the codes, may influence the computed UHECR spectrum and composition, and the associated cosmogenic neutrino and photon fluxes. Following up on our previous work where we studied the effects of these uncertainties on the UHECR spectrum and composition, here we quantify those on neutrino fluxes and production rates of photons, electrons, and positrons, using the Monte Carlo codes CRPropa and SimProp, in various astrophysical scenarios. We show that cosmogenic neutrinos are more sensitive to the choice of EBL model than UHECRs, whereas the overall cosmogenic gamma-ray production rates are relatively independent of propagation details. We also find significant differences between neutrino fluxes predicted by the latest released versions of CRPropa and SimProp, and discuss their causes and possible improvements in future versions of the codes. ",Secondary neutrino and gamma-ray fluxes from SimProp and CRPropa
99,1088658322247495681,154024287,Jake Clark 👻🎃,"[""#astronomyTwitter we've found some #exoplanets!! Four newly discovered big bois were uncovered by scouring through old archival data. Check out the @arxiv link for more: <LINK>\n\n#HD7449c, #HD65216c, #HD89744c and #HD92788c \n@usqedu, @UNSWPhysics @KutztownU <LINK>"", 'Oddly enough, the whole point of the paper was to see IF these singular eccentric exoplanetary systems were actually two planet systems on circular-resonant orbits. But, it seems like those exoplanets are indeed there not and masking as two.', ""But the big surprise came from orbital fitting routines returning long period fits. #HD7449c is a 19 Jupiter mass exoplanet (presumably a very successful exoplanet classed as a brown dwarf) on a ~42 year long orbit. #Saturn's orbit is only 29!"", ""So that brings us to THE SECOND PAPER ACCEPTED TODAY!!! Wooo. It turns out that two planets, in mutual mean-motion resonance could actually 'appear' to be a singular highly eccentric planet: https://t.co/vk8WcJ6Y4i"", ""Welp, let's get some physics up in this beast! We wanted to know at what eccentricities can this behaviour occur at. Is there a defined region where you can say with some certainty that it is INDEED a single planet AND is there a DANGER ZONE?"", ""And as it turns out, there is!!\nIf an exoplanet is discovered with an eccentricity greater than ~0.5, then it's most certainly a singular exoplanet. BUT, if your exoplanet has an eccentricity of around 0.2-0.4 then, my friend, you're in the ... https://t.co/R1v943IdCX"", '@ExoCytherean, @JontiHorner, have I missed anything here?']",https://arxiv.org/abs/1901.08471,"We examine eight known single-eccentric planetary systems in light of recently released large data archives and new analysis techniques. For four of these systems (HD 7449, HD 65216, HD 89744, HD 92788) we find evidence for additional long-period companions. HD 65216c is a Jupiter analog, with a period of 14.7 yr, $e=0.18$, and m sin $i$ of 2M_Jup, whilst the remaining candidate companions move on as-yet-incomplete orbits. Our results highlight the importance of revisiting the analysis of known exoplanetary systems when new data become available, particularly given the possibility that poorly-sampled data might previously have led to the detection of a 'false-positive' single eccentric planet, when the system in question actually contains two (or more) planets on near-circular orbits. ",Truly eccentric. I. Revisiting eight single-eccentric planetary systems
100,1088363887169359872,3232627976,Khyati Malhan,"['We found 8 new streams in the Inner galaxy (1-10 kpc) in the #GaiaDR2  catalogue. <LINK> (a/ R. Ibata, @nfmartin1980). Following map shows structures that lied at &gt;8sigma threshold,  corresponding to model [Fe/H]=-1.6 (colored in d_sun and Lz). @Fysikum  @TheOKC <LINK>', 'Here is shown the orbits of these 8 streams (the names are taken from Norse mythology, 8 of the 11 rivers that existed in Ginnungagap at the beginning of the world) https://t.co/oxi9jCkPm9', 'Comparison of the orbital properties (in terms of Lz vs r_peri)  of the Gaia globular clusters with the streams that we detect here. Properties of Fimbulthul stream are in particular quite interesting, as they lie very close to that of the  massive cluster ωCentauri. https://t.co/sEMlCE8gFM', 'With the continued improvements  in  the  accuracy  and  depth  of  @ESAGaia #Gaia data expected  over  the  coming  years  we  foresee  being  able to greatly extend the detection horizon for faint stellar streams  from  ancient  low-mass  progenitors.', '@ReadDark @nfmartin1980 @Fysikum @TheOKC Thankyou!  Its hard to find any overlapping between the detected structures here and any of the previously detected streams. The ones that did were identified separately by us (like GD-1, Jhelum). Please have a look at the Fig1 of the sister paper https://t.co/99i47Dd19k for ref)']",https://arxiv.org/abs/1901.07566,"We present the discovery of a large population of stellar streams that surround the inner Galaxy, found in the Gaia DR2 catalog using the new STREAMFINDER algorithm. Here we focus on the properties of eight new high-significance structures found at Heliocentric distances between 1 and 10 kpc and at Galactic latitudes $|b|>20\deg$, named Slidr, Sylgr, Ylgr, Fimbulthul, Sv\""ol, Fj\""orm, Gj\""oll and Leiptr. Spectroscopic measurements of seven of the streams confirm the detections, which are based on Gaia astrometry and photometry alone, and show that these streams are predominantly metal-poor. The sample possesses diverse orbital properties, although most of the streams appear to be debris of inner-halo globular clusters. Many more candidate streams are visible in our maps, but require follow-up spectroscopy to confirm their nature. We also explain in detail the workings of the algorithm, and gauge the incidence of false detections by running the algorithm on a smooth model of the Gaia catalog. ","The Streams of the Gaping Abyss: A population of entangled stellar
  streams surrounding the Inner Galaxy"
101,1083553238844985350,42846980,jason polakis,"['A cool @WIRED writeup by @issielapowsky on our upcoming @NDSSSymposium paper with @kostasdrk @panagiotisilia and @sotirisioannidi where we explore the privacy risks of public location metadata, and present a case study on Twitter.  <LINK> (1/5) <LINK>', 'In a nutshell, GPS coordinates from your geo-tagged tweets can be used to pinpoint the actual postal address of where you live and work, as well as infer sensitive information about you involving medical issues, religious beliefs, and sexual-related behavior.  (2/5)', 'To make matters worse, older versions of the Twitter app would include exact GPS coordinates in tweet metadata even when users geo-tagged tweets with very coarse location tags (e.g., ""New York City""). And while app versions after April 2015 are less invasive, older tweets  (3/5)', ""remain accessible through the public API. Users concerned about their older tweets can delete their historical Twitter location metadata here: https://t.co/3ievF6aP8y .Still, that won't delete that data from any third parties that have already collected it. (4/5)"", ""Furthermore, our techniques could potentially be used with users' location data collected from other sources, so users should be extra cautious with any website or mobile app that requests access to location data. Small correction: no AI techniques were used in our study. (5/5)""]",https://arxiv.org/abs/1901.00897,"The exposure of location data constitutes a significant privacy risk to users as it can lead to de-anonymization, the inference of sensitive information, and even physical threats. In this paper we present LPAuditor, a tool that conducts a comprehensive evaluation of the privacy loss caused by publicly available location metadata. First, we demonstrate how our system can pinpoint users' key locations at an unprecedented granularity by identifying their actual postal addresses. Our experimental evaluation on Twitter data highlights the effectiveness of our techniques which outperform prior approaches by 18.9%-91.6% for homes and 8.7%-21.8% for workplaces. Next we present a novel exploration of automated private information inference that uncovers ""sensitive"" locations that users have visited (pertaining to health, religion, and sex/nightlife). We find that location metadata can provide additional context to tweets and thus lead to the exposure of private information that might not match the users' intentions. We further explore the mismatch between user actions and information exposure and find that older versions of the official Twitter apps follow a privacy-invasive policy of including precise GPS coordinates in the metadata of tweets that users have geotagged at a coarse-grained level (e.g., city). The implications of this exposure are further exacerbated by our finding that users are considerably privacy-cautious in regards to exposing precise location data. When users can explicitly select what location data is published, there is a 94.6% reduction in tweets with GPS coordinates. As part of current efforts to give users more control over their data, LPAuditor can be adopted by major services and offered as an auditing tool that informs users about sensitive information they (indirectly) expose through location metadata. ","Please Forget Where I Was Last Summer: The Privacy Risks of Public
  Location (Meta)Data"
