,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1459239552963465216,315927239,Liangyuan Hu,"[""That's right. Check out our new sensitivity analysis methods paper for unmeasured confounding with multiple treatments, to appear in AOAS (<LINK>)! Software is available via the CIMTx R package (<LINK>) <LINK>""]",https://arxiv.org/abs/2012.06093,"In the absence of a randomized experiment, a key assumption for drawing causal inference about treatment effects is the ignorable treatment assignment. Violations of the ignorability assumption may lead to biased treatment effect estimates. Sensitivity analysis helps gauge how causal conclusions will be altered in response to the potential magnitude of departure from the ignorability assumption. However, sensitivity analysis approaches for unmeasured confounding in the context of multiple treatments and binary outcomes are scarce. We propose a flexible Monte Carlo sensitivity analysis approach for causal inference in such settings. We first derive the general form of the bias introduced by unmeasured confounding, with emphasis on theoretical properties uniquely relevant to multiple treatments. We then propose methods to encode the impact of unmeasured confounding on potential outcomes and adjust the estimates of causal effects in which the presumed unmeasured confounding is removed. Our proposed methods embed nested multiple imputation within the Bayesian framework, which allow for seamless integration of the uncertainty about the values of the sensitivity parameters and the sampling variability, as well as use of the Bayesian Additive Regression Trees for modeling flexibility. Expansive simulations validate our methods and gain insight into sensitivity analysis with multiple treatments. We use the SEER-Medicare data to demonstrate sensitivity analysis using three treatments for early stage non-small cell lung cancer. The methods developed in this work are readily available in the R package SAMTx. ","A flexible sensitivity analysis approach for unmeasured confounding with
  multiple treatments and a binary outcome with application to SEER-Medicare
  lung cancer data"
1,1459133055692587013,823277120944242689,Will Kinney,"['A thread on this new paper, with my PhD student Nina Stein.\n<LINK> 1/', 'An important question to as about cosmological models is: does the universe extend infinitely into the past, or is there a boundary, corresponding to a beginning of time? 2/', 'The standard Big Bang cosmology, for example, is past-finite, with the universe emerging from an initial singularity. 3/', 'More ambiguous is the inflationary universe, which postulates a period of exponential expansion BEFORE the Big Bang. Since the size of the universe is exponential in time, there is no obvious past boundary. You can take time to negative infinity with no problem. 4/', 'It turns out, though, that the inflationary universe is also past-finite, which was proved by Borde, Guth, and Vilenkin in a beautiful paper in 2001. 5/\nhttps://t.co/4mq8fx2oYt', 'This strikes many people as unsatisfying, which has led theorists to search for cyclic solutions, which repeat infinitely into both past and future. 6/', 'Cyclic cosmology has a big problem: the Second Law of thermodynamics. Enropy in the universe increases from cycle to cycle. This spoils the perfect periodicity required, which was realized by Tolman in a seminal paper in 1931: 7/\n\nhttps://t.co/1kPdin8PY0', ""This August, Ijjas and Steinhardt claimed to have solved Tolman's almost century-old entropy problem in cyclic cosmology. 8/\n\nhttps://t.co/ds8zTbEFQq"", 'We took a closer look. 9/', 'The way the IS model works is to construct a cosmology which expand exponentially at late time (infkation generated by dark energy), but turns around and re-contracts extremely slowly (ekpyrotic contraction). 10/', 'The rapid expansion dilutes the entropy in any finite patch of the universe effectively to zero, but the slow contraction means that the universe does not shrink in size again as it evolves to the big crunch. It stays big. 11/', 'This repeats indefinitely, so that the rate of expansion (Hubble parameter H) is periodic, but the size of the universe increases from cycle to cycle, diluting the entropy each time. 12/ https://t.co/IzVXvlmjDp', 'Since in a universe with a flat geometry, we are free to arbitrarily rescale what we mean by ""size"", the argument is that we can simply rescale lengths at each bounce, and the cosmology is perfectly periodic while still shedding entropy. 13/', 'That is, the growth of the universe is an irrelevant conformal factor, which can be scaled away. Clever! 14/', 'But us this really past-infinite? One way to make thus question precise is to ask whether the solution is geodesically complete? 15/', 'A ""geodesic"" is the path through spacetime of an inertial (weightless) observer. The time measured along that path by the observer is called the ""proper time"". 16/', 'For a spacetime manifold to be truly infinite, it must necessarily extend infinitely into the past and the future for ALL geodesic paths. This is called geodesic completeness. 16/', 'A spacetime that expands exponentially (called de Sitter space) is both past- and future-infinite for observers at rest relative to the expansion, but Borde, Guth, and Vilenkin (BGV) showed that there are an infinite number of other geodesics which are past-FINITE. 17/', 'Therefore the de Sitter space of inflationary cosmology is geodesically past-incomplete. Bummer. 18/', 'We asked: Is the Ijjas/Steinhardt cosmology geodesically complete? The answer is no. 19/', 'The proof is insanely simple: we constuct two de Sitter spaces which expand at the same average rate as the cylcic model from one bounce to the next. 20/ https://t.co/oIr1uxl61u', 'They are built so that the length of a geodesic in the cyclic case is bounded from below by one de Sitter solution, and bounded from above by the other. 21/ https://t.co/9aBU4HHCTg', ""That's it. We're done. Since the BGV theorem tells us that the length of the geodesic path is finite in the bounding de Sitter spaces, so too must it be finite in the cyclic universe. \n\nQED. 22/"", ""This is true for ANY cyclic universe which sheds entropy by growth between cycles, regardless of the details of the dynamics. The mechanism proposed by Ijjas and Steinhardt does not solve Tolman's entropy problem, it just re-casts it as a geodesically incomplete spacetime. 23/"", 'FIN 24/24 https://t.co/4DM788YVSC', '@LucaAmb We suspect, but have not yet proven, that the geodesic incompleteness problem and the entropy are in fact exactly the same problem.  Future work.', ""@GertvandenBer14 @skdh Good question. Don't know the answer."", '@AdkHomeroom Mostly the former, I think. See this piece ftom @StartsWithABang for example. (Ethan also argues here for a very different perspective on the BGV theorem.)\n\nhttps://t.co/5KbMkl6XVb', ""@ThomasVanRiet2 I don't think so. As long as one geodesic makes it through to past infinity, BGV still holds."", '@GertvandenBer14 @skdh @penrose My intuition is that our bound applies to Conformal Cyclic Cosmology as well, but proving that is another matter.', ""@mcwy Thanks Mark! It's nice having a smart grad student.""]",https://arxiv.org/abs/2110.15380,"We consider recently proposed bouncing cosmological models for which the Hubble parameter is periodic in time, but the scale factor grows from one cycle to the next as a mechanism for shedding entropy. Since the scale factor for a flat universe is equivalent to an overall conformal factor, it has been argued that this growth corresponds to a physically irrelevant rescaling, and such bouncing universes can be made perfectly cyclic, extending infinitely into the past and future. We show that any bouncing universe which uses growth of the scale factor to dissipate entropy must necessarily be geodesically past-incomplete, and therefore cannot be truly cyclic in time. ",Cyclic Cosmology and Geodesic Completeness
2,1458830297173221380,62604839,Paulo Simões,"['New paper alert! A solar flare driven by thermal conduction observed in mid-infrared, led by Fernando López. Happy with this analysis of IR and E-OVSA data. Soon in @AandA_journal \n@arXiv_BR @_arXiv_astro_ph \n<LINK>']",https://arxiv.org/abs/2110.15751,"The mid-infrared (mid-IR) range has been mostly unexplored for the investigation of solar flares. It is only recently that new mid-IR flare observations have begun opening a new window into the response and evolution of the solar chromosphere. These new observations have been mostly performed by the AR30T and BR30T telescopes that are operating in Argentina and Brazil, respectively. We present the analysis of SOL2019-05-15T19:24, a GOES class C2.0 solar flare observed at 30~THz (10$\ \mu$m) by the ground-based telescope AR30T. Our aim is to characterize the evolution of the flaring atmosphere and the energy transport mechanism in the context of mid-IR emission. We performed a multi-wavelength analysis of the event by complementing the mid-IR data with diverse ground- and space-based data from the Solar Dynamics Observatory (SDO), the H--$\alpha$ Solar Telescope for Argentina (HASTA), and the Expanded Owens Valley Solar Array (EOVSA). Our study includes the analysis of the magnetic field evolution of the flaring region and of the development of the flare. The mid-IR images from AR30T show two bright and compact flare sources that are spatially associated with the flare kernels observed in ultraviolet (UV) by SDO. We confirm that the temporal association between mid-IR and UV fluxes previously reported for strong flares is also observed for this small flare. The EOVSA microwave data revealed flare spectra consistent with thermal free-free emission, which lead us to dismiss the existence of a significant number of non-thermal electrons. We thus consider thermal conduction as the primary mechanism responsible for energy transport. Our estimates for the thermal conduction energy and total radiated energy fall within the same order of magnitude, reinforcing our conclusions. ",A solar flare driven by thermal conduction observed in mid-infrared
3,1455918753863159812,3043593248,Hazarapet Tunanyan,['#1 on @paperswithcode on 5 benchmark datasets. Achieving a new SOTA of salient object detection problem.\n\npaper: <LINK>\npaperswithcode: <LINK> <LINK>'],https://arxiv.org/abs/2110.11887,"Deep learning solutions of the salient object detection problem have achieved great results in recent years. The majority of these models are based on encoders and decoders, with a different multi-feature combination. In this paper, we show that feature concatenation works better than other combination methods like multiplication or addition. Also, joint feature learning gives better results, because of the information sharing during their processing. We designed a Complementary Extraction Module (CEM) to extract necessary features with edge preservation. Our proposed Excessiveness Loss (EL) function helps to reduce false-positive predictions and purifies the edges with other weighted loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding flow (G) makes the prediction more accurate by providing high-level complementary information to shallower layers. Experimental results show that the proposed model outperforms the state-of-the-art methods on all benchmark datasets under three evaluation metrics. ","C$^{4}$Net: Contextual Compression and Complementary Combination Network
  for Salient Object Detection"
4,1455857131123531783,1120650694644596737,Paris Avgeriou,"['Architecture smells cause major maintenance headaches. But, which smells do software practitioners  care about the most? And how do they deal with them in practice? For this &amp; more, read our new @ieeesoftware paper <LINK>\nOr the preprint <LINK>']",https://arxiv.org/abs/2110.06750,"Architectural Technical Debt (ATD) is considered as the most significant type of TD in industrial practice. In this study, we interview 21 software engineers and architects to investigate a specific type of ATD, namely architectural smells (AS). Our goal is to understand the phenomenon of AS better and support practitioners to better manage it and researchers to offer relevant support. The findings of this study provide insights on how practitioners perceive AS and how they introduce them, the maintenance and evolution issues they experienced and associated to the presence of AS, and what practices and tools they adopt to manage AS. ",The perception of Architectural Smells in industrial practice
5,1455664708032282624,3018222517,Samuel Stanton,"['Happy to share a new #NeurIPS2021 paper on scalable online Gaussian processes with Wesley Maddox and @andrewgwils \n\n<LINK>  (1/n) <LINK>', 'Online data collection (eg BayesOpt) relies on acquisition functions to choose new points \n\nBetter acq fns == smarter choices\n\nExact GPs are slow but work well with newer acq fns Sparse GPs are fast but harder to use (2/n)', 'We rederive and generalize streaming SGPR (proposed by @thdbui in https://t.co/osAuIpwq7x) to any variational GP, relying on Laplace approximations to extend to non-Gaussian likelihoods.\n\nAs a result we can combine SVGPs with lookahead acq. fns like knowledge gradient.\n (3/n)', 'To bring everything together we demonstrate that SVGPs can be very effective in the context of active learning, black-box optimization, and adaptive control. \n\ncode: https://t.co/p7T92QWPR6\n(4/n)']",https://arxiv.org/abs/2110.15172,"With a principled representation of uncertainty and closed form posterior updates, Gaussian processes (GPs) are a natural choice for online decision making. However, Gaussian processes typically require at least $\mathcal{O}(n^2)$ computations for $n$ training points, limiting their general applicability. Stochastic variational Gaussian processes (SVGPs) can provide scalable inference for a dataset of fixed size, but are difficult to efficiently condition on new data. We propose online variational conditioning (OVC), a procedure for efficiently conditioning SVGPs in an online setting that does not require re-training through the evidence lower bound with the addition of new data. OVC enables the pairing of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even with non-Gaussian likelihoods. We show OVC provides compelling performance in a range of applications including active learning of malaria incidence, and reinforcement learning on MuJoCo simulated robotic control tasks. ","Conditioning Sparse Variational Gaussian Processes for Online
  Decision-making"
6,1455646121334824969,110103071,Andrej Risteski,"['Not all losses are created equal: the Noise Contrastive Estimation (NCE) edition. Brand new paper with my amazing students @BingbinL and @ElanRosenfeld , and co-advisor Pradeep Ravikumar. Paper here: <LINK> Thread below.', 'The main idea behind NCE is to learn a distribution p given samples from it, by learning a distinguisher between samples from p and samples from a ""noise distribution"" q, typically trained using a log loss.', 'The main benefit of NCE over max likelihood is that if p has a difficult partition fn (e.g. lies in an exponential family), the partition function can be treated as an additional parameter. The optimal distinguisher can be shown to be log(p/q), so p can be recovered from it.', 'Empirically, if q is ""far"" from p, a ""density chasm"" was observed in https://t.co/navXECzRJ8: training w/ a reasonable computational budget doesn\'t work. We explore why this happens when p, q are in an exponential family (making the loss convex), and propose possible fixes.', '1) We show the main issue is a flat loss landscape and happens even for simple cases, e.g. p, q are Gaussians w/ unknown means. Formally, we show GD and Newton with standard choices of step size require an exponential (in diameter for the location of the means) number of steps.', '2) We show that *normalized* gradient descent can help if the loss is ""well conditioned"" --- i.e. the Hessian near the optimum has a good condition number. This helps in the case when p, q are not too far in the Bhattacharya distance sense.', '3) We show that changing from log loss to exp loss can help (we call this *eNCE*): for any exponential family, NGD converges in # of steps depending polynomially on the diameter of the parameters for the family, and a bound on 2nd and 3rd moments in the family.', '4) We empirically show (albeit on toy data, Gaussian and MNIST) that NGD performs better than GD both when using log and exp loss. Moreover, given a reasonable computational budget, exp loss generally enjoys a faster convergence. (Though tends to be numerically worse behaved.)', 'Plenty of space for future work for exploring the effects of different losses, in terms of behavior of landscape, statistical properties, numerical stability.']",https://arxiv.org/abs/2110.11271,"Noise-contrastive estimation (NCE) is a statistically consistent method for learning unnormalized probabilistic models. It has been empirically observed that the choice of the noise distribution is crucial for NCE's performance. However, such observations have never been made formal or quantitative. In fact, it is not even clear whether the difficulties arising from a poorly chosen noise distribution are statistical or algorithmic in nature. In this work, we formally pinpoint reasons for NCE's poor performance when an inappropriate noise distribution is used. Namely, we prove these challenges arise due to an ill-behaved (more precisely, flat) loss landscape. To address this, we introduce a variant of NCE called ""eNCE"" which uses an exponential loss and for which normalized gradient descent addresses the landscape issues provably when the target and noise distributions are in a given exponential family. ","Analyzing and Improving the Optimization Landscape of Noise-Contrastive
  Estimation"
7,1455640166006095873,1279209709912961025,Alex Robey,"['Excited to announce that our paper “Adversarial Robustness with Semi-Infinite Constrained Learning” was accepted at #NeurIPS2021.  In this paper, we propose a new, equivalent formulation for adversarial training!🚀\n\nPaper: <LINK>\nCode: <LINK>\n\n1/', 'You might be thinking: ""I\'ve seen so many papers on adversarial robustness recently.  What\'s new in this one?""\n\nWe feel the same way!  That\'s why in this paper our goal was to come up with a totally new perspective on the adversarial training problem.\n￼\n2/ https://t.co/Wd8I9DmQpp', 'Our first result is to show that the standard min-max adv. training problem is equivalent to a new, stochastic optimization problem over probability distributions λ.\n\n3/ https://t.co/uplpPhM1eJ', 'Unlike the original adv. training problem, this new problem is a *linear*, variational problem in λ.  Also, notice that the max is now *outside* both of the expectations!\n\n/4 https://t.co/KAFskIyf59', 'Question: What does this reformulation give us?  \n\nAnswer: Under mild conditions on the loss, we can derive a *closed-form solution* for the optimal distribution λ* to the maximization problem.\n\n/5 https://t.co/ByAIaJbPGK', 'Informally, this means that if we can sample from λ*, we can replace the maximization with an expectation.\n\nThus, we can solve a (much) more standard minimization problem rather than a challenging min-max problem.\n\n/6 https://t.co/3z2tMtohdt', 'However, sampling from this distribution λ* can be challenging.  Thus, we use classical tools from Hamiltonian Monte Carlo (HMC) to obtain approximate samples from λ*.\n\n/7 https://t.co/YdBg0A325I', 'The result: A new algorithm for training adversarially robust models, which we call \n\nDual Adversarial LEarning == DALE.\n\nDALE combines HMC sampling with a primal-dual iteration to mitigate the trade-off between robustness &amp; accuracy.\n\n/8 https://t.co/MVyM21jViB', 'We compare models trained with DALE and numerous other SOTA baselines.  Two takeaways:\n\n1. DALE beats all baselines in adv. acc. on CIFAR-10 by 2%.\n\n2. DALE can simultaneously achieve &gt;85% clean acc. &amp; 50% adv. acc. on CIFAR-10.  \n\n9/ https://t.co/7I6xMyggPN', 'Further experiments show that our primal-dual approach forces classifiers to better mitigate the trade-off between robustness and accuracy.\n\n/10 https://t.co/vkEmbQ0giL', ""This work wouldn't have been possible without my amazing collaborators: Luiz F. O. Chamon (co-first author), @pappasg69, @HamedSHassani, and Alejandro Ribeiro.\n\nIf you have any questions, feel free to reach out on Twitter or by email: arobey1@seas.upenn.edu\n\n/11 https://t.co/4pH5bqvkBq""]",https://arxiv.org/abs/2110.15767,"Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of adversarial training, particularly with respect to when and why adversarial training works. In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions, which we characterize completely. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Monte Carlo approach of which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10. Our code is available at: this https URL ",Adversarial Robustness with Semi-Infinite Constrained Learning
8,1455510784289689606,1193950453941383168,Iason Gabriel,"['Delighted to share a new paper on artificial intelligence and distributive justice today! \n\n<LINK>\n\nThank you to everyone who made it possible 🙏 <LINK>', 'In a nutshell 🌰: the basic structure of society is composite of socio-technical systems. AI plays an increasingly important role in these systems. Therefore, principles of distributive justice, including fair equality of opportunity and the difference principle, apply to AI.', 'Special thanks to @Abebab, @jhimmelreich, @wsisaac , @shakir_za, @weidingerlaura, @bsmith13, @AllanDafoe, @SeanLegassick, the reviewers at Daedelus, and the socio-technical research group at @DeepMind ✨']",https://arxiv.org/abs/2110.14419,"This paper explores the relationship between artificial intelligence and principles of distributive justice. Drawing upon the political philosophy of John Rawls, it holds that the basic structure of society should be understood as a composite of socio-technical systems, and that the operation of these systems is increasingly shaped and influenced by AI. As a consequence, egalitarian norms of justice apply to the technology when it is deployed in these contexts. These norms entail that the relevant AI systems must meet a certain standard of public justification, support citizens rights, and promote substantively fair outcomes -- something that requires specific attention be paid to the impact they have on the worst-off members of society. ",Towards a Theory of Justice for Artificial Intelligence
9,1455463802934280192,916940326132224000,Virginie Do,"['Happy to share our paper “Two-sided fairness in rankings via Lorenz dominance”, accepted to #NeurIPS2021 (w/ @scorbettdavies, J. Atif &amp; Nicolas Usunier) 🎊 We propose a new framework for fair recommendation, grounded in welfare economics. \n<LINK> 1/4', 'Our goal: improve the experience of the worse-off users and/or content producers. \nWe use Generalized Lorenz curves to understand who benefits or bears the cost of a fairness intervention. Rankings are considered *Lorenz efficient* if they produce non-dominated Lorenz curves. 2/4 https://t.co/Ctw2PLJRJr', 'This guarantees\n(a) Pareto efficiency\n(b) equity: utility is redistributed from better-off to worse-off, keeping total utility constant. 3/4', 'We generate rankings by maximizing concave welfare functions. No single state of ""perfect fairness"" here - rather a variety of acceptable tradeoffs, which choice is context-dependent. Our approach, unlike those based on fairness constraints, always satisfies Lorenz efficiency 4/4', '@facebookai @Paris_Dauphine @psl_univ @DauphineMiles @NeurIPSConf']",https://arxiv.org/abs/2110.15781,"We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility. ",Two-sided fairness in rankings via Lorenz dominance
10,1455351686109622272,803882049484398592,Takuya Yoshioka,"['Check our new paper on VarArray, a new ""array-geometry-agnostic"" speech separation model. Unlike conventional practices, we did end-to-end evaluation with different meeting corpora, public (AMI) and private, showing real WER impacts. \n\nPaper: <LINK> <LINK>', 'Also, in another paper from our summer intern Zhuohuang Zhang, we applied all-neural beamforming (with modifications) to address the overlapped speech problem in the real meeting transcription task. \n\nPaper: https://t.co/CTzN2iKSvT https://t.co/lIGV526Msl']",https://arxiv.org/abs/2110.05745,"Continuous speech separation using a microphone array was shown to be promising in dealing with the speech overlap problem in natural conversation transcription. This paper proposes VarArray, an array-geometry-agnostic speech separation neural network model. The proposed model is applicable to any number of microphones without retraining while leveraging the nonlinear correlation between the input channels. The proposed method adapts different elements that were proposed before separately, including transform-average-concatenate, conformer speech separation, and inter-channel phase differences, and combines them in an efficient and cohesive way. Large-scale evaluation was performed with two real meeting transcription tasks by using a fully developed transcription system requiring no prior knowledge such as reference segmentations, which allowed us to measure the impact that the continuous speech separation system could have in realistic settings. The proposed model outperformed a previous approach to array-geometry-agnostic modeling for all of the geometry configurations considered, achieving asclite-based speaker-agnostic word error rates of 17.5% and 20.4% for the AMI development and evaluation sets, respectively, in the end-to-end setting using no ground-truth segmentations. ",VarArray: Array-Geometry-Agnostic Continuous Speech Separation
11,1455180236971253764,725994666953428993,Chen Zhao,"['Training SOTA Open-Domain QA systems needs expensive evidence labels. Not for us, but we are equally good!\n\nExcited to share our new #EMNLP21 paper, Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation\n<LINK> \n(1/6) <LINK>', 'Instead of annotating evidence (i.e., passages or chain of passages) for training Open-Domain QA systems, we focus on a much cheaper weakly-supervised setting that only has question-answer pairs during training. \n(2/6)', 'We propose a hard-EM approach (DistDR) to find evidence as distant training signals. We iteratively improve over a weak retriever by alternately finding evidence from the up-to-date model (Hard E-step) and encouraging the model to learn the most likely evidence (M-step).\n(3/6) https://t.co/9SxvUEgcGh', 'DistDR is on par with fully-supervised state-of-the-art methods on both a multi-hop QA benchmark (HotpotQA) and a single-hop QA benchmark (NaturalQuestions). \n(4/6)', 'Our analysis on multi-hop QA further indicates that albeit some (~30%) predicted evidence differs from the annotation, it’s still helpful! \nAs DistDR can find alternative evidence; some questions only need a single passage as evidence (and DistDR finds it). \n(5/6) https://t.co/hbqhSq1x0W', 'Our code is at https://t.co/ALvGSDoDXb, EMNLP video link https://t.co/GoYm4EutMm. Joint work w/ @XiongChenyan , @boydgraber and @haldaume3. \n(6/6)']",https://arxiv.org/abs/2110.04889,"Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question-answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DistDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence. Without using any evidence labels, DistDR is on par with fully-supervised state-of-the-art methods on both multi-hop and single-hop QA benchmarks. Our analysis confirms that DistDR finds more accurate evidence over iterations, which leads to model improvements. ","Distantly-Supervised Evidence Retrieval Enables Question Answering
  without Evidence Annotation"
12,1455152986020356101,1534851038,Chronis Patapis,"['New paper on arXiv today: We explored the potential of #MIRI MRS  on board #JWST  for characterising the chemical composition of directly imaged (giant) exoplanet atmospheres.  \n<LINK>\n\n@AstroEvert @n_whiteford @adiglauser @AlistairGlasse @paulmolli\n \n1/3', 'The MIRI-MRS instrument provides spatially resolved spectroscopy, meaning you get an image where each pixel corresponds to a spectrum. With the spectral resolution provided we can disentangle molecular signatures of the planet atmosphere from the noise.\n2/3', ""This is a complimentary way of constraining the chemistry of the atmosphere that can be combined with other data. \nThe MRS will be a unique instrument since it operates in the mid-infrared wavelengths (5-28 um) where we don't have any observational evidence at the moment!\n3/3""]",https://arxiv.org/abs/2110.15756,"The Medium Resolution Spectrometer on board JWST/MIRI will give access to mid-IR spectra while retaining spatial information. With the unparalleled sensitivity of JWST and the MIRI detectors, the MRS has the potential to revolutionise our understanding of giant exoplanet atmospheres. Molecular mapping is a promising detection and characterisation technique used to study the spectra of directly imaged exoplanets. We aim to examine the feasibility and application of this technique to MRS observations. We used the instrument simulator MIRISIM to create mock observations of resolved star and exoplanet systems. As an input for the simulator, we used stellar and planet parameters from literature, with the planet spectrum being modelled with the radiative transfer code petitRADTRANS. After processing the raw data with the JWST pipeline, we high pass filter the data to account for the stellar point spread function, and used a forward modelling approach to detect the companions and constrain the chemical composition of their atmospheres through their molecular signatures. We identified limiting factors in spectroscopic characterisation of directly imaged exoplanets with the MRS and simulated observations of two representative systems, HR8799 and GJ504. In both systems, we could detect the presence of multiple molecules that were present in the input model of their atmospheres. We used two different approaches with single molecule forward models, used in literature, that are sensitive to detecting mainly H$_2$O, CO, CH$_4$, and NH$_3$, and a log-likelihood ratio test that uses full atmosphere forward models and is sensitive to a larger number of less dominant molecular species. We show that the MIRI MRS can be used to characterise widely separated giant exoplanets in the mid-IR using molecular mapping. ","Direct emission spectroscopy of exoplanets with the medium resolution
  imaging spectrometer on board JWST MIRI: I. Molecular mapping and sensitivity
  to instrumental effects"
13,1455119081368080384,823277120944242689,Will Kinney,"['New paper.\n<LINK>', ""@MarcusErve First question: Beats the hell out of me. \n\nSecond question: That's accomplished in these models with a very fine-tuned scalar field potential."", ""@Mat_Hunt True, but the BGV theorem does tell you that you can't avoid dealing with the quantum gravity limit.""]",https://arxiv.org/abs/2110.15380,"We consider recently proposed bouncing cosmological models for which the Hubble parameter is periodic in time, but the scale factor grows from one cycle to the next as a mechanism for shedding entropy. Since the scale factor for a flat universe is equivalent to an overall conformal factor, it has been argued that this growth corresponds to a physically irrelevant rescaling, and such bouncing universes can be made perfectly cyclic, extending infinitely into the past and future. We show that any bouncing universe which uses growth of the scale factor to dissipate entropy must necessarily be geodesically past-incomplete, and therefore cannot be truly cyclic in time. ",Cyclic Cosmology and Geodesic Completeness
14,1454193919508418564,52381876,Edward Frenkel,"['New paper, about a new class of quantum integrable models, weaves together several threads from my recent (and not so recent) research. Worked pretty hard on it in the last few months, and quite happy how it turned out. 🤓 Find it on @arXiv:\n<LINK>\n#math #quantum <LINK>']",https://arxiv.org/abs/2110.14600,"We propose a novel quantum integrable model for every non-simply laced simple Lie algebra ${\mathfrak g}$, which we call the folded integrable model. Its spectra correspond to solutions of the Bethe Ansatz equations obtained by folding the Bethe Ansatz equations of the standard integrable model associated to the quantum affine algebra $U_q(\widehat{\mathfrak g'})$ of the simply-laced Lie algebra ${\mathfrak g}'$ corresponding to ${\mathfrak g}$. Our construction is motivated by the analysis of the second classical limit of the deformed ${\mathcal W}$-algebra of ${\mathfrak g}$, which we interpret as a ""folding"" of the Grothendieck ring of finite-dimensional representations of $U_q(\widehat{\mathfrak g'})$. We conjecture, and verify in a number of cases, that the spaces of states of the folded integrable model can be identified with finite-dimensional representations of $U_q({}^L\widehat{\mathfrak g})$, where $^L\widehat{\mathfrak g}$ is the (twisted) affine Kac-Moody algebra Langlands dual to $\widehat{\mathfrak g}$. We discuss the analogous structures in the Gaudin model which appears in the limit $q \to 1$. Finally, we describe a conjectural construction of the simple ${\mathfrak g}$-crystals in terms of the folded $q$-characters. ",Folded quantum integrable models and deformed W-algebras
15,1454080159632482311,2930047588,Andrew Vanderburg,"['New paper out last night by me and @Astro_JRod on exomoons! <LINK> A thread:', 'Shortly after I defended my PhD back in 2017, I was feeling pretty burned out and wanted to work on something new and different for a while. I was inspired by a question Andy Mayo (now at UC Berkeley) asked about detecting exomoons.', 'Oops, somehow my thread got disconnected! Here is the next one in the sequence: https://t.co/6FiNmXPAuJ']",https://arxiv.org/abs/2110.14650,"We place the first constraints on binary planets and exomoons from Doppler monitoring of directly imaged exoplanets. We model radial velocity observations of HR 8799 b, c, and d from Ruffio et al. (2021) and determine upper limits on the $m\sin{i}$ of short-period binary planets and satellites. At 95% confidence, we rule out companions orbiting the three planets more massive than $m\sin{i} = 2 M_J$ with orbital periods shorter than 5 days. We achieve our tightest constraints on moons orbiting HR 8799 c, where with 95% confidence we rule out out edge-on Jupiter-mass companions in periods shorter than 5 days and edge-on half-Jupiter-mass moons in periods shorter than 1 day. These radial velocity observations come from spectra with resolution 20 times lower than typical radial velocity instruments and were taken using a spectrograph that was designed before the first directly imaged exoplanet was discovered. Similar datasets from new and upcoming instruments will probe significantly lower exomoon masses. ","First Doppler Limits on Binary Planets and Exomoons in the HR 8799
  System"
16,1454072486249865217,156312765,Kenda Knowles,"[""Our paper on the #astronomy arXiv today showing the potential for @SKA_Africa's MeerKAT to find many new high-z radio galaxy candidates! <LINK> #radioastronomy #galaxies""]",https://arxiv.org/abs/2110.14986,"We present results from a search for high-redshift radio galaxy (H$z$RG) candidates using 1.28 GHz data in the Abell 2751 field drawn from the MeerKAT Galaxy Cluster Legacy Survey (MGCLS). We use the H$z$RG criteria that a radio source is undetected in all-sky optical and infrared catalogues, and has a very steep radio spectrum. We cross-match the radio catalogue against multi-wavelength galaxy catalogues from DECaLS and AllWISE. For those radio sources with no multi-wavelength counterpart, we further implement a radio spectral index criterium of $\alpha < -1$, using in-band spectral index measurements from the wide-band MeerKAT data. Using a 5$\sigma$ signal-to-noise cut on the radio flux densities, we find a total of 274 HzRG candidates: 179 ultra-steep spectrum sources, and 95 potential candidates which cannot be ruled out as they have no spectral information available. The spectral index assignments in this work are complete above a flux density of 0.3 mJy, at least an order of magnitude lower than existing studies in this frequency range or when extrapolating from lower frequency limits. Our faintest HzRG candidates with and without an in-band spectral index measurement have a 1.28\,GHz flux density of 57 $\pm$ 8 $\mu$Jy and 68 $\pm$ 13 $\mu$Jy, respectively. Although our study is not complete down to these flux densities, our results indicate that the sensitivity and bandwidth of the MGCLS data makes them a powerful radio resource to search for H$z$RG candidates in the Southern sky, with 20 of the MGCLS pointings having similar image quality as the Abell~2751 field and full coverage in both DECaLS and AllWISE. Data at additional radio frequencies will be needed for the faintest source populations, which could be provided in the near future by the MeerKAT UHF band (580 -- 1015 MHz) at a similar resolution ($\sim$ 8-10 arcsec). ",Searching for high-z radio galaxies with the MGCLS
17,1453994946860519431,1070325662253166594,Aadarsh Sahoo,"['CoMix is now available on ArXiv! We introduce a new temporal contrastive learning approach for unsupervised video domain adaptation by jointly leveraging: video speed, background mixing, and target self-supervision.\nPaper link: <LINK>\n#NeurIPS2021 <LINK>', 'Either you walk indoors or walk outside, action recognition should not be affected by the ""shift"" in domain/background. Focussing on action semantics is crucial. Our proposed contrastive background mixing component helps in that: https://t.co/AlOtrHGErF', 'https://t.co/G0rxPdK7KD', 'Project page: https://t.co/E1dgpPFB3a\nCodes and poster coming soon!\nJoint work with @rutavms @rpanda89 @kate_saenko_ @AbirDasUCR']",https://arxiv.org/abs/2110.15128,"Unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain has attracted much attention in recent years. While many domain adaptation techniques have been proposed for images, the problem of unsupervised domain adaptation in videos remains largely underexplored. In this paper, we introduce Contrast and Mix (CoMix), a new contrastive learning framework that aims to learn discriminative invariant feature representations for unsupervised video domain adaptation. First, unlike existing methods that rely on adversarial learning for feature alignment, we utilize temporal contrastive learning to bridge the domain gap by maximizing the similarity between encoded representations of an unlabeled video at two different speeds as well as minimizing the similarity between different videos played at different speeds. Second, we propose a novel extension to the temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains. Moreover, we also integrate a supervised contrastive learning objective using target pseudo-labels to enhance discriminability of the latent space for video domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed approach over state-of-the-art methods. Project page: this https URL ","Contrast and Mix: Temporal Contrastive Video Domain Adaptation with
  Background Mixing"
18,1453958610803822621,1968365508,Samaya Nissanke (she/her) 💙,"['So proud of Sarah Gossan, Evan Hall &amp; our new paper on “optimizing the third generation of gravitational wave observatories for Galactic astrophysics” on ArXiv today <LINK>', 'Focusing on Galactic sources, we consider the position &amp; orientation of the third generation &amp; current GW detectors including Einstein Telescope &amp; Cosmic Explorer including possible locations for the latter in Australia and Brazil in addition to N.America &amp; Europe. https://t.co/Vk9Fg0pL2g', 'This paper moves away from considering extragalactic compact mergers, the only sources we have seen in GWs &amp; refocuses attention on the plethora of weaker GW emitting Galactic sources including core collapse supernova, fallback accretion onto neutron stars to form black holes,...', 'Accretion neutron stars in binaries, Thorne—Zytkow objects, accretion induced collapse of white dwarfs, spin down of isolated neutron stars, pulsar glitches, magnets flares...etc', 'Sarah has introduced a new framework &amp; approach to evaluate Galactic science by defining novel figures of merit such as the detector antenna function. 23 pages &amp; an extensive study laying the foundations for exciting new research! https://t.co/U9Q5Fu0n66', 'As the paper stresses, we emphasis the impact of new observatories on the land, nearby communities &amp; environment &amp; that this tenet must be central to the development, construction and operation of any observatory.', 'Sarah led and with Evan did the huge work presented in this paper &amp; I am so privileged to have been involved in this pandemic paper, such a fun and inclusive collaboration &amp; so grateful to have Sarah as a GRAPPA visitor over the past years.']",https://arxiv.org/abs/2110.15322,"Gravitational-wave (GW) astrophysics is a rapidly expanding field, with plans to enhance the global ground-based observatory network through the addition of larger, more sensitive observatories: Einstein Telescope and Cosmic Explorer. These observatories will allow us to peer deeper into the sky, collecting GW events from farther away and earlier in the Universe. Within our own Galaxy, there is a plethora of interesting GW sources, including core-collapse supernovae, phenomena in isolated neutron stars and pulsars, and potentially novel sources. As GW observatories are directionally sensitive, their placement on the globe will affect the observation of Galactic sources. We analyze the performance of one-, two-, and three-observatory networks, both for sources at the Galactic center, as well as a source population distributed over the Galactic disk. We find that, for a single Cosmic Explorer or Einstein Telescope observatory, placement at near-equatorial latitudes provides the most reliable observation of the Galactic center. When a source population distributed over the Galactic disk is considered, the observatory location is less impactful, although equatorial observatories still confer an advantage over observatories at more extreme latitudes. For two- and three-node networks, the longitudes of the observatories additionally become important for consistent observation of the Galaxy. ","Optimizing the third generation of gravitational-wave observatories for
  Galactic astrophysics"
19,1453958009470664707,891659025879769089,松井鉄平（teppei matsui）,['Posted a new paper. Deep learning-based counterfactual explanation in fMRI <LINK>'],https://arxiv.org/abs/2110.14927,"Deep neural networks (DNNs) can accurately decode task-related information from brain activations. However, because of the nonlinearity of the DNN, the decisions made by DNNs are hardly interpretable. One of the promising approaches for explaining such a black-box system is counterfactual explanation. In this framework, the behavior of a black-box system is explained by comparing real data and realistic synthetic data that are specifically generated such that the black-box system outputs an unreal outcome. Here we introduce a novel generative DNN (counterfactual activation generator, CAG) that can provide counterfactual explanations for DNN-based classifiers of brain activations. Importantly, CAG can simultaneously handle image transformation among multiple classes associated with different behavioral tasks. Using CAG, we demonstrated counterfactual explanation of DNN-based classifiers that learned to discriminate brain activations of seven behavioral tasks. Furthermore, by iterative applications of CAG, we were able to enhance and extract subtle spatial brain activity patterns that affected the classifier's decisions. Together, these results demonstrate that the counterfactual explanation based on image-to-image transformation would be a promising approach to understand and extend the current application of DNNs in fMRI analyses. ","Counterfactual Explanation of Brain Activity Classifiers using
  Image-to-Image Transfer by Generative Adversarial Network"
20,1453921062165655554,1373472847750893569,Stanley H. Chan,['New paper!\n\nHow to deblur an image under heavy photon shot noise?  \n<LINK>\n\nPrior art: can handle blurry but not much noise\nOurs: can handle blurry and very heavy Poisson shot noise\n\n#computervision #imageprocessing <LINK>'],https://arxiv.org/abs/2110.15314,"Image deblurring in photon-limited conditions is ubiquitous in a variety of low-light applications such as photography, microscopy and astronomy. However, the presence of photon shot noise due to low-illumination and/or short exposure makes the deblurring task substantially more challenging than the conventional deblurring problems. In this paper we present an algorithm unrolling approach for the photon-limited deblurring problem by unrolling a Plug-and-Play algorithm for a fixed number of iterations. By introducing a three-operator splitting formation of the Plug-and-Play framework, we obtain a series of differentiable steps which allows the fixed iteration unrolled network to be trained end-to-end. The proposed algorithm demonstrates significantly better image recovery compared to existing state-of-the-art deblurring approaches. We also present a new photon-limited deblurring dataset for evaluating the performance of algorithms. ",Photon Limited Non-Blind Deblurring Using Algorithm Unrolling
21,1453895296166023171,1094750915444310016,Alex Pizzuto,"['New paper day!\n\nMay I [re-]introduce you to TauRunner, a python package to propagate particles at ultra high energies?\n\nThis work – led by @IbrahimSafa1 &amp; Jeff Lazar (contributions from myself, @carguelles314, et al.) – has been a blast\n\nPossible 🧵 later\n\n<LINK>']",https://arxiv.org/abs/2110.14662,"In the past decade IceCube's observations have revealed a flux of astrophysical neutrinos extending to $10^{7}~\rm{GeV}$. The forthcoming generation of neutrino observatories promises to grant further insight into the high-energy neutrino sky, with sensitivity reaching energies up to $10^{12}~\rm{GeV}$. At such high energies, a new set of effects becomes relevant, which was not accounted for in the last generation of neutrino propagation software. Thus, it is important to develop new simulations which efficiently and accurately model lepton behavior at this scale. We present TauRunner a PYTHON-based package that propagates neutral and charged leptons. TauRunner supports propagation between $10~\rm{GeV}$ and $10^{12}~\rm{GeV}$. The package accounts for all relevant secondary neutrinos produced in charged-current tau neutrino interactions. Additionally, tau energy losses of taus produced in neutrino interactions is taken into account, and treated stochastically. Finally, TauRunner is broadly adaptable to divers experimental setups, allowing for user-specified trajectories and propagation media, neutrino cross sections, and initial spectra. ","TauRunner: A Public Python Program to Propagate Neutral and Charged
  Leptons"
22,1453812947633926149,2411222281,Dr./Prof. Meredith MacGregor,"['If you need a break from the dumpster fire, check out our new paper led by Ward Howard who joined my group as a postdoc this fall presenting the first statistical analysis of stellar flares in 20sec cadence TESS data: <LINK>', 'Long story short- flares are COMPLICATED when you study them with high time resolution!']",https://arxiv.org/abs/2110.13155,"A 20 second cadence TESS monitoring campaign of 226 low-mass flare stars during Cycle 3 recorded 3792 stellar flares of >10^32 erg. We explore the time-resolved emission and substructure in 440 of the largest flares observed at high S/N, 97% of which released energies of >10^33 erg. We discover degeneracy present at 2 minute cadence between sharply-peaked and weakly-peaked flares is common, although 20 second cadence breaks these degeneracies. We better resolve the rise phases and find 46% of large flares exhibit substructure during the rise phase. We observe 49 candidate quasi-periodic pulsations (QPP) and confirm 17 at 3+ sigma. Most of our QPPs have periods less than 10 minutes, suggesting short period optical QPPs are common. We find QPPs in both the rise and decay phases of flares, including a rise-phase QPP in a large flare from Proxima Cen. We confirm the Davenport et al. (2014) template provides a good fit to most classical flares observed at high cadence, although 9% favor Gaussian peaks instead. We characterize the properties of complex flares, finding 17% of complex flares exhibit ""peak-bump"" morphologies composed of a large, highly impulsive peak followed by a second more gradual Gaussian peak. We also estimate the UVC surface fluences of temperate planets at flare peak and find 1/3 of 10^34 erg flares reach the D90 dose of D. Radiodurans in just 20 seconds in the absence of an atmosphere. ","No Such Thing as a Simple Flare: Substructure and QPPs Observed in a
  Statistical Sample of 20 Second Cadence TESS Flares"
23,1453662907217158144,2444302555,Ludovic Denoyer,"['Happy to release a new paper about exploration in reinforcement learning (without reward). Together with @alelazaric @pa_kamienny and @jean_tarbou, we propose to learn a tree-structured policy to cover the set of states in any environments. \n\nArxiv paper: <LINK> <LINK>', 'It was possible (at least no too hard) to implement complex hierarchical policies by using SaLinA 😁😁 and the benchmarked implementation of the many RL algorithms we propose that are all agnostic to the policy structure: https://t.co/zqpgEP7Mne']",https://arxiv.org/abs/2110.14457,"Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE, which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a specific region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufficiently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines. ","Direct then Diffuse: Incremental Unsupervised Skill Discovery for State
  Covering and Goal Reaching"
24,1453627725579792385,246314044,Matteo Pompili,"['Our new paper is on the arXiv. We teleport a qubit across our multi-node quantum network, from Charlie to Alice! Lots of improvements to make this possible. \n<LINK>']",https://arxiv.org/abs/2110.11373,"Future quantum internet applications will derive their power from the ability to share quantum information across the network. Quantum teleportation allows for the reliable transfer of quantum information between distant nodes, even in the presence of highly lossy network connections. While many experimental demonstrations have been performed on different quantum network platforms, moving beyond directly connected nodes has so far been hindered by the demanding requirements on the pre-shared remote entanglement, joint qubit readout and coherence times. Here we realize quantum teleportation between remote, non-neighboring nodes in a quantum network. The network employs three optically connected nodes based on solid-state spin qubits. The teleporter is prepared by establishing remote entanglement on the two links, followed by entanglement swapping on the middle node and storage in a memory qubit. We demonstrate that once successful preparation of the teleporter is heralded, arbitrary qubit states can be teleported with fidelity above the classical bound, even with unit efficiency. These results are enabled by key innovations in the qubit readout procedure, active memory qubit protection during entanglement generation and tailored heralding that reduces remote entanglement infidelities. Our work demonstrates a prime building block for future quantum networks and opens the door to exploring teleportation-based multi-node protocols and applications. ",Qubit teleportation between non-neighboring nodes in a quantum network
25,1453615727219515399,772332610475417600,Jan Ditzen,"['A new version of #xtbreak for analysis of structural breaks in @Stata is available on GitHub and SSC. \n\nA working paper describing the package is also available:\n\n<LINK>\n\nWhat does #xtbreak do? A short 🧵', '#xtbreak is a powerful toolbox to analyse single and multiple structural breaks in @Stata. It can find the number of breaks and their location in time series and panel data without any prior knowledge.', 'Once the number of breaks is known, #xtbreak estimates the location of such and calculates confidence intervals.\nIt can also test the hypothesis that break(s) occurred at a specific point in time.', 'In case the breakpoints are unknown, it can test 3 hypothesis:\n1. no breaks vs. s breaks\n2. no break vs. up to s breaks\n3. s breaks vs. s+1 breaks', 'Interested in using it?\n\nIt can be installed in Stata:\n\nssc install xtbreak\n\nor \n\nnet install xtbreak, from(https://t.co/GkgWDpUUWX)\n\nMore info is on GitHub (https://t.co/GkgWDpUUWX), the Stata Forum (https://t.co/XgoYcoqySz) and the working paper  (https://t.co/5txdhACCGq).', '#xtbreak is joint work Yiannis Karavias (@econ_ub) and Joakim Westerlund (@EconomicsLund @Deakin).']",https://arxiv.org/abs/2110.14550,"Identifying structural change is a crucial step in analysis of time series and panel data. The longer the time span, the higher the likelihood that the model parameters have changed as a result of major disruptive events, such as the 2007-2008 financial crisis and the 2020 COVID-19 outbreak. Detecting the existence of breaks, and dating them is therefore necessary not only for estimation purposes but also for understanding drivers of change and their effect on relationships. This article introduces a new community contributed command called xtbreak, which provides researchers with a complete toolbox for analysing multiple structural breaks in time series and panel data. xtbreak can detect the existence of breaks, determine their number and location, and provide break date confidence intervals. The new command is used to explore changes in the relationship between COVID-19 cases and deaths in the US using both country-level time series data and state-level panel data. ","Testing and Estimating Structural Breaks in Time Series and Panel Data
  in Stata"
26,1453596795544563716,1926797899,Sagnik Chatterjee,"['Yesterday we @BQiiit  (my advisor Debajyoti Bera, and I) uploaded a new paper on arxiv on Quantum Boosting using Domain-Partitioning hypotheses.\n<LINK> <LINK>', 'We give an adaptive boosting algorithm for converting a weak quantum PAC learner into a strong quantum PAC learner while achieving a speedup (in the VC dimension) over classical boosting algorithms.', 'In our work, we provide provable guarantees for the Quantum RealBoost algorithm and also lay the foundations for quantizing an entire family of AdaBoost variants such as GentleBoost, ModestBoost, etc.']",https://arxiv.org/abs/2110.12793,"Boosting is an ensemble learning method that converts a weak learner into a strong learner in the PAC learning framework. Freund and Schapire gave the first classical boosting algorithm for binary hypothesis known as AdaBoost, and this was recently adapted into a quantum boosting algorithm by Arunachalam et al. Their quantum boosting algorithm (which we refer to as Q-AdaBoost) is quadratically faster than the classical version in terms of the VC-dimension of the hypothesis class of the weak learner but polynomially worse in the bias of the weak learner. In this work we design a different quantum boosting algorithm that uses domain partitioning hypotheses that are significantly more flexible than those used in prior quantum boosting algorithms in terms of margin calculations. Our algorithm Q-RealBoost is inspired by the ""Real AdaBoost"" (aka. RealBoost) extension to the original AdaBoost algorithm. Further, we show that Q-RealBoost provides a polynomial speedup over Q-AdaBoost in terms of both the bias of the weak learner and the time taken by the weak learner to learn the target concept class. ",Quantum Boosting using Domain-Partitioning Hypotheses
27,1453551530812993537,1350139361824673792,Duan Jiafei,"[""A new workshop paper from my group accepted to #NeurIPS2021 workshop!\n\nWe worked with Prof Renée's group to reconstruct her classic work on VOE in a 3D environment for training AI in high-level physical reasoning using abstract features and rules. \n<LINK>""]",https://arxiv.org/abs/2110.05836,"Recent work in cognitive reasoning and computer vision has engendered an increasing popularity for the Violation-of-Expectation (VoE) paradigm in synthetic datasets. Inspired by work in infant psychology, researchers have started evaluating a model's ability to discriminate between expected and surprising scenes as a sign of its reasoning ability. Existing VoE-based 3D datasets in physical reasoning only provide vision data. However, current cognitive models of physical reasoning by psychologists reveal infants create high-level abstract representations of objects and interactions. Capitalizing on this knowledge, we propose AVoE: a synthetic 3D VoE-based dataset that presents stimuli from multiple novel sub-categories for five event categories of physical reasoning. Compared to existing work, AVoE is armed with ground-truth labels of abstract features and rules augmented to vision data, paving the way for high-level symbolic predictions in physical reasoning tasks. ","AVoE: A Synthetic 3D Dataset on Understanding Violation of Expectation
  for Artificial Cognition"
28,1453543938187476996,77815209,Brian Skinner,"['The universe famously has a small fine structure constant 𝛼≈1/137. But some materials act like synthetic 2D universes with very different 𝛼.\n\nSo what happens when 𝛼 &gt;&gt; 1? In a new paper @The_Correlator and I explore what happens to electron crystals:\n<LINK>', 'It turns out that the value of the electron charge becomes equal to √(ħc) and the interaction between electrons stops depending on distance.', '@aavishkar_patel Okay, I lied somewhat. V(r) depends logarithmically on distance, and this is just strong enough to hold it together.', '@VergaraLautaro sorry sorry, you and @aavishkar_patel are catching me in the typical theorist act of thinking of a log as a constant.', ""@VergaraLautaro @aavishkar_patel But it's still surprising (to me), since we are talking about a 2D system embedded in a 3D universe, where typically the Coulomb interaction is 1/r."", '@anderssandberg @The_Correlator I have to admit that I know essentially nothing about electroweak unification']",https://arxiv.org/abs/2110.13921,"We consider the fate of the Wigner crystal state in a two dimensional system of massive Dirac electrons as the effective fine structure constant $\alpha$ is increased. In a Dirac system, larger $\alpha$ naively corresponds to stronger electron-electron interactions, but it also implies a stronger interband dielectric response that effectively renormalizes the electron charge. We calculate the critical density and critical temperature associated with quantum and thermal melting of the Wigner crystal state using two independent approaches. We show that at $\alpha \gg 1$, the Wigner crystal state is best understood in terms of logarithmically-interacting electrons, and that both the critical density and the melting temperature approach a universal, $\alpha$-independent value. We discuss our results in the context of recent experiments in twisted bilayer graphene near the magic angle. ",Wigner crystallization at large fine structure constant
29,1453348747778433025,325194378,Antonio Valerio Miceli Barone,"['Neural language models can accurately model text by exploiting long-distance correlations that however tend to be fragile w.r.t. distribution shift.\n\nIn this new paper with @alexandrabirch1 and @RicoSennrich , we attempt to fix them:\n<LINK>', 'The strongest correlations in a text occur at a short distance. N-grams model can only consider these short-distance correlations, which makes them relatively robust at the cost of having a generally poor predictive accuracy.', 'Modern LMs achieve much better accuracy by exploiting weak, long-distance ""shortcut"" correlations, but this makes them fragile out-of-distribution.\nWe want to automatically adapt and use more or less long-distance correlations depending on how in-distribution or OOD the data is.', 'In a RNNLM we can achieve this by doing OOD detection and manipulating the state:\nwe initialize the RNN with the all-zero state, which corresponds to high entropy of the output distribution, since at the beginning many tokens are possible https://t.co/vOgQJ63m8c', 'At each step we update the RNN (a GRU) with the observed token as usual, then we estimate how much OOD it is by applying the Random Network Distillation method by Burda et al. https://t.co/x0WMKVfwnj', 'The more OOD the state is, the more we scale it towards zero. This both increases the output distribution entropy (the model becomes less certain about the next token) and purges the state from old, distracting information (older tokens are exponentially decaying in the state).', 'We obtain perplexity improvements in out-of-domain test sets while maintaining perplexity in in-distribution test sets.']",https://arxiv.org/abs/2110.13229,"Neural machine learning models can successfully model language that is similar to their training distribution, but they are highly susceptible to degradation under distribution shift, which occurs in many practical applications when processing out-of-domain (OOD) text. This has been attributed to ""shortcut learning"": relying on weak correlations over arbitrary large contexts. We propose a method based on OOD detection with Random Network Distillation to allow an autoregressive language model to automatically disregard OOD context during inference, smoothly transitioning towards a less expressive but more robust model as the data becomes more OOD while retaining its full context capability when operating in-distribution. We apply our method to a GRU architecture, demonstrating improvements on multiple language modeling (LM) datasets. ","Distributionally Robust Recurrent Decoders with Random Network
  Distillation"
30,1453246557902475276,1283150444,Maurizio Pierini,"['1. New paper on the @arXiv: a proposal for a two-step (trap &amp; detect) detector for very-long-lived particles, that could emerge from LHC collision and be trapped in material (e.g., Split SUSY gluinos, but not only). 🧵👇  <LINK> <LINK>', '2. The idea is to build a brass absorber with 400 1cm x 1cm x 2m rods, put next to each other in a place next to an LHC detector (e.g., @CMSExperiment ) and a configuration that can be optimized targeting a given new physics scenario (= mass and acceptance).', '3. The stopping probability changes depending on the features of the particle to be stopped, but there is some probability, also thanks to the dense absorber material (-&gt; compact design in constrained space) https://t.co/MSGklAjsBt', '4. After exposure, the rods are moved in a different experimental area, where they are put in a Liquid Argon bath (something like a small @DUNEScience module). Then we wait for a decay to happen, similarly to what Dark Matter experiments (@XENONexperiment ) do under mountains https://t.co/0CvCyPLpft', ""5. Clearly we need a muon veto system, because we cannot put it under a mountain, and we don't have to do that in the future (but we might in a second step, see below)"", '6. With 1 week exposure, 1 week to move, and 2 weeks to detect, we can already prove previously unproved territory, e.g., scenarios with gluino masses close to the dark matter mass (so that little happens and the LHC experiments might have missed it) and very long lifetimes (&gt;d) https://t.co/yvciCl335j', '7. We estimate the cost to be &lt; 1M CHF (being generous), assuming that we can recycle most of the infrastructure (cryostat, LAr system, etc) from existing @CERN facilities.', '8. It started with a Summer Student project by Jasmine Simms and it became something very concrete with the dedicated work of a fantastic team: Jean Kieseler, Juliette Alimena, @Thea_kaa from the #CERN CMS group and Alex Kish from @Fermilab', '9. This is similar to how Magnetic Monopoles are searched at CERN, but it also implies a second experiment for detection (not just a B scan). Similar ideas were proposed in the past, based on water tanks (so too big for the confined LHC caverns). The rods help here https://t.co/2oELzE8NDD', '10. If this happens one day, it could be pushed to probe very-low energetic decays (e.g., very degenerate mass spectra). In that case, I am afraid, we will have to move the rods in some underground experiment to look at very faint signals, like for Dark Matter detection', '11. A great advantage in this design: when you take cosmic data to test your apparatus, you are building a data control sample with which you can measure the background. Best would be to have two copies of the detector, one wit exposed rods and one without to measure cosmic noise', '@marcodelmastro @CERN Apparently there is one which is not used, sitting in Prevessin. And, at some point, ProtoDune will stop (unless someone comes with a clever idea of what to do with it). Here we are...', '@1cRebeca @arxiv We have ideas...', '@marcodelmastro @CERN Alex knows. It some old neutrino detector prototype of some kind', '@marcodelmastro @CERN But I am more after ProtoDUNE post-protoDUNE life.', '@1cRebeca @arxiv This is better than any idea I had...', '@marcodelmastro @CERN No... I would never dare to touch your toys without permission.', ""@AlessandroStru4 You are right. But our it's a benchmark. A proxy for something that would have that signature (and maybe has nothing to do with DM). Don't take it too strictly. At this stage, we should go beyond things that make sense (since we didn't find them).""]",https://arxiv.org/abs/2110.13837,"We propose to implement a two-stage detection strategy for exotic long-lived particles that could be produced at the CERN LHC, become trapped in detector material, and decay later. The proposed strategy relies on an array of metal rods, combined to form a high-density target. In a first stage, the rods are exposed to radiation from LHC collisions in one of the experimental caverns. In a second stage, they are individually immersed in liquid argon in a different experimental hall, where out-of-time decays could produce a detectable signal. Using a benchmark case of long-lived gluino pair production, we show that this experiment would be sensitive to a wide range of masses. Such an experiment would have unique sensitivity to gluino-neutralino mass splittings down to 3 GeV, in previously uncovered particle lifetimes ranging from days to years. ",Detecting long-lived particles trapped in detector material at the LHC
31,1453211310976552965,1436796062,Cliff Burgess,"['New paper out. Had always believed it wrong to use supergravity potentials for late universe cosmology bcs SM loops would ruin the supersymmetric structure at low energies. This paper argues that point of view is actually wrong.\n\n [2110.13275] <LINK>', 'You can check these loop corrections because there are tools now for how to couple supergravity to non supersymmetric matter. Turns out auxiliary fields play an important role in arguments about technical naturalness, in a way that matters for the scalar potential.', 'So it is possible that susy does survive at low energies after all, but it is the gravity sector that is supersymmetric not the standard model sector. This is often what classical calculations in UV completions give too: weakest coupled sector breaks supersymmetry the least.', 'A supersymmetric gravity sector gives structure to dark sector models. When combined with scale invariance (as is generic in eg string vacua) the dilaton multiplet has all the properties required by ‘axion homeopathy’ to evade solar system tests.', 'So am reconsidering the particle physics aversion to light scalars. A supersymmetric gravity sector does not in itself solve the naturalness problems such scalars have. But it seems to have features that can help when combined with other ideas. More about these other ideas soon!', '@A_n_Elk Who you gonna call? 😁', '@SeamusBlackley Yes I think so. Even after you integrate out all SM fields down almost to the Hubble scale the auxiliary fields can be playing a role (as long as the gravitino is that light).', '@WKCosmo I think so, in principle, though you would need more details to see precisely how light. (You mean electrically charged here, rather than some dark sector charge?) Susy in the gravity sector is not in itself a get-out-of-jail-free card for small-mass naturalness problems though.', '@WKCosmo The normal issues about loops of heavy particles giving big mass corrections (and phenomenological problems if electrically charged fields vary in space) still apply. \n\nThere seems a loophole that allows light scalars in specific circumstances though, which is worth exploring', '@VergaraLautaro Good question. Part three of the series puts in ingredients to deal with naturalness and dark energy. Then there are so many ways to know the worry is it is filed out :)', '@VergaraLautaro When did technical naturalness (as opposed to aesthetic naturalness) stop being a thing to think about?', '@VergaraLautaro No, not eg for the cosmological constant or very light but unprotected masses. If a parameter in an EFT is say 0.01 &amp; not technically natural then the corresponding parameter in the UV might be 938287532357.93 but integrating out then gives loop a contribution 938287532357.92.']",https://arxiv.org/abs/2110.13275,"Use of supergravity equations in astronomy and late-universe cosmology is often criticized on three grounds: (i) phenomenological success usually depends on the supergravity form for the scalar potential applying at the relevant energies; (ii) the low-energy scalar potential is extremely sensitive to quantum effects involving very massive particles and so is rarely well-approximated by classical calculations of its form; and (iii) almost all Standard Model particles count as massive for these purposes and none of these are supersymmetric. Why should Standard Model loops preserve the low-energy supergravity form even if supersymmetry is valid at energies well above the electroweak scale? We use recently developed tools for coupling supergravity to non-supersymmetric matter to estimate the loop effects of heavy non-supersymmetric particles on the low-energy effective action, and provide evidence that the supergravity form is stable against integrating out such particles (and so argues against the above objection). This suggests an intrinsically supersymmetric picture of Nature where supersymmetry survives to low energies within the gravity sector but not the visible sector (for which supersymmetry is instead non-linearly realized). We explore the couplings of both sectors in this picture and find that the presence of auxiliary fields in the gravity sector makes the visible sector share many features usually attributed to linearly realized supersymmetry although (unlike for the MSSM) a second Higgs doublet is not required for all Yukawa couplings to be non-vanishing and changes the dimension of the operator generating the Higgs mass. We discuss the naturalness of this picture and some of the implications it might have when searching for dark-sector physics. ","Who's Afraid of the Supersymmetric Dark? The Standard Model vs
  Low-Energy Supergravity"
32,1453129622388940803,1247911066132234241,Max Daniels,"['I\'m excited to share a new paper, ""Score-based Generative Models for Large-scale Optimal Transport""\n\nCome see our work at NeurIPS 2021, or on the ArXiv: \n<LINK>\n\nFor example, see our method transport blurred CelebA to CelebA: <LINK>', ""Oops, it's transparent! It should be: https://t.co/LUtrtYQpte""]",https://arxiv.org/abs/2110.03237,"We consider the fundamental problem of sampling the optimal transport coupling between given source and target distributions. In certain cases, the optimal transport plan takes the form of a one-to-one mapping from the source support to the target support, but learning or even approximating such a map is computationally challenging for large and high-dimensional datasets due to the high cost of linear programming routines and an intrinsic curse of dimensionality. We study instead the Sinkhorn problem, a regularized form of optimal transport whose solutions are couplings between the source and the target distribution. We introduce a novel framework for learning the Sinkhorn coupling between two distributions in the form of a score-based generative model. Conditioned on source data, our procedure iterates Langevin Dynamics to sample target data according to the regularized optimal coupling. Key to this approach is a neural network parametrization of the Sinkhorn problem, and we prove convergence of gradient descent with respect to network parameters in this formulation. We demonstrate its empirical success on a variety of large scale optimal transport tasks. ",Score-based Generative Neural Networks for Large-Scale Optimal Transport
33,1453076894505521157,60893773,James Bullock,['Enjoyed being part of this new paper led by Mauro Bernardini &amp; Robert Feldmann et al. Introduces EMBER a Deep Learning framework to predict baryon fields using dark-matter-only sims.  Relies on both U-Net and Wasserstein Generative Adversarial Networks <LINK> <LINK>'],https://arxiv.org/abs/2110.11970,"Hydrodynamic simulations provide a powerful, but computationally expensive, approach to study the interplay of dark matter and baryons in cosmological structure formation. Here we introduce the EMulating Baryonic EnRichment (EMBER) Deep Learning framework to predict baryon fields based on dark-matter-only simulations thereby reducing computational cost. EMBER comprises two network architectures, U-Net and Wasserstein Generative Adversarial Networks (WGANs), to predict two-dimensional gas and HI densities from dark matter fields. We design the conditional WGANs as stochastic emulators, such that multiple target fields can be sampled from the same dark matter input. For training we combine cosmological volume and zoom-in hydrodynamical simulations from the Feedback in Realistic Environments (FIRE) project to represent a large range of scales. Our fiducial WGAN model reproduces the gas and HI power spectra within 10% accuracy down to ~10 kpc scales. Furthermore, we investigate the capability of EMBER to predict high resolution baryon fields from low resolution dark matter inputs through upsampling techniques. As a practical application, we use this methodology to emulate high-resolution HI maps for a dark matter simulation of a L=100 Mpc/h comoving cosmological box. The gas content of dark matter haloes and the HI column density distributions predicted by EMBER agree well with results of large volume cosmological simulations and abundance matching models. Our method provides a computationally efficient, stochastic emulator for augmenting dark matter only simulations with physically consistent maps of baryon fields. ","From EMBER to FIRE: predicting high resolution baryon fields from dark
  matter simulations with Deep Learning"
34,1453038307546476552,7984662,Clayton Shonkwiler,"['New paper: Toric Symplectic Geometry and Full Spark Frames, written with Tom Needham.\n\n<LINK>\n\n(1/7)', 'Here’s one way of approaching the basic problem: let d &lt; N and consider d × N complex matrices where we pre-specify the singular values of the matrix and the norms of the columns. Such conditions are common in various signal processing applications.\n\n(2/7)', 'Now, for something like compressed sensing, you would really like it to be true that every d × d submatrix is invertible. So the question: given a random matrix with the prescribed data, what is the probability that every d × d minor is invertible?\n\n(3/7)', 'Sometimes your data is incompatible, so there are no such matrices. This isn’t the only condition, but to have such matrices the sum of the squares of the singular values must equal the sum of the squares of the column norms.\n\n(4/7)', 'It can also happen that the space of matrices with the prescribed data is non-empty, but *all* matrices in the space have some singular d × d minor, and hence the probability is zero.\n\n(5/7)', ""Our main theorem characterizes exactly when these two possibilities happen and shows that in all other cases the probability is equal to 1. Along the way we determine when these spaces are manifolds and characterize the local structure of singularities when they're not.\n\n(6/7)"", 'The main idea is to show that these spaces are closely related to certain highly structured and extremely symmetric manifolds called toric symplectic manifolds (or toric varieties, if you’re more of an algebraic geometer).\n\n(7/7)']",https://arxiv.org/abs/2110.11295,"The collection of $d \times N$ complex matrices with prescribed column norms and prescribed (nonzero) singular values forms a compact algebraic variety, which we refer to as a frame space. Elements of frame spaces -- i.e., frames -- are used to give robust representations of complex-valued signals, so that geometrical and measure-theoretic properties of frame spaces are of interest to the signal processing community. This paper is concerned with the following question: what is the probability that a frame drawn uniformly at random from a given frame space has the property that any subset of $d$ of its columns gives a basis for $\mathbb{C}^d$? We show that the probability is one, generalizing recent work of Cahill, Mixon and Strawn. To prove this, we first show that frame spaces are related to highly structured objects called toric symplectic manifolds. This relationship elucidates the geometric meaning of eigensteps -- certain spectral invariants of a frame -- and should be a more broadly applicable tool for studying probabilistic questions about the structure of frame spaces. As another application of our symplectic perspective, we completely characterize the norm and spectral data for which the corresponding frame space has singularities, answering some open questions in the frame theory literature. ",Toric Symplectic Geometry and Full Spark Frames
35,1453028770672046087,1277309540313300995,Soufiane Hayou,['A new paper on PAC-Bayes! Joint work with @bobby_he and @KDziugaite.\nWe use probabilistic masks (spike-and-slab) to fine-tune pruning masks. We also optimize a data-dependent PAC-Bayes bound to obtain pruned networks with tight generalization certificates.\n<LINK>'],https://arxiv.org/abs/2110.11804,"We study an approach to learning pruning masks by optimizing the expected loss of stochastic pruning masks, i.e., masks which zero out each weight independently with some weight-specific probability. We analyze the training dynamics of the induced stochastic predictor in the setting of linear regression, and observe a data-adaptive L1 regularization term, in contrast to the dataadaptive L2 regularization term known to underlie dropout in linear regression. We also observe a preference to prune weights that are less well-aligned with the data labels. We evaluate probabilistic fine-tuning for optimizing stochastic pruning masks for neural networks, starting from masks produced by several baselines. In each case, we see improvements in test error over baselines, even after we threshold fine-tuned stochastic pruning masks. Finally, since a stochastic pruning mask induces a stochastic neural network, we consider training the weights and/or pruning probabilities simultaneously to minimize a PAC-Bayes bound on generalization error. Using data-dependent priors, we obtain a selfbounded learning algorithm with strong performance and numerically tight bounds. In the linear model, we show that a PAC-Bayes generalization error bound is controlled by the magnitude of the change in feature alignment between the 'prior' and 'posterior' data. ","Probabilistic fine-tuning of pruning masks and PAC-Bayes self-bounded
  learning"
36,1453022510530928647,61434104,Scott H. Hawley,"['Proud of our new physics-ML paper on tracking drum transients: ""espiownage: Tracking Transients in Steelpan Drum Strikes Using Surveillance Technology""! With @achmorrison and student Grant Morgan. \nPaper: <LINK>\nCode, data &amp; supplements: <LINK> <LINK>', 'We exceeded prior results in error rates, accuracy, and error bars, and we developed a new method that agrees with other methods but generalizes to other instruments!', 'This was submitted to #ML4PS. Now that the double-blind review period is over we’re excited to share these results via a slightly-longer paper than the 4 pages submitted. Adding another word would have put us into 5 pages but now we include another figure and some clarifications.', 'This was an intense 18-day sprint. We rewrote the old code from scratch with nbdev, @fastdotai &amp; Icevision. The only old code was the GUI data editor, but we added rapid data-cleaning workflow where model predictions were fed to the editor, and users shown ""top losses"" first. https://t.co/N56KDQbUoi', ""The more-accurate-than-before method we used was to crop to bounding boxes via Icevision's state-of-the-art object detector https://t.co/0Ib87gPerK"", 'And then we count rings in the crops using a convnet that outputs a single regression value.  Actually this was just @fastdotai image classification where the range of the final sigmoid was extended so that we stayed in the roughly-linear part of the sigmoid. https://t.co/xRUFbuv9lf', 'This is what gave us tighter error bars and higher accuracies, allowing us to confirm earlier ""preliminary physics"" results, including that the rise in amplitude preceded what you hear! https://t.co/R75moUyOGJ', 'But there were a few days before this where the bounding box detector wasn\'t giving us predictions (CUDA OOM), so we developed a surrogate ""segmentation regression"" method (like a depth map but not quite) that would predict ring counts anywhere on the image: https://t.co/nzTfm0I49r', 'We were ecstatic to find that this new method agreed so closely with the more ""trustworthy"" count-and-crop method: https://t.co/pgZM5yWh4s', '...AND were blown away that these maps could generalize and be applied *via inference only* to new (non-steelpan) instruments and non-elliptical antinode region shapes! https://t.co/e7xsZ8P4dQ', ""The timescale of this project demanded that coding, data-cleaning, training, etc, be done in an efficient, well-documented manner to which we credit nbdev (@jeremyphoward, @GuggerSylvain, @HamelHusain). And transfer learning with @fastdotai and @airctic21's IceVision."", 'Ultimately our paper was not accepted. One of the two reviewers read our paper inattentively, demonstrated surprising lack of domain knowledge, and didn’t even glance at our copious Supplementary Materials. There is no appeal process, so we will submit elsewhere.', 'Expect more physics to come! With this new data obtained from all the video frames and the seg-reg maps, we\'ve made editor into a ""data explorer"": You can click on any pixel and immediately get a graph of the time series at that point!  (this data is not released yet)', 'One more movie to share, this time segmentation-regression mapping: https://t.co/JUuf3QC8Fa', 'clarification: ""preceeded"" as in, after the initial decay.  Or at least ""is out of sync with"".', '^Ooops, almost forgot to mention that experiment tracking via @weights_biases was *essential* to pulling off this project!', 'Just to be clear: All the code &amp; Supplementary Materials are Jupyter Notebooks that you can run on Colab for free! i.e. full transparency and reproducibility of (most of) our results. The unlabeled video frames have not been released, but all training/test data &amp; code are posted.']",https://arxiv.org/abs/2110.12261,"We present an improvement in the ability to meaningfully track features in high speed videos of Caribbean steelpan drums illuminated by Electronic Speckle Pattern Interferometry (ESPI). This is achieved through the use of up-to-date computer vision libraries for object detection and image segmentation as well as a significant effort toward cleaning the dataset previously used to train systems for this application. Besides improvements on previous metric scores by 10% or more, noteworthy in this project are the introduction of a segmentation-regression map for the entire drum surface yielding interference fringe counts comparable to those obtained via object detection, as well as the accelerated workflow for coordinating the data-cleaning-and-model-training feedback loop for rapid iteration allowing this project to be conducted on a timescale of only 18 days. ","espiownage: Tracking Transients in Steelpan Drum Strikes Using
  Surveillance Technology"
37,1453021100946952196,1109974447954382848,「Alexandres Lazar」,"['New paper day! The IMF of the first stars is a crucial quantity of early cosmic history, but it is presently unknown. Our paper proposes a way to constrain its form using the brightest high-redshift transients (i.e., the first stars most violent deaths): <LINK> <LINK>']",https://arxiv.org/abs/2110.11956,"The emergence of the first, so-called Population III (Pop III), stars shaped early cosmic history in ways that crucially depends on their initial mass function (IMF). However, because of the absence of direct observational constraints, the detailed IMF remains elusive. Nevertheless, numerical simulations agree in broad terms that the first stars were typically massive and should often end their lives in violent, explosive deaths. These fates include extremely luminous pair-instability supernovae (PISNe) and bright gamma-ray bursts (GRBs), the latter arising from the collapse of rapidly rotating progenitor stars into black holes. These high-redshift transients are expected to be within the detection limits of upcoming space telescope missions, allowing to place effective constraints on the shape of the primordial IMF that is not easily accessible with other probes. This paper presents a framework to probe the Pop III IMF, utilizing the cosmological source densities of high-redshift PISNe and GRBs. Considering these transients separately could provide useful constraints on the Pop III IMF, but tighter bounds are obtainable by combining PISN and GRB counts. This combined diagnostic is more robust as it is independent of the underlying Pop III star formation rate density, an unknown prior. Future surveys promise to capture most high-redshift GRBs across the entire sky, but high-redshift PISN searches with future telescopes, e.g. Roman Space Telescope, will likely be substantially incomplete. Nevertheless, we demonstrate that even such lower bounds on the PISN count will be able to provide key constraints on the primordial IMF, in particular, if it is top-heavy or not. ",Probing the initial mass function of the first stars with transients
38,1453012103292461072,1242224281,Laura Parker,"['New paper day: <LINK> Work led by Ian Roberts, postdoc @UniLeidenNews, showing optically selected ram pressure candidate galaxies in UNIONS imaging @CFHTelescope <LINK>']",https://arxiv.org/abs/2110.12714,"We present a search for disturbed, candidate ram pressure stripping galaxies across more than 50 spectroscopically selected SDSS groups and clusters. Forty-eight ram pressure candidates are visually identified in these systems using high quality UNIONS imaging from the Canada-France Hawaii Telescope, covering ~6200 and ~2800 square degrees in the u- and r-bands respectively. Ram pressure candidates are found in groups and clusters spanning a wide range in halo mass and include ~30 ram pressure candidates in the group regime ($M_h < 10^{14}$). The observed frequency of ram pressure candidates shows substantial scatter with group/cluster mass, but on average is larger in clusters ($M_h > 10^{14}\,M_\odot$) than groups ($M_h < 10^{14}\,M_\odot$) by a factor of ~2. We find that ram pressure candidates are most commonly low-mass galaxies and have enhanced star formation rates relative to star-forming field galaxies. The enhancement in star formation is largely independent of galaxy mass and strongest for galaxies in clusters. As a result of the large survey footprint and excellent image quality from UNIONS, we are able to identify disturbed galaxies, potentially affected by ram pressure stripping, across a wide range of host environment. ",Ram Pressure Candidates in UNIONS
39,1452991809253257221,734477951866183681,Agustín Bonifacio,"['New paper on ""Cycles to compute the full set of many-to-many stable matchings"" available at ArXiv!\n\n<LINK>']",https://arxiv.org/abs/2110.11846#,"In a many-to-many matching model in which agents' preferences satisfy substitutability and the law of aggregate demand, we present an algorithm to compute the full set of stable matchings. This algorithm relies on the idea of ""cycles in preferences"" and generalizes the algorithm presented in Roth and Sotomayor (1990) for the one-to-one model. ",Cycles to compute the full set of many-to-many stable matchings
40,1452943944095092739,304697294,Sandor Kruk 🇺🇦,['#PaperDay: @galaxyzoo work led by @mike_walmsley_ uses #ArtificialIntelligence to search for galaxies similar to your favourites  and also find anomalies in the sky.\n\nBlog on Zoobot 👉<LINK>\nPaper 👉<LINK> <LINK>'],https://arxiv.org/abs/2110.12735,"Astronomers have typically set out to solve supervised machine learning problems by creating their own representations from scratch. We show that deep learning models trained to answer every Galaxy Zoo DECaLS question learn meaningful semantic representations of galaxies that are useful for new tasks on which the models were never trained. We exploit these representations to outperform existing approaches at several practical tasks crucial for investigating large galaxy samples. The first task is identifying galaxies of similar morphology to a query galaxy. Given a single galaxy assigned a free text tag by humans (e.g. `#diffuse'), we can find galaxies matching that tag for most tags. The second task is identifying the most interesting anomalies to a particular researcher. Our approach is 100\% accurate at identifying the most interesting 100 anomalies (as judged by Galaxy Zoo 2 volunteers). The third task is adapting a model to solve a new task using only a small number of newly-labelled galaxies. Models fine-tuned from our representation are better able to identify ring galaxies than models fine-tuned from terrestrial images (ImageNet) or trained from scratch. We solve each task with very few new labels; either one (for the similarity search) or several hundred (for anomaly detection or fine-tuning). This challenges the longstanding view that deep supervised methods require new large labelled datasets for practical use in astronomy. To help the community benefit from our pretrained models, we release our fine-tuning code zoobot. Zoobot is accessible to researchers with no prior experience in deep learning. ","Practical Galaxy Morphology Tools from Deep Supervised Representation
  Learning"
41,1452892660910501897,928668522242244608,Mike Walmsley,"['CNN trained on Galaxy Zoo can solve new tasks they were never trained for. \n\nPaper thread! <LINK> /n <LINK>', 'Our latest GZ models learn to solve every GZ question at once. Solving this broad task makes them learn a semantically meaningful representation, which we can use to...2/n', '1) Find similar galaxies to a query galaxy. Play with it at https://t.co/BAfLeZOCyV\n\n2) Find the anomalies you are personally most interested in, by understanding visual similarity and learning from your feedback.\n\nand most importantly, 3) ...', 'Finetune to solve new problems. Pretraining on GZ works much better than Imagenet or from scratch. You can classify rings with just ~10 examples.\n\nFind your own galaxies with this Colab: https://t.co/C5m8aSN6rv \n\nAll the code is public and documented. Includes pretrained models.', 'More in our blogs for researchers (https://t.co/B3KljmRrDb), @galaxyzoo volunteers (https://t.co/1R5L9ceywZ) and of course the paper itself (https://t.co/J1EFJuQ8oo).', 'Special thanks to Michelle Lochner for her Astronomaly work, which inspired the anomaly-finding section https://t.co/KBv7txZXNn. We plan on adding this new method into Astronomaly.\n\nAlso special thanks to @radastrat  and @chrislintott  for entertaining my random tangents.']",https://arxiv.org/abs/2110.12735,"Astronomers have typically set out to solve supervised machine learning problems by creating their own representations from scratch. We show that deep learning models trained to answer every Galaxy Zoo DECaLS question learn meaningful semantic representations of galaxies that are useful for new tasks on which the models were never trained. We exploit these representations to outperform existing approaches at several practical tasks crucial for investigating large galaxy samples. The first task is identifying galaxies of similar morphology to a query galaxy. Given a single galaxy assigned a free text tag by humans (e.g. `#diffuse'), we can find galaxies matching that tag for most tags. The second task is identifying the most interesting anomalies to a particular researcher. Our approach is 100\% accurate at identifying the most interesting 100 anomalies (as judged by Galaxy Zoo 2 volunteers). The third task is adapting a model to solve a new task using only a small number of newly-labelled galaxies. Models fine-tuned from our representation are better able to identify ring galaxies than models fine-tuned from terrestrial images (ImageNet) or trained from scratch. We solve each task with very few new labels; either one (for the similarity search) or several hundred (for anomaly detection or fine-tuning). This challenges the longstanding view that deep supervised methods require new large labelled datasets for practical use in astronomy. To help the community benefit from our pretrained models, we release our fine-tuning code zoobot. Zoobot is accessible to researchers with no prior experience in deep learning. ","Practical Galaxy Morphology Tools from Deep Supervised Representation
  Learning"
42,1452847659308359682,2305644236,Ryan Ridden,"[""💥 New paper!💥\nWith both @NASAHubble and @NASA_TESS we were able to get key early observations of SN2020fqv that let us piece together what happened at the explosive end of this star's life. \n\nCheck out the paper for more info! \n<LINK> <LINK>""]",https://arxiv.org/abs/2110.10742,"We present observations of SN 2020fqv, a Virgo-cluster Type II core-collapse supernova (CCSN) with a high temporal resolution light curve from the Transiting Exoplanet Survey Satellite (TESS) covering the time of explosion; ultraviolet (UV) spectroscopy from the Hubble Space Telescope (HST) starting 3.3 days post-explosion; ground-based spectroscopic observations starting 1.1~days post-explosion; along with extensive photometric observations. Massive stars have complicated mass-loss histories leading up to their death as CCSNe, creating circumstellar medium (CSM) with which the SNe interact. Observations during the first few days post-explosion can provide important information about the mass-loss rate during the late stages of stellar evolution. Model fits to the quasi-bolometric light curve of SN 2020fqv reveal ~0.23 $M_{\odot}$ of CSM confined within ~1450 $R_{\odot}$ ($10^{14}$ cm) from its progenitor star. Early spectra (<4 days post-explosion), both from HST and ground-based observatories, show emission features from high-ionization metal species from the outer, optically thin part of this CSM. We find that the CSM is consistent with an eruption caused by the injection of $\sim$$5\times 10^{46}$ erg into the stellar envelope $\sim$300 days pre-explosion, potentially from a nuclear burning instability at the onset of oxygen burning. Light-curve fitting, nebular spectroscopy, and pre-explosion \textit{HST} imaging consistently point to a red supergiant (RSG) progenitor with $M_{\rm ZAMS}$$\approx$$13.5$--$15 \, M_{\odot}$, typical for SN~II progenitor stars. This finding demonstrates that a typical RSG, like the progenitor of SN 2020fqv, has a complicated mass-loss history immediately before core collapse. ","Progenitor and Close-In Circumstellar Medium of Type II Supernova
  2020fqv from High-Cadence Photometry and Ultra-Rapid UV Spectroscopy"
43,1452842758767403009,29912873,Mike Brown,"[""New Planet Nine paper just released: <LINK>\n\nI'm excited about this paper for 3 different reasons (sadly, none of the reasons are that we found P9)"", '(as usual, you can read much more detail at https://t.co/MiAnVFCglw)', ""(1) We developed some sweet new algorithms to link possible transients across the 3 years of the ZTF survey, and they now work well. We could have found any P9 that was detected any 7 times over the 3 years. Ecliptic? Sure. Galactic plane? No big deal. It's a good survey."", '(2) Along with the paper, we released a full Planet Nine reference population, statistically drawn from the samples of our previous paper, and used it to understand how much parameter space the ZTF survey ruled out  https://t.co/b1omjLF2GS', '(2.1) ZTF, amazingly, ruled out 56% of parameter space for P9. Pretty much everything brighter than ~20.7 would have been detected except for a few spots in the south. Also the southern galactic plane. Ugh the southern galactic plane.', '(3) But also exciting is that with this reference population, anyone can download the data, see where the predictions put P9, see which parts of parameter space need searching.', '(3.1) Have a telescope? Doing a survey? This will help guide you to where the predictions say P9 should be hiding and tell you if you are going deep enough (or too deep bless you and your huge glass).', ""(end) I like this paper a lot and think the algorithms and reference population will be very helpful for the search. So use them. There is still 46% of parameter space left to go, but we are closing in. Let's go."", '@FieryPhoenix7 I vote 100%. But I am always optimistic, so should probably be ignored.', '@realgregtoshi George', ""@astrokiwi if we wait long enough everything will eventually emerge.  i don't have any plans for the next hundred years, so no big deal.""]",https://arxiv.org/abs/2110.13117,"Recent estimates of the characteristics of Planet Nine have suggested that it could be closer than originally assumed. Such a Planet Nine would also be brighter than originally assumed, suggesting the possibility that it has already been observed in wide-field moderate-depth surveys. We search for Planet Nine in the Zwicky Transient Facility public archive and find no candidates. Using known asteroids to calculate the magnitude limit of the survey, we find that we should have detected Planet Nine throughout most of the northern portion of its predicted orbit -- including within the galactic plane -- to a 95% detection efficiency of approximately $V=20.5$. To aid in understanding detection limits for this and future analyses, we present a full-sky synthetic Planet Nine population drawn from a statistical sampling of predicted Planet Nine orbits. We use this reference population to estimate that this survey rules out 56% of predicted Planet Nine phase space, and we demonstrate how future analyses can use the same synthetic population to continue to constrain the amount of parameter space effectively searched for Planet Nine. ","A search for Planet Nine using the Zwicky Transient Facility public
  archive"
44,1452818407334817796,750009830786605057,"Michele L. Silverstein, Ph.D. (she/her/hers)","['New paper! The LHS 1678 exoplanet system includes an M dwarf in the Gaia/Jao Gap, 2 confirmed small planets discovered by TESS (1 ultra-short period &amp; 1 Venus-zone), a sneaky brown dwarf w/ a decades-long orbit, and a candidate 3rd planet in 4:3 resonance! <LINK>', ""One of the coolest things about this system, in my opinion, is that in the past, the star likely expanded &amp; contracted over billions of years. We know this bc of its association with a gap in the HR diagram. We don't know how this affected the how the planets' formed or evolved!"", ""LHS 1678 is arguably the only TESS planet host so far clearly in the gap. Until our LHS 1678 paper, no one had published a paper raising the issue of how this relates to exoplanets (to my knowledge). For reference, here's the Gaia/Jao gap discovery paper https://t.co/KFnbMEQSBs"", ""I don't use twitter a whole lot, and it's late here. So I will stop here for now. But I think every member of this system is exciting for a different reason, and I hope you will feel compelled to look at the paper and see so for yourself. :)""]",https://arxiv.org/abs/2110.12079,"We present the TESS discovery of the LHS 1678 (TOI-696) exoplanet system, comprised of two approximately Earth-sized transiting planets and a likely astrometric brown dwarf orbiting a bright ($V_J$=12.5, $K_s$=8.3) M2 dwarf at 19.9 pc. The two TESS-detected planets are of radius 0.70$\pm$0.04 $R_\oplus$ and 0.98$\pm$0.06 $R_\oplus$ in 0.86-day and 3.69-day orbits, respectively. Both planets are validated and characterized via ground-based follow-up observations. HARPS RV monitoring yields 97.7 percentile mass upper limits of 0.35 $M_\oplus$ and 1.4 $M_\oplus$ for planets b and c, respectively. The astrometric companion detected by the CTIO/SMARTS 0.9m has an orbital period on the order of decades and is undetected by other means. Additional ground-based observations constrain the companion to being a high-mass brown dwarf or smaller. Each planet is of unique interest; the inner planet has an ultra-short period, and the outer planet is in the Venus zone. Both are promising targets for atmospheric characterization with the JWST and mass measurements via extreme-precision radial velocity. A third planet candidate of radius 0.9$\pm$0.1 $R_\oplus$ in a 4.97-day orbit is also identified in multi-Cycle TESS data for validation in future work. The host star is associated with an observed gap in the lower main sequence of the Hertzsprung-Russell diagram. This gap is tied to the transition from partially- to fully-convective interiors in M dwarfs, and the effect of the associated stellar astrophysics on exoplanet evolution is currently unknown. The culmination of these system properties makes LHS 1678 a unique, compelling playground for comparative exoplanet science and understanding the formation and evolution of small, short-period exoplanets orbiting low-mass stars. ","The LHS 1678 System: Two Earth-Sized Transiting Planets and an
  Astrometric Companion Orbiting an M Dwarf Near the Convective Boundary at 20
  pc"
45,1452663039556145154,1375527360666107904,John Martyn,"['Happy to share our new paper “Efficient Fully-Coherent Hamiltonian Simulation”! <LINK>\n\nWe analyze simulation algorithms that succeed with arbitrarily high probability 1-δ and only require a single copy of the initial state, a property we call ""fully-coherent"". <LINK>', 'While prior fully-coherent algorithms have a ln(1/δ) multiplicative factor in their query complexity, here we develop a novel algorithm that achieves a complexity additive in ln(1/δ)! And we demonstrate the advantage of our algorithm by simulating the Heisenberg model! https://t.co/OZU8CD030X']",https://arxiv.org/abs/2110.11327,"Hamiltonian simulation is a fundamental problem at the heart of quantum computation, and the associated simulation algorithms are useful building blocks for designing larger quantum algorithms. In order to be successfully concatenated into a larger quantum algorithm, a Hamiltonian simulation algorithm must succeed with arbitrarily high success probability $1-\delta$ while only requiring a single copy of the initial state, a property which we call fully-coherent. Although optimal Hamiltonian simulation has been achieved by quantum signal processing (QSP), with query complexity linear in time $t$ and logarithmic in inverse error $\ln(1/\epsilon)$, the conventional algorithm is not fully-coherent as it only succeeds with probability close to $1/4$. While this simulation algorithm can be made fully-coherent by employing amplitude amplification at the expense of appending a $\ln(1/\delta)$ multiplicative factor to the query complexity, here we develop a new fully-coherent Hamiltonian simulation algorithm that achieves a query complexity additive in $\ln(1/\delta)$: $\Theta\big( \|\mathcal{H}\| |t| + \ln(1/\epsilon) + \ln(1/\delta)\big)$. We accomplish this by introducing the concept of a pre-transformation: we first compress the spectrum of the Hamiltonian with an affine transformation and subsequently apply to it a QSP polynomial that approximates the complex exponential only over the range of the compressed spectrum. We further numerically analyze the complexity of this algorithm and demonstrate its application to the simulation of the Heisenberg model in constant and time-dependent external magnetic fields. We believe that this efficient fully-coherent Hamiltonian simulation algorithm can serve as a useful subroutine in quantum algorithms where maintaining coherence is paramount. ",Efficient Fully-Coherent Hamiltonian Simulation
46,1452606495586095111,1144194850746540032,Jung-Woo Ha,"['Happy to share our paper on new practical CL setup. Typical CL assumes that inferences happen at the end of tasks. In reality, we don\'t know task boundary. So, ""i-blurry"" address an online, class incremental blurry setup with anytime inference.\n@ppolon \n<LINK>']",https://arxiv.org/abs/2110.10031,"Despite rapid advances in continual learning, a large body of research is devoted to improving performance in the existing setups. While a handful of work do propose new continual learning setups, they still lack practicality in certain aspects. For better practicality, we first propose a novel continual learning setup that is online, task-free, class-incremental, of blurry task boundaries and subject to inference queries at any moment. We additionally propose a new metric to better measure the performance of the continual learning methods subject to inference queries at any moment. To address the challenging setup and evaluation protocol, we propose an effective method that employs a new memory management scheme and novel learning techniques. Our empirical validation demonstrates that the proposed method outperforms prior arts by large margins. Code and data splits are available at this https URL ","Online Continual Learning on Class Incremental Blurry Task Configuration
  with Anytime Inference"
47,1452538432207601674,23980621,"Brett Morris, PhD","['New paper: a method for inferring 2D temperature maps of giant planets from phase curves. We rely on a constrained set of spherical harmonics which produce shapes like GCM temperature maps with few free parameters.\n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'Why bother? \n\n1) In the JWST era, we will have phase curves with near-equitable mixtures of thermal emission and reflected light. Joint inference will be necessary!\n2) Comparing phase curves across bandpasses should infer an underlying temperature map at each pressure/wavelength. https://t.co/JzFj6mUKHY', '3) We wanted to fit GCM temperature maps directly with the same framework that we use to fit thermal phase curves. \n4) Phase curves constrain only the longitudinal variations in temperature. If we can do (3), then we can use GCM map shape priors to inform latitudinal dimension.', ""The method: use @KevinHeng1 2014 shallow-water derivations to produce basis maps that describe the temperature field. One of this paper's 272 equations (no joke) can describe T-maps after considering effects of rotation, heating, and drag/friction.\n\nPaper: https://t.co/cmjN0pte5Y https://t.co/LgxnUfcZlk"", 'The result: there are five free parameters which describe the temperature map at the lowest spherical harmonic order, but two describe latitudinal variations and we can derive priors for them from GCMs. This means we need only ~three free parameters to fit a thermal phase curve! https://t.co/4qLXtRxtgn', 'We used this ""hml"" model to fit two-channel Spitzer observations of 8 hot Jupiters, re-reduced by Brice-Olivier Demory, and found results consistent with those in the literature. \n\nOne key difference though is our interpretation of Bond albedos and heat redist efficiencies... https://t.co/8USTovKtx7', ""The Cowan &amp; Agol A_B and epsilon derivation assumes a single dayside and a single nightside temperature, and doesn't hold when you have a temperature map with more than two temperatures. We derive 2D versions of A_B and epsilon, and report the values for 8 hot Jupiters. https://t.co/OqloGL3HAS"", 'Here are temperature maps for eight hot Jupiters, where the latitudinal dimension is extrapolated from GCMs and the longitudinal dimension is direct from the phase curve: https://t.co/4flnjvlye5', 'And here are temperature map parameters derived for (1) GCMs by fitting their temperature maps directly (blue); and (2) maps inferred from Spitzer phase curves (black/red). We can use the same parameters for both methods and compare! https://t.co/KSa2K4MQGJ', 'The code for computing these phase curves and the temperature maps which produce them is open source, and implemented in cython, theano, and JAX, so you can get your Bayesian inference on, ASAP!\n\nCode: https://t.co/JdssRDnxPd\nDocs: https://t.co/jLx0KK9Ent', 'Big thanks to @KevinHeng1 for encouraging me to do this, and having so much patience with this paper over the last two years; to @kjonesastro for being my constant sounding board and debugging-companion on this work; and to our awesome referee who improved the manuscript.', ""@V_Parmentier @EleeAstro Thanks! It's a different technique, based on shallow-water models to predict the perturbations to the temperature map.""]",https://arxiv.org/abs/2110.11837,"Thermal phase curves of exoplanet atmospheres have revealed temperature maps as a function of planetary longitude, often by sinusoidal decomposition of the phase curve. We construct a framework for describing two-dimensional temperature maps of exoplanets with mathematical basis functions derived for a fluid layer on a rotating, heated sphere with drag/friction, which are generalizations of spherical harmonics. These basis functions naturally produce physically-motivated temperature maps for exoplanets with few free parameters. We investigate best practices for applying this framework to temperature maps of hot Jupiters by splitting the problem into two parts: (1) we constrain the temperature map as a function of latitude by tuning the basis functions to reproduce general circulation model (GCM) outputs, since disk-integrated phase curve observations do not constrain this dimension; and (2) we infer the temperature maps of real hot Jupiters using original reductions of several Spitzer phase curves, which directly constrain the temperature variations with longitude. The resulting phase curves can be described with only three free parameters per bandpass -- an efficiency improvement over the usual five or so used to describe sinusoidal decompositions of phase curves. Upon obtaining the hemispherically averaged dayside and nightside temperatures, the standard approach would be to use zero-dimensional box models to infer the Bond albedo and redistribution efficiency. We elucidate the limitation of these box models by demonstrating that negative Bond albedos may be obtained due to a choice of boundary condition on the nightside temperature. We propose generalized definitions for the Bond albedo and heat redistribution efficiency for use with two-dimensional (2D) temperature maps. Open-source software called kelp is provided to efficiently compute these phase curves. ",Physically-motivated basis functions for temperature maps of exoplanets
48,1452495886743851014,1277309540313300995,Soufiane Hayou,"['New paper on implicit regularization in DNNs!\nWe show that the implicit regularization induced by training algorithms is not only due to batch noise; there is another **structural** implicit regularization effect that is purely due to the architecture. \n\n<LINK>', '1) We introduce the Equilibrium Hypothesis which basically conjectures that layers that achieve a **balance in information** between forward and backward propagation are more **effectively trained**.', ""2) For Fully-connected DNNs, we obtain a nice formula for layer indices that achieve this Equilibrium, it's given by l = O(L^{3/5}) where L is the depth of the network and l is the layer index.""]",https://arxiv.org/abs/2110.11749,"Modern Deep Neural Networks (DNNs) exhibit impressive generalization properties on a variety of tasks without explicit regularization, suggesting the existence of hidden regularization effects. Recent work by Baratin et al. (2021) sheds light on an intriguing implicit regularization effect, showing that some layers are much more aligned with data labels than other layers. This suggests that as the network grows in depth and width, an implicit layer selection phenomenon occurs during training. In this work, we provide the first explanation for this alignment hierarchy. We introduce and empirically validate the Equilibrium Hypothesis which states that the layers that achieve some balance between forward and backward information loss are the ones with the highest alignment to data labels. Our experiments demonstrate an excellent match with the theoretical predictions. ","The Equilibrium Hypothesis: Rethinking implicit regularization in Deep
  Neural Networks"
49,1452113554744623105,59962128,Satoshi Matsuoka,"['Our new paper \'Digital transformation of droplet/aerosol infection risk assessment realized on ""Fugaku"" for the fight against COVID-19\' is out on Arxiv. <LINK> More than 1000 digital twins of aerosol infection scenarios was evaluated over 17.5mil Fugaku nodeHrs']",https://arxiv.org/abs/2110.09769,"The fastest supercomputer in 2020, Fugaku, has not only achieved digital transformation of epidemiology in allowing end-to-end, detailed quantitative modeling of COVID-19 transmissions for the first time, but also transformed the behavior of the entire Japanese public through its detailed analysis of transmission risks in multitudes of societal situations entailing heavy risks. A novel aerosol simulation methodology was synthesized out of a combination of a new CFD methods meeting industrial demands, CUBE, which not only allowed the simulations to scale massively with high resolution required for micrometer virus-containing aerosol particles, but also extremely rapid time-to-solution due to its ability to generate the digital twins representing multitudes of societal situations in minutes not week, attaining true overall application high performance; such simulations have been running for the past 1.5 years on Fugaku, cumulatively consuming top supercomputer-class resources and the result communicated by the media as well as becoming official public policies. ","Digital transformation of droplet/aerosol infection risk assessment
  realized on ""Fugaku"" for the fight against COVID-19"
50,1451862497288286212,795877354266456064,KoheiKamadaPhys,"['Submitted a new paper with Soichiro and Hiromasa on arXiv, \n<LINK> . \nWe studied a reheating scenario (or how to realize the hot Big Bang Universe) in the inflationary models with ""kination"" (the Universe dominated by the kinetic energy of a scalar field).', 'We considered the U(1) gauge field amplification through the Chern-Simons coupling between the inflaton scalar and the gauge field and the subsequent Schwinger effect (including the thermalization of the produced particles).', 'Realizing a hot Big Bang Universe after kination is one of the most important problems in this scenario, but our proposal would be more realistic and reasonable than other existing proposals. \nSoichiro and Hiromasa did a very  hard work especially during their thesis writings!']",https://arxiv.org/abs/2110.10822,"In a class of (pseudoscalar) inflation, inflationary phase is followed by a kination phase, where the Universe is dominated by the kinetic energy of the inflaton that runs away in a vanishing scalar potential. In this class of postinflationary evolution of the Universe, reheating of the Universe cannot be achieved by the inflaton particle decay, which requires its coherent oscillation in a quadratic potential. In this study, we explore the U(1) gauge field production through the Chern-Simons coupling between the pseudoscalar inflaton and the gauge field during the kination era and examine the subsequent pair-particle production induced by the amplified gauge field known as the Schwinger effect, which can lead to reheating of the Universe. We find that with a rough estimate of the Schwinger effect for the Standard Model hyper U(1) gauge field and subsequent thermalization of the pair-produced particles, a successful reheating of the Universe can be achieved by their eventual domination over the kinetic energy of the inflaton, with some reasonable parameter sets. This can be understood as a concrete realization of the ""Schwinger reheating"". Constraints from the later-time cosmology are also discussed. ","Gauge Field Production and Schwinger Reheating in Runaway Axion
  Inflation"
51,1451610887177965568,1253058777059917825,Juliette Bruce,"['New Paper!! \n\n""Characterizing Multigraded Regularity on Products of Projective Spaces"" - with Lauren Cranton Heller and Mahrud Sayrafi (@mahrudsay).\n\n<LINK>\n\n1/n', ""Short: We give an algebraic characterization of multigraded Castelnuovo-Mumford regularity.\n\nMedium: We explore the relationship between: 1) multigraded Castelnuovo-Mumford regularity, 2) truncations, 3) Betti #'s, and 4) virtual resolutions.\n\nLonger....\n\n2/n"", 'In 1966 Mumford introduced the Castelnuovo–Mumford regularity of a coherent sheaf on projective space. \n\nRoughly you should think of it as a number associated to a sheaf that is: \n1) defined in terms of certain cohomology vanishing,\n2) a measure of geometric complexity. \n\n3/n', 'Why did he care? It turns out this sort of measure of geo. complexity (and a way to uni. bound it) is useful tool when constructing moduli spaces (Hilbert scheme etc).\n\nThe fun slogan in my head is that regularity is what helps you email your favorite scheme to a friend. \n\n4/n', 'Flash forward to 1984, Eisenbud and Goto showed that  regularity, something entirely geometric, can also be captured algebraically! \n\n5/n', 'In particular, if M is the section module of your sheaf you can compute the regularity in terms of either \n1) local cohomology, \n2) truncations of M having linear resolutions, or \n3) the minimal res. of M!!\n\nPut differently the conditions in the picture are equivalent!\n\n6/n https://t.co/uAbMXfZsMh', 'Amazing! This is one of those beautiful results that I love because it highlights the amazing relationship between coherent sheaves on projective (i.e. geometry) and graded modules over the polynomial ring (i.e. algebra)\n\n** insert that meme of two hands meeting**\n\n7/n', ""Flash forward again to the 2000's and  we've got toric varieties and people are beginning to build a similar algebra -to-geometry bridge: \n\nsheaves on a toric variety &lt;---&gt; G-graded modules on the Cox ring\n\nBut life is complicated because that G=Pic X is not always a Z. :/ \n\n8/n"", 'In this spirit, in 2004, Maclagan &amp; Smith introduce the notion of multigraded Castelnuovo-Mumford regularity for a sheaf on a toric variety!\n\nFollowing the original idea of Mumford they define it in terms of certain cohomology vanishing. \n\nBut.... \n\nhttps://t.co/QLzXRcB98B\n\n9/n', ""Since that G is not always a Z, multigraded Castelnuovo-Mumford regularity is no longer a number... or even a single element of G, but instead a it's a semi-group inside of G. (Here G is the Picard group of the toric variety.) \n\n10/n"", 'For example, the multigraded regularity of a smooth hyperelliptic curve of genus 4 embedded into P1 × P2 as a curve of degree (2, 8) is (probably) the semi-group given in pink...\n\n11/n https://t.co/qB0e6s4iNL', 'Ok well this makes life more complicated, but what about the nice connections between regularity and algebraic things like truncations having linear resolutions and Betti number?\n\n12/n', 'Well Maclagan and Smith and later many people (Sidman, Van Tuyl, Hering, H`a, Chardin, etc.) showed\nthat there are relationships between these things, for example, you can ""bound"" regularity in terms of Betti numbers, but the connects were never as tight as we might hope.\n\n13/n', 'And for good reason! \n\nAs Lauren, Mahrud, and I show the multigraded regularity of a module is **not** determined by either its multigraded Betti number or its truncation having a linear resolution.\n\nWell that stinks....\n\n14/n https://t.co/8XpQOKbcMn', ""Brief aside: Since we are now searching for a semi-group instead of just a number, and the nice geomtety-to-algebra bridge fails actually computing multigraded regularity is extremely difficult. \n\nM2 quickly grinds to a halt even for fairly simple examples (say CI's). :'( \n\n15/n"", ""Well this sucks. Our bridge is broken and we can't compute things. 😭😭\n\nHere is where our new work comes in to try and give us some hope!! Well at least for products of projective spaces. (Note in this case G\\cong Z^r for some r). \n\n16/n"", ""First, we show that while you can't characterize a the multigraded reg. of a module M by when the truncation M_{&gt;=d} has a linear resolution, you can characterize it in terms of M_{&gt;=d} having what we call a **quasi-linear** resolution.\n\n17/n https://t.co/esCOTyiTHl"", 'Roughly quasi-linear is a weaker than linear where one allows a few extra Betti numbers to appear in your resolution. \n\n(Aside where this condition comes from, how it generalizes, and what its other implications are is still somewhat mysterious, at least to me.) \n\n18/n https://t.co/SNNWveSlJ9', ""Great! Our we've managed to get one lane of our bridge back up and open!! \n\nEven better since M2 is good a computing free resolutions this means we now have a computationally easier way to check that a module is d-regular. (Although computing this for all d is still open.) \n\n19/n"", 'Next we turn our attention to connection between the multigraded Betti numbers of M and multigraded regularity. \n\nAs our examples above showed we can hope to re-build this part of the bridge fully. \n\n20/n', 'However, we are able to use our quasi-linear characterization to show that the multigraded Betti numbers gives a new bound on the multigraded regularity!! \n\nIn fact our bound recovers the original condition of Eisenbud and Goto for a single projective space. \n\n21/n https://t.co/DU2DoN4CZM', 'In general the bound is not equal to the multigraded reg., but we are able to show that for basically all complete intersections arising geometrically it is sharp!!! \n\nAgain before this I believe the only case of this that was know (and in the lit.) was for a hypersurface!\n\n22/n https://t.co/aewTEOJUsT', 'Ok this is getting a little too long, but we also do a bunch of other stuff in this paper like:\n\n+ Giving a cohomological criteria for M_{&gt;=d} having a linear resolution,\n+ Introduce the notion of ""minimal"" virtual resolutions and explore their structure, \n+ much much more\n\n23/n', 'Briefly what is the keywords for the special sauce that helps us do all of this: \n1) lots and lots of spectral sequences, \n2) Fourier-Mukai transforms, \n3) virtual resolutions,\n4) cotangent sheaves. \n\n24/n https://t.co/dVZxnR9inr', ""This was a ton of fun to work on!! \n\nIf you'd like to hear more of this story invite me -- or even better Lauren or Mahrud -- (we will all be on the various markets in the next few years) to give a talk about it!!\n\nPS: Any mistakes or historical fibs are entirely my own. \n\n25/n"", 'I will end by saying that I think a good motto is: \n\nBuilding the geometry-to-algebra bridge on arbitrary toric varieties is much much harder than on P^n.\n\nThat said I am hopefully this project will bear a lot of interesting fruit in the coming years!! \n\n26/n']",https://arxiv.org/abs/2110.10705,"We explore the relationship between multigraded Castelnuovo-Mumford regularity, truncations, Betti numbers, and virtual resolutions. We prove that on a product of projective spaces $X$, the multigraded regularity region of a module $M$ is determined by the minimal graded free resolutions of the truncations $M_{\geq\mathbf{d}}$ for $\mathbf{d}\in\operatorname{Pic}X$. Further, by relating the minimal graded free resolutions of $M$ and $M_{\geq\mathbf{d}}$ we provide a new bound on multigraded regularity of $M$ in terms of its Betti numbers. Using this characterization of regularity and this bound we also compute the multigraded Castelnuovo-Mumford regularity for a wide class of complete intersections. ",Characterizing Multigraded Regularity on Products of Projective Spaces
52,1451583718330540035,1025939776401158144,Alexander J. Sutherland,"['New paper (<LINK>, joint with Curtis Heberle) posted, so its time for a thread!\n\nQ: Given a set of polynomials of degree at most d, how many variables do they need to be in to guarantee we can find a solution by solving polynomials of degree at most d? | 1/17', 'We recover an algorithm of Sylvester (from 1887!) that addresses this question and give a modern description of the algorithm in terms of algebraic geometry (i.e. in terms of finding rational points on varieties).\n\nMore on this to come! | 2/17', 'We then use this algorithm to establish new upper bounds on RD(n), the resolvent degree of the general degree n polynomial.\n\nRD(n) is essentially how ""difficult"" it is to solve a degree n polynomial (see https://t.co/J2I2VIypTu for more on resolvent degree). | 3/17', 'Now, consider a set S = {f_1,...,f_s} of homogeneous polynomials in x_0,...,x_r of degree at most d.\n\nWe can consider the set of points P in ℙ^r (r-dimensional projective space) such that f_j(P)=0 for all 1≤j≤s. | 4/17', 'These points form an algebraic variety, which we denote by 𝕍(S). \n\nWe can re-state our question as ""how can we determine a point of 𝕍(S) by solving polynomials of degree at most d?"" | 5/17', 'We can find a point of 𝕍(S) directly by solving a polynomial of degree whose degree is the product deg(f_1) ∙∙∙ deg(f_s), but this is much larger than d!\n\n(! for emphasis, not a factorial) | 6/17', ""Now, let S' = S \\{f_1} = {f_2,...,f_s}.\n\nIf we could find a line L ⊆ 𝕍(S'), then we could find a point of 𝕍(S) by solving a polynomial of degree deg(f_1) (by looking at L∩𝕍(f_1) = L∩𝕍(S) ).\n\nSo, how can we find lines on varieties? | 7/17"", 'In my previous paper (https://t.co/KkjQatgbzK), I talk about the polar cone C(V;P) of a variety V at a point P.\n\nKey property: Any point Q of C(V;P) \\ {P} determines a line on V!\n\nAlso, C(V;P) only introduces polynomials / hypersurfaces of strictly smaller degree! | 8/17', ""Let's start outlining the algorithm!\n\n1.0) We start with 𝕍(S). \n\n1.1) We pass to a subsystem S' = S\\{f}, where f has maximal degree (which we call d) and consider 𝕍(S').\n\n1.2) It is easier* to find a point of S' than S, so we assume that we have a point P' of 𝕍(S') | 9/17"", ""1.3) We pass to the polar cone C(𝕍(S'),P'), which only introduces hypersurfaces of degree strictly less than d.\n\n1.4) Intersect C(𝕍(S'),P') with a hyperplane H that does not contain P'.\n\nNote that every point Q of C(𝕍(S'),P)∩H determines a line on 𝕍(S')! | 10/17"", 'So, we can determine a point of 𝕍(S) if we can determine a point Q of C(𝕍(S\'),P)∩H - which has one fewer hypersurface of maximal degree!\n\nThis is the core idea of what Sylvester calls the ""formula of reduction"". | 11/17', '2) Repeat steps 1.1 thru 1.4 as many times as necessary to get rid of all of the hypersurfaces of maximal degree.\n\nThis is the core idea of what Sylvester calls the ""formula of obliteration,"" which is why we call his algorithm the ""obliteration algorithm."" | 12/17', '3) Repeat step 2 until you are left with intersection of only hyperplanes. \n\nYou can determine a point of this intersection as soon as it is non-empty and the algorithm is done!\n\n*The ""easier"" in step 1.2 can (and is!) made precise in the paper | 13/17', 'We then apply this algorithm to certain varieties (iterated polar cones of Tschirnhaus complete intersections) to obtain the new upper bounds in the picture below! | 14/17 https://t.co/Li2Xe1wmse', 'To the best of my knowledge, this paper, along with the previous works https://t.co/KkjQatgbzK (Sutherland) and https://t.co/ohzSKKKs5W (Wolfson),  exhaust the techniques from both the classical and modern literature for getting upper bounds on RD(n). | 15/17', 'With this in mind, we lay out questions about both upper bounds on RD(n) and about the question from the first tweet of this thread in Subsection 4.3.\n\nI would give the questions here, but explaining the notation here would be a bit much | 16/17', 'I think this paper is really neat, so if you found this thread interesting, consider checking it out!\n\nYou can also check out https://t.co/KUkklelL53 (my website) or https://t.co/iimyvYeBom (which has a bunch of resolvent degree resources) for more.\n\nThanks for reading! | 17/17', ""@ananavaty97 @ClaudioJacobo @SC_Griffith @benblumsmith @sbagley @virtualcourtney I though some of y'all might find this interesting!""]",https://arxiv.org/abs/2110.08670,"For each $n$, let RD$(n)$ denote the minimum $d$ for which there exists a formula for the general polynomial of degree $n$ in algebraic functions of at most $d$ variables. In this paper, we recover an algorithm of Sylvester for determining non-zero solutions of systems of homogeneous polynomials, which we present from a modern algebro-geometric perspective. We then use this geometric algorithm to determine improved thresholds for upper bounds on RD$(n)$. ",Upper Bounds on Resolvent Degree via Sylvester's Obliteration Algorithm
53,1451581183477878789,3885912072,Marshall Johnson,"[""New paper day! I'm pleased to present our paper on measuring the spin-orbit alignment of the young transiting planet V1298 Tau b: <LINK> (1/N)"", 'We observed a single (partial) transit with three high-resolution spectrographs: TRES at FLWO, LBT/PEPSI, and Keck/HIRES. We see a large RV trend, likely due to starspots on the young star, but also a positive jump in the RVs at ingress: the Rossiter-McLaughlin effect! (2/N) https://t.co/CGNbvGIyed', 'We fit RVs taken for several weeks around transit with a Gaussian process to deal with the stellar variability (left), which allowed us to nicely model the R-M effect (right) and show the orbit is well-aligned. Combined with the stellar rotation, it is aligned in 3D (3/N) https://t.co/AjbZJzQlOE', 'We also revise the age of V1298 Tau using Gaia EDR3 data, finding a slightly older age than the discovery paper (David et al. 2019), at 28 +/- 4 Myr. (4/N) https://t.co/RR4PqtjlSu', 'My analysis of the stellar line profiles unfortunately failed to turn up the signal of the planet, but the PEPSI data show two beautiful spot complexes rotating across the stellar surface (the bright streaks in the plot), separated by ~45 degrees. (5/N) https://t.co/l24KlEZHTP', 'V1298 Tau b adds to the small but growing number of young planets with aligned orbits. We do not know of any planets with ages &lt;100 Myr with inclined orbits. Maybe orbits are tilted on longer timescales? But the numbers are still small... (6/N) https://t.co/sdaVdROIbu', ""...and it's not clear that the young planet population really evolves into the same population that's been observed around field stars. We need more young planets and more measurements! (7/N)"", ""One other cool thing is that, thanks to @afeinstein20's paper on planet c, V1298 Tau is one of only three systems with spin-orbit alignment measurements for multiple planets. We use this to show that V1298 Tau b &amp; c are consistent with coplanar, as expected (8/N) https://t.co/QBpg0tYrZx"", 'Thanks to all of my co-authors, especially Trevor David who did great work on the R-M fits. Co-authors on Twitter include @georgezhouastro @amannastro @rodluger @SarahCBlunt @Paul_Dalba @exoplaneteer (sorry if I missed anyone) (9/N)', 'Finally, thanks to Eric Gaidos &amp; Teru Hirano for coordinating submission of their paper on V1298 Tau b to arXiv. Go check out their paper here: https://t.co/BKz9tjYnIy They also find an aligned orbit but a younger age (10-25 Myr), and possible He absorption (10/10)', '(Bonus cat pic) Thanks also to Erwin for keeping me company during 19 long months of work from home during which much of this work was accomplished (11/10) https://t.co/aoMp49NJyN']",https://arxiv.org/abs/2110.10707,"The alignment of planetary orbits with respect to the stellar rotation preserves information on their dynamical histories. Measuring this angle for young planets help illuminate the mechanisms that create misaligned orbits for older planets, as different processes could operate over timescales ranging from a few Myr to a Gyr. We present spectroscopic transit observations of the young exoplanet V1298 Tau b; we update the age of V1298 Tau to be $28\pm4$ Myr based on Gaia EDR3 measurements. We observed a partial transit with Keck/HIRES and LBT/PEPSI, and detected the radial velocity anomaly due to the Rossiter-McLaughlin effect. V1298 Tau~b has a prograde, well-aligned orbit, with $\lambda = 4_{-10}^{+7 \circ}$. By combining the spectroscopically-measured $v\sin i_{\star}$ and the phtometrically-measured rotation period of the host star we also find that the orbit is aligned in 3D, $\psi = 8_{-7}^{+4 \circ}$ deg. Finally, we combine our obliquity constraints with a previous measurement for the interior planet V1298 Tau c to constrain the mutual inclination between the two planets to be $i_{\mathrm{mut}}=0^{\circ} \pm 19^{\circ}$. This measurements adds to the growing number of well-aligned planets at young ages, hinting that misalignments may be generated over timescales of longer than tens of Myr. The number of measurements, however, is still small, and this population may not be representative of the older planets that have been observed to date. We also present the derivation of the relationship between $i_{\mathrm{mut}}$, $\lambda$, and $i$ for two planets. ",An Aligned Orbit for the Young Planet V1298 Tau b
54,1451530922088558595,1152338625654226944,Megan Mansfield,"['Happy new paper day! 🎉 I’m excited to share a paper that has been in the works for quite a while and was just published at Nature Astronomy: “A unique hot Jupiter spectral sequence with evidence for compositional diversity” <LINK> <LINK>', 'In this paper, we performed a population study of all the hot Jupiters that have been observed in secondary eclipse with Hubble between 1.1-1.7 microns. 6 of these are new spectra we reduced for this paper, and the rest are from previous work. https://t.co/VGyEwrsKph', 'We defined a metric to measure how strong the main water absorption band in these spectra was for each planet. This gives us information on how much water the planet has and what the temperature-pressure profile on the planet’s dayside is like. https://t.co/6SC0J5Dcpe', 'We calculated the water feature strengths for our data, as well as for a bunch of 1D models. We found 3 main regimes of planet spectra: cool planets have absorption features, mid-temp planets have emission features, and hot planets have featureless spectra. https://t.co/EcAGaph1iq', 'We also compared these hot Jupiter data/models to data/models for brown dwarfs. Brown dwarfs at low temperatures look a lot like low-T hot Jupiters! But at high temperatures only hot Jupiters show thermal inversions. https://t.co/U6lFLzBWui', 'We found that the water feature strengths shown in the data match our models pretty well! Cool to see that we can pretty much explain the whole population with some simple equilibrium models….', '…but that’s not the whole story, because our data show a lot more scatter than our models. We found that changing the metallicity and C/O ratio in our models could reproduce this scatter, so perhaps this is evidence of these planets having distinct atmospheric compositions! https://t.co/EOSIeCZ5SY', 'How to know for sure? We’ll have to study these planets more, with JWST or high-resolution telescopes, to measure more molecules and better understand their compositions.', 'Thank you to all my collaborators for making this awesome paper possible! Mike Line, Jacob Bean, @jjfplanet, @V_Parmentier, @LindsLikesSpace, Eliza Kempton, @exoEhsan, @ExoSing, Mercedes López-Morales, @Claire_Baxt, @jmdesert, Mark Swain, and Gael Roudier. https://t.co/WMcWshOm3l', 'Also, thanks to University of Arizona science writer Daniel Stolte for writing an article that makes my research approachable to my non-science family! https://t.co/Gv8yj2QZdf', '@DianaPowell8 Thanks, and good question! We tested out some simple models with clouds (supplement fig.4) and basically showed that they damp out the features, as expected. But we didn’t really look in detail at what compositions you’d need if you add in clouds - perhaps an idea for the future!', '@decaelus Thank you!']",https://arxiv.org/abs/2110.11272,"The emergent spectra of close-in, giant exoplanets (""hot Jupiters"") are expected to be distinct from those of self-luminous objects with similar effective temperatures because hot Jupiters are primarily heated from above by their host stars rather than internally from the release of energy from their formation. Theoretical models predict a continuum of dayside spectra for hot Jupiters as a function of irradiation level, with the coolest planets having absorption features in their spectra, intermediate-temperature planets having emission features due to thermal inversions, and the hottest planets having blackbody-like spectra due to molecular dissociation and continuum opacity from the H- ion. Absorption and emission features have been detected in the spectra of a number of individual hot Jupiters, and population-level trends have been observed in photometric measurements. However, there has been no unified, population-level study of the thermal emission spectra of hot Jupiters such as has been done for cooler brown dwarfs and transmission spectra of hot Jupiters. Here we show that hot Jupiter secondary eclipse spectra centered around a water absorption band at 1.4 microns follow a common trend in water feature strength with temperature. The observed trend is broadly consistent with model predictions for how the thermal structures of solar-composition planets vary with irradiation level. Nevertheless, the ensemble of planets exhibits some degree of scatter around the mean trend for solar composition planets. The spread can be accounted for if the planets have modest variations in metallicity and/or elemental abundance ratios, which is expected from planet formation models. (abridged abstract) ","A unique hot Jupiter spectral sequence with evidence for compositional
  diversity"
55,1451528665255317507,1436796062,Cliff Burgess,"['New paper today: a Brans Dicke type scalar (ie a dilaton) whose matter coupling shd be seen in solar system tests, can evade those tests if accompanied by an axion with weak matter couplings.\n\n[2110.10352] Axion Homeopathy: Screening Dilaton Interactions <LINK>', 'It works because the axion-dilaton interactions can cause the large matter-dilaton coupling to give rise to a dominantly axion field - which does not influence test body motion as much - instead of a dilaton one.', 'The required axion-dilaton couplings arise for free in super symmetry. \n\nWe call it ‘axion homeopathy’ because the effect can survive for extremely small axion couplings, so long as they are nonzero.\n\nThe upshot is that gravitationally coupled light dilaton need not be ruled out.', 'We backed into this thinking about an approach to the cosmological constant problem (stay tuned) one of whose problems was a obligatory prediction of a dilaton with too-large matter couplings. It turned out to escape bounds anyway because it is accompanied by the required axion.', 'How can a significant effect survive at very small axion coupling? \n\nThe field eqs predict circular orbits (target-space geodesic) in the axion-dilaton plane whose radius varies inversely with axion-matter coupling. For zero coupling this is a straight line with constant axion.', 'For small nonzero coupling the solution is close to this line *if* boundary condition says axion derivative vanishes but is elsewhere on the circle for any other bc. In particular non commuting limits mean Brans Dicke need not be right if dilaton is large at infinity.', '@TM_Eubanks That is what we show in the paper. The PPN parameter is what matters because test particles mostly respond to the (Jordan frame) metric. The axion changes the PPN prediction, making it smaller than the naive Brans Dicke prediction.', '@TM_Eubanks Making *the deviation from GR* smaller, is what I should have written', '@TM_Eubanks I see, yes these would certainly be different than GR, though their precise size is likely model dependent. I suspect before you get to post-post PPN there will be other larger effects that will be the leading bounds, though we are still trying to figure out what these are.', '@lnmcjames I don’t think these fields need change that speed (any more than electromagnetic ones do)']",https://arxiv.org/abs/2110.10352,"Cosmologically active Brans-Dicke (or dilaton) scalar fields are generically ruled out by solar system tests of gravity unless their couplings to ordinary matter are much suppressed relative to gravitational strength, and this is a major hindrance when building realistic models of light dilatons coupled to matter. We propose a new mechanism for evading such bounds if matter also couples to a light axion, that exploits nonlinear target-space curvature interactions to qualitatively change how the fields respond to a gravitating source. We find that dilaton-matter couplings that would be excluded in the absence of an axion can become acceptable given an additional small axion-matter coupling, and this is possible because the axion-dilaton interactions end up converting the would-be dilaton profile into an axion profile. The trajectories of matter test bodies are then controlled by the much weaker axion-matter couplings and can easily be small enough to escape detection. We call this mechanism Axion Homeopathy because the evasion of the dilaton-coupling bounds persists for extremely small axion couplings provided only that they are nonzero. We explore the mechanism using axio-dilaton equations that are SL(2,R) invariant (as often appear in string compactifications), since for these the general solutions exterior to a spherically symmetric source can be found analytically. We use this solution to compute the relevant PPN parameters, $\beta$ and $\gamma$, and verify that their difference from unity can be much smaller than would have been true in the absence of axion-matter couplings and so can therefore evade the experimental bounds. ",Axion Homeopathy: Screening Dilaton Interactions
56,1451507919636377600,3139883618,Daniel Nevo,"['New preprint led by Tamir Zehavi: \n“A matching framework for truncation by death problems”\n\n<LINK>\n\nI am particularly excited about this one, first student-led paper from my group. Such a rewarding part of our job. More to come! <LINK>']",https://arxiv.org/abs/2110.10186,"Even in a carefully designed randomized trial, outcomes for some study participants can be missing, or more precisely, ill-defined, because participants had died prior to date of outcome collection. This problem, known as truncation by death, means that the treated and untreated are no longer balanced with respect to covariates determining survival. To overcome this problem, researchers often utilize principal stratification and focus on the Survivor Average Causal Effect (SACE). The SACE is the average causal effect among the subpopulation that will survive regardless of treatment status. In this paper, we present a new approach based on matching for SACE identification and estimation. We provide an identification result for the SACE that motivates the use of matching to restore the balance among the survivors. We discuss various practical issues, including the choice of distance measures, possibility of matching with replacement, post-matching crude and model-based SACE estimators, and non-parametric tests. Our simulation results demonstrate the flexibility and advantages of our approach. Because the cross-world assumptions needed for SACE identification can be too strong and are unfalsifiable, we also present sensitivity analysis techniques and illustrate their use in real data analysis. Finally, a recent alternative for SACE that does not demand cross-world unfalsifiable assumptions targets the conditional separable effects. We show how our approach can also be utilized to estimate these causal effects. ",A matching framework for truncation by death problems
57,1451458391965151235,1042167698908700672,Johannes Brandstetter,"[""Wow, wanna see how to beat CLIP with the new CLOOB? Fantastic work lead by my colleagues @fuerst_andreas and @LizRumetshofer (Sepp Hochreiter's group) applying modern Hopfield networks to image-text data.\n\nPaper: <LINK>\nBlogpost: <LINK> <LINK>"", 'May also be interesting and related to the paper of Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, @Yubei_Chen, @ylecun as they use the same objective\n\nhttps://t.co/0dpypS90Ua', 'For those who want a refresh on Hopfield networks check out the great video by @ykilcher\n“Hopfield Networks is All You Need (Paper Explained)”\nand have a read through my previous blog post\nhttps://t.co/rxwS5lEMBI', 'Co-authors: @VietTralala @HRamses2 Fei Tang, Johannes Lehner, David Kreil @m_k_kopp Günter Klambauer @angela_bitto Sepp Hochreiter']",https://arxiv.org/abs/2110.11316,"CLIP yielded impressive results on zero-shot transfer learning tasks and is considered as a foundation model like BERT or GPT3. CLIP vision models that have a rich representation are pre-trained using the InfoNCE objective and natural language supervision before they are fine-tuned on the particular tasks. Though CLIP excels at zero-shot transfer learning, it suffers from explaining away, that is, it focuses too much on few specific features and/or insufficiently extracts the covariance structure in the data. The former problem of focusing on few features only is caused by a saturation of the InfoNCE objective, which is severe for high mutual information. The latter problem of insufficiently exploiting the covariance structure is caused by a deficiency in extracting feature associations and co-occurrences. We introduce ""Contrastive Leave One Out Boost"" (CLOOB), which uses the InfoLOOB objective and modern Hopfield networks. In contrast to InfoNCE, the InfoLOOB objective (leave one out bound) does not saturate and works well for high mutual information. Modern Hopfield networks, on the other hand, allow to use retrieved embeddings, which have an enriched covariance structure via co-occurrences of stored features. We compare CLOOB to CLIP after pre-training on the Conceptual Captions and the YFCC dataset with respect to their zero-shot transfer learning performance on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer learning across all considered architectures and datasets. ",CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP
58,1451441894945402897,526115229,Kevin Heng,"['New paper led by CSH Fellow Pushkar Kopparla. Cautionary tale when trying to use GCMs to predict atmospheric variability of exoplanets. <LINK>', 'This is also special for me as I make it a point *not* to publish papers directly with the CSH Fellows. It’s an echo of time spent with Peter Goldreich and Scott Tremaine.']",https://arxiv.org/abs/2110.10925,"General circulation models are often used to explore exoclimate parameter spaces and classify atmospheric circulation regimes. Models are tuned to give reasonable climate states for standard test cases, such as the Held-Suarez test, and then used to simulate diverse exoclimates by varying input parameters such as rotation rates, instellation, atmospheric optical properties, frictional timescales and so on. In such studies, there is an implicit assumption that the model which works reasonably well for the standard test case will be credible at all points in an arbitrarily wide parameter space. Here, we test this assumption using the open-source general circulation model THOR to simulate atmospheric circulation on tidally locked Earth-like planets with rotation periods of 0.1 to 100 days. We find that the model error, as quantified by the ratio between physical and spurious numerical contributions to the angular momentum balance, is extremely variable across this range of rotation periods with some cases where numerical errors are the dominant component. Increasing model grid resolution does improve errors but using a higher-order numerical diffusion scheme can sometimes magnify errors for finite-volume dynamical solvers. We further show that to minimize error and make the angular momentum balance more physical within our model, the surface friction timescale must be smaller than the rotational timescale. ","General Circulation Model Errors are Variable across Exoclimate
  Parameter Spaces"
59,1451389585305391114,1161312102486667264,Keith Burghardt,"['In a new paper with Matheus Schmitz and @goranmuric, we create a new open source package to detect anti-vaccine users UP TO A YEAR before they write anti-vaxx hashtags!\nOther findings: avax users are more partisan and negative\n<LINK> <LINK>']",https://arxiv.org/abs/2110.11333,"Vaccine hesitancy has a long history but has been recently driven by the anti-vaccine narratives shared online, which significantly degrades the efficacy of vaccination strategies, such as those for COVID-19. Despite broad agreement in the medical community about the safety and efficacy of available vaccines, a large number of social media users continue to be inundated with false information about vaccines and, partly because of this, became indecisive or unwilling to be vaccinated. The goal of this study is to better understand anti-vaccine sentiment, and work to reduce its impact, by developing a system capable of automatically identifying the users responsible for spreading anti-vaccine narratives. We introduce a publicly available Python package capable of analyzing Twitter profiles to assess how likely that profile is to spread anti-vaccine sentiment in the future. The software package is built using text embedding methods, neural networks, and automated dataset generation. It is trained on over one hundred thousand accounts and several million tweets. This model will help researchers and policy-makers understand anti-vaccine discussion and misinformation strategies, which can further help tailor targeted campaigns seeking to inform and debunk the harmful anti-vaccination myths currently being spread. Additionally, we leverage the data on such users to understand what are the moral and emotional characteristics of anti-vaccine spreaders. ",A Python Package to Detect Anti-Vaccine Users on Twitter
60,1451371463643176962,901266828655284225,Brian Metzger,"['It got buried in the arXiv double batch, but we have a new paper on a toy model for fast radio bursts - <LINK>  .  We propose a possible way to unify the dichotomy (longer bursts exhibit narrow time-integrated spectra) found by CHIME.  @navinsridhar']",https://arxiv.org/abs/2110.10738,"We introduce a toy model for the time-frequency structure of fast radio bursts (FRB), in which the observed emission is produced as a narrowly-peaked intrinsic spectral energy distribution sweeps down in frequency across the instrumental bandpass as a power-law in time. Though originally motivated by emission models which invoke a relativistic shock, the model could in principle apply to a wider range of emission scenarios. We quantify the burst's detectability using the frequency bandwidth over which most of its signal-to-noise ratio (SNR) is accumulated. We demonstrate that by varying just a single parameter of the toy model-the power-law index \beta of the frequency drift rate-one can transform a long (and hence preferentially time-resolved) burst with a narrow time-integrated spectrum into a shorter burst with a broad power-law time-integrated spectrum. We suggest that burst-to-burst diversity in the value of \beta could generate the dichotomy between burst duration and frequency-width recently found by CHIME. In shock models, the value of \beta is related to the radial density profile of external medium, which in light of the preferentially longer duration of bursts from repeating sources may point to diversity in the external environments surrounding repeating versus one-off FRB sources. ","A Toy Model for the Time-Frequency Structure of Fast Radio Bursts:
  Implications for the CHIME Burst Dichotomy"
61,1450931382360870921,14981648,The Disordered Cosmos by Chanda Prescod-Weinstein,"['The good news of the week is the following:\n1. Brand new preprint with @kay314159 and our graduate student Tony Mirasola! This paper extends earlier work showing how to calculate the impact of axion dark matter self-interactions on condensation times!\n\n<LINK>', '2. A paper I wrote with my graduate student Noah Glennon -- and which was on the arXiv last year -- was published in the peer review journal Physical Review D. Thanks to the ref, it improved significantly! (The arXiv paper reflects this pubbed one.)\n\nhttps://t.co/pyifeVKszM', 'In this paper, Noah and I show -- among other things -- that self-interactions can really affect the dynamical evolution of ultralight axions orbiting a central potential. This is using a mod of PyUltraLight that Noah built called PySiUltraLight.', '3. Subscribers to @newscientist got a treat: my little love note to @NASAHubble! And @NASARoman. And the astronauts of the Space Shuttle era. \n\nhttps://t.co/Mt1NCyvBqt', 'I am able to make doing all of these things part of my intellectual life because Karsten Pohl helped create this spacetime for me. \n\nI will be thinking of him tomorrow when I read at the @BKLYNlibrary Lit Prize Shortlist Celebration, which you can attend: https://t.co/zoqeKnK39l', 'Also see everyone at the Boston Book Festival on Saturday where I will be on a panel of memoirists talking about how #DisorderedCosmos is not a memoir!']",https://arxiv.org/abs/2110.08921,"We investigate the condensation time of self-interacting axion-like particles in a gravitational well, extending the prior work [arXiv:2007.07438] which showed that the Wigner formalism is a good analytic approach to describe a condensing scalar field. In the present work, we use this formalism to affirm that $\phi^4$ self-interactions will take longer than necessary to support the time scales associated with structure formation, making gravity a necessary part of the process to bring axion dark matter into a solitonic form. Here we show that when the axions' virial velocity is taken into account, the time scale associated with self-interactions will scale as $\lambda^2$. This is consistent with recent numerical estimates, and it confirms that the Wigner formalism described in prior work~\cite{Relax} is a helpful analytic framework to check computational work for potential numerical artifacts. ","Analysis of Bose-Einstein condensation times for self-interacting scalar
  dark matter"
62,1450832033282932741,225895485,Andreas Madsen,"[""Can you trust attention explanations in #NLProc? What about other explanations? I'm excited to present our answer to these questions in a new paper: <LINK>\n\nI'm proud of this work. I hope you find it useful. If you do, consider sharing or ❤️. What do we find?... <LINK>"", 'This work was done in collaboration with @ncmeade, @vaibhav_adlakha, and @sivareddyg. I have more work on this direction planned, so follow us if you are interested.\nOkay, now to the summary with lots of diagrams and plots :) 2/9 …', 'Attention is regularly used to explain to NLP models. However, it is unknown if attention is a valid explanation. Because attention only relates to internal embeddings, it is not a mathematical guarantee that it also relates to the input tokens. 3/9 … https://t.co/ddqOGsKERr', 'The debate on “Is attention explanation?” continues every year, but with little consensus or progress on how to measure it. The question is very hard because we can’t annotate what a correct explanation is. 4/9 … https://t.co/QD7cqjjJZS', 'We adapt and improve ROAR by @sarahookr et al. (https://t.co/MUiE8280Os) which have been used in computer vision to answer a similar question. We make it work on NLP and fix a known issue related to redundancies in the dataset. 5/9 … https://t.co/SqAGjG3ciU', 'So, is attention a valid explanation? Well, surprisingly perhaps, it is task-dependent. More surprisingly, other more methods which were not valid explanations in Computer Vision, like “gradient” and “integrated gradient”, are valid explanations in NLP. 6/9 … https://t.co/91X3wRneja', 'But more importantly than these results, is the methodology of the measurement. I think many of the previous methods in NLP, are speculative at best. But the ROAR principle by @sarahookr et al. (https://t.co/MUiE8280Os), is really sound! 7/9 …', ""ROAR's foundational principle is: if information is truly important, then removing it from the dataset and retraining the model should result in a worse model. Importantly, this can be compared with removing random information. 8/9 …"", 'I hope by introducing and improving ROAR for the NLP community, we can establish a new foundation for interpretability research in NLP. 9/9 …', '@anmarasovic @sarahookr Thanks! I will be sure to check out this paper too :)', '@anmarasovic Definitely. Transformer models are challenging to test because they are more expensive to retrain. However, I have a planned paper that solves that. And I definitely want to include more importance measures for that publication.', '@zhi_bruce_wen 1) ""Faithful"" is not absolute. Better importance measures are still needed. 2) Importance measures might work on average, but fail on single observations. 3) Direct comparison of different methods is hard because their scale is very different. 4) Rank-based scores are unstable.', '@zhi_bruce_wen Let\'s say the sentence is ""[relevant A][relevant B][irrelevant]"", two different importance measures might select a different [relevant _] token as the most important. A direct comparison score will show they are inconsistent. However, ROAR will still show they are faithful.', '@ItsAmitJena Thanks!', ""@aerinykim I'm not sold on the IG method, so I am surprised it performed well on SST by such a large margin. For IMDB Integrated gradient also performs best, so maybe there is a connection with sentiment classification. However, I don't know what that connection would be."", ""@aerinykim PS: I don't add something based on expectations or outcome. I believe that to be bad science. The datasets used here are an exact match to used in previous literature, and were hence predetermined."", ""@zhi_bruce_wen You might be right but I can't really see the relationship."", ""@zhi_bruce_wen I think it's a mistake to think of faithfulness as an absolute. Something is not either faithful or unfaithful, it's simply a degree of faithful. In our faithfulness table we try to give an exact ratio number.\nWhat you decide to trust is a matter of option and application."", ""@hllo_wrld Unfortunately no. As model's gets deeper, they mix token embedding more which affects the faithfulness of attention. This is something I'm planning on investigating in future work.""]",https://arxiv.org/abs/2110.08412,"To explain NLP models, many methods inform which inputs tokens are important for a prediction. However, an open question is if these methods accurately reflect the model's logic, a property often called faithfulness. In this work, we adapt and improve a recently proposed faithfulness benchmark from computer vision called ROAR (RemOve And Retrain), by Hooker et al. (2019). We improve ROAR by recursively removing dataset redundancies, which otherwise interfere with ROAR. We adapt and apply ROAR, to popular NLP importance measures, namely attention, gradient, and integrated gradients. Additionally, we use mutual information as an additional baseline. Evaluation is done on a suite of classification tasks often used in the faithfulness of attention literature. Finally, we propose a scalar faithfulness metric, which makes it easy to compare results across papers. We find that, importance measures considered to be unfaithful for computer vision tasks perform favorably for NLP tasks, the faithfulness of an importance measure is task-dependent, and the computational overhead of integrated gradient is rarely justified. ","Evaluating the Faithfulness of Importance Measures in NLP by Recursively
  Masking Allegedly Important Tokens and Retraining"
63,1450809787663388679,1325826172199055362,Dave Jensen,"['New paper out, joint with Gabi Farkas and Sam Payne! In it, we tell you everything you could possibly want to know about curves of genus 13, including the fact that the moduli space of Pryms R_13 is of general type. 1/6\n<LINK>', 'The Bertram-Feinberg-Mukai Conjecture predicts that the general curve of genus 13 has finitely many rank 2 vector bundles with canonical determinant and 8 sections. We show that it has exactly 3 such bundles. 2/6', 'The set of curves with only 2 such bundles is a divisor in M_13. It is the first known example of an effective divisor in M_g of slope less than 6+10/g. (For more on the search for divisors of small slope on M_g, check out the survey below.) 3/6\nhttps://t.co/zgP8fFEOEX', 'All of these results rely on a new case of the Strong Maximal Rank Conjecture. We prove this using tropical methods, similar to our earlier work in genus 22 and 23. 4/6', 'In the interest of humility, let me tell you a few things our paper does NOT do. We do not know if the divisor we construct has the smallest slope among all effective divisors on M_13. We spent some time trying to prove this. 5/6', ""Also, a personal goal of mine for this project was to streamline our tropical approach from the earlier paper, making it clearer and more accessible. I think it's safe to say that I did not accomplish this. 6/6"", '@JulietteBruce12 The methods are very similar; the differences are technical. Basically, if you run the machine from our earlier paper, it only shows that the relevant map has corank at most 1, but we need to show it’s surjective. Unfortunately, this is done by ad hoc case analysis.', '@ProfNoahGian Thank you, I’m very happy with it!', '@TyLKelly Thank you!']",https://arxiv.org/abs/2110.09553,"The paper is devoted to highlighting several novel aspects of the moduli space of curves of genus 13, the first genus g where phenomena related to K3 surfaces no longer govern the birational geometry of M_g. We compute the class of the non-abelian Brill-Noether divisor on M_13 of curves that have a stable rank 2 vector bundle with many sections. This provides the first example of an effective divisor on M_g with slope less than 6+10/g. Earlier work on the Slope Conjecture suggested that such divisors may not exist. The main geometric application of our result is a proof that the Prym moduli space of genus 13 is of general type. Among other things, we also prove the Bertram-Feinberg-Mukai and the Strong Maximal Rank Conjectures on M_13 ","The non-abelian Brill-Noether divisor on $\overline{\mathcal{M}}_{13}$
  and the Kodaira dimension of $\overline{\mathcal{R}}_{13}$"
64,1450525450195902467,309164285,Jorge S. Diaz,"['📢New paper alert! ""Blast wave kinematics: theory, experiments, and applications."" A collaboration with @Dr_SamRigby started via Twitter followed by back-and-forth questions &amp; half-cooked answers for almost a year. An adventure into an unknown field for me <LINK> <LINK>']",https://arxiv.org/abs/2110.09488,"Measurements of the time of arrival of shock waves from explosions can serve as powerful markers of the evolution of the shock front for determining crucial parameters driving the blast. Using standard theoretical tools and a simple ansatz for solving the hydrodynamics equations, a general expression for the Mach number of the shock front is derived. Dimensionless coordinates are introduced allowing a straightforward visualization and direct comparison of blast waves produced by a variety of explosions, including chemical, nuclear, and laser-induced plasmas. The results are validated by determining the yield of a wide range of explosions, using data from gram-size charges to thermonuclear tests. ","Blast wave kinematics: theory, experiments, and applications"
65,1450469284392669190,2324423269,Peyman 𝕄𝕀𝕃𝔸ℕ𝔽𝔸ℝ,"[""New Paper: \nNumerical sol'n of PDEs often requires very fine grid resolution for stability &amp; is therefore expensive\n\nBLADE is a learnable filter-bank, solves image PDEs fast &amp; accurately, operating more reliably at coarse resolutions than classic methods.\n<LINK> <LINK>""]",https://arxiv.org/abs/2110.08327,"Partial differential equations (PDEs) are typically used as models of physical processes but are also of great interest in PDE-based image processing. However, when it comes to their use in imaging, conventional numerical methods for solving PDEs tend to require very fine grid resolution for stability, and as a result have impractically high computational cost. This work applies BLADE (Best Linear Adaptive Enhancement), a shallow learnable filtering framework, to PDE solving, and shows that the resulting approach is efficient and accurate, operating more reliably at coarse grid resolutions than classical methods. As such, the model can be flexibly used for a wide variety of problems in imaging. ",Solving Image PDEs with a Shallow Network
66,1450459992717410308,188363758,Federico Pernici,['Excited to share our new paper preprint: “Fine-Grained Adversarial Semi-supervised Learning”. We achieve 65.4% on the Semi-Supervised iNaturalist-Aves dataset (8 points more than the best performing method).\n<LINK>'],https://arxiv.org/abs/2110.05848,"In this paper we exploit Semi-Supervised Learning (SSL) to increase the amount of training data to improve the performance of Fine-Grained Visual Categorization (FGVC). This problem has not been investigated in the past in spite of prohibitive annotation costs that FGVC requires. Our approach leverages unlabeled data with an adversarial optimization strategy in which the internal features representation is obtained with a second-order pooling model. This combination allows to back-propagate the information of the parts, represented by second-order pooling, onto unlabeled data in an adversarial training setting. We demonstrate the effectiveness of the combined use by conducting experiments on six state-of-the-art fine-grained datasets, which include Aircrafts, Stanford Cars, CUB-200-2011, Oxford Flowers, Stanford Dogs, and the recent Semi-Supervised iNaturalist-Aves. Experimental results clearly show that our proposed method has better performance than the only previous approach that examined this problem; it also obtained higher classification accuracy with respect to the supervised learning methods with which we compared. ",Fine-Grained Adversarial Semi-supervised Learning
67,1450426761645367297,2723963928,"Elissa M. Redmiles, Ph.D.","[""Curious how general privacy sentiment changed with COVID (2019 -&gt; 2021)? \n\nCheck out our new preprint &amp; @angelicagoetzen's first paper (&amp; a first authored one at that) coauthored w/ @umdcs Samuel Dooley \n\n<LINK>\n\n1/6 <LINK>"", ""We compared @pewinternet's 2019 privacy data (https://t.co/n2jznaJbVq) with our own samples collected exactly 1 and 2 years later (2020 &amp; 2021) on four data use attitude questions. https://t.co/bnVMHo75eE"", 'Comparing June 2019 to June 2020 (3 months after the first US lockdown): \nAcceptance of data collection by the government for terrorism prevention decreased\nHypothesis: public health displaced terrorism priority (supported by Pew data)\n\n2/6', 'In the same time period, acceptance of health-related data use went up:\n\n1) People were more accepting of the use of their fitness tracking data for research on heart disease\nHypothesis: the salience of medical research was increased by the pandemic\n\n3/6', ""2) People became more accepting of social media companies analyzing people's posts to detect and intervene in mental health. \nHypothesis: salience of mental health increased\n\n4/6"", 'By June 2021:\nAcceptance of law enforcement use of genetic data (e.g., data shared with 23&amp;Me) significantly decreased.\nHypothesis: rise in visibility of the Black Lives Matter movement drove increased salience / concern about law enforcement data use\n\n5/6', 'Pre-pandemic: variance in opinion within demographic groups (e.g., men vs. women). \n\nThe pandemic brought people together (esp. re: gov &amp; law enforcement data use).\n\nBUT: Democrat &amp; Republic attitudes remain divergent &amp; FLIPPED after the 2020 presidential elections.\n\n6/6']",https://arxiv.org/abs/2110.09437,"People's privacy sentiments influence changes in legislation as well as technology design and use. While single-point-in-time investigations of privacy sentiment offer useful insight, study of people's privacy sentiments over time is also necessary to better understand and anticipate evolving privacy attitudes. In this work, we use repeated cross-sectional surveys (n=6,676) to model the sentiments of people in the U.S. toward collection and use of data for government- and health-related purposes from 2019-2021. After the onset of COVID-19, we observe significant decreases in respondent acceptance of government data use and significant increases in acceptance of health-related data uses. While differences in privacy attitudes between sociodemographic groups largely decreased over this time period, following the 2020 U.S. national elections, we observe some of the first evidence that privacy sentiments may change based on the alignment between a user's politics and the political party in power. Our results offer insight into how privacy attitudes may have been impacted by recent events and allow us to identify potential predictors of changes in privacy attitudes during times of geopolitical or national change. ",Ctrl-Shift: How Privacy Sentiment Changed from 2019 to 2021
68,1450311928400596995,1263773978604126208,"池田思朗, Shiro Ikeda","['Our new paper is accepted for publication in ApJ. Applying sparse modeling imaging for ALMA data.  Congratulations, Yamaguchi-san.\n\nALMA Super-resolution Imaging of T Tau: r = 12 au Gap in the Compact Dust Disk around T Tau N <LINK>']",https://arxiv.org/abs/2110.00974,"Based on Atacama Large Millimeter/submillimeter Array (ALMA) observations, compact protoplanetary disks with dust radii of $r\lesssim 20-40$ au were found to be dominant in nearby low-mass star formation regions. However, their substructures have not been investigated because of the limited spatial resolution achieved so far. We apply a newly developed super-resolution imaging technique utilizing sparse modeling (SpM) to explore several au-scale structures in such compact disks. SpM imaging can directly solve for the incomplete sampling of visibilities in the spatial frequency and potentially improve the fidelity and effective spatial resolution of ALMA images. Here, we present the results of the application to the T Tau system. We use the ALMA 1.3 mm continuum data and achieve an effective spatial resolution of $\sim 30\%$ (5 au) compared with the conventional CLEAN beam size at a resolution of 17 au. The reconstructed image reveals a new annular gap structure at $r= 12$ au in the T Tau N compact disk with a dust radius of 24 au, and resolves the T Tau Sa and Sb binary into two sources. If the observed gap structure in the T Tau N disk is caused by an embedded planet, we estimate a Saturn-mass planet when the viscous parameter of the disk is $10^{-3}$. Ultimately, ALMA observations with enough angular resolution and sensitivity should be able to verify the consistency of the super-resolution imaging and definitely confirm the existence of this disk substructure. ","ALMA Super-resolution Imaging of T Tau: r = 12 au Gap in the Compact
  Dust Disk around T Tau N"
69,1450280007247482885,226785003,Raghav Kunnawalkam Elayavalli,"[""New paper time! <LINK> We prepared a new tune which we called the 'Detroit' tune of the <LINK> event generator including p+p data form RHIC to Tevatron to LHC 1/10"", 'We initiated this within the @RHIC_STAR collaboration late last year with the goal that we absolutely need better description of p+p data at smaller center of mass energies! @MatthewJKelsey, myself and a few more members of STAR decide to produce a new tune 2/10', 'PYTHIA has a lot of parameters that one can play with but we choose 5 that are focuses on the underlying event and color re-connection. You can see the 1-D fit contours between the parameters and along the diagonal the Detroit values in blue compared to Monash in yellow 3/10 https://t.co/yCtMirt5lB', 'You see that they are very different! what is the consequence of this different? the sqrt-s scaling of the scale parameter that controls the transition between the parton shower and the underlying event is significantly different between the different tunes! 4/10 https://t.co/SSJttP0sKr', 'In other words, setting the reference energy at lower values and scaling/extrapolating up works much better than starting up high energies and going down! This is also similar to what happened in the fits of the strong coupling constant 5/10', 'whats the result of this new tune? We now fit data at both center of mass energy = 200 GeV and 13 TeV simultaneously, better than the existing/default Monash tune 6/10 https://t.co/lek6JmW4fr', 'so natural question one can ask - have we made something that just automatically fits the data everywhere? answer is no! It absolutely does not work for particle production in the forward rapidities. You see that we actually made the comparison worse than Monash :( 7/10 https://t.co/s11yTL0vKx', 'forward rapidities are very important in the near future! we have the STAR forward upgrade taking data in the next month and also the EIC will be sooner than we think so we need to understand particle production across all energies and all kinematics 8/10', 'we encourage collaborations at both RHIC and LHC to use the Detroit tune in their PYTHIA 8 generation, details of the baseline and the different eigentunes are available in the paper as better description of data is one requirement for precision measurements 9/10', 'Lastly, this study motivates a kind of global tuning exercise with data across all collision systems. lets open  the hood of not only PYTHIA but also other general purpose monte carlos especially with parameters that directly control particle production. 10/10', ""@AethyrRY Yup that's the latest version of PYTHIA when we started the process and we also updated the PDF that's used.""]",https://arxiv.org/abs/2110.09447,"We report an underlying event tune for the PYTHIA 8 Monte Carlo event generator that is applicable for hadron collisions primarily at $\sqrt{s}$ ranges available at the Relativistic Heavy-Ion Collider (RHIC). We compare our new PYTHIA 8 tuned predictions to mid-rapidity inclusive $\pi^{\pm}$ spectra, jet sub-structure, Drell-Yan production, and underlying event measurements from RHIC and the Tevatron, as well as underlying event data from the Large Hadron Collider. With respect to the default PYTHIA 8 Monash Tune, the new `Detroit' tune shows significant improvements in the description of the experimental data. Additionally, we explore the validity of PYTHIA 8 predictions for forward rapidity $\pi$ in $\sqrt{s}$ = 200 GeV collisions, where neither tune is able to sufficiently describe the data. We advocate for the new tune to be used for PYTHIA 8 studies at current and future RHIC experiments, and discuss future tuning exercises at lower center-of-mass energies, where forward/backward kinematics are essential at the upcoming Electron-Ion collider. ",PYTHIA 8 underlying event tune For RHIC energies
70,1450246388550414337,1437185924354478082,Barry McKernan,"['New paper out! Starfall!\n<LINK>', 'There are stellar mass black holes in AGN disks, sure, but there are also stars. Stars that orbit in the same sense as the gas in the disk can become massive O(100Msun) &amp; undying! E.g.\nhttps://t.co/gC8kS1cRY7', ""Here we look at stars that move backwards through the AGN gas disk. They're ~ half the stars in the disk when it forms. These stars feel intense drag &amp; fall inwards fast (&lt;0.1Myr) through the disk towards the central supermassive black hole (SMBH). https://t.co/9YV45zRNQj"", 'As the population of backwards stars grows at small disk radii, due to starfall, interactions can occur, binaries can form &amp; scatterings can happen. Chaotic scatterings, particularly between binaries &amp; singles, can send stars rocketing towards the central shred zone. https://t.co/S59bpyUdIu', 'What happens next depends on the mass of the SMBH &amp; whether a star is flung to one side or the other of the SMBH....', 'If the star is sent too close to the SMBH (&lt;=100million Msun), the tidal forces across it cause it to rupture, spilling its guts at high speed around the SMBH &amp; into the inner disk, creating a tidal disruption event (TDE)', 'If the star is sent around the SMBH against the flow of gas, e.g. https://t.co/aO4BCzTsSJ', 'then the angular momentum of the unbound ejecta flung outwards cancels the angular momentum of the inner disk and causes much of the inner disk to slump onto the SMBH, clearing out a small cavity. https://t.co/4TtobVnGxn', 'The result is an over-luminous TDE (tidal disruption event), followed by a low AGN state afterwards as the disk recovers on the viscous timescale. Cartoon Lightcurve is: https://t.co/eXJDTISFN8', ""If the star goes the other way around the SMBH, the unbound ejecta adds angular momentum to the inner disk &amp; pushes it outwards temporarily. The result is a more 'normal' TDE, followed by a higher AGN state as the ejecta + inner disk accrete on the viscous timescale. Lightcurve: https://t.co/RYRsVctPn9"", ""We expect roughly equal numbers of these 'prograde' and 'retrograde' TDEs in AGN. Particularly early on as starfall allows this population to build up &amp; scatter in the inner disk. So, AGN TDEs are a nice probe of 'turning on' AGN."", 'Now we need to find AGN flares that look like this! Stay tuned.', 'With: @saavikford, @kantyellow, @AdamSJermyn, @doccosmos, Dan Stern, Nate Leigh, Taeho Ryu']",https://arxiv.org/abs/2110.03741,"As active galactic nuclei (AGN) `turn on', some stars end up embedded in accretion disks around supermassive black holes (SMBHs) on retrograde orbits. Such stars experience strong headwinds, aerodynamic drag, ablation and orbital evolution on short timescales. Loss of orbital angular momentum in the first $\sim 0.1$~Myr of an AGN leads to a heavy rain of stars (`starfall') into the inner disk and onto the SMBH. A large AGN loss cone ($\theta_{\rm AGN,lc}$) can result from binary scatterings in the inner disk and yield tidal disruption events (TDEs). Signatures of starfall include optical/UV flares that rise in luminosity over time, particularly in the inner disk. If the SMBH mass is $M_{\rm SMBH} \ge 10^{8}M_{\odot}$, flares truncate abruptly and the star is swallowed. If $M_{\rm SMBH}<10^{8}M_{\odot}$, and if the infalling orbit lies within $\theta_{\rm AGN,lc}$, the flare is followed by a TDE which can be prograde or retrograde relative to the AGN inner disk. Retrograde AGN TDEs are over-luminous and short-lived as in-plane ejecta collide with the inner disk and a lower AGN state follows. Prograde AGN TDEs add angular momentum to inner disk gas and so start off looking like regular TDEs but are followed by an AGN high state. Searches for such flare signatures test models of AGN `turn on', SMBH mass, as well as disk properties and the embedded population. ",Starfall: A heavy rain of stars in 'turning on' AGN
71,1450180790482980865,704559922143322112,Stephen Casper,"['A new paper is out from some collaborators and I on how networks develop modular clusters of neurons. <LINK>', ""It's a followup paper from https://t.co/RdcMqCPg1m.""]",https://arxiv.org/abs/2110.08058,"A neural network is locally specialized to the extent that parts of its computational graph (i.e. structure) can be abstractly represented as performing some comprehensible sub-task relevant to the overall task (i.e. functionality). Are modern deep neural networks locally specialized? How can this be quantified? In this paper, we consider the problem of taking a neural network whose neurons are partitioned into clusters, and quantifying how functionally specialized the clusters are. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal local specialization and that statistical methods can be used to automatedly screen for sets of neurons that can be understood abstractly. ",Quantifying Local Specialization in Deep Neural Networks
72,1450152907270066176,179764224,Adam Burgasser,"[""New paper Monday! @astroganze has identified 164 late-M, L and T dwarfs in deep HST/WFC3 grism data; that's ultracool dwarfs with spectra out past 1 kpc. Look for paper 2 coming soon where he use this sample to constrain the structure of the Milky Way <LINK>""]",https://arxiv.org/abs/2110.07672,"Ultracool dwarf stars and brown dwarfs provide a unique probe of large-scale Galactic structure and evolution; however, until recently spectroscopic samples of sufficient size, depth, and fidelity have been unavailable. Here, we present the identification of 164 M7--T9 ultracool dwarfs in 0.6~deg$^2$ of deep, low-resolution, near-infrared spectroscopic data obtained with the Hubble Space Telescope Wide Field Camera 3 instrument as part of the WFC3 Infrared Spectroscopic Parallel Survey and the 3D-HST survey. We describe the methodology by which we isolate ultracool dwarf candidates from over 200,000 spectra, and show that selection by machine learning classification is superior to spectral index-based methods in terms of completeness and contamination. We use the spectra to accurately determine classifications and spectrophotometric distances, the latter reaching to ~2 kpc for L dwarfs and ~400pc for T dwarfs. ","Beyond the Local Volume I: Surface Densities of Ultracool Dwarfs in Deep
  HST/WFC3 Parallel Fields"
73,1450118943679602693,781777461268709376,Baptiste Rozière,"['New paper on unsupervised code translation! <LINK>\nWe show that by using automatically generated unit tests we can filter out invalid back-translation samples, and reduce the error rate by up to 35% in some language pairs! <LINK>', 'Using EvoSuite, an open-source software leveraging evolutionary methods, we automatically create multilingual unit tests for hundreds of thousands of functions found on github and use them to filter the outputs of unsupervised translation models for Java, Python and C++.', 'We significantly improve the performance of code translation models for every studied language pair. Our method is completely unsupervised and could easily be generalized to more programming languages. With @JieMarinaZhang @f_charton @Mark_Harman @syhw @GuillaumeLample']",https://arxiv.org/abs/2110.06773,"With little to no parallel data available for programming languages, unsupervised methods are well-suited to source code translation. However, the majority of unsupervised machine translation approaches rely on back-translation, a method developed in the context of natural language translation and one that inherently involves training on noisy inputs. Unfortunately, source code is highly sensitive to small changes; a single token can result in compilation failures or erroneous programs, unlike natural languages where small inaccuracies may not change the meaning of a sentence. To address this issue, we propose to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus. We found that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state-of-the-art for all language pairs studied. In particular, for Java $\to$ Python and Python $\to$ C++ we outperform the best previous methods by more than 16% and 24% respectively, reducing the error rate by more than 35%. ",Leveraging Automated Unit Tests for Unsupervised Code Translation
74,1450077624697511937,39460226,Peter Sjögårde,"['How can science be visualized so that both overview and detail are provided? I address this question in a new paper, using examples of Covid research and Open Access publishing.\n<LINK>\nSee example map of papers cited by Covid papers: <LINK> <LINK>']",https://arxiv.org/abs/2110.07917,"Overlay maps of science are global base maps over which subsets of publications can be projected. Such maps can be used to monitor, explore, and study research through its publication output. Most maps of science, including overlay maps, are flat in the sense that they visualize research fields at one single level. Such maps generally fail to provide both overview and detail about the research being analyzed. The aim of this study is to improve overlay maps of science to provide both features in a single visualization. I created a map based on a hierarchical classification of publications, including broad disciplines for overview and more granular levels to incorporate detailed information. The classification was obtained by clustering articles in a citation network of about 17 million publication records in PubMed from 1995 onwards. The map emphasizes the hierarchical structure of the classification by visualizing both disciplines and the underlying specialties. To show how the visualization methodology can help getting both overview of research and detailed information about its topical structure, I projected two overlay maps onto the base map: (1) open access publishing and (2) coronavirus/Covid-19 research. ",Improving overlay maps of science: combining overview and detail
75,1450072826308964358,2816968963,Miguel A.F. Sanjuán,"['Glad to know that our new paper ""Noise activates escapes in closed Hamiltonian systems""\nAlexandre R. Nieto, Jesus M. Seoane, Miguel A.F. Sanjuan \n\nhas been accepted in #CNSNS @ElsevierPhysics \n\n<LINK> <LINK>']",https://arxiv.org/abs/2110.07284,"In this manuscript we show that a noise-activated escape phenomenon occurs in closed Hamiltonian systems. Due to the energy fluctuations generated by the noise, the isopotential curves open up and the particles can eventually escape in finite times. This drastic change in the dynamical behavior turns the bounded motion into a chaotic scattering problem. We analyze the escape dynamics by means of the average escape time, the probability basins and the average escape time distribution. We obtain that the main characteristics of the scattering are different from the case of noisy open Hamiltonian systems. In particular, the noise-enhanced trapping, which is ubiquitous in Hamiltonian systems, does not play the main role in the escapes. On the other hand, one of our main findings reveals a transition in the evolution of the average escape time insofar the noise is increased. This transition separates two different regimes characterized by different algebraic scaling laws. We provide strong numerical evidence to show that the complete destruction of the stickiness of the KAM islands is the key reason under the change in the scaling law. This research unlocks the possibility of modeling chaotic scattering problems by means of noisy closed Hamiltonian systems. For this reason, we expect potential application to several fields of physics such us celestial mechanics and astrophysics, among others. ",Noise activates escapes in closed Hamiltonian systems
76,1450070861709561871,1278590255285796864,Ido Irani 🇺🇦,"['Check out our new paper on core-collpase supernovae exploding in elliptical host galaxies! (1/3) <LINK>', 'Core-collapse supernovae are thought to be the terminal explosions of young and massive stars. These are normally not found in ellipticals, which have an old stellar population, depleted of young and massive stars.  (2/3)', 'In our paper, we show 3 supernovae exploding near ellipticals - a rare subpopulation of supernovae. We discuss different ways to use these rare supernovae to study elliptical galaxies.']",https://arxiv.org/abs/2110.02252,"We present observations of three Core-collapse supernovae (CCSNe) in elliptical hosts, detected by the Zwicky Transient Facility Bright Transient Survey (BTS). SN 2019ape is a SN Ic that exploded in the main body of a typical elliptical galaxy. Its properties are consistent with an explosion of a regular SN Ic progenitor. A secondary g-band light curve peak could indicate interaction of the ejecta with circumstellar material (CSM). An H$\alpha$-emitting source at the explosion site suggests a residual local star formation origin. SN 2018fsh and SN 2020uik are SNe II which exploded in the outskirts of elliptical galaxies. SN 2020uik shows typical spectra for SNe II, while SN 2018fsh shows a boxy nebular H$\alpha$ profile, a signature of CSM interaction. We combine these 3 SNe with 7 events from the literature and analyze their hosts as a sample. We present multi-wavelength photometry of the hosts, and compare this to archival photometry of all BTS hosts. Using the spectroscopically complete BTS we conclude that $0.3\%^{+0.3}_{-0.1}$ of all CCSNe occur in elliptical galaxies. We derive star-formation rates and stellar masses for the host-galaxies and compare them to the properties of other SN hosts. We show that CCSNe in ellipticals have larger physical separations from their hosts compared to SNe Ia in elliptical galaxies, and discuss implications for star-forming activity in elliptical galaxies. ","Less than 1% of Core-Collapse Supernovae in the local universe occur in
  elliptical galaxies"
77,1450016232871239692,4109233092,Michael Walter,"['Quantum tools for classical statistics! New joint paper with Cole Franks, Akshay Ramachandran, and @roliveira641: <LINK>']",https://arxiv.org/abs/2110.07583,"The matrix normal model, the family of Gaussian matrix-variate distributions whose covariance matrix is the Kronecker product of two lower dimensional factors, is frequently used to model matrix-variate data. The tensor normal model generalizes this family to Kronecker products of three or more factors. We study the estimation of the Kronecker factors of the covariance matrix in the matrix and tensor models. We show nonasymptotic bounds for the error achieved by the maximum likelihood estimator (MLE) in several natural metrics. In contrast to existing bounds, our results do not rely on the factors being well-conditioned or sparse. For the matrix normal model, all our bounds are minimax optimal up to logarithmic factors, and for the tensor normal model our bound for the largest factor and overall covariance matrix are minimax optimal up to constant factors provided there are enough samples for any estimator to obtain constant Frobenius error. In the same regimes as our sample complexity bounds, we show that an iterative procedure to compute the MLE known as the flip-flop algorithm converges linearly with high probability. Our main tool is geodesic strong convexity in the geometry on positive-definite matrices induced by the Fisher information metric. This strong convexity is determined by the expansion of certain random quantum channels. We also provide numerical evidence that combining the flip-flop algorithm with a simple shrinkage estimator can improve performance in the undersampled regime. ","Near optimal sample complexity for matrix and tensor normal models via
  geodesic convexity"
78,1450014382105120770,556691118,Vijay Dwivedi,"['Presenting new work on GNNs/(Graph)Transformers:\n\n""Graph Neural Networks with Learnable Structural and Positional Representations""\nwith Anh Tuan Luu, Thomas Laurent, Yoshua Bengio and @xbresson.\n\nPaper: <LINK>\nCode: <LINK>\n\n#2minutebrief 👇 <LINK>', 'Nodes in graphs do not have canonical positional information, like words in sentences. This causes limitations such as the lack of (global) structural information when MP-GNNs learn on graphs. As a result, models cannot distinguish isomorphic nodes or other graph symmetries.', 'In this work, we consider this topic of graph PEs and propose a framework named LSPE that can be used with any MP-GNNs to learn positional and structural feature representations at the same time, thus effectively capturing the two properties and tuning these w.r.t. to the task.', 'In summary, LSPE enhances capabilities of an MP-GNN as:\n\n1. Input layer: PEs are initialized with k-dim Random Walk that encodes the landing probabilities of a node to itself in 1 to k steps. This leads to unique PEs for nodes which have distinct k-hop neighborhoods in the graph. https://t.co/iN0lxMlnsA', '2. At the GNN layers, both the structural and positional representations are updated with separate learnable parameters but following the same analytical update function of a GNN instance chosen.\n\nExample MPGNNs-LSPE👇 https://t.co/RoUxQ2CZxf', '3. At the final layer, the learned structural and positional representations are fused to output the resultant node features which is then used for the learning task being dealt with. In addition, a positional loss is used to tune the final layer positional features.', 'Above simple steps improves several MP-GNNs and Transformer-GNNs providing a performance boost of up to 64% on molecular datasets. At the same time, we retain the efficient linear complexity of message-passing while generating more expressive node embedding.\n\nMore in paper!']",https://arxiv.org/abs/2110.07875,"Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge graphs and natural language processing. A major issue with arbitrary graphs is the absence of canonical positional information of nodes, which decreases the representation power of GNNs to distinguish e.g. isomorphic nodes and other graph symmetries. An approach to tackle this issue is to introduce Positional Encoding (PE) of nodes, and inject it into the input layer, like in Transformers. Possible graph PE are Laplacian eigenvectors. In this work, we propose to decouple structural and positional representations to make easy for the network to learn these two essential properties. We introduce a novel generic architecture which we call LSPE (Learnable Structural and Positional Encodings). We investigate several sparse and fully-connected (Transformer-like) GNNs, and observe a performance increase for molecular datasets, from 1.79% up to 64.14% when considering learnable PE for both GNN classes. ","Graph Neural Networks with Learnable Structural and Positional
  Representations"
79,1449057451773415425,156804540,Francisco Rodrigues,"[""Our new paper on Arxiv is out. We show the importance of brain cortical connections to perform the automatic diagnosis of Alzheimer's disease and schizophrenia. \n<LINK>""]",https://arxiv.org/abs/2110.06140,"Mental disorders are among the leading causes of disability worldwide. The first step in treating these conditions is to obtain an accurate diagnosis, but the absence of established clinical tests makes this task challenging. Machine learning algorithms can provide a possible solution to this problem, as we describe in this work. We present a method for the automatic diagnosis of mental disorders based on the matrix of connections obtained from EEG time series and deep learning. We show that our approach can classify patients with Alzheimer's disease and schizophrenia with a high level of accuracy. The comparison with the traditional cases, that use raw EEG time series, shows that our method provides the highest precision. Therefore, the application of deep neural networks on data from brain connections is a very promising method to the diagnosis of neurological disorders. ","EEG functional connectivity and deep learning for automatic diagnosis of
  brain disorders: Alzheimer's disease and schizophrenia"
80,1449056177757827074,775002133041152001,Panagiotis Tsiotras,['Our new paper on shaping large population behaviors of selfish autonomous agents using mean-field interactions is now available on arXiv!\n<LINK>'],https://arxiv.org/abs/2110.07469,"Mean-field games (MFG) were introduced to efficiently analyze approximate Nash equilibria in large population settings. In this work, we consider entropy-regularized mean-field games with a finite state-action space in a discrete time setting. We show that entropy regularization provides the necessary regularity conditions, that are lacking in the standard finite mean field games. Such regularity conditions enable us to design fixed-point iteration algorithms to find the unique mean-field equilibrium (MFE). Furthermore, the reference policy used in the regularization provides an extra means, through which one can control the behavior of the population. We first formulate the problem as a stochastic game with a large population of $N$ homogeneous agents. We establish conditions for the existence of a Nash equilibrium in the limiting case as $N$ tends to infinity, and we demonstrate that the Nash equilibrium for the infinite population case is also an $\epsilon$-Nash equilibrium for the $N$-agent regularized game, where the sub-optimality $\epsilon$ is of order $\mathcal{O}\big(1/\sqrt{N}\big)$. Finally, we verify the theoretical guarantees through a resource allocation example and demonstrate the efficacy of using a reference policy to control the behavior of a large population of agents. ","Shaping Large Population Agent Behaviors Through Entropy-Regularized
  Mean-Field Games"
81,1449030504947216394,928301283034914817,Lihua Lei,"['New working paper <LINK>\n\nWe found a deep connection b/t PAC-learning &amp;familywise error rate (FWER) control, two nearly non-overlapping areas. Tools used by drug companies (graphical approach for FWER) are very useful for computer vision!#mltwitter #statstwitter <LINK>']",https://arxiv.org/abs/2110.01052,"We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithm works with any underlying model and (unknown) data-generating distribution and does not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use our framework to provide new calibration methods for several core machine learning tasks with detailed worked examples in computer vision and tabular medical data. ","Learn then Test: Calibrating Predictive Algorithms to Achieve Risk
  Control"
82,1449020838473240579,1138012858988617728,Hannes Stärk,"['A video explanation📽️ is now available for our new 3D Infomax paper: ""3D Infomax improves GNNs for Molecular Property Prediction""! 🕸️\n\nPaper: <LINK>\nCode to pre-train your own GNN: <LINK>\nVideo: <LINK>\n1/2', ""The video explains how 3D Infomax teaches a GNN to reason about the 3D geometry of molecules given only their 2D graphs, which improves the GNN's property predictions!\n\nCo-authors: @GabriCorso @sacdallago @guennemann @pl219_Cambridge @valence_ai @dom_beaini @TOSSOUPrudencio \n2/2""]",https://arxiv.org/abs/2110.04126,"Molecular property prediction is one of the fastest-growing applications of deep learning with critical real-world impacts. Including 3D molecular structure as input to learned models improves their performance for many molecular tasks. However, this information is infeasible to compute at the scale required by several real-world applications. We propose pre-training a model to reason about the geometry of molecules given only their 2D molecular graphs. Using methods from self-supervised learning, we maximize the mutual information between 3D summary vectors and the representations of a Graph Neural Network (GNN) such that they contain latent 3D information. During fine-tuning on molecules with unknown geometry, the GNN still generates implicit 3D information and can use it to improve downstream tasks. We show that 3D pre-training provides significant improvements for a wide range of properties, such as a 22% average MAE reduction on eight quantum mechanical properties. Moreover, the learned representations can be effectively transferred between datasets in different molecular spaces. ",3D Infomax improves GNNs for Molecular Property Prediction
83,1448853405347295263,19048020,Sean Lawton,"['New paper: Mixed Hodge structures on character varieties of nilpotent groups, <LINK>']",https://arxiv.org/abs/2110.07060,Let R be the connected component of the identity of the variety of representations of a finitely generated nilpotent group N into a connected reductive complex affine algebraic group G. We determine the mixed Hodge structure on the representation variety R and on the character variety R//G. We obtain explicit formulae (both closed and recursive) for the mixed Hodge polynomial of these representation and character varieties. ,Mixed Hodge structures on character varieties of nilpotent groups
84,1448845443115335685,207256884,John Preskill,"['Our new paper is about using a quantum computer to simulate systems with unbounded variables --- for example, a lattice gauge theory in which the electric field can take arbitrarily large values. 1/5\n<LINK>', 'We show that, under suitable conditions, quantum states can be accurately approximated even if we impose an upper limit on the unbounded variable. This allows us to simulate the system efficiently using a truncated Hamiltonian.  2/5', 'Our arguments don’t apply to all systems with unbounded variables. In particular, we were not able to analyze the case of a self-coupled scalar field, the system we initially set out to study. 3/5', 'Sometimes things don’t work out at first, but if you don’t give up you manage to turn lemons into lemonade. It pays to stay optimistic. 4/5', 'I’m very grateful to my wonderful collaborators, especially Yu Tong who did most of the hard work. 5/5']",https://arxiv.org/abs/2110.06942,"Quantum many-body systems involving bosonic modes or gauge fields have infinite-dimensional local Hilbert spaces which must be truncated to perform simulations of real-time dynamics on classical or quantum computers. To analyze the truncation error, we develop methods for bounding the rate of growth of local quantum numbers such as the occupation number of a mode at a lattice site, or the electric field at a lattice link. Our approach applies to various models of bosons interacting with spins or fermions, and also to both abelian and non-abelian gauge theories. We show that if states in these models are truncated by imposing an upper limit $\Lambda$ on each local quantum number, and if the initial state has low local quantum numbers, then an error at most $\epsilon$ can be achieved by choosing $\Lambda$ to scale polylogarithmically with $\epsilon^{-1}$, an exponential improvement over previous bounds based on energy conservation. For the Hubbard-Holstein model, we numerically compute a bound on $\Lambda$ that achieves accuracy $\epsilon$, obtaining significantly improved estimates in various parameter regimes. We also establish a criterion for truncating the Hamiltonian with a provable guarantee on the accuracy of time evolution. Building on that result, we formulate quantum algorithms for dynamical simulation of lattice gauge theories and of models with bosonic modes; the gate complexity depends almost linearly on spacetime volume in the former case, and almost quadratically on time in the latter case. We establish a lower bound showing that there are systems involving bosons for which this quadratic scaling with time cannot be improved. By applying our result on the truncation error in time evolution, we also prove that spectrally isolated energy eigenstates can be approximated with accuracy $\epsilon$ by truncating local quantum numbers at $\Lambda=\textrm{polylog}(\epsilon^{-1})$. ",Provably accurate simulation of gauge theories and bosonic systems
85,1448840107973357583,716990588109852672,Yingtong Dou,"['New Paper (and Dataset) Out! 🔔\nWe (@lmcui) have collected an English-Chinese cross-lingual #COVID19 fake news dataset with rich metadata. Thanks for the comments from @elaine_yuan and @YingdanL_kk!\nPaper: <LINK>\nDataset: <LINK>\n#NLProc\n🧵(1/3)', 'Our dataset includes bilingual news pieces for the same news event and comprehensive metadata for each news. Hope it could help advance the research in cross-lingual fact-checking!\n\n#misinformation  #researchpaper #computationalsocialscience\n🧵(2/3)', 'This project was motivated by the two following articles discussing misinformation in immigrant communities. Both articles are worth reading!\n\n@nytimes @cathyparkhong https://t.co/D1nTyKYhAC \n\n@voxdotcom @terrygtnguyen\nhttps://t.co/ZtTt3JItb6\n🧵(3/3)']",https://arxiv.org/abs/2110.06495,"The COVID-19 pandemic poses a great threat to global public health. Meanwhile, there is massive misinformation associated with the pandemic which advocates unfounded or unscientific claims. Even major social media and news outlets have made an extra effort in debunking COVID-19 misinformation, most of the fact-checking information is in English, whereas some unmoderated COVID-19 misinformation is still circulating in other languages, threatening the health of less-informed people in immigrant communities and developing countries. In this paper, we make the first attempt to detect COVID-19 misinformation in a low-resource language (Chinese) only using the fact-checked news in a high-resource language (English). We start by curating a Chinese real&fake news dataset according to existing fact-checking information. Then, we propose a deep learning framework named CrossFake to jointly encode the cross-lingual news body texts and capture the news content as much as possible. Empirical results on our dataset demonstrate the effectiveness of CrossFake under the cross-lingual setting and it also outperforms several monolingual and cross-lingual fake news detectors. The dataset is available at this https URL ",Cross-lingual COVID-19 Fake News Detection
86,1448810891105947650,1556664198,Kyle Cranmer,"['New paper! This work was led by the awesome Siddharth Mishra-Sharma (@kdqg1 ). We use Simulation-Based Inference to study the excess of gamma rays in the galactic center, which may be related to dark matter\n\n<LINK>\n\n@iaifi_news @NYUDataScience @NYUPhysics <LINK>', 'About a year ago we also studied this excess and suggested the use of Gaussian Processes to add flexibility to the model of photon emission https://t.co/hwq8vfW6jc', 'This time we attempt to disentangle the excess emission from smooth dark matter like component from unresolved astrophysical point\nsources e.g., millisecond pulsars. https://t.co/WPYqXlSjj3', 'The point sources have different spatial structure and statistics for photon counts that can be difficult to model with traditional inference techniques. But the forward model fairly straightforward. The problem is that latent variables make the likelihood intractable. Enter SBI!', 'With Simulation-based inference we focus on learning a probabilistic relationship between the 18 parameters that describe the photon emission and the maps they produce https://t.co/mTE0nIOOpg', 'We also use spherical graph neural networks to learn (implicit) summary statistics and conditional normalizing flows to model the final posterior.  🔥', 'We also do a lot of robustness tests, signal injection tests, etc. https://t.co/DP3jpbBWZ5', 'So check it out!\nAnd thanks to my awesome collaborator Sid @kdqg1, I learned a lot!\n\nhttps://t.co/LLbi6CPanp https://t.co/BxXwBHJ5HL', 'Bonus:  check out this cool figure from Sid’s last paper\n“[Submitted on 4 Oct 2021]\nInferring dark matter substructure with astrometric lensing beyond the power spectrum”\nhttps://t.co/EPlfGVhkgi https://t.co/N69BCpAAHV']",https://arxiv.org/abs/2110.06931,"The nature of the Fermi gamma-ray Galactic Center Excess (GCE) has remained a persistent mystery for over a decade. Although the excess is broadly compatible with emission expected due to dark matter annihilation, an explanation in terms of a population of unresolved astrophysical point sources e.g., millisecond pulsars, remains viable. The effort to uncover the origin of the GCE is hampered in particular by an incomplete understanding of diffuse emission of Galactic origin. This can lead to spurious features that make it difficult to robustly differentiate smooth emission, as expected for a dark matter origin, from more ""clumpy"" emission expected for a population of relatively bright, unresolved point sources. We use recent advancements in the field of simulation-based inference, in particular density estimation techniques using normalizing flows, in order to characterize the contribution of modeled components, including unresolved point source populations, to the GCE. Compared to traditional techniques based on the statistical distribution of photon counts, our machine learning-based method is able to utilize more of the information contained in a given model of the Galactic Center emission, and in particular can perform posterior parameter estimation while accounting for pixel-to-pixel spatial correlations in the gamma-ray map. This makes the method demonstrably more resilient to certain forms of model misspecification. On application to Fermi data, the method generically attributes a smaller fraction of the GCE flux to unresolved point sources when compared to traditional approaches. We nevertheless infer such a contribution to make up a non-negligible fraction of the GCE across all analysis variations considered, with at least $38^{+9}_{-19}\%$ of the excess attributed to unresolved point sources in our baseline analysis. ","A neural simulation-based inference approach for characterizing the
  Galactic Center $\gamma$-ray excess"
87,1448790945613365251,2956121356,Russ Salakhutdinov,"['Many real world questions can’t be answered deterministically. Instead, answers can only be true if certain conditions apply.\n\nCheck out our new dataset ConditionalQA: an extremely challenging but super exciting task!\n\nDataset <LINK>\nPaper <LINK> <LINK>']",https://arxiv.org/abs/2110.06884,"We describe a Question Answering (QA) dataset that contains complex questions with conditional answers, i.e. the answers are only applicable when certain conditions apply. We call this dataset ConditionalQA. In addition to conditional answers, the dataset also features: (1) long context documents with information that is related in logically complex ways; (2) multi-hop questions that require compositional logical reasoning; (3) a combination of extractive questions, yes/no questions, questions with multiple answers, and not-answerable questions; (4) questions asked without knowing the answers. We show that ConditionalQA is challenging for many of the existing QA models, especially in selecting answer conditions. We believe that this dataset will motivate further research in answering complex questions over long documents. Data and leaderboard are publicly available at \url{this https URL}. ","ConditionalQA: A Complex Reading Comprehension Dataset with Conditional
  Answers"
88,1448717928262615040,1316159755254075392,Moya Chen,['New paper - Teach E2E task-oriented models use of new APIs by generating synthetic dialogues with a user simulator; self-train on the successful ones with maybe a sprinkle of examples. Repeat. Profit. \n\nPaper: <LINK>\nSee thread below for results + highlights. <LINK>'],https://arxiv.org/abs/2110.06905,"We demonstrate that large language models are able to simulate Task Oriented Dialogues in novel domains, provided only with an API implementation and a list of goals. We show these simulations can formulate online, automatic metrics that correlate well with human evaluations. Furthermore, by checking for whether the User's goals are met, we can use simulation to repeatedly generate training data and improve the quality of simulations themselves. With no human intervention or domain-specific training data, our simulations bootstrap end-to-end models which achieve a 37\% error reduction in previously unseen domains. By including as few as 32 domain-specific conversations, bootstrapped models can match the performance of a fully-supervised model with $10\times$ more data. To our knowledge, this is the first time simulations have been shown to be effective at bootstrapping models without explicitly requiring any domain-specific training data, rule-engineering, or humans-in-the-loop. ","Teaching Models new APIs: Domain-Agnostic Simulators for Task Oriented
  Dialogue"
89,1448665987658633216,775002133041152001,Panagiotis Tsiotras,['Our new paper on distribution steering for non-Gaussian noise using characteristic functions is now available on ArXiv! <LINK>'],https://arxiv.org/abs/2110.06808,"We propose to solve a constrained distribution steering problem, i.e., steering a stochastic linear system from an initial distribution to some final, desired distribution subject to chance constraints. We do so by characterizing the cumulative distribution function in the chance constraints and by using the absolute distance between two probability density functions using the corresponding characteristic functions. We consider discrete-time, time-varying linear systems with affine feedback. We demonstrate the proposed approach on a 2D double-integrator perturbed by various disturbances and initial conditions. ","Distribution Steering for Discrete-Time Linear Systems with General
  Disturbances using Characteristic Functions"
90,1448643395149185025,1002688565946601472,Connor W. Coley,"['1/ Very excited to share a new paper by Keir Adams and @lucky_pattanaik introducing a new type of invariance for geometric deep learning designed for 3D molecular representations with defined stereochemistry #compchem | <LINK>', '2/ Molecules aren\'t 2D graphs (when stereochemistry is defined) nor are they 3D conformers (when they exist in conformational ensembles), but 4D multi-instance methods are costly and still don\'t work well. An ideal molecular representation would describe its ""2.5D"" identity', ""3/ We start with 3D representations. If the model is E(3) invariant, it can't separate enantiomers that differ by a reflection. If it is SE(3) invariant, it can, but how meaningfully? Differences b/w enantiomers are subtle-- perhaps more subtle than difference b/w conformers"", '4/ Here\'s where the new invariance comes in: we\'d like to learn from 3D molecular conformers but be ""InterRoto"" invariant, i.e., allow rotations around single bonds so that a set of conformers interconvertible by these rotations will produce the same representation https://t.co/wkqoSnLQN2', ""5/ To accomplish this, Keir proposed using an internal coordinate representation of molecules. A pair of enantiomers might differ only by their torsion angles, but it's essential to understand how torsions are coupled to each other, since a pair of conformers can do the same"", '6/ He identified a way to embed these coupled torsions that are defined with respect to a particular bond. Applying arbitrary rotations to the bond will yield a circle when plotting the sum of sines against the sum of cosines; if we encode the radius, we are InterRoto invariant! https://t.co/vD4crqwiCG', '7/ Further, if the model is allowed to learn a phase shift parameterized by features of the 4 constituent atoms that make up a torsion angle, two tetrahedral stereocenters can be distinguished by the radii of these circles as defined by their neighboring internal bonds', '8/ A contrastive learning task on 3D conformers designed to *separate stereoisomers* but *group conformers corresponding to the same stereoisomer* confirms that E(3) models cannot do this, and SE(3) models can still get confused by differences b/w conformers of the same molecule https://t.co/aK3cpuqVkY', '9/ We look at a few supervised learning tasks to show that we achieve a benefit in performance: (1) predicting optical rotation signs, and (2) ranking enantiomers with respect to docking score https://t.co/976nH5jMzO', ""10/ We see a modest improvement in performance on these tasks relative to both SphereNet (as a representative SE(3) invariant network) and our earlier 2.5D graph model, Tetra-DMPNN. Even though we're using 3D conformers, our performance doesn't drop when we use only one as input"", '11/end While we focus on tetrahedral centers here, the goal is to continue evolving beyond 2D graphs and 3D conformers to capture more sophisticated forms of stereoisomerism that current machine learning models (and even SMILES representations) fail to capture']",https://arxiv.org/abs/2110.04383,"Molecular chirality, a form of stereochemistry most often describing relative spatial arrangements of bonded neighbors around tetrahedral carbon centers, influences the set of 3D conformers accessible to the molecule without changing its 2D graph connectivity. Chirality can strongly alter (bio)chemical interactions, particularly protein-drug binding. Most 2D graph neural networks (GNNs) designed for molecular property prediction at best use atomic labels to na\""ively treat chirality, while E(3)-invariant 3D GNNs are invariant to chirality altogether. To enable representation learning on molecules with defined stereochemistry, we design an SE(3)-invariant model that processes torsion angles of a 3D molecular conformer. We explicitly model conformational flexibility by integrating a novel type of invariance to rotations about internal molecular bonds into the architecture, mitigating the need for multi-conformer data augmentation. We test our model on four benchmarks: contrastive learning to distinguish conformers of different stereoisomers in a learned latent space, classification of chiral centers as R/S, prediction of how enantiomers rotate circularly polarized light, and ranking enantiomers by their docking scores in an enantiosensitive protein pocket. We compare our model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs to demonstrate that our model achieves state of the art performance when learning chiral-sensitive functions from molecular structures. ","Learning 3D Representations of Molecular Chirality with Invariance to
  Bond Rotations"
91,1448639971976310790,1247872005912891392,Owain Evans,"['New paper on truthful AI!\nWe introduce a definition of “lying” for AI\nWe explore how to train truthful ML models\nWe propose institutions to support *standards* for truthful AI\nWe weigh costs/benefits (economy + AI Safety)\n(w/ coauthors at Oxford &amp; OpenAI)\n<LINK>', 'Today, lying is a human problem. Over time, AI will generate an increasing share of text/speech. AI enables personalised, scalable deception. It needs no intention to deceive -- “lies” emerge from optimising text for rewards (e.g. sales, clicks, or Likes). https://t.co/7RcjeIsiuK', 'We introduce “negligent falsehoods” as a generalisation of lies. Truthful AI avoids negligent falsehoods. \nCreating AI that’s both truthful and trusted could improve human epistemics and trust among humans (as well as reducing risk from lying AI). https://t.co/88f3FiKwQC', 'Creating truthful AI is a technical challenge (e.g. finding right ML training objective) and a governance challenge. \nWe suggest *certifying* truthful AI before deployment and *adjudicating* violations after. https://t.co/JfUfuSYI79', 'The paper brings together ideas from Machine Learning, AI policy, and AI safety/alignment. \nThere is a 7-page Executive Summary covering the key ideas.\nAuthors: @OwainEvans_UK, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, @avitalbalwit, @pwillsit, @LucaFRighetti, W. Saunders']",http://arxiv.org/abs/2110.06674,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set. ",Truthful AI: Developing and governing AI that does not lie
92,1448636823320805380,416560047,Ken Duncan,"[""Like super-massive black holes in the early Universe? A fan of big ol' radio jets? Then you'll love this new paper led by Anniek Gloudemans: <LINK> which systematically explores the low-frequency radio properties of the high-redshift quasar population.""]",https://arxiv.org/abs/2110.06222,"Optically luminous quasars at $z > 5$ are important probes of super-massive black hole (SMBH) formation. With new and future radio facilities, the discovery of the brightest low-frequency radio sources in this epoch would be an important new probe of cosmic reionization through 21-cm absorption experiments. In this work, we systematically study the low-frequency radio properties of a sample of 115 known spectroscopically confirmed $z>5$ quasars using the second data release of the Low Frequency Array (LOFAR) Two Metre Sky survey (LoTSS-DR2), reaching noise levels of $\sim$80 $\mu$Jy beam$^{-1}$ (at 144 MHz) over an area of $\sim5720$ deg$^2$. We find that 41 sources (36%) are detected in LoTSS-DR2 at $>2 \sigma$ significance and we explore the evolution of their radio properties (power, spectral index, and radio loudness) as a function of redshift and rest-frame ultra-violet properties. We obtain a median spectral index of $-0.29^{+0.10}_{-0.09}$ by stacking 93 quasars using LoTSS-DR2 and Faint Images of the Radio Sky at Twenty Centimetres (FIRST) data at 1.4 GHz, in line with observations of quasars at $z<3$. We compare the radio loudness of the high-$z$ quasar sample to a lower-$z$ quasar sample at $z\sim2$ and find that the two radio loudness distributions are consistent with no evolution, although the low number of high-z quasars means that we cannot rule out weak evolution. Furthermore, we make a first order empirical estimate of the $z=6$ quasar radio luminosity function, which is used to derive the expected number of high-$z$ sources that will be detected in the completed LoTSS survey. This work highlights the fact that new deep radio observations can be a valuable tool in selecting high-$z$ quasar candidates for follow-up spectroscopic observations by decreasing contamination of stellar dwarfs and reducing possible selection biases introduced by strict colour cuts. ",Low frequency radio properties of the $z&gt;5$ quasar population
93,1448622211653267460,3143062656,Bo-Yu Chen,"['Happy to share our new preprint about using DDSP and GAN to generate DJ transition.\n\nWork done during the internship in Sony with @ddman1101, Wei-Hsiang Liao, @_marmarti, @mittu1204 and @affige_yang 🤩🤩🤩🤩\n\n🎛 Paper: <LINK> \n🎛 Demo: <LINK> <LINK>']",https://arxiv.org/abs/2110.06525,"A central task of a Disc Jockey (DJ) is to create a mixset of mu-sic with seamless transitions between adjacent tracks. In this paper, we explore a data-driven approach that uses a generative adversarial network to create the song transition by learning from real-world DJ mixes. In particular, the generator of the model uses two differentiable digital signal processing components, an equalizer (EQ) and a fader, to mix two tracks selected by a data generation pipeline. The generator has to set the parameters of the EQs and fader in such away that the resulting mix resembles real mixes created by humanDJ, as judged by the discriminator counterpart. Result of a listening test shows that the model can achieve competitive results compared with a number of baselines. ","Automatic DJ Transitions with Differentiable Audio Effects and
  Generative Adversarial Networks"
94,1448620347033587716,1138762581164855298,Christoph Ternes,"['New paper, <LINK> We discuss the current status of the reactor antineutrino anomaly. Using the newest flux models, the RAA is resolved. We also show that reactor experiments exclude the region of sterile neutrino parameters preferred by Gallium experiments.']",https://arxiv.org/abs/2110.06820,"We study the status of the reactor antineutrino anomaly in light of recent reactor flux models obtained with the conversion and summation methods. We present a new improved calculation of the IBD yields of the standard Huber-Mueller (HM) model and those of the new models. We show that the reactor rates and the fuel evolution data are consistent with the predictions of the Kurchatov Institute (KI) conversion model and with those of the Estienne-Fallot (EF) summation model, leading to a plausible robust demise of the reactor antineutrino anomaly. We show that the results of several goodness of fit tests favor the KI and EF models over other models that we considered. We also discuss the implications of the new reactor flux models for short-baseline neutrino oscillations due to active-sterile oscillations. We show that reactor data give upper bounds on active-sterile neutrino mixing that are not very different for the reactor flux models under consideration and are in tension with the large mixing required by the Gallium anomaly that has been refreshed by the recent results of the BEST experiment. ",Reactor antineutrino anomaly in light of recent flux model refinements
95,1448614056949161988,219963826,Geri Skenderi,"['Proud of our team for this one, our new #WACV2022 paper proposes a new benchmark and an effective semi-supervised technique to tackle the video-to-shop problem. Paper and code:\n<LINK>\n<LINK> <LINK>', '@JoppiChristian @godimarcovr']",https://arxiv.org/abs/2110.02627,"Retrieving clothes which are worn in social media videos (Instagram, TikTok) is the latest frontier of e-fashion, referred to as ""video-to-shop"" in the computer vision literature. In this paper we present MovingFashion, the first publicly available dataset to cope with this challenge. MovingFashion is composed of 14855 social videos, each one of them associated to e-commerce ""shop"" images where the corresponding clothing items are clearly portrayed. In addition, we present a network for retrieving the shop images in this scenario, dubbed SEAM Match-RCNN. The model is trained by image-to-video domain adaptation, allowing to use video sequences where only their association with a shop image is given, eliminating the need of millions of annotated bounding boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted sum of few frames (10) of a social video is enough to individuate the correct product within the first 5 retrieved items in a 14K+ shop element gallery with an accuracy of 80%. This provides the best performance on MovingFashion, comparing exhaustively against the related state-of-the-art approaches and alternative baselines. ",MovingFashion: a Benchmark for the Video-to-Shop Challenge
96,1448575385583726593,72030668,Gilles Louppe,"['New paper📢""Averting A Crisis In Simulation-Based Inference"" <LINK> -- a huge team effort led by my students @joeri_hermans and @ArnaudDelaunoy', 'By definition, approximate Bayesian inference results in approximate posteriors. But are these approximate posteriors actually adequate enough for scientific inquiry? Shall we trust these approximate posteriors to constrain model parameters or reject theoretical models?', 'In this work, we extensively benchmark current algorithms for simulation-based inference and gather the following empirical observations. https://t.co/mr9POVIMmP', 'In short, we find that all algorithms may lead to overconfident approximations, i.e. to posteriors that are ""smaller than they should be"". In this figure, this happens every time a curve is below the diagonal 👇😟 https://t.co/6Je6uR0GQp', 'Unfortunately, this implies that if these approximate posteriors were used to constrain parameters of interest, then actually plausible parameters could be regarded as implausible, which in turn could lead to wrong scientific conclusions.', 'Faced with this empirical evidence, we argue that research efforts should now consider theoretical and methodological developments of conservative approximate inference algorithms.', 'For now, we find that ensembling leads systematically to more conservative inference. Averaging approximate posteriors moves the curves upwards, closer or above the diagonal. https://t.co/8gnNUvlGS0']",https://arxiv.org/abs/2110.06581,"We present extensive empirical evidence showing that current Bayesian simulation-based inference algorithms are inadequate for the falsificationist methodology of scientific inquiry. Our results collected through months of experimental computations show that all benchmarked algorithms -- (S)NPE, (S)NRE, SNL and variants of ABC -- may produce overconfident posterior approximations, which makes them demonstrably unreliable and dangerous if one's scientific goal is to constrain parameters of interest. We believe that failing to address this issue will lead to a well-founded trust crisis in simulation-based inference. For this reason, we argue that research efforts should now consider theoretical and methodological developments of conservative approximate inference algorithms and present research directions towards this objective. In this regard, we show empirical evidence that ensembles are consistently more reliable. ",Averting A Crisis In Simulation-Based Inference
97,1448565286328848385,73085549,Tim Hoffmann,['new paper: <LINK>\nReally proud of this one. We show that compact Bonnet pairs exist.'],https://arxiv.org/abs/2110.06335,We explicitly construct a pair of immersed tori in three dimensional Euclidean space that are related by a mean curvature preserving isometry. These are the first examples of compact Bonnet pairs. This resolves a longstanding open problem on whether the metric and mean curvature function determine a unique compact surface. We use the relationship between Bonnet pairs and isothermic surfaces. As a part of our construction we give a complete classification of isothermic surfaces with one family of planar curvature lines and of the corresponding cylinders. ,Compact Bonnet Pairs: isometric tori with the same curvatures
98,1448564975572770818,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper by @meetlilychenv2, me and others about using quantum annealers to solve network problems <LINK> work performed @JQCDurNew and @DurhamQLM <LINK>']",https://arxiv.org/abs/2110.06321,"Energy efficient routing in wireless sensor networks has attracted attention from researchers in both academia and industry, most recently motivated by the opportunity to use SDN (software defined network)-inspired approaches. These problems are NP-hard, with algorithms needing computation time which scales faster than polynomial in the problem size. Consequently, heuristic algorithms are used in practice, which are unable to guarantee optimally. In this short paper, we show proof-of-principle for the use of a quantum annealing processor instead of a classical processor, to find optimal or near-optimal solutions very quickly. Our preliminary results for small networks show that this approach using quantum computing has great promise and may open the door for other significant improvements in the efficacy of network algorithms. ","Controller-based Energy-Aware Wireless Sensor Network Routing using
  Quantum Algorithms"
99,1448450791224791045,1111199984077164544,Paolo Perrone,"['New paper on lenses, probability, and weighted categories!\n<LINK>', '@myers_jaz Thank you!', '@DavidCorfield8 @SchreiberUrs Interesting... It seems to be related indeed!']",https://arxiv.org/abs/2110.06591,"This paper makes mathematically precise the idea that conditional probabilities are analogous to path liftings in geometry. The idea of lifting is modelled in terms of the category-theoretic concept of a lens, which can be interpreted as a consistent choice of arrow liftings. The category we study is the one of probability measures over a given standard Borel space, with morphisms given by the couplings, or transport plans. The geometrical picture is even more apparent once we equip the arrows of the category with weights, which one can interpret as ""lengths"" or ""costs"", forming a so-called weighted category, which unifies several concepts of category theory and metric geometry. Indeed, we show that the weighted version of a lens is tightly connected to the notion of submetry in geometry. Every weighted category gives rise to a pseudo-quasimetric space via optimization over the arrows. In particular, Wasserstein spaces can be obtained from the weighted categories of probability measures and their couplings, with the weight of a coupling given by its cost. In this case, conditionals allow one to form weighted lenses, which one can interpret as ""lifting transport plans, while preserving their cost"". ",Lifting couplings in Wasserstein spaces
100,1448374577864708099,992271134,Isaac Malsky,['New paper! <LINK>'],https://arxiv.org/abs/2110.05593,"The advent of high-resolution spectroscopy as a method for exoplanet atmospheric characterization has expanded our capability to study non-transiting planets, increasing the number of planets accessible for observation. Many of the most favorable targets for atmospheric characterization are hot Jupiters, where we expect large spatial variation in physical conditions such as temperature, wind speed, and cloud coverage, making viewing geometry important. Three-dimensional models have generally simulated observational properties of hot Jupiters assuming edge-on viewing, which neglects planets without near edge-on orbits. As the first investigation of how orbital inclination manifests in high-resolution emission spectra, we use a General Circulation Model to simulate the atmospheric structure of Upsilon Andromedae b, a non-transiting hot Jupiter. In order to accurately capture scattering from clouds, we implement a generalized two-stream radiative transfer routine for inhomogeneous multiple scattering atmospheres. We compare models with and without clouds, as cloud coverage intensifies spatial variations. Cloud coverage increases both the net Doppler shifts and the variation of the continuum flux amplitude over the course of the planet's orbit. As orbital inclination decreases, four key features also decrease in both the clear and cloudy models: 1) the average continuum flux level, 2) the amplitude of the variation in continuum with orbital phase, 3) net Doppler shifts of spectral lines, and 4) Doppler broadening in the spectra. Models capable of treating inhomogeneous cloud coverage and different viewing geometries are critical in understanding high-resolution emission spectra, enabling an additional avenue to investigate these extreme atmospheres. ","Modeling the high-resolution emission spectra of clear and cloudy
  non-transiting hot Jupiters"
101,1448366944852262916,1424001956,Paul schrater,"['New Paper using Sheaves to create consistent knowledge graph embeddings that subsumes standard approaches and allows for priors, conditioning, and more.  <LINK>']",https://arxiv.org/abs/2110.03789,"Knowledge graph embedding involves learning representations of entities -- the vertices of the graph -- and relations -- the edges of the graph -- such that the resulting representations encode the known factual information represented by the knowledge graph are internally consistent and can be used in the inference of new relations. We show that knowledge graph embedding is naturally expressed in the topological and categorical language of \textit{cellular sheaves}: learning a knowledge graph embedding corresponds to learning a \textit{knowledge sheaf} over the graph, subject to certain constraints. In addition to providing a generalized framework for reasoning about knowledge graph embedding models, this sheaf-theoretic perspective admits the expression of a broad class of prior constraints on embeddings and offers novel inferential capabilities. We leverage the recently developed spectral theory of sheaf Laplacians to understand the local and global consistency of embeddings and develop new methods for reasoning over composite relations through harmonic extension with respect to the sheaf Laplacian. We then implement these ideas to highlight the benefits of the extensions inspired by this new perspective. ","Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph
  Embedding"
102,1448364157498060802,1331816112502104130,DeWeese Lab,"['We’re thrilled to announce our new paper, in which we derive and test a ✨first-principles theory of generalization in deep learning!✨ We affectionately call it the Theory of Eigenlearning. 🧵⤵\n\n<LINK>', 'Recent breakthroughs have shown that ∞-width nets are actually very simple: their evolution’s described by a “neural tangent kernel” (NTK), and their final learned functions (when trained on MSE loss) are given by kernel regression with that NTK. (2/n)', 'We show that wide net learning is easily understood in the eigenbasis of the NTK: the network more easily learns high-eigenvalue eigenfunctions, which typically correspond to low spatial freq, quantifying the well-known but vague notion of “spectral bias” to low freqs. (3/n) https://t.co/Imki19Nzll', 'We derive simple expressions for the generalization of NTK regression, then show that they accurately predict generalization *even in finite nets!* We focus on a simple new measure of generalization we call ""learnability""; in Fig, exp (dots) agrees v well w/theory (curves) (4/n) https://t.co/zPP9omyrEb', 'We also prove a fundamental conservation law governing the inductive bias of wide nets: all NTKs have the same fixed budget of “learnability” that they must divvy up among their eigenmodes. Fig shows how the sum of these learnabilities is always the trainset size. (5/n) https://t.co/5U6T9Snpw6', 'This work was done by lab members Jamie Simon and Maddie Dickens. It builds on this pioneering work from @blake__bordelon, @canatar_a, and @CPehlevan, which you should also check out! https://t.co/QeDKdT6U7E', 'If you have any questions, drop us a tweet or an email! (7/7)']",https://arxiv.org/abs/2110.03922,"Kernel regression is an important nonparametric learning algorithm with an equivalence to neural networks in the infinite-width limit. Understanding its generalization behavior is thus an important task for machine learning theory. In this work, we provide a theory of the inductive bias and generalization of kernel regression using a new measure characterizing the ""learnability"" of a given target function. We prove that a kernel's inductive bias can be characterized as a fixed budget of learnability, allocated to its eigenmodes, that can only be increased with the addition of more training data. We then use this rule to derive expressions for the mean and covariance of the predicted function and gain insight into the overfitting and adversarial robustness of kernel regression and the hardness of the classic parity problem. We show agreement between our theoretical results and both kernel regression and wide finite networks on real and synthetic learning tasks. ","A Theory of the Inductive Bias and Generalization of Kernel Regression
  and Wide Neural Networks"
103,1448363335271866371,1074448308578336768,Tony Z. Zhao,"['""Insert anything into anything!""\n\nNew paper applying offline RL to industrial insertion. Test it with 12 new tasks, 100/100 success rate on all of them, with only 6 minutes of finetuning time on average!\n\n📝 <LINK>\n🌎 <LINK> <LINK>']",https://arxiv.org/abs/2110.04276,"Reinforcement learning (RL) can in principle let robots automatically adapt to new tasks, but current RL methods require a large number of trials to accomplish this. In this paper, we tackle rapid adaptation to new tasks through the framework of meta-learning, which utilizes past tasks to learn to adapt with a specific focus on industrial insertion tasks. Fast adaptation is crucial because prohibitively large number of on-robot trials will potentially damage hardware pieces. Additionally, effective adaptation is also feasible in that experience among different insertion applications can be largely leveraged by each other. In this setting, we address two specific challenges when applying meta-learning. First, conventional meta-RL algorithms require lengthy online meta-training. We show that this can be replaced with appropriately chosen offline data, resulting in an offline meta-RL method that only requires demonstrations and trials from each of the prior tasks, without the need to run costly meta-RL procedures online. Second, meta-RL methods can fail to generalize to new tasks that are too different from those seen at meta-training time, which poses a particular challenge in industrial applications, where high success rates are critical. We address this by combining contextual meta-learning with direct online finetuning: if the new task is similar to those seen in the prior data, then the contextual meta-learner adapts immediately, and if it is too different, it gradually adapts through finetuning. We show that our approach is able to quickly adapt to a variety of different insertion tasks, with a success rate of 100% using only a fraction of the samples needed for learning the tasks from scratch. Experiment videos and details are available at this https URL ",Offline Meta-Reinforcement Learning for Industrial Insertion
104,1448360409224843270,1377629674432438272,Jake Lustig-Yaeger,"['New paper! \n\nAlthough most known exoplanets are detected because they transit, the vast majority of exoplanets don’t transit. These are their stories. {Duh Dun}.    \n\n<LINK> <LINK>', 'Last year, @kevinbstevenson wrote a paper introducing the concept of Planetary Infrared Excess, or PIE, as a novel method to characterize the atmospheres of transiting and non-transiting planets alike. https://t.co/1TEds1UUnW', 'Exoplanets are MUCH cooler than stars. Fact. Because of this planets and stars emit light with a much different spectrum, and the PIE technique uses broad wavelength spectra to uniquely resolve both light sources, despite them being spatially unresolved. https://t.co/ewLt9MDcHn', 'In collaboration with @kevinbstevenson @mayorgalc @NorBidTheStars @_astronomay @izenplanet @mommascientist, we set out to determine how well the PIE technique will work to study the atmospheres of hot Jupiters.', 'First, we cooked up a PIE model to investigate whether it’s possible to retrieve information about exoplanet atmospheres while simultaneously modeling the light emitted by the star and the much fainter planet. https://t.co/sp41I845Wd', 'Second, we examined whether or not stellar parameters can masquerade as planetary parameters or vice versa (i.e., are there significant planet-star degeneracies?). With broad enough wavelength coverage, we found the two sources to be separable and not degenerate. https://t.co/77oGnbRSPR', 'We then repeated our analyses using different JWST instruments that cover different wavelength ranges and using different exposure times to identify the optimal use of JWST time/data. We found that a combination of NIRISS+NIRSpec+MIRI performs best. https://t.co/7KMbmWE1dE', 'Then we forgot everything we knew about the planet’s radius and tried to see if we could constrain it using PIE. Again, broad wavelength data using all three instruments is optimal. https://t.co/tNogMy58nd', 'All in all, it looks like PIE is on the menu for follow-up study and validation using JWST ERS and Cycle 1 observations!\n\nFor more details including how we handled potential pitfalls, like exozodi dust and absolute flux calibration, check out the paper on the arxiv nearest you. https://t.co/Sy4Jgr3rAL', ""And here's a link to the accepted version of the paper on  arxiv: https://t.co/osFAg7Rhzw https://t.co/3JnlHci6Xf""]",http://arxiv.org/abs/2110.02247,"To increase the sample size of future atmospheric characterization efforts, we build on the planetary infrared excess (PIE) technique that has been proposed as a means to detect and characterize the thermal spectra of transiting and non-transiting exoplanets using sufficiently broad wavelength coverage to uniquely constrain the stellar and planetary spectral components from spatially unresolved observations. We performed simultaneous retrievals of stellar and planetary spectra for the archetypal planet WASP-43b in its original configuration and a non-transiting configuration to determine the efficacy of the PIE technique for characterizing the planet's nightside atmospheric thermal structure and composition using typical out-of-transit JWST observations. We found that using PIE with JWST should enable the stellar and planetary spectra to be disentangled with no degeneracies seen between the two flux sources, thus allowing robust constraints on the planet's nightside thermal structure and water abundance to be retrieved. The broad wavelength coverage achieved by combining spectra from NIRISS, NIRSpec, and MIRI enables PIE retrievals that are within 10% of the precision attained using traditional secondary eclipse measurements, although mid-IR observations with MIRI alone may face up to 3.5 times lower precision on the planet's irradiation temperature. For non-transiting planets with unconstrained radius priors, we were able to identify and break the degeneracy between planet radius and irradiation temperature using data that resolved the peak of both the stellar and planetary spectra, thus potentially increasing the number of planets amenable to atmospheric characterization with JWST and other future mission concepts. ","Retrieving Exoplanet Atmospheres using Planetary Infrared Excess:
  Prospects for the Nightside of WASP-43 b and other Hot Jupiters"
105,1448350418636005393,22168984,Nicholas FitzGerald,['New paper with @MsdJ29 @_theopompus @feishaAI @professorwcohen at Google Research: Mention Memory <LINK> <LINK>'],https://arxiv.org/abs/2110.06176,"Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining. ","Mention Memory: incorporating textual knowledge into Transformers
  through entity mention attention"
106,1448296299560853504,584142796,Carl Rodriguez,"['Extremely cool new paper from @ClaireShiYe on modeling 47 Tuc and all the compact objects (particularly neutron stars and millisecond pulsars in it)!  <LINK>', 'Of particular note: Claire shows that you *can\'t* reproduce 47 Tuc by starting from an equilibrium King model; you need to start with a shallower profile.  In this case, she started with an ""Elson"" profile inspired by observations of young massive clusters in the local universe.']",https://arxiv.org/abs/2110.05495,"The globular cluster 47 Tucanae (47 Tuc) is one of the most massive star clusters in the Milky Way and is exceptionally rich in exotic stellar populations. For several decades it has been a favorite target of observers, and yet it is computationally very challenging to model because of its large number of stars ($N\gtrsim 10^6$) and high density. Here we present detailed and self-consistent 47 Tuc models computed with the \texttt{Cluster Monte Carlo} code (\texttt{CMC}). The models include all relevant dynamical interactions coupled to stellar and binary evolution, and reproduce various observations, including the surface brightness and velocity dispersion profiles, pulsar accelerations, and numbers of compact objects. We show that the present properties of 47 Tuc are best reproduced by adopting an initial stellar mass function that is both bottom-heavy and top-light relative to standard assumptions \citep[as in, e.g.,][]{Kroupa2001}, and an initial Elson profile \citep{Elson1987} that is overfilling the cluster's tidal radius. We include new prescriptions in \texttt{CMC} for the formation of binaries through giant star collisions and tidal captures, and we show that these mechanisms play a crucial role in the formation of neutron star binaries and millisecond pulsars in 47 Tuc; our best-fit model contains $\sim 50$ millisecond pulsars, $80\%$ of which are formed through giant collisions and tidal captures. Our models also suggest that 47 Tuc presently contains up to $\sim 200$ stellar-mass black holes, $\sim 5$ binary black holes, $\sim 15$ low-mass X-ray binaries, and $\sim 300$ cataclysmic variables. ",Compact Object Modeling in the Globular Cluster 47 Tucanae
107,1448197718799646721,929633835309981698,mmatsuo,['Our new paper is now on arXiv😊 \nWe propose a spin-transfer mechanism induced by inertial motion in a magnetic bilayer system composed of two host media and a narrow vacuum gap in between.\n\n<LINK>'],https://arxiv.org/abs/2110.05871,"We propose a spin transport induced by inertial motion. Our system is composed of two host media and a narrow vacuum gap in between. One of the hosts is sliding at a constant speed relative to the other. This mechanical motion causes the Doppler effect that shifts the density of states and the nonequilibrium distribution function in the moving medium. Those shifts induce the difference in the distribution function between the two media and result in tunnelling spin current. The spin current is calculated from the Schwinger-Keldysh formalism with a spin tunnelling Hamiltonian. This scheme does not require either temperature difference, voltage or chemical potential. ",Motion-induced spin transfer
108,1448184923706302465,18359713,Ofer Lavi,"['Are you tired of having the same conversation again and again? Have a look at our new @emnlpmeeting paper, now on arXiv. <LINK>. A fresh view on the good old edit distance with a pinch of neural sentence embeddings. @IBMResearch @boknilev @adiyossLC @oren_data']",https://arxiv.org/abs/2110.05780,"Dialog is a core building block of human natural language interactions. It contains multi-party utterances used to convey information from one party to another in a dynamic and evolving manner. The ability to compare dialogs is beneficial in many real world use cases, such as conversation analytics for contact center calls and virtual agent design. We propose a novel adaptation of the edit distance metric to the scenario of dialog similarity. Our approach takes into account various conversation aspects such as utterance semantics, conversation flow, and the participants. We evaluate this new approach and compare it to existing document similarity measures on two publicly available datasets. The results demonstrate that our method outperforms the other approaches in capturing dialog flow, and is better aligned with the human perception of conversation similarity. ","We've had this conversation before: A Novel Approach to Measuring Dialog
  Similarity"
109,1448085867747291141,907621053547126785,Pratyush Tiwary,"['New paper! We combine information bottleneck approach with local molecular field (LMF) type ideas to ask how much long range forces matter for transition state dynamics &amp; rates in the classic problem of NaCl dissociation in water. Turns out very little! <LINK> <LINK>', '@TimothyDuignan There might be a connection! However I must point out that dilute and concentrated regimes can be very different. We are in the most dilute regime in our preprint']",https://arxiv.org/abs/2110.05646,"We study NaCl ion-pair dissociation in a dilute aqueous solution using computer simulations both for the full system with long range Coulomb interactions and for a well chosen reference system with short range intermolecular interactions. Analyzing results using concepts from Local Molecular Field (LMF) theory and the recently proposed AI-based analysis tool ""State predictive information bottleneck"" (SPIB) we show that the system with short range interactions can accurately reproduce the transition rate for the dissociation process, the dynamics for moving between the underlying metastable states, and the transition state ensemble. Contributions from long range interactions can be largely neglected for these processes because long range forces from the direct interionic Coulomb interactions are almost completely canceled ($>90\%$) by those from solvent interactions over the length scale where the transition takes place. Thus for this important monovalent ion-pair system, short range forces alone are able to capture detailed consequences of the collective solvent motion, allowing the use of physically suggestive and computationally efficient short range models for the disassociation event. We believe that the framework here should be applicable to disentangling mechanisms for more complex processes such as multivalent ion disassociation, where previous work has suggested that long range contributions may be more important. ","Influence of long range forces on the transition states and dynamics of
  NaCl ion-pair dissociation in water"
110,1447986706435641345,76009287,Desh Raj,"['📢 New paper on ArXiv 📢\n\n""Injecting text and cross-lingual supervision in few-shot learning from self-supervised models""\n\nabs: <LINK>\npdf: <LINK>', 'How do you create a hybrid ASR system for a new language X with only 15 mins of transcribed speech?\n\nAnswer: Use XLSR-53, transcribed speech from other languages, and extra text from language X https://t.co/UCwPLIRv2Q', 'Step 1: Initialize the acoustic model from XLSR-53 available from @huggingface \n\n(We also tried other models but XLSR-53 works best due to diversity of pre-training languages.)', 'Step 2: Replace output layer with a phonemic (senone) output that predicts X-SAMPA based universal phoneset and fine-tune acoustic model on transcribed speech from other languages.', 'Step 3: Fine-tune on 15 min of X with lattice-free MMI objective, and expand the denominator graph with additional text from X. https://t.co/gkMtkldJfS', 'On 3 low-resource languages (Pashto, Haitian, Georgian), we got WERs matching those using 10 hours of transcribed speech.\n\nWith the full transcribed speech (~80h), we achieve SOTA WER on Pashto and Haitian.', 'This work was led by my colleague Matthew Wiesner (https://t.co/IUCBvYMPNr). I learned a lot from him about tricks with lexicons, decoding graphs, etc. that seem obsolete in the ""end-to-end"" world but turn out to be immensely useful!', 'BTW, Matthew has a nice toolkit for PyTorch-based training of acoustic models (with recently added init. from HF models) while still using LF-MMI training and WFST-decoding from Kaldi. Check it out here: https://t.co/gfJ9gugeom']",https://arxiv.org/abs/2110.04863,"Self-supervised model pre-training has recently garnered significant interest, but relatively few efforts have explored using additional resources in fine-tuning these models. We demonstrate how universal phoneset acoustic models can leverage cross-lingual supervision to improve transfer of pretrained self-supervised representations to new languages. We also show how target-language text can be used to enable and improve fine-tuning with the lattice-free maximum mutual information (LF-MMI) objective. In three low-resource languages these techniques greatly improved few-shot learning performance. ","Injecting Text and Cross-lingual Supervision in Few-shot Learning from
  Self-Supervised Models"
111,1447936262757834758,1402610104696979459,Palina Salanevich,"['A new paper on audio enhancement via online non-negative matrix factorization, just submitted: <LINK> <LINK>']",https://arxiv.org/abs/2110.03114,"We propose a method for noise reduction, the task of producing a clean audio signal from a recording corrupted by additive noise. Many common approaches to this problem are based upon applying non-negative matrix factorization to spectrogram measurements. These methods use a noiseless recording, which is believed to be similar in structure to the signal of interest, and a pure-noise recording to learn dictionaries for the true signal and the noise. One may then construct an approximation of the true signal by projecting the corrupted recording on to the clean dictionary. In this work, we build upon these methods by proposing the use of \emph{online} non-negative matrix factorization for this problem. This method is more memory efficient than traditional non-negative matrix factorization and also has potential applications to real-time denoising. ",On audio enhancement via online non-negative matrix factorization
112,1447894959626063874,1205923801709588481,Yuval Kirstain,"['I’m thrilled to share our new work “A Few More Examples May Be Worth *Billions* of Parameters”.\n\nJoint w/ @PSH_Lewis @riedelcastro @omerlevy_\nPaper: <LINK> \n\n1/4 <LINK>', 'While scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends on the *task’s format*.\n\n2/4 https://t.co/1ssoX1K4iE', 'For classification, multiple choice, and extractive QA, annotating a few more examples may yield similar benefits as adding *billions* of model parameters. However, in open QA tasks, parameters are of immense value that just cannot be traded with labeled data.\n\n3/4 https://t.co/iHyfNdkRiT', 'We reached this conclusion after training thousands of models and conducting ablation experiments where we convert one format into another!\n\nCheck out the paper for more :)\n\n4/4 https://t.co/3shlUXHwVq']",http://arxiv.org/abs/2110.04374,"We investigate the dynamics of increasing the number of model parameters versus the number of labeled examples across a wide variety of tasks. Our exploration reveals that while scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends on the task's format. Specifically, in open question answering tasks, enlarging the training set does not improve performance. In contrast, classification, extractive question answering, and multiple choice tasks benefit so much from additional examples that collecting a few hundred examples is often ""worth"" billions of parameters. We hypothesize that unlike open question answering, which involves recalling specific information, solving strategies for tasks with a more restricted output space transfer across examples, and can therefore be learned with small amounts of labeled data. ",A Few More Examples May Be Worth Billions of Parameters
113,1447867635442585601,3228486315,Daniele Grattarola,"['In our new paper, we introduce a unifying and modular framework for graph pooling: Select, Reduce, Connect.\nWe also propose a taxonomy of pooling and show why small-graph classification is not telling us the full story.\n\nArxiv: <LINK>\n\nTime for a 🧵on 🎱: <LINK>', 'Let\'s start from SRC, the ""message-passing"" of pooling.\n\nS: Selects (some) input nodes to map to one (or more) supernodes. Essentially decides what information is contained in the new nodes. \n\nR: Reduces the supernodes to singletons.\n\nC: decides how the new nodes are Connected. https://t.co/kkMFW4bHCj', ""Why is this important? Well, it's a simple idea, but it lets us unify seemingly incompatible things like DiffPool and TopK under a common framework. \n\nSRC is also a useful template for the community to share their contributions (e.g., in Spektral or PyG). https://t.co/r2t1HeyY0I"", 'The taxonomy follows from this unification. \nWe have dense/sparse, trainable/non-trainable, fixed/adaptive (number of output nodes), and hierarchical/global methods. \nAlmost all combinations of these four are found in the wild. https://t.co/xKYd6AcFv2', 'And then we deal with the evaluation problem. \nThe short story is that classification accuracy is only one possible tool, but it ignores things like attributes and structure preservation. \nAlso, for small graphs, the differences between methods are not that clear. https://t.co/gIKmuPITsD', ""This also helps explain the paper by Mesquita et al.(https://t.co/t7aFEqOZVQ), who are often misinterpreted as saying that pooling is useless.\nThey really say that dense pooling, on small graphs, after msg-passing, is basically all the same.\n\nDon't pool (🙃) everything together."", ""So what are the takeaways?\nThe safe choice is to use NDP or Graclus. They work well enough most of the time.\nIf you're chasing benchmark accuracy, then use trainable dense methods (DiffPool, MinCut).\nA trainable, dense, and adaptive method is missing and will likely get to SOTA."", 'Given these insights, my current standing on the ""usefulness"" debate is that not all problems benefit from pooling, a priori, and definitely not a single type of pooling. In CNNs pooling makes sense because it gives us invariance to small shifts, but in GNNs this is not immediate', ""My idea is that we should use pooling when the structure of the problem suggests an inherent hierarchy, levels of organization that can't be captured by very-local MP.\n\nBasically all successes in ML come from choosing the right inductive bias, so why should it be different here?"", 'Embedding (or pre-computing) that hierarchy as part of the data may lead to better results than having a one-size-fits-all pooling algorithm, essentially moving to a paradigm where the GNN input is graph+hierarchy.\n\nThis will be an interesting direction to explore.', 'Finally, this paper was a collaboration with the amazing @TruckersHitch and @FilippoMariaBi1, who have spent countless hours trying to decipher the chaos that is graph pooling and what it means to have a ""good"" pooled graph. Huge thanks to them!', ""Code: https://t.co/Z4tCwRdI1b\n\nIf we've missed your paper in Table 2, feel free to reach out and we'll categorize it and add it!!"", '@Azarkhalili Absolutely, the code is already designed to fit into Spektral so I will merge it soon into the library! Stay tuned']",https://arxiv.org/abs/2110.05292,"Inspired by the conventional pooling layers in convolutional neural networks, many recent works in the field of graph machine learning have introduced pooling operators to reduce the size of graphs. The great variety in the literature stems from the many possible strategies for coarsening a graph, which may depend on different assumptions on the graph structure or the specific downstream task. In this paper we propose a formal characterization of graph pooling based on three main operations, called selection, reduction, and connection, with the goal of unifying the literature under a common framework. Following this formalization, we introduce a taxonomy of pooling operators and categorize more than thirty pooling methods proposed in recent literature. We propose criteria to evaluate the performance of a pooling operator and use them to investigate and contrast the behavior of different classes of the taxonomy on a variety of tasks. ",Understanding Pooling in Graph Neural Networks
114,1447819559470260228,225401718,Aurel Schneider,"['Let me mention this new paper from last week: we combine weak lensing data with gas observations to constrain baryonic feedback and cosmology. <LINK>', 'In a nutshell, we find evidence for very strong baryonic feedback effects. These effects lead to a suppression of the matter power spectrum that is stronger than predicted by most hydrodynamical simulations: https://t.co/uWxr40XVPR', 'In terms of cosmology, strong baryonic effects cannot solve the S8-tension between KiDS and Planck. However, the tension gets reduced from 3.8 to 2.8 sigma once the WL signal is combined with Xray and kSZ data: https://t.co/355jb6VX5z', 'This work confirms the original analysis from KiDS while using a more generally viable baryonic feedback model that is motivated by observations.', 'Many thanks to Konrad Kuijken, @AstroRoyalScot, and the KiDS science team for making their data publicly available!']",https://arxiv.org/abs/2110.02228,"Modern weak-lensing observations are becoming increasingly sensitive to baryonic feedback processes which are still poorly understood. So far, this challenge has been faced either by imposing scale-cuts in the data or by modelling baryonic effects with simple, one-parameter models. In this paper, we rely on a more general, seven-parameter prescription of baryonic feedback effects, which is primarily motivated by observations and has been shown to agree with a plethora of hydrodynamical simulations. By combining weak-lensing data from the Kilo-Degree Survey (KiDS-1000) with observations of gas around galaxy clusters, we are able to constrain baryonic parameters and learn more about feedback and cosmology. In particular, we use cluster gas fractions from X-ray data and gas profiles from kinematic Sunyaev-Zeldovich (kSZ) observations to provide evidence for baryonic feedback that is stronger than predicted by most hydrodynamical simulations. In terms of the matter power spectrum, we report a beyond-percent effect at wave-modes above $k\sim 0.1-0.45$ h/Mpc and a maximum suppression of $12-33$ percent at $k\sim7$ h/Mpc (68 percent confidence level). Regarding the combined parameter $\Sigma_8=\sigma_8(\Omega_m/0.3)^{0.58}$, we find the known tension with the Planck satellite data to be reduced from 3.8 to 2.9 $\sigma$ once baryonic effects are fully included in the analysis pipeline. The tension is further decreased to 2.6 $\sigma$ when the weak-lensing data is combined with X-ray and kSZ observations. We conclude that, while baryonic feedback effects become more important in modern weak-lensing surveys, they are unlikely to act as the main culprit for the observed $\Sigma_8$-tension. ","Constraining baryonic feedback and cosmology with weak-lensing, X-ray,
  and kinematic Sunyaev-Zeldovich observations"
115,1447793397675220997,1287092413194878976,Jesse Michael Han,"['New paper, new SOTA in unsupervised neural machine translation, joint with colleagues at @OpenAI:\n\nUnsupervised Neural Machine Translation with Generative Language Models Only\n\n<LINK> <LINK>', ""We use few-shot learning to amplify GPT-3's zero-shot translations and create fine-tuning datasets for machine translation with no supervision. We can distill these data into much smaller models before running iterative backtranslation at @OpenAI scale."", 'Our best reported model is 100x smaller than GPT-3, yet after distillation + backtranslation attains a new unsupervised SOTA of 42.1 BLEU on the WMT14 English-French benchmark.', 'This is joint work with some amazing teammates at @OpenAI: @ibab_ml, @HarriLEdwards, @arvind_io, @txhf, @spolu, @machinaut, @recurseparadox, @model_mechanic, @AlecRad, @ilyasut.']",https://arxiv.org/abs/2110.05448,"We show how to derive state-of-the-art unsupervised neural machine translation systems from generatively pre-trained language models. Our method consists of three steps: few-shot amplification, distillation, and backtranslation. We first use the zero-shot translation ability of large pre-trained language models to generate translations for a small set of unlabeled sentences. We then amplify these zero-shot translations by using them as few-shot demonstrations for sampling a larger synthetic dataset. This dataset is distilled by discarding the few-shot demonstrations and then fine-tuning. During backtranslation, we repeatedly generate translations for a set of inputs and then fine-tune a single language model on both directions of the translation task at once, ensuring cycle-consistency by swapping the roles of gold monotext and generated translations when fine-tuning. By using our method to leverage GPT-3's zero-shot translation capability, we achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1. ","Unsupervised Neural Machine Translation with Generative Language Models
  Only"
116,1447766100591140874,2851946747,Yuzhen Lu,"['Check out the new paper ""Performance Evaluation of Deep Transfer Learning on Multiclass Identification of Common Weed Species in Cotton Production Systems"", posted on @arxiv : <LINK>. Data and codes are open-sourced.']",https://arxiv.org/abs/2110.04960,"Precision weed management offers a promising solution for sustainable cropping systems through the use of chemical-reduced/non-chemical robotic weeding techniques, which apply suitable control tactics to individual weeds. Therefore, accurate identification of weed species plays a crucial role in such systems to enable precise, individualized weed treatment. This paper makes a first comprehensive evaluation of deep transfer learning (DTL) for identifying common weeds specific to cotton production systems in southern United States. A new dataset for weed identification was created, consisting of 5187 color images of 15 weed classes collected under natural lighting conditions and at varied weed growth stages, in cotton fields during the 2020 and 2021 field seasons. We evaluated 27 state-of-the-art deep learning models through transfer learning and established an extensive benchmark for the considered weed identification task. DTL achieved high classification accuracy of F1 scores exceeding 95%, requiring reasonably short training time (less than 2.5 hours) across models. ResNet101 achieved the best F1-score of 99.1% whereas 14 out of the 27 models achieved F1 scores exceeding 98.0%. However, the performance on minority weed classes with few training samples was less satisfactory for models trained with a conventional, unweighted cross entropy loss function. To address this issue, a weighted cross entropy loss function was adopted, which achieved substantially improved accuracies for minority weed classes. Furthermore, a deep learning-based cosine similarity metrics was employed to analyze the similarity among weed classes, assisting in the interpretation of classifications. Both the codes for model benchmarking and the weed dataset are made publicly available, which expect to be be a valuable resource for future research in weed identification and beyond. ","Performance Evaluation of Deep Transfer Learning on Multiclass
  Identification of Common Weed Species in Cotton Production Systems"
117,1447618499656966148,3422471637,Elias Kammoun,"['#new_paper Check out our paper where we present a new model of the Optical/UV/X-ray spectral energy distribution of AGN, within the context of thermal reverberation. The model adopts an iterative interaction between the disc and the corona (<LINK>) 1/n <LINK>', 'In this context, part of the X-rays irradiating the accretion disc will be emitted as an X-ray reflection. The other part will be absorbed by the disc and re-emitted in the form of thermal radiation with a certain time delay. 2/n', 'We assume the presence of two coronæ, located above and below the disc, responsible of this radiation. We also assume that below a given transition radius (rtrans) all the accretion disc power is transferred to the corona. The transferred fraction is set by the BH spin. 3/n https://t.co/Ti4m5EWz1Y', ""We explored the effects of various parameter on the SED: spin, accretion rate, corona height, transferred power, inclination, etc. The model conserves the energy and the number of photons in the disc-corona system. Here's an example of the effect of the corona height 4/n https://t.co/Y0zQWOg83l"", 'Finally, we applied the model to the average broadband SED of NGC 5548.  5/n https://t.co/vwzZnWzRXy', 'We combined the results of the SED fitting to the ones from time-lag analysis (https://t.co/j3tVJZs0eB). We confirmed that the BH in NGC 5548 is rapidly spinning. The model calculates a posteriori the size of the corona for a given accretion rate, height, and X-ray spectrum 6/n https://t.co/pGwuXzPwBr', 'Final remark: this model cannot be applied to any simultaneous Optical/UV/X-ray SED. The observed optical/UV spectra are the response of the X-ray activity  hours and days before the time of the observations (depending on the time lags). 7/n', ""Final remark Cont'd:Thus, applying the model to observations that are shorter than the time delays may lead to erroneous results. n/n"", '@lmallick_astro It is indeed.. it was kind of surprising when we found out this.']",https://arxiv.org/abs/2110.01249,"We develop a new physical model for the broadband spectral energy distribution (SED) of X-ray illuminated accretion discs, that takes into account the mutual interaction of the accretion disc and the X-ray corona, including all relativistic effects. We assume a Keplerian, optically thick and geometrically thin accretion disc and an X-ray source in the lamp-post geometry that emits an isotropic power-law spectrum with a high-energy cut-off. We assume that all the energy that would be released by thermal radiation in the standard disc model in its innermost part, is transported to the corona, effectively cooling the disc in this region. We include the disc heating due to thermalisation of the absorbed part of the disc illumination by X-ray corona. The X-ray reflection from the disc is also included. We compute the X-ray luminosity and the low-energy X-ray cut-off through an iterative process, taking full account of the interplay between the X-ray illumination of the disc and the resulting accretion disc spectrum which enters the corona so that the energy balance is preserved. The corona radius is also computed from the conservation of the photon's number during Comptonization. We discuss the model SEDs and their dependence on system parameters. The disc-corona interaction has profound effects - it constrains the X-ray luminosity and changes the shape and normalisation of the UV/optical blue bump. We use the new code to fit the broad-band SED of a typical Seyfert 1 galaxy, NGC 5548. We infer a high black-hole spin, an intermediate system inclination, and an accretion rate below 10% of Eddington. The X-ray luminosity in this source could be supported by 45-70% of the accretion energy dissipated in the disc. The new model, named KYNSED, is publicly available to be used for fitting AGN SEDs inside the XSPEC spectral analysis tool. ","A physical model for the broadband energy spectrum of X-ray illuminated
  accretion discs: fitting the spectral energy distribution of NGC 5548"
118,1447588040487182339,1138012858988617728,Hannes Stärk,"['Our new paper is out!⚛️\n3D Infomax improves GNNs for Molecular Property Prediction: <LINK>\n\nWith @GabriCorso @sacdallago @guennemann @pl219_Cambridge and @dom_beaini @TOSSOUPrudencio from @valence_ai 🤗\n\nUse 3D to improve for molecules with unknown 3D! 👇\n1/4 <LINK>', ""I'll present the paper tomorrow in a talk at the @Cambridge_CL AI Seminar: https://t.co/cx8MJcb2VF at 2:15pm CEST\n\nIn short:\nWe teach a GNN to reason about the 3D geometry of molecules given only their 2D graphs which improves the GNN's molecular property predictions by ~22%!\n2/4 https://t.co/ZpUyH4i9Aj"", 'Step 1: Use 3D Infomax pre-training to teach a 2D Net to generate latent 3D information (using molecules with known 3D)\n\nStep 2: Fine-tune to predict properties of molecules with unknown 3D: the 2D Net still generates implicit 3D and uses it to improve predictions!\n3/4 https://t.co/qBLAVstdGL', 'For most molecules, there are multiple likely 3D arrangements of the atoms. We found that leveraging more of them can further improve predictions which 3D Infomax achieves.\nFor some pre-training datasets using multiple 3D structures per molecule is essential to improve!\n4/4 https://t.co/L3Bh3BLrzh', '@simonbatzner @GabriCorso @sacdallago @guennemann @pl219_Cambridge @dom_beaini @TOSSOUPrudencio @valence_ai Dankeschön @simonbatzner, it truly makes me happy to hear that coming from you! 🤗\n(I might be somewhat of a fan of your work)', ""@abhik1368 @GabriCorso @sacdallago @guennemann @pl219_Cambridge @dom_beaini @TOSSOUPrudencio @valence_ai I don't know what classifies as field-based (explanation appreciated), but still:\n\nUsed as additional features for the pre-trained GNN: Yes\n\nUsed during pre-training as features carrying 3D information instead of the conformers: I doubt it works as good, but we have not tried it.""]",https://arxiv.org/abs/2110.04126,"Molecular property prediction is one of the fastest-growing applications of deep learning with critical real-world impacts. Including 3D molecular structure as input to learned models improves their performance for many molecular tasks. However, this information is infeasible to compute at the scale required by several real-world applications. We propose pre-training a model to reason about the geometry of molecules given only their 2D molecular graphs. Using methods from self-supervised learning, we maximize the mutual information between 3D summary vectors and the representations of a Graph Neural Network (GNN) such that they contain latent 3D information. During fine-tuning on molecules with unknown geometry, the GNN still generates implicit 3D information and can use it to improve downstream tasks. We show that 3D pre-training provides significant improvements for a wide range of properties, such as a 22% average MAE reduction on eight quantum mechanical properties. Moreover, the learned representations can be effectively transferred between datasets in different molecular spaces. ",3D Infomax improves GNNs for Molecular Property Prediction
119,1447578234007142405,349321075,Oncel Tuzel,"['“Token pooling” is a new token downsampling method that achieves the state-of-the-art accuracy/computation trade-off for visual transformers. \n\nPaper: <LINK>\nw. Dmitrii Marin, Jen-Hao Rick Chang, @anuragranj, Anish Prabhu, @morastegari \n#MachineLearning #Apple <LINK>']",https://arxiv.org/abs/2110.03860,"Despite the recent success in many applications, the high computational requirements of vision transformers limit their use in resource-constrained settings. While many existing methods improve the quadratic complexity of attention, in most vision transformers, self-attention is not the major computation bottleneck, e.g., more than 80% of the computation is spent on fully-connected layers. To improve the computational complexity of all layers, we propose a novel token downsampling method, called Token Pooling, efficiently exploiting redundancies in the images and intermediate token representations. We show that, under mild assumptions, softmax-attention acts as a high-dimensional low-pass (smoothing) filter. Thus, its output contains redundancy that can be pruned to achieve a better trade-off between the computational cost and accuracy. Our new technique accurately approximates a set of tokens by minimizing the reconstruction error caused by downsampling. We solve this optimization problem via cost-efficient clustering. We rigorously analyze and compare to prior downsampling methods. Our experiments show that Token Pooling significantly improves the cost-accuracy trade-off over the state-of-the-art downsampling. Token Pooling is a simple and effective operator that can benefit many architectures. Applied to DeiT, it achieves the same ImageNet top-1 accuracy using 42% fewer computations. ",Token Pooling in Vision Transformers
120,1447554074807832584,1062794433618460672,Michael Weissman,"[""On #GRExit &amp; pseudoscience in physics education research, new trash paper has come out since my last tweet. New critique included here:<LINK>. I'll submit to journal after hearing reactions.\nDo we really have to just accept this crap? &amp; act as if we believed it? <LINK>""]",https://arxiv.org/abs/2110.04266,"Finding good educational policies requires sound estimates of their potential effects. Methods for making such estimates, i.e. finding causal estimands, have made great progress in the last few decades. Nevertheless, serious errors in causal reasoning have been found previously in papers n a leading physics education journal, Physical Review Physics Education Research. Here we examine three more recent papers from that journal that present explicit methods of causal inference. The methods given include major errors, including in identifying causal mediation, choosing variables to control for, and imputing missing data. ",Methods of Causal Inference in Physics Education Research
121,1447504622776725511,1047809884551569414,Thejs Brinckmann,"['New paper last week! We showed popular methods for computing the non-linear galaxy power spectrum can lead to biases for future surveys and propose a theoretical uncertainty accounting for discrepancies between the codes to evade the problem\n\nCheck it out! <LINK>', 'We considered non-linear estimators Halofit, HMCode and EuclidEmulator, which disagree somewhat on BAO scales and smaller. Otherwise very successful at predicting the non-linear power spectrum, these relatively small differences will prove important for future surveys like Euclid', 'Only considering scales as small as k = 0.4 h/Mpc we still find biases up to 6 sigma in the estimation of individual parameters for a mock data analysis with a Planck + Euclid galaxy clustering setup. Similar results were found by Martinelli et al. 2010.12382 for galaxy lensing.', 'We propose a proof-of-concept theoretical uncertainty, building on Sprenger et al. 1801.08331, tailored to the disagreement between the non-linear estimators. The result is the bias in parameter estimations is reduced from 6 sigma to 1 sigma or less, at a 20% loss in sensitivity.', ""We find this a promising avenue for delivering robust and precise results from future large-scale structure surveys, while accepting that our knowledge of non-linear structure formation isn't perfect. More work can be done to tune the theoretical uncertainty for optimal results.""]",https://arxiv.org/abs/2110.01488,"We implement EuclidEmulator (version 1), an emulator for the non-linear correction of the matter power spectrum, into the MCMC forecasting code MontePython. We compare the performance of Halofit, HMCode, and EuclidEmulator1, both at the level of power spectrum prediction and at the level of posterior probability distributions of the cosmological parameters, for different cosmological models and different galaxy power spectrum wave number cut-offs. We confirm that the choice of the power spectrum predictor has a non-negligible effect on the computed sensitivities when doing cosmological parameter forecasting, even for a conservative wave number cut-off of $0.2\,h\,{\rm Mpc}^{-1}$. We find that EuclidEmulator1 is on average up to $17\%$ more sensitive to the cosmological parameters than the other two codes, with the most significant improvements being for the Hubble parameter of up to $42\%$ and the equation of state of dark energy of up to $26\%$, depending on the case. In addition, we point out that the choice of the power spectrum predictor contributes to the risk of computing a significantly biased mean cosmology when doing parameter estimations. For the four tested scenarios we find biases, averaged over the cosmological parameters, of between 0.5 and 2$\sigma$ (from below $1\sigma$ up to $6\sigma$ for individual parameters). This paper provides a proof of concept that this risk can be mitigated by taking a well-tailored theoretical uncertainty into account as this allows to reduce the bias by a factor of 2 to 5, depending on the case under consideration, while keeping posterior credibility contours small: the standard deviations are amplified by a factor of $\leq1.4$ in all cases. ","Parameter inference with non-linear galaxy clustering: accounting for
  theoretical uncertainties"
122,1447496520975138817,761263821881221120,Dr. Roberta Amato,"['My latest paper!! New experimental data on the scattering of low-energy protons from a sample of the optics of the future ESA X-ray mission @AthenaXobs. These data are the key to minimise the bkg and achieve the ambitious goals of the mission\n<LINK>', 'This work was a team effort and I personally want to thank my colleagues from @uni_tue and my two supervisors prof. A. Santangelo and T. Mineo.', 'This paper was written, submitted, reviewed, and approved during the pandemic. Here is my message to all the people out there struggling because of covid: Hold on! Better times always come! 💪🏼\n#astronomy #research #WomenInSTEM']",https://arxiv.org/abs/2110.04122,"Soft protons are a potential threat for X-ray missions using grazing incidence optics, as once focused onto the detectors they can contribute to increase the background and possibly induce radiation damage as well. The assessment of these undesired effects is especially relevant for the future ESA X-ray mission Athena, due to its large collecting area. To prevent degradation of the instrumental performance, which ultimately could compromise some of the scientific goals of the mission, the adoption of ad-hoc magnetic diverters is envisaged. Dedicated laboratory measurements are fundamental to understand the mechanisms of proton forward scattering, validate the application of the existing physical models to the Athena case and support the design of the diverters. In this paper we report on scattering efficiency measurements of soft protons impinging at grazing incidence onto a Silicon Pore Optics sample, conducted in the framework of the EXACRAD project. Measurements were taken at two different energies, ~470 keV and ~170 keV, and at four different scattering angles between 0.6 deg and 1.2 deg. The results are generally consistent with previous measurements conducted on eROSITA mirror samples, and as expected the peak of the scattering efficiency is found around the angle of specular reflection. ","Scattering efficiencies measurements of soft protons at grazing
  incidence from an Athena Silicon Pore Optics sample"
123,1447469055145807873,1369007721270435842,Eleftheria Malami,['New paper on arXiv\n<LINK> <LINK>'],https://arxiv.org/abs/2110.04240,"The $B^0_s\to D_s^\mp K^\pm$ system allows a determination of the Unitarity Triangle angle $\gamma$. Intrigued by an LHCb analysis showing a surprisingly large result in tension with other $\gamma$ measurements, we point out that also the rates of the individual $B^0_s\to D_s^\mp K^\pm$ channels show puzzling patterns, in accordance with similar decays. We present a formalism to include New-Physics effects and apply it to the data. Interestingly, new contributions of moderate size could accommodate the data. Utilising this formalism in the high-precision $B$ physics era may establish new sources of CP violation. ",Revealing New Physics in $B^0_s\to D_s^\mp K^\pm$ Decays
124,1447412894417649664,59582985,Andreas Wallraff,"['Check out the new paper on ""Large-bandwidth transduction between an optical single #quantum-dot molecule and a superconducting resonator"" which was posted on @arxiv recently: \n<LINK>\nThis is a @NCCR_QSIT collaboration between four @ETH_physics @ETH_en labs. <LINK>']",https://arxiv.org/abs/2110.03230,"Quantum transduction between the microwave and optical domains is an outstanding challenge for long-distance quantum networks based on superconducting qubits. For all transducers realized to date, the generally weak light-matter coupling does not allow high transduction efficiency, large bandwidth, and low noise simultaneously. Here we show that a large electric dipole moment of an exciton in an optically active self-assembled quantum dot molecule (QDM) efficiently couples to a microwave resonator field at a single-photon level. This allows for transduction between microwave and optical photons without coherent optical pump fields to enhance the interaction. With an on-chip device, we demonstrate a sizeable single-photon coupling strength of 16 MHz. Thanks to the fast exciton decay rate in the QDM, the transduction bandwidth between an optical and microwave resonator photon reaches several 100s of MHz. ","Large-bandwidth transduction between an optical single quantum-dot
  molecule and a superconducting resonator"
125,1447362946779082754,2781150596,Innes Bigaran,"['New paper today! \n<LINK>\n(continuing my trend of puns in titles)\ntogether with @RVolkas \n\nWe revisit scalar-leptoquark solutions to anomalies in the muon and electron anomalous magnetic moments, and extend parameter space to include CPV', 'By extending to complex LQ couplings, we look at the impact of this parameter space on lepton EDMs, particularly the muon EDM. We also address a common critique of chirally-enhanced  solutions to AMMs: the radiative correction to lepton masses', 'Restricting the size of correction to lepton masses to minimise fine-tuning, limits the LQ mass to &lt; ~ order 4 TeV. To generate muon AMM at one loop via  top-quark, and not mess with the muon mass, predicts a muon EDM &lt;~ 1e-22: a magnitude set to be probed at future experiments!', ""@jazzwhiz It depends: For example, if FNAL Muon g-2 saw a muon EDM at 1e-21 (design sensitivity) it wouldn't be able to be explained by these models . But these models *could* explain an EDM if it were probed at PSI (~ 1e-23 sensitivity). I'm not 100% on the timelines for these though"", '@HEPAdelaide @RVolkas Well, if you need a seminar speaker any time you know where to find me! ;)']",https://arxiv.org/abs/2110.03707,"We study the two scalar leptoquarks capable of generating chirally-enhanced, sign-dependent contributions to lepton magnetic dipole moments (MDMs) and electric dipole moments (EDMs), $R_2\sim (\mathbf{3}, \mathbf{2}, 7/6)$ and $S_1\sim (\mathbf{3}, \mathbf{1}, -1/3)$. We consider the case in which the electron and muon sectors are decoupled, and leptoquark couplings are assigned complex values. Adopting the coupling ansatz that the electron dipole operator is generated by charm-containing loops, and muon dipole operator by top-containing loops, we find that both minimal leptoquark models remain viable solutions for reconciling anomalies in the muon and electron MDMs, accounting for either of the two current (disparate) electron MDM results from Cs and Rb interferometry experiments. We also examine the correlated corrections to the muon and electron masses generated by these models, and argue that to minimise fine-tuning this introduces an upper bound on viable leptoquark ($\phi$) masses, $m_{\phi}<\mathcal{O}(4)$ TeV. Similar arguments allow us to make a prediction for the upper bound of the muon EDM generated by these models, $|d_\mu|< \mathcal{O}(10^{-22})\; e$ cm, which could be within reach of upcoming experimental programs, including Muon g$-$2 at Fermilab (FNAL), and muEDM at Paul Scherrer Institute (PSI). ","Reflecting on chirality: CP-violating extensions of the single
  scalar-leptoquark solutions for the $(g-2)_{e,\mu}$ puzzles and their
  implications for lepton EDMs"
126,1446604996573687811,704559922143322112,Stephen Casper,"['A new paper from me + some talented coauthors on finding robust, interpretable weaknesses in neural networks and using them to learn about [spurious] feature-class associations. <LINK>']",https://arxiv.org/abs/2110.03605,"It is well understood that modern deep networks are vulnerable to adversarial attacks. However, conventional attack methods fail to produce adversarial perturbations that are intelligible to humans, and they pose limited threats in the physical world. To study feature-class associations in networks and better understand their vulnerability to attacks in the real world, we develop feature-level adversarial perturbations using deep image generators and a novel optimization objective. We term these feature-fool attacks. We show that they are versatile and use them to generate targeted feature-level attacks at the ImageNet scale that are simultaneously interpretable, universal to any source image, and physically-realizable. These attacks reveal spurious, semantically-describable feature/class associations that can be exploited by novel combinations of objects. We use them to guide the design of ""copy/paste"" adversaries in which one natural image is pasted into another to cause a targeted misclassification. Code is available at this https URL ","One Thing to Fool them All: Generating Interpretable, Universal, and
  Physically-Realizable Adversarial Features"
127,1446518686152593409,335130183,Minqi Jiang,"['🏎️ Replay-Guided Adversarial Environment Design\n\nPrioritized Level Replay (PLR) is secretly a form of unsupervised environment design. This leads to new theory improving PLR + impressive zero-shot transfer, like driving the Nürburgring Grand Prix.\n\npaper: <LINK> <LINK>', 'Like living organisms, RL agents are shaped by their environment. How can we improve RL agents by designing the environment instead of the agent? We show that random search, as done by PLR, is surprisingly effective for designing useful environments. https://t.co/LPg360XjRx', 'PLR prioritizes randomly-sampled levels with higher learning potential, leading to auto-curricula that improve generalization. We view PLR as a ""curriculum game"" between a student and two teachers: a generator choosing random levels and a curator selecting for learning potential. https://t.co/w2R08zu3tm', 'This theoretically unifies PLR with other unsupervised environment design (UED) methods like PAIRED, resulting in a version called Robust PLR (PLR⊥), and a replay-based version of PAIRED, called REPAIRED—each provably resulting in a minimax regret policy at equilibrium. https://t.co/JzvBc4T3g5', 'The modification to robustify PLR is counterintuitive: In addition to replacing the value loss priority scores with a regret-approximating score, we only train the agent on trajectories from replayed levels. That is, we improve our agents by training on *less* data. https://t.co/Tr7faNlkWL', 'We test the generalization of agents trained via our “replay-guided” methods to those trained via replay-free  baselines in a maze domain, and show improved zero-shot transfer to challenging human-designed mazes. https://t.co/KyawCnXQeL', 'But our agents quickly grew bored of mazes, so we took them to the speedway. We trained them on racetracks dynamically generated by each method, and test zero-shot transfer to twenty Formula 1 tracks, where Robust PLR agents take home the trophy. https://t.co/w71LoXMBBE', 'Despite its effectiveness, level replay is only half of the game. More sophisticated versions of the generator, whose levels the curator (PLR) selects for replay, should enable even further gains. Stay tuned for some developments in this direction.', 'I owe much of these exciting results to the contributions of my collaborators, who braved with me an, at times, obscure road full of twists and turns. Congrats to @MichaelD1729, @jparkerholder, @j_foerst, @egrefen, and @_rockt. You can chat with us about this work @NeurIPSConf.']",https://arxiv.org/abs/2110.02439,"Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR$^{\perp}$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR$^{\perp}$ improves the performance of PAIRED, from which it inherited its theoretical framework. ",Replay-Guided Adversarial Environment Design
128,1446499611795812355,1115817574078582785,Prithviraj Ammanabrolu,"['🚨New paper alert🚨\nSituated Dialogue Learning through Procedural Environment Generation\n\nWe train goal oriented RL agents to interact with actions and dialogue in situated worlds by training on a curriculum created by a learned text env generator!!\n<LINK> <LINK>', ""Situated dialogue agents trained with RL suffer from a long tail in the distribution of tasks to perform and entities they interact with. They really learn only from the head and memorize the tail, meaning they don't generalize well in novel scenarios"", 'Tl;dr you can take relatively small amounts of crowd sourced env+task data, train a content generator and use that to correct for the long tail of the collected task distribution. Training towards a more uniform task distr via curriculums improves 0 shot perf on novel envs https://t.co/hqhZMfGaNb', 'Many complications due to everything being in rich natural language. Lots of things to figure out like: how to parametrize task distributions in pure text envs (hint look at the verbs), how to jointly generate worlds+quests in text (i.e. env+task)\nw/ Renee Jia, @mark_riedl', 'Check out more papers doing everything from lifelong learning to interactive world building in the LIGHT framework by some very cool people at @parlai_parley \nhttps://t.co/QDlvgjrkgG', ""Here's more threads on this by @mark_riedl (and @ak92501)\nhttps://t.co/uofKVZQlJ0""]",https://arxiv.org/abs/2110.03262,"We teach goal-driven agents to interactively act and speak in situated environments by training on generated curriculums. Our agents operate in LIGHT (Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text adventure game wherein an agent perceives and interacts with the world through textual natural language. Goals in this environment take the form of character-based quests, consisting of personas and motivations. We augment LIGHT by learning to procedurally generate additional novel textual worlds and quests to create a curriculum of steadily increasing difficulty for training agents to achieve such goals. In particular, we measure curriculum difficulty in terms of the rarity of the quest in the original training distribution -- an easier environment is one that is more likely to have been found in the unaugmented dataset. An ablation study shows that this method of learning from the tail of a distribution results in significantly higher generalization abilities as measured by zero-shot performance on never-before-seen quests. ",Situated Dialogue Learning through Procedural Environment Generation
129,1446392438654898178,1139943231922331652,Chandreyee Maitra,['If you have finished submitting @ESA_XMM proposals please take a look at this paper of discovery of new super-soft sources in the LMC using archival XMM-Newton data. There is still so much treasure waiting to be discovered in archival observations👇\n<LINK>'],https://arxiv.org/abs/2110.02165,"Super-soft X-ray sources were established as a heterogeneous class of objects from observations of the Large Magellanic Cloud (LMC). We have searched for new sources of this class in the X-ray images obtained from the XMM-Newton survey of the LMC and additional archival observations. We first selected candidates by visual inspection of the image, and screened out artefacts which can mimic super-soft X-ray sources as well as bright foreground stars which create optical loading on the CCD image. We finally obtained 4 new super-soft X-ray sources for which we performed detailed X-ray timing and spectral analysis and searched for possible optical counterparts to identify their nature. XMMU J050452.0-683909 is identified as the central star of the planetary nebula SMP LMC21 in the LMC. We suggest XMMU J051854.8-695601 and XMMU J050815.1-691832 as new soft intermediate polars from the nature of their X-ray spectrum. Their estimated absorption-corrected luminosities and the blackbody radii indicate that they are located in our Galaxy, rather than the LMC. We discovered coherent pulsations of 497 s from XMMU J044626.6-692011 which indicates a magnetic cataclysmic variable nature of the source. The location of XMMU J044626.6-692011 in the LMC or our Galaxy is less clear. It could either be a white dwarf in the LMC with nuclear burning on its surface near the Eddington limit, or another soft intermediate polar in our Galaxy. The discovery of new super-soft X-ray sources makes a significant contribution to the known population in our own Galaxy. An observed higher density of sources in the direction of the Magellanic Clouds can likely be explained by the relatively low Galactic column density in their direction as well as a large number of existing observations sensitive at low X-ray energies. ","Discovery of four super-soft X-ray sources in XMM-Newton observations of
  the Large Magellanic Cloud"
130,1446280897850843138,1146409804610584577,Alexander Jahn,"['Can tensor networks on regular hyperbolic geometries compute actual CFTs? In our new paper, we show that they can! While being useful toy models for holography. \n\nThanks a lot to @jenseisert and friends! <LINK>']",https://arxiv.org/abs/2110.02972,"Key aspects of the AdS/CFT correspondence can be captured in terms of tensor network models on hyperbolic lattices. For tensors fulfilling the matchgate constraint, these have previously been shown to produce disordered boundary states whose site-averaged ground state properties match the translation-invariant critical Ising model. In this work, we substantially sharpen this relationship by deriving disordered local Hamiltonians generalizing the critical Ising model whose ground and low-energy excited states are accurately represented by the matchgate ansatz without any averaging. We show that these Hamiltonians exhibit multi-scale quasiperiodic symmetries captured by an analytical toy model based on layers of the hyperbolic lattice, breaking the conformal symmetries of the critical Ising model in a controlled manner. We provide a direct identification of correlation functions of ground and low-energy excited states between the disordered and translation-invariant models and give numerical evidence that the former approaches the latter in the large bond dimension limit. This establishes tensor networks on regular hyperbolic tilings as an effective tool for the study of conformal field theories. Furthermore, our numerical probes of the bulk parameters corresponding to boundary excited states constitute a first step towards a tensor network bulk-boundary dictionary between regular hyperbolic geometries and critical boundary states. ",Boundary theories of critical matchgate tensor networks
131,1446276490514026498,973404788,Bei Zhou,"['New paper! <LINK> with @ProfJohnBeacom. We study neutrino-induced dimuons, a phenomenon that has only been seen in accelerator neutrino experiments, as a new event class of neutrino telescopes like IceCube, IceCube-Gen2.  @uw_icecube  1/6 <LINK>', '@ProfJohnBeacom The dimuons are mainly from neutrino-nucleus deep-inelastic scatter and W-boson production. We develop a calculational framework and show that for 10 years, IceCube can detect ≃400 dimuons and IceCube-Gen2 can detect ≃1200!   2/6 https://t.co/gYuPDTRGqC', '@ProfJohnBeacom These dimuons have very important physics potentials, including probing high-energy QCD, enabling the first detection of W-boson production (a process that will be very important for high energy neutrinos but has never been identified), and new physics.   3/6 https://t.co/ybjf3usXlZ', '@ProfJohnBeacom More excitingly, we find 19 dimuon candidates from analyzing IceCube public data! We are not totally sure if they are dimuon signals yet due to the reasons detailed in the paper, but all aspects of them match our prediction.    4/6 https://t.co/XdZbGbl2Hz', '@ProfJohnBeacom @uw_icecube Whether they are real dimuons or some new background (or signal!), it’s important to understand them by IceCube collaboration.    5/6', '@ProfJohnBeacom @uw_icecube The continued success of neutrino physics &amp; astrophysics depends on developing new tools to get the most out of the data. Developing new event classes is an important part. Our theory and observation contributions help open a valuable new direction for neutrino telescopes.    6/6']",https://arxiv.org/abs/2110.02974,"Neutrino telescopes allow powerful probes of high-energy astrophysics and particle physics. Their power is increased when they can isolate different event classes, e.g., by flavor, though that is not the only possibility. Here we focus on a new event class for neutrino telescopes: dimuons, two energetic muons from one neutrino interaction. We make new theoretical and observational contributions. For the theoretical part, we calculate dimuon production cross sections and detection prospects via deep-inelastic scattering (DIS; where we greatly improve upon prior work) and $W$-boson production (WBP; where we present first results). We show that IceCube should have $\simeq 400$ dimuons ($\simeq 8$ from WBP) in its current data and that IceCube-Gen2, with a higher threshold but a larger exposure, could detect $\simeq 1200$ dimuons ($\simeq 30$ from WBP) in 10 years. These dimuons are almost all produced by atmospheric neutrinos. For the observational part, we perform a simple but conservative analysis of IceCube public data, finding 19 candidate dimuon events. Subsequent to our paper appearing, visual inspection of these events by the IceCube Collaboration reveals that they are not real dimuons, but instead arise from an internal reconstruction error that identifies some single muons crossing the dust layer as two separate muons. To help IceCube and the broader community with future dimuon searches, we include the updated full details of our analysis. Together, these theoretical and observational contributions help open a valuable new direction for neutrino telescopes, one especially important for probing high-energy QCD and new physics. ","Dimuons in Neutrino Telescopes: New Predictions and First Search in
  IceCube"
132,1446214415335018500,2956121356,Russ Salakhutdinov,['New work on the Information Geometry of Unsupervised Reinforcement Learning -- analyzing unsupervised skill discovery algorithms based on mutual information maximization:\n\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2110.02719,"How can a reinforcement learning (RL) agent prepare to solve downstream tasks if those tasks are not known a priori? One approach is unsupervised skill discovery, a class of algorithms that learn a set of policies without access to a reward function. Such algorithms bear a close resemblance to representation learning algorithms (e.g., contrastive learning) in supervised learning, in that both are pretraining algorithms that maximize some approximation to a mutual information objective. While prior work has shown that the set of skills learned by such methods can accelerate downstream RL tasks, prior work offers little analysis into whether these skill learning algorithms are optimal, or even what notion of optimality would be appropriate to apply to them. In this work, we show that unsupervised skill discovery algorithms based on mutual information maximization do not learn skills that are optimal for every possible reward function. However, we show that the distribution over skills provides an optimal initialization minimizing regret against adversarially-chosen reward functions, assuming a certain type of adaptation procedure. Our analysis also provides a geometric perspective on these skill learning methods. ",The Information Geometry of Unsupervised Reinforcement Learning
133,1446181795523792916,1123977306979041288,Sylvia Biscoveanu,"['Excited to finally share a new paper on the kilonova discovery potential of the Wide-field Infrared Transient Explorer (WINTER): <LINK>\nwith Danielle Frostig, @gmo_omg, Viraj Karambelkar, @sasomao and others @MITKavli, @PMACaltech, and @IJCLab \n\n1/4', ""We perform an end-to-end simulation of a realistic population of BNS mergers that could be observed during @LIGO-@ego_virgo-@KAGRA_PR's next observing run including low latency detection and localization, full parameter estimation, and EM followup \n\n2/4 https://t.co/Hd3LGOaHyI"", 'We find that particularly for red kilonovae, it will be more advantageous to follow them up in the infrared rather than optical due to the brighter and longer-lived emission. WINTER will discover up to 8 new kilonovae from BNS in O4! \n\n3/4', 'All the data from the simulations including the output of the PyCBC low-latency gravitational-wave pipeline, the Bayestar localizations, bilby parameter estimation, and a notebook for optimizing the EM followup strategy are available: https://t.co/Kgif64N6BH\n\n4/4', '@geodesicvoyager Thanks! Yeah the 8 vs 5 just depends on the assumed merger rate since we consider three options. You can see predictions for a wide range of rates and kilonova models in Table 2 🙂']",https://arxiv.org/abs/2110.01622,"The Wide-Field Infrared Transient Explorer (WINTER) is a new 1 $\text{deg}^2$ seeing-limited time-domain survey instrument designed for dedicated near-infrared follow-up of kilonovae from binary neutron star (BNS) and neutron star-black hole mergers. WINTER will observe in the near-infrared Y, J, and short-H bands (0.9-1.7 microns, to $\text{J}_{AB}=21$ magnitudes) on a dedicated 1-meter telescope at Palomar Observatory. To date, most prompt kilonova follow-up has been in optical wavelengths; however, near-infrared emission fades more slowly and depends less on geometry and viewing angle than optical emission. We present an end-to-end simulation of a follow-up campaign during the fourth observing run (O4) of the LIGO, Virgo, and KAGRA interferometers, including simulating 625 BNS mergers, their detection in gravitational waves, low-latency and full parameter estimation skymaps, and a suite of kilonova lightcurves from two different model grids. We predict up to five new kilonovae independently discovered by WINTER during O4, given a realistic BNS merger rate. Using a larger grid of kilonova parameters, we find that kilonova emission is $\approx$2 times longer-lived and red kilonovae are detected $\approx$1.5 times further in the infrared than in the optical. For 90% localization areas smaller than 150 (450) $\rm{deg}^{2}$, WINTER will be sensitive to more than 10% of the kilonova model grid out to 350 (200) Mpc. We develop a generalized toolkit to create an optimal BNS follow-up strategy with any electromagnetic telescope and present WINTER's observing strategy with this framework. This toolkit, all simulated gravitational-wave events, and skymaps are made available for use by the community. ","An Infrared Search for Kilonovae with the WINTER Telescope. I. Binary
  Neutron Star Mergers"
134,1446162932945985565,1203016782178443264,Jacob Krantz,"['How does the choice of action space affect language-guided embodied navigators? 🤖\n\nIn our new paper ""Waypoint Models for Instruction-guided Navigation in Continuous Environments"", we discover implications for simulation and reality. 🧵\n\nOral at #iccv2021!\n<LINK> <LINK>', 'We develop a class of highly configurable waypoint prediction networks to explore a spectrum of action spaces.\n\nWe vary the ""expressivity"" from coarse-grained heading prediction up to continuous-valued waypoint prediction and train each model with large-scale RL. https://t.co/1PlM2RhdXA', 'We find that more expressive waypoint models result in simpler trajectories that are faster to execute on a real robot (by 2-3x!), but lower-level actions can better approximate shortest paths.\n\nAlong the way, we set a new SotA on the VLN-CE leaderboard! https://t.co/cyOQF1GLxz', 'Project page: https://t.co/yBK76qERDw\n\nA big thanks to my collaborators Aaron Gokaslan (@SkyLi0n), Dhruv Batra (@DhruvBatraDB), Stefan Lee (@stefmlee), and Oleksandr Maksymets (@o_maksymets).']",https://arxiv.org/abs/2110.02207,"Little inquiry has explicitly addressed the role of action spaces in language-guided visual navigation -- either in terms of its effect on navigation success or the efficiency with which a robotic agent could execute the resulting trajectory. Building on the recently released VLN-CE setting for instruction following in continuous environments, we develop a class of language-conditioned waypoint prediction networks to examine this question. We vary the expressivity of these models to explore a spectrum between low-level actions and continuous waypoint prediction. We measure task performance and estimated execution time on a profiled LoCoBot robot. We find more expressive models result in simpler, faster to execute trajectories, but lower-level actions can achieve better navigation metrics by approximating shortest paths better. Further, our models outperform prior work in VLN-CE and set a new state-of-the-art on the public leaderboard -- increasing success rate by 4% with our best model on this challenging task. ","Waypoint Models for Instruction-guided Navigation in Continuous
  Environments"
135,1446142842045231111,410939902,Erik Meijer,"['New paper <LINK> from our Differentiable Programming team with lots of kudos to @xipengshen who was our honored guest during his sabbatical.\n\n(if you are also interested in a sabbatical or postdoc with us, please ping me)']",https://arxiv.org/abs/2110.02307,"This paper presents a novel optimization for differentiable programming named coarsening optimization. It offers a systematic way to synergize symbolic differentiation and algorithmic differentiation (AD). Through it, the granularity of the computations differentiated by each step in AD can become much larger than a single operation, and hence lead to much reduced runtime computations and data allocations in AD. To circumvent the difficulties that control flow creates to symbolic differentiation in coarsening, this work introduces phi-calculus, a novel method to allow symbolic reasoning and differentiation of computations that involve branches and loops. It further avoids ""expression swell"" in symbolic differentiation and balance reuse and coarsening through the design of reuse-centric segment of interest identification. Experiments on a collection of real-world applications show that coarsening optimization is effective in speeding up AD, producing several times to two orders of magnitude speedups. ",Coarsening Optimization for Differentiable Programming
136,1446136483589537796,349321075,Oncel Tuzel,['Style equalization is a new paper from our research team @Apple to train controllable generative sequence models that can mimic speech or handwriting styles using a single reference example.\n\nPaper: <LINK>\nSynthesis videos: <LINK>\nR. Chang et al. <LINK>'],https://arxiv.org/abs/2110.02891,"Controllable generative sequence models with the capability to extract and replicate the style of specific examples enable many applications, including narrating audiobooks in different voices, auto-completing and auto-correcting written handwriting, and generating missing training samples for downstream recognition tasks. However, typical training algorithms for these controllable sequence generative models suffer from the training-inference mismatch, where the same sample is used as content and style input during training but different samples are given during inference. In this paper, we tackle the training-inference mismatch encountered during unsupervised learning of controllable generative sequence models. By introducing a style transformation module that we call style equalization, we enable training using different content and style samples and thereby mitigate the training-inference mismatch. To demonstrate its generality, we applied style equalization to text-to-speech and text-to-handwriting synthesis on three datasets. Our models achieve state-of-the-art style replication with a similar mean style opinion score as the real data. Moreover, the proposed method enables style interpolation between sequences and generates novel styles. ","Style Equalization: Unsupervised Learning of Controllable Generative
  Sequence Models"
137,1446120461968961545,28840722,Phil Long,"['New paper with @niladrichat called ""Foolish Crowds Support Benign Overfitting"": <LINK>.']",https://arxiv.org/abs/2110.02914,"We prove a lower bound on the excess risk of sparse interpolating procedures for linear regression with Gaussian data in the overparameterized regime. We apply this result to obtain a lower bound for basis pursuit (the minimum $\ell_1$-norm interpolant) that implies that its excess risk can converge at an exponentially slower rate than OLS (the minimum $\ell_2$-norm interpolant), even when the ground truth is sparse. Our analysis exposes the benefit of an effect analogous to the ""wisdom of the crowd"", except here the harm arising from fitting the $\textit{noise}$ is ameliorated by spreading it among many directions -- the variance reduction arises from a $\textit{foolish}$ crowd. ",Foolish Crowds Support Benign Overfitting
138,1446096959933145090,775785554889863169,Shir Gur,['Stop training single image models and go META!\n\nOur new paper “Meta internal learning” was accepted to @NeurIPSConf 2021. With Raphael Bensadoun @GalantiTomer and @liorwolf\n\nabs: <LINK>\ncode: <LINK> <LINK>'],https://arxiv.org/abs/2110.02900,"Internal learning for single-image generation is a framework, where a generator is trained to produce novel images based on a single image. Since these models are trained on a single image, they are limited in their scale and application. To overcome these issues, we propose a meta-learning approach that enables training over a collection of images, in order to model the internal statistics of the sample image more effectively. In the presented meta-learning approach, a single-image GAN model is generated given an input image, via a convolutional feedforward hypernetwork $f$. This network is trained over a dataset of images, allowing for feature sharing among different models, and for interpolation in the space of generative models. The generated single-image model contains a hierarchy of multiple generators and discriminators. It is therefore required to train the meta-learner in an adversarial manner, which requires careful design choices that we justify by a theoretical analysis. Our results show that the models obtained are as suitable as single-image GANs for many common image applications, significantly reduce the training time per image without loss in performance, and introduce novel capabilities, such as interpolation and feedforward modeling of novel images. ",Meta Internal Learning
139,1446041079652945922,1350544263071883267,Julia Harz,"['If you are curious about what we can learn from potential photon emission from coherent elastic neutrino nucleus scattering (CEvNS), check out our new paper: <LINK> @ChandanHati6 @suchi_kulkarni <LINK>']",https://arxiv.org/abs/2110.02233,"In the presence of transition magnetic moments between active and sterile neutrinos, the search for a Primakoff upscattering process at Coherent Elastic Neutrino Nucleus Scattering (CE$\nu$NS) experiments provide stringent constraints on the neutrino magnetic moment. We show that a radiative upscattering process with a photon emitted in the final state can provide a novel experimental mode to probe neutrino transition magnetic moments beyond existing limits. Furthermore, the differential distributions for such a radiative mode can also potentially be sensitive to the Dirac vs. Majorana nature of the sterile state mediating the process. This can provide valuable insights into the nature and mass generation mechanism of the light active neutrinos. ","Probing Active-Sterile Neutrino Transition Magnetic Moments with Photon
  Emission from CE$\nu$NS"
140,1446036933805809664,2999702157,Anton Ilderton,"[""Our new paper is out today on the #arXiv. It has #lasers! It has #particle #beams! It has sharks! Alright it doesn't have sharks.\n<LINK>\n\n@HiggsCentre @PhysAstroEd @EdinburghUni \n\nWith colleagues from @EdinUniMaths and @plym_math! <LINK>"", '@plym_math @HiggsCentre @PhysAstroEd @EdinburghUni @EdinUniMaths One day. One beautiful day....']",https://arxiv.org/abs/2110.02567,"We consider the scattering of probe particles on an ultra-boosted beam of charge, in the case that the fields of the beam are strong and must be treated non-perturbatively. We show that the fields of the ultra-boosted beam act as stochastic plane waves - scattering amplitudes (of elastic scattering, nonlinear Compton and nonlinear Breit-Wheeler) are obtained without approximation by averaging plane wave scattering amplitudes over all possible plane wave parameters. The relevant plane waves are ultra-short and, as such, scattering on ultra-boosted beams does not exhibit the conjectured strong-field behaviour of QED based on the locally constant field approximation. ",Particle-beam scattering from strong-field QED
141,1445938330219208704,2956121356,Russ Salakhutdinov,['New work on joint optimization of the model and the policy for model-based RL.\n\nPaper: <LINK>\n\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2110.02758,"Many model-based reinforcement learning (RL) methods follow a similar template: fit a model to previously observed data, and then use data from that model for RL or planning. However, models that achieve better training performance (e.g., lower MSE) are not necessarily better for control: an RL agent may seek out the small fraction of states where an accurate model makes mistakes, or it might act in ways that do not expose the errors of an inaccurate model. As noted in prior work, there is an objective mismatch: models are useful if they yield good policies, but they are trained to maximize their accuracy, rather than the performance of the policies that result from them. In this work, we propose a single objective for jointly training the model and the policy, such that updates to either component increases a lower bound on expected return. This joint optimization mends the objective mismatch in prior work. Our objective is a global lower bound on expected return, and this bound becomes tight under certain assumptions. The resulting algorithm (MnM) is conceptually similar to a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policy is updated to avoid states where the model predictions are unrealistic. ",Mismatched No More: Joint Model-Policy Optimization for Model-Based RL
142,1445921088798560259,1019417352784490503,Mehmet F. Demirel,"['📢 NEW PAPER! We propose a novel attentive GNN called AWARE that uses walk-aggregation (rather than standard K-hop). Our theoretical analysis presents the first provable guarantees of weighted GNNs! Empirical performance on 65 tasks is strong as well!\n<LINK>', 'AWARE is an end-to-end supervised GNN that aggregates info about the walks in the graph using attention schemes on three levels: vertex, walk, and graph. This enables it to emphasize useful info for the downstream task while diminishing harmful/irrelevant ones. https://t.co/CaxGzw5C7C', 'Out of 65 tasks from both molecular property prediction and social network domains, AWARE performs the best in 33, and ranks in top-3 in 53, overall giving better performance than both traditional graph methods and recent GNN variants. https://t.co/dvs5HtqNE2', ""It is also an interpretable algorithm. We show in the paper that the AWARE's walk attention mechanism is able to highlight important substructures in the graph that are important for the downstream prediction task. https://t.co/Wt9SZeyiRy"", 'The code can be found at https://t.co/3Lr5pGhd2f!']",https://arxiv.org/abs/2110.02667,"Graph neural networks (GNNs) have been shown to possess strong representation power, which can be exploited for downstream prediction tasks on graph-structured data, such as molecules and social networks. They typically learn representations by aggregating information from the K-hop neighborhood of individual vertices or from the enumerated walks in the graph. Prior studies have demonstrated the effectiveness of incorporating weighting schemes into GNNs; however, this has been primarily limited to K-hop neighborhood GNNs so far. In this paper, we aim to extensively analyze the effect of incorporating weighting schemes into walk-aggregating GNNs. Towards this objective, we propose a novel GNN model, called AWARE, that aggregates information about the walks in the graph using attention schemes in a principled way to obtain an end-to-end supervised learning method for graph-level prediction tasks. We perform theoretical, empirical, and interpretability analyses of AWARE. Our theoretical analysis provides the first provable guarantees for weighted GNNs, demonstrating how the graph information is encoded in the representation, and how the weighting schemes in AWARE affect the representation and learning performance. We empirically demonstrate the superiority of AWARE over prior baselines in the domains of molecular property prediction (61 tasks) and social networks (4 tasks). Our interpretation study illustrates that AWARE can successfully learn to capture the important substructures of the input graph. ",An Analysis of Attentive Walk-Aggregating Graph Neural Networks
143,1445915069318107140,3883277674,Barak Shoshany ⚡,"['New paper out: ""Wormhole Time Machines and Multiple Histories"", with my student Jared Wogan.\n\nIt contains a detailed NON-TECHNICAL introduction, at a level suitable for the general public. I plan to do that in all my papers from now on. Check it out!\n\n<LINK>', '@positivearrow I actually posted about that a while ago: https://t.co/nUxRA9SNcf', ""@Maxkhalifa96 @StarshipBuilder Thanks! FTL travel can be exploited for time travel, but doesn't automatically lead to it. My conjecture is that new histories are created if and only if time travel occurs.""]",https://arxiv.org/abs/2110.02448,"In a previous paper, we analyzed a class of time travel paradoxes which cannot be resolved using Novikov's self-consistency conjecture, meaning that the system is fundamentally and irreparably inconsistent. We proved that the paradoxes can nonetheless always be resolved, and the system made consistent, by assuming that traveling back in time creates a new independent history (or timeline), such that any changes to the past affect only the new history and not the original one. Therefore, we argued that if time travel is possible, that would necessarily imply the existence of multiple histories. However, our proof was obtained using a simplistic and unrealistic toy model, which was formulated using contrived laws of physics. The purpose of the present paper is to define and analyze a new model of time travel paradoxes, which is fully compatible with all known physics - provided, of course, that time travel itself is possible. This model consists of a wormhole time machine in 3+1 spacetime dimensions, which can be either permanent (existing eternally) or temporary (activated only for a short time). We define the spacetime topology and geometry of the model, calculate the geodesics of objects passing through the time machine, and prove that this model inevitably leads to paradoxes which cannot be resolved using Novikov's conjecture, but can be resolved using multiple histories. This result provides more substantial support to our claim that time travel necessarily implies multiple histories. ",Wormhole Time Machines and Multiple Histories
144,1445794053086347276,1138462876677468161,Sachin,"['New paper! We introduce MobileViT, a light-weight vision transformer. MobileViT delivers better performance across different tasks, shows better generalization capabilities, and are robust to hyper-parameters (e.g., basic augmentation). (1/5)\n\nPaper: <LINK> <LINK>', '(2/5) Transformers in the clothing of convolutions; allowing to reap the benefits of convolutions (e.g., spatial bias) and transformers (e.g., global processing)\n\nStandard Convolution: Unfolding + Matrix multiplication + Folding\nMobileViT: Unfolding + Transformers + Folding https://t.co/jxx45C7ECM', 'We compare with state-of-the-art CNN- and ViT-based methods. For a similar parameter budget, MobileViT delivers significantly better performance on the ImageNet dataset, that too with basic data augmentation (random resize and horizontal flipping). https://t.co/cxQf6d6tKi', '(4/5) We also introduce multi-scale sampler to improve training efficiency. With our multi-scale sampler, MobileViT requires 1.6 times fewer updates as compared to PyTorch’s DDP. https://t.co/wfj8Ed7Rty', '(5/5) With our sampler, ResNet-50 achieves 78.6 on ImageNet (150 epochs; SGD; basic augmentation; 1e-4 weight decay), which is similar to ResNet strike back, but with simpler training recipe (78.1; 100 epochs; LAMB; adv augmentation; 0.01 weight decay)\nhttps://t.co/TCU9TtvgFl https://t.co/UedCraWzoj', 'Work done with @morastegari']",https://arxiv.org/abs/2110.02178,"Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: this https URL ","MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision
  Transformer"
145,1445664635898654722,1562913787,Nathan Moynihan,"[""I've got a new paper out today, and you should probably read it. <LINK>""]",https://arxiv.org/abs/2110.02209,"We explore topologically massive gauge theories using the covariant colour kinematics duality recently introduced by Cheung and Mangan. We show that the massive bi-adjoint scalar field is simply related to topologically massive gauge theory by the duality, and that enacting the same duality on the gauge theory produces topologically massive gravity coupled to a scalar or, equivalently, an antisymmetric field. We also show that different choices for the replacement of the colour structure constants with kinematic structure constants lead to different theories, including a topologically massive generalisation of Born-Infeld theory. ",Massive Covariant Colour-Kinematics in 3D
146,1445428818911633409,717093911487979520,Harrison Ritz,"[""Excited to share a preprint of our new theory paper\n'Cognitive control as a multivariate optimization problem'\n\n<LINK>\n\nCognitive control has an inverse problem, and motor control might have the solution\n\n🧵⤵️ <LINK>"", 'Cognitive control is really complex.\n\nWhen people face errors, conflict, and reward, they appear to deploy many overlapping strategies. https://t.co/CHKRNxMKvy', 'People often appear to *simultaneously* use multiple strategies, such as in post-error adjustments to threshold and interference. https://t.co/hUoBt8wzUe', 'In theories like EVC, this complexity has been addressed through a decision-making lens, proposing that people optimize the costs and benefits of different control configurations.', 'In recent work, @JasonLeng5 @debyee29 and I found that when people are faced with rewards and punishment, they jointly configure multiple control signals in agreement with the optimal strategy https://t.co/brIKy1KbM6', 'Unfortunately, this optimization is *really* hard. \n\nThis process needs to solve an Inverse Problem, mapping from outcomes (e.g., goals) to actions (e.g., control signals). \n\nInverse problems are hard to solve in general, and are likely ill-posed for complex neural systems. https://t.co/CCsaQ0ByaP', 'Thankfully, there are precedents for this problem.\n\nFor 100 years, motor control has struggled with the same problem, exploring how to solve degeneracy in motor actions. https://t.co/L19AOZNUj7', 'A core solution to this problem in the motor domain has been that motor effort costs regularize this inverse problem, allowing for well-posed control optimization (cf. Michael Jordan, Mitsuo Kawato)', 'Like motor control, cognitive control is degenerate. \nLike motor control, cognitive control is effortful.\n\nMaybe mental effort costs are needed to make decisions about cognitive control. https://t.co/UwSN4Xialk', 'This regularization can also be cast in Bayesian terms as shrinkage towards a prior.\n\nIf these priors are control defaults (e.g., learned control policies or habits), then this can provide a natural explanation for the relationship between effort, automaticity, and learning.', 'Cognitive control may also be able to benefit from the algorithms used for inverse problems in motor control. https://t.co/bU1wwREjB0', 'The Linear Quadratic Regulator is a standard model in the motor domain, offering analytic solutions (!) to optimal control.  \n\nWe review several interesting parallels in the cognitive domain (cf. excellent work from @DaniSBassett). https://t.co/6bsNbZ5IlY', 'A regularization perspective on effort also aligns with several awesome recent theories (e.g., @wouterkool @payampiray @ZenonAlexandre). Here, we emphasize the degeneracy of control specification and links to motor control.', 'While regularization is likely not the *only* reason for effort, any degenerate control system would want to regularize.\n\nThis perspective also offers a different perspective on constraints, where they come from the power and flexibility of the brain, rather than its limitations.', 'Hopefully these ideas can help guide new tests and extensions of how we optimize multivariate cognitive control! \n\nI think motor control (and optimal control theory) can offer a much-needed grounding for our models of latent control processes.', 'Thanks so much to my amazing co-authors @JasonLeng5 and @amitaishenhav 🤠🤠🤠\n\nThanks also to my comps committee @LnccBrown and @BadreLab 🧠🧠']",https://arxiv.org/abs/2110.00668,"Research has characterized the various forms cognitive control can take, including enhancement of goal-relevant information, suppression of goal-irrelevant information, and overall inhibition of potential responses, and has identified computations and neural circuits that underpin this multitude of control types. Studies have also identified a wide range of situations that elicit adjustments in control allocation (e.g., those eliciting signals indicating an error or increased processing conflict), but the rules governing when a given situation will give rise to a given control adjustment remain poorly understood. Significant progress has recently been made on this front by casting the allocation of control as a decision-making problem, and developing unifying and normative models that prescribe when and how a change in incentives and task demands will result in changes in a given form of control. Despite their successes, these models, and the experiments that have been developed to test them, have yet to face their greatest challenge: deciding how to allocate control across the multiplicity of control signals that one could engage at any given time. Here, we will lay out the complexities of the inverse problem inherent to cognitive control allocation, and their close parallels to inverse problems within motor control (e.g., choosing between redundant limb movements). We discuss existing solutions to motor control`s inverse problems drawn from optimal control theory, which have proposed that effort costs act to regularize actions and transform motor planning into a well-posed problem. These same principles may help shed light on how our brains optimize over complex control configuration, while providing a new normative perspective on the origins of mental effort. ",Cognitive control as a multivariate optimization problem
147,1445363547039428612,1223527910503403521,Diptimoy Ghosh,['Our new paper <LINK>\nWe show that Super-Kamiokande provides the strongest constraint on Dark Matter - Neutrino cross-section for DM masses below a few MeV when we utilize the boost of DM particles due to scattering with the diffuse supernova neutrino background.'],https://arxiv.org/abs/2110.00025,"We derive new constraints on combination of dark matter - electron cross-section ($\sigma_{\chi e}$) and dark matter - neutrino cross-section ($\sigma_{\chi \nu}$) utilising the gain in kinetic energy of the dark matter (DM) particles due to scattering with the cosmic ray electrons and the diffuse supernova neutrino background (DSNB). Since the flux of the DSNB neutrinos is comparable to the CR electron flux in the energy range $\sim 1\,{\rm MeV} - 50 \,{\rm MeV}$, scattering with the DSNB neutrinos can also boost low-mass DM significantly in addition to the boost due to interaction with the cosmic ray electrons. We use the XENON1T as well as the Super-Kamiokande data to derive bounds on $\sigma_{\chi e}$ and $\sigma_{\chi \nu}$. While our bounds for $\sigma_{\chi e}$ are comparable with those in the literature, we show that the Super-Kamiokande experiment provides the strongest constraint on $\sigma_{\chi \nu}$ for DM masses below a few MeV. ",Exclusion limits on Dark Matter-Neutrino Scattering Cross-section
148,1445287046390730755,1352309163779641344,Javier Rivera-Dean,['New paper out! Quantum optics of strongly laser-driven atoms and generation of high photon number optical cat states\n\nLink: <LINK>\nZenodo: (in progress...)'],http://arxiv.org/abs/2110.01032,"Recently we have demonstrated the quantum nature of light in strongly laser driven atoms, and we have shown how the process of high harmonic generation can be used for the creation of highly non-classical light states, in particular superpositions of two coherent states, i.e., optical Schr\""{o}dinger ""cat"" states [M. Lewenstein et al., Generation of optical Schr\""odinger cat states in intense laser-matter interactions, Nat. Phys., 17 1104-1108 (2021)]. Here, we investigate the quantum optical description of the interaction, incorporating many atoms and the back-action of the high harmonic generation or above threshold ionization processes on the coherent state of the driving laser field. We show how the conditioning on high harmonic generation and above threshold ionization can lead to non-classical light states, and we discuss the key parameters that can be used for controlling and characterizing the non-classical states obtained after high harmonic generation. The theoretical results have been experimentally confirmed by demonstrating how the obtained coherent state superposition changes from an optical ""cat"" to ""kitten"" state by changing the number of atoms participating in the high harmonic generation process. Additionally, we experimentally verify the applicability of the approach for generating high photon number non-classical light states. This is shown by demonstrating the generation of a 9--photon optical ""cat"" state which, to our knowledge, is the highest photon number optical ""cat"" state experimentally reported. Our findings anticipate the development of new methods that naturally lead to the creation of high photon number controllable coherent state superpositions, advancing investigations in quantum technology. ","Quantum optics of strongly laser--driven atoms and generation of high
  photon number optical cat states"
149,1445200114482651140,321794593,José G. Fernández-Trincado,"['Our new accepted paper today on ArXiv 👉🏻 Nice MUSE data of FSR 1776 led by Bruno Días (@AstroBDias) 👉🏻 FSR 1776: a new globular cluster in the Galactic bulge? <LINK>', 'https://t.co/SXiIASx79E']",https://arxiv.org/abs/2110.00868,"(ABRIDGED) Recent near-IR surveys have uncovered a plethora of new globular cluster (GC) candidates towards the Milky Way bulge. These new candidates need to be confirmed as real GCs and properly characterised. We investigate the physical nature of FSR 1776. This object was originally classified as an intermediate-age open cluster and has recently been re-discovered independently and classified as a GC candidate (Minni 23). Firstly, we aim at confirming its GC nature; secondly we determine its physical parameters. The confirmation of the cluster existence is checked using the radial velocity (RV) distribution of a MUSE data cube centred at FSR 1776. The cluster parameters are derived from isochrone fitting to the RV-cleaned colour-magnitude diagrams (CMDs) from visible and near-infrared photometry. The predicted RV distribution for the FSR 1776 coordinates, considering only contributions from the bulge and disc field stars, is not enough to explain the observed MUSE RV distribution. The extra population (12\% of the sample) is FSR 1776 with an average RV of $-103.7\pm 0.4~{\rm km}\,{\rm s}^{-1}$. The CMDs reveal that it is 10$\pm$1~Gyr old and metal-rich, with [Fe/H]$_{phot}\approx + 0.2\pm$0.2, [Fe/H]$_{spec}=~+0.02\pm0.01~(\sigma~=~0.14$~dex), located at the bulge distance of 7.24$\pm$0.5~kpc with A$_{\rm V}$ $\approx$ 1.1~mag. The mean cluster proper motions are ($\langle\mu_{\alpha}\rangle,\langle\mu_{\delta}\rangle$) $=$ ($-2.3\pm1.1,-2.6\pm0.8$) ${\rm mas\, yr^{-1}}$.} FSR 1776 is an old GC located in the Galactic bulge with a super-solar metallicity, among the highest for a Galactic GC. This is consistent with predictions for the age-metallicity relation of the bulge, being FSR 1776 the probable missing link between typical GCs and the metal-rich bulge field. High-resolution spectroscopy of a larger field of view and deeper CMDs are now required for a full characterisation. ",FSR 1776: a new globular cluster in the Galactic bulge?
150,1445064348116783106,2594977088,Alban Sauret,"['Whereas we often rely on monodisperse suspensions in the lab (much simpler!), many industrial applications involves a range of particle size. We characterized how the polydispersity affect the dip coating process in our new paper  <LINK> @UCSBengineering @NSF <LINK>', 'Thanks to Deok-Hoon Jeong @dh_racoon93 for leading this work with Michael Ka Ho Lee, Virgile Thievenaz @vthieven and Martin Bazant !']",https://arxiv.org/abs/2110.00052,"Dip-coating consists in withdrawing a substrate from a bath to coat it with a thin liquid layer. This process is well-understood for homogeneous fluids, but heterogeneities such as particles dispersed in the liquid lead to more complex situations. Indeed, particles introduce a new length scale, their size, in addition to the thickness of the coating film. Recent studies have shown that at first order, the thickness of the coating film for monodisperse particles can be captured by an effective capillary number based on the viscosity of the suspension, providing that the film is thicker than the particle diameter. However, suspensions involved in most practical applications are polydisperse, characterized by a wide range of particle sizes, introducing additional length scales. In this study, we investigate the dip coating of suspensions having a bimodal size distribution of particles. We show that the effective viscosity approach is still valid in the regime where the coating film is thicker than the diameter of the largest particles, although bidisperse suspensions are less viscous than monodisperse suspensions of the same solid fraction. We also characterize the intermediate regime that consists of a heterogeneous coating layer and where the composition of the film is different from the composition of the bath. A model to predict the probability of entraining the particles in the liquid film depending on their sizes is proposed and captures our measurements. In this regime, corresponding to a specific range of withdrawal velocities, capillarity filters the large particles out of the film. ",Dip-coating of bidisperse particulate suspensions
151,1445055521904549890,875537970425720833,David Schuster,"['Excited to share our new open-source quantum instrumentation control kit (QICK), developed with @Fermilab, with @physicsfpgaeda, and @nanonoodle (and others without twitter accounts). The paper is at <LINK> and the firmware/source is at <LINK> <LINK>', 'It is based on the @XilinxInc RFSoC platform (currently the ZCU111 eval board) with 8 dacs @ 6.5GS/s and 8 adcs @ 4GS/s. There is an optional daughter board that includes microwave synthesizers, the entire warm RF chain, and precision dacs for flux/gate biasing.', 'Even with just the eval board and some inexpensive analog components, one can fully control a superconducting qubit experiment including direct synthesis of qubit control pulses, single-shot readout, and real-time feedback. Control is through python with no PC required. https://t.co/GRGTI62M5k', ""We'd love to build community around this platform and other open quantum hardware efforts.  So get in touch if you'd like to try this out or contribute."", ""@gkasprow Excited to talk, let's arrange via email.""]",https://arxiv.org/abs/2110.00557,"We introduce a Xilinx RFSoC-based qubit controller (called the Quantum Instrumentation Control Kit, or QICK for short) which supports the direct synthesis of control pulses with carrier frequencies of up to 6 GHz. The QICK can control multiple qubits or other quantum devices. The QICK consists of a digital board hosting an RFSoC (RF System-on-Chip) FPGA \cite{zcu111}, custom firmware and software and an optional companion custom-designed analog front-end board. We characterize the analog performance of the system, as well as its digital latency, important for quantum error correction and feedback protocols. We benchmark the controller by performing standard characterizations of a transmon qubit. We achieve an average Clifford gate fidelity of $\mathcal{F}_{avg}=99.93\%$. All of the schematics, firmware, and software are open-source \cite{QICKrepo}. ","The QICK (Quantum Instrumentation Control Kit): Readout and control for
  qubits and detectors"
152,1445007474327793665,732494566545203201,David Klindt,"['New paper on score-based generative classifiers (SBGCs) <LINK>\n\nDiffusion models have produced impressive results <LINK>\n\nWe show how they can be used as classifiers on CIFAR-10.\n\nWork w/ @zimmerrol @schott_lukas @YsongStanford @adric_dunn\n\n(1/6)', 'While previous methods have shown a trade-off between generative and classification performance, our SBGC model achieves new state-of-the-art performances both in likelihoods and classification accuracy for generative classifiers on CIFAR-10.\n\n(2/6) https://t.co/iRl9XCTRAK', 'In the past, generative classifiers (analysis-by-synthesis) have been shown to increase adversarial robustness on MNIST https://t.co/OSMjtHjHX1\n\nHowever, so far these results have not been extended to complex natural image datasets such as CIFAR-10.\n\n(3/6)', 'Previous work showed that interpolating between images increases likelihoods, suggesting model failure on out-of-distribution data https://t.co/WBwvfWUSsK @jh_jacobsen\n\nBy contrast, our SBGC model correctly produces convex interpolation curves.\n\n(4/6) https://t.co/KWWUDkRBSI', 'Nevertheless, we find that our model spectacularly fails against gradient-based adversarial attacks.\n\nWe argue that SBGCs have no structural advantage over discriminative classifiers and that analysis-by-synthesis alone is not sufficient for out-of-distribution robustness.\n\n(5/6) https://t.co/TCZTbPiuIy', 'Still, our work shows that SBGCs can achieve very competitive likelihoods and classification accuracies which encourage further research!\n\nThanks for fun discussions and feedback @poolio @wgrathwohl @yash_j_sharma @wielandbr @dylanpaiton @eero_simoncelli\n\n(6/6)']",http://arxiv.org/abs/2110.00473,"The tremendous success of generative models in recent years raises the question whether they can also be used to perform classification. Generative models have been used as adversarially robust classifiers on simple datasets such as MNIST, but this robustness has not been observed on more complex datasets like CIFAR-10. Additionally, on natural image datasets, previous results have suggested a trade-off between the likelihood of the data and classification accuracy. In this work, we investigate score-based generative models as classifiers for natural images. We show that these models not only obtain competitive likelihood values but simultaneously achieve state-of-the-art classification accuracy for generative classifiers on CIFAR-10. Nevertheless, we find that these models are only slightly, if at all, more robust than discriminative baseline models on out-of-distribution tasks based on common image corruptions. Similarly and contrary to prior results, we find that score-based are prone to worst-case distribution shifts in the form of adversarial perturbations. Our work highlights that score-based generative models are closing the gap in classification accuracy compared to standard discriminative models. While they do not yet deliver on the promise of adversarial and out-of-domain robustness, they provide a different approach to classification that warrants further research. ",Score-Based Generative Classifiers
153,1444930365748129795,2778729792,Saquib Sarfraz,['Checkout our latest work accepted at #NeurIPS2021 on leveraging model uncertainty for adapting object detectors to new domains. Practical use in #AutonomousDriving. A very fruitful collaboration with #ITU #MBZUAI and #DaimlerTSS\npaper: <LINK> <LINK> <LINK>'],https://arxiv.org/abs/2110.00249,"We study adapting trained object detectors to unseen domains manifesting significant variations of object appearance, viewpoints and backgrounds. Most current methods align domains by either using image or instance-level feature alignment in an adversarial fashion. This often suffers due to the presence of unwanted background and as such lacks class-specific alignment. A common remedy to promote class-level alignment is to use high confidence predictions on the unlabelled domain as pseudo labels. These high confidence predictions are often fallacious since the model is poorly calibrated under domain shift. In this paper, we propose to leverage model predictive uncertainty to strike the right balance between adversarial feature alignment and class-level alignment. Specifically, we measure predictive uncertainty on class assignments and the bounding box predictions. Model predictions with low uncertainty are used to generate pseudo-labels for self-supervision, whereas the ones with higher uncertainty are used to generate tiles for an adversarial feature alignment stage. This synergy between tiling around the uncertain object regions and generating pseudo-labels from highly certain object regions allows us to capture both the image and instance level context during the model adaptation stage. We perform extensive experiments covering various domain shift scenarios. Our approach improves upon existing state-of-the-art methods with visible margins. ","Synergizing between Self-Training and Adversarial Learning for Domain
  Adaptive Object Detection"
154,1444870513398935557,1373003924005818369,BICEP/Keck,"['Alongside BK18, check out our new paper describing the performance of the BICEP3 instrument during its first three years of science observation:\n<LINK> <LINK>']",https://arxiv.org/abs/2110.00482,"We report on the design and performance of the BICEP3 instrument and its first three-year data set collected from 2016 to 2018. BICEP3 is a 52cm aperture, refracting telescope designed to observe the polarization of the cosmic microwave background (CMB) on degree angular scales at 95GHz. It started science observation at the South Pole in 2016 with 2400 antenna-coupled transition-edge sensor (TES) bolometers. The receiver first demonstrated new technologies such as large-diameter alumina optics, Zotefoam infrared filters, and flux-activated SQUIDs, allowing $\sim 10\times$ higher optical throughput compared to the Keck design. BICEP3 achieved instrument noise-equivalent temperatures of 9.2, 6.8 and 7.1$\mu\text{K}_{\text{CMB}}\sqrt{\text{s}}$ and reached Stokes $Q$ and $U$ map depths of 5.9, 4.4 and 4.4$\mu$K-arcmin in 2016, 2017 and 2018, respectively. The combined three-year data set achieved a polarization map depth of 2.8$\mu$K-arcmin over an effective area of 585 square degrees, which is the deepest CMB polarization map made to date at 95GHz. ","BICEP / Keck XV: The BICEP3 CMB Polarimeter and the First Three Year
  Data Set"
155,1459888302438367241,12309242,Onur Mutlu,"['""Uncovering In-DRAM #RowHammer Protection Mechanisms: A New Methodology, Custom RowHammer Patterns, and Implications"", MICRO\'21 talk, Hasan Hassan. \n\nTomorrow 6:30pm Zurich time. \n\nYoutube: <LINK>\n\nPaper: <LINK>\n\n@SAFARI_ETH_CMU @ETH_en @CSatETH', 'This work introduces a rigorous methodology for uncovering the operational principles &amp; details of RowHammer protection mechanisms employed in modern DDR4 DRAM chips. We show that one can use this methodology to circumvent existing protections &amp; cause many more RowHammer bitflips', 'Our methodology, U-TRR (Uncovering TRR) can help enable fundamentally-secure and robust solutions to RowHammer. \n\nCollaborative research with @kavehrazavi and @vvdveen.\n\nFull paper: https://t.co/94xVVr8ZLM \n\nFull Talk Slides (PDF): https://t.co/ImY6SPPF7r']",https://arxiv.org/abs/2110.10603,"The RowHammer vulnerability in DRAM is a critical threat to system security. To protect against RowHammer, vendors commit to security-through-obscurity: modern DRAM chips rely on undocumented, proprietary, on-die mitigations, commonly known as Target Row Refresh (TRR). At a high level, TRR detects and refreshes potential RowHammer-victim rows, but its exact implementations are not openly disclosed. Security guarantees of TRR mechanisms cannot be easily studied due to their proprietary nature. To assess the security guarantees of recent DRAM chips, we present Uncovering TRR (U-TRR), an experimental methodology to analyze in-DRAM TRR implementations. U-TRR is based on the new observation that data retention failures in DRAM enable a side channel that leaks information on how TRR refreshes potential victim rows. U-TRR allows us to (i) understand how logical DRAM rows are laid out physically in silicon; (ii) study undocumented on-die TRR mechanisms; and (iii) combine (i) and (ii) to evaluate the RowHammer security guarantees of modern DRAM chips. We show how U-TRR allows us to craft RowHammer access patterns that successfully circumvent the TRR mechanisms employed in 45 DRAM modules of the three major DRAM vendors. We find that the DRAM modules we analyze are vulnerable to RowHammer, having bit flips in up to 99.9% of all DRAM rows. ","Uncovering In-DRAM RowHammer Protection Mechanisms: A New Methodology,
  Custom RowHammer Patterns, and Implications"
156,1455275416269168641,16507835,Dr. Saiph Savage 🏛🧪,['My lab presented at the Digital Worker Inquiry event our new CSCW research on tools to quantifying invisible labor. \n\nIt was nice to connect with worker collectives who are also developing tools that use worker data to give power to workers\n👉See our paper:<LINK> <LINK>'],https://arxiv.org/abs/2110.00169,"Crowdsourcing markets provide workers with a centralized place to find paid work. What may not be obvious at first glance is that, in addition to the work they do for pay, crowd workers also have to shoulder a variety of unpaid invisible labor in these markets, which ultimately reduces workers' hourly wages. Invisible labor includes finding good tasks, messaging requesters, or managing payments. However, we currently know little about how much time crowd workers actually spend on invisible labor or how much it costs them economically. To ensure a fair and equitable future for crowd work, we need to be certain that workers are being paid fairly for all of the work they do. In this paper, we conduct a field study to quantify the invisible labor in crowd work. We build a plugin to record the amount of time that 100 workers on Amazon Mechanical Turk dedicate to invisible labor while completing 40,903 tasks. If we ignore the time workers spent on invisible labor, workers' median hourly wage was $3.76. But, we estimated that crowd workers in our study spent 33% of their time daily on invisible labor, dropping their median hourly wage to $2.83. We found that the invisible labor differentially impacts workers depending on their skill level and workers' demographics. The invisible labor category that took the most time and that was also the most common revolved around workers having to manage their payments. The second most time-consuming invisible labor category involved hyper-vigilance, where workers vigilantly watched over requesters' profiles for newly posted work or vigilantly searched for labor. We hope that through our paper, the invisible labor in crowdsourcing becomes more visible, and our results help to reveal the larger implications of the continuing invisibility of labor in crowdsourcing. ",Quantifying the Invisible Labor in Crowd Work
157,1453873978783666179,967825967673638913,Richard Feder,"['New paper 🚨🚨🚨 Using sub-millimeter/mm observations from Herschel-SPIRE/Bolocam, we’ve measured the cluster gas temperature in galaxy cluster RXJ1347 through relativistic corrections to the Sunyaev-Zel’dovich (SZ) effect! (thread ⬇️)\n<LINK> <LINK>', 'Galaxy clusters are some of the largest virialized objects in the universe. The one we study, RXJ1347, is among the most massive clusters known, at around 10^15 solar masses. A major area of study is in understanding the thermodynamic activity within these clusters..', 'While X-ray measurements are well developed for this, their sensitivity is poorer for gas at larger cluster radii or for higher redshift clusters. Also, the temperature of the gas can be hot enough that it resides outside the X-ray energy range of Chandra/XMM/etc..', 'Another way to study clusters is through the thermal SZ effect, which is sourced by CMB light that passes through the cluster and gets distorted by hot gas within the cluster. Electrons in this gas move fast enough that relativistic corrections are needed to model the SZ spectra https://t.co/fFqdq5LnxL', 'So, if you can measure this distortion precisely enough, you can back out the gas temperature.', 'Cluster SZ measurements offer a complementary window to studying gas thermodynamics, with a linear dependence on electron density (X-ray sensitivity typically goes as density squared) and a relatively uniform redshift selection function..', 'However, the sub-mm measurement does not come without its own challenges! While SPIRE has the raw sensitivity to the SZ effect signal, contamination from cosmic infrared background (CIB) galaxies and cirrus dust makes things difficult (see this mock realization for example) https://t.co/QE1JZ8vJgV', 'To account for these astrophysical contaminants, we extend the forward modeling framework of probabilistic cataloging (PCAT) to jointly infer the SZ effect signal along with a point source model for CIB and a non-parametric Fourier component model for the diffuse emission..', 'Then, after correcting for the effect of strong gravitational lensing, we use a detailed model of the SZ spectrum to constrain the gas temperature to be 22.4 +10.6/-12.0 keV. This is in good agreement with independent X-ray estimates of 17.4 +/- 2.3 keV. https://t.co/3lVoQdBGKD', 'While physicists use weird units, I want to convey how hot this gas actually is — 22.4 keV corresponds to roughly 500 million degrees Fahrenheit! This is ~20x hotter than the core of our Sun 🔥🔥🔥', 'While this work was largely a pathfinder analysis, we have a sample of up to 39 additional clusters waiting to be analyzed! I’m really excited to see what we learn about clusters and their environments.', 'many thanks to @mazerquazer who co-led this work, along with lots of help from @astrodude10 @TansuDaylan @Zembot @stevefreddale and other co-authors not on twitter', 'one more thing: we are working on publishing our software (PCAT-DE) in the coming months as a general purpose point source + diffuse signal fitter. Stay tuned!']",https://arxiv.org/abs/2110.13932,"We present a measurement of the relativistic corrections to the thermal Sunyaev-Zel'dovich (SZ) effect spectrum, the rSZ effect, toward the massive galaxy cluster RX J1347.5-1145 by combining sub-mm images from Herschel-SPIRE with mm-wave Bolocam maps. Our analysis simultaneously models the SZ effect signal, the population of cosmic infrared background (CIB) galaxies, and galactic cirrus dust emission in a manner that fully accounts for their spatial and frequency-dependent correlations. Gravitational lensing of background galaxies by RX J1347.5-1145 is included in our methodology based on a mass model derived from HST observations. Utilizing a set of realistic mock observations, we employ a forward modelling approach that accounts for the non-Gaussian covariances between observed astrophysical components to determine the posterior distribution of SZ effect brightness values consistent with the observed data. We determine a maximum a posteriori (MAP) value of the average Comptonization parameter of the intra-cluster medium (ICM) within R$_{2500}$ to be $\langle y \rangle_{2500} = 1.56 \times 10^{-4}$, with corresponding 68~per cent credible interval $[1.42,1.63] \times 10^{-4}$, and a MAP ICM electron temperature of $\langle \textrm{T}_{\textrm{sz}} \rangle_{2500} = 22.4$~keV with 68~per cent credible interval spanning $[10.4,33.0]$~keV. This is in good agreement with the pressure-weighted temperature obtained from {\it Chandra} X-ray observations, $\langle \textrm{T}_{\textrm{x,pw}}\rangle_{2500} = 17.4 \pm 2.3$~keV. We aim to apply this methodology to comparable existing data for a sample of 39 galaxy clusters, with an estimated uncertainty on the ensemble mean $\langle \textrm{T}_{\textrm{sz}} \rangle_{2500}$ at the $\simeq 1$~keV level, sufficiently precise to probe ICM physics and to inform X-ray temperature calibration. ","Measurement of the Relativistic Sunyaev-Zeldovich Corrections in RX
  J1347.5-1145"
158,1453704737220530181,2279967715,Michael Tremmel,"['Hey new simulation just dropped! Check out this paper led by PhD student Yueying Ni examining SMBHs in the new Astrid simulation, a large-volume (250 Mpc/h), high-res (333 billion particles!) cosmological simulation that implements improved SMBH dynamics!\n\n<LINK>', ""Yueying Ni is finishing her PhD soon and is on the job market. She is brilliant and was a leader in developing, running, and analyzing this simulation. I have no doubt that she'd be an asset in your department/group as a postdoc!"", 'For more details on the SMBH dynamics implemented in the simulation (obviously my favorite part) check out this paper led by another impressive PhD student, Nianyi Chen\n\nhttps://t.co/GcshWra0mE']",https://arxiv.org/abs/2110.14154,"We present the evolution of black holes (BHs) and their relationship with their host galaxies in Astrid, a large-volume cosmological hydrodynamical simulation with box size 250 $h^{-1} \rm Mpc$ containing $2\times5500^3$ particles evolved to z=3. Astrid statistically models BH gas accretion and AGN feedback to their environments, applies a power-law distribution for BH seed mass $M_{\rm sd}$, uses a dynamical friction model for BH dynamics and executes a physical treatment of BH mergers. The BH population is broadly consistent with empirical constraints on the BH mass function, the bright end of the luminosity functions, and the time evolution of BH mass and accretion rate density. The BH mass and accretion exhibit a tight correlation with host stellar mass and star formation rate. We trace BHs seeded before z>10 down to z=3, finding that BHs carry virtually no imprint of the initial $M_{\rm sd}$ except those with the smallest $M_{\rm sd}$, where less than 50\% of them have doubled in mass. Gas accretion is the dominant channel for BH growth compared to BH mergers. With dynamical friction, Astrid predicts a significant delay for BH mergers after the first encounter of a BH pair, with a typical elapse time of about 200 Myrs. There are in total $4.5 \times 10^5$ BH mergers in Astrid at z>3, $\sim 10^3$ of which have X-ray detectable EM counterparts: a bright kpc scale dual AGN with $L_X>10^{43}$ erg/s. BHs with $M_{\rm BH} \sim 10^{7-8} M_{\odot}$ experience the most frequent mergers. Galaxies that host BH mergers are unbiased tracers of the overall $M_{\rm BH} - M_{*}$ relation. Massive ($>10^{11} M_{\odot}$) galaxies have a high occupation number (>10) of BHs, and hence host the majority of BH mergers. ",The ASTRID simulation: the evolution of Supermassive Black Holes
159,1453554928534884352,716990588109852672,Yingtong Dou,"['New Paper Out!🔔\n\nI am glad to announce that our paper collaborated with @GrabSG has been accepted by the @ieeebigdata!\n\nPaper Link: <LINK>\n\n#researchpaper #artificialintelligence #graphmining #machinelearning #bigdata <LINK>', 'This paper discusses how to design the graph scheme for non-attributed data entities for #frauddetection. And we first verify the effectiveness of graph contrastive learning for fraud detection on industrial-scale data.']",https://arxiv.org/abs/2110.01171,"Fraud detection problems are usually formulated as a machine learning problem on a graph. Recently, Graph Neural Networks (GNNs) have shown solid performance on fraud detection. The successes of most previous methods heavily rely on rich node features and high-fidelity labels. However, labeled data is scarce in large-scale industrial problems, especially for fraud detection where new patterns emerge from time to time. Meanwhile, node features are also limited due to privacy and other constraints. In this paper, two improvements are proposed: 1) We design a graph transformation method capturing the structural information to facilitate GNNs on non-attributed fraud graphs. 2) We propose a novel graph pre-training strategy to leverage more unlabeled data via contrastive learning. Experiments on a large-scale industrial dataset demonstrate the effectiveness of the proposed framework for fraud detection. ",Deep Fraud Detection on Non-attributed Graph
160,1453325207691726849,1214214537043296256,Theresa Eimer,"[""Do you know CARL? 🦙No, not the murder llama, our new benchmark for testing your agent's generalization, representations and more!\n\nYou can check out our paper at the EcoRL workshop at #NeurIPS2021 , on arxiv (<LINK>) or GitHub (<LINK>)! <LINK>""]",https://arxiv.org/abs/2110.02102,"While Reinforcement Learning has made great strides towards solving ever more complicated tasks, many algorithms are still brittle to even slight changes in their environment. This is a limiting factor for real-world applications of RL. Although the research community continuously aims at improving both robustness and generalization of RL algorithms, unfortunately it still lacks an open-source set of well-defined benchmark problems based on a consistent theoretical framework, which allows comparing different approaches in a fair, reliable and reproducibleway. To fill this gap, we propose CARL, a collection of well-known RL environments extended to contextual RL problems to study generalization. We show the urgent need of such benchmarks by demonstrating that even simple toy environments become challenging for commonly used approaches if different contextual instances of this task have to be considered. Furthermore, CARL allows us to provide first evidence that disentangling representation learning of the states from the policy learning with the context facilitates better generalization. By providing variations of diverse benchmarks from classic control, physical simulations, games and a real-world application of RNA design, CARL will allow the community to derive many more such insights on a solid empirical foundation. ",CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning
161,1450853554206482432,942694791707545600,Thomas Scialom,"['📢New Paper Alert\nHow do metrics perform across tasks?\n\n𝐁𝐄𝐀𝐌𝐞𝐭𝐫𝐢𝐜𝐬: 𝐀 𝐁𝐞𝐧𝐜𝐡𝐦𝐚𝐫𝐤 𝐟𝐨𝐫 𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐢𝐨𝐧 𝐄𝐯𝐚𝐥𝐮𝐚𝐭𝐢𝐨𝐧\n=&gt; Multi Task/Lingual/Modal\n\nPaper: <LINK>\nCode: <LINK>\n\n👇 <LINK>', 'Generative models are becoming increasingly important in NLP. This makes research on\nevaluation metrics of critical importance. However, there is currently no simple, unified way to\ncompare, analyze or evaluate metrics across a representative set of tasks. \n\n2/', 'To this purpose, we release BEAMetrics, the Benchmark to Evaluate Automatic Metrics. \n\nAcross its 11 datasets, BEAMetrics is multi[task, lingual and dimensional]\n\n3/ https://t.co/8zcwPqA0Yu', 'I am particularly excited about including 2 open-ended Question Answering evaluation sets. Indeed, QA has long been considered as an extractive task before the recent advances for abstractive models.\n\n4/ https://t.co/0c0Ikod64Z', 'Already some interesting findings:\n- BERTScore becomes increasingly more popular. But its performance is unequal across tasks: on summarization, it often reflects human judgment worst than ROUGE or BLEU.\n\n5/', 'I hope BEAMetrics will serve the community to stimulate research into future metrics that address the challenges of evaluating flexible generative models of language.\n\nA collaboration @DeepMind &amp; @RecitalAI\nBig thanks to my amazing coauthor @FelixHill84 🙏\n\n6/6']",https://arxiv.org/abs/2110.09147,"Natural language processing (NLP) systems are increasingly trained to generate open-ended text rather than classifying between responses. This makes research on evaluation metrics for generated language -- functions that score system output given the context and/or human reference responses -- of critical importance. However, different metrics have different strengths and biases, and reflect human intuitions better on some tasks than others. There is currently no simple, unified way to compare, analyse or evaluate metrics across a representative set of tasks. Here, we describe the Benchmark to Evaluate Automatic Metrics (BEAMetrics), a resource to make research into new metrics itself easier to evaluate. BEAMetrics users can quickly compare existing and new metrics with human judgements across a diverse set of tasks, quality dimensions (fluency vs. coherence vs. informativeness etc), and languages. As generation experts might predict, BEAMetrics reveals stark task-dependent differences between existing metrics, and consistently poor performance on tasks with complex answer spaces or high reliance on general knowledge. While this analysis highlights a critical issue facing current research practice, BEAMetrics also contribute to its resolution by facilitating research into better metrics -- particularly those that can account for the complex interaction between context and general knowledge inherent to many modern NLP applications. BEAMetrics is available under the MIT License: this https URL ",BEAMetrics: A Benchmark for Language Generation Evaluation Evaluation
162,1450376093425360899,1063986211881009153,Bashar Alhafni,"['🚨 New Dataset Alert 🚨\n\nWe are releasing the Arabic Parallel Gender Corpus v2.0 (APGC v2.0) for gender identification and rewriting in contexts involving one or two target users. Joint work with @nyhabash and @hbouamor at @CamelNlp \n\nPaper: <LINK> (1/n)', 'This corpus expands on its previous version (APGC v1.0) which was introduced by @nyhabash et al. in: https://t.co/FTvNPDEFhQ by adding second person targets as well as increasing the total number of sentences over 6.5 times (~80K sentences), reaching over 590K words (2/n)', 'We annotated ~63K Arabic sentences from the English-Arabic OpenSubtitles 2018 based on the genders of their first and second person references. In case a gendered reference exists, we introduce all the possible opposite gender forms (3/n) https://t.co/sp4nK1H7Pq', 'We also provide word-level gender annotations for all the Arabic sentences in the corpus (4/n) https://t.co/1dYkIxTeb7', 'The corpus has multiple parallel components: four combinations of 1st and 2nd person in feminine and masculine grammatical genders, as well as English (as we got it from OpenSubtitles 2018) (5/n) https://t.co/hQ6LhyACbY', 'We show that our corpus can also be used to detect and quantify bias in gender-unaware machine translation systems targeting Arabic. We machine translated the English sentences into Arabic and evaluated them based on different gender specificity factors in Arabic and English(6/n) https://t.co/ioZVWwQggJ', ""So if you're working on gender bias, post-editing MT output, or personalization in general, you should check our corpus out! (n/n)""]",https://arxiv.org/abs/2110.09216,"Gender bias in natural language processing (NLP) applications, particularly machine translation, has been receiving increasing attention. Much of the research on this issue has focused on mitigating gender bias in English NLP models and systems. Addressing the problem in poorly resourced, and/or morphologically rich languages has lagged behind, largely due to the lack of datasets and resources. In this paper, we introduce a new corpus for gender identification and rewriting in contexts involving one or two target users (I and/or You) -- first and second grammatical persons with independent grammatical gender preferences. We focus on Arabic, a gender-marking morphologically rich language. The corpus has multiple parallel components: four combinations of 1st and 2nd person in feminine and masculine grammatical genders, as well as English, and English to Arabic machine translation output. This corpus expands on Habash et al. (2019)'s Arabic Parallel Gender Corpus (APGC v1.0) by adding second person targets as well as increasing the total number of sentences over 6.5 times, reaching over 590K words. Our new dataset will aid the research and development of gender identification, controlled text generation, and post-editing rewrite systems that could be used to personalize NLP applications and provide users with the correct outputs based on their grammatical gender preferences. We make the Arabic Parallel Gender Corpus (APGC v2.0) publicly available. ",The Arabic Parallel Gender Corpus 2.0: Extensions and Analyses
163,1450072730087276544,872594072,Luca Maestrini,['New paper about streamlined variational inference on arXiv: <LINK>\nWe show fast and accurate variational inference for linear mixed models with fixed effects subject to selection. \nGreat collaboration with my PhD student at University of Padua and my UTS group!'],https://arxiv.org/abs/2110.07048,"Linear mixed models are a versatile statistical tool to study data by accounting for fixed effects and random effects from multiple sources of variability. In many situations, a large number of candidate fixed effects is available and it is of interest to select a parsimonious subset of those being effectively relevant for predicting the response variable. Variational approximations facilitate fast approximate Bayesian inference for the parameters of a variety of statistical models, including linear mixed models. However, for models having a high number of fixed or random effects, simple application of standard variational inference principles does not lead to fast approximate inference algorithms, due to the size of model design matrices and inefficient treatment of sparse matrix problems arising from the required approximating density parameters updates. We illustrate how recently developed streamlined variational inference procedures can be generalized to make fast and accurate inference for the parameters of linear mixed models with nested random effects and global-local priors for Bayesian fixed effects selection. Our variational inference algorithms achieve convergence to the same optima of their standard implementations, although with significantly lower computational effort, memory usage and time, especially for large numbers of random effects. Using simulated and real data examples, we assess the quality of automated procedures for fixed effects selection that are free from hyperparameters tuning and only rely upon variational posterior approximations. Moreover, we show high accuracy of variational approximations against model fitting via Markov Chain Monte Carlo sampling. ",Sparse Linear Mixed Model Selection via Streamlined Variational Bayes
164,1449080001932894208,1406456816410660867,Jana Doppa,"['New JAIR @JAIR_Editor paper on output space entropy search framework for solving a variety of multi-objective optimization problems with expensive black-box functions (w/ @syrineblk and @deshwal_aryan) \n\nPaper: <LINK>\nCode: <LINK>\n\n🧵👇 <LINK>', 'These problems arise in many science and engineering applications such as hardware design to trade-off power and performance, materials design, and Auto ML tasks (e.g., hyper-parameter tuning). https://t.co/ahrfQe2Nyh', 'The key challenge is to select the sequence of experiments (inputs for function evaluation) by trading-off exploration and exploitation to find the Optimal pareto set (input space) by minimizing the overall cost of experiments. https://t.co/3YaUKNjfK9', 'For the single-fidelity setting (expensive and accurate function evaluations), we select the experiment that will maximize the information gain about the optimal Pareto front (output space). \n\nAdvantages: improved anytime accuracy of Pareto set and computational efficiency. https://t.co/OJSS4eS3Zt', 'For the constrained setting (invalid designs can be identified only by doing expensive experiments), MESMOC algorithm instantiate the same principle to approximate the constrained Pareto set.\n\nCode: https://t.co/w5bz8zbbG8 https://t.co/J7wVznr9wQ', 'For the discrete multi-fidelity setting (evaluations that vary in accuracy and resource cost), MF-OSEMO selects the experiment that maximizes the information gain per unit cost about the optimal Pareto front.\n\nCode: https://t.co/VbxCgjY8XQ https://t.co/bqPwDq7ZTO', 'For the continuous-fidelity setting, where the number of function approximations are huge, iMOCA instantiates this principle over the joint input and fidelity-vector space.\n\nCode: https://t.co/WuumE3jaqQ https://t.co/3DDSqHY9RC', 'These algorithms allow us to find high-quality Pareto solutions with lower cost for experiments. https://t.co/d4UPeTWiNy', 'This work builds on excellent work on max-value entropy search for single-objective BO by @ziwphd and @StefanieJegelka.\n\nThanks to the @JAIR_Editor reviewers and associate editor for their constructive feedback in improving the paper.\n\nWork funded by @NSF.\n\n/End', 'Tagging @SigOpt']",https://arxiv.org/abs/2110.06980,"We consider the problem of black-box multi-objective optimization (MOO) using expensive function evaluations (also referred to as experiments), where the goal is to approximate the true Pareto set of solutions by minimizing the total resource cost of experiments. For example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. The key challenge is to select the sequence of experiments to uncover high-quality solutions using minimal resources. In this paper, we propose a general framework for solving MOO problems based on the principle of output space entropy (OSE) search: select the experiment that maximizes the information gained per unit resource cost about the true Pareto front. We appropriately instantiate the principle of OSE search to derive efficient algorithms for the following four MOO problem settings: 1) The most basic em single-fidelity setting, where experiments are expensive and accurate; 2) Handling em black-box constraints} which cannot be evaluated without performing experiments; 3) The discrete multi-fidelity setting, where experiments can vary in the amount of resources consumed and their evaluation accuracy; and 4) The em continuous-fidelity setting, where continuous function approximations result in a huge space of experiments. Experiments on diverse synthetic and real-world benchmarks show that our OSE search based algorithms improve over state-of-the-art methods in terms of both computational-efficiency and accuracy of MOO solutions. ","Output Space Entropy Search Framework for Multi-Objective Bayesian
  Optimization"
165,1448853017243971584,2337598033,Geraint F. Lewis,"['Paper day! - new stream results from <LINK> led by @sazabi_li @alexanderpji\nAndrew Pace, @deniserkal, Sergey Koposov, @norashipp\nand the rest of the gang.\n\nRead about the orbital and chemical properties of one dozen stellar streams! \n \n<LINK> <LINK>']",https://arxiv.org/abs/2110.06950,"We report the kinematic, orbital, and chemical properties of 12 stellar streams with no evident progenitors, using line-of-sight velocities and metallicities from the Southern Stellar Stream Spectroscopic Survey ($S^5$), proper motions from $Gaia$ EDR3, and distances derived from distance tracers or the literature. This data set provides the largest homogeneously analyzed set of streams with full 6D kinematics and metallicities. All streams have heliocentric distances between ${\sim}10-50$ kpc. The velocity and metallicity dispersions show that half of the stream progenitors were disrupted dwarf galaxies (DGs), while the other half originated from disrupted globular clusters (GCs), hereafter referred to as DG and GC streams. Based on the mean metallicities of the streams and the mass-metallicity relation, the luminosities of the progenitors of the DG streams range between Carina and Ursa Major I ($-9.5\lesssim M_V\lesssim-5.5$). Four of the six GC streams have mean metallicities of [Fe/H]$< -2$, more metal-poor than typical Milky Way (MW) GCs at similar distances. Interestingly, the 300S and Jet GC streams are the only streams on retrograde orbits in our dozen stream sample. Finally, we compare the orbital properties of the streams with known DGs and GCs in the MW, finding several possible associations. Some streams appear to have been accreted with the recently discovered Gaia-Enceladus-Sausage system, and others suggest that GCs were formed in and accreted together with the progenitors of DG streams whose stellar masses are similar to Draco to Carina ($\sim10^5-10^6M_\odot$). ",$S^5$: The Orbital and Chemical Properties of One Dozen Stellar Streams
166,1448668532632432641,4020498861,Sarah Schwettmann,"['New for #ICCV21: Toward a Visual Concept Vocabulary for GAN Latent Space \n✨w/ @evanqed @davidbau @metasj @jacobandreas &amp; torralba \n\nPaper: <LINK>\nWebsite: <LINK>\n\nWhat visual concepts are shared by humans and GANs?\nHow can we discover them? \n1/n <LINK>', 'Our bottom-up approach captures concepts at different levels of abstraction in a single vocabulary.\n\nDirections represent not only details like color, texture &amp; rotation, but also higher-level aspects of visual experience, like what makes a scene more ‘welcoming’ or ‘spooky’\n3/n https://t.co/LMpf6ZLky7', 'Perceptual salience is built-in: our vocabulary is learned from a new dataset of directions labeled with their semantics. \n\n4/n https://t.co/w8MFPOvngj', 'We disentangle these annotations into a glossary of “primitive” visual transformations associated with single concepts. \n\nThe concepts generalize across latent space + image class, and compose! Compound concepts not present in annotations are recognized across observers.\n5/n https://t.co/kQHN6I8hA8']",https://arxiv.org/abs/2110.04292,"A large body of recent work has identified transformations in the latent spaces of generative adversarial networks (GANs) that consistently and interpretably transform generated images. But existing techniques for identifying these transformations rely on either a fixed vocabulary of pre-specified visual concepts, or on unsupervised disentanglement techniques whose alignment with human judgments about perceptual salience is unknown. This paper introduces a new method for building open-ended vocabularies of primitive visual concepts represented in a GAN's latent space. Our approach is built from three components: (1) automatic identification of perceptually salient directions based on their layer selectivity; (2) human annotation of these directions with free-form, compositional natural language descriptions; and (3) decomposition of these annotations into a visual concept vocabulary, consisting of distilled directions labeled with single words. Experiments show that concepts learned with our approach are reliable and composable -- generalizing across classes, contexts, and observers, and enabling fine-grained manipulation of image style and content. ",Toward a Visual Concept Vocabulary for GAN Latent Space
167,1448633037558800391,1005395495949406208,Francesco Locatello,"['New paper on learning object-centric video models and a new data set. \n\nThe FISHBOWL data set is an aquarium with multiple fishes with different textures. We propose a new dead-leaf model generating scenes with novel occlusions and interventions. \n\nLink: <LINK> <LINK>', 'Work led by @m_tangemann, w/ @stes_io, @JKugelgen , @pegehler, @ThomasBrox, @matthiaskue, @MatthiasBethge, and @bschoelkopf.\nCollaboration between @awscloud @AmazonScience @MPI_IS @bethgelab @Cambridge_Uni @UniFreiburg @uni_tue  @EPFL_en']",https://arxiv.org/abs/2110.06562,"Learning generative object models from unlabelled videos is a long standing problem and required for causal scene modeling. We decompose this problem into three easier subtasks, and provide candidate solutions for each of them. Inspired by the Common Fate Principle of Gestalt Psychology, we first extract (noisy) masks of moving objects via unsupervised motion segmentation. Second, generative models are trained on the masks of the background and the moving objects, respectively. Third, background and foreground models are combined in a conditional ""dead leaves"" scene model to sample novel scene configurations where occlusions and depth layering arise naturally. To evaluate the individual stages, we introduce the Fishbowl dataset positioned between complex real-world scenes and common object-centric benchmarks of simplistic objects. We show that our approach allows learning generative models that generalize beyond the occlusions present in the input videos, and represent scenes in a modular fashion that allows sampling plausible scenes outside the training distribution by permitting, for instance, object numbers or densities not observed in the training set. ",Unsupervised Object Learning via Common Fate
168,1448625877735014403,1570476014,Yi-Hsuan Yang,['Our new paper on audio-domain DJ transition generation using DDSP+GAN is out!\n*paper- <LINK>\n*demo- <LINK> <LINK>'],https://arxiv.org/abs/2110.06525,"A central task of a Disc Jockey (DJ) is to create a mixset of mu-sic with seamless transitions between adjacent tracks. In this paper, we explore a data-driven approach that uses a generative adversarial network to create the song transition by learning from real-world DJ mixes. In particular, the generator of the model uses two differentiable digital signal processing components, an equalizer (EQ) and a fader, to mix two tracks selected by a data generation pipeline. The generator has to set the parameters of the EQs and fader in such away that the resulting mix resembles real mixes created by humanDJ, as judged by the discriminator counterpart. Result of a listening test shows that the model can achieve competitive results compared with a number of baselines. ","Automatic DJ Transitions with Differentiable Audio Effects and
  Generative Adversarial Networks"
169,1448392160232017924,410963515,Sam Blackshear,"[""New Move paper: <LINK>. I am excited about this work because it provides some formal and empirical validation of Move's encapsulation features. (1/N)"", 'A program that maintains key safety properties even when interacting with arbitrary untrusted code is said to enjoy robust safety. Proving that a program written in a mainstream language is robustly safe is typically challenging because it requires static verification tools (2/N)', 'that work precisely even in the presence of language features like dynamic dispatch and shared mutability. The emerging Move programming language was designed to support strong encapsulation and static verification in the service of secure smart contract programming. (3/N)', 'However, the language design has not been analyzed using a theoretical framework like robust safety.\n\nIn this paper, we define robust safety for the Move language and introduce a generic framework for static tools that wish to enforce it. Our framework consists of two (4/N)', 'abstract components: a program verifier that can prove an invariant holds in a closed-world setting (e.g., the Move Prover), and a novel encapsulator that checks if the verifier’s result generalizes to an open-world setting. We formalize an escape analysis as an (5/N)', 'instantiation of the encapsulator and prove that it attains the required security properties.\n\nFinally, we implement our encapsulator as an extension to the Move Prover and use the combination to analyze a representative benchmark set of real-world Move programs. This (6/N)', 'toolchain certifies &gt;99% of the Move modules we analyze, validating that automatic enforcement of strong security properties like robust safety is practical for Move &lt;end&gt;']",https://arxiv.org/abs/2110.05043,"A program that maintains key safety properties even when interacting with arbitrary untrusted code is said to enjoy \emph{robust safety}. Proving that a program written in a mainstream language is robustly safe is typically challenging because it requires static verification tools that work precisely even in the presence of language features like dynamic dispatch and shared mutability. The emerging \move programming language was designed to support strong encapsulation and static verification in the service of secure smart contract programming. However, the language design has not been analyzed using a theoretical framework like robust safety. In this paper, we define robust safety for the \move language and introduce a generic framework for static tools that wish to enforce it. Our framework consists of two abstract components: a program verifier that can prove an invariant holds in a closed-world setting (e.g., the Move Prover), and a novel \emph{encapsulator} that checks if the verifier's result generalizes to an open-world setting. We formalise an escape analysis as an instantiation of the encapsulator and prove that it attains the required security properties. Finally, we implement our encapsulator as an extension to the Move Prover and use the combination to analyze a representative benchmark set of real-world \move programs. This toolchain certifies $>$99\% of the \move modules we analyze, validating that automatic enforcement of strong security properties like robust safety is practical for \move. ",Robust Safety for Move
170,1447986706435641345,76009287,Desh Raj,"['📢 New paper on ArXiv 📢\n\n""Injecting text and cross-lingual supervision in few-shot learning from self-supervised models""\n\nabs: <LINK>\npdf: <LINK>', 'How do you create a hybrid ASR system for a new language X with only 15 mins of transcribed speech?\n\nAnswer: Use XLSR-53, transcribed speech from other languages, and extra text from language X https://t.co/UCwPLIRv2Q', 'Step 1: Initialize the acoustic model from XLSR-53 available from @huggingface \n\n(We also tried other models but XLSR-53 works best due to diversity of pre-training languages.)', 'Step 2: Replace output layer with a phonemic (senone) output that predicts X-SAMPA based universal phoneset and fine-tune acoustic model on transcribed speech from other languages.', 'Step 3: Fine-tune on 15 min of X with lattice-free MMI objective, and expand the denominator graph with additional text from X. https://t.co/gkMtkldJfS', 'On 3 low-resource languages (Pashto, Haitian, Georgian), we got WERs matching those using 10 hours of transcribed speech.\n\nWith the full transcribed speech (~80h), we achieve SOTA WER on Pashto and Haitian.', 'This work was led by my colleague Matthew Wiesner (https://t.co/IUCBvYMPNr). I learned a lot from him about tricks with lexicons, decoding graphs, etc. that seem obsolete in the ""end-to-end"" world but turn out to be immensely useful!', 'BTW, Matthew has a nice toolkit for PyTorch-based training of acoustic models (with recently added init. from HF models) while still using LF-MMI training and WFST-decoding from Kaldi. Check it out here: https://t.co/gfJ9gugeom']",https://arxiv.org/abs/2110.04863,"Self-supervised model pre-training has recently garnered significant interest, but relatively few efforts have explored using additional resources in fine-tuning these models. We demonstrate how universal phoneset acoustic models can leverage cross-lingual supervision to improve transfer of pretrained self-supervised representations to new languages. We also show how target-language text can be used to enable and improve fine-tuning with the lattice-free maximum mutual information (LF-MMI) objective. In three low-resource languages these techniques greatly improved few-shot learning performance. ","Injecting Text and Cross-lingual Supervision in Few-shot Learning from
  Self-Supervised Models"
171,1447919822042456071,916709762062012416,Guangwei,"['<LINK> New paper! We observed the atmosphere of hot Jupiter WASP-74b and found evidence for aerosols and potentially high C/O ratio! <LINK>', 'Special thanks to @_astronomay for analyzing the Spitzer data and demonstrating the challenges of Spitzer data reduction. And most importantly for matching the pink purple blue color scheme! https://t.co/GXHZEiq2YJ', 'Also thanks to @ExoSing @kevinbstevenson @JDLothringer @StellarPlanet and all other collaborators for the help!']",https://arxiv.org/abs/2110.04415,"Planets are like children with each one being unique and special. A better understanding of their collective properties requires a deeper understanding of each planet. Here we add the transit and eclipse spectra of hot Jupiter WASP-74b into the ever growing dataset of exoplanet atmosphere spectral library. With six transits and three eclipses using the Hubble Space Telescope (HST) and Spitzer Space Telescope (\textit{Spitzer}), we present the most complete and precise atmospheric spectra of WASP-74b. We found no evidence for TiO/VO nor super-Rayleigh scattering reported in previous studies. The transit shows a muted water feature with strong Rayleigh scattering extending into the infrared. The eclipse shows a featureless blackbody-like WFC3/G141 spectrum and a weak methane absorption feature in the Spitzer 3.6 $\mu m$ band. Future James Webb Space Telescope (JWST) follow up observations are needed to confirm these results. ","The Hubble PanCET program: Transit and Eclipse Spectroscopy of the Hot
  Jupiter WASP-74b"
172,1446364228005343273,1229357581,Florian Huber,"['Have a look at our new paper which develops a flexible panel quantile regression for analyzing  growth-at-risk dynamics here: <LINK>\njoint work with T. E. Clark (@ClevelandFed), G. Koop (@UniStrathclyde), M. Marcellino (@Unibocconi ), M. Pfarrhofer (@PLUS_1622)']",https://arxiv.org/abs/2110.03411,"We develop a Bayesian non-parametric quantile panel regression model. Within each quantile, the response function is a convex combination of a linear model and a non-linear function, which we approximate using Bayesian Additive Regression Trees (BART). Cross-sectional information at the pth quantile is captured through a conditionally heteroscedastic latent factor. The non-parametric feature of our model enhances flexibility, while the panel feature, by exploiting cross-country information, increases the number of observations in the tails. We develop Bayesian Markov chain Monte Carlo (MCMC) methods for estimation and forecasting with our quantile factor BART model (QF-BART), and apply them to study growth at risk dynamics in a panel of 11 advanced economies. ","Investigating Growth at Risk Using a Multi-country Non-parametric
  Quantile Factor Model"
173,1445482065407844368,247800333,Ahmad طه,"[""🚨 New paper: \n\nWhere should traffic sensors be placed on highways🛣️🚗? <LINK>\n\nRead the paper--maybe you'll find convincing answers :)\n\nIn Press, IEEE Transactions on Intelligent Transportation Systems\n\nWork with Suyash, Sebastian, @cclaudel70, and Taposh""]",https://arxiv.org/abs/2110.00912,"This paper investigates the practical engineering problem of traffic sensors placement on stretched highways with ramps. Since it is virtually impossible to install bulky traffic sensors on each highway segment, it is crucial to find placements that result in optimized network-wide, traffic observability. Consequently, this results in accurate traffic density estimates on segments where sensors are not installed. The substantial contribution of this paper is the utilization of control-theoretic observability analysis -- jointly with integer programming -- to determine traffic sensor locations based on the nonlinear dynamics and parameters of traffic networks. In particular, the celebrated asymmetric cell transmission model is used to guide the placement strategy jointly with observability analysis of nonlinear dynamic systems through Gramians. Thorough numerical case studies are presented to corroborate the proposed theoretical methods and various computational research questions are posed and addressed. The presented approach can also be extended to other models of traffic dynamics. ",Where Should Traffic Sensors Be Placed on Highways?
174,1457801995557822468,1014318741700538368,Yi-Shin Lin,"['<LINK>\nWe develop a computational model basing on three simple cognitive principles, maximizing utility, discrete motor primitive, and sensory evidence accumulation to account for road user interactions. Find software and details in the preprint.']",https://arxiv.org/abs/2110.11015,"Many models account for the traffic flow of road users but few take the details of local interactions into consideration and how they could deteriorate into safety-critical situations. Building on the concept of sensorimotor control, we develop a modeling framework applying the principles of utility maximization, motor primitives, and intermittent action decisions to account for the details of interactive behaviors among road users. The framework connects these principles to the decision theory and is applied to determine whether such an approach can reproduce the following phenomena: When two pedestrians travel on crossing paths, (a) their interaction is sensitive to initial asymmetries, and (b) based on which, they rapidly resolve collision conflict by adapting their behaviors. When a pedestrian crosses the road while facing an approaching car, (c) either road user yields to the other to resolve their conflict, akin to the pedestrian interaction, and (d) the outcome reveals a specific situational kinematics, associated with the nature of vehicle acceleration. We show that these phenomena emerge naturally from our modeling framework when the model can evolve its parameters as a consequence of the situations. We believe that the modeling framework and phenomenon-centered analysis offer promising tools to understand road user interactions. We conclude with a discussion on how the model can be instrumental in studying the safety-critical situations when including other variables in road-user interactions. ",A Utility Maximization Model of Pedestrian and Driver Interactions
175,1455640166006095873,1279209709912961025,Alex Robey,"['Excited to announce that our paper “Adversarial Robustness with Semi-Infinite Constrained Learning” was accepted at #NeurIPS2021.  In this paper, we propose a new, equivalent formulation for adversarial training!🚀\n\nPaper: <LINK>\nCode: <LINK>\n\n1/', 'You might be thinking: ""I\'ve seen so many papers on adversarial robustness recently.  What\'s new in this one?""\n\nWe feel the same way!  That\'s why in this paper our goal was to come up with a totally new perspective on the adversarial training problem.\n￼\n2/ https://t.co/Wd8I9DmQpp', 'Our first result is to show that the standard min-max adv. training problem is equivalent to a new, stochastic optimization problem over probability distributions λ.\n\n3/ https://t.co/uplpPhM1eJ', 'Unlike the original adv. training problem, this new problem is a *linear*, variational problem in λ.  Also, notice that the max is now *outside* both of the expectations!\n\n/4 https://t.co/KAFskIyf59', 'Question: What does this reformulation give us?  \n\nAnswer: Under mild conditions on the loss, we can derive a *closed-form solution* for the optimal distribution λ* to the maximization problem.\n\n/5 https://t.co/ByAIaJbPGK', 'Informally, this means that if we can sample from λ*, we can replace the maximization with an expectation.\n\nThus, we can solve a (much) more standard minimization problem rather than a challenging min-max problem.\n\n/6 https://t.co/3z2tMtohdt', 'However, sampling from this distribution λ* can be challenging.  Thus, we use classical tools from Hamiltonian Monte Carlo (HMC) to obtain approximate samples from λ*.\n\n/7 https://t.co/YdBg0A325I', 'The result: A new algorithm for training adversarially robust models, which we call \n\nDual Adversarial LEarning == DALE.\n\nDALE combines HMC sampling with a primal-dual iteration to mitigate the trade-off between robustness &amp; accuracy.\n\n/8 https://t.co/MVyM21jViB', 'We compare models trained with DALE and numerous other SOTA baselines.  Two takeaways:\n\n1. DALE beats all baselines in adv. acc. on CIFAR-10 by 2%.\n\n2. DALE can simultaneously achieve &gt;85% clean acc. &amp; 50% adv. acc. on CIFAR-10.  \n\n9/ https://t.co/7I6xMyggPN', 'Further experiments show that our primal-dual approach forces classifiers to better mitigate the trade-off between robustness and accuracy.\n\n/10 https://t.co/vkEmbQ0giL', ""This work wouldn't have been possible without my amazing collaborators: Luiz F. O. Chamon (co-first author), @pappasg69, @HamedSHassani, and Alejandro Ribeiro.\n\nIf you have any questions, feel free to reach out on Twitter or by email: arobey1@seas.upenn.edu\n\n/11 https://t.co/4pH5bqvkBq""]",https://arxiv.org/abs/2110.15767,"Despite strong performance in numerous applications, the fragility of deep learning to input perturbations has raised serious questions about its use in safety-critical domains. While adversarial training can mitigate this issue in practice, state-of-the-art methods are increasingly application-dependent, heuristic in nature, and suffer from fundamental trade-offs between nominal performance and robustness. Moreover, the problem of finding worst-case perturbations is non-convex and underparameterized, both of which engender a non-favorable optimization landscape. Thus, there is a gap between the theory and practice of adversarial training, particularly with respect to when and why adversarial training works. In this paper, we take a constrained learning approach to address these questions and to provide a theoretical foundation for robust learning. In particular, we leverage semi-infinite optimization and non-convex duality theory to show that adversarial training is equivalent to a statistical problem over perturbation distributions, which we characterize completely. Notably, we show that a myriad of previous robust training techniques can be recovered for particular, sub-optimal choices of these distributions. Using these insights, we then propose a hybrid Langevin Monte Carlo approach of which several common algorithms (e.g., PGD) are special cases. Finally, we show that our approach can mitigate the trade-off between nominal and robust performance, yielding state-of-the-art results on MNIST and CIFAR-10. Our code is available at: this https URL ",Adversarial Robustness with Semi-Infinite Constrained Learning
176,1455463802934280192,916940326132224000,Virginie Do,"['Happy to share our paper “Two-sided fairness in rankings via Lorenz dominance”, accepted to #NeurIPS2021 (w/ @scorbettdavies, J. Atif &amp; Nicolas Usunier) 🎊 We propose a new framework for fair recommendation, grounded in welfare economics. \n<LINK> 1/4', 'Our goal: improve the experience of the worse-off users and/or content producers. \nWe use Generalized Lorenz curves to understand who benefits or bears the cost of a fairness intervention. Rankings are considered *Lorenz efficient* if they produce non-dominated Lorenz curves. 2/4 https://t.co/Ctw2PLJRJr', 'This guarantees\n(a) Pareto efficiency\n(b) equity: utility is redistributed from better-off to worse-off, keeping total utility constant. 3/4', 'We generate rankings by maximizing concave welfare functions. No single state of ""perfect fairness"" here - rather a variety of acceptable tradeoffs, which choice is context-dependent. Our approach, unlike those based on fairness constraints, always satisfies Lorenz efficiency 4/4', '@facebookai @Paris_Dauphine @psl_univ @DauphineMiles @NeurIPSConf']",https://arxiv.org/abs/2110.15781,"We consider the problem of generating rankings that are fair towards both users and item producers in recommender systems. We address both usual recommendation (e.g., of music or movies) and reciprocal recommendation (e.g., dating). Following concepts of distributive justice in welfare economics, our notion of fairness aims at increasing the utility of the worse-off individuals, which we formalize using the criterion of Lorenz efficiency. It guarantees that rankings are Pareto efficient, and that they maximally redistribute utility from better-off to worse-off, at a given level of overall utility. We propose to generate rankings by maximizing concave welfare functions, and develop an efficient inference procedure based on the Frank-Wolfe algorithm. We prove that unlike existing approaches based on fairness constraints, our approach always produces fair rankings. Our experiments also show that it increases the utility of the worse-off at lower costs in terms of overall utility. ",Two-sided fairness in rankings via Lorenz dominance
177,1455271822501482503,1444381431195648000,Felix A. Palm,"['In our latest collaboration, led by Bachelor student \n@RejaWilke, we studied Bose-Einstein condensation on a wheel 🎡 and found hardcore bosons to condense at tunable momentum in the presence of arbitrarily small, modulated hopping to the center site: <LINK> <LINK>']",http://arxiv.org/abs/2110.15770,"We introduce a mechanism stabilizing a one-dimensional quantum many-body phase, characterized by a certain wave vector $k_0$, from a $k_0$-modulated coupling to a center site, via the protection of an emergent $\mathbb Z_2$ symmetry. We illustrate this mechanism by constructing the solution of the full quantum many-body problem of hardcore bosons on a wheel geometry, which are known to form a Bose-Einstein condensate. The robustness of the condensate is shown numerically by adding nearest-neighbor interactions to the wheel Hamiltonian. We identify the energy scale that controls the protection of the emergent $\mathbb Z_2$ symmetry. We discuss further applications such as geometrically inducing finite-momentum condensates. Since our solution strategy is based on a generic mapping from a wheel geometry to a projected ladder, our analysis can be applied to various related problems with extensively scaling coordination numbers. ","Symmetry-protected Bose-Einstein condensation of interacting hardcore
  Bosons"
178,1455131794169270275,995661858508976129,Xingang Pan,"['Our #NeurIPS2021 paper ShadeGAN is released at <LINK>.\nWhile most 3D-aware GANs are trained via the multi-view constraint, we propose a multi-lighting constraint that resolves the shape-color ambiguity and leads to more accurate 3D shapes!\n<LINK> <LINK>']",https://arxiv.org/abs/2110.15678,"The advancement of generative radiance fields has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumination explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efficient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on multiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at this https URL ","A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware
  Image Synthesis"
179,1454147943955513345,45675087,Devi Parikh,"['A study led by @safinahaali on whether AI models can inspire creativity across modalities! We find that generated visuals inspire human creativity in storytelling. Specifically, they benefit divergent aspects of creativity but hinder convergent thinking.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2110.14810,"Can visual artworks created using generative visual algorithms inspire human creativity in storytelling? We asked writers to write creative stories from a starting prompt, and provided them with visuals created by generative AI models from the same prompt. Compared to a control group, writers who used the visuals as story writing aid wrote significantly more creative, original, complete and visualizable stories, and found the task more fun. Of the generative algorithms used (BigGAN, VQGAN, DALL-E, CLIPDraw), VQGAN was the most preferred. The control group that did not view the visuals did significantly better in integrating the starting prompts. Findings indicate that cross modality inputs by AI can benefit divergent aspects of creativity in human-AI co-creation, but hinders convergent thinking. ",Telling Creative Stories Using Generative Visual Aids
180,1453986675625975810,321943790,Mikko J.S. Auvinen,"['A year ago, we set to create the most accurate #indoor flow model in the world in order to understand and control #airborne transmission of #coronavirus. Now this exceptional study is documented and submitted for peer-review. Our preprint is published at <LINK>']",http://arxiv.org/abs/2110.14348,"High-resolution large-eddy simulation (LES) is exploited to study indoor air turbulence and its effect on the dispersion of respiratory virus-laden aerosols and subsequent transmission risks. The methodology is applied to assess two dissimilar approaches to reduce transmission risks: a strategy to augment the indoor ventilation capacity with portable air purifiers and a strategy to utilize partitioning by exploiting portable space dividers. To substantiate the physical relevance of the LES model, a set of experimental aerosol concentration measurements are carried out, and their results are used for validating the LES model results. The obtained LES dispersion results are subjected to pathogen exposure and infection probability analysis. Wells-Riley probability model is extended to rely on realistic time- and space-dependent concentration fields to yield time- and space-dependent infection probability fields. The use of air purifiers leads to greater reduction in absolute risks compared to the analytical Wells-Riley model, which fails to predict the original risk level. However, the two models do agree on the relative risk reduction. The spatial partitioning strategy is demonstrated to have an undesirable effect when employed without other measures. The partitioning approach may yield positive results when employed together with targeted air purifier units. The LES-based results are examined in juxtaposition with the classical Wells-Riley model, which is shown to significantly underestimate the infection probability, highlighting the importance of employing accurate indoor turbulence modeling when evaluating different risk-reduction strategies. ","High-resolution large-eddy simulation of indoor turbulence and its
  effect on airborne transmission of respiratory pathogens; model validation
  and infection probability analysis"
181,1453971417754570758,892452667330633728,Caio Filippo Corro,"['Preventing posterior collapse in variational autoencoders for text generation via decoder regularization: we propose to use fraternal dropout to (try to) fix the posterior collapse problem in VAE for text generation. \n<LINK> <LINK>', 'This work was done by Alban Petit when he was a master student, it will be presented @ NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.']",https://arxiv.org/abs/2110.14945,"Variational autoencoders trained to minimize the reconstruction error are sensitive to the posterior collapse problem, that is the proposal posterior distribution is always equal to the prior. We propose a novel regularization method based on fraternal dropout to prevent posterior collapse. We evaluate our approach using several metrics and observe improvements in all the tested configurations. ","Preventing posterior collapse in variational autoencoders for text
  generation via decoder regularization"
182,1453727027723341825,726837554000084993,Jeremy Bailin,"[""Paper day! We've studied the effect of supernova self-enrichment in globular clusters (GCs) to figure out what the Milky Way's GCs looked like when they formed based on their current metallicity and iron abundance spread and/or mass. 1/3 <LINK>"", 'Self enrichment (a) makes GCs more metal-rich than the gas they originally formed from, and (b) introduces star-to-star variation in iron abundance. We can measure (b) and use it to correct for (a)! The correction is usually small, but can be up to 0.5 dex. 2/3 https://t.co/b06jmwKAAF', 'This allows us to get a more accurate view of how individual pieces of the Milky Way enriched themselves with heavy elements over time.\n\n""We"" = me + excellent undergrad researcher Ryker von Klar. Keep an eye out for his grad school apps next year! 3/3 https://t.co/iUslGfcytc']",https://arxiv.org/abs/2110.14571,"Intrinsic iron abundance spreads in globular clusters, although usually small, are very common, and are signatures of self enrichment: some stars within the cluster have been enriched by supernova ejecta from other stars within the same cluster. We use the Bailin (2018) self enrichment model to predict the relationship between properties of the protocluster -- its mass and the metallicity of the protocluster gas cloud -- and the final observable properties today -- its current metallicity and the internal iron abundance spread. We apply this model to an updated catalog of Milky Way globular clusters where the initial mass and/or the iron abundance spread is known to reconstruct their initial metallicities. We find that with the exception of the known anomalous bulge cluster Terzan 5 and three clusters strongly suspected to be nuclear star clusters from stripped dwarf galaxies, the model provides a good lens for understanding their iron spreads and initial metallicities. We then use these initial metallicities to construct age-metallicity relations for kinematically-identified major accretion events in the Milky Way's history. We find that using the initial metallicity instead of the current metallicity does not alter the overall picture of the Milky Way's history, since the difference is usually small, but does provide information that can help distinguish which accretion event some individual globular clusters with ambiguous kinematics should be associated with, and points to potential complexity within the accretion events themselves. ","Globular Cluster Intrinsic Iron Abundance Spreads: II. Protocluster
  Metallicities and the Age-Metallicity Relations of Milky Way Progenitors"
183,1453662907217158144,2444302555,Ludovic Denoyer,"['Happy to release a new paper about exploration in reinforcement learning (without reward). Together with @alelazaric @pa_kamienny and @jean_tarbou, we propose to learn a tree-structured policy to cover the set of states in any environments. \n\nArxiv paper: <LINK> <LINK>', 'It was possible (at least no too hard) to implement complex hierarchical policies by using SaLinA 😁😁 and the benchmarked implementation of the many RL algorithms we propose that are all agnostic to the policy structure: https://t.co/zqpgEP7Mne']",https://arxiv.org/abs/2110.14457,"Learning meaningful behaviors in the absence of reward is a difficult problem in reinforcement learning. A desirable and challenging unsupervised objective is to learn a set of diverse skills that provide a thorough coverage of the state space while being directed, i.e., reliably reaching distinct regions of the environment. In this paper, we build on the mutual information framework for skill discovery and introduce UPSIDE, which addresses the coverage-directedness trade-off in the following ways: 1) We design policies with a decoupled structure of a directed skill, trained to reach a specific region, followed by a diffusing part that induces a local coverage. 2) We optimize policies by maximizing their number under the constraint that each of them reaches distinct regions of the environment (i.e., they are sufficiently discriminable) and prove that this serves as a lower bound to the original mutual information objective. 3) Finally, we compose the learned directed skills into a growing tree that adaptively covers the environment. We illustrate in several navigation and control environments how the skills learned by UPSIDE solve sparse-reward downstream tasks better than existing baselines. ","Direct then Diffuse: Incremental Unsupervised Skill Discovery for State
  Covering and Goal Reaching"
184,1453527034483941377,764045672,Mayank Agarwal,"['Delighted to share our upcoming #NeurIPS2021 paper ""On sensitivity of meta-learning to support data""(<LINK>)\n\nWe study the sensitivity of meta-learning methods to adaptation data, and show the existence of... (1/2) <LINK>', 'unaltered, in-distribution, and natural images that, when used for adaptation, yield accuracy as low as 4% or as high as 95% on standard few-shot image classification benchmarks. (2/2)', '@icepieces Thank you Yas!']",https://arxiv.org/abs/2110.13953,"Meta-learning algorithms are widely used for few-shot learning. For example, image recognition systems that readily adapt to unseen classes after seeing only a few labeled examples. Despite their success, we show that modern meta-learning algorithms are extremely sensitive to the data used for adaptation, i.e. support data. In particular, we demonstrate the existence of (unaltered, in-distribution, natural) images that, when used for adaptation, yield accuracy as low as 4\% or as high as 95\% on standard few-shot image classification benchmarks. We explain our empirical findings in terms of class margins, which in turn suggests that robust and safe meta-learning requires larger margins than supervised learning. ",On sensitivity of meta-learning to support data
185,1453280189358780418,1314104323,The Anh Han,['New pre-print where we studied commitment formation games and what institutional incentives is most efficient at promoting participation and compliance. Early version and comments welcome <LINK>'],https://arxiv.org/abs/2110.13307#,"Both conventional wisdom and empirical evidence suggests that arranging a prior commitment or agreement before an interaction enhances the chance of reaching mutual cooperation. Yet it is not clear what mechanisms can promote the participation in and compliance with such a commitment, especially when the former is costly and deviating from the latter is profitable. Prior work either considers regimented commitments where compensation is assumed enforceable from dishonest committers, or assume implicit commitments from every individual (so they are all in and thus being treated as such). Here we develop a theory of participation and compliance with respect to an explicit prior commitment under institutional incentives where individuals, at first, decide whether or not to join a cooperative agreement to play a one-shot social dilemma game. Using a mathematical model, we determine when participating in a costly commitment and complying with it, is an evolutionary stable strategy (ESS) when playing against all other possible strategies, and results in high levels of cooperation in the population. We show that, given a sufficient budget for providing incentives, reward of commitment compliant behaviours better promotes cooperation than punishment of non-compliant ones. Moreover, by sparing part of this budget for rewarding those who are willing to participate in a commitment, the overall frequency of cooperation can be significantly enhanced, for both reward and punishment. Finally, we find that, surprisingly, the presence of errors in a participation decision favours evolutionary stability of commitment compliant strategies and higher levels of cooperation. ","Institutional Incentives for the Evolution of Committed Cooperation:
  Ensuring Participation is as Important as Enhancing Compliance"
186,1453211732352987136,2629724339,shoubaneh,['How distinguishable are modified gravity theories with all their tunable parameters in a cosmic shear probe?  we propose dimensionality reduction:  <LINK>'],https://arxiv.org/abs/2110.13171,"We propose to use Self-Organizing Maps (SOM) to map the impact of physical models onto observables. Using this approach, we are able to determine how theories relate to each other given their signatures. In cosmology this will be particularly useful to determine cosmological models (such as dark energy, modified gravity or inflationary models) that should be tested by the new generation of experiments. As a first example, we apply this approach to the representation of a subset of the space of modified gravity theories probed by cosmic shear. We therefore train a SOM on shear correlation functions in the f(R), dilaton and symmetron models. The results indicate these three theories have similar signatures on shear for small values of their parameters but the dilaton has different signature for higher values. We also show that modified gravity (especially the dilaton model) has a different impact on cosmic shear compared to a dynamical dark energy so both need to be tested by galaxy surveys. ","Categorizing models using Self-Organizing Maps: an application to
  modified gravity theories probed by cosmic shear"
187,1453000270397513754,929973145,Dr David Sobral 💫🌌,['(Re)Solving Reionization with Lyα: How Bright Lyα Emitters account for the z≈2−8 Cosmic Ionizing Background (<LINK>). We find that a highly ionizing minority of galaxies with MUV&lt;−17 (LAEs) accounts for the entire ionizing budget from star-forming galaxies. <LINK>'],https://arxiv.org/abs/2110.11967,"The cosmic ionizing emissivity from star-forming galaxies has long been anchored to UV luminosity functions. Here we introduce an emissivity framework based on Ly$\alpha$ emitters (LAEs), which naturally hones in on the subset of galaxies responsible for the ionizing background due to the intimate connection between the production and escape of Ly$\alpha$ and LyC photons. Using constraints on the escape fractions of bright LAEs ($L_{\rm{Ly\alpha}}>0.2 L^{*}$) at $z\approx2$ obtained from resolved Ly$\alpha$ profiles, and arguing for their redshift-invariance, we show that: (i) quasars and LAEs together reproduce the relatively flat emissivity at $z\approx2-6$, which is non-trivial given the strong evolution in both the star-formation density and quasar number density at these epochs and (ii) LAEs produce late and rapid reionization between $z\approx6-9$ under plausible assumptions. Within this framework, the $>10\times$ rise in the UV population-averaged $f_{\rm{esc}}$ between $z\approx3-7$ naturally arises due to the same phenomena that drive the growing Ly$\alpha$ emitter fraction with redshift. Generally, a LAE dominated emissivity yields a peak in the distribution of the ionizing budget with UV luminosity as reported in latest simulations. Using our adopted parameters ($f_{\rm{esc}}=50\%$, $\xi_{\rm{ion}}=10^{25.9}$ Hz erg$^{-1}$ for half the bright LAEs), a highly ionizing minority of galaxies with $M_{\rm UV}<-17$ accounts for the entire ionizing budget from star-forming galaxies. Rapid flashes of LyC from such rare galaxies produce a ""disco"" ionizing background. We conclude proposing tests to further develop our suggested Ly$\alpha$-anchored formalism. ","(Re)Solving Reionization with Ly{\alpha}: How Bright Ly{\alpha} Emitters
  account for the $z\approx2-8$ Cosmic Ionizing Background"
188,1452989546107334659,3091570785,Claire McKay Bowen,"['Check out this paper that Felipe (@floridastate), @awunderground, Joshua (@RANDCorporation), and I wrote! We conducted an extensive feasibility study on various differentially private summary statistics and regression analyses for tax data!\n\n<LINK>\n\n@opendp_org <LINK>', '@floridastate @awunderground @RANDCorporation @opendp_org Also, we provide all our code and data (except for the confidential tax data).\n\nhttps://t.co/1h4FpniBMZ', 'Thank you to our funders: @SloanFoundation and @NCSESgov.\n\nSpecial thanks to our collaborators @IRSnews (Barry Johnson and Victoria Bryant).', 'Thank you to our awesome @urbaninstitute team + other collaborators: @lenburman, John Czajka, Surachai Khitatrakun, @GrahamIMac, @Rob_McClelland, Silke Taylor, @khueyama, Doug Wissoker, and @NoahZwiefel.\n\nShout out to @Gabe_Morr for reviewing our code.', 'Finally, thank you to our advisory board members: @john_abowd, Jim Cilke, @jasondebacker, Nada Eissa, @RickEcon, Dan Feenberg, @MaxGhenis, @NickRHart, @MattHJensen, Barry Johnson, Ithai Lurie, @machanavajjhala, Shelly Martinez, @ramoffitt3, @amy__ohara, Jerry Reiter,...', 'Emmanuel Saez, Wade Shen, Aleksandra Slavkovi´c, Salil Vadhan, and @larsvil.']",https://arxiv.org/abs/2110.12055,"Federal administrative tax data are invaluable for research, but because of privacy concerns, access to these data is typically limited to select agencies and a few individuals. An alternative to sharing microlevel data are validation servers, which allow individuals to query statistics without accessing the confidential data. This paper studies the feasibility of using differentially private (DP) methods to implement such a server. We provide an extensive study on existing DP methods for releasing tabular statistics, means, quantiles, and regression estimates. We also include new methodological adaptations to existing DP regression algorithms for using new data types and returning standard error estimates. We evaluate the selected methods based on the accuracy of the output for statistical analyses, using real administrative tax data obtained from the Internal Revenue Service Statistics of Income (SOI) Division. Our findings show that a validation server would be feasible for simple statistics but would struggle to produce accurate regression estimates and confidence intervals. We outline challenges and offer recommendations for future work on validation servers. This is the first comprehensive statistical study of DP methodology on a real, complex dataset, that has significant implications for the direction of a growing research field. ","A Feasibility Study of Differentially Private Summary Statistics and
  Regression Analyses for Administrative Tax Data"
189,1452876708051394561,976037939292594177,Alessandro Ignesti,"['#paperday Today on arxiv we introduce the Point-to-point trend exctractor (or PT-REX), a flexible Python code which eases the study of spatial correlations between different emissions of an extended source. <LINK> <LINK>']",https://arxiv.org/abs/2110.12720,"Investigating the spatial correlation between different emissions in an extended astrophysical source can provide crucial insights into their physical connection, hence it can be the key to understand the nature of the system. The point-to-point analysis of surface brightness is a reliable method to do such an analysis. In this work, we present PT-REX, a software to carry out these studies between radio and X-ray emission in extended sources. We discuss how to reliably carry out this analysis and its limitation and we introduce the Monte Carlo point-to-point analysis, which allows to extend this approach to poorly-resolved sources. Finally we present and discuss the application of our tool to study the diffuse radio emission in a galaxy cluster. ","Introducing PT-REX, the Point-to-point TRend EXtractor"
190,1452871665264758784,976037939292594177,Alessandro Ignesti,"['#paperday Today on arxiv our new study of the jellyfish galaxy JW100 based on LOFAR, MeerKAT and VLA observations. We investigate the magnetic fields and the connection between the synchrotron spectrum and the star formation history. <LINK> <LINK>', 'This galaxy is part of the Abell 2626 galaxy cluster, which hosts also the amazing Kite radio source https://t.co/PYAhMpCXar']",https://arxiv.org/abs/2110.12719,"Ram pressure stripping is a crucial evolutionary driver for cluster galaxies. It is thought to be able to accelerate the evolution of their star formation, trigger the activity of their central active galactic nucleus (AGN) and the interplay between the galactic and environmental gas, and eventually dissipate their gas reservoir. We explored the outcomes of ram pressure stripping by studying the non-thermal radio emission of the jellyfish galaxy JW100 in the cluster Abell 2626 ($z=0.055$) by combining LOFAR, MeerKAT, and VLA observations from 0.144 to 5.5 GHz. We studied the integrated spectra of the stellar disk, the stripped tail and the AGN, mapped the spectral index over the galaxy, and constrained the magnetic field intensity to be between 11 and 18 $\mu$G in the disk and $<10$ $\mu$G in the tail. The stellar disk radio emission is dominated by a radiatively old plasma, likely related to an older phase of high star formation rate. This suggests that the star formation was quickly quenched by a factor of 4 in a few $10^7$ yr. The radio emission in the tail is consistent with the stripping scenario, where the radio plasma originally accelerated in the disk is then displaced in the tail. The morphology of the radio and X-ray emissions supports the scenario of accretion of the magnetized environmental plasma onto the galaxy. The AGN non-thermal spectrum indicates that the relativistic electron acceleration may have occurred simultaneously with a central ionized gas outflow, thus suggesting a physical connection between the two processes. ","GASP XXXVIII: The LOFAR-MeerKAT-VLA view on the non-thermal side of a
  jellyfish galaxy"
191,1452855109214425088,422672164,Dr Michael Reidinger,"['Molecular Chemistry for Dark Matter II: Recombination, Molecule Formation, and Halo MassFunction in Atomic Dark Matter\n\n""specific example, we study atomic dark matter, consisting of a heavy particle &amp; a light particle charged under a dark electromagnetism""\n<LINK>']",https://arxiv.org/abs/2110.11964#,"Dissipative dark matter predicts rich observable phenomena that can be tested with future large-scale structure surveys. As a specific example, we study atomic dark matter, consisting of a heavy particle and a light particle charged under a dark electromagnetism. In particular, we calculate the cosmological evolution of atomic dark matter focusing on dark recombination and dark-molecule formation. We have obtained the relevant interaction-rate coefficients by re-scaling the rates for normal hydrogen, and evolved the abundances for ionized, atomic, and molecular states using a modified version of RecFAST++. We also provide an analytical approximation for the final abundances. We then calculate the effects of the atomic dark matter on the linear power spectrum, which enter through a dark-photon diffusion and dark acoustic oscillations. At the formation time, the atomic dark matter model suppresses halo abundances on scales smaller than the diffusion scale, just like the warm dark matter models suppress the abundance below the free-streaming scale. The subsequent evolution with radiative cooling, however, will alter the halo mass function further. ","Molecular Chemistry for Dark Matter II: Recombination, Molecule
  Formation, and Halo Mass Function in Atomic Dark Matter"
192,1451862497288286212,795877354266456064,KoheiKamadaPhys,"['Submitted a new paper with Soichiro and Hiromasa on arXiv, \n<LINK> . \nWe studied a reheating scenario (or how to realize the hot Big Bang Universe) in the inflationary models with ""kination"" (the Universe dominated by the kinetic energy of a scalar field).', 'We considered the U(1) gauge field amplification through the Chern-Simons coupling between the inflaton scalar and the gauge field and the subsequent Schwinger effect (including the thermalization of the produced particles).', 'Realizing a hot Big Bang Universe after kination is one of the most important problems in this scenario, but our proposal would be more realistic and reasonable than other existing proposals. \nSoichiro and Hiromasa did a very  hard work especially during their thesis writings!']",https://arxiv.org/abs/2110.10822,"In a class of (pseudoscalar) inflation, inflationary phase is followed by a kination phase, where the Universe is dominated by the kinetic energy of the inflaton that runs away in a vanishing scalar potential. In this class of postinflationary evolution of the Universe, reheating of the Universe cannot be achieved by the inflaton particle decay, which requires its coherent oscillation in a quadratic potential. In this study, we explore the U(1) gauge field production through the Chern-Simons coupling between the pseudoscalar inflaton and the gauge field during the kination era and examine the subsequent pair-particle production induced by the amplified gauge field known as the Schwinger effect, which can lead to reheating of the Universe. We find that with a rough estimate of the Schwinger effect for the Standard Model hyper U(1) gauge field and subsequent thermalization of the pair-produced particles, a successful reheating of the Universe can be achieved by their eventual domination over the kinetic energy of the inflaton, with some reasonable parameter sets. This can be understood as a concrete realization of the ""Schwinger reheating"". Constraints from the later-time cosmology are also discussed. ","Gauge Field Production and Schwinger Reheating in Runaway Axion
  Inflation"
193,1451620449054433283,2577596593,Chelsea Finn,"['Large language models (LLMs) often make mistakes that are difficult to correct.\n\nWe study the problem of quickly editing these models:\nPaper: <LINK>\nCode: <LINK>\n\nw/ @_eric_mitchell_, C. Lin, @ABosselut, @chrmanning\n\nthread 🧵👇 <LINK>', 'We assume a pre-trained model &amp; a dataset that covers many possible model edits\n \nThen, we meta-train a model editor that predicts a model update that:\n- edits the model\n- otherwise keeps the model behavior the same\n\n(2/4) https://t.co/N8ZT3KE93o', 'You can train model editors for massive models (e.g. GPT-J, T5-11B) in &lt;1 day on a single GPU.\n\nEdits with the resulting model editor are extremely fast, with edit success rate of 80-90%.\n(3/4) https://t.co/YGWT2jdALy', 'While I think that MEND makes significant progress on the problem of model editing, still lots of open challenges including:\n- deeper edit generalization\n- making many model edits\n\nIt was also a lot of fun to collaborate with @stanfordnlp!\n(4/4)', '@jackclarkSF @chrmanning @_eric_mitchell_ @ABosselut Thanks Jack!']",https://arxiv.org/abs/2110.11309,"While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that produces effective edits for models with tens of millions to over 10 billion parameters. Implementation available at this https URL ",Fast Model Editing at Scale
194,1451583718330540035,1025939776401158144,Alexander J. Sutherland,"['New paper (<LINK>, joint with Curtis Heberle) posted, so its time for a thread!\n\nQ: Given a set of polynomials of degree at most d, how many variables do they need to be in to guarantee we can find a solution by solving polynomials of degree at most d? | 1/17', 'We recover an algorithm of Sylvester (from 1887!) that addresses this question and give a modern description of the algorithm in terms of algebraic geometry (i.e. in terms of finding rational points on varieties).\n\nMore on this to come! | 2/17', 'We then use this algorithm to establish new upper bounds on RD(n), the resolvent degree of the general degree n polynomial.\n\nRD(n) is essentially how ""difficult"" it is to solve a degree n polynomial (see https://t.co/J2I2VIypTu for more on resolvent degree). | 3/17', 'Now, consider a set S = {f_1,...,f_s} of homogeneous polynomials in x_0,...,x_r of degree at most d.\n\nWe can consider the set of points P in ℙ^r (r-dimensional projective space) such that f_j(P)=0 for all 1≤j≤s. | 4/17', 'These points form an algebraic variety, which we denote by 𝕍(S). \n\nWe can re-state our question as ""how can we determine a point of 𝕍(S) by solving polynomials of degree at most d?"" | 5/17', 'We can find a point of 𝕍(S) directly by solving a polynomial of degree whose degree is the product deg(f_1) ∙∙∙ deg(f_s), but this is much larger than d!\n\n(! for emphasis, not a factorial) | 6/17', ""Now, let S' = S \\{f_1} = {f_2,...,f_s}.\n\nIf we could find a line L ⊆ 𝕍(S'), then we could find a point of 𝕍(S) by solving a polynomial of degree deg(f_1) (by looking at L∩𝕍(f_1) = L∩𝕍(S) ).\n\nSo, how can we find lines on varieties? | 7/17"", 'In my previous paper (https://t.co/KkjQatgbzK), I talk about the polar cone C(V;P) of a variety V at a point P.\n\nKey property: Any point Q of C(V;P) \\ {P} determines a line on V!\n\nAlso, C(V;P) only introduces polynomials / hypersurfaces of strictly smaller degree! | 8/17', ""Let's start outlining the algorithm!\n\n1.0) We start with 𝕍(S). \n\n1.1) We pass to a subsystem S' = S\\{f}, where f has maximal degree (which we call d) and consider 𝕍(S').\n\n1.2) It is easier* to find a point of S' than S, so we assume that we have a point P' of 𝕍(S') | 9/17"", ""1.3) We pass to the polar cone C(𝕍(S'),P'), which only introduces hypersurfaces of degree strictly less than d.\n\n1.4) Intersect C(𝕍(S'),P') with a hyperplane H that does not contain P'.\n\nNote that every point Q of C(𝕍(S'),P)∩H determines a line on 𝕍(S')! | 10/17"", 'So, we can determine a point of 𝕍(S) if we can determine a point Q of C(𝕍(S\'),P)∩H - which has one fewer hypersurface of maximal degree!\n\nThis is the core idea of what Sylvester calls the ""formula of reduction"". | 11/17', '2) Repeat steps 1.1 thru 1.4 as many times as necessary to get rid of all of the hypersurfaces of maximal degree.\n\nThis is the core idea of what Sylvester calls the ""formula of obliteration,"" which is why we call his algorithm the ""obliteration algorithm."" | 12/17', '3) Repeat step 2 until you are left with intersection of only hyperplanes. \n\nYou can determine a point of this intersection as soon as it is non-empty and the algorithm is done!\n\n*The ""easier"" in step 1.2 can (and is!) made precise in the paper | 13/17', 'We then apply this algorithm to certain varieties (iterated polar cones of Tschirnhaus complete intersections) to obtain the new upper bounds in the picture below! | 14/17 https://t.co/Li2Xe1wmse', 'To the best of my knowledge, this paper, along with the previous works https://t.co/KkjQatgbzK (Sutherland) and https://t.co/ohzSKKKs5W (Wolfson),  exhaust the techniques from both the classical and modern literature for getting upper bounds on RD(n). | 15/17', 'With this in mind, we lay out questions about both upper bounds on RD(n) and about the question from the first tweet of this thread in Subsection 4.3.\n\nI would give the questions here, but explaining the notation here would be a bit much | 16/17', 'I think this paper is really neat, so if you found this thread interesting, consider checking it out!\n\nYou can also check out https://t.co/KUkklelL53 (my website) or https://t.co/iimyvYeBom (which has a bunch of resolvent degree resources) for more.\n\nThanks for reading! | 17/17', ""@ananavaty97 @ClaudioJacobo @SC_Griffith @benblumsmith @sbagley @virtualcourtney I though some of y'all might find this interesting!""]",https://arxiv.org/abs/2110.08670,"For each $n$, let RD$(n)$ denote the minimum $d$ for which there exists a formula for the general polynomial of degree $n$ in algebraic functions of at most $d$ variables. In this paper, we recover an algorithm of Sylvester for determining non-zero solutions of systems of homogeneous polynomials, which we present from a modern algebro-geometric perspective. We then use this geometric algorithm to determine improved thresholds for upper bounds on RD$(n)$. ",Upper Bounds on Resolvent Degree via Sylvester's Obliteration Algorithm
195,1451576943485210630,807894948,Vivek Kulkarni,"[""How can you incorporate social factors (for eg. time, geography) which influence language use and understanding into large-scale LM's? With @TheShubhanshu and @aria42, we propose a simple pre-training method for this. <LINK> (Findings of EMNLP 2021) #emnlp2021 <LINK>"", '@TheShubhanshu @aria42 The 3 minute version: https://t.co/z9vmO2Wrhs']",https://arxiv.org/abs/2110.10319,"While large-scale pretrained language models have been shown to learn effective linguistic representations for many NLP tasks, there remain many real-world contextual aspects of language that current approaches do not capture. For instance, consider a cloze-test ""I enjoyed the ____ game this weekend"": the correct answer depends heavily on where the speaker is from, when the utterance occurred, and the speaker's broader social milieu and preferences. Although language depends heavily on the geographical, temporal, and other social contexts of the speaker, these elements have not been incorporated into modern transformer-based language models. We propose a simple but effective approach to incorporate speaker social context into the learned representations of large-scale language models. Our method first learns dense representations of social contexts using graph representation learning algorithms and then primes language model pretraining with these social context representations. We evaluate our approach on geographically-sensitive language-modeling tasks and show a substantial improvement (more than 100% relative lift on MRR) compared to baselines. ",LMSOC: An Approach for Socially Sensitive Pretraining
196,1451371463643176962,901266828655284225,Brian Metzger,"['It got buried in the arXiv double batch, but we have a new paper on a toy model for fast radio bursts - <LINK>  .  We propose a possible way to unify the dichotomy (longer bursts exhibit narrow time-integrated spectra) found by CHIME.  @navinsridhar']",https://arxiv.org/abs/2110.10738,"We introduce a toy model for the time-frequency structure of fast radio bursts (FRB), in which the observed emission is produced as a narrowly-peaked intrinsic spectral energy distribution sweeps down in frequency across the instrumental bandpass as a power-law in time. Though originally motivated by emission models which invoke a relativistic shock, the model could in principle apply to a wider range of emission scenarios. We quantify the burst's detectability using the frequency bandwidth over which most of its signal-to-noise ratio (SNR) is accumulated. We demonstrate that by varying just a single parameter of the toy model-the power-law index \beta of the frequency drift rate-one can transform a long (and hence preferentially time-resolved) burst with a narrow time-integrated spectrum into a shorter burst with a broad power-law time-integrated spectrum. We suggest that burst-to-burst diversity in the value of \beta could generate the dichotomy between burst duration and frequency-width recently found by CHIME. In shock models, the value of \beta is related to the radial density profile of external medium, which in light of the preferentially longer duration of bursts from repeating sources may point to diversity in the external environments surrounding repeating versus one-off FRB sources. ","A Toy Model for the Time-Frequency Structure of Fast Radio Bursts:
  Implications for the CHIME Burst Dichotomy"
197,1450853184772005889,1513921046,Preet,"['My first paper is now on the arXiv! <LINK>\n\nWe find evidence of bursty star formation and satellite accretion embedded in the present day elemental abundance distributions ([Mg/Fe] versus [Fe/H]) of simulated dwarf galaxies.', 'We looked at the present day, global stellar abundances of 8 galaxies (shown here). Of these, m11h, m11b, and m10q served as case studies for different abundances features. https://t.co/82VtMepfSt', ""In the case of m11h, we find that a secondary enrichment feature (encircled region 'II') is the result of a satellite accretion ~1 Gyr prior to present day. You can see the satellite galaxy that merged in the right panel! https://t.co/Cl8BdbsBA1"", ""In the other two, we find two drastically different effects of bursty star formation. m11b experiences recurrent, bursty star formation which leaves 'stripes' in the abundances. m10q experienced a single large burst, which left a gap in the general abundance trend. https://t.co/LYJMI39mtq"", 'Additional details can be found in the paper, where we visualize the bursty star formation and go into greater detail about the feedback effects responsible for these features. We also find that JWST/ELTs should be able to resolve the break in m10q!', 'Finally, a big thanks to my advisors and mentors, @sloebman, @AndrewWetzel, without whom I could not have written this paper or conducted this exciting research. A big thanks to my co-authors Claude-André Faucher-Giguère, Kareem El-Badry, and Jeremy Bailin\n#astrophysics #galaxies']",https://arxiv.org/abs/2110.08287,"We investigate stellar elemental abundance patterns at z = 0 in 8 low-mass (M_* = 10^6 - 10^9 M_sun) galaxies in the Feedback in Realistic Environments (FIRE-2) cosmological simulations. Using magnesium (Mg) as a representative alpha-element, we explore stellar abundance patterns in [Mg/Fe] versus [Fe/H], which follow an overall monotonic trend that evolved slowly over time. Beyond this, we explore 3 notable secondary features in enrichment (found in three different case-study galaxies) that arise from a galaxy merger or bursty star formation. First, we observe a secondary track with a lower [Mg/Fe] than the main trend. At z = 0, stars from this track are predominantly found within 2-6 kpc of the center; they were accreted in a 1:3 total-mass-ratio merger ~ 0.4 Gyr ago. Second, we find a distinct elemental bi-modality that forms following a strong burst in star formation in a galaxy at t_lookback ~ 10 Gyr. This burst quenched star formation for ~ 0.66 Gyr, allowing Ia supernovae to enrich the system with iron before star formation resumed. Third, we examine stripes in enrichment that run roughly orthogonal to the dominant [Mg/Fe] versus [Fe/H] trend; these stripes correspond to short bursts of star formation during which core-collapse supernovae enrich the surrounding medium with Mg (and Fe) on short timescales. If observed, these features would substantiate the utility of elemental abundances in revealing the assembly and star formation histories of dwarf galaxies. We explore the observability of these features for upcoming spectroscopic studies. Our results show that precise measurements of elemental abundance patterns can reveal critical events in the formation histories of low-mass galaxies. ","Predictions for Complex Distributions of Stellar Elemental Abundances in
  Low-Mass Galaxies"
198,1450836240413102085,35626342,Pierre-Luc Dallaire-Demers,['Which electronic systems can we simulate with log(n) qubits? Check out the latest results from @gblzq who studied variational algorithms for free-fermionic models in compressed space. This was done in the context of a @qosfoundation mentorship project.\n<LINK>'],https://arxiv.org/abs/2110.09550,"Variational quantum eigensolvers (VQE) are one of the possible tasks that can be performed in noisy intermediate-scale quantum (NISQ) computers. While one of the limitations of NISQ platforms is their restricted number of available qubits, the theory of compressable matchgate circuits can be used to circumvent this limitation for certain types of Hamiltonians. We show how VQE algorithms can be used to find the ground state of quadratic fermionic Hamiltonians, providing an expressible ansatz in a logarithmic number of qubits. In particular, for systems of $n$ orbitals encoded to 2-local qubit models with nearest neighbour interactions, the ground state energy can be evaluated with $O\left(\log n\right)$ sets of measurements. This result is invariant of the dimensions in which the $n$ sites are arranged. ","Variational Quantum Eigensolver in Compressed Space for
  Nearest-Neighbour Quadratic Fermionic Hamiltonians"
199,1450832033282932741,225895485,Andreas Madsen,"[""Can you trust attention explanations in #NLProc? What about other explanations? I'm excited to present our answer to these questions in a new paper: <LINK>\n\nI'm proud of this work. I hope you find it useful. If you do, consider sharing or ❤️. What do we find?... <LINK>"", 'This work was done in collaboration with @ncmeade, @vaibhav_adlakha, and @sivareddyg. I have more work on this direction planned, so follow us if you are interested.\nOkay, now to the summary with lots of diagrams and plots :) 2/9 …', 'Attention is regularly used to explain to NLP models. However, it is unknown if attention is a valid explanation. Because attention only relates to internal embeddings, it is not a mathematical guarantee that it also relates to the input tokens. 3/9 … https://t.co/ddqOGsKERr', 'The debate on “Is attention explanation?” continues every year, but with little consensus or progress on how to measure it. The question is very hard because we can’t annotate what a correct explanation is. 4/9 … https://t.co/QD7cqjjJZS', 'We adapt and improve ROAR by @sarahookr et al. (https://t.co/MUiE8280Os) which have been used in computer vision to answer a similar question. We make it work on NLP and fix a known issue related to redundancies in the dataset. 5/9 … https://t.co/SqAGjG3ciU', 'So, is attention a valid explanation? Well, surprisingly perhaps, it is task-dependent. More surprisingly, other more methods which were not valid explanations in Computer Vision, like “gradient” and “integrated gradient”, are valid explanations in NLP. 6/9 … https://t.co/91X3wRneja', 'But more importantly than these results, is the methodology of the measurement. I think many of the previous methods in NLP, are speculative at best. But the ROAR principle by @sarahookr et al. (https://t.co/MUiE8280Os), is really sound! 7/9 …', ""ROAR's foundational principle is: if information is truly important, then removing it from the dataset and retraining the model should result in a worse model. Importantly, this can be compared with removing random information. 8/9 …"", 'I hope by introducing and improving ROAR for the NLP community, we can establish a new foundation for interpretability research in NLP. 9/9 …', '@anmarasovic @sarahookr Thanks! I will be sure to check out this paper too :)', '@anmarasovic Definitely. Transformer models are challenging to test because they are more expensive to retrain. However, I have a planned paper that solves that. And I definitely want to include more importance measures for that publication.', '@zhi_bruce_wen 1) ""Faithful"" is not absolute. Better importance measures are still needed. 2) Importance measures might work on average, but fail on single observations. 3) Direct comparison of different methods is hard because their scale is very different. 4) Rank-based scores are unstable.', '@zhi_bruce_wen Let\'s say the sentence is ""[relevant A][relevant B][irrelevant]"", two different importance measures might select a different [relevant _] token as the most important. A direct comparison score will show they are inconsistent. However, ROAR will still show they are faithful.', '@ItsAmitJena Thanks!', ""@aerinykim I'm not sold on the IG method, so I am surprised it performed well on SST by such a large margin. For IMDB Integrated gradient also performs best, so maybe there is a connection with sentiment classification. However, I don't know what that connection would be."", ""@aerinykim PS: I don't add something based on expectations or outcome. I believe that to be bad science. The datasets used here are an exact match to used in previous literature, and were hence predetermined."", ""@zhi_bruce_wen You might be right but I can't really see the relationship."", ""@zhi_bruce_wen I think it's a mistake to think of faithfulness as an absolute. Something is not either faithful or unfaithful, it's simply a degree of faithful. In our faithfulness table we try to give an exact ratio number.\nWhat you decide to trust is a matter of option and application."", ""@hllo_wrld Unfortunately no. As model's gets deeper, they mix token embedding more which affects the faithfulness of attention. This is something I'm planning on investigating in future work.""]",https://arxiv.org/abs/2110.08412,"To explain NLP models, many methods inform which inputs tokens are important for a prediction. However, an open question is if these methods accurately reflect the model's logic, a property often called faithfulness. In this work, we adapt and improve a recently proposed faithfulness benchmark from computer vision called ROAR (RemOve And Retrain), by Hooker et al. (2019). We improve ROAR by recursively removing dataset redundancies, which otherwise interfere with ROAR. We adapt and apply ROAR, to popular NLP importance measures, namely attention, gradient, and integrated gradients. Additionally, we use mutual information as an additional baseline. Evaluation is done on a suite of classification tasks often used in the faithfulness of attention literature. Finally, we propose a scalar faithfulness metric, which makes it easy to compare results across papers. We find that, importance measures considered to be unfaithful for computer vision tasks perform favorably for NLP tasks, the faithfulness of an importance measure is task-dependent, and the computational overhead of integrated gradient is rarely justified. ","Evaluating the Faithfulness of Importance Measures in NLP by Recursively
  Masking Allegedly Important Tokens and Retraining"
200,1450412197977395204,999204024296574976,Clément Vignac,"['New preprint available (with @pafrossard): \nWe explain why exchangeability is not needed in GANs and VAEs for sets and graphs, and propose Top-N, an efficient way to parametrize probabilistic decoders.\n<LINK>\n(1/6)', 'One-shot generators typically generate a first set or graph by sampling points iid. Transformer or graph neural network layers are then applied to model complex dependencies. Unfortunately, i.i.d. sampling introduces a lot of randomness in the model, making training difficult.', 'To solve this issue, we introduce Top-N as a replacement for i.i.d. sampling in any GAN or VAE. Top-N selects the N most relevant points in a reference set using a differentiable mechanism, and then incorporates information from the prior vector. \n(3/6) https://t.co/qyTLCEQo12', 'The resulting architecture is easier to train, has better final performance and extrapolates well to larger sets.\n(4/6)', 'On the theory side, we extend the usual definition of equivariance and say that a learning algorithm is equivariant if the training dynamics do not depend on the group element used to represent the training data. \n(5/6)', 'Using this definition, we show that exchangeability is not needed in GANs and VAEs: the fact that the output of a function is a set  is an hypothesis, not something that needs to be proved. Functions that always returns the same permutation are therefore perfectly ok\n(6/6)']",http://arxiv.org/abs/2110.02096,"This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. This architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation, a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15% at SetMNIST reconstruction, by 33% at object detection on CLEVR, generates sets that are 74% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9. ",Top-N: Equivariant set and graph generation without exchangeability
201,1450271644195360770,906973065606717441,Xiaowen Feng,"['Our lab @lh3lh3 developed a metagenome assembler, hifiasm-meta (<LINK>), which  reconstructs @PacBio Hifi libraries to tens to hundreds of complete circular bacterial genomes. We hope these high-quality contigs to offer a new means to study microbial communities.', 'The assembly needs no manual intervention or auxilliary data. Sequence divergence between all pairs of the complete circular contigs is &gt;1%, suggesting little redundancy. We are always looking to test on more samples and improve. Consider give it a try:  https://t.co/H1jusjgw3m', ""We would like to thank @ChengChhy @DPortik for their help and suggestions throughout the course of hifiasm-meta's developement, and @ Wei Fan for sharing the chicken gut microbiome dataset with us. Thank you so much!""]",https://arxiv.org/abs/2110.08457,"Current metagenome assemblers developed for short sequence reads or noisy long readswere not optimized for accurate long reads. Here we describe hifiasm-meta, a new metagenome assembler that exploits the high accuracy of recent data. Evaluated on seven empirical datasets, hifiasm-meta reconstructed tens to hundreds of complete circular bacterial genomes per dataset, consistently outperforming other metagenome assemblers. ",Metagenome assembly of high-fidelity long reads with hifiasm-meta
202,1450082062409637889,1099349776867491840,Myrthe Reuver,"['My @ArgMining_2021 paper @ #EMNLP2021, w @suzan , Roser Morante, &amp; @antske: Is Stance Detection Topic-Independent? 🤔🧐\n\nWe systematically reproduce stance detection work + find unseen topic (gun control, cloning) &amp; class (pro, con) affect performance.📈\n\n<LINK> <LINK>', ""Our reproduction is of substantial work by @Nils_Reimers (2019) et. al.!\n\nI will present this paper on Nov 10 at the @ArgMining_2021 workshop at #EMNLP2021, and if all goes well I'll actually be in Punta Cana! 🤞\n\nhttps://t.co/4axyuJkyFH""]",https://arxiv.org/abs/2110.07693,"Cross-topic stance detection is the task to automatically detect stances (pro, against, or neutral) on unseen topics. We successfully reproduce state-of-the-art cross-topic stance detection work (Reimers et. al., 2019), and systematically analyze its reproducibility. Our attention then turns to the cross-topic aspect of this work, and the specificity of topics in terms of vocabulary and socio-cultural context. We ask: To what extent is stance detection topic-independent and generalizable across topics? We compare the model's performance on various unseen topics, and find topic (e.g. abortion, cloning), class (e.g. pro, con), and their interaction affecting the model's performance. We conclude that investigating performance on different topics, and addressing topic-specific vocabulary and context, is a future avenue for cross-topic stance detection. ","Is Stance Detection Topic-Independent and Cross-topic Generalizable? --
  A Reproduction Study"
203,1448972598923173913,3417706503,Felipe Herrera,"['New Preprint: ""Semi-empirical quantum optics for mid-infrared molecular nanophotonics""\n\nWe study near-field light-matter interaction with probe nanotips, tip-controlled strong coupling, Fano effects, and a new nonlinear vibrational blockade effect. \n\n<LINK> <LINK>', 'Project began in March 2020 by the unstoppable Johan Triana @hjoxan, PhD student Mauricio Arias, and followed by hundreds of hours of discussion with the group of Markus Raschke at @JILAscience .\n\nClose theory collaboration between @Ciencia_Usach and @udeconcepcion.']",https://arxiv.org/abs/2110.07371,"Nanoscale infrared (IR) resonators with sub-diffraction limited mode volumes and open geometries have emerged as new platforms for implementing cavity quantum electrodynamics (QED) at room temperature. The use of infrared (IR) nano-antennas and tip nanoprobes to study strong light-matter coupling of molecular vibrations with the vacuum field can be exploited for IR quantum control with nanometer and femtosecond resolution. In order to accelerate the development of molecule-based quantum nano-photonic devices in the mid-IR, we develop a generally applicable semi-empirical quantum optics approach to describe light-matter interaction in systems driven by mid-IR femtosecond laser pulses. The theory is shown to reproduce recent experiments on the acceleration of the vibrational relaxation rate in infrared nanostructures, and also provide physical insights for the implementation of coherent phase rotations of the near-field using broadband nanotips. We then apply the quantum framework to develop general tip-design rules for the experimental manipulation of vibrational strong coupling and Fano interference effects in open infrared resonators. We finally propose the possibility of transferring the natural anharmonicity of molecular vibrational levels to the resonator near-field in the weak coupling regime, to implement intensity-dependent phase shifts of the coupled system response with strong pulses. Our semi-empirical quantum theory is equivalent to first-principles techniques based on Maxwell's equations, but its lower computational cost suggests its use a rapid design tool for the development of strongly-coupled infrared nanophotonic hardware for applications ranging from quantum control of materials to quantum information processing. ",Semi-empirical Quantum Optics for Mid-Infrared Molecular Nanophotonics
204,1448926901263818760,75249390,Axel Maas,"['THE #EPSHEP21 proceedings of my master student Franziska Reiner are now available at <LINK> - we find that subtle SM effects, which are usually not accounted for, can have substantial impact for future ~1 TeV lepton colliders in certain cases.']",https://arxiv.org/abs/2110.07312,"Usually, the Bloch-Nordsieck theorem is expected to be violated in weak interactions, as the corresponding asymptotic states are not weak singlets. However, a manifest gauge-invariant approach replaces the asymptotic states with composite states, and thereby restores the Bloch- Nordsieck theorem. The Froehlich-Morchio-Strocchi mechanism ensures that this is in agreement with existing phenomenology. We outline the necessary steps of this construction. ",Bloch-Nordsieck restoration in llbar -&gt; qqbar
205,1448818925634867200,230918258,Kazuaki Takasan / 高三 和晃,"['New preprint (<LINK>) with S. Sumita @smiphys3141 (RIKEN) and Y. Yanase (Kyoto) is available on arXiv today! We propose a new scenario for realizing and controlling topological superconductivity with electric current. If you are interested, please check it out.']",https://arxiv.org/abs/2110.06959,"We show that finite current in superconductors can induce topological phase transitions, as a result of the deformation of the quasiparticle spectrum by a finite center-of-mass (COM) momentum of the Cooper pairs. To show the wide applicability of this mechanism, we examine the topological properties of three prototypical systems, the Kitaev chain, $s$-wave superconductors, and $d$-wave superconductors. We introduce a finite COM momentum as an external field corresponding to supercurrent and show that all the models exhibit current-induced topological phase transitions. We also discuss the possibility of observing the phase transitions in experiments and the relation to the other finite COM momentum pairing states. ",Supercurrent-induced topological phase transitions
206,1448810891105947650,1556664198,Kyle Cranmer,"['New paper! This work was led by the awesome Siddharth Mishra-Sharma (@kdqg1 ). We use Simulation-Based Inference to study the excess of gamma rays in the galactic center, which may be related to dark matter\n\n<LINK>\n\n@iaifi_news @NYUDataScience @NYUPhysics <LINK>', 'About a year ago we also studied this excess and suggested the use of Gaussian Processes to add flexibility to the model of photon emission https://t.co/hwq8vfW6jc', 'This time we attempt to disentangle the excess emission from smooth dark matter like component from unresolved astrophysical point\nsources e.g., millisecond pulsars. https://t.co/WPYqXlSjj3', 'The point sources have different spatial structure and statistics for photon counts that can be difficult to model with traditional inference techniques. But the forward model fairly straightforward. The problem is that latent variables make the likelihood intractable. Enter SBI!', 'With Simulation-based inference we focus on learning a probabilistic relationship between the 18 parameters that describe the photon emission and the maps they produce https://t.co/mTE0nIOOpg', 'We also use spherical graph neural networks to learn (implicit) summary statistics and conditional normalizing flows to model the final posterior.  🔥', 'We also do a lot of robustness tests, signal injection tests, etc. https://t.co/DP3jpbBWZ5', 'So check it out!\nAnd thanks to my awesome collaborator Sid @kdqg1, I learned a lot!\n\nhttps://t.co/LLbi6CPanp https://t.co/BxXwBHJ5HL', 'Bonus:  check out this cool figure from Sid’s last paper\n“[Submitted on 4 Oct 2021]\nInferring dark matter substructure with astrometric lensing beyond the power spectrum”\nhttps://t.co/EPlfGVhkgi https://t.co/N69BCpAAHV']",https://arxiv.org/abs/2110.06931,"The nature of the Fermi gamma-ray Galactic Center Excess (GCE) has remained a persistent mystery for over a decade. Although the excess is broadly compatible with emission expected due to dark matter annihilation, an explanation in terms of a population of unresolved astrophysical point sources e.g., millisecond pulsars, remains viable. The effort to uncover the origin of the GCE is hampered in particular by an incomplete understanding of diffuse emission of Galactic origin. This can lead to spurious features that make it difficult to robustly differentiate smooth emission, as expected for a dark matter origin, from more ""clumpy"" emission expected for a population of relatively bright, unresolved point sources. We use recent advancements in the field of simulation-based inference, in particular density estimation techniques using normalizing flows, in order to characterize the contribution of modeled components, including unresolved point source populations, to the GCE. Compared to traditional techniques based on the statistical distribution of photon counts, our machine learning-based method is able to utilize more of the information contained in a given model of the Galactic Center emission, and in particular can perform posterior parameter estimation while accounting for pixel-to-pixel spatial correlations in the gamma-ray map. This makes the method demonstrably more resilient to certain forms of model misspecification. On application to Fermi data, the method generically attributes a smaller fraction of the GCE flux to unresolved point sources when compared to traditional approaches. We nevertheless infer such a contribution to make up a non-negligible fraction of the GCE across all analysis variations considered, with at least $38^{+9}_{-19}\%$ of the excess attributed to unresolved point sources in our baseline analysis. ","A neural simulation-based inference approach for characterizing the
  Galactic Center $\gamma$-ray excess"
207,1448639971976310790,1247872005912891392,Owain Evans,"['New paper on truthful AI!\nWe introduce a definition of “lying” for AI\nWe explore how to train truthful ML models\nWe propose institutions to support *standards* for truthful AI\nWe weigh costs/benefits (economy + AI Safety)\n(w/ coauthors at Oxford &amp; OpenAI)\n<LINK>', 'Today, lying is a human problem. Over time, AI will generate an increasing share of text/speech. AI enables personalised, scalable deception. It needs no intention to deceive -- “lies” emerge from optimising text for rewards (e.g. sales, clicks, or Likes). https://t.co/7RcjeIsiuK', 'We introduce “negligent falsehoods” as a generalisation of lies. Truthful AI avoids negligent falsehoods. \nCreating AI that’s both truthful and trusted could improve human epistemics and trust among humans (as well as reducing risk from lying AI). https://t.co/88f3FiKwQC', 'Creating truthful AI is a technical challenge (e.g. finding right ML training objective) and a governance challenge. \nWe suggest *certifying* truthful AI before deployment and *adjudicating* violations after. https://t.co/JfUfuSYI79', 'The paper brings together ideas from Machine Learning, AI policy, and AI safety/alignment. \nThere is a 7-page Executive Summary covering the key ideas.\nAuthors: @OwainEvans_UK, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, @avitalbalwit, @pwillsit, @LucaFRighetti, W. Saunders']",http://arxiv.org/abs/2110.06674,"In many contexts, lying -- the use of verbal falsehoods to deceive -- is harmful. While lying has traditionally been a human affair, AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI ""lies"" (i.e. falsehoods that are actively selected for). Human truthfulness is governed by social norms and by laws (against defamation, perjury, and fraud). Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI, and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy, and mitigate risks of worst-case AI futures. Establishing norms or laws of AI truthfulness will require significant work to: (1) identify clear truthfulness standards; (2) create institutions that can judge adherence to those standards; and (3) develop AI systems that are robustly truthful. Our initial proposals for these areas include: (1) a standard of avoiding ""negligent falsehoods"" (a generalisation of lies that is easier to assess); (2) institutions to evaluate AI systems before and after real-world deployment; and (3) explicitly training AI systems to be truthful via curated datasets and human interaction. A concerning possibility is that evaluation mechanisms for eventual truthfulness standards could be captured by political interests, leading to harmful censorship and propaganda. Avoiding this might take careful attention. And since the scale of AI speech acts might grow dramatically over the coming decades, early truthfulness standards might be particularly important because of the precedents they set. ",Truthful AI: Developing and governing AI that does not lie
208,1448583212469473281,1347909035069206528,Anton Frisk Kockum,['New preprint out today with my industrial PhD student David Fitzek: <LINK>. We study how quantum computing can be applied to the logistics problem of heterogeneous vehicle routing. Collaboration between @wacqt_sweden @chalmersuniv and @VolvoGroup.'],https://arxiv.org/abs/2110.06799,"Quantum computing offers new heuristics for combinatorial problems. With small- and intermediate-scale quantum devices becoming available, it is possible to implement and test these heuristics on small-size problems. A candidate for such combinatorial problems is the heterogeneous vehicle routing problem (HVRP): the problem of finding the optimal set of routes, given a heterogeneous fleet of vehicles with varying loading capacities, to deliver goods to a given set of customers. In this work, we investigate the potential use of a quantum computer to find approximate solutions to the HVRP using the quantum approximate optimization algorithm (QAOA). For this purpose we formulate a mapping of the HVRP to an Ising Hamiltonian and simulate the algorithm on problem instances of up to 21 qubits. We find that the number of qubits needed for this mapping scales quadratically with the number of customers. We compare the performance of different classical optimizers in the QAOA for varying problem size of the HVRP, finding a trade-off between optimizer performance and runtime. ","Applying quantum approximate optimization to the heterogeneous vehicle
  routing problem"
209,1448459267346821127,929633835309981698,mmatsuo,"['Our paper is now on arXiv😊We propose spin-motive force driven by a surface acoustic wave, resulting in both dc &amp; second harmonic voltage. In contrast to the conventional ones, it requires no sophisticated device structures or strong spin-orbit materials. \n<LINK>']",https://arxiv.org/abs/2110.06552,"The spin-motive force (SMF) in a simple ferromagnetic monolayer caused by a surface acoustic wave is studied theoretically via spin-vorticity coupling (SVC). The SMF has two mechanisms. The first is the SVC-driven SMF, which produces the first harmonic electromotive force, and the second is the interplay between the SVC and the magentoelastic coupling, which produces the d.c. and second harmonic electromotive forces. We show that these electric voltages induced by a Rayleigh-type surface acoustic wave can be detected in polycrystalline nickel. No sophisticated device structures, non-collinear magnetic structures, or strong spin-orbit materials are used in our approach. Consequently, it is intended to broaden the spectrum of SMF applications considerably. ",Spin elastodynamic motive force
210,1448298592968581124,275920905,Dan Elsender,"['🚨 MY FIRST EVER PAPER IS ON THE ARXIV \n\nWe discuss how metallicity impacts the statistical properties of discs around young stars, accepted by MNRAS\n\nFind it on arxiv at: <LINK>\n\n🧵 below (figs are in a fun xkcd style):', 'We perform 4 smoothed particle hydrodynamic calculations of star cluster formation from a 500 solar mass molecular cloud, with identical initial conditions and varying metallicity (3, 1, 0.1, 0.01 times solar or in [Fe/H] 0.477, 0, -1, -2)', 'The calculations include radiative transfer (no stellar feedback) and a diffuse ISM model, this allows separate treatment of gas, dust, and radiation field temperatures.\n\nAfter second collapse phase the protostar is replaced with a sink particle', ""The radii of discs is represented via a 'characteristic radius', based upon power law density profile (Tazzari et al. 2017), which is the radius containing 63.2% of the disc mass"", 'We find that the radii of isolated discs (without companion within 2000au) decrease with metallicity, and the distribution of radii follows that from a previous study (Bate 2018) https://t.co/cmLiBbyiVN', 'This trend with metallicity continues for the radii of discs in multiple systems, the median radius of system discs in the 3 solar metallicity calculation is ~65au and just ~20au in the lowest metallicity calculation https://t.co/VcRlEzVDuc', 'When looking at relative orientations (between discs, orbital planes, protostellar spins) in bound pairs of protostars we see dependence on metallicity. Orientation of discs in wide pairs (&gt;100au) in the lowest metallicity calculation are essentially randomly distributed https://t.co/D01dmCkB98', 'Additionally, protostellar spins are sensitive to metallicity and have a strong preference for alignment in the lowest metallicity calculation. This is likely due to the increased rate of small-scale fragmentation caused by rapid cooling of gas in low opacity (low metallicity) https://t.co/j0S9nniXxA', 'Comparing the mass of system discs from the solar metallicity calculation with observed masses of discs about Class 0/I objects we see that there is a good agreement between the simulated disc masses and observed disc masses from the Perseus and Orion regions https://t.co/cB0U1KmJ2X', 'When comparing the disc radii with observations of discs about Class 0/I objects (only currently have data from Orion, see VANDAM survey) we see an excellent agreement between simulated and observed discs https://t.co/XNbbC9wcZu', 'Please see the paper for further details https://t.co/KjX8B5fH4G \n\nIt has been accepted by MNRAS and is due of publication soon!', 'Typo 😅 (not present in the paper) lighter blue distribution is Perseus Class 0']",https://arxiv.org/abs/2110.05501,"We present the analysis of the properties of large samples of protostellar discs formed in four radiation hydrodynamical simulations of star cluster formation. The four calculations have metallicities of 0.01, 0.1, 1 and 3 times solar metallicity. The calculations treat dust and gas temperatures separately and include a thermochemical model of the diffuse interstellar medium. We find the radii of discs of bound protostellar systems tend to decrease with decreasing metallicity, with the median characteristic radius of discs in the 0.01 and 3 times solar metallicity calculations being $\approx20$ and $\approx65$ au, respectively. Disc masses and radii of isolated protostars also tend to decrease with decreasing metallicity. We find that the circumstellar discs and orbits of bound protostellar pairs, and the two spins of the two protostars are all less well aligned with each other with lower metallicity than with higher metallicity. These variations with metallicity are due to increased small scale fragmentation due to lower opacities and greater cooling rates with lower metallicity, which increase the stellar multiplicity and increase dynamical interactions. We compare the disc masses and radii of protostellar systems from the solar metallicity calculation with recent surveys of discs around Class 0 and I objects in the Orion and Perseus star-forming regions. The masses and radii of the simulated discs have similar distributions to the observed Class 0 and I discs. ","The statistical properties of protostellar discs and their dependence on
  metallicity"
211,1448248330371354624,1352621567906361354,RicardoPuebla,"['Excited to see our results on critical quantum metrology out in arxiv where we study the different scaling regimes that one can get in critical fully-connected models <LINK>\nBig thanks to Louis Garbe  @AbahObinna @QuantoSimone for the hard work!', 'check out also the very interesting work by K. Gietka, L. Ruks and @thomasbusch https://t.co/0z8ZkEebCU']",https://arxiv.org/abs/2110.04144,"Phase transitions represent a compelling tool for classical and quantum sensing applications. It has been demonstrated that quantum sensors can in principle saturate the Heisenberg scaling, the ultimate precision bound allowed by quantum mechanics, in the limit of large probe number and long measurement time. Due to the critical slowing down, the protocol duration time is of utmost relevance in critical quantum metrology. However, how the long-time limit is reached remains in general an open question. So far, only two dichotomic approaches have been considered, based on either static or dynamical properties of critical quantum systems. Here, we provide a comprehensive analysis of the scaling of the quantum Fisher information for different families of protocols that create a continuous connection between static and dynamical approaches. In particular, we consider fully-connected models, a broad class of quantum critical systems of high experimental relevance. Our analysis unveils the existence of universal precision-scaling regimes. These regimes remain valid even for finite-time protocols and finite-size systems. We also frame these results in a general theoretical perspective, by deriving a precision bound for arbitrary time-dependent quadratic Hamiltonians. ","Critical Quantum Metrology with Fully-Connected Models: From Heisenberg
  to Kibble-Zurek Scaling"
212,1448197718799646721,929633835309981698,mmatsuo,['Our new paper is now on arXiv😊 \nWe propose a spin-transfer mechanism induced by inertial motion in a magnetic bilayer system composed of two host media and a narrow vacuum gap in between.\n\n<LINK>'],https://arxiv.org/abs/2110.05871,"We propose a spin transport induced by inertial motion. Our system is composed of two host media and a narrow vacuum gap in between. One of the hosts is sliding at a constant speed relative to the other. This mechanical motion causes the Doppler effect that shifts the density of states and the nonequilibrium distribution function in the moving medium. Those shifts induce the difference in the distribution function between the two media and result in tunnelling spin current. The spin current is calculated from the Schwinger-Keldysh formalism with a spin tunnelling Hamiltonian. This scheme does not require either temperature difference, voltage or chemical potential. ",Motion-induced spin transfer
213,1448111383783120900,906617293928587269,Junxian He,"['We show that prefix tuning/prompt tuning is essentially a form of adapter, which inspires a unified view to organize parameter-efficient tuning methods and propose better variants: <LINK> <LINK>', 'https://t.co/sis80LEz6V']",https://arxiv.org/abs/2110.04366,"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks. ",Towards a Unified View of Parameter-Efficient Transfer Learning
214,1447979702442881024,212147661,Yu Bai,"['🆕""When Can We Learn General-Sum Markov Games with a Large Number of Players Sample-Efficiently?""\n\n<LINK>\n\nWe theoretically study what RL can learn in multi-player general-sum MGs without exp(# players) samples.\n\nJoint w/ Ziang Song (Peking U.) &amp; @WispyMay. \n\n🧵', 'For general-sum Markov games with m&gt;=2 players, it is known that learning the commonly studied Nash Equilibria requires exp(\\Omega(m)) samples. \n\nWe instead look at two relaxed solution concepts: Correlated Equilibria (CE) and Coarse Correlated Equilibria (CCE).\n\n2/n', 'We design the first line of algorithms that learn CE/CCE in general-sum Markov games with samples that only depend on the max action space (over all players), instead of the product of the action spaces which is at least exp(\\Omega(m)).\n\n3/n', 'We then turn to Markov Potential Games (MPGs), a well-studied subclass of general-sum MGs. In this case, there exist recent algorithms that can find Nash with poly(m, 1/eps) samples.\n\nWe design an alternative algorithm with similar poly(m) and improved dependence on eps.\n\n4/n', 'Overall, we hope these results could shed light on what can be learned in general-sum MGs, when we do not have the luxury of observing exp(m) samples and recovering the full game.\n\nComments and suggestions are welcome!\n\n5/n, n=5']",https://arxiv.org/abs/2110.04184,"Multi-agent reinforcement learning has made substantial empirical progresses in solving games with a large number of players. However, theoretically, the best known sample complexity for finding a Nash equilibrium in general-sum games scales exponentially in the number of players due to the size of the joint action space, and there is a matching exponential lower bound. This paper investigates what learning goals admit better sample complexities in the setting of $m$-player general-sum Markov games with $H$ steps, $S$ states, and $A_i$ actions per player. First, we design algorithms for learning an $\epsilon$-Coarse Correlated Equilibrium (CCE) in $\widetilde{\mathcal{O}}(H^5S\max_{i\le m} A_i / \epsilon^2)$ episodes, and an $\epsilon$-Correlated Equilibrium (CE) in $\widetilde{\mathcal{O}}(H^6S\max_{i\le m} A_i^2 / \epsilon^2)$ episodes. This is the first line of results for learning CCE and CE with sample complexities polynomial in $\max_{i\le m} A_i$. Our algorithm for learning CE integrates an adversarial bandit subroutine which minimizes a weighted swap regret, along with several novel designs in the outer loop. Second, we consider the important special case of Markov Potential Games, and design an algorithm that learns an $\epsilon$-approximate Nash equilibrium within $\widetilde{\mathcal{O}}(S\sum_{i\le m} A_i / \epsilon^3)$ episodes (when only highlighting the dependence on $S$, $A_i$, and $\epsilon$), which only depends linearly in $\sum_{i\le m} A_i$ and significantly improves over existing efficient algorithm in the $\epsilon$ dependence. Overall, our results shed light on what equilibria or structural assumptions on the game may enable sample-efficient learning with many players. ","When Can We Learn General-Sum Markov Games with a Large Number of
  Players Sample-Efficiently?"
215,1447942998419980290,380420289,Denizalp,"['My forthcoming NeurIPS paper with my iconic advisor Amy Greenwald is up. We study min-max optimization problems with dependent feasible sets, which seems to have been first studied as ""Wald\'s maximin model"" and which we call min-max Stackelberg games.\n<LINK>', 'We provide to the best of our knowledge the first polynomial-time algorithms to solve min-max Stackelberg games. The problem we study has many applications such as the training of neural networks for optimal auctions, and robust optimization.', 'In our paper, we also show that the computation of competitive equilibria in Fisher markets is an instance of a min-max Stackelberg game. This allows us to uncover an equivalence between games and a large class of markets as well as methods to solve games and market dynamics.', 'Share, comment, subscribe for more content.🥰', '@sbmeti 🥰😘']",https://arxiv.org/abs/2110.05192,"Min-max optimization problems (i.e., min-max games) have been attracting a great deal of attention because of their applicability to a wide range of machine learning problems. Although significant progress has been made recently, the literature to date has focused on games with independent strategy sets; little is known about solving games with dependent strategy sets, which can be characterized as min-max Stackelberg games. We introduce two first-order methods that solve a large class of convex-concave min-max Stackelberg games, and show that our methods converge in polynomial time. Min-max Stackelberg games were first studied by Wald, under the posthumous name of Wald's maximin model, a variant of which is the main paradigm used in robust optimization, which means that our methods can likewise solve many convex robust optimization problems. We observe that the computation of competitive equilibria in Fisher markets also comprises a min-max Stackelberg game. Further, we demonstrate the efficacy and efficiency of our algorithms in practice by computing competitive equilibria in Fisher markets with varying utility structures. Our experiments suggest potential ways to extend our theoretical results, by demonstrating how different smoothness properties can affect the convergence rate of our algorithms. ",Convex-Concave Min-Max Stackelberg Games
216,1447867635442585601,3228486315,Daniele Grattarola,"['In our new paper, we introduce a unifying and modular framework for graph pooling: Select, Reduce, Connect.\nWe also propose a taxonomy of pooling and show why small-graph classification is not telling us the full story.\n\nArxiv: <LINK>\n\nTime for a 🧵on 🎱: <LINK>', 'Let\'s start from SRC, the ""message-passing"" of pooling.\n\nS: Selects (some) input nodes to map to one (or more) supernodes. Essentially decides what information is contained in the new nodes. \n\nR: Reduces the supernodes to singletons.\n\nC: decides how the new nodes are Connected. https://t.co/kkMFW4bHCj', ""Why is this important? Well, it's a simple idea, but it lets us unify seemingly incompatible things like DiffPool and TopK under a common framework. \n\nSRC is also a useful template for the community to share their contributions (e.g., in Spektral or PyG). https://t.co/r2t1HeyY0I"", 'The taxonomy follows from this unification. \nWe have dense/sparse, trainable/non-trainable, fixed/adaptive (number of output nodes), and hierarchical/global methods. \nAlmost all combinations of these four are found in the wild. https://t.co/xKYd6AcFv2', 'And then we deal with the evaluation problem. \nThe short story is that classification accuracy is only one possible tool, but it ignores things like attributes and structure preservation. \nAlso, for small graphs, the differences between methods are not that clear. https://t.co/gIKmuPITsD', ""This also helps explain the paper by Mesquita et al.(https://t.co/t7aFEqOZVQ), who are often misinterpreted as saying that pooling is useless.\nThey really say that dense pooling, on small graphs, after msg-passing, is basically all the same.\n\nDon't pool (🙃) everything together."", ""So what are the takeaways?\nThe safe choice is to use NDP or Graclus. They work well enough most of the time.\nIf you're chasing benchmark accuracy, then use trainable dense methods (DiffPool, MinCut).\nA trainable, dense, and adaptive method is missing and will likely get to SOTA."", 'Given these insights, my current standing on the ""usefulness"" debate is that not all problems benefit from pooling, a priori, and definitely not a single type of pooling. In CNNs pooling makes sense because it gives us invariance to small shifts, but in GNNs this is not immediate', ""My idea is that we should use pooling when the structure of the problem suggests an inherent hierarchy, levels of organization that can't be captured by very-local MP.\n\nBasically all successes in ML come from choosing the right inductive bias, so why should it be different here?"", 'Embedding (or pre-computing) that hierarchy as part of the data may lead to better results than having a one-size-fits-all pooling algorithm, essentially moving to a paradigm where the GNN input is graph+hierarchy.\n\nThis will be an interesting direction to explore.', 'Finally, this paper was a collaboration with the amazing @TruckersHitch and @FilippoMariaBi1, who have spent countless hours trying to decipher the chaos that is graph pooling and what it means to have a ""good"" pooled graph. Huge thanks to them!', ""Code: https://t.co/Z4tCwRdI1b\n\nIf we've missed your paper in Table 2, feel free to reach out and we'll categorize it and add it!!"", '@Azarkhalili Absolutely, the code is already designed to fit into Spektral so I will merge it soon into the library! Stay tuned']",https://arxiv.org/abs/2110.05292,"Inspired by the conventional pooling layers in convolutional neural networks, many recent works in the field of graph machine learning have introduced pooling operators to reduce the size of graphs. The great variety in the literature stems from the many possible strategies for coarsening a graph, which may depend on different assumptions on the graph structure or the specific downstream task. In this paper we propose a formal characterization of graph pooling based on three main operations, called selection, reduction, and connection, with the goal of unifying the literature under a common framework. Following this formalization, we introduce a taxonomy of pooling operators and categorize more than thirty pooling methods proposed in recent literature. We propose criteria to evaluate the performance of a pooling operator and use them to investigate and contrast the behavior of different classes of the taxonomy on a variety of tasks. ",Understanding Pooling in Graph Neural Networks
217,1447504622776725511,1047809884551569414,Thejs Brinckmann,"['New paper last week! We showed popular methods for computing the non-linear galaxy power spectrum can lead to biases for future surveys and propose a theoretical uncertainty accounting for discrepancies between the codes to evade the problem\n\nCheck it out! <LINK>', 'We considered non-linear estimators Halofit, HMCode and EuclidEmulator, which disagree somewhat on BAO scales and smaller. Otherwise very successful at predicting the non-linear power spectrum, these relatively small differences will prove important for future surveys like Euclid', 'Only considering scales as small as k = 0.4 h/Mpc we still find biases up to 6 sigma in the estimation of individual parameters for a mock data analysis with a Planck + Euclid galaxy clustering setup. Similar results were found by Martinelli et al. 2010.12382 for galaxy lensing.', 'We propose a proof-of-concept theoretical uncertainty, building on Sprenger et al. 1801.08331, tailored to the disagreement between the non-linear estimators. The result is the bias in parameter estimations is reduced from 6 sigma to 1 sigma or less, at a 20% loss in sensitivity.', ""We find this a promising avenue for delivering robust and precise results from future large-scale structure surveys, while accepting that our knowledge of non-linear structure formation isn't perfect. More work can be done to tune the theoretical uncertainty for optimal results.""]",https://arxiv.org/abs/2110.01488,"We implement EuclidEmulator (version 1), an emulator for the non-linear correction of the matter power spectrum, into the MCMC forecasting code MontePython. We compare the performance of Halofit, HMCode, and EuclidEmulator1, both at the level of power spectrum prediction and at the level of posterior probability distributions of the cosmological parameters, for different cosmological models and different galaxy power spectrum wave number cut-offs. We confirm that the choice of the power spectrum predictor has a non-negligible effect on the computed sensitivities when doing cosmological parameter forecasting, even for a conservative wave number cut-off of $0.2\,h\,{\rm Mpc}^{-1}$. We find that EuclidEmulator1 is on average up to $17\%$ more sensitive to the cosmological parameters than the other two codes, with the most significant improvements being for the Hubble parameter of up to $42\%$ and the equation of state of dark energy of up to $26\%$, depending on the case. In addition, we point out that the choice of the power spectrum predictor contributes to the risk of computing a significantly biased mean cosmology when doing parameter estimations. For the four tested scenarios we find biases, averaged over the cosmological parameters, of between 0.5 and 2$\sigma$ (from below $1\sigma$ up to $6\sigma$ for individual parameters). This paper provides a proof of concept that this risk can be mitigated by taking a well-tailored theoretical uncertainty into account as this allows to reduce the bias by a factor of 2 to 5, depending on the case under consideration, while keeping posterior credibility contours small: the standard deviations are amplified by a factor of $\leq1.4$ in all cases. ","Parameter inference with non-linear galaxy clustering: accounting for
  theoretical uncertainties"
218,1447187635538067460,1139031485900632064,Xiaoxiao Wang,"[""It's my honor to introduce our recent preprint on deep learning of fMRI. We find that: \n1. attention module improves performance.\n2. transfer learning efficient across dataset. \n3. model can learn in individual brain space.\n<LINK>""]",https://arxiv.org/abs/2110.00920,"Decoding brain cognitive states from neuroimaging signals is an important topic in neuroscience. In recent years, deep neural networks (DNNs) have been recruited for multiple brain state decoding and achieved good performance. However, the open question of how to interpret the DNN black box remains unanswered. Capitalizing on advances in machine learning, we integrated attention modules into brain decoders to facilitate an in-depth interpretation of DNN channels. A 4D convolution operation was also included to extract temporo-spatial interaction within the fMRI signal. The experiments showed that the proposed model obtains a very high accuracy (97.4%) and outperforms previous researches on the 7 different task benchmarks from the Human Connectome Project (HCP) dataset. The visualization analysis further illustrated the hierarchical emergence of task-specific masks with depth. Finally, the model was retrained to regress individual traits within the HCP and to classify viewing images from the BOLD5000 dataset, respectively. Transfer learning also achieves good performance. A further visualization analysis shows that, after transfer learning, low-level attention masks remained similar to the source domain, whereas high-level attention masks changed adaptively. In conclusion, the proposed 4D model with attention module performed well and facilitated interpretation of DNNs, which is helpful for subsequent research. ","Attention module improves both performance and interpretability of 4D
  fMRI decoding neural network"
219,1446405119441002510,1322479795662397442,Constantinos Siettos,['In our new preprint we show how one can perform #forecasting of #timeseries with nonlinear #manifoldlearning! Applications in #forex and more! Find out more here <LINK>'],https://arxiv.org/abs/2110.03625,"We address a three-tier numerical framework based on manifold learning for the forecasting of high-dimensional time series. At the first step, we embed the time series into a reduced low-dimensional space using a nonlinear manifold learning algorithm such as Locally Linear Embedding and Diffusion Maps. At the second step, we construct reduced-order regression models on the manifold, in particular Multivariate Autoregressive (MVAR) and Gaussian Process Regression (GPR) models, to forecast the embedded dynamics. At the final step, we lift the embedded time series back to the original high-dimensional space using Radial Basis Functions interpolation and Geometric Harmonics. For our illustrations, we test the forecasting performance of the proposed numerical scheme with four sets of time series: three synthetic stochastic ones resembling EEG signals produced from linear and nonlinear stochastic models with different model orders, and one real-world data set containing daily time series of 10 key foreign exchange rates (FOREX) spanning the time period 03/09/2001-29/10/2020. The forecasting performance of the proposed numerical scheme is assessed using the combinations of manifold learning, modelling and lifting approaches. We also provide a comparison with the Principal Component Analysis algorithm as well as with the naive random walk model and the MVAR and GPR models trained and implemented directly in the high-dimensional space. ",Time Series Forecasting Using Manifold Learning
220,1446391470236254209,1244601028466655232,Hurault Samuel,"['<LINK>\n\nWe propose a new convergent Plug&amp;Play (PnP) scheme with novel theoretical guarantees and state-of-the-art image restoration performance. We train a deep denoiser as an exact gradient descent step on a functional parameterized by a deep neural network. 1/3', 'Once plugged in a PnP framework, the resulting fixed-point algorithm is guaranteed to converge to a stationary point of an explicit functional. This PnP algorithm reaches state-of-the art  on various ill-posed IR tasks like deblurring, super-resolution or inpainting. (2/3)', 'Code available here : https://t.co/dklTNYoJRr']",https://arxiv.org/abs/2110.03220,"Plug-and-Play methods constitute a class of iterative algorithms for imaging problems where regularization is performed by an off-the-shelf denoiser. Although Plug-and-Play methods can lead to tremendous visual performance for various image problems, the few existing convergence guarantees are based on unrealistic (or suboptimal) hypotheses on the denoiser, or limited to strongly convex data terms. In this work, we propose a new type of Plug-and-Play methods, based on half-quadratic splitting, for which the denoiser is realized as a gradient descent step on a functional parameterized by a deep neural network. Exploiting convergence results for proximal gradient descent algorithms in the non-convex setting, we show that the proposed Plug-and-Play algorithm is a convergent iterative scheme that targets stationary points of an explicit global functional. Besides, experiments show that it is possible to learn such a deep denoiser while not compromising the performance in comparison to other state-of-the-art deep denoisers used in Plug-and-Play schemes. We apply our proximal gradient algorithm to various ill-posed inverse problems, e.g. deblurring, super-resolution and inpainting. For all these applications, numerical results empirically confirm the convergence results. Experiments also show that this new algorithm reaches state-of-the-art performance, both quantitatively and qualitatively. ",Gradient Step Denoiser for convergent Plug-and-Play
221,1446306787389362177,28991794,Muktabh Mayank,"['Our recent work at @ParallelDots , where we propose a model to check for Retail POSMs part by part using a model trained on top of descriptors of keypoints matched by superpoint. <LINK>']",https://arxiv.org/abs/2110.03646,"Point of Sale Materials(POSM) are the merchandising and decoration items that are used by companies to communicate product information and offers in retail stores. POSMs are part of companies' retail marketing strategy and are often applied as stylized window displays around retail shelves. In this work, we apply computer vision techniques to the task of verification of POSMs in supermarkets by telling if all desired components of window display are present in a shelf image. We use Convolutional Neural Network based unsupervised keypoint matching as a baseline to verify POSM components and propose a supervised Neural Network based method to enhance the accuracy of baseline by a large margin. We also show that the supervised pipeline is not restricted to the POSM material it is trained on and can generalize. We train and evaluate our model on a private dataset composed of retail shelf images. ","Using Keypoint Matching and Interactive Self Attention Network to verify
  Retail POSMs"
222,1446276490514026498,973404788,Bei Zhou,"['New paper! <LINK> with @ProfJohnBeacom. We study neutrino-induced dimuons, a phenomenon that has only been seen in accelerator neutrino experiments, as a new event class of neutrino telescopes like IceCube, IceCube-Gen2.  @uw_icecube  1/6 <LINK>', '@ProfJohnBeacom The dimuons are mainly from neutrino-nucleus deep-inelastic scatter and W-boson production. We develop a calculational framework and show that for 10 years, IceCube can detect ≃400 dimuons and IceCube-Gen2 can detect ≃1200!   2/6 https://t.co/gYuPDTRGqC', '@ProfJohnBeacom These dimuons have very important physics potentials, including probing high-energy QCD, enabling the first detection of W-boson production (a process that will be very important for high energy neutrinos but has never been identified), and new physics.   3/6 https://t.co/ybjf3usXlZ', '@ProfJohnBeacom More excitingly, we find 19 dimuon candidates from analyzing IceCube public data! We are not totally sure if they are dimuon signals yet due to the reasons detailed in the paper, but all aspects of them match our prediction.    4/6 https://t.co/XdZbGbl2Hz', '@ProfJohnBeacom @uw_icecube Whether they are real dimuons or some new background (or signal!), it’s important to understand them by IceCube collaboration.    5/6', '@ProfJohnBeacom @uw_icecube The continued success of neutrino physics &amp; astrophysics depends on developing new tools to get the most out of the data. Developing new event classes is an important part. Our theory and observation contributions help open a valuable new direction for neutrino telescopes.    6/6']",https://arxiv.org/abs/2110.02974,"Neutrino telescopes allow powerful probes of high-energy astrophysics and particle physics. Their power is increased when they can isolate different event classes, e.g., by flavor, though that is not the only possibility. Here we focus on a new event class for neutrino telescopes: dimuons, two energetic muons from one neutrino interaction. We make new theoretical and observational contributions. For the theoretical part, we calculate dimuon production cross sections and detection prospects via deep-inelastic scattering (DIS; where we greatly improve upon prior work) and $W$-boson production (WBP; where we present first results). We show that IceCube should have $\simeq 400$ dimuons ($\simeq 8$ from WBP) in its current data and that IceCube-Gen2, with a higher threshold but a larger exposure, could detect $\simeq 1200$ dimuons ($\simeq 30$ from WBP) in 10 years. These dimuons are almost all produced by atmospheric neutrinos. For the observational part, we perform a simple but conservative analysis of IceCube public data, finding 19 candidate dimuon events. Subsequent to our paper appearing, visual inspection of these events by the IceCube Collaboration reveals that they are not real dimuons, but instead arise from an internal reconstruction error that identifies some single muons crossing the dust layer as two separate muons. To help IceCube and the broader community with future dimuon searches, we include the updated full details of our analysis. Together, these theoretical and observational contributions help open a valuable new direction for neutrino telescopes, one especially important for probing high-energy QCD and new physics. ","Dimuons in Neutrino Telescopes: New Predictions and First Search in
  IceCube"
223,1446188786203959311,252608121,Yaneer Bar-Yam,"['Modeling complex systems: A case study of compartmental models in epidemiology\n\nwith PK Kollepara, @AlexSiegenfeld \n\nWe ""show how assumptions can constrain model outcomes to a narrow portion of the wide landscape of potential epidemic behaviors""\n\n1/\n\n<LINK>', ""No model accurately captures all the details of the system that it represents, but some models are nonetheless accurate because certain large-scale behaviors don't depend on all these details. The key to good modeling is understanding which details matter and which do not. \n\n2/""]",https://arxiv.org/abs/2110.02947,"Compartmental epidemic models have been widely used for predicting the course of epidemics, from estimating the basic reproduction number to guiding intervention policies. Studies commonly acknowledge these models' assumptions but less often justify their validity in the specific context in which they are being used. Our purpose is not to argue for specific alternatives or modifications to compartmental models, but rather to show how assumptions can constrain model outcomes to a narrow portion of the wide landscape of potential epidemic behaviors. This concrete examination of well-known models also serves to illustrate general principles of modeling that can be applied in other contexts. ","Modeling complex systems: A case study of compartmental models in
  epidemiology"
224,1445921088798560259,1019417352784490503,Mehmet F. Demirel,"['📢 NEW PAPER! We propose a novel attentive GNN called AWARE that uses walk-aggregation (rather than standard K-hop). Our theoretical analysis presents the first provable guarantees of weighted GNNs! Empirical performance on 65 tasks is strong as well!\n<LINK>', 'AWARE is an end-to-end supervised GNN that aggregates info about the walks in the graph using attention schemes on three levels: vertex, walk, and graph. This enables it to emphasize useful info for the downstream task while diminishing harmful/irrelevant ones. https://t.co/CaxGzw5C7C', 'Out of 65 tasks from both molecular property prediction and social network domains, AWARE performs the best in 33, and ranks in top-3 in 53, overall giving better performance than both traditional graph methods and recent GNN variants. https://t.co/dvs5HtqNE2', ""It is also an interpretable algorithm. We show in the paper that the AWARE's walk attention mechanism is able to highlight important substructures in the graph that are important for the downstream prediction task. https://t.co/Wt9SZeyiRy"", 'The code can be found at https://t.co/3Lr5pGhd2f!']",https://arxiv.org/abs/2110.02667,"Graph neural networks (GNNs) have been shown to possess strong representation power, which can be exploited for downstream prediction tasks on graph-structured data, such as molecules and social networks. They typically learn representations by aggregating information from the K-hop neighborhood of individual vertices or from the enumerated walks in the graph. Prior studies have demonstrated the effectiveness of incorporating weighting schemes into GNNs; however, this has been primarily limited to K-hop neighborhood GNNs so far. In this paper, we aim to extensively analyze the effect of incorporating weighting schemes into walk-aggregating GNNs. Towards this objective, we propose a novel GNN model, called AWARE, that aggregates information about the walks in the graph using attention schemes in a principled way to obtain an end-to-end supervised learning method for graph-level prediction tasks. We perform theoretical, empirical, and interpretability analyses of AWARE. Our theoretical analysis provides the first provable guarantees for weighted GNNs, demonstrating how the graph information is encoded in the representation, and how the weighting schemes in AWARE affect the representation and learning performance. We empirically demonstrate the superiority of AWARE over prior baselines in the domains of molecular property prediction (61 tasks) and social networks (4 tasks). Our interpretation study illustrates that AWARE can successfully learn to capture the important substructures of the input graph. ",An Analysis of Attentive Walk-Aggregating Graph Neural Networks
225,1445913649042886656,1418313824109801475,Darryl Zachary Seligman,['Have you ever wondered what the birth of a comet looks like up close? We could send an orbit matching spacecraft from Jupiter to a Centaur as it gets launched into the inner Solar System to find out!  Paper is accepted @AAS_PSJ  and talk @DPSMeeting  <LINK> <LINK>'],https://arxiv.org/abs/2110.02822,"The compositional and morphological evolution of minor bodies in the Solar System is primarily driven by the evolution of their heliocentric distances, as the level of incident solar radiation regulates cometary activity. We investigate the dynamical transfer of Centaurs into the inner Solar System, facilitated by mean motion resonances with Jupiter and Saturn. The recently discovered object, P/2019 LD2, will transition from the Centaur region to the inner Solar System in 2063. In order to contextualize LD2, we perform N-body simulations of a population of Centaurs and JFCs. Objects between Jupiter and Saturn with Tisserand parameter $T_J\sim$3 are transferred onto orbits with perihelia $q<4$au within the next 1000 years with notably high efficiency. Our simulations show that there may be additional LD2-like objects transitioning into the inner Solar System in the near-term future, all of which have low $\Delta$V with respect to Jupiter. We calculate the distribution of orbital elements resulting from a single Jovian encounter and show that objects with initial perihelia close to Jupiter are efficiently scattered to $q<4$au. Moreover, approximately $55\%$ of the transitioning objects in our simulated population experience at least 1 Jovian encounter prior to reaching $q<4$au. We demonstrate that a spacecraft stationed near Jupiter would be well-positioned to rendezvous, orbit match, and accompany LD2 into the inner Solar System, providing an opportunity to observe the onset of intense activity in a pristine comet $\textit{in situ}$. Finally, we discuss the prospect of identifying additional targets for similar measurements with forthcoming observational facilities. ","A Sublime Opportunity: The Dynamics of Transitioning Cometary Bodies and
  the Feasibility of $\textit{In Situ}$ Observations of The Evolution of Their
  Activity"
226,1445794870354870278,3119778197,Hanie Sedghi,"['🆕📰“Exploring the limits of large scale pre-training”\n\n<LINK>\n\nWe systematically study the effect of scaling\nup data, model size and training time in image recognition on a wide range of downstream tasks, pinpoint the limits, the reasons &amp; provide guidelines.\n🧵 <LINK>', 'We establish that scaling doesn’t lead to a one-model-fits-all solution. As US acc. improves, DS acc. saturates to values &lt;&lt; 100%. This gap depends on the relationship between US &amp; DS tasks.+in set of models with similar US accuracy, the best model for different DS tasks varies.', 'We investigate more than 4800 experiments on Vision Transformers,\nMLP-Mixers &amp; ResNets with No. of parameters ranging from ten million to\nten billion, trained on the largest scale of available image data (JFT,\nImageNet21K) &amp; evaluated on &gt; 20 downstream image recognition tasks https://t.co/137zv2J22B', 'We propose a model for downstream performance that reflects the saturation\nphenomena &amp; captures the nonlinear relationship in upstream and downstream performance. The model is fitted to the upper hull of trained networks &amp; is robust to sample size variations &amp; sampling biases.', 'We study how scaling up the model size, data size, and compute affects DS performance and show that these parameters impact DS performance mainly through the US performance. 5/10 https://t.co/UrTy1jqHJv', 'Delving deeper to understand the reasons that give rise to\nthese phenomena, we show that the saturation behavior we observe is closely\nrelated to the way that representations evolve through the layers of the\nModels. 6/10 https://t.co/iJ6FA4Y0nT', 'We showcase an even more extreme scenario where performance on upstream and\ndownstream are at odds with each other: in order to have a better\nDS performance, we need to hurt US accuracy.+the optimal hyper-parameters for the head used in pre-training are different for US and DS. https://t.co/puKEwVkFso', 'The reason behind this discrepancy: by changing head hyper-parameters such as WD &amp; LR, we push the information compressed in the head down to lower layers which leads to performance degradation on US and performance improvement on DS tasks that are related to the US task. 8/10', 'Our observations are robust to several choices: US data size, no. of shots, transfer vs few-shot setting &amp; architecture. We assert that we should make design choices that improve performance on a breadth of DS tasks +when investing on scaling look at an extra axis: data diversity', 'with my awesome collaborators @bneyshabur @samiraabnar @m__dehghani  10/10']",https://arxiv.org/abs/2110.02095,"Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might observe that improvements in pre-training would transfer favorably to most downstream tasks. In this work, we systematically study this phenomena and establish that, as we increase the upstream accuracy, the performance of downstream tasks saturates. In particular, we investigate more than 4800 experiments on Vision Transformers, MLP-Mixers and ResNets with number of parameters ranging from ten million to ten billion, trained on the largest scale of available image data (JFT, ImageNet21K) and evaluated on more than 20 downstream image recognition tasks. We propose a model for downstream performance that reflects the saturation phenomena and captures the nonlinear relationship in performance of upstream and downstream tasks. Delving deeper to understand the reasons that give rise to these phenomena, we show that the saturation behavior we observe is closely related to the way that representations evolve through the layers of the models. We showcase an even more extreme scenario where performance on upstream and downstream are at odds with each other. That is, to have a better downstream performance, we need to hurt upstream accuracy. ",Exploring the Limits of Large Scale Pre-training
227,1445469040944848896,135114281,Ameya Joshi,['Our NeurIPS-2021 paper on Differentiable Spline Approximations is now on @arxiv_org : <LINK>. We propose gradient based optimization for splines and show some really cool applications in 3D reconstruction and PDEs!\n@chomd90 @BabuaSpeaks @baskinscience @adarshk'],https://arxiv.org/abs/2110.01532,"The paradigm of differentiable programming has significantly enhanced the scope of machine learning via the judicious use of gradient-based optimization. However, standard differentiable programming methods (such as autodiff) typically require that the machine learning models be differentiable, limiting their applicability. Our goal in this paper is to use a new, principled approach to extend gradient-based optimization to functions well modeled by splines, which encompass a large family of piecewise polynomial models. We derive the form of the (weak) Jacobian of such functions and show that it exhibits a block-sparse structure that can be computed implicitly and efficiently. Overall, we show that leveraging this redesigned Jacobian in the form of a differentiable ""layer"" in predictive models leads to improved performance in diverse applications such as image segmentation, 3D point cloud reconstruction, and finite element analysis. ",Differentiable Spline Approximations
228,1445310502347984896,2718504998,Francisco Aros,"['Paper day! In this work, we show how the effects of a central IMBH in the binary stars in simulated GCs can help us to find hints of the presence of such massive objects. Follow this thread for a summary of our main results. <LINK>', ""2/ During a GC evolution, binary stars segregate towards the cluster centre, increasing the binary fraction within the GC's core. The presence of an IMBH in the cluster core will halt mass segregation and fewer binaries will populate the centre."", '3/ The binaries that do make it to the centre would likely be destroyed by the IMBH or by 3-4 body interactions with other stars due to the high stellar densities surrounding the IMBH. This produces a depletion of binaries towards the GCs centre (Fig 6). https://t.co/eW6NR8wUEJ', '4/ If we focus on specific regions of the GCs, say the within the core and between one and two half-light, we can separate GCs with a depleted binary population (Fig 7). GC with an IMBH have lower binary fractions (all clusters started with 10% fbin) than those without one. https://t.co/Fzx2hCF4iz', '5/ Binary stars are known to also contaminate the measured line-of-sight velocity dispersion. By comparing the velocity dispersion in the core with and without binary contamination we can also see that clusters with an IMBH separate for those without one (Fig 10) https://t.co/WeLNFDOOCL', '6/ These signatures work as integrated quantities and can help to select good GCs candidates for hosting a central IMBH. Furthermore, they are complementary to more extensive dynamical and observational analysis looking for IMBHs in GCs.']",https://arxiv.org/abs/2110.00590,"The dynamical evolution of globular clusters (GCs) is tied to their binary population, as binaries segregate to the cluster centre, leading to an increased binary fraction in the core. This central overabundance of mainly hard binaries can serve as a source of energy for the cluster and has a significant effect on the observed kinematics, such as artificially increasing the observed line-of-sight velocity dispersion. We analyse the binary fractions and distributions of 95 simulated GCs, with and without an intermediate-mass black hole (IMBH) in their centre. We show that an IMBH will not only halt the segregation of binaries towards the cluster centre, but also, directly and indirectly, disrupt the binaries that segregate, thus depleting binaries in the cluster core. We illustrate this by showing that clusters with an IMBH have fewer binaries and flatter radial binary distributions than their counterparts without one. These differences in the binary fraction and distribution provide an additional indicator for the presence of a central IMBH in GCs. In addition, we analyse the effects of the binary fraction on the line-of-sight velocity dispersion in the simulated GCs and find that binaries can cause an overestimation of up to $70\%$ of the velocity dispersion within the core radius. Using recent VLT/MUSE observations of NGC 3201 by Giesers et al. (2019), we find an overestimation of $32.2\pm7.8\%$ in the velocity dispersion that is consistent with the simulations and illustrates the importance of accurately accounting for the binary population when performing kinematic or dynamical analysis. ","Using Binaries in Globular Clusters to Catch Sight of Intermediate-Mass
  Black Holes"
229,1445013507351584770,2364317023,Dorsa Sadigh,"['Wouldn’t you want to use a slider to help your robot learn your preferences?\n\nWe propose a reward learning algorithm that learns from scaled feedback and outperforms choice feedback comparisons.\n\npaper: <LINK>\n\nw/ N. Wilde, @ebiyik_, S. Smith at @corl_conf <LINK>']",https://arxiv.org/abs/2110.00284,"Today's robots are increasingly interacting with people and need to efficiently learn inexperienced user's preferences. A common framework is to iteratively query the user about which of two presented robot trajectories they prefer. While this minimizes the users effort, a strict choice does not yield any information on how much one trajectory is preferred. We propose scale feedback, where the user utilizes a slider to give more nuanced information. We introduce a probabilistic model on how users would provide feedback and derive a learning framework for the robot. We demonstrate the performance benefit of slider feedback in simulations, and validate our approach in two user studies suggesting that scale feedback enables more effective learning in practice. ",Learning Reward Functions from Scale Feedback
230,1444997609291087873,145986026,Erdem Bıyık,"['My 2nd paper at @corl_conf this year is online: <LINK>. We propose scale feedback as a more nuanced way of providing comparisons. It outperforms standard choice feedback and can be extended with the same active learning techniques, e.g. mutual info optimization. <LINK>', '@corl_conf Code: https://t.co/AelvSYdw8y\nVideo: https://t.co/ojZsQTAGw7\n\nw/ Nils Wilde, @DorsaSadigh, Stephen L. Smith. #CoRL2021']",https://arxiv.org/abs/2110.00284,"Today's robots are increasingly interacting with people and need to efficiently learn inexperienced user's preferences. A common framework is to iteratively query the user about which of two presented robot trajectories they prefer. While this minimizes the users effort, a strict choice does not yield any information on how much one trajectory is preferred. We propose scale feedback, where the user utilizes a slider to give more nuanced information. We introduce a probabilistic model on how users would provide feedback and derive a learning framework for the robot. We demonstrate the performance benefit of slider feedback in simulations, and validate our approach in two user studies suggesting that scale feedback enables more effective learning in practice. ",Learning Reward Functions from Scale Feedback
231,1444966812286865410,1422651883349594117,Bingzhao Zhu,['Happy to share my first paper accepted to #NeurIPS2021. We propose an algorithm to train decision graphs by recursively replacing nodes with micro decision trees. Our model is a more efficient and accurate alternative to decision trees.\n\nLink: <LINK>'],https://arxiv.org/abs/2110.00392,"Decision trees have been widely used as classifiers in many machine learning applications thanks to their lightweight and interpretable decision process. This paper introduces Tree in Tree decision graph (TnT), a framework that extends the conventional decision tree to a more generic and powerful directed acyclic graph. TnT constructs decision graphs by recursively growing decision trees inside the internal or leaf nodes instead of greedy training. The time complexity of TnT is linear to the number of nodes in the graph, and it can construct decision graphs on large datasets. Compared to decision trees, we show that TnT achieves better classification performance with reduced model size, both as a stand-alone classifier and as a base estimator in bagging/AdaBoost ensembles. Our proposed model is a novel, more efficient, and accurate alternative to the widely-used decision trees. ",Tree in Tree: from Decision Trees to Decision Graphs
232,1457332809786294277,16083005,Jing Liu,['Our work latest work on dense retrieval - 🚀RocketQAv2 will be presented at #EMNLP2021! We propose a joint training approach for dense passage retrieval and passage re-ranking. \n\nPaper📄:<LINK>\nCode💾: <LINK>\nVideo🎥: <LINK> <LINK>'],https://arxiv.org/abs/2110.07367,"In various natural language processing tasks, passage retrieval and passage re-ranking are two key procedures in finding and ranking relevant information. Since both the two procedures contribute to the final performance, it is important to jointly optimize them in order to achieve mutual improvement. In this paper, we propose a novel joint training approach for dense passage retrieval and passage re-ranking. A major contribution is that we introduce the dynamic listwise distillation, where we design a unified listwise training approach for both the retriever and the re-ranker. During the dynamic distillation, the retriever and the re-ranker can be adaptively improved according to each other's relevance information. We also propose a hybrid data augmentation strategy to construct diverse training instances for listwise training approach. Extensive experiments show the effectiveness of our approach on both MSMARCO and Natural Questions datasets. Our code is available at this https URL ","RocketQAv2: A Joint Training Method for Dense Passage Retrieval and
  Passage Re-ranking"
233,1448633037558800391,1005395495949406208,Francesco Locatello,"['New paper on learning object-centric video models and a new data set. \n\nThe FISHBOWL data set is an aquarium with multiple fishes with different textures. We propose a new dead-leaf model generating scenes with novel occlusions and interventions. \n\nLink: <LINK> <LINK>', 'Work led by @m_tangemann, w/ @stes_io, @JKugelgen , @pegehler, @ThomasBrox, @matthiaskue, @MatthiasBethge, and @bschoelkopf.\nCollaboration between @awscloud @AmazonScience @MPI_IS @bethgelab @Cambridge_Uni @UniFreiburg @uni_tue  @EPFL_en']",https://arxiv.org/abs/2110.06562,"Learning generative object models from unlabelled videos is a long standing problem and required for causal scene modeling. We decompose this problem into three easier subtasks, and provide candidate solutions for each of them. Inspired by the Common Fate Principle of Gestalt Psychology, we first extract (noisy) masks of moving objects via unsupervised motion segmentation. Second, generative models are trained on the masks of the background and the moving objects, respectively. Third, background and foreground models are combined in a conditional ""dead leaves"" scene model to sample novel scene configurations where occlusions and depth layering arise naturally. To evaluate the individual stages, we introduce the Fishbowl dataset positioned between complex real-world scenes and common object-centric benchmarks of simplistic objects. We show that our approach allows learning generative models that generalize beyond the occlusions present in the input videos, and represent scenes in a modular fashion that allows sampling plausible scenes outside the training distribution by permitting, for instance, object numbers or densities not observed in the training set. ",Unsupervised Object Learning via Common Fate
234,1448329935664279555,2691163094,Devin Incerti,"['External controls are intriguing but bias is a concern. Across 14 old studies, we found that trial controls survived longer---even after PS adjustment---than external controls (HR=0.90).  We propose a meta-analytic framework to adjust for such bias.\n\n<LINK>']",https://arxiv.org/abs/2110.03827,"While randomized controlled trials (RCTs) are the gold standard for estimating treatment effects in medical research, there is increasing use of and interest in using real-world data for drug development. One such use case is the construction of external control arms for evaluation of efficacy in single-arm trials, particularly in cases where randomization is either infeasible or unethical. However, it is well known that treated patients in non-randomized studies may not be comparable to control patients -- on either measured or unmeasured variables -- and that the underlying population differences between the two groups may result in biased treatment effect estimates as well as increased variability in estimation. To address these challenges for analyses of time-to-event outcomes, we developed a meta-analytic framework that uses historical reference studies to adjust a log hazard ratio estimate in a new external control study for its additional bias and variability. The set of historical studies is formed by constructing external control arms for historical RCTs, and a meta-analysis compares the trial controls to the external control arms. Importantly, a prospective external control study can be performed independently of the meta-analysis using standard causal inference techniques for observational data. We illustrate our approach with a simulation study and an empirical example based on reference studies for advanced non-small cell lung cancer. In our empirical analysis, external control patients had lower survival than trial controls (hazard ratio: 0.907), but our methodology is able to correct for this bias. An implementation of our approach is available in the R package ecmeta. ",A meta-analytic framework to adjust for bias in external control studies
235,1447919822042456071,916709762062012416,Guangwei,"['<LINK> New paper! We observed the atmosphere of hot Jupiter WASP-74b and found evidence for aerosols and potentially high C/O ratio! <LINK>', 'Special thanks to @_astronomay for analyzing the Spitzer data and demonstrating the challenges of Spitzer data reduction. And most importantly for matching the pink purple blue color scheme! https://t.co/GXHZEiq2YJ', 'Also thanks to @ExoSing @kevinbstevenson @JDLothringer @StellarPlanet and all other collaborators for the help!']",https://arxiv.org/abs/2110.04415,"Planets are like children with each one being unique and special. A better understanding of their collective properties requires a deeper understanding of each planet. Here we add the transit and eclipse spectra of hot Jupiter WASP-74b into the ever growing dataset of exoplanet atmosphere spectral library. With six transits and three eclipses using the Hubble Space Telescope (HST) and Spitzer Space Telescope (\textit{Spitzer}), we present the most complete and precise atmospheric spectra of WASP-74b. We found no evidence for TiO/VO nor super-Rayleigh scattering reported in previous studies. The transit shows a muted water feature with strong Rayleigh scattering extending into the infrared. The eclipse shows a featureless blackbody-like WFC3/G141 spectrum and a weak methane absorption feature in the Spitzer 3.6 $\mu m$ band. Future James Webb Space Telescope (JWST) follow up observations are needed to confirm these results. ","The Hubble PanCET program: Transit and Eclipse Spectroscopy of the Hot
  Jupiter WASP-74b"
236,1446397412810055684,1389991722202144773,Maria Sofia Bucarelli,"['Check out our paper on arXiv!\nWe propose a generalization of the standard neuron: NEWRON. We discuss a specific example that allows us to interpret the functioning of the network itself.\n<LINK>\nFederico Siciliano, @gtolomei and @fabreetseo']",https://arxiv.org/abs/2110.02775,"In this work, we formulate NEWRON: a generalization of the McCulloch-Pitts neuron structure. This new framework aims to explore additional desirable properties of artificial neurons. We show that some specializations of NEWRON allow the network to be interpretable with no change in their expressiveness. By just inspecting the models produced by our NEWRON-based networks, we can understand the rules governing the task. Extensive experiments show that the quality of the generated models is better than traditional interpretable models and in line or better than standard neural networks. ","NEWRON: A New Generalization of the Artificial Neuron to Enhance the
  Interpretability of Neural Networks"
237,1444872657854287877,954385250754293760,Yuki,['Our accepted paper for IROS 2021 is now on arxiv \nWe propose robust design optimization for multi-finger gripper and verify in the hardware experiment \n\n<LINK>'],https://arxiv.org/abs/2110.00083,"Current rigid linkage grippers are limited in flexibility, and gripper design optimality relies on expertise, experiments, or arbitrary parameters. Our proposed rigid gripper can accommodate irregular and off-center objects through a whippletree mechanism, improving adaptability. We present a whippletree-based rigid under-actuated gripper and its parametric design multi-objective optimization for a one-wall climbing task. Our proposed objective function considers kinematics and grasping forces simultaneously with a mathematical metric based on a model of an object environment. Our multi-objective problem is formulated as a single kinematic objective function with auto-tuning force-based weight. Our results indicate that our proposed objective function determines optimal parameters and kinematic ranges for our under-actuated gripper in the task environment with sufficient grasping forces. ","An Under-Actuated Whippletree Mechanism Gripper based on Multi-Objective
  Design Optimization with Auto-Tuned Weights"
