,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1493297588459618304,2210861,Jacob Andreas,"['How do we get more human-like generalization in the age of big NN sequence models? New paper by @akyurekekin shows how to formulate compositionality as a constraint on data distributions, offering model-agnostic tools for modeling systems like language.\n\n<LINK>', 'I\'ve always been jealous of colleagues who work on data generated by well-understood physical laws and get to use fancy words like ""equivariance"" when reasoning about the properties they want their models to satisfy.', 'The principle of compositionality is not a physical law, but it might be the closest thing we have for reasoning about how language works.', 'And like in physical systems, the assumption that language is compositional implies that data distributions associated with NLP tasks must satisfy certain symmetries.\n\nWe can identify these symmetries automatically---sometimes even without a precise understanding of our tasks!---', 'and translate them into model agnostic tools for controlling model generalization.']",https://arxiv.org/abs/2201.12926,"Standard deep network models lack the inductive biases needed to generalize compositionally in tasks like semantic parsing, translation, and question answering. A large body of work in natural language processing seeks to overcome this limitation with new model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general framework for compositional modeling that instead formulates compositionality as a constraint on data distributions. We prove that for any task factorizable into a lexicon and a composition function, there exists a family of data transformation functions that are guaranteed to produce new, well-formed examples when applied to training data. We further show that it is possible to identify these data transformations even when the composition function is unknown (e.g. when we do not know how to write or infer a symbolic grammar). Using these transformation functions to perform data augmentation for ordinary RNN and transformer sequence models, we obtain state-of-the-art results on the CLEVR-CoGenT visual question answering dataset, and results comparable to specialized model architectures on the COGS semantic parsing dataset. ",Compositionality as Lexical Symmetry
1,1491000370033745924,1230320689250553856,Christian Malacaria,"[""I'm so proud of our new paper now accepted on #ApJ!\n\nA fleet of X-ray telescopes #NICER @NuSTAR_Science @NASASwift @AstroSat3 in collaboration w/ #XMAG colleagues @BIGfalke @pragati2707 et many al.!\n\nCome&amp;see how it feels when it's #accretingontheedge:\n\n<LINK> <LINK>"", '#XMAG: @ColleenWilsonH @LapaSokolova @oznerold']",https://arxiv.org/abs/2201.11376,"Accreting X-ray pulsars (XRPs) undergo luminous X-ray outbursts during which the luminosity-dependent spectral and timing features of the neutron star's emission can be analyzed in detail, thus shedding light on the accretion regime at work. We took advantage of a monitoring campaign performed with NuSTAR, Swift/XRT, AstroSat and NICER, to follow the Be/X-ray Binary 2S 1553-542 along one of its rare outbursts and trace its spectral and timing evolution. We report the discovery of a luminosity-dependent cyclotron line energy for the first time in this source. The pulse profiles and pulsed fraction also show variability along the outburst, consistently with the interpretation that the source transitions from the sub-critical to the super-critical accretion regime, separated by a critical luminosity of L$_{crit}\approx4\times10^{37}$ erg/s. ","Accreting on the edge: a luminosity-dependent cyclotron line in the
  Be/X-ray Binary 2S 1553-542 accompanied by accretion regimes transition"
2,1488850789372243968,206818334,Ivan Oseledets,['We present the new approximate backward functions  which reduce the memory for activations. Interesting math behind! \nPaper about activation functions:\n<LINK>\nAbout linear layers: \n<LINK>\nСode:\n<LINK>'],https://arxiv.org/abs/2202.00441,"Memory footprint is one of the main limiting factors for large neural network training. In backpropagation, one needs to store the input to each operation in the computational graph. Every modern neural network model has quite a few pointwise nonlinearities in its architecture, and such operation induces additional memory costs which -- as we show -- can be significantly reduced by quantization of the gradients. We propose a systematic approach to compute optimal quantization of the retained gradients of the pointwise nonlinear functions with only a few bits per each element. We show that such approximation can be achieved by computing optimal piecewise-constant approximation of the derivative of the activation function, which can be done by dynamic programming. The drop-in replacements are implemented for all popular nonlinearities and can be used in any existing pipeline. We confirm the memory reduction and the same convergence on several open benchmarks. ","Few-Bit Backward: Quantized Gradients of Activation Functions for Memory
  Footprint Reduction"
3,1488759813571891201,1237312746,Rosanne Liu,"['In case you ""bookmarked"" our paper but haven\'t actually read it, over the lunar new year\'s we dropped in a much better version with, among other things, a TL;DR of sections 🤓\n<LINK> <LINK> <LINK>']",https://arxiv.org/abs/2201.05610,"How do neural network image classifiers respond to simpler and simpler inputs? And what do such responses reveal about the learning process? To answer these questions, we need a clear measure of input simplicity (or inversely, complexity), an optimization objective that correlates with simplification, and a framework to incorporate such objective into training and inference. Lastly we need a variety of testbeds to experiment and evaluate the impact of such simplification on learning. In this work, we measure simplicity with the encoding bit size given by a pretrained generative model, and minimize the bit size to simplify inputs in training and inference. We investigate the effect of such simplification in several scenarios: conventional training, dataset condensation and post-hoc explanations. In all settings, inputs are simplified along with the original classification task, and we investigate the trade-off between input simplicity and task performance. For images with injected distractors, such simplification naturally removes superfluous information. For dataset condensation, we find that inputs can be simplified with almost no accuracy degradation. When used in post-hoc explanation, our learning-based simplification approach offers a valuable new tool to explore the basis of network decisions. ",When less is more: Simplifying inputs aids neural network understanding
4,1488600719414345735,31147249,Herbert Roitblat,['My new paper on FOMO (fear of missing out) in eDiscovery has been posted on ARXIV. You might find it interesting.  <LINK>.  #ediscovery #informationretrieval'],http://arxiv.org/abs/2201.12376,"In eDiscovery, a party to a lawsuit or similar action must search through available information to identify those documents and files that are relevant to the suit. Search efforts tend to identify less than 100% of the relevant documents and courts are frequently asked to adjudicate whether the search effort has been reasonable, or whether additional effort to find more of the relevant documents is justified. This article provides a method for estimating the probability that significant additional information will be found from extended effort. Modeling and two data sets indicate that the probability that facts/topics exist among the so-far unidentified documents that have not been observed in the identified documents is low for even moderate levels of Recall. ",Probably Reasonable Search in eDiscovery
5,1488599586767835144,223923566,David Van Horn,"[""New paper: A Formal Model of Checked C, to appear at CSF'22. <LINK>""]",https://arxiv.org/abs/2201.13394,"We present a formal model of Checked C, a dialect of C that aims to enforce spatial memory safety. Our model pays particular attention to the semantics of dynamically sized, potentially null-terminated arrays. We formalize this model in Coq, and prove that any spatial memory safety errors can be blamed on portions of the program labeled unchecked; this is a Checked C feature that supports incremental porting and backward compatibility. While our model's operational semantics uses annotated (""fat"") pointers to enforce spatial safety, we show that such annotations can be safely erased: Using PLT Redex we formalize an executable version of our model and a compilation procedure from it to an untyped C-like language, and use randomized testing to validate that generated code faithfully simulates the original. Finally, we develop a custom random generator for well-typed and almost-well-typed terms in our Redex model, and use it to search for inconsistencies between our model and the Clang Checked C implementation. We find these steps to be a useful way to co-develop a language (Checked C is still in development) and a core model of it. ",A Formal Model of Checked C
6,1488564053983301647,1139101764786053121,Matthew Orkney,"['New EDGE paper on the arXiv today! I investigate what it takes to explain the unusual properties of the star cluster in Eridanus II, focussing on its high ellipticity. Also see the linked movies on the EDGE homepage!\n<LINK>\n<LINK>']",https://arxiv.org/abs/2201.13434,"The Eridanus II (EriII) 'ultra-faint' dwarf has a large ($15\,\text{pc}$) and low mass ($4.3\times10^3\,\text{M}_\odot$) star cluster (SC) offset from its centre by $23\pm3\,\text{pc}$ in projection. Its size and offset are naturally explained if EriII has a central dark matter core, but such a core may be challenging to explain in a $\Lambda$CDM cosmology. In this paper, we revisit the survival and evolution of EriII's SC, focussing for the first time on its puzzingly large ellipticity ($0.31^{+0.05}_{-0.06}$). We perform a suite of 960 direct $N$-body simulations of SCs, orbiting within a range of spherical background potentials fit to ultra-faint dwarf galaxy simulations. We find only two scenarios that come close to explaining EriII's SC. In the first, EriII has a low density dark matter core (of size $\sim70\,\text{pc}$ and density $\lesssim2\times10^8\,\text{M}_{\odot}\,\text{kpc}^{-3}$). In this model, the high ellipticity of EriII's SC is set at birth, with the lack of tidal forces in the core allowing its ellipticity to remain frozen in for long times. In the second, EriII's SC orbits in a partial core, with its high ellipticity owing to its imminent tidal destruction. However, this latter model predicts more substantial tidal tails around EriII's SC that should have already been seen in the data, leading us to favour the cored model. We discuss potential caveats to these findings, and the implications of the cored model for galaxy formation and the nature of dark matter. ","EDGE: the puzzling ellipticity of Eridanus II's star cluster and its
  implications for dark matter at the heart of an ultra-faint dwarf"
7,1488421960824537089,948528424926220288,Johannes Røsok Eskilt,"[""Excited that my new paper is out! <LINK> I'm extremely grateful for the help I've received from so many people! In the paper, I look at the frequency dependence of the cosmic birefringence signal found in Planck data, and I include the LFI for the first time"", 'I sample the birefringence angle individually for each frequency channel, and I sample a power-law model. I get that the signal is very consistent with being frequency-independent, which rules out Faraday rotation as the cause of the signal https://t.co/ZvA485jHfY', 'But more work needs to be done to understand the physics of the polarized foreground emission, both dust and synchrotron, before we can know the statistical significance of the measurements']",https://arxiv.org/abs/2201.13347,"We present new constraints on the frequency dependence of the cosmic birefringence angle from the Planck data release 4 polarization maps. An axion field coupled to electromagnetism predicts a nearly frequency-independent birefringence angle, $\beta_\nu = \beta$, while Faraday rotation from local magnetic fields and Lorentz violating theories predict a cosmic birefringence angle that is proportional to the frequency, $\nu$, to the power of some integer $n$, $\beta_\nu \propto \nu^n$. In this work, we first sample $\beta_\nu$ individually for each polarized HFI frequency band in addition to the 70 GHz channel from the LFI. We also constrain a power-law formula for the birefringence angle, $\beta_\nu=\beta_0(\nu/\nu_0)^n$, with $\nu_0 = 150$ GHz. For a nearly full-sky measurement, $f_{\text{sky}}=0.93$, we find $\beta_0 = 0.26^{\circ}\pm0.11^\circ$ $(68\% \text{ C.L.})$ and $n=-0.45^{+0.61}_{-0.82}$ when we ignore the intrinsic $EB$ correlations of the polarized foreground emission, and $\beta_0 = 0.33^\circ \pm 0.12^\circ$ and $n=-0.37^{+0.49}_{-0.64}$ when we use a filamentary dust model for the foreground $EB$. Next, we use all the polarized Planck maps, including the 30 and 44 GHz frequency bands. These bands have a negligible foreground contribution from polarized dust emission. We, therefore, treat them separately. Without any modeling of the intrinsic $EB$ of the foreground, we generally find that the inclusion of the 30 and 44 GHz frequency bands raises the measured values of $\beta_\nu$ and tightens $n$. At nearly full-sky, we measure $\beta_0=0.29^{\circ+0.10^\circ}_{\phantom{\circ}-0.11^\circ}$ and $n=-0.35^{+0.48}_{-0.47}$. Assuming no frequency dependence, we measure $\beta=0.33^\circ \pm 0.10^\circ$. If our measurements have effectively mitigated the $EB$ of the foreground, our constraints are consistent with a mostly frequency-independent signal of cosmic birefringence. ","Frequency-Dependent Constraints on Cosmic Birefringence from the LFI and
  HFI Planck Data Release 4"
8,1488340817211240449,1191056593476915200,Ray Bai,"['1/2 It\'s been awhile since I\'ve written a purely theoretical stats paper. But here one is! New preprint, ""On the proof of posterior contraction for sparse generalized linear models with multivariate responses"" (with Shao-Hsuan Wang and Hsin-Hsiung Huang): <LINK>', '2/2 We derive new theoretical results for Bayesian generalized linear models  (GLMs) with multivariate, correlated responses. Though we focused on theory in this paper, we are hard at work on a 100% applied companion paper featuring a cool application of our method! Stay tuned!']",https://arxiv.org/abs/2201.12839,"In recent years, the literature on Bayesian high-dimensional variable selection has rapidly grown. It is increasingly important to understand whether these Bayesian methods can consistently estimate the model parameters. To this end, shrinkage priors are useful for identifying relevant signals in high-dimensional data. For multivariate linear regression models with Gaussian response variables, Bai and Ghosh (2018) proposed a multivariate Bayesian model with shrinkage priors (MBSP) for estimation and variable selection in high-dimensional settings. However, the proofs of posterior consistency for the MBSP method (Theorems 3 and 4 of Bai and Ghosh (2018) were incorrect. In this paper, we provide a corrected proof of Theorems 3 and 4 of Bai and Ghosh (2018). We leverage these new proofs to extend the MBSP model to multivariate generalized linear models (GLMs). Under our proposed model (MBSP-GLM), multiple responses belonging to the exponential family are simultaneously modeled and mixed-type responses are allowed. We show that the MBSP-GLM model achieves strong posterior consistency when $p$ grows at a subexponential rate with $n$. Furthermore, we quantify the posterior contraction rate at which the posterior shrinks around the true regression coefficients and allow the dimension of the responses $q$ to grow as $n$ grows. Thus, we strengthen the previous results on posterior consistency, which did not provide rate results. This greatly expands the scope of the MBSP model to include response variables of many data types, including binary and count data. To the best of our knowledge, this is the first posterior contraction result for multivariate Bayesian GLMs. ","On the proof of posterior contraction for sparse generalized linear
  models with multivariate responses"
9,1488333631177166851,1462264773450481664,Christopher Chamberland,['Glad to see our SMT paper appear on the arXiv with a really great summer intern Noah Shutty. New systematic methods to construct fault-tolerant circuits <LINK>.'],https://arxiv.org/abs/2201.12450,"Universal fault-tolerant quantum computers will require the use of efficient protocols to implement encoded operations necessary in the execution of algorithms. In this work, we show how satisfiability modulo theories (SMT) solvers can be used to automate the construction of Clifford circuits with certain fault-tolerance properties and apply our techniques to a fault-tolerant magic state preparation protocol. Part of the protocol requires converting magic states encoded in the color code to magic states encoded in the surface code. Since the teleportation step involves decoding a color code merged with a surface code, we develop a new decoding algorithm applicable to such codes. ","Finding fault-tolerant Clifford circuits using satisfiability modulo
  theories solvers and decoding merged color-surface codes"
10,1488218563894730752,1003652696723873792,Max Gaspari,"['New exciting paper on probing multiphase gas condensation in non-central galaxies with SOFIA, ALMA, MUSE, VLA observations and simulations. The #BlackHoleWeather develops also here but in a more compressed core, rather than an extended rain!\n<LINK>\n#astrophysics <LINK>', 'Some key numerical predictions and comparison with the #ChaoticColdAccretion (CCA) theory. Most of the cold/warm clouds (left) in non-central galaxies can be consistently explained via CCA (internal condensation), with a nuclear rain traced via the C-ratio (right).\n#astronomy https://t.co/9HLhi3Wt5F']",https://arxiv.org/abs/2201.09330,"We investigate the cold and warm gas content, kinematics, and spatial distribution of six local massive elliptical galaxies to probe the origin of the multiphase gas in their atmospheres. We report new observations, including SOFIA [CII], ALMA CO, MUSE H$\alpha$+[NII] and VLA radio observations. These are complemented by a large suite of multiwavelength archival datasets, including thermodynamical properties of the hot gas and radio jets, which are leveraged to investigate the role of AGN feeding/feedback in regulating the multiphase gas content. Our galaxy sample shows a significant diversity in cool gas content, spanning filamentary and rotating structures. In our non-central galaxies, the distribution of such gas is often concentrated, at variance with the more extended features observed in central galaxies. Misalignment between the multiphase gas and stars suggest that stellar mass loss is not the primary driver. A fraction of the cool gas might be acquired via galaxy interactions, but we do not find quantitative evidence of mergers in most of our systems. Instead, key evidence supports the origin via condensation out of the diffuse halo. Comparing with Chaotic Cold Accretion (CCA) simulations, we find that our cool gas-free galaxies are likely in the overheated phase of the self-regulated AGN cycle, while for our galaxies with cool gas the k-plot and AGN power correlation corroborate the phase of CCA feeding in which the condensation rain is triggering more vigorous AGN heating. The related C-ratio further shows that central/non-central galaxies are expected to generate an extended/inner rain, consistent with our sample. ","Probing multiphase gas in local massive elliptical galaxies via
  multiwavelength observations"
11,1488210344874954752,865393920,Dr. Renee Ludlam,"['New paper alert! 📢\n\n""Radius Constraint from Reflection Modeling of Cygnus X-2 with NuSTAR [@NuSTAR_Science] and NICER [@keithgendreau]""\n\n<LINK> <LINK>', 'We analyze the spectral properties of the neutron star (NS) low-mass X-ray binary Cygnus X-2 in various intensity states. The system is interesting since there is an existing mass (M) estimate of the NS, hence a radius (R) estimate provides information on the NS M-R plane. https://t.co/Zd4hS84H0Q', ""The data obtained with NuSTAR and NICER show a beautiful, prominent iron line that is broadened due to various effects in the innermost accretion disk that become stronger closer to the NS. Therefore, determining the disk's position provides an upper limit on the NS's radius. https://t.co/1HJl6CALVG"", 'We obtain a conservative upper limit on the NS radius and compare to other current state-of-the-art methods. Each of these provide an independent check of the allowed region on the M-R plane when trying to collectively determine the state of the matter that exists within NSs. https://t.co/KHIUU4bg5y']",https://arxiv.org/abs/2201.11767,"We present a spectral analysis of NuSTAR and NICER observations of the luminous, persistently accreting neutron star (NS) low-mass X-ray binary Cygnus X-2. The data were divided into different branches that the source traces out on the Z-track of the X-ray color-color diagram; namely the horizontal branch, normal branch, and the vertex between the two. The X-ray continuum spectrum was modeled in two different ways that produced a comparable quality fit. The spectra showed clear evidence of a reflection component in the form of a broadened Fe K line, as well as a lower energy emission feature near 1 keV likely due to an ionized plasma located far from the innermost accretion disk. We account for the reflection spectrum with two independent models (relxillns and rdblur*rfxconv). The inferred inclination is in agreement with earlier estimates from optical observations of ellipsoidal light curve modeling (relxillns: $i=67^{\circ}\pm4^{\circ}$, rdblur*rfxconv: $i=60^{\circ}\pm10^{\circ}$). The inner disk radius remains close to the NS ($R_{\rm in}\leq1.15\ R_{\mathrm{ISCO}}$) regardless of the source position along the Z-track or how the 1 keV feature is modeled. Given the optically determined NS mass of $1.71\pm0.21\ M_{\odot}$, this corresponds to a conservative upper limit of $R_{\rm in}\leq19.5$ km for $M=1.92\ M_{\odot}$ or $R_{\rm in}\leq15.3$ km for $M=1.5\ M_{\odot}$. We compare these radius constraints to those obtained from NS gravitational wave merger events and recent NICER pulsar light curve modeling measurements. ","Radius Constraints from Reflection Modeling of Cygnus X-2 with NuSTAR
  and NICER"
12,1488197709400100864,334470578,Chris J. Maddison,"['What’s the best representation for covariate shift? Our new ICLR paper gives simple optimality conditions; uses them to derive new SSL objectives and give insight into augmentation design.\n\n<LINK>\n<LINK>\n\n1st auths @YangjunR, @yanndubs', 'We give the first characterization of necessary and sufficient conditions for optimal reps. under covariate shift in idealized settings. \n\nConditions are natural: you want the rep. that is both discriminative for downstream tasks and has invariant support across all domains. https://t.co/6H7u6NLVqf', 'We also show that if target label info. is not available, then you cannot learn useful representations. This may explain the failure of current domain generalization (DG) methods. https://t.co/woLhyoxEIp', 'So what can we do? Self-supervised learning (SSL) can help! We derive practical SSL objectives that combine classic ideas (InfoNCE) with new domain bottlenecks to learn optimal reps.', 'Key reqs: data must cover potential domains and augmentations must maintain task info. while removing domain info. If you treat image -&gt; alt-text as an augmentation, we think these reqs. are satisfied by CLIP but not SimCLR (may explain CLIP’s superior robustness). https://t.co/6Gmpu42Pln', 'Experiments: As our theory suggests, CLIP significantly outperforms previous DG methods and finetuning CLIP with our CAD bottleneck achieves even better results! https://t.co/k3No528cx2', ""Caveats: the analysis is v. idealized. It's a first pass on opt. conditions and the design of augmentations for robust reps. Lots left to explore.\n\nTake-home: to solve covariate shift, we should collect more data. Augmentations (if cheaper than labels) are one way."", '@ElanRosenfeld Sadly this is a bit of a complex question. Our setup is two-stage: rep. learning then linear readout (re-train linear classifier w/ source data). We did oracle valid. for rep. learning, but source valid. for linear readout. The reason: we were asking which reps. are optimal...', '@ElanRosenfeld and optimal reps need some target label information. I agree that this makes it harder to compare to existing methods that do not use oracle selection, but frankly, the theory is ex. pessimistic w/o target labels (and I think most experiments in that regimes are usually noise)...', '@ElanRosenfeld The experiment that would have resolved this would be to train the representation on CLIP data that is labelled with a domain. Then we could have avoided the issues faced in the rep. learning stage. We wanted to do this (and tried approximations), but CLIP data is not available.', ""@ElanRosenfeld Thanks for the feedback! We're very happy to clarify this statement in the abstract to soften it."", ""@ElanRosenfeld You can reproduce our reps. with our released code. It's fast, we finetune a small MLP on top of CLIP to get the new rep. Also, for transparency, we found source selection for the rep. to not work well, which is consistent with our (and others) claim that target info is needed."", ""@ElanRosenfeld There's more on this choice in our appendix, page 38 of arxiv version."", ""@unsorsodicorda For a rep. to bound the joint risk (lambda) it needs some kind of target label info. and to ensure bounds on the A-distance it will need some kind of target input info. Shai's impossib. paper shows that each alone is not sufficient. Both consistent with our conclusion."", ""@unsorsodicorda That sounds right. In other words, you got lucky. No free lunch in our case doesn't say that you always do poorly. It says, if you can do well in some cases, then you have to do poorly in others.""]",http://arxiv.org/abs/2201.00057,"Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate shifts. Our objective has two components. First, a representation must remain discriminative for the task, i.e., some predictor must be able to simultaneously minimize the source and target risk. Second, the representation's marginal support needs to be the same across source and target. We make this practical by designing self-supervised objectives that only use unlabelled data and augmentations to train robust representations. Our objectives give insights into the robustness of CLIP, and further improve CLIP's representations to achieve SOTA results on DomainBed. ",Optimal Representations for Covariate Shift
13,1488193767685300225,755924666,Brant Robertson,"['New paper on the arXiv by @ucsc Comp Astro Research Group PhD student Ryan Hausen, ""FitsMap: A Simple, Lightweight Tool For Displaying Interactive Astronomical Image and Catalog Data""! <LINK>', '@BenneHolwerda @ucsc DS9 is great for some things and not others :) I use it all the time. FitsMap is great for sharing images and catalogs with collaborators from a web server :)']",https://arxiv.org/abs/2201.12308,"The visual inspection of image and catalog data continues to be a valuable aspect of astronomical data analysis. As the scale of astronomical image and catalog data continues to grow, visualizing the data becomes increasingly difficult. In this work, we introduce FitsMap, a simple, lightweight tool for visualizing astronomical image and catalog data. FitsMap only requires a simple web server and can scale to over gigapixel images with tens of millions of sources. Further, the web-based visualizations can be viewed performantly on mobile devices. FitsMap is implemented in Python and is open source (this https URL). ","FitsMap: A Simple, Lightweight Tool For Displaying Interactive
  Astronomical Image and Catalog Data"
14,1488147237142212608,1101220947607146497,Alon Jacovi,"['What does it mean for a user to ""understand"" the explanation of an AI\'s decision?\n\nNew paper 🙌 \n<LINK>\nDiagnosing AI Explanation Methods with Folk Concepts of Behavior\nw\\ @BastingsJasmijn @sebgehr @yoavgo @fajtak \n\nDigest:🧵 <LINK>', 'When a person interacts with an explainable AI system and says ""Ok, now I understand this AI\'s decision"" - we want to understand how this happened.\n\nLet\'s start with the ""outcome"" of the explanation (red arrow ^): the explainee\'s mental model about the decision &amp; what caused it', 'We borrow from the fields of folk-psychology and theory of mind, and present a framework based on *coherence*, meaning lack of contradictions with the explanation and other decisions.\n\nTo say that an explanation succeeded, it means that the explainee has a coherent mental model. https://t.co/i7N6VjXnG4', 'We go much deeper in the paper, but a few important takeaways:\n1. There\'s no such thing as explaining ""one decision"". Explanation requires a *generalizing* mental model.\n2. Coherence is empirical, not theoretical (evidence-based, via contradictions).', 'Next, the mental model is actor-centric. This means perceiving actors that are capable of holding internal representations, and act on them.\n\nAll of these are important parts of the ""story"": if one is missing, the user will assume it! https://t.co/BedLu1zyD5', 'For example, here we explain that the inner representation (intentionality) includes recognizing ""whiskers"". But we don\'t say *what* in the image caused this representation. The user might assume that the whiskers in the image are to blame, even if they aren\'t. https://t.co/VNEgTtwJoz', 'And the same issue happens if we only explain ""what in the image"", but not what it represents to the model.\n\nThis is just an example of a class of contradictions we can derive from the actor-centric framework. https://t.co/VEw6hr8iVq', 'Much more in the paper of course :) please reach out for any feedback/questions!', ""@nsaphra @BastingsJasmijn @sebgehr @yoavgo @fajtak Not sure I 100% follow, but if you mean why a specific metric becomes trusted in our discipline, then I'd probably look at institutionalization rather than psychology (AI as an institution + role of benchmarking in the institution's structure), but I'm not too informed there..."", '@nsaphra @BastingsJasmijn @sebgehr @yoavgo @fajtak sources: https://t.co/d7wGp0xoiT\nwhich is e.g. used by \nhttps://t.co/tUaQ9y5it3\nto look at AI as an institution (with the role of trust in that institution) \nexcerpts are from the latter https://t.co/Vd5MsXoInr', 'adding my other thread with more impressions: https://t.co/ssuOugJ96u']",https://arxiv.org/abs/2201.11239,"When explaining AI behavior to humans, how is the communicated information being comprehended by the human explainee, and does it match what the explanation attempted to communicate? When can we say that an explanation is explaining something? We aim to provide an answer by leveraging theory of mind literature about the folk concepts that humans use to understand behavior. We establish a framework of social attribution by the human explainee, which describes the function of explanations: the concrete information that humans comprehend from them. Specifically, effective explanations should be coherent (communicate information which generalizes to other contrast cases), complete (communicating an explicit contrast case, objective causes, and subjective causes), and interactive (surfacing and resolving contradictions to the generalization property through iterations). We demonstrate that many XAI mechanisms can be mapped to folk concepts of behavior. This allows us to uncover their modes of failure that prevent current methods from explaining effectively, and what is necessary to enable coherent explanations. ",Diagnosing AI Explanation Methods with Folk Concepts of Behavior
15,1488131801906581507,747212842470891520,Shane Gu,"['Can pre-trained language models be used for offline RL? We look to answer this question in our new work and demonstrate SoTA-level performance on various offline RL benchmarks when adapting pre-trained LMs for RL 🤯\n\npaper: <LINK>\ncode: <LINK> 1/ <LINK>', 'We look at adapting pre-trained language models (e.g. GPT2) and image models (e.g. ImageGPT) for Decision Transformer in offline RL and show consistent improvement in performance over all strong baselines, e.g.. DT, TD3+BC, CQL: 2/ https://t.co/1PwlKvwPbY', 'Interestingly, we find that vision init does not converge, whereas even a small pre-trained language model ChibiT (where チビ means small or mini in Japanese 😆) on Wiki has improvements over DT and comparable to GPT2. Perhaps some similarities in RL trajectories &amp; language 🤔 3/', 'Also we find, that initializing with pre-trained language models improves convergence time by 3-6x, speeding up training significantly: 4/ https://t.co/LobtDPeTC9', 'We also find that LM initialization help scaling too, allowing you to use MUCH LARGER models (e.g. GPT-x) for offline RL. Perhaps it’s the best way to apply transformers for bigger offline RL benchmarks of the future: 5/ https://t.co/fmMhcyM6QV', 'We hope this work initiates more work on pre-training for offline RL as well as interest in more generic sequence modeling techniques that extend across various domains (e.g. NLP and RL).\nIf anyone has any ideas or suggestions let us know! 6/', ""This is in collaboration with @machelreid @_yutaroyamada. It's great fun to work on an interdisciplinary project with researchers outside your field, learning/complaining about each other's field :) Looking forward to more of such collaboration! 7/"", ""@ayyar First in NLP there are tons of transfer across tasks (@machelreid :) ). Then we were also inspired by frozen transformer (https://t.co/UnEFufbIg7, tho it was nice to find that freezing wasn't sufficient in this generative case!) And of course, Solomonoff induction :)""]",https://arxiv.org/abs/2201.12122,"Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains. ",Can Wikipedia Help Offline Reinforcement Learning?
16,1488097190249648129,27158046,Dr Sarah McIntyre,"['We have a new preprint, a review/opinion paper, ""Naturalistic stimuli in touch research"". Feedback welcome! <LINK>']",https://arxiv.org/abs/2201.11868,"Neural mechanisms of touch are typically studied in laboratory settings using robotic or other types of well-controlled devices. Such stimuli are very different from highly complex naturalistic human-to-human touch interactions. The lack of scientifically useful naturalistic stimuli hampers progress, particularly in social touch research. Vision science, on the other hand, has benefitted from inventions such as virtual reality systems that have provided researchers with precision control of naturalistic stimuli. In the field of touch research, producing and manipulating stimuli is particularly challenging due to the complexity of skin mechanics. Here we review the history of touch neuroscience focusing on the contrast between strictly controlled and naturalistic stimuli and compare with vision science. We discuss new methods that may overcome the obstacles with precision-controlled tactile stimuli, and recent successes in naturalistic texture production. In social touch research, precise tracking and measurement of naturalistic human-to-human touch interactions offers exciting new possibilities. ",Naturalistic stimuli in touch research
17,1488077423996461059,948868654447415296,Christina Giannoula,"['Check out our new paper: ""SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Systems"".\n\nStay tuned for further updates on this work and the source code release.\n<LINK>\n\n@_onurmutlu_  @SAFARI_ETH_CMU  @cslab_ntua']",https://arxiv.org/abs/2201.05072,"Several manufacturers have already started to commercialize near-bank Processing-In-Memory (PIM) architectures. Near-bank PIM architectures place simple cores close to DRAM banks and can yield significant performance and energy improvements in parallel applications by alleviating data access costs. Real PIM systems can provide high levels of parallelism, large aggregate memory bandwidth and low memory access latency, thereby being a good fit to accelerate the widely-used, memory-bound Sparse Matrix Vector Multiplication (SpMV) kernel. This paper provides the first comprehensive analysis of SpMV on a real-world PIM architecture, and presents SparseP, the first SpMV library for real PIM architectures. We make three key contributions. First, we implement a wide variety of software strategies on SpMV for a multithreaded PIM core and characterize the computational limits of a single multithreaded PIM core. Second, we design various load balancing schemes across multiple PIM cores, and two types of data partitioning techniques to execute SpMV on thousands of PIM cores: (1) 1D-partitioned kernels to perform the complete SpMV computation only using PIM cores, and (2) 2D-partitioned kernels to strive a balance between computation and data transfer costs to PIM-enabled memory. Third, we compare SpMV execution on a real-world PIM system with 2528 PIM cores to state-of-the-art CPU and GPU systems to study the performance and energy efficiency of various devices. SparseP software package provides 25 SpMV kernels for real PIM systems supporting the four most widely used compressed matrix formats, and a wide range of data types. Our extensive evaluation provides new insights and recommendations for software designers and hardware architects to efficiently accelerate SpMV on real PIM systems. ","SparseP: Towards Efficient Sparse Matrix Vector Multiplication on Real
  Processing-In-Memory Systems"
18,1487974609962123265,980555624663715840,Ryan LaRose,"['New paper analyzing zero-noise extrapolation in the presence of time-correlated noise, including a recommendation for how to scale noise so the final error-mitigated result is more accurate.\n\nCollaboration with @JHUAPL @msuqis @unitaryfund\n\n<LINK>']",https://arxiv.org/abs/2201.11792,"Zero-noise extrapolation is a quantum error mitigation technique that has typically been studied under the ideal approximation that the noise acting on a quantum device is not time-correlated. In this work, we investigate the feasibility and performance of zero-noise extrapolation in the presence of time-correlated noise. We show that, in contrast to white noise, time-correlated noise is harder to mitigate via zero-noise extrapolation because it is difficult to scale the noise level without also modifying its spectral distribution. This limitation is particularly strong if ""local"" gate-level methods are applied for noise scaling. However, we find that ""global"" noise scaling methods, e.g., global unitary folding, can be sufficiently reliable even in the presence of time-correlated noise. We also introduce gate Trotterization as a new noise scaling technique that may be of independent interest. ",Reducing the impact of time-correlated noise on zero-noise extrapolation
19,1487451884701073411,801793180899360768,Hendrik Schuff,"['Check out our new paper! (w/ @alon_jacovi, Heike Adel, @yoavgo and Thang Vu)\n\nHuman Interpretation of Saliency-based Explanation Over Text\n<LINK>\n\nWe investigate human perception of heat map explanations and find that intuitive understanding is biased.\n\n1/5 <LINK>', 'Saliency attribution methods score how important parts of a model’s input are to the model decision and are often visualised using heat maps.\nPrevious work focused on developing and verifying attribution methods. Less is known about how humans interpret these explanations.\n\n2/5', 'We conduct various user studies to investigate whether superficial and unrelated factors (e.g., word length) influence human self-reported importance ratings.\nWe collect user feedback and statistically analyse importance ratings using a GAMM model.\n\n3/5 https://t.co/sTaHj1CdOi', ""We find that numerous factors such as word length, sentence length and learning effects affect human importance ratings. These factors shouldn't affect importance, because the explanation already objectively has importance, but they do.\n\n4/5 https://t.co/qnz5PlqZh8"", 'We present two bias correction methods and demonstrate their ability to compensate the distorting influence of word length and repeated exposure.\nDetails are in paper. Thanks!\n\n5/5 https://t.co/MHk36SNAPt']",http://arxiv.org/abs/2201.11569,"While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople's interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees' importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation. ",Human Interpretation of Saliency-based Explanation Over Text
20,1487004492734377988,1173944192822927361,astrid.eichhorn,['New paper out with my postdoc Gustavo P. de Brito: We strengthen the evidence for the predictive power of asymptotic safety for #quantumgravity and matter. This could enable tests of quantum gravity with already existing data from particle physics. <LINK>'],https://arxiv.org/abs/2201.11402,"We explore the effect of quantum gravity on matter within a Renormalization Group framework. First, our results provide an explicit example of how misleading conclusions can be drawn by analyzing the gravitational contributions to beta functions, instead of analyzing universal quantities, such as critical exponents, that can be extracted from the beta functions. This could be key to explain differences between perturbative studies and Functional Renormalization Group studies. Second, we strengthen the evidence that asymptotically safe gravity could generate a predictive ultraviolet completion for matter theories with gauge interactions, even in the limit of vanishing dimensionful regulator function. We also find that the situation can be more subtle with higher-order, gravity-induced matter interactions. ","Nonvanishing gravitational contribution to matter beta functions for
  vanishing dimensionful regulators"
21,1486949175367458816,335479530,Mattia Walschaers,"['A new day, a new #quantum paper on the @arxiv. With the help of @mgessnr, Matteo Fadel, and @NicolasTreps our student @Clopetegui98 showed that you can witness non-Gaussian quantum steering using only homodyne detection, using metrological inequalities!\n<LINK>', ""Would you like to better understand such non-Gaussian quantum correlations? Me too! That's why you can come and join our group to do exactly that: https://t.co/02sx4nK2H1""]",https://arxiv.org/abs/2201.11439,"Quantum correlations are at the core of current developments in quantum technologies. Certification protocols of entanglement and steering, suitable for continuous-variable non-Gaussian states are scarce and generally highly demanding from an experimental point of view. We propose a protocol based on Fisher information for witnessing steering in general continuous-variable bipartite states, through homodyne detection. It proves to be relevant for the detection of non-Gaussian steering in scenarios where witnesses based on Gaussian features like the covariance matrix are shown to fail. ",Homodyne detection of non-Gaussian quantum steering
22,1486891959369273352,1019760963569049601,Almog Yalinewich,"['New paper on the arxiv, and this one is a moonshot. We propose that massive naked stars perform a ""gun salute"" when they collapse to a black hole, and shoot a moon mass buckshot at the speed of light. Explanation below\n<LINK>\n\n1/4 <LINK>', 'The figure below (from Burrows et al 2005) shows two bumps in the lightcurve. The first is the prompt GRB and the second is the X ray flare. This emission could come from two relativistic shells with different Lorentz factors. But how would you produce these two shells?\n\n2/4 https://t.co/OQWucSVRIL', 'Enter our model. We look at what happens when a GRB jet erupts from the star. The jet drags along a thin baryonic layer from the star, accelerates it to some moderate Lorentz factor, and then the baryonic shell breaks up due to the Rayleigh Taylor instability.\n\n3/4 https://t.co/5tCEeLQ5oQ', 'After breakup, the jet material streams past the debris, so it can attain a high Lorentz factor, while the debris remains at a lower Lorentz factor. Thus we get the two-shell structure required to explain both the prompt GRB and the X ray flares.\n\n4/4 https://t.co/9LuHLIXl9r']",https://arxiv.org/abs/2201.11177,"In this work we consider the eruption of a tenuous relativistic hydrodynamic jet from a dense baryonic envelope. As the jet moves out and away, it carries along and continues to accelerate a layer of baryonic material which we refer to as the plug. We solve the relativistic equations of motion for the trajectory of the plug, and verify it using a relativistic hydrodynamic simulation. We show that under these conditions, the plug breaks up at a radius larger by a factor of a few from the radius of the envelope, due to the onset of the Rayleigh Taylor instability. After breakup the jet continues to accelerate to higher Lorentz factors while the plug fragments maintain a moderate Lorentz factor. The presence of slower moving ejecta can explain late time features of GRBs such as X ray flares without recourse to a long lived engine. ",Plug Disintegration in GRB Jet Eruption
23,1486882480041136128,259445763,Ryotaku@D＝∞,['Our new paper is out ! <LINK>'],https://arxiv.org/abs/2201.11687,"We explore the phase space of non-uniform black branes compactified on oblique lattices with a large number of dimensions. We find the phase diagrams for different periodicities and angles, and determine the thermodynamically preferred phases for each lattice configuration. In a range of angles, we observe that some phases become metastable. ",Lattice Black Branes at Large $D$
24,1486805419045572612,1062724264799989761,Alex Kontorovich,"['New paper on arxiv!\n\n<LINK>\n\nThis is joint with my former student, Xin Zhang, now Asst Prof at Hong Kong U.\n\nFor a long time, people studied the length spectrum (set of lengths of closed geodesics, with multiplicity) on hyperbolic manifolds, especially in ... <LINK>', ""2/ relation to the trace formula, etc. We look at the length *set* (that is, without multiplicity). In general, there's not much one can say, but if the group is arithmetic, or a subgroup of such, then things get interesting. There's an elementary relation between ..."", '3/ lengths of closed geodesics and traces of (hyperbolic conjugacy classes of) Γ := the fundamental group of the manifold.\n\nAs a baby example, think of the modular surface, upper half plane mod Γ=SL(2,Z). Which integers occur as traces of Γ? Well, all of them! If you want to...', ""4/ find a matrix with trace t, say, t=101, pick any two numbers with a+d=t, say, a=40 and d=61, and factor ad-1=2439=b*c=9*271, say (or just 1*2439...). There's your matrix: {{40,9},{271,61}} with trace 101 and determinant ad-bc=1.\n\nLess baby example: upper half plane mod..."", '5/ Γ=the orthogonal group preserving the (indefinite, ternary, anisotropic/ℚ) quadratic form x^2+y^2-3z^2. I worked out this group in excruciating detail (at the request of a student) here: https://t.co/y9yQ9NKnUG The upshot is that any ℤ-solution to:\na²-3b²-3c²+d²=1\ngives...', ""6/ a matrix in Γ with trace 2a. (So the group is just the norm-1 elements of this quaternion division algebra; the equation above is the norm.) If you want to make the trace be 2a=100 (not 101!...), that's the same as asking whether there's an integer point on the quadric..."", ""7/\n3b²+3c²-d²=a²-1=2499.\nThis reduces to a classical question of representations of numbers by indefinite quadratic forms. (In case you're wondering, there are solutions to this, e.g., b=19, c=50, d=78.)\n\nAnyway, the question is, can we say something about trace sets in other ..."", '8/ groups? The answer is: yes and no. There are well-understood ""congruence"" obstructions (like why 101 can\'t be a trace in the second example). Even for arithmetic lattices, there can be further (""Brauer-Manin"") obstructions to solving, e.g., ternary quadratic polynomials, thus', '9/ preventing numbers occurring as traces.\n\nWhat we conjecture generally, and prove for punctured Zariski-dense subgroups of the modular group, is ""Asymptotic Length-Saturation"". That is, 100% of the numbers that aren\'t excluded by congruences, actually arise as traces.', ""10/ I could say more, but if you've gotten this far, hopefully you can just go and read the introduction. It was a really fun (several years in the making) project, using quite a variety of techniques, from L-functions and Siegel zeros, to expanders from thin groups, and even"", ""11/ Hilbert's Nullstellensatz made an appearance... Hope you enjoy!""]",https://arxiv.org/abs/2201.10955,"We formulate the Asymptotic Length-Saturation Conjecture on the length sets of closed geodesics on hyperbolic manifolds whose fundamental groups are subarithmetic, that is, contained in an arithmetic group. We prove the first instance of the conjecture for punctured, Zariski dense covers of the modular surface. The tools involved include the Orbital Circle Method, expansion and counting in congruence towers of thin groups, estimates for exponential sums, bilinear forms, and quadratic L-series. ",On Length Sets of Subarithmetic Hyperbolic Manifolds
25,1486617114194624514,1216114532,Adam Carnall,"['New paper today by @MassiHamadouche looking at the interrelations between mass, size, age and metallicity for massive quiescent galaxies at 0.6 &lt; z &lt; 1.3. We find both age and metallicity gradients on the size-mass plane are necessary to explain our data: <LINK>.', '@scorfano @MassiHamadouche Thanks! Will take a look.']",https://arxiv.org/abs/2201.10576,"We study the relationships between stellar mass, size and age within the quiescent population, using two mass-complete spectroscopic samples with $\mathrm{log_{10}}(M_{\star}/\mathrm{M_{\odot}})>10.3$, taken from VANDELS at $1.0<z<1.3$, and LEGA-C at $0.6<z<0.8$. Using robust D$_{n}$4000 values, we demonstrate that the well-known 'downsizing' signature is already in place by $z\simeq1.1$, with D$_{n}$4000 increasing by $\simeq0.1$ across a $\simeq$ 1 dex mass interval for both VANDELS and LEGA-C. We then proceed to investigate the evolution of the quiescent galaxy stellar mass-size relation from $z\simeq1.1$ to $z\simeq0.7$. We find the median size increases by a factor of $1.9\pm{0.1}$ at $\mathrm{log_{10}}(M_{\star}/\mathrm{M_{\odot}})=10.5$, and see tentative evidence for flattening of the relation, finding slopes of $\alpha=0.72\pm0.06$ and $\alpha=$ $0.56\pm0.04$ for VANDELS and LEGA-C respectively. We finally split our sample into galaxies above and below our fitted mass-size relations, to investigate how size and D$_{n}$4000 correlate. For LEGA-C, we see a clear difference, with larger galaxies found to have smaller D$_{n}$4000 at fixed stellar mass. Due to the faintness and smaller numbers of the VANDELS sample, we cannot confirm whether a similar relation exists at $z\simeq1.1$. We consider whether differences in stellar age or metallicity are most likely to drive this size-D$_{n}$4000 relation, finding that any metallicity differences are unlikely to fully explain the observed offset, meaning smaller galaxies must be older than their larger counterparts. We find the observed evolution in size, mass and D$_{n}$4000 across the $\simeq2$ Gyr from $z\sim1.1$ to $z\sim0.7$ can be explained by a simple toy model in which VANDELS galaxies evolve passively, whilst experiencing a series of minor mergers. ","A combined VANDELS and LEGA-C study: the evolution of quiescent galaxy
  size, stellar mass and age from $\mathbf{\textit{z} = 0.6}$ to
  $\mathbf{\textit{z} = 1.3}$"
26,1486265584392232960,1235148346618261508,Taha Yassine,['Our new paper on location based beamforming using deep learning has been accepted to be presented at ICASSP 2022.\n\nGive it a read here: <LINK>\n\n#6G #MIMO #machinelearning #deeplearning #AI'],https://arxiv.org/abs/2201.01386,"Massive MIMO systems are highly efficient but critically rely on accurate channel state information (CSI) at the base station in order to determine appropriate precoders. CSI acquisition requires sending pilot symbols which induce an important overhead. In this paper, a method whose objective is to determine an appropriate precoder from the knowledge of the user's location only is proposed. Such a way to determine precoders is known as location based beamforming. It allows to reduce or even eliminate the need for pilot symbols, depending on how the location is obtained. the proposed method learns a direct mapping from location to precoder in a supervised way. It involves a neural network with a specific structure based on random Fourier features allowing to learn functions containing high spatial frequencies. It is assessed empirically and yields promising results on realistic synthetic channels. As opposed to previously proposed methods, it allows to handle both line-of-sight (LOS) and non-line-of-sight (NLOS) channels. ",Deep learning for location based beamforming with NLOS channels
27,1486243947303378945,802543221943439360,Andrea Caputo,"['<LINK>\nPaper day!\nIn this Letter, for the first time, we use a supernova (SN) population with particularly low explosion energies as the most sensitive calorimeters to constrain BSM models in which new particles can decay in SM products. <LINK>', 'This applies, for example, to axion or scalar coupling to photons and electrons. For a long time people asked the wrong question, which sounded like: can new particles help SN explosion? The reasoning should be really the opposite, we need to avoid large energy deposition!', 'The variety of SN explosions is so large, with low-energy SNe, failed SNe, etc, that the really crucial point is to make sure BSM particle does NOT contribute too much to the energetic.', 'The trick is that the emission of new particles basically depends only on the SN core conditions, which are always pretty similar. If you help one SN exploding (because you want to justify 1987A), then you are in contradiction with other tons of observations.']",https://arxiv.org/abs/2201.09890,"The hot and dense core formed in the collapse of a massive star is a powerful source of hypothetical feebly-interacting particles such as sterile neutrinos, dark photons, axion-like particles (ALPs), and others. Radiative decays such as $a\to2\gamma$ deposit this energy in the surrounding material if the mean free path is less than the radius of the progenitor star. For the first time, we use a supernova (SN) population with particularly low explosion energies as the most sensitive calorimeters to constrain this possibility. These SNe are observationally identified as low-luminosity events with low ejecta velocities and low masses of ejected $^{56}$Ni. Their low energies limit the energy deposition from particle decays to less than about 0.1 B, where $1~{\rm B~(bethe)}=10^{51}~{\rm erg}$. For 1-500 MeV-mass ALPs, this generic argument excludes ALP-photon couplings $G_{a\gamma\gamma}$ in the $10^{-10}$-$10^{-8}~{\rm GeV}^{-1}$ range. ",Low-Energy Supernovae Severely Constrain Radiative Particle Decays
28,1486200896547483648,2932678322,Keaton Bell,"['19 new pulsating helium-atmosphere white dwarfs were discovered by @ZachVanderbosch! This fantastic new paper constrains the extent of the instability strip, and explores how pulsation properties scale with effective temperature. <LINK> <LINK>']",https://arxiv.org/abs/2201.09893,"We present a dedicated search for new pulsating helium-atmosphere (DBV) white dwarfs from the Sloan Digital Sky Survey using the McDonald 2.1m Otto Struve Telescope. In total we observed 55 DB and DBA white dwarfs with spectroscopic temperatures between 19,000 and 35,000K. We find 19 new DBVs and place upper limits on variability for the remaining 36 objects. In combination with previously known DBVs, we use these objects to provide an update to the empirical extent of the DB instability strip. With our sample of new DBVs, the red edge is better constrained, as we nearly double the number of DBVs known between 20,000 and 24,000K. We do not find any new DBVs hotter than PG 0112+104, the current hottest DBV at $T_{\mathrm{eff}}\,{\approx}$ 31,000K, but do find pulsations in four DBVs with temperatures between 27,000 and 30,000K, improving empirical constraints on the poorly defined blue edge. We investigate the ensemble pulsation properties of all currently known DBVs, finding that the weighted mean period and total pulsation power exhibit trends with effective temperature that are qualitatively similar to the pulsating hydrogen-atmosphere white dwarfs. ","The Pulsating Helium-Atmosphere White Dwarfs I: New DBVs from the Sloan
  Digital Sky Survey"
29,1486163128782098436,232287209,Dallas Card,"['Really excited to share new work on the whose language is more likely to be represented in pretraining corpora for large language models, with @ssgrn @skdreier24 @ekgade Leroy Wang @blarrywang @LukeZettlemoyer and @nlpnoah! \nPaper: <LINK> <LINK>', 'Despite the large number of LLMs that have been built, most have used very similar training data, especially books, news, and Wikipedia. Although these corpora are widely available, they all come with their own biases and have issues with author diversity. (2/8)', 'Text from web scrapes like Common Crawl offers one way of going beyond the familiar sources, but some sort of filtering is typically required. A common approach is to filter for content that looks like so-called ""high quality"" sources: again books, news, and Wikipedia.', 'In this paper, we use a new dataset of US highschool newspapers to test whose language is more or less likely to be included by the filtering approach used in the GPT-3 paper.', 'In addition to topical and stylistic effects, we find significant differences based on factors like wealth, education, and rural vs. urban locations.', 'In particular, newspapers from rural schools are significantly less likely to be selected than those from urban schools, even when controlling for wealth, parental education, school size, and other factors.', ""We also find that the quality scores from the filter generally don't align with other (subjective) measures of quality, like factuality, graded essay scores, or literary acclaim across genres."", ""It's obviously impossible to avoid some selection/filtering in choosing training data for LLMs, but there is room for greater clarity and transparency about why certain data is being used, and for testing whose language is likely to be included or excluded by these choices. (8/8)"", 'More from @ssgrn about these issues here: https://t.co/D5WjMHdqij']",https://arxiv.org/abs/2201.10474,"Language models increasingly rely on massive web dumps for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and newswire often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles -- written by students from across the country -- we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban ZIP codes are more likely to be classified as high quality. We then demonstrate that the filter's measurement of quality is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts. ","Whose Language Counts as High Quality? Measuring Language Ideologies in
  Text Data Selection"
30,1486152591935377410,416861161,Suchin Gururangan,"['In our new interdisciplinary work, “Whose Language Counts as High Quality?”,  we empirically demonstrate that the data selection procedures for language models like GPT-3 implicitly favor text written by authors from powerful social positions.\nPaper: <LINK>\n🧵👇 <LINK>', 'Web dumps are one of the key data sources for training LMs. But in their raw form, they have lots of undesirable content (e.g., boilerplate, hatespeech). Researchers typically apply tools called *quality filters* to select text considered “high quality” from these sources. /2', 'We argue that quality filtering implies a language ideology – a sociolinguistics term for a subjective belief about language use. These ideologies are often implicit/undocumented. What language is high quality enough to be included in the corpus? Whose language is excluded? /3', 'We focus on the quality filter used for GPT-3. It’s a classifier trained to discriminate between a random sample of Common Crawl and certain texts assumed to be high quality: Wikipedia, Books, and popular web content (e.g., news, blogs). /4', 'We first summarize extensive literature on these “high quality” data sources, which show that they exhibit systematic authorship biases, where a minority of authors (from relatively powerful social positions) write a disproportionate amount of content. /5', 'These biases are driven by the power structures that underlie these data sources. The books industry is mostly white and male-dominated. Newsrooms are mostly white, and present Anglo-American perspectives. Even though anyone can edit Wikipedia, not everyone does. /6', 'Then, to study whose language the GPT-3 quality filter favors, we scrape a large set of US high school newspapers. We augment our dataset with demographic data from the US Census and National Center for Education Statistics, using school ZIP codes. /7', 'We then replicate the GPT-3 quality filter, and apply it to our dataset. We first observe that the quality filter tends to value longer texts written in the third person, discussing topics most prolifically available on the Web (e.g. Trump/election stuff, sports). /8', 'We then demonstrate that articles from larger public schools in wealthy, educated, and urban areas of the U.S. tend to be considered higher quality. These effects are consistent controlling for many different factors. /9', 'Our results suggest there is no such thing as a general-purpose corpus, since selecting data for language models is highly subjective. The data curator must adopt a language ideology, and it will likely conflict with other perspectives of what makes text high quality. /11', 'A key issue is that language ideologies in data selection go undocumented and implicit. We believe the community could be much more intentional and transparent in their data selection practices. These decisions may be informed by specific downstream applications! /12', 'This large project was completed with an amazing group of interdisciplinary collaborators, who I learned a ton from (@dallascard, @SKdreier24, @ekgade, Leroy Wang, @blarrywang, @lsz, @nlpnoah). If you have any questions or comments, do reach out! /13', '@dallascard @SKDreier24 @ekgade @BlarryWang @lsz @nlpnoah ^tag correction: I mean @LukeZettlemoyer!', '@BlancheMinerva Thanks for the feedback Stella! We’ll tidy up the appendix for sure. I agree, doc-/claim-level  factuality would be even better for that investigation, but I’m not super familiar with the space - do you have any pointers to good datasets for that?']",https://arxiv.org/abs/2201.10474,"Language models increasingly rely on massive web dumps for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and newswire often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles -- written by students from across the country -- we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban ZIP codes are more likely to be classified as high quality. We then demonstrate that the filter's measurement of quality is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts. ","Whose Language Counts as High Quality? Measuring Language Ideologies in
  Text Data Selection"
31,1485965019015696388,1317779633572634625,Martin Weyssow,"['Our new vision paper is available on ArXiv: <LINK>\nIn this @ICSEconf paper, we propose a multi-modal learning framework to modeling the programming world by learning from multiple levels of semantics of code.\n#icse22 #deeplearning #multimodal #ml4se #codesearch <LINK>']",http://arxiv.org/abs/2201.03346,"The progress made in code modeling has been tremendous in recent years thanks to the design of natural language processing learning approaches based on state-of-the-art model architectures. Nevertheless, we believe that the current state-of-the-art does not focus enough on the full potential that data may bring to a learning process in software engineering. Our vision articulates on the idea of leveraging multi-modal learning approaches to modeling the programming world. In this paper, we investigate one of the underlying idea of our vision whose objective based on concept graphs of identifiers aims at leveraging high-level relationships between domain concepts manipulated through particular language constructs. In particular, we propose to enhance an existing pretrained language model of code by joint-learning it with a graph neural network based on our concept graphs. We conducted a preliminary evaluation that shows gain of effectiveness of the models for code search using a simple joint-learning method and prompts us to further investigate our research vision. ","Better Modeling the Programming World with Code Concept Graphs-augmented
  Multi-modal Learning"
32,1485823494529200129,3241924438,Akshansh Mishra,"['New paper entitled ""Machine Learning Algorithms for Prediction of Penetration Depth and Geometrical Analysis of Weld in Friction Stir Spot Welding Process"" is available on arXiv. \n\n#arXiv #MachineLearning #Welding \n\n<LINK>']",https://arxiv.org/abs/2201.09725,"Nowadays, manufacturing sectors harness the power of machine learning and data science algorithms to make predictions for the optimization of mechanical and microstructure properties of fabricated mechanical components. The application of these algorithms reduces the experimental cost beside leads to reduce the time of experiments. The present research work is based on the prediction of penetration depth using Supervised Machine Learning algorithms such as Support Vector Machines (SVM), Random Forest Algorithm, and Robust Regression algorithm. A Friction Stir Spot Welding (FSSW) was used to join two elements of AA1230 aluminum alloys. The dataset consists of three input parameters: Rotational Speed (rpm), Dwelling Time (seconds), and Axial Load (KN), on which the machine learning models were trained and tested. It observed that the Robust Regression machine learning algorithm outperformed the rest of the algorithms by resulting in the coefficient of determination of 0.96. The research work also highlights the application of image processing techniques to find the geometrical features of the weld formation. ","Machine Learning Algorithms for Prediction of Penetration Depth and
  Geometrical Analysis of Weld in Friction Stir Spot Welding Process"
33,1485814129197785091,1347114826267504641,Sarah Betti,"[""New astronomy paper alert! We detect low levels of water ice in a disk around a young Herbig star, AB Aur, out to ~150 AU!  We also find a highly red disk compared to models in the NIR and detect the inner spiral arms in L' for the first time.  🌠⭐️💥\n<LINK>"", ""This paper is in collaboration with another paper now out on arxiv by Sebastián Jorquera who looks for the planetary mass companions around the disk of AB Aur in L' band for the first time! \nhttps://t.co/8sZWFqcLol""]",https://arxiv.org/abs/2201.08868,"We present near-infrared Large Binocular Telescope Interferometer LMIRCam imagery of the disk around the Herbig Ae/Be star AB Aurigae. A comparison of surface brightness at Ks (2.16 ${\mu}$m), H2O narrowband (3.08 ${\mu}$m), and L' (3.7 ${\mu}$m) allows us to probe the presence of icy grains in this (pre)transitional disk environment. By applying Reference Differential Imaging PSF subtraction, we detect the disk at high signal to noise in all three bands. We find strong morphological differences between bands, including asymmetries consistent with observed spiral arms within 100 AU in L'. An apparent deficit of scattered light at 3.08 ${\mu}$m relative to bracketing wavelengths (Ks and L') is evocative of ice absorption at the disk surface layer. However, the ${\Delta}$(Ks-H2O) color is consistent with grains with little to no ice (0-5% by mass). The ${\Delta}$(H2O-L') color, conversely, suggests grains with a much higher ice mass fraction (~0.68), and the two colors cannot be reconciled under a single grain population model. Additionally, we find the extremely red ${\Delta}$(Ks-L') disk color cannot be reproduced under conventional scattered light modeling with any combination of grain parameters or reasonable local extinction values. We hypothesize that the scattering surfaces at the three wavelengths are not co-located, and optical depth effects result in each wavelength probing the grain population at different disk surface depths. The morphological similarity between Ks and H2O suggests their scattering surfaces are near one another, lending credence to the ${\Delta}$(Ks-H2O) disk color constraint of < 5% ice mass fraction for the outermost scattering disk layer. ","Detection of Near-Infrared Water Ice at the Surface of the
  (pre)Transitional Disk of AB Aur: Informing Icy Grain Abundance, Composition,
  and Size"
34,1485800431745863682,326843207,Yuta Notsu,"['Our new paper (K. Namekata @KosOlo8 et al.) is now accepted to ApJ Letter and available at <LINK> \n\n“Discovery of a Long-Duration Superflare on a Young Solar-Type Star EK Draconis with Nearly Similar Time Evolution for Hα and White-Light Emissions”', 'This ApJ Letter paper is the second paper of our spectroscopic observation project (led by @KosOlo8 using @Obs_kyoto_u Seimei telescope) of young solar-type superflare stars (e.g., EK Dra).  The first paper was published in Nature Astronomy last month (see the attached tweets). https://t.co/hyEmBzkKCb', 'This second flare did not show signs of filament eruptions.  However,it showed another feature.\nThe time evolution and duration of the Hα flare are surprisingly almost the same as the white-light flare, which is different from general M-dwarf (super-)flares and solar flares. https://t.co/FxiH7nrWm6', 'This unexpected time evolution may suggest that different radiation mechanisms than general solar flares are predominant: (1) radiation from (off-limb) flare loops, and (2) re-radiation via radiative backwarming 👀', 'And the detected flare is the largest superflare on a solar-type star ever detected by optical spectroscopy 🧨']",https://arxiv.org/abs/2201.09416,"Young solar-type stars are known to show frequent ""superflares"", which may severely influence the habitable worlds on young planets via intense radiations and coronal mass ejections. Here we report an optical spectroscopic and photometric observation of a long-duration superflare on the young solar-type star EK Draconis (50-120 Myr age) with the Seimei telescope and $Transiting$ $Exoplanet$ $Survey$ $Satellite$ ($TESS$). The flare energy 2.6$\times$10$^{34}$ erg and white-light flare duration 2.2 hr are much larger than those of the largest solar flares, and this is the largest superflare on a solar-type star ever detected by optical spectroscopy. The H$\alpha$ emission profile shows no significant line asymmetry, meaning no signature of a filament eruption, unlike the only previous detection of a superflare on this star (Namekata et al. 2021, $Nat.Astron$). Also, it did not show significant line broadening, indicating that the non-thermal heating at the flare footpoints are not essential or that the footpoints are behind the limb. The time evolution and duration of the H$\alpha$ flare are surprisingly almost the same as those of the white-light flare, which is different from general M-dwarf (super-)flares and solar flares. This unexpected time evolution may suggest that different radiation mechanisms than general solar flares are predominant, as follows: (1) radiation from (off-limb) flare loops, and (2) re-radiation via radiative backwarming, in both of which the cooling timescales of flare loops could determine the timescales of H$\alpha$ and white light. ","Discovery of a Long-Duration Superflare on a Young Solar-Type Star EK
  Draconis with Nearly Similar Time Evolution for H$\alpha$ and White-Light
  Emissions"
35,1485715701264379905,972871592,Jon Wakefield,['Paper with @peteragao is on arXiv <LINK> and proposes a new approach to smoothed model-assisted small area estimation. A version of this paper was a recent winner of the ASA GSS/SRMS/SSS (survey research methods) student paper competition'],https://arxiv.org/abs/2201.08775,"In countries where population census and sample survey data are limited, generating accurate subnational estimates of health and demographic indicators is challenging. Existing model-based geostatistical methods leverage covariate information and spatial smoothing to reduce the variability of estimates but often assume the survey design is ignorable, which may be inappropriate given the complex design of household surveys typically used in this context. On the other hand, small area estimation approaches common in the survey statistics literature do not incorporate both unit-level covariate information and spatial smoothing in a design-consistent way. We propose a new smoothed model-assisted estimator that accounts for survey design and leverages both unit-level covariates and spatial smoothing, bridging the survey statistics and model-based geostatistics perspectives. Under certain assumptions, the new estimator can be viewed as both design-consistent and model-consistent, offering potential benefits from both perspectives. We demonstrate our estimator's performance using both real and simulated data, comparing it with existing design-based and model-based estimators. ",Smoothed Model-Assisted Small Area Estimation
36,1485666194187333637,3433220662,Anthony Bonato,"['New paper on arXiv ""On Meyniel extremal families of graphs,"" where we explore graphs with the conjectured asymptotically largest cop number. Includes some new bounds using techniques from hypergraphs. \n<LINK> <LINK>']",http://arxiv.org/abs/2201.08719,"We provide new constructions of Meyniel extremal graphs, which are families of graphs with the conjectured largest asymptotic cop number. Using spanning subgraphs, we prove that there are an exponential number of new Meyniel extremal families with specified degrees. Using a linear programming problem on hypergraphs, we explore the degrees in families that are not Meyniel extremal. We give the best-known upper bound on the cop number of vertex-transitive graphs with a prescribed degree. We find new Meyniel extremal families of regular graphs with large chromatic number, large diameter, and explore the connection between Meyniel extremal graphs and bipartite graphs. ",On Meyniel extremal families of graphs
37,1485550143692812288,1206030745,Luc Le Magoarou,['Is channel state information necessary for MIMO precoding?\nCan location-based beamforming work even for NLOS propagation?\n\nThese questions are tackled in our new paper to be presented at ICASSP this year (special session on machine…<LINK> <LINK>'],https://arxiv.org/abs/2201.01386,"Massive MIMO systems are highly efficient but critically rely on accurate channel state information (CSI) at the base station in order to determine appropriate precoders. CSI acquisition requires sending pilot symbols which induce an important overhead. In this paper, a method whose objective is to determine an appropriate precoder from the knowledge of the user's location only is proposed. Such a way to determine precoders is known as location based beamforming. It allows to reduce or even eliminate the need for pilot symbols, depending on how the location is obtained. the proposed method learns a direct mapping from location to precoder in a supervised way. It involves a neural network with a specific structure based on random Fourier features allowing to learn functions containing high spatial frequencies. It is assessed empirically and yields promising results on realistic synthetic channels. As opposed to previously proposed methods, it allows to handle both line-of-sight (LOS) and non-line-of-sight (NLOS) channels. ",Deep learning for location based beamforming with NLOS channels
38,1485530792600158210,948528424926220288,Johannes Røsok Eskilt,['I have a new paper on multifield dark energy that is finally out! Written with @YasharAkrami @adamsolo @ValeriVardanyan\n<LINK>'],https://arxiv.org/abs/2201.08841,"We numerically and analytically explore the background cosmological dynamics of multifield dark energy with highly non-geodesic or ""spinning"" field-space trajectories. These extensions of standard single-field quintessence possess appealing theoretical features and observable differences from the cosmological standard model. At the level of the cosmological background, we perform a phase-space analysis and identify approximate attractors with late-time acceleration for a wide range of initial conditions. Focusing on two classes of field-space geometry, we derive bounds on parameter space by demanding viable late-time acceleration and the absence of gradient instabilities, as well as from the de Sitter swampland conjecture. ",Cosmological dynamics of multifield dark energy
39,1485487926066692096,1113856096119197699,Lucas Lamata,['<LINK>\n\nNew paper! A rewarding collaboration with M. L. Olivera Atencio and J. Casado-Pascual from @unisevilla @fisicaUS and Sigmund Kohler from @CSIC @icmmcsic'],https://arxiv.org/abs/2201.08665,"The response of dissipative systems to multi-chromatic fields exhibits generic properties which follow from the discrete time-translation symmetry of each driving component. We derive these properties and illustrate them with paradigmatic examples of classical and quantum dissipative systems. In addition, some computational aspects, in particular a matrix continued-fraction method, are discussed. Moreover, we propose possible implementations with quantum optical settings. ",Universal patterns in multifrequency-driven dissipative systems
40,1484639703613718535,1440688886615736320,Mohammad Farazmand,"['Excited to announce our new paper: \n<LINK>\nIt is truly the most unexpected and baffling result of my career:\nTwo reduced-order modeling methods, which follow very different philosophies, coincide when applied to the nonlinear Schrodinger (#NLS) equation!', 'One method, reduced Lagrangian, relies on the variational structure of NLS. The other method, RONS, is completely unaware of the Lagrangian. Yet the resulting reduced-order models coincide! \nOpen question: Is this more generally true for all PDEs with a variational structure?']",https://arxiv.org/abs/2201.07953,"We consider reduced-order modeling of nonlinear dispersive waves described by a class of nonlinear Schrodinger (NLS) equations. We compare two nonlinear reduced-order modeling methods: (i) The reduced Lagrangian approach which relies on the variational formulation of NLS and (ii) The recently developed method of reduced-order nonlinear solutions (RONS). First, we prove the surprising result that, although the two methods are seemingly quite different, they can be obtained from the real and imaginary parts of a single complex-valued master equation. Furthermore, for the NLS equation in a stationary frame, we show that the reduced Lagrangian method fails to predict the correct group velocity of the waves whereas RONS predicts the correct group velocity. Finally, for the modified NLS equation, where the reduced Lagrangian approach is inapplicable, the RONS reduced-order model accurately approximates the true solutions. ",Shape-morphing reduced-order models for nonlinear Schrodinger equations
41,1484569647668072452,80454546,Greg Gilbert,"['My new paper, “Accurate modeling of grazing transits using umbrella sampling” is on the arXiv today! Check it out here: <LINK>\n\nA thread 👇🏼', 'Let’s talk about grazing transits! They’re rare, but important for determining accurate exoplanet population statistics   1/', '“Grazing” means the impact parameter, b, is greater than the planet-star radius ratio, r=Rp/Rs   2/ https://t.co/mrnL8UnT1v', 'The challenge: even when a planet’s trajectory is non-grazing, for most typical signal-to-noise cases, the orbital geometry is ambiguous from the transit photometry   3/ https://t.co/siJYNj8mTT', 'That means we need to model grazing transits even for cases when we’re pretty sure the ground-truth is that the orbit is non-grazing   4/', ""But, it turns out that standard Monte Carlo sampling methods are not very good a switching between grazing and non-grazing transit models. In mathematical terms, there's a topological bottleneck   5/ https://t.co/ITAC4EwJxX"", 'Running the same transit model multiple times can produce different results, and what’s worse, sometimes there’s no indication that anything has gone wrong!   6/ https://t.co/kbxl9ro38E', 'Looking at Kepler data, it’s clear that something is amiss with existing r-b measurements   7/ https://t.co/fkXBesUJvC', 'For individual targets, we can run longer chains or include additional info in the model (e.g. stellar spectra), but for survey-scale survey work, we need better efficiency   8/', 'So how do we obtain accurate planetary parameter estimates from ambiguous transit light curves? The answer: umbrella sampling   9/', 'Umbrella sampling is a technique related to importance sampling that’s been in everyday use among biochemists and molecular dynamicists since the 1970s, but it has only recently gained the attention of astronomers   10/', 'The core concept behind umbrella sampling is quite simple - we just split the complicated sampling problem into sub-regions, sample from these regions independently, and then recombine the samples afterward into a single joint posterior   11/ https://t.co/VSxLDFmp0c', 'There’s a bit of math that goes into determining how to weight the sub-regions relative to one another (you’ll have to read the paper for that), but basically, that’s it!   12/', 'You can also check out this Python tutorial I’ve put together which provides a simple step-by-step introduction to umbrella sampling: https://t.co/44FYB4lS3i   13/', 'In order to apply umbrella sampling to transit modeling, I split the problem into grazing, non-grazing, and transition regions of the posterior (again, see the paper for the gory math)   14/', 'When I fit a few synthetic transits using this method, I find that umbrella sampling consistently yields more robust results compared to standard methods   15/ https://t.co/ZyqJajpBmt', 'Often times, results from umbrella sampling and standard sampling are consistent, but the kicker is that we can’t be sure we can trust standard MCMC methods until *after* we’ve done the umbrella sampling analysis   16/', 'The takeaway: when you fit a transit model, unless you can rule out grazing geometries, either use umbrella sampling or run multiple chains for a very long time to ensure convergence (which will be far, far less efficient)   17/', 'More generally, umbrella sampling is a tool designed to help sample from any complicated posterior (bimodal peaks, degeneracy ridges, long tails), so it has huge potential impact in all areas of astronomy. Tell your non-exoplanet friends!   18/', 'When writing my paper, I built on the work of Matthews+ 2018, which introduced umbrella sampling to the astronomy literature. I recommend checking that paper out too\n\nhttps://t.co/vGjKKjjNXK   19/', 'A final thought: this was the last paper I wrote as a PhD student @UChicago, so I want to give a huge thank you to my thesis committee: Fred Ciesla, Andrey Kravtsov, Leslie Rogers, and of course my thesis advisor Dan Fabrycky   20/', 'Questions? Comments? I’d love to hear from you and share this work. DMs open!', '@aussiastronomer Of course! 😁\n\nTop left: grey/black is ground truth bimodal distribution we wish to sample. The dashed lines, ψ, are ""bias functions"" which restrict the sampler to one of three umbrellas. To calculate umbrella weights, the umbrellas *must* overlap', '@aussiastronomer Top right: solid lines, π, are biased posterior sub-distributions. Notice how for the middle (orange) distribution, the umbrella bias has flattened out the bottleneck.', ""@aussiastronomer Bottom left: unbiased sub-distributions after removing the effects of the biasing potential. The sub-distributions are offset from one another by an unknown amount; that's the secret sauce of umbrella sampling - determining the relative umbrella weights"", '@aussiastronomer Bottom right: after calculating relative umbrella weights, we can recombine into the joint posterior.\n\nI made a Python tutorial for reproducing this figure, which you can find at https://t.co/44FYB4lS3i\n\nLet me know if you have more questions!', '@MartianColonist @jonKzink @WD1856_534b For this paper, I limited discussion to Rp/Rs &lt; 1 (for convenience more than anything), but the technique is general and could be modified in a straightforward manner to accommodate Rp/Rs &gt;1 and WDs (see Sec 3.1 of the paper for more on this point)', '@AgolEric If you jumped right into an N-planet model, yes.\n\nI recommend first fitting an exploratory model using the transition umbrella for each planet independently. Often you can rule in/out grazing geometries this way.\n\nSee, e.g. KOI-1426 https://t.co/8lgfw0U8te']",https://arxiv.org/abs/2201.08350,"Grazing transits present a special problem for statistical studies of exoplanets. Even though grazing planetary orbits are rare (due to geometric selection effects), for many low to moderate signal-to-noise cases, a significant fraction of the posterior distribution is nonetheless consistent with a grazing geometry. A failure to accurately model grazing transits can therefore lead to biased inferences even for cases where the planet is not actually on a grazing trajectory. With recent advances in stellar characterization, the limiting factor for many scientific applications is now the quality of available transit fits themselves, and so the time is ripe to revisit the transit fitting problem. In this paper, we model exoplanet transits using a novel application of umbrella sampling and a geometry-dependent parameter basis that minimizes covariances between transit parameters. Our technique splits the transit fitting problem into independent Monte Carlo sampling runs for the grazing, non-grazing, and transition regions of the parameter space, which we then recombine into a single joint posterior probability distribution using a robust weighting scheme. Our method can be trivially parallelized and so requires no increase in the wall clock time needed for computations. Most importantly, our method produces accurate estimates of exoplanet properties for both grazing and non-grazing orbits, yielding more robust results than standard methods for many common star-planet configurations. ",Accurate modeling of grazing transits using umbrella sampling
42,1484543767923380225,1285579351950598144,Karolina Stanczak,['New #NLProc paper: a latent-variable model for intrinsic probing using VA\n\n1) We analyse information distribution and \n2) find overlap in the neurons encoding linguistic features across langs! \n\n<LINK>\n@ltorroba1 @adinamwilliams @ryandcotterell @IAugenstein <LINK>'],https://arxiv.org/abs/2201.08214,"The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. Indeed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization. In this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute, but also to pinpoint where this attribute is encoded. We propose a novel latent-variable formulation for constructing intrinsic probes and derive a tractable variational approximation to the log-likelihood. Our results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature. Finally, we find empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax. ",A Latent-Variable Model for Intrinsic Probing
43,1484533699462975488,1015712980275748864,Konstantin Antipin,"['My new paper ""Tensor products of entangled subspaces and their properties from diagrammatic constructions"", <LINK>']",https://arxiv.org/abs/2201.07918,"Construction of genuinely entangled multipartite subspaces with certain characteristics has become a relevant task in various branches of quantum information. Here we show that such subspaces can be obtained from an arbitrary collection of bipartite entangled subspaces under joining of their adjacent subsystems. In addition, it is shown that direct sums of such constructions under certain conditions are genuinely entangled. These facts are then used in detecting entanglement of tensor products of mixed states and constructing subspaces that are distillable across every bipartite cut, where for the former application we include example with the analysis of genuine entanglement of a tripartite state obtained from two Werner states. ","Tensor products of entangled subspaces and their properties from
  diagrammatic constructions"
44,1484528838268891138,23000769,Christopher Conselice,['New paper on the arXiv led by Berta Margalef-Bentabol looking at the formation of spiral galaxies at 1 &lt; z &lt; 3 with HST.  These are more common than you might think at this epoch - and we can watch them grow and evolve.  Check it out!\n\n<LINK>'],https://arxiv.org/abs/2201.06334,"Many aspects concerning the formation of spiral and disc galaxies remain unresolved, despite their discovery and detailed study over the past $150$ years. As such, we present the results of an observational search for proto-spiral galaxies and their earliest formation, including the discovery of a significant population of spiral-like and clumpy galaxies at $z>1$ in deep \textit{Hubble Space Telescope} CANDELS imaging. We carry out a detailed analysis of this population, characterising their number density evolution, masses, star formation rates and sizes. Overall, we find a surprisingly high overall number density of massive $M_{*} >10^{10}\mathrm{M}_{\odot}$ spiral-like galaxies (including clumpy spirals) at $z > 1$ of $0.18\,{\rm per}\, \mathrm{arcmin}^{-2}$. We measure and characterise the decline in the number of these systems at higher redshift using simulations to correct for redshift effects in identifications, finding that the true fraction of spiral-like galaxies grows at lower redshifts as $\sim$ $(1+z)^{-1.1}$. This is such that the absolute numbers of spirals increases by a factor of $\sim 10$ between $z = 2.5$ and $z = 0.5$. We also demonstrate that these spiral-like systems have large sizes at $z>2$, and high star formation rates, above the main-sequence, These galaxies represent a major mode of galaxy formation in the early universe, perhaps driven by the spiral structure itself. We finally discuss the origin of these systems, including their likely formation through gas accretion and minor mergers, but conclude that major mergers are an unlikely cause. ","Observations of the Initial Formation and Evolution of Spiral galaxies
  at $1 &lt; z &lt; 3$ in the CANDELS fields"
45,1484476985405497347,50574497,Ervin Dervishaj,['Glad to finally share the arXiv version and code of our paper! We derive a new matrix factorization model for #recsys from two key issues when using GANs for collaborative filtering.\narXiv: <LINK>\nCode: <LINK>\n#recsys #GANs #MachineLearning <LINK>'],https://arxiv.org/abs/2201.08042,"Proposed in 2014, Generative Adversarial Networks (GAN) initiated a fresh interest in generative modelling. They immediately achieved state-of-the-art in image synthesis, image-to-image translation, text-to-image generation, image inpainting and have been used in sciences ranging from medicine to high-energy particle physics. Despite their popularity and ability to learn arbitrary distributions, GAN have not been widely applied in recommender systems (RS). Moreover, only few of the techniques that have introduced GAN in RS have employed them directly as a collaborative filtering (CF) model. In this work we propose a new GAN-based approach that learns user and item latent factors in a matrix factorization setting for the generic top-N recommendation problem. Following the vector-wise GAN training approach for RS introduced by CFGAN, we identify 2 unique issues when utilizing GAN for CF. We propose solutions for both of them by using an autoencoder as discriminator and incorporating an additional loss function for the generator. We evaluate our model, GANMF, through well-known datasets in the RS community and show improvements over traditional CF approaches and GAN-based models. Through an ablation study on the components of GANMF we aim to understand the effects of our architectural choices. Finally, we provide a qualitative evaluation of the matrix factorization performance of GANMF. ",GAN-based Matrix Factorization for Recommender Systems
46,1484373941078552579,96135022,Mark Riedl,"['LaMDA paper is finally out. This is Google’s new big conversational dialogue model and successor to Meena <LINK>', 'One of the more interesting things about it is that they use multi-task training to train it to generate language responses but also to generate knowledge retrieval queries. They train it on crowd workers using a fact checking tool.', 'Like Meena, they use classifiers for inappropriate content to filter the dialogue training data and to filter generated outputs. They have spent considerable effort on this classifier.', 'Overall, a huge engineering effort. Nothing terribly out of the ordinary for large language models at this particular moment of time (even incorporating retrieval into generative language models)']",https://arxiv.org/abs/2201.08239,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency. ",LaMDA: Language Models for Dialog Applications
47,1484353587706691584,11778512,Mason Porter,"['New working paper: ""A Non-Expert\'s Introduction to Data Ethics for Mathematicians"": <LINK>\n\nNote: This manuscript is not yet refereed. I am sure there are many useful changes to make. Private comments and suggestions are appreciated (mason[at]math[.]ucla[.]edu). <LINK>']",https://arxiv.org/abs/2201.07794,"I give a short introduction to data ethics. My focal audience is mathematicians, but I hope that my discussion will also be useful to others. I am not an expert about data ethics, and my article is only a starting point. I encourage readers to examine the resources that I discuss and to continue to reflect carefully on data ethics and on the societal implications of data and data analysis throughout their lives. ",A Non-Expert's Introduction to Data Ethics for Mathematicians
48,1484286107202297857,1143639019823206402,Sahnawaz Alam,"['Our new paper on ""Two-stroke Quantum Measurement Heat Engine"" is out. Comments are welcome.\n\n<LINK>\n\n@iitgn @PWr_Wroclaw\n #QuantumThermodynamics <LINK>']",http://arxiv.org/abs/2201.06303,We propose and analyze the theoretical model for a two-stroke quantum heat engine with one of the heat baths replaced by a non-selective quantum measurement. We show that the engine's invariant reference state depends on whether the cycle is monitored or unmonitored via diagnostic measurements to determine the engine's work output. We explore in detail the average work output and fluctuations of the proposed heat engine for the monitored and unmonitored cases. We also identify unitary work strokes for which the invariant states can support coherences in the energy basis leading to differing predictions for the average energy change during the unitary work strokes and the average work from the standard two-projective measurement approach. ,Two-stroke Quantum Measurement Heat Engine
49,1484259890939969539,1202973751316537348,Marco Fenucci,"['📢📢 New #paper available on Partial Differential Equations and Applications, by @SpringerNature! This is the fourth paper extracted from my #PhD thesis, and a preprint is freely available on @arxiv #AcademicTwitter \n<LINK> <LINK>', 'This work has been possible also thanks to the support from @Stardust_H2020 #MSCA']",https://arxiv.org/abs/2201.01205,"We first take into account variational problems with periodic boundary conditions, and briefly recall some sufficient conditions for a periodic solution of the Euler-Lagrange equation to be either a directional, a weak, or a strong local minimizer. We then apply the theory to circular orbits of the Kepler problem with potentials of type $1/r^\alpha, \, \alpha > 0$. By using numerical computations, we show that circular solutions are strong local minimizers for $\alpha > 1$, while they are saddle points for $\alpha \in (0,1)$. Moreover, we show that for $\alpha \in (1,2)$ the global minimizer of the action over periodic curves with degree $2$ with respect to the origin could be achieved on non-collision and non-circular solutions. After, we take into account the figure-eight solution of the 3-body problem, and we show that it is a strong local minimizer over a particular set of symmetric periodic loops. ","Local minimality properties of circular motions in $1/r^\alpha$
  potentials and of the figure-eight solution of the 3-body problem"
50,1484214394334498819,60893773,James Bullock,"['Awesome new paper by @ZachHafen + Jonathan Stern + FIRE collaboration: What is required to make a thin disk galaxy?  Thin disks are formed via ""rotating cooling flows"" while irregular galaxies are fed by disordered cold-mode accretion <LINK> <LINK>', 'Thin disk formation appears to require gas accretion from sub-sonic (hot-mode) inflow from the CGM: gas has time to mix and become coherently aligned *prior* to joining the galaxy.  Irregular galaxies are fed by cold-mode gas that remains disordered and hits the disk decoherently https://t.co/WPhkE67Dfy', 'Could provide a clue to why lower mass galaxies and higher redshift galaxies tend to be irregular, while thin-disks emerge only later in cosmic time and almost never in the lowest mass galaxies.  Lots of great work on the way on this subject: @alexbgurvich @AstroBananna ++', 'Thanks to all the collaborators including these tweeters @alexbgurvich @AstroBananna @AndrewWetzel @MBKplus @jorgito__moreno @kjb_astro @PFHopkins_Astro', 'Should have tagged @UCIPhysAstro - as @ZachHafen is the McCue Fellow there.', '@GrumpyKelson @ZachHafen Thanks.  Will have a look. Another paper coming by @AstroBananna looks at bulge formation. We think that higher disk fractions at lower masses would require more hot-mode (or sub-sonic) accretion there, could happen with lower halo baryon fractions than current sims produce']",https://arxiv.org/abs/2201.07235,"We use FIRE simulations to study disk formation in z~0, Milky Way-mass galaxies, and conclude that a key ingredient for the formation of thin stellar disks is the ability for accreting gas to develop an aligned angular momentum distribution via internal cancellation *prior* to joining the galaxy. Among galaxies with a high fraction of their young stars (>70%) in a thin disk (h/R~0.1) we find that: (i) hot, virial-temperature gas dominates the inflowing gas mass on halo scales (>~20 kpc), with radiative losses offset by compression heating; (ii) this hot accretion proceeds until angular momentum support slows inward motion, at which point the gas cools to T~10^4 K or less; (iii) prior to cooling, the accreting gas develops an angular momentum distribution that is aligned with the galaxy disk, and while cooling transitions from a quasi-spherical spatial configuration to a more flattened, disk-like configuration. We show that the existence of this ""rotating cooling flow"" accretion mode is strongly correlated with the fraction of stars forming in a thin disk among a sample of 17 z~0 galaxies spanning a halo mass range of 10^10.5 solar masses to 10^12 solar masses, or a stellar mass range 10^8 solar masses to 10^11 solar masses. Notably, galaxies with a thick disk or irregular morphology do not undergo significant angular momentum alignment of gas prior to accretion and show no correspondence between halo gas cooling and flattening. Our results suggest that rotating cooling flows (or, more generally, rotating subsonic flows) that become coherent and angular momentum-supported prior to direct deposition onto the galaxy are likely a necessary condition for the formation of thin, star-forming disk galaxies in a LambdaCDM universe. ",Hot-mode accretion and the physics of thin-disk galaxy formation
51,1484191308260335617,1275869553650696195,Sarah Mubeen,"['New paper! Our review on how various factors can influence the results of pathway enrichment analysis is now available.\n\nWith @ddomingof @ApitiusHofmann @AKodamullil  \n\npreprint: <LINK>', 'We outline the work of major comparative studies on pathway enrichment analysis and outline the factors that are most influential to the analysis and how they affect results. #PathwayEnrichment #genomics #bioinformatics https://t.co/33ouvcmd9l']",https://arxiv.org/abs/2201.05593,"Pathway enrichment analysis has become a widely used knowledge-based approach for the interpretation of biomedical data. Its popularity has led to an explosion of both enrichment methods and pathway databases. While the elegance of pathway enrichment lies in its simplicity, multiple factors can impact the results of such an analysis which may not be accounted for. Researchers may fail to give influential aspects their due, resorting instead to popular methods and gene set collections, or default settings. Despite ongoing efforts to establish set guidelines, meaningful results are still hampered by a lack of consensus or gold standards around how enrichment analysis should be conducted. Nonetheless, such concerns have prompted a series of benchmark studies specifically focused on evaluating the influence of various factors on pathway enrichment results. In this review, we organize and summarize the findings of these benchmarks to provide a comprehensive overview on the influence of these factors. Our work covers a broad spectrum of factors, spanning from methodological assumptions to those related to prior biological knowledge, such as pathway definitions and database choice. In doing so, we aim to shed light on how these aspects can lead to insignificant, uninteresting, or even contradictory results. Finally, we conclude the review by proposing future benchmarks as well as solutions to overcome some of the challenges which originate from the outlined factors. ",On the influence of several factors on pathway enrichment analysis
52,1484173794923012099,171674815,Mark Marley,"[""Our new paper on polarization of brown dwarfs led by postdoc Aritra Chakrabarty from Sujan Sengupta's group. <LINK>"", 'Aritra is making improvements in the calculation of polarization that should lead to a better exploration of parameter space for inhomogeneous objects. https://t.co/dz4KTX7x8p']",https://arxiv.org/abs/2201.07613,"Young self-luminous giant exoplanets are expected to be oblate in shape owing to the high rotational speeds observed for some objects. Similar to the case of brown dwarfs, the thermal emission from these planets should be polarized by scatterings of molecules and condensate cloud particles, and the rotation-induced asymmetry of the planet's disk would yield to net non-zero detectable polarization. Considering an anisotropic atmosphere, we present here a three-dimensional approach to estimate the disk-averaged polarization that arises due to the oblateness of the planets. We solve the multiple-scattering vector radiative transfer equations at each location on the planet's disk and calculate the local Stokes vectors and then calculate the disk-integrated flux and linear polarization. For a cloud-free atmosphere, the polarization signal is observable only in the visible wavelength region. However, the presence of clouds in the planetary atmospheres leads to a detectable amount of polarization in the infrared wavelength region where the planetary thermal emission peaks. Considering different broad-band filters of the SPHERE-IRDIS instrument of the Very Large Telescope, we present generic models for the polarization at different wavelength bands as a function of their rotation period. We also present polarization models for the Exoplanets $\beta$ Pic b and ROXs 42B b as two representative cases which can guide future observations. Our insights on the polarization of young giant planets presented here would be useful for the upcoming polarimetric observations of the directly imaged planets. ","Polarization of Rotationally Oblate Self-Luminous Exoplanets with
  Anisotropic Atmospheres"
53,1484112726448386048,10666172,Sabine Hossenfelder,"['New paper with Tobias Mistele and @DudeDarkmatter <LINK>', '@nippleointment @DudeDarkmatter ha']",https://arxiv.org/abs/2201.07282,"We make rotation curve fits to test the superfluid dark matter model. Our aim is to investigate whether superfluid dark matter provides satisfactory fits to galactic rotation curves with reasonable stellar mass-to-light ratios. We fitted the superfluid dark matter model to the rotation curves of 169 galaxies in the SPARC sample. We found that the mass-to-light ratios obtained with superfluid dark matter are generally acceptable in terms of stellar populations. However, the best fit mass-to-light ratios have an unnatural dependence on the size of the galaxy in that giant galaxies have systematically lower mass-to-light ratios than dwarf galaxies. A second finding is that the superfluid often fits the rotation curves best when the superfluid's force does not closely resemble that of Modified Newtonian Dynamics (MOND). In that case, we can no longer expect superfluid dark matter to reproduce the phenomenologically observed scaling-relations that make MOND appealing. If, on the other hand, we consider only solutions whose force approximates MOND well, then the total mass of the superfluid is in tension with gravitational lensing data. We conclude that even the best fits with superfluid dark matter are still unsatisfactory. ",Galactic Mass-to-Light Ratios With Superfluid Dark Matter
54,1484073784357404673,948528424926220288,Johannes Røsok Eskilt,['Super excited that my new paper on cosmic birefringence is out! <LINK>'],https://arxiv.org/abs/2201.07682,"We search for the signature of parity-violating physics in the cosmic microwave background, called cosmic birefringence, using the Planck data release 4. We initially find a birefringence angle of $\beta=0.30\pm0.11$ (68% C.L.) for nearly full-sky data. The values of $\beta$ decrease as we enlarge the Galactic mask, which can be interpreted as the effect of polarized foreground emission. Two independent ways to model this effect are used to mitigate the systematic impact on $\beta$ for different sky fractions. We choose not to assign cosmological significance to the measured value of $\beta$ until we improve our knowledge of the foreground polarization. ",Cosmic Birefringence from Planck Data Release 4
55,1483983358560374791,769369833259479040,Yuto Minami,['Our new paper is now available.\n<LINK>'],https://arxiv.org/abs/2201.07682,"We search for the signature of parity-violating physics in the cosmic microwave background, called cosmic birefringence, using the Planck data release 4. We initially find a birefringence angle of $\beta=0.30\pm0.11$ (68% C.L.) for nearly full-sky data. The values of $\beta$ decrease as we enlarge the Galactic mask, which can be interpreted as the effect of polarized foreground emission. Two independent ways to model this effect are used to mitigate the systematic impact on $\beta$ for different sky fractions. We choose not to assign cosmological significance to the measured value of $\beta$ until we improve our knowledge of the foreground polarization. ",Cosmic Birefringence from Planck Data Release 4
56,1483807861759254528,781250438725308416,Nicholas Galitzki,['New paper alert! Ever thought of making a cryogenic carbon fiber truss? More than you wanted to know can be found here: <LINK>'],https://arxiv.org/abs/2201.06094,"We present the design and measured performance of a new carbon fiber strut design that is used in a cryogenically cooled truss for the Simons Observatory Small Aperture Telescope (SAT). The truss consists of two aluminum 6061 rings separated by 24 struts. Each strut consists of a central carbon fiber tube fitted with two aluminum end caps. We tested the performance of the strut and truss by (i) cryogenically cycling and destructively pull-testing strut samples, (ii) non-destructively pull-testing the final truss, and (iii) measuring the thermal conductivity of the carbon fiber tubes. We found that the strut strength is limited by the mounting fasteners and the strut end caps, not the epoxy adhesive or the carbon fiber tube. This result is consistent with our numerical predictions. Our thermal measurements suggest that the conductive heat load through the struts (from 4 K to 1 K) will be less than 1 mW. This strut design may be a promising candidate for use in other cryogenic support structures. ","The Simons Observatory: Design and Measured Performance of a Carbon
  Fiber Strut for a Cryogenic Truss"
57,1483774893623857157,952949678533849088,Kareem El-Badry,"['New paper! We search spectra from the @APOGEEsurvey for mass-transfer binaries in which one star is being spun up by accretion from the other. <LINK> 1/n <LINK>', 'In the last couple years, several unusual binaries have been serendipitously discovered that contain a rapidly rotating star with emission lines (a ""Be star"") and a lower-mass stripped companion. We want to find their progenitors, and quantify how common this evolution is. 2/n', 'So we systematically searched APOGEE spectra of hot and luminous stars for Be stars with mass-transfer companions, which may evolve into Be + stripped star binaries like LB-1 and HR 6819. 3/n https://t.co/Rxb3JS39lf', ""We found exactly one! It's called HD 15124. 4/n https://t.co/GP13h6qNCK"", 'To better understand the system, we got a bunch of follow-up data... 5/n https://t.co/DvwYxGXYti', 'Modeling this data suggests the object is indeed in the short-lived mass-transfer phase we were looking for, and that it will evolve to become a detached Be + stripped star binary (like LB-1, HR 6819, and NGC 1850 BH1). 6/n https://t.co/xB499sEDV9', 'The existence of one object in this short-lived phase implies that there are probably (many) more that have already passed through it. We infer that about 30% of Be stars have gone though a similar mass-transfer phase. 7/n https://t.co/l9j1gjsDRB', 'In parallel, other groups have been finding similar binaries in a later stage of their evolution, when the stripped star is a small, hot core helium burning star. Analyzing all these objects together will test our models for how these binaries for and evolve. Fun stuff! 8/8 https://t.co/5brdpurRan']",https://arxiv.org/abs/2201.05614,"Motivated by recent suggestions that many Be stars form through binary mass transfer, we searched the APOGEE survey for Be stars with bloated, stripped companions. From a well-defined parent sample of 297 Be stars, we identified one mass-transfer binary, HD 15124. The object consists of a main-sequence Be star ($M_{\rm Be}=5.3\pm 0.6 \,M_{\odot}$) with a low-mass ($M_{\rm donor}=0.92\pm 0.22\,M_{\odot}$), subgiant companion on a 5.47-day orbit. The emission lines originate in an accretion disk caused by ongoing mass transfer, not from a decretion disk as in classical Be stars. Both stars have surface abundances bearing imprint of CNO processing in the donor's core: the surface helium fraction is $Y_{\rm He}\approx 0.6$, and the nitrogen-to-carbon ratio is 1000 times the solar value. The system's properties are well-matched by binary evolution models in which mass transfer begins while a $3-5\,M_{\odot}$ donor leaves the main sequence, with the secondary becoming the Be star. These models predict that the system will soon become a detached Be + stripped star binary like HR 6819 and LB-1, with the stripped donor eventually contracting to become a core helium-burning sdOB star. Discovery of one object in this short-lived ($\sim$1 Myr) evolutionary phase implies the existence of many more that have already passed through it and are now Be + sdOB binaries. We infer that $(28_{-16}^{+27})\,\%$ of Be stars have stripped companions, most of which are faint. Together with the dearth of main-sequence companions to Be stars and recent discovery of numerous Be + sdOB binaries in the UV, our results imply that binarity plays an important role in the formation of Be stars. ","Birth of a Be star: an APOGEE search for Be stars forming through binary
  mass transfer"
58,1483750761871941634,725183149769105411,Ryan Mann,"['New paper with Tyler Helmuth: Efficient Algorithms for Approximating Quantum Partition Functions at Low Temperature <LINK> <LINK>', ""@AlhambraAlvaro Thanks! We assume that we know the ground states of the classical Hamiltonian, but Peierls' condition helps too.""]",https://arxiv.org/abs/2201.06533,"We establish an efficient approximation algorithm for the partition functions of a class of quantum spin systems at low temperature, which can be viewed as stable quantum perturbations of classical spin systems. Our algorithm is based on combining the contour representation of quantum spin systems of this type due to Borgs, Koteck\'y, and Ueltschi with the algorithmic framework developed by Helmuth, Perkins, and Regts, and Borgs et al. ","Efficient Algorithms for Approximating Quantum Partition Functions at
  Low Temperature"
59,1483748640267112451,1028710908,Claire Davies,"['🚨🚨New paper alert🚨🚨\n\nMy latest first-author paper ""Scattering and sublimation: a multi-scale view of micron-sized dust in the inclined disc of HD 145718” has been accepted for publication in MNRAS @RAS_Journals &amp; can be accessed on @arXiv at <LINK>\n\n👀 🧵 <LINK>', 'Our observations of this young star (HD 145718) were conducted as part of G-LIGHTS. G-LIGHTS surveyed roughly 30 disc-hosting stars with the @PlanetImager at @GeminiObs [pictured, credit: Gemini Observatory] in Chile. Find out more about the survey: https://t.co/MKIYTFiWPw\n2/n https://t.co/elYHBnM4L8', ""After some data processing wizardry by @EvanAstro, these observations gave us a snapshot of the starlight that is scattered by HD 145718's disc [pictured]. The elliptical and off-centre shape of the scattered light immediately tells us that this disc is seen close to edge-on\n3/n https://t.co/6ZIIxIcTuK"", 'Previous near-infrared interferometric observations, millimetre interferometric observations, and spectrophotometric monitoring had all previously indicated that the disc was likely to be highly inclined so this wasn’t entirely unexpected\n4/n', 'The more intriguing part of our scattered light images of HD 145718 was the arc feature [AH in image] which traces the rear side of the disc. Combined with the ellipse feature [EH in image], this allowed us to probe the vertical extent of the disc scattering surface\n5/n https://t.co/m0QcfL9MGc', ""I employed 2 methods to do this: \n1) geometric modelling of isophotes (i.e. contours of surface brightness), extracted from the scattered light images;\n2) Monte Carlo radiative transfer modelling of the full disc with @tharries' TORUS code\n6/n"", 'Our isophote fitting revealed consistent results for the disc’s geometry in the J- and H-band, with an inclination between 67-71°, a major axis position angle between 0.6° east of north &amp; 1.0° west of north, and a scattering surface aspect ratio (h/r) of 0.10-0.16\n7/n https://t.co/aZvNYjBLZG', 'Our radiative transfer modelling required additional data (cos there’s way too many free parameters to the model otherwise). We collated photometry &amp; IR spectra using SEDBYS (https://t.co/Q3g5wcLJS0) &amp; obtained new infrared interferometry with MIRC-X @CHARAArray [pictured]\n8/n https://t.co/pMeu74r4A4', ""We also looked for previous characterisations of HD 145718's star so that we could keep these parameters fixed in our models, thus reducing the model's free parameters. However, we found that the star’s variability likely affected previous measurements...\n9/n"", '...we re-evaluated the star’s luminosity, radius &amp; visual extinction, finding a consistent L* &amp; R* for bright &amp; faint epochs so long as the total-to-selective extinction increased from 3.1 to 5.0 during dimming events. This suggests large grains exist in the occulting medium\n10/n https://t.co/38DSWVJnZE', 'This left us with 7 parameters to vary: the gas pressure scale height, the settled height of large grains, the scale height radial power law exponent, the maximum size of grains in the disc surface, the outer disc radius, the inclination, and the major axis position angle\n11/n https://t.co/cQPI6twY71', 'We found further evidence for the presence of larger grains in the disc: in the surface layers of the outer disc, grains larger than a few tenths of a micron are required to replicate the pacman-like features seen in our scattered light images…\n12/n https://t.co/XycltzrQji', '…and close to the sublimation rim, grains larger than this must be settled to 10% of the gas pressure scale height in order to replicate the shape of the visibilities extracted from our @CHARAArray data\n13/n https://t.co/VjfRwaVoC3', 'Try as we might, we couldn’t fit the K-band interferometry within the constraints of our radiative transfer model. \n\nWe propose that this is because our model does not incorporate extended emission from disc winds + recommend future modelling efforts take this into account\n14/n', 'Finally, a thanks to my collaborators for their assistance with this work. It’s been a pleasure working with you! @tharries @AstroMonnier @EvanAstro @NarsiAnugu @astrokraus @AstroAaronL @TylerBGardner @cypri2 @TenTheoten (and others not on twitter)\n\nEnd/']",https://arxiv.org/abs/2201.06472,"We present multi-instrument observations of the disc around the Herbig~Ae star, HD~145718, employing geometric and Monte Carlo radiative transfer models to explore the disc orientation, the vertical and radial extent of the near infrared (NIR) scattering surface, and the properties of the dust in the disc surface and sublimation rim. The disc appears inclined at $67-71^{\circ}$, with position angle, PA\,$=-1.0-0.6^{\circ}$, consistent with previous estimates. The NIR scattering surface extends out to $\sim75\,$au and we infer an aspect ratio, $h_{\rm{scat}}(r)/r\sim0.24$ in $J$-band; $\sim0.22$ in $H$-band. Our GPI images and VLTI+CHARA NIR interferometry suggest that the disc surface layers are populated by grains $\gtrsim \lambda/2\pi$ in size, indicating these grains are aerodynamically supported against settling and/or the density of smaller grains is relatively low. We demonstrate that our geometric analysis provides a reasonable assessment of the height of the NIR scattering surface at the outer edge of the disc and, if the inclination can be independently constrained, has the potential to probe the flaring exponent of the scattering surface in similarly inclined ($i\gtrsim70^{\circ}$) discs. In re-evaluating HD~145718's stellar properties, we found that the object's dimming events - previously characterised as UX~Or and dipper variability - are consistent with dust occultation by grains larger, on average, than found in the ISM. This occulting dust likely originates close to the inferred dust sublimation radius at $0.17\,$au. ","Scattering and sublimation: a multi-scale view of $\mu$m-sized dust in
  the inclined disc of HD 145718"
60,1483741814293213185,1131118132054188032,Amin Mekacher,"['New paper! \n\n<LINK>\n\nWith @a_papasavva , we release a dataset for Voat, the fringe alternative to Reddit which had to shutdown in 2020.  \n1/15', 'Our dataset includes no less than 2.3M submissions and 16.2M comments, as well as 110K user profiles and 7K community (subverses) profiles.\n2/15', ""Voat's raison d'être was to become a free speech alternative for online communities banned from major social platforms. As our analysis shows, several major bans enforced by Reddit lead to a massive influx of new users registering on Voat.\n3/15"", 'This dataset can therefore offer some precious insights about the logistics of deplatforming, for example how a collective migration can be spearheaded by one or several major figures within the community (such as for r/GreatAwakening) in a matter of days. \n4/15', ""Within Voat's flagship communities, 3 are directly connected to QAnon (i.e v/QRV, v/theawakening and v/GreatAwakening) and contributed a fair share to the platform's overall activity.\n5/15 https://t.co/El4JeNJjlG"", 'These communities quickly rallied on Voat after being banned from Reddit and, as such, kept contributing to building their conspiratorial narrative without the threat of being censored again.\n6/15', 'Among these three subverses, v/QRV is the community that was championed by Q in one of their drops to become a new home for the Reddit QAnon subreddit.\n7/15', 'Last year, I wrote an editorial in collaboration with @labacdotdev to show how a handful of users became very prolific posters on v/GreatAwakening, thus acting as the “bakers” who curated the news and shared a few selected links with their peers.\nhttps://t.co/VXZn7HzAI1\n8/15', 'Our dataset also contains the links that were shared along every submission we are releasing. As shown on the graph below, the news ecosystem on Voat is mainly composed of alternative news sources, social media links and image hosting services.\n9/15 https://t.co/qqGFivmllY', 'Voat users therefore end up spiraling down in the belief that mainstream media cannot be trusted, which leads to a distrust of any official institution, a stance that fringe media outlets are also amplifying.    \n10/15', 'Finally, even though Voat can intrisically be considered as a fringe platform, analyzing how users interacted on the platform highlights the existence of smaller, isolated echo chambers\n11/15 https://t.co/v9dr0bnfSh', 'These communities, where users interact a lot between each other but do not mingle on other subverses, include the QAnon cluster as well as v/fatpeoplehate\n12/15', 'Their conversations are more likely to share several properties usually attributed to echo chambers, and can be a valuable resource for any researcher interested in analyzing how their specific narrative evolves over time, especially on a fringe platform\n13/15', 'Even though Voat never gathered as much public attention as other fringe platforms, I believe that it can be used as a valuable study case to anyone interested in understanding how alternative platforms profiteer from successive bans enforced by large social networks.\n14/15', 'It also raises some questions about the long-time impact of deplatforming. Voat shut down due to a lack of financial resources, but it still offered a safe place to meet and discuss for over 6 yrs for any user disenfranchised by mainstream social platforms.\n15/15']",https://arxiv.org/abs/2201.05933,"Voat.co was a news aggregator website that shut down on December 25, 2020. The site had a troubled history and was known for hosting various banned subreddits. This paper presents a dataset with over 2.3M submissions and 16.2M comments posted from 113K users in 7.1K subverses (the equivalent of subreddit for Voat). Our dataset covers the whole lifetime of Voat, from its developing period starting on November 8, 2013, the day it was founded, April 2014, up until the day it shut down (December 25, 2020). This work presents the largest and most complete publicly available Voat dataset, to the best of our knowledge. Along with the release of this dataset, we present a preliminary analysis covering posting activity and daily user and subverse registration on the platform so that researchers interested in our dataset can know what to expect. Our data may prove helpful to false news dissemination studies as we analyze the links users share on the platform, finding that many communities rely on alternative news press, like Breitbart and GatewayPundit, for their daily discussions. In addition, we perform network analysis on user interactions finding that many users prefer not to interact with subverses outside their narrative interests, which could be helpful to researchers focusing on polarization and echo chambers. Also, since Voat was one of the platforms banned Reddit communities migrated to, we are confident our dataset will motivate and assist researchers studying deplatforming. Finally, many hateful and conspiratorial communities were very popular on Voat, which makes our work valuable for researchers focusing on toxicity, conspiracy theories, cross-platform studies of social networks, and natural language processing. ","""I Can't Keep It Up."" A Dataset from the Defunct Voat.co News Aggregator"
61,1483661765913112578,934808651830833153,Contijoch Research Laboratory,"['🚨New Paper Alert 🚨\nMotion during #ComputedTomography can degrade image quality. \nLed by @KunalMGupta, we propose a new #CTReconstruction approach which leverages #NeuralNetworks #Differentiable #Rendering\narXiv: <LINK>\nwebsite: <LINK>\n🧵 1/', 'CardiacCT #YesCCT is difficult because the❤️never stops moving! so parts like coronary arteries can end up blurry! Trad. methods use motion models which are case-specific solutions. \nInstead, we proposed a general approach using #differentiable #rendering to recon.\n2/', 'Here you can see what happens when a small vessel (coronary) moves during a CT scan. In a standard recon (FBP), it looks very blurry (2nd row). Our approach (NCT) gives nice results even with very large motions!\n3/ https://t.co/Mu7LDjZX03', 'CardiacCT can also image heart motion (movies!) but can walls can be blurry when they move. \n\nNCT naturally takes advantage of scenarios with multiple gantry rotations.\n\nThis improves reconstruction of all timepoints during the heart cycle!\n4/ https://t.co/Eb22xX5eQc', 'Pushing it a step further, we look at how well NCT works during really complex motions (severe topological changes). Without changing anything about NCT, it worked really well!\n5/ https://t.co/V0zwC0DLcj', 'For more details (how we did it), check out the paper and project website. #ComputerVision #DeepLearning\n\nThis works is a collaboration between @ucsdjacobs @ucsd_cse @ucsdbe and @UCSDHealth @UCSDImaging.\nIt wouldn’t have been possible without support from @NIH @NHLBI.\n\n6/', 'And stay tuned as we work to bring this new approach to clinical data!\n7/']",https://arxiv.org/abs/2201.06574,"Motion during acquisition of a set of projections can lead to significant motion artifacts in computed tomography reconstructions despite fast acquisition of individual views. In cases such as cardiac imaging, motion may be unavoidable and evaluating motion may be of clinical interest. Reconstructing images with reduced motion artifacts has typically been achieved by developing systems with faster gantry rotation or using algorithms which measure and/or estimate the displacements. However, these approaches have had limited success due to both physical constraints as well as the challenge of estimating/measuring non-rigid, temporally varying, and patient-specific motions. We propose a novel reconstruction framework, NeuralCT, to generate time-resolved images free from motion artifacts. Our approaches utilizes a neural implicit approach and does not require estimation or modeling of the underlying motion. Instead, boundaries are represented using a signed distance metric and neural implicit framework. We utilize `analysis-by-synthesis' to identify a solution consistent with the acquired sinogram as well as spatial and temporal consistency constraints. We illustrate the utility of NeuralCT in three progressively more complex scenarios: translation of a small circle, heartbeat-like change in an ellipse's diameter, and complex topological deformation. Without hyperparameter tuning or change to the architecture, NeuralCT provides high quality image reconstruction for all three motions, as compared to filtered backprojection, using mean-square-error and Dice metrics. ",Neural Computed Tomography
62,1483631013238648832,848540707077865472,Hiroyuki Nakano,"['New paper by Kinugawa (@KinugawaTomoya) et al. on two neutron star-black hole binaries, GW200105 and GW200115. Figures 1 and 2 are impressive!\n<LINK>']",https://arxiv.org/abs/2201.06713,"Two neutron star (NS) black hole (BH) binaries, GW200105 and GW200115 found in the LIGO/Virgo O3b run have smaller BH mass of $6-9$$M_{\odot}$ which is consistent with Population I and II origin. Our population synthesis simulations using $10^6$ Population I and II binaries with appropriate initial parameters show consistent binary mass, event rate, and no detection of radio pulsar (PSR) and BH binaries in our Galaxy so far. Especially, we found possible progenitors of GW200105 and GW200115 which were formed at redshift $z=0.15$ and $z=1.6$ with binary mass of $(34M_{\odot},\, 9.2M_{\odot})$ and $(23.7M_{\odot},\, 10.6M_{\odot})$, respectively. The final masses of these binaries are $(6.85M_{\odot},\,2.14M_{\odot})$ and $(6.04M_{\odot},\,1.31M_{\odot})$ which look like $(9.0_{-1.7}^{+1.7}M_{\odot},\, 1.91_{-0.24}^{+0.33}M_{\odot})$ of GW200105 and $(5.9_{-2.5}^{+2.0}M_{\odot},\,1.44_{-0.29}^{+0.85}M_{\odot})$ of GW200115, respectively. We also estimate that $4-20$ PSR-BH binaries in our galaxy will be observed by SKA. The existence of NS-BH binaries in our galaxy can be confirmed in future SKA era. ","Neutron star black hole binaries in LIGO/Virgo O3b run were formed from
  Population I/II binaries"
63,1483627624496721923,1541914753,Gerelt,['A new paper on testable implications of price heterogeneity (with John Quah): <LINK>\nWe show (i) a generality of price heterogeneity; \n(ii) an observational equivalence between preference heterogeneity and stable price heterogeneity.'],https://arxiv.org/abs/2201.03784,"We explore heterogenous prices as a source of heterogenous or stochastic demand. Heterogenous prices could arise either because there is actual price variation among consumers or because consumers (mis)perceive prices differently. Our main result says the following: if heterogenous prices have a distribution among consumers that is (in a sense) stable across observations, then a model where consumers have a common utility function but face heterogenous prices has precisely the same implications as a heterogenous preference/random utility model (with no price heterogeneity). ",Price Heterogeneity as a source of Heterogenous Demand
64,1483449137387483140,1038120916117606400,Benjamin Remy,"['New Cosmo ∩ ML paper out! We propose a new method to solve the mass-mapping inverse problem by sampling from the posterior distribution with a neural prior.\n\n<LINK>\n\nWork with François Lanusse, @Niall_Jeffrey, Jia Liu, @JLStarck, Ken Osato &amp; Tim Schrabback\n(1/n) <LINK>', 'We show that we are able to sample convergence maps with the expected power spectrum using highly efficient annealed HMC sampling https://t.co/Cyql6rEXcr', 'The prior was learned with denoising score matching over high resolution hydrodynamical simulations \\kappaTNG (Osato et al. 2021) https://t.co/L9kGcGuwow', 'We are thus able to provide unprecedented resolution of mass-map reconstruction, alongside uncertainty quantification through the posterior distribution 🤩 https://t.co/mazKE2niDe', 'Find the code and data on the associated github repo: https://t.co/VF5Yzgk4dy using JAX, Haiku &amp; TFP']",https://arxiv.org/abs/2201.05561,"Weak lensing mass-mapping is a useful tool to access the full distribution of dark matter on the sky, but because of intrinsic galaxy ellipticies and finite fields/missing data, the recovery of dark matter maps constitutes a challenging ill-posed inverse problem. We introduce a novel methodology allowing for efficient sampling of the high-dimensional Bayesian posterior of the weak lensing mass-mapping problem, and relying on simulations for defining a fully non-Gaussian prior. We aim to demonstrate the accuracy of the method on simulations, and then proceed to applying it to the mass reconstruction of the HST/ACS COSMOS field. The proposed methodology combines elements of Bayesian statistics, analytic theory, and a recent class of Deep Generative Models based on Neural Score Matching. This approach allows us to do the following: 1) Make full use of analytic cosmological theory to constrain the 2pt statistics of the solution. 2) Learn from cosmological simulations any differences between this analytic prior and full simulations. 3) Obtain samples from the full Bayesian posterior of the problem for robust Uncertainty Quantification. We demonstrate the method on the $\kappa$TNG simulations and find that the posterior mean significantly outperfoms previous methods (Kaiser-Squires, Wiener filter, Sparsity priors) both on root-mean-square error and in terms of the Pearson correlation. We further illustrate the interpretability of the recovered posterior by establishing a close correlation between posterior convergence values and SNR of clusters artificially introduced into a field. Finally, we apply the method to the reconstruction of the HST/ACS COSMOS field and yield the highest quality convergence map of this field to date. ",Probabilistic Mass Mapping with Neural Score Estimation
65,1483105354754015235,22216766,Noah Stephens-Davidowitz,"[""New paper: A Tight Reverse Minkowski Inequality for the Epstein Zeta Function <LINK> with Yael Eisenberg and Oded Regev.\n\nThis is my wonderful student Yael's first paper! I've been trying to solve this problem for ~4 years."", '@galois_counter Paul Epstein: https://t.co/gU9Hamif5M']",https://arxiv.org/abs/2201.05201,"We prove that if $\mathcal{L} \subset \mathbb{R}^n$ is a lattice such that $\det(\mathcal{L}') \geq 1$ for all sublattices $\mathcal{L}' \subseteq \mathcal{L}$, then \[ \sum_{\substack{\mathbf{y}\in\mathcal{L}\\\mathbf{y}\neq\mathbf0}} (\|\mathbf{y}\|^2+q)^{-s} \leq \sum_{\substack{\mathbf{z} \in \mathbb{Z}^n\\\mathbf{z}\neq\mathbf{0}}} (\|\mathbf{z}\|^2+q)^{-s} \] for all $s > n/2$ and all $0 \leq q \leq (2s-n)/(n+2)$, with equality if and only if $\mathcal{L}$ is isomorphic to $\mathbb{Z}^n$. ",A Tight Reverse Minkowski Inequality for the Epstein Zeta Function
66,1483102899890708481,1138744787664871424,Yulin Liu,['I am very glad to post a new empirical paper on Ethereum Gas Fee.\n<LINK>\n@ethereum @VitalikButerin @ethereumJoseph @eth_classic'],https://arxiv.org/abs/2201.05574,"Transaction fee mechanism (TFM) is an essential component of a blockchain protocol. However, a systematic evaluation of the real-world impact of TFMs is still absent. Using rich data from the Ethereum blockchain, mempool, and exchanges, we study the effect of EIP-1559, one of the first deployed TFMs that depart from the traditional first-price auction paradigm. We conduct a rigorous and comprehensive empirical study to examine its causal effect on blockchain transaction fee dynamics, transaction waiting time and security. Our results show that EIP-1559 improves the user experience by making fee estimation easier, mitigating intra-block difference of gas price paid, and reducing users' waiting times. However, EIP-1559 has only a small effect on gas fee levels and consensus security. In addition, we found that when Ether's price is more volatile, the waiting time is significantly higher. We also verify that a larger block size increases the presence of siblings. These findings suggest new directions for improving TFM. ","Empirical Analysis of EIP-1559: Transaction Fees, Waiting Time, and
  Consensus Security"
67,1483093557661806598,434726765,L. Y. Aaron Yung,['Today is Paper Day! Check out the new Stochastic Order Redshift Technique (SORT) method that improves redshift measurements in large photometric surveys! It is also the first paper that features my 2 square degree #Roman lightcones!\n\n<LINK>'],https://arxiv.org/abs/2201.05258,"The stochastic order redshift technique (SORT) is a simple, efficient, and robust method to improve cosmological redshift measurements. The method relies upon having a small ($\sim$10 per cent) reference sample of high-quality redshifts. Within pencil-beam-like sub-volumes surrounding each galaxy, we use the precise dN/d$z$ distribution of the reference sample to recover new redshifts and assign them one-to-one to galaxies such that the original rank order of redshifts is preserved. Preserving the rank order is motivated by the fact that random variables drawn from Gaussian probability density functions with different means but equal standard deviations satisfy stochastic ordering. The process is repeated for sub-volumes surrounding each galaxy in the survey. This results in every galaxy with an uncertain redshift being assigned multiple ""recovered"" redshifts from which a new redshift estimate can be determined. An earlier paper applied SORT to a mock Sloan Digital Sky Survey at $z \lesssim$ 0.2 and accurately recovered the two-point correlation function on scales $\gtrsim$4 $h^{-1}$Mpc. In this paper, we test the performance of SORT in surveys spanning the redshift range 0.75$<z<$2.25. We used two mock surveys extracted from the Small MultiDark-Planck and Bolshoi-Planck N-body simulations with dark matter haloes that were populated by the Santa Cruz semi-analytic model. We find that SORT is able to improve redshift estimates and recover distinctive large-scale features of the cosmic web. Further, it provides unbiased estimates of the redshift-space two-point correlation function $\xi(s)$ on scales $\gtrsim$2.5 $h^{-1}$Mpc, as well as local densities in regions of average or higher density. This may allow improved understanding of how galaxy properties relate to their local environments. ","Galaxy Correlation Function and Local Density from Photometric Redshifts
  Using the Stochastic Order Redshift Technique (SORT)"
68,1483052005119700997,77592002,Ahmad,"['New paper on arxiv! Investigating stellar winds in a galactic spiral arm. <LINK>\n\nIn this case, the initial conditions were extracted from a galaxy simulation. No more isolated spherical clouds - clouds interact together and feel the galactic potential. (1/2) <LINK>', ""In short, winds don't do much to the gas compared with photoionisation - but they create small bubbles and affect how star formation is distributed over clusters. (2/2) https://t.co/2R9SGTagoH""]",https://arxiv.org/abs/2201.04141,"The role of different stellar feedback mechanisms in giant molecular clouds is not well understood. This is especially true for regions with many interacting clouds as would be found in a galactic spiral arm. In this paper, building on previous work by Bending et al., we extract a $500\times500\times100$ pc section of a spiral arm from a galaxy simulation. We use smoothed particle hydrodynamics (SPH) to re-simulate the region at higher resolution (1 M$_\odot$ per particle). We present a method for momentum-driven stellar winds from main sequence massive stars, and include this with photoionization, self-gravity, a galactic potential, and ISM heating/cooling. We also include cluster-sink particles with accretion radii of 0.78 pc to track star/cluster formation. The feedback methods are as robust as previous models on individual cloud scales (e.g. Dale et al.). We find that photoionization dominates the disruption of the spiral arm section, with stellar winds only producing small cavities (at most $\sim$ 30 pc). Stellar winds do not affect the resulting cloud statistics or the integrated star formation rate/efficiency, unlike ionization, which produces more stars, and more clouds of higher density and higher velocity dispersion compared to the control run without feedback. Winds do affect the sink properties, distributing star formation over more low-mass sinks ($\sim 10^2$ M$_\odot$) and producing fewer high-mass sinks ($\sim 10^3$ M$_\odot$). Overall, stellar winds play at best a secondary role compared to photoionization, and on many measures, they have a negligible impact. ",Stellar winds and photoionization in a spiral arm
69,1482989506609438720,45105022,Riccardo Sapienza,['New paper! Self-organized lasers of reconfigurable colloidal assemblies - together with \u2066@Giorgio_Volpe\u2069 \u2066@saxedh\u2069 Manish Trivedi and Kit NG  <LINK>'],https://arxiv.org/abs/2201.05427,"Biological cells self-organize into living materials that uniquely blend structure with functionality and responsiveness to the environment. The integration of similar life-like features in man-made materials remains challenging, yet desirable to manufacture active, adaptive and autonomous systems. Here we show the self-organization of programmable random lasers from the reversible out-of-equilibrium self-assembly of colloids. Random lasing originates from the optical amplification of light undergoing multiple scattering within the dissipative colloidal assemblies and therefore is crucially dependent on their self-organization behavior. Under external light stimuli, these dynamic random lasers are responsive and present a continuously tunable laser threshold. They can thus reconfigure and cooperate by emulating the ever-evolving spatiotemporal relationship between structure and functionality typical of living matter. ",Self-organized lasers of reconfigurable colloidal assemblies
70,1482167537219809280,740183345036853248,Chaminda Bandara,['Our new paper - ChageFormer: A Transformer-Based Siamese Network for Change Detection is now online.\n\narXiv link: <LINK>.\n\nCode: <LINK>.\n@IEEE_GRSS #RemoteSensing <LINK>'],https://arxiv.org/abs/2201.01293,"This paper presents a transformer-based Siamese network architecture (abbreviated by ChangeFormer) for Change Detection (CD) from a pair of co-registered remote sensing images. Different from recent CD frameworks, which are based on fully convolutional networks (ConvNets), the proposed method unifies hierarchically structured transformer encoder with Multi-Layer Perception (MLP) decoder in a Siamese network architecture to efficiently render multi-scale long-range details required for accurate CD. Experiments on two CD datasets show that the proposed end-to-end trainable ChangeFormer architecture achieves better CD performance than previous counterparts. Our code is available at this https URL ",A Transformer-Based Siamese Network for Change Detection
71,1482125389891715074,710263466351681536,Max Vladymyrov 🇺🇦,"['I’m excited to share our new paper on HyperTransformers, a novel architecture for few-shot learning able to generate the weights of a CNN directly from a given support set. 🧵👇\n\n📜: <LINK> with Andrey Zhmoginov and Mark Sandler. <LINK>', '2) We train a transformer model to `convert` a few-shot task description into a small CNN network specialized in solving it on new images. https://t.co/MJhaS4vQv5', '3) This effectively decouples a high-capacity transformer generator from a much smaller inference model. It is different from most of the existing methods, e.g. MAML where the generator and the executing model share the same architecture.', '4) CNN weights are generated layer-by-layer from a combination of layer embedding (features from the last generated layer), and image w/ class embeddings (features directly from the data). The final weights are extracted from output of self-attention (similar to [CLS] tokens). https://t.co/uKuKiHTx5z', '5) What is cool is that we can also add unlabeled samples from the support set into the mix, effectively allowing for semi-supervised few-shot learning!', '6) HyperTransformers are comparable in performance to many competing methods on miniIImageNet and tieredImageNet datasets. https://t.co/Fr3jS8qucQ', '7) But our method especially shines for the case of small target CNN architectures, where the large capacity of the transformer model is the most useful and noticeable. For the 8-channels model we are seeing 5-10% improvement over MAML++! https://t.co/56Y3iPD7hq', '8) As it turns out, for small target models, where every neuron matters, it is important to generate the whole network from a given support set. For larger target models even generating only the last logits layers appears to be sufficient. https://t.co/rdF2l6GhCj', '9) We are really excited about the direction of using Transformers to guide the construction and performance of smaller specialized models e.g. in low-power settings. This has a lot of applications in the areas where high-performance compact personalized networks are being used. https://t.co/34HhwfjRKy']",https://arxiv.org/abs/2201.04182,"In this work we propose a HyperTransformer, a transformer-based model for few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable. Finally, we extend our approach to a semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance. ","HyperTransformer: Model Generation for Supervised and Semi-Supervised
  Few-Shot Learning"
72,1482112281064398850,852846184557416448,Anirban Roy,"['Our new paper (with myself, @ajvengelen, V. Gluscevic, and N. Battaglia) shows how we can measure the circumgalactic medium of @VRubinObs galaxies, applying a method developed for reionization (by @CoraDvorkin &amp; K. Smith) to CMB data from @DOE_Stage_4_CMB \n<LINK>']",https://arxiv.org/abs/2201.05076,"As cosmic microwave background (CMB) photons traverse the Universe, anisotropies can be induced via Thomson scattering (proportional to the integrated electron density; optical depth) and inverse Compton scattering (proportional to the integrated electron pressure; thermal Sunyaev-Zel'dovich effect). Measurements of anisotropy in optical depth $\tau$ and Compton $y$ parameter are imprinted by the galaxies and galaxy clusters and are thus sensitive to the thermodynamic properties of circumgalactic medium and intergalactic medium. We use an analytic halo model to predict the power spectrum of the optical depth ($\tau\tau$), the cross-correlation between the optical depth and the Compton $y$ parameter ($\tau y$), as well as the cross-correlation between the optical depth and galaxy clustering ($\tau g$), and compare this model to cosmological simulations. We constrain the optical depths of halos at $z\lesssim 3$ using a technique originally devised to constrain patchy reionization at a much higher redshift range. The forecasted signal-to-noise ratio is 2.6, 8.5, and 13, respectively, for a CMB-S4-like experiment and a VRO-like optical survey. We show that a joint analysis of these probes can constrain the amplitude of the density profiles of halos to 6.5% and the pressure profile to 13%, marginalizing over the outer slope of the pressure profile. These constraints translate to astrophysical parameters related to the physics of galaxy evolution, such as the gas mass fraction, $f_{\rm g}$, which can be constrained to 5.3% uncertainty at $z\sim 0$, assuming an underlying model for the shape of the density profile. The cross-correlations presented here are complementary to other CMB and galaxy cross-correlations since they do not require spectroscopic galaxy redshifts and are another example of how such correlations are a powerful probe of the astrophysics of galaxy evolution. ","Probing the circumgalactic medium with CMB polarization statistical
  anisotropy"
73,1482003175121080320,4309304548,Eric Michael Smith,"['New paper with Orion Hsu, Rebecca Qian, @stephenroller, @yboureau, and @jaseweston: <LINK>\n\nHuman dialogue evaluation is still an open problem (just like auto evaluation)! Different methods are preferable in different conditions, with no overall winner. <LINK>']",https://arxiv.org/abs/2201.04723,"At the heart of improving conversational AI is the open problem of how to evaluate conversations. Issues with automatic metrics are well known (Liu et al., 2016, arXiv:1603.08023), with human evaluations still considered the gold standard. Unfortunately, how to perform human evaluations is also an open problem: differing data collection methods have varying levels of human agreement and statistical sensitivity, resulting in differing amounts of human annotation hours and labor costs. In this work we compare five different crowdworker-based human evaluation methods and find that different methods are best depending on the types of models compared, with no clear winner across the board. While this highlights the open problems in the area, our analysis leads to advice of when to use which one, and possible future directions. ","Human Evaluation of Conversations is an Open Problem: comparing the
  sensitivity of various methods for evaluating dialogue agents"
74,1481944928242520067,717709692517163008,Stefanie Barz,"[""Today: new insights into how distinguishability and mixedness affect #quantum interference!\nIncluding a nice and intuitive picture of what's going on!\nCheck out our new paper on the #arXiv\n<LINK>\n\nwith @Uni_Stuttgart @QETLabsBristol @MitQpg @IQSTpress <LINK>""]",https://arxiv.org/abs/2201.04655,"We study the impact of distinguishability and mixedness -- two fundamental properties of quantum states -- on quantum interference. We show that these can influence the interference of multiple particles in different ways, leading to effects that cannot be observed in the interference of two particles alone. This is demonstrated experimentally by interfering three independent photons in pure and mixed states and observing their different multiphoton interference, despite exhibiting the same two-photon Hong-Ou-Mandel (HOM) interference. Besides its fundamental relevance, our observation has important implications for quantum technologies relying on photon interference. ",Distinguishability and mixedness in quantum interference
75,1481916191459446784,2706556174,Carlos Rodríguez-Pardo,"['✨Upcoming paper on Generative Models! ✨\n\n✅We present SeamlessGAN, a new method for texture synthesis. It can generate multiple tileable texture stacks from a single input!\n\n🖥️Project: <LINK>\n📚Arxiv: <LINK> \n\nCoauthored with @ele_g2 \n🧵(1/N) <LINK>', '💡Key idea: Using a pre-trained GAN, we can spatially tile the latent spaces of our fully-convolutional generator. This makes it create multiple copies of a texture, with seamless borders between them. Cropping one, we obtain a single texture, which tiles seamlessly with itself. https://t.co/GWEpuXrTdK', '🔍The discriminator provides local quality estimations for the generated textures, which we use as guidance for a texture sampling algorithm. https://t.co/4WQzJIG8Vz', '🔦We also extend previous work on texture synthesis, allowing for the generation of tileable texture stacks, with multiple spatially-coherent maps. https://t.co/yDjPCGKOdR', 'SeamlessGAN works well with textures of different varieties, semantic content or levels of stochasticity. SeamlessGAN is competitive with previous methods, on off-the-shelf quality metrics and computational cost. https://t.co/OiZlKLnemU', ""We started working on this project almost 2 years ago and seeing this published (along with our previous couple of papers, accepted in December) feels like finishing a chapter of my PhD journey ☺️. I'm excited for what's to come. https://t.co/qYHEHMfWnO"", 'We will be using this thread for updates on the paper! It is getting some traction:\nhttps://t.co/xLDVt9mQw3']",https://arxiv.org/abs/2201.05120,"We present SeamlessGAN, a method capable of automatically generating tileable texture maps from a single input exemplar. In contrast to most existing methods, focused solely on solving the synthesis problem, our work tackles both problems, synthesis and tileability, simultaneously. Our key idea is to realize that tiling a latent space within a generative network trained using adversarial expansion techniques produces outputs with continuity at the seam intersection that can be then be turned into tileable images by cropping the central area. Since not every value of the latent space is valid to produce high-quality outputs, we leverage the discriminator as a perceptual error metric capable of identifying artifact-free textures during a sampling process. Further, in contrast to previous work on deep texture synthesis, our model is designed and optimized to work with multi-layered texture representations, enabling textures composed of multiple maps such as albedo, normals, etc. We extensively test our design choices for the network architecture, loss function and sampling parameters. We show qualitatively and quantitatively that our approach outperforms previous methods and works for textures of different types. ",SeamlessGAN: Self-Supervised Synthesis of Tileable Texture Maps
76,1481813852970962944,780129186153426944,Manki,"['yay! my new paper is posted\n\n<LINK>', '@jmcnamara18 thanks a lot!!', 'Of course, I find typos after the paper is posted..']",https://arxiv.org/abs/2201.04634,"We study non-perturbative superpotential generated by D(-1)-branes in type IIB compactifications on orientifolds of Calabi-Yau threefold hypersurfaces. To compute D-instanton superpotential, we study F-theory compactification on toric complete intersection elliptic Calabi-Yau fourfolds. We take the Sen-limit, but with finite $g_s,$ in F-theory compactification with a restriction that all D7-branes are carrying SO(8) gauge groups, which we call the global Sen-limit. In the global Sen-limit, the axio-dilaton is not varying in the compactification manifold. We compute the Picard-Fuchs equations of elliptic Calabi-Yau fourfolds in the global Sen-limit, and show that the Picard-Fuchs equations of the elliptic fourfolds split into that of the underlying Calabi-Yau threefolds and of the elliptic fiber. We then demonstrate that this splitting property of the Picard-Fuchs equation implies that the fourform period of the elliptic Calabi-Yau fourfolds in the global Sen-limit does not contain exponentially suppressed terms $\mathcal{O}(e^{-\pi/g_s})$. With this result, we finally show that in the global Sen-limit, superpotential of the underlying type IIB compactification does not receive D(-1)-instanton contributions. ",D-Instanton Superpotential In String Theory
77,1481760317784748032,1208260954762268678,William Misener,"[""New paper alert! 🚨 Really excited to share this submitted work on sub-Neptune interiors, read it here: <LINK>. We find that current atmospheric mass estimates for these planets could be wrong! 😱 How? It's all about the interior-atmosphere interactions... 1/12 <LINK>"", 'A lot of models of these sorts of planets assume an interior structure with discrete layers, like a silicate core/mantle with a hydrogen atmosphere on top, with little interaction between them. 2/12 https://t.co/syTvd1ewC7', 'But this layered picture might not be quite right: the base of a sub-Neptune atmosphere, especially a young one, is likely really hot (&gt;5000 K). At these temperatures, you might not get an abrupt transition from a pure silicate magma ocean to a pure H/He atmosphere. 3/12 https://t.co/nsSxJouscX', ""Instead, recent work has found that at the conditions at the base of a young sub-Neptune's atmosphere, significant SiO is stable in the vapor phase in the atmosphere at chemical equilibrium (Schlichting+Young21). 4/12 https://t.co/nHQxKaXued"", 'So in this paper, we take a look at the implications of all this silicate vapor on atmospheric structure. We find that silicate vapor abundance falls off quickly as altitude increases. Due to this compositional gradient, convection is actually inhibited near the base! 5/12', 'Why? Normal convection happens because hot gas is less dense than cool gas and rises. But if the hot gas also holds enough heavy vapor, its larger weight can make it more dense than the cooler gas above it, so it tends to sink back down if lifted. 6/12 https://t.co/sI5yWZZ2FI', ""Here's a diagram of how these atmospheres are structured if you account for this: right above the silicate core/mantle, we find a radiative region. Once there's too little silicate vapor to weigh the hot hydrogen gas down, the atmosphere transitions to a convective region. 7/12 https://t.co/5uo6WF9aS3"", ""That diagram isn't to scale: we typically find that the radiative region is very narrow in radius. This leads to a step-like temperature profile, where high base temperatures plunge before leveling off to follow an adiabatic profile (cf. a fully adiabatic profile in black). 8/12 https://t.co/SjZBRhTnet"", ""As you can see from that figure, this radiative region has a big effect on the planet's radius (the dots in the figure)! Specifically, it makes planets a lot smaller than if they had fully convective atmospheres and the same atmospheric mass and base temperature. 9/12"", ""In other words, the atmospheric mass you'd infer from a planet's radius if you include silicates is much (~5x) more than what you'd get if you assumed its atmosphere was fully convective. The effect is strongest for young planets. We ran time evolution sims to show this. 10/12 https://t.co/0zyqiC2amf"", 'The differences expected depend on other variables too, like planet mass, equilibrium temperatures, opacities, and initial conditions, that you can check out in the paper. This work has been submitted but not yet reviewed, so any comments are welcome! Thanks for reading! 11/12 https://t.co/3eP170Nd2g', ""And, as a shameless self-plug, if this work sounds so cool that you want all your friends/foes to hear about it, I'm very open to talk invitations! 😊😬😂 12/12""]",https://arxiv.org/abs/2201.04299,"Substantial silicate vapor is expected to be in chemical equilibrium at temperature conditions typical of the silicate-atmosphere interface of sub-Neptune planets, which can exceed 5000 K. Previous models of the atmospheric structure and evolution of these exoplanets, which have been used to constrain their atmospheric mass fractions, have neglected this compositional coupling. In this work, we show that silicate vapor in a hydrogen-dominated atmosphere acts as a condensable species, decreasing in abundance with altitude. The resultant mean molecular weight gradient inhibits convection at temperatures above $\sim 4000$ K, inducing a near-surface radiative layer. This radiative layer decreases the planet's total radius compared to a planet with the same base temperature and a convective, pure H/He atmosphere. Therefore, we expect silicate vapor to have major effects on the inferred envelope mass fraction and thermal evolution of sub-Neptune planets. We demonstrate that differences in radii, and hence in inferred atmospheric masses, are largest for planets which have larger masses, equilibrium temperatures, and atmospheric mass fractions. The effects are largest for younger planets, but differences can persist on gigayear time-scales for some sub-Neptunes. For a $10 M_\oplus$ planet with $T_\mathrm{eq}=1000$ K and an age of $\sim 300$ Myr, an observed radius consistent with an atmospheric mass fraction of 10% when accounting for silicate vapor would be misinterpreted as indicating an atmospheric mass fraction of 2% if a H/He-only atmosphere were assumed. The presence of silicate vapor in the atmosphere is also expected to have important implications for the accretion and loss of primordial hydrogen atmospheres. ","The importance of silicate vapor in determining the structure, radii,
  and envelope mass fractions of sub-Neptunes"
78,1481695396753670144,939498802767044608,Stephan,"['Do prices go up or down? New preprint + code on applying multi-agent RL to a real-business cycle #economics model with consumers, firms, and government. \n\nBy excellent past intern Michael Curry!\n\nPaper:<LINK>\nCode: <LINK>', ""Michael's here: @michaeljcurry1 !"", 'and of course the other coauthors @alexrtrott, Soham Phade, and @yubai01 !']",https://arxiv.org/abs/2201.01163,"Real economies can be modeled as a sequential imperfect-information game with many heterogeneous agents, such as consumers, firms, and governments. Dynamic general equilibrium (DGE) models are often used for macroeconomic analysis in this setting. However, finding general equilibria is challenging using existing theoretical or computational methods, especially when using microfoundations to model individual agents. Here, we show how to use deep multi-agent reinforcement learning (MARL) to find $\epsilon$-meta-equilibria over agent types in microfounded DGE models. Whereas standard MARL fails to learn non-trivial solutions, our structured learning curricula enable stable convergence to meaningful solutions. Conceptually, our approach is more flexible and does not need unrealistic assumptions, e.g., continuous market clearing, that are commonly used for analytical tractability. Furthermore, our end-to-end GPU implementation enables fast real-time convergence with a large number of RL economic agents. We showcase our approach in open and closed real-business-cycle (RBC) models with 100 worker-consumers, 10 firms, and a social planner who taxes and redistributes. We validate the learned solutions are $\epsilon$-meta-equilibria through best-response analyses, show that they align with economic intuitions, and show our approach can learn a spectrum of qualitatively distinct $\epsilon$-meta-equilibria in open RBC models. As such, we show that hardware-accelerated MARL is a promising framework for modeling the complexity of economies based on microfoundations. ","Analyzing Micro-Founded General Equilibrium Models with Many Agents
  using Deep Reinforcement Learning"
79,1481684873484111878,914066073435148288,marta_desimone,"['#StarFormingRegions are definitely not quiet and calm environments...winds, explosions, high velocity outflows coming from all around. \nWhat about a train of #shocks as result of the clash of an expanding #bubble? Check out our new #SOLIS paper💥✨:  <LINK> <LINK>']",https://arxiv.org/abs/2201.03434,"There is evidence that the star formation process is linked to the intricate net of filaments in molecular clouds, which may be also due to gas compression from external triggers. We studied the southern region of the Perseus NGC 1333 molecular cloud, known to be heavily shaped by similar external triggers, to shed light on the process that perturbed the filament where the Class 0 IRAS4 protostars lie. We use new IRAM-NOEMA observations of SiO and CH3OH, both known to trace violent events as shocks, toward IRAS 4A as part of the Large Program Seeds Of Life in Space (SOLIS). We detected three parallel elongated ($>$6000 au) structures, called fingers, with narrow line profiles (~1.5 $km s^{-1}$) peaked at the cloud systemic velocity, tracing gas with high density (5-20 $10^5 cm^{-3}$) and high temperature (80-160 K). They are chemically different, with the northern finger traced by both SiO and CH3OH ([CH3OH]/[SiO]~160-300), while the other two only by SiO ([CH3OH]/[SiO]$<$ 40). Among various possibilities, a train of three shocks, distanced by $>$5000 yr, would be consistent with the observations if a substantial fraction of silicon, frozen onto the grain mantles, is released by the shocks.We suggest that the shock train is due to an expanding gas bubble, coming behind NGC 1333 from the southwest and clashing against the filament, where IRAS 4A lies. Finally, we propose a solution to the two-decades long debate on the nature and origin of the widespread narrow SiO emission observed in the south part of NGC 1333, namely that it is due to unresolved trains of shocks. ","A train of shocks at 3000 au scale? Exploring the clash of an expanding
  bubble into the NGC 1333 IRAS 4 region. SOLIS XIV"
80,1481551888587886596,2906135523,JudeCroston,"['*New paper!* @OgNimaeb’s latest, epic, @LOFAR radio galaxies paper is out today: <LINK>. We dug deep into the disconnect between morphology and accretion mode and what controls each of these… (1/5)', 'This plot is my favourite - it shows **only jets of similar power (a single decade in 150MHz luminosity)** and what I’m fairly sure is the very clearest evidence that host galaxy (stellar) mass strongly influences jet disruption and so FR class… (2/5) https://t.co/8liuQ1zzII', 'And we see a beautifully clear connection between specific star-formation rate and accretion mode, presumably both driven by availability of dense cold gas…(3/5) https://t.co/5WSGeq5BPE', 'We find a complete disconnect in the drivers of accretion class and morphology, and indistinguishable FR2 structures of both accretion modes (this is a slightly more twitter-friendly v. of paper Fig 6)… (4/5) https://t.co/s2T4AhigSX', 'Lots more in the paper, &amp; thanks to the many people in the LOFAR deep fields project who made it possible (inc @nudomarinero, @dunkenj, @cygnus_ww - apols if I missed anyone else on twitter) (5/5)']",https://arxiv.org/abs/2201.04433,"Radio-loud active galaxies have two accretion modes [radiatively inefficient (RI) and radiatively efficient (RE)], with distinct optical and infrared signatures, and two jet dynamical behaviours, which in arcsec- to arcmin-resolution radio surveys manifest primarily as centre- or edge-brightened structures [Fanaroff-Riley (FR) class I and II]. The nature of the relationship between accretion mode and radio morphology (FR class) has been the subject of long debate. We present a comprehensive investigation of this relationship for a sample of 286 well-resolved radio galaxies in the LOFAR Two-metre Sky Survey Deep Fields (LoTSS-Deep) first data release, for which robust morphological and accretion mode classifications have been made. We find that two-thirds of luminous FRII radio galaxies are RI, and identify no significant differences in the visual appearance or source dynamic range (peak/mean surface brightness) of the RI and RE FRIIs, demonstrating that both RI and RE systems can produce FRII structures. We also find a significant population of low-luminosity FRIIs (predominantly RI), supporting our earlier conclusion that FRII radio structures can be produced at all radio luminosities. We demonstrate that in the luminosity range where both morphologies are present, the probability of producing FRI or FRII radio morphology is directly linked to stellar mass, while across all morphologies and luminosities, RE accretion occurs in systems with high specific star formation rate, presumably because this traces fuel availability. In summary, the relationship between accretion mode and radio morphology is very indirect, with host-galaxy environment controlling these two key parameters in different ways. ",Accretion mode versus radio morphology in the LOFAR Deep Fields
81,1481302300815761410,450276753,Phil Maffettone,"['New paper with a more pedagogical angle. Less of new ML techniques, more conceptualizing these techniques for users at @BrookhavenLab and how to deploy them during beamtime for on-the-fly analysis or adaptive experimentation. \n<LINK>']",https://arxiv.org/abs/2201.03550,"Imaging, scattering, and spectroscopy are fundamental in understanding and discovering new functional materials. Contemporary innovations in automation and experimental techniques have led to these measurements being performed much faster and with higher resolution, thus producing vast amounts of data for analysis. These innovations are particularly pronounced at user facilities and synchrotron light sources. Machine learning (ML) methods are regularly developed to process and interpret large datasets in real-time with measurements. However, there remain conceptual barriers to entry for the facility general user community, whom often lack expertise in ML, and technical barriers for deploying ML models. Herein, we demonstrate a variety of archetypal ML models for on-the-fly analysis at multiple beamlines at the National Synchrotron Light Source II (NSLS-II). We describe these examples instructively, with a focus on integrating the models into existing experimental workflows, such that the reader can easily include their own ML techniques into experiments at NSLS-II or facilities with a common infrastructure. The framework presented here shows how with little effort, diverse ML models operate in conjunction with feedback loops via integration into the existing Bluesky Suite for experimental orchestration and data management. ","Machine learning enabling high-throughput and remote operations at
  large-scale user facilities"
82,1481212268348067843,167105924,Jayne Birkby,"['New year, new paper! Big congrats to lead author Eleanor Spring! High res spectra can potentially reveal O2 biosignatures in light reflected by rocky exoplanets but nature never makes it easy. Everything is spinning &amp; rarely in sync: <LINK> @eleanor_spring 🧵 1/N <LINK>', 'Hot Jupiters &amp; some hab zone worlds often orbit faster than their host star spins. @eleanor_spring shows it makes spectral lines in star light reflected by planets look rotationally broadened &amp; harder to detect with HRS. BUT! Min impact on @Proximab16. Onwards to the ELTs! 2/N', 'Is 51 Peg b highly reflective? Previous studies are at odds, but none account for differential broadening. If we do the same, our ground-based upper limit gives a very dark world 76ppm(!) &amp; Ag&lt;0.2 at nominal HJ radius. But included, a brighter world is still possible Ag&lt;0.6. 4/N', 'Check out the difference when injecting a bright model planet into the data with (left) and without (right) the broadening. The induced broadening reduces S/N by ~x3 here! 5/N https://t.co/S7wdFpThJb', 'Tell me more you say? Ok, well, we used the exoplanet-hunting precision RV machines HARPS/-N, which were key to eliminating even tiny RV variations that would otherwise reduced our sensitivity. EPRV for win! 6/N', 'Bonus for stellar activity fans! Treat yourself to a look at Fig 12. Those stellar residuals near superior conjunction could be due to activity causing small variations in line depth during the spectral sequence. Affects only a small phase window, but when planet is brightest 7/N', ""Bonus for HRS fans! Clean spectra directly, or the CCFs? We find similar sensitivity either way, so +1 for options! But CCF approach uses observed templates that miss exoplanet's imprinted albedo function. For abundances, may need to clean spectra &amp; use model template instead 8/N"", 'Big thanks to @eleanor_spring for her epic hard work on these data &amp; all our co-As. If you’re inspired to try it out yourself, all the spectra are public &amp; the paper contains lots of details about how it was all done! Still lots to improve on before the ELTs see first light. 9/9']",https://arxiv.org/abs/2201.03600,"The extreme contrast ratios between stars and their planets at optical wavelengths make it challenging to isolate the light reflected by exoplanet atmospheres. Yet, these reflective properties reveal key processes occurring in the atmospheres, and they also span wavelengths that include the potential O$_2$ biosignature. High resolution cross-correlation spectroscopy (HRCCS) offers a robust avenue for developing techniques to extract exoplanet reflection spectra. We aimed to extract the optical reflected light spectrum of the non-transiting hot Jupiter 51 Peg b by adapting techniques designed to remove tellurics in infrared HRCCS to instead remove optical stellar lines. Importantly, we investigated the so far neglected impact of the broadening of the reflected host star spectrum due to the difference between the stellar rotation and the planet's orbital velocity. We used 484, R=115000 optical spectra of 51 Peg b from HARPS-N and HARPS, which we aligned to the exact stellar rest frame in order to effectively remove the contaminating host star. However, some stellar residuals remained, likely due to stellar activity. We cross-correlated with an appropriately broadened synthetic stellar model to search for the planet's Doppler-shifting spectrum. We detect no significant reflected light from 51 Peg b and report a S/N=3 upper limit on the contrast ratio of 76.0 ppm (7.60x10$^{-5}$) when including broadening, and 24.0 ppm (2.40x10$^{-5}$) without. These upper limits rule out radius and albedo combinations of previously claimed detections. Broadening can significantly impact the ability of HRCCS to extract reflected light spectra and must be considered when determining the contrast ratio, radius, and albedo of the planet. Asynchronous systems (Prot,$_{\star}\ne$ Porb) are most affected, including most hot Jupiters as well as Earth-size planets in the traditional habitable zones of some M-dwarfs. ","Black Mirror: The impact of rotational broadening on the search for
  reflected light from 51 Pegasi b with high resolution spectroscopy"
83,1481209705725046785,3232627976,Khyati Malhan,"['Paper on @arxiv : ""New constraints on the dark matter  density profiles of dwarf galaxies from proper motions of (accreted) globular cluster streams"". \nw/ @monica_valluri  @ktfreese &amp; R.Ibata\n<LINK>\n@ESAGaia #GaiaEDR3 #darkmatter <LINK>', 'Background: Many of the observed GC streams of the MW are of ""accreted"" origin -- i.e., these GC streams originally evolved within their parent dwarf galaxies and only later merged with the MW  [see this video by R.G.Carlberg] https://t.co/bLtGpC5ncy', 'This implies that the present-day physical properties of (accreted) GC streams can inform us about the #darkmatter density profiles inside their parent dwarf galaxies (cusp/core?), and ultimately inform us about the nature of #darkmatter (cold/warm/fuzzy?). https://t.co/GKhK23JjMQ', 'This idea -- of using (acc.) GC streams to measure #darkmatter density profiles inside their parent dwarf galaxies -- was first described in 2021 in https://t.co/ccxBNxy3MB. https://t.co/vHoPQLSekQ', 'In this previous paper, we showed that GCs that are accreted within cuspy CDM subhalos produce streams with larger physical widths (w) and higher line-of-sight velocity dispersions (σvlos) as compared to those streams that accrete inside cored subhalos. [see image] https://t.co/y1Gc5qxyJV', 'Result: In this new paper, we show that the dispersion in the tangential velocity of streams (σvTan) is another parameter that is also sensitive to the central DM density profile of their parent dwarfs. These σvTan values of MW streams can be computed using excellent #GaiaEDR3', 'In simulations (""black"" and ""gray"" points), we find that cuspy CDM subhalos produce streams with larger σvTan as compared to cored subhalos.\nThrough observations, we use #GaiaEDR3 to compute σvTan of 5 streams --  ""GD-1"", ""Sylgr"", ""Phlegethon"", ""Fjorm"" and ""Gjoll"" (color points). https://t.co/nTe5k1axQU', 'Thus, the comparison of σvTan values between simulations and observations indicates that the progenitor GC of these five MW streams were most likely accreted inside cored #darkmatter subhalos (with M_subhalo&gt;∼ 10^{8−9} M_sun).', 'This work was supported by @TheOKC @mpi_astro  @AvHStiftung  @IAU_org :)']",https://arxiv.org/abs/2201.03571,"The central density profiles in low-mass and dwarf galaxy halos depend strongly on the nature of dark matter. Recently, in Malhan et al. (2021), we employed N-body simulations to show that the cuspy cold dark matter (CDM) subhalos predicted by cosmological simulations can be differentiated from cored subhalos using the properties of accreted globular cluster streams -- those stellar streams produced from the tidal stripping of globular clusters that initially evolved within their parent dwarf galaxies and only later merged with the Milky Way. In particular, we previously found that clusters that are accreted within cuspy CDM subhalos produce streams with larger physical widths and higher line-of-sight velocity dispersions as compared to those streams that accrete inside cored subhalos. Here, we use the same suite of simulations to demonstrate that the dispersion in the tangential velocity of streams ($\sigma_{v_\mathrm{Tan}}$) is another parameter that is also sensitive to the central DM density profile of their parent dwarfs. We find that globular clusters that were accreted from cuspy CDM subhalos produce streams with larger $\sigma_{v_\mathrm{Tan}}$ than those that were accreted inside cored subhalos. Furthermore, we use Gaia EDR3 observations of multiple GC streams to compare their $\sigma_{v_\mathrm{Tan}}$ values with simulations. This comparison indicates that the five observed streams we analyze are more likely to be associated with globular clusters of `accreted' rather than `in situ' origin. We also find evidence that the progenitor globular clusters of these streams were probably accreted inside cored DM subhalos (with $M_{\rm subhalo}\buildrel > \over \sim$ $10^{8-9}M_{\odot}$). ","New constraints on the dark matter density profiles of dwarf galaxies
  from proper motions of globular cluster streams"
84,1481184462213943296,127070843,Michael Sentef,"['First paper of the new year: Cavity engineering of Hubbard U via phonon polaritons <LINK>\n@MPSDHamburg and RWTH Aachen <LINK>', 'In which we show how the combination of nonlinear electron-phonon coupling (relevant for instance in organic materials, https://t.co/tdE9a1xmHh) and coupling of phonons to photons in a cavity can increase or decrease Hubbard U, depending on the state of the cavity']",https://arxiv.org/abs/2201.04128,"Pump-probe experiments have suggested the possibility to control electronic correlations by driving infrared-active phonons with resonant midinfrared laser pulses. In this work we study two possible microscopic nonlinear electron-phonon interactions behind these observations, namely coupling of the squared lattice displacement either to the electronic density or to the double occupancy. We investigate whether photon-phonon coupling to quantized light in an optical cavity enables similar control over electronic correlations. We first show that inside a dark cavity electronic interactions increase, ruling out the possibility that $T_c$ in superconductors can be enhanced via effectively decreased electron-electron repulsion through nonlinear electron-phonon coupling in a cavity. We further find that upon driving the cavity, electronic interactions decrease. Two different regimes emerge: (i) a strong coupling regime where the phonons show a delayed response at a time proportional to the inverse coupling strength, and (ii) an ultra-strong coupling regime where the response is immediate when driving the phonon polaritons resonantly. We further identify a distinctive feature in the electronic spectral function when electrons couple to phonon polaritons involving an infrared-active phonon mode, namely the splitting of the shake-off band into three bands. This could potentially be observed by angle-resolved photoemission spectroscopy. ",Cavity engineering of Hubbard $U$ via phonon polaritons
85,1481167388838174721,776765039726460929,Carlo Felice Manara,"['Also new paper by Testi &amp; Natta (and me, @giulod @ilaria_pascucci @jpw_hawaii @astrodesim ) is out! <LINK>\nWe reassessed all disk populations in Ophiucus (L1688) and other nearby star-forming regions using the recent @ESAGaia work to study disk evolution. <LINK>']",https://arxiv.org/abs/2201.04079,"(Abridged) We present a study of the disk population in L1688, the densest and youngest region in Ophiuchus, and we compare it with other nearby regions of different age, namely Lupus, Chamaeleon I, Corona Australis, Taurus and Upper Scorpius. We select our L1688 sample using a combination of criteria (ALMA data, Gaia, optical/near-IR spectroscopy) and determine stellar and disk properties, specifically stellar mass (Mstar), average population age, mass accretion rate (Macc) and disk dust mass (Mdust). a) In L1688 the relations between Macc and Mstar, Mdust and Mstar, and Macc and Mdust have a roughly linear trend with slopes 1.8-1.9 for the first two relations and ~1 for the third, similarly to what found in the other regions. b) When ordered according to the characteristic age of each region, Macc decreases as 1/t, when corrected for the different stellar mass content; Mdust follows roughly the same trend between 0.5 and 5 Myr, but has an increase of a factor ~3 at ages of 2-3 Myr. We suggest that this could result from an earlier planet formation, followed by collisional fragmentation that temporarily replenishes the millimeter-size grain population. c) The dispersion of Macc and Mdust around the best-fitting relation with Mstar, as well as that of Macc versus Mdust are large: we find that the dispersions have continuous distributions with a log-normal shape and similar width (~0.8 dex). The amount of dust observed at ~1 Myr does not appear to be sufficient to assemble the majority of planetary systems, which suggests an earlier planetary cores formation. The dust mass traces to a large extent the disk gas mass evolution. Two properties remain puzzling: the steep dependence of Macc and Mdust on Mstar and the cause of the large dispersion in the three relations analyzed in this paper, in particular the one of the Macc versus Mdust relation. ","The protoplanetary disk population in the rho-Ophiuchi region L1688 and
  the time evolution of Class II YSOs"
86,1481167298253803521,1150810215023091712,Dr. Emrah Tiras,['Our new paper is on arXiv (with @Kandemir__M and Dr. Fischer): <LINK>\nWe developed a joint-simulation framework for segmented neutrino detectors around the globe. \n--- \nKullanımda olan segmentlere ayrılmış nötrino dedektörlerini ortak bir simülasyonda toparladık.'],https://arxiv.org/abs/2201.03689,"NuSD: Neutrino Segmented Detector is a Geant4-based user application that simulates inverse beta decay event in a variety of segmented scintillation detectors developed by different international collaborations. This simulation framework uses a combination of cross-programs and libraries including Geant4, ROOT and CLHEP developed and used by high energy physics community. It will enable the neutrino physics community to simulate and study neutrino interactions within different detector concepts using a single program. In addition to neutrino simulations in segmented detectors, this program can also be used for various research projects that use of scintillation detectors for different physics purposes. ","NuSD: A Geant4 based simulation framework for segmented anti-neutrino
  detectors"
87,1481019555510054912,1185977761032110080,Kazumasa Ohno (大野 和正),"['We posted a new paper about transmission spectrum of ringed exoplanets🪐\n<LINK>\nRings may give alternative explanation for flat spectra of extremely low-density exoplanets. Also check the HST observation of a puffy cold giant, HIP41378 f!\n<LINK>', 'We derived analytical model of how to include ring effect in transmission spectra. We considered all possible viewing geometry, in contrast to face-on ring in my previous paper(https://t.co/dtJ8InvHtU). https://t.co/v8bnT0KJsC', 'Transmission spectrum is computed by summing up the transmittance of annulus at each radial distance. For ringed planets, we need to split the annulus into ring-free and ring-overlapped parts, as in Range E in the above figure. We provide how to split the annulus.', 'We also derived a simple  model that can allow us to ""post-process"" the ring effects in a pre-computed ring-free spectrum. So, you can readily implement the ring effects in your spectrum computed by any spectrum code using our analytical prescription.', 'In general, ring flattens the spectrum significantly except for nearly edge-on rings. There are two factors flattening the spectrum. One is the overlap of atmospheric annulus and projected ring. Second is the overestimation of atmospheric scale height for ringed planets. https://t.co/W5iVOAftXw', 'Ring creates flat spectrum even at longer wavelength, in contrast to hazy spectrum showing spectral slope. So, we expect JWST observations can distinguish haze and ring. Alam et al. (https://t.co/ftpAqoVWwk) and Ohno &amp; Tanaka (https://t.co/dtJ8InvHtU) discussed about this point. https://t.co/ywWN3DRyKt', ""We also briefly discussed if ring's spectral feature is observable. The feature could emerge if the ring's optical depth is around unity and if tiny particles present. Although these factors are not predictable, the feature, if detected, can further tell us about ring's property https://t.co/9Yznwpuz0s"", 'These are the main parts of the paper, so I stop thread here, as the paper is still under review. I am grateful to @jjfplanet for supporting me and Munazza Alam and @PlanetaryGao at @CarnegiePlanets and HIP41378 f observation team for involving me to this exciting project!', '@johannateske @jjfplanet @PlanetaryGao @CarnegiePlanets Thanks for kind words!', '@MartianColonist @jjfplanet @PlanetaryGao @CarnegiePlanets Thanks for kind words! Your question at ESO conference also motivated this project!', '@nespinozap @jjfplanet @PlanetaryGao @CarnegiePlanets Thanks for kind words, Nestor!']",https://arxiv.org/abs/2201.02794,"Recent observations revealed that several extremely low-density exoplanets show featureless transmission spectra. While atmospheric aerosols are a promising explanation for both the low density and featureless spectra, there is another attractive possibility: the presence of circumplanetary rings. Previous studies suggested that rings cause anomalously large transit radii. However, it remains poorly understood how rings affect the transmission spectrum. Here, we provide a framework to characterize the transmission spectra of ringed exoplanets. We develop an analytical prescription to include rings in the transmission spectra for arbitrarily viewing geometries. We also establish a simple post-processing model that can include the ring's effects on precomputed ring-free spectra. The ring flattens the transmission spectrum for a wide range of viewing geometries, consistent with the featureless spectra of extremely low-density exoplanets. Near-future observations by JWST at longer wavelengths would be able to distinguish the aerosol and ring scenarios. We also find that rocky rings might cause a silicate feature at $\sim$10 $\mu$m if the ring's optical depth is around unity. Thus, the ring's spectral features, if detected, would provide tight constrains on the physical properties of exoplanetary rings. We also discuss the ring's stability and suggest that thick rings are sustainable only at the equilibrium temperature of $\lesssim$300 K for the ring's age comparable to Kepler planets. This might indicate the intrinsic deficit of thick rings in the Kepler samples, unless rings are much younger than the planets as suggested for Saturn. ","A Framework for Characterizing Transmission Spectra of Exoplanets with
  Circumplanetary Rings"
88,1481005647378456578,282700930,utku,"['Matching fine-tuning performance with a linear probe! Check out our new paper: “Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning” 👇!\n📜<LINK>\n📁<LINK> with @dumoulinv @hugo_larochelle and @mc_mozer <LINK>', '2) A linear classifier on top of the last layer makes sense when transferring to similar domains. But what if we transfer to dissimilar domains? This is where intermediate features shine🌞! We find a (-)correlation between the accuracy gains and domain affinity. https://t.co/mePI5Qxhik', '3)  Furthermore, we show that the fine-tuning solution can be approximated using intermediate features. This leads us to Head2Toe. Head2Toe selects and combines the most useful intermediate features using a single linear layer and aims to match fine-tuning performance. https://t.co/eB3w3au1Cg', '4) We benchmark Head2Toe on VTAB-1k (19 classification tasks, each with 1000 examples) using a ResNet50. Baselines using all intermediate features alone bring ~7% improvement over Linear. Adding the feature selection (Head2Toe) provides another +2% and matches Fine-tuning🎯. https://t.co/YVcrjfCJRf', '5) We achieve similar results with the ViT-B/16 model. What if we fine-tune the model after selecting the intermediate features? When we do ❌-validation to decide whether to fine-tune the backbone or not (Head2Toe-FT+), we get an intriguing 🌠5% improvement over fine-tuning. https://t.co/jLTQD4HEPE', '6) 💰“Linear” is cheap but often performs poorly compared to fine-tuning. With Head2Toe, we match fine-tuning performance, without sacrificing simplicity or cost of Linear. On average Head2Toe requires only 5x more storage than Linear and 100x less than Fine-tuning. https://t.co/BnurD0aXS3', '7) We find that feature-wise selection used by Head2Toe generalizes better than other alternatives such as selecting entire layers. We hypothesise that including only the most important features from each layer reduces overfitting, which results in better predictions 🤹. https://t.co/LHnJuLMR4y', '8) Do different tasks select different features ? We find that, apart from a small fraction of tasks, most tasks seem to share less than 20% of the features, which highlights the importance of doing the feature selection for each target task separately 🤹. https://t.co/tHlYeiXBwK', ""9) …and for me the most important results of the paper 🌟: Throwing more features at the algorithm brings better results📈! We haven't pushed the limits yet; and are looking forward to applying Head2Toe to larger architectures and to different modalities. https://t.co/Rutv1ipp6t"", 'x) There are many promising follow-up ideas to pursue. If you are interested, please reach out📨! I believe that studying transfer and improving how algorithms scale is critical for making large models practical. We can all help. All we need is a large pretrained model and a gpu.', '@tw_killian @dumoulinv @hugo_larochelle @mc_mozer I guess we had to constrained of ""short and catchy""', '@tw_killian @dumoulinv @hugo_larochelle @mc_mozer thanks', '@tw_killian @dumoulinv @hugo_larochelle @mc_mozer oo now I get it\nhttps://t.co/LoBUeUGpMu', ""@realRahulRK @dumoulinv @hugo_larochelle @mc_mozer I think all it depends how you feel about the work. Did you get enough feedback or do you expect the paper to change a lot? We already submitted our work and got feedback. We are at a point, where we don't expect the paper change much, thus sharing putting on arxiv."", '@realRahulRK @dumoulinv @hugo_larochelle @mc_mozer Another benefit of Arxiv is, though not ideal, you learn about related work you missed (https://t.co/VqxE0eIdkx)']",https://arxiv.org/abs/2201.03529,"Transfer-learning methods aim to improve performance in a data-scarce target domain using a model pretrained on a data-rich source domain. A cost-efficient strategy, linear probing, involves freezing the source model and training a new classification head for the target domain. This strategy is outperformed by a more costly but state-of-the-art method -- fine-tuning all parameters of the source model to the target domain -- possibly because fine-tuning allows the model to leverage useful information from intermediate layers which is otherwise discarded by the later pretrained layers. We explore the hypothesis that these intermediate layers might be directly exploited. We propose a method, Head-to-Toe probing (Head2Toe), that selects features from all layers of the source model to train a classification head for the target-domain. In evaluations on the VTAB-1k, Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost hundred folds or more, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning. ","Head2Toe: Utilizing Intermediate Representations for Better Transfer
  Learning"
89,1480920576285892612,79918104,Chris Amato,"[""I'm excited about our new #AAAI2022 paper that seeks to better understand actor-critic methods for multi-agent RL. In particular, while state-based critics are popular, we show they have theoretical and empirical drawbacks that need to be considered. \n\n<LINK>""]",https://arxiv.org/abs/2201.01221,"Centralized Training for Decentralized Execution, where training is done in a centralized offline fashion, has become a popular solution paradigm in Multi-Agent Reinforcement Learning. Many such methods take the form of actor-critic with state-based critics, since centralized training allows access to the true system state, which can be useful during training despite not being available at execution time. State-based critics have become a common empirical choice, albeit one which has had limited theoretical justification or analysis. In this paper, we show that state-based critics can introduce bias in the policy gradient estimates, potentially undermining the asymptotic guarantees of the algorithm. We also show that, even if the state-based critics do not introduce any bias, they can still result in a larger gradient variance, contrary to the common intuition. Finally, we show the effects of the theories in practice by comparing different forms of centralized critics on a wide range of common benchmarks, and detail how various environmental properties are related to the effectiveness of different types of critics. ","A Deeper Understanding of State-Based Critics in Multi-Agent
  Reinforcement Learning"
90,1480891707071905798,19606850,David Manheim,"['Why do some artificial intelligence safety researchers view ""Highly Reliable Agent Designs"" or ""Agent Foundations"" research as useful, or even critical? (And why do others disagree?)\n\nA new paper by Issa Rice, and myself, explains: <LINK>\n\nThread (1/6)', 'There\'s been a lot of discussion about risks from advanced AI, and several ""agendas"" for achieving safety. (See, for example, https://t.co/9RglQVNb3c ) \n\nOne, championed by researchers at @MIRIBerkeley, is ""Highly Reliable Agent Designs"" (HRAD)\n(2/6) https://t.co/imDi8Hdsta', '@MIRIBerkeley This is only one of a number of approaches, but it is fundamentally different from most proposals in that this work attempts to understand the problem more clearly, and formally / mathematically.\n(3/6)', ""@MIRIBerkeley Because it's different, many people seem unsure what it is trying to do. \nSo in the paper we extend the discussion from Issa's original post - https://t.co/wqBqMTNdnP - and lay out four separate cases which have been presented for the value of this work.\n(4/6) https://t.co/Wb0WSBF5v2"", '@MIRIBerkeley The first two cases for the work, incidental utility and deconfusion, seem widely accepted, but are insufficient on their own for allowing AI safety. The latter two are more ambitious, and few people fully accept the arguments, but they would provide a provably safe AI.\n(5/6)', ""@MIRIBerkeley In short, if we think that AI safety is a potentially important issue, and if we're less than certain that other approaches will be sufficient, there are a variety of ways that this work is critical, as the diagram shows.\n\nNow, go read the paper ;) https://t.co/15suw7F6JZ\n(6/6) https://t.co/KNc1tsK75F"", '@MIRIBerkeley PS. Thanks to the Long Term Futures Fund for funding this work, and to the Modelling Transformative AI team, https://t.co/CmN2pE2iWY (including @daniel_eth,) as well as those who provided feedback on the paper, including @robbensinger, @RohinMShah, and @romanyam. https://t.co/Zm139fIjSY', '@MIRIBerkeley @daniel_eth @robbensinger @rohinmshah @romanyam Also, I should have tagged the first author, who did most of the work on the paper: @riceissa.']",https://arxiv.org/abs/2201.02950,"Several different approaches exist for ensuring the safety of future Transformative Artificial Intelligence (TAI) or Artificial Superintelligence (ASI) systems, and proponents of different approaches have made different and debated claims about the importance or usefulness of their work in the near term, and for future systems. Highly Reliable Agent Designs (HRAD) is one of the most controversial and ambitious approaches, championed by the Machine Intelligence Research Institute, among others, and various arguments have been made about whether and how it reduces risks from future AI systems. In order to reduce confusion in the debate about AI safety, here we build on a previous discussion by Rice which collects and presents four central arguments which are used to justify HRAD as a path towards safety of AI systems. We have titled the arguments (1) incidental utility,(2) deconfusion, (3) precise specification, and (4) prediction. Each of these makes different, partly conflicting claims about how future AI systems can be risky. We have explained the assumptions and claims based on a review of published and informal literature, along with consultation with experts who have stated positions on the topic. Finally, we have briefly outlined arguments against each approach and against the agenda overall. ","Arguments about Highly Reliable Agent Designs as a Useful Path to
  Artificial Intelligence Safety"
91,1480855297690419200,106843613,Jacob Haqq Misra,"['My new paper with @ben_hayworth gives a description, validation, and code release of the HEXTOR energy balance climate model, which can be used for both Earth-like and tidally-locked planets\n\n<LINK>']",https://arxiv.org/abs/2201.02685,"This paper describes the Habitable Energy balance model for eXoplaneT ObseRvations (HEXTOR), which is a model for calculating latitudinal temperature profiles on Earth and other rapidly rotating planets. HEXTOR includes a lookup table method for calculating the outgoing infrared radiative flux and planetary albedo, which provides improvements over other approaches at parameterizing radiative transfer in an energy balance model. Validation cases are presented for present-day Earth and other Earth-sized planets with aquaplanet and land planet conditions from 0 to 45 degrees obliquity. A tidally locked coordinate system is also implemented in the energy balance model, which enables calculation of the horizontal temperature profile for planets in synchronous rotation around low mass stars. This coordinate transformed model is applied to cases for TRAPPIST-1e as defined by the TRAPPIST Habitable Atmosphere Intercomparison protocol, which demonstrates better agreement with general circulation models compared to the latitudinal energy balance model. Advances in applying energy balance models to exoplanets can be made by using general circulation models as a benchmark for tuning as well as by conducting intercomparisions between energy balance models with different physical parameterizations. ","An Energy Balance Model for Rapidly and Synchronously Rotating
  Terrestrial Planets"
92,1480798108028223488,776765039726460929,Carlo Felice Manara,['New paper by PhD student @ESO and Konkoli Obs Gabriella Zsidi. <LINK>\nWe study the accretion variability of CR Cha on timescales from minutes to a decade. The peak is at weeks-month timescale. \n(expect more to come from Gabriella and from #PENELLOPELP) <LINK>'],https://arxiv.org/abs/2201.03396,"Classical T Tauri stars are surrounded by a circumstellar disk from which they are accreting material. This process is essential in the formation of Sun-like stars. Although often described with simple and static models, the accretion process is inherently time variable. Our aim is to examine the accretion process of the low-mass young stellar object CR Cha on a wide range of timescales from minutes to a decade by analyzing both photometric and spectroscopic observations from 2006, 2018, and 2019. We carried out period analysis on the light curves of CR Cha from the TESS mission and the ASAS-SN and the ASAS-3 databases. We studied the color variations of the system using $I,J,H,K$-band photometry obtained contemporaneously with the TESS observing window. We analyzed the amplitude, timescale, and the morphology of the accretion tracers found in a series of high-resolution spectra obtained in 2006 with the AAT/UCLES, in 2018 with the HARPS, and in 2019 with the ESPRESSO and the FEROS spectrographs. All photometric data reveal periodic variations compatible with a 2.327 days rotational period, which is stable in the system over decades. Moreover, the ASAS-SN and ASAS-3 data hint at a long-term brightening by 0.2 mag, between 2001 and 2008, and of slightly less than 0.1 mag in the 2015 - 2018 period. The near-infrared color variations can be explained by either changing accretion rate or changes in the inner disk structure. Our results show that the amplitude of the variations in the H$\alpha$ emission increases on timescales from hours to days/weeks, after which it stays similar even when looking at decadal timescales. On the other hand, we found significant morphological variations on yearly/decadal timescales, indicating that the different physical mechanisms responsible for the line profile changes, such as accretion or wind, are present to varying degrees at different times. ","Accretion variability from minutes to decade timescales in the classical
  T Tauri star CR Cha"
93,1480746665690632194,1196266674954985472,Nirmal Raj,"['1/n New paper with @HostertMatheus, @davemckeen, and Maxim Pospelov.\n<LINK>\n""Dark sectors in neutron-shining-through-a-wall and nuclear absorption signals""\nThread follows. <LINK>', '2/n Particle species w/ the same quantum numbers can ""mix"", e.g. you can emit a photon and measure it elsewhere as a Z boson. We explored how to discover feeble quantum mixings b/w the neutron and a hypothetical ""dark neutron"", a species that could resolve many puzzles in Nature.', '3/n Quantum mixing lets you ""shine neutrons through a wall"".  Throw a neutron at a wall, and it cd detect it as a dark neutron and let it thru, which cd then regenerate as a neutron on the other side. See attached cartoon where a batter faces a neutron bowler through a wall. https://t.co/jXO1aESFlh', '4/n We show that this process can be exploited at IsoDAR, an imminent experiment consisting of a very intense proton beam paired to a very large detector, to be situated deep underground at (most likely) Yemilab, South Korea.', '5/n While the original design of IsoDAR is to do important physics with neutrinos, the shielding of the beam target cd be a ""wall"" thru which beam-produced neutrons could shine &amp; then show up in the detector. Thus, for free, IsoDAR will be a world-leading hunter of dark neutrons. https://t.co/7IHiXtG3RQ', ""6/n Next, dark neutrons could make up the #darkmatter in our galaxy (and the universe beyond). They could then slip through the Earth's layers, touch underground detectors and convert to neutrons -- promptly eaten up by detector nuclei with flashes of light spilling over. https://t.co/Lqh66Ceodq"", '7/n (And that can be used to constrain even more feeble mixings between the neutron and dark neutron.)', '8/n Finally, we explored a few other promising ways to find dark neutrons -- how to reinterpret searches for ultracold neutrons disappearing from their traps, how to catch dark neutrons at spallation sources, and so on.']",https://arxiv.org/abs/2201.02603,"We propose new searches for $n^\prime$, a dark baryon that can mix with the Standard Model neutron. We show that IsoDAR, a proposal to place an intense cyclotron near a large-volume neutrino detector deep underground, can look for $n\to n^\prime \to n$ transitions with much lower backgrounds than surface experiments. This neutron-shining-through-a-wall search would be possible without any modifications to the experiment and would provide the strongest laboratory constraints on the $n$-$n^\prime$ mixing for a wide range of mass splittings. We also consider dark neutrons as dark matter and show that their nuclear absorption at deep-underground detectors such as SNO and Borexino places some of the strongest limits in parameter space. Finally, we describe other $n^\prime$ signatures, such as neutrons shining through walls at spallation sources, reactors, and the disappearance of ultracold neutrons. ","Dark sectors in neutron-shining-through-a-wall and nuclear absorption
  signals"
94,1480734101086748676,232294292,Gary Marcus 🇺🇦,"['Everything you always wanted to know about the Winograd Scheme Challenge but were afraid to ask. \n\nLLMs have beaten it, yet common sense remains unsolved... \n\nNew paper in Arxiv by Kocijan, Davis, Lukasiewicz, Morgenstern and myself. <LINK>']",https://arxiv.org/abs/2201.02387,"The Winograd Schema Challenge -- a set of twin sentences involving pronoun reference disambiguation that seem to require the use of commonsense knowledge -- was proposed by Hector Levesque in 2011. By 2019, a number of AI systems, based on large pre-trained transformer-based language models and fine-tuned on these kinds of problems, achieved better than 90% accuracy. In this paper, we review the history of the Winograd Schema Challenge and assess its significance. ",The Defeat of the Winograd Schema Challenge
95,1480721567118098438,857378262640611328,Jason Steffen,"['New paper dropped from my former postdoc Chenliang Huang and graduate student David Rice (@Magrathea_staff)\n\n<LINK>\n\nA wicked fast, open source planetary structure code. <LINK>']",https://arxiv.org/abs/2201.03094,"MAGRATHEA is an open-source planet structure code that considers the case of fully differentiated spherically symmetric interiors. Given the mass of each layer and the surface temperature, the code iterates the boundary conditions of the hydrostatic equations using the method of shooting to a fitting point in order to find the planet radius. The first version of MAGRATHEA supports a maximum of four layers of iron, silicates, water, and ideal gas. With a few exceptions, the temperature profile can be chosen between isothermal, isentropic, and user-defined functions. The user has many options for the phase diagram and equation of state in each layer and we document how to add additional equations of state. We present MAGRATHEA's capabilities and discuss its applications. We encourage the community to participate in the development of MAGRATHEA at this https URL ","MAGRATHEA: an open-source spherical symmetric planet interior structure
  code"
96,1480678989790257152,1281776703895994370,Artur Izmaylov,['Being greedy and other ways to beat classical shadow tomography using measurement grouping techniques in our new paper on further reduction of the number of measurements for molecular Hamiltonian expectation value estimation <LINK>'],https://arxiv.org/abs/2201.01471,"Obtaining the expectation value of an observable on a quantum computer is a crucial step in the variational quantum algorithms. For complicated observables such as molecular electronic Hamiltonians, a common strategy is to present the observable as a linear combination of measurable fragments. The main problem of this approach is a large number of measurements required for accurate sampling of the observable's expectation value. We consider several partitioning schemes based on grouping of commuting multi-qubit Pauli products with the goal of minimizing the number of measurements. Three main directions are explored: 1) grouping commuting operators using the greedy approach, 2) involving non-local unitary transformations for measuring, and 3) taking advantage of compatibility of some Pauli products with several measurable groups. The last direction gives rise to a general framework that not only provides improvements over previous methods but also connects measurement grouping approaches with recent advances in techniques of shadow tomography. Following this direction, we develop two new measurement schemes that achieve a severalfold reduction in the number of measurements for a set of model molecules compared to previous state-of-the-art methods. ","Deterministic improvements of quantum measurements with grouping of
  compatible operators, non-local transformations, and covariance estimates"
97,1480368742265667584,916709762062012416,Guangwei,"['<LINK> New paper! We found very strong water and CO emission features on ultra-hot Jupiter KELT-20b which orbits an A type star! Providing evidence that strong FUV/UV flux from the star is the key to drive thermal inversion in exoplanet atmospheres!', 'Check out the beautiful water and CO emission features! And the awesome ATMO retrieval fit with a reduced chi-sq of 1.00 !! https://t.co/YsPjSUhpuB', 'We also compared KELT-20b with other hot Jupiters and it has the largest water (based on metric in Mansfield et al. 2021) and CO (inferred from Spitzer 4.5 vs WFC3 brightness temperature ratio) emission features ever measured in any exoplanets to date! https://t.co/IGDbCZkzXA', 'Thanks @JDLothringer (Lothringer &amp; Barman 2019) for the original inspiration for this project! Thanks @ExoSing for the retrieval! And all other collaborators! @cornerof_thesky @tadkomacek']",https://arxiv.org/abs/2201.02261,"Know thy star, know thy planetary atmosphere. Every exoplanet with atmospheric measurements orbits around a star, and the stellar environment directly affects the planetary atmosphere. Here we present the emission spectrum of ultra-hot Jupiter KELT-20b which provides an observational link between host star properties and planet atmospheric thermal structure. It is currently the only planet with thermal emission measurements in the $T_{eq}\sim$2200K range that orbits around an early A-type star. By comparing it with other similar ultra-hot Jupiters around FGK stars, we can better understand how different host star types influence planetary atmospheres. The emission spectrum covers 0.6 to 4.5 $\mu m$ with data from TESS, HST WFC3/G141, and Spitzer 4.5 $\mu m$ channel. KELT-20b has a 1.4 $\mu m$ water feature strength metric of S$_{H_2O}$ = -0.097$\pm$0.02 and a blackbody brightness temperature difference of 528K between WFC3/G141 (T$_b$=2402$\pm$14K) and Spitzer 4.5 $\mu m$ channel (T$_b$=2930$\pm59$K). These very large H$_2$O and CO emission features combined with the A-type host star make KELT-20b a unique planet among other similar hot Jupiters. The abundant FUV, NUV, and optical radiation from its host star (T$_{eff}=8720\pm250$K) is expected to be the key that drives its strong thermal inversion and prominent emission features based on previous PHOENIX models calculations. ","Strong H$_2$O and CO emission features in the spectrum of KELT-20b
  driven by stellar UV irradiation"
98,1480364616710606853,1424524891785674752,Francisco Villaescusa-Navarro,"['Can we infer cosmological parameters with one single galaxy? We have investigated this crazy idea in our new paper <LINK>, and the answer, according to our simulations and models, seems to be yes. <LINK>', 'We have trained neural networks using hundreds of thousands of individual galaxies from the @camels_project.  We find that knowing the properties of a single, generic, galaxy allows our models to infer the value of Omega_m with a 10% precision, accounting for astrophysical', 'uncertainties, as modeled in CAMELS. We believe that the explanation behind these results is that galaxy properties live in a low-dimensional manifold that is sensitive to Omega_m (or perhaps Omega_b/Omega_m). https://t.co/pp3sGtXmQv', 'We note however that our models are not robust (yet!); i.e. they are simulation dependent. We would love to get feedback on this surprising result. With @DavidSpergel, @eelregit, @PabloLemosP']",https://arxiv.org/abs/2201.02202,"Galaxies can be characterized by many internal properties such as stellar mass, gas metallicity, and star-formation rate. We quantify the amount of cosmological and astrophysical information that the internal properties of individual galaxies and their host dark matter halos contain. We train neural networks using hundreds of thousands of galaxies from 2,000 state-of-the-art hydrodynamic simulations with different cosmologies and astrophysical models of the CAMELS project to perform likelihood-free inference on the value of the cosmological and astrophysical parameters. We find that knowing the internal properties of a single galaxy allow our models to infer the value of $\Omega_{\rm m}$, at fixed $\Omega_{\rm b}$, with a $\sim10\%$ precision, while no constraint can be placed on $\sigma_8$. Our results hold for any type of galaxy, central or satellite, massive or dwarf, at all considered redshifts, $z\leq3$, and they incorporate uncertainties in astrophysics as modeled in CAMELS. However, our models are not robust to changes in subgrid physics due to the large intrinsic differences the two considered models imprint on galaxy properties. We find that the stellar mass, stellar metallicity, and maximum circular velocity are among the most important galaxy properties to determine the value of $\Omega_{\rm m}$. We believe that our results can be explained taking into account that changes in the value of $\Omega_{\rm m}$, or potentially $\Omega_{\rm b}/\Omega_{\rm m}$, affect the dark matter content of galaxies. That effect leaves a distinct signature in galaxy properties to the one induced by galactic processes. Our results suggest that the low-dimensional manifold hosting galaxy properties provides a tight direct link between cosmology and astrophysics. ",Cosmology with one galaxy?
99,1480358744512999434,11346882,Larry Lee,"['New paper day! Getting it out after years of exploring this space — Tackling combinatorial problems at particle colliders with Machine Learning™. It’s full of action and intrigue, and I hope you take a look! (Also it’s pretty short and pretty pretty)\n\n<LINK> <LINK>', 'Work largely performed by Anthony Badea of @harvardphysics. Also w/ @JohnHuth1, @CodeMonkey8, @RiccardoPoggi, Will Fawcett, and myself from @UTKPhysAstro']",https://arxiv.org/abs/2201.02205,"High-multiplicity signatures at particle colliders can arise in Standard Model processes and beyond. With such signatures, difficulties often arise from the large dimensionality of the kinematic space. For final states containing a single type of particle signature, this results in a combinatorial problem that hides underlying kinematic information. We explore using a neural network that includes a Lorentz Layer to extract high-dimensional correlations. We use the case of squark decays in $R$-Parity-violating Supersymmetry as a benchmark, comparing the performance to that of classical methods. ","Solving Combinatorial Problems at Particle Colliders Using Machine
  Learning"
100,1480258071167483907,771716816817324032,Francesco Tudisco,['Our new paper provides the first known lower bound on the number of nodal domains of the graph p-Laplacian. One of the notable consequences: the higher-order Cheeger inequality for the one-Laplacian is *exact* on trees. <LINK>'],https://arxiv.org/abs/2201.01248,"Inspired by the linear Schr\""odinger operator, we consider a generalized $p$-Laplacian operator on discrete graphs and present new results that characterize several spectral properties of this operator with particular attention to the nodal domain count of its eigenfunctions. Just like the one-dimensional continuous $p$-Laplacian, we prove that the variational spectrum of the discrete generalized $p$-Laplacian on forests is the entire spectrum. Moreover, we show how to transfer Weyl's inequalities for the Laplacian operator to the nonlinear case and prove new upper and lower bounds on the number of nodal domains of every eigenfunction of the generalized $p$-Laplacian on generic graphs, including variational eigenpairs. In particular, when applied to the linear case $p=2$, in addition to recovering well-known features, the new results provide novel properties of the linear Schr\""odinger operator. ",Nodal domain count for the generalized graph $p$-Laplacian
101,1479896332009680897,1236348926153895937,Ricardo Salinas,['Probably worth mentioning we had a new paper using Gemini data this week <LINK>'],https://arxiv.org/abs/2201.01759,"NGC 4382 is a merger-remnant galaxy that has been classified as morphological type E2, S0, and even Sa. In this work, we performed a photometric and spectroscopic analysis of the globular cluster (GC) system of this peculiar galaxy in order to provide additional information about its history. We used a combination of photometric data in different filters, and multi-object and long-slit spectroscopic data obtained using the Gemini/GMOS instrument. The photometric analysis of the GC system, using the Gaussian Mixture Model algorithm in the colour plane, reveals a complex colour distribution within $R_\mathrm{gal}<5$ arcmin (26.1 kpc), showing four different groups: the typical blue and red subpopulations, a group with intermediate colours, and the fourth group towards even redder colours. From the spectroscopic analysis of 47 GCs, confirmed members of NGC\,4382 based on radial velocities, we verified 3 of the 4 photometric groups from the analysis of their stellar populations using the ULySS code. NGC 4382 presents the classic blue ($10.4\pm2.8$ Gyr, $\mathrm{[Fe/H]}=-1.48\pm0.18$ dex) and red ($12.1\pm2.3$ Gyr, $\mathrm{[Fe/H]}=-0.64\pm0.26$ dex) GCs formed earlier in the lifetime of the galaxy, and a third group of young GCs ($2.2\pm0.9$ Gyr; $\mathrm{[Fe/H]}=-0.05\pm0.28$ dex). Finally, analysis of long-slit data of the galaxy reveals a luminosity-weighted mean age for the stellar population of $\sim$2.7 Gyr, and an increasing metallicity from [Fe/H]=$-0.1$ to $+0.2$ dex in $R_\mathrm{gal}<10$ arcsec (0.87 kpc). These values, and other morphological signatures in the galaxy, are in good agreement with the younger group of GCs, indicating a common origin as a result of a recent merger. ","The complex globular cluster system of the S0 galaxy NGC 4382 in the
  outskirts of the Virgo Cluster"
102,1479519929451769859,1069796467248754688,Irene Zhang,"['New research vision paper just dropped on arXiv! Super excited about this new research direction on carbon-aware datacenter software with Tom Anderson, Adam Belay, @asafcidon, Mosharah Chowdhury (and funded by the @NSF  and @vmwareresearch). <LINK>']",https://arxiv.org/abs/2201.02120,"The end of Dennard scaling and the slowing of Moore's Law has put the energy use of datacenters on an unsustainable path. Datacenters are already a significant fraction of worldwide electricity use, with application demand scaling at a rapid rate. We argue that substantial reductions in the carbon intensity of datacenter computing are possible with a software-centric approach: by making energy and carbon visible to application developers on a fine-grained basis, by modifying system APIs to make it possible to make informed trade offs between performance and carbon emissions, and by raising the level of application programming to allow for flexible use of more energy efficient means of compute and storage. We also lay out a research agenda for systems software to reduce the carbon footprint of datacenter computing. ",Treehouse: A Case For Carbon-Aware Datacenter Software
103,1479455154038616070,953616889,Justin Read,"[""New paper out today by Sivertsson et al. on how well we can estimate the density of dark matter near the Sun (rhodm) given that the Milky Way's disc is wobbling, ringing and not axisymmetric: \n\n<LINK>\n\n[1/4]"", 'We use N-body simulations of a Milky Way bomarded by a satellite accretion, either recently (like Sagittarius) or in the distant past (like Gaia-Sausage-Enceladus) to test local mass modelling methods.\n\n[2/4]', 'We find that typical assumptions like a locally flat rotation curve can yield systematic errors on rhodm of up to order unity! However, including these terms (that are measurable, at least in principle) reduces systematic errors to ~30%.\n\n[3/4]', ""At higher accuracy than this, we need to worry about the time dependent terms - i.e. the fact that the Milky Way's disc is wobbling and ringing. It remains to be seen if these terms can be understood well enough to yield estimates of rhodm at better than 30% precision.\n\n[4/4]""]",https://arxiv.org/abs/2201.01822,"The density of dark matter near the Sun is important for experiments hunting for dark matter particles in the laboratory, and for constraining the local shape of the Milky Way's dark matter halo. Estimates to date have typically assumed that the Milky Way's stellar disc is axisymmetric and in a steady-state. Yet the Milky Way disc is neither, exhibiting prominent spiral arms and a bar, and vertical and radial oscillations. We assess the impact of these assumptions on determinations of the local dark matter density by applying a free-form, steady-state, Jeans method to two different N-body simulations of Milky Way-like galaxies. In one, the galaxy has experienced an ancient major merger, similar to the hypothesized Gaia-Sausage-Enceladus; in the other, the galaxy is perturbed more recently by the repeated passage and slow merger of a Sagittarius-like dwarf galaxy. We assess the impact of each of the terms in the Jeans-Poisson equations on our ability to correctly extract the local dark matter density from the simulated data. We find that common approximations employed in the literature - axisymmetry and a locally flat rotation curve - can lead to significant systematic errors of up to a factor ~1.5 in the recovered surface mass density ~2kpc above the disc plane, implying a fractional error on the local dark matter density of order unity. However, once we add in the tilt term and the rotation curve term in our models, we obtain an unbiased estimate, consistent with the true value within our 95% confidence intervals for realistic 20% uncertainties on the baryonic surface density of the disc. Other terms - the axial tilt, 2:nd Poisson and time dependent terms - contribute less than 10% to the local dark matter density (given current data) and can be safely neglected for now. In the future, as more data become available, these terms will need to be included in the analysis. ","Estimating the local dark matter density in a non-axisymmetric wobbling
  disc"
104,1479430336035672066,213005784,Josué Tonelli-Cueto,"['First preprint of 2022 is out! New bounds for the best rank-one approx. ratio of [partially] symmetric tensors—aka [multi]homogeneus polynomials—using probability.\n\n<LINK>\n\n#research #math #paper #preprint #tensors #polynomials #approximaton #probability\n#arxiv', '@_pbrdng Out of an e-mail asking Khazhgali if a probabilistic technique in another paper would be interesting in the world of symmetric tensors.']",https://arxiv.org/abs/2201.02191,"We provide new upper and lower bounds on the minimum possible ratio of the spectral and Frobenius norms of a (partially) symmetric tensor. In the particular case of general tensors our result recovers a known upper bound. For symmetric tensors our upper bound unveils that the ratio of norms has the same order of magnitude as the trivial lower bound $1/\sqrt{n^{d-1}}$, when the order of a tensor $d$ is fixed and the dimension of the underlying vector space $n$ tends to infinity. However, when $n$ is fixed and $d$ tends to infinity, our lower bound is better than $1/\sqrt{n^{d-1}}$. ",Probabilistic bounds on best rank-one approximation ratio
105,1479392397503176707,953282872550584321,Patrick Tamburo 🌲,"[""🌲\nNew paper on arXiv today, with an overview of the PINES survey, our reduction pipeline, and early results! Here's our estimated transit detection sensitivity, showing that we're sensitive to sub-Neptunes around ~97% of our 393 L/T dwarf sample. \n<LINK> <LINK>"", 'And a huge thank you to my collaborators for their hard work on this project: @phildelio, @AllieMMcCarthy, @johannamvos, @astro_daniella, @jfaherty, @philosicist, @AgolEric, @jnskinner, @ssagear (and those not on Twitter). Exciting things to come!', ""@brettmor @phildelio @AllieMMcCarthy @johannamvos @astro_daniella @jfaherty @philosicist @AgolEric @jnskinner @ssagear @ErikMeier18 It's a very handy tool for us!""]",https://arxiv.org/abs/2201.01794,"We describe the Perkins INfrared Exosatellite Survey (PINES), a near-infrared photometric search for short-period transiting planets and moons around a sample of 393 spectroscopically confirmed L- and T-type dwarfs. PINES is performed with Boston University's 1.8 m Perkins Telescope Observatory, located on Anderson Mesa, Arizona. We discuss the observational strategy of the survey, which was designed to optimize the number of expected transit detections, and describe custom automated observing procedures for performing PINES observations. We detail the steps of the $\texttt{PINES Analysis Toolkit}$ ($\texttt{PAT}$), software that is used to create light curves from PINES images. We assess the impact of second-order extinction due to changing precipitable water vapor on our observations and find that the magnitude of this effect is minimized in Mauna Kea Observatories $\textit{J}$-band. We demonstrate the validity of $\texttt{PAT}$ through the recovery of a transit of WASP-2 b and known variable brown dwarfs, and use it to identify a new variable L/T transition object: the T2 dwarf WISE J045746.08-020719.2. We report on the measured photometric precision of the survey and use it to estimate our transit detection sensitivity. We find that for our median brightness targets, assuming contributions from white noise only, we are sensitive to the detection of 2.5 $R_\oplus$ planets and larger. PINES will test whether the increase in sub-Neptune-sized planet occurrence with decreasing host mass continues into the L and T dwarf regime. ","The Perkins INfrared Exosatellite Survey (PINES) I. Survey Overview,
  Reduction Pipeline, and Early Results"
106,1479104736158633987,788878862,Sam Gill,['New paper on arrival this morning! An EBLM close to the brown dwarf boundary! Read here -&gt; <LINK>'],https://arxiv.org/abs/2201.01713,"We are using precise radial velocities from CORALIE together with precision photometry from the Next Generation Transit Survey (NGTS) to follow up stars with single-transit events detected with the Transiting Exoplanet Survey Satellite (TESS). As part of this survey we identified a single transit on the star TIC-320687387, a bright (T=11.6) G-dwarf observed by TESS in Sector 13 and 27. From subsequent monitoring of TIC-320687387 with CORALIE, NGTS, and Lesedi we determined that the companion, TIC-320687387 B,is a very low-mass star with a mass of $96.2 \pm _{2.0}^{1.9} M_J$ and radius of $1.14 \pm _{0.02}^{0.02} R_J$ placing it close to the hydrogen burning limit ($\sim 80 M_J$). TIC-320687387 B has a wide and eccentric orbit, with a period of 29.77381 days and an eccentricity of $0.366 \pm 0.003$. Eclipsing systems such as TIC-320687387 AB allow us to test stellar evolution models for low-mass stars, which in turn are needed to calculate accurate masses and radii for exoplanets orbiting single low-mass stars. The wide orbit of TIC-320687387 B makes it particularly valuable as its evolution can be assumed to be free from perturbations caused by tidal interactions with its G-type host star. ","TIC-320687387 B: a long-period eclipsing M-dwarf close to the hydrogen
  burning limit"
107,1479022090334388228,43706025,Edward,"['hey everyone, check out this hot new paper on optical emission in tidal disruption events you should totally read: <LINK>']",https://arxiv.org/abs/2201.01535,"A significant number of tidal disruption events (TDEs) radiate primarily at optical and ultraviolet (UV) wavelengths, with only weak soft X-ray components. One model for this optical excess proposes that thermal X-ray emission from a compact accretion disc is reprocessed to longer wavelengths by an optically thick envelope. Here, we explore this reprocessing scenario in the context of an optically thick accretion disc wind. Using state-of-the-art Monte Carlo radiative transfer and ionization software, we produce synthetic UV and optical spectra for wind and disc-hosting TDEs. Our models are inspired by observations, spanning a realistic range of accretion rates and wind kinematics. We find that such outflows can efficiently reprocess the disc emission and produce the broad Balmer and helium recombination features commonly seen in TDEs and exhibit asymmetric red wings. Moreover, the characteristic colour temperature of the reprocessed spectral energy distribution (SED) is much lower than that of the accretion disc. We show explicitly how changes in black hole mass, accretion rate and wind properties affect the observed broadband SED and line spectrum. In general, slower, denser winds tend to reprocess more radiation and produce stronger Balmer emission. Most of the outflows we consider are too highly ionized to produce UV absorption features, but this is sensitive to the input SED. For example, truncating the inner disc at just 4 $R_{ISCO}$ lowers the wind ionization state sufficiently to produce UV absorption features for sight lines looking into the wind ","Optical line spectra of tidal disruption events from reprocessing in
  optically thick outflows"
108,1479014885363228682,1127689057,Damian Ruck,"['“Engagement Outweighs Exposure to Partisan and Unreliable News within Google Search”. New paper on arxiv with @RERobertson, @_Jon_Green, @Ognyanova, @bowlinearl, @davidlazer. <LINK>']",https://arxiv.org/abs/2201.00074,"Popular online platforms such as Google Search have the capacity to expose billions of users to partisan and unreliable news. Yet, the content they show real users is understudied due to the technical challenges of independently obtaining such data, and the lack of data sharing agreements that include it. Here we advance on existing digital trace methods using a two-wave study in which we captured not only the URLs participants clicked on while browsing the web (engagement), but also the URLs they saw while using Google Search (exposure). Using surveys paired with engagement and exposure data collected around the 2018 and 2020 US elections, we found that strong Republicans engaged with more partisan and unreliable news than strong Democrats did, despite the two groups being exposed to similar amounts of partisan and unreliable news in their Google search results. Our results suggest the search engine is not pushing strong partisans into ""filter bubbles,"" but strong Republicans are asymmetrically selecting into ""echo chambers."" These findings hold across both study waves, align with work on social media and web browsing, and provide a rare look at the relationship between exposure and engagement. Our research highlights the importance of users' choices, and our approach moves the field closer to the independent, longitudinal, and cross-platform studies it needs to evaluate the impact of online search and social media platforms. ","Engagement Outweighs Exposure to Partisan and Unreliable News within
  Google Search"
109,1478928304115798017,748767636012478467,Dr. Rachel L. S. Frisbie,"[""New paper day 🎉🎉 \n\nWhile I'm not formally doing astronomy anymore, I'm excited to share this final chapter of my dissertation! We present an analysis of X-ray observations from Lakhchaura 2018 and the black-hole feedback valve model from Voit 2020.\n\n<LINK>"", 'Big thanks to @astrophysics and @CosmicPerspectv along with my other co-authors for helping me get this out in the midst of new parenthood and a whole host of other challenges!', ""Here's my favorite plot! The highlight is that within the parameters the Voit 2020 model applies to, we get agreement with observations. There are some notable exceptions you can find described in the paper! https://t.co/eEPHOvVmdE""]",https://arxiv.org/abs/2201.01306,"The Voit et al. (2020) black hole feedback valve model predicts relationships between stellar velocity dispersion and atmospheric structure among massive early-type galaxies. In this work, we test that model using the Chandra archival sample of 49 early-type galaxies from Lakhchaura et al. (2018). We consider relationships between stellar velocity dispersion and entropy profile slope, multiphase gas extent, and the ratio of cooling time to freefall time. We also define subsamples based on data quality and entropy profile properties that clarify those relationships and enable more specific tests of the model predictions. We find that the atmospheric properties of early-type galaxies generally align with the predictions of the Voit et al. (2020) model, in that galaxies with greater stellar velocity dispersion tend to have radial profiles of pressure, gas density, and entropy with steeper slopes and less extended multiphase gas. Quantitative agreement with the model predictions improves when the sample is restricted to have low central entropy and stellar velocity dispersion of between 220 and 300 km/s. ","Relationships Between Stellar Velocity Dispersion and the Atmospheres of
  Early-Type Galaxies"
110,1478690483245301760,472253730,Mirek Kratochvil,"[""We've got a new interactive dimensionality reduction tool BlosSOM🌼 and it's quite fun to use, so let's do a🧵 with videos.\nFor impatient: it helps with #cytometry data, the paper is here at <LINK> and software at <LINK> <LINK>"", ""Basically, there's a GPU version of EmbedSOM that is so fast that you can run dimensionality reduction of hundreds of thousands of points in each frame in a 30fps app. The perf plot below is for 1 million points. https://t.co/HgwFTXq0kt"", 'In turn, you can easily modify the landmarks&amp;parameters of EmbedSOM interactively and see the result right away, which is quite powerful. For example, you can help your DR algorithm by untangling some pathways, and find good smoothness settings: https://t.co/8zLMZ6iBwr', ""We did this because of exploring cytometry datases -- let's have a look at Levine dataset (yellow points are CD3+). Here I'm creating a tiny SOM grind and letting it map the data (basically what FlowSOM would do). https://t.co/bQxIJnkAve"", 'Apart from dragging the clusters to your preferred destinations (possibly solving some global layout ambiguities), there are goodies to help you with the global&amp;local layouting. I added a few more landmarks and ran the ""tSNE model trick"" to organize them automatically: https://t.co/1lRMqK26za', 'Similar trick with graph-based layouting (I select k=3 nearest neighbors and run force-based layout of landmarks): https://t.co/8LNIDeO0Ex', '(If you now have a desperate urge to drag the CD4 and CD8 cells to the same side of the embedding (I forgot to do that, sorry 🤣), you can do it yourself, BlosSOM is free&amp;opensource at https://t.co/3lc2y8Eq6c )', 'Finally, thanks to everyone involved! Especially the programming team from @matfyz @MFF_UK , and scientists and biologists from @uhkt_cz and @IOCBPrague (and @ELIXIRCZ!) for making this possible.\n\n(🧵 over, finishing with the complimentary flat mammoth screenshot 🗜️🐘) https://t.co/P1A4mBs0zw', 'Ofcourse I mean SOM ""grid"", not ""grind"". 😅']",https://arxiv.org/abs/2201.00701,"Dimensionality reduction methods have found vast application as visualization tools in diverse areas of science. Although many different methods exist, their performance is often insufficient for providing quick insight into many contemporary datasets, and the unsupervised mode of use prevents the users from utilizing the methods for dataset exploration and fine-tuning the details for improved visualization quality. We present BlosSOM, a high-performance semi-supervised dimensionality reduction software for interactive user-steerable visualization of high-dimensional datasets with millions of individual data points. BlosSOM builds on a GPU-accelerated implementation of the EmbedSOM algorithm, complemented by several landmark-based algorithms for interfacing the unsupervised model learning algorithms with the user supervision. We show the application of BlosSOM on realistic datasets, where it helps to produce high-quality visualizations that incorporate user-specified layout and focus on certain features. We believe the semi-supervised dimensionality reduction will improve the data visualization possibilities for science areas such as single-cell cytometry, and provide a fast and efficient base methodology for new directions in dataset exploration and annotation. ","Scalable semi-supervised dimensionality reduction with GPU-accelerated
  EmbedSOM"
111,1478632300917280769,1295482230,John Antoniadis (Γιάννης),"['Very excited for my student’s @astrosaba first, first author paper! He proposes a new type of explosions related to stars with ONeMg cores. He shows that some of these stars end may their life in a Ia-like SN  instead of forming a NS (via an ECSN) 👇\n<LINK> <LINK>']",https://arxiv.org/abs/2201.00871,"(abridged) When stripped from their hydrogen-rich envelopes, stars with initial masses between $\sim$7 and 11 M$_\odot$ develop massive degenerate cores and collapse. Depending on the final structure and composition, the outcome can range from a thermonuclear explosion, to the formation of a neutron star in an electron-capture supernova (ECSN). It has been recently demonstrated that stars in this mass range may initiate explosive oxygen burning when their central densities are still below $\rho_{\rm c} \lesssim 10^{9.6}$ g cm$^{-3}$. This makes them interesting candidates for Type Ia Supernovae -- which we call (C)ONe SNe Ia -- and might have broader implications for the formation of neutron stars via ECSNe. Here, we model the evolution of 252 helium-stars with initial masses in the $0.8-3.5$ M$_\odot$ range, and metallicities between $Z=10^{-4}$ and $0.02$. We use these models to constrain the central densities, compositions and envelope masses at the time of explosive oxygen ignition. We further investigate the sensitivity of these properties to mass-loss rate assumptions using additional models with varying wind efficiencies. We find that helium-stars with masses between $\sim$1.8 and 2.7 M$_\odot$ evolve onto $1.35-1.37$ M$_\odot$ (C)ONe cores that initiate explosive burning at central densities between $\rm \log_{10}(\rho_c)\sim 9.3$ and 9.6. We constrain the amount of residual carbon retained after core carbon burning, and conclude that it plays a critical role in determining the final outcome: Cores with residual carbon mass fractions of $X_{\rm min}(\rm{{^{12}}C}) \gtrsim 0.004$ result in (C)ONe SNe Ia, while those with lower carbon mass fractions become ECSNe. We find that (C)ONe SNe Ia are more likely to occur at high metallicities, whereas at low metallicities ECSNe dominate. ","Thermonuclear and Electron-Capture Supernovae from Stripped-Envelope
  Stars"
112,1478538709767176196,104529881,Diogo Souto,"['New M dwarf paper on ArXiv today! \n<LINK>\n\nDetailed Chemical Abundances for a Benchmark Sample of M Dwarfs from the APOGEE Survey\nI had a lot of fun with this one. Look at our main findings on this thread.', 'We studied 21 M dwarfs observed by APOGEE (11 in wide binary systems), and we derived atmospheric parameters and chemical abundances of Fe, C, O, Na, Mg, Al, Si, K, Ca, Cr, Mn, and Ni. We did study Ti and V, but we still need to improve on those.', 'Figures 1 and 2 show portions of the APOGEE spectra of two studied stars with similar Teff and log g but different metallicities. Nice to see the spectral lines most sensitive to metallicity with this figure. https://t.co/cjqHhbfkzd', 'We also studied three M dwarf hosting low-mass exoplanets (GJ 411, GJ 15 A, and GJ 338 B); however, we did not go to a robust star-planet connection study in this paper. If you are interested in looking at those exoplanets, use our abundance data. =D', 'Our major result is that the M dwarf abundances (secondaries in wide binary systems) compare well with those from warmer primaries (delta = 0.04 +- 0.04 dex). This indicates that the abundance scale for\nthe M dwarfs is precise. https://t.co/Ui9OCEOcZk', 'We also compare our atmospheric parameters and abundances with those from the ASPCAP pipeline reported by APOGEE DR16. The uncalibrated Teff and the calibrated log g are overall ok, but abundances still show a significant offset (of about 0.20 dex).\nSo careful in using those. https://t.co/IFMr1A1EeS', 'We used our abundances to investigate the Galactic abundance trends using M dwarfs. One of the few studies like this so far.', 'I liked these figures very much =)\nOverall our M dwarf abundances are in line with those from warmer main-sequence stars in the solar neighborhood. \nBut Mg, Si, Al, and Ca for our metal-rich sample (&gt; 0.00 dex) are somewhat underabundant compared to the Galactic trends. https://t.co/HJozaVP0a4', ""This work was possible due to the huge efforts of the @APOGEEsurvey and @sdssurveys teams. I thank all of you evolved in this survey very much!\nThat's it! If you have any comments let me know, I hope you all can enjoy reading our work. Have a great 2022, and vaccinate yourself!""]",https://arxiv.org/abs/2201.00891,"Individual chemical abundances for fourteen elements (C, O, Na, Mg, Al, Si, K, Ca, Ti, V, Cr, Mn, Fe, and Ni) are derived for a sample of M-dwarfs using high-resolution near-infrared $H$-band spectra from the SDSS-IV/APOGEE survey. The quantitative analysis included synthetic spectra computed with 1-D LTE plane-parallel MARCS models using the APOGEE DR17 line list to determine chemical abundances. The sample consists of eleven M-dwarfs in binary systems with warmer FGK-dwarf primaries and ten measured interferometric angular diameters. To minimize atomic diffusion effects, [X/Fe] ratios are used to compare M-dwarfs in binary systems and literature results for their warmer primary stars, indicating good agreement ($<$0.08 dex) for all studied elements. The mean abundance differences in Primaries-this work M-dwarfs is -0.05$\pm$0.03 dex. It indicates that M-dwarfs in binary systems are a reliable way to calibrate empirical relationships. A comparison with abundance, effective temperature, and surface gravity results from the ASPCAP pipeline (DR16) finds a systematic offset of [M/H], $T_{\rm eff}$, log$g$ = +0.21 dex, -50 K, and 0.30 dex, respectively, although ASPCAP [X/Fe] ratios are generally consistent with this study. The metallicities of the M dwarfs cover the range of [Fe/H] = -0.9 to +0.4 and are used to investigate Galactic chemical evolution via trends of [X/Fe] as a function of [Fe/H]. The behavior of the various elemental abundances [X/Fe] versus [Fe/H] agrees well with the corresponding trends derived from warmer FGK-dwarfs, demonstrating that the APOGEE spectra can be used to Galactic chemical evolution using large samples of selected M-dwarfs. ","Detailed Chemical Abundances for a Benchmark Sample of M Dwarfs from the
  APOGEE Survey"
113,1478392626810433547,159526922,Christophe,['New paper based on the work that @ingoluetkebohle and I have been doing on tracing ROS 2!\nros2_tracing: Multipurpose Low-Overhead Framework for Real-Time Tracing of ROS 2\npaper: <LINK>\ncode: <LINK>\n#goROS #ROS2 @OpenRoboticsOrg @rosorg'],https://arxiv.org/abs/2201.00393,"Testing and debugging have become major obstacles for robot software development, because of high system complexity and dynamic environments. Standard, middleware-based data recording does not provide sufficient information on internal computation and performance bottlenecks. Other existing methods also target very specific problems and thus cannot be used for multipurpose analysis. Moreover, they are not suitable for real-time applications. In this paper, we present ros2_tracing, a collection of flexible tracing tools and multipurpose instrumentation for ROS 2. It allows collecting runtime execution information on real-time distributed systems, using the low-overhead LTTng tracer. Tools also integrate tracing into the invaluable ROS 2 orchestration system and other usability tools. A message latency experiment shows that the end-to-end message latency overhead, when enabling all ROS 2 instrumentation, is on average 0.0033 ms, which we believe is suitable for production real-time systems. ROS 2 execution information obtained using ros2_tracing can be combined with trace data from the operating system, enabling a wider range of precise analyses, that help understand an application execution, to find the cause of performance bottlenecks and other issues. The source code is available at: this https URL ","ros2_tracing: Multipurpose Low-Overhead Framework for Real-Time Tracing
  of ROS 2"
114,1478200339455422464,913238472357437445,Fuminobu TAKAHASHI,"['Our new paper is out today. It is about the axion DM in the presence of a tiny extra PQ breaking. We show that the axion abundance can be either reduced or enhanced depending on which vacuum the axion first starts to oscillate around. \n<LINK>', 'The axion abundance can be reduced when the axion starts to oscillate about a minimum, which gradually approaches the origin where the strong CP phase vanishes. This is similar to https://t.co/0NCuXLVec0 \nhttps://t.co/msDvkJOjS8\nwhich use the Witten effect of hidden monopoles.', 'On the other hand, it is enhanced when confined to the wrong vacuum until non-perturbative QCD effects become relevant. Surprisingly, the final abundance does not depend on the decay constant of the axion! The axion can account for all DM even for arbitrarily small fa!', 'This interesting observation was first made by Higaki, Kitajima, and two of us (Kwang Sik Jeong and myself) in https://t.co/5uEFeDAnao in a context of the clockwork QCD axion, but we have studied this trapping effect in detail and defined the viable parameter space.', 'Such scenario of trapping the axion in another vacuum in the early universe was recently considered in https://t.co/42zU5YgSBs, in a context of Z_N axion.  Since the PQ sym is expected to be broken other than QCD, I believe our paper has an interesting impact on axion DM search.']",https://arxiv.org/abs/2201.00681,"We study cosmological effects of explicit Peccei-Quinn breaking on the QCD axion dark matter. We find that the axion abundance decreases or increases significantly depending on the initial position, even for a tiny Peccei-Quinn breaking that satisfies the experimental bound of the neutron electric dipole measurements. If the axion first starts to oscillate around a wrong vacuum and if it gets trapped there until the false vacuum disappears due to non-perturbative QCD effects, its abundance increases significantly and is independent of the decay constant $f_a$, as first pointed out in [JHEP 06 (2016) 150]. Thus, the axion produced by the trapping mechanism can explain dark matter even when the decay constant is close to the lower limit due to stellar cooling arguments. On the other hand, if the axion starts to oscillate about a potential minimum close to the low-energy vacuum, its abundance is significantly reduced because of the adiabatic suppression mechanism. This relaxes the upper limit of the axion window to large values of $f_a$. We also discuss how the axionic isocurvature perturbation is affected by the Peccei-Quinn breaking term, and show that it can be suppressed in both regimes. In particular, the isocurvature bound on the inflation scale is relaxed by many orders of magnitudes for $f_a \lesssim 10^{11}{\rm GeV}$ compared to the conventional scenario. ","Cosmological effects of Peccei-Quinn symmetry breaking on QCD axion dark
  matter"
115,1493338180304748547,590175311,Benedict Guttman-Kenney,"['♥️A valentine’s gift ♥️: New short working paper `Buy Now, Pay Later (BNPL)...On Your Credit Card’ with Chris Firth &amp; @johngathergood @ChicagoBooth  @WarwickBSchool @UoNEconomics\n\nPaper: <LINK>\n\nThread ⬇️ 1/9  #saywhatyoufound #householdfinance #BNPL #econtwitter <LINK>', 'BNPL is a FinTech credit option to defer payments into 1+ (often &lt;5) 0% interest instalments.\n\nNext time you shop online you’ll likely see BNPL firms (e.g. Afterpay/Clearpay, Affirm, Klarna) as a payment options 👀\n\n🇬🇧 BNPL is larger than payday lending market at its peak! 2/9 https://t.co/NQEpDmaECG', ""BNPL is largely unregulated. But UK &amp; US financial regulators (CFPB, FCA) are considering BNPL regulation. \n\nWhere's the BNPL research to inform them? Searches on ArXiv found 0 papers, on SSRN 1 (law) paper.🤯\n\nOur paper provides a quantitative starting point for this topic 3/9 https://t.co/jK7XhKYPS7"", 'We use 🇬🇧 credit card transactions data to study BNPL.\n\nFinding 1: BNPL instalments are commonly charged to credit cards: 19.5% of active credit cards in our 2021 data have BNPL instalment.💳 \n\nWe estimate (with many caveats!) 40-50% of all UK BNPL is charged to credit cards. 4/9 https://t.co/RoD2mNlm8d', 'Why does this matter? \n\nTransforming a 0% interest, amortizing BNPL debt to credit card debt where typical interest rates are 20% and amortization schedules decades-long raises doubts on consumers’ ability to repay BNPL debt (e.g. see here rules applying to regulated credit). 5/9 https://t.co/EJDcfa1D9r', 'BNPL is a rapidly growing market around the world.🌎🌍🌏\n\nFinding 2: Charging BNPL to credit cards is growing rapidly too. 6/9 https://t.co/pfJud0oGq3', 'Finding 3: Charging BNPL to credit cards is most common among younger consumers. It is common across all parts of the 🇬🇧. 7/9 https://t.co/3x6PNTa7cs', 'Finding 4: Charging BNPL to credit cards is most common in the most deprived areas (local government districts) in England. \n\nMoving from least to most deprived area has ~30% higher BNPL usage.\n\nThe most vulnerable people using BNPL most is a warning flag for regulators. 8/9 https://t.co/uFT3sG6zlr', ""There’s an important need for more academic research to understand BNPL and inform UK and US BNPL regulations. \n\nOur paper is just a start.\n\nWe hope you 'buy' the paper now, and it prompts new BNPL research 'later'! 9/9 ♥️\n\nhttps://t.co/H0VMEudWG1 https://t.co/CNHpcDSJJh"", 'unroll @threadreaderapp']",https://arxiv.org/abs/2201.01758,"We provide the first economic research on `buy now, pay later' (BNPL): an unregulated FinTech credit product enabling consumers to defer payments interest-free into instalments. In 2021 transactions by BNPL firms are charged to $19.5\%$ of active credit cards in our UK data. Charging a $0\%$ interest, amortizing BNPL debt to credit cards, where typical interest rates are $20\%$ and amortization schedules decades-long, raises doubts on consumers' ability to pay for BNPL. Such charging of BNPL to credit cards is most prevalent among younger consumers and in the most deprived geographies. ","Buy Now, Pay Later (BNPL)...On Your Credit Card"
116,1488264091701592073,2585907650,Leslie Smith,"['I saw an interesting new paper called ""Prospective Learning: Back to the Future"" at <LINK>\nThe authors state that ability to learn for the future is key to intelligence.  \nFood for thought.']",https://arxiv.org/abs/2201.07372,"Research on both natural intelligence (NI) and artificial intelligence (AI) generally assumes that the future resembles the past: intelligent agents or systems (what we call 'intelligence') observe and act on the world, then use this experience to act on future experiences of the same kind. We call this 'retrospective learning'. For example, an intelligence may see a set of pictures of objects, along with their names, and learn to name them. A retrospective learning intelligence would merely be able to name more pictures of the same objects. We argue that this is not what true intelligence is about. In many real world problems, both NIs and AIs will have to learn for an uncertain future. Both must update their internal models to be useful for future tasks, such as naming fundamentally new objects and using these objects effectively in a new context or to achieve previously unencountered goals. This ability to learn for the future we call 'prospective learning'. We articulate four relevant factors that jointly define prospective learning. Continual learning enables intelligences to remember those aspects of the past which it believes will be most useful in the future. Prospective constraints (including biases and priors) facilitate the intelligence finding general solutions that will be applicable to future problems. Curiosity motivates taking actions that inform future decision making, including in previously unmet situations. Causal estimation enables learning the structure of relations that guide choosing actions for specific outcomes, even when the specific action-outcome contingencies have never been observed before. We argue that a paradigm shift from retrospective to prospective learning will enable the communities that study intelligence to unite and overcome existing bottlenecks to more effectively explain, augment, and engineer intelligences. ",Prospective Learning: Back to the Future
117,1487111460429910023,850415526602059777,Vikram Dwarkadas,['Will the CTA be able to detect Type IIP supernovae after they explode? The answer can be found in our new paper accepted to MNRAS: <LINK>.'],https://arxiv.org/abs/2201.09583,"Type II-P supernov\ae~(SNe), the most common core-collapse SNe type, result from the explosions of red supergiant stars. Their detection in the radio domain testifies of the presence of relativistic electrons, and shows that they are potentially efficient energetic particle accelerators. If hadrons can also be accelerated, these energetic particles are expected to interact with the surrounding medium to produce a gamma-ray signal even in the multi--TeV range. The intensity of this signal depends on various factors, but an essential one is the density of the circumstellar medium. Such a signal should however be limited by electron-positron pair production arising from the interaction of the gamma-ray photons with optical photons emitted by the supernova photosphere, which can potentially degrade the gamma-ray signal by over ten orders of magnitude in the first days/weeks following the explosion. We calculate the gamma-gamma opacity from a detailed modelling of the time evolution of the forward shock and supernova photosphere, taking a full account of the non-isotropy of the photon interactions. We discuss the time-dependent gamma-ray TeV emission from type II-P SNe as a function of the stellar progenitor radius and mass-loss rate, as well as the explosion energy and mass of the ejected material. We evaluate the detectability of the SNe with the next generation of Cherenkov telescopes. We find that, while most extragalactic events may be undetectable, type II-P SNe exploding in our Galaxy or in the Magellanic Clouds should be detected by gamma-ray observatories such as the upcoming Cherenkov Telescope Array. ","The first days of type II-P core collapse supernovae in the gamma-ray
  range"
118,1486338138179923972,5850692,Aaron Roth,"['Excited about this new paper with @Ira_Sass and @mkearnsupenn!  We show how to easily reduce model error on a group on which error is discovered to be sub-optimal, without increasing it for anyone else. It can be used as part of a bias bounty design. <LINK> <LINK>', '@shortstein @Ira_Sass @mkearnsupenn Well, as always it depends on what you mean by ""fairness"". But we do show that tradeoffs are not necessary if your concerns have the form ""You have failed to learn a good predictive model for this group of people""', '@mmitchell_ai @Ira_Sass @mkearnsupenn Thanks, that means a lot -- I hope you like it!', '@oliverdjohnson @Ira_Sass @mkearnsupenn Yes! Click on the arxiv link at the very end of the tweet.']",https://arxiv.org/abs/2201.10408,"We propose and analyze an algorithmic framework for ""bias bounties"": events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event. ",An Algorithmic Framework for Bias Bounties
119,1484992395670343681,24750136,Jackie Faherty,"[""Brown dwarfs!!!! They are a powerhouse in advancing our understanding of both stars and planets.  We've got an amazing new paper and result which demonstrates how... (a thread 🧵) <LINK> <LINK>"", 'Remember the Spitzer Space Telescope?  The true precursor to #JWST ?  Well it was a revolutionary telescope and in the final GO call for proposals (RIP Spitzer) I led one that was awarded 591.4 hours to obtain light curves of 23 YOUNG brown dwarfs. THATS A LOT OF TIME! https://t.co/zFn9G7s9WK', ""Some of those targets were NEW discoveries made by @jgagneastro that looked crazy enticing as isolated planetary mass objects (I'll refer back to the MOST exciting one -- a T5 called 2MASS J07180871−6415310-- in a second)"", 'Every one of our targets was selected because it had a mass close to (just above or just below) the deuterium burning boundary.  In other words these were PLANETARY MASS OBJECTS IN ISOLATION!!!! And with Spitzer, we could STUDY THEIR WEATHER!!!', 'Each source was stared at in ch1 for 23 hours.  @johannamvos joined my team right when the data was being acquired. She became the boss of the project and I can not sing her praises enough to anyone reading this and looking to put her on their shortlist. https://t.co/ILGH0mR5xB', 'What we found was VARIABILITY!  AKA WEATHER!!!!  Why Weather?  Because warm BDs are covered in silicate clouds.  Think of them as Jupiter but a few onion layers down on that planet.  And you know the weather is crazy on Jupiter right?  Great Red Spot anyone? https://t.co/F0LSGzeIVG', 'Are you visually curious? Let me take this moment to promote a mural located at 650 Fulton St in Brooklyn that was created by the talented @Alonglastname and is an homage to brown dwarfs, their weather, the women that study them and the window on the Universe they open for us. https://t.co/OwkGGZTANG', ""As for the paper: here's some highlights: 13 of the 23 objects are bonafide young and showed brightness variability with a range of amplitudes and light curve shapes. https://t.co/It5MIbVaPZ"", ""My personal favorite is the very red T5 dwarf J0718. Kinematics indicate it could be in the ~24Myr beta pic moving group. The period is ~1 hr (FAST!) and its got a huge 2.14% amplitude. If beta pic its ~3 MJup. If field (more likely) it's bigger but an outstanding T dwarf rotator https://t.co/q9IvW01DES"", 'One thing we wanted to test was whether young brown dwarfs were more variable than older ones.  In other words do they throw more (weather related) tantrums when they are babies like humans do. https://t.co/aTax03vLwG', 'The answer is that  young brown dwarfs are highly likely to display variability across the L2-T4 spectral type range while the older brown dwarf variability occurrence rate drops for spectral types &gt;L9. So yes! Tantrum away baby brown dwarfs. https://t.co/evxKIm8nfa', 'My personal favorite plot is Fig 12: rotation period vs age color coded by mass. This plot is a compilation of  brown dwarfs, giant exoplanet companions, and super low mass stars with rotation periods and its basically our best holistic angular momentum map of these populations. https://t.co/7rL41nJvEH', 'We created this @OpenSpaceProj visualization of known BDs (big red), the full Spitzer sample (smaller red sample), and our contribution (smallest red sample) so you could see where these sources map to in the local solar neighborhood (100 light year grid pops up for scale) https://t.co/tPPhDHr9Ff', 'One more shoutout to @johannamvos who is the lead author of this work and who is truly an extraordinary scientist.  Other amazing co-authors include @jgagneastro @astromarkmarley @smetchev2232 @johngizis @emilylurice @kellecruz  Read the paper for more! https://t.co/YJY1wXm6TO']",https://arxiv.org/abs/2201.04711,"We present a survey for photometric variability in young, low-mass brown dwarfs with the Spitzer Space Telescope. The 23 objects in our sample show robust signatures of youth and share properties with directly-imaged exoplanets. We present three new young objects: 2MASS J03492367$+$0635078, 2MASS J09512690 $-$8023553 and 2MASS J07180871$-$6415310. We detect variability in 13 young objects, and find that young brown dwarfs are highly likely to display variability across the L2--T4 spectral type range. In contrast, the field dwarf variability occurrence rate drops for spectral types $>$L9. We examine the variability amplitudes of young objects and find an enhancement in maximum amplitudes compared to field dwarfs. We speculate that the observed range of amplitudes within a spectral type may be influenced by secondary effects such as viewing inclination and/or rotation period. We combine our new rotation periods with the literature to investigate the effects of mass on angular momentum evolution. While high mass brown dwarfs ($>30 M_{\mathrm{Jup}}$) spin up over time, the same trend is not apparent for lower mass objects ($<30 M_{\mathrm{Jup}}$), likely due to the small number of measured periods for old, low-mass objects. The rotation periods of companion brown dwarfs and planetary-mass objects are consistent with those of isolated objects with similar ages and masses, suggesting similar angular momentum histories. Within the AB Doradus group, we find a high variability occurrence rate and evidence for common angular momentum evolution. The results are encouraging for future variability searches in directly-imaged exoplanets with facilities such as the James Webb Space Telescope and 30-meter telescopes. ","Let the Great World Spin: Revealing the Stormy, Turbulent Nature of
  Young Giant Exoplanet Analogs with the Spitzer Space Telescope"
120,1484370336804077569,4666231375,Konstantin Batygin,"[""How fast are winds on Hot Jup's? And how deep is the circulation layer? In a new paper led by Henrik Knierim, we argue that the depth of the weather layer can be constrained from planetary radii. Because Ohmic dissipation! Details here: <LINK>""]",https://arxiv.org/abs/2201.08209,"The inflated radii of giant short-period extrasolar planets collectively indicate that the interiors of hot Jupiters are heated by some anomalous energy dissipation mechanism. Although a variety of physical processes have been proposed to explain this heating, recent statistical evidence points to the confirmation of explicit predictions of the Ohmic dissipation theory, elevating this mechanism as the most promising candidate for resolving the radius inflation problem. In this work, we present an analytic model for the dissipation rate and derive a simple scaling law that links the magnitude of energy dissipation to the thickness of the atmospheric weather layer. From this relation, we find that the penetration depth influences the Ohmic dissipation rate by an order of magnitude. We further investigate the weather layer depth of hot Jupiters from the extent of their inflation and show that, depending on the magnetic field strength, hot Jupiter radii can be maintained even if the circulation layer is relatively shallow. Additionally, we explore the evolution of zonal wind velocities with equilibrium temperature by matching our analytic model to statistically expected dissipation rates. From this analysis, we deduce that the wind speed scales approximately as $1/\sqrt{T_\mathrm{eq}-T_0}$, where $T_0$ is a constant that equals $T_0 \sim 1000~\mathrm{K}-1800~\mathrm{K}$ depending on planet-specific parameters (radius, mass, etc.). This work outlines inter-related constraints on the atmospheric flow and the magnetic field of hot Jupiters and provides a foundation for future work on the Ohmic heating mechanism. ","Shallowness of circulation in hot Jupiters -- Advancing the Ohmic
  dissipation model"
121,1484126049864716291,62115778,Sven Apel,"['Pretty happy with our brand new @ICSEconf paper introducing the notion of feature causality in configurable systems (building on the seminal work by Halpern and Pearl). \n<LINK> <LINK>', '@BNuseibeh @ICSEconf Thanks a lot @BNuseibeh! That makes me happy again 😀']",https://arxiv.org/abs/2201.07280,"Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions. ",Causality in Configurable Software Systems
122,1483873639011344385,826808603566813185,Dr Uttara Tipnis,"['Check out our new paper on arXiv, Continuous-Time Probabilistic Models for Longitudinal Electronic Health Records (<LINK>) @Livermore_Lab @DeptVetAffairs @VeteransHealth']",https://arxiv.org/abs/2201.03675,"Analysis of longitudinal Electronic Health Record (EHR) data is an important goal for precision medicine. Difficulty in applying Machine Learning (ML) methods, either predictive or unsupervised, stems in part from the heterogeneity and irregular sampling of EHR data. We present an unsupervised probabilistic model that captures nonlinear relationships between variables over continuous-time. This method works with arbitrary sampling patterns and captures the joint probability distribution between variable measurements and the time intervals between them. Inference algorithms are derived that can be used to evaluate the likelihood of future using under a trained model. As an example, we consider data from the United States Veterans Health Administration (VHA) in the areas of diabetes and depression. Likelihood ratio maps are produced showing the likelihood of risk for moderate-severe vs minimal depression as measured by the Patient Health Questionnaire-9 (PHQ-9). ","Continuous-Time Probabilistic Models for Longitudinal Electronic Health
  Records"
123,1483697389701636096,595628453,Aggelos Kiayias,['want to learn more about blockchain governance? see our new systematization paper freshly uploaded here <LINK>'],https://arxiv.org/abs/2201.07188,"Blockchain systems come with a promise of decentralization that often stumbles on a roadblock when key decisions about modifying the software codebase need to be made. This is attested by the fact that both of the two major cryptocurrencies, Bitcoin and Ethereum, have undergone hard forks that resulted in the creation of alternative systems, creating confusion and opportunities for fraudulent activities. These events, and numerous others, underscore the importance of Blockchain governance, namely the set of processes that blockchain platforms utilize in order to perform decision-making and converge to a widely accepted direction for the system to evolve. While a rich topic of study in other areas, governance of blockchain platforms is lacking a well established set of methods and practices that are adopted industry wide. This makes the topic of blockchain governance a fertile domain for a thorough systematization that we undertake in this work. We start by distilling a comprehensive array of properties for sound governance systems drawn from academic sources as well as grey literature of election systems and blockchain white papers. These are divided into seven categories, confidentiality, verifiability, accountability, sustainability, Pareto efficiency, suffrage and liveness that capture the whole spectrum of desiderata of governance systems. We proceed to classify ten well-documented blockchain systems. While all properties are satisfied, even partially, by at least one system, no system that satisfies most of them. Our work lays out a foundation for assessing blockchain governance processes. While it highlights shortcomings and deficiencies in currently deployed systems, it can also be a catalyst for improving these processes to the highest possible standard with appropriate trade-offs, something direly needed for blockchain platforms to operate effectively in the long term. ",SoK: Blockchain Governance
124,1482178710354776064,354995326,Nauman Dawalatabad,['New paper: Our recent work on model compression published at IEEE @asru2021 is now available on @arxiv (<LINK>)\nWe show simple knowledge distillation can achieve high compression for on-device ASR model. \nThnks to co-authors @samsungresearch'],https://arxiv.org/abs/2201.02741,"Speech recognition on smart devices is challenging owing to the small memory footprint. Hence small size ASR models are desirable. With the use of popular transducer-based models, it has become possible to practically deploy streaming speech recognition models on small devices [1]. Recently, the two-pass model [2] combining RNN-T and LAS modules has shown exceptional performance for streaming on-device speech recognition. In this work, we propose a simple and effective approach to reduce the size of the two-pass model for memory-constrained devices. We employ a popular knowledge distillation approach in three stages using the Teacher-Student training technique. In the first stage, we use a trained RNN-T model as a teacher model and perform knowledge distillation to train the student RNN-T model. The second stage uses the shared encoder and trains a LAS rescorer for student model using the trained RNN-T+LAS teacher model. Finally, we perform deep-finetuning for the student model with a shared RNN-T encoder, RNN-T decoder, and LAS rescorer. Our experimental results on standard LibriSpeech dataset show that our system can achieve a high compression rate of 55% without significant degradation in the WER compared to the two-pass teacher model. ",Two-Pass End-to-End ASR Model Compression
125,1479431716049670148,593911501,James Cadman,"['New paper out today! We examine the possibility of triggered fragmentation from binary companions in self-gravitating discs. <LINK> <LINK>', 'Observations (e.g Fontanive et al. 2019) find an excess of binary companions to close in (&lt; 1AU), massive planets/BDs (7-60 Mjup), suggesting that stellar multiplicity may play a role in their formation. There are also indications that these objects may have formed through GI.', 'Using SPH to explore a large range of binary parameter space (in separation, eccentricity and inclination) we find a sweet spot where intermediate separation binaries (a~100-400AU) can trigger fragmentation in a marginally unstable disc.', 'The sweet spot found in binary parameter space is consistent with the projected separations observed to show an excess of close in, massive planets/BDs. Could triggered fragmentation contribute to this excess?', 'This work was done with lots of kind help from @astroclem @cassidentprone and @theresphysics', '@theresphysics @TomHaworthAstro @astroclem @cassidentprone That would be interesting. We could definitely set up runs with circumprimary and circumsecondary discs. I’m not sure about the current status of phantom’s dust growth algorithms, or using multiple dust types - would have to look into that.']",https://arxiv.org/abs/2201.02032,"Observations of systems hosting close in ($<1$ AU) giant planets and brown dwarfs ($M\gtrsim7$ M$_{\rm Jup}$) find an excess of binary star companions, indicating that stellar multiplicity may play an important role in their formation. There is now increasing evidence that some of these objects may have formed via fragmentation in gravitationally unstable discs. We present a suite of 3D smoothed particle hydrodynamics (SPH) simulations of binary star systems with circumprimary self-gravitating discs, which include a realistic approximation to radiation transport, and extensively explore the companion's orbital parameter space for configurations which may trigger fragmentation. We identify a ""sweet spot"" where intermediate separation binary companions ($100$ AU $\lesssim a\lesssim400$ AU) can cause a marginally stable disc to fragment. The exact range of ideal binary separations is a function of the companion's eccentricity, inclination and mass. Heating is balanced by efficient cooling, and fragmentation occurs inside a spiral mode driven by the companion. Short separation, disc penetrating binary encounters ($a\lesssim100$ AU) are prohibitive to fragmentation, as mass stripping and disc heating quench any instability. This is also true of binary companions with high orbital eccentricities ($e\gtrsim0.75$). Wide separation companions ($a\gtrsim500$ AU) have little effect on the disc properties for the setup parameters considered here. The sweet spot found is consistent with the range of binary separations which display an excess of close in giant planets and brown dwarfs. Hence we suggest that fragmentation triggered by a binary companion may contribute to the formation of these substellar objects. ",Binary companions triggering fragmentation in self-gravitating discs
126,1489222128587218944,1149606436084699136,Ulrich Pennig,"['One of the papers that I worked on with D. Evans during lockdown is now on the arXiv: <LINK> We study the symmetry groups of certain infinite tensor product algebras equipped with a circle action and reveal the rich topological structure of this group.', 'I find the result very satisfying and it is close to a complete answer in the case of circle actions (with lots of room for generalisations). I am tempted to write a little explanatory blurb about it on my homepage.']",https://arxiv.org/abs/2201.13364,"We develop an equivariant Dixmier-Douady theory for locally trivial bundles of $C^*$-algebras with fibre $D \otimes \mathbb{K}$ equipped with a fibrewise $\mathbb{T}$-action, where $\mathbb{T}$ denotes the circle group and $D = \operatorname{End}\left(V\right)^{\otimes \infty}$ for a $\mathbb{T}$-representation $V$. In particular, we show that the group of $\mathbb{T}$-equivariant $*$-automorphisms $\operatorname{Aut}_{\mathbb{T}}(D \otimes \mathbb{K})$ is an infinite loop space giving rise to a cohomology theory $E^*_{D,\mathbb{T}}(X)$. Isomorphism classes of equivariant bundles then form a group with respect to the fibrewise tensor product that is isomorphic to $E^1_{D,\mathbb{T}}(X) \cong [X, B\operatorname{Aut}_{\mathbb{T}}(D \otimes \mathbb{K})]$. We compute this group for tori and compare the case $D = \mathbb{C}$ to the equivariant Brauer group for trivial actions on the base space. ","Equivariant higher Dixmier-Douady Theory for circle actions on
  UHF-algebras"
127,1489177702997413890,1063607700,giacomo veneri,['DANNTe: a case study of a #turbomachinery  sensor virtualization under domain shift\n\nA real application\n\nWe propose an adversarial learning method to tackle a Domain Adaptation (DA) time series regression task (DANNTe). The regress…<LINK> <LINK>'],https://arxiv.org/abs/2201.03850,"We propose an adversarial learning method to tackle a Domain Adaptation (DA) time series regression task (DANNTe). The regression aims at building a virtual copy of a sensor installed on a gas turbine, to be used in place of the physical sensor which can be missing in certain situations. Our DA approach is to search for a domain-invariant representation of the features. The learner has access to both a labelled source dataset and an unlabeled target dataset (unsupervised DA) and is trained on both, exploiting the minmax game between a task regressor and a domain classifier Neural Networks. Both models share the same feature representation, learnt by a feature extractor. This work is based on the results published by Ganin et al. arXiv:1505.07818; indeed, we present an extension suitable to time series applications. We report a significant improvement in regression performance, compared to the baseline model trained on the source domain only. ","DANNTe: a case study of a turbo-machinery sensor virtualization under
  domain shift"
128,1488485323747344390,437340420,Kees Storm,"[""Liquid crystal networks can change volume in electric fields; cool #SoftMatter with a host of applications from soft robots to self-cleaning surfaces. We've developed a Landau theory to study the dynamics of thin-film actuation. Congrats Guido! Preprint: <LINK> <LINK>""]",https://arxiv.org/abs/2201.12214,"Recently, Van der Kooij and co-workers recognised three distinct, transient regimes in the dynamics of electrically-deforming liquid crystal networks [Van der Kooij et al., Nat. Commun. 10, 1 (2019)]. Based on a Landau-theoretical framework, which encompasses spatially resolved information, we interpret these regimes: initially, the response is dominated by thermal noise, then the top of the film expands, followed by a permeation of this response into the bulk. An important signature of this interpretation is a significant dependence of the regime time scales on film thickness, where we observe a clear thin-film-to-bulk transition. The point of transition coincides with the emergence of spatial inhomogeneities in the bulk, i.e., domain formation, and should be avoided due to the less predictable steady-state expansion it gives rise to. Finally, we show that this domain formation can be suppressed by decreasing the initial thickness of the film, and increasing the linear dimensions of the mesogens, or their orientational order when crosslinked into the network, though this comes at the cost of the deformation magnitude. Our results contribute to achieving finer control over how smart liquid crystal network coatings are activated. ","Transient response and domain formation in electrically deforming liquid
  crystal networks"
129,1488436542372786177,1228373893758562310,Aliaksandr Hubin 🤍❤️🤍 💙💛,['Proud to share the first version of preprint of the first paper by my first master student (based on thesis) <LINK>. We introduce and study the idea of combining SGD to compute the marginal likelihood with MCMC to explore the model space and show convergence.'],https://arxiv.org/abs/2201.13198,"It is common practice to use Laplace approximations to compute marginal likelihoods in Bayesian versions of generalised linear models (GLM). Marginal likelihoods combined with model priors are then used in different search algorithms to compute the posterior marginal probabilities of models and individual covariates. This allows performing Bayesian model selection and model averaging. For large sample sizes, even the Laplace approximation becomes computationally challenging because the optimisation routine involved needs to evaluate the likelihood on the full set of data in multiple iterations. As a consequence, the algorithm is not scalable for large datasets. To address this problem, we suggest using a version of a popular batch stochastic gradient descent (BSGD) algorithm for estimating the marginal likelihood of a GLM by subsampling from the data. We further combine the algorithm with Markov chain Monte Carlo (MCMC) based methods for Bayesian model selection and provide some theoretical results on the convergence of the estimates. Finally, we report results from experiments illustrating the performance of the proposed algorithm. ",A subsampling approach for Bayesian model selection
130,1488418924043456512,406721961,Siavash Ahrar,"['I studied couple of good examples, and a tweetorial on making effective 🧵s ... And as promised here is a summary of our preprint  <LINK> intro/motivation section is long (my bad) since we usually get questions about the ""why"" behind the work.', 'We show pneumatic digital computers (Finite State Machines) made from ufluidic valves &amp; channels which use vacuum/air (pressure difference) instead of electricity.  We use the computers to automate ufluidics.  We also show programmability using a ""punch-card"" 🥊 🃏approach (2/n) https://t.co/k2CmT0Z3ju', 'Today ""computer"" is synonymous with an electrical machine. This was not always the case and not all computers operate (or should operate) via electrical signals. For ex, during 50-60s, Fluidics was a viable alternative for computation  (Computer 💻 != Electronics ⚡️3/n) https://t.co/Rcld4PwJBc', 'Fluidics are circuits that use liquid 🍺or gases as their signals - check out their fascinating history [e.g., ref 29, 30]. Ultimately, Fluidics lost the battle(s) over speed, miniaturization,and integration to electronics (4/n) https://t.co/eQpQjU0ec4', ""So why should we even consider  computing with anything other than electricity in 2022? (isn't it 2020?) Why do we need a pneumatic or any other sort of computer besides ⚡️💻? An excellent general answer to the above can be found in Drew Endy's 2011 NYT essay. (5/n)"", 'Drew\'s team had demonstrated digital logic gates made out of DNA 🧬  He wrote: ""A better way to think about the future of computing might be to ask when and where we could improve our ability to compute upon information that we greatly care about"" (6/n)', 'In ufluidics (and soft robotics...hi Baymax!) one of the critical type of information that we greatly care about is in the form of pneumatic signals. Since above systems (valve-based ufluidics specifically) are governed by time-varying pressure signals (7/n) https://t.co/s76MY7BqcR', 'In the absence of pneumatic computers we have to generate control signals with⚡️💻 --&gt; solenoid valves + manifold systems --&gt; series of tubes --&gt; ufluidics.  These steps and external controllers make the previously small chip into a large, expensive, and cumbersome system. (8/n)', 'To address this gap and build our Finite State Machines (FSMs), we used pneumaticly actuated, normally closed valves first demonstrated by awesome @wgrover (thank you Will for all your support over the years :) 9/n', ""In our approach valves work like NMOS-only TTL. We have previously made: ring oscillators, semi-autonomous fluid handling systems, and digital counters. But a computer (FSM) hard-wired or programmable had remained absent. So let's look at our data (10/n) https://t.co/KvEIMUWxRt"", 'We started with hard-wired FSMs both with and without (not shown) user input. We can think of an FSM as a simple sequential circuit that encodes a finite # of states and ""proper"" translations between them. Imagine a 🚦; it can be controlled with an FSM 🟢-&gt;🟡-&gt;🔴', 'Here is our 2-bit FSM with user input. Circuit diagram (A), and state transition, and the glass chip (B) are shown. Timing diagram (C) is generated from a video recording (measured via reflection of the light from the indicator valves opening /closing) (12/n) https://t.co/Y8MhNRT4n0', 'Next we integrated a 2-bit FSM with a  ufluidic rotary mixer to create a self-contained ufluidic device. We also used a pneumatic push-button to advance tbe program states (meter,mix,...) in the system (please watch the movie S1 from the arXiv) https://t.co/WfAVMIBI9n', 'Next, we used a hot-encoded 4-bit FSM to automate our previously demonstrated serial dilution ladder (1:1 dilution + keep all 4 diluted stages) @RajeManasi figured out the difficult task of integrating this chip :) (Please watch the movie S2 to see this cool system in action) https://t.co/jtNgK8nqc4', ""At this point we wondered if we could make a programmable pneumatic FSM. We loved the idea behind electronic FPGAs, since they could be reconfigured by the user based on the user's design. So we used a similar (older strategy) called programmable logic array (PLA) (15/n) https://t.co/KTqdRsogFO"", 'Our basic idea was that at the start of an experiment, the user could configure/connect channels to route signals to gates to define their desired combinational function (map different excitations for their FSM). We were inspired by punch-card computers ... (16/n)', 'so we realized that the elastomdric membrane inside the chip can be bored to route a pneumatic signal from one layer of the chip to another.  Changing the patterns at the start of an experiment, then implements a new FSM inside the same physical chip. (17/n)', 'We show 3 different FSM implanted and verified with the same glass chip.  Only the configuration of bored/punched patterns inside the membrane is changed. E.g., red dots implement the red state transition diagram. (Please See videos 3 and 4 for the circuit operation) (18/n) https://t.co/x0eI4cf7gH', 'With these hard-wired and programmable FSM we have demonstrated pneumatic digital computers that use signals besides electricity.  We also show the applicayon of these computers for the embedded control of microfluidics (19/n)', 'As stated earlier the key goal of our computers is not to compete with electronics. After all the largest FSM presented here is 4-bits, fastest chip runs only at a handful of Hertz (5Hz), and max valve number is ~80 (An Apple M1 has 16 billion transistors). (20/n)', 'Rather the pneumatic computer\'s key strength is to directly compute on the ""right"" type of signals (and provide embedded ccomputation and control) without the need for cumbersome and expensive signal transduction. (21/n)', 'The work presented in the 🧵 preprint was accomplished under the mentorship of @elliothui and with collaboration with @RajeManasi and @IrceBeltran']",https://arxiv.org/abs/2201.09755,"Alternative computing approaches that interface readily with physical systems are well suited for embedded control of those systems. We demonstrate finite state machines implemented as pneumatic circuits of microfluidic valves, and we employ these controllers to direct microfluidic liquid handling procedures such as serial dilution on the same chip. These monolithic integrated systems require only power to be supplied externally, in the form of a vacuum source. User input can be provided directly to the chip by covering pneumatic ports with a finger. State machines with up to four bits of state memory are demonstrated, and next-state combinational logic can be fully reprogrammed by changing the hole-punch pattern on a membrane in the chip. These pneumatic computers demonstrate a new framework for the embedded control of physical systems. ",Pneumatic Computers for Embedded Control of Microfluidics
131,1487451884701073411,801793180899360768,Hendrik Schuff,"['Check out our new paper! (w/ @alon_jacovi, Heike Adel, @yoavgo and Thang Vu)\n\nHuman Interpretation of Saliency-based Explanation Over Text\n<LINK>\n\nWe investigate human perception of heat map explanations and find that intuitive understanding is biased.\n\n1/5 <LINK>', 'Saliency attribution methods score how important parts of a model’s input are to the model decision and are often visualised using heat maps.\nPrevious work focused on developing and verifying attribution methods. Less is known about how humans interpret these explanations.\n\n2/5', 'We conduct various user studies to investigate whether superficial and unrelated factors (e.g., word length) influence human self-reported importance ratings.\nWe collect user feedback and statistically analyse importance ratings using a GAMM model.\n\n3/5 https://t.co/sTaHj1CdOi', ""We find that numerous factors such as word length, sentence length and learning effects affect human importance ratings. These factors shouldn't affect importance, because the explanation already objectively has importance, but they do.\n\n4/5 https://t.co/qnz5PlqZh8"", 'We present two bias correction methods and demonstrate their ability to compensate the distorting influence of word length and repeated exposure.\nDetails are in paper. Thanks!\n\n5/5 https://t.co/MHk36SNAPt']",http://arxiv.org/abs/2201.11569,"While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople's interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees' importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation. ",Human Interpretation of Saliency-based Explanation Over Text
132,1487072337690304514,349243149,Maxime Trebitsch,"[""Today on the #arXiv, a paper I'm super excited to be involved in, led by the fantastic Sophia Flury: the first results from the Low-Redshift Lyman Continuum Survey (<LINK>), a survey of 66 galaxies at z~0.3 where we find LyC escape from 35 of them! 1/6 <LINK>"", 'This is the first paper of a series, but this is already a HUGE increase in the number of detections, and it *significantly* expands the parameter space explored for Lyman Continuum emitters in terms of their physical properties. 2/6 https://t.co/y0zFFQ4Odg', 'I will not go in details, but this first paper presents the sample and the range of properties of the selected objects. There are a few more papers submitted (+ one accepted: https://t.co/eBVmBHKf8j), so stay tuned for exciting Lyman Continuum Escape science! 3/6', ""It's really an amazing project to be part of! Some of the authors are on twitter (@amorinric, @dyonysos, @JeremyBlaizot, @MCMCastellano, @astrosteven, @antihayestamine, @alh_ast, @caseypapovich, @scorfano) and they might have cleverer things to say about the survey! 4/6"", ""One more thing to finish. I'm a theorist, but I'm super lucky to have been included in this project from the start. I have attended many telecons discussing the details of the COS data reduction, basically following how we went from getting the photons to writing the paper. 5/6"", ""This has been extremely enlightening, and just like I would fully recommend to all observers to try and be involved with their theorists friends, I think it's super important for theorists to do the same! 6/6""]",https://arxiv.org/abs/2201.11716,"The origins of Lyman continuum (LyC) photons responsible for the reionization of the universe are as of yet unknown and highly contested. Detecting LyC photons from the epoch of reionization is not possible due to absorption by the intergalactic medium, which has prompted the development of several indirect diagnostics to infer the rate at which galaxies contribute LyC photons to reionize the universe by studying lower-redshift analogs. We present the Low-redshift Lyman Continuum Survey (LzLCS) comprising measurements made with HST/COS for a z=0.2-0.4 sample of 66 galaxies. After careful processing of the FUV spectra, we obtain a total of 35 Lyman continuum emitters (LCEs) detected with 97.725% confidence, nearly tripling the number of known local LCEs. We estimate escape fractions from the detected LyC flux and upper limits on the undetected LyC flux, finding a range of LyC escape fractions up to 50%. Of the 35 LzLCS LCEs, 12 have LyC escape fractions greater than 5%, more than doubling the number of known local LCEs with cosmologically relevant LyC escape. ","The Low-Redshift Lyman Continuum Survey I: New, Diverse Local
  Lyman-Continuum Emitters"
133,1486991608507777026,216550726,John R. Weaver,['How do supermassive black holes interact with their accretion discs? Long-term variability can teach us a lot. Find out the unexpected physics we may have uncovered:\n<LINK>\n\n@DAWNCopenhagen \n@PhysAstroStAnd \n@GrundforskFond \n\nCredit: NOIRLab/NSF/AURA/J. da Silva <LINK>'],https://arxiv.org/abs/2201.11134,"We present a study of 9242 spectroscopically-confirmed quasars with multi-epoch ugriz photometry from the SDSS Southern Survey. By fitting a separable linear model to each quasar's spectral variations, we decompose their five-band spectral energy distributions into variable (disc) and non-variable (host galaxy) components. In modelling the disc spectra, we include attenuation by dust on the line of sight through the host galaxy to its nucleus. We consider five commonly used attenuation laws, and find that the best description is by dust similar to that of the Small Magellanic Cloud, inferring a lack of carbonaceous grains from the relatively weak 2175AA absorption feature. We go on to construct a composite spectrum for the quasar variations spanning 700 to 8000AA. By varying the assumed power-law $L_{\nu}\propto\nu^\alpha$ spectral slope, we find a best-fit value $\alpha=0.71\pm0.02$, excluding at high confidence the canonical $L_{\nu}\propto\nu^{1/3}$ prediction for a steady-state accretion disc with a $T\propto r^{-3/4}$ temperature profile. The bluer spectral index of the observed quasar variations instead supports the model of Mummery & Balbus in which a steeper temperature profile, $T\propto r^{-7/8}$, develops as a result of finite magnetically-induced stress at the innermost stable circular orbit extracting energy and angular momentum from the black hole spin. ","Dust and the intrinsic spectral index of quasar variations: hints of
  finite stress at the innermost stable circular orbit"
134,1486891959369273352,1019760963569049601,Almog Yalinewich,"['New paper on the arxiv, and this one is a moonshot. We propose that massive naked stars perform a ""gun salute"" when they collapse to a black hole, and shoot a moon mass buckshot at the speed of light. Explanation below\n<LINK>\n\n1/4 <LINK>', 'The figure below (from Burrows et al 2005) shows two bumps in the lightcurve. The first is the prompt GRB and the second is the X ray flare. This emission could come from two relativistic shells with different Lorentz factors. But how would you produce these two shells?\n\n2/4 https://t.co/OQWucSVRIL', 'Enter our model. We look at what happens when a GRB jet erupts from the star. The jet drags along a thin baryonic layer from the star, accelerates it to some moderate Lorentz factor, and then the baryonic shell breaks up due to the Rayleigh Taylor instability.\n\n3/4 https://t.co/5tCEeLQ5oQ', 'After breakup, the jet material streams past the debris, so it can attain a high Lorentz factor, while the debris remains at a lower Lorentz factor. Thus we get the two-shell structure required to explain both the prompt GRB and the X ray flares.\n\n4/4 https://t.co/9LuHLIXl9r']",https://arxiv.org/abs/2201.11177,"In this work we consider the eruption of a tenuous relativistic hydrodynamic jet from a dense baryonic envelope. As the jet moves out and away, it carries along and continues to accelerate a layer of baryonic material which we refer to as the plug. We solve the relativistic equations of motion for the trajectory of the plug, and verify it using a relativistic hydrodynamic simulation. We show that under these conditions, the plug breaks up at a radius larger by a factor of a few from the radius of the envelope, due to the onset of the Rayleigh Taylor instability. After breakup the jet continues to accelerate to higher Lorentz factors while the plug fragments maintain a moderate Lorentz factor. The presence of slower moving ejecta can explain late time features of GRBs such as X ray flares without recourse to a long lived engine. ",Plug Disintegration in GRB Jet Eruption
135,1486771587374358529,20697617,Josh Peek,"['Really cool paper out on ArXiv today by Gabrielle Contardo, @davidwhogg, Jason Hunt, me, and Yen-Chi Chen, on a topic I have been fascinated by for 3 years now: how do we find unexpected gaps in data sets? <LINK>', 'My main contribution to this project is having posed the question, based on thinking about the Jao+/Gaia gap in the main sequence. Gabrielle and co figured out how to use geometrical and topological data analysis to robustly find these gaps in large data sets.']",https://arxiv.org/abs/2201.10674,"Discoveries of gaps in data have been important in astrophysics. For example, there are kinematic gaps opened by resonances in dynamical systems, or exoplanets of a certain radius that are empirically rare. A gap in a data set is a kind of anomaly, but in an unusual sense: Instead of being a single outlier data point, situated far from other data points, it is a region of the space, or a set of points, that is anomalous compared to its surroundings. Gaps are both interesting and hard to find and characterize, especially when they have non-trivial shapes. We present methods to address this problem. First, we present a methodological approach to identify critical points, a criterion to select the most relevant ones and use those to trace the `valleys' in the density field. We then build on the observed properties of critical points to propose a novel gappiness criterion that can be computed at any point in the data space. This allows us to identify a broader variety of gaps, either by highlighting regions of the data-space that are `gappy' or by selecting data points that lie in local under densities. We also explore methodological ways to make the detected gaps robust to changes in the density estimation and noise in the data. We illustrate our methods on the velocity distribution of nearby stars in the Milky Way disk plane, which exhibits gaps that could originate from different processes. Identifying and characterizing those gaps could help determine their origins. ","The emptiness inside: Finding gaps, valleys, and lacunae with geometric
  data analysis"
136,1486682110630973445,968435714475069445,Massissilia Hamadouche,"['My first paper is out on arXiv today! We study two samples of massive quiescent galaxies at 0.6 &lt; z &lt; 1.3, looking at the relationships between size, mass, age and metallicity: <LINK>']",https://arxiv.org/abs/2201.10576,"We study the relationships between stellar mass, size and age within the quiescent population, using two mass-complete spectroscopic samples with $\mathrm{log_{10}}(M_{\star}/\mathrm{M_{\odot}})>10.3$, taken from VANDELS at $1.0<z<1.3$, and LEGA-C at $0.6<z<0.8$. Using robust D$_{n}$4000 values, we demonstrate that the well-known 'downsizing' signature is already in place by $z\simeq1.1$, with D$_{n}$4000 increasing by $\simeq0.1$ across a $\simeq$ 1 dex mass interval for both VANDELS and LEGA-C. We then proceed to investigate the evolution of the quiescent galaxy stellar mass-size relation from $z\simeq1.1$ to $z\simeq0.7$. We find the median size increases by a factor of $1.9\pm{0.1}$ at $\mathrm{log_{10}}(M_{\star}/\mathrm{M_{\odot}})=10.5$, and see tentative evidence for flattening of the relation, finding slopes of $\alpha=0.72\pm0.06$ and $\alpha=$ $0.56\pm0.04$ for VANDELS and LEGA-C respectively. We finally split our sample into galaxies above and below our fitted mass-size relations, to investigate how size and D$_{n}$4000 correlate. For LEGA-C, we see a clear difference, with larger galaxies found to have smaller D$_{n}$4000 at fixed stellar mass. Due to the faintness and smaller numbers of the VANDELS sample, we cannot confirm whether a similar relation exists at $z\simeq1.1$. We consider whether differences in stellar age or metallicity are most likely to drive this size-D$_{n}$4000 relation, finding that any metallicity differences are unlikely to fully explain the observed offset, meaning smaller galaxies must be older than their larger counterparts. We find the observed evolution in size, mass and D$_{n}$4000 across the $\simeq2$ Gyr from $z\sim1.1$ to $z\sim0.7$ can be explained by a simple toy model in which VANDELS galaxies evolve passively, whilst experiencing a series of minor mergers. ","A combined VANDELS and LEGA-C study: the evolution of quiescent galaxy
  size, stellar mass and age from $\mathbf{\textit{z} = 0.6}$ to
  $\mathbf{\textit{z} = 1.3}$"
137,1486677148538605569,1306523919428521984,Simran Tinani,"['The performance of decoding algorithms on the well-studied LDPC codes depend strongly on the cycles in their parity-check matrices. In this work, we present a number theoretic framework for studying these cycles. <LINK>']",https://arxiv.org/abs/2201.11112,"LDPC codes constructed from permutation matrices have recently attracted the interest of many researchers. A crucial point when dealing with such codes is trying to avoid cycles of short length in the associated Tanner graph, i.e. obtaining a possibly large girth. In this paper, we provide a framework to obtain constructions of such codes. We relate criteria for the existence of cycles of a certain length with some number-theoretic concepts, in particular with the so-called Sidon sets. In this way we obtain examples of LDPC codes with a certain girth. Finally, we extend our constructions to also obtain irregular LDPC codes. ",A Number Theoretic Approach to Cycles in LDPC Codes
138,1486617114194624514,1216114532,Adam Carnall,"['New paper today by @MassiHamadouche looking at the interrelations between mass, size, age and metallicity for massive quiescent galaxies at 0.6 &lt; z &lt; 1.3. We find both age and metallicity gradients on the size-mass plane are necessary to explain our data: <LINK>.', '@scorfano @MassiHamadouche Thanks! Will take a look.']",https://arxiv.org/abs/2201.10576,"We study the relationships between stellar mass, size and age within the quiescent population, using two mass-complete spectroscopic samples with $\mathrm{log_{10}}(M_{\star}/\mathrm{M_{\odot}})>10.3$, taken from VANDELS at $1.0<z<1.3$, and LEGA-C at $0.6<z<0.8$. Using robust D$_{n}$4000 values, we demonstrate that the well-known 'downsizing' signature is already in place by $z\simeq1.1$, with D$_{n}$4000 increasing by $\simeq0.1$ across a $\simeq$ 1 dex mass interval for both VANDELS and LEGA-C. We then proceed to investigate the evolution of the quiescent galaxy stellar mass-size relation from $z\simeq1.1$ to $z\simeq0.7$. We find the median size increases by a factor of $1.9\pm{0.1}$ at $\mathrm{log_{10}}(M_{\star}/\mathrm{M_{\odot}})=10.5$, and see tentative evidence for flattening of the relation, finding slopes of $\alpha=0.72\pm0.06$ and $\alpha=$ $0.56\pm0.04$ for VANDELS and LEGA-C respectively. We finally split our sample into galaxies above and below our fitted mass-size relations, to investigate how size and D$_{n}$4000 correlate. For LEGA-C, we see a clear difference, with larger galaxies found to have smaller D$_{n}$4000 at fixed stellar mass. Due to the faintness and smaller numbers of the VANDELS sample, we cannot confirm whether a similar relation exists at $z\simeq1.1$. We consider whether differences in stellar age or metallicity are most likely to drive this size-D$_{n}$4000 relation, finding that any metallicity differences are unlikely to fully explain the observed offset, meaning smaller galaxies must be older than their larger counterparts. We find the observed evolution in size, mass and D$_{n}$4000 across the $\simeq2$ Gyr from $z\sim1.1$ to $z\sim0.7$ can be explained by a simple toy model in which VANDELS galaxies evolve passively, whilst experiencing a series of minor mergers. ","A combined VANDELS and LEGA-C study: the evolution of quiescent galaxy
  size, stellar mass and age from $\mathbf{\textit{z} = 0.6}$ to
  $\mathbf{\textit{z} = 1.3}$"
139,1486351835489312783,109571291,Daniel González,"['Fresh from the arXiv:\n\nWe study the non-equilibrium dynamics of a simple lattice gauge theory, where local symmetry constraints can prevent thermalization. We show, in particular, how quantum many-body scars appear in the deconfined phase of the theory 👇🏽\n\n<LINK>']",https://arxiv.org/abs/2201.10260,"The weak ergodicity breaking induced by quantum many-body scars (QMBS) represents an intriguing concept that has received great attention in recent years due to its relation to unusual non-equilibrium behaviour. Here we reveal that this phenomenon can occur in a previously unexplored regime of a lattice gauge theory, where QMBS emerge due to the presence of an extensive number of local constraints. In particular, by analyzing the gauged Kitaev model, we provide an example where QMBS appear in a regime where charges are deconfined. By means of both numerical and analytical approaches, we find a variety of scarred states far away from the regime where the model is integrable. The presence of these states is revealed both by tracing them directly from the analytically reachable limit, as well as by quantum quenches showing persistent oscillations for specific initial states. ",Scar States in Deconfined $\mathbb{Z}_2$ Lattice Gauge Theories
140,1486204009958494209,937194665991987200,Enrique Solano,['Another masterpiece towards Neuromorphic Quantum Computing and Quantum Neural Networks with quantum memristors <LINK> We study the interplay of multipartite entanglement with memristivity in linear and triangular arrays @QuantumFlagship @QuantenTech @QuantumDaily <LINK>'],https://arxiv.org/abs/2201.10309,"We study the entanglement and memristive properties of three coupled quantum memristors. We consider quantum memristors based on superconducting asymmetric SQUID architectures which are coupled via inductors. The three quantum memristors are arranged in two different geometries: linear and triangular coupling configurations. We obtain a variety of correlation measures, including bipartite entanglement and tripartite negativity. We find that, for identical quantum memristors, entanglement and memristivity follow the same behavior for the triangular case and the opposite one in the linear case. Finally, we study the multipartite correlations with the tripartite negativity and entanglement monogamy relations, showing that our system has genuine tripartite entanglement. Our results show that quantum correlations in multipartite memristive systems have a non-trivial role and can be used to design quantum memristor arrays for quantum neural networks and neuromorphic quantum computing architectures. ",Tripartite entanglement in quantum memristors
141,1485965019015696388,1317779633572634625,Martin Weyssow,"['Our new vision paper is available on ArXiv: <LINK>\nIn this @ICSEconf paper, we propose a multi-modal learning framework to modeling the programming world by learning from multiple levels of semantics of code.\n#icse22 #deeplearning #multimodal #ml4se #codesearch <LINK>']",http://arxiv.org/abs/2201.03346,"The progress made in code modeling has been tremendous in recent years thanks to the design of natural language processing learning approaches based on state-of-the-art model architectures. Nevertheless, we believe that the current state-of-the-art does not focus enough on the full potential that data may bring to a learning process in software engineering. Our vision articulates on the idea of leveraging multi-modal learning approaches to modeling the programming world. In this paper, we investigate one of the underlying idea of our vision whose objective based on concept graphs of identifiers aims at leveraging high-level relationships between domain concepts manipulated through particular language constructs. In particular, we propose to enhance an existing pretrained language model of code by joint-learning it with a graph neural network based on our concept graphs. We conducted a preliminary evaluation that shows gain of effectiveness of the models for code search using a simple joint-learning method and prompts us to further investigate our research vision. ","Better Modeling the Programming World with Code Concept Graphs-augmented
  Multi-modal Learning"
142,1485814129197785091,1347114826267504641,Sarah Betti,"[""New astronomy paper alert! We detect low levels of water ice in a disk around a young Herbig star, AB Aur, out to ~150 AU!  We also find a highly red disk compared to models in the NIR and detect the inner spiral arms in L' for the first time.  🌠⭐️💥\n<LINK>"", ""This paper is in collaboration with another paper now out on arxiv by Sebastián Jorquera who looks for the planetary mass companions around the disk of AB Aur in L' band for the first time! \nhttps://t.co/8sZWFqcLol""]",https://arxiv.org/abs/2201.08868,"We present near-infrared Large Binocular Telescope Interferometer LMIRCam imagery of the disk around the Herbig Ae/Be star AB Aurigae. A comparison of surface brightness at Ks (2.16 ${\mu}$m), H2O narrowband (3.08 ${\mu}$m), and L' (3.7 ${\mu}$m) allows us to probe the presence of icy grains in this (pre)transitional disk environment. By applying Reference Differential Imaging PSF subtraction, we detect the disk at high signal to noise in all three bands. We find strong morphological differences between bands, including asymmetries consistent with observed spiral arms within 100 AU in L'. An apparent deficit of scattered light at 3.08 ${\mu}$m relative to bracketing wavelengths (Ks and L') is evocative of ice absorption at the disk surface layer. However, the ${\Delta}$(Ks-H2O) color is consistent with grains with little to no ice (0-5% by mass). The ${\Delta}$(H2O-L') color, conversely, suggests grains with a much higher ice mass fraction (~0.68), and the two colors cannot be reconciled under a single grain population model. Additionally, we find the extremely red ${\Delta}$(Ks-L') disk color cannot be reproduced under conventional scattered light modeling with any combination of grain parameters or reasonable local extinction values. We hypothesize that the scattering surfaces at the three wavelengths are not co-located, and optical depth effects result in each wavelength probing the grain population at different disk surface depths. The morphological similarity between Ks and H2O suggests their scattering surfaces are near one another, lending credence to the ${\Delta}$(Ks-H2O) disk color constraint of < 5% ice mass fraction for the outermost scattering disk layer. ","Detection of Near-Infrared Water Ice at the Surface of the
  (pre)Transitional Disk of AB Aur: Informing Icy Grain Abundance, Composition,
  and Size"
143,1485713175131660296,1237134419879747587,Hanjia Lyu,"['New preprint: <LINK>\nTo understand the global sentiment on the Israel-Palestine conflict in 2021, we devise an observational study to understand the friendliness of countries, agglomerated by the sentiments of tweets. <LINK>']",http://arxiv.org/abs/2201.05961,"The Israel-Palestine Conflict, one of the most enduring conflicts in history, dates back to the start of the 19th century and has deeply rooted complex issues in politics, demography, religion, and other aspects, making it harder to attain resolve. To understand the global sentiment on the conflict, we devise an observational study to understand the friendliness of countries, agglomerated by the sentiments of tweets. We collect Twitter data using popular hashtags around and specific to the conflict containing opinions neutral or partial to the two parties. We use different sentiment analysis tools and methods to classify tweets into pro-Palestinian, pro-Israel, or neutral. This paper further describes the implementation of data mining methodologies to obtain insights into the global notion of the conflict and attempts to reason about countries' partiality toward a side in the conflict. ",Taking sides: Public Opinion over the Israel-Palestine Conflict in 2021
144,1485605101448409092,1680478232,Marta Mauri,"['We are excited to share our work on the challenging topic of generalization in classical and quantum generative models!\n<LINK>\nWe propose a new framework and generalization metrics, that are also useful tools to rigorously define practical quantum advantage. 1/8 <LINK>', 'Our approach considers the different behaviours a trained model can exhibit when generating data, exemplified here using the 3x3 Bars and Stripes dataset. We introduce various generalization types, and we propose metrics that can quantify them across various trained models. 2/8 https://t.co/Gq2926qfzm', 'Generalization requires a generative model to produce new samples (in G_new), and not to just data-copy from D_train. New data might not show the desired pattern shared by elements in the solution space S: generalization should assess if samples are new and valid (in G_sol). 3/8 https://t.co/ka5dFE3kIq', 'Our validity-based framework allows to evaluate if a model can learn a given fixed feature in the training set and produce new samples with the same feature (e.g., stripes). A 3D evaluation scheme, based on Fidelity, Rate, and Coverage, is proposed to quantify this behaviour. 4/8', 'Our quality-based framework allows to evaluate if a trained model can learn to produce novel samples that not only exhibit a given feature shared by the training data, but are also high-quality samples, according to a suitable associated cost or score (e.g., low cost bar). 5/8', 'We use our framework to evaluate two families of generative models: GANs (classical) and Tensor Network Born Machines (quantum-inspired) on a portfolio optimization use case. While we choose these models for our benchmarks, we highlight that our metrics are model-agnostic! 6/8', 'Our validity-based metrics show that the quantum-inspired model is a clear winner, achieving superior generalization values to three GAN variants. The TNBM is (3.76, 3.75, 68.2) times better than the GANs across (Fidelity, Rate, Coverage) values! 7/8 https://t.co/xSVS1OG3yT', 'As a highlight of quality-based metrics, we demonstrate that the TNBM is able to generate queries that are of better quality than those in the training set. The quantum-inspired model produces 15x more of these samples than the classical GAN. 8/8 https://t.co/4XLYQD0rCj']",https://arxiv.org/abs/2201.08770,"Defining and accurately measuring generalization in generative models remains an ongoing challenge and a topic of active research within the machine learning community. This is in contrast to discriminative models, where there is a clear definition of generalization, i.e., the model's classification accuracy when faced with unseen data. In this work, we construct a simple and unambiguous approach to evaluate the generalization capabilities of generative models. Using the sample-based generalization metrics proposed here, any generative model, from state-of-the-art classical generative models such as GANs to quantum models such as Quantum Circuit Born Machines, can be evaluated on the same ground on a concrete well-defined framework. In contrast to other sample-based metrics for probing generalization, we leverage constrained optimization problems (e.g., cardinality constrained problems) and use these discrete datasets to define specific metrics capable of unambiguously measuring the quality of the samples and the model's generalization capabilities for generating data beyond the training set but still within the valid solution space. Additionally, our metrics can diagnose trainability issues such as mode collapse and overfitting, as we illustrate when comparing GANs to quantum-inspired models built out of tensor networks. Our simulation results show that our quantum-inspired models have up to a $68 \times$ enhancement in generating unseen unique and valid samples compared to GANs, and a ratio of 61:2 for generating samples with better quality than those observed in the training set. We foresee these metrics as valuable tools for rigorously defining practical quantum advantage in the domain of generative modeling. ",Evaluating Generalization in Classical and Quantum Generative Models
145,1485567090006568966,72806677,Alfonso Semeraro,"['🚨New preprint!\n\nWe studied how COVID-19 vaccines were framed by the press in terms of emotions and semantic associations \n📎<LINK>\n\nw/ @sal_vilella  @MassimoSt  @giaruffo @CognoscoL @exetercompsci @diunito \n\nthread 🧵👇', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito in mainstream media (main newspapers) “vaccine” was consistently associated to positive emotions like trust and anticipation, and less disgust, while such emotional prominence is not visible in alternative sources (blogs and counter-information) 2/10 https://t.co/tO4Vgi0Zwi', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito not all vaccines were ~framed~ equally, tho. While mainstream news titles portrayed AstraZeneca with positive emotions, alternative ones conveyed sadness (but less disgust).  3/10 https://t.co/FgkdMxRCh5', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito On 15 March 2021, the administration of AZ was suspended temporarily due to the reporting of a few cases of thrombosis. This is a major event in the story of the vaccination campaign, and of how newspapers reported on vaccines and potential undesired effects. 4/10', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito Before suspension, the attention was all on Pfizer and its possible adverse reactions, like “allergy”, “fever”, “reaction”, “risk”, especially in alternative news. Around the suspension date the press focused on AstraZeneca, and Pfizer lost centrality. 5/10 https://t.co/jFPPRvTBJY', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito AZ became more central, but it underwent a semantic and emotive shift. While once associated with neutral concepts, it was now linked to negative words like “threat”, “death” and “thrombosis” 6/10 https://t.co/dgDFgiMHWT', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito “thrombosis” joined the conversation, reported by both kinds of outlets. “death” recurred since day 1 of the pandemic, but it was negatively re-framed after the suspension: it lost its trust+anticipation connotation (hope that vaccines would work?) and it became fearful 7/10 https://t.co/tsziXnMrT1', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito Other details in the article. For this work we sourced more than 5000 Italian🇮🇹 news articles about COVID-19 vaccines that were massively re-shared on Facebook (5M) and Twitter (200k), including 17 different outlets over an 8 months time period. 8/10 https://t.co/L0z5n7mDC6', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito We extracted the semantic frames of words through Formamentis networks that highlight semantic association between words in texts. Emotions were drawn with the python library PyPlutchik 9/10\n\nFormamentis: https://t.co/Z0deqFXqt1\nPyPlutchik: https://t.co/YrSc0SKnkw', '@sal_vilella @MassimoSt @giaruffo @CognoscoL @exetercompsci @diunito Alternative/Mainstream classifications can reflect different emotional narratives of the same events. Our approach unveils the structure of these narratives, enabling further understanding of massively read content!\n\nCheck the details here: https://t.co/1BmMHXMf3y https://t.co/qE4XcF6Xjq']",https://arxiv.org/abs/2201.07538,"Since their announcement in November 2020, COVID-19 vaccines were largely debated by the press and social media. With most studies focusing on COVID-19 disinformation in social media, little attention has been paid to how mainstream news outlets framed COVID-19 narratives compared to alternative sources. To fill this gap, we use cognitive network science and natural language processing to reconstruct time-evolving semantic and emotional frames of 5745 Italian news, that were massively re-shared on Facebook and Twitter, about COVID-19 vaccines. We found consistently high levels of trust/anticipation and less disgust in the way mainstream sources framed the general idea of ""vaccine/vaccino"". These emotions were crucially missing in the ways alternative sources framed COVID-19 vaccines. More differences were found within specific instances of vaccines. Alternative news included titles framing the AstraZeneca vaccine with strong levels of sadness, absent in mainstream titles. Mainstream news initially framed ""Pfizer"" along more negative associations with side effects than ""AstraZeneca"". With the temporary suspension of the latter, on March 15th 2021, we identified a semantic/emotional shift: Even mainstream article titles framed ""AstraZeneca"" as semantically richer in negative associations with side effects, while ""Pfizer"" underwent a positive shift in valence, mostly related to its higher efficacy. ""Thrombosis"" entered the frame of vaccines together with fearful conceptual associations, while ""death"" underwent an emotional shift, steering towards fear in alternative titles and losing its hopeful connotation in mainstream titles. Our findings expose crucial aspects of the emotional narratives around COVID-19 vaccines adopted by the press, highlighting the need to understand how alternative and mainstream media report vaccination news. ","Writing about COVID-19 vaccines: Emotional profiling unravels how
  mainstream and alternative press framed AstraZeneca, Pfizer and vaccination
  campaigns"
146,1484543767923380225,1285579351950598144,Karolina Stanczak,['New #NLProc paper: a latent-variable model for intrinsic probing using VA\n\n1) We analyse information distribution and \n2) find overlap in the neurons encoding linguistic features across langs! \n\n<LINK>\n@ltorroba1 @adinamwilliams @ryandcotterell @IAugenstein <LINK>'],https://arxiv.org/abs/2201.08214,"The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. Indeed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization. In this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute, but also to pinpoint where this attribute is encoded. We propose a novel latent-variable formulation for constructing intrinsic probes and derive a tractable variational approximation to the log-likelihood. Our results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature. Finally, we find empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax. ",A Latent-Variable Model for Intrinsic Probing
147,1484451949386817536,1238834561892659207,Jessica Di Cocco,"['We were informed of an error in our dataset used for the article ""How Populist are Parties?"" (doi:10.1017/pan.2021.29). You find the updated dataset here <LINK> and an addendum/corrigendum where we explain the error here <LINK> \n@BernardoMonechi', 'We thank @michaelj505 and @Robert_A_Huber for finding the erorr!']",https://arxiv.org/abs/2201.07972,"This paper is a corrigendum and addendum to the previously published article: 'How Populist are Parties? Measuring Degrees of Populism in Party Manifestos Using Supervised Machine Learning' (Political Analysis, 1-17. doi:10.1017/pan.2021.29). These corrigendum and addendum were prepared to correct errors in data labelling and show some extra insights not included in the previously published paper. Here, we report these corrections and point to some additional conclusions by focusing on the effects of the label reshuffling per parties and years and presenting new figures wherever appropriate. We show that although the simplified labelling method proposed in the previously-published article can induce biases in the correlations with expert scores, random labelling reduces correlations significantly. We show that this is also true for correlations based on a manually-coded data set. These modifications are based on other evidence and results reported in detail in a future publication. ","Corrigendum and addendum to: How Populist are Parties? Measuring Degrees
  of Populism in Party Manifestos Using Supervised Machine Learning"
148,1484448383649599491,1376919770394677254,Raimel A. Medina,"['Can we avoid the emergence of barren plateaus in the initialization and throughout the optimization of VQAs?\n\nWe propose a general algorithm that does that relying on the shadow tomography scheme <LINK>.\n[🧵1/n]', 'We introduce the notion of weak barren plateaus (WBPs), in order to diagnose and avoid BPs in variational quantum optimization.\n\nWBPs emerge when the entanglement of a local subsystem exceeds a certain threshold identified by the entanglement of a fully scrambled state. [🧵2/n]', 'In contrast to BPs, WBPs can be efficiently diagnosed using the few-body density matrices and we show that their absence is a sufficient condition for avoiding BPs 💪! [🧵3/n]', 'Based on the notion of WBPs, we propose an algorithm that can be readily implemented on available NISQ devices ⚛️\n\nThe algorithm uses classical shadows to efficiently estimate the value of the cost function, its gradients, and the 2-Renyi entropy of small subsystems. [🧵 4/n] https://t.co/t1tsE0qqtN', 'Our results indicate that entanglement, in addition to playing a crucial role for circumventing BPs at the launch of the VQE, is also important for achieving a good optimization performance🙌 [🧵5/n]', 'It was a long journey but a very fun one! Big thanks ☺️ to all my collaborators @steve_quantum, @RichardKueng, Alex Michailidis and Maksym Serbyn. [🧵6/n]', '@DWierichs Oh, yes! That’s a typo. Thanks for pointing it out', '@ColesQuantum @steve_quantum @RichardKueng Thanks!']",https://arxiv.org/abs/2201.08194,"Variational quantum algorithms are promising algorithms for achieving quantum advantage on near-term devices. The quantum hardware is used to implement a variational wave function and measure observables, whereas the classical computer is used to store and update the variational parameters. The optimization landscape of expressive variational ans\""atze is however dominated by large regions in parameter space, known as barren plateaus, with vanishing gradients which prevents efficient optimization. In this work we propose a general algorithm to avoid barren plateaus in the initialization and throughout the optimization. To this end we define a notion of weak barren plateaus (WBP) based on the entropies of local reduced density matrices. The presence of WBPs can be efficiently quantified using recently introduced shadow tomography of the quantum state with a classical computer. We demonstrate that avoidance of WBPs suffices to ensure sizable gradients in the initialization. In addition, we demonstrate that decreasing the gradient step size, guided by the entropies allows to avoid WBPs during the optimization process. This paves the way for efficient barren plateau free optimization on near-term devices. ",Avoiding barren plateaus using classical shadows
149,1484361628476866565,1238886955372470274,Xiaohui Chen,['Happy to announce that the paper <LINK> will appear in AISTATS. We propose a sketch-and-lift algo that first estimates cluster centroids using a subsampled SDP and then propagates the solution to the full data in linear time. Exact recovery guarantee is provided.'],https://arxiv.org/abs/2201.08226,"Semidefinite programming (SDP) is a powerful tool for tackling a wide range of computationally hard problems such as clustering. Despite the high accuracy, semidefinite programs are often too slow in practice with poor scalability on large (or even moderate) datasets. In this paper, we introduce a linear time complexity algorithm for approximating an SDP relaxed $K$-means clustering. The proposed sketch-and-lift (SL) approach solves an SDP on a subsampled dataset and then propagates the solution to all data points by a nearest-centroid rounding procedure. It is shown that the SL approach enjoys a similar exact recovery threshold as the $K$-means SDP on the full dataset, which is known to be information-theoretically tight under the Gaussian mixture model. The SL method can be made adaptive with enhanced theoretic properties when the cluster sizes are unbalanced. Our simulation experiments demonstrate that the statistical accuracy of the proposed method outperforms state-of-the-art fast clustering algorithms without sacrificing too much computational efficiency, and is comparable to the original $K$-means SDP with substantially reduced runtime. ","Sketch-and-Lift: Scalable Subsampled Semidefinite Program for $K$-means
  Clustering"
150,1484067470986002435,2813466842,Javed Iqbal,"['Reach out to our new Preprint, ""Combining Scale-Invariance and Uncertainty for Self-Supervised Domain Adaptation of Foggy Scenes Segmentation"", led by Dr. Mohsen Ali and Dr. Rehan Hafiz.\n\n<LINK>\n\nIn this work, we propose scale-invariance…<LINK>']",https://arxiv.org/abs/2201.02588,"This paper presents FogAdapt, a novel approach for domain adaptation of semantic segmentation for dense foggy scenes. Although significant research has been directed to reduce the domain shift in semantic segmentation, adaptation to scenes with adverse weather conditions remains an open question. Large variations in the visibility of the scene due to weather conditions, such as fog, smog, and haze, exacerbate the domain shift, thus making unsupervised adaptation in such scenarios challenging. We propose a self-entropy and multi-scale information augmented self-supervised domain adaptation method (FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported by the empirical evidence that an increase in fog density results in high self-entropy for segmentation probabilities, we introduce a self-entropy based loss function to guide the adaptation method. Furthermore, inferences obtained at different image scales are combined and weighted by the uncertainty to generate scale-invariant pseudo-labels for the target domain. These scale-invariant pseudo-labels are robust to visibility and scale variations. We evaluate the proposed model on real clear-weather scenes to real foggy scenes adaptation and synthetic non-foggy images to real foggy scenes adaptation scenarios. Our experiments demonstrate that FogAdapt significantly outperforms the current state-of-the-art in semantic segmentation of foggy images. Specifically, by considering the standard settings compared to state-of-the-art (SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy Driving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes to Foggy Zurich. ","Combining Scale-Invariance and Uncertainty for Self-Supervised Domain
  Adaptation of Foggy Scenes Segmentation"
151,1484043224495058948,1333669971679801345,Brittany Reid,"['Our paper ""Software Engineering User Study Recruitment on Prolific: An Experience Report"" is now online. We contacted 680 participants with our screening survey and invited 55 to our user study. <LINK> @MWagnerRedChair @marcelodamorim @ctreude <LINK>']",https://arxiv.org/abs/2201.05348,"Online participant recruitment platforms such as Prolific have been gaining popularity in research, as they enable researchers to easily access large pools of participants. However, participant quality can be an issue; participants may give incorrect information to gain access to more studies, adding unwanted noise to results. This paper details our experience recruiting participants from Prolific for a user study requiring programming skills in Node.js, with the aim of helping other researchers conduct similar studies. We explore a method of recruiting programmer participants using prescreening validation, attention checks and a series of programming knowledge questions. We received 680 responses, and determined that 55 met the criteria to be invited to our user study. We ultimately conducted user study sessions via video calls with 10 participants. We conclude this paper with a series of recommendations for researchers. ","Software Engineering User Study Recruitment on Prolific: An Experience
  Report"
152,1483661765913112578,934808651830833153,Contijoch Research Laboratory,"['🚨New Paper Alert 🚨\nMotion during #ComputedTomography can degrade image quality. \nLed by @KunalMGupta, we propose a new #CTReconstruction approach which leverages #NeuralNetworks #Differentiable #Rendering\narXiv: <LINK>\nwebsite: <LINK>\n🧵 1/', 'CardiacCT #YesCCT is difficult because the❤️never stops moving! so parts like coronary arteries can end up blurry! Trad. methods use motion models which are case-specific solutions. \nInstead, we proposed a general approach using #differentiable #rendering to recon.\n2/', 'Here you can see what happens when a small vessel (coronary) moves during a CT scan. In a standard recon (FBP), it looks very blurry (2nd row). Our approach (NCT) gives nice results even with very large motions!\n3/ https://t.co/Mu7LDjZX03', 'CardiacCT can also image heart motion (movies!) but can walls can be blurry when they move. \n\nNCT naturally takes advantage of scenarios with multiple gantry rotations.\n\nThis improves reconstruction of all timepoints during the heart cycle!\n4/ https://t.co/Eb22xX5eQc', 'Pushing it a step further, we look at how well NCT works during really complex motions (severe topological changes). Without changing anything about NCT, it worked really well!\n5/ https://t.co/V0zwC0DLcj', 'For more details (how we did it), check out the paper and project website. #ComputerVision #DeepLearning\n\nThis works is a collaboration between @ucsdjacobs @ucsd_cse @ucsdbe and @UCSDHealth @UCSDImaging.\nIt wouldn’t have been possible without support from @NIH @NHLBI.\n\n6/', 'And stay tuned as we work to bring this new approach to clinical data!\n7/']",https://arxiv.org/abs/2201.06574,"Motion during acquisition of a set of projections can lead to significant motion artifacts in computed tomography reconstructions despite fast acquisition of individual views. In cases such as cardiac imaging, motion may be unavoidable and evaluating motion may be of clinical interest. Reconstructing images with reduced motion artifacts has typically been achieved by developing systems with faster gantry rotation or using algorithms which measure and/or estimate the displacements. However, these approaches have had limited success due to both physical constraints as well as the challenge of estimating/measuring non-rigid, temporally varying, and patient-specific motions. We propose a novel reconstruction framework, NeuralCT, to generate time-resolved images free from motion artifacts. Our approaches utilizes a neural implicit approach and does not require estimation or modeling of the underlying motion. Instead, boundaries are represented using a signed distance metric and neural implicit framework. We utilize `analysis-by-synthesis' to identify a solution consistent with the acquired sinogram as well as spatial and temporal consistency constraints. We illustrate the utility of NeuralCT in three progressively more complex scenarios: translation of a small circle, heartbeat-like change in an ellipse's diameter, and complex topological deformation. Without hyperparameter tuning or change to the architecture, NeuralCT provides high quality image reconstruction for all three motions, as compared to filtered backprojection, using mean-square-error and Dice metrics. ",Neural Computed Tomography
153,1483501184757518339,3045030351,Wesley Cota,"['📝 New preprint!\n\nWe developed a model to study vaccination strategies using real-world data of different diseases (COVID-19 and seasonal influenza) and countries (Brazil, Germany, and Uganda)\n\nw/ @mr_schulenburg, @ghscosta271, @silviojrufv\n\n🔗 <LINK> <LINK>', 'First, we considered the demographics and contact patterns of Brazil together with the infection fatality ratio (IFR) of COVID-19 and seasonal influenza.\n\nFor the vaccination dynamics, we considered age-dependent efficacy against death and infection. https://t.co/Hj9Y2ZAqIn', 'Comparing different strategies, we find that prioritizing the elderly (DAP, blue) is more effective (a,c,e) for COVID-19 than it is for seasonal influenza, especially in high transmission scenarios (parameter ϖ)\n\nThe non-linear effects are stronger for the least effective (b,d,f) https://t.co/tTVnRzofta', 'These effects also appear as a function of the delay tᵥ to start the vaccination, for the (a) most and (b) least effective.\n\nAlso, the reduction of the number of deaths and recovered is drastically affected by this delay, especially without social distancing ({U}, unmitigated). https://t.co/MomwS1cwhj', 'The results of the optimal strategy (a,c,e) are qualitatively equivalent for different demographics, social contact patterns, such as for Germany and Uganda, while those for least effective (b,d,f) reveal a complex dependence on epidemiological parameters. https://t.co/zU1H9DYYQB', 'Our theoretical analysis shows that the outcomes depend non-linearly on the epidemiological scenarios and parameters. \n\nThis study raises important issues that can be underestimated in applied mathematical or statistical epidemiological modeling.\n\n🔗 https://t.co/56qtvDWEti', 'Fio em português: https://t.co/nF1CgS7EhX']",http://arxiv.org/abs/2201.02869,"Effective strategies of vaccine prioritization are essential to mitigate the impacts of severe infectious diseases. We investigate the role of infection fatality ratio (IFR) and social contact matrices on vaccination prioritization using a compartmental epidemic model fueled by real-world data of different diseases (COVID-19 and influenza) and countries (Brazil, Germany, and Uganda). Our study confirms that massive and early vaccination is extremely effective to reduce the disease fatality if the contagion is mitigated, but the effectiveness is increasingly reduced as vaccination beginning delays in an uncontrolled epidemiological scenario. The optimal and least effective prioritization strategies depend non-linearly on epidemiological variables. Regions of the epidemiological parameter space, in which prioritizing the most vulnerable is more effective than the most contagious individuals, are substantially broader for COVID-19's IFR in comparison with influenza. Demographics and social contact matrices deform the phase diagrams but do not alter their qualitative shapes. ","Effects of infection fatality ratio and social contact matrices on
  vaccine prioritization strategies"
154,1483449137387483140,1038120916117606400,Benjamin Remy,"['New Cosmo ∩ ML paper out! We propose a new method to solve the mass-mapping inverse problem by sampling from the posterior distribution with a neural prior.\n\n<LINK>\n\nWork with François Lanusse, @Niall_Jeffrey, Jia Liu, @JLStarck, Ken Osato &amp; Tim Schrabback\n(1/n) <LINK>', 'We show that we are able to sample convergence maps with the expected power spectrum using highly efficient annealed HMC sampling https://t.co/Cyql6rEXcr', 'The prior was learned with denoising score matching over high resolution hydrodynamical simulations \\kappaTNG (Osato et al. 2021) https://t.co/L9kGcGuwow', 'We are thus able to provide unprecedented resolution of mass-map reconstruction, alongside uncertainty quantification through the posterior distribution 🤩 https://t.co/mazKE2niDe', 'Find the code and data on the associated github repo: https://t.co/VF5Yzgk4dy using JAX, Haiku &amp; TFP']",https://arxiv.org/abs/2201.05561,"Weak lensing mass-mapping is a useful tool to access the full distribution of dark matter on the sky, but because of intrinsic galaxy ellipticies and finite fields/missing data, the recovery of dark matter maps constitutes a challenging ill-posed inverse problem. We introduce a novel methodology allowing for efficient sampling of the high-dimensional Bayesian posterior of the weak lensing mass-mapping problem, and relying on simulations for defining a fully non-Gaussian prior. We aim to demonstrate the accuracy of the method on simulations, and then proceed to applying it to the mass reconstruction of the HST/ACS COSMOS field. The proposed methodology combines elements of Bayesian statistics, analytic theory, and a recent class of Deep Generative Models based on Neural Score Matching. This approach allows us to do the following: 1) Make full use of analytic cosmological theory to constrain the 2pt statistics of the solution. 2) Learn from cosmological simulations any differences between this analytic prior and full simulations. 3) Obtain samples from the full Bayesian posterior of the problem for robust Uncertainty Quantification. We demonstrate the method on the $\kappa$TNG simulations and find that the posterior mean significantly outperfoms previous methods (Kaiser-Squires, Wiener filter, Sparsity priors) both on root-mean-square error and in terms of the Pearson correlation. We further illustrate the interpretability of the recovered posterior by establishing a close correlation between posterior convergence values and SNR of clusters artificially introduced into a field. Finally, we apply the method to the reconstruction of the HST/ACS COSMOS field and yield the highest quality convergence map of this field to date. ",Probabilistic Mass Mapping with Neural Score Estimation
155,1483032604655992832,978179721157595136,Joana Fraxanet,"['I am very happy to share our latest work, which can now be found on arXiv:\n\n<LINK>\n\nIn this paper, we study the effect of long-range superconducting pairing on the localization properties of a quasi-periodic chain. 👇🧵 \n@ICFOnians @adauphin4', 'Understanding metal-insulator transitions has been one of the central questions in condensed matter physics. One of the most paradigmatic examples of such transitions is Anderson localization, in which a system becomes insulating in presence of disorder. https://t.co/JrQl1wGrfu', 'The study of Anderson localization in disordered systems needs at least two-dimensions and averaging over many disorder realizations. In contrast, quasi-periodic systems have gained a lot of attention as an alternative to explore localization and criticality. https://t.co/FkZv9a4Ud6', 'Quasiperiodic systems are neither periodic nor disordered, but somewhere in between. One of the most simple examples is the Aubry-André-Harper (AAH) model, a one-dimensional model resulting from the superposition of two incommensurate lattices. https://t.co/1uyjZA8zMQ', 'Turns out that the AAH model features a metal-insulator transition! For certain parameters all the states of the system suddently become localized. Moreover, exactly at the transition point, the states of the system are critical and show multifractal properties. https://t.co/eys0gVUEUQ', 'One can ask what happens when considering more complicated systems, since nature is usually not that simplistic. In particular, what happens when particles interact in such a context? Does the nature of these interactions affect the localization properties and the transition? https://t.co/MXuJQKhG6c', 'In our work, we answer these questions for the case of a model with long-range superconducting pairing. Indeed, we see that the nature of the transition changes remarkably, leading to extended multifractal regimes and hybridization of energy bands with different properties. https://t.co/wLHSOIpmru', 'Finally, we also put together a toolbox of different methods to characterize localized, ergodic and multifractal states. The code to reproduce all the data and the figures can be found in  https://t.co/Qw2Bc78Mr6.', '(PS: In case anyone is interested in the topological properties of this same model, we published a study one year ago in Physical Review Research: https://t.co/GKcNSZdw8A)', '@aBohrdt @ICFOnians @adauphin4 Thanks!😊']",https://arxiv.org/abs/2201.05458,"In the presence of quasiperiodic potentials, the celebrated Kitaev chain presents an intriguing phase diagram with ergodic, localized and and multifractal states. In this work, we generalize these results by studying the localization properties of the Aubry-Andr\'e-Harper model in the presence of long-range hopping and superconducting pairing amplitudes. These amplitudes decay with power-law exponents $\xi$ and $\alpha$ respectively. To this end, we review and compare a toolbox of global and local characterization methods in order to investigate different types of transitions between ergodic, localized and multifractal states. We report energy-dependent transitions from ergodic to multifractal states for pairing terms with $\alpha<1$ and energy-dependent transitions from ergodic to localized states with an intermediate multifractal region for $\alpha>1$. The size of the intermediate multifractal region depends not only on the value of the superconducting pairing term $\Delta$, but also on the energy band. The transitions are not described by a mobility edge, but instead we report hybridization of bands with different types of localization properties. This leads to coexisting multifractal regimes where fractal dimensions follow different distributions. ","Localization and multifractal properties of the long-range Kitaev chain
  in the presence of an Aubry-Andr\'e-Harper modulation"
156,1482424863478456328,2942501034,Saurabh Garg,"['""Can we predict OOD performance given access to unlabeled target data?""\n\nWe investigate methods to predict target domain performance and find a simple method that does surprisingly well. \n\nPaper: <LINK> \n\nwith Siva B, @zacharylipton, @bneyshabur, @HanieSedghi\n\n1/ <LINK>', 'Our proposed method ATC outperforms contemporary methods in predicting target accuracy on a variety of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDs, CIFAR, and MNIST). \n\n2/ https://t.co/gDKjCpCASA', ""Idea: ATC learns a threshold on the model's confidence such that the fraction of examples above the threshold matches the validation source accuracy and predicts the target accuracy as the fraction of unlabeled examples with model confidence above the threshold. \n\n3/"", 'ATC is simple to implement within existing frameworks. Additionally, we observe that the scatter plot of predicted accuracy with ATC versus true OOD accuracy lies on the desired line y = x. \n\n4/ https://t.co/V2KaVOWRIc', 'We also explore the theoretical foundations of the problem. We prove that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift.\n\n5/', 'Finally, analyzing our method on toy distributions, we provide insights into when it works. \n\nPaper: https://t.co/2P7QQl4vR2\n\n6/6 https://t.co/yIjriCsa1r']",https://arxiv.org/abs/2201.04234,"Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (Wilds, ImageNet, Breeds, CIFAR, and MNIST). In our experiments, ATC estimates target performance $2$-$4\times$ more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works. ",Leveraging Unlabeled Data to Predict Out-of-Distribution Performance
157,1481760317784748032,1208260954762268678,William Misener,"[""New paper alert! 🚨 Really excited to share this submitted work on sub-Neptune interiors, read it here: <LINK>. We find that current atmospheric mass estimates for these planets could be wrong! 😱 How? It's all about the interior-atmosphere interactions... 1/12 <LINK>"", 'A lot of models of these sorts of planets assume an interior structure with discrete layers, like a silicate core/mantle with a hydrogen atmosphere on top, with little interaction between them. 2/12 https://t.co/syTvd1ewC7', 'But this layered picture might not be quite right: the base of a sub-Neptune atmosphere, especially a young one, is likely really hot (&gt;5000 K). At these temperatures, you might not get an abrupt transition from a pure silicate magma ocean to a pure H/He atmosphere. 3/12 https://t.co/nsSxJouscX', ""Instead, recent work has found that at the conditions at the base of a young sub-Neptune's atmosphere, significant SiO is stable in the vapor phase in the atmosphere at chemical equilibrium (Schlichting+Young21). 4/12 https://t.co/nHQxKaXued"", 'So in this paper, we take a look at the implications of all this silicate vapor on atmospheric structure. We find that silicate vapor abundance falls off quickly as altitude increases. Due to this compositional gradient, convection is actually inhibited near the base! 5/12', 'Why? Normal convection happens because hot gas is less dense than cool gas and rises. But if the hot gas also holds enough heavy vapor, its larger weight can make it more dense than the cooler gas above it, so it tends to sink back down if lifted. 6/12 https://t.co/sI5yWZZ2FI', ""Here's a diagram of how these atmospheres are structured if you account for this: right above the silicate core/mantle, we find a radiative region. Once there's too little silicate vapor to weigh the hot hydrogen gas down, the atmosphere transitions to a convective region. 7/12 https://t.co/5uo6WF9aS3"", ""That diagram isn't to scale: we typically find that the radiative region is very narrow in radius. This leads to a step-like temperature profile, where high base temperatures plunge before leveling off to follow an adiabatic profile (cf. a fully adiabatic profile in black). 8/12 https://t.co/SjZBRhTnet"", ""As you can see from that figure, this radiative region has a big effect on the planet's radius (the dots in the figure)! Specifically, it makes planets a lot smaller than if they had fully convective atmospheres and the same atmospheric mass and base temperature. 9/12"", ""In other words, the atmospheric mass you'd infer from a planet's radius if you include silicates is much (~5x) more than what you'd get if you assumed its atmosphere was fully convective. The effect is strongest for young planets. We ran time evolution sims to show this. 10/12 https://t.co/0zyqiC2amf"", 'The differences expected depend on other variables too, like planet mass, equilibrium temperatures, opacities, and initial conditions, that you can check out in the paper. This work has been submitted but not yet reviewed, so any comments are welcome! Thanks for reading! 11/12 https://t.co/3eP170Nd2g', ""And, as a shameless self-plug, if this work sounds so cool that you want all your friends/foes to hear about it, I'm very open to talk invitations! 😊😬😂 12/12""]",https://arxiv.org/abs/2201.04299,"Substantial silicate vapor is expected to be in chemical equilibrium at temperature conditions typical of the silicate-atmosphere interface of sub-Neptune planets, which can exceed 5000 K. Previous models of the atmospheric structure and evolution of these exoplanets, which have been used to constrain their atmospheric mass fractions, have neglected this compositional coupling. In this work, we show that silicate vapor in a hydrogen-dominated atmosphere acts as a condensable species, decreasing in abundance with altitude. The resultant mean molecular weight gradient inhibits convection at temperatures above $\sim 4000$ K, inducing a near-surface radiative layer. This radiative layer decreases the planet's total radius compared to a planet with the same base temperature and a convective, pure H/He atmosphere. Therefore, we expect silicate vapor to have major effects on the inferred envelope mass fraction and thermal evolution of sub-Neptune planets. We demonstrate that differences in radii, and hence in inferred atmospheric masses, are largest for planets which have larger masses, equilibrium temperatures, and atmospheric mass fractions. The effects are largest for younger planets, but differences can persist on gigayear time-scales for some sub-Neptunes. For a $10 M_\oplus$ planet with $T_\mathrm{eq}=1000$ K and an age of $\sim 300$ Myr, an observed radius consistent with an atmospheric mass fraction of 10% when accounting for silicate vapor would be misinterpreted as indicating an atmospheric mass fraction of 2% if a H/He-only atmosphere were assumed. The presence of silicate vapor in the atmosphere is also expected to have important implications for the accretion and loss of primordial hydrogen atmospheres. ","The importance of silicate vapor in determining the structure, radii,
  and envelope mass fractions of sub-Neptunes"
158,1481656937435447300,1411308931,Szymon Talaga,"['New thing on @arxiv - ""Measuring similarity- and complementarity-driven relations in networks"".\n\nWe propose two families of coefficients measuring the extent to which relations are driven by similarity or complementarity (difference+synergy).\n\n<LINK>\n\n🧵 [1/18]', 'They are defined in terms of counts of different types of paths and cycles (i.e. combinatorically), so conceptually they are akin to the clustering coefficient(s) we all know and love! \n\nHowever, they are motivated by rather general geometric arguments. [2/18]', 'It is well-known that similarity-driven relations are linked to the overrepresentation of triangles as their characteristic motif.\n\nThis is partially captured by the local clustering coef and also recently proposed closure coef (check the paper!) [3/18]\n\nhttps://t.co/F63mEgcSo6', 'However, both of them are limited to only one perspective. The former is based on wedge triples and checks whether ""my friends are friends with each other"" and the latter on head triples and checks whether ""friends of my friends are my friends"".\n\nSo why not combine them? [4/18] https://t.co/A4X28yhw3v', 'So, we define a structural similarity coefficient, which can be seen as a weighted average of the clustering and closure coefficients. [5/18] https://t.co/DV7TLSmsRk', 'Remarkably, calling it ""structural"" is really justified in this case, as we prove it is linked to structural equivalence between the ego and its neighbors.\n\nIt can be also defined globally (in this case it is equal to the global clustering) and for individual edges!  [6/18]', 'But what about complementarity! Starting from a simple geometric model we argue that quadrangles (4-cycles) are its characteristic motif.\n\nWe then use the same logic as before to define appropriate clustering, closure and structural coefficients. [7/18] https://t.co/qiiCerHxrE', 'Why complementarity matters? Because quadrangles (with no chordal/diagonal edges) are linked to the presence of dense bipartite-like subgraphs!\n\nAnd again, we show that our notion of structural complementarity is related to particular patterns of structural equivalence. [8/18]', 'But what if quadrangles have diagonal edges? This leads to the notion of weak complementarity which allows for the simultaneous presence of similarity-driven relations and therefore relaxes the requirement of local bipartiteness. [9/18] https://t.co/lZNDngDdbi', 'We also redefine all the measures for weighted relations by extending the approach to defining weighted clustering proposed by Barrat et al. (https://t.co/6fLlEFKnek).\n\nThe idea is simple, we weight everything by the average edge weight of the underlying triple/quadruple. [10/18] https://t.co/KTAEU2pv7I', 'Ok, but does all of this makes sense for real-world networks? Well, yes it does!\n\nWe show that our coefs distinguish between friendship (similarity) and health advice (complementarity) networks. Especially, when coefs are calibrated against a proper configuration model. [11/18] https://t.co/Pg3shBTjUv', 'Next, we demostrate that networks from different domains are driven by similarity and complementarity to different extents and that the structural coefficients we propose can be used to distinguish between them. [12/18] https://t.co/yFKzs36J8J', 'Finally, we analyze a network of co-appereances in ""A Storm of Swords"" (third ""Game of Thrones"" book) and show that the edge-wise coefficients can be used to detect groups of structurally similar/complementary nodes. [13/18] https://t.co/VBxAZ4RufG', 'The network is weighted and indeed the weighted coefficients perform better. They produce results qualitatively similar to an SBM partition obtained with @graphtool.\n\nAnd remarkably, the results correspond to three main plotlines and geographical regions in the book! [14/18]', 'Last but not least, we implemented all methods as a Python package (it will arrive at PyPI upon publication). \n\nAnd Crucially, it is quite fast, as it uses @numba_jit to JIT-compile main routines to highly optimized C code. [15/18]', 'Finally, some acknowledgments. A significant part of this work was inspired by my conversation with @jkbren and Ivan Voitalov few years ago in Boston. In particular, it was then when I first heard about the notion of ""complementarity-driven relations"". [16/18]', 'I also would like to thank @tiagopeixoto for hosting the @netzschleuder repository. We used it to get all the data used in the project. It is really one of those things that make the life of network scholars so much easier! Many kudos for that! [17/18]', 'Finally, I would like to recommed everyone this paper: https://t.co/IgqsFDc6F8\n\nIt introduces a general framework for fitting soft configuration models. And all the methods are implemented in an excellent (Python) package called NEMtropy. We used it a lot and it is great! [18/18]']",https://arxiv.org/abs/2201.03664,"The principle of similarity, or homophily, is often used to explain prevalent patterns observed in complex networks such as transitivity and abundance of triangles (3-cycles). On the other hand, many phenomena, from division of labor to protein-protein interactions (PPI), are driven by complementarity, or differences and synergy. However, the principle of complementarity has not been operationalized in network terms. Here we show that complementarity is linked to the abundance of quadrangles (4-cycles) and the presence of dense bipartite-like subgraphs. Starting from simple geometric arguments we link similarity and complementarity to their characteristic network motifs and introduce two families of coefficients: (1) structural similarity generalizing the notions of local clustering and closure and capturing the full spectrum of similarity-driven structures; (2) analogous complementarity coefficients based on quadrangles instead of triangles. We demonstrate on a variety of social and biological networks that the coefficients capture important structural properties which can be related to meaningful domain-specific phenomena. We show that they can be used to distinguish between different kinds of social relations and measure an increasing structural diversity of PPI networks across the tree of life. Our results suggest that for some types of relations assuming homophily may not be adequate. Moreover, they can be used to inform link prediction methods and decide when assuming triadic or tetradic closure is more appropriate. More generally, we provide a novel set of tools for studying the structure of networks in terms of complementarity between their elements. We also provide an efficient Python implementation of the proposed methods. ","Structural complementarity and similarity: linking relational principles
  to network motifs"
159,1481598181087588352,549128696,Vicky Fawcett,['Paper day! 🎉 \nUsing X-shooter spectra we find that red quasars are red due to dust (see gif). We also find no differences in the accretion properties between red and blue quasars and detect a similar prevalence of outflows.\n<LINK> <LINK>'],https://arxiv.org/abs/2201.04139,"We have recently found fundamental differences in the radio properties of red quasars when compared to typical blue quasars. In this paper we use X-shooter data, providing spectral coverage from $\sim 3000-25000$ Ang, of a sample of 40 red and blue luminous quasars at $1.45<z<1.65$ to explore the connections between the radio, emission-line, and accretion-disc properties. We fit various dust-extinction curves to the data and find that dust reddening can fully explain the observed colours for the majority of the red quasars in our sample, with moderate extinctions ranging from Av$\sim 0.06-0.7$ mags. We confront our spectra with a simple thin accretion-disc model and find this can describe the continua of both the blue and red quasars, once corrected for dust extinction; we also find no significant differences in the accretion properties. We detect ionized outflows in a number of red and blue quasars, but do not find any significant evidence that they are more prevalent in the red quasar population. Overall our findings imply that the radio emission is more closely connected to circumnuclear/ISM opacity rather than accretion disc or outflow differences. ","Fundamental differences in the properties of red and blue quasars:
  measuring the reddening and accretion properties with X-shooter"
160,1481370712795496452,256513537,Dr Chiara Mingarelli,"[""The International Pulsar Timing Array has just published our first gravitational-wave search on DR2! While DR2 contains @NANOGrav's 9-yr data, combined with older EPTA and PPTA data, we still find a correlated red noise signal (like NG's 12.5yr result!) <LINK> <LINK>"", ""Using @jacaseyclyde's latest results, we can also interpret this signal in terms of the minimum black hole mass and number density of black holes contributing to a potential gravitational-wave background signal,  as well as the volume of the background. https://t.co/Z8yiIqmEgX"", 'What about more recent data? Never fear, we have also compared this IPTA result to the latest PTA data, and it looks great!\n\nThe next big step is combining some new datasets for an IPTA DR3. This huge effort is being led by Deborah Good, a @FlatironCCA  and @UConn postdoc. https://t.co/oqZeVA5xDb']",https://arxiv.org/abs/2201.03980,"We searched for an isotropic stochastic gravitational wave background in the second data release of the International Pulsar Timing Array, a global collaboration synthesizing decadal-length pulsar-timing campaigns in North America, Europe, and Australia. In our reference search for a power law strain spectrum of the form $h_c = A(f/1\,\mathrm{yr}^{-1})^{\alpha}$, we found strong evidence for a spectrally-similar low-frequency stochastic process of amplitude $A = 3.8^{+6.3}_{-2.5}\times10^{-15}$ and spectral index $\alpha = -0.5 \pm 0.5$, where the uncertainties represent 95\% credible regions, using information from the auto- and cross-correlation terms between the pulsars in the array. For a spectral index of $\alpha = -2/3$, as expected from a population of inspiralling supermassive black hole binaries, the recovered amplitude is $A = 2.8^{+1.2}_{-0.8}\times10^{-15}$. Nonetheless, no significant evidence of the Hellings-Downs correlations that would indicate a gravitational-wave origin was found. We also analyzed the constituent data from the individual pulsar timing arrays in a consistent way, and clearly demonstrate that the combined international data set is more sensitive. Furthermore, we demonstrate that this combined data set produces comparable constraints to recent single-array data sets which have more data than the constituent parts of the combination. Future international data releases will deliver increased sensitivity to gravitational wave radiation, and significantly increase the detection probability. ","The International Pulsar Timing Array second data release: Search for an
  isotropic Gravitational Wave Background"
161,1481193458161356800,1109044782809206784,Ginette Lafit 💚,"['In this new preprint, we (@fjnogales, @ivanmarce1, Ruben Zamar, and me) propose the use of a robust covariance estimator based on multivariate Winsorization for sparse estimation of the precision matrix of a Gaussian graphical model\n<LINK>\n1/5', 'We investigate the performance of Glasso under cellwise contamination (right panel) which differs from the classical casewise contamination model (left panel) because each cell of a data has a probability to be independently contaminated\n2/5 https://t.co/MxWNZN9B6k', 'In the context of cellwise outliers, traditional robust estimators of the covariance matrix are not well suited because they rely on linear combinations of the observations which have a high probability of being contaminated. This is known as outliers propagation.\n3/5', 'In the presence of cellwise outliers, the Graphical lasso is no longer robust. As a result, we cannot use the precision matrix estimated by Glasso to learn about conditional independence in a Gaussian graphical model.\n4/5', 'Thus we propose a robust estimator of Glasso by plugging in a robust estimator of the covariance matrix based on bivariate winsorization. The proposal has a competitive behavior, regarding the recovery of the graph in comparison with existing approaches.\n5/5']",https://arxiv.org/abs/2201.03659,"We propose the use of a robust covariance estimator based on multivariate Winsorization in the context of the Tarr-Muller-Weber framework for sparse estimation of the precision matrix of a Gaussian graphical model. Likewise Croux-Ollerer's precision matrix estimator, our proposed estimator attains the maximum finite sample breakdown point of 0.5 under cellwise contamination. We conduct an extensive Monte Carlo simulation study to assess the performance of ours and the currently existing proposals. We find that ours has a competitive behavior, regarding the the estimation of the precision matrix and the recovery of the graph. We demonstrate the usefulness of the proposed methodology in a real application to breast cancer data. ",Robust graphical lasso based on multivariate Winsorization
162,1481167388838174721,776765039726460929,Carlo Felice Manara,"['Also new paper by Testi &amp; Natta (and me, @giulod @ilaria_pascucci @jpw_hawaii @astrodesim ) is out! <LINK>\nWe reassessed all disk populations in Ophiucus (L1688) and other nearby star-forming regions using the recent @ESAGaia work to study disk evolution. <LINK>']",https://arxiv.org/abs/2201.04079,"(Abridged) We present a study of the disk population in L1688, the densest and youngest region in Ophiuchus, and we compare it with other nearby regions of different age, namely Lupus, Chamaeleon I, Corona Australis, Taurus and Upper Scorpius. We select our L1688 sample using a combination of criteria (ALMA data, Gaia, optical/near-IR spectroscopy) and determine stellar and disk properties, specifically stellar mass (Mstar), average population age, mass accretion rate (Macc) and disk dust mass (Mdust). a) In L1688 the relations between Macc and Mstar, Mdust and Mstar, and Macc and Mdust have a roughly linear trend with slopes 1.8-1.9 for the first two relations and ~1 for the third, similarly to what found in the other regions. b) When ordered according to the characteristic age of each region, Macc decreases as 1/t, when corrected for the different stellar mass content; Mdust follows roughly the same trend between 0.5 and 5 Myr, but has an increase of a factor ~3 at ages of 2-3 Myr. We suggest that this could result from an earlier planet formation, followed by collisional fragmentation that temporarily replenishes the millimeter-size grain population. c) The dispersion of Macc and Mdust around the best-fitting relation with Mstar, as well as that of Macc versus Mdust are large: we find that the dispersions have continuous distributions with a log-normal shape and similar width (~0.8 dex). The amount of dust observed at ~1 Myr does not appear to be sufficient to assemble the majority of planetary systems, which suggests an earlier planetary cores formation. The dust mass traces to a large extent the disk gas mass evolution. Two properties remain puzzling: the steep dependence of Macc and Mdust on Mstar and the cause of the large dispersion in the three relations analyzed in this paper, in particular the one of the Macc versus Mdust relation. ","The protoplanetary disk population in the rho-Ophiuchi region L1688 and
  the time evolution of Class II YSOs"
163,1481165253207633921,776765039726460929,Carlo Felice Manara,"['Paper by Alessia Rota based on the Thesis done @LaStatale @ESO is out: <LINK>\nWe studied the size of protoplanetary disks in multiple stellar systems using the CO gas emission. We find larger dust-to-gas size ratios as in singles, and typical eccentricities. <LINK>', 'This was a follow-up of paper I https://t.co/0tNI4SDkf3 and of the @almaobs  survey of Taurus disks led by @GregHerczeg Feng Long (https://t.co/T8umzYpSeR - https://t.co/9Wn6uCOMbj) and including  colleagues from @DustbustersA and not (@astroannamio @giulod @StefanoFacchi14)']",https://arxiv.org/abs/2201.03588,"The formation of multiple stellar systems is a natural by-product of the star-formation process, and its impact on the properties of protoplanetary discs and on the formation of planets is still to be fully understood. To date, no detailed uniform study of the gas emission from a sample of protoplanetary discs around multiple stellar systems has been performed. Here we analyse new ALMA observations at a $\sim$21 au resolution of the molecular CO gas emission targeting discs in eight multiple stellar systems in the Taurus star-forming regions. $^{12}$CO gas emission is detected around all primaries and in seven companions. With these data, we estimate the inclination and the position angle for all primary discs and for five secondary or tertiary discs, and measure the gas disc radii of these objects with a cumulative flux technique on the spatially resolved zeroth moment images. When considering the radius including 95\% of the flux as a metric, the estimated gas disc size in multiple stellar systems is found to be on average $\sim 4.2$ times larger than the dust disc size. This ratio is higher than what was recently found in a population of more isolated and single systems. On the contrary, when considering the radius including 68\% of the flux, no difference between multiple and single discs is found in the distribution of ratios. This discrepancy is due to the sharp truncation of the outer dusty disc observed in multiple stellar systems. The measured gas disc sizes are consistent with tidal truncation models in multiple stellar systems assuming eccentricities of $\sim0.15$-$0.5$, as expected in typical binary systems. ","Observational constraints on disc sizes in protoplanetary discs in
  multiple systems in the Taurus region. II. Gas disc sizes"
164,1481128367906701312,220592189,Guanyu Zhu,"['Excited to share the recent work with Arpit and Tomas <LINK>. We study the error correction and decoding properties for the Fractal Topological Code in 2+epsilon dimension discovered in our previous work which is able to perform non-Cliffor logical CCZ gate.', 'We developed a cellular automaton decoder based on the sweep decoder to correct the string excitations which has self-correcting and single-shot properties. The corresponding threshold remains similar to the value in 3D surface code.', 'We also used the matching decoder to correct the point excitation, where the threshold can be mapped to the critical point of a confinement-deconfinement phase transition on a fractal, tunable via the fractal dimension.', 'The matching threshold is improved compared to the 3D surface code and approaches the threshold of the 2D surface code in the limit epsilon -&gt; 0.', '@quantum_jake Thanks Jake!  Yeah, this code actually works :)']",https://arxiv.org/abs/2201.03568,"Recently, a class of fractal surface codes (FSCs), has been constructed on fractal lattices with Hausdorff dimension $2+\epsilon$, which admits a fault-tolerant non-Clifford CCZ gate. We investigate the performance of such FSCs as fault-tolerant quantum memories. We prove that there exist decoding strategies with non-zero thresholds for bit-flip and phase-flip errors in the FSCs with Hausdorff dimension $2+\epsilon$. For the bit-flip errors, we adapt the sweep decoder, developed for string-like syndromes in the regular 3D surface code, to the FSCs by designing suitable modifications on the boundaries of the holes in the fractal lattice. Moreover, our adaptation of the sweep decoder for the FSCs maintains its self-correcting and single-shot nature. For the phase-flip errors, we employ the minimum-weight-perfect-matching (MWPM) decoder for the point-like syndromes. We report a sustainable fault-tolerant threshold ($\sim 1.7\%$) under phenomenological noise for the sweep decoder and the code capacity threshold (lower bounded by $2.95\%$) for the MWPM decoder for a particular FSC with Hausdorff dimension $D_H\approx2.966$. The latter can be mapped to the critical point of a zero-temperature confinement-Higgs transition on the fractal lattice, which is tunable via the Hausdorff dimension. ",Quantum error correction with fractal topological codes
165,1481069740420628480,747383862154559488,Kuniyuki Takahashi,"['Paper accepted for RA-L is now available!\nWe propose a method for grasping any specified amount of food that tends to tangle, such as shredded cabbage.\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2201.00933,"Food packing industries typically use seasonal ingredients with immense variety that factory workers manually pack. For small pieces of food picked by volume or weight that tend to get entangled, stick or clump together, it is difficult to predict how intertwined they are from a visual examination, making it a challenge to grasp the requisite target mass accurately. Workers rely on a combination of weighing scales and a sequence of complex maneuvers to separate out the food and reach the target mass. This makes automation of the process a non-trivial affair. In this study, we propose methods that combines 1) pre-grasping to reduce the degree of the entanglement, 2) post-grasping to adjust the grasped mass using a novel gripper mechanism to carefully discard excess food when the grasped amount is larger than the target mass, and 3) selecting the grasping point to grasp an amount likely to be reasonably higher than target grasping mass with confidence. We evaluate the methods on a variety of foods that entangle, stick and clump, each of which has a different size, shape, and material properties such as volumetric mass density. We show significant improvement in grasp accuracy of user-specified target masses using our proposed methods. ","Target-mass Grasping of Entangled Food using Pre-grasping &
  Post-grasping"
166,1480847770315591680,1389661062312742919,Tarik P. Cysne,['Very happy with the result of this collaboration. We studied the orbital Hall effect in TMDs using two distinct approaches. Check our preprint! \uf04a #orbitronics #orbital #TMDs\n<LINK>'],https://arxiv.org/abs/2201.03491,"Using an effective Dirac model, we study the orbital Hall effect (OHE) in bilayers of transition metal dichalcogenides with 2H stacking (2H-TMD). We use first-order perturbation theory in the interlayer coupling of the bilayer system to obtain analytical expressions for the orbital Hall conductivity in the linear response regime. We use two distinct descriptions of the orbital angular momentum (OAM) operator: The first one is the intra-atomic approximation that considers only the intrasite contribution to the OAM [Cysne et al. Phys. Rev. Lett. 126, 056601 (2021)]. The second one uses the Berry-phase formula of the orbital (valley) magnetic moment to describe the OAM operator [Bhowal and Vignale, Phys. Rev. B 103, 195309 (2021)]. This approach includes both intersite and intrasite contributions to the OAM. Our results suggest that the two approaches agree qualitatively in describing the OHE in bilayers of 2H-TMDs, although they present some quantitative differences. We also show that interlayer coupling plays an essential role in understanding the OHE in the unbiased bilayer of 2H-TMD. This coupling causes the Bloch states to become combinations of states of individual layers, demanding the consideration of the non-Abelian structure of the orbital magnetic moment. As we discuss throughout the work, the emerging picture of transport of OAM in the unbiased bilayer of 2H-TMDs based on OHE is very different from the usual picture based on the valley Hall effect, shedding new lights on previous experimental results. We also discuss the effect of the inclusion of a gate-voltage bias in the bilayer system. Our work gives support to recent theoretical predictions on OHE in two-dimensional materials. ","Orbital Hall effect in bilayer transition metal dichalcogenides: From
  the intra-atomic approximation to the orbital magnetic moment approach"
167,1480798108028223488,776765039726460929,Carlo Felice Manara,['New paper by PhD student @ESO and Konkoli Obs Gabriella Zsidi. <LINK>\nWe study the accretion variability of CR Cha on timescales from minutes to a decade. The peak is at weeks-month timescale. \n(expect more to come from Gabriella and from #PENELLOPELP) <LINK>'],https://arxiv.org/abs/2201.03396,"Classical T Tauri stars are surrounded by a circumstellar disk from which they are accreting material. This process is essential in the formation of Sun-like stars. Although often described with simple and static models, the accretion process is inherently time variable. Our aim is to examine the accretion process of the low-mass young stellar object CR Cha on a wide range of timescales from minutes to a decade by analyzing both photometric and spectroscopic observations from 2006, 2018, and 2019. We carried out period analysis on the light curves of CR Cha from the TESS mission and the ASAS-SN and the ASAS-3 databases. We studied the color variations of the system using $I,J,H,K$-band photometry obtained contemporaneously with the TESS observing window. We analyzed the amplitude, timescale, and the morphology of the accretion tracers found in a series of high-resolution spectra obtained in 2006 with the AAT/UCLES, in 2018 with the HARPS, and in 2019 with the ESPRESSO and the FEROS spectrographs. All photometric data reveal periodic variations compatible with a 2.327 days rotational period, which is stable in the system over decades. Moreover, the ASAS-SN and ASAS-3 data hint at a long-term brightening by 0.2 mag, between 2001 and 2008, and of slightly less than 0.1 mag in the 2015 - 2018 period. The near-infrared color variations can be explained by either changing accretion rate or changes in the inner disk structure. Our results show that the amplitude of the variations in the H$\alpha$ emission increases on timescales from hours to days/weeks, after which it stays similar even when looking at decadal timescales. On the other hand, we found significant morphological variations on yearly/decadal timescales, indicating that the different physical mechanisms responsible for the line profile changes, such as accretion or wind, are present to varying degrees at different times. ","Accretion variability from minutes to decade timescales in the classical
  T Tauri star CR Cha"
168,1480153924053180416,769860811,ezequiel smucler,"['Happy to share this new joint work with Andrea <LINK>. We study the selection of adjustment sets in graphical models where variables have costs, and propose an algorithm for computing an  adjustment set that is optimal (efficient) among those having minimum cost', '@pianophase https://t.co/Sq6R4oxrOb, haceme el fork a R']",http://arxiv.org/abs/2201.02037,"We study the selection of adjustment sets for estimating the interventional mean under an individualized treatment rule. We assume a non-parametric causal graphical model with, possibly, hidden variables and at least one adjustment set comprised of observable variables. Moreover, we assume that observable variables have positive costs associated with them. We define the cost of an observable adjustment set as the sum of the costs of the variables that comprise it. We show that in this setting there exist adjustment sets that are minimum cost optimal, in the sense that they yield non-parametric estimators of the interventional mean with the smallest asymptotic variance among those that control for observable adjustment sets that have minimum cost. Our results are based on the construction of a special flow network associated with the original causal graph. We show that a minimum cost optimal adjustment set can be found by computing a maximum flow on the network, and then finding the set of vertices that are reachable from the source by augmenting paths. The optimaladj Python package implements the algorithms introduced in this paper. ","A note on efficient minimum cost adjustment sets in causal graphical
  models"
169,1479034972057260032,1105880439506587648,sophiaalthammer,"['Handling long queries &amp; documents in your corpus🔍📄?  In our #ecir2022 paper we propose PARM for dense doc-to-doc retrieval, which liberates dense models from their limited input length! @s_hofstaetter @suzan @allanhanbury @DossierProject \n<LINK> <LINK>', 'PARM is a Paragraph Aggregation Retrieval Model and retrieves documents on the paragraph-level. For the aggregation in PARM we propose Vector-based Reciprocal Rank Fusion aggregation (VRRF) which combines the advantages of occurrence-based aggregation and the topical embeddings! https://t.co/K3Lv1Dr1Il', 'We focus on the document-to-document retrieval task of legal case retrieval and conduct experiments on the COLIEE and CaseLaw collection from @IELabGroup and show improved recall effectiveness for first stage retrieval with PARM!', 'To get started, check our released code and pretrained models on our Github page https://t.co/EJnz5eZNGT']",https://arxiv.org/abs/2201.01614,"Dense passage retrieval (DPR) models show great effectiveness gains in first stage retrieval for the web domain. However in the web domain we are in a setting with large amounts of training data and a query-to-passage or a query-to-document retrieval task. We investigate in this paper dense document-to-document retrieval with limited labelled target data for training, in particular legal case retrieval. In order to use DPR models for document-to-document retrieval, we propose a Paragraph Aggregation Retrieval Model (PARM) which liberates DPR models from their limited input length. PARM retrieves documents on the paragraph-level: for each query paragraph, relevant documents are retrieved based on their paragraphs. Then the relevant results per query paragraph are aggregated into one ranked list for the whole query document. For the aggregation we propose vector-based aggregation with reciprocal rank fusion (VRRF) weighting, which combines the advantages of rank-based aggregation and topical aggregation based on the dense embeddings. Experimental results show that VRRF outperforms rank-based aggregation strategies for dense document-to-document retrieval with PARM. We compare PARM to document-level retrieval and demonstrate higher retrieval effectiveness of PARM for lexical and dense first-stage retrieval on two different legal case retrieval collections. We investigate how to train the dense retrieval model for PARM on limited target data with labels on the paragraph or the document-level. In addition, we analyze the differences of the retrieved results of lexical and dense retrieval with PARM. ","PARM: A Paragraph Aggregation Retrieval Model for Dense
  Document-to-Document Retrieval"
170,1478929106779660291,897423883313307648,Dominik Kempa,"['I am very excited to post that together with Tomasz Kociumaka we propose the first dynamic suffix array with O(polylog(n))-time queries and updates! This is the first solution (30+ years after the discovery of suffix array) with all polylog(n) operations!\n<LINK>', 'Dynamic SA is one of the most hopelessly elusive problems in string algorithms. No solution with O(polylog(n)) queries and updates (even amortized or expected) was known before, not even for character substitutions only. Our solution is deterministic and all times are worst-case.']",https://arxiv.org/abs/2201.01285,"The suffix array $SA[1..n]$ of a text $T$ of length $n$ is a permutation of $\{1,\ldots,n\}$ describing the lexicographical ordering of suffixes of $T$, and it is considered to be among of the most important data structures in string algorithms, with dozens of applications in data compression, bioinformatics, and information retrieval. One of the biggest drawbacks of the suffix array is that it is very difficult to maintain under text updates: even a single character substitution can completely change the contents of the suffix array. Thus, the suffix array of a dynamic text is modelled using suffix array queries, which return the value $SA[i]$ given any $i\in[1..n]$. Prior to this work, the fastest dynamic suffix array implementations were by Amir and Boneh. At ISAAC 2020, they showed how to answer suffix array queries in $\tilde{O}(k)$ time, where $k\in[1..n]$ is a trade-off parameter, with $\tilde{O}(\frac{n}{k})$-time text updates. In a very recent preprint [2021], they also provided a solution with $O(\log^5 n)$-time queries and $\tilde{O}(n^{2/3})$-time updates. We propose the first data structure that supports both suffix array queries and text updates in $O({\rm polylog}\,n)$ time (achieving $O(\log^4 n)$ and $O(\log^{3+o(1)} n)$ time, respectively). Our data structure is deterministic and the running times for all operations are worst-case. In addition to the standard single-character edits (character insertions, deletions, and substitutions), we support (also in $O(\log^{3+o(1)} n)$ time) the ""cut-paste"" operation that moves any (arbitrarily long) substring of $T$ to any place in $T$. We complement our structure by a hardness result: unless the Online Matrix-Vector Multiplication (OMv) Conjecture fails, no data structure with $O({\rm polylog}\,n)$-time suffix array queries can support the ""copy-paste"" operation in $O(n^{1-\epsilon})$ time for any $\epsilon>0$. ",Dynamic Suffix Array with Polylogarithmic Queries and Updates
171,1478653041792401408,2656302854,Ronald Drimmel 🇺🇦,"['Paper day! Using &gt;400 #MilkyWay classical #Cepheids in #GaiaEDR3, we find a modest but significant metallicity term, giving us a Period-Luminosity-Metallicity relation. Leavitt law goes 3D!\nAccepted for publication and now on the arXiv: <LINK> <LINK>', 'Validating with LMC Cepheids we get good agreement with the geometric eclipsing binary data. (bottom left panel) https://t.co/jfnCjzPL7j', 'To be noted that to do this calibration we avoided using Cepheids brighter than G mag = 6 which, as has been recently noted, leads to problematic results: \nhttps://t.co/lHvHfu6Zxg', 'Cepheids are well-known standard candles on the cosmic distance ladder, but I confess my interest in them is closer to home.  These are relatively young stars, so potentially useful for mapping the #MilkyWay, as noted in an earlier paper last year (Poggio et al 2021). https://t.co/K8OUnjTZTb', 'That was using almost 2000 young classical Cepheids, but soon, with #GaiaDR3, we will have even more! https://t.co/7WdunI2Wvp']",https://arxiv.org/abs/2201.01126,"Classical Cepheids (DCEPs) represent a fundamental tool to calibrate the extragalactic distance scale. However, they are also powerful stellar population tracers in the context of Galactic studies. The forthcoming Data Release 3 (DR3) of the Gaia mission will allow us to study, with unprecedented detail, the structure, the dynamics, and the chemical properties of the Galactic disc, and in particular of the spiral arms, where most Galactic DCEPs reside. In this paper, we aim to quantify the metallicity dependence of the Galactic DCEPs' period-Wesenheit ($PWZ$) relation in the Gaia bands. We adopted a sample of 499 DCEPs with metal abundances from high-resolution spectroscopy, in conjunction with Gaia Early Data Release 3 parallaxes and photometry to calibrate a $PWZ$ relation in the Gaia bands. We find a significant metallicity term, of the order of $-$0.5 mag/dex, which is larger than the values measured in the near-infrared (NIR) bands by different authors. Our best $PWZ$ relation is $W=(-5.988\pm0.018)-(3.176\pm0.044)(\log P-1.0)-(0.520\pm0.090){\rm [Fe/H]}$. We validated our $PWZ$ relations by using the distance to the Large Magellanic Cloud as a benchmark, finding very good agreement with the geometric distance provided by eclipsing binaries. As an additional test, we evaluated the metallicity gradient of the young Galactic disc, finding $-0.0527 \pm 0.0022$ dex/kpc, which is in very good agreement with previous results. ","Classical Cepheid period-Wesenheit-metallicity relation in the Gaia
  bands"
172,1478630290084012037,1071069610139639808,Michal H. Kolář,"['The first bachelor thesis in our group has turned to a #preprint. On a set of halogenated molecules, we have studied how an off-center partial charge affects the rest of the molecule. \n\ntl;dr Up to 3 covalent bonds from the halogen. \n\n<LINK> <LINK>']",https://arxiv.org/abs/2201.00890,"Partial atomic charges belong to key concepts of computational chemistry. In some cases, however, they fail in describing the electrostatics of molecules. One such example is the $\sigma$-hole, a region of positive electrostatic potential located on halogens and other atoms. In molecular mechanics, the $\sigma$-hole is often modeled as a pseudo-atom with a positive partial charge located off the halogen nucleus. Here we address a question, to what extent the pseudo-atom affects partial charges of other atoms in the molecule. To this aim, we have thoroughly analyzed partial charges of over 2300 halogenated molecules from the ZINC database calculated by the Restricted Electrostatic Potential (RESP) method and compared them with the charges fitted by RESP including the pseudo-atom. We show that the pseudo-atom improves charge fitting for a vast majority of molecules. The $\sigma$-hole, modeled as the off-center charge, affects the atoms within three covalent bonds from the halogen. ","The Effect of Off-Center $\sigma$-Hole on the Atom-Centered Partial
  Charges in Halogenated Molecules"
173,1488434222775902210,1364749022,Haitham Bou Ammar,"['Designing fast antibodies is critical in successfully countering diseases. In this paper, we collaborate with @victorgreiff team and with @DanyBouAmmar from @AUBMC_Official to propose an effective Bayesian optimisation solver: <LINK> <LINK>', 'We show: \n1. A new way to search for antibody CDRH3s using Bayesian optimisation with various kernels, including Protein Bert warps.', '2. We devise a constraint acquisition maximiser that finds new CDRH3 within a trust region while being feasible w.r.t antibody development constraints.', '3. We show high-affinity and super+ high-affinity binders in only 38 and 85 trials, respectively.', '@ImanisMind @KhanAsif__']",https://arxiv.org/abs/2201.12570,"Antibodies are canonically Y-shaped multimeric proteins capable of highly specific molecular recognition. The CDRH3 region located at the tip of variable chains of an antibody dominates antigen-binding specificity. Therefore, it is a priority to design optimal antigen-specific CDRH3 regions to develop therapeutic antibodies to combat harmful pathogens. However, the combinatorial nature of CDRH3 sequence space makes it impossible to search for an optimal binding sequence exhaustively and efficiently, especially not experimentally. Here, we present AntBO: a Combinatorial Bayesian Optimisation framework enabling efficient in silico design of the CDRH3 region. Ideally, antibodies should bind to their target antigen and be free from any harmful outcomes. Therefore, we introduce the CDRH3 trust region that restricts the search to sequences with feasible developability scores. To benchmark AntBO, we use the Absolut! software suite as a black-box oracle because it can score the target specificity and affinity of designed antibodies in silico in an unconstrained fashion. The results across 188 antigens demonstrate the benefit of AntBO in designing CDRH3 regions with diverse biophysical properties. In under 200 protein designs, AntBO can suggest antibody sequences that outperform the best binding sequence drawn from 6.9 million experimentally obtained CDRH3s and a commonly used genetic algorithm baseline. Additionally, AntBO finds very-high affinity CDRH3 sequences in only 38 protein designs whilst requiring no domain knowledge. We conclude AntBO brings automated antibody design methods closer to what is practically viable for in vitro experimentation. ","AntBO: Towards Real-World Automated Antibody Design with Combinatorial
  Bayesian Optimisation"
174,1486674972508663815,1364749022,Haitham Bou Ammar,['🚨Robotics and Planning  Ppl🚨 \n\nWe formalise constraint primitives via geometric backtracking. We propose an efficient BO algorithm based on constraint primitives. \nEven more! ☝️ We devise a transfer learning mechanism across tasks with zero effort.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2201.09612,"Searching for bindings of geometric parameters in task and motion planning (TAMP) is a finite-horizon stochastic planning problem with high-dimensional decision spaces. A robot manipulator can only move in a subspace of its whole range that is subjected to many geometric constraints. A TAMP solver usually takes many explorations before finding a feasible binding set for each task. It is favorable to learn those constraints once and then transfer them over different tasks within the same workspace. We address this problem by representing constraint knowledge with transferable primitives and using Bayesian optimization (BO) based on these primitives to guide binding search in further tasks. Via semantic and geometric backtracking in TAMP, we construct constraint primitives to encode the geometric constraints respectively in a reusable form. Then we devise a BO approach to efficiently utilize the accumulated constraints for guiding node expansion of an MCTS-based binding planner. We further compose a transfer mechanism to enable free knowledge flow between TAMP tasks. Results indicate that our approach reduces the expensive exploration calls in binding search by 43.60to 71.69 when compared to the baseline unguided planner. ",Learning Geometric Constraints in Task and Motion Planning
175,1486514165355143170,1429626075743674368,Daniel Beaglehole,"['1/n\nIn 1921, Borel formulated the now well-studied Colonel Blotto game. In 2016, a UMD team found the first polynomial time algorithm to compute its Nash equilibria (NE). In this work, we devise the first general approximation algorithm to compute its NE. \n<LINK>', '2/n\nThe algorithm simulates uncoupled rounds of multiplicative weights update (MWU) between the players. For n soldiers and k battlefields, the number of strategies is O(n^k), but we can perform MWU implicitly anyway!', '3/n\nThe central idea is to sample from the distribution produced by MWU without ever performing the updates explicitly. The Colonel Blotto game has unique structure that allows us to do this.', '4/n \nThis project would not have existed without Prof. Alex Andoni at Columbia, who suggested during our nearest neighbors search project that we might be able to implicitly sample from MWU for instance optimal hashing.', '5/n=5 \nI am also indebted to @MHop_Theory for his feedback on early drafts of this work (and his writing advice in general :)']",https://arxiv.org/abs/2201.10758,"In the storied Colonel Blotto game, two colonels allocate $a$ and $b$ troops, respectively, to $k$ distinct battlefields. A colonel wins a battle if they assign more troops to that particular battle, and each colonel seeks to maximize their total number of victories. Despite the problem's formulation in 1921, the first polynomial-time algorithm to compute Nash equilibrium (NE) strategies for this game was discovered only quite recently. In 2016, \citep{ahmadinejad_dehghani_hajiaghayi_lucier_mahini_seddighin_2019} formulated a breakthrough algorithm to compute NE strategies for the Colonel Blotto game\footnote{To the best of our knowledge, the algorithm from \citep{ahmadinejad_dehghani_hajiaghayi_lucier_mahini_seddighin_2019} has computational complexity $O(k^{14}\max\{a,b\}^{13})$}, receiving substantial media coverage (e.g. \citep{Insider}, \citep{NSF}, \citep{ScienceDaily}). In this work, we present the first known $\epsilon$-approximation algorithm to compute NE strategies in the two-player Colonel Blotto game in runtime $\widetilde{O}(\epsilon^{-4} k^8 \max\{a,b\}^2)$ for arbitrary settings of these parameters. Moreover, this algorithm computes approximate coarse correlated equilibrium strategies in the multiplayer (continuous and discrete) Colonel Blotto game (when there are $\ell > 2$ colonels) with runtime $\widetilde{O}(\ell \epsilon^{-4} k^8 n^2 + \ell^2 \epsilon^{-2} k^3 n (n+k))$, where $n$ is the maximum troop count. Before this work, no polynomial-time algorithm was known to compute exact or approximate equilibrium (in any sense) strategies for multiplayer Colonel Blotto with arbitrary parameters. Our algorithm computes these approximate equilibria by a novel (to the author's knowledge) sampling technique with which we implicitly perform multiplicative weights update over the exponentially many strategies available to each player. ",An Efficient Approximation Algorithm for the Colonel Blotto Game
176,1483053310190006276,877147424321613828,Mingtian,['Local autoregressive models can be very useful in some applications like lossless compression. We found there is also a parallelization scheme that allows efficient sampling and decoding in those models. See our recent work <LINK> for a discussion.'],https://arxiv.org/abs/2201.05213,"The recently proposed Neural Local Lossless Compression (NeLLoC), which is based on a local autoregressive model, has achieved state-of-the-art (SOTA) out-of-distribution (OOD) generalization performance in the image compression task. In addition to the encouragement of OOD generalization, the local model also allows parallel inference in the decoding stage. In this paper, we propose a parallelization scheme for local autoregressive models. We discuss the practicalities of implementing this scheme, and provide experimental evidence of significant gains in compression runtime compared to the previous, non-parallel implementation. ",Parallel Neural Local Lossless Compression
177,1481223812280729600,1318562163875786756,Ory Schnitzer,['New preprint with Richard Porter (@BristolUniMaths). We study the asymptotic nature of exponentially narrowband and strong resonances in a simple acoustics setup that supports so-called quasi-bound states in the continuum.\n\n<LINK>'],https://arxiv.org/abs/2201.03554,"Localised wave oscillations in an open system that do not decay or grow in time, despite their frequency lying within a continuous spectrum of radiation modes carrying energy to or from infinity, are known as bound states in the continuum (BIC). Small perturbations from the typically delicate conditions for BIC almost always result in the waves weakly coupling with the radiation modes, leading to leaky states called quasi-BIC that have a large quality factor. We study the asymptotic nature of this weak coupling in the case of acoustic waves interacting with a rigid substrate featuring a partially partitioned slit -- a setup that supports quasi-BIC that exponentially approach BIC as the slit is made increasingly narrow. In that limit, we use the method of matched asymptotic expansions in conjunction with reciprocal relations to study those quasi-BIC and their resonant excitation. In particular, we derive a leading approximation for the exponentially small imaginary part of each wavenumber eigenvalue (inversely proportional to quality factor), which is beyond all orders of the expansion for the wavenumber eigenvalue itself. Furthermore, we derive a leading approximation for the exponentially large amplitudes of the states in the case where they are resonantly excited by a plane wave at oblique incidence. These resonances occur in exponentially narrow wavenumber intervals and are physically manifested in cylindrical-dipolar waves emanating from the slit aperture and exponentially large field enhancements inside the slit. The asymptotic approximations are validated against numerical calculations. ","Acoustics of a partially partitioned narrow slit connected to a
  half-plane: case study for exponential quasi-bound states in the continuum
  and their resonant excitation"
178,1480507159196712963,3324833151,Teymoor Saifollahi,['When astronomers and computer scientists sit together -&gt; <LINK>\n\nThis work is a follow-up study of my previous work on Fornax UCD/GCs (<LINK>). Here we tackled some of the issues in dealing with astronomical data and how ML/AI detects UCD/GCs. <LINK>'],https://arxiv.org/abs/2201.01604,"Compact stellar systems such as Ultra-compact dwarfs (UCDs) and Globular Clusters (GCs) around galaxies are known to be the tracers of the merger events that have been forming these galaxies. Therefore, identifying such systems allows to study galaxies mass assembly, formation and evolution. However, in the lack of spectroscopic information detecting UCDs/GCs using imaging data is very uncertain. Here, we aim to train a machine learning model to separate these objects from the foreground stars and background galaxies using the multi-wavelength imaging data of the Fornax galaxy cluster in 6 filters, namely u, g, r, i, J and Ks. The classes of objects are highly imbalanced which is problematic for many automatic classification techniques. Hence, we employ Synthetic Minority Over-sampling to handle the imbalance of the training data. Then, we compare two classifiers, namely Localized Generalized Matrix Learning Vector Quantization (LGMLVQ) and Random Forest (RF). Both methods are able to identify UCDs/GCs with a precision and a recall of >93 percent and provide relevances that reflect the importance of each feature dimension %(colors and angular sizes) for the classification. Both methods detect angular sizes as important markers for this classification problem. While it is astronomical expectation that color indices of u-i and i-Ks are the most important colors, our analysis shows that colors such as g-r are more informative, potentially because of higher signal-to-noise ratio. Besides the excellent performance the LGMLVQ method allows further interpretability by providing the feature importance for each individual class, class-wise representative samples and the possibility for non-linear visualization of the data as demonstrated in this contribution. We conclude that employing machine learning techniques to identify UCDs/GCs can lead to promising results. ","Detection of extragalactic Ultra-Compact Dwarfs and Globular Clusters
  using Explainable AI techniques"
179,1478243699771412481,1411871581,Huaxiu Yao,"['Super-excited to share our new work about out-of-distribution robustness (<LINK>). \n\nWe propose a simple mixup-based method to learn invariant functions via selective augmentation.', 'Motivated by mixup, our method encourages learning invariant functions and cancel out domain-related information by\n\n(1) interpolating samples with the same label but different domains;\n\n(2) interpolating samples with the same domain but different labels. https://t.co/hJXGweTJwA', 'Our method is easy to implement and well-suited to domain shifts and subpopulation shifts.\n\nThe results are cool in nine benchmarks in domain shifts (left figure) and subpopulation shifts (right figure) https://t.co/0pLuhWzzmk', 'We also qualitatively show that our method leads to stronger invariant functions https://t.co/UtB7mg43vR', 'Under a linear setting, we finally provide a theoretical analysis of the phenomena distilled from the empirical study and show our method leads to smaller worst-domain error compared with ERM and vanilla mixup.', 'a wonderful collaboration w/ @__YuWang__, Sai Li, @zlj11112222, @liang_weixin, @james_y_zou, @chelseabfinn']",https://arxiv.org/abs/2201.00299,"Machine learning algorithms typically assume that training and test examples are drawn from the same distribution. However, distribution shift is a common problem in real-world applications and can cause models to perform dramatically worse at test time. In this paper, we specifically consider the problems of subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior works often seek to explicitly regularize internal representations or predictors of the model to be domain invariant, we instead aim to learn invariant predictors without restricting the model's internal representations or predictors. This leads to a simple mixup-based technique which learns invariant predictors via selective augmentation called LISA. LISA selectively interpolates samples either with the same labels but different domains or with the same domain but different labels. Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts, and we find that LISA consistently outperforms other state-of-the-art methods and leads to more invariant predictors. We further analyze a linear setting and theoretically show how LISA leads to a smaller worst-group error. ",Improving Out-of-Distribution Robustness via Selective Augmentation
