,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1480589754856992768,705950701986275328,Tzanio Kolev,"['üëá New paper from the MARBL team at @Livermore_Comp \n\nüî• ""Matrix-free approaches for GPU acceleration of a high-order finite element hydrodynamics application using MFEM, Umpire, and RAJA""\n\n‚úì arXiv: <LINK>\n\n#GPU #FEM #MultiPhysics <LINK>']",http://arxiv.org/abs/2112.07075,"With the introduction of advanced heterogeneous computing architectures based on GPU accelerators, large-scale production codes have had to rethink their numerical algorithms and incorporate new programming models and memory management strategies in order to run efficiently on the latest supercomputers. In this work we discuss our co-design strategy to address these challenges and achieve performance and portability with MARBL, a next-generation multi-physics code in development at Lawrence Livermore National Laboratory. We present a two-fold approach, wherein new hardware is used to motivate both new algorithms and new abstraction layers, resulting in a single source application code suitable for a variety of platforms. Focusing on MARBL's ALE hydrodynamics package, we demonstrate scalability on different platforms and highlight that many of our innovations have been contributed back to open-source software libraries, such as MFEM (finite element algorithms) and RAJA (kernel abstractions). ","Matrix-free approaches for GPU acceleration of a high-order finite
  element hydrodynamics application using MFEM, Umpire, and RAJA"
1,1478540490349985795,950844713657094145,Maxwell Ramstead,"[""I‚Äôm happy to share my commentary on the interesting and provocative target new paper in @bbsjournal on the free-energy principle and active inference, ‚ÄúThe emperor's new Markov blankets‚Äù by @JBruineberg, @krysdolega, @joe_dewhurst, and @manuelbaltieri\n<LINK> 1/4"", 'My commentary critically discusses two points. First, I question their distinction between two kinds of Markov blankets, ‚ÄúPearl blankets‚Äù and ‚ÄúFriston blankets.‚Äù I argue that these are not distinct objects, but the same object under a different mathematical interpretation 2/4', 'I then address their distinction between inference with a model and inference within a model in light of instrumentalist approaches to science. I argue that although it is used to address ontological questions, the FEP is a new chapter of physics or mechanics, not metaphysics 3/4', 'I‚Äôm grateful to the authors for providing us with a timely opportunity to discuss the formal constructs and philosophical implications of the FEP. I plan to expand the considerations in my commentary into stand-alone papers over the course of 2022. Congratulations! 4/4']",https://arxiv.org/abs/2112.15528,"In their target paper, Bruineberg and colleagues provide us with a timely opportunity to discuss the formal constructs and philosophical implications of the free-energy principle. I critically discuss their proposed distinction between Pearl blankets and Friston blankets. I then critically assess the distinction between inference with a model and inference within a model in light of instrumentalist approaches to science. ",The empire strikes back: Some responses to Bruineberg and colleagues
2,1478139585200902147,1337206242369036288,Emil Valdez,"['My new working paper on ""Compositional Data Regression in Insurance with Exponential Family PCA"" with my colleague, Prof. G. Gan. <LINK> @arxiv #compositionaldata #ePCA']",https://arxiv.org/abs/2112.14865,"Compositional data are multivariate observations that carry only relative information between components. Applying standard multivariate statistical methodology directly to analyze compositional data can lead to paradoxes and misinterpretations. Compositional data also frequently appear in insurance, especially with telematics information. However, such type of data does not receive deserved special treatment in most existing actuarial literature. In this paper, we explore and investigate the use of exponential family principal component analysis (EPCA) to analyze compositional data in insurance. The method is applied to analyze a dataset obtained from the U.S. Mine Safety and Health Administration. The numerical results show that EPCA is able to produce principal components that are significant predictors and improve the prediction accuracy of the regression model. The EPCA method can be a promising useful tool for actuaries to analyze compositional data. ",Compositional Data Regression in Insurance with Exponential Family PCA
3,1478006839237296132,1266475355453501440,Frank E. Curtis,"['Check out our new paper on the worst-case complexity of an algorithm for ùò§ùò∞ùòØùò¥ùòµùò≥ùò¢ùò™ùòØùò¶ùò• stochastic optimization:\n\n<LINK>\n\nShout out to Michael O‚ÄôNeill who led this work.  The analysis is quite sophisticated and groundbreaking, in my humble opinion :-)']",https://arxiv.org/abs/2112.14799,"A worst-case complexity bound is proved for a sequential quadratic optimization (commonly known as SQP) algorithm that has been designed for solving optimization problems involving a stochastic objective function and deterministic nonlinear equality constraints. Barring additional terms that arise due to the adaptivity of the monotonically nonincreasing merit parameter sequence, the proved complexity bound is comparable to that known for the stochastic gradient algorithm for unconstrained nonconvex optimization. The overall complexity bound, which accounts for the adaptivity of the merit parameter sequence, shows that a result comparable to the unconstrained setting (with additional logarithmic factors) holds with high probability. ","Worst-Case Complexity of an SQP Method for Nonlinear Equality
  Constrained Stochastic Optimization"
4,1477851677831331840,974769773539155968,Daniel Baumann,"['New paper on the ""Ionization of Gravitational Atoms"" with Giovanni Maria Tomaselli (a fantastic new PhD student), John Stout and @gfbertone, building on earlier work with @horngsheng and @RAPortoUY.\n<LINK> A short summary follows üëáüèº', 'Gravitational atoms are clouds of ultralight bosons generated by superradiance around rotating black holes.\n(The mathematical structure is almost the same as for the hydrogen atom.) https://t.co/i1cQmS6OKA', 'With Chia and Porto, we studied what would happen when such atoms are part of binary systems.\nWe found that the gravitational perturbation due to the companion can induce resonant transition between the different states of the cloud. (A bit like Rabi oscillations for atoms.) https://t.co/C9zMF21g36', 'In the new paper, we study the transitions from bound to unbound states, which ionize the atom. \n(A bit like the photoelectric effect.) https://t.co/fljRTVFc1z', 'We find that the effect can be large, with the power emitted in scalar waves overwhelming the power lost due to GW emission. The signal contains sharp features which arise when the bound state begins to resonate with the continuum. https://t.co/1lwuO4UZ5K', 'Using the conservation of energy and angular momentum, we computed the backreaction on the orbital dynamics.\nWe show that ionization (and accretion) shorten the merger time significantly. https://t.co/uOUhv3uXPR', 'The mass of the cloud and the companion develop a significant time dependence. https://t.co/LmWjLQlQUj', 'The sharp features in the ionized power lead to kinks in the frequency evolution of the GW signals. https://t.co/6Tm5W4DrQJ', 'More work needs to be done to develop waveforms for these systems that can be used in future data analysis. (We hope some of our friends working on GW data analysis may be inspired to look into this.)', '[All figures were made by John Stout.]']",https://arxiv.org/abs/2112.14777,"Superradiant instabilities may create clouds of ultralight bosons around rotating black holes, forming so-called ""gravitational atoms."" It was recently shown that the presence of a binary companion can induce resonant transitions between bound states of these clouds, whose backreaction on the binary's orbit leads to characteristic signatures in the emitted gravitational waves. In this work, we show that the interaction with the companion can also trigger transitions from bound to unbound states of the cloud -- a process that we refer to as ""ionization"" in analogy with the photoelectric effect in atomic physics. The orbital energy lost in the process overwhelms the losses due to gravitational wave emission and contains sharp features carrying information about the energy spectrum of the cloud. Moreover, we also show that if the companion is a black hole, then the part of the cloud impinging on the event horizon will be absorbed. This ""accretion"" leads to a significant increase of the companion's mass, which alters the dynamical evolution and ensuing waveform of the binary. We argue that a combined treatment of resonances, ionization, and accretion is crucial to discover and characterize gravitational atoms with upcoming gravitational wave detectors. ",Ionization of Gravitational Atoms
5,1477313568685056001,586538662,Dotan,"['Our recent on Autonomous dozers (submitted to ICRA) is nowavailable on Arxiv. \n\nPaper: <LINK>\nVideo: <LINK>\n\nTL:DR We created a new method for autonomous bulldozers to preform the grading task using RL and BC algorithms. üëá', '2/2 Our trained agent, AGPNet, reaches human-level performance and outperforms current state-of-the-art machine learning methods for the autonomous grading task. In addition, our agent is capable of generalizing from random scenarios to unseen real world problems.', '3/3 Finally, we show our policy which trained in simulation on a real prototype dozer we built in our Lab.']",https://arxiv.org/abs/2112.10877,"In this work, we establish heuristics and learning strategies for the autonomous control of a dozer grading an uneven area studded with sand piles. We formalize the problem as a Markov Decision Process, design a simulation which demonstrates agent-environment interactions and finally compare our simulator to a real dozer prototype. We use methods from reinforcement learning, behavior cloning and contrastive learning to train a hybrid policy. Our trained agent, AGPNet, reaches human-level performance and outperforms current state-of-the-art machine learning methods for the autonomous grading task. In addition, our agent is capable of generalizing from random scenarios to unseen real world problems. ",AGPNet -- Autonomous Grading Policy Network
6,1476671554901590018,188645961,Daine Danielson,['New paper with Robert Wald:\n\nA positive result for recently proposed tabletop experiments in quantum gravity will imply the discovery of the graviton.\n\n<LINK>\n#quantum #gravity #physics'],https://arxiv.org/abs/2112.10798,"We argue that if the Newtonian gravitational field of a body can mediate entanglement with another body, then it should also be possible for the body producing the Newtonian field to entangle directly with on-shell gravitons. Our arguments are made by revisiting a gedankenexperiment previously analyzed by Belenchia et al., which showed that a quantum superposition of a massive body requires both quantized gravitational radiation and local vacuum fluctuations of the spacetime metric in order to avoid contradictions with complementarity and causality. We provide a precise and rigorous description of the entanglement and decoherence effects occurring in this gedankenexperiment, thereby significantly improving upon the back-of-the-envelope estimates given in the analysis of Belenchia et al. and also showing that their conclusions are valid in much more general circumstances. As a by-product of our analysis, we show that under the protocols of the gedankenexperiment, there is no clear distinction between entanglement mediated by the Newtonian gravitational field of a body and entanglement mediated by on-shell gravitons emitted by the body. This suggests that Newtonian entanglement implies the existence of graviton entanglement and supports the view that the experimental discovery of Newtonian entanglement may be viewed as implying the existence of the graviton. ",Gravitationally Mediated Entanglement: Newtonian Field vs. Gravitons
7,1476645929235083276,745477700626714625,Akansha Singh Bansal,"['Just released the final chapter of my Ph.D. thesis in our new paper: <LINK> \n\nWe present a general model for solar nowcasting from abundant, readily available multispectral satellite data using self-supervised learning.\xa0\n\nWith @TrapitBansal &amp; David Irwin  üßµ1/5 <LINK>', 'Specifically, we develop deep auto-regressive models using CNN and LSTM that are globally trained across multiple locations to predict raw future observations of the spatio-temporal data collected by the recently launched GOES-R series of satellites.\xa02/5', ""Our model estimates a location's future solar irradiance based on satellite observations, which we feed to a regression model trained on smaller site-specific solar data to provide near-term solar photovoltaic (PV) forecasts that account for site-specific characteristics. 3/5"", 'Our approach is more widely deployable compared to models relying on sky cameras that need localized special hardware to be installed at a site. Across 25 solar sites in the US, we show that our approach yields errors close to that of a model using ground-truth observations. 4/5', 'While solar nowcasting is the primary focus here, such autoregressive models trained on abundant satellite data should be more generally useful for many applications such as detecting anomalies like wildfires, forecasting cloud cover, precipitation nowcasting, and more. 5/5']",http://arxiv.org/abs/2112.13974,"Solar energy is now the cheapest form of electricity in history. Unfortunately, significantly increasing the grid's fraction of solar energy remains challenging due to its variability, which makes balancing electricity's supply and demand more difficult. While thermal generators' ramp rate -- the maximum rate that they can change their output -- is finite, solar's ramp rate is essentially infinite. Thus, accurate near-term solar forecasting, or nowcasting, is important to provide advance warning to adjust thermal generator output in response to solar variations to ensure a balanced supply and demand. To address the problem, this paper develops a general model for solar nowcasting from abundant and readily available multispectral satellite data using self-supervised learning. Specifically, we develop deep auto-regressive models using convolutional neural networks (CNN) and long short-term memory networks (LSTM) that are globally trained across multiple locations to predict raw future observations of the spatio-temporal data collected by the recently launched GOES-R series of satellites. Our model estimates a location's future solar irradiance based on satellite observations, which we feed to a regression model trained on smaller site-specific solar data to provide near-term solar photovoltaic (PV) forecasts that account for site-specific characteristics. We evaluate our approach for different coverage areas and forecast horizons across 25 solar sites and show that our approach yields errors close to that of a model using ground-truth observations. ","A Moment in the Sun: Solar Nowcasting from Multispectral Satellite Data
  using Self-Supervised Learning"
8,1476629180766998540,384900803,Shantanu Basu,"['New paper from our #research group, an analytic model for why young stars undergo episodic bursts of mass accumulation. Led by our outstanding PhD candidate Indrani Das! Nice way to close out the year. @westernuPhysAst  @westernuScience  #astronomy <LINK>']",https://arxiv.org/abs/2112.13856,"We develop a semi-analytic formalism for the determination of the evolution of the stellar mass accretion rate for specified density and velocity profiles that emerge from the runaway collapse of a prestellar cloud core. In the early phase, when the infall of matter from the surrounding envelope is substantial, the star accumulates mass primarily because of envelope-induced gravitational instability in a protostellar disc. In this phase, we model the envelope mass accretion rate from the isothermal free-fall collapse of a molecular cloud core. The disc gains mass from the envelope, and also transports matter to the star via a disc accretion mechanism that includes episodic gravitational instability and mass accretion bursts according to the Toomre $Q$-criterion. In the early phase the envelope accretion is dominant, whereas in the late phase the disc accretion is dominant. In the disc accretion phase, mass is accreted on to the star due to gravitational torques within the spiral structures in the disc, in a manner that analytic theory suggests has a mass accretion rate $\propto t^{-6/5}$. Our model provides a self-consistent evolution of the mass accretion rate by joining the spherical envelope accretion with the disc accretion and accounts for the presence of episodic accretion bursts at appropriate times. We show using a simple example that the burst mode is essential to explain the long-standing 'luminosity problem' of young stellar objects. The bursts are needed to provide a good match to the observed distribution of bolometric luminosities. In contrast, a smoothly time-dependent mass accretion rate, whether monotonically increasing or decreasing, is unable to do so. Our framework reproduces key elements of detailed numerical simulations of disc accretion and can aid in developing intuition about the basic physics as well as in comparing theory with observations. ","A semi-analytic model for the temporal evolution of the episodic
  disc-to-star accretion rate during star formation"
9,1476369750695444481,1093387119148462081,Daniel Green,"['New paper today with @DD_Baumann: <LINK>\n\nWe argue that equilateral non-Gaussianity is easier to distinguish from nonlinear evolution in LSS than previously thought\n\nKey idea: inflationary correlations are nonlocal in space, while late-time evolution is local <LINK>', 'The reason this is not obvious is that locality is hard-coded into the maps, not individual correlation functions (especially in fourier space).  \n\nThe degeneracy between nonlinear evolution (e.g. biasing) and non-Gaussianity in the bispectrum is broken by high-point information https://t.co/aojR9rYgMr', 'We can see this explicitly that adding more local bias parameters to a bispectrum analysis weakens constraints on fnl while it has little impact on the map level constraints.\n\nThis is not true if we start including non-local terms, i.e. matter that can move cosmological distances https://t.co/jwbVGGK7Fw', 'This is a preliminary analysis, we need to see if these improvements are possible on real data.  However, it does suggest a more optimistic view of measuring primordial non-Gaussianity in LSS.  Hopefully, what we describe is what will come out of simulation based inference anyway']",https://arxiv.org/abs/2112.14645,"Primordial non-Gaussianity is a sensitive probe of the inflationary era, with a number of important theoretical targets living an order of magnitude beyond the reach of current CMB constraints. Maps of the large-scale structure of the universe, in principle, have the raw statistical power to reach these targets, but the complications of nonlinear evolution are thought to present serious, if not insurmountable, obstacles to reaching these goals. In this paper, we will argue that the challenge presented by nonlinear structure formation has been overstated. The information encoded in primordial non-Gaussianity resides in nonlocal correlations of the density field at three or more points separated by cosmological distances. In contrast, nonlinear evolution only alters the density field locally and cannot create or destroy these long-range correlations. This locality property of the late-time non-Gaussianity is obscured in Fourier space and in the standard bispectrum searches for primordial non-Gaussianity. We therefore propose to measure non-Gaussianity in the position space maps of the large-scale structure. As a proof of concept, we study the case of equilateral non-Gaussianity, for which the degeneracy with late-time nonlinearities is the most severe. We show that a map-level analysis is capable of breaking this degeneracy and thereby significantly improve the constraining power over previous estimates. Our findings suggest that ""simulation-based inference"" involving the forward modeling of large-scale structure maps has the potential to dramatically impact the search for primordial non-Gaussianity. ",The Power of Locality: Primordial Non-Gaussianity at the Map Level
10,1475867341896491008,770642628,Nahuel Andr√©s,['Last but not least! A new paper about anisotropic solar wind turbulence: <LINK> at @Fisica_UBA @IAFE_Oficial and collaborators at @LabPhysPlasmas'],https://arxiv.org/abs/2112.13748,"Context. The presence of a magnetic guide field induces several types of anisotropy in solar wind turbulence. The energy cascade rate between scales in the inertial range depends strongly on the direction of this magnetic guide field, splitting the energy cascade according to the parallel and perpendicular directions with respect to magnetic guide field. Aims. Using more than 2 years of Parker Solar Probe (PSP) observations, the isotropy and anisotropy energy cascade rates are investigated. The variance and spectral anisotropy ratios, the kinetic and magnetic energies and the both normalized cross-helicity and residual energy are studied. The connection between the heliocentric distance, the local temperature of the plasma and the energy cascade components is made. Methods. Using exact relations for fully developed magnetohydrodynamic (MHD) turbulence, the incompressible energy cascade rate is computed. In particular, using the isotropy and 2D and slab assumptions, the isotropic, perpendicular and parallel energy cascade rate components are estimated. Results. The variance anisotropy ratios, for both kinetic and magnetic fields, do not exhibit a dependence with respect to the heliocentric distance, $r$. While the kinetic spectral anisotropy ratio shows a dependence with $r$, the magnetic spectral anisotropy does not. A strong correlation between the isotropic and anisotropic energy cascade rates and the temperature is found. A clear dominance of the perpendicular cascades over the parallel cascades as PSP approaches to the Sun is observed. A dominant 2D cascade/geometry over the slab component in slow solar wind turbulence in the largest MHD scales is observed. ","About the incompressible energy cascade rate in anisotropic solar wind
  turbulence"
11,1475724913625505797,1074313879612784640,Anna Abalkina,"['This is my new preprint on @arxiv ""Publication and collaboration anomalies in academic papers originating from a paper mill: evidence from Russia"". At least 303 papers were identified. and a set of predictors of a Russian paper mill were proposed.\n<LINK> <LINK>']",http://arxiv.org/abs/2112.13322,"This study attempts to detect papers originating from the Russia-based paper mill International publisher LLC. A total of 1009 offers published during 2019-2021 on the 123mi.ru website were analysed. The study allowed us to identify at least 434 papers that are potentially linked to the paper mill including one preprint, a duplication paper and 15 republications of papers erroneously published in hijacked journals. Evidence of suspicious provenance from the paper mill is provided: matches in title, number of coauthorship slots, year of publication, country of the journal, country of a coauthorship slot and similarities of abstracts. These problematic papers are coauthored by scholars associated with at least 39 countries and submitted both to predatory and reputable journals. This study also demonstrates collaboration anomalies and the phenomenon of suspicious collaboration in questionable papers and examines the predictors of the Russia-based paper mill. The value of coauthorship slots offered by International Publisher LLC in 2019-2021 is estimated at $6.5 million. Since the study analysed a particular paper mill, it is likely that the number of papers with forged authorship is much higher. ","Publication and collaboration anomalies in academic papers originating
  from a paper mill: evidence from a Russia-based paper mill"
12,1475661834220941321,321794593,Jos√© G. Fern√°ndez-Trincado,['Our new accepted paper tonight on ArXiv about ‚ÄúUnveiling the nature of 12 new low-luminosity Galactic Globular Cluster Candidates‚Äù by @ElisaGarro1 üëâüèª <LINK>'],https://arxiv.org/abs/2112.13591,"The Galactic globular cluster system is incomplete, especially in the low latitude regions of the Galactic bulge and disk. We report the physical characterization of twelve star clusters in the Milky Way, most of which are explored here for the first time. Our aim is determining their main physical parameters, such as reddening and extinction, metallicity, age, total luminosity, mean cluster proper motions (PMs), distances, in order to unveil their physical nature. We study the clusters using optical and near-infrared (NIR) datasets. We use the Gaia Early Data Release 3 (EDR3) PMs in order to perform a PM-decontamination procedure and build final catalogues. We match the Gaia EDR3 with the VISTA Variables in the V\'ia L\'actea extended (VVVX) survey and Two Micron All Sky survey (2MASS) in the NIR, in order to construct complete colour-magnitude diagrams (CMDs) and investigate the clusters properties. The extinctions are evaluated using existing reddening maps, spanning $0.09\lesssim A_{Ks}\lesssim 0.86$ mag and $0.89 \lesssim A_{G}\lesssim 4.72$ mag in the NIR and optical, respectively. We obtain their heliocentric distances, that range from about 4 to 20 kpc, placing these clusters at $3\lesssim R_{G}\lesssim 14$ kpc from the Galactic centre. The best PARSEC isochrone fit yields a metallicity range of $-1.8<$[Fe/H]$<+0.3$ and an approximative age range of $2<$Age$<14$ Gyr. We find that all clusters have low-luminosities, with $-6.9<M_{V}<-3.5$ mag. Based on our photometric analysis, we confirm the OC nature for Kronberger100, while we classify Patchick125 as a metal-poor GC, Ferrero54 as a metal-rich GC, and ESO92-18 as a possible old OC or young GC. The classification as GC candidates is also suggested for Kronberger99, Patchick122, Patchick126, Riddle15, FSR190 and Gaia2. We also conclude that Kronberger119 and Kronberger143 might be either old OCs or young GCs. ","Unveiling the nature of 12 new low-luminosity Galactic Globular Cluster
  Candidates"
13,1475651359718453259,1342973500219277312,Irem Ergunüá∫üá¶,"['üì£üì£ü•≥ü•≥ New paper alert: <LINK>', 'Tomorrow, I will prepare a thread about what this paper is all about. This will include what FL is, how it can be made more secure and how we can achieve efficiency using gradient sparsification. I will go into more details about our solution. Stay tuned if interested! üôÉ']",https://arxiv.org/abs/2112.12872,"Secure aggregation is a popular protocol in privacy-preserving federated learning, which allows model aggregation without revealing the individual models in the clear. On the other hand, conventional secure aggregation protocols incur a significant communication overhead, which can become a major bottleneck in real-world bandwidth-limited applications. Towards addressing this challenge, in this work we propose a lightweight gradient sparsification framework for secure aggregation, in which the server learns the aggregate of the sparsified local model updates from a large number of users, but without learning the individual parameters. Our theoretical analysis demonstrates that the proposed framework can significantly reduce the communication overhead of secure aggregation while ensuring comparable computational complexity. We further identify a trade-off between privacy and communication efficiency due to sparsification. Our experiments demonstrate that our framework reduces the communication overhead by up to 7.8x, while also speeding up the wall clock training time by 1.13x, when compared to conventional secure aggregation benchmarks. ",Sparsified Secure Aggregation for Privacy-Preserving Federated Learning
14,1475645937645400066,767659609,Yoshihiko Hasegawa,"['My new paper ""Relativistic entropy production for quantum field in cavity"" appeared in arXiv.\n<LINK>']",https://arxiv.org/abs/2112.13712,"A non-uniformly accelerated quantum field in a cavity undergoes the coordinate transformation of annihilation and creation operators, known as the Bogoliubov transformation. In this Letter, we consider the entropy production of a quantum field in a cavity induced by the Bogoliubov transformation. Dividing modes in the cavity into the system and environment, we obtain the lower bound of the entropy production which is defined by the sum of the von Neumann entropy in the system and the dissipated heat in the environment. The obtained lower bound is a signature of a refined second law of thermodynamics for a quantum field in a cavity and can be interpreted as the Landauer principle, which gives the thermodynamic cost of changing information contained in the system. Moreover, it gives an upper bound for the quantum mutual information, which can quantify the extent of information scrambling in the cavity due to the acceleration. ",Relativistic entropy production for quantum field in cavity
15,1474412734519267328,3102898266,Ali Mustufa,"['1/ Happy to announce our paper ""CPPE-5: Medical Personal Protective Equipment Dataset"", a üßµ\n\nWe introduce a new challenging image dataset with the goal to allow the study of subordinate categorization of medical PPE unlike any existing dataset\n\nPaper: <LINK> <LINK>', '2/ Trying to do so is not possible with other popular large-scale data sets that focus on broad-level categories. We also find that currently there exists no such dataset and critique the few *similar* datasets that either consist of ‚Ä¶', '3/ artificial images, focus only on a single category (eg. mask), far less data, or do not focus on medical scenarios altogether which I believe is super helpful\n\nThe dataset also allows easily deploying in complex scenes with other objects as well due to the following features', '4/ The dataset contains:\n- high-quality images and annotations (~4.7 bbox/image)\n- real-life images unlike any current dataset\n- a majority of non-iconic images in their natural context often with multiple other objects\n- annotations for 5 PPE items https://t.co/AViCMep7GQ', '5/ We perform algorithmic analysis on the dataset and open-source some of the top-performing models on this dataset in the code repository: https://t.co/XdToiwR8cV\n\nThe PyTorch and TensorFlow data loader, tutorial to load this dataset, a training tutorial and more are in the repo https://t.co/RG3WwZ3s6d', '6/ We are thankful to @Google @TensorFlow for a generous research grant and TPU Research Cloud for providing access to TPUs\n\ncc: lovely collaborating with @rishit_dagli']",https://arxiv.org/abs/2112.09569,"We present a new challenging dataset, CPPE - 5 (Medical Personal Protective Equipment), with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories (such as PASCAL VOC, ImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained on this dataset to be used in practical scenarios in complex scenes, our dataset mainly contains images that show complex scenes with several objects in each scene in their natural context. The image collection for this dataset focusing on: obtaining as many non-iconic images as possible and making sure all the images are real-life images unlike other existing datasets in this area. Our dataset includes 5 object categories (coveralls, face shield, gloves, mask, and goggles) and each image is annotated with a set of bounding boxes and positive labels. We present a detailed analysis of the dataset in comparison to other popular broad category datasets as well as datasets focusing on personal protective equipments, we also find that at present there exist no such publicly available datasets. Finally we also analyze performance and compare model complexities on baseline and state-of-the-art models for bounding box results. Our code, data, and trained models are available at this https URL . ",CPPE-5: Medical Personal Protective Equipment Dataset
16,1474390633141968899,1345813205440995333,Ben Freed,"['Check out our new paper! We automatically decompose large MA problems to be solved efficiently with #reinforcementlearning while maintaining cooperation ü¶æ.(with @_AdityaKapoor_, @ianabraha, @jeffschneider, Jeff Schneider, and @howiechoset) #deeplearning\n\n<LINK>']",http://arxiv.org/abs/2112.12740,"One of the preeminent obstacles to scaling multi-agent reinforcement learning to large numbers of agents is assigning credit to individual agents' actions. In this paper, we address this credit assignment problem with an approach that we call \textit{partial reward decoupling} (PRD), which attempts to decompose large cooperative multi-agent RL problems into decoupled subproblems involving subsets of agents, thereby simplifying credit assignment. We empirically demonstrate that decomposing the RL problem using PRD in an actor-critic algorithm results in lower variance policy gradient estimates, which improves data efficiency, learning stability, and asymptotic performance across a wide array of multi-agent RL tasks, compared to various other actor-critic approaches. Additionally, we relate our approach to counterfactual multi-agent policy gradient (COMA), a state-of-the-art MARL algorithm, and empirically show that our approach outperforms COMA by making better use of information in agents' reward streams, and by enabling recent advances in advantage estimation to be used. ",Learning Cooperative Multi-Agent Policies with Partial Reward Decoupling
17,1474317968599400462,294101019,Ana Vldv,"['üì¢I am very excited to share this new paper in which we analyse the theoretical and political impact of fairness in biometrics:\n\n""There is an elephant in the room: Towards a critique on the use of fairness in biometrics""\n\n#Fairness #Biometrics #Migration\n\n<LINK>', '""Algorithmic fairness cannot distribute justice in scenarios which are broken or intended purpose is to discriminate, such as biometrics deployed at the border.""', '""We argue that a critical questioning of the use of fairness in biometrics systems should also be focused on the historical, political and social contexts in which biometrics are deployed.""', '""Bringing Grimke‚Äôs political consciousness to our discussion about the regulation of biometrics, we suggest that until migrants get their digital and fundamental rights, we shall never have ours.""']",https://arxiv.org/abs/2112.11193,"In 2019, the UK's Immigration and Asylum Chamber of the Upper Tribunal dismissed an asylum appeal basing the decision on the output of a biometric system, alongside other discrepancies. The fingerprints of the asylum seeker were found in a biometric database which contradicted the appellant's account. The Tribunal found this evidence unequivocal and denied the asylum claim. Nowadays, the proliferation of biometric systems is shaping public debates around its political, social and ethical implications. Yet whilst concerns towards the racialised use of this technology for migration control have been on the rise, investment in the biometrics industry and innovation is increasing considerably. Moreover, fairness has also been recently adopted by biometrics to mitigate bias and discrimination on biometrics. However, algorithmic fairness cannot distribute justice in scenarios which are broken or intended purpose is to discriminate, such as biometrics deployed at the border. In this paper, we offer a critical reading of recent debates about biometric fairness and show its limitations drawing on research in fairness in machine learning and critical border studies. Building on previous fairness demonstrations, we prove that biometric fairness criteria are mathematically mutually exclusive. Then, the paper moves on illustrating empirically that a fair biometric system is not possible by reproducing experiments from previous works. Finally, we discuss the politics of fairness in biometrics by situating the debate at the border. We claim that bias and error rates have different impact on citizens and asylum seekers. Fairness has overshadowed the elephant in the room of biometrics, focusing on the demographic biases and ethical discourses of algorithms rather than examine how these systems reproduce historical and political injustices. ","There is an elephant in the room: Towards a critique on the use of
  fairness in biometrics"
18,1474213751205158918,2845244144,‚ü®Michael |‚öõÔ∏è| Bromley‚ü©,['New paper on #arXiv just in time for xmas  -  <LINK> . Led by my PhD student and we convinced two experimental groups to do experiments demonstrating our theory. #QuantumXmas <LINK>'],https://arxiv.org/abs/2112.12408,"Quantum systems with exact analytic solutions are rare, with most systems of interest requiring perturbative or other approximate methods to produce theoretical models. Husimi's 1953 solution to the Schrodinger equation for the linearly driven (see-saw) harmonic oscillator results in an exact solution for a wavepacket spatially translated but otherwise unperturbed by the driving force. In this work, we consider this Husimi driving scheme applied to a Bose-Einstein condensate (BEC), and further extend the theoretical solution to include interacting many-body systems in arbitrary states. We experimentally implement this solution in both optically- and magnetically-trapped BEC systems, subject to resonant or off-resonant driving by a linear magnetic potential. The observed trajectories of the centre-of-mass agree with theory, showing minimal excitation of the condensate. Based on these results, we propose Husimi driving as a platform for precision control of one-body, few-body, and many-body systems, and demonstrate its application to high-fidelity condensate transport at 72 times faster than adiabatic speeds. We demonstrate the platform's utility in trap frequency measurement and introduce a Husimi driving-based scheme for atom interferometry. ",Husimi-driven many-body systems realised with Bose-Einstein condensates
19,1474078600857071625,223440240,Nathan Kallus,"['Excited to post new paper with the amazing @Jacobb_Douglas &amp; Kevin Guo\n""Doubly-Valid/Doubly-Sharp Sensitivity Analysis for Causal Inference with Unmeasured Confounding""\n<LINK>\nIn this time of uncertainty it\'s good to have some checks on your causal inferences 1/n <LINK>', 'So, sensitivity analysis to unobserved confounding is paramount. A central model for such confounding is the marginal sensitivity model (MSM) of Tan (2006), but existing estimators for the bounds MSM implies on ATE can be unsharp and... well... sensitive (to nuisance estimation)', 'Combining an existing method together with a new one based on distributionally robust optimization (DRO), we develop the DVDS estimator, which generalizes the usual doubly robust estimator without unobserved confounding. But it has some cool new properties in the confounded case', 'It has two special properties: (1) double sharpness (DS) reflects a usual double/local robustness and efficiency for the *sharp* bounds on the ATE (ie, tightest possible) when most nuisances are consistent. (FYI some popular methods are not sharp even under perfect nuisances.)', 'More interestingly, it has a cool new robustness property that is special to partial id problems: (2) double validity (DV) which says that it still estimates valid bounds (albeit conservative) when most nuisances are inconsistent (including a pesky quantile regression).', 'Even when things are so bad that asymptotic normality breaks, +-2 std errs has valid 95% coverage, which we prove using a cool bootstrap coupling trick. This extensive robustness is pretty slick given sensitivity analysis is all about worrying about bad stuff messing up analysis.', 'DV is a completely new ""double robustness""-type phenomenon that is special just to partial id problems. We think it is probably more pervasive in such problems, beyond MSM ATE bounds. Think it may be the next frontier for credible inference!', 'The MSM-DRO connection is also interesting: suggests many possibly-useful generalizations to the MSM based on recent modeling and methodological advances in the growing lit on DRO empirical risk minimization. Lots to think about in that direction too...']",https://arxiv.org/abs/2112.11449,"We study the problem of constructing bounds on the average treatment effect in the presence of unobserved confounding under the marginal sensitivity model of Tan (2006). Combining an existing characterization involving adversarial propensity scores with a new distributionally robust characterization of the problem, we propose novel estimators of these bounds that we call ""doubly-valid/doubly-sharp"" (DVDS) estimators. Double sharpness corresponds to the fact that DVDS estimators consistently estimate the tightest possible (i.e., sharp) bounds implied by the sensitivity model even when one of two nuisance parameters is misspecified and achieve semiparametric efficiency when all nuisance parameters are suitably consistent. Double validity is an entirely new property for partial identification: DVDS estimators still provide valid, though not sharp, bounds even when most nuisance parameters are misspecified. In fact, even in cases when DVDS point estimates fail to be asymptotically normal, standard Wald confidence intervals may remain valid. In the case of binary outcomes, the DVDS estimators are particularly convenient and possesses a closed-form expression in terms of the outcome regression and propensity score. We demonstrate the DVDS estimators in a simulation study as well as a case study of right heart catheterization. ","Doubly-Valid/Doubly-Sharp Sensitivity Analysis for Causal Inference with
  Unmeasured Confounding"
20,1474069982292197382,10547982,"Orad Reshef, PhD","['We just posted a new #photonics paper on @arxiv on Surface Lattice Resonances! We figured out how to generate an arbitrary number of resonances in a metasurface, and to place them at arbitrary frequencies, aided by Fourier Transforms. \n\nRead more here:\n<LINK>']",https://arxiv.org/abs/2112.11625,"Resonances in optical systems are useful for many applications, such as frequency comb generation, optical filtering, and biosensing. However, many of these applications are difficult to implement in optical metasurfaces because traditional approaches for designing multi-resonant nanostructures require significant computational and fabrication efforts. To address this challenge, we introduce the concept of Fourier lattice resonances (FLRs) in which multiple desired resonances can be chosen a priori and used to dictate the metasurface design. Because each resonance is supported by a distinct surface lattice mode, each can have a high quality factor. Here, we experimentally demonstrate several metasurfaces with arbitrarily placed resonances (e.g., at 1310 and 1550 nm) and Q-factors as high as 800 in a plasmonic platform. This flexible procedure requires only the computation of a single Fourier transform for its design, and is based on standard lithographic fabrication methods, allowing one to design and fabricate a metasurface to fit any specific, optical-cavity-based application. This work represents an important milestone towards the complete control over the transmission spectrum of a metasurface. ",Fourier-Engineered Plasmonic Lattice Resonances
21,1474045413007179776,2710269284,Graham Kerr,"['New paper, led by Yan Xu (@NJIT_Physics) accepted in ApJ to finish the year, continuing our work investigating the behaviour of #solarflare ribbon fronts, with dimming found across the He I 10830 line observed at high res by #bigbearsolarobservatory: <LINK>']",https://arxiv.org/abs/2112.09949,"This study presents a C3.0 flare observed by the BBSO/GST and IRIS, on 2018-May-28 around 17:10 UT. The Near Infrared Imaging Spectropolarimeter (NIRIS) of GST was set to spectral imaging mode to scan five spectral positions at $\pm$ 0.8 \AA, $\pm$ 0.4 \AA and line center of He I 10830. At the flare ribbon's leading edge the line is observed to undergo enhanced absorption, while the rest of the ribbon is observed to be in emission. When in emission, the contrast compared to the pre-flare ranges from about $30~\%$ to nearly $100~\%$ at different spectral positions. Two types of spectra, ""convex"" shape with higher intensity at line core and ""concave"" shape with higher emission in the line wings, are found at the trailing and peak flaring areas, respectively. On the ribbon front, negative contrasts, or enhanced absorption, of about $\sim 10\% - 20\%$ appear in all five wavelengths. This observation strongly suggests that the negative flares observed in He I 10830 with mono-filtergram previously were not caused by pure Doppler shifts of this spectral line. Instead, the enhanced absorption appears to be a consequence of flare energy injection, namely non-thermal collisional ionization of helium caused by the precipitation of high energy electrons, as found in our recent numerical modeling results. In addition, though not strictly simultaneous, observations of Mg II from the IRIS spacecraft, show an obvious central reversal pattern at the locations where enhanced absorption of He I 10830 is seen, which is in consistent with previous observations. ","Multi-Passband Observations of A Solar Flare over the He I 10830 \AA\
  line"
22,1473935084776570883,1232964338,Shaikh saad,['our new paper; and a great job done by @ajay_kaladharan : <LINK>'],https://arxiv.org/abs/2112.12041,"We study the gravitational wave imprints of left-right symmetric model equipped with universal seesaw mechanism allowing for the natural generation of hierarchical masses of the Standard Model fermions. The scalar sector of this model is the minimal one, consisting of only two Higgs doublets. Following the construction of the full thermal potential for this model, we perform a scan of the entire parameter space and identify the region in which the cosmic phase transition associated with the left-right symmetry breaking gives gravitational wave signals detectable by a variety of planned space-based interferometers. Then we also discuss the relevant collider implications of this beyond the Standard Model scenario. ","Gravitational Wave Imprints of Left-Right Symmetric Model with Minimal
  Higgs Sector"
23,1473468913853812741,223458855,Michael A. Fedderke,"['New paper! We point out that some inner Solar System asteroids work as test masses for a GW detector in the microhertz band, with impressive strain sensitivity. We discuss conceptual link ideas (e.g., laser ranging with timing by an optical atomic clock). <LINK>']",https://arxiv.org/abs/2112.11431,"A major challenge for gravitational-wave (GW) detection in the $\mu$Hz band is engineering a test mass (TM) with sufficiently low acceleration noise. We propose a GW detection concept using asteroids located in the inner Solar System as TMs. Our main purpose is to evaluate the acceleration noise of asteroids in the $\mu$Hz band. We show that a wide variety of environmental perturbations are small enough to enable an appropriate class of $\sim 10$ km-diameter asteroids to be employed as TMs. This would allow a sensitive GW detector in the band $\text{(few)} \times 10^{-7} \text{Hz} \lesssim f_{\text{GW}} \lesssim \text{(few)} \times 10^{-5} \text{Hz}$, reaching strain $h_c \sim 10^{-19}$ around $f_{\text{GW}} \sim 10 \mu$Hz, sufficient to detect a wide variety of sources. To exploit these asteroid TMs, human-engineered base stations could be deployed on multiple asteroids, each equipped with an electromagnetic (EM) transmitter/receiver to permit measurement of variations in the distance between them. We discuss a potential conceptual design with two base stations, each with a space-qualified optical atomic clock measuring the round-trip EM pulse travel time via laser ranging. Tradespace exists to optimize multiple aspects of this mission: for example, using a radio-ranging or interferometric link system instead of laser ranging. This motivates future dedicated technical design study. This mission concept holds exceptional promise for accessing this GW frequency band. ",Asteroids for microhertz gravitational-wave detection
24,1473383342443696129,1087982703675392000,Xian Li,"['üåçFew-shot learning beyond Englishüåè\nüì¢ Announcing XGLMs, a series of multilingual autoregressive languages models setting new SoTA on few-shot learning and outperforming English-centric models (e.g. GPT-3).\n\nPaper:\xa0<LINK>\nModels and code:\xa0<LINK> <LINK>', 'üßµOur largest model with 7.5B parameters outperforms GPT-3 of comparable size in multilingual commonsense reasoning (+7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings).', 'On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. https://t.co/Kok3GImzAQ', 'We provide an in-depth analysis of multilingual in-context learning, including evaluation protocol, crosslingual transfer in prompts and examples, and comparisons with finetuning approach.', 'Finally, we study both multilingual and English-centric models for social value tasks, e.g. hate speech detection and gender bias. There is room for improvement across models, possibly due to challenges posed by cultural and social clues in the few-shot learning paradigm.', 'In the spirit of transparency and accountability we include detailed data and model cards.', 'Joint work w/ @VictoriaLinML @artetxem @tbmihaylov Tianlu Wang, Shuohui Chen, Daniel Simig, @myleott @NamanGoyal21 @shruti_bhosale @JefferyDuu @ramakanth1729 @sam_shleifer Punit Singh Koura, @vishrav @LukeZettlemoyer @zkozareva Mona Diab @vesko_st  and awesome colleagues @MetaAI']",https://arxiv.org/abs/2112.10668,"Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 translation directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We present a detailed analysis of where the model succeeds and fails, showing in particular that it enables cross-lingual in-context learning on some tasks, while there is still room for improvement on surface form robustness and adaptation to tasks that do not have a natural cloze form. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models. ",Few-shot Learning with Multilingual Language Models
25,1473345804127584270,4365927557,Dr. Jake Turner üåÖ,"['**New Paper***\n\n""Characterizing the WASP-4 system with TESS &amp; radial velocity data: Constraints on the cause of the hot Jupiter\'s changing orbit &amp; evidence of an outer planet""\n\n<LINK>\n@Cornell @CSInst \n\nCo-authors: @lauraflaggastro @AstroAndrew123 @DrRayJay \n1/20 <LINK>', 'Observations of orbital decay on close-in planets enhances our understanding of the hot Jupiter population and their evolution\n\n#Exoplanets\n2/20 https://t.co/DUIJUqGWD3', 'To date, WASP-12b is one of the few hot Jupiters\nconfirmed to have a varying period. \n\nEarlier this year, we confirmed its decaying orbit with TESS observations\n\nBesides WASP-12b, WASP-4b is a well-studied system with hints of a changing period...\n\nhttps://t.co/JBz2bF84mk\n\n3/20', 'WASP-4b is a typical hot Jupiter (~1700 K) discovered in 2008. It orbits its star in 1.3 days! \n\nIts atmosphere was found to be cloudy &amp; highly inefficient at heat redistribution. \n\nBut what about its orbit? \n#Exoplanets\n4/20 https://t.co/TXyHPbjrbA', 'Bouma et al. (2019) first reported an orbital period variation of WASP-4b using @NASA_TESS\n\nSouthworth et al. (2019) confirmed that the period was decaying with additional ground-based observations &amp; found a decay rate of -9.2 msec/yr\n#Exoplanets\n5/20 https://t.co/KT9DT2DItt', 'Bouma et al. (2020) obtained RV data on WASP- 4b using HIRES on Keck &amp; found that the observed orbital period variation could be explained by the system accelerating toward the Sun\n\n#Exoplanets\n6/20 https://t.co/6LPHxNsqpt', 'Recently, Baluev et al. 2020 confirmed a period change in WASP-4b‚Äôs orbit but do not confirm the RV acceleration found by Bouma et al. (2020). \n\nHowever, Baluev et al. (2020) did not include the new RV HIRES data from Bouma et al. (2020) in their analysis. \n\n#Exoplanets\n7/20 https://t.co/hSTEqjwsAL', 'Motivated by the possible changing period of WASP- 4b, we analyze all sectors from TESS &amp; combine the results with all transit, occultation, &amp; RV measurements\n\nAltogether, the transit data span 13 years from 2007-2020 and the RV data span 12 years from 2007-2019.\n#Exoplanets\n8/20 https://t.co/30ijf3MQiR', 'As in our previous study, we modeled each individual transit (the phase folded light curve is below)\n\nThese data were modeled with the EXOplanet Modeling Package (EXOMOP) developed in my PhD\n#Exoplanets\n9/20 https://t.co/LRbx5mx6kb', 'Unfortunately, we do not see an occultation of WASP-4b in the @NASA_TESS data. \n\nWe find a 3-sigma upper limit on the geometric albedo of 0.017. Our upper limit is consistent with the overall trend that hot Jupiters are very dark (like coal) \n\n#Exoplanets\n10/20 https://t.co/OKqyrwbmMK', 'We confirm using all the @NASA_TESS sectors &amp; ground-based transits that the period of WASP-4b is changing\n\n#Exoplanets\n11/20 https://t.co/taHM8dVLNN', 'Our analysis shows that the full RV data set is consistent with no acceleration towards the Earth. \n\nWe only find this by modeling all RV data (we reproduce the result from Bouma et al. 2020 using only their data)\n\n#Exoplanets\n12/20 https://t.co/kfGhB6jjJ8', 'Instead, we find evidence of a possible additional planet in the WASP-4 system\n\n(RV model with two-planets below)\n#Exoplanets\n13/20 https://t.co/AMdFfvBKOo', 'WASP-4c, the possible additional planet, orbits at a distance of 6.82 AU with an orbital period of 7001 days. \n\nWASP-4c has a Msin(i) of 5.47 MJup. Assuming WASP-4c is near co-planar with WASP-4b, we find a planetary mass between 5.47-6.50 MJup\n\n#Exoplanets\n14/20 https://t.co/pBBoU3q16S', 'More RV measurements (and other types of data) are needed to verify the existence of this second planet around WASP-4.\n\n#Exoplanets\n15/20', 'We find that the TTVS of all of the WASP-4b transits cannot be explained by the second planet but can be explained with either a decaying orbit or apsidal precession, with a slight preference for orbital decay. \n\n#Exoplanets\n16/20 https://t.co/4UONr4FqEL', 'The orbital decay &amp; apsidal precession models exhibit very different timing variations in the mid-2020s (see below). \n\nTherefore, we predict that it will be possible to conclusively determine the cause of WASP-4b‚Äôs changing orbit by then.\n\n#Exoplanets\n17/20 https://t.co/5u4yMykT4G', 'We find a decay rate of -7.33¬±0.71 msec/year &amp; an orbital decay lifetime of 15.77 Myr assuming the system is undergoing orbital decay. \n\nWe derive a modified tidal quality factor of 5.1x10^4, which is an order of magnitude lower than other hot Jupiter systems.\n\n#Exoplanets\n18/20 https://t.co/1WBXV5JkEr', 'The discovery of WASP-4c makes the WASP-4 system unique, as it would be the most widely separated companion of a transiting hot Jupiter known to date.\n\nWe do expect many more of these types of systems to be discovered as more RV datasets will have longer baselines\n19/20 https://t.co/odz6LEHyIB', 'More observations are needed to determine conclusively the cause of WASP-4b‚Äôs changing orbit and confirm the existence of an outer companion.\n\nSo stay tuned! üòÉüî≠\n20/20']",https://arxiv.org/abs/2112.09621,"Orbital dynamics provide valuable insights into the evolution and diversity of exoplanetary systems. Currently, only one hot Jupiter, WASP-12b, is confirmed to have a decaying orbit. Another, WASP-4b, exhibits hints of a changing orbital period that could be caused by orbital decay, apsidal precession, or the acceleration of the system towards the Earth. We have analyzed all data sectors from NASA's Transiting Exoplanet Survey Satellite together with all radial velocity (RV) and transit data in the literature to characterize WASP-4b's orbit. Our analysis shows that the full RV data set is consistent with no acceleration towards the Earth. Instead, we find evidence of a possible additional planet in the WASP-4 system, with an orbital period of ~7000 days and $M_{c}sin(i)$ of $5.47^{+0.44}_{-0.43} M_{Jup}$. Additionally, we find that the transit timing variations of all of the WASP-4b transits cannot be explained by the second planet but can be explained with either a decaying orbit or apsidal precession, with a slight preference for orbital decay. Assuming the decay model is correct, we find an updated period of 1.338231587$\pm$0.000000022 days, a decay rate of -7.33$\pm$0.71 msec/year, and an orbital decay timescale of 15.77$\pm$1.57 Myr. If the observed decay results from tidal dissipation, we derive a modified tidal quality factor of $Q^{'}_{*}$ = 5.1$\pm$0.9$\times10^4$, which is an order of magnitude lower than values derived for other hot Jupiter systems. However, more observations are needed to determine conclusively the cause of WASP-4b's changing orbit and confirm the existence of an outer companion. ","Characterizing the WASP-4 system with TESS and radial velocity data:
  Constraints on the cause of the hot Jupiter's changing orbit and evidence of
  an outer planet"
26,1473311659926163465,1079172757009514496,Kristina Wicke,"['New paper out on the arXiv: \n<LINK>\n\nJoint work with Mareike Fischer and @Laura_Kubatko: ""Effects of discordance between species and gene trees on phylogenetic diversity conservation"".']",https://arxiv.org/abs/2112.10717,"Phylogenetic diversity indices such as the Fair Proportion (FP) index are frequently discussed as prioritization criteria in biodiversity conservation. They rank species according to their contribution to overall diversity by taking into account the unique and shared evolutionary history of each species as indicated by its placement in an underlying phylogenetic tree. Traditionally, phylogenetic trees were inferred from single genes and the resulting gene trees were assumed to be a valid estimate for the species tree, i.e., the ""true"" evolutionary history of the species under consideration. However, nowadays it is common to sequence whole genomes of hundreds or thousands of genes, and it is often the case that conflicting genealogical histories exist in different genes throughout the genome, resulting in discordance between individual gene trees and the species tree. Here, we analyze the effects of gene and species tree discordance on prioritization decisions based on the FP index. In particular, we consider the ranking order of taxa induced by (i) the FP index on a species tree, and (ii) the expected FP index across all gene tree histories associated with the species tree. On one hand, we show that for particular tree shapes, the two rankings always coincide. On the other hand, we show that for all leaf numbers greater than or equal to five, there exist species trees for which the two rankings differ. Finally, we illustrate the variability in the rankings obtained from the FP index across different gene tree and species tree estimates for an empirical multilocus mammal data set. ","Effects of discordance between species and gene trees on phylogenetic
  diversity conservation"
27,1473278175337717762,297011825,Jacob Deasy,"['New paper ""Heavy-tailed denoising score matching"" <LINK> with @simidjievskin and @pl219_Cambridge is out! We extend DSM beyond Gaussian noise to improve generative performance, avoid class imbalances, and motivate L√©vy process diffusion! <LINK>']",https://arxiv.org/abs/2112.09788,"Score-based model research in the last few years has produced state of the art generative models by employing Gaussian denoising score-matching (DSM). However, the Gaussian noise assumption has several high-dimensional limitations, motivating a more concrete route toward even higher dimension PDF estimation in future. We outline this limitation, before extending the theory to a broader family of noising distributions -- namely, the generalised normal distribution. To theoretically ground this, we relax a key assumption in (denoising) score matching theory, demonstrating that distributions which are differentiable almost everywhere permit the same objective simplification as Gaussians. For noise vector norm distributions, we demonstrate favourable concentration of measure in the high-dimensional spaces prevalent in deep learning. In the process, we uncover a skewed noise vector norm distribution and develop an iterative noise scaling algorithm to consistently initialise the multiple levels of noise in annealed Langevin dynamics (LD). On the practical side, our use of heavy-tailed DSM leads to improved score estimation, controllable sampling convergence, and more balanced unconditional generative performance for imbalanced datasets. ",Heavy-tailed denoising score matching
28,1473231243693240320,65213099,Filippo Vicentini,"['Check out our paper accompanying the latest new release of @NetKetOrg : 3.3, coming with a lot of new features such as chunking, time evolution drivers, continuous systems support, full summation states and much more! <LINK>\n\npip install --upgrade netket\n1/N', 'The paper comes with several discussed examples for the new and past features. An highlight is one using Heun solver to study a quench, courtesy of Damian Hofmann. You can also use adaptive solvers with error estimation induced by the QGT! Check out https://t.co/robXsflDeo', ""Don't like our TDVP implementation and want full control? Sec 4.4 is for you! The loop for dynamics is only 20 python lines long. And you can also use our #jax-based pytree-compatible RK integrators! (BTW @SingularMattrix  i'd love to see diffeq4jax one day)"", ""Want to study continuous systems? Very fresh new feature contributed by Gabriel in Sec 6! And the hessian won't blow up your memory thanks to chunking the computation in small blocks. Natural-gradient compatible! just `vstate.chunk_size = 1024` and tune this number"", ""Want to study excited states? Group-convolutional neural networks are a powerful tool, with a tutorial at Sec 7.3! You can define arbitrary discrete groups. [ps: they aren't as bad as people claim those days... just remember to use complex parameters if you have a sign]"", 'Lindblad master equation? Dynamics and steady-state are supported as long as your dissipators are K-local. Sections 4.2 and 8.2. And with qutip integration checking your solutions for small systems is easier than ever. just `netket_object.to_qutip()`.', 'Wonder why your architecture is not converging but unsure where to start? Use the new `vqs.ExactState` to perform calculations with full summation over the hilbert space to eliminate sampling errors. The interface is the same! You can even do dynamics with it.', ""Wonder why NetKet can do Natural Gradient/SR on a lattice of 1000 spins with a network with 1 million parameters on a single GPU without exploding the memory? Read Appendix B on our QGT implementation doing 'Fisher-vector products'."", ""Would you like to see new features? Join us! Have you published a new algo or NQS architecture?  Contribute it! https://t.co/YOsetuyTRl We're growing and always looking for new contributors to the project!""]",https://arxiv.org/abs/2112.10526,"We introduce version 3 of NetKet, the machine learning toolbox for many-body quantum physics. NetKet is built around neural-network quantum states and provides efficient algorithms for their evaluation and optimization. This new version is built on top of JAX, a differentiable programming and accelerated linear algebra framework for the Python programming language. The most significant new feature is the possibility to define arbitrary neural network ans\""atze in pure Python code using the concise notation of machine-learning frameworks, which allows for just-in-time compilation as well as the implicit generation of gradients thanks to automatic differentiation. NetKet 3 also comes with support for GPU and TPU accelerators, advanced support for discrete symmetry groups, chunking to scale up to thousands of degrees of freedom, drivers for quantum dynamics applications, and improved modularity, allowing users to use only parts of the toolbox as a foundation for their own code. ",NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems
29,1473012510311665664,1149154781195206657,Rahul Jayaraman,"['New paper day! ‚ú®\n\n<LINK>', 'In this paper, we evaluate whether or not a particular star, observed by TESS, has a ""rigidly rotating magnetosphere."" In these early B-type stars, plasma builds up between the Kepler co-rotation radius and the Alfv√©n (or magnetospheric) radius, creating a dense magnetosphere.', 'The rapid rotation of these stars, combined with the dense magnetosphere, leads to rotationally-modulated hydrogen line emission. One of the ""slam-dunk"" signatures of these stars is the presence of a ""double-horned"" emission profile around H-alpha or the Brackett series (n=4).', 'However, identifying these stars in the past required significant targeted spectropolarimetric observations. This, of course, is very time-consuming, with no guaranteed chance of success! In this paper, we argue that TESS light curves can be used to detect these stars.', 'These RRM stars show very distinctive signatures in their light curves. We could potentially train a classifier to sniff out these stars -- which would allow folks to propose targeted spectropolarimetric observations of these stars, rather than relying on some form of serendipity', 'Much of this paper is dedicated to the follow-up work we did on this one particular star to show that yes, this is indeed a viable process that can reliably detect these stars (TESS light curve --&gt; targeted spectropolarimetry --&gt; analysis of magnetospheric parameters)', 'The one caveat is that because the hydrogen-line emission is rotationally modulated, the star may show different emission signatures at different phases. As a result, observations would still need to be taken at different phases to find evidence for the ""double-horned"" emission.', ""However, our work suggests that analysis of TESS light curves can basically give us a potential target list for spectropolarimetric follow-up with little effort. And isn't that, after all, what most astronomers want? ;)"", 'For some context, this is what the light curves of RRM stars look like. The top panel is HD 135348 (""our"" star), the middle 4 are other  known RRM stars (ones displaying H-alpha emission), and the bottom one is a ""typical"" star with a run-of-the-mill magnetosphere -- not an RRM. https://t.co/jq4u2diTCI']",https://arxiv.org/abs/2112.07676,"We report the detection and characterization of a new magnetospheric star, HD 135348, based on photometric and spectropolarimetric observations. The TESS light curve of this star exhibited variations consistent with stars known to possess rigidly rotating magnetospheres (RRMs), so we obtained spectropolarimetric observations using the Robert Stobie Spectrograph (RSS) on the Southern African Large Telescope (SALT) at four different rotational phases. From these observations, we calculated the longitudinal magnetic field of the star $\langle B_z \rangle$, as well as the Alfv\'en and Kepler radii, and deduced that this star contains a centrifugal magnetosphere. However, an archival spectrum does not exhibit the characteristic ""double-horned"" emission profile for H$\alpha$ and the Brackett series that has been observed in many other RRM stars. This could be due to the insufficient rotational phase coverage of the available set of observations, as the spectra of these stars significantly vary with the star's rotation. Our analysis underscores the use of TESS in photometrically identifying magnetic star candidates for spectropolarimetric follow-up using ground-based instruments. We are evaluating the implementation of a machine learning classifier to search for more examples of RRM stars in TESS data. ","Could the Magnetic Star HD 135348 Possess a Rigidly Rotating
  Magnetosphere?"
30,1473011163638030339,15311609,James Reed,"[""Interested in learning the design principles and technical decisions that went into PyTorch's new `torch.fx` program transformation framework? Learn all that and more from our new paper on arXiv <LINK>"", ""@Dr_flerken @PyTorch Can you elaborate? I just whipped up a small transform in terms of the `nn.Transformer` module: https://t.co/c7ZDbsIQOa. That works. Would be glad to look into whatever issue you're having"", '@RaineyCode Hi @RaineyCode, let us know any abbreviations/terms that are not easy to understand from the paper and we can help!']",https://arxiv.org/abs/2112.08429,"Modern deep learning frameworks provide imperative, eager execution programming interfaces embedded in Python to provide a productive development experience. However, deep learning practitioners sometimes need to capture and transform program structure for performance optimization, visualization, analysis, and hardware integration. We study the different designs for program capture and transformation used in deep learning. By designing for typical deep learning use cases rather than long tail ones, it is possible to create a simpler framework for program capture and transformation. We apply this principle in torch.fx, a program capture and transformation library for PyTorch written entirely in Python and optimized for high developer productivity by ML practitioners. We present case studies showing how torch.fx enables workflows previously inaccessible in the PyTorch ecosystem. ","Torch.fx: Practical Program Capture and Transformation for Deep Learning
  in Python"
31,1472949320735416322,881543426,Dr. Chase Worley,['New paper posted on the arXiV <LINK>'],https://arxiv.org/abs/2112.09613,We introduce Hadamard matrices whose entries are quaternionic. We then go on to provide classification of quaternionic Hadamard matrices of circulant core of orders 2 through 5. We also introduce quaternionic Hadamard matrices of Butson type and ways to create quaternionic Hadamard matrices from real and complex Hadamard matrices. Examples are shown that showcase how Hadamard matrices over the quaternions are richer than Hadamard matrices over the complex numbers. ,On Some Quaternionic Hadamard Matrices of Small Order
32,1472932698834731008,795877354266456064,KoheiKamadaPhys,"['New paper appeared today! \n<LINK>\nWe have shown that (the exponent of) the vacuum decay rate is evaluated in the Lorentzian formalism with the help of Picard-Lefschetz theory. Now the real dynamics of the vacuum decay is clearer.', 'While in the Euclidean formalism decay rate is evaluated only for the ""critical"" bubbles, in the Lorentzian formalism the decay rate for the bubble with any size is calculable. \nTakumi did a great job for this study.']",https://arxiv.org/abs/2112.09284,"We apply the Lorentzian path integral to the decay of a false vacuum and estimate the false-vacuum decay rate. To make the Lorentzian path integral convergent, the deformation of an integral contour is performed by following the Picard-Lefschetz theory. We show that the nucleation rate of a critical bubble, for which the corresponding bounce action is extremized, has the same exponent as the Euclidean approach. We also extend our computation to the nucleation of a bubble larger or smaller than the critical one to which the Euclidean formalism is not applicable. ",Vacuum decay in the Lorentzian path integral
33,1472930170600243200,1016659760672886785,Rishit Dagli #KubeCon,"['1/ Happy to announce our paper ""CPPE-5: Medical Personal Protective Equipment Dataset"", a üßµ\n\nWe introduce a new challenging image dataset with the goal to allow the study of subordinate categorization of medical PPE unlike any existing dataset\n\nPaper: <LINK> <LINK>', '2/ Trying to do so is not possible with other popular large-scale data sets that focus on broad-level categories. We also find that currently there exists no such dataset and critique the few *similar* datasets that either consist of ‚Ä¶', '3/ artificial images, focus only on a single category (eg. mask), far less data, or do not focus on medical scenarios altogether which I believe is super helpful\n\nThe dataset also allows easily deploying in complex scenes with other objects as well due to the following features', '4/ The dataset contains:\n- high-quality images and annotations (~4.7 bbox/image)\n- real-life images unlike any current dataset\n- a majority of non-iconic images in their natural context often with multiple other objects\n- annotations for 5 PPE items https://t.co/CbqkEEWvXg', '5/ We perform algorithmic analysis on the dataset and open-source some of the top-performing models on this dataset in the code repository: https://t.co/M0SarshXob\n\nThe PyTorch and TensorFlow data loader, tutorial to load this dataset, a training tutorial and more are in the repo https://t.co/z7k49fFCeU', '6/ We are thankful to @Google @TensorFlow for a generous research grant and TPU Research Cloud for providing access to TPUs\n\ncc: lovely collaborating with @ialimustufa']",https://arxiv.org/abs/2112.09569,"We present a new challenging dataset, CPPE - 5 (Medical Personal Protective Equipment), with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories (such as PASCAL VOC, ImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained on this dataset to be used in practical scenarios in complex scenes, our dataset mainly contains images that show complex scenes with several objects in each scene in their natural context. The image collection for this dataset focusing on: obtaining as many non-iconic images as possible and making sure all the images are real-life images unlike other existing datasets in this area. Our dataset includes 5 object categories (coveralls, face shield, gloves, mask, and goggles) and each image is annotated with a set of bounding boxes and positive labels. We present a detailed analysis of the dataset in comparison to other popular broad category datasets as well as datasets focusing on personal protective equipments, we also find that at present there exist no such publicly available datasets. Finally we also analyze performance and compare model complexities on baseline and state-of-the-art models for bounding box results. Our code, data, and trained models are available at this https URL . ",CPPE-5: Medical Personal Protective Equipment Dataset
34,1471988546340675584,168652692,Kathy Vivas,"['Antlia 2 is one of the largest satellite galaxies of the Milky Way, as large as the Magellanic Clouds, but with an extremely low surface brightness.  It is so diffuse that it remained hidden until just a few years ago. Our new paper on Antlia 2 is out: <LINK>', 'Challenging object. Not only is ultra diffuse but it is also far away from us, and located at a low galactic latitude, so huge foreground contamination. DECam + RRLyrae stars are an ideal combination to study the galaxy. We found &gt;300 RR Lyrae stars in Antlia 2! https://t.co/mdjARSjbcq', 'We needed 4 DECam fields to cover the galaxy and found RR Lyrae stars over the full field. Most likely we are missing stars outside our footprint. Just remember how big is the field of view of DECam compared with a full Moon. Antlia 2 is really huge! https://t.co/UQNDoN7ASh', 'RR Lyrae stars are standard candles and allowed us to get a distance to Antlia 2 of 124 kpc. We found the galaxy is elongated along the line of sight, with one of the sides of the galaxy closer to us than the other, in agreement with models in which Ant 2 is disrupting. https://t.co/3XmtaaVgsv', 'Antlia 2 was first discovered by the detection of 3 RR Lyrae stars from Gaia which were close in the sky. Funny thing: once the stellar population of the galaxy was identified those RRLyrae stars did not seem to belong to the galaxy. They were too close!', 'With our full variability search we are now pretty sure those stars were not actually RR Lyrae stars, but Anomalous Cepheids in Antlia 2, which are brighter than RR Lyrae stars. Antlia 2 has 8 anomalous Cepheids. https://t.co/VhHWlSuanl', 'Thanks for reading! This paper was possible thanks to the hard work of a great team! @ClaraMarvaz , Alistair Walker, @sazabi_li, Vasily Belokurov and @deniserkal. Cannot end this thread without showing some r, i light curves (I ‚ù§Ô∏è light curves!) https://t.co/SBgoLCJk2S']",https://arxiv.org/abs/2112.08467,"We report 350 pulsating variable stars found in four DECam fields ($\sim 12$ sq. deg.) covering the Antlia 2 satellite galaxy. The sample of variables includes 318 RR Lyrae stars and eight anomalous Cepheids in the galaxy. Reclassification of several objects designated previously to be RR Lyrae as Anomalous Cepheids gets rid of the satellite's stars intervening along the line of sight. This in turn removes the need for prolific tidal disruption of the dwarf, in agreement with the recently updated proper motion and peri-centre measurements based on Gaia EDR3. There are also several bright foreground RR Lyrae stars in the field, and two distant background variables located $\sim 45$ kpc behind Antlia 2. We found RR Lyrae stars over the full search area, suggesting that the galaxy is very large and likely extends beyond our observed area. The mean period of the RRab in Antlia 2 is 0.599 days, while the RRc have a mean period of 0.368 days, indicating the galaxy is an Oosterhoff-intermediate system. The distance to Antlia 2 based on the RR Lyrae stars is $124.1$ kpc ($\mu_0=20.47$) with a dispersion of $5.4$ kpc. We measured a clear distance gradient along the semi-major axis of the galaxy, with the South-East side of Antlia 2 being $\sim13$ kpc farther away from the North-West side. This elongation along the line of sight is likely due to the ongoing tidal disruption of Ant 2. ",Variable Stars in the giant satellite galaxy Antlia 2
35,1471936826847813636,4902145390,Gordan Krnjaic,"['Cheers to a heroic effort by Rodolfo Capdevilla @rmcapdevilla along with David Curtin @drc83 and Yoni Kahn in our new paper to comprehensively test all singlet particles that can resolve the muon g-2 anomaly <LINK>', 'There are (too) many new physics models proposed to address g-2, so why another paper about this? Most models involve heavy (&gt; 100 GeV) electroweak charged particles that we know how to test at colliders.', 'However, there is a special class of ""singlet"" models that involve no new electroweak particles in their leading contribution to g-2. These particles need only couple to the muon and can live anywhere between the MeV (from cosmology) and TeV scales (from unitarity)', ""this class of models is much more predictive since the effect on g-2 only depends on one particle's mass and coupling to the muon. We find that a combined program of fixed-target, B-factory, LHC, and future *muon* collider searches can get test nearly every possible model""]",https://arxiv.org/abs/2112.08377,"We comprehensively study all viable new-physics scenarios that resolve the muon $(g-2)_\mu$ anomaly with only Standard Model singlet particles coupled to muons via renormalizable interactions. Since such models are only viable in the MeV -- TeV mass range and require sizable muon couplings, they predict abundant accelerator production through the same interaction that resolves the anomaly. We find that a combination of fixed-target (NA64$\mu$, $M^3$), $B$-factory (BABAR, Belle II), and collider (LHC, muon collider) searches can cover nearly all viable singlets scenarios, independently of their decay modes. In particular, future muon collider searches offer the only certain test of singlets above the GeV scale, covering all higher masses up to the TeV-scale unitarity limit for these models. Intriguingly, we find that $\mathcal{O}(100~\mathrm{GeV})$ muon colliders may yield better coverage for GeV-scale singlets compared to TeV-scale concepts, which has important implications for the starting center-of-mass energy of a staged muon collider program. ",Systematically Testing Singlet Models for $(g-2)_\mu$
36,1471890295247810560,3276611372,Vidit Nanda,"[""Here's a medium-sized thread about this brand new paper with the lovely @GesineReinert and our amazing PhD student Tadas Temƒçinas. I'll tell you a bit about what's in this paper, and then a bit about what's not in this paper: \n\n<LINK>\n\n(1/12)"", 'Computing (simplicial) homology efficiently requires matrix diagonalisation of boundary maps in chain complexes which encode incidence relations among the various simplices.\n\n(2/12) https://t.co/UiI7QoDbmy', ""When we have lots of simplices, the boundary operators get huge and complexity soars cubically. To fix this, one uses discrete Morse theoretic reductions. See @pi_ene's nice tutorial on DMT here: https://t.co/pp4Pwz9xQN\n\n(3/12)"", 'An immediate question is: what is the smallest reduced chain complex you could get by performing such reductions on a given simplicial complex? This is a difficult optimisation problem replete with lots of scary-looking np-hardness papers:\n\nhttps://t.co/CwoXAltREh\n\n(4/12)', 'Our paper takes a cheerier approach! We quantify the behaviour of a specific type of discrete Morse function (i.e., lexicographical) on clique (i.e., Rips) complexes of Erdos-Renyi random graphs to measure what benefit one should ""expect at random"" as a null model.\n\n(5/12) https://t.co/0aqhE26Pbk', ""In particular, we give a central limit theorem (CLT) for the joint distribution of critical simplices -- ones that can't be reduced away from the chain complex -- for lexicographical discrete Morse functions on random clique complexes:\n\n(6/12) https://t.co/J3UOSC2vN7"", 'The random variables involved here are topologically natural but probabilistically weird: nearby simplex indicators are dependent, faraway ones are independent. So we needed a new abstract multivariate CLT to approximate these...\n\n(7/12) https://t.co/MISGP6H7Jl', ""The secret ingredient in all of this is Stein's method, which gives a cool way to characterise multivariate Gaussian distributions! This is bread-and-butter to @GesineReinert and now to Tadas, but the fact that this characterisation works is voodoo to me.\n\n(8/12) https://t.co/MOzSXLe43t"", 'From this abstract CLT we are able to build Gaussian approximations for other topologically interesting random variables, eg: U-statistics, clique counts and (my personal favourite) the number of simplices in the link of a given simplex of a random simplicial complex.\n\n(9/12)', ""Okay, so what's *not* in the paper? Tadas's original PhD project. He worked very hard on it, producing miracles every week, simplifying the task more and more until it turned out to depend (non-trivially!) on a well-known, hard open problem. I was dejected, but... \n\n(10/12)"", ""Tadas was a champion! He scheduled an autopsy meeting with 3 questions: (1) what went wrong, (2) what can we learn, and (3) what's next. This is far more mature than anything I've done during my many PhD ups and downs. He's landed on his feet, as you can see.\n\n(11/12)"", ""The closest thing to an overarching moral lesson that I've learned from all this is: sure, be tenacious and give your research your best shot. But also, know where the parachutes are, and don't hesitate to use them if it becomes necessary. Godspeed :)\n\n(12/12)"", 'PS: another moral lesson is that discrete Morse theory is awesome. On the flag complex of a G(n,p) graph, the ratio of expected values E(critical simplices)/E(all simplices) is 1/n, with the variance of critical simplices being smaller than that of all simplices.\n\n(13/12)']",https://arxiv.org/abs/2112.08922,"Acyclic partial matchings on simplicial complexes play an important role in topological data analysis by facilitating efficient computation of (persistent) homology groups. Here we describe probabilistic properties of critical simplex counts for such matchings on clique complexes of Bernoulli random graphs. In order to accomplish this goal, we generalise the notion of a dissociated sum to a multivariate setting and prove an abstract multivariate central limit theorem using Stein's method. As a consequence of this general result, we are able to extract central limit theorems not only for critical simplex counts, but also for generalised U-statistics (and hence for clique counts in Bernoulli random graphs) as well as simplex counts in the link of a fixed simplex in a random clique complex. ","A Multivariate CLT for Dissociated Sums with Applications to
  U-Statistics and Random Complexes"
37,1471857638220644353,1153187867897860096,Nikita Nikolaev,"['I‚Äôve just finished writing a new #paper which generalises the humble but powerful Implicit Function Theorem to asymptotic situations, bringing in modern techniques of #resurgence and Borel resummation. #maths\n<LINK>']",http://arxiv.org/abs/2112.08792,"We prove an Asymptotic Implicit Function Theorem in the setting of Gevrey asymptotics with respect to a parameter. The unique implicitly defined solution admits a Gevrey asymptotic expansion and furthermore it is the Borel resummation of the corresponding implicitly defined formal power series solution. The main theorem can therefore be rephrased as an Implicit Function Theorem for Borel summable power series. As an application, we give a diagonal or Jordan decomposition for holomorphic matrices in Gevrey asymptotic families. ",Gevrey Asymptotic Implicit Function Theorem
38,1471826011809538049,945445796098473984,Patrick Schnider,"['A new paper on the @arxiv. In this one, with coauthors from a recent workshop, we consider partitions of complete geometric graphs into almost plane subgraphs.\n\n<LINK>']",http://arxiv.org/abs/2112.08456,"Recently, the second and third author showed that complete geometric graphs on $2n$ vertices in general cannot be partitioned into $n$ plane spanning trees. Building up on this work, in this paper, we initiate the study of partitioning into beyond planar subgraphs, namely into $k$-planar and $k$-quasi-planar subgraphs and obtain first bounds on the number of subgraphs required in this setting. ",Edge Partitions of Complete Geometric Graphs (Part 2)
39,1471823867404509194,824380220367060992,Dr. Amy McGovern,['Our new paper on AI and Ethics for environmental sciences is on arxiv - submitted to EDS for first issue &amp; next paper in prep for AIES!  Fun and important work with @DJGagneDos @Iebertu Ann Bostrom @ai2enviro #AIEthics #ArtificialIntelligence #wxtwitter\n<LINK>'],https://arxiv.org/abs/2112.08453,"Given the growing use of Artificial Intelligence (AI) and machine learning (ML) methods across all aspects of environmental sciences, it is imperative that we initiate a discussion about the ethical and responsible use of AI. In fact, much can be learned from other domains where AI was introduced, often with the best of intentions, yet often led to unintended societal consequences, such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system. A common misconception is that the environmental sciences are immune to such unintended consequences when AI is being used, as most data come from observations, and AI algorithms are based on mathematical formulas, which are often seen as objective. In this article, we argue the opposite can be the case. Using specific examples, we demonstrate many ways in which the use of AI can introduce similar consequences in the environmental sciences. This article will stimulate discussion and research efforts in this direction. As a community, we should avoid repeating any foreseeable mistakes made in other domains through the introduction of AI. In fact, with proper precautions, AI can be a great tool to help {\it reduce} climate and environmental injustice. We primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences. ","The Need for Ethical, Responsible, and Trustworthy Artificial
  Intelligence for Environmental Sciences"
40,1471809213471117313,2656302854,Ronald Drimmel üá∫üá¶,"['New coauthered paper on the ArXiv today on the Radcliff Wave, led by Lekshmi Thulasidharan.  We find that the kinematics of young objects indeed show evidence of wave-like motion, but that these vertical motions extend well beyond the Radcliff Wave itself.\n<LINK> <LINK>', ""I also just noticed that the author list is appropriately ordered: The first three are women (who did all the work) followed by four old guys who gave lots of suggestions.  ü§¶\u200d‚ôÇÔ∏è (How's that for #OpenScience and #OverlyHonestMethods?)"", ""Oh wait! Silly me. @mac0598 isn't an old guy! Excuse me as I help myself out the door. ü§¶\u200d‚ôÇÔ∏èü§¶\u200d‚ôÇÔ∏è"", ""@rareflwr41 @mac0598 Well, you're right.  I was speaking a bit tongue in cheek. We were all having too much fun on this project to feel old, and are fortunate that Elena and Lekshmi invited us to participate.""]",https://arxiv.org/abs/2112.08390,"The Radcliffe Wave (RW) is a recently discovered sinusoidal vertical feature of dense gas in the proximity of the Sun. In the disk plane, it is aligned with the Local Arm. However, the origin of its vertical undulation is still unknown. This study constrains the kinematics of the RW, using young stars and open clusters as tracers, and explores the possibility of this oscillation being part of a more extended vertical mode. We study the median vertical velocity trends of the young stars and clusters along with the RW and extend it further to the region beyond it. We discover a kinematic wave in the Galaxy, distinct from the warp, with the amplitude of oscillation depending on the age of the stellar population. We perform a similar analysis in the N-body simulation of a satellite as massive as the Sagittarius dwarf galaxy impacting the galactic disk. When projected in the plane, the spiral density wave induced by the satellite impact is aligned with the RW, suggesting that both may be the response of the disk to an external perturbation. However, the observed kinematic wave is misaligned. It appears as a kinematic wave travelling radially, winding up faster than the density wave matched by the RW, questioning its origin. If a satellite galaxy is responsible for this kinematic wave, we predict the existence of a vertical velocity dipole that should form across the disk and this may be measurable with the upcoming Gaia DR3 and DR4. ",Evidence of a vertical kinematic oscillation beyond the Radcliffe Wave
41,1471788963316871172,804001347431497729,Graphs and Networks by S. R. Kingan,['Posted a new paper on arxiv!\n<LINK>'],https://arxiv.org/abs/2112.08825,"For $k \ge 4$, let $Q_{2k}$ and $V_{2k}$ denote the ladder and M\""obius ladder on $2k$ vertices, respectively. We prove results that build on a result by Wormald that states that any cyclically $4$-connected cubic graph other than $Q_8$ or $V_8$ is obtained from a smaller cyclically $4$-connected cubic graph by bridging a pair of non-adjacent edges. We introduce the concept of cycle spread, which generalizes the edge pair distance defined by Wormald, and show that the set of pairs of edges that needs to be considered in order to obtain all cyclically $4$-connected cubic graphs is smaller than the set of all pairs of non-adjacent edges. We prove that all non-planar cyclically $4$-connected cubic graphs with at least $10$ vertices, other than the M\""obius ladders and the Petersen graph, are obtained from $Q_8$ by bridging pairs of edges with cycle spread at least $(1,2)$. Moreover every graph obtained in this way is non-planar, cyclically $4$-connected, and cubic. All planar cyclically $4$-connected cubic graphs with at least $10$ vertices except for the ladders are obtained from the ladders by bridging pairs of edges with cycle spread at least $(1,2)$. We implemented an algorithm based on these results using McKay's nauty system for isomorphism checking. ",On cyclically 4-connected cubic graphs
42,1471775625367236608,956539964795301889,Jacopo Bertolotti,"['New paper on @arxiv!\n""Ghost Image Processing"" <LINK>\nA short thread about what it is about.\n1/', 'Ghost imaging is an imaging modality where, instead of recording the light on a pixellated camera, you send a sequence of (known) patterns on the object and measure the transmitted/reflected/etc light with a ""bucket"" detector.\n2/\nhttps://t.co/DuVC1GuRXS', 'The idea is that you are representing the object using the basis formed by your sequence of patterns, and the light you measure are the coefficients of the expansion. But what if you use a different basis to reconstruct the image than the one you used to illuminate the object?\n3/', 'What you get is not the original image, but the image multiplied by a matrix (which in the end is the same as saying your image convolved with some kernel).\nAs you can freely choose which bases to use, you can freely choose which kernel to convolve your image with.\n4/', 'Convolving with a kernel is a common image processing technique to extract information (e.g. find the edges) from an image. Obviously you could do the convolution step after measuring the image, but if you have noise it is going to mess with your convolution.\n5/', 'On the other hand, if you measure directly the ""processed"" image, you keep the artefacts due to noise to a minimum.\nHow much better is this? It depends a lot on how much noise you have, and what kind of noise you have. But it is a nice degree of freedom to keep in mind üòÉ\n6/6', '@schrodingerskit This explanation works great with anyone who is very familiar with the concept of basis and change of basis. And terribly with everyone else.\nExplaining is always a matter of finding a balance between different audiences. Sometimes I do it better, sometimes I mess it up üòâ', '@wangtnetlab As per first tweet in the thread: ""Ghost Image Processing""', '@schrodingerskit I often think about the value (or lack of value) of full-frontal teaching, but so far I didn\'t manage to make up my mind (and most ""brilliant solutions"" I see only work in a fairy universe where unicorns roam free).\nTeaching is hard.', '@bruko 2D\nYou can get a 3D image if you both send a sequence of known patterns to illuminate AND measure the resulting image with a camera.\n(There are other tricks that also work, but this is the simplest.)\n\nps\nTotally not a stupid question üôÇ', '@HEURISTICS19 Usually? None.\nUsing the technique we propose? Any.\n\nWe use an edge-detection kernel (equation 3 in the paper) as an example to show the principle.', '@riemannsurface1 @arxiv Actually, those were good puns. Pity they were not intended.', '@GAntoniciello @arxiv Thanks üòÉ']",https://arxiv.org/abs/2112.07671,"In computational ghost imaging the object is illuminated with a sequence of known patterns, and the scattered light is collected using a detector that has no spatial resolution. Using those patterns and the total intensity measurement from the detector, one can reconstruct the desired image. Here we study how the reconstructed image is modified if the patterns used for the reconstruction are not the same as the illumination patterns, and show that one can choose how to illuminate the object, such that the reconstruction process behaves like a spatial filtering operation on the image. The ability to measure directly a processed image, allows one to bypass the post-processing steps, and thus avoid any noise amplification they imply. As a simple example we show the case of an edge-detection filter. ",Ghost Image Processing
43,1471750676392857602,969131557075156992,Julien Berestycki,"['New paper : <LINK>', 'The branching Brownian motion (or BBM) is the simplest branching particle system one can think of: particles move around as Brownian motions and spawn new particles at rate 1. (there are many interesting variants of this process).', '-&gt; Natural model of expanding population / growth diffusion phenomena\n-&gt; Dual (in 1d) to the celebrated Fisher-Kolmogorov-Pterovskii-Piskunov equation (F-KPP for short), one of the simplest PDE with traveling wave solutions.', 'So how fast does this bad boy grow? In 1d we know a lot: if we call M_t the position of the rightmost particle, then M_t/t \\to \\sqrt 2 with proba. 1. And Bramson proved in 1983 that in fact M_t is around \\sqt 2 t -\\frac{3}{2\\sqrt 2} \\log t .']",https://arxiv.org/abs/2112.08407,"We consider a branching Brownian motion in $\mathbb{R}^d$ with $d \geq 1$ in which the position $X_t^{(u)}\in \mathbb{R}^d$ of a particle $u$ at time $t$ can be encoded by its direction $\theta^{(u)}_t \in \mathbb{S}^{d-1}$ and its distance $R^{(u)}_t$ to 0. We prove that the {\it extremal point process} $\sum \delta_{\theta^{(u)}_t, R^{(u)}_t - m_t^{(d)}}$ (where the sum is over all particles alive at time $t$ and $m^{(d)}_t$ is an explicit centring term) converges in distribution to a randomly shifted decorated Poisson point process on $\mathbb{S}^{d-1} \times \mathbb{R}$. More precisely, the so-called {\it clan-leaders} form a Cox process with intensity proportional to $D_\infty(\theta) e^{-\sqrt{2}r} ~\mathrm{d} r ~\mathrm{d} \theta $, where $D_\infty(\theta)$ is the limit of the derivative martingale in direction $\theta$ and the decorations are i.i.d. copies of the decoration process of the standard one-dimensional branching Brownian motion. This proves a conjecture of Stasi\'nski, Berestycki and Mallein (Ann. Inst. H. Poincar\'{e} 57:1786--1810, 2021), and builds on that paper and on Kim, Lubetzky and Zeitouni (arXiv:2104.07698). ","The extremal point process of branching Brownian motion in
  $\mathbb{R}^d$"
44,1471746106757304321,90047221,Robin Kothari,"['New paper on the arXiv with Daochen Wang, @aarthims, @akapoor_av8r, and @MartinQuantum on Quantum Algorithms for Reinforcement Learning with a Generative Model. Presented at ICML 2021. Summary: No more than  a quadratic quantum speedup in this model.\n<LINK> <LINK>']",https://arxiv.org/abs/2112.08451,"Reinforcement learning studies how an agent should interact with an environment to maximize its cumulative reward. A standard way to study this question abstractly is to ask how many samples an agent needs from the environment to learn an optimal policy for a $\gamma$-discounted Markov decision process (MDP). For such an MDP, we design quantum algorithms that approximate an optimal policy ($\pi^*$), the optimal value function ($v^*$), and the optimal $Q$-function ($q^*$), assuming the algorithms can access samples from the environment in quantum superposition. This assumption is justified whenever there exists a simulator for the environment; for example, if the environment is a video game or some other program. Our quantum algorithms, inspired by value iteration, achieve quadratic speedups over the best-possible classical sample complexities in the approximation accuracy ($\epsilon$) and two main parameters of the MDP: the effective time horizon ($\frac{1}{1-\gamma}$) and the size of the action space ($A$). Moreover, we show that our quantum algorithm for computing $q^*$ is optimal by proving a matching quantum lower bound. ",Quantum Algorithms for Reinforcement Learning with a Generative Model
45,1471727491006054402,60084334,Frank Verstraete,['Some of the most satisfying research results are the ones that become obvious once formulated in the right language. Check the new paper <LINK> of my student @Loolooke to see this principle at work in describing dualities with categories/tensor networks.'],http://arxiv.org/abs/2112.09091,"We present a systematic recipe for generating duality transformations in one-dimensional quantum lattice models with abelian, non-abelian or categorical symmetries. Dual models can be characterized by equivalent but distinct realizations of a given symmetry, encoded into fusion categories. A duality is described by a pair of distinct module categories, over these fusion categories, whose data give rise to distinct realizations of the full set of symmetric operators. In general, dual realizations of non-abelian symmetries give rise to non-invertible symmetries. The novelty of our approach is the explicit construction of all symmetric operators together with intertwiners, in the form of matrix product operators, that convert local symmetric operators of one realization into local symmetric operators of its dual. This guarantees that the structure constants of the algebra of all symmetric operators, the so-called bond algebra, are equal in both dual realizations. Concurrently, the intertwiners map local order operators into non-local disorder operators. Families of dual Hamiltonians are then designed by taking linear combinations of the corresponding symmetric operators. We illustrate this approach by establishing matrix product operator intertwiners for dualities such as Kramers-Wannier, Jordan-Wigner, Kennedy-Tasaki and the IRF-vertex correspondence, as well as for new ones in a model with the exotic Haagerup categorical symmetry. Finally, we comment on generalizations to higher dimensions. ","Dualities in one-dimensional quantum lattice models: symmetric
  Hamiltonians and matrix product operator intertwiners"
46,1471723686923227138,929973145,Dr David Sobral üí´üåå,"['New paper just accepted in ApJüì°:üí´The LEGA-C of nature and nurture in stellar populations of galaxies at z~0.6-1.0üí´: D4000 and H-delta reveal different assembly histories for quiescent galaxies in different environments:  <LINK>  üåå <LINK>', 'Galaxy evolution is driven by a variety of physical processes that proceed at different rates for different dark matter haloes and environments. A record of this evolution is preserved in galaxy stellar populations, which we can access using absorption-line spectroscopy üî≠:üëá https://t.co/3UQyD2BxE5', 'In this paper we explore the outstanding LEGA-C survey (DR3; see https://t.co/VN78t0MsMl) to investigate the role of the environment and stellar mass on stellar populations (using D4000 and H-delta) at z~0.6-1.0 in the COSMOS field, from poor fields to some rich clusters üëá https://t.co/8Cq2Mm4l3s', 'We separate galaxies in terms of star-forming and quiescent, and also centrals and satellites, and leverage the statistical power and depth of LEGA-C to investigate the roles of stellar mass and environment in setting the quiescent fraction, 7 Gyrs ago: https://t.co/RMJXYA4peM', 'We reveal significant gradients  in D4000 and H-delta equivalent width (EW) distributions over the stellar mass vs environment 2D spaces at z~0.6-1.0. D4000 and H-delta EWs primarily depend on mass, but they also *depend on environment at fixed stellar mass* for massive galaxies: https://t.co/oijL6AA2xY', 'By splitting the sample into star-forming galaxies and quiescent galaxies we reveal that the significant environmental trends of D4000 and H-delta EW when controlling for stellar mass are *fully* driven by quiescent galaxies üò≤üëá https://t.co/Cfw7wIam9u', 'Perhaps surprisingly: ü§î regardless of being centrals or satellites, star-forming galaxies reveal D4000 and H-delta EWs which depend strongly on their stellar mass and are completely independent of the environment at 0.6&lt;z&lt;1.0: https://t.co/fvn1vVlY3X', 'Interestingly, the *environmental trends seen for satellite galaxies are fully driven by the trends that hold only for quiescent galaxies*, combined with the strong environmental dependency of the quiescent fraction at fixed stellar mass üåå https://t.co/F7lwhXnYL4', 'What next? LEGA-C allows to go beyond just 2 indicators. By modelling the full star formation histories of each galaxy we will be able to gain a much deeper insight, so stay tuned and check out other recent LEGA-C papers (+ download the public catalogue https://t.co/VN78t0MsMl) https://t.co/4FOxqXiyv0']",https://arxiv.org/abs/2112.08372,"Galaxy evolution is driven by a variety of physical processes which are predicted to proceed at different rates for different dark matter haloes and environments across cosmic times. A record of this evolution is preserved in galaxy stellar populations, which we can access using absorption-line spectroscopy. Here we explore the large LEGA-C survey (DR3) to investigate the role of the environment and stellar mass on stellar populations at z~0.6-1.0 in the COSMOS field. Leveraging the statistical power and depth of LEGA-C, we reveal significant gradients in D4000 and H-delta equivalent widths (EWs) distributions over the stellar mass vs environment 2D spaces for the massive galaxy population (M>10^10 M$_{\odot}$) at z~0.6-1.0. D4000 and H-delta EWs primarily depend on stellar mass, but they also depend on environment at fixed stellar mass. By splitting the sample into centrals and satellites, and in terms of star-forming galaxies and quiescent galaxies, we reveal that the significant environmental trends of D4000 and H-delta EW when controlling for stellar mass are driven by quiescent galaxies. Regardless of being centrals or satellites, star-forming galaxies reveal D4000 and H-delta EWs which depend strongly on their stellar mass and are completely independent of the environment at 0.6<z<1.0. The environmental trends seen for satellite galaxies are fully driven by the trends that hold only for quiescent galaxies, combined with the strong environmental dependency of the quiescent fraction at fixed stellar mass. Our results are consistent with recent predictions from simulations that point towards massive galaxies forming first in over-densities or the most compact dark matter haloes. ","The LEGA-C of nature and nurture in stellar populations of galaxies at
  z~0.6-1.0: D4000 and H-delta reveal different assembly histories for
  quiescent galaxies in different environments"
47,1471592968712818692,110103071,Andrej Risteski,"['New paper on the landscape and training dynamics of VAEs when trained on data supported on low-dimensional manifolds. Joint work with Fred Koehler, @thebigmehtaphor and Chenghui Zhou.  <LINK> <LINK>', 'Story starts w a very nice paper by Dai-Wipf https://t.co/Z8KW3CjEbf, who propose a 2-stage training algo for VAEs. Stage 1: Heuristic analysis of the standard VAE loss suggests optimal generator is supptd on the right manifold. Stage 2: Recovers the right density on manifold.', 'Via a combination of theory+empirics, we show the full picture for Stage 1 is subtle. Even in the linear case---i.e. data is generated by a linear generator, and trained gen/enc are linear--- there exist minima s.t. generator support is a superset of the ground truth manifold.', 'Training dynamics matter: in the linear case, we prove implicit bias of the GD dynamics is such that the generator recovers the right support. In the nonlinear case, via simulations, we show VAE training frequently recovers bad minima --- even for simple low-dim manifolds.', 'Feels like there is a lot more to the story: e.g. maybe under more data distribution assumptions, something like this could work?  Sample quality does improve in D-W --- is there a sense in which it ""partially"" works? Is there a ""multi-stage"" version of the strategy in D-W?']",https://arxiv.org/abs/2112.06868,"Variational Autoencoders (VAEs) are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower dimensional manifold. Recent work by Dai and Wipf (2019) suggests that on low-dimensional data, the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. In this paper, via a combination of theoretical and empirical results, we show that the story is more subtle. Precisely, we show that for linear encoders/decoders, the story is mostly true and VAE training does recover a generator with support equal to the ground truth manifold, but this is due to the implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that the VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold. ","Variational autoencoders in the presence of low-dimensional data:
  landscape and implicit bias"
48,1471422004930748416,989097257986293760,Benjamin Steinegger,"['New paper led by @giulioburgio with @_AlexArenas. We study how homophily in the vaccine adoptions affects the spreading dynamics. We find three distinct regimes for the dependence of the epidemic size on the level of homophily. 1/6 \n<LINK>', 'More specifically, we find that, depending on vaccine efficacy,  vaccine adoption and epidemic pressure,  homophily can increase, decrease or affect non monotonously the epidemic size. 2/6', 'We previously found these three regimes  in the case of digital proximity tracing. Furthermore, we show that these regimes naturally emerge for any prophylactic tool that reduces the transmission probability such as face masks or social distancing. 3/6\n\nhttps://t.co/DF8mSf8TXQ', 'Just yesterday, we became aware that @takayukihir, @abbas_k_rizi, @bolozna and @JariSaramaki were working on the same problem. Their findings are very much equivalent to ours. 4/6\nhttps://t.co/44mvYSGFMx', 'While we consider a leaky vaccine in the mean-field case and in a physical contact network, they consider an all-or-nothing vaccine in random networks. This two studies combined illustrate how robust the phenomenology is. 5/6', 'Interesting to see people working on the same topic independently. They also did a similar study to ours on digital proximity tracing and found equivalent results. \nShout-out to al the people involved in these two studies! 6/6\nhttps://t.co/p495bAZ0mH']",https://arxiv.org/abs/2112.08240,"Physical contacts do not occur randomly, rather, individuals with similar socio-demographic and behavioural characteristics are more likely to interact among them, a phenomenon known as homophily. Concurrently, the same characteristics correlate with the adoption of prophylactic tools. As a result, the latter do not unfold homogeneously in a population, affecting their ability to control the spread of infectious diseases. Here, focusing on the case of vaccines, we reveal three different dynamical regimes as a function of the mixing rate between vaccinated and non vaccinated individuals. Specifically, depending on the epidemic pressure, vaccine coverage and efficacy, we find the attack rate to decrease, increase or vary non monotonously with respect to the mixing rate. We corroborate the phenomenology through Monte Carlo simulations on a temporal physical contact network. Besides vaccines, our findings hold for a wide range of prophylactic tools, indicating a universal mechanism in spreading dynamics ",Homophily impacts the success of vaccine roll-outs
49,1471285451352801280,1001049754787368960,Dr. Yu-Dai Tsai,"['New paper with Josh Eby and Marianna Safronova:\n<LINK>\n""SpaceQ - Direct Detection of Ultralight Dark Matter with Space Quantum Sensors""', '@dr_guangtou @Yeqzids', '@jmgeventhorizon', '@jmgeventhorizon Thank you! I will get back to you regarding the recording! Sorry that it took so long. Also, it would be great to talk with you about this new paper!', ""@jmgeventhorizon Wait, let's maybe finish the previous one first :D Sorry that it took so long but will definitely send you the comments regarding that. Thanks again!""]",https://arxiv.org/abs/2112.07674,"Recent advances in quantum sensors, including atomic clocks, enable searches for a broad range of dark matter candidates. The question of the dark matter distribution in the Solar system critically affects the reach of dark matter direct detection experiments. Partly motivated by the NASA Deep Space Atomic Clock (DSAC), we show that space quantum sensors present new opportunities for ultralight dark matter searches, especially for dark matter states bound to the Sun. We show that space quantum sensors can probe unexplored parameter space of ultralight dark matter, covering theoretical relaxion targets motivated by naturalness and Higgs mixing. If an atomic clock were able to make measurements on the interior of the solar system, it could probe this highly sensitive region directly and set very strong constraints on the existence of such a bound-state halo in our solar system. We present sensitivity projections for space-based probes of ultralight dark matter which couples to electron, photon, and gluon fields, based on current and future atomic, molecular, and nuclear clocks. ","SpaceQ -- Direct Detection of Ultralight Dark Matter with Space Quantum
  Sensors"
50,1471239122308124672,927974200274427904,Bruce Macintosh,"['Actual science thread: I wanted to highlight a new paper by our group in a different area than normal - astrometric detection of Earth-like planets, the limits imposed by stellar activity, and how to overcome those limits. \nKaplan-Lipkin et al 2021, <LINK>', 'Spectra of Earth-like worlds orbiting sun-like stars will require the big mission recommended by #astro2020. Interpreting those spectra will require planetary masses. \nThat could be through RV/Doppler, but that‚Äôs  hard due to stellar activity. (Smart people are working on this!)', 'Astrometric planet detection is an alternative. The sun moves around the solar system barycenter by more than its own diameter  in a complex pattern driven mostly by Jupiter but with a small component due to the Earth  (Figure form Malbet et al 2011) https://t.co/QekfdPrswO', 'Technologically this is much harder but that‚Äôs just engineering :) Previous work has shown that stellar activity has much less effect on astrometry. We wanted to quantify that and look at whether it could be further reduced.', 'We estimate this by looking at our sun, but we can‚Äôt use real solar data; ground-based datasets have gaps, and space-based data is usually in narrowbands picked to maximize the effects of sunspots - very different than broadband astrometry.', 'Shapiro et al (https://t.co/ZWlv8iLgbE) have a wonderful paper where they generated a long-duration (30+ year) synthetic model of the sun, populating it with observed sunspots and features to generate synthetic images at all wavelengths.', 'They kindly shared their data, and we converted those into temporal power spectra and in turn to detectable exoplanet limits in the star-limited case. The results were (consistent with previous studies) really good - you can easily see 0.1 objects in 1 AU orbits around the sun. https://t.co/FgIkywxixB', 'But what if you want to do better, or what if you‚Äôre looking at a messier star? (We know the sun is quieter than many stars.) Starspots and bright faculae have different spectra than the disk of the sun, so the effect on the photocenter will vary with wavelength.', 'Generally the photocenter moves less at redder wavelengths, but the motions remain somewhat correlated. (Plot shows photocenter motion in different epochs between different combos of wavelengths - Gaia BGR, Total solar irradiance, and a near-IR ""SJ"" band) https://t.co/wgakx4kYxf', 'By combining observations at different wavelengths, you can further attenuate the stellar activity noise by a factor of ten - down below Pluto mass for a 1 AU orbit. Or see Earths around much more active stars. (Comparison between Gaia G and vs combining G and R) https://t.co/BLNANFMzxR', 'Of course, building a telescope that can do this is hard but there are concepts - LVUOIR, SIM, Toliman, NEAT. We highlight how multi band astrometry can be very helpful for distinguishing between different noise sources.', 'It‚Äôs a very nice paper by a very sharp student, Avi Kaplan-Lipkin, one of several great Stanford @kipac1 students applying to graduate schools near you.']",https://arxiv.org/abs/2112.06383,"Astrometry has long been a promising technique for exoplanet detection. At the theoretical limits, astrometry would allow for the detection of smaller planets than previously seen by current exoplanet search methods, but stellar activity may make these theoretical limits unreachable. Astrometric jitter of a Sun-like star due to magnetic activity in its photosphere induces apparent variability in the photocenter of order $0.5\ \textrm{m}R_\odot$. This jitter creates a fundamental astrophysical noise floor preventing detection of lower-mass planets in a single spectral band. By injecting planet orbits into simulated solar data at five different passbands, we investigate mitigation of this fundamental astrometric noise using correlations across passbands. For a true solar analog and a planet at 1 au semimajor axis, the $6\sigma$ detection limit set by stellar activity for an ideal telescope at the best single passband is $0.01$ Earth masses. We found that pairs of passbands with highly correlated astrometric jitter due to stellar activity, but with less motion in the redder band, enable higher-precision measurements of the common signal from the planet. Using this method improves detectable planet masses at 1 au by up to a factor of $10$, corresponding to at best $0.005$ Earth masses for a Sun-like star with a perfect telescope. Given these results, we recommend that future astrometry missions consider proceeding with two or more passbands to reduce noise due to stellar activity. ","Multiwavelength Mitigation of Stellar Activity in Astrometric Planet
  Detection"
51,1471206237899411458,1326946405378764801,Mohan Sarovar,"['New paper on the arXiv today:\n""Quantum simulation of weak-field light-matter interactions""\n\n<LINK>\n\n1/3', 'We construct an approach to simulate response functions of materials interacting with quantum fields. The key innovation is a way to simulate the effect of continuum fields with a small, discrete number of controllable bosonic modes.\n \n2/3', 'Any trapped-ion or circuit-QED groups interested in an experimental demonstration?\n\n3/3']",https://arxiv.org/abs/2112.07177,"Simulation of the interaction of light with matter, including at the few-photon level, is important for understanding the optical and optoelectronic properties of materials, and for modeling next-generation non-linear spectroscopies that use entangled light. At the few-photon level the quantum properties of the electromagnetic field must be accounted for with a quantized treatment of the field, and then such simulations quickly become intractable, especially if the matter subsystem must be modeled with a large number of degrees of freedom, as can be required to accurately capture many-body effects and quantum noise sources. Motivated by this we develop a quantum simulation framework for simulating such light-matter interactions on platforms with controllable bosonic degrees of freedom, such as vibrational modes in the trapped ion platform. The key innovation in our work is a scheme for simulating interactions with a continuum field using only a few discrete bosonic modes, which is enabled by a Green's function (response function) formalism. We develop the simulation approach, sketch how the simulation can be performed using trapped ions, and then illustrate the method with numerical examples. Our work expands the reach of quantum simulation to important light-matter interaction models and illustrates the advantages of extracting dynamical quantities such as response functions from quantum simulations. ",Quantum simulation of weak-field light-matter interactions
52,1471198011531268100,349215461,Ken Shen,"[""1/ New paper thread!  This one was led by Thomas Fitzpatrick, who worked on it with me when he was an undergrad at Berkeley.  He's now a master's student at UChicago currently applying for PhD programs.  Keep an eye out for him!\n\n<LINK>"", '2/ Explosions of white dwarfs with masses roughly that of the Sun (""sub-Chandrasekhar-mass"" models) have become a leading scenario for Type Ia supernovae for a wide variety of reasons.  However, the production of neutron-rich iron-group isotopes has remained a problem.', '3/ By that, I mean isotopes of elements near iron on the periodic table that have one or two more neutrons than protons in their nuclei: e.g., iron-54 (28 neutrons vs. 26 protons), cobalt-55 (28 vs. 27), and nickel-57 (29 vs. 28).', ""4/ Sub-Chandrasekhar-mass models produce less neutron-rich stuff than models with heavier white dwarfs, and (apparently) less neutron-rich stuff than we infer from observations.  But it's been unclear how robust those results are to uncertainties in thermonuclear reaction rates."", '5/ The rates we measure in terrestrial labs are typically not directly applicable to astrophysical settings.  We have to extrapolate them to the parameter space of interest and/or use theory to calculate them, so they can have very large uncertainties.', '6/ Thomas took an explosion model of a 1 Solar mass white dwarf and re-ran it a ton of times with different multiplicative factors for each reaction rate, bracketing the range of uncertainties.', '7/ The bright regions of this figure show the isotopes that are most affected when reactions are increased by a factor of 10.  There are some interesting changes to elements between magnesium and silicon. https://t.co/tv1FEs29vV', ""8/ But the iron-group elements seem pretty immune to changes in the nuclear reaction rates.  That makes some sense, since a lot of this material is produced in nuclear statistical equilibrium, where the exact rates don't matter."", ""9/ So, nuclear reaction rate uncertainties don't appear to be the answer to the issue of neutron-rich nucleosynthesis in exploding white dwarfs of 1 Solar mass."", ""10/ However, below 1 Solar mass, exploding white dwarfs produce more material out of nuclear statistical equilibrium.  So we'd expect reaction rate uncertainties to play a much larger role!  Guess there's more work to be done in the future..."", ""11/ It's also not 100% definite (to me) that observations require a lot of neutron-rich material.  Some of the observations are sort of inconclusive, and in some cases, Chandrasekhar-mass models also don't do a great job."", ""12/12 The issue of neutron-rich nucleosynthesis is one of the strongest pieces of evidence against sub-Chandra Type Ia models, but it isn't a slam dunk.  Given all the other evidence supporting sub-Chandra explosions, I think (hope?) this issue will get resolved in the future.""]",https://arxiv.org/abs/2112.06951,"The precise nature of Type Ia supernova (SN Ia) progenitors remains a mystery, but the detonation of a sub-Chandrasekhar-mass white dwarf (WD) has become a promising candidate. There is a growing body of work suggesting that the carbon core detonation of a sub-Chandrasekhar-mass WD can be triggered by the detonation of a helium shell accreted from a companion WD, through either inward shock convergence near the center or direct edge-lit detonation. This ""double-detonation"" SN Ia can be triggered by a small helium shell and is therefore well approximated by the detonation of a bare carbon-oxygen white dwarf (C/O WD). The impacts of uncertainties in experimentally and theoretically determined nuclear reaction rates on nucleosynthesis in the detonations of sub-Chandrasekhar-mass WDs have not yet been fully explored. We investigate the sensitivity of this model to nuclear reaction rate uncertainties to better constrain the nucleosynthetic yields resulting from these phenomena and identify the nuclear reaction rates whose uncertainties have the most significant impacts on nucleosynthesis. We find that the chemical abundances, and in particular those of the iron-group elements, are relatively insensitive to these nuclear reaction rate uncertainties. ","Impacts of Nuclear Reaction Rate Uncertainties on Sub-Chandrasekhar-Mass
  White Dwarf Detonations"
53,1471175148975890445,865092946684674048,Piotr Tempczyk,['Our new paper on semi-supervised semantic segmentation with Dominik Filipiak and Marek Cygan is on @arxiv <LINK>'],https://arxiv.org/abs/2112.07528,"We present n-CPS - a generalisation of the recent state-of-the-art cross pseudo supervision (CPS) approach for the task of semi-supervised semantic segmentation. In n-CPS, there are n simultaneously trained subnetworks that learn from each other through one-hot encoding perturbation and consistency regularisation. We also show that ensembling techniques applied to subnetworks outputs can significantly improve the performance. To the best of our knowledge, n-CPS paired with CutMix outperforms CPS and sets the new state-of-the-art for Pascal VOC 2012 with (1/16, 1/8, 1/4, and 1/2 supervised regimes) and Cityscapes (1/16 supervised). ","n-CPS: Generalising Cross Pseudo Supervision to n Networks for
  Semi-Supervised Semantic Segmentation"
54,1471169152421347333,166527685,Riccardo Di Clemente,"['üöÄüöÄüöÄ Can we understand how #consensus emerged in #Reddit on the #GMESQUEEZE? check our new paper <LINK>\nTo the moon with @Gius_C, Anna Mancini, Antonio Desiderio üöÄüöÄüöÄ ! <LINK>']",https://arxiv.org/abs/2112.07059,"The short squeeze of GameStop (GME) shares in mid-January 2021, primarily orchestrated by retail investors of the Reddit r/wallstreetbets community, represents a paramount example of collective coordination action on social media, resulting in large-scale consensus formation and significant market impact. In this work we characterise the structure and time evolution of Reddit conversation data, showing that the occurrence and sentiment of GME-related comments (representing how much users are engaged with GME) increased significantly much before the short squeeze actually took place. We then introduce a model of opinion dynamics where user engagement can trigger a self-reinforcing mechanism leading to the emergence of consensus on the short squeeze operation. We observe a clear phase transition from heterogeneous to homogeneous opinions as engagement grows, with the presence of hubs easing the formation of diffuse consensus. Our results shed light on the increasingly important phenomenon of self-organized collective actions taking place on social networks. ","Self-induced emergence of consensus in social networks: Reddit and the
  GameStop short squeeze"
55,1471164612460650498,751326416495517697,Nandan Thakur,"['üö®New Paper Alertüö®\nWe present üî•GPL: an effective approach to domain adapt your dense retriever!\n\nPreviously inüçªBEIR: dense retrievers unable to generalize on OOD domains. GPL solves this efficiently, without any labeled data and 9 nDCG@10 ‚¨ÜÔ∏èon BEIRüöÄ\n\n<LINK> <LINK>']",https://arxiv.org/abs/2112.07577,"Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at this https URL ","GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of
  Dense Retrieval"
56,1471138693771673604,373885410,Athul Paul Jacob,"['‚≠êNew paper‚≠ê\nHow do you build AI agents that are both strong and human-like? Regularize search towards a human policy! In chess, Go and no-press Diplomacy, we get SOTA human prediction accuracy while being much stronger than imitation learning.\n<LINK>\n(1/9)üßµüëá <LINK>', 'Even though self-play AI algorithms based on search exceed top humans in several games, the resulting policies are often inhuman. While imitation learning is effective at predicting human actions, it does not match the strength of expert humans.\n(2/9)', 'We show that KL-regularizing search towards a human imitation-learned policy results in strong and human-like gameplay in perfect-information games like chess and Go as well as in imperfect-information games like no-press Diplomacy.\n(3/9)', 'In chess and Go, we show that standard MCTS (green) with a human imitation learned policy prior and value function surpasses prior state-of-the-art results (blue) for human prediction accuracy, while also being substantially stronger than the imitation learned policies.\n(4/9) https://t.co/6bZ9JTVyCu', 'MCTS is ineffective in imperfect-information games like Poker and Diplomacy. In these games, algorithms based on regret minimization are the leading approaches.\n(5/9)', 'We introduce piKL-hedge, the first regret minimization algorithm to incorporate a cost term proportional to the KL divergence between the search policy and a human-imitation learned policy. We study piKL-hedge in no-press Diplomacy.\n(6/9)', 'piKL-Hedge (green) can produce policies that predict human play with the same accuracy as imitation learning (blue) while being stronger by a factor of 1.4;  or alternately a policy that outperforms unregularized search (yellow) while achieving greater prediction accuracy.\n(7/9) https://t.co/M1Zf9nGjKN', 'Huge thanks to all my awesome co-authors, @lightvector1*, @gabrfarina*, @adamlerer, @anton_bakhtin, @jacobandreas, and @polynoamial!\n(8/9)', 'This work was done during my internship with the FAIR multi-agent learning group (who are hiring interns!). Special thanks to @polynoamial for hosting me!\n(9/9)', '@egrefen Certainly related! KL-based objectives have been used extensively in RL. Our focus in this paper has been towards studying how KL can be leveraged in different inference-time search-based methods to allow for stronger agents while also improving their ""human-likeness"".', '@egrefen Exactly!']",https://arxiv.org/abs/2112.07544,"We consider the task of building strong but human-like policies in multi-agent decision-making problems, given examples of human behavior. Imitation learning is effective at predicting human actions but may not match the strength of expert humans, while self-play learning and search techniques (e.g. AlphaZero) lead to strong performance but may produce policies that are difficult for humans to understand and coordinate with. We show in chess and Go that regularizing search based on the KL divergence from an imitation-learned policy results in higher human prediction accuracy and stronger performance than imitation learning alone. We then introduce a novel regret minimization algorithm that is regularized based on the KL divergence from an imitation-learned policy, and show that using this algorithm for search in no-press Diplomacy yields a policy that matches the human prediction accuracy of imitation learning while being substantially stronger. ",Modeling Strong and Human-Like Gameplay with KL-Regularized Search
57,1471131626293932048,1215058162551873537,Elizabeth Bondi-Kelly,"['I\'m thrilled to share my @DeepMind internship work in our new #AAAI2022 paper: ""Role of Human-AI Interaction in Selective Prediction"" with Raphael Koster, @hannahsheahan, @MartinJChadwick, @yorambac, @TaylanCemgilML, @ulrichpaquet, and @DjDvij, <LINK> (1/4) <LINK>', 'Selective prediction systems learn to defer to humans when the predictions of the AI are unreliable. Rather than rely on historical labels or simulation as in prior work, we test these systems with real human participants in the domain of camera trap data for #conservation. (2/4)', ""We find evidence that humans are influenced by information about the selective prediction system's prediction and decision to defer, implying that similar studies are needed before deploying selective prediction systems in high-stakes domains. (3/4)"", ""Thanks to all co-authors and collaborators at @DeepMind for such a wonderful internship experience! I can't wait to see where this research takes us next. (4/4)""]",https://arxiv.org/abs/2112.06751,"Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation. However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves. We show that this is not the case by performing experiments to quantify human-AI interaction in the context of selective prediction. In particular, we study the impact of communicating different types of information to humans about the AI system's decision to defer. Using real-world conservation data and a selective prediction system that improves expected accuracy over that of the human or AI system working individually, we show that this messaging has a significant impact on the accuracy of human judgements. Our results study two components of the messaging strategy: 1) Whether humans are informed about the prediction of the AI system and 2) Whether they are informed about the decision of the selective prediction system to defer. By manipulating these messaging components, we show that it is possible to significantly boost human performance by informing the human of the decision to defer, but not revealing the prediction of the AI. We therefore show that it is vital to consider how the decision to defer is communicated to a human when designing selective prediction systems, and that the composite accuracy of a human-AI team must be carefully evaluated using a human-in-the-loop framework. ",Role of Human-AI Interaction in Selective Prediction
58,1471131442709155843,748500295345385472,Carlos Arg√ºelles üåà,['This new paper (<LINK>) was lots of fun to write and work on with @IbrahimSafa1 @pochoarus and my new student Pavel Z!'],https://arxiv.org/abs/2112.06937,"High-energy muon- and electron-neutrinos yield a non-negligible flux of tau neutrinos as they propagate through Earth. In this letter, we address the impact of this additional component in the PeV and EeV energy regimes for the first time. This contribution is predicted to be significantly larger than the atmospheric background above 300 TeV, and alters current and future neutrino telescopes' capabilities to discover a cosmic tau-neutrino flux. Further we demonstrate that Earthskimming neutrino experiments, designed to observe tau neutrinos, will be sensitive to cosmogenic neutrinos even in extreme scenarios without a primary tau-neutrino component. ",Tau Appearance from High-Energy Neutrino Interactions
59,1471105516701114373,1562913787,Nathan Moynihan,['New paper out today: <LINK>'],https://arxiv.org/abs/2112.07556,"We study the variance in the measurement of observables during scattering events, as computed using amplitudes. The classical regime, characterised by negligible uncertainty, emerges as a consequence of an infinite set of relationships among multileg, multiloop amplitudes in a momentum-transfer expansion. We discuss two non-trivial examples in detail: the six-point tree and the five-point one-loop amplitudes in scalar QED. We interpret these relationships in terms or a coherent exponentiation of radiative effects in the classical limit which generalises the eikonal formula, and show how to recover the impulse, including radiation reaction, from this generalised eikonal. Finally, we incorporate the physics of spin into our framework. ",The Uncertainty Principle and Classical Amplitudes
60,1471052426413322240,1273473804438667265,Haritz Puerto,"['New preprint!! How to combine pretrained QA agents to create a multi-agent model that can solve any question?\n\nCheck our paper ""MetaQA: Combining Expert Agents for Multi-Skill Question Answering""\nPaper: <LINK>\nw/ @gozde_gul_sahin and @IGurevych at @UKPLab #NLProc <LINK>', '@gozde_gul_sahin @IGurevych @UKPLab Code: https://t.co/0R18KO4Lif #MachineLearning #NLProc']",https://arxiv.org/abs/2112.01922,"The recent explosion of question answering (QA) datasets and models has increased the interest in the generalization of models across multiple domains and formats by either training on multiple datasets or by combining multiple models. Despite the promising results of multi-dataset models, some domains or QA formats may require specific architectures, and thus the adaptability of these models might be limited. In addition, current approaches for combining models disregard cues such as question-answer compatibility. In this work, we propose to combine expert agents with a novel, flexible, and training-efficient architecture that considers questions, answer predictions, and answer-prediction confidence scores to select the best answer among a list of answer candidates. Through quantitative and qualitative experiments we show that our model i) creates a collaboration between agents that outperforms previous multi-agent and multi-dataset approaches in both in-domain and out-of-domain scenarios, ii) is highly data-efficient to train, and iii) can be adapted to any QA format. We release our code and a dataset of answer predictions from expert agents for 16 QA datasets to foster future developments of multi-agent systems on this https URL ",MetaQA: Combining Expert Agents for Multi-Skill Question Answering
61,1470958559827529740,108681761,William Keel,"['Paper day! Accepted for MNRAS: two new distant AGN-ionized clouds, at least one from faded AGN. Paper w/@AlexeiMoiseev1 et al. 128 galaxies in TELPERION survey imaged @SARA_Obs, followup with 1.5-2.5-6m telescopes in Russia. \n<LINK> <LINK>', 'I finally found some of these clouds myself! However, Dutch colleagues will quickly note that ""Keel voorwerp"" would be an inappropriately confusing way to refer to them (albeit pretty amusing).']",https://arxiv.org/abs/2112.07084,"We present a narrowband [O III] imaging survey of 111 AGN hosts and 17 merging-galaxy systems, in search of distant extended emission-line regions (EELRs) around AGN (either extant or faded). Our data reach deeper than detection from the broadband SDSS data, and cover a wider field than some early emission-line surveys used to study extended structure around AGN. Spectroscopic followup confirms two new distant AGN-ionized clouds, in the merging systems NGC 235 and NGC 5514, projected at 26 and 75 kpc from the nuclei (respectively). We also recover the previously-known region in NGC 7252. These results strengthen the connection between EELRs and tidal features; kinematically quiescent distant EELRs are virtually always photoionized tidal debris. We see them in ~10% of the galaxies in our sample with tidal tails. Energy budgets suggest that the AGN in NGC 5514 has faded by >3 times during the extra light-travel time ~250,000 years from the nucleus to the cloud and then to the observer; strong shock emission in outflows masks the optical signature of the AGN. For NGC 235 our data are consistent with but do not unequivocally require variation over ~85,000 years. In addition to these very distant ionized clouds, luminous and extensive line emission within four galaxies - IC 1481, ESO 362-G08, NGC 5514, and NGC 7679. IC 1481 shows apparent ionization cones, a rare combination with its LINER AGN spectrum. In NGC 5514, we measure a 7-kpc shell expanding at ~370 km/s west of the nucleus. ","The TELPERION Survey for Distant [O III] Clouds around Luminous and
  Hibernating AGN"
62,1470925560985575426,169081481,Hanno Rein üí´,"[""üö® New paper! It's the first radial velocity detection of a circumbinary planet (Kepler 16). The lead on this was Amaury Triaud. <LINK>"", 'Unfortunately, not a lot of interesting dynamics going on here. Keplerian orbits were good enough to fit the data. üí´']",https://arxiv.org/abs/2112.06584,"The radial velocity method is amongst the most robust and most established means of detecting exoplanets. Yet, it has so far failed to detect circumbinary planets despite their relatively high occurrence rates. Here, we report velocimetric measurements of Kepler-16A, obtained with the SOPHIE spectrograph, at the Observatoire de Haute-Provence's 193cm telescope, collected during the BEBOP survey for circumbinary planets. Our measurements mark the first radial velocity detection of a circumbinary planet, independently determining the mass of Kepler-16~(AB)~b to be $0.313 \pm 0.039\,{\rm M}_{\rm Jup}$, a value in agreement with eclipse timing variations. Our observations demonstrate the capability to achieve photon-noise precision and accuracy on single-lined binaries, with our final precision reaching $\rm 1.5~m\,s^{-1}$ on the binary and planetary signals. Our analysis paves the way for more circumbinary planet detections using radial velocities which will increase the relatively small sample of currently known systems to statistically relevant numbers, using a method that also provides weaker detection biases. Our data also contain a long-term radial velocity signal, which we associate with the magnetic cycle of the primary star. ","BEBOP III. Observations and an independent mass measurement of Kepler-16
  (AB) b -- the first circumbinary planet detected with radial velocities"
63,1470808493548576774,3667426463,Xinting Yu,"['New paper on Arxiv by my student Austin Dymont: <LINK>, we just submitted this paper to ApJ but wanna get it out before JWST. We use HST transmission spectroscopy observations to search trends between exoplanet haziness vs planetary/stellar parameters. A thread:', '(1/12) Our focus is exoplanets with equilibrium temperature &lt; 1000 K, where photochemical hazes are the dominant opacity sources flattening the transmission spectra. The goal is to compile existing observations to understand why some planets appear to be hazier than the others.', '(2/12) To quantify the haziness of each exoplanet, we use a ‚Äúwater amplitude‚Äù metric, by normalizing the water feature strength of the HST/WFC3 transmission spectra. We then did a blind search to find trend(s) between the haziness metric and various planetary/stellar properties.', '(3/12) First takeaway, with the addition of new exoplanet data (23 vs 6 in previous work), previously identified linear trend no long hold: no more linear trend between exoplanet haziness and equilibrium temperature, planet radius, and atmospheric hydrogen/helium fractions.', '(4/12) This doesn‚Äôt mean there is no trend though -- for example, we identify a quadratic trend between exoplanet haziness and Teq in my nature astronomy paper: https://t.co/VobfkhRaq6, which suggests planets with lower Teq actually clear up due to unique hazes properties.', '(5/12) 2nd takeaway: there are indeed some better linear trends we found. The best trends we find are exoplanet haziness vs. atmospheric scale height, planet surface gravity, and bulk density. Puffier atmospheres, and lower surface gravity and density lead to hazier exoplanets.', '(6/12) We also identified some tentative linear trends for stellar forcing parameters, between exoplanet haziness vs. planet orbital eccentricity and stellar age. Planets with more circular orbits and orbiting younger stars tend to be hazier.', '(7/12) Third takeaway: we also use a simple microphysics motivated model to see if we can analytically find trends between exoplanet haziness and a combination of planetary parameters ‚Äî and we find a linear trend between exoplanet haziness vs. sqrt(T)/g.', '(8/12) However, we noticed with the addition of new exoplanet data, it is harder to find a ‚Äúperfect‚Äù linear trend (even our best one is not great!). That means planet haziness is probably not simply controlled by a single or simple combination of planetary/stellar parameters.', '(9/12) We still need more observations + lab experiments + models to fully understand the complex physical and chemical processes that lead to the hazy atmospheres for warm exoplanets. That being said, whether hazy or not, I think transmission spectra data are always valuable!', '(10/12) I am an experimentalist studying exoplanet hazes but always wanna know how observations look like and how our experimental results can translate/explain those. That‚Äôs how this project starts, +many thanks to my amazing collaborators @planetKohno, Xi Zhang, and @jjfplanet.', '(11/12) As the field evolves with more transit observations, our results might also change. Thus, we make a public-available website archiving all the data: https://t.co/d0YWEeFjjQ (still half-developed). This enables us to add new observations + keep track of updated trends.', ""(12/12) I think that's it!""]",https://arxiv.org/abs/2112.06173,"Relatively little is understood about the atmospheric composition of temperate to warm exoplanets (equilibrium temperature $T_{\rm eq}<$ 1000 K), as many of them are found to have uncharacteristically flat transmission spectra. Their flattened spectra are likely due to atmospheric opacity sources such as planet-wide photochemical hazes and condensation clouds. We compile the transmission spectra of 23 warm exoplanets previously observed by the \textit{Hubble Space Telescope} and quantify the haziness of each exoplanet using a normalized amplitude of the water absorption feature ($A_{\rm H}$). By examining the relationships between $A_{\rm H}$ and various planetary and stellar forcing parameters, we endeavor to find correlations of haziness associated with planetary properties. Our analysis shows that the previously identified linear trends between $A_{\rm H}$ and $T_{\rm{eq}}$ or hydrogen-helium envelope mass fraction (f$_{\rm{HHe}}$) break down with the addition of new exoplanet data. Among all the parameters we investigated, atmospheric scale height ($H$), planet gravity ($g_{\rm p}$), and planet density ($\rho_{\rm p}$) hold the most statistically significant linear or linear logarithmic correlations with $A_{\rm H}$ ($p\leq0.02$). We also tentatively identified positive correlations for eccentricity ($e$) and stellar age ($t_{\rm age}$) with $A_{\rm H}$. Specifically, lower $H$, higher $g_{\rm p}$, $\rho_{\rm p}$, $e$, or $t_{\rm age}$ lead to clearer atmospheres. However, none of the parameters show very strong linear correlations with $A_{\rm H}$, suggesting that haziness in warm exoplanets is not simply controlled by any single planetary/stellar parameter. Additional observations and laboratory experiments are needed to fully understand the complex physical and chemical processes that lead to the hazy/cloudy atmospheres in warm exoplanets. ","Cleaning our Hazy Lens: Statistical Trends in Transmission Spectra of
  Warm Exoplanets"
64,1470803745034358790,1410352235196739585,Benjam√≠n Idini,['New Jupiter paper! @NASAJuno continues collecting data on Jupiter‚Äôs tides and we need theory and models to understand what we are seeing. Here we illuminate on Jupiter‚Äôs tides using fully analytical methods. Tides in rapidly rotating bodies are tough! <LINK> <LINK>'],http://arxiv.org/abs/2112.05901,"NASA's Juno mission recently reported Jupiter's high-degree (degree $\ell$, azimuthal order $m$ $=4,2$) Love number $k_{42}=1.289\pm0.063$ ($1\sigma$), an order of magnitude above the hydrostatic $k_{42}$ obtained in a nonrotating Jupiter model. After numerically modeling rotation, the hydrostatic $k_{42}=1.743\pm0.002$ is still $7\sigma$ away from the observation, raising doubts about our understanding of Jupiter's tidal response. Here, we use first-order perturbation theory to explain the hydrostatic $k_{42}$ result analytically. We use a simple Jupiter equation of state ($n=1$ polytrope) to obtain the fractional change in $k_{42}$ when comparing a rotating model with a nonrotating model. Our analytical result shows that the hydrostatic $k_{42}$ is dominated by the tidal response at $\ell=m=2$ coupled into the spherical harmonic $\ell,m=4,2$ by the planet's oblate figure. The $\ell=4$ normalization in $k_{42}$ introduces an orbital factor $(a/s)^2$ into $k_{42}$, where $a$ is the satellite semimajor axis and $s$ is Jupiter's average radius. As a result, different Galilean satellites produce a different $k_{42}$. We conclude that high-degree tesseral Love numbers ($\ell> m$, $m\geq2$) are dominated by lower-degree Love numbers and thus provide little additional information about interior structure, at least when they are primarily hydrostatic. Our results entail important implications for a future interpretation of the currently observed Juno $k_{42}$. After including the coupling from the well-understood $\ell=2$ dynamical tides ($\Delta k_2 \approx -4\%$), Jupiter's hydrostatic $k_{42}$ requires an unknown dynamical effect to produce a fractional correction $\Delta k_{42}\approx-11\%$ in order to fit Juno's observation within $3\sigma$. Future work is required to explain the required $\Delta k_{42}$. ",The lost meaning of Jupiter's high-degree Love numbers
65,1470770972496211977,14551614,Jason Weston,"[""üö®New paperüö® SOTA dialogue models are not winning Oscars anytime soon, as they cannot effectively stay in character. \n\nWe analyze and propose methods to measure &amp; mitigate -- but it's still an open problem.\n\n<LINK>\n@shtruk @JackUrbs Arthur Szlam @jaseweston <LINK>""]",https://arxiv.org/abs/2112.05843,"State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who is speaking can perform well; and further, these can be used as automated metrics. Finally, we evaluate a wide variety of mitigation methods, including changes to model architecture, training protocol, and decoding strategy. Our best models reduce mistaken identity issues by nearly 65% according to human annotators, while simultaneously improving engagingness. Despite these results, we find that maintaining character identity still remains a challenging problem. ","Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an
  Identity"
66,1470764141300355079,601568012,Dr. Michelle Ntampaka,"['We have a new paper out today!  tl;dr: if we‚Äôre careful about how we engineer our deep learning architectures, we can build models that are inherently more interpretable and trustworthy. \n\n<LINK>', 'We explore an ML approach to infer cosmology from cluster mock observations.  Figure 5 shows our architecture.  It mimics a human approach to the same problem, which makes our model inherently more interpretable! https://t.co/lB0x0OfkA8', 'We spend most of the paper interpreting.  We show that ML can be used to make a new discovery, rather than just being a tool to black-box your way from point A to point B.  Our model pointed us to a new self-calibration mode for x-ray surveys of galaxy clusters, in S 5.2.1', ""This is my favorite research thread I've ever pursued, and as a result, the paper long, both in time invested and ink spilt.  Good collaborators are hard to find, and it was a tremendous pleasure to work with @AlexeyVikhlinin on this research!""]",https://arxiv.org/abs/2112.05768,"We present a deep machine learning (ML) approach to constraining cosmological parameters with multi-wavelength observations of galaxy clusters. The ML approach has two components: an encoder that builds a compressed representation of each galaxy cluster and a flexible CNN to estimate the cosmological model from a cluster sample. It is trained and tested on simulated cluster catalogs built from the Magneticum simulations. From the simulated catalogs, the ML method estimates the amplitude of matter fluctuations, sigma_8, at approximately the expected theoretical limit. More importantly, the deep ML approach can be interpreted. We lay out three schemes for interpreting the ML technique: a leave-one-out method for assessing cluster importance, an average saliency for evaluating feature importance, and correlations in the terse layer for understanding whether an ML technique can be safely applied to observational data. These interpretation schemes led to the discovery of a previously unknown self-calibration mode for flux- and volume-limited cluster surveys. We describe this new mode, which uses the amplitude and peak of the cluster mass PDF as anchors for mass calibration. We introduce the term ""overspecialized"" to describe a common pitfall in astronomical applications of machine learning in which the ML method learns simulation-specific details, and we show how a carefully constructed architecture can be used to check for this source of systematic error. ","The Importance of Being Interpretable: Toward An Understandable Machine
  Learning Encoder for Galaxy Cluster Cosmology"
67,1470718986035908610,920320855577645056,Louis Legrand,"['If you are interested in next generation methods to measure the CMB lensing power spectrum, check out our new paper, today on arXiv: <LINK> <LINK>']",https://arxiv.org/abs/2112.05764,"Precise reconstruction of the cosmic microwave background lensing potential can be achieved with deep polarization surveys by iteratively removing lensing-induced $B$ modes. We introduce a lensing spectrum estimator and its likelihood for such optimal iterative reconstruction. Our modelling share similarities to the state-of-the-art likelihoods for quadratic estimator-based (QE) lensing reconstruction. In particular, we generalize the $N_L^{(0)}$ and $N_L^{(1)}$ lensing biases, and design a realization-dependent spectrum debiaser, making this estimator robust to uncertainties in the data modelling. We demonstrate unbiased recovery of the cosmology using map-based reconstructions, focussing on lensing-only cosmological constraints and neutrino mass measurement in combination with CMB spectra and acoustic oscillation data. We find this spectrum estimator is essentially optimal and with a diagonal covariance matrix. For a CMB-S4 survey, this likelihood can double the constraints on the lensing amplitude compared to the QE on a wide range of scales, while at the same time keeping numerical cost under control and being robust to errors. ","Lensing power spectrum of the Cosmic Microwave Background with deep
  polarization experiments"
68,1470703121462546433,338922968,Jess Thorne,"[""Today was a double paper day for @devilsurvey with a new paper from @astrowelshluke uploaded to arXiv! \n\nIf you're interested in the dispersion of the star-forming main-sequence and its evolution since z=0.7 or a fan of pretty plots have a read! \n<LINK>"", 'This is my favourite figure from the paper which is my nomination for @PlotAstro this week! https://t.co/OichgBF30N']",https://arxiv.org/abs/2112.06279,"We present the evolution of the star-formation dispersion - stellar mass relation ($\sigma_{SFR}$-M$_{\star}$) in the DEVILS D10 region using new measurements derived using the ProSpect spectral energy distribution fitting code. We find that $\sigma_{SFR}$-M$_{\star}$ shows the characteristic 'U-shape' at intermediate stellar masses from 0.1<z<0.7 for a number of metrics, including using the deconvolved intrinsic dispersion. A physical interpretation of this relation is the combination of stochastic star-formation and stellar feedback causing large scatter at low stellar masses and AGN feedback causing asymmetric scatter at high stellar masses. As such, the shape of this distribution and its evolution encodes detailed information about the astrophysical processes affecting star-formation, feedback and the lifecycle of galaxies. We find that the stellar mass that the minimum ${\sigma}_{SFR}$ occurs evolves linearly with redshift, moving to higher stellar masses with increasing lookback time and traces the turnover in the star-forming sequence. This minimum ${\sigma}_{SFR}$ point is also found to occur at a fixed specific star-formation rate (sSFR) at all epochs (sSFR~10$^{-9.6}$yr$^{-1}$). The physical interpretation of this is that there exists a maximum sSFR at which galaxies can internally self-regulate on the tight sequence of star-formation. At higher sSFRs, stochastic stellar processes begin to cause galaxies to be pushed both above and below the star-forming sequence leading to increased SFR dispersion. As the Universe evolves, a higher fraction of galaxies will drop below this sSFR threshold, causing the dispersion of the low-stellar mass end of the star-forming sequence to decrease with time. ","Deep Extragalactic VIsible Legacy Survey (DEVILS): Evolution of the
  $\sigma_{\mathrm{SFR}}$-M$_{\star}$ relation and implications for
  self-regulated star formation"
69,1470698239812935681,409363641,Daniel Russo,"['[NEW PAPER] Curious about why people become #developers? Our new study just accepted on @emsejournal with @andresmasegosa &amp; @kjstol  explores the relations between personality traits and need for cognition! Pre-print: <LINK> ... 1/6 <LINK>', 'To run the analysis, we used #Bayesian statistics. The use of Bayesian approaches can be very complex and confusing. In this paper, we show how to run a Bayesian multi-model linear regression using @JASPStats with replication package: https://t.co/r4V19y1b35 ... 2/6', 'Take aways: 1) If someone scores high in need for cognition, there is a higher chance to become a developer. 2) To attract software professionals, organizations should provide cognitive recruitment messages and not emotional ones ... 3/6', '3) Software team composition with an individual high in need for cognition enhances task-relevant information and collaborative team identification, potentially leading to better team performance ... 4/6', '4) Developers tend to generate new ideas and are open to them. They are sensitive to organizational fairness and have a high work ethic. Also, individual innovation behavior is likely to be high ... 5/6', 'Thanks, @LeroCentre @scienceirel, for the support provided! 6/6']",https://arxiv.org/abs/2112.06610,"There is considerable anecdotal evidence suggesting that software engineers enjoy engaging in solving puzzles and other cognitive efforts. A tendency to engage in and enjoy effortful thinking is referred to as a person's 'need for cognition.' In this article we study the relationship between software engineers' personality traits and their need for cognition. Through a large-scale sample study of 483 respondents we collected data to capture the six 'bright' personality traits of the HEXACO model of personality, and three `dark' personality traits. Data were analyzed using several methods including a multiple Bayesian linear regression analysis. The results indicate that ca. 33% of variation in developers' need for cognition can be explained by personality traits. The Bayesian analysis suggests four traits to be of particular interest in predicting need for cognition: openness to experience, conscientiousness, honesty-humility, and emotionality. Further, we also find that need for cognition of software engineers is, on average, higher than in the general population, based on a comparison with prior studies. Given the importance of human factors for software engineers' performance in general, and problem solving skills in particular, our findings suggest several implications for recruitment, working behavior, and teaming. ","From Anecdote to Evidence: The Relationship Between Personality and Need
  for Cognition of Developers"
70,1470698088713142275,314171681,Laura Baudis,['New paper on the @arxiv: we characterised wavelength-shifting reflectors for @LEGEND_Science and other future experiments aiming to observe scintillation light in liquid argon. Work by PhD student Gabriela Araujo and (former) master student Vera Wu: <LINK> <LINK>'],https://arxiv.org/abs/2112.06675,"Detectors based on liquid argon (LAr) often require surfaces that can shift vacuum ultraviolet (VUV) light and reflect the visible shifted light. For the LAr instrumentation of the LEGEND-200 neutrinoless double beta decay experiment, several square meters of wavelength-shifting reflectors (WLSR) were prepared: the reflector Tetratex (TTX) was in-situ evaporated with the wavelength shifter tetraphenyl butadiene (TPB). For even larger detectors, TPB evaporation will be more challenging and plastic films of polyethylene naphthalate (PEN) are considered as an option to ease scalability. In this work, we first characterized the absorption (and reflectivity) of PEN, TPB (and TTX) films in response to visible light. We then measured TPB and PEN coupled to TTX in a LAr setup equipped with a VUV sensitive photomultiplier tube. The effective light yield in the setup was first measured using an absorbing reference sample, and the VUV reflectivity of TTX quantified. The characterization and simulation of the setup along with the measurements and modelling of the optical parameters of TPB, PEN and TTX allowed to estimate the quantum efficiency (QE) of TPB and PEN in LAr (at 87K) for the first time: these were found to be above 67% and 49%, respectively (at 90% CL). These results provide relevant input for the optical simulations of experiments that use TPB in LAr, such as LEGEND-200, and for experiments that plan to use TPB or PEN to shift VUV scintillation light. ","R&D of Wavelength-Shifting Reflectors and Characterization of the
  Quantum Efficiency of Tetraphenyl Butadiene and Polyethylene Naphthalate in
  Liquid Argon"
71,1470674562023100416,1310552063999438849,Hauke Group,"['New paper <LINK> aiming at understanding how quantum link gauge theories approach the quantum field theory limit, by @JCHalimeh, Maarten Van Damme, @TVZache, Debasish Banerjee, and @PhilippHauke <LINK>']",https://arxiv.org/abs/2112.04501,"Realizations of gauge theories in setups of quantum synthetic matter open up the possibility of probing salient exotic phenomena in condensed matter and high-energy physics, along with potential applications in quantum information and science technologies. In light of the impressive ongoing efforts to achieve such realizations, a fundamental question regarding quantum link model regularizations of lattice gauge theories is how faithfully they capture the quantum field theory limit of gauge theories. Recent work [Zache, Van Damme, Halimeh, Hauke, and Banerjee, (arXiv:2104.00025)] has shown through analytic derivations, exact diagonalization, and infinite matrix product state calculations that the low-energy physics of $\mathrm{U}(1)$ quantum link models approaches the quantum field theory limit already at small link spin length $S$. Here, we show that the approach to this limit also lends itself to the far-from-equilibrium quench dynamics of lattice gauge theories, as demonstrated by our numerical simulations of the Loschmidt return rate and the chiral condensate in infinite matrix product states, which work directly in the thermodynamic limit. Similar to our findings in equilibrium that show a distinct behavior between half-integer and integer link spin lengths, we find that criticality emerging in the Loschmidt return rate is fundamentally different between half-integer and integer spin quantum link models in the regime of strong electric-field coupling. Our results further affirm that state-of-the-art finite-size ultracold-atom and NISQ-device implementations of quantum link lattice gauge theories have the real potential to simulate their quantum field theory limit even in the far-from-equilibrium regime. ","Achieving the quantum field theory limit in far-from-equilibrium quantum
  link models"
72,1470674171545927680,1381477369903513601,Ilker Birbil @ UvA,"['We‚Äôve just released our new meta-learning algorithm:\n\nLESS - LEarning with Subset Stacking\n\nLESS is flexible, fast, and accurate. If you get a chance to try it, please let us know how it works for you. \n\nPaper: <LINK>\nCode &amp; Tutorials: <LINK> <LINK>', 'The current version of LESS works for regression, and we will hopefully add classification by early 2022.', '@danial_esm We have tried the straightforward parallelisation and reported our results. The idea about using ‚Äúlocal agents‚Äù sounds interesting. https://t.co/f4XrVydE7J']",https://arxiv.org/abs/2112.06251,"We propose a new algorithm that learns from a set of input-output pairs. Our algorithm is designed for populations where the relation between the input variables and the output variable exhibits a heterogeneous behavior across the predictor space. The algorithm starts with generating subsets that are concentrated around random points in the input space. This is followed by training a local predictor for each subset. Those predictors are then combined in a novel way to yield an overall predictor. We call this algorithm ""LEarning with Subset Stacking"" or LESS, due to its resemblance to method of stacking regressors. We compare the testing performance of LESS with the state-of-the-art methods on several datasets. Our comparison shows that LESS is a competitive supervised learning method. Moreover, we observe that LESS is also efficient in terms of computation time and it allows a straightforward parallel implementation. ",Learning with Subset Stacking
73,1470657732982317058,16434310,chrislintott,"[""Let me tell you about my new paper, the first from an exciting new project with @astrokiwi and @ted_mackereth predicting the properties of interstellar objects such as 'Oumuamua using what we know about the Milky Way (1/n) <LINK> <LINK>"", ""@astrokiwi @ted_mackereth The project started with me being intrigued by @astrokiwi's work on how interstellar objects (ISOs) might affect planet formation. Plus, interstellar objects which come from OTHER SOLAR SYSTEMS are just cool. (2/n)"", '@astrokiwi @ted_mackereth It all started with a pint in the Lamb &amp; Flag, and then later a drink in a bar @megschwamb took us to with a tree in it, and agreed to follow up on half an idea I had to see if ISOs from different origins would appear different (3/n) https://t.co/wmFLNF6zB7', '@astrokiwi @ted_mackereth @megschwamb We first thought about ISOs from galaxies that had merged with the Milky Way. In lockdown, I got up early here in Oxford to talk to Michelle in New Zealand, and then reported back at the end of the day - her early morning. (4/n)', '@astrokiwi @ted_mackereth @megschwamb Reading papers from @ted_mackereth, we realised we could use simulations of Milky Way like galaxies, which keep track of star formation, to predict what ISOs we should see for a given galaxy history. (5/n)', ""@astrokiwi @ted_mackereth @megschwamb Ted's papers are very clear and well-written, but I knew I was opening a can of worms using simulation data I hadn't played with before. Luckily, Ted was enthusiastic about the project and joined us to see what we could say about ISOs (6/n)"", ""@astrokiwi @ted_mackereth @megschwamb I think this is the second (relatively) early-career collaborator I've ended up working with just because their papers were well written - it's worth presenting results clearly, people! (7/n)"", '@astrokiwi @ted_mackereth @megschwamb Even if sometimes that means a very long twitter thread (8/n)', ""@astrokiwi @ted_mackereth @megschwamb Anyway, I still think this is a fabulous idea, because it means while we can learn about ISOs from understanding the galaxy, observing more interstellar objects with @VRubinObs might help us test ideas about the Milky Way's history (9/n)"", '@astrokiwi @ted_mackereth @megschwamb @VRubinObs We make a bucketload of assumptions - like saying every star contributes to the ISO population, that those we see are a random sample , and that only planetesimals that form far from their star (beyond the ice line) are ejected. (10/n)', '@astrokiwi @ted_mackereth @megschwamb @VRubinObs Some of those guesses may turn out to be rubbish - our PhD student, @Astrohopkins, is testing them - but they allow us to get started. We find two kinds of ISO - those with lots of water and those where the water is less important (11/n)', '@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins (Type I and Type II interstellar objects, anyone?!? 12/n)', '@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins You can see in this plot how the relative mix of water-rich and water-poor ISOs changes when you compare two galaxies with different histories. The water-rich ISOs are formed early, from relatively pristine (low-metallicity) gas. (13/n) https://t.co/cmr5yUyo1X', ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins We can even make a prediction for what the ratio of water-rich to water-poor ISOs in our Milky Way will be, which we can then test as more of them are discovered. It's fun to actually make a prediction! That's SCIENCE! (14/n)"", ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins Both 'Oumuamua and Borisov, the two ISOs we've seen so far, belong to the relatively water-poor population. With two objects, that clearly may be coincidence, though I'm rather fond of a speculation that came, I think, from @astrokiwi (15/n) https://t.co/IAvy5ZVyce"", ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins If water-rich ISOs are produced early on, in low-metallicity gas, it may be that they don't have much dust. These objects would be more easily effected by approach to the Sun, and would sublimate away while still in the outer Solar System. (16/n)"", ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins I would guess such a population of puny comets doesn't exist, but if it does - then you heard it here first! It's a nice example of how thinking about these wandering objects in the context of their formation leads to new thoughts (17/n)"", '@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins We might also speculate that if different kinds of ISOs come from different stellar populations, there might be a connection between their composition and where they appear on the sky. Again, maybe. (18/n)', ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins The really fun thing is that if our predictions are wrong we know it's telling us EITHER that we're missing something in our understanding of the Milky Way or planetesimal formation. The fun bit will be finding out which - much more work ahead! (19/n)"", ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins I've always liked projects that connect different areas, but this - involving cosmological simulations, galactic structure, star formation, astrochemistry, small body science &amp; planetary takes this biscuit. Hope you find it fun too. (20/20)"", ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins PS If you've read this far you probably want another link to the paper: https://t.co/qP42EjQkpd AND I will buy you a drink if you say the secret password - #itsneveraliens - to me while I'm in the Lamb &amp; Flag which, thank the lord, reopens in February. (21/20)"", '@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins PS Thanks to @pedroTGferreira who told me to insist on getting sabbatical time, which led to this project. (22/20)', ""@tigerrrboy @astrokiwi @ted_mackereth @megschwamb I might as well. So pleased it's reopening."", ""@astrokiwi @ted_mackereth @megschwamb @VRubinObs @AstroHopkins @PedroTGFerreira PPS Here's Michele's take: https://t.co/mcfBchOJfp"", '@chrisenorth @astrokiwi @ted_mackereth @AstroPythag Of course!', '@astrokiwi @ted_mackereth @AstroHopkins Type I and Type Ia? #helping', '@vicgrinberg @astrokiwi @ted_mackereth @AstroHopkins Yes! Great idea. Michele and the planetary people will be so happy us extragalactic type got involved.', '@asteronomer @astrokiwi @ted_mackereth That‚Äôs half the fun', 'PPPS Ted‚Äôs woken up https://t.co/dJdheEbLba', '@ted_mackereth Ah! Welcome back']",https://arxiv.org/abs/2112.05773,"Planetesimals inevitably bear the signatures of their natal environment, preserving in their composition a record of the metallicity of their system's original gas and dust, albeit one altered by the formation process. When planetesimals are dispersed from their system of origin, this record is carried with them. As each star is likely to contribute at least $10^{12}$ interstellar objects, the Galaxy's drifting population of interstellar objects (ISOs) provides an overview of the properties of its stellar population through time. Using the EAGLE cosmological simulation and models of protoplanetary formation, our modelling predicts an ISO population with a bimodal distribution in their water mass fraction. Objects formed in low-metallicity, typically older, systems have a higher water fraction than their counterparts formed in high-metallicity protoplanetary disks, and these water-rich objects comprise the majority of the population. Both detected ISOs seem to belong to the lower water fraction population; these results suggest they come from recently formed systems. We show that the population of ISOs in galaxies with different star formation histories will have different proportions of objects with high and low water fractions. This work suggests that it is possible that the upcoming Vera C. Rubin Observatory Legacy Survey of Space and Time will detect a large enough population of ISOs to place useful constraints on models of protoplanetary disks, as well as galactic structure and evolution. ","Predicting the water content of interstellar objects from galactic star
  formation histories"
74,1470603428313370624,16434310,chrislintott,['New paper with @astrokiwi &amp; \u2066@ted_mackereth\u2069: Predicting the water content of interstellar objects from galactic star formation histories <LINK>'],https://arxiv.org/abs/2112.05773,"Planetesimals inevitably bear the signatures of their natal environment, preserving in their composition a record of the metallicity of their system's original gas and dust, albeit one altered by the formation process. When planetesimals are dispersed from their system of origin, this record is carried with them. As each star is likely to contribute at least $10^{12}$ interstellar objects, the Galaxy's drifting population of interstellar objects (ISOs) provides an overview of the properties of its stellar population through time. Using the EAGLE cosmological simulation and models of protoplanetary formation, our modelling predicts an ISO population with a bimodal distribution in their water mass fraction. Objects formed in low-metallicity, typically older, systems have a higher water fraction than their counterparts formed in high-metallicity protoplanetary disks, and these water-rich objects comprise the majority of the population. Both detected ISOs seem to belong to the lower water fraction population; these results suggest they come from recently formed systems. We show that the population of ISOs in galaxies with different star formation histories will have different proportions of objects with high and low water fractions. This work suggests that it is possible that the upcoming Vera C. Rubin Observatory Legacy Survey of Space and Time will detect a large enough population of ISOs to place useful constraints on models of protoplanetary disks, as well as galactic structure and evolution. ","Predicting the water content of interstellar objects from galactic star
  formation histories"
75,1470599513589137408,312448486,Dr. Karan Jani,"['üö® NEW PAPER ALERT üö® \n\nIntroducing BRAHMA ‚Äì a first of its kind thermodynamical framework for black hole merger astrophysics. \n\nThis is the first, first author paper of our undergraduate student @VanderbiltU, Patrick Hu (junior in Physics + Music)!!\n\nüéá <LINK> <LINK>', ""BRAHMA: BinaRy blAck Hole Merger entropy Analysis\n\nHere's a schematic flow of the BRAHMA Framework : from a signal in LIGO-Virgo, to computing entropies and finding new astrophysical constraints. https://t.co/BgpyLGq8RS"", 'At the heart of BRAHMA is a new thermodynamical parameter: Merger Entropy Index.\n\nBlack hole mergers are irreversible processes. Due to the divine 2nd Law of Thermodynamics, every merger increases the entropy of the universe.\n\nThe Index measures the efficiency of entropy transfer https://t.co/UMPdhNkaoi', 'We apply BRAHMA on the most exotic black hole system ever discovered in astronomy - GW190521. \n\nAnd (to my personal surprise), BRAHMA can quite substantially narrow the allowed spins and mass of this system than what we knew before. https://t.co/L9n4HjtJww']",https://arxiv.org/abs/2112.06856,"We introduce the Merger Entropy Index ($\mathcal{I}_\mathrm{BBH}$), a new parameter to measure the efficiency of entropy transfer for any generic binary black hole merger in General Relativity. We find that $\mathcal{I}_\mathrm{BBH}$ is bounded between an asymptotic maximum and minimum. For the observed population of mergers detected by LIGO and Virgo, we find that $\mathcal{I}_\mathrm{BBH}$ is $\lesssim30\%$ of its theoretical maximum. By imposing the thermodynamical consistency between the pre- and post-merger states through $\mathcal{I}_\mathrm{BBH}$, we showcase BRAHMA -- a novel framework to infer the properties and astrophysical implications of gravitational-wave detections. For GW190521 -- the heaviest confirmed binary black hole merger observed so far -- our framework rules out high mass-ratio, negative effective inspiral spin, and electromagnetic counterpart claims. Furthermore, our analysis provides an independent confirmation that GW190521 belongs to a separate population. ",Thermodynamics to infer the astrophysics of binary black hole mergers
76,1470598965842350080,1373472847750893569,Stanley H. Chan,"['New paper.\n\nWhile I was writing my lecture note about SNR, I realize how little is available in the literature. So I spent two weeks typing all these out with my student. \n\n<LINK>\n\n@ImageSensorsW and @IEEEimaging: This could be useful to your readers.', 'A few new attempts in writing:\n- Not following the recipe of Intro+related+method+experiment+conclusion\n- No ablation study, no comparison with 10+ methods\n- No fancy demo\n- Only a few plain figures\n\nFeel goodüòÄ']",https://arxiv.org/abs/2112.05817,"The signal-to-noise ratio (SNR) of a digital image sensor is typically defined as the ratio between the mean over the standard deviation of the sensor's output, thus known as the output-referred SNR. For sensors with a large full-well capacity, the output-referred SNR demonstrates the well-known linear response in the log-log scale. However, as the input exposure approaches the full-well capacity, the vanishing randomness of the saturated pixel will cause this output-referred SNR to artificially go to infinity. Since modern digital image sensors have a small pitch and hence a small full-well capacity, the shortcomings of the output-referred SNR motivated the development of a theoretical concept known as the exposure-referred SNR, first reported in some sensors and computer vision papers in the 1990's and more since 2010. Some intuitions of the exposure-referred SNR have been discussed in the past, but little is known how the exposure-referred SNR can be rigorously derived. Recognizing the significance of such an analysis to all present and future small pixels, this paper presents a theoretical analysis to justify the definition and answer four questions: (1) What is the correct definition of SNR? (2) How is the output-referred SNR related to the exposure-referred SNR? (3) For simple noise models, the SNRs can be analytically derived, but for complex noise models, how to numerically compute the SNR? (4) What utilities can the exposure-referred SNR bring to solving imaging tasks? New theoretical results are shown to confirm the validity of the exposure-referred SNR for image sensors of any bit-depth and full-well capacity. ",Exposure-Referred Signal-to-Noise Ratio for Digital Image Sensors
77,1470581104415416320,338922968,Jess Thorne,"['New paper day!\n\nWe use the SED fitting code ProSpect with an incorporated AGN component to identify and quantify ~75,000 AGN in the @devilsurvey and GAMA surveys!\n\n<LINK>', 'We find good agreement with AGN selected using broad and/or narrow emission lines, mid-infrared colour criteria, and X-ray selected AGN. https://t.co/3VRnOVhgcP', 'We also find that our derived AGN luminosities (just from fitting the FUV-FIR) are in good agreement with X-ray luminosities from @chandraxray! https://t.co/A0opX3R4vw', 'Using just our SED identified AGN we map the evolution of the AGN luminosity function from z=0-2 and find good agreement with previous observations! https://t.co/C7uSaHaODv', ""and for those who care about how the inclusion of an AGN component will impact derived galaxy properties - we find that stellar mass, age, and metallicity don't change much but star formation rates can be overpredicted by up to ~2 dex if you don't fit for an AGN!"", 'with @aaronrobotham @astrowelshluke @SabineBellstedt @MJIBrown @Scott_Croom @GalaxyGroves @blackholejets @nseymouruk @imogenwhittam @AstroRobino @BenneHolwerda']",https://arxiv.org/abs/2112.06366,"Active galactic nuclei (AGN) are typically identified through radio, mid-infrared, or X-ray emission or through the presence of broad and/or narrow emission lines. AGN can also leave an imprint on a galaxy's spectral energy distribution (SED) through the re-processing of photons by the dusty torus. Using the SED fitting code ProSpect with an incorporated AGN component, we fit the far ultraviolet to far-infrared SEDs of $\sim$494,00 galaxies in the D10-COSMOS field and $\sim$230,000 galaxies from the GAMA survey. By combining an AGN component with a flexible star formation and metallicity implementation, we obtain estimates for the AGN luminosities, stellar masses, star formation histories, and metallicity histories for each of our galaxies. We find that ProSpect can identify AGN components in 91 per cent of galaxies pre-selected as containing AGN through narrow-emission line ratios and the presence of broad lines. Our ProSpect-derived AGN luminosities show close agreement with luminosities derived for X-ray selected AGN using both the X-ray flux and previous SED fitting results. We show that incorporating the flexibility of an AGN component when fitting the SEDs of galaxies with no AGN has no significant impact on the derived galaxy properties. However, in order to obtain accurate estimates of the stellar properties of AGN host galaxies, it is crucial to include an AGN component in the SED fitting process. We use our derived AGN luminosities to map the evolution of the AGN luminosity function for $0<z<2$ and find good agreement with previous measurements and predictions from theoretical models. ","Deep Extragalactic VIsible Legacy Survey (DEVILS): Identification of AGN
  through SED Fitting and the Evolution of the Bolometric AGN Luminosity
  Function"
78,1470507033073967110,791705191175360512,Niels Warburton,"['In our new paper, led by Philip Lynch (@PhysLynch), we compute the first inspirals into a Kerr black hole using the gravitational self-force. We also apply near-identity transformations so inspirals with millions of cycles can be computed in milliseconds. <LINK>']",https://arxiv.org/abs/2112.05651,"We develop the first model for extreme mass-ratio inspirals (EMRIs) into a rotating massive black hole driven by the gravitational self-force. Our model is based on an action angle formulation of the method of osculating geodesics for eccentric, equatorial (i.e., spin-aligned) motion in Kerr spacetime. The forcing terms are provided by an efficient spectral interpolation of the first-order gravitational self-force in the outgoing radiation gauge. We apply a near-identity (averaging) transformation to eliminate all dependence of the orbital phases from the equations of motion, while maintaining all secular effects of the first-order gravitational self-force at post-adiabatic order. This implies that the model can be evolved without having to resolve all $\mathcal{O}(10^6)$ orbit cycles of an EMRI, yielding an inspiral model that can be evaluated in less than a second for any mass-ratio. In the case of a non-rotating central black hole, we compare inspirals evolved using self-force data computed in the Lorenz and radiation gauges. We find that the two gauges generally produce differing inspirals with a deviation of comparable magnitude to the conservative self-force correction. This emphasizes the need for including the (currently unknown) dissipative second order self-force to obtain gauge independent, post-adiabatic waveforms. ",Eccentric self-forced inspirals into a rotating black hole
79,1470466107672825861,1346905158706483203,Connor Lawless,"[""Excited to announce that our new #AAAI2022 paper 'Interpretable clustering via Multi-Polytope Machines' is up on arxiv! We present a MINLP formulation for interpretable clustering that jointly clusters points and constructs polytopes around each cluster. \n\n<LINK>"", 'The secret sauce in our approach are additional constraints on the hyperplanes in each polytope that allow us to make cluster explanations more interpretable (recovering popular model classes like rule sets and score cards). https://t.co/Q0KCUDZtwA', 'This was work with my amazing (twitter-less) colleagues Lam, Dzung, Jayant, and Chandra at IBM Research from an epic virtual summer internship.']",https://arxiv.org/abs/2112.05653,"Clustering is a popular unsupervised learning tool often used to discover groups within a larger population such as customer segments, or patient subtypes. However, despite its use as a tool for subgroup discovery and description - few state-of-the-art algorithms provide any rationale or description behind the clusters found. We propose a novel approach for interpretable clustering that both clusters data points and constructs polytopes around the discovered clusters to explain them. Our framework allows for additional constraints on the polytopes - including ensuring that the hyperplanes constructing the polytope are axis-parallel or sparse with integer coefficients. We formulate the problem of constructing clusters via polytopes as a Mixed-Integer Non-Linear Program (MINLP). To solve our formulation we propose a two phase approach where we first initialize clusters and polytopes using alternating minimization, and then use coordinate descent to boost clustering performance. We benchmark our approach on a suite of synthetic and real world clustering problems, where our algorithm outperforms state of the art interpretable and non-interpretable clustering algorithms. ",Interpretable Clustering via Multi-Polytope Machines
80,1470420339536678920,1368538327809409025,Johan Henriksson,['In this new paper with Ashish Kakkar and Brian McPeak we present an explicit construction of higher-genus partition functions of 2d CFTs deriving from error-correcting codes. Hopefully this will give some new handles (pun intended) on the modular bootstrap <LINK> <LINK>'],https://arxiv.org/abs/2112.05168,"Higher genus modular invariance of two-dimensional conformal field theories (CFTs) is a largely unexplored area. In this paper, we derive explicit expressions for the higher genus partition functions of a specific class of CFTs: code CFTs, which are constructed using classical error-correcting codes. In this setting, the $\mathrm{Sp}(2g,\mathbb Z)$ modular transformations of genus $g$ Riemann surfaces can be recast as a simple set of linear maps acting on $2^g$ polynomial variables, which comprise an object called the code enumerator polynomial. The CFT partition function is directly related to the enumerator polynomial, meaning that solutions of the linear constraints from modular invariance immediately give a set of seemingly consistent partition functions at a given genus. We then find that higher genus constraints, plus consistency under degeneration limits of the Riemann surface, greatly reduces the number of possible code CFTs. This work provides a step towards a full understanding of the constraints from higher genus modular invariance on 2d CFTs. ",Classical Codes and Chiral CFTs at Higher Genus
81,1470419481310732292,100773830,Mauro Dragoni,"['Our new #AAAI2022 paper ""Machine Learning for Utility Prediction in Argument-Based Computational Persuasion"" is now online <LINK>. Great work with @ivandonadello @looselycorrect and Antony Hunter. #AI #DigitalHealth']",https://arxiv.org/abs/2112.04953,"Automated persuasion systems (APS) aim to persuade a user to believe something by entering into a dialogue in which arguments and counterarguments are exchanged. To maximize the probability that an APS is successful in persuading a user, it can identify a global policy that will allow it to select the best arguments it presents at each stage of the dialogue whatever arguments the user presents. However, in real applications, such as for healthcare, it is unlikely the utility of the outcome of the dialogue will be the same, or the exact opposite, for the APS and user. In order to deal with this situation, games in extended form have been harnessed for argumentation in Bi-party Decision Theory. This opens new problems that we address in this paper: (1) How can we use Machine Learning (ML) methods to predict utility functions for different subpopulations of users? and (2) How can we identify for a new user the best utility function from amongst those that we have learned? To this extent, we develop two ML methods, EAI and EDS, that leverage information coming from the users to predict their utilities. EAI is restricted to a fixed amount of information, whereas EDS can choose the information that best detects the subpopulations of a user. We evaluate EAI and EDS in a simulation setting and in a realistic case study concerning healthy eating habits. Results are promising in both cases, but EDS is more effective at predicting useful utility functions. ","Machine Learning for Utility Prediction in Argument-Based Computational
  Persuasion"
82,1470411998487846918,1186617598512975875,ivandonadello,"['Our new #AAAI2022 paper ""Machine Learning for Utility Prediction in Argument-Based Computational Persuasion"" is now online <LINK>. Great work with @looselycorrect @maurodragoni and Antony Hunter. #AI #DigitalHealth']",https://arxiv.org/abs/2112.04953,"Automated persuasion systems (APS) aim to persuade a user to believe something by entering into a dialogue in which arguments and counterarguments are exchanged. To maximize the probability that an APS is successful in persuading a user, it can identify a global policy that will allow it to select the best arguments it presents at each stage of the dialogue whatever arguments the user presents. However, in real applications, such as for healthcare, it is unlikely the utility of the outcome of the dialogue will be the same, or the exact opposite, for the APS and user. In order to deal with this situation, games in extended form have been harnessed for argumentation in Bi-party Decision Theory. This opens new problems that we address in this paper: (1) How can we use Machine Learning (ML) methods to predict utility functions for different subpopulations of users? and (2) How can we identify for a new user the best utility function from amongst those that we have learned? To this extent, we develop two ML methods, EAI and EDS, that leverage information coming from the users to predict their utilities. EAI is restricted to a fixed amount of information, whereas EDS can choose the information that best detects the subpopulations of a user. We evaluate EAI and EDS in a simulation setting and in a realistic case study concerning healthy eating habits. Results are promising in both cases, but EDS is more effective at predicting useful utility functions. ","Machine Learning for Utility Prediction in Argument-Based Computational
  Persuasion"
83,1470363959295287298,1417075693,Biprateep Dey,['Can neural networks learn physically meaningful properties of galaxies? Can we teach them to figure out how far galaxies are? Check out our new paper where we explore some of these questions: <LINK> <LINK>'],https://arxiv.org/abs/2112.03939,"Studies of cosmology, galaxy evolution, and astronomical transients with current and next-generation wide-field imaging surveys (like LSST) are all critically dependent on estimates of galaxy redshifts from imaging data alone. Capsule networks are a new type of neural network architecture that is better suited for identifying morphological features of the input images than traditional convolutional neural networks. We use a deep capsule network trained on the $ugriz$ images, spectroscopic redshifts, and Galaxy Zoo spiral/elliptical classifications of $\sim$400,000 SDSS galaxies to do photometric redshift estimation. We achieve a photometric redshift prediction accuracy and a fraction of catastrophic outliers that are comparable to or better than current state-of-the-art methods while requiring less data and fewer trainable parameters. Furthermore, the decision-making of our capsule network is much more easily interpretable as capsules act as a low-dimensional encoding of the image. When the capsules are projected on a 2-dimensional manifold, they form a single redshift sequence with the fraction of spirals in a region exhibiting a gradient roughly perpendicular to the redshift sequence. We perturb encodings of real galaxy images in this low-dimensional space to create synthetic galaxy images that demonstrate the image properties (e.g., size, orientation, and surface brightness) encoded by each dimension. We also show how strongly galaxy properties (e.g., magnitudes, colours, and stellar mass) are correlated with each capsule dimension. Finally, we publicly release the code for our capsule network, our estimated redshifts, and additional catalogues. ","Photometric Redshifts from SDSS Images with an Interpretable Deep
  Capsule Network"
84,1470332065811861508,1055833277221822464,Miles Lucas,"['A new paper just hit the arxiv (accepted into Astronomical Journal)! ‚ÄúAn Imaging Search for Post-MS Planets of Sirius B‚Äù <LINK>\n\nA short threadüßµ (1/6)', 'This article is about high-contrast images we took of Sirius B. They are the most sensitive direct images of the white dwarf, to date! You might be curious, ‚Äúwhat kind of planet would you expect to find around a white dwarf?‚Äù (2/6) https://t.co/ojChEc04Yq', 'Well, we consider the idea that a planet was formed or involved in a common envelope stage, a triple evolution dynamical instability, or formed from a post-AGB disk, all of which are exotic second-generation planet evolution pathways. (3/6) https://t.co/ChRjrv1qWi', 'The separations we resolve around Sirius B are only valid for such an exotic post-MS planet, because we estimate a planet would have to be ~30 AU from Sirius B, currently, to have avoided envelopment or tidal shredding during the giant branch evolution of Sirius B (4/6)', 'We use the ATMO2020 and Sonora Bobcat planetary evolution models to consider various post-MS effects like metallicity and formation age to determine 5-sigma mass sensitivity limits from our observations. We reach sub-Jupiter mass sensitivity at sub-AU separations. (5/6) https://t.co/zJRP3FgGms', 'Finally, all the code and data is public, available at https://t.co/pkMT1CO0ZU \nI wrote almost all of this in #julialang with a strong use of PyCall.jl for language interoperability, getting the best of the speed and expressiveness of Julia, and the ecosystem of Python (6/6)']",https://arxiv.org/abs/2112.05234,"We present deep imaging of Sirius B, the closest and brightest white dwarf, to constrain post-main-sequence planetary evolution in the Sirius system. We use Keck/NIRC2 in L'-band (3.776 $\mu$m) across three epochs in 2020 using the technique of angular differential imaging. Our observations are speckle-limited out to 1 AU and background-limited beyond. The 5$\sigma$ detection limits from our best performing epoch are 17 to 20.4 L' absolute magnitude. We consider multiple planetary formation pathways in the context of Sirius B's evolution to derive mass sensitivity limits, and achieve sub-Jupiter sensitivities at sub-AU separations, reaching 1.6 $\mathrm{M_J}$ to 2.4 $\mathrm{M_J}$ at 0.5 AU down to a sensitivity of 0.7 $\mathrm{M_J}$ to 1.2 $\mathrm{M_J}$ at >1 AU. Consistent with previous results, we do not detect any companions around Sirius B. Our strong detection limits demonstrate the potential of using high-contrast imaging to characterize nearby white dwarfs. ",An Imaging Search for Post-Main-Sequence Planets of Sirius B
85,1470217150606426113,1273467805283659777,Pang Wei Koh,"[""We're excited to announce WILDS v2.0, which adds unlabeled data to 8 datasets! This lets us benchmark methods for domain adaptation &amp; representation learning. All labeled data &amp; evaluations are unchanged.\n \n(New) paper: <LINK>\nWebsite: <LINK>\n\nüßµ <LINK>"", 'Unlabeled data can be a powerful source of leverage. It comes from a mixture of:\n- source domains (same as the labeled training data)\n- target domains (same as the labeled test data) \n- extra domains with no labeled data. \n\nWe illustrate this for the GlobalWheat dataset: https://t.co/kT11wMPDaa', 'We evaluated domain adaptation, self-training, &amp; self-supervised methods on these datasets. Unfortunately, many methods did not do better than standard supervised training, despite using additional unlabeled data.\n\nThis table shows OOD test performance; higher numbers are better. https://t.co/WgjVMZ79mG', 'In contrast, prior work has shown these methods to be successful on standard domain adaptation tasks such as DomainNet, which we replicate below. This underscores the importance of developing and evaluating methods on a broad variety of distribution shifts. https://t.co/dPPPeJZYCT', ""We've added the unlabeled data loaders + method implementations to our Python package: https://t.co/S73kjDxMis. They're easy to use: check out the code snippet below! \n\nWe've also updated our leaderboards to accept submissions with and without unlabeled data. https://t.co/Buy612P2IX"", ""We've uploaded the exact commands and hyperparameters used in our paper, as well as trained model checkpoints, to https://t.co/qI7yvTWGsT. This is thanks to @tonyh_lee, who oversaw all of the experimental infrastructure and made it fully reproducible on @CodaLabWS."", ""We're grateful to everyone who helped us with WILDS and the v2.0 update: https://t.co/1CAsr8JV99. \n\nWe'd also like to thank Jiang et al. for https://t.co/CSIYF8gcFT and Zhang et al. for https://t.co/Kla5i4C9Y9, which were very helpful references for our method implementations."", 'This was joint work with @shiorisagawa* @tonyh_lee* IrenaGao*, and @sangmichaelxie @kendrick_shen @ananyaku @weihua916 @michiyasunaga HenrikMarklund @sarameghanbeery @EtienneDavid @IanStavness @guowei_net @jure @kate_saenko_ @tatsu_hashimoto @svlevine @chelseabfinn @percyliang.', ""We'll be presenting this at the DistShift workshop at NeurIPS. Find us at our poster on Dec 13, 1-3pm Pacific Time: https://t.co/gid3wBSqb6\n \nRead our paper for more details and analysis: https://t.co/m95JSY9LbJ""]",http://arxiv.org/abs/2112.05090,"Machine learning systems deployed in the wild are often trained on a source distribution but deployed on a different target distribution. Unlabeled data can be a powerful point of leverage for mitigating these distribution shifts, as it is frequently much more available than labeled data and can often be obtained from distributions beyond the source distribution as well. However, existing distribution shift benchmarks with unlabeled data do not reflect the breadth of scenarios that arise in real-world applications. In this work, we present the WILDS 2.0 update, which extends 8 of the 10 datasets in the WILDS benchmark of distribution shifts to include curated unlabeled data that would be realistically obtainable in deployment. These datasets span a wide range of applications (from histology to wildlife conservation), tasks (classification, regression, and detection), and modalities (photos, satellite images, microscope slides, text, molecular graphs). The update maintains consistency with the original WILDS benchmark by using identical labeled training, validation, and test sets, as well as the evaluation metrics. On these datasets, we systematically benchmark state-of-the-art methods that leverage unlabeled data, including domain-invariant, self-training, and self-supervised methods, and show that their success on WILDS is limited. To facilitate method development and evaluation, we provide an open-source package that automates data loading and contains all of the model architectures and methods used in this paper. Code and leaderboards are available at this https URL ",Extending the WILDS Benchmark for Unsupervised Adaptation
86,1469386317440200709,473190579,Naomi Gendler,"['Really excited about our new paper! We show how, in a large class of string theory compactifications, the strong CP problem is automatically solved by the Peccei-Quinn mechanism. We also analyze dark matter and astro bounds on axions in our stringy EFTs!\n\n<LINK>']",https://arxiv.org/abs/2112.04503,"We show that the strong CP problem is solved in a large class of compactifications of string theory. The Peccei-Quinn mechanism solves the strong CP problem if the CP-breaking effects of the ultraviolet completion of gravity and of QCD are small compared to the CP-preserving axion potential generated by low-energy QCD instantons. We characterize both classes of effects. To understand quantum gravitational effects, we consider an ensemble of flux compactifications of type IIB string theory on orientifolds of Calabi-Yau hypersurfaces in the geometric regime, taking a simple model of QCD on D7-branes. We show that the D-brane instanton contribution to the neutron electric dipole moment falls exponentially in $N^4$, with $N$ the number of axions. In particular, this contribution is negligible in all models in our ensemble with $N>17$. We interpret this result as a consequence of large $N$ effects in the geometry that create hierarchies in instanton actions and also suppress the ultraviolet cutoff. We also compute the CP breaking due to high-energy instantons in QCD. In the absence of vectorlike pairs, we find contributions to the neutron electric dipole moment that are not excluded, but that could be accessible to future experiments if the scale of supersymmetry breaking is sufficiently low. The existence of vectorlike pairs can lead to a larger dipole moment. Finally, we show that a significant fraction of models are allowed by standard cosmological and astrophysical constraints. ",PQ Axiverse
87,1469385595814232067,1324170692401639424,Robert McGehee,"['I\'m very excited about my new paper with @GillyElor and Aaron Pierce: ""Maximizing Direct Detection with HYPER Dark Matter."" So, here\'s your Fri physicsüßµ1/n\n\n<LINK> <LINK>', 'In this paper, we addressed 2 Qs: #1 What is the maximum cross section for sub-GeV DM scattering off nucleons? #2 Is there a DM candidate which may be detected at future experiments with a cross section as large as this maximum while still accounting for its relic abundance. 2/n', 'The answer to #1: 10^(-36) - 10^(-30) cm^2 for DM masses from 10 keV - 100 MeV. We estimated this by only including present-day bounds on DM and a scalar mediator, which connects the DM to the visible sector. 3/n', 'To establish the model-independence of this max cross section, we considered variations of our starting simple assumptions. No common vector mediators (visibly and invisibly decaying dark photons, B-L, B) had a larger max cross section! 4/n', 'The answer to #2: yes! We named them HighlY interactive ParticlE Relics (HYPERs). 5/n', 'In HYPER models, a dark sector phase transition causes the mediator to decrease its mass to its present-day value. This occurs after the DM abundance freezes-in and boosts the present-day direct detection cross section. 6/n', 'Since DM-SM interactions get a late-time ""boost,"" we must also verify that the DM abundance doesn\'t change. For parts of HYPER (parameter) space, DM-number-changing processes must be suppressed, causing the direct detection cross section to be smaller than the maximum one. 7/n', 'But, for many HYPERs, we find that they are at (or fairly close to) the maximum consistent cross section! 8/n https://t.co/zZCk75LslK', 'This is particularly exciting because HYPERs populate a parameter space which is imminently testable by many future direct detection efforts but has few DM benchmarks. 9/n', 'In the future, we want to do the same analysis and HYPER model building for sub-GeV DM scattering off electrons which would require an even lower dark sector phase transition temperature. 10/n', 'It is an interesting question as to whether such a low phase transition temperature could modify or remove bounds on the mediator from HB stars, which were essential in our derivation of the maximum cross section for this work. Endüßµ']",https://arxiv.org/abs/2112.03920,"We estimate the maximum direct detection cross section for sub-GeV dark matter scattering off nucleons. For dark matter masses in the range of $10 \text{ keV }- 100 \text{ MeV}$, cross sections greater than $10^{-36}$- $10^{-30} \,\text{cm}^2$ seem implausible. We introduce a dark matter candidate which realizes this maximum cross section: HighlY interactive ParticlE Relics (HYPERs). After HYPERs freeze-in, a dark sector phase transition decreases the mass of the mediator which connects HYPERs to the visible sector. This increases the HYPER's direct detection cross section, but in such a way as to leave the HYPER's abundance unaffected and avoid conflict with measurements of Big Bang Nucleosynthesis and the Cosmic Microwave Background. HYPERs present a benchmark for direct detection experiments in a parameter space with few known dark matter models. ",Maximizing Direct Detection with HYPER Dark Matter
88,1469369642091270147,929390967970508800,Erik B Myklebust,"['Our new paper is now on Arxiv <LINK> \nWill appear in @SW_Journal soon. With @ejimenez_ruiz @ChenJiaoyan1 @derboyausleu', '@NIVAforskning']",https://arxiv.org/abs/2112.04605,"We have created a knowledge graph based on major data sources used in ecotoxicological risk assessment. We have applied this knowledge graph to an important task in risk assessment, namely chemical effect prediction. We have evaluated nine knowledge graph embedding models from a selection of geometric, decomposition, and convolutional models on this prediction task. We show that using knowledge graph embeddings can increase the accuracy of effect prediction with neural networks. Furthermore, we have implemented a fine-tuning architecture that adapts the knowledge graph embeddings to the effect prediction task and leads to better performance. Finally, we evaluate certain characteristics of the knowledge graph embedding models to shed light on the individual model performance. ","Prediction of Adverse Biological Effects of Chemicals Using Knowledge
  Graph Embeddings"
89,1469349885153452033,218565952,David Renter√≠a,"['New paper: Reconstructing partonic kinematics at colliders with Machine Learning. \n<LINK>', 'Using a Neural Network for reconstructing the parton kinematic at proton-proton collisions https://t.co/vCTEHhGdkK']",https://arxiv.org/abs/2112.05043,"In the context of high-energy physics, a reliable description of the parton-level kinematics plays a crucial role for understanding the internal structure of hadrons and improving the precision of the calculations. Here, we study the production of one hadron and a direct photon, including up to Next-to-Leading Order Quantum Chromodynamics and Leading-Order Quantum Electrodynamics corrections. Using a code based on Monte-Carlo integration, we simulate the collisions and analyze the events to determine the correlations among measurable and partonic quantities. Then, we use these results to feed three different Machine Learning algorithms that allow us to find the momentum fractions of the partons involved in the process, in terms of suitable combinations of the final state momenta. Our results are compatible with previous findings and suggest a powerful application of Machine-Learning to model high-energy collisions at the partonic-level with high-precision. ",Reconstructing partonic kinematics at colliders with Machine Learning
90,1469335310131609605,28840722,Phil Long,"['New paper with Rocco Servedio called ""The perils of being unhinged: On the accuracy of classifiers minimizing a noise-robust convex loss"": <LINK>.']",https://arxiv.org/abs/2112.04590,"Van Rooyen et al. introduced a notion of convex loss functions being robust to random classification noise, and established that the ""unhinged"" loss function is robust in this sense. In this note we study the accuracy of binary classifiers obtained by minimizing the unhinged loss, and observe that even for simple linearly separable data distributions, minimizing the unhinged loss may only yield a binary classifier with accuracy no better than random guessing. ","The perils of being unhinged: On the accuracy of classifiers minimizing
  a noise-robust convex loss"
91,1469315503575506944,916335261004517376,Itai Gat,"['New #AAAI2022 paper: Latent Space Explanation by Intervention\n\nJoint work with \nGuy Lorberbom @idansc @TamirHazan2\n\n<LINK>\n\n1/6 <LINK>', 'The success of deep neural nets heavily relies on their ability to encode complex relations between their input and their output. While this property serves to fit the training data well, it also obscures the mechanism that drives prediction.\n\n2/6', 'We aim to reveal hidden concepts by employing an intervention mechanism that shifts the predicted class based on discrete variational autoencoders. An explanatory model then visualizes the encoded information from any hidden layer and its intervened representation. \n\n3/6', 'We integrate discriminative and explanatory functional information into a single information-theoretic framework. We then introduce a regularization term that encourages high shared information between discriminative and explanatory learners.\n\n4/6', 'By the assessment of differences between the original representation and the intervened representation, one can determine the concepts that can alter the class.\n\n5/6', 'We demonstrate the effectiveness of our approach on CelebA, where we show various visualizations of bias in the data and suggest different interventions to reveal and change bias.\n\n6/6']",https://arxiv.org/abs/2112.04895,"The success of deep neural nets heavily relies on their ability to encode complex relations between their input and their output. While this property serves to fit the training data well, it also obscures the mechanism that drives prediction. This study aims to reveal hidden concepts by employing an intervention mechanism that shifts the predicted class based on discrete variational autoencoders. An explanatory model then visualizes the encoded information from any hidden layer and its corresponding intervened representation. By the assessment of differences between the original representation and the intervened representation, one can determine the concepts that can alter the class, hence providing interpretability. We demonstrate the effectiveness of our approach on CelebA, where we show various visualizations for bias in the data and suggest different interventions to reveal and change bias. ",Latent Space Explanation by Intervention
92,1469276384971403268,91634245,Brad Marston,"['New paper on ‚ÄúTopology of rotating stratified fluids with and without background shear flow‚Äù now posted on the arXiv.  It was a pleasure working his my co-authors  @ZhuZiyan and Chris Li.  <LINK>', '*with*']",https://arxiv.org/abs/2112.04691,"Poincar\'e-gravity modes described by the shallow water equations in a rotating frame have non-trivial topology, providing a new perspective on the origin of equatorially trapped Kelvin and Yanai waves. We investigate the topology of rotating shallow water equations and continuously stratified primitive equations in the presence of a background sinusoidal shear flow. The introduction of a background shear flow not only breaks the Hermiticity and homogeneity of the system but also leads to instabilities. We show that singularities in the phase of the Poincar\'e waves of the unforced shallow-water equations and primitive equations persist in the presence of shear. Thus the bulk Poincar\'e bands have non-trivial topology and we expect and confirm the persistence of the equatorial waves in the presence of shear along the equator where the Coriolis parameter $f$ changes sign. ","Topology of rotating stratified fluids with and without background shear
  flow"
93,1469242837195796480,724609851884724225,Ana Belen Sainz,"['New paper out!\n\nNeither incompatibility among measurements nor the assumption of freedom of choice is necessary for witnessing failures of generalized noncontextuality. Also, such failures can be witnessed using arbitrarily inefficient detectors.\n\n<LINK>\n\n@ictqt']",https://arxiv.org/abs/2112.04521,"The formalism of generalized probabilistic theories (GPTs) was originally developed as a way to characterize the landscape of conceivable physical theories. Thus, the GPT describing a given physical theory necessarily includes all physically possible processes. We here consider the question of how to provide a GPT-like characterization of a particular experimental setup within a given physical theory. We show that the resulting characterization is not generally a GPT in and of itself-rather, it is described by a more general mathematical object that we introduce and term an accessible GPT fragment. We then introduce an equivalence relation, termed cone equivalence, between accessible GPT fragments (and, as a special case, between standard GPTs). We give a number of examples of experimental scenarios that are best described using accessible GPT fragments, and where moreover cone-equivalence arises naturally. We then prove that an accessible GPT fragment admits of a classical explanation if and only if every other fragment that is cone-equivalent to it also admits of a classical explanation. Finally, we leverage this result to prove several fundamental results regarding the experimental requirements for witnessing the failure of generalized noncontextuality. In particular, we prove that neither incompatibility among measurements nor the assumption of freedom of choice is necessary for witnessing failures of generalized noncontextuality, and, moreover, that such failures can be witnessed even using arbitrarily inefficient detectors. ","Accessible fragments of generalized probabilistic theories, cone
  equivalence, and applications to witnessing nonclassicality"
94,1469235125007306752,16434310,chrislintott,"[""New paper day! Led by the indefatigable @drbecky_ and @KarenLMasters, along with the @galaxyzoo team, we've written about why, if you're trying to select galaxies by shape you really really shouldn't use colour as a proxy: <LINK>"", ""@drbecky_ @KarenLMasters @galaxyzoo This is somewhat old news, but - especially for people using machine learning to classify images of galaxies - we hope it'll be a useful reminder. Remember: friends don't let friends use colour as a proxy for morphology."", '@drbecky_ @KarenLMasters @galaxyzoo It‚Äôs a good word.']",https://arxiv.org/abs/2112.04507,"The galaxy population is strongly bimodal in both colour and morphology, and the two measures correlate strongly, with most blue galaxies being late-types (spirals) and most early-types, typically ellipticals, being red. This observation has led to the use of colour as a convenient selection criteria to make samples which are then labelled by morphology. Such use of colour as a proxy for morphology results in necessarily impure and incomplete samples. In this paper, we make use of the morphological labels produced by Galaxy Zoo to measure how incomplete and impure such samples are, considering optical (ugriz), NUV and NIR (JHK) bands. The best single colour optical selection is found using a threshold of g-r = 0.742, but this still results in a sample where only 56% of red galaxies are smooth and 56% of smooth galaxies are red. Use of the NUV gives some improvement over purely optical bands, particularly for late-types, but still results in low purity/completeness for early-types. No significant improvement is found by adding NIR bands. With any two bands, including NUV, a sample of early-types with greater than two-thirds purity cannot be constructed. Advances in quantitative galaxy morphologies have made colour-morphology proxy selections largely unnecessary going forward; where such assumptions are still required, we recommend studies carefully consider the implications of sample incompleteness/impurity. ","Quantifying the Poor Purity and Completeness of Morphological Samples
  Selected by Galaxy Colour"
95,1469219185112850438,3213868013,German Sborlini,"['New paper on arXiv: <LINK>\n\nA great collaboration with @RogerHer, David Renteria and Pia Zurita!! \n\n@IFICorpuscular @desy @UASinaloa #physics #science #MachineLearning <LINK>']",https://arxiv.org/abs/2112.05043,"In the context of high-energy physics, a reliable description of the parton-level kinematics plays a crucial role for understanding the internal structure of hadrons and improving the precision of the calculations. Here, we study the production of one hadron and a direct photon, including up to Next-to-Leading Order Quantum Chromodynamics and Leading-Order Quantum Electrodynamics corrections. Using a code based on Monte-Carlo integration, we simulate the collisions and analyze the events to determine the correlations among measurable and partonic quantities. Then, we use these results to feed three different Machine Learning algorithms that allow us to find the momentum fractions of the partons involved in the process, in terms of suitable combinations of the final state momenta. Our results are compatible with previous findings and suggest a powerful application of Machine-Learning to model high-energy collisions at the partonic-level with high-precision. ",Reconstructing partonic kinematics at colliders with Machine Learning
96,1469138089708855297,1252993183686025219,Oliver Philcox,"['New paper! Misha Ivanov &amp; I present the first joint full-shape analysis of the galaxy power spectrum and bispectrum using @sdssurveys data.\n\nWe find sigma8 = 0.72+-0.03, H0 = 68.3+-0.8, S8 = 0.75+-0.04, with the bispectrum improving sigma8 by 13%!\n\n<LINK> <LINK>', ""The analysis uses the power spectrum multipoles, the real-space power spectrum extension, the reconstructed power spectrum, and the bispectrum model. For the first time, spectra are measured using *unwindowed* estimators, so the mask doesn't need to be included in the theory!"", 'Our LCDM constraints are mostly consistent with Planck, but we find a slightly low S8, matching weak lensing probes. We also get strong constraints on galaxy bias parameters! https://t.co/LZ2AnQ1Ikt', 'All the data products are publicly available (https://t.co/NHYkXag5rx) and the pipeline can be easily reapplied to @desisurvey and @ESA_Euclid data.\n\nComing soon: bispectrum multipoles, neutrino masses, primordial non-Gaussianity...']",http://arxiv.org/abs/2112.04515,"We present a full $\Lambda$CDM analysis of the BOSS DR12 dataset, including information from the power spectrum multipoles, the real-space power spectrum, the reconstructed power spectrum and the bispectrum monopole. This is the first analysis to feature a complete treatment of the galaxy bispectrum, including a consistent theoretical model and without large-scale cuts. Unlike previous works, the statistics are measured using window-free estimators: this greatly reduces computational costs by removing the need to window-convolve the theory model. Our pipeline is tested using a suite of high-resolution mocks and shown to be robust and precise, with systematic errors far below the statistical thresholds. Inclusion of the bispectrum yields consistent parameter constraints and shrinks the $\sigma_8$ posterior by $13\%$ to reach $<5\%$ precision; less conservative analysis choices would reduce the error-bars further. Our constraints are broadly consistent with Planck: in particular, we find $H_0 = 69.6^{+1.1}_{-1.3}\,\mathrm{km}\,\mathrm{s}^{-1}\mathrm{Mpc}^{-1}$, $\sigma_8 = 0.692^{+0.035}_{-0.041}$ and $n_s=0.870^{+0.067}_{-0.064}$, including a BBN prior on the baryon density. When $n_s$ is set by Planck, we find $H_0 = 68.31^{+0.83}_{-0.86}\,\mathrm{km}\,\mathrm{s}^{-1}\mathrm{Mpc}^{-1}$ and $\sigma_8 = 0.722^{+0.032}_{-0.036}$. Our $S_8$ posterior, $0.751\pm0.039$, is consistent with weak lensing studies, but lower than Planck. Constraints on the higher-order bias parameters are significantly strengthened from the inclusion of the bispectrum, and we find no evidence for deviation from the dark matter halo bias relations. These results represent the most complete full-shape analysis of BOSS DR12 to-date, and the corresponding spectra will enable a variety of beyond-$\Lambda$CDM analyses, probing phenomena such as the neutrino mass and primordial non-Gaussianity. ","The BOSS DR12 Full-Shape Cosmology: $\Lambda$CDM Constraints from the
  Large-Scale Galaxy Power Spectrum and Bispectrum Monopole"
97,1469018586073247749,1240430202255261696,Amaury Hayat,"['More math and computational biology with neural networks ! We predict the equilibriums of metabolic networks with a deep language model.  Our new paper with @f_charton, Benedetto Piccoli, Nate Merrill, and Sean McQuade @Rutgers_Camden @RutgersCCIB <LINK> <LINK>']",https://arxiv.org/abs/2112.03588,"We show that deep learning models, and especially architectures like the Transformer, originally intended for natural language, can be trained on randomly generated datasets to predict to very high accuracy both the qualitative and quantitative features of metabolic networks. Using standard mathematical techniques, we create large sets (40 million elements) of random networks that can be used to train our models. These trained models can predict network equilibrium on random graphs in more than 99% of cases. They can also generalize to graphs with different structure than those encountered at training. Finally, they can predict almost perfectly the equilibria of a small set of known biological networks. Our approach is both very economical in experimental data and uses only small and shallow deep-learning model, far from the large architectures commonly used in machine translation. Such results pave the way for larger use of deep learning models for problems related to biological networks in key areas such as quantitative systems pharmacology, systems biology, and synthetic biology. ",A deep language model to predict metabolic network equilibria
98,1469016556864692225,1202165863895449600,Fran√ßois Charton,"['Can you know if a metabolic network has an equilibrium and which ? Transformers can ! We predict graph equilibriums and their associated flows with very high precision. 1/4\nNew paper on Arxiv <LINK>\nwith @Amaury_Hayat @RutgersCCIB @Rutgers_Camden <LINK>', 'Metabolic graphs can represent many things, from the elimination of cholesterol from plaque to the carbon cycle of engineered bacterias 2/4 https://t.co/eUoRaTyE3V', 'We train transformers on randomly generated graphs to predict if there is an equilibrium and what are the associated flows, with high accuracy 3/4 https://t.co/xS92NK9ln8', 'Interestingly, models trained on synthetic data generalize to biological networks, and also to graphs with different structures than seen at train time 4/4 https://t.co/lEMFjJddx1']",https://arxiv.org/abs/2112.03588,"We show that deep learning models, and especially architectures like the Transformer, originally intended for natural language, can be trained on randomly generated datasets to predict to very high accuracy both the qualitative and quantitative features of metabolic networks. Using standard mathematical techniques, we create large sets (40 million elements) of random networks that can be used to train our models. These trained models can predict network equilibrium on random graphs in more than 99% of cases. They can also generalize to graphs with different structure than those encountered at training. Finally, they can predict almost perfectly the equilibria of a small set of known biological networks. Our approach is both very economical in experimental data and uses only small and shallow deep-learning model, far from the large architectures commonly used in machine translation. Such results pave the way for larger use of deep learning models for problems related to biological networks in key areas such as quantitative systems pharmacology, systems biology, and synthetic biology. ",A deep language model to predict metabolic network equilibria
99,1469004311388692483,729517590305898496,Gerardo Dur√°n-Mart√≠n,"['Our new paper on subspace neural bandits is now on arXiv <LINK>.  In this work, @karalleyna, @sirbayes, and I present a new algorithm for online (sequential) inference in Bayesian neural networks (1/3)', 'We put together two main ideas. Namely, learning the parameters of an MLP in an affine subspace and fitting a neural network sequentially via the extended Kalman filter (EKF) algorithm. (2/3)', 'We show the suitability of our method for tacking contextual bandit problems by comparing it to current SOTA results on a range of datasets and performing online training of a 1 million-parameter neural bandit. (3/3) https://t.co/gt2GiSK0g2', 'P.S. We have a GitHub repo https://t.co/lMqGTPZrvg to reproduce the results.']",https://arxiv.org/abs/2112.00195,"In this paper we present a new algorithm for online (sequential) inference in Bayesian neural networks, and show its suitability for tackling contextual bandit problems. The key idea is to combine the extended Kalman filter (which locally linearizes the likelihood function at each time step) with a (learned or random) low-dimensional affine subspace for the parameters; the use of a subspace enables us to scale our algorithm to models with $\sim 1M$ parameters. While most other neural bandit methods need to store the entire past dataset in order to avoid the problem of ""catastrophic forgetting"", our approach uses constant memory. This is possible because we represent uncertainty about all the parameters in the model, not just the final linear layer. We show good results on the ""Deep Bayesian Bandit Showdown"" benchmark, as well as MNIST and a recommender system. ",Efficient Online Bayesian Inference for Neural Bandits
100,1468988847992324102,1064262393981820928,Benedikt Diemer,"[""New paper on the arXiv today! This one's been 3.5 years in the making, and it feels great to finally have this idea see the light of day. A üßµ on what it's about and why I think it's important.\n\n<LINK> <LINK>"", 'The density profiles of dark matter halos have been studied to death, and there are a ton of fitting functions that describe them pretty well (NFW, Einasto, Hernquist, Burkert, etc). But those are only supposed to capture the 1-halo term, that is, the matter orbiting in the halo.', 'This orbiting term decreases with radius and eventually becomes less important than matter falling into the halo for the first time.', ""Here's the kicker: we have no idea what the density profile of the orbiting term actually looks like near its edge! The fitting functions make a prediction, but they aren't constrained there because they could only ever be trained on the total (orbiting + infalling) profiles..."", ""Until now! I coded up an algorithm that tracks particles in a simulation while they fall into halos. It finds their first pericenter and then switches them from the infalling to the orbiting term. This way, we can look at the terms separately and understand what's going on."", 'Perhaps no surprise: the orbiting term looks nothing like the conventional fits at large r. It has a really sharp edge where the largest particle orbits reach their apocenter. The infalling term is also more complicated than thought with a strong dependence on the accretion rate.', 'With this dataset, we can now design a better, more physical fitting function (coming in Paper II) and fundamentally understand why the edge of the orbiting term takes on the shape it does.', ""There's a ton more in the paper, but it's gonna get too long for twitter. Even a 21-page MNRAS paper was way too short for all the figures, so I'm hosting extra online figures at https://t.co/nEzGwuSEPD"", ""Also, the code for the orbiting-infalling separation is public. Please reach out if you're interested in using it, or in working with this dataset - there's still a ton to discover about halo density profiles!\n\nhttps://t.co/0VTQ5h5OCx""]",https://arxiv.org/abs/2112.03921,"The density profiles of dark matter haloes can potentially probe dynamics, fundamental physics, and cosmology, but some of the most promising signals reside near or beyond the virial radius. While these scales have recently become observable, the profiles at large radii are still poorly understood theoretically, chiefly because the distribution of orbiting matter (the one-halo term) is partially concealed by particles falling into halos for the first time. We present an algorithm to dynamically disentangle the orbiting and infalling contributions by counting the pericentric passages of billions of simulation particles. We analyse dynamically split profiles out to 10 R200m across a wide range of halo mass, redshift, and cosmology. We show that the orbiting term experiences a sharp truncation at the edge of the orbit distribution. Its sharpness and position are mostly determined by the mass accretion rate, confirming that the entire profile shape primarily depends on halo dynamics and secondarily on mass, redshift, and cosmology. The infalling term also depends on the accretion rate for fast-accreting haloes but is mostly set by the environment for slowly accreting haloes, leading to a diverse array of shapes that does not conform to simple theoretical models. While the resulting scatter in the infalling term reaches 1 dex, the scatter in the orbiting term is only between 0.1 and 0.4 dex and almost independent of radius. We demonstrate a tight correspondence between the redshift evolution in LCDM and the slope of the matter power spectrum. Our code and data are publicly available. ","A dynamics-based density profile for dark haloes. I. Algorithm and basic
  results"
101,1468936846927450117,871354083151667200,Tomek Korbak,"['How to fine-tune conditional language models with minimal catastrophic forgetting? A thread about our new #NeurIPS21 #CtrlGen workshop paper with @hadyelsahar, @germank and @MarcDymetman. <LINK> 1/7 <LINK>', ""@hadyelsahar @germank @MarcDymetman Imagine you have a @GitHubCopilot-like Python LM that you want to force to generate Python functions that always compile and respect PEP8. Or imagine you're using a T5 summariser and, concerned about hallucination, are looking for ways of forcing the model to be factual. 2/7"", 'In general, the goal is to fine-tune a pre-trained model to generate samples x satisfying a constraint, b(x). However, the fine-tuned model must stay close to the original,\xa0a, in order to avoid catastrophic forgetting. 3/7', 'We represent the desired behaviour of a fine-tuned model conditioned on a context c as a normalised product of two factors: the original model (conditioned on c) and a binary scorer b(x,c). 4/7 https://t.co/QyPfeueg0n', 'To approximate the target distribution p_c representing desired behaviour, we train a new model œÄ_Œ∏ to minimise expected the cross-entropy between œÄŒ∏ conditioned on a context c (drawn from œÑ) and its target distribution p_c. 5/10 https://t.co/OnitxhyWAB', 'For summarisation, we found that CPDG, our method indeed increases the number of relevant named entities in summaries (prediction-source) which in turn increases the Rouge-1 score of generated summaries (computed with respect to their ground-truth summaries). 6/7 https://t.co/BSbSiwC0bQ', 'For code generation, we were able to increase the compilability and decreases the number of PEP8 violations while staying close to the statistics of the original model (e.g. in terms of average sample length). This is in contrast with RL baselines. 7/7 https://t.co/5SP9p0xdHi', ""Come see our poster on Monday, 13 December at NeurIPS CtrlGen workshop! And here's link to the paper again: https://t.co/1TVyMxQptu""]",https://arxiv.org/abs/2112.00791,"Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g. hallucination in abstractive summarization or wrong format in automatic code generation). This raises an important question on how to adapt pre-trained generative models to a new task without destroying its capabilities. Recent work has suggested to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Unfortunately, this approach is limited to unconditional distributions, represented by unconditional EBMs. In this paper, we extend this approach to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on three different control objectives across two tasks: summarization with T5 and code generation with GPT-Neo. Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting. ","Controlling Conditional Language Models with Distributional Policy
  Gradients"
102,1468884072118239236,1193950453941383168,Iason Gabriel,"[""Hi everyone! You can now read our new paper on the ethical and social risks of large language models on @arxiv. It's the product of more than a year's research into these Qs led by @weidingerlaura, and builds upon fantastic work by others in the field üåü\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2112.04359,"This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs. ",Ethical and social risks of harm from Language Models
103,1468851558628696066,84888361,Miriam Rengel,"['Artwork created by <LINK> rendering the title of my new paper ""Ground-based HCN submillimetre measurements in Titan\'s atmosphere: an intercomparison with Herschel observations"". Paper on <LINK> <LINK>']",https://arxiv.org/abs/2112.04125,"The aim of this study is to measure the vertical distribution of HCN on Titan's stratosphere using ground-based submm observations acquired quasi-simultaneously with the Herschel ones. This allows us to perform a consistency check between space and ground-based observations and to build a reference mean HCN vertical profile in Titan's stratosphere. Using APEX and IRAM 30-m, we obtained the spectral emission of HCN (4-3) and (3-2) lines. We applied a line-by-line radiative transfer code to calculate the synthetic spectra of HCN, and a retrieval algorithm based on optimal estimation to retrieve the temperature and HCN distributions. Our derived HCN abundance profiles are consistent with an increase from 40 ppb at ~100 km to 4 ppm at ~200 km, which is an altitude region where the HCN signatures are sensitive. We also demonstrate that the retrieved HCN distribution is sensitive to the data information. Comparisons between our results and the values from Herschel show similar abundance distributions, with maximum differences of 2.5 ppm ranging between 100 and 300 km. These comparisons also allow us to inter-validate both data sets and indicate reliable and consistent measurements. The inferred abundances are also consistent with the distribution in previous observational studies, with the profiles from ALMA, Cassini/CIRS, and SMA (the latest ones below ~230 km). Our HCN profile is also comparable to photochemical models by Krasnopolsky (2014) and Vuitton et al. (2019) below 230 km and consistent with that of Loison et al. (2015) above 250 km. However, it appears to show large differences with respect to the estimates by Loison et al. (2015), Dobrijevic & Loison (2018), and Lora et al. (2018) below 170 km, and by Dobrijevic & Loison (2018) and Lora et al. (2018) above 400 km, although they are similar in shape. We conclude that these particular photochemical models need improvement. ","Ground-based HCN submillimetre measurements in Titan's atmosphere: an
  intercomparison with Herschel observations"
104,1468848089293328384,1141380787163582464,Niko Hauzenberger,"['New working paper (with @FlorianHuber8, Massimiliano Marcellino and @petznico): ""Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty"" <LINK>']",https://arxiv.org/abs/2112.01995,"We develop a non-parametric multivariate time series model that remains agnostic on the precise relationship between a (possibly) large set of macroeconomic time series and their lagged values. The main building block of our model is a Gaussian Process prior on the functional relationship that determines the conditional mean of the model, hence the name of Gaussian Process Vector Autoregression (GP-VAR). We control for changes in the error variances by introducing a stochastic volatility specification. To facilitate computation in high dimensions and to introduce convenient statistical properties tailored to match stylized facts commonly observed in macro time series, we assume that the covariance of the Gaussian Process is scaled by the latent volatility factors. We illustrate the use of the GP-VAR by analyzing the effects of macroeconomic uncertainty, with a particular emphasis on time variation and asymmetries in the transmission mechanisms. Using US data, we find that uncertainty shocks have time-varying effects, they are less persistent during recessions but their larger size in these specific periods causes more than proportional effects on real growth and employment. ",Gaussian Process Vector Autoregressions and Macroeconomic Uncertainty
105,1468690962251722754,1325108931199438848,EckfordLab,"['Information needed to gain an evolutionary fitness advantage can be described by rate distortion theory.\n\nNew paper on arXiv, accepted in Phys Rev E: \n\n<LINK>']",https://arxiv.org/abs/2112.02193,"The existing concept of the ""fitness value of information"" provides a theoretical upper bound on the fitness advantage of using information concerning a fluctuating environment. Using concepts from rate-distortion theory, we develop a theoretical framework to answer a different pair of questions: What is the minimal amount of information needed for a population to achieve a certain growth rate? What is the minimal amount of information gain needed for one sub-population to achieve a certain average selection coefficient over another? We introduce a correspondence between fitness and distortion and solve for the rate-distortion functions of several systems using analytical and numerical methods. Because accurate information processing is energetically costly, our approach provides a theoretical basis for understanding evolutionary ""design principles"" underlying information-cost trade-offs. ",Minimal informational requirements for fitness
106,1468642899285925892,979379437069271043,Pedro Machado,"['New paper with @RyanPlestid, Brdar and de Gouv√™a. We predict observable resonant nu-electron scattering events at @FASERexperiment! All standard physics. Having collaborators like that makes life much easier ;-)\n\nCheck out this excellent thread by Ryan!\n<LINK> <LINK>']",https://arxiv.org/abs/2112.03283,"We consider the resonant production and detection of charged mesons in existing and near-future neutrino scattering experiments with $E_\nu \lesssim 1$ TeV, characteristic of high-energy atmospheric neutrinos or collider-sourced neutrino beams. The most promising candidate is the reaction $\bar{\nu}_e e^-\rightarrow \rho^-\rightarrow \pi^- \pi^0$. We discuss detection prospects at the LHC's forward physics facility with nuclear emulsion (FASER$\nu$) and liquid argon detectors (FLArE) and estimate the number of expected resonance-mediated events in the existing data set of IceCube. We also outline possible detection strategies for the different experimental environments. We predict dozens of events at the forward physics facility and identify cuts with order one signal efficiency that could potentially suppress backgrounds at FASER$\nu$, yielding a signal-to-background ratio larger than 1. Antineutrino-induced $s$-channel meson resonances are yet unobserved Standard Model scattering processes which offer a realistic target for near-term experiments. ",Resonances in $\bar\nu_e-e^-$ scattering below a TeV
107,1468597687985528838,953616889,Justin Read,"['New EDGE paper led by Martin Rey (with @apontzen,@TheAstroProf,@MatthewOrkney,@AmelieSaintonge,@stacyyckim,@drpayeldas) on the highly variable HI content of the very faintest galaxies. Likely to be v. important for doing cosmology with HI galaxy counts!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2112.03280,"We show how the interplay between feedback and mass-growth histories introduces scatter in the relationship between stellar and neutral gas properties of field faint dwarf galaxies ($M_{\star} \lessapprox 10^{6} M_{\odot}$). Across a suite of cosmological, high-resolution zoomed simulations, we find that dwarf galaxies of stellar masses $10^5 \leq M_{\star} \leq 10^{6} M_{\odot}$ are bimodal in their cold gas content, being either HI-rich or HI-deficient. This bimodality is generated through the coupling between (i) the modulation of HI contents by the background of ultraviolet radiation (UVB) at late times and (ii) the significant scatter in the stellar-mass-halo-mass relationship induced by reionization. Furthermore, our HI-rich dwarfs exhibit disturbed and time-variable neutral gas distributions primarily due to stellar feedback. Over the last four billion years, we observe order-of-magnitude changes around the median $M_{HI}$, factor-of-a-few variations in HI spatial extents, and spatial offsets between HI and stellar components regularly exceeding the galaxies' optical sizes. Time variability introduces further scatter in the $M_{\star}-M_{HI}$ relation and affects a galaxy's detectability in HI at any given time. These effects will need to be accounted for when interpreting observations of the population of faint, HI-bearing dwarfs by the combination of optical and radio wide, deep surveys. ","EDGE: What shapes the relationship between HI and stellar observables in
  faint dwarf galaxies?"
108,1468446256947412994,89555917,Bin Liu,"['I have a new paper, entitled ""Robust Sequential Online Prediction with Dynamic Ensemble of Multiple Models: A Concise Introduction"", released on arXiv at <LINK>. This is the first paper in the literature that gives a systematic introduction to BDEMM.']",https://arxiv.org/abs/2112.02374,"In this paper, I give a concise introduction to a generic theoretical framework termed Bayesian Dynamic Ensemble of Multiple Models (BDEMM), which is used for robust sequential online prediction with time series data. This framework has three major features: (1) it employs a model pool, rather than a single model, to capture possible statistical regularities underlying the data; (2) the model pool consists of multiple weighted candidate models, wherein the model weights are adapted online to capture possible temporal evolutions of the data; (3) the adaptation for the model weights follows Bayesian formalism. These features together define BDEMM. To make this introduction comprehensive, I describe BDEMM from five perspectives, namely the basic theories, its different forms of algorithmic implementations, its applications, its connections to related research, open resources for algorithm implementations, followed by a discussion of practical issues for applying it and some open problems that are worth further research. ","Robust Sequential Online Prediction with Dynamic Ensemble of Multiple
  Models: A Concise Introduction"
109,1468421888640757763,1152655010871820288,Kevin Chen,"['Excited to share a new working paper taking a closer look at causal inference in school choice settings, where students are matched to schools via algorithms. The paper derives the identified causal estimands under heterogeneous treatment effects (1/n)\n\n<LINK> <LINK>', 'An important insight, thanks to inspiring work from @BlueprintMIT (among others), is that these algorithms generate variation that allows us to estimate causal effects with minimal assumptions (2/n)', 'School choice has a couple additional intricacies: (1) school assignments are made jointly, and so are not independent across students: Identification is not as simple as in selection-on-observables with iid data! (2) For lots of student-school pairs, P(assignment) = 0 (3/n)', 'In light of üëÜ, what does identification mean here and which causal effects are identified absent further assumptions? \n\nThe paper proposes identification notions and characterizes the set of causal effects identified. (Too long to fit in this tweet!) \n(4/n)', 'It also provides estimators and derives their asymptotic properties, taking fully into account the non-iid-ness of the data. (5/n)', ""Lastly, it examines a few regression/propensity score-based estimators under fully heterogeneous treatment effects. \n\nLack of overlap due to P(assignment) = 0 can be dangerous for regression-adjustment estimators if one's not careful with automatic covariate dropping üö©üö©(6/n) https://t.co/lQ8DE0Jfkz"", 'Also, when the school choice features both lottery- and RDD-type variation, propensity score-based estimators pool over both types of variations. The catch is that, in such a pooling, the RDD-type variation gets *asymptotically vanishing* weights! (7/n)', 'This is because the RDD estimand is over a ""thin"" set of measure zero, and so it cannot be estimated at a ‚àön-rate. As a result, the much-more-precisely-estimated effect coming from lotteries dominates in the estimator. (8/n)\n\nhttps://t.co/1nigly8an5', 'Thanks for listening to my ted talk---all comments/questions welcome! (n/n)']",https://arxiv.org/abs/2112.03872,"We study identification and estimation of treatment effects in common school choice settings, under unrestricted heterogeneity in individual potential outcomes. We propose two notions of identification, corresponding to design- and sampling-based uncertainty, respectively. We characterize the set of causal estimands that are identified for a large variety of school choice mechanisms, including ones that feature both random and non-random tie-breaking; we discuss their policy implications. We also study the asymptotic behavior of nonparametric estimators for these causal estimands. Lastly, we connect our approach to the propensity score approach proposed in Abdulkadiroglu, Angrist, Narita, and Pathak (2017a, forthcoming), and derive the implicit estimands of the latter approach, under fully heterogeneous treatment effects. ",Nonparametric Treatment Effect Identification in School Choice
110,1468279538681466882,995337609084715013,Evan Bauer,"['New paper out on arxiv today with @vedantchandra, @kenjshen, and @jotajotahermes! <LINK>', 'There are three hypervelocity runaway objects that we think got sling-shotted out of white dwarf binary systems after their companions went supernova. Here are their velocities as measured by @ESAGaia EDR3 (solid) and DR2 (dashed). https://t.co/rAR58vVM4F', ""They're moving very fast! But we can say more than just that. We can combine constraints from orbital dynamics (shaded regions) with white dwarf mass-radius relations (black and purple curves) to infer something about their masses and/or temperatures. https://t.co/8WK7aF3EfP"", 'The faster a runaway is running, the smaller it had to be to achieve that orbital velocity before the companion supernova. For white dwarfs, smaller means more massive.', ""Two of the three runaways appear to be moving so fast that they had to be around 0.8-1.1 solar masses. That's pretty massive for a white dwarf!"", 'The other one (D6-2) is much slower, which implies that it was rather puffy for a white dwarf. So it was either on the very low mass end for a white dwarf, or maybe tidally heated and puffed up.\nhttps://t.co/VkU61FbYyX', ""Plenty more to figure out about these objects, but I hope this work at least elucidates some of the constraints on their potential masses for us to work with.\n\n(and JJ might have been right about the tides after all...it's very hard to say exactly what role they might play)"", ""Lastly, I'd also encourage everyone to take a look at a related paper on D6-2 led by @vedantchandra.\nhttps://t.co/Vu7KO7ryWY"", ""@adrianprw @vedantchandra @kenjshen @jotajotahermes There's some good material on that in section 4.2 of Ken's original discovery paper for these systems:\nhttps://t.co/MoJL3pLGsw"", '@adrianprw @vedantchandra @kenjshen @jotajotahermes https://t.co/zNas47PTKz']",https://arxiv.org/abs/2112.03189,"The recently proposed ""dynamically driven double-degenerate double-detonation"" (D6) scenario posits that Type Ia supernovae (SNe) may occur during dynamically unstable mass transfer between two white dwarfs (WDs) in a binary. This scenario predicts that the donor WD may then survive the explosion and be released as a hypervelocity runaway, opening up the exciting possibility of identifying remnant stars from D6 SNe and using them to study the physics of detonations that produce Type Ia SNe. Three candidate D6 runaway objects have been identified in Gaia data. The observable runaway velocity of these remnant objects represents their orbital speed at the time of SN detonation. The orbital dynamics and Roche lobe geometry required in the D6 scenario place specific constraints on the radius and mass of the donor WD that becomes the hypervelocity runaway. In this letter, we calculate the radii required for D6 donor WDs as a function of the runaway velocity. Using mass-radius relations for WDs, we then constrain the masses of the donor stars as well. With measured velocities for each of the three D6 candidate objects based on Gaia EDR3, this work provides a new probe of the masses and mass ratios in WD binary systems that produce SN detonations and hypervelocity runaways. ","Masses of White Dwarf Binary Companions to Type Ia Supernovae Measured
  from Runaway Velocities"
111,1468275897476255745,1310552063999438849,Hauke Group,"['Understanding the exotic phases of gauge theories is crucial to facilitate their quantum simulation. In <LINK> we map out the ground-state phase diagram of quantum link electrodynamics in (2+1)-d. A new paper by T. Hashizume, @JCHalimeh @PhilippHauke, D. Banerjee <LINK>']",https://arxiv.org/abs/2112.00756,"The exploration of phase diagrams of strongly interacting gauge theories coupled to matter in lower dimensions promises the identification of exotic phases and possible new universality classes, and it facilitates a better understanding of salient phenomena in Nature, such as confinement or high-temperature superconductivity. The emerging new techniques of quantum synthetic matter experiments as well as efficient classical computational methods with matrix product states have been extremely successful in one spatial dimension, and are now motivating such studies in two spatial dimensions. In this work, we consider a $\mathrm{U}(1)$ quantum link lattice gauge theory where the gauge fields, represented by spin-$\frac{1}{2}$ operators are coupled to a single flavor of staggered fermions. Using matrix product states on infinite cylinders with increasing diameter, we conjecture its phase diagram in $(2+1)$-d. This model allows us to smoothly tune between the $\mathrm{U}(1)$ quantum link and the quantum dimer models by adjusting the strength of the fermion mass term, enabling us to connect to the well-studied phases of those models. Our study reveals a rich phase diagram with exotic phases and interesting phase transitions to a potential liquid-like phase. It thus furthers the collection of gauge theory models that may guide future quantum-simulation experiments. ",Ground-state phase diagram of quantum link electrodynamics in $(2+1)$-d
112,1468238551301115910,3319563187,Xiaohui Fan,"[""Paper day: it has been long been a puzzle of why there were so few lensed high-redshift quasars discovered; Minghao Yue @_4075167561013's new paper suggests that lensing fraction might have been overestimated (with @feigewang, @AstroJinyi). <LINK> <LINK>""]",https://arxiv.org/abs/2112.02821,"The observed lensed fraction of high-redshift quasars $(\sim0.2\%)$ is significantly lower than previous theoretical predictions $(\gtrsim4\%)$. We revisit the lensed fraction of high-redshift quasars predicted by theoretical models, where we adopt recent measurements of galaxy velocity dispersion functions (VDFs) and explore a wide range of quasar luminosity function (QLF) parameters. We use both analytical methods and mock catalogs which give consistent results. For ordinary QLF parameters and the depth of current high-redshift quasar surveys $(m_z\lesssim22)$, our model suggests a multiply-imaged fraction of $F_\text{multi}\sim 0.4\%-0.8\%$. The predicted lensed fraction is $\sim1\%-6\%$ for the brightest $z_s\sim6$ quasars $(m_z\lesssim19)$, depending on the QLF. The systematic uncertainties of the predicted lensed fraction in previous models can be as large as $2-4$ times and are dominated by the VDF. Applying VDFs from recent measurements decreases the predicted lensed fraction and relieves the tension between observations and theoretical models. Given the depth of current imaging surveys, there are $\sim15$ lensed quasars at $z_s>5.5$ detectable over the sky. Upcoming sky surveys like the LSST survey and the {\em Euclid} survey will find several tens of lensed quasars at this redshift range. ",Revisiting the Lensed Fraction of High-Redshift Quasars
113,1468199594110636032,142654552,Thilo Gross,['We have a new introductory paper to generalized modelling: <LINK>\n\n:)'],https://arxiv.org/abs/2112.00360,"Many current challenges involve understanding the complex dynamical interplay between the constituents of systems. Typically, the number of such constituents is high, but only limited data sources on them are available. Conventional dynamical models of complex systems are rarely mathematically tractable and their numerical exploration suffers both from computational and data limitations. Here we review generalized modeling, an alternative approach to formulating dynamical models. We argue that this approach deals elegantly with the uncertainties that exist in real world data and enables analytical insight or highly efficient numerical investigation. We provide a survey of recent successes of generalized modeling and a guide to the application of this modeling approach in future studies such as complex integrative ecological models. ",Generalized Modeling: A survey and guide
114,1468189412680945671,1283150444,Maurizio Pierini,"['New paper on @arxiv: how to use an #autoencoder to detect jets from dark-sector showers (semi-visible jets) <LINK>', 'Facts: a plain AE with substructure quantities does an excellent job; adding particle-based info didn‚Äôt help (but a more complex architecture could change that in the future); no real advantage in using a #VAE; big advantage to use AE rather than a simple PCA (non linear problem) https://t.co/57iA5rBj29', 'Plugging the AE in a typical #LHC search would boost sensitivity across a wide variety of scenarios, with good generalization robustness from one signal signature to another https://t.co/hxQEqCKj4Q']",https://arxiv.org/abs/2112.02864,"The production of dark matter particles from confining dark sectors may lead to many novel experimental signatures. Depending on the details of the theory, dark quark production in proton-proton collisions could result in semivisible jets of particles: collimated sprays of dark hadrons of which only some are detectable by particle collider experiments. The experimental signature is characterised by the presence of reconstructed missing momentum collinear with the visible components of the jets. This complex topology is sensitive to detector inefficiencies and mis-reconstruction that generate artificial missing momentum. With this work, we propose a signal-agnostic strategy to reject ordinary jets and identify semivisible jets via anomaly detection techniques. A deep neural autoencoder network with jet substructure variables as input proves highly useful for analyzing anomalous jets. The study focuses on the semivisible jet signature; however, the technique can apply to any new physics model that predicts signatures with anomalous jets from non-SM particles. ",Autoencoders for Semivisible Jet Detection
115,1468108378475937793,1191386359707029505,Animesh Mukherjee,"['New paper: ""Marching with the Pink Parade: Evaluating Visual Search Recommendations for Non-binary Clothing Items"". @acm_chi 2022 (case studies)\n\nWith @siddsjaiswal. \n\n#discrimination #visualsearch #lgbtq #Amazon  #stylesnap #beaglevision #lykdat\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2112.02384,"Fashion, a highly subjective topic is interpreted differently by all individuals. E-commerce platforms, despite these diverse requirements, tend to cater to the average buyer instead of focusing on edge cases like non-binary shoppers. This case study, through participant surveys, shows that visual search on e-commerce platforms like Amazon, Beagle.Vision and Lykdat, is particularly poor for non-binary clothing items. Our comprehensive quantitative analysis shows that these platforms are more robust to binary clothing inputs. The non-binary clothing items are recommended in a haphazard manner, as observed through negative correlation coefficients of the ranking order. The participants also rate the non-binary recommendations lower than the binary ones. Another intriguing observation is that male raters are more inclined to make binary judgements compared to female raters. Thus it is clear that these systems are not inclusive to the minority, disadvantaged communities of society, like LGBTQ+ people. We conclude with a call to action for the e-commerce platforms to take cognizance of our results and be more inclusive. ","Marching with the Pink Parade: Evaluating Visual Search Recommendations
  for Non-binary Clothing Items"
116,1468051409052389378,4827295586,Kevin Wagner,"['New paper day! This one has been a long time coming (since my first days as a graduate student at @azstewobs with @dapai). The Scorpion Planet Survey is now available at <LINK> and to appear soon in AJ. See the lengthy thread below for more details:', 'Super-Jupiters at ~10-100 au (like those in this gif) are extreme outcomes of the planet formation process due to the timescales required to at such large separations. Therefore, the occurrence rate of these planets is a powerful constraint on theoretical planet formation models. https://t.co/4qZpItfryL', 'Regardless of model assumptions, not many giant planets are predicted to form at 100 au. Preliminary indications showed that A-type stars (those ~2x as massive than the Sun) might host slightly more wide-orbit giant planets‚Äìindicating these less common stars as ideal targets.', 'Even if wide-orbit super-Jupiters are slightly more frequent around A-stars, a large survey sample size is still required to constrain their occurrence rates. This requires observing out to distances of ~100-200 pc in order to reach percentage-level statistics.', 'Until extreme-AO systems (like @SPHERE_outreach), this simply wasn‚Äôt possible. Now, we can detect the outer planets of an HR 8799-like systems around any A-star at ~100 pc in under an hour. This enables exoplanet imaging surveys to target many young A-stars for the first time.', 'The Sco-Cen region contains the largest number of nearby young A-stars (the stars need to be young so that the planets are bright). Identifying Sco-Cen as a promising planet hunting ground was a brilliant idea of @danielapai years before I arrived as a graduate student.', 'This was a fun project for a beginning graduate student because learning the techniques of exoplanet imaging was challenging to say the least. The long duration of the survey also enabled us to dive deeper into some fun science along the way, such as the spiral disk in HD 100453: https://t.co/zDIPNdcdiF', 'In 2019, just before observatories shut down around the world, we obtained our last follow-up observations. This enabled us to confirm one companion in our survey: HIP 75056Ab, a ~20-30 Jupiter-mass brown dwarf in a ~20 au orbit: https://t.co/UcBzBR9DzO', 'We also included all other data on A-stars that were available in the SPHERE archive. Two other systems in our sample were also discovered to host planets (see Rameau+2013 and Chauvin+2017). The attached image, which shows all of them together, is among my favorites in the paper: https://t.co/h2kPvXh61A', 'By assessing the sensitivity through simulated planet injection and retrieval tests, and also comparing the survey results to Monte Carlo simulations, we were able to place constraints on the underlying planet frequency, as shown in the following plots: https://t.co/26vvqv615e', 'The main result: super-Jupiters at 100 au around A-stars are rare (&lt;10% for 5-35 MJup companions with 95% confidence; consistent with other surveys: e.g., Nielsen+2019, Vigan+2021). Second, and equally important, there is a decreasing frequency with increasing semi-major axis.', 'At ~30 au, the frequency could be as high as 45% (again with 95% confidence). This is evidenced by our discovery of HIP 75056Ab. Due to its small separation of just 0.125‚Äù (~15 au), we had a very small chance of detecting such a companion around any given star.', 'The results of the Scorpion sure are consistent with a scenario in which the majority of wide-orbit super-Jovian planets form via core accretion, perhaps supplemented in their occurrence rate at ~100 au by planet-planet scattering (see, e.g., Marleau+2019).', 'Finally, the increasing frequency with decreasing separation is exciting, as new telescopes that can see planets closer to nearby stars will be able to discover many more planets! Thanks for reading to the end and to @danielapai and other collaborators for their original ideas.', 'Survey* üòÇ']",https://arxiv.org/abs/2112.02168,"The first directly imaged exoplanets indicated that wide-orbit giant planets could be more common around A-type stars. However, the relatively small number of nearby A-stars has limited the precision of exoplanet demographics studies to $\gtrsim$10%. We aim to constrain the frequency of wide-orbit giant planets around A-stars using the VLT/SPHERE extreme adaptive optics system, which enables targeting $\gtrsim$100 A-stars between 100$-$200 pc. We present the results of a survey of 84 A-stars within the nearby $\sim$5$-$17 Myr-old Sco OB2 association. The survey detected three companions$-$one of which is a new discovery (HIP75056Ab), whereas the other two (HD 95086b and HIP65426b) are now-known planets that were included without a priori knowledge of their existence. We assessed the image sensitivity and observational biases with injection and recovery tests combined with Monte Carlo simulations to place constraints on the underlying demographics. We measure a decreasing frequency of giant planets with increasing separation, with measured values falling between 10$-$2% for separations of 30$-$100 au, and 95% confidence-level (CL) upper limits of $\lesssim$45$-$8% for planets on 30$-$100 au orbits, and $\lesssim$5% between 200$-$500 au. These values are in excellent agreement with recent surveys of A-stars in the solar neighborhood$-$supporting findings that giant planets at $\lesssim$100 au are more frequent around A-stars than around solar-type hosts. Finally, the relatively low occurrence rate of super-Jupiters on wide orbits, the positive correlation with stellar mass, and the inverse correlation with orbital separation are consistent with core accretion being their dominant formation mechanism. ","The Scorpion Planet Survey: Wide-Orbit Giant Planets Around Young A-type
  Stars"
117,1467944501356421127,1217628182611927040,Boaz Barak,"['1/4 New paper with Xun Gao, Marcin Kalinowski, Chi-Ning Chou @jerrychou82 , Misha Lukin, and Soonwon Choi on ""spoofing quantum advantage"". \n\nUnlike other recent works we don\'t beat the quantum noisy circuits, but..\n\n<LINK>', '2/4 we use much less classical resources (single GPU) and still get ""in the ballpark"" (~ one order of magnitude worse) of cross-entropy benchmark (XEB) results. As in our prior work https://t.co/ci5ftCuJed we spoof the benchmark *without* fully simulating circuit. https://t.co/Gpj4MelgrT', '3/4 Also, depending on architecture the classical algorithm may scale *better* than a noisy quantum circuit, meaning that increasing the number of qubits makes the classical XEB better than the noisy quantum one. https://t.co/ktS51ReSSX', ""4/4 Bottom line is not that quantum computers don't offer computational advantage, but (as many agree) improving fidelity much more important than increasing number of qubits. Also, need other benchmarks than XEB, ideally for tasks that are independently useful!""]",https://arxiv.org/abs/2112.01657,"Demonstrating quantum advantage requires experimental implementation of a computational task that is hard to achieve using state-of-the-art classical systems. One approach is to perform sampling from a probability distribution associated with a class of highly entangled many-body wavefunctions. It has been suggested that this approach can be certified with the Linear Cross-Entropy Benchmark (XEB). We critically examine this notion. First, in a ""benign"" setting where an honest implementation of noisy quantum circuits is assumed, we characterize the conditions under which the XEB approximates the fidelity. Second, in an ""adversarial"" setting where all possible classical algorithms are considered for comparison, we show that achieving relatively high XEB values does not imply faithful simulation of quantum dynamics. We present an efficient classical algorithm that, with 1 GPU within 2s, yields high XEB values, namely 2-12% of those obtained in experiments. By identifying and exploiting several vulnerabilities of the XEB, we achieve high XEB values without full simulation of quantum circuits. Remarkably, our algorithm features better scaling with the system size than noisy quantum devices for commonly studied random circuit ensembles. To quantitatively explain the success of our algorithm and the limitations of the XEB, we use a theoretical framework in which the average XEB and fidelity are mapped to statistical models. We illustrate the relation between the XEB and the fidelity for quantum circuits in various architectures, with different gate choices, and in the presence of noise. Our results show that XEB's utility as a proxy for fidelity hinges on several conditions, which must be checked in the benign setting but cannot be assumed in the adversarial setting. Thus, the XEB alone has limited utility as a benchmark for quantum advantage. We discuss ways to overcome these limitations. ",Limitations of Linear Cross-Entropy as a Measure for Quantum Advantage
118,1467897870246289411,52360091,Javier M. Duarte,"['New paper introducing an automatic conversion tool in <LINK> for graph neural networks on FPGAs for charged particle tracking, which scales to thousands of nodes and edges with microsecond-level inference <LINK> <LINK> <LINK>']",https://arxiv.org/abs/2112.02048,"The determination of charged particle trajectories in collisions at the CERN Large Hadron Collider (LHC) is an important but challenging problem, especially in the high interaction density conditions expected during the future high-luminosity phase of the LHC (HL-LHC). Graph neural networks (GNNs) are a type of geometric deep learning algorithm that has successfully been applied to this task by embedding tracker data as a graph -- nodes represent hits, while edges represent possible track segments -- and classifying the edges as true or fake track segments. However, their study in hardware- or software-based trigger applications has been limited due to their large computational cost. In this paper, we introduce an automated translation workflow, integrated into a broader tool called $\texttt{hls4ml}$, for converting GNNs into firmware for field-programmable gate arrays (FPGAs). We use this translation tool to implement GNNs for charged particle tracking, trained using the TrackML challenge dataset, on FPGAs with designs targeting different graph sizes, task complexites, and latency/throughput requirements. This work could enable the inclusion of charged particle tracking GNNs at the trigger level for HL-LHC experiments. ",Graph Neural Networks for Charged Particle Tracking on FPGAs
119,1467875902864429057,2347666219,John F Donoghue,"['Our new paper giving a brief overview of our ongoing study of Quadratic Gravity: \n\n<LINK>\n\nQuadratic gravity is a renormalizeable QFT for quantum gravity. However, it violates some features of QFT.', 'Our overarching question is whether it nevertheless is a viable field theory. Please read the paper to see what we know about this question.']",https://arxiv.org/abs/2112.01974,"We provide a brief overview of what is known about Quadratic Gravity, which includes terms quadratic in the curvatures in the fundamental action. This is proposed as a renormalizeable UV completion for quantum gravity which continues to use the metric as the fundamental dynamical variable. However, there are unusual field-theoretic consequences because the propagators contain quartic momentum dependence. At the present stage of our understanding, Quadratic Gravity continues to be a viable candidate for a theory of quantum gravity. ",On Quadratic Gravity
120,1467855192280420362,21540543,Grace Lindsay,"['üö®New paper on arXiv! In this work, we wanted to know how the visual system of a fully-embodied reinforcement learning agent compares to a network with the same architecture embedded in systems trained through supervised or unsupervised learning. <LINK> More üëáüßµ <LINK>', 'For the RL system, we used the virtual rodent of https://t.co/3vyG40slO1. This agent takes in egocentric visual and proprioceptive information and uses it to control a realistic rodent body to perform four different tasks. https://t.co/250t062Ras', ""The visual encoder of that agent is a multi-layer ResNet (A). We use this same architecture for all our models, and train them using sampled egocentric images from the rodent's actions (B). https://t.co/gyzgrkSO2L"", 'We explored 8 comparison models: four trained in a supervised way to predict state-related variables, three trained with unsupervised algorithms, and one untrained. https://t.co/UZYopIS4IN', 'First, transfer learning tests showed that the final layer of the RL model contained enough information to perform decently on all our supervised tasks https://t.co/6U81OrkE0s', 'Next we used representational similarity analyses to compare recorded activity. The networks all showed surprisingly high similarity at the first recorded layer, but by the final layer became much more distinct (diagonal blocks show random 3 instantiations of each) https://t.co/bHHw9QAgRQ', 'Looking at the actual dissimilarity matrices (how dissimilarly a given neural population represents two images), it became clear that the RL model represented all image much more differently (higher dissimilarity values) than other networks did https://t.co/hkUC0C30JW', 'This led us to explore the sparsity and dimensionality of the RL representation. Indeed, at most layers---but particularly the last layer---the RL model has a very sparse and high dimensional representation of the images, compared to the other models.', 'To understand if the RL agent develops this high dimensional representation in order to directly control a high-dimensional output space (its body), we tried training the visual system directly to produce action outputs.', 'Surprisingly, we found that visual inputs cannot be used to predict immediate motor actions. In fact, in a model given both visual and proprioceptive inputs, the network learns to zero-out the visual inputs and simply rely on proprioception https://t.co/yJQhns0JV6', 'This suggests that the agent uses its visual input for long-term motor planning and goal-directed behaviors---not for immediate control of its body. This has implications for the difficulty of studying modules of a larger system in isolation. /end', '@enasmel The training details are in the original virtual rodent paper (linked above, screenshot below). I agree it would be fun (though computationally expensive!) to compare. https://t.co/kzhjIkkEA1', '@patrickmineault The visual and proprioceptive system both work on one time step at a time, but in the full RL agent they feed into some recurrent layers that could be integrating over time', ""@ShahabBakht True! The artificial environment for the virtual rodent is quite visually-specific though, so I wouldn't be surprised if it hasn't learned the kind of general visual features a real mouse would."", '@neuroecology thanks!', ""@vkakerbeck @TimKietzmann Yea we haven't been able to go to deep into it yet really (and it would be good to see if it arises in our model from other RL settings too).  I'll take a look!""]",https://arxiv.org/abs/2112.02027,"Artificial neural systems trained using reinforcement, supervised, and unsupervised learning all acquire internal representations of high dimensional input. To what extent these representations depend on the different learning objectives is largely unknown. Here we compare the representations learned by eight different convolutional neural networks, each with identical ResNet architectures and trained on the same family of egocentric images, but embedded within different learning systems. Specifically, the representations are trained to guide action in a compound reinforcement learning task; to predict one or a combination of three task-related targets with supervision; or using one of three different unsupervised objectives. Using representational similarity analysis, we find that the network trained with reinforcement learning differs most from the other networks. Using metrics inspired by the neuroscience literature, we find that the model trained with reinforcement learning has a sparse and high-dimensional representation wherein individual images are represented with very different patterns of neural activity. Further analysis suggests these representations may arise in order to guide long-term behavior and goal-seeking in the RL agent. Finally, we compare the representations learned by the RL agent to neural activity from mouse visual cortex and find it to perform as well or better than other models. Our results provide insights into how the properties of neural representations are influenced by objective functions and can inform transfer learning approaches. ","Divergent representations of ethological visual inputs emerge from
  supervised, unsupervised, and reinforcement learning"
121,1467848223863853057,1202165863895449600,Fran√ßois Charton,"['Transformers can be trained to solve problems of linear algebra (matrix transposition, addition, multiplication, inversion and eigenvalues) to very high accuracy. 1/4\nOur new paper is on Arxiv: <LINK> <LINK>', 'Given enough examples, models trained on random matrices with independent and identically distributed (iid) coefficients (Wigner matrices) can predict with high precision. 2/4 https://t.co/ZPemc9wZgs', 'Models trained on Wigner matrices struggle to generalize to matrices with different eigenvalue distributions (e.g. positive, Laplace).\nModels trained on matrices with Laplace-distributed eigenvalues (non-iid coefficients), generalize to Wigner and positive matrices. 3/4 https://t.co/TBKCSZZkpl', 'Training from samples including matrices of different dimensions helps the model to scale-up. 4/4 https://t.co/Fq8GIag5HU']",https://arxiv.org/abs/2112.01898,"Most applications of transformers to mathematics, from integration to theorem proving, focus on symbolic computation. In this paper, we show that transformers can be trained to perform numerical calculations with high accuracy. We consider problems of linear algebra: matrix transposition, addition, multiplication, eigenvalues and vectors, singular value decomposition, and inversion. Training small transformers (up to six layers) over datasets of random matrices, we achieve high accuracies (over 90%) on all problems. We also show that trained models can generalize out of their training distribution, and that out-of-domain accuracy can be greatly improved by working from more diverse datasets (in particular, by training from matrices with non-independent and identically distributed coefficients). Finally, we show that few-shot learning can be leveraged to re-train models to solve larger problems. ",Linear algebra with transformers
122,1467828392779464712,1084300516543381505,endo_suguru,"['<LINK>  Our new paper is out! This error-mitigated metrology inspired by virtual distillation can filter out fluctuating noise and mitigate systematic errors. The scaling of the sensitivity can also be recovered!', '@gyken_post „ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„ÅôÔºÅ']",https://arxiv.org/abs/2112.01850,"Quantum metrology with entangled resources aims to achieve sensitivity scaling beyond the standard quantum limit by harnessing quantum effects even in the presence of environmental noise. So far, scaling has been mainly discussed from the viewpoint of reducing statistical errors under the assumption of perfect knowledge of a noise model. However, we cannot always obtain complete information about a noise model due to coherence time fluctuations, which are frequently observed in experiments. Such unknown fluctuating noise leads to systematic errors and nullifies the quantum advantages. Here, we propose an error-mitigated quantum metrology that can filter out unknown fluctuating noise with the aid of purification-based quantum error mitigation. We demonstrate that our protocol mitigates systematic errors and recovers superclassical scaling in a practical situation with time-inhomogeneous bias-inducing noise. ",Error-mitigated quantum metrology
123,1467820791010152452,572479189,Manlio De Domenico,"['New paper out! We use data from @covid19obs to gain some new insights on the #infodemic phenomenon &amp; its dynamics for #covid19\n\nA great collaboration including @pppiergiorgio @ricgallotti @ComuneLab &amp; @EugeniaPolizzi @giulia_ndr @LABSS_CNR \n\nA thread: 1/\n\n<LINK> <LINK>', ""Which factors facilitate the share of misinformation in online settings?\n\nMany, and social psychology provided elements to support the role of social influence on individual's beliefs and actions.\n\nIn our work we explore ‚Äúpluralistic ignorance‚Äù.\n\n2/ https://t.co/J0VsHlEmJR"", 'Online users perceive that the majority of their peers thinks/acts in some way, but this perception doesnt need to be necessarily effective:\nMajority of members of a group privately disagree w/ an opinion/social norm, yet (incorrectly) believe that the other members accept it\n\n3/ https://t.co/qcLwerbOrt', ""So individuals believe that they'll lose social reputation from other members of their\ngroup if they behave how they wish. Therefore,\npeople are likely to act in accordance with the\nperceived majority even if they privately disapprove of the behavior.\n\n4/"", 'We have tried to find hints of this theory in the data from the COVID19 Infodemic Observatory, inspecting the role of the underlying communication network in sharing specific type of content, providing ground to develop misperception about the opinion of the majority\n\n5/ https://t.co/i0smpIx8gl', ""We focused on ~8M interactions among 1.6M users in the U.S. from Jan 2020 to May 2020.\nRemarkably, most of the content type tend to be shared by few users, w/ a high centralization around them (creators) and many potential 'consumers'.\n\n6/ https://t.co/MiqaGvyejN"", 'Summarizing the whole dynamics into a simple diagram, we confirm that the basin of consumers is huge, with a dynamic exchange between the two groups although a stable core exists.\n\nConsumers: exposed to pluralistic ignorance!\n\nA galaxy of other users surrounds the two groups.\n\n7/ https://t.co/lrSfypk68e', 'Transitions between the two groups are fluid, with return times up to more than 1 month.\n\n1) the probability of returning to either one of the two fake news groups decreases w/ time\n2) it is always more likely to return to the same group that a user left than in the other one\n\n8/ https://t.co/g4lwIwt09o', ""Our prior allows to verify the existence of a causality relationship between the social signals observed from creators and consumers. \n\nIt exists, but it's not possible to reach a unique/definitive conclusion for two main reasons: \n\n9/ https://t.co/2S4DAfWDac"", '1) there is a very strong feedback loop that propagates causal effects over fast time scales\n\nor \n\n2) the two variables are driven by a third, external or hidden, common cause\n\nTherefore, further research is needed here before making any claim.\n\n10/', ""TL;DR\n\nIn the group sharing unreliable online content about #COVID19: 14% of users act as creators, 86% act as consumers.\n\nThere are fluid transitions btw the two groups.\n\nOverexposure of creators' tweets might lead to pluralistic ignorance, suggesting a path to #infodemic\n\n/end https://t.co/TpE0fQh6Qo"", '@richardclegg @covid19obs @pppiergiorgio @ricgallotti @ComuneLab @EugeniaPolizzi @giulia_ndr @LABSS_CNR @FBK_research @DFAUnipd @SCBriand @research2policy @TDPurnat Sure, the best is to read the details about how the tweets are gathered, bias quantified, etc. For the period of interest, the main reference is our paper on assessing infodemic risk, where we provide crystal clear explanation of everything.\n\nhttps://t.co/6TYKcY5NLm', ""@danbri @covid19obs @pppiergiorgio @ricgallotti @ComuneLab @EugeniaPolizzi @giulia_ndr @LABSS_CNR @FBK_research @DFAUnipd @SCBriand @research2policy @TDPurnat Not yet, but it's worth stressing that we don't employ any type of AI algorithm to recognize unreliable content.\n\nWe rely on expert-coded classification of sources (web dns). Therefore we see only the tip of the iceberg, but we are very confident that what we see is correct.""]",https://arxiv.org/abs/2112.01304,"Online platforms play a relevant role in the creation and diffusion of false or misleading news. Concerningly, the COVID-19 pandemic is shaping a communication network - barely considered in the literature - which reflects the emergence of collective attention towards a topic that rapidly gained universal interest. Here, we characterize the dynamics of this network on Twitter, analyzing how unreliable content distributes among its users. We find that a minority of accounts is responsible for the majority of the misinformation circulating online, and identify two categories of users: a few active ones, playing the role of ""creators"", and a majority playing the role of ""consumers"". The relative proportion of these groups ($\approx$14% creators - 86% consumers) appears stable over time: Consumers are mostly exposed to the opinions of a vocal minority of creators, that could be mistakenly understood as of representative of the majority of users. The corresponding pressure from a perceived majority is identified as a potential driver of the ongoing COVID-19 infodemic. ","The voice of few, the opinions of many: evidence of social biases in
  Twitter COVID-19 fake news sharing"
124,1467667472035766273,1272909280941879296,John F Wu,"[""Check out our new paper on arXiv: <LINK>\n\n(I'm gonna write a proper Twitter thread on this tomorrow)"", 'For a summary of results + the code, take a look at our GitHub repo: https://t.co/J5Vvxn23Gu']",https://arxiv.org/abs/2112.01542,"We present ""Extending the Satellites Around Galactic Analogs Survey"" (xSAGA), a method for identifying low-$z$ galaxies on the basis of optical imaging, and results on the spatial distributions of xSAGA satellites around host galaxies. Using spectroscopic redshift catalogs from the SAGA Survey as a training data set, we have optimized a convolutional neural network (CNN) to identify $z < 0.03$ galaxies from more distant objects using image cutouts from the DESI Legacy Imaging Surveys. From the sample of $> 100,000$ CNN-selected low-$z$ galaxies, we identify $>20,000$ probable satellites located between 36-300 projected kpc from NASA-Sloan Atlas central galaxies in the stellar mass range $9.5 < \log(M_\star/M_\odot) < 11$. We characterize the incompleteness and contamination for CNN-selected samples, and apply corrections in order to estimate the true number of satellites as a function of projected radial distance from their hosts. Satellite richness depends strongly on host stellar mass, such that more massive host galaxies have more satellites, and on host morphology, such that elliptical hosts have more satellites than disky hosts with comparable stellar masses. We also find a strong inverse correlation between satellite richness and the magnitude gap between a host and its brightest satellite. The normalized satellite radial distribution between 36-300 kpc does not depend strongly on host stellar mass, morphology, or magnitude gap. The satellite abundances and radial distributions we measure are in reasonable agreement with predictions from hydrodynamic simulations. Our results deliver unprecedented statistical power for studying satellite galaxy populations, and highlight the promise of using machine learning for extending galaxy samples of wide-area surveys. ","Extending the SAGA Survey (xSAGA) I: Satellite Radial Profiles as a
  Function of Host Galaxy Properties"
125,1466923189691428865,1441424997516554242,Jorge Garc√≠a Rojas,"[""My new paper on 2D spectroscopy of some peculiar planetary nebulae using #MUSE at @ESO's VLT <LINK>""]",https://arxiv.org/abs/2112.00480,"We present MUSE deep integral-field unit spectroscopy of three planetary nebulae(PNe) with high abundance discrepancy factors (ADF > 20): NGC 6778, M 1-42 and Hf 2-2. We have constructed flux maps for more than 40 emission lines, and use them to build extinction, electron temperature (T$_e$), electron density (n$_e$), and ionic abundances maps of a number of ionic species. The effects of the contribution of recombination to the auroral [N II] and [O II] lines on T$_e$ and the abundance maps of low-ionization species are evaluated using recombination diagnostics. As a result, low T$_e$ values and a downward gradient of T$_e$ are found toward the inner zones of each PN. Spatially, this nearly coincides with the increase of abundances of heavy elements measured using recombination lines in the inner regions of PNe, and strongly supports the presence of two distinct gas phases: a cold and metal-rich and a warm one with ""normal"" metal content. We have simultaneously constructed, for the first time, the ADF maps of O$^+$ and O$^{2+}$ and found that they centrally peak for all three PNe under study. We show that the main issue when trying to compute realistic abundances from either ORLs or CELs is to estimate the relative contribution of each gas component to the H I emission, and we present a method to evaluate it. It is also found that, for the studied high-ADF PNe, the amount of oxygen in the cold and warm regions is of the same order. ",MUSE spectroscopy of planetary nebulae with high abundance discrepancies
126,1466817091462262785,893787241,Benjamin Cordier,"['New paper, Biology and Medicine in the Landscape of Quantum Advantages, is up on arXiv. \n<LINK>']",https://arxiv.org/abs/2112.00760,"Quantum computing holds significant potential for applications in biology and medicine, spanning from the simulation of biomolecules to machine learning approaches for subtyping cancers on the basis of clinical features. This potential is encapsulated by the concept of a quantum advantage, which is typically contingent on a reduction in the consumption of a computational resource, such as time, space, or data. Here, we distill the concept of a quantum advantage into a simple framework that we hope will aid researchers in biology and medicine pursuing the development of quantum applications. We then apply this framework to a wide variety of computational problems relevant to these domains in an effort to i) assess the potential of quantum advantages in specific application areas and ii) identify gaps that may be addressed with novel quantum approaches. Bearing in mind the rapid pace of change in the fields of quantum computing and classical algorithms, we aim to provide an extensive survey of applications in biology and medicine that may lead to practical quantum advantages. ",Biology and medicine in the landscape of quantum advantages
127,1466779927097847819,1079149961433047041,Gerson J. Ferreira,"[""Our new paper on theory and experimental transport measurements of triple HgTe quantum wells. The Shubnikov-de Haas (SdH) oscillation frequencies match surprisingly well with the theory. But there's room for improvements on the SdH theory for Dirac models\n\n<LINK>""]",https://arxiv.org/abs/2112.01307,"Quantum wells formed by layers of HgTe between Hg$_{1-x}$Cd$_x$Te barriers lead to two-dimensional (2D) topological insulators, as predicted by the BHZ model. Here, we theoretically and experimentally investigate the characteristics of triple HgTe quantum wells. We describe such heterostructure with a three dimensional $8\times 8$ Kane model, and use its eigenstates to derive an effective 2D Hamiltonian for the system. From these we obtain a phase diagram as a function of the well and barrier widths and we identify the different topological phases composed by zero, one, two, and three sets of edge states hybridized along the quantum wells. The phase transitions are characterized by a change of the spin Chern numbers and their corresponding band inversions. Complementary, transport measurements are experimentally investigated on a sample close to the transition line between the phases with one and two sets of edges states. Accordingly, for this sample we predict a gapless spectrum with low energy bulk conduction bands given by one parabolic and one Dirac band, and with edge states immersed in the bulk valance bands. Consequently, we show that under these conditions, local and non-local transport measurements are inconclusive to characterize a sole edge state conductivity due to bulk conductivity. On the other hand, Shubnikov-de Haas (SdH) oscillations show an excellent agreement with our theory. Particularly, we show that the measured SdH oscillation frequencies agrees with our model and show clear signatures of the coexistence of a parabolic and Dirac bands. ",Engineering topological phases in triple HgTe/CdTe quantum wells
128,1466764797140217860,968566761674207232,Michael Harhay,"['""On the robustness and precision of mixed-model analysis of covariance in cluster-randomized trials""\n\nNew paper (preprint) led by @BingkaiWang, w/ Dylan Small, @tmorris_mrc, &amp; @FrankFanLi! \n\n<LINK> <LINK>']",https://arxiv.org/abs/2112.00832,"In the analyses of cluster-randomized trials, a standard approach for covariate adjustment and handling within-cluster correlations is the mixed-model analysis of covariance (ANCOVA). The mixed-model ANCOVA makes stringent assumptions, including normality, linearity, and a compound symmetric correlation structure, which may be challenging to verify and may not hold in practice. When mixed-model ANCOVA assumptions are violated, the validity and efficiency of the model-based inference for the average treatment effect are currently unclear. In this article, we prove that the mixed-model ANCOVA estimator for the average treatment effect is consistent and asymptotically normal under arbitrary misspecification of its working model. Under equal randomization, we further show that the model-based variance estimator for the mixed-model ANCOVA estimator remains consistent, clarifying that the confidence interval given by standard software is asymptotically valid even under model misspecification. Beyond robustness, we also provide a caveat that covariate adjustment via mixed-model ANCOVA may lead to precision loss compared to no adjustment when the covariance structure is misspecified, and describe when a cluster-level ANCOVA becomes more efficient. These results hold under both simple and stratified randomization, and are further illustrated via simulations as well as analyses of three cluster-randomized trials. ",On the mixed-model analysis of covariance in cluster-randomized trials
129,1466749550597713926,1185609272354312193,Loredana Bellantuono,['üö® New paper out! üö®\nMind the gap in university rankings: a complex network approach towards fairness\n<LINK> \n\nAddressing structural #bias in #universityrankings with a data-driven #debiasing strategy\n#complexnetworks #datascienceforsocialgood'],https://arxiv.org/abs/2112.01341,"University rankings are increasingly adopted for academic comparison and success quantification, even to establish performance-based criteria for funding assignment. However, rankings are not neutral tools, and their use frequently overlooks disparities in the starting conditions of institutions. In this research, we detect and measure structural biases that affect in inhomogeneous ways the ranking outcomes of universities from diversified territorial and educational contexts. Moreover, we develop a fairer rating system based on a fully data-driven debiasing strategy that returns an equity-oriented redefinition of the achieved scores. The key idea consists in partitioning universities in similarity groups, determined from multifaceted data using complex network analysis, and referring the performance of each institution to an expectation based on its peers. Significant evidence of territorial biases emerges for official rankings concerning both the OECD and Italian university systems, hence debiasing provides relevant insights suggesting the design of fairer strategies for performance-based funding allocations. ","Mind the gap in university rankings: a complex network approach towards
  fairness"
130,1466707506072334339,561899047,Aki Vehtari,"['New review paper on prior elicitation: Prior knowledge elicitation: The past, present, and future <LINK> with @MikkolaPetrus, @aloctavodia, @suyoghc, M Hartmann, @OriolAbril, @somewhaton, @henri_pesonen, J Corander, I, @samikaski, @paulbuerkner, A Klami; @FCAI_fi <LINK>', ""@JessicaHullman @MikkolaPetrus @aloctavodia @suyoghc @OriolAbril @somewhaton @henri_pesonen @samikaski @paulbuerkner @FCAI_fi @YeaseulKim Thanks! I was aware of your great work on visualization (and we're citing one), but clearly we missed/forgot the connection to prior elicitation. We'll add these, too""]",https://arxiv.org/abs/2112.01380,"Specification of the prior distribution for a Bayesian model is a central part of the Bayesian workflow for data analysis, but it is often difficult even for statistical experts. Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions, and offers a solution to the prior specification problem, in principle. In practice, however, we are still fairly far from having usable prior elicitation tools that could significantly influence the way we build probabilistic models in academia and industry. We lack elicitation methods that integrate well into the Bayesian workflow and perform elicitation efficiently in terms of costs of time and effort. We even lack a comprehensive theoretical framework for understanding different facets of the prior elicitation problem. Why are we not widely using prior elicitation? We analyze the state of the art by identifying a range of key aspects of prior knowledge elicitation, from properties of the modelling task and the nature of the priors to the form of interaction with the expert. The existing prior elicitation literature is reviewed and categorized in these terms. This allows recognizing under-studied directions in prior elicitation research, finally leading to a proposal of several new avenues to improve prior elicitation methodology. ","Prior knowledge elicitation: The past, present, and future"
131,1466629144406413319,90047221,Robin Kothari,"['New paper on the arXiv with @Ankit_garg06, @PNetrapalli, and @shlshrf, which will be presented as a spotlight talk at NeurIPS 2021. Summary: We show nearly optimal quantum and randomized lower bounds for smooth convex optimization.\n<LINK>\n<LINK> <LINK>']",https://arxiv.org/abs/2112.01118,"We study the complexity of optimizing highly smooth convex functions. For a positive integer $p$, we want to find an $\epsilon$-approximate minimum of a convex function $f$, given oracle access to the function and its first $p$ derivatives, assuming that the $p$th derivative of $f$ is Lipschitz. Recently, three independent research groups (Jiang et al., PLMR 2019; Gasnikov et al., PLMR 2019; Bubeck et al., PLMR 2019) developed a new algorithm that solves this problem with $\tilde{O}(1/\epsilon^{\frac{2}{3p+1}})$ oracle calls for constant $p$. This is known to be optimal (up to log factors) for deterministic algorithms, but known lower bounds for randomized algorithms do not match this bound. We prove a new lower bound that matches this bound (up to log factors), and holds not only for randomized algorithms, but also for quantum algorithms. ","Near-Optimal Lower Bounds For Convex Optimization For All Orders of
  Smoothness"
132,1466627657810939904,1329340625783828482,Eduardo Vitral,"[""It's paper day! <LINK>\nIn this new work that @BoldriniP and I just submitted, we seek to differentiate attributes between globular clusters that were formed inside their own dark matter mini-halo, and those who were not.  \nCheck out the thread below! @astroIAP <LINK>"", 'It is hard to know how globular star clusters were formed! They are generally very old (several Gyr) and usually too small to be well resolved in cosmological simulations or in high redshift data. However, many proposed formation scenarios seem realistic, and could coexist!', 'In this work, we focus on the scenario where they are formed in their own dark matter (mini-)halo, and use N-body simulations to seek dynamical and morphological differences between globular clusters that were formed inside their own dark matter mini-halo, and those who were not.', 'We base our simulations on the Fornax dwarf galaxy and its main five globular clusters (we set them with and without dark matter), to probe a realistic scenario. Below is a image of this system (Credit: ESO/Digitized Sky Survey 2, with post-treatment from @Giusepp39181130) https://t.co/oNmzAfwyD6', 'We find that due to the increment of mass from the dark matter mini-halo, globular clusters (GCs) with dark matter have bigger tidal radii, causing dark matter to work as a shield against tidal stripping, being itself stripped beforehand the stars... Such as in the figures! üòÖ https://t.co/ExvsoF8GhZ', 'Here, we show how this shield works for one of the GCs! The dark matter (DM) velocity dispersion is low (blue region) inside the tidal radius, which in turn is bigger than the region where GC stars are! Outside it, the DM is affected, and the dispersion increases (red region). https://t.co/AQNQbBTlGh', 'As a consequence, tidal effects such as inflation of the stellar velocity dispersion, development of prominent tidal tails (as in the video above), ellipticity increase and diffusion of the stellar distribution profile are generally much milder in GCs originally embedded in DM. https://t.co/SSXXprGCHg', 'Nevertheless, GCs can still lose their DM shield (as for GCs 1 and 2 below), if they experience more severe tidal forces, which will strip much of their DM mini-halo. https://t.co/seOQf9T5i7', 'We also find that, given the diffuse DM distribution, the density ratio of DM to GC stars (black squares) up to roughly 10 pc from the clusters centers is of the order of 1% in GCs that retain most of their mini-halo... This renders the search for DM in GCs super difficult! https://t.co/XC5vYq2dig', 'The points above (better presented in the paper) are a valuable compilation of properties expected from GCs originally embedded in DM mini-halos, and can serve as guidance for those wishing to search for DM in GCs, by means of the rich data sets from @ESAGaia, for example!', 'If you are still interested, I invite you to go check the paper, and send us a mail if you have any comments that we might incorporate in the re-submissionüòÄ']",https://arxiv.org/abs/2112.01265,"We seek to differentiate dynamical and morphological attributes between globular clusters that were formed inside their own dark matter mini-halo, and those who were not. For that, we employ high resolution $N$-body simulations of globular clusters with (and without) an enveloping dark matter mini-halo, orbiting a host galaxy. We set the same prescriptions of the Fornax dwarf spheroidal galaxy and its main five globular clusters, and use $N$-body particles for all components (i.e., stars and dark matter, for both Fornax and its clusters). For clusters embedded in dark matter, we observe that the increment of mass from the extra dark component triggers a tidal radius growth that allows the mini-halo to work as a protective shield against tidal stripping, being itself stripped beforehand the stars. Consequently, tidal effects such as inflation of the stellar velocity dispersion, development of prominent tidal tails, ellipticity increase and diffusion of the stellar distribution profile are generally much milder in clusters originally embedded in dark matter. However, this shielding effect becomes negligible after an important amount of dark matter has been stripped, which happens faster for clusters having simultaneously short orbital periods, low typical orbital radii and relatively high eccentricities. Finally, we notice that even for clusters that retain a large amount of dark matter at redshift zero, their inner regions are still predominantly composed of stars, with the typical density ratio of dark matter to cluster stars being of the order of $1\%$ up to roughly $10~$pc away from the clusters' centre. ",Orbiting globular clusters formed in dark matter mini-halos
133,1466587248594341893,39640065,Dan Scolnic,"['New Pantheon+ paper led by grad student Anthony Carr, with famous advisor @tamarastro, and including myself, @Astro_KhaleD, @DillonBrout, @erikpeterson23 - <LINK> - this paper does a super comprehensive review of redshifts of all the SNeIa used for cosmology.', ""This paper meant a lot to me as over last couple of years, there have been claims that due to some small differences in redshifts of same objects in different analyses, supernova studies weren't reproducible and maybe then dark energy didn't exist.  Instead of ignoring that.."", ""Anthony dove in, and he led a super deep clean of any redshift used for our big compilation papers.  This covers all conversions, host-galaxy assignments, various corrections... - and he finds - it doesn't change H0 or w much, but we couldn't be totally sure till he did this."", 'Stay tuned for next week.  Some really really exciting things to come!']",https://arxiv.org/abs/2112.01471,"We examine the redshifts of a comprehensive set of published Type Ia supernovae, and provide a combined, improved catalog with updated redshifts. We improve on the original catalogs by using the most up-to-date heliocentric redshift data available; ensuring all redshifts have uncertainty estimates; using the exact formulae to convert heliocentric redshifts into the Cosmic Microwave Background (CMB) frame; and utilizing an improved peculiar velocity model that calculates local motions in redshift-space and more realistically accounts for the external bulk flow at high-redshifts. In total we reviewed 2821 supernova redshifts; 534 are comprised of repeat-observations of the same supernovae and 1764 pass the cosmology sample quality cuts. We found 5 cases of missing or incorrect heliocentric corrections, 44 incorrect or missing supernova coordinates, 230 missing heliocentric or CMB frame redshifts, and 1200 missing redshift uncertainties. Of the 2287 unique Type Ia supernovae in our sample (1594 of which satisfy cosmology-sample cuts) we updated 990 heliocentric redshifts. The absolute corrections range between $10^{-8} \leq \Delta z \leq 0.038$, and RMS$(\Delta z) \sim 3\times 10^{-3}$. The sign of the correction was essentially random, so the mean and median corrections are small: $4\times 10^{-4}$ and $4\times 10^{-6}$ respectively. We examine the impact of these improvements for $H_0$ and the dark energy equation of state $w$ and find that the cosmological results change by $\Delta H_0 = -0.11$ km s$^{-1}$ Mpc$^{-1}$ and $\Delta w = -0.001$, both significantly smaller than previously reported uncertainties for $H_0$ of 1.4 km s$^{-1}$ Mpc$^{-1}$ and $w$ of 0.04 respectively. ","The Pantheon+ Analysis: Improving the Redshifts and Peculiar Velocities
  of Type Ia Supernovae Used in Cosmological Analyses"
134,1466449032092389379,2377407248,Daniel Whiteson,"['New paper!\n\n       ""New physics in tri-boson event topologies""\n\n<LINK>\n\nExperimental studies led by UCI *undergraduate* Jesus Caridad Ramirez.', 'As the LHC datasets grow, rare events become a new playground for seeing rare events and looking for new physics. We studied the three-boson final states: https://t.co/HVRNTgrOD4', 'Because there are a lot of ways to get three bosons, which can all decay a lot of ways, there were a LOT of variations for Jesus to study, such as: https://t.co/qs4gu4ozjV', 'And: https://t.co/slS3oT5vs3', 'AND: https://t.co/Kls5Hhh8Js', 'All of which were interpreted in effective field theories that describe a wide set of models by our theory friends: https://t.co/zFBnjDdkr0', '@KyleCranmer Next paper: ""Sarcasm on twitter""']",https://arxiv.org/abs/2112.00137,"We present a study of the sensitivity to models of new physics of proton collisions resulting in three electroweak bosons. As a benchmark, we analyze models in which an exotic scalar field $\phi$ is produced in association with a gauge boson ($V=\gamma$ or $Z$). The scalar then decays to a pair of bosons, giving the process $pp\rightarrow \phi V\rightarrow V'V""V$. We interpret our results in a set of effective field theories where the exotic scalar fields couple to the Standard Model through pairs of electroweak gauge bosons. We estimate the sensitivity of the LHC and HL-LHC datasets and find sensitivity to cross sections in the 10 fb -- 0.5 fb range, corresponding to scalar masses of 500 GeV to 2 TeV and effective operator coefficients up to 35 TeV. ",New physics in triboson event topologies
135,1466366737209769990,1138762581164855298,Christoph Ternes,"['New paper today, <LINK> . We calculate the sensitivity of JUNO and TAO to Large Extra Dimensions and to light sterile neutrinos, and the possibility to eventually distinguish the two scenarios using reactor neutirnos.', 'We find that competitive bounds can be set in both scenarios. In particular, regions of parameter space preferred by Gallium experiments can be tested in both scenarios.']",https://arxiv.org/abs/2112.00379,"We study the sensitivity of JUNO and TAO to the oscillations induced by two well-motivated scenarios beyond the standard model: Large Extra Dimensions (LED) and light sterile neutrinos in the context of 3+1 neutrino mixing. We find that JUNO+TAO can set competitive bounds on the parameter space of each scenario. In particular, we find that JUNO+TAO can be competitive with MINOS, DUNE or KATRIN in the context of LED. If LED are present in nature, we show that the parameters could be measured with a similar precision as the standard oscillation parameters. We also show that JUNO+TAO can test nearly all of the parameter space preferred by Gallium experiments in the context of 3+1 mixing. Finally, we discuss the possibility to distinguish the two scenarios from each other. ",Short-baseline oscillation scenarios at JUNO and TAO
136,1466339199754416136,776765039726460929,Carlo Felice Manara,['Read this thread  on our new paper led by @ajbohn and wonderfully explained here by @mattkenworthy \n<LINK> <LINK>'],http://arxiv.org/abs/2112.00123,"For several transition disks (TDs), dark regions interpreted as shadows have been observed in scattered light imaging and are hypothesized to originate from misalignments between distinct disk regions. We aim to investigate the presence of misalignments in TDs. We study the inner disk geometries of 20 well-known transition disks with VLTI/GRAVITY observations and use complementary $^{12}$CO and $^{13}$CO molecular line data from ALMA to derive the orientation of the outer disk regions. We fit simple models to the GRAVITY data to derive the inner disks inclination and position angles. The outer disk geometries were derived from Keplerian fits to the ALMA velocity maps and compared to the inner disk constraints. We also predicted the locations of shadows for significantly misaligned systems. Our analysis reveals six disks to exhibit significant misalignments between their inner and outer disks. The predicted shadow positions agree well with the scattered light images of HD100453 and HD142527, and we find supporting evidence for a shadow in the disk around CQ Tau. In the other three targets for which we infer significantly misaligned disks, V1247 Ori, V1366 Ori, and RY Lup, we do not see any evident sign of shadows in the scattered light images. The scattered light shadows observed in DoAr44, HD135344B, and HD139614 are consistent with our observations, yet the underlying morphology is likely too complex to be described by our models and the accuracy achieved by our observations. Whereas we can derive precise constraints on the potential shadow positions for well-resolved inner disks around HAeBe stars, the statistical uncertainties for the marginally resolved inner disks around the TTS of our sample make it difficult to extract conclusive constraints for the presence of shadows in these systems. ",Probing inner and outer disk misalignments in transition disks
137,1466297726593748992,872188363,Sandro Tacchella,"['Want to learn more about H-alpha emission in local galaxies? Please check out our new paper (<LINK>), where we performed simulations that include radiative transfer, non-equilibrium thermochemistry, and dust evolution. <LINK>', 'I have really learned a lot about H-alpha and radiative transfer in general when doing this project. It was super fun to work with Aaron Smith, Rahul Kannan &amp; team ‚Äì a fruitful collaboration between observers and theorists.', 'One important takeaway for the Halpha-SFR connection: dust (~28%) and helium (~9%) absorption together with escape (~6%) reduces the available budget for hydrogen line emission by nearly half (~57%) in MW-like galaxies. https://t.co/P9UO02SJhv', 'We find that the fraction of HŒ± emission from collisional excitation amounts to 5‚àí10% and that scattering boosts the HŒ± luminosity by ‚àº40%. The intrinsic contribution of the diffused ionized gas (DIG) to the total Halpha luminosity is a factor of ~2 lower than the observed one. https://t.co/hifiEzwzFU', 'Also check out our companion paper, where we focus on the physics of Ly-alpha escape from disk-like galaxies: https://t.co/P23uqcA5qS', '@dr_guangtou Yes somethings do and we look at the edge-on vs face-on projection.']",https://arxiv.org/abs/2112.00027,"The nebular recombination line H$\alpha$ is widely used as a star-formation rate (SFR) indicator in the local and high-redshift Universe. We present a detailed H$\alpha$ radiative transfer study of high-resolution isolated Milky-Way and Large Magellanic Cloud simulations that include radiative transfer, non-equilibrium thermochemistry, and dust evolution. We focus on the spatial morphology and temporal variability of the H$\alpha$ emission, and its connection to the underlying gas and star formation properties. The H$\alpha$ and H$\beta$ radial and vertical surface brightness profiles are in excellent agreement with observations of nearby galaxies. We find that the fraction of H$\alpha$ emission from collisional excitation amounts to $f_{\rm col}\sim5-10\%$, only weakly dependent on radius and vertical height, and that scattering boosts the H$\alpha$ luminosity by $\sim40\%$. The dust correction via the Balmer decrement works well (intrinsic H$\alpha$ emission recoverable within $25\%$), though the dust attenuation law depends on the amount of attenuation itself both on spatially resolved and integrated scales. Important for the understanding of the H$\alpha$-SFR connection is the dust and helium absorption of ionizing radiation (Lyman continuum [LyC] photons), which are about $f_{\rm abs}\approx28\%$ and $f_{\rm He}\approx9\%$, respectively. Together with an escape fraction of $f_{\rm esc}\approx6\%$, this reduces the available budget for hydrogen line emission by nearly half ($f_{\rm H}\approx57\%$). We discuss the impact of the diffuse ionized gas, showing - among other things - that the extraplanar H$\alpha$ emission is powered by LyC photons escaping the disc. Future applications of this framework to cosmological (zoom-in) simulations will assist in the interpretation of spectroscopy of high-redshift galaxies with the upcoming James Webb Space Telescope. ","H-alpha emission in local galaxies: star formation, time variability and
  the diffuse ionized gas"
138,1466229884385177612,1591961539,Maxwell Nye,"['New paper! <LINK>\nWe show that huge language models (137B params!) can be trained to solve algorithmic tasks by ‚Äúshowing their work‚Äù---writing intermediate text to a scratchpad. This ‚Äúscratchpad‚Äù technique even allows us to predict the execution of Python code. <LINK>', 'In https://t.co/0I0sXS1lSZ we showed that large language models could write code, but had trouble ""executing"" programs. Here we show that execution accuracy improves if models are trained to predict the entire execution trace (i.e., predict the intermediate states line-by-line).', 'We evaluate on the MBPP dataset https://t.co/Q1oHNHpgFb.\n\nWe also show that results improve when the training data is augmented with traces from the model‚Äôs own failed attempts to solve programming problems, providing a cheap form of additional training data. https://t.co/rH8o2VdP55', 'This technique is simple, can be easily implemented with off-the-shelf transformer models, and applies to other algorithmic tasks as well. We additionally show results for long-addition with 8+ digit numbers and evaluating polynomials. https://t.co/HluF7Ub9iz', 'Special thanks to my internship host @gstsdn!\n\nThis work was also done in collaboration with great folks at Google: Anders Andreassen, @guygr, @hmichalewski, @jacobaustin132, @Bieber, @dmdohan, Aitor Lewkowycz, @MaartenBosma, @jluan, @RandomlyWalking']",https://arxiv.org/abs/2112.00114,"Large pre-trained language models perform remarkably well on tasks that can be done ""in one pass"", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation ""step by step"", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a ""scratchpad"". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations. ","Show Your Work: Scratchpads for Intermediate Computation with Language
  Models"
139,1476602076939661324,806996203,Sumon Biswas,"[""Preprint of our ICSE'22 paper is available <LINK>\nResearching on the architecture &amp; design of data science pipelines OR building new pipelines? Take a look. @hridesh \n\nGlad to end the year by sharing @ICSEconf paper. Hopefully will meet all this time @Pittsburgh <LINK>""]",https://arxiv.org/abs/2112.01590,"Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large. ","The Art and Practice of Data Science Pipelines: A Comprehensive Study of
  Data Science Pipelines In Theory, In-The-Small, and In-The-Large"
140,1475862569587859477,2289389762,Dr. Carl E. Fieldsüí•,"['New paper alert üö® \n\nIn this project, we follow convective shell burning in the 3D collapse of a rapidly rotating massive star! üí´ \n\nRead more about this work here: <LINK> <LINK>', 'A key result: we were able to follow the redistribution of angular momentum within the stellar interior and find that this can lead to differences at collapse when compared to 1D models. üí•', 'The consequence of these differences in 3D can have an impact on explosion properties and neutron star mass/spin estimates relevant to @LIGO !', 'And before anyone asks, yes we are thinking about magnetic fields üß≤, so stay tuned!', '@TravisCourt Exactly !', '@AstroHyde Thank you ! Excited to see what we can come up with.', '@sanjanacurtis Thank you, Dr. Curtis üòá', '@neutrinoceros @yt_astro Yes !', '@starsumner Fair. It does agree to the initial conditions in region far from the convective regions.', '@CapricePhillips Thank you! üòá']",https://arxiv.org/abs/2112.12800,"We report on the three-dimensional (3D) hydrodynamic evolution to iron core-collapse of a rapidly rotating 16 $M_{\odot}$ star. For the first time, we follow the 3D evolution of the angular momentum (AM) distribution in the iron core and convective shell burning regions for the final 10 minutes up to and including gravitational instability and core-collapse. In 3D, we find that convective regions show efficient AM transport that leads to an AM profile that differs in shape and magnitude from $\texttt{MESA}$ within a few shell convective turnover timescales. For different progenitor models, such as those with tightly coupled Si/O convective shells, efficient AM transport in 3D simulations could lead to a significantly different AM distribution in the stellar interior affecting estimates of the natal neutron star or black hole spin. Our results suggest that 3D AM transport in convective and rotating shell burning regions are critical components in models of massive stars and could qualitatively alter the explosion outcome and inferred compact remnant properties. ",The Three-Dimensional Collapse of a Rapidly Rotating 16 $M_{\odot}$ Star
141,1475577901324115976,850415526602059777,Vikram Dwarkadas,"['Our paper on long-term study of 3 new ULXs in NGC 891, including a new Chandra source, accepted. <LINK>. Work started by undergrad Victoria Cirillo (Fordham), then taken over and completed by  Nicholas Earley (UChicago). Nicholas is now applying to grad school. <LINK>']",https://arxiv.org/abs/2112.12212,"We perform empirical fits to the \emph{Chandra} and \emph{XMM-Newton} spectra of three ultraluminous X-ray sources (ULXs) in the edge-on spiral galaxy NGC 891, monitoring the region over a seventeen year time window. One of these sources has been visible since the early 1990s with \emph{ROSAT} and has been observed multiple times with \emph{Chandra} and \emph{XMM-Newton}. Another has been visible since 2011. We build upon prior analyses of these sources by analyzing all available data at all epochs. Where possible \emph{Chandra} data is used, since its superior spatial resolution allows for more effective isolation of the emission from each individual source, thus providing a better determination of their spectral properties. We also identify a new transient ULX, CXOU J022230.1+421937, which faded from view over the course of a two month period from Nov 2016 to Jan 2017. Modeling of each source at every epoch was conducted using six different models ranging from thermal bremsstrahlung to accretion disk models. Unfortunately, but as is common with many ULXs, no single model yielded a much better fit than the others. The two known sources had unabsorbed luminosities that remained fairly consistent over five or more years. Various possibilities for the new transient ULX are explored. ",A Long-term Study of Ultraluminous X-ray Sources in NGC 891
142,1473966971607826435,1285579351950598144,Karolina Stanczak,"['New paper with the amazing @verratrix: ""Quantifying Gender Biases Towards Politicians on Reddit"".\n\ntl;dr:\n1) equal interest in ‚ôÄÔ∏è/‚ôÇÔ∏è politicians\n2) gender bias in right &amp; left-leaning subreddits\n3) politicians described wrt üëóüë©\u200düëß\u200düëß\n\n<LINK>\n#NLProc \n@IAugenstein <LINK>']",https://arxiv.org/abs/2112.12014,"Despite attempts to increase gender parity in politics, global efforts have struggled to ensure equal female representation. This is likely tied to implicit gender biases against women in authority. In this work, we present a comprehensive study of gender biases that appear in online political discussion. To this end, we collect 10 million comments on Reddit in conversations about male and female politicians, which enables an exhaustive study of automatic gender bias detection. We address not only misogynistic language, but also benevolent sexism in the form of seemingly positive attitudes examining both sentiment and dominance attributed to female politicians. Finally, we conduct a multi-faceted study of gender bias towards politicians investigating both linguistic and extra-linguistic cues. We assess 5 different types of gender bias, evaluating coverage, combinatorial, nominal, sentimental and lexical biases extant in social media language and discourse. Overall, we find that, contrary to previous research, coverage and sentiment biases suggest equal public interest in female politicians. However, the results of the nominal and lexical analyses suggest this interest is not as professional or respectful as that expressed about male politicians. Female politicians are often named by their first names and are described in relation to their body, clothing, or family; this is a treatment that is not similarly extended to men. On the now banned far-right subreddits, this disparity is greatest, though differences in gender biases still appear in the right and left-leaning subreddits. We release the curated dataset to the public for future studies. ",Quantifying Gender Biases Towards Politicians on Reddit
143,1473431124814749706,1326939843767775232,Fong Group at Northwestern,"['New paper alert!\n\nIn this work, PhD Student Jillian Rastinejad and team collect and analyze over 650 optical candidate counterparts to events from the third GW observing run, eliminating &gt;90% of them as viable kilonovae using publicly-available tools. \n<LINK> <LINK>', 'The pie chart represents the 653 candidates in our sample and the colored slices show the fraction of candidates eliminated using each of our real-time tools', 'Along the way, we determine that pre-merger detections from public surveys and host galaxy catalogs (which together eliminate &gt;50% of candidates in our sample) are powerful real-time tools for future GW-kilonova searches.', '@arxivabs Thank you!']",https://arxiv.org/abs/2112.09701,"We present a comprehensive analysis of 653 optical candidate counterparts reported during the third gravitational wave (GW) observing run. Our sample concentrates on candidates from the 15 events (published in GWTC-2, GWTC-3 or not retracted on GraceDB) that had a >1% chance of including a neutron star in order to assess their viability as true kilonovae. In particular, we leverage tools available in real time, including pre-merger detections and cross-matching with catalogs (i.e. point source, variable star, quasar and host galaxy redshift datasets), to eliminate 65% of candidates in our sample. We further employ spectroscopic classifications, late-time detections and light curve behavior analyses, and conclude that 66 candidates remain viable kilonovae. These candidates lack sufficient information to determine their classifications, and the majority would require luminosities greater than that of AT2017gfo. Pre-merger detections in public photometric survey data and comparison of catalogued host galaxy redshifts with the GW event distances are critical to incorporate into vetting procedures, as these tools eliminated >20% and >30% of candidates, respectively. We expect that such tools which leverage archival information will significantly reduce the strain on spectroscopic and photometric follow-up resources in future observing runs. Finally, we discuss the critical role prompt updates from GW astronomers to the EM community play in reducing the number of candidates requiring vetting. ","A Systematic Exploration of Kilonova Candidates from Neutron Star
  Mergers During the Third Gravitational Wave Observing Run"
144,1472874430091214848,105768989,Francesco Bonchi,"['New paper alert: "" Dense and well-connected subgraph detection in dual networks"" with @Tsourolampis T. Chen, D. Garcas-Soriano, A. Miyauchi accepted at  #SIAMSDM22 Pre-print available <LINK> #DataScience #Algorithms  #NetworkScience']",https://arxiv.org/abs/2112.03337,"Dense subgraph discovery is a fundamental problem in graph mining with a wide range of applications \cite{gionis2015dense}. Despite a large number of applications ranging from computational neuroscience to social network analysis, that take as input a {\em dual} graph, namely a pair of graphs on the same set of nodes, dense subgraph discovery methods focus on a single graph input with few notable exceptions \cite{semertzidis2019finding,charikar2018finding,reinthal2016finding,jethava2015finding}. In this work, we focus the following problem: given a pair of graphs $G,H$ on the same set of nodes $V$, how do we find a subset of nodes $S \subseteq V$ that induces a well-connected subgraph in $G$ and a dense subgraph in $H$? Our formulation generalizes previous research on dual graphs \cite{Wu+15,WuZLFJZ16,Cui2018}, by enabling the {\em control} of the connectivity constraint on $G$. We propose a novel mathematical formulation based on $k$-edge connectivity, and prove that it is solvable exactly in polynomial time. We compare our method to state-of-the-art competitors; we find empirically that ranging the connectivity constraint enables the practitioner to obtain insightful information that is otherwise inaccessible. Finally, we show that our proposed mining tool can be used to better understand how users interact on Twitter, and connectivity aspects of human brain networks with and without Autism Spectrum Disorder (ASD). ",Dense and well-connected subgraph detection in dual networks
145,1471864652141404166,947153529789018114,Mubarak Shah,"['Check out new paper:\nGeometric Feature Learning for 3D Meshes\n<LINK> \nWe contribute a novel hierarchical neural network, PicassoNet-II, to learn highly discriminative features from 3D meshes. PicassoNet-II accepts primitive geometrics and fine textures of mesh.']",https://arxiv.org/abs/2112.01801,"Geometric feature learning for 3D meshes is central to computer graphics and highly important for numerous vision applications. However, deep learning currently lags in hierarchical modeling of heterogeneous 3D meshes due to the lack of required operations and/or their efficient implementations. In this paper, we propose a series of modular operations for effective geometric deep learning over heterogeneous 3D meshes. These operations include mesh convolutions, (un)pooling and efficient mesh decimation. We provide open source implementation of these operations, collectively termed \textit{Picasso}. The mesh decimation module of Picasso is GPU-accelerated, which can process a batch of meshes on-the-fly for deep learning. Our (un)pooling operations compute features for newly-created neurons across network layers of varying resolution. Our mesh convolutions include facet2vertex, vertex2facet, and facet2facet convolutions that exploit vMF mixture and Barycentric interpolation to incorporate fuzzy modelling. Leveraging the modular operations of Picasso, we contribute a novel hierarchical neural network, PicassoNet-II, to learn highly discriminative features from 3D meshes. PicassoNet-II accepts primitive geometrics and fine textures of mesh facets as input features, while processing full scene meshes. Our network achieves highly competitive performance for shape analysis and scene parsing on a variety of benchmarks. We release Picasso and PicassoNet-II on Github this https URL ",Geometric Feature Learning for 3D Meshes
146,1471425398860877829,426509606,Yamir Moreno,"['New paper out ""Epidemic spreading in populations of mobile agents with adaptive behavioral response"" (<LINK>). Here, we present and analyze a model for disease spreading in temporal networks of mobile agents that incorporates local behavioral responses. (1/3) <LINK>', 'We show that avoiding contacts with infectees considerably decreases the stationary prevalence when the spatial density of agents is low. However, for higher densities, the mechanism causes an abrupt phase transition, where a new bistable phase appears. (2/3) https://t.co/gozVFgsXYL', 'We develop a semi-analytic approach for the case when the mobility is fast compared to the disease dynamics, and use it to argue that the bistability is caused by the emergence of spatial clusters of susceptible agents. Kudos to co-authors @paulocv92 @SrAleta @FranciscoICMC 3/3. https://t.co/r7HpTgM7vN', '@thilogross @paulocv92 @SrAleta @FranciscoICMC @BIFI_Instituto @ISI_Fondazione Thanks, Thilo. And I fully agree.', ""@dques2766 @paulocv92 @SrAleta @FranciscoICMC @BIFI_Instituto @ISI_Fondazione This is a nice idea indeed. We haven't explored such a scenario."", '@dques2766 @paulocv92 @SrAleta @FranciscoICMC @BIFI_Instituto @ISI_Fondazione Thanks, David']",https://arxiv.org/abs/2112.07829,"Despite the advanced stage of epidemic modeling, there is a major demand for methods to incorporate behavioral responses to the spread of a disease such as social distancing and adoption of prevention methods. Mobility plays an important role in epidemic dynamics and is also affected by behavioral changes, but there are many situations in which real mobility data is incomplete or inaccessible. We present a model for epidemic spreading in temporal networks of mobile agents that incorporates local behavioral responses. Susceptible agents are allowed to move towards the opposite direction of infected agents in their neighborhood. We show that this mechanism considerably decreases the stationary prevalence when the spatial density of agents is low. However, for higher densities, the mechanism causes an abrupt phase transition, where a new bistable phase appears. We develop a semi-analytic approach for the case when the mobility is fast compared to the disease dynamics, and use it to argue that the bistability is caused by the emergence of spatial clusters of susceptible agents. Finally, we characterize the temporal networks formed in the fast mobility regime, showing how the degree distributions and other metrics are affected by the behavioral mechanism. Our work incorporates results previously known from adaptive networks into the population of mobile agents, which can be further developed to be used in mobility-driven models. ","Epidemic spreading in populations of mobile agents with adaptive
  behavioral response"
147,1471423847706476546,1073974163704958976,Ahmad Beirami,['Check out our new work where we uncover that state-of-the-art dialog state tracking models are not robust to inevitable distribution shifts in real world. \n\nPaper link: <LINK> <LINK>'],https://arxiv.org/abs/2112.08321,"Recent neural models that extend the pretrain-then-finetune paradigm continue to achieve new state-of-the-art results on joint goal accuracy (JGA) for dialogue state tracking (DST) benchmarks. However, we call into question their robustness as they show sharp drops in JGA for conversations containing utterances or dialog flows with realistic perturbations. Inspired by CheckList (Ribeiro et al., 2020), we design a collection of metrics called CheckDST that facilitate comparisons of DST models on comprehensive dimensions of robustness by testing well-known weaknesses with augmented test sets. We evaluate recent DST models with CheckDST and argue that models should be assessed more holistically rather than pursuing state-of-the-art on JGA since a higher JGA does not guarantee better overall robustness. We find that span-based classification models are resilient to unseen named entities but not robust to language variety, whereas those based on autoregressive language models generalize better to language variety but tend to memorize named entities and often hallucinate. Due to their respective weaknesses, neither approach is yet suitable for real-world deployment. We believe CheckDST is a useful guide for future research to develop task-oriented dialogue models that embody the strengths of various methods. ","CheckDST: Measuring Real-World Generalization of Dialogue State Tracking
  Performance"
148,1470785421760213009,2308157108,Peyman Passban,['Check out our new paper on the combination of Neural Machine Translation and Federated Learning <LINK>\nJoint work w/ my colleagues Tanya Roosta &amp; Ankit Chadha #Amazon #NMT #NLProc #FederatedLearning <LINK>'],https://arxiv.org/abs/2112.06135,"Training neural machine translation (NMT) models in federated learning (FL) settings could be inefficient both computationally and communication-wise, due to the large size of translation engines as well as the multiple rounds of updates required to train clients and a central server. In this paper, we explore how to efficiently build NMT models in an FL setup by proposing a novel solution. In order to reduce the communication overhead, out of all neural layers we only exchange what we term ""Controller"" layers. Controllers are a small number of additional neural components connected to our pre-trained architectures. These new components are placed in between original layers. They act as liaisons to communicate with the central server and learn minimal information that is sufficient enough to update clients. We evaluated the performance of our models on five datasets from different domains to translate from German into English. We noted that the models equipped with Controllers preform on par with those trained in a central and non-FL setting. In addition, we observed a substantial reduction in the communication traffic of the FL pipeline, which is a direct consequence of using Controllers. Based on our experiments, Controller-based models are ~6 times less expensive than their other peers. This reduction is significantly important when we consider the number of parameters in large models and it becomes even more critical when such parameters need to be exchanged for multiple rounds in FL settings. ","Communication-Efficient Federated Learning for Neural Machine
  Translation"
149,1470775307389083665,1171357907574824961,≈Åukasz Tychoniec,"['In new paper led by Alex Cridland, we test feasibility of planetesimal formation in very young protostars with streaming instability (SI). It appears that difference of dust and gas infall can cause enhancement of dust-to-gas ratio, which then triggers SI\n<LINK>', 'Mass that is available for streaming instability peaks at the Class I stage and is sufficient to further form the core of gas giant planet. If large grains are present in the envelope the SI can be possible even in the Class 0 stage. https://t.co/tdzY1dOznp']",https://arxiv.org/abs/2112.06734,"(Abridged) Recent surveys of young star formation regions have shown that the average Class II object does not have enough dust mass to make the cores of giant planets. Younger Class 0/I objects have enough dust in their embedded disk, which begs the questions: can the first steps of planet formation occur in these younger systems? The first step is building the first planetesimals, generally believed to be the product of the streaming instability. Hence the question can be restated: are the physical conditions of embedded disks conducive to the growth of the streaming instability? Here we model the collapse of a `dusty' proto-stellar cloud to show that if there is sufficient drift between the falling gas and dust, regions of the embedded disk can become sufficiently enhanced in dust to drive the streaming instability. We include four models, three with different dust grain sizes and one with a different initial cloud angular momentum to test a variety of collapse trajectories. We find a `sweet spot' for planetesimal formation for grain sizes of a few 10s of micron since they fall sufficiently fast relative to the gas to build a high dust-to-gas ratio along the disk midplane, but have slow enough radial drift speeds in the embedded disk to maintain the high dust-to-gas ratio. Unlike the gas, which is held in hydrostatic equilibrium for a time due to gas pressure, the dust can begin collapsing from all radii at a much earlier time. The streaming instability can produce at least between 7-35 M$_\oplus$ of planetesimals in the Class 0/I phase of our smooth embedded disks, depending on the size of the falling dust grains. This first generation of planetesimals could represent the first step in planet formation, and occurs earlier in the lifetime of the young star than is traditionally thought. ","Early planet formation in embedded protostellar disks: Setting the stage
  for the first generation of planetesimals"
150,1470309225481924608,967825967673638913,Richard Feder,['New paper from the CIBER collaboration with polarization spectrum measurements of the near-infrared zodiacal light! Led by @takimoto_koji  <LINK>'],https://arxiv.org/abs/2112.05350,"We report the first measurement of the zodiacal light (ZL) polarization spectrum in the near-infrared between 0.8 and 1.8 $\mu$m. Using the low-resolution spectrometer (LRS) on board the Cosmic Infrared Background Experiment (CIBER), calibrated for absolute spectrophotometry and spectropolarimetry, we acquire long-slit polarization spectral images of the total diffuse sky brightness towards five fields. To extract the ZL spectrum, we subtract contribution of other diffuse radiation, such as the diffuse galactic light (DGL), the integrated star light (ISL), and the extragalactic background light (EBL). The measured ZL polarization spectrum shows little wavelength dependence in the near-infrared and the degree of polarization clearly varies as a function of the ecliptic coordinates and solar elongation. Among the observed fields, the North Ecliptic Pole shows the maximum degree of polarization of $\sim$ 20$\%$, which is consistent with an earlier observation from the Diffuse Infrared Background Experiment (DIRBE) aboard on the Cosmic Background Explorer (COBE). The measured degree of polarization and its solar elongation dependence are reproduced by the empirical scattering model in the visible band and also by the Mie scattering model for large absorptive particles, while the Rayleigh scattering model is ruled out. All of our results suggest that the interplanetary dust is dominated by large particles. ","Polarization spectrum of near infrared zodiacal light observed with
  CIBER"
151,1469346454401658882,1318918318221414400,Luis Miralles TU Dublin,"['New paper Explainable AI for B5G/6G: Technical Aspects, Use Cases, and Research Challenges <LINK>']",https://arxiv.org/abs/2112.04698,"When 5G began its commercialisation journey around 2020, the discussion on the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth, coverage, reliability, energy efficiency, lower latency, and, more importantly, an integrated ""human-centric"" network system powered by artificial intelligence (AI). Such a 6G network will lead to an excessive number of automated decisions made every second. These decisions can range widely, from network resource allocation to collision avoidance for self-driving cars. However, the risk of losing control over decision-making may increase due to high-speed data-intensive AI decision-making beyond designers and users' comprehension. The promising explainable AI (XAI) methods can mitigate such risks by enhancing the transparency of the black box AI decision-making process. This survey paper highlights the need for XAI towards the upcoming 6G age in every aspect, including 6G technologies (e.g., intelligent radio, zero-touch network management) and 6G use cases (e.g., industry 5.0). Moreover, we summarised the lessons learned from the recent attempts and outlined important research challenges in applying XAI for building 6G systems. This research aligns with goals 9, 11, 16, and 17 of the United Nations Sustainable Development Goals (UN-SDG), promoting innovation and building infrastructure, sustainable and inclusive human settlement, advancing justice and strong institutions, and fostering partnership at the global level. ","Explainable AI for B5G/6G: Technical Aspects, Use Cases, and Research
  Challenges"
152,1467899466267123714,1316746180085403655,Pierre Colombo,"['Our paper: InfoLM: A New Metric to Evaluate Summarisation &amp; Data2Text Generation has been accepted @RealAAAI #AAAI2022. You can find it here: <LINK> ! @telecomparis @CS__Research', 'InfoLM is a family of untrained metrics that can be viewed as a string-based metric but it can robustly handle synonyms and paraphrases using various measures of informations (e.g f-divergences, Fisher Rao). It can be seen as an extension of PRISM @mjpost.']",https://arxiv.org/abs/2112.01589,"Assessing the quality of natural language generation systems through human annotation is very expensive. Additionally, human annotation campaigns are time-consuming and include non-reusable human labour. In practice, researchers rely on automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU) have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms. In this paper, we introduce InfoLM a family of untrained metrics that can be viewed as a string-based metric that addresses the aforementioned flaws thanks to a pre-trained masked language model. This family of metrics also makes use of information measures allowing the adaptation of InfoLM to various evaluation criteria. Using direct assessment, we demonstrate that InfoLM achieves statistically significant improvement and over $10$ points of correlation gains in many configurations on both summarization and data2text generation. ",InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation
153,1481761320252755969,1089916548481978368,zuerchlab,['In our latest work we find that nonlinear light emissions of a 2D-TMD driven by a strong midinfrared field show angular shifts that encode material specific dynamical symmetry properties of the crystal interacting with the laser Ô¨Åeld. <LINK> @UCB_Chemistry <LINK>'],https://arxiv.org/abs/2112.13032,"High-harmonic generation (HHG) in solids has been touted as a way to probe ultrafast dynamics and crystal symmetries in condensed matter systems. Here, we investigate the polarization properties of high-order harmonics generated in monolayer MoS$_2$, as a function of crystal orientation relative to the mid-infrared laser field polarization. At several different laser wavelengths we experimentally observe a prominent angular shift of the parallel-polarized odd harmonics for energies above approximately 3.5 eV, and our calculations indicate that this shift originates in subtle differences in the recombination dipole strengths involving multiple conduction bands. This observation is material specific and is in addition to the angular dependence imposed by the dynamical symmetry properties of the crystal interacting with the laser field, and may pave the way for probing the vectorial character of multi-band recombination dipoles. ","Signatures of multi-band effects in high-harmonic generation in
  monolayer MoS$_2$"
154,1481325857859784704,1089916548481978368,zuerchlab,['Happy to see Lars‚Äôs first group manuscript posted: <LINK>. We study how intense femtosecond X-ray free-electron laser pulses induce competing nonlinear responses in a condensed-phase system where competition depends on material-specific properties. @UCB_Chemistry <LINK>'],https://arxiv.org/abs/2112.12585,"The interaction of intense light with matter gives rise to competing nonlinear responses that can dynamically change material properties. Prominent examples are saturable absorption (SA) and two-photon absorption (TPA), which dynamically increase and decrease the transmission of a sample depending on pulse intensity, respectively. The availability of intense soft X-ray pulses from free-electron lasers (FEL) has led to observations of SA and TPA in separate experiments, leaving open questions about the possible interplay between and relative strength of the two phenomena. Here, we systematically study both phenomena in one experiment by exposing graphite films to soft X-ray FEL pulses of varying intensity, with the FEL energy tuned to match carbon 1s to $\pi^*$ or 1s to $\sigma^*$ transitions. It is observed for lower intensities that the nonlinear contribution to the absorption is dominated by SA attributed to ground-state depletion; for larger intensities ($>10^{14}$ W/cm$^2$), TPA becomes more dominant. The relative strengths of the two phenomena depend in turn on the specific transition driven by the X-ray pulse. Both observations are consistent with our real-time electronic structure calculations. Our results reveal the competing contributions of distinct nonlinear material responses to spectroscopic signals measured in the X-ray regime, demonstrating an approach of general utility for interpreting FEL spectroscopies. ","Saturable absorption of free-electron laser radiation by graphite near
  the carbon K-edge"
155,1481151491243139076,883039700,Lenka Zdeborova,"['I studied many inference problems on random graphs in the past, but the graph alignment is challenging: <LINK> We still do not know what are the constants for optimal behavior nor algorithms. A lot of work to do still here.', '@egavves Yes, this size is the parameter d, the influence is discussed in the paper.']",https://arxiv.org/abs/2112.13079,"The problem of aligning Erd\""os-R\'enyi random graphs is a noisy, average-case version of the graph isomorphism problem, in which a pair of correlated random graphs is observed through a random permutation of their vertices. We study a polynomial time message-passing algorithm devised to solve the inference problem of partially recovering the hidden permutation, in the sparse regime with constant average degrees. We perform extensive numerical simulations to determine the range of parameters in which this algorithm achieves partial recovery. We also introduce a generalized ensemble of correlated random graphs with prescribed degree distributions, and extend the algorithm to this case. ","Aligning random graphs with a sub-tree similarity message-passing
  algorithm"
156,1479069371037364225,1215010368759701505,Eleanor D'Arcy,"['First paper dayüéâ <LINK>. We propose a methodology for predicting the number and sizes of wildfires in the US, with an emphasis on extreme eventsüöíüî• This is joint work with @callumbarltrop, Rob Shooter &amp; Emma Simpson, and motivated by the #EVA2021 data challenge']",https://arxiv.org/abs/2112.15372,"This paper details the methodology proposed by the \textit{Lancaster Ducks} team for the EVA 2021 conference data challenge. This aim of this challenge was to predict the number and size of wildfires over the contiguous US between 1993-2015, with more importance placed on extreme events. Our approach proposes separate methods for modelling the bodies and tails of the distributions of both wildfire variables. For the former, a hierarchical clustering technique is proposed to first group similar locations, with a non-parametric approach subsequently used to model the non-extreme data. To capture tail behaviour, separate techniques derived from univariate extreme value theory are proposed for both variables. For the count data, a generalised Pareto distribution with a generalised additive model structure is used to capture effects from covariates on values above a high threshold. For burnt area, a non-stationary generalised Pareto distribution enables us to capture the tail behaviour of proportions obtained through a transformation of observed area data. The resulting predictions are shown to perform reasonably well, improving on the benchmark method proposed in the challenge outline. We also provide a discussion about the limitations of our modelling framework and evaluate ways in which it could be extended. ","A flexible, semi-parametric, cluster-based approach for predicting
  wildfire extremes across the contiguous United States"
157,1478776399330611201,319715947,Charlene,"['New preprint!\n\n""One Bad Apple Spoils the Bunch: Transaction DoS in MimbleWimble Blockchains""\n\nWe find that in combination, Dandelion++ and transaction aggregation make MimbleWimble susceptible to a new type of denial-of-service attacks.\n\nLink: <LINK>']",https://arxiv.org/abs/2112.13009,"As adoption of blockchain-based systems grows, more attention is being given to privacy of these systems. Early systems like BitCoin provided few privacy features. As a result, systems with strong privacy guarantees, including Monero, Zcash, and MimbleWimble have been developed. Compared to BitCoin, these cryptocurrencies are much less understood. In this paper, we focus on MimbleWimble, which uses the Dandelion++ protocol for private transaction relay and transaction aggregation to provide transaction content privacy. We find that in combination these two features make MimbleWimble susceptible to a new type of denial-of-service attacks. We design, prototype, and evaluate this attack on the Beam network using a private test network and a network simulator. We find that by controlling only 10% of the network nodes, the adversary can prevent over 45% of all transactions from ending up in the blockchain. We also discuss several potential approaches for mitigating this attack. ","One Bad Apple Spoils the Bunch: Transaction DoS in MimbleWimble
  Blockchains"
158,1478461463928918022,926859515986923520,Jonathan Lorraine,"['When minimizing objectives, randomly initializing + optimizing can fail to find different solutions. We give a branching optimization method for this using Lyapunov exponents..1/11\n<LINK> @AAMAS2022\nWith @PaulVicol @jparkerholder @TalKachman @Luke_Metz @j_foerst <LINK>', 'Ridge rider (RR) finds diverse solutions in single-objective optimization by branching optimization at saddle points. Our method - Generalized RR (GRR) - branches at bifurcation points where small parameter changes lead to different learning dynamics..2/11 https://t.co/EoOaCkY7Pj', 'In single-objective optimization, the parameter updates form a conservative vector field, where saddle points are the only relevant bifurcation. However, in differentiable games the updates can form a non-conservative field, giving rise to new bifurcations - ex., Hopf..3/11 https://t.co/FtFbimW4P2', 'Ridge rider only finds saddle bifurcations. But, how can we find more general bifurcations, like those occurring in differentiable games? With Lyapunov exponent based objectives, leveraging a broad body of work from dynamical systems! 4/11 https://t.co/cXMlYcoLOl', 'Why Lyapunov exponents? They measure optimization trajectory separation speed, for perturbations in different directions (shown in blue/green). Trajectories separate fastest at bifurcations, where even small perturbations can cause trajectories to go to different solutions..5/11 https://t.co/mdMXSizktH', 'Leveraging automatic differentiation, it‚Äôs simple to use gradient-based optimization on the exponents to move our starting parameters to regions where bifurcations occur..6/11 https://t.co/WjL31fWBzB', 'You can create toy problems with various bifurcations using your favorite differentiable games ‚Äì ex., the Iterated Prisoner‚Äôs Dilemma (IPD) or GANs ‚Äì by taking a random subspace from each player‚Äôs parameters, which they optimize in. The exponent is largest near bifurcations..7/11 https://t.co/QpYuljSi5Z', 'By varying the sampled subspace and game, there is a wide range of phenomena. If you‚Äôre working on optimization in differentiable games and looking for interesting, visualizable, 2-D diagnostic problems try out this technique! 8/11 https://t.co/eB1ggPxF03', 'We include ablations over various design choices. For example, how long of an optimization horizon is necessary to find bifurcations..9/11 https://t.co/yc9v2kFGKJ', 'We scale up our method, finding diverse solutions in the iterated prisoner‚Äôs dilemma and evaluating the approximate Lyapunov exponent for GANs..10/11\n\nAccepted @aamas2022 #AAMAS2022\n\nhttps://t.co/y2m6I1tiPb\n\nWork helped by @VectorInst @UofTCompSci @MetaAI @GoogleAI @AI_Radboud', 'Our work is inspired by and builds on ‚ÄúRidge Rider: Finding Diverse Solutions by Following Eigenvectors of the Hessian‚Äù..11/11\nfrom @jparkerholder @Luke_Metz @cinjoncin Hengyuan Hu @adamlerer @_aletcher @alex_peys @aldopacchiano @j_foerst\n\nhttps://t.co/VcSWBZio3L', '@3blue1brown\nCurious to see what happens if we use our method on the ‚ÄúNewton Fractal‚Äù ‚Äì i.e., optimizing a cubic with Newton‚Äôs method.  A great problem to test recursively branchingüòÄ\n\nhttps://t.co/T6Ipv3OzzA']",http://arxiv.org/abs/2112.14570,"Ridge Rider (RR) is an algorithm for finding diverse solutions to optimization problems by following eigenvectors of the Hessian (""ridges""). RR is designed for conservative gradient systems (i.e., settings involving a single loss function), where it branches at saddles - easy-to-find bifurcation points. We generalize this idea to non-conservative, multi-agent gradient systems by proposing a method - denoted Generalized Ridge Rider (GRR) - for finding arbitrary bifurcation points. We give theoretical motivation for our method by leveraging machinery from the field of dynamical systems. We construct novel toy problems where we can visualize new phenomena while giving insight into high-dimensional problems of interest. Finally, we empirically evaluate our method by finding diverse solutions in the iterated prisoners' dilemma and relevant machine learning problems including generative adversarial networks. ",Lyapunov Exponents for Diversity in Differentiable Games
159,1478079765722673154,937194665991987200,Enrique Solano,['First masterpiece of 2022 <LINK> We propose the implementation of the Swap Test using charge qubits in double quantum dots. This work enhances the toolbox of quantum machine learning in semiconductor quantum computers @QuantumFlagship @QuantenTech @QuantumDaily <LINK>'],https://arxiv.org/abs/2112.15517,"We propose the implementation of the Swap Test using a charge qubit in a double quantum dot. The Swap Test is a fundamental quantum subroutine in quantum machine learning and other applications for estimating the fidelity of two unknown quantum states by measuring an auxiliary qubit. Our proposal uses a controlled three-qubit gate which is natural to quantum-dot charge qubits. It allows us to implement a Swap Test with a circuit depth of six layers, and an estimated gate time of less than 3 ns, that is below the coherence time of double quantum dots. This work paves the way for enhancing the toolbox of quantum machine learning developments in semiconductor qubits. ",Swap Test with Quantum-Dot Charge Qubits
160,1478008315858886660,1243709419,Patrick Shafto,"['New: ""Probabilistic inverse Optimal Transport"" with Wei-ting Chiu and Pei Wang. We analyze the geometric constraints on inferring ground costs from couplings and propose and validate MCMC algorithms for inference. \n\n<LINK> <LINK>']",https://arxiv.org/abs/2112.09754,"Optimal transport (OT) formalizes the problem of finding an optimal coupling between probability measures given a cost matrix. The inverse problem of inferring the cost given a coupling is Inverse Optimal Transport (IOT). IOT is less well understood than OT. We formalize and systematically analyze the properties of IOT using tools from the study of entropy-regularized OT. Theoretical contributions include characterization of the manifold of cross-ratio equivalent costs, the implications of model priors, and derivation of an MCMC sampler. Empirical contributions include visualizations of cross-ratio equivalent effect on basic examples and simulations validating theoretical results. ",Probabilistic Inverse Optimal Transport
161,1476575324196511746,937194665991987200,Enrique Solano,"['Last masterpiece 2021 <LINK> We propose encoding non-Markovian quantum dynamics of one and two coupled quantum memristors onto digital quantum computers, paving the way to Neuromorphic Quantum Computing in NISQ era @QuantumFlagship @QuantenTech @QuantumDaily <LINK>']",https://arxiv.org/abs/2112.14660,"We propose the encoding of memristive quantum dynamics on a digital quantum computer. Using a set of auxiliary qubits, we simulate an effective non-Markovian environment inspired by a collisional model, reproducing memristive features between expectation values of different operators in a single qubit. We numerically test our proposal in an IBM quantum simulator with 32 qubits, obtaining the pinched hysteresis curve that is characteristic of a quantum memristor. Furthermore, we extend our method to the case of two coupled quantum memristors, opening the door to the study of neuromorphic quantum computing in the NISQ era. ",Quantum Memristors with Quantum Computers
162,1476519046816448514,1285579351950598144,Karolina Stanczak,"['In ""A Survey on Gender Bias in Natural Language Processing"" with @IAugenstein we present a study of 304 papers on gender bias in NLP. Despite the growing interest, we find 4 major limitations and see overcoming them as crucial for future research.\n\n<LINK>\n#NLProc <LINK>']",https://arxiv.org/abs/2112.14168,"Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We suggest recommendations towards overcoming these limitations as a guide for future research. ",A Survey on Gender Bias in Natural Language Processing
163,1474412734519267328,3102898266,Ali Mustufa,"['1/ Happy to announce our paper ""CPPE-5: Medical Personal Protective Equipment Dataset"", a üßµ\n\nWe introduce a new challenging image dataset with the goal to allow the study of subordinate categorization of medical PPE unlike any existing dataset\n\nPaper: <LINK> <LINK>', '2/ Trying to do so is not possible with other popular large-scale data sets that focus on broad-level categories. We also find that currently there exists no such dataset and critique the few *similar* datasets that either consist of ‚Ä¶', '3/ artificial images, focus only on a single category (eg. mask), far less data, or do not focus on medical scenarios altogether which I believe is super helpful\n\nThe dataset also allows easily deploying in complex scenes with other objects as well due to the following features', '4/ The dataset contains:\n- high-quality images and annotations (~4.7 bbox/image)\n- real-life images unlike any current dataset\n- a majority of non-iconic images in their natural context often with multiple other objects\n- annotations for 5 PPE items https://t.co/AViCMep7GQ', '5/ We perform algorithmic analysis on the dataset and open-source some of the top-performing models on this dataset in the code repository: https://t.co/XdToiwR8cV\n\nThe PyTorch and TensorFlow data loader, tutorial to load this dataset, a training tutorial and more are in the repo https://t.co/RG3WwZ3s6d', '6/ We are thankful to @Google @TensorFlow for a generous research grant and TPU Research Cloud for providing access to TPUs\n\ncc: lovely collaborating with @rishit_dagli']",https://arxiv.org/abs/2112.09569,"We present a new challenging dataset, CPPE - 5 (Medical Personal Protective Equipment), with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories (such as PASCAL VOC, ImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained on this dataset to be used in practical scenarios in complex scenes, our dataset mainly contains images that show complex scenes with several objects in each scene in their natural context. The image collection for this dataset focusing on: obtaining as many non-iconic images as possible and making sure all the images are real-life images unlike other existing datasets in this area. Our dataset includes 5 object categories (coveralls, face shield, gloves, mask, and goggles) and each image is annotated with a set of bounding boxes and positive labels. We present a detailed analysis of the dataset in comparison to other popular broad category datasets as well as datasets focusing on personal protective equipments, we also find that at present there exist no such publicly available datasets. Finally we also analyze performance and compare model complexities on baseline and state-of-the-art models for bounding box results. Our code, data, and trained models are available at this https URL . ",CPPE-5: Medical Personal Protective Equipment Dataset
164,1474353824877842432,791705191175360512,Niels Warburton,"[""One last paper for the year, but we've saved the best for last. Here we present gravitational waveforms computed using second-order (in the mass ratio) self-force theory. We find remarkable agreement with NR waveforms for mass ratios of 10:1 or smaller. <LINK> <LINK>"", 'This work is in collaboration with @barry_wardell, Adam Pound, Jeremy Miller, Leanne Durkan, and Alexandre Le Tiec.']",https://arxiv.org/abs/2112.12265,"We produce gravitational waveforms for nonspinning compact binaries undergoing a quasicircular inspiral. Our approach is based on a two-timescale expansion of the Einstein equations in second-order self-force theory, which allows first-principles waveform production in milliseconds. Although the approach is designed for extreme mass ratios, our waveforms agree remarkably well with those from full numerical relativity, even for comparable-mass systems. Our results will be invaluable in accurately modelling extreme-mass-ratio inspirals for the LISA mission and intermediate-mass-ratio systems currently being observed by the LIGO-Virgo-KAGRA Collaboration. ","Gravitational waveforms for compact binaries from second-order
  self-force theory"
165,1474352787366551571,109571291,Daniel Gonz√°lez,"['With all the Xmas fuss I almost forgot that our last preprint of 2021 just hit arXiv. Nice collaboration between @ICFOnians and @FerlainoGroup from @iqoqi where we study how to prepare and detect a strongly-correlated topological phase with dipolar atoms\n\n<LINK>', 'Con esto cerramos por ahora el chiringuito. Les dejo con uno de los mejores villancicos que se han escrito por estos lares. Disfruten del post-solsticio üî•üî•\n\nhttps://t.co/iMaQ9MMd91']",https://arxiv.org/abs/2112.08785,"We investigate the topological properties of the bond order wave phase arising in the extended Fermi-Hubbard model. In particular, we uncover a topological sector, which remained elusive in previous finite-size numerical studies due to boundary effects. We first show that, for an infinite system, the bond order wave regime is characterized by two degenerate bulk states corresponding to the trivial and topological sectors. The latter turns out to be indeed characterized by an even degeneracy of the entanglement spectrum and longe-range order of a string correlation function. For finite size systems, we show that the topological sector can be stabilized by imposing a suitable border potential. This therefore provides a concrete protocol for the observation of topologically protected degenerate edge modes in finite-size systems. Furthermore, we show that the bulk of the system is characterized by exotic solitonic solutions interpolating between the trivial and topological sectors. Finally, we propose an implementation and detection scheme of this strongly-correlated topological phase in a quantum simulator based on dipolar Fermi gases in optical lattices. ","Revealing the topological nature of the bond order wave in a strongly
  correlated quantum system"
166,1474119561129742345,1339691759719342085,Sebastian Will,"['Check out our latest preprint - if you have ever wondered whether carbon molecules can be laser cooled this paper may offer some answers. Thanks so  much to Niccolo Bigagli for leading this study that is a bit more out of the box than what we usually do <LINK> <LINK>', 'Hope you enjoy it and let us know your thoughts!']",https://arxiv.org/abs/2112.10745,We report on a scheme for laser cooling of $^{12}$C$_2$. We have calculated the branching ratios for cycling and repumping transitions and calculated the number of photon scatterings required to achieve deflection and laser cooling of a beam of $C_2$ molecules under realistic experimental conditions. Our results demonstrate that C$_2$ cooling using the Swan ($d^3\Pi_\text{g} \leftrightarrow a^3\Pi_\text{u}$) and Duck ($d^3\Pi_\text{g} \leftrightarrow c^3\Sigma_\text{u}^+$) bands is achievable via techniques similar to state-of-the-art molecular cooling experiments. The Phillips ($A^1\Pi_\text{u} \leftrightarrow X^1\Sigma_\text{g}^+$) and Ballik-Ramsay ($b^3\Sigma_\text{g}^- \leftrightarrow a^3\Pi_\text{u}$) bands offer the potential for narrow-line cooling. This work opens up a path to cooling of molecules with carbon-carbon bonds and may pave the way toward quantum control of organic molecules. ,Laser Cooling Scheme for the Carbon Dimer ($^{12}$C$_2$)
167,1474089465408888872,1304363050791772160,Florio M. Ciaglia,"['What can Lie algebras tell us about Jordan algebras? Among other things which I surely do not know, they can tell us how to find the Fisher-Rao and the Bures-Helstrom metric tensor as if we were looking for the canonical symplectic form on coadjoint orbits\n<LINK>']",https://arxiv.org/abs/2112.09781,"Inspired by Kirillov's theory of coadjoint orbits, we develop a structure theory for finite dimensional Jordan algebras. Given a Jordan algebra ${\mathcal{J}}$, we define a generalized distribution $\mathcal{H}^{{\mathcal{J}}}$ on its dual space ${\mathcal{J}}^\star$ which is canonically determined by the Jordan product in ${\mathcal{J}}$, is invariant under the action of what we call the structure group of ${\mathcal{J}}$, and carries a naturally-defined pseudo-Riemannian bilinear form ${\mathcal{G}}_{\xi}$ at each point. We then turn to the case of positive Jordan algebras and classify the orbits of ${\mathcal{J}}^\star$ under the structure group action. We show that the only orbits which are also leaves of $\mathcal{H}^{{\mathcal{J}}}$ are those in the closure of the cone of squares or its negative, and these are the only orbits where this pseudo-Riemannian bilinear form determines a Riemannian metric tensor ${\mathcal{G}}$. We discuss applications of our construction to both classical and quantum information geometry by showing that, for appropriate choices of ${\mathcal{J}}$, the Riemannian metric tensor ${\mathcal{G}}$ coincides with the Fisher-Rao metric on non-normalized probability distributions on a finite sample space, or with the Bures-Helstrom metric for non-normalized, faithful quantum states of a finite-level quantum system. ",What Lie algebras can tell us about Jordan algebras
168,1473985099914002433,399331417,Max Falkenberg,"['Preprint in time for Christmas! üéÖ\n\n""Growing climate polarisation on social media""\n<LINK>\n\nUsing a hefty new dataset covering the online discussion from #COP20 to #COP26, we find a very large recent increase in polarised views on climate.\n\nSome highlights üßµüëá', 'COP has been around for a long time, but engagement with #COP26 far exceeded any previous conference. A lot of people are now very invested in the COP process! https://t.co/zbbvWNMvRt', 'Broadly speaking, our work studies the polarisation during COP between a cluster we call the ideological majority (where the bulk of accounts are found), and and ideological minority.\n\nThe majority are largely supportive of COP and climate action. The minority less so...', 'Between #COP20 and #COP25, the discussion around COP was relatively uniform. In #COP21, only 3 of the top 300 most retweeted accounts are outside the ideological majority (the spike at +1 below). They are @JunkScience, @BjornLomborg, and the now suspended Tony Heller https://t.co/mAgCc0aW74', 'Recently however, right-wing engagement has shot up. During #COP26, 56 of the top 300 accounts fell outside the majority cluster. Some key players? @GBNEWS, @PrisonPlanet, @SteveBakerHW and many more. https://t.co/bn5IxvBjZp', 'One interesting observation: Not everyone in this minority cluster are climate focused. Several primarily campaign against further Covid-19 restrictions, e.g. @JamesMelville\n\nThis is captured in the hashtags used by the minority group, where #COVID19 comes out on top', ""The dominant hashtags used by each ideological group? Those in blue are used mainly by the minority, those in yellow mainly by the majority. The minority don't like Nicola Sturgeon very much... https://t.co/aH8hcRSGiy"", 'The overall result? Between #COP20 and #COP25, we observe no statistically significant polarisation in the online discussion around COP. In #COP26 this has risen dramatically. https://t.co/TN3RLlXQjT', ""Of course we shouldn't forget about the ideological majority where we find most accounts in the COP discussion. Who is there? The majority of traditional news media, politicians, activists, and major international climate organisations. There are of course divisions within..."", '...this majority cluster aswell. But relative to the divide between the majority cluster, and the minority who, broadly speaking, are not very supportive of the COP process, these divides are almost invisible.', 'With #COP27, and #COP28 and ... upcoming. It will be interesting to see how these structural divides evolve in the future, and what affect they have on the negotiations.', 'Of course this is just a preprint - more to come early 2022!\n\nWork in a very Italian collaboration with @a_baronca, @Walter4C, @zollofab, @DeveloperGale, @___Maddalena___ , @miscomplexity, @WarrenPearce, @Nicco84394204 and @FrancescaLaros1', 'And @AminMekacher!! Oops...']",https://arxiv.org/abs/2112.12137,"Climate change and political polarisation are two critical social and political issues of the 21st century. However, their interaction remains understudied. Here, we investigate the online discussion around the UN Conference of The Parties on Climate Change (COP) using social media data from 2014 to 2021. First, we highlight that cross-platform engagement peaked during COP26. Second, focusing on Twitter, we reveal a large increase in ideological polarisation during COP26, following low ideological polarisation between COP20 and COP25. Finally, we show that this increase is driven by growing right-wing engagement (a 4-fold increase since COP21), while we find no evidence of polarisation driven by individuals moving between ideological groups. With future climate action reliant on negotiations at COP27 and beyond, our results highlight the importance of monitoring polarisation in the public climate discourse and how this may impact political action. ",Growing polarisation around climate change on social media
169,1473794351213936640,304025698,Richard Easther,"['[Technical] On the ArXiV just before the break, with @drdrparky and Benjamin Bahr-Kalus -- we find that the latest BICEP3/Keck results put tight constraints on simple models of inflation  <LINK>', 'In particular, the latest bounds on the spectrum of density perturbations and the gravitational wave background imply that all simple inflationary models predict that inflation lasts for too long (Contours # of e-folds after the pivot leaves the horizon and posterior for N) https://t.co/Zw5ryxVNBt', ""The upshot is that the cosmological data has reached the point where are forced to update our priors on the inflationary parameter space. This doesn't kill inflation, as there is no requirement for inflation to be simple (and there are arguments that simple models are fragile)"", 'But if we add the next-to-leading order term in the potential to control the duration of inflation it also predicts that the density perturbations have more scale dependence than we see in simple models... Which boosts the ""running"" https://t.co/Rp4mawF0iz', 'In the process we revisit ideas about ""universality classes"" in inflation, and show that the expectations these provide are modified if the gravitational wave background is very small... Currently, observations limit the amplitude of gravitational waves from inflation to about...', ""4% of the density perturbation background, and that will improve by two orders of magnitude in the next decade. But if they don't see a gw signal, we can argue that the next-most-simple inflationary models will produce a larger than expected running of the scalar spectrum..."", '... and this will be at the threshold of detectability in 2030.  So there is a see-saw in the inflationary parameter space, very low ""r"" might make for a larger running. https://t.co/YxcpjwfLlM', 'And we get a clearer idea of what the consequences of a null result for sensitive primordial gravitational wave will be for theories of the very early universe. \n\n[Setting expectations for inflation is complex; we are thinking about physics at energies a trillion times...', '... beyond the LHC; but inflation has  made a bunch of successful predictions, but there has never been real consensus on the tensor background. But we can now look carefully at the options that are consistent with observations, and that is quite a step for cosmology]', 'Also -- big shoutout to CHIME radio telescope, CMB-S4 and SPHEREx  (not sure about twitter handles, but @SimonsObs @NASAJPL #CHIMETelescope  @DunlapInstitute??)']",https://arxiv.org/abs/2112.10922,"Inflationary cosmology proposes that the early Universe undergoes accelerated expansion, driven, in simple scenarios, by a single scalar field, or inflaton. The form of the inflaton potential determines the initial spectra of density perturbations and gravitational waves. We show that constraints on the duration of inflation together with the BICEP3/Keck bounds on the gravitational wave background imply that higher derivatives of the potential are nontrivial with a confidence of 99%. Such terms contribute to the scale-dependence, or running, of the density perturbation spectrum. We clarify the ""equivalence classes"" of inflationary models in this limit, showing that models with a very small gravitational wave background typically have a larger running and that if this background is not observed by pending experiments the running could be at the threshold of detectability. Correlated expectations for the running and gravitational wave background provide an new avenue by which future observations may yield insight into possible inflationary mechanisms. ",Born to Run: Inflationary Dynamics and Observational Constraints
170,1473751912373633024,990433714948661250,Sergey Levine,"['In the real world, humans learn autonomously, without a simulator, and without magically resetting the world instantly to retry. Can we shift from episodic RL, and study ""autonomous"" RL instead that works more like that?\nPaper: <LINK>\nArchit\'s thread below üëá <LINK>']",https://arxiv.org/abs/2112.09605,"Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when attempting to take RL algorithms developed for episodic simulated environments and run them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy. ",Autonomous Reinforcement Learning: Formalism and Benchmarking
171,1473666732745232387,1347185828108365825,Philipp Sudek,['Paper Day!\nWe studied the sensitivity of the redshift distribution to galaxy demographics. It is the next step to overcome the limitations of spectroscopic and photometric redshifts.\n#cosmology #Astrophysics #galaxy\n\n<LINK>\n\n@UoPCosmology @portsmouthuni'],https://arxiv.org/abs/2112.11345,"Photometric redshifts are commonly used to measure the distribution of galaxies in large surveys. However, the demands of on-going and future large-scale cosmology surveys place very stringent limits on the redshift performance that are difficult to meet. A new approach to meet this precision need is forward modelling, which is underpinned by realistic simulations. In the work presented here, we use simulations to study the sensitivity of redshift distributions to the underlying galaxy population demographics. We do this by varying the redshift evolving parameters of the Schechter function for two galaxy populations, star-forming and quenched galaxies. Each population is characterised by eight parameters. We find that the redshift distribution of shallow surveys, such as SDSS, are mainly sensitive to the parameters for quenched galaxies. However, for deeper surveys such as DES and HSC, the star-forming parameters have a stronger impact on the redshift distribution. Specifically, the slope of the characteristic magnitude, $a_\mathrm{M}$, for star-forming galaxies has overall the strongest impact on the redshift distribution. Decreasing $a_\mathrm{M}$ by 148 % (its given uncertainty) shifts the mean redshift by ${\sim} 45$ %. We explore which combination of colour and magnitude measurements are most sensitive to $a_\mathrm{M}$ and we find that each colour-magnitude pair is similarly affected by a modification of $a_\mathrm{M}$. ",The Sensitivity of the Redshift Distribution to Galaxy Demographics
172,1473570001948925952,822867138,Bradley Kavanagh,"['One last paper before Christmas! With colleagues at @IFCA_CSIC_UC, we studied the Dark Axion Portal model: <LINK>\n\nWe tried to pin-point the parameter space where Dark Photons &amp; QCD Axions could co-exist as #DarkMatter, and be detected in upcoming direct searches <LINK>', ""Big congratulations to former Masters Student Juan Cortabitarte Guti√©rrez, who did the lion's share of this work as part of his Masters project at @unican @UIMP. https://t.co/s67lHvUz2k"", 'Axions may be detectable with haloscopes, and eV-scale Dark Photons with low threshold CCDs (both of which are being developed at @IFCA_CSIC_UC).\n\nOne cool idea we explored with this work is that you could perhaps directly detect these two different kinds of DM simultaneously! https://t.co/0q7Rk0lwsa', 'A big thanks to my collaborators on helping to bring together these different ideas. \n\nAnd a special shout out to @cajohare for maintaining https://t.co/26fz7uSnHR. Some of our plots bear a striking resemblance to his...', '@DD_Baumann @cajohare This is the real peer review üòÑ\n\nAlso - DAMMIT! I knew I was going to miss one üòí', ""@_subodhpatil @cajohare Hahaha. Thanks. That's the great thing about trying to keep the plotting scripts in good condition (https://t.co/dVfRTxfI7x). It should only take me a couple of minutes to bump up the font sizes for talks!"", ""@DrTonyPadilla @IFCA_CSIC_UC It's 18 degrees here today, so I'm also still quite happy with the Santander address üòÜ"", ""@DrTonyPadilla @IFCA_CSIC_UC That's very true. It's actually been raining constantly the last few weeks, so I'm just appreciating some warmth and dryness!"", '@_subodhpatil @cajohare It could be worse (https://t.co/Vv6Geq3KpG)... https://t.co/BPManzNUTl', '@_subodhpatil @cajohare This plot is so self-referential. https://t.co/tjNDu36Mq9']",https://arxiv.org/abs/2112.11387,"The Dark Axion Portal provides a model for Dark Matter (DM) in which both Dark Photons $\gamma^\prime$ and Axions $a$ can contribute to the present day abundance of DM. We study the parameter space of the Dark Axion Portal to pinpoint regions of the parameter space where $\gamma^\prime$ and $a$ can be produced with sufficient abundance to account for the cosmic DM density, while still being detectable in planned direct detection and axion haloscope experiments. In particular, we explore the production of eV-scale Dark Photons in the Dark Axion Portal, taking into account a possible kinetic mixing between the dark and visible photons, which is essential for the detection of dark photons through absorption in direct searches. We show that a non-zero kinetic mixing does not generally spoil the phenomenology of the model, leaving both the axion and dark photon stable. Viable production mechanisms point to a sub-dominant population of dark photons making up $\lesssim 10\%$ of the DM, with the remainder consisting of axion DM. Dark photons in the mass range $m_{\gamma^\prime} \sim 20-200\,\mathrm{eV}$ and axions in the mass range $m_a \sim 30 - 400\,\mu\mathrm{eV}$ may be produced with these abundances self-consistently in the Dark Axion Portal and are within the reach of future direct searches. ",Cosmology and direct detection of the Dark Axion Portal
173,1473457226601963523,1444381431195648000,Felix A. Palm,"['In our latest work, we propose a way to extract the central charge from snapshots. We also discuss other signatures for the bosonic Laughlin state in coupled chains. <LINK>\n\nSpecial thanks to @aBohrdt for her advice on creating even nicer figures ;) <LINK>']",https://arxiv.org/abs/2112.10763,"Experimental realizations of topologically ordered states of matter, such as fractional quantum Hall states, with cold atoms are now within reach. In particular, optical lattices provide a promising platform for the realization and characterization of such states, where novel detection schemes enable an unprecedented microscopic understanding. Here we show that the central charge can be directly measured in current cold atom experiments using the number entropy as a proxy for the entanglement entropy. We perform density-matrix renormalization-group simulations of Hubbard-interacting bosons on coupled chains subject to a magnetic field with $\alpha=\frac{1}{4}$ flux quanta per plaquette. Tuning the inter-chain hopping, we find a transition from a trivial quasi-one dimensional phase to the topologically ordered Laughlin state at magnetic filling factor $\nu=\frac{1}{2}$ for systems of three or more chains. We resolve the transition using the central charge, on-site correlations, momentum distributions and the many-body Chern number. Additionally, we propose a scheme to experimentally estimate the central charge from Fock basis snapshots. The model studied here is experimentally realizable with existing cold atom techniques and the proposed observables pave the way for the detection and classification of a larger class of interacting topological states of matter. ","Snapshot-based detection of $\frac{1}{2}$-Laughlin states: coupled
  chains and central charge"
174,1473398267681640454,892059194240532480,Mikel Artetxe,"['We are releasing a family of dense and MoE language models with up to 13B and 1.1T parameters. We find that MoEs are more efficient, but the gap narrows at scale and varies greatly across domains and tasks.\n\nPaper: <LINK>\n\nModels &amp; code: <LINK> <LINK>', 'Joint work with @shruti_bhosale, @NamanGoyal21, @tbmihaylov, @myleott, @sam_shleifer, @VictoriaLinML, @JefferyDuu, @sriniiyer88, @ramakanth1729, Giri Anantharaman, @xl_nlp, Shuohui Chen, @halilakin, Mandeep Baines, @louismrt, Xing Zhou, @punitkoura, Brian O‚ÄôHoro, @jffwng,', '@LukeZettlemoyer, Mona Diab, @zkozareva &amp; @vesko_st']",https://arxiv.org/abs/2112.10684,"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use. ",Efficient Large Scale Language Modeling with Mixtures of Experts
175,1473378413662507011,3254308720,Scott Emmons,"['What is essential for policy learning from fixed/precollected data? We find that supervised learning with just a depth-two MLP is competitive with SoTA algorithms. No TD learning, advantage reweighting, or Transformers!\n\n<LINK>\n<LINK> <LINK>', '@MishaLaskin Thanks! We focus on MDPs (fully observable), not POMDPs. In preliminary experiments, we find larger contexts often help DT in the fully-observable D4RL Gym Locomotion tasks. So larger contexts in Atari could also just be a different way to provide more model capacity', ""@TrentonBricken Thank you! We haven't tried that, but having agents learn to produce their own goals and goal spaces is a neat direction for future work that we mention in the conclusion :)""]",http://arxiv.org/abs/2112.10751,"Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which algorithmic components are necessary? Through extensive experiments, we boil supervised learning for offline RL down to its essential elements. In every environment suite we consider, simply maximizing likelihood with a two-layer feedforward MLP is competitive with state-of-the-art results of substantially more complex methods based on TD learning or sequence modeling with Transformers. Carefully choosing model capacity (e.g., via regularization or architecture) and choosing which information to condition on (e.g., goals or rewards) are critical for performance. These insights serve as a field guide for practitioners doing Reinforcement Learning via Supervised Learning (which we coin ""RvS learning""). They also probe the limits of existing RvS methods, which are comparatively weak on random data, and suggest a number of open problems. ",RvS: What is Essential for Offline RL via Supervised Learning?
176,1472930170600243200,1016659760672886785,Rishit Dagli #KubeCon,"['1/ Happy to announce our paper ""CPPE-5: Medical Personal Protective Equipment Dataset"", a üßµ\n\nWe introduce a new challenging image dataset with the goal to allow the study of subordinate categorization of medical PPE unlike any existing dataset\n\nPaper: <LINK> <LINK>', '2/ Trying to do so is not possible with other popular large-scale data sets that focus on broad-level categories. We also find that currently there exists no such dataset and critique the few *similar* datasets that either consist of ‚Ä¶', '3/ artificial images, focus only on a single category (eg. mask), far less data, or do not focus on medical scenarios altogether which I believe is super helpful\n\nThe dataset also allows easily deploying in complex scenes with other objects as well due to the following features', '4/ The dataset contains:\n- high-quality images and annotations (~4.7 bbox/image)\n- real-life images unlike any current dataset\n- a majority of non-iconic images in their natural context often with multiple other objects\n- annotations for 5 PPE items https://t.co/CbqkEEWvXg', '5/ We perform algorithmic analysis on the dataset and open-source some of the top-performing models on this dataset in the code repository: https://t.co/M0SarshXob\n\nThe PyTorch and TensorFlow data loader, tutorial to load this dataset, a training tutorial and more are in the repo https://t.co/z7k49fFCeU', '6/ We are thankful to @Google @TensorFlow for a generous research grant and TPU Research Cloud for providing access to TPUs\n\ncc: lovely collaborating with @ialimustufa']",https://arxiv.org/abs/2112.09569,"We present a new challenging dataset, CPPE - 5 (Medical Personal Protective Equipment), with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories (such as PASCAL VOC, ImageNet, Microsoft COCO, OpenImages, etc). To make it easy for models trained on this dataset to be used in practical scenarios in complex scenes, our dataset mainly contains images that show complex scenes with several objects in each scene in their natural context. The image collection for this dataset focusing on: obtaining as many non-iconic images as possible and making sure all the images are real-life images unlike other existing datasets in this area. Our dataset includes 5 object categories (coveralls, face shield, gloves, mask, and goggles) and each image is annotated with a set of bounding boxes and positive labels. We present a detailed analysis of the dataset in comparison to other popular broad category datasets as well as datasets focusing on personal protective equipments, we also find that at present there exist no such publicly available datasets. Finally we also analyze performance and compare model complexities on baseline and state-of-the-art models for bounding box results. Our code, data, and trained models are available at this https URL . ",CPPE-5: Medical Personal Protective Equipment Dataset
177,1471809213471117313,2656302854,Ronald Drimmel üá∫üá¶,"['New coauthered paper on the ArXiv today on the Radcliff Wave, led by Lekshmi Thulasidharan.  We find that the kinematics of young objects indeed show evidence of wave-like motion, but that these vertical motions extend well beyond the Radcliff Wave itself.\n<LINK> <LINK>', ""I also just noticed that the author list is appropriately ordered: The first three are women (who did all the work) followed by four old guys who gave lots of suggestions.  ü§¶\u200d‚ôÇÔ∏è (How's that for #OpenScience and #OverlyHonestMethods?)"", ""Oh wait! Silly me. @mac0598 isn't an old guy! Excuse me as I help myself out the door. ü§¶\u200d‚ôÇÔ∏èü§¶\u200d‚ôÇÔ∏è"", ""@rareflwr41 @mac0598 Well, you're right.  I was speaking a bit tongue in cheek. We were all having too much fun on this project to feel old, and are fortunate that Elena and Lekshmi invited us to participate.""]",https://arxiv.org/abs/2112.08390,"The Radcliffe Wave (RW) is a recently discovered sinusoidal vertical feature of dense gas in the proximity of the Sun. In the disk plane, it is aligned with the Local Arm. However, the origin of its vertical undulation is still unknown. This study constrains the kinematics of the RW, using young stars and open clusters as tracers, and explores the possibility of this oscillation being part of a more extended vertical mode. We study the median vertical velocity trends of the young stars and clusters along with the RW and extend it further to the region beyond it. We discover a kinematic wave in the Galaxy, distinct from the warp, with the amplitude of oscillation depending on the age of the stellar population. We perform a similar analysis in the N-body simulation of a satellite as massive as the Sagittarius dwarf galaxy impacting the galactic disk. When projected in the plane, the spiral density wave induced by the satellite impact is aligned with the RW, suggesting that both may be the response of the disk to an external perturbation. However, the observed kinematic wave is misaligned. It appears as a kinematic wave travelling radially, winding up faster than the density wave matched by the RW, questioning its origin. If a satellite galaxy is responsible for this kinematic wave, we predict the existence of a vertical velocity dipole that should form across the disk and this may be measurable with the upcoming Gaia DR3 and DR4. ",Evidence of a vertical kinematic oscillation beyond the Radcliffe Wave
178,1471796813204234241,957689165902118912,Alexandre Dauphin,"['Fresh from the arxivsüôÇ We study an interaction-induced topological phase in the extended Fermi-Hubbard model. We characterize both its bulk and edge topology. Finally, we discuss how to engineer and detect such a phase with dipolar gases.\n\n‚û°Ô∏è<LINK> <LINK>']",https://arxiv.org/abs/2112.08785,"We investigate the topological properties of the bond order wave phase arising in the extended Fermi-Hubbard model. In particular, we uncover a topological sector, which remained elusive in previous finite-size numerical studies due to boundary effects. We first show that, for an infinite system, the bond order wave regime is characterized by two degenerate bulk states corresponding to the trivial and topological sectors. The latter turns out to be indeed characterized by an even degeneracy of the entanglement spectrum and longe-range order of a string correlation function. For finite size systems, we show that the topological sector can be stabilized by imposing a suitable border potential. This therefore provides a concrete protocol for the observation of topologically protected degenerate edge modes in finite-size systems. Furthermore, we show that the bulk of the system is characterized by exotic solitonic solutions interpolating between the trivial and topological sectors. Finally, we propose an implementation and detection scheme of this strongly-correlated topological phase in a quantum simulator based on dipolar Fermi gases in optical lattices. ","Revealing the topological nature of the bond order wave in a strongly
  correlated quantum system"
179,1471547180162985985,1059553685691342848,Yunhao (Robin) Tang,"['In MAML RL, inner updates are based on N trajs. Unbiased grad estimates are available, however, working algos are often implemented w/ bias. Why the gap?\n \n<LINK>\n \nWe (1) show unbiased estimate has var O(N) (2) propose biased estimate w/ both var and bias O(1/N)']",https://arxiv.org/abs/2112.07328,"Despite the empirical success of meta reinforcement learning (meta-RL), there are still a number poorly-understood discrepancies between theory and practice. Critically, biased gradient estimates are almost always implemented in practice, whereas prior theory on meta-RL only establishes convergence under unbiased gradient estimates. In this work, we investigate such a discrepancy. In particular, (1) We show that unbiased gradient estimates have variance $\Theta(N)$ which linearly depends on the sample size $N$ of the inner loop updates; (2) We propose linearized score function (LSF) gradient estimates, which have bias $\mathcal{O}(1/\sqrt{N})$ and variance $\mathcal{O}(1/N)$; (3) We show that most empirical prior work in fact implements variants of the LSF gradient estimates. This implies that practical algorithms ""accidentally"" introduce bias to achieve better performance; (4) We establish theoretical guarantees for the LSF gradient estimates in meta-RL regarding its convergence to stationary points, showing better dependency on $N$ than prior work when $N$ is large. ","Biased Gradient Estimate with Drastic Variance Reduction for Meta
  Reinforcement Learning"
180,1471422004930748416,989097257986293760,Benjamin Steinegger,"['New paper led by @giulioburgio with @_AlexArenas. We study how homophily in the vaccine adoptions affects the spreading dynamics. We find three distinct regimes for the dependence of the epidemic size on the level of homophily. 1/6 \n<LINK>', 'More specifically, we find that, depending on vaccine efficacy,  vaccine adoption and epidemic pressure,  homophily can increase, decrease or affect non monotonously the epidemic size. 2/6', 'We previously found these three regimes  in the case of digital proximity tracing. Furthermore, we show that these regimes naturally emerge for any prophylactic tool that reduces the transmission probability such as face masks or social distancing. 3/6\n\nhttps://t.co/DF8mSf8TXQ', 'Just yesterday, we became aware that @takayukihir, @abbas_k_rizi, @bolozna and @JariSaramaki were working on the same problem. Their findings are very much equivalent to ours. 4/6\nhttps://t.co/44mvYSGFMx', 'While we consider a leaky vaccine in the mean-field case and in a physical contact network, they consider an all-or-nothing vaccine in random networks. This two studies combined illustrate how robust the phenomenology is. 5/6', 'Interesting to see people working on the same topic independently. They also did a similar study to ours on digital proximity tracing and found equivalent results. \nShout-out to al the people involved in these two studies! 6/6\nhttps://t.co/p495bAZ0mH']",https://arxiv.org/abs/2112.08240,"Physical contacts do not occur randomly, rather, individuals with similar socio-demographic and behavioural characteristics are more likely to interact among them, a phenomenon known as homophily. Concurrently, the same characteristics correlate with the adoption of prophylactic tools. As a result, the latter do not unfold homogeneously in a population, affecting their ability to control the spread of infectious diseases. Here, focusing on the case of vaccines, we reveal three different dynamical regimes as a function of the mixing rate between vaccinated and non vaccinated individuals. Specifically, depending on the epidemic pressure, vaccine coverage and efficacy, we find the attack rate to decrease, increase or vary non monotonously with respect to the mixing rate. We corroborate the phenomenology through Monte Carlo simulations on a temporal physical contact network. Besides vaccines, our findings hold for a wide range of prophylactic tools, indicating a universal mechanism in spreading dynamics ",Homophily impacts the success of vaccine roll-outs
181,1471252871274500101,939946467153924097,Jiacheng Xu,"['‚òû New preprint with @gregd_nlp: <LINK>\nWe propose a new search algorithm for neural text generation models to efficiently produce thousands of outputs encoded as a latticeüï∏Ô∏è. \nTwo key ideas: (1) best first search, (2) hypothesis recombination. Thread üßµ <LINK>', 'Motivation: we started thinking about how to get more factual outputs out of summarization models. Techniques like beam search just don‚Äôt give enough diverse options to rerank over ‚Äì but what if we could generate a massive number of high-quality generation outputs?', 'Our idea: construct a lattice for text generation which could hold a massive number of generation options in a compact space. Two ideas make this possible. https://t.co/uQXMt3tKsV', '(1): Best-first search replacing beam search. This ends up being a more efficient way to explore the space and avoids building hypotheses that get pruned, which is wasteful if we want lots of options.', '(2): Hypothesis recombination: we merge similar paths to produce a lattice structure üï∏Ô∏è. This encodes hypotheses much more compactly and allows us to explore the space faster by expanding multiple hypotheses at once (e.g., generating more in the figure extends both paths so far). https://t.co/63vRtRLXHx', 'The results show that we can often encode thousands of generation candidates of fairly good quality. We additionally show that these remain mostly grammatical (they don‚Äôt lose quality from recombination) and encode diverse, high-quality outputs. https://t.co/jjD3WGIhRF', 'Code: https://t.co/HFWPOb2PSU\nExamples: https://t.co/NHdU93eo6M\nIf you‚Äôre interested in customizing or reranking generation outputs, we encourage you to give this a try! We‚Äôre very excited about the potential of this technique.', '@varun_kr @gregd_nlp Yes. You can apply any downstream reranker/classifer to get the one you want. You can also treat the lattice as a collection of local graphs, so we can know/learn that ""a good/great/nice movie"" is probably fine but ""located in Austin/LA/NYC ..."" is probably wrong.', '@shreydesai @gregd_nlp A potential direction is to learn from local structures in the lattice to know 1) how LMs prioritize decisions/predictions, 2) how confident they are, and how to calibrate/prune/understand them.']",https://arxiv.org/abs/2112.07660,"Conditional neural text generation models generate high-quality outputs, but often concentrate around a mode when what we really want is a diverse set of options. We present a search algorithm to construct lattices encoding a massive number of generation options. First, we restructure decoding as a best-first search, which explores the space differently than beam search and improves efficiency by avoiding pruning paths. Second, we revisit the idea of hypothesis recombination: we can identify pairs of similar generation candidates during search and merge them as an approximation. On both summarization and machine translation, we show that our algorithm encodes thousands of diverse options that remain grammatical and high-quality into one lattice. This algorithm provides a foundation for building downstream generation applications on top of massive-scale diverse outputs. ",Massive-scale Decoding for Text Generation using Lattices
182,1471144565696602112,78346516,Cristi√°n Bravo,"['Final preprint of the year! In this work, with Matthew Stevenson and Christophe Mues, we study the use of LiDAR data for societal, organizational and business applications. \n\n<LINK>', 'We find that unsupervised embeddings can be used to summarize this data and allow its use by those who normally would not have the resources to do so.', 'You just need to pay the initial cost of creating the embeddings but this could be even centralized by government or universities and made available to the public.', ""LiDAR data is generally freely available in advanced economies and has been mostly neglected in the social sciences. If your organization has a demographic component (and whose hasn't!) then you can certainly leverage our results to improve your models."", 'As always: preprints are early-stage papers and will change before final publication.', 'I forgot to @ Matt! The first author is @mattpstevenson of course.']",https://arxiv.org/abs/2112.01421,"LiDAR (short for ""Light Detection And Ranging"" or ""Laser Imaging, Detection, And Ranging"") technology can be used to provide detailed three-dimensional elevation maps of urban and rural landscapes. To date, airborne LiDAR imaging has been predominantly confined to the environmental and archaeological domains. However, the geographically granular and open-source nature of this data also lends itself to an array of societal, organizational and business applications where geo-demographic type data is utilised. Arguably, the complexity involved in processing this multi-dimensional data has thus far restricted its broader adoption. In this paper, we propose a series of convenient task-agnostic tile elevation embeddings to address this challenge, using recent advances from unsupervised Deep Learning. We test the potential of our embeddings by predicting seven English indices of deprivation (2019) for small geographies in the Greater London area. These indices cover a range of socio-economic outcomes and serve as a proxy for a wide variety of downstream tasks to which the embeddings can be applied. We consider the suitability of this data not just on its own but also as an auxiliary source of data in combination with demographic features, thus providing a realistic use case for the embeddings. Having trialled various model/embedding configurations, we find that our best performing embeddings lead to Root-Mean-Squared-Error (RMSE) improvements of up to 21% over using standard demographic features alone. We also demonstrate how our embedding pipeline, using Deep Learning combined with K-means clustering, produces coherent tile segments which allow the latent embedding features to be interpreted. ","Deep residential representations: Using unsupervised learning to unlock
  elevation data for geo-demographic prediction"
183,1471070813637984257,804069495253962752,David Mart√≠nez Delgado,"[""We have posted our new MEGARA @GTCtelescope study of the blue stellar stream of NGC 7241, possibly one of  the lowest mass streams detected beyond the Local Group. And we find the stream's progenitor is suffering a star-formation burst!\n<LINK>  (Credit: @ngc1535) <LINK>""]",https://arxiv.org/abs/2112.07029,"We study the striking case of a blue narrow stream with a possible globular cluster-like progenitor around the Milky Way-size galaxy NGC 7241 and its foreground dwarf companion. We present a follow-up spectroscopic study of this stream based on data taken with the MEGARA instrument at the 10.4-m Gran Telescopio Canarias using the integral field spectroscopy mode. Although our data suggest that this compact object in the stream is actually a foreground Milky Way halo star, we detect emission lines overlapping a less compact, bluer and fainter blob of the stream that is clearly visible in both ultra-violet and optical deep images. From its heliocentric systemic radial velocity derived from the [OIII] 5007A lines (V_syst= 1548.58+/-1.80 km\s^-1) and new UV and optical broad-band photometry, we conclude that this over-density could be the actual core of the stream, with an absolute magnitude of Mg~ -10 and a g-r = 0.08+/- 0.11, consistent with a remnant of a low-mass dwarf satellite undergoing a current episode of star formation. From the width of the stream, we calculate that the progenitor mass is between 6.4 x 10^6 Mo -2.7 x 10^7 Mo, which is typical of a dwarf galaxy. These estimates suggest that this is one of the lowest mass streams detected so far beyond the Local Group. We find that blue stellar streams containing star formation regions are commonly predicted by high-resolution cosmological simulations of galaxies lighter than the Milky Way. This scenario is consistent with the processes explaining the bursty star formation history of some dwarf satellites, which are followed by a gas depletion and a fast quenching once they enter within the virial radius of their host galaxies. Thus, it is likely that the stream's progenitor is suffering a star-formation burst comparable to those that have shaped the star-formation history of several Local Group dwarfs in the last few Gigayears. ","Once in a blue stream: Detection of recent star formation in the NGC
  7241 stellar stream with MEGARA"
184,1470794357280108544,1011816374379929600,Nick Young,"['The final paper of my @PERLatMSU dissertation with @physicistdanny has been submitted! In this paper, we find that rubric-based admissions might represent a different approach to graduate admissions than the traditional process. <LINK> üßµ 1/10 <LINK>', ""ICYMI, here's the first paper in the series describing what we already knew about rubric-based graduate admissions in physics. 2/10 https://t.co/n8AieLElfO"", ""What we didn't know yet was whether rubric-based graduate admissions represented a different way to admit students.\n\nOr if it was just a new tool to keep up the same exclusionary process departments have been using for decades. 3/10"", 'To find out, we looked at 7 years of admissions data from our physics &amp; astronomy department. 4 years of data came from before the implementation of the rubric and 3 years of data came from after the implementation rubric. 4/10', 'In previous work, we were able to build machine learning models to understand how the admissions committee admitted applications during those first 4 years. 5/10 https://t.co/82WkJc8p8j', 'Therefore, we should be able to do it again and see what parts of the application were most predictive of admission. 6/10', ""Well, it didn't quite work that way. Instead, despite trying a variety of methods, we were unable to construct a useful model this time. 7/10"", ""When we compared the applicants from the two time periods, we didn't find too many differences, suggesting that it must have been the admissions process that changed. 8/10"", ""But, there's still work to do to strengthen that conclusion. Maybe it is was our methods that were the problem. 9/10"", 'Or maybe the admissions process became more holistic as intended. We will hopefully find out as more institutions try rubric-based admissions for their graduate program! 10/10', '@physicistdanny @DrBuncher @PERLatMSU Yep, it cites them and they are also linked in the comments on the arXiv landing page for this paper.']",https://arxiv.org/abs/2112.06886,"Rubric-based admissions are claimed to help make the graduate admissions process more equitable, possibly helping to address the historical and ongoing inequities in the U.S. physics graduate school admissions process that have often excluded applicants from minoritized races, ethnicities, genders, and backgrounds. Yet, no studies have examined whether rubric-based admissions methods represent a fundamental change of the admissions process or simply represent a new tool that achieves the same outcome. To address that, we developed supervised machine learning models of graduate admissions data collected from our department over a seven-year period. During the first four years, our department used a traditional admissions process and switched to a rubric-based process for the following three years, allowing us to compare which parts of the applications were used to drive admissions decisions. We find that faculty focused on applicants' physics GRE scores and grade point averages when making admissions decisions before the implementation of the rubric. While we were able to develop a sufficiently good model whose results we could trust for the data before the implementation of the rubric, we were unable to do so for the data collected after the implementation of the rubric, despite multiple modifications to the algorithms and data such as implementing Tomek Links. Our inability to model the second data set despite being able to model the first combined with model comparison analyses suggests that rubric-based admissions does change the underlying process. These results suggest that rubric-based holistic review is a method that could make the graduate admissions process in physics more equitable. ","Rubric-based holistic review represents a change from traditional
  graduate admissions approaches in physics"
185,1470770972496211977,14551614,Jason Weston,"[""üö®New paperüö® SOTA dialogue models are not winning Oscars anytime soon, as they cannot effectively stay in character. \n\nWe analyze and propose methods to measure &amp; mitigate -- but it's still an open problem.\n\n<LINK>\n@shtruk @JackUrbs Arthur Szlam @jaseweston <LINK>""]",https://arxiv.org/abs/2112.05843,"State-of-the-art dialogue models still often stumble with regards to factual accuracy and self-contradiction. Anecdotally, they have been observed to fail to maintain character identity throughout discourse; and more specifically, may take on the role of their interlocutor. In this work we formalize and quantify this deficiency, and show experimentally through human evaluations that this is indeed a problem. In contrast, we show that discriminative models trained specifically to recognize who is speaking can perform well; and further, these can be used as automated metrics. Finally, we evaluate a wide variety of mitigation methods, including changes to model architecture, training protocol, and decoding strategy. Our best models reduce mistaken identity issues by nearly 65% according to human annotators, while simultaneously improving engagingness. Despite these results, we find that maintaining character identity still remains a challenging problem. ","Am I Me or You? State-of-the-Art Dialogue Models Cannot Maintain an
  Identity"
186,1470620929558740992,161398503,Hao Tang,"[""We'd like to share our experience on teaching quantum information technologies and a practical module for online and offline undergraduate students. üòÉIt may be a useful case study for education on quantum sciences and technologies.  <LINK>""]",https://arxiv.org/abs/2112.06548,"Quantum Information Technologies and a Practical Module is a new course we launch at Shanghai Jiao Tong University targeting at the undergraduate students who major in a variety of engineering disciplines. We develop a holistic curriculum for quantum computing covering the quantum hardware, quantum algorithms and applications. The quantum computing approaches include the universal digital quantum computing, analog quantum computing and the hybrid quantum-classical variational quantum computing that is tailored to the noisy intermediate-scale quantum (NISQ) technologies nowadays. Besides, we set a practical module to bring student closer to the real industry needs. The students would form a team of three to use any quantum approach to solve a problem in fields like optimization, finance, machine learning, chemistry and biology. Further, this course is selected into the Jiao Tong Global Virtual Classroom Initiative, so that it is open to global students in Association of Pacific Rim Universities at the same time with the offline students, in a specifically updated classroom. The efforts in curriculum development, practical module setting and blended learning make this course a good case study for education on quantum sciences and technologies. ","Teaching quantum information technologies and a practical module for
  online and offline undergraduate students"
187,1470388250925842451,729647321642881025,Janusz Petkowski,"['We have concluded an 18-month long concept study of a series of missions to Venus to look for life in the clouds ‚Äì life as we do not know it, might I add. Congratulations to the leader of the VLF Collaboration - Prof. Sara Seager and the VLF team the work.\n<LINK> <LINK>']",https://arxiv.org/abs/2112.05153,"The Venus Life Finder Missions are a series of focused astrobiology mission concepts to search for habitability, signs of life, and life itself in the Venus atmosphere. While people have speculated on life in the Venus clouds for decades, we are now able to act with cost-effective and highly-focused missions. A major motivation are unexplained atmospheric chemical anomalies, including the ""mysterious UV-absorber"", tens of ppm O$_2$, SO$_2$ and H$_2$O vertical abundance profiles, the possible presence of PH$_3$ and NH$_3$, and the unknown composition of Mode 3 cloud particles. These anomalies, which have lingered for decades, might be tied to habitability and life's activities or be indicative of unknown chemistry itself worth exploring. Our proposed series of VLF missions aim to study Venus' cloud particles and to continue where the pioneering in situ probe missions from nearly four decades ago left off. The world is poised on the brink of a revolution in space science. Our goal is not to supplant any other efforts but to take advantage of an opportunity for high-risk, high-reward science, which stands to possibly answer one of the greatest scientific mysteries of all, and in the process pioneer a new model of private/public partnership in space exploration. ",Venus Life Finder Mission Study
188,1470327845528518656,196749454,Natalie Hogg,"['Paper day! We use the validity of the distance duality relation as a consistency check for beyond LCDM cosmological models, and find a mild 2 sigma inconsistency between the latest SH0ES H0 measurement and the validity of the DDR üëÄ <LINK>', 'We also provide the first measurement of H0 and the distance duality relation to percentage accuracy using a combination of SNIa, BAO and cosmic chronometers! This measurement helps us to check the consistency of e.g. massive neutrinos and non-zero spatial curvature with the DDR.']",https://arxiv.org/abs/2112.05701,"The Etherington reciprocity theorem, or distance duality relation (DDR), relates the mutual scaling of cosmic distances in any metric theory of gravity where photons are massless and propagate on null geodesics. In this paper, we make use of the DDR to build a consistency check based on its degeneracy with the Hubble constant, $H_0$. We parameterise the DDR using the form $\eta(z) = 1+ \epsilon z$, thus only allowing small deviations from its standard value. We use a combination of late time observational data to provide the first joint constraints on the Hubble parameter and $\epsilon$ with percentage accuracy: $H_0 = 68.6 \pm 2.5$ kms$^{-1}$Mpc$^{-1}$ and $\epsilon = 0.001^{+0.023}_{-0.026}$. We build our consistency check using these constraints and compare them with the results obtained in extended cosmological models using cosmic microwave background data. We find that extensions to $\Lambda$CDM involving massive neutrinos and/or additional dark radiation are in perfect agreement with the DDR, while models with non-zero spatial curvature show a preference for DDR violation, i.e., $\epsilon \ne 0 $ at the level of $\sim 1.5 \sigma$. Most importantly, we find a mild 2$\sigma$ discrepancy between the validity of the DDR and the latest publicly available Cepheid-calibrated SNIa constraint on $H_0$. We discuss the potential consequences of this for both the Etherington reciprocity theorem and the $H_0$ tension. ",The resilience of the Etherington--Hubble relation
189,1469138089708855297,1252993183686025219,Oliver Philcox,"['New paper! Misha Ivanov &amp; I present the first joint full-shape analysis of the galaxy power spectrum and bispectrum using @sdssurveys data.\n\nWe find sigma8 = 0.72+-0.03, H0 = 68.3+-0.8, S8 = 0.75+-0.04, with the bispectrum improving sigma8 by 13%!\n\n<LINK> <LINK>', ""The analysis uses the power spectrum multipoles, the real-space power spectrum extension, the reconstructed power spectrum, and the bispectrum model. For the first time, spectra are measured using *unwindowed* estimators, so the mask doesn't need to be included in the theory!"", 'Our LCDM constraints are mostly consistent with Planck, but we find a slightly low S8, matching weak lensing probes. We also get strong constraints on galaxy bias parameters! https://t.co/LZ2AnQ1Ikt', 'All the data products are publicly available (https://t.co/NHYkXag5rx) and the pipeline can be easily reapplied to @desisurvey and @ESA_Euclid data.\n\nComing soon: bispectrum multipoles, neutrino masses, primordial non-Gaussianity...']",http://arxiv.org/abs/2112.04515,"We present a full $\Lambda$CDM analysis of the BOSS DR12 dataset, including information from the power spectrum multipoles, the real-space power spectrum, the reconstructed power spectrum and the bispectrum monopole. This is the first analysis to feature a complete treatment of the galaxy bispectrum, including a consistent theoretical model and without large-scale cuts. Unlike previous works, the statistics are measured using window-free estimators: this greatly reduces computational costs by removing the need to window-convolve the theory model. Our pipeline is tested using a suite of high-resolution mocks and shown to be robust and precise, with systematic errors far below the statistical thresholds. Inclusion of the bispectrum yields consistent parameter constraints and shrinks the $\sigma_8$ posterior by $13\%$ to reach $<5\%$ precision; less conservative analysis choices would reduce the error-bars further. Our constraints are broadly consistent with Planck: in particular, we find $H_0 = 69.6^{+1.1}_{-1.3}\,\mathrm{km}\,\mathrm{s}^{-1}\mathrm{Mpc}^{-1}$, $\sigma_8 = 0.692^{+0.035}_{-0.041}$ and $n_s=0.870^{+0.067}_{-0.064}$, including a BBN prior on the baryon density. When $n_s$ is set by Planck, we find $H_0 = 68.31^{+0.83}_{-0.86}\,\mathrm{km}\,\mathrm{s}^{-1}\mathrm{Mpc}^{-1}$ and $\sigma_8 = 0.722^{+0.032}_{-0.036}$. Our $S_8$ posterior, $0.751\pm0.039$, is consistent with weak lensing studies, but lower than Planck. Constraints on the higher-order bias parameters are significantly strengthened from the inclusion of the bispectrum, and we find no evidence for deviation from the dark matter halo bias relations. These results represent the most complete full-shape analysis of BOSS DR12 to-date, and the corresponding spectra will enable a variety of beyond-$\Lambda$CDM analyses, probing phenomena such as the neutrino mass and primordial non-Gaussianity. ","The BOSS DR12 Full-Shape Cosmology: $\Lambda$CDM Constraints from the
  Large-Scale Galaxy Power Spectrum and Bispectrum Monopole"
190,1468866414014603273,1207928467305771008,Pierluigi Rinaldi,"['For the first time, we are able to study the galaxy starburst/main-sequence bimodality over five decades in stellar mass at z ~ 3 - 6.5. #astrotwitter #astrostuffs #astronomy #astrophysics #research\n\nCheck it out: <LINK>']",https://arxiv.org/abs/2112.03935,"We study the relation between stellar mass (M*) and star formation rate (SFR) for star-forming galaxies over approximately five decades in stellar mass (5.5 <~ log10(M*/Msun) <~ 10.5) at z ~ 3-6.5. This unprecedented coverage has been possible thanks to the joint analysis of blank non-lensed fields (COSMOS/SMUVS) and cluster lensing fields (Hubble Frontier Fields) which allow us to reach very low stellar masses. Previous works have revealed the existence of a clear bimodality in the SFR-M* plane with a star-formation Main Sequence and a starburst cloud at z ~ 4-5. Here we show that this bimodality extends to all star-forming galaxies and is valid in the whole redshift range z ~ 3-6.5. We find that starbursts constitute at least 20% of all star-forming galaxies with M* >~ 10^9 Msun at these redshifts and reach a peak of 40% at z=4-5. More importantly, 60% to 90% of the total SFR budget at these redshifts is contained in starburst galaxies, indicating that the starburst mode of star-formation is dominant at high redshifts. Almost all the low stellar-mass starbursts with log10(M*/Msun) <~ 8.5 have ages comparable to the typical timescales of a starburst event, suggesting that these galaxies are being caught in the process of formation. Interestingly, galaxy formation models fail to predict the starburst/main-sequence bimodality and starbursts overall, suggesting that the starburst phenomenon may be driven by physical processes occurring at smaller scales than those probed by these models. ","The galaxy starburst/main-sequence bimodality over five decades in
  stellar mass at z ~ 3-6.5"
191,1468608182830387200,1072220386237009921,Guillaume Lajoie,"['New Preprint: We explore multi-scale feature learning dynamics in a simple teacher-student model. We find intriguing generalization error mechanisms leading to insights on training-wise ""double-descent"" phenomena. w/ @mpezeshki91, @AmartyaMitra, Y. Bengio\n<LINK>']",https://arxiv.org/abs/2112.03215,"A key challenge in building theoretical foundations for deep learning is the complex optimization dynamics of neural networks, resulting from the high-dimensional interactions between the large number of network parameters. Such non-trivial dynamics lead to intriguing behaviors such as the phenomenon of ""double descent"" of the generalization error. The more commonly studied aspect of this phenomenon corresponds to model-wise double descent where the test error exhibits a second descent with increasing model complexity, beyond the classical U-shaped error curve. In this work, we investigate the origins of the less studied epoch-wise double descent in which the test error undergoes two non-monotonous transitions, or descents as the training time increases. By leveraging tools from statistical physics, we study a linear teacher-student setup exhibiting epoch-wise double descent similar to that in deep neural networks. In this setting, we derive closed-form analytical expressions for the evolution of generalization error over training. We find that double descent can be attributed to distinct features being learned at different scales: as fast-learning features overfit, slower-learning features start to fit, resulting in a second descent in test error. We validate our findings through numerical experiments where our theory accurately predicts empirical findings and remains consistent with observations in deep neural networks. ",Multi-scale Feature Learning Dynamics: Insights for Double Descent
192,1468533684450041857,1159570181401845761,Stefan Prohazka,"['<LINK> The new work ""Carrollian and celestial spaces at infinity"" together with Jos√©, Emil and Jakob just appeared on the arXiv. To better understand holography we study low dimensional homogeneous spaces of Poincar√©. #Physics #arXiv', 'This homog. spaces are connected to (blown-up) asymptotic infinities (Spi, Ti, Ni, scri). To some extend this mirrors this aspect of (A)dS/CFT, but (for good reasons as we explain) it is more subtle.', 'I find it amazing how much ground can be covered by basically looking merely at the Poincar√© group and its homogenous spaces, invariants, etc. For a safe pronunciation of Ni see footnote 2 and https://t.co/DD3jX6H5Le ;).']",https://arxiv.org/abs/2112.03319,"We show that the geometry of the asymptotic infinities of Minkowski spacetime (in $d+1$ dimensions) is captured by homogeneous spaces of the Poincar\'e group: the blow-ups of spatial (Spi) and timelike (Ti) infinities in the sense of Ashtekar--Hansen and a novel space Ni fibering over $\mathscr{I}$. We embed these spaces \`a la Penrose--Rindler into a pseudo-euclidean space of signature $(d+1,2)$ as orbits of the same Poincar\'e subgroup of O$(d+1,2)$. We describe the corresponding Klein pairs and determine their Poincar\'e-invariant structures: a carrollian structure on Ti, a pseudo-carrollian structure on Spi and a ""doubly-carrollian"" structure on Ni. We give additional geometric characterisations of these spaces as grassmannians of affine hyperplanes in Minkowski spacetime: Spi is the (double cover of the) grassmannian of affine lorentzian hyperplanes; Ti is the grassmannian of affine spacelike hyperplanes and Ni fibers over the grassmannian of affine null planes, which is $\mathscr{I}$. We exhibit Ni as the fibred product of $\mathscr{I}$ and the lightcone over the celestial sphere. We also show that Ni is the total space of the bundle of scales of the conformal carrollian structure on $\mathscr{I}$ and show that the symmetry algebra of its doubly-carrollian structure is isomorphic to the symmetry algebra of the conformal carrollian structure on $\mathscr{I}$; that is, the BMS algebra. We show how to reconstruct Minkowski spacetime from any of its asymptotic geometries, by establishing that points in Minkowski spacetime parametrise certain lightcone cuts in the asymptotic geometries. We include an appendix comparing with (A)dS and observe that the de Sitter groups have no homogeneous spaces which could play the r\^ole that the celestial sphere plays in flat space holography. ",Carrollian and celestial spaces at infinity
193,1468168655728959490,2559430915,Bratislav Svetozar,['Here is our latest pre-print on Physically-consistent neural networks for building thermal modeling: <LINK>. I hope you find it as interesting and exciting to read as much we were while working on it. I hope also that you find a good use for it in your work.'],https://arxiv.org/abs/2112.03212,"Due to their high energy intensity, buildings play a major role in the current worldwide energy transition. Building models are ubiquitous since they are needed at each stage of the life of buildings, i.e. for design, retrofitting, and control operations. Classical white-box models, based on physical equations, are bound to follow the laws of physics but the specific design of their underlying structure might hinder their expressiveness and hence their accuracy. On the other hand, black-box models are better suited to capture nonlinear building dynamics and thus can often achieve better accuracy, but they require a lot of data and might not follow the laws of physics, a problem that is particularly common for neural network (NN) models. To counter this known generalization issue, physics-informed NNs have recently been introduced, where researchers introduce prior knowledge in the structure of NNs to ground them in known underlying physical laws and avoid classical NN generalization issues. In this work, we present a novel physics-informed NN architecture, dubbed Physically Consistent NN (PCNN), which only requires past operational data and no engineering overhead, including prior knowledge in a linear module running in parallel to a classical NN. We formally prove that such networks are physically consistent -- by design and even on unseen data -- with respect to different control inputs and temperatures outside and in neighboring zones. We demonstrate their performance on a case study, where the PCNN attains an accuracy up to $40\%$ better than a classical physics-based resistance-capacitance model on $3$-day long prediction horizons. Furthermore, despite their constrained structure, PCNNs attain similar performance to classical NNs on the validation data, overfitting the training data less and retaining high expressiveness to tackle the generalization issue. ","Physically Consistent Neural Networks for building thermal modeling:
  theory and analysis"
194,1468054353261076480,1115985999178420224,Towards AI,"['#arXiv #machinelearning [csLG] Neural Pseudo-Label Optimism for the Bank Loan Problem. (arXiv:2112.02185v1 [cs.LG]) <LINK> #mw\n\nWe study a class of classification problems best exemplified by the \\emph{bank loan} problem, where a lender decides whether or not t‚Ä¶']",http://arxiv.org/abs/2112.02185,"We study a class of classification problems best exemplified by the \emph{bank loan} problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender's algorithm to ``get stuck'' with a self-fulfilling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating infinite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present Pseudo-Label Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. \PLOT{} adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and finally uses the resulting \emph{optimistic} model for decision making. \PLOT{} achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that \PLOT{} satisfies a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data. ",Neural Pseudo-Label Optimism for the Bank Loan Problem
195,1467884753529540620,1272909280941879296,John F Wu,"[""üìúüìú It's paper day! üìúüìú\n\n‚û°Ô∏è <LINK> \n\ntldr: We use a CNN to find z&lt;0.03 satellite galaxies, and we measure their radial profiles around central host galaxies\n\nüßµThread below... 1/"", 'Check out these z&lt;0.03 (top) and higher-z (bottom) galaxies!\n\nThe ones on the left are spectroscopically confirmed through the SAGA (Satellites Around Galactic Analogs) Survey. \n\nThe ones on the right are selected by a convolutional neural network (CNN) based on their imaging! 2/ https://t.co/SmIVxeRPqb', 'We find &gt;100,000 CNN-selected galaxies!\n\nWe identify low-z *satellite* galaxies by finding CNN-selected galaxies within 300 projected kpc of spectroscopically confirmed z&lt;0.03 galaxies. 3/ https://t.co/5nRLT4suMZ', 'We can then compute the radial profile of satellites, i.e., the number of satellites as a function of projected separation from the host galaxy.\n\nThrough cross-validation tests, we can also measure completeness and contamination, and correct for these (see fig above). 4/', 'Okay, enough of the methods.\n\nMAIN RESULT #1: Host galaxies with higher stellar mass have more satellites!\n\n5/ https://t.co/k5s5H7287P', ""MAIN RESULT #2: Satellites are more abundant around *massive elliptical* galaxies!\n\nThere is a covariant effect between host galaxies' stellar mass and morphology.\n\n6/ https://t.co/DWDULKzNrC"", ""MAIN RESULT #3: The satellite abundance within a host galaxy's virial radius strongly correlates with the magnitude gap (=the difference in magnitude between the host and its brightest satellite).\n\nLook at those beautiful trends! üòç \n\n7/ https://t.co/z5SEMb6jnZ"", 'MAIN RESULT #4: The normalized radial distribution of satellites (i.e. the shape of the radial profile) does *not* vary at all with host galaxy properties! \n\nHere we see no variance as a function of host mass, but the same is also true for host morphology and magnitude gap. \n\n8/ https://t.co/TCbcCP9AJf', 'MAIN RESULT #5: Our findings confirm and extend key SAGA results!\n\n9/ https://t.co/BtnBpvDsfO', ""There's much more, including lots of technical details, and an initial comparison to hydrodynamic simulations. There's also a lot of promise for future directions! \n\nRead it all in the paper: https://t.co/Jw5WwdGqaX \n\n10/"", ""OR if you're curious about our code ü§ñ -- training CNNs, analyzing the data, etc -- check out our code on our Github: https://t.co/ia1KMvv8Vj\n\n11/"", 'Finally, I want to give a massive thanks to our amazing collaborators: \n\nJosh Peek @jegpeek, Erik Tollerud @eteq Yao-Yuan Mao @yaoyuanmao Ethan Nadler, Risa Wechsler @RisaWechsler, Marla Geha @mgeha, Nitya Kallivayalil @astronitya, and Ben Weiner.\n\n12/', 'This xSAGA paper would not be possible without their help or their work through the spectacular SAGA Survey (https://t.co/R9a1D9B1TC)!\n\n13/13']",https://arxiv.org/abs/2112.01542,"We present ""Extending the Satellites Around Galactic Analogs Survey"" (xSAGA), a method for identifying low-$z$ galaxies on the basis of optical imaging, and results on the spatial distributions of xSAGA satellites around host galaxies. Using spectroscopic redshift catalogs from the SAGA Survey as a training data set, we have optimized a convolutional neural network (CNN) to identify $z < 0.03$ galaxies from more distant objects using image cutouts from the DESI Legacy Imaging Surveys. From the sample of $> 100,000$ CNN-selected low-$z$ galaxies, we identify $>20,000$ probable satellites located between 36-300 projected kpc from NASA-Sloan Atlas central galaxies in the stellar mass range $9.5 < \log(M_\star/M_\odot) < 11$. We characterize the incompleteness and contamination for CNN-selected samples, and apply corrections in order to estimate the true number of satellites as a function of projected radial distance from their hosts. Satellite richness depends strongly on host stellar mass, such that more massive host galaxies have more satellites, and on host morphology, such that elliptical hosts have more satellites than disky hosts with comparable stellar masses. We also find a strong inverse correlation between satellite richness and the magnitude gap between a host and its brightest satellite. The normalized satellite radial distribution between 36-300 kpc does not depend strongly on host stellar mass, morphology, or magnitude gap. The satellite abundances and radial distributions we measure are in reasonable agreement with predictions from hydrodynamic simulations. Our results deliver unprecedented statistical power for studying satellite galaxy populations, and highlight the promise of using machine learning for extending galaxy samples of wide-area surveys. ","Extending the SAGA Survey (xSAGA) I: Satellite Radial Profiles as a
  Function of Host Galaxy Properties"
196,1467803071988314114,1347909035069206528,Anton Frisk Kockum,"['New preprint out today with Karl Hammar et int. Mats Granath @wacqt_sweden @chalmersuniv @goteborgsuni: <LINK>. We propose a new decoding algorithm for quantum error-correction codes.', 'Our decoder is an intermediate option between maximum-likelihood decoders and heuristic approaches like minimum-weight matching. The decoder is agnostic to the error rate, but uses information about the noise bias (whether phase-flip or bit-flip errors are more likely).']",https://arxiv.org/abs/2112.01977,"Efficient high-performance decoding of topological stabilizer codes has the potential to crucially improve the balance between logical failure rates and the number and individual error rates of the constituent qubits. High-threshold maximum-likelihood decoders require an explicit error model for Pauli errors to decode a specific syndrome, whereas lower-threshold heuristic approaches such as minimum weight matching are ""error agnostic"". Here we consider an intermediate approach, formulating a decoder that depends on the bias, i.e., the relative probability of phase-flip to bit-flip errors, but is agnostic to error rate. Our decoder is based on counting the number and effective weight of the most likely error chains in each equivalence class of a given syndrome. We use Metropolis-based Monte Carlo sampling to explore the space of error chains and find unique chains, that are efficiently identified using a hash table. Using the error-rate invariance the decoder can sample chains effectively at an error rate which is higher than the physical error rate and without the need for ""thermalization"" between chains in different equivalence classes. Applied to the surface code and the XZZX code, the decoder matches maximum-likelihood decoders for moderate code sizes or low error rates. We anticipate that, because of the compressed information content per syndrome, it can be taken full advantage of in combination with machine-learning methods to extrapolate Monte Carlo-generated data. ",Error-rate-agnostic decoding of topological stabilizer codes
197,1466491733823340554,2416760538,Peter Gao,"[""Happy to have been a part of this study led by Danica Adams from @Caltech! We tried to understand what causes the diversity in optical albedos of hot Jupiters within a narrow temperature range, and found that it's really complicated!\n\n<LINK>""]",https://arxiv.org/abs/2112.00041,"Optical secondary eclipse measurements made by \emph{Kepler} reveal a diverse set of geometric albedos for hot Jupiters with equilibrium temperatures between $1550-1700$ K. The presence or absence of high altitude condensates, such as Mg$_2$SiO$_4$, Fe, Al$_2$O$_3$, and TiO$_2$, can significantly alter optical albedos, but these clouds are expected to be confined to localized regions in the atmospheres of these tidally locked planets. Here, we present 3D general circulation models and corresponding cloud and albedo maps for six hot Jupiters with measured optical albedos in this temperature range. We find that the observed optical albedos of K2-31b and K2-107b are best matched by either cloud free models or models with relatively compact cloud layers, while Kepler-8b and Kepler-17b's optical albedos can be matched by moderately extended ($f_{sed}$ = 0.1) parametric cloud models. HATS-11b has a high optical albedo, corresponding to models with bright Mg$_2$SiO$_4$ clouds extending to very low pressures ($f_{sed}$ = 0.03). We are unable to reproduce Kepler-7b's high albedo, as our models predict that the dayside will be dominated by dark Al$_2$O$_3$ clouds at most longitudes. We compare our parametric cloud model with a two-zone microphysical cloud model (\texttt{CARMA}). We find that even after accounting for the 3D thermal structure, no single cloud model can explain the full range of observed albedos within the sample. We conclude that a better knowledge of the vertical mixing profiles, cloud radiative feedback, cloud condensate properties, and atmospheric metallicities is needed in order to explain the unexpected diversity of albedos in this temperature range. ","Spatially Resolved Modeling of Optical Albedos for a Sample of Six Hot
  Jupiters"
198,1475151380805525505,882180067870351360,Holcman,"['Motivated by gene activation by the first arriving transcription factors, we studied: \n\nthe ""Arrival time for the fastest among N switching stochastic particles""\n<LINK>\n\nMethods are modeling, asymptotic and stochastic  simulations.']",https://arxiv.org/abs/2112.12760,"The first arrivals among $N$ Brownian particles is ubiquitous in the life sciences, as it often trigger cellular processes from the molecular level. We study here the case where stochastic particles, which represent molecules, proteins or molecules can switch between two states inside the non-negative real line. The switching process is modeled as a two-state Markov chain and particles can only escape in state 1. We estimate the fastest arrival time by solving asymptotically the Fokker-Planck equations for three different initial distributions: Dirac-delta, uniformly distributed and long-tail decay. The derived formulas reveal that the fastest particle avoid switching when the switching rates are much smaller than the diffusion time scale, but switches twice when the diffusion is state 2 is much faster than in state 1. The present results are compared to stochastic simulations revealing the range of validity of the derived formulas. ",Arrival time for the fastest among $N$ switching stochastic particles
199,1472739395400728576,2679086784,Larry Han,"['Our new preprint, ""Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects"" is out! In our real data analysis, we compare outcomes for U.S. veterans who received Moderna vs. Pfizer vaccines. Read on to find out more! <LINK>']",https://arxiv.org/abs/2112.09313,"Federated learning of causal estimands may greatly improve estimation efficiency by leveraging data from multiple study sites, but robustness to heterogeneity and model mis-specifications is vital for ensuring validity. We develop a Federated Adaptive Causal Estimation (FACE) framework to incorporate heterogeneous data from multiple sites to provide treatment effect estimation and inference for a flexibly specified target population of interest. FACE accounts for site-level heterogeneity in the distribution of covariates through density ratio weighting. To safely incorporate source sites and avoid negative transfer, we introduce an adaptive weighting procedure via a penalized regression, which achieves both consistency and optimal efficiency. Our strategy is communication-efficient and privacy-preserving, allowing participating sites to only share summary statistics once with other sites. We conduct both theoretical and numerical evaluations of FACE, and apply it to conduct a comparative effectiveness study of BNT162b2 (Pfizer) and mRNA-1273 (Moderna) vaccines on COVID-19 outcomes in U.S. veterans using electronic health records from five VA regional sites. We show that compared to traditional methods, FACE meaningfully increases the precision of treatment effect estimates, with reductions in standard errors ranging from $26\%$ to $67\%$. ",Federated Adaptive Causal Estimation (FACE) of Target Treatment Effects
200,1471821226347839488,1116294572630519809,Ben Saunders,"['Our paper ""Skeletal Graph Self-Attention: Embedding a Skeleton Inductive Bias into Sign Language Production"" is available at <LINK>. We represent sign language sequences in a skeletal graph structure and propose a novel Skeletal Graph Self-Attention layer for SLP']",https://arxiv.org/abs/2112.05277,"Recent approaches to Sign Language Production (SLP) have adopted spoken language Neural Machine Translation (NMT) architectures, applied without sign-specific modifications. In addition, these works represent sign language as a sequence of skeleton pose vectors, projected to an abstract representation with no inherent skeletal structure. In this paper, we represent sign language sequences as a skeletal graph structure, with joints as nodes and both spatial and temporal connections as edges. To operate on this graphical structure, we propose Skeletal Graph Self-Attention (SGSA), a novel graphical attention layer that embeds a skeleton inductive bias into the SLP model. Retaining the skeletal feature representation throughout, we directly apply a spatio-temporal adjacency matrix into the self-attention formulation. This provides structure and context to each skeletal joint that is not possible when using a non-graphical abstract representation, enabling fluid and expressive sign language production. We evaluate our Skeletal Graph Self-Attention architecture on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset, achieving state-of-the-art back translation performance with an 8% and 7% improvement over competing methods for the dev and test sets. ","Skeletal Graph Self-Attention: Embedding a Skeleton Inductive Bias into
  Sign Language Production"
201,1470763114572439553,1315232054271934464,Giovanni Russo,"['How can we integrate data-driven and model-based control?  Find it out with the latest work by @FrancescoDeLel4 @coraggio_marco @mircomusolesi @mdiberna  and myself:\n\n<LINK>', 'In the paper we are proposing an architecture where a feedback controller derived on an approximate environment model tutors a learning algorithm.\n\nResults point out that, by doing so, we can improve data efficiency of the learning process!']",https://arxiv.org/abs/2112.06018,"We present an architecture where a feedback controller derived on an approximate model of the environment assists the learning process to enhance its data efficiency. This architecture, which we term as Control-Tutored Q-learning (CTQL), is presented in two alternative flavours. The former is based on defining the reward function so that a Boolean condition can be used to determine when the control tutor policy is adopted, while the latter, termed as probabilistic CTQL (pCTQL), is instead based on executing calls to the tutor with a certain probability during learning. Both approaches are validated, and thoroughly benchmarked against Q-Learning, by considering the stabilization of an inverted pendulum as defined in OpenAI Gym as a representative problem. ","Control-Tutored Reinforcement Learning: Towards the Integration of
  Data-Driven and Model-Based Control"
202,1469335570992271373,19149703,Karina Voggel ‚ú®üî≠üèÉüèº‚Äç‚ôÄÔ∏è,"['Today the paper of Antoine Dumont, one of the students I worked with in Utah is out! He did a spectroscopic study of GCs in Centaurus A, and we found that many have higher dynamical masses than expected, indicating that they are stripped nuclei. <LINK>', 'Antoine is also currently on the post-doc job market so if you come across his application he has been truly amazing to work with.', 'Sorry I linked straight to the pdf, here is the link to the arxiv landing page https://t.co/OUy81FV97C']",https://arxiv.org/abs/2112.04504,"The dense central regions of tidally disrupted galaxies can survive as ultra-compact dwarfs (UCDs) that hide among the luminous globular clusters (GCs) in the halo of massive galaxies. An exciting confirmation of this model is the detection of overmassive black holes in the centers of some UCDs, which also lead to elevated dynamical mass-to-light ratios ($M/L_{dyn}$). Here we present new high-resolution spectroscopic observations of 321 luminous GC candidates in the massive galaxy NGC 5128/Centaurus A. Using these data we confirm 27 new luminous GCs, and measure velocity dispersions for 57 luminous GCs (with $g$-band luminosities between $2.5 \times 10^5$ and $2.5 \times 10^7 L_{\odot}$), of which 48 are new measurements. Combining these data with size measurements from Gaia, we determine the $M/L_{dyn}$ for all 57 luminous GCs. We see a clear bimodality in the $M/L_{dyn}$ distribution, with a population of normal GCs with mean $M/L_{dyn}=1.51\pm0.31$, and a second population of $\sim$20 GCs with elevated mean $M/L_{dyn}=2.68\pm0.22$. We show that black holes with masses $\sim4$-18 % of the luminous GCs can explain the elevated mass-to-light ratios. Hence, it is plausible that the NGC 5128 sources with elevated $M/L_{dyn}$ are mostly stripped galaxy nuclei that contain massive central black holes, though future high spatial resolution observations are necessary to confirm this hypothesis for individual sources. We also present a detailed discussion of an extreme outlier, \textit{VHH81-01}, one of the largest and most massive GC in NGC 5128, making it an exceptionally strong candidate to be a tidally stripped nucleus. ","A population of luminous globular clusters and stripped nuclei with
  elevated mass to light ratios around NGC 5128"
203,1468679940895920136,186701821,Aldo Pacchiano,"['In this Neurips 2021 paper (<LINK>) we study a class of classification problems exemplified by the bank loan problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued. (1/n)', 'Thus modeled decisions affect the data available to the lender for future decisions. As a result, it is possible for the lender‚Äôs algorithm to ‚Äúget stuck‚Äù with a self-fulfilling model. (2/n)', 'This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating infinite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. (3/n)', 'However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present Pseudo- Label Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. (4/n)', 'PLOT adds optimistic pseudo-labels to the subset of decision points the current model is deciding on, trains the model on all data so far (including these new points along with their optimistic labels), and finally uses the resulting optimistic model for decision making. (5/n)', 'The PLOT decision boundary with and without pseudo-labels\n\nhttps://t.co/kYOJj3Lf6z\n \nPLOT achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. (6/n)', 'Thanks to all that came to our poster. We have also created a colab demo to experiment with multiple algorithms for the bank loan setting on a variety of public datasets (see https://t.co/Iu5gBKF2o1). (7/n)\n\nJoint work w/ @j_foerst , @alexandercberg,  @edwardjchou and Shaun Singh']",https://arxiv.org/abs/2112.02185,"We study a class of classification problems best exemplified by the \emph{bank loan} problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender's algorithm to ``get stuck'' with a self-fulfilling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating infinite regret. In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present Pseudo-Label Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. \PLOT{} adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and finally uses the resulting \emph{optimistic} model for decision making. \PLOT{} achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that \PLOT{} satisfies a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data. ",Neural Pseudo-Label Optimism for the Bank Loan Problem
204,1468241038804459526,1226456279457161217,Adam Gosztolai,"['With @ramdya we wrote a review on the use of network theory to study individual and collective animal behaviours. <LINK> (1/4)', 'We argue that network theory allows uniting the historically separate fields of neuroscience and collective animal behaviour to study information processing by networks on multiple levels (2/4) https://t.co/dLd9pk32J0', ""The key to this is the current surge of 'big data' which characterises the connectivity and the dynamics of neuronal and animal interaction networks. (3/4) https://t.co/cnuVyGH3rm"", 'We survey four hot topics in network theory - information flow, controllability, symmetry and geometry - and illustrate their uses in linking multimodal datasets and to further our understanding of the computations that underpin animal behaviour. (4/4) https://t.co/y9VL3RkCRj']",https://arxiv.org/abs/2112.02361,"A major goal shared by neuroscience and collective behavior is to understand how dynamic interactions between individual elements give rise to behaviors in populations of neurons and animals, respectively. This goal has recently become within reach thanks to techniques providing access to the connectivity and activity of neuronal ensembles as well as to behaviors among animal collectives. The next challenge using these datasets is to unravel network mechanisms generating population behaviors. This is aided by network theory, a field that studies structure-function relationships in interconnected systems. Here we review studies that have taken a network view on modern datasets to provide unique insights into individual and collective animal behaviors. Specifically, we focus on how analyzing signal propagation, controllability, symmetry, and geometry of networks can tame the complexity of collective system dynamics. These studies illustrate the potential of network theory to accelerate our understanding of behavior across ethological scales. ","Connecting the dots in ethology: applying network theory to understand
  neural and animal collectives"
