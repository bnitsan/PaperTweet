,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,784808045712924672,2401166844,Lars N. Andersen,['New paper out! <LINK>'],https://arxiv.org/abs/1609.09725,"We consider the general problem of estimating probabilities which arise as a union of dependent events. We propose a flexible series of estimators for such probabilities, and describe variance reduction schemes applied to the proposed estimators. We derive efficiency results of the estimators in rare-event settings, in particular those associated with extremes. Finally, we examine the performance of our estimators in a numerical example. ","Efficient simulation for dependent rare events with applications to
  extremes"
1,784274338820202500,3165225777,Arild Nøkland,"['New version of ""Direct Feedback Alignment"" paper is out. Thanks to Colin Raffel for feedback on theorem and proof <LINK>']",http://arxiv.org/abs/1609.01596,"Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error back-propagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45% error on the permutation invariant MNIST task. ",Direct Feedback Alignment Provides Learning in Deep Neural Networks
2,783570243117613056,3091000564,Piotr Bródka,"['Check our new paper: ""Sequential Seeding in Complex Networks: Trading Speed for Coverage"" <LINK> <LINK>']",https://arxiv.org/abs/1609.07526,"Information spreading in complex networks is often modeled as diffusing information with certain probability from nodes that possess it to their neighbors that do not. Information cascades are triggered when the activation of a set of initial nodes (seeds) results in diffusion to large number of nodes. Here, several novel approaches for seed initiation that replace the commonly used activation of all seeds at once with a sequence of initiation stages are introduced. Sequential strategies at later stages avoid seeding highly ranked nodes that are already activated by diffusion active between stages. The gain arises when a saved seed is allocated to a node difficult to reach via diffusion. Sequential seeding and a single stage approach are compared using various seed ranking methods and diffusion parameters on real complex networks. The experimental results indicate that, regardless of the seed ranking method used, sequential seeding strategies deliver better coverage than single stage seeding in about 90% of cases. Longer seeding sequences tend to activate more nodes but they also extend the duration of diffusion. Various variants of sequential seeding resolve the trade-off between the coverage and speed of diffusion differently. ",Balancing Speed and Coverage by Sequential Seeding in Complex Networks
3,782946679922327554,3199605543,Afonso S. Bandeira,['New paper on Resilience of the Littlewood-Offord Problem: <LINK>'],https://arxiv.org/abs/1609.08136,"Consider the sum $X(\xi)=\sum_{i=1}^n a_i\xi_i$, where $a=(a_i)_{i=1}^n$ is a sequence of non-zero reals and $\xi=(\xi_i)_{i=1}^n$ is a sequence of i.i.d. Rademacher random variables (that is, $\Pr[\xi_i=1]=\Pr[\xi_i=-1]=1/2$). The classical Littlewood-Offord problem asks for the best possible upper bound on the concentration probabilities $\Pr[X=x]$. In this paper we study a resilience version of the Littlewood-Offord problem: how many of the $\xi_i$ is an adversary typically allowed to change without being able to force concentration on a particular value? We solve this problem asymptotically, and present a few interesting open problems. ",Resilience for the Littlewood-Offord Problem
4,782239296417857541,3467184629,Alireza Valizadeh,['Our new paper on the #correlation of #oscillators when receiving time-shifted #common_inputs \n<LINK>'],https://arxiv.org/abs/1609.08320,"Shared upstream dynamical processes are frequently the source of common inputs in various physical and biological systems. However, due to finite signal transmission speeds and differences in the distance to the source, time shifts between otherwise common inputs are unavoidable. Since common inputs can be a source of correlation between the elements of multi-unit dynamical systems, regardless of whether these elements are directly connected with one another or not, it is of importance to understand their impact on synchronization. As a canonical model that is representative for a variety of different dynamical systems, we study limit-cycle oscillators that are driven by stochastic time-shifted common inputs. We show that if the oscillators are coupled, time shifts in stochastic common inputs do not simply shift the distribution of the phase differences, but rather the distribution actually changes as a result. The best synchronization is therefore achieved at a precise intermediate value of the time shift, which is due to a resonance-like effect with the most probable phase difference that is determined by the deterministic dynamics. ",Synchronization of oscillators through time-shifted common inputs
5,781837065495404544,260317629,Gordon Berman,['New paper by @UgkkU (w/ @shaevitz @David_L_Stern Jessica Cande &amp; me) on quantifying fly courtship dynamics <LINK>'],https://arxiv.org/abs/1609.09345,"Social behaviors involving the interaction of multiple individuals are complex and frequently crucial for an animal's survival. These interactions, ranging across sensory modalities, length scales, and time scales, are often subtle and difficult to quantify. Contextual effects on the frequency of behaviors become even more difficult to quantify when physical interaction between animals interferes with conventional data analysis, e.g. due to visual occlusion. We introduce a method for quantifying behavior in courting fruit flies that combines high-throughput video acquisition and tracking of individuals with recent unsupervised methods for capturing an animal's entire behavioral repertoire. We find behavioral differences in paired and solitary flies of both sexes, identifying specific behaviors that are affected by social and spatial context. Our pipeline allows for a comprehensive description of the interaction between multiple individuals using unsupervised machine learning methods, and will be used to answer questions about the depth of complexity and variance in fruit fly courtship. ","An Unsupervised Method for Quantifying the Behavior of Interacting
  Individuals"
6,781783779027324928,5620142,Edward Grefenstette 🇪🇺,['Check out @TomasKocisky’s new @DeepMindAI paper on Semantic Parsing with Semi-Supervised Sequential Autoencoders! <LINK>'],https://arxiv.org/abs/1609.09315,We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. ,Semantic Parsing with Semi-Supervised Sequential Autoencoders
7,781773304063483904,1601296094,David Bowler,"['Our new paper on electronic structure of silicon, and how thick a slab you need (thicker than you think…) <LINK>']",https://arxiv.org/abs/1609.09262,"We investigate the influence of slab thickness on the electronic structure of the Si(100)-p(2x2) surface in density functional theory (DFT) calculations, considering both density of states and band structure. Our calculations, with slab thicknesses of up to 78 atomic layers, reveal that the slab thickness profoundly affects the surface band structure, particularly the dangling bond states of the silicon dimers near the Fermi level. We find that, to precisely reproduce the surface bands, the slab thickness needs to be large enough to completely converge the bulk bands in the slab. In case of the Si(100) surface, the dispersion features of the surface bands, such as the band shape and width, converge when the slab thickness is larger than 30 layers. Complete convergence of both the surface and bulk bands in the slab is only achieved when the slab thickness is greater than 60 layers. ","Importance of bulk states for the electronic structure of semiconductor
  surfaces: implications for finite slabs"
8,781581375480729600,29405175,sylvain Gigan,['new paper from the group : machine learning with light <LINK> (collab. F. @KrzakalaF and G. Wainrib @Physique_ENS )'],https://arxiv.org/abs/1609.05204,"Echo-State Networks and Reservoir Computing have been studied for more than a decade. They provide a simpler yet powerful alternative to Recurrent Neural Networks, every internal weight is fixed and only the last linear layer is trained. They involve many multiplications by dense random matrices. Very large networks are difficult to obtain, as the complexity scales quadratically both in time and memory. Here, we present a novel optical implementation of Echo-State Networks using light-scattering media and a Digital Micromirror Device. As a proof of concept, binary networks have been successfully trained to predict the chaotic Mackey-Glass time series. This new method is fast, power efficient and easily scalable to very large networks. ",Scaling up Echo-State Networks with multiple light scattering
9,780854671149633536,1442906958,Richard Socher,['Great new paper by @Smerity: <LINK> Allows NLP models to predict unseen words from context+dataset: <LINK> <LINK>'],http://arxiv.org/abs/1609.07843,Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus. ,Pointer Sentinel Mixture Models
10,780827701217288193,88806960,Dr. Vivienne Baldassare,['New paper on arxiv about X-ray/UV properties of dwarf galaxies with active galactic nuclei: <LINK>'],http://arxiv.org/abs/1609.07148,"We present new Chandra X-ray Observatory and Hubble Space Telescope observations of eight optically selected broad-line AGN candidates in nearby dwarf galaxies ($z<0.055$). Including archival Chandra observations of three additional sources, our sample contains all ten galaxies from Reines et al. (2013) with both broad H$\alpha$ emission and narrow-line AGN ratios (6 AGNs, 4 Composites), as well as one low-metallicity dwarf galaxy with broad H$\alpha$ and narrow-line ratios characteristic of star formation. All eleven galaxies are detected in X-rays. Nuclear X-ray luminosities range from $L_{0.5-7 \rm{keV}}\approx5\times10^{39}$ to $1\times10^{42}$ $\rm{erg}\rm{s^{-1}}$. In all cases except for the star forming galaxy, the nuclear X-ray luminosities are significantly higher than would be expected from X-ray binaries, providing strong confirmation that AGN and composite dwarf galaxies do indeed host actively accreting BHs. Using our estimated BH masses (which range from $\sim7\times10^{4}-1\times10^{6}~M_{\odot}$), we find inferred Eddington fractions ranging from $\sim0.1-50\%$, i.e. comparable to massive broad-line quasars at higher redshift. We use the HST imaging to determine the ratio of ultraviolet to X-ray emission for these AGN, finding that they appear to be less X-ray luminous with respect to their UV emission than more massive quasars (i.e. $\alpha_{\rm OX}$ values an average of 0.36 lower than expected based on the relation between $\alpha_{\rm OX}$ and $2500{\rm \AA}$ luminosity). Finally, we discuss our results in the context of different accretion models onto nuclear BHs. ",X-ray and Ultraviolet Properties of AGN in Nearby Dwarf Galaxies
11,780778554468343808,238675940,Sara Imari Walker,"['New paper w/ Enrico Boriello on classifying the complexity of CAs with info. theory: <LINK>, some thoughts on biology too!']",http://arxiv.org/abs/1609.07554,"A novel, information-based classification of elementary cellular automata is proposed that circumvents the problems associated with isolating whether complexity is in fact intrinsic to a dynamical rule, or if it arises merely as a product of a complex initial state. Transfer entropy variations processed by the system split the 256 elementary rules into three information classes, based on sensitivity to initial conditions. These classes form a hierarchy such that coarse-graining transitions observed among elementary cellular automata rules predominately occur within each information- based class, or much more rarely, down the hierarchy. ",An information-based classification of Elementary Cellular Automata
12,780571916600041473,2562351306,Alexander Engelen,"['Sharpening our view of the early Universe with ""Delensing Beyond the B Modes"" - our new paper.  <LINK>', ""@ajvengelen Definitely the most controversial thing that you'll hear about tonight.""]",http://arxiv.org/abs/1609.08143,"Gravitational lensing by large-scale structure significantly impacts observations of the cosmic microwave background (CMB): it smooths the acoustic peaks in temperature and $E$-mode polarization power spectra, correlating previously uncorrelated modes; and it converts $E$-mode polarization into $B$-mode polarization. The act of measuring and removing the effect of lensing from CMB maps, or delensing, has been well studied in the context of $B$ modes, but little attention has been given to the delensing of the temperature and $E$ modes. In this paper, we model the expected delensed $T$ and $E$ power spectra to all orders in the lensing potential, demonstrating the sharpening of the acoustic peaks and a significant reduction in lens-induced power spectrum covariances. We then perform cosmological forecasts, demonstrating that delensing will yield improved sensitivity to parameters with upcoming surveys. We highlight the breaking of the degeneracy between the effective number of neutrino species and primordial helium fraction as a concrete application. We also show that delensing increases cosmological information as long as the measured lensing reconstruction is included in the analysis. We conclude that with future data, delensing will be crucial not only for primordial $B$-mode science but for a range of other observables as well. ",CMB Delensing Beyond the B Modes
13,780411288551845889,2303004390,Brandon Amos,"[""Our new paper 'Input Convex Neural Networks': <LINK> The code is also online at: <LINK> <LINK>""]",https://arxiv.org/abs/1609.07152,"This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases. ",Input Convex Neural Networks
14,780313551961464832,568787781,Dr Tim Walton (d²=0),"['Good start to week: latest paper on arXiv, outlining a new compact gravitational pulse solution &amp; its implications <LINK>']",http://arxiv.org/abs/1609.07322,"It has been suggested that single and double jets observed emanating from certain astrophysical objects may have a purely gravitational origin. We discuss new classes of plane-fronted and pulsed gravitational wave solutions to the equation for perturbations of Ricci-flat spacetimes around Minkowski metrics, as models for the genesis of such phenomena. These solutions are classified in terms of their chirality and generate a family of non-stationary spacetime metrics. Particular members of these families are used as backgrounds in analysing time-like solutions to the geodesic equation for test particles. They are found numerically to exhibit both single and double jet-like features with dimensionless aspect ratios suggesting that it may be profitable to include such backgrounds in simulations of astrophysical jet dynamics from rotating accretion discs involving electromagnetic fields. ",On Gravitational Chirality as the Genesis of Astrophysical Jets
15,779672575685386240,779636032476176385,Alan McKenzie,"['Just added a new paper <LINK> to <LINK> called *A discrete, finite multiverse* <LINK>']",http://arxiv.org/abs/1609.04050,"The Many Worlds Interpretation (MWI) famously avoids the issue of wave function collapse. Different MWI trees representing the same quantum events can have different topologies, depending upon the observer. However, they are all isomorphic to the group of block universes containing all of the outcomes of all of the events, and so, in that sense, the group of block universes is a more fundamental representation. Different branches of the MWI tree, representing different universes in MWI, ultimately share the same quantum state in a common ancestor branch. This branching topology is incompatible with that of the Minkowski block universe; the resolution is to replace the branches with discrete, parallel block universes, each of which extends from the trunk to the outermost twigs. The number of universes in a branch is proportional to its thickness which, in turn, depends upon the absolute square of the probability amplitude for the state in that branch. Every quantum event may be represented by a kernel of universes, which is the smallest group of universes that will reproduce the quantum probabilities of the outcomes of that event. By considering the ratios of the probabilities of the outcomes of any event, it can be shown that the number of universes in every kernel must finite, as must be the total number of universes in the multiverse. Further, every universe in the multiverse must be finite both in space and time. Another consequence is that quantum probabilities must be rational, which suggests that quantum mechanics is only an approximation to a discrete theory. A corollary is that not every conceivable universe exists in the multiverse, no doubt to the disappointment of those who enjoy alternate-history novels. ","A discrete, finite multiverse"
16,779310525318516736,2235411914,Surya Ganguli,['Our new #NIPS2016 paper: doing Bayesian integrals in high dimensions by solving an optimization problem <LINK> #AI #BigData'],http://arxiv.org/abs/1609.07060,"When recovering an unknown signal from noisy measurements, the computational difficulty of performing optimal Bayesian MMSE (minimum mean squared error) inference often necessitates the use of maximum a posteriori (MAP) inference, a special case of regularized M-estimation, as a surrogate. However, MAP is suboptimal in high dimensions, when the number of unknown signal components is similar to the number of measurements. In this work we demonstrate, when the signal distribution and the likelihood function associated with the noise are both log-concave, that optimal MMSE performance is asymptotically achievable via another M-estimation procedure. This procedure involves minimizing convex loss and regularizer functions that are nonlinearly smoothed versions of the widely applied MAP optimization problem. Our findings provide a new heuristic derivation and interpretation for recent optimal M-estimators found in the setting of linear measurements and additive noise, and further extend these results to nonlinear measurements with non-additive noise. We numerically demonstrate superior performance of our optimal M-estimators relative to MAP. Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration underlying MMSE inference, and high dimensional convex optimization underlying M-estimation. In essence we show that the former difficult integral may be computed by solving the latter, simpler optimization problem. ","An equivalence between high dimensional Bayes optimal inference and
  M-estimation"
17,779223487932198912,1702174146,Janis Keuper,['My new paper:  <LINK> #deeplearning #scaling <LINK>'],http://arxiv.org/abs/1609.06870,"This paper presents a theoretical analysis and practical evaluation of the main bottlenecks towards a scalable distributed solution for the training of Deep Neuronal Networks (DNNs). The presented results show, that the current state of the art approach, using data-parallelized Stochastic Gradient Descent (SGD), is quickly turning into a vastly communication bound problem. In addition, we present simple but fixed theoretic constraints, preventing effective scaling of DNN training beyond only a few dozen nodes. This leads to poor scalability of DNN training in most practical scenarios. ","Distributed Training of Deep Neural Networks: Theoretical and Practical
  Limits of Parallel Scalability"
18,778878921668456448,555239997,Mario Livio,"['New paper: ""Why Are Pulsar Planets Rare?"" <LINK>']",http://arxiv.org/abs/1609.06409,"Pulsar timing observations have revealed planets around only a few pulsars. We suggest that the rarity of these planets is due mainly to two effects. First, we show that the most likely formation mechanism requires the destruction of a companion star. Only pulsars with a suitable companion (with an extreme mass ratio) are able to form planets. Second, while a dead zone (a region of low turbulence) in the disk is generally thought to be essential for planet formation, it is most probably rare in disks around pulsars because of the irradiation from the pulsar. The irradiation strongly heats the inner parts of the disk pushing the inner boundary of the dead zone out. We suggest that the rarity of pulsar planets can be explained by the low probability for these two requirements - a very low-mass companion and a dead zone - to be satisfied. ",Why are pulsar planets rare?
19,778471989337915393,2203468841,Dr Jade Powell,['My new paper about glitches in @LIGO :) <LINK>'],http://arxiv.org/abs/1609.06262,"The data taken by the advanced LIGO and Virgo gravitational-wave detectors contains short duration noise transients that limit the significance of astrophysical detections and reduce the duty cycle of the instruments. As the advanced detectors are reaching sensitivity levels that allow for multiple detections of astrophysical gravitational-wave sources it is crucial to achieve a fast and accurate characterization of non-astrophysical transient noise shortly after it occurs in the detectors. Previously we presented three methods for the classification of transient noise sources. They are Principal Component Analysis for Transients (PCAT), Principal Component LALInference Burst (PC-LIB) and Wavelet Detection Filter with Machine Learning (WDF-ML). In this study we carry out the first performance tests of these algorithms on gravitational-wave data from the Advanced LIGO detectors. We use the data taken between the 3rd of June 2015 and the 14th of June 2015 during the 7th engineering run (ER7), and outline the improvements made to increase the performance and lower the latency of the algorithms on real data. This work provides an important test for understanding the performance of these methods on real, non stationary data in preparation for the second advanced gravitational-wave detector observation run, planned for later this year. We show that all methods can classify transients in non stationary data with a high level of accuracy and show the benefits of using multiple classifiers. ","Classification methods for noise transients in advanced
  gravitational-wave detectors II: performance tests on Advanced LIGO data"
20,778427050252050432,252867237,Juan Miguel Arrazola,"['New paper on a new topic! Watch out quantum thermodynamics, here we come!\n<LINK> <LINK>']",http://arxiv.org/abs/1609.06011,"The triumph of heat engines is their ability to convert the disordered energy of thermal sources into useful mechanical motion. In recent years, much effort has been devoted to generalizing thermodynamic notions to the quantum regime, partly motivated by the promise of surpassing classical heat engines. Here, we instead adopt a bottom-up approach: we propose a realistic autonomous heat engine that can serve as a testbed for quantum effects in the context of thermodynamics. Our model draws inspiration from actual piston engines and is built from closed-system Hamiltonians and weak bath coupling terms. We analytically derive the performance of the engine in the classical regime via a set of nonlinear Langevin equations. In the quantum case, we perform numerical simulations of the master equation. Finally, we perform a dynamic and thermodynamic analysis of the engine's behaviour for several parameter regimes in both the classical and quantum case, and find that the latter exhibits a consistently lower efficiency due to additional noise. ",Autonomous Rotor Heat Engine
21,778392566697910272,3216845588,Gijs Mulders,['New paper out!\nCombining @NASAKepler and #LAMOST data:\nA Super-Solar Metallicity For Stars With Hot Rocky Exoplanets\n<LINK> <LINK>'],http://arxiv.org/abs/1609.05898,"The host star metallicity provide a measure of the conditions in protoplanetary disks at the time of planet formation. Using a sample of over 20,000 Kepler stars with spectroscopic metallicities from the LAMOST survey, we explore how the exoplanet population depends on host star metallicity as a function of orbital period and planet size. We find that exoplanets with orbital periods less than 10 days are preferentially found around metal-rich stars ([Fe/H]~ 0.15 +- 0.05 dex). The occurrence rates of these hot exoplanets increases to ~30% for super-solar metallicity stars from ~10% for stars with a sub-solar metallicity. Cooler exoplanets, that resides at longer orbital periods and constitute the bulk of the exoplanet population with an occurrence rate of >~ 90%, have host-star metallicities consistent with solar. At short orbital periods, P<10 days, the difference in host star metallicity is largest for hot rocky planets (<1.7 R_Earth), where the metallicity difference is [Fe/H] =~ 0.25 +- 0.07 dex. The excess of hot rocky planets around metal-rich stars implies they either share a formation mechanism with hot Jupiters, or trace a planet trap at the protoplanetary disk inner edge which is metallicity-dependent. We do not find statistically significant evidence for a previously identified trend that small planets toward the habitable zone are preferentially found around low-metallicity stars. Refuting or confirming this trend requires a larger sample of spectroscopic metallicities. ",A Super-Solar Metallicity For Stars With Hot Rocky Exoplanets
22,778129046676865024,355616314,Murray Shanahan,"['New paper with Marta Garnelo and @KaiLashArul ""Towards deep symbolic reinforcement learning"" #AI - <LINK>']",https://arxiv.org/abs/1609.05518,"Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game. ",Towards Deep Symbolic Reinforcement Learning
23,778032918992130048,36396172,Diego R. Amancio,"['Our new preprint paper is out! ""Patterns of authors contribution in papers"" <LINK> Sup. Info.: <LINK>']",http://arxiv.org/abs/1609.05545,"Science is becoming increasingly more interdisciplinary, giving rise to more diversity in the areas of expertise within research labs and groups. This also have brought changes to the role researchers in scientific works. As a consequence, multi-authored scientific papers have now became a norm for high quality research. Unfortunately, such a phenomenon induces bias to existing metrics employed to evaluate the productivity and success of researchers. While some metrics were adapted to account for the rank of authors in a paper, many journals are now requiring a description of the specific roles of each author in a publication. Surprisingly, the investigation of the relationship between the rank of authors and their contributions has been limited to a few studies. By analyzing such kind of data, here we show, quantitatively, that the regularity in the authorship contributions decreases with the number of authors in a paper. Furthermore, we found that the rank of authors and their roles in papers follows three general patterns according to the nature of their contributions, such as writing, data analysis, and the conduction of experiments. This was accomplished by collecting and analyzing the data retrieved from PLoS ONE and by devising an entropy-based measurement to quantify the effective number of authors in a paper according to their contributions. The analysis of such patterns confirms that some aspects of the author ranking are in accordance with the expected convention, such as the fact that the first and last authors are more likely to contribute more in a scientific work. Conversely, such analysis also revealed that authors in the intermediary positions of the rank contribute more in certain specific roles, such as the task of collecting data. This indicates that the an unbiased evaluation of researchers must take into account the distinct types of scientific contributions. ",Patterns of authors contribution in scientific manuscripts
24,776692099307737088,1601296094,David Bowler,"['Our new paper on charge-density waves in TiSe2, and resilience to Ti interstitials #2Dmaterials #TMDC <LINK>']",http://arxiv.org/abs/1609.04164,"In Ti-intercalated self-doped $1T$-TiSe$_2$ crystals, the charge density wave (CDW) superstructure induces two nonequivalent sites for Ti dopants. Recently, it has been shown that increasing Ti doping dramatically influences the CDW by breaking it into phase-shifted domains. Here, we report scanning tunneling microscopy and spectroscopy experiments that reveal a dopant-site dependence of the CDW gap. Supported by density functional theory, we demonstrate that the loss of the longrange phase coherence introduces an imbalance in the intercalated-Ti site distribution and restrains the CDW gap closure. This local resilient behavior of the $1T$-TiSe$_2$ CDW reveals a novel mechanism between CDW and defects in mutual influence. ","Local resilience of the $1T$-TiSe$_2$ charge density wave to Ti
  self-doping"
25,776614919311687680,403778734,Zehan Wang,"['Just released a new paper on using GANs for super resolution! Very proud of our team! #magicpony #TwitterCortex\n<LINK>', ""Also for most of us its our first arXiv submission. Ever! We're now one of *those* people. Nice one @LedigChr @trustswz and team!""]",https://arxiv.org/abs/1609.04802,"Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method. ","Photo-Realistic Single Image Super-Resolution Using a Generative
  Adversarial Network"
26,775673610354692096,16174436,Sune Lehmann,"['super short summary of our new paper or arXiv: ""It\'s kind of like the Dunbar number - but for geospatial behavior"" <LINK>']",http://arxiv.org/abs/1609.03526,"Recent seminal works on human mobility have shown that individuals constantly exploit a small set of repeatedly visited locations. A concurrent literature has emphasized the explorative nature of human behavior, showing that the number of visited places grows steadily over time. How to reconcile these seemingly contradicting facts remains an open question. Here, we analyze high-resolution multi-year traces of $\sim$40,000 individuals from 4 datasets and show that this tension vanishes when the long-term evolution of mobility patterns is considered. We reveal that mobility patterns evolve significantly yet smoothly, and that the number of familiar locations an individual visits at any point is a conserved quantity with a typical size of $\sim$25 locations. We use this finding to improve state-of-the-art modeling of human mobility. Furthermore, shifting the attention from aggregated quantities to individual behavior, we show that the size of an individual's set of preferred locations correlates with the number of her social interactions. This result suggests a connection between the conserved quantity we identify, which as we show can not be understood purely on the basis of time constraints, and the `Dunbar number' describing a cognitive upper limit to an individual's number of social relations. We anticipate that our work will spark further research linking the study of Human Mobility and the Cognitive and Behavioral Sciences. ",Evidence for a Conserved Quantity in Human Mobility
27,774174414728421376,494870213,Thomas Haworth,['My new paper on externally irradiated protoplanetary discs (the birthplaces of planets). <LINK>'],http://arxiv.org/abs/1609.02153,"There is growing theoretical and observational evidence that protoplanetary disc evolution may be significantly affected by the canonical levels of far ultraviolet (FUV) radiation found in a star forming environment, leading to substantial stripping of material from the disc outer edge even in the absence of nearby massive stars. In this paper we perform the first full radiation hydrodynamic simulations of the flow from the outer rim of protoplanetary discs externally irradiated by such intermediate strength FUV fields, including direct modelling of the photon dominated region (PDR) which is required to accurately compute the thermal properties. We find excellent agreement between our models and the semi-analytic models of Facchini et al. (2016) for the profile of the flow itself, as well as the mass loss rate and location of their ""critical radius"". This both validates their results (which differed significantly from prior semi-analytic estimates) and our new numerical method, the latter of which can now be applied to elements of the problem that the semi--analytic approaches are incapable of modelling. We also obtain the composition of the flow, but given the simple geometry of our models we can only hint at some diagnostics for future observations of externally irradiated discs at this stage. We also discuss the potential for these models as benchmarks for future photochemical-dynamical codes. ","Photochemical-dynamical models of externally FUV irradiated
  protoplanetary discs"
28,774083404887273472,1576235694,Michael Brown,['Jake Crossett (@MonashUni) has a new paper on environment driven quenching in galaxy groups. <LINK> <LINK>'],http://arxiv.org/abs/1609.02311,"We have investigated the effect of group environment on residual star formation in galaxies, using Galex NUV galaxy photometry with the SDSS group catalogue of Yang et al. (2007). We compared the (NUV $- r$) colours of grouped and non-grouped galaxies, and find a significant increase in the fraction of red sequence galaxies with blue (NUV $- r$) colours outside of groups. When comparing galaxies in mass matched samples of satellite (non-central), and non-grouped galaxies, we found a > 4{\sigma} difference in the distribution of (NUV $- r$) colours, and an (NUV $- r$) blue fraction $> 3{\sigma}$ higher outside groups. A comparison of satellite and non-grouped samples has found the NUV fraction is a factor of $\sim2$ lower for satellite galaxies between $10^{10.5}M_{\bigodot}$ and $10^{10.7}M_{\bigodot}$, showing that higher mass galaxies are more able to form stars when not influenced by a group potential. There was a higher (NUV $- r$) blue fraction of galaxies with lower Sersic indices (n < 3) outside of groups, not seen in the satellite sample. We have used stellar population models of Bruzual & Charlot (2003) with multiple burst, or exponentially declining star formation histories to find that many of the (NUV $- r$) blue non-grouped galaxies can be explained by a slow ($\sim 2$ Gyr) decay of star formation, compared to the satellite galaxies. We suggest that taken together, the difference in (NUV $- r$) colours between samples can be explained by a population of secularly evolving, non-grouped galaxies, where star formation declines slowly. This slow channel is less prevalent in group environments where more rapid quenching can occur. ",NUV signatures of environment driven galaxy quenching in SDSS groups
29,773692923657408512,2272134908,Claudia Lagos,"[""New paper today! This time I'm going deep into angular momentum of galaxies:\n<LINK> <LINK>"", '@caastro_arc Sure! Short story: galaxies have a range of angular momenta and we identified two channels decreasing it: mergers and quenching']",http://arxiv.org/abs/1609.01739,"We use the EAGLE cosmological hydrodynamic simulation suite to study the specific angular momentum of galaxies, $j$, with the aims of (i) investigating the physical causes behind the wide range of $j$ at fixed mass and (ii) examining whether simple, theoretical models can explain the seemingly complex and non-linear nature of the evolution of $j$. We find that $j$ of the stars, $j_{\rm stars}$, and baryons, $j_{\rm bar}$, are strongly correlated with stellar and baryon mass, respectively, with the scatter being highly correlated with morphological proxies such as gas fraction, stellar concentration, (u-r) intrinsic colour, stellar age and the ratio of circular velocity to velocity dispersion. We compare with available observations at $z=0$ and find excellent agreement. We find that $j_{\rm bar}$ follows the theoretical expectation of an isothermal collapsing halo under conservation of specific angular momentum to within $\approx 50$%, while the subsample of rotation-supported galaxies are equally well described by a simple model in which the disk angular momentum is just enough to maintain marginally stable disks. We extracted evolutionary tracks of the stellar spin parameter of EAGLE galaxies and found that the fate of their $j_{\rm stars}$ at $z=0$ depends sensitively on their star formation and merger histories. From these tracks, we identified two distinct physical channels behind low $j_{\rm stars}$ galaxies at $z=0$: (i) galaxy mergers, and (ii) early star formation quenching. The latter can produce galaxies with low $j_{\rm stars}$ and early-type morphologies even in the absence of mergers. ",Angular momentum evolution of galaxies in EAGLE
30,773545382743797761,120814510,Haroldo V. Ribeiro,['Our new paper on discriminating image textures via entropic measures was accepted at Chaos Solitons Fractals. arXiv: <LINK>'],http://arxiv.org/abs/1609.01625,"The aim of this paper is to further explore the usefulness of the two-dimensional complexity-entropy causality plane as a texture image descriptor. A multiscale generalization is introduced in order to distinguish between different roughness features of images at small and large spatial scales. Numerically generated two-dimensional structures are initially considered for illustrating basic concepts in a controlled framework. Then, more realistic situations are studied. Obtained results allow us to confirm that intrinsic spatial correlations of images are successfully unveiled by implementing this multiscale symbolic information-theory approach. Consequently, we conclude that the proposed representation space is a versatile and practical tool for identifying, characterizing and discriminating image textures. ","Discriminating image textures with the multiscale two-dimensional
  complexity-entropy causality plane"
31,773497586162565120,48712353,Sungjin Ahn 🇺🇦,"['Our new paper: Let the RNN find the latent *hierarchical temporal* structure! ""Hierarchical Multiscale RNN"" <LINK>']",http://arxiv.org/abs/1609.01704,"Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling. ",Hierarchical Multiscale Recurrent Neural Networks
32,772847241459556352,106843613,Jacob Haqq Misra,['New co-authored paper led by @NatashaBatalha: were the martian valleys caused by freeze-thaw climate cycling?\n<LINK>'],https://arxiv.org/abs/1609.00602,"For decades, scientists have tried to explain the evidence for fluvial activity on early Mars, but a consensus has yet to emerge regarding the mechanism for producing it. One hypothesis suggests early Mars was warmed by a thick greenhouse atmosphere. Another suggests that early Mars was generally cold but was warmed occasionally by impacts or by episodes of enhanced volcanism. These latter hypotheses struggle to produce the amounts of rainfall needed to form the martian valleys, but are consistent with inferred low rates of weathering compared to Earth. Here, we provide a geophysical mechanism that could have induced cycles of glaciation and deglaciation on early Mars. Our model produces dramatic climate cycles with extended periods of glaciation punctuated by warm periods lasting up to 10 Myr, much longer than those generated in other episodic warming models. The cycles occur because stellar insolation was low, and because CO2 outgassing is not able to keep pace with CO2 consumption by silicate weathering followed by deposition of carbonates. While CO2 by itself is not able to deglaciate early Mars in our model, we assume that the greenhouse effect is enhanced by substantial amounts of H2 outgassed from Mars' reduced crust and mantle. Our hypothesis can be tested by future Mars exploration that better establishes the time scale for valley formation. ",Climate Cycling on Early Mars Caused by the Carbonate-Silicate Cycle
33,771620139808595970,22399655,Ryota Kanai💡,"['Our new paper on Neural Coarse-Graining. This automatically finds coarse grained, predictive features   <LINK>']",http://arxiv.org/abs/1609.00116,"We present a loss function for neural networks that encompasses an idea of trivial versus non-trivial predictions, such that the network jointly determines its own prediction goals and learns to satisfy them. This permits the network to choose sub-sets of a problem which are most amenable to its abilities to focus on solving, while discarding 'distracting' elements that interfere with its learning. To do this, the network first transforms the raw data into a higher-level categorical representation, and then trains a predictor from that new time series to its future. To prevent a trivial solution of mapping the signal to zero, we introduce a measure of non-triviality via a contrast between the prediction error of the learned model with a naive model of the overall signal statistics. The transform can learn to discard uninformative and unpredictable components of the signal in favor of the features which are both highly predictive and highly predictable. This creates a coarse-grained model of the time-series dynamics, focusing on predicting the slowly varying latent parameters which control the statistics of the time-series, rather than predicting the fast details directly. The result is a semi-supervised algorithm which is capable of extracting latent parameters, segmenting sections of time-series with differing statistics, and building a higher-level representation of the underlying dynamics from unlabeled data. ","Neural Coarse-Graining: Extracting slowly-varying latent degrees of
  freedom with neural networks"
34,771592212488392705,1710697381,Diego F. Torres,"['New paper (A&amp;A):\nThe 2015 outburst of the accreting MSP IGR J17511–3057 as seen by INTEGRAL, Swift and XMM. <LINK>']",https://arxiv.org/abs/1609.00187,"We report on INTEGRAL, Swift and XMM-Newton observations of IGR J17511-3057 performed during the outburst that occurred between March 23 and April 25, 2015. The source reached a peak flux of 0.7(2)E-9 erg/cm$^2$/s and decayed to quiescence in approximately a month. The X-ray spectrum was dominated by a power-law with photon index between 1.6 and 1.8, which we interpreted as thermal Comptonization in an electron cloud with temperature > 20 keV . A broad ({\sigma} ~ 1 keV) emission line was detected at an energy (E = 6.9$^{+0.2}_{-0.3}$ keV) compatible with the K{\alpha} transition of ionized Fe, suggesting an origin in the inner regions of the accretion disk. The outburst flux and spectral properties shown during this outburst were remarkably similar to those observed during the previous accretion event detected from the source in 2009. Coherent pulsations at the pulsar spin period were detected in the XMM-Newton and INTEGRAL data, at a frequency compatible with the value observed in 2009. Assuming that the source spun up during the 2015 outburst at the same rate observed during the previous outburst, we derive a conservative upper limit on the spin down rate during quiescence of 3.5E-15 Hz/s. Interpreting this value in terms of electromagnetic spin down yields an upper limit of 3.6E26 G/cm$^3$ to the pulsar magnetic dipole (assuming a magnetic inclination angle of 30{\deg}). We also report on the detection of five type-I X-ray bursts (three in the XMM-Newton data, two in the INTEGRAL data), none of which indicated photospheric radius expansion. ","The 2015 outburst of the accreting millisecond pulsar IGR J17511-3057 as
  seen by INTEGRAL, Swift and XMM-Newton"
35,771590977299488772,16434310,chrislintott,"['A new @galaxyzoo paper out today, led by the amazing @becky1505. This one was hard-fought! <LINK>']",http://arxiv.org/abs/1609.00023,"We present a population study of the star formation history of 1244 Type 2 AGN host galaxies, compared to 6107 inactive galaxies. A Bayesian method is used to determine individual galaxy star formation histories, which are then collated to visualise the distribution for quenching and quenched galaxies within each population. We find evidence for some of the Type 2 AGN host galaxies having undergone a rapid drop in their star formation rate within the last 2 Gyr. AGN feedback is therefore important at least for this population of galaxies. This result is not seen for the quenching and quenched inactive galaxies whose star formation histories are dominated by the effects of downsizing at earlier epochs, a secondary effect for the AGN host galaxies. We show that histories of rapid quenching cannot account fully for the quenching of all the star formation in a galaxy's lifetime across the population of quenched AGN host galaxies, and that histories of slower quenching, attributed to secular (non-violent) evolution, are also key in their evolution. This is in agreement with recent results showing both merger-driven and non-merger processes are contributing to the co-evolution of galaxies and supermassive black holes. The availability of gas in the reservoirs of a galaxy, and its ability to be replenished, appear to be the key drivers behind this co-evolution. ","Galaxy Zoo: Evidence for rapid, recent quenching within a population of
  AGN host galaxies"
36,781784347699507200,21400651,Karl Moritz Hermann,"['Our new paper on semi-supervised autoencoders for sequences: <LINK>', '@karlmoritz I was told to use hash-tags. Apparently I am doing this wrong. #deepmind @deepmindai #autoencoder #emnlp2016 #derplearning']",https://arxiv.org/abs/1609.09315,We present a novel semi-supervised approach for sequence transduction and apply it to semantic parsing. The unsupervised component is based on a generative model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. ,Semantic Parsing with Semi-Supervised Sequential Autoencoders
37,778519801555775488,49533606,Dr Emily Drabek-Maunder 🏳️‍🌈🏳️‍⚧️🇺🇸🇬🇧🇪🇺,"['My new paper on the protoplanetary disk LkCa 15 is out! <LINK> #postdoclife #longtimecoming #astronomy', '.@r_d_alexander Maybe! CO is opt thick in cav, HCO+ is thin. IMO, more molecular modelling is needed since HCO+/CO chem network is related.', '.@r_d_alexander I agree! I would just also like to reconcile the disk models derived from diff molecules.', '.@r_d_alexander HCO+ results still hold - it definitely seems like there has to be higher surf dens in the cav to produce HCO+ line-wings.']",https://arxiv.org/abs/1609.05894,"LkCa 15 is an extensively studied star in the Taurus region known for its pre-transitional disk with a large inner cavity in dust continuum and normal gas accretion rate. The most popular hypothesis to explain the LkCa 15 data invokes one or more planets to carve out the inner cavity, while gas continues to flow across the gap from the outer disk onto the central star. We present spatially unresolved HCO+ J=4-3 observations of the LkCa 15 disk from the JCMT and model the data with the ProDiMo code. We find that: (1) HCO+ line-wings are clearly detected, certifying the presence of gas in the cavity within <50 AU of the star. (2) Reproducing the observed line-wing flux requires both a significant suppression of cavity dust (by a factor >10^4 compared to the ISM) and a substantial increase in the gas scale-height within the cavity (H_0/R_0 ~ 0.6). An ISM dust-to-gas ratio (d:g=10^-2) yields too little line-wing flux regardless of the scale-height or cavity gas geometry, while a smaller scale-height also under predicts the flux even with a reduced d:g. (3) The cavity gas mass is consistent with the surface density profile of the outer disk extended inwards to the sublimation radius (corresponding to mass M_d ~ 0.03 M_sun), and masses lower by a factor >10 appear to be ruled out. ","HCO+ Detection of Dust-Depleted Gas in the Inner Hole of the LkCa 15
  Pre-Transitional Disk"
38,775618602191945728,297986128,Alyssa Drake,['New paper on the Lya LF on arxiv today: <LINK>'],https://arxiv.org/abs/1609.02920,"We present the first estimate of the Ly{\alpha} luminosity function using blind spectroscopy from the Multi Unit Spectroscopic Explorer, MUSE, in the Hubble Deep Field South. Using automatic source-detection software, we assemble a homogeneously-detected sample of 59 Ly{\alpha} emitters covering a flux range of -18.0 < log10 (F) < -16.3 (erg s^-1 cm^-2), corresponding to luminosities of 41.4 < log10 (L) < 42.8 (erg s^-1). As recent studies have shown, Ly{\alpha} fluxes can be underestimated by a factor of two or more via traditional methods, and so we undertake a careful assessment of each object's Ly{\alpha} flux using a curve-of-growth analysis to account for extended emission. We describe our self-consistent method for determining the completeness of the sample, and present an estimate of the global Ly{\alpha} luminosity function between redshifts 2.91 < z < 6.64 using the 1/Vmax estimator. We find the luminosity function is higher than many number densities reported in the literature by a factor of 2 - 3, although our result is consistent at the 1{\sigma} level with most of these studies. Our observed luminosity function is also in good agreement with predictions from semi-analytic models, and shows no evidence for strong evolution between the high- and low-redshift halves of the data. We demonstrate that one's approach to Ly{\alpha} flux estimation does alter the observed luminosity function, and caution that accurate flux assessments will be crucial in measurements of the faint end slope. This is a pilot study for the Ly{\alpha} luminosity function in the MUSE deep-fields, to be built on with data from the Hubble Ultra Deep Field which will increase the size of our sample by almost a factor of 10. ","MUSE Deep-Fields: The Lya Luminosity Function in the Hubble Deep Field
  South at 2.91 &lt; z &lt; 6.64"
39,773205141017153536,3238448948,Carl T. Bergstrom,['Our new paper models the effect of publication bias on the construction of scientific facts <LINK> <LINK>'],https://arxiv.org/abs/1609.00494,"In the process of scientific inquiry, certain claims accumulate enough support to be established as facts. Unfortunately, not every claim accorded the status of fact turns out to be true. In this paper, we model the dynamic process by which claims are canonized as fact through repeated experimental confirmation. The community's confidence in a claim constitutes a Markov process: each successive published result shifts the degree of belief, until sufficient evidence accumulates to accept the claim as fact or to reject it as false. In our model, publication bias --- in which positive results are published preferentially over negative ones --- influences the distribution of published results. We find that when readers do not know the degree of publication bias and thus cannot condition on it, false claims often can be canonized as facts. Unless a sufficient fraction of negative results are published, the scientific process will do a poor job at discriminating false from true claims. This problem is exacerbated when scientists engage in p-hacking, data dredging, and other behaviors that increase the rate at which false positives are published. If negative results become easier to publish as a claim approaches acceptance as a fact, however, true and false claims can be more readily distinguished. To the degree that the model accurately represents current scholarly practice, there will be serious concern about the validity of purported facts in some areas of scientific research. ",Publication bias and the canonization of false facts
