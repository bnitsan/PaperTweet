,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1314619038736187392,1185338621785604096,Shirley A. Hayati,"[""Our #EMNLP2020 paper introduces INSPIRED, a new dialog dataset for sociable recommendation. \n\nWhen humans recommend to their friends, it's a combination of chit-chat, persuasion, questions, and answers. INSPIRED contains these cases. \n\n<LINK> <LINK>"", 'We studied the association of the sociable strategies and successfulness of a recommendation and found that sociable strategies help 😉.\n\nOur dialog system 🤖 with these strategies outperform the baseline transformer model in both automatic metrics and human evaluations. 🥳🥳🥳 https://t.co/3vk3YLVIUT', 'Github 💻: https://t.co/rRjEwYIWfL']",https://arxiv.org/abs/2009.14306,"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories. ",INSPIRED: Toward Sociable Recommendation Dialog Systems
1,1313587138349408263,550979881,Klim Zaporojets,"['We are very excited to introduce DWIE: a new document-level entity-centric Information Extraction dataset that covers the following tasks: NER, Coreference Resolution, Relation Extraction, and Entity Linking.\n\nPaper: <LINK>\nGitHub: <LINK>\n\n[1/7]', 'DWIE is conceived as an entity-centric dataset that describes (both explicit and implicit) interactions and properties of conceptual entities on the level of the complete document.  \n[2/7] https://t.co/hSzY3P3NMT', ""The DWIE's document-level annotation approach allows to capture relations between entities whose mentions are located further apart compared to other similar datasets. \n[3/7] https://t.co/5P4GfB5hAr"", 'We introduce a new ""Soft Entity-Level"" evaluation metric for the NER and Relation Extraction tasks, in line with the entity-centric nature of DWIE. It allows for the measurements to not be dominated by predictions on more frequently mentioned entities.\n[4/7]', 'We present an end-to-end neural graph-based architecture that allows document-level message passing between mention spans. Our results on DWIE show a consistent improvement when using different neural graph propagation techniques. \n[5/7]', 'Thanks to all the amazing colleagues and collaborators @ugentnlp, Johannes Deleu, @thomeestr, Chris Develder for this exciting project! \n[6/7]', 'And if you have read this far: I am looking for collaborations to apply for a PhD research stay. If you are interested, please drop me a line! Preference for Denmark/EU. \n[7/7]']",https://arxiv.org/abs/2009.12626,"This paper presents DWIE, the 'Deutsche Welle corpus for Information Extraction', a newly created multi-task dataset that combines four main Information Extraction (IE) annotation subtasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document. This contrasts with currently dominant mention-driven approaches that start from the detection and classification of named entity mentions in individual sentences. Further, DWIE presented two main challenges when building and evaluating IE models for it. First, the use of traditional mention-level evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can result in measurements dominated by predictions on more frequently mentioned entities. We tackle this issue by proposing a new entity-driven metric that takes into account the number of mentions that compose each of the predicted and ground truth entities. Second, the document-level multi-task annotations require the models to transfer information between entity mentions located in different parts of the document, as well as between different tasks, in a joint learning setting. To realize this, we propose to use graph-based neural message passing techniques between document-level mention spans. Our experiments show an improvement of up to 5.5 F1 percentage points when incorporating neural graph propagation into our joint model. This demonstrates DWIE's potential to stimulate further research in graph neural networks for representation learning in multi-task IE. We make DWIE publicly available at this https URL ","DWIE: an entity-centric dataset for multi-task document-level
  information extraction"
2,1313227228155711495,101810581,Animesh Garg,"['Our new paper on hierarchical framework that combines model-based control and RL for learning robust quadruped controllers\n\nPaper: <LINK>\nVideo: <LINK>\n\nX. Da @zhaomingxie @HoellerDavid B. Boots @AnimaAnandkumar @yukez @BuckBabich at  @NVIDIAAI <LINK> <LINK>', 'This work got a primetime mention in #JensenHuang Keynote at @nvidia #GTC20 this year. \n\nWatch it here: https://t.co/jNQYlenCNJ https://t.co/B0j97rBAY4']",https://arxiv.org/abs/2009.10019,"We present a hierarchical framework that combines model-based control and reinforcement learning (RL) to synthesize robust controllers for a quadruped (the Unitree Laikago). The system consists of a high-level controller that learns to choose from a set of primitives in response to changes in the environment and a low-level controller that utilizes an established control method to robustly execute the primitives. Our framework learns a controller that can adapt to challenging environmental changes on the fly, including novel scenarios not seen during training. The learned controller is up to 85~percent more energy efficient and is more robust compared to baseline methods. We also deploy the controller on a physical robot without any randomization or adaptation scheme. ","Learning a Contact-Adaptive Controller for Robust, Efficient Legged
  Locomotion"
3,1312709885499305985,897971748,barnabe.eth,"['Our new paper, Data-Driven Models of Selfish Routing, was accepted for #wine2020! It follows work on routing games started during my PhD that grew to become a combination of experimental and theoretical results <LINK>', 'It starts from an observation we made in a previous paper (Routing games in the wild, also chapter 5 of my thesis here: https://t.co/fXfsq7V7px) where we bounded empirically the price of anarchy of a real routing system, Singapore, via a large-scale data collection experiment', ""Price of anarchy is a widely studied measure of system performance, comparing system congestion between a centralised setting where agents follow a benevolent dictator's orders and a decentralised setting where agents optimise for themselves. More here! https://t.co/14mWz4z8Nt"", 'Our data looked at the routing system, namely the travel time of students going to school in the morning. The bound we determined empirically was quite a bit lower from the well-known theoretical worst case bound. Is there something particular about real world routing systems?', 'Turns out there is, and the reason is very ""micro"". We make the assumption that agents rule out routes that are just too unreasonable (for instance, you wouldn\'t go through Kuala Lumpur to reach the other side of Singapore). Commuters don\'t have infinite knowledge after all! https://t.co/UTJkmAtoS0', 'When agents only consider routes that are almost as good as the ""fastest"" route (details in the paper), it rules out the really bad routing networks that give rise to the theoretical worst case bounds (eg Pigou network), and we get more realistic bounds https://t.co/gUZpNBRUcY', 'The model our team in Singapore derived from the data matched work done by another team in Italy, who discovered independently the same, with much deeper theoretical results. We joined forces to combine their insights with our data and produced this paper (Fin)']",https://arxiv.org/abs/2009.12871,"We investigate traffic routing both from the perspective of theory as well as real world data. First, we introduce a new type of games: $\theta$-free flow games. Here, commuters only consider, in their strategy sets, paths whose free-flow costs (informally their lengths) are within a small multiplicative $(1+\theta)$ constant of the optimal free-flow cost path connecting their source and destination, where $\theta\geq0$. We provide an exhaustive analysis of tight bounds on PoA($\theta$) for arbitrary classes of cost functions, both in the case of general congestion/routing games as well as in the special case of path-disjoint networks. Second, by using a large mobility dataset in Singapore, we inspect minute-by-minute decision-making of thousands of commuters, and find that $\theta=1$ is a good estimate of agents' route (pre)selection mechanism. In contrast, in Pigou networks, the ratio of the free-flow costs of the routes, and thus $\theta$, is \textit{infinite}; so, although such worst case networks are mathematically simple, they correspond to artificial routing scenarios with little resemblance to real world conditions, opening the possibility of proving much stronger Price of Anarchy guarantees by explicitly studying their dependency on $\theta$. For example, in the case of the standard Bureau of Public Roads (BPR) cost model, where$c_e(x)= a_e x^4+b_e$, and for quartic cost functions in general, the standard PoA bound for $\theta=\infty$ is $2.1505$, and this is tight both for general networks as well as path-disjoint and even parallel-edge networks. In comparison, for $\theta=1$, the PoA in the case of general networks is only $1.6994$, whereas for path-disjoint/parallel-edge networks is even smaller ($1.3652$), showing that both the route geometries as captured by the parameter $\theta$ as well as the network topology have significant effects on PoA. ","Data-Driven Models of Selfish Routing: Why Price of Anarchy Does Depend
  on Network Topology"
4,1312511129013297154,3245312065,Xuezhe Ma (Max),"['Check our new optimizer, which outperforms SGD and variants of Adam on both convergence speed and generalization.\npaper: <LINK>\ncode: <LINK>\n\nMy last work done @LTIatCMU ?\n@USC_ISI @USCViterbi @CSatUSC <LINK>', 'For image classification (1st pic), we used ResNet-110 on CIFAR-10 and ResNeXt-50 on ImageNet.\nFor language modeling (2nd pic and 1st table), we used two-layer LSTM on one billion words.\nFor NMT (2nd table), we trained Transformer-base models on WMT-14 EN-DE.']",https://arxiv.org/abs/2009.13586,"In this paper, we introduce Apollo, a quasi-Newton method for nonconvex stochastic optimization, which dynamically incorporates the curvature of the loss function by approximating the Hessian via a diagonal matrix. Importantly, the update and storage of the diagonal approximation of Hessian is as efficient as adaptive first-order optimization methods with linear complexity for both time and memory. To handle nonconvexity, we replace the Hessian with its rectified absolute value, which is guaranteed to be positive-definite. Experiments on three tasks of vision and language show that Apollo achieves significant improvements over other stochastic optimization methods, including SGD and variants of Adam, in term of both convergence speed and generalization performance. The implementation of the algorithm is available at this https URL ","Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for
  Nonconvex Stochastic Optimization"
5,1311739566777348098,1002538331039846400,Nikolaos Syrrakos,['Interested in particle physics calculations relevant to scattering processes at the LHC? Check out our new paper! <LINK>\n#Amplitudes #particlephysics'],http://arxiv.org/abs/2009.13917,"We present analytic expressions in terms of polylogarithmic functions for all three families of planar two-loop five-point Master Integrals with one off-shell leg. The calculation is based on the Simplified Differential Equations approach. The results are relevant to the study of many $2\to 3$ scattering processes of interest at the LHC, especially for the leading-color $W+2$ jets production. ","Analytic representation of all planar two-loop five-point Master
  Integrals with one off-shell leg"
6,1311669325577752576,17239073,Atabey Kaygun,"['My new math paper on the arXiv \\o/ ""Birational Equivalences and Generalized Weyl Algebras"" <LINK>']",https://arxiv.org/abs/2009.14801,We calculate suitably localized Hochschild homologies of various quantum groups and Podle\'s spheres after realizing them as generalized Weyl algebras (GWAs). We use the fact that every GWA is birationally equivalent to a smash product with a 1-torus. We also address and solve the birational equivalence problem and the birational smoothness problem for GWAs. ,Birational Equivalences and Generalized Weyl Algebras
7,1311622087396732928,17819190,Vaishak Belle,['New working paper: Principles and Practice of #Explainable #Machine #Learning --  <LINK>'],https://arxiv.org/abs/2009.11698,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods -- machine learning (ML) and pattern recognition models in particular -- so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature, or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. ",Principles and Practice of Explainable Machine Learning
8,1311594375588327426,76615317,Rich Bielby,"['In other news... Yay! New paper out today, Dutta et al with new insights into the CGM from the MAGG survey. Boom. <LINK>']",https://arxiv.org/abs/2009.14219,"We present a study of the metal-enriched cool halo gas traced by MgII absorption around 228 galaxies at z~0.8-1.5 within 28 quasar fields from the MUSE Analysis of Gas around Galaxies (MAGG) survey. We observe no significant evolution in the MgII equivalent width versus impact parameter relation and in the MgII covering fraction compared to surveys at z<~0.5. The stellar mass, along with distance from galaxy centre, appears to be the dominant factor influencing the MgII absorption around galaxies. With a sample that is 90% complete down to a star formation rate of ~0.1 Msun/yr and up to impact parameters ~250-350 kpc from quasars, we find that the majority (67^{+12}_{-15}% or 14/21) of the MgII absorption systems are associated with more than one galaxy. The complex distribution of metals in these richer environments adds substantial scatter to previously-reported correlations. Multiple galaxy associations show on average five times stronger absorption and three times higher covering fraction within twice the virial radius than isolated galaxies. The dependence of MgII absorption on galaxy properties disfavours the scenario in which a widespread intra-group medium dominates the observed absorption. This leaves instead gravitational interactions among group members or hydrodynamic interactions of the galaxy haloes with the intra-group medium as favoured mechanisms to explain the observed enhancement in the MgII absorption strength and cross section in rich environments. ","MUSE Analysis of Gas around Galaxies (MAGG) -- II: Metal-enriched halo
  gas around z~1 galaxies"
9,1311470502838333440,326843207,Yuta Notsu,"['Our new paper is accepted !!\n  \n""Time-resolved spectroscopy and photometry of an M dwarf flare star YZ Canis Minoris with OISTER and TESS: Blue asymmetry in Hα line during the non-white light flare""\n\nMaehara, Notsu, Namekata, Honda, Kowalski, et al. \n<LINK>', 'We present the results from spectroscopic and photometric observations of the \nM-type flare star YZ CMi during the Transiting Exoplanet Survey Satellite (TESS) observation period. \n\n3 flares are detected, and one of them shows “blue asymmetry” of H-alpha line. https://t.co/4H3un4cCJa', 'Blue asymmetries can be used for discussing stellar mass ejections.\n\nThe estimated mass is comparable to expectations from the empirical relation between the flare X-ray energy and mass of upward-moving material for stellar flares and solar CMEs. https://t.co/MfJy08VSfp', 'In contrast, the estimated kinetic energy is roughly 2 orders of magnitude smaller than that expected from the relation between flare X-ray energy and kinetic energy for solar CMEs. This could be understood by the difference in the velocity between CMEs and prominence eruptions.', 'Also discussions on flare frequency, duration, and rotation from TESS data https://t.co/ppgjFB2qHF']",https://arxiv.org/abs/2009.14412,"In this paper, we present the results from spectroscopic and photometric observations of the M-type flare star YZ CMi in the framework of the Optical and Infrared Synergetic Telescopes for Education and Research (OISTER) collaborations during the Transiting Exoplanet Survey Satellite (TESS) observation period. We detected 145 white-light flares from the TESS light curve and 4 H$\alpha$ flares from the OISTER observations performed between 2019-01-16 and 2019-01-18. Among them, 3 H$\alpha$ flares were associated with white-light flares. However, one of them did not show clear brightening in continuum; during this flare, the H$\alpha$ line exhibited blue-asymmetry which has lasted for $\sim 60$ min. The line of sight velocity of the blue-shifted component is $-80$ - $-100$ km s$^{-1}$. This suggests that there can be upward flows of chromospheric cool plasma even without detectable red/NIR continuum brightening. By assuming that the blue-asymmetry in H$\alpha$ line was caused by a prominence eruption on YZ CMi, we estimated the mass and kinetic energy of the upward-moving material to be $10^{16}$ - $10^{18}$ g and $10^{29.5}$ - $10^{31.5}$ erg, respectively. The estimated mass is comparable to expectations from the empirical relation between the flare X-ray energy and mass of upward-moving material for stellar flares and solar CMEs. In contrast, the estimated kinetic energy for the non-white-light flare on YZ CMi is roughly $2$ orders of magnitude smaller than that expected from the relation between flare X-ray energy and kinetic energy for solar CMEs. This could be understood by the difference in the velocity between CMEs and prominence eruptions. ","Time-resolved spectroscopy and photometry of an M dwarf flare star YZ
  Canis Minoris with OISTER and TESS: Blue asymmetry in H$\alpha$ line during
  the non-white light flare"
10,1311466572528017410,746440524052082688,Nicolas Delfosse,"['You like the surface code. But would you like a better performance using less qubits? Check our new paper with Matt Hastings: <LINK>.', '@jfitzsimons @kenbrownquantum So if I well understand you want to use the [[2,1,1]] code with stabilizers ZZ. That could help for correction of X errors since dX=2 but not for correction of Z errors because dZ=1. I guess that could help in a biased setting though.', ""@jfitzsimons @kenbrownquantum Thanks for clarifying, that's interesting. Do you think that the definition of the homological product can be extended to this code? It would suffice to have a description of your singlet code as a chain complex but it's not obvious to me."", ""@jfitzsimons @kenbrownquantum It would be interesting to explore further I don't know if such a code could benefit for the same advantages of the homological product but I like your idea of starting from a smaller erasure code""]",https://arxiv.org/abs/2009.14226,"Homological product codes are a class of codes that can have improved distance while retaining relatively low stabilizer weight. We show how to build union-find decoders for these codes, using a union-find decoder for one of the codes in the product and a brute force decoder for the other code. We apply this construction to the specific case of the product of a surface code with a small code such as a $[[4,2,2]]$ code, which we call an augmented surface code. The distance of the augmented surface code is the product of the distance of the surface code with that of the small code, and the union-find decoder, with slight modifications, can decode errors up to half the distance. We present numerical simulations, showing that while the threshold of these augmented codes is lower than that of the surface code, the low noise performance is improved. ",Union-Find Decoders For Homological Product Codes
11,1311344157898821634,902085689495261185,Yann Dubois,"['New paper: we characterize optimal representations for supervised learning, and show how to ~learn them! Our framework gives 1) a regularizer and 2) a predictor of generalization in DL.\n\n<LINK> (NeurIPS spotlight)\nwith @douwekiela @davidjschwab @Rama_vedantam\n1/7 <LINK>', ""The idea: if the desired classifiers CANNOT distinguish examples with the same label (V-minimality) then they won't distinguish train/test examples and thus must generalize. From there, you only need to ensure good train performance (V-sufficiency) to be optimal. \n2/7 https://t.co/bOqNDaZ5Pi"", 'Typically sup. rep. are learned using ERM or Information Bottleneck (IB). But:\nERM: does not favor generalization\nIB: is agnostic to the classifier used\n\nDecodable IB takes into account the clf to ensure:\n- best achievable test performance\n- PAC estimation guarantees\n3/7 https://t.co/YNuAW0eXBY', 'Empirically, DIB learns representations that usually obtain good performance of downstream classifiers. Average- and worst-case neg. log likelihood on CIFAR10 (caveats in paper):\n4/7 https://t.co/6Czt3oSrPr', 'The degree of optimality can also be used to ~predict generalization in DL, i.e., take the output of any layer in a trained network and ask whether or not the rest of the network can predict random labels from it. This is often highly correlated with generalization: \n5/7 https://t.co/0giHtx0qdf', 'Open problems:\n- can we extend DIB to self-supervised learning ?\n- can we derive generalization bounds for approx. DIB ?\n- does SGD naturally enforce V-minimality ?\n6/7', 'I’m really grateful for all the feedback and encouragements I received from amazing people: @brandondamos @StefanoErmon @StephaneDeny @MathieuEmile @arimorcos @mnick @j_foerst @cjmaddison @roydanroy @DavidDuvenaud @RickyTQChen @ylecun @JaviAC7 @AmartyaSanyal\n7/7', '@_lychrel Code will be available around the 22nd of October. For domain adaptation DIB would look a little similar to DANN (Ganin et al.), the key difference being that the architecture of the domain classifier needs to be the same as the label predictor.', ""@_lychrel I'd have to think more about it, but it seems like a promising idea !"", '@BlackHC @brandondamos @StefanoErmon @StephaneDeny @MathieuEmile @arimorcos @mnick @j_foerst @cjmaddison @roydanroy @DavidDuvenaud @RickyTQChen @ylecun @JaviAC7 @AmartyaSanyal Thanks!\nYou can replace H_V[Y] with H[Y] if Y is discrete (with a small additional assumption). Check footnote 4.\n\nThat might make it a little confusing if you know V-information but hopefully simpler for other people. Good catch though ...', '@BlackHC @brandondamos @StefanoErmon @StephaneDeny @MathieuEmile @arimorcos @mnick @j_foerst @cjmaddison @roydanroy @DavidDuvenaud @RickyTQChen @ylecun @JaviAC7 @AmartyaSanyal (and to be clear in practice it makes no difference regardless of assumptions because it is a constant w.r.t. the representation)']",https://arxiv.org/abs/2009.12789,"We address the question of characterizing and finding optimal representations for supervised learning. Traditionally, this question has been tackled using the Information Bottleneck, which compresses the inputs while retaining information about the targets, in a decoder-agnostic fashion. In machine learning, however, our goal is not compression but rather generalization, which is intimately linked to the predictive family or decoder of interest (e.g. linear classifier). We propose the Decodable Information Bottleneck (DIB) that considers information retention and compression from the perspective of the desired predictive family. As a result, DIB gives rise to representations that are optimal in terms of expected test performance and can be estimated with guarantees. Empirically, we show that the framework can be used to enforce a small generalization gap on downstream classifiers and to predict the generalization ability of neural networks. ","Learning Optimal Representations with the Decodable Information
  Bottleneck"
12,1311328604756860931,966760075074461697,Brian Thomas,"['Our new paper is out on the arXiv today, revisiting the question of the threat posed by gamma-ray bursts in light of recent detections of TeV photons - check it out!\n<LINK>', ""@XimenaAbrevaya There's a couple of recent ones on supernovae in case you missed them:\nhttps://t.co/f2RBXbXR5V\nhttps://t.co/rCWs4wUL7p""]",http://arxiv.org/abs/2009.14078,"We analyze the additional effect on planetary atmospheres of recently detected gamma-ray burst afterglow photons in the range up to 1 TeV. For an Earth-like atmosphere we find that there is a small additional depletion in ozone versus that modeled for only prompt emission. We also find a small enhancement of muon flux at the planet surface. Overall, we conclude that the additional afterglow emission, even with TeV photons, does not result in a significantly larger impact over that found in past studies. ",Gamma Ray Bursts: Not so Much Deadlier than We Thought
13,1311269642921533441,104207361,Amos Folarin,"['Our new Paper (preprint) from @RADARCNS: ""Relationship between Major #Depression Symptom Severity and Sleep"" using &gt;2-Years of #Fitbit data and &gt;2k PHQ8 scores from  n=368 participants \n👉<LINK>\n@yuezhou_zhang, @richdobson, @MatthewHotopf @phidatalab @radar_base <LINK>']",https://arxiv.org/abs/2009.12983,"Research in mental health has implicated sleep pathologies with depression. However, the gold standard for sleep assessment, polysomnography, is not suitable for long-term, continuous, monitoring of daily sleep, and methods such as sleep diaries rely on subjective recall, which is qualitative and inaccurate. Wearable devices, on the other hand, provide a low-cost and convenient means to monitor sleep in home settings. The main aim of this study was to devise and extract sleep features, from data collected using a wearable device, and analyse their correlation with depressive symptom severity and sleep quality, as measured by the self-assessed Patient Health Questionnaire 8-item. Daily sleep data were collected passively by Fitbit wristband devices, and depressive symptom severity was self-reported every two weeks by the PHQ-8. The data used in this paper included 2,812 PHQ-8 records from 368 participants recruited from three study sites in the Netherlands, Spain, and the UK.We extracted 21 sleep features from Fitbit data which describe sleep in the following five aspects: sleep architecture, sleep stability, sleep quality, insomnia, and hypersomnia. Linear mixed regression models were used to explore associations between sleep features and depressive symptom severity. The z-test was used to evaluate the significance of the coefficient of each feature. We tested our models on the entire dataset and individually on the data of three different study sites. We identified 16 sleep features that were significantly correlated with the PHQ-8 score on the entire dataset. Associations between sleep features and the PHQ-8 score varied across different sites, possibly due to the difference in the populations. ","The Relationship between Major Depression Symptom Severity and Sleep
  Collected Using a Wristband Wearable Device: Multi-centre Longitudinal
  Observational Study"
14,1311213429609431041,1150364933542109184,Vihang Patil,['We mined a diamond in the minecraft  ObtainDiamond environment using our new method. \nPaper link: <LINK> <LINK>'],https://arxiv.org/abs/2009.14108,"Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks can often be hierarchically decomposed into sub-tasks. A step in the Q-function can be associated with solving a sub-task, where the expectation of the return increases. RUDDER has been introduced to identify these steps and then redistribute reward to them, thus immediately giving reward if sub-tasks are solved. Since the problem of delayed rewards is mitigated, learning is considerably sped up. However, for complex tasks, current exploration strategies as deployed in RUDDER struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Typically the number of demonstrations is small and RUDDER's LSTM model as a deep learning method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER's safe exploration and lessons replay buffer. Second, we replace RUDDER's LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations as known from bioinformatics. Align-RUDDER inherits the concept of reward redistribution, which considerably reduces the delay of rewards, thus speeding up learning. Align-RUDDER outperforms competitors on complex artificial tasks with delayed reward and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently. Github: this https URL, YouTube: this https URL ",Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution
15,1311211047332843520,1030511102,Marius-Constantin Dinu,['Check out our new paper Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution <LINK>\n\nVideo: <LINK> \nBlog: <LINK>\n\n@mrkhof @wehungpatil @jbrandi6 @SirJAM_Armedi \n@IARAInews @EnliteAi'],https://arxiv.org/abs/2009.14108,"Reinforcement Learning algorithms require a large number of samples to solve complex tasks with sparse and delayed rewards. Complex tasks can often be hierarchically decomposed into sub-tasks. A step in the Q-function can be associated with solving a sub-task, where the expectation of the return increases. RUDDER has been introduced to identify these steps and then redistribute reward to them, thus immediately giving reward if sub-tasks are solved. Since the problem of delayed rewards is mitigated, learning is considerably sped up. However, for complex tasks, current exploration strategies as deployed in RUDDER struggle with discovering episodes with high rewards. Therefore, we assume that episodes with high rewards are given as demonstrations and do not have to be discovered by exploration. Typically the number of demonstrations is small and RUDDER's LSTM model as a deep learning method does not learn well. Hence, we introduce Align-RUDDER, which is RUDDER with two major modifications. First, Align-RUDDER assumes that episodes with high rewards are given as demonstrations, replacing RUDDER's safe exploration and lessons replay buffer. Second, we replace RUDDER's LSTM model by a profile model that is obtained from multiple sequence alignment of demonstrations. Profile models can be constructed from as few as two demonstrations as known from bioinformatics. Align-RUDDER inherits the concept of reward redistribution, which considerably reduces the delay of rewards, thus speeding up learning. Align-RUDDER outperforms competitors on complex artificial tasks with delayed reward and few demonstrations. On the MineCraft ObtainDiamond task, Align-RUDDER is able to mine a diamond, though not frequently. Github: this https URL, YouTube: this https URL ",Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution
16,1311201665538600963,3175429519,John Gargalionis,"['New paper on the arXiv today with @RVolkas at the intersection of automated model building, the physics of neutrino mass, and patterns of deviation from the SM:\n\nExploding operators for Majorana neutrino masses and beyond 💣💥 <LINK>\n\n👇', ""We describe an algorithm for automated model building from effective operators and use it to generate minimal models of Majorana neutrino mass. We're also publishing our example code and the database of ~500,000 Lagrangians\n\nhttps://t.co/XQ9HT5Jkw5"", 'The algorithm generates lepton-number violating models from operators of odd mass-dimension in the SM EFT. In short, a series of rewrite rules resolves the contact interaction by replacing combinations of fields with Lorentz and SM irreps. Here\'s a branch in the ""completion tree"" https://t.co/O4uu3XmPbr', 'The models violate L by 2 units and the ν mass comes about through the dim-5 Weinberg operator, usually generated through loops. These models can be very predictive, and oscillation data puts an upper bound (dots) on the NP scale (Λ), shown here for models by number of fields. https://t.co/0RTLbkwYUR', 'As an example, the model shown below is one of only five that we identify with ≤3 fields living in the range [0.7, 100] TeV. In this case the NP must be below 10 TeV for the model to accommodate the atmospheric Δm². https://t.co/v7yVcISjSC', 'The zoo of models and fields generated in this way is interesting more broadly: it gives us a sense of the patterns that underlie possible tree-level deviation from the SM, and perhaps which operator coefficients are more likely to display observable deviations.', 'Under our assumptions, we show that it is impossible for many models and certain operators to contribute dominantly to L violation by two units, ruling them out as playing a dominant role in the neutrino-mass generation.', 'Considering only the ~11,000 models that we identify could contribute dominantly to the neutrino masses, we find that leptoquarks are the most common exotic scalars introduced, while vectorlike quarks are the most common fermions. https://t.co/np0EOHMoLz', 'There is an intricate web of connections (edges) between fields (nodes) that appear together in the models, and there is much still to understand and explore. The graph is a visual representation of this. (Only a handful of nodes are labelled.) https://t.co/4nda6fIviC', 'We have made our database of models publicly available. Finding a good way of working with a database of symbolic values in a general way has been difficult, so currently the main interface is through our example code.\n\nhttps://t.co/5UEAG0iUud', ""Our hope is that through theoretical arguments, oscillation data, and automated pheno we can rule out a large number of the models. Those remaining that are testable should be studied. If they're falsified, we build a stronger circumstantial case for the models that can't be.""]",http://arxiv.org/abs/2009.13537,"Building UV completions of lepton-number-violating effective operators has proved to be a useful way of studying and classifying models of Majorana neutrino mass. In this paper we describe and implement an algorithm that systematises this model-building procedure. We use the algorithm to generate computational representations of all of the tree-level completions of the operators up to and including mass-dimension 11. Almost all of these correspond to models of radiative neutrino mass. Our work includes operators involving derivatives, updated estimates for the bounds on the new-physics scale associated with each operator, an analysis of various features of the models, and a look at some examples. We find that a number of operators do not admit any completions not also generating lower-dimensional operators or larger contributions to the neutrino mass, ruling them out as playing a dominant role in the neutrino-mass generation. Additionally, we show that there are at most five models containing three or fewer exotic multiplets that predict new physics that must lie below 100 TeV. Accompanying this work we also make available a searchable database containing all of our results and the code used to find the completions. We emphasise that our methods extend beyond the study of neutrino-mass models, and may be useful for generating completions of high-dimensional operators in other effective field theories. ",Exploding operators for Majorana neutrino masses and beyond
17,1311138912274993152,2541941466,Alba Cervera-Lierta,"['New paper out! We present the Meta-VQE, an algorithm that learns the ground state energy profile of a parametrized Hamiltonian. Check it out 👇\n<LINK>\n<LINK>\n@JakobKottmann @A_Aspuru_Guzik #matterlab @chemuoft @UofTCompSci @VectorInst @CIFAR_News', 'I will publish a thread about it in a few hours 😃', '@gpassosgomes @JakobKottmann @A_Aspuru_Guzik @chemuoft @UofTCompSci @VectorInst @CIFAR_News Thanks! Great week for #matterlab! 😃']",https://arxiv.org/abs/2009.13545,"We present the meta-VQE, an algorithm capable to learn the ground state energy profile of a parametrized Hamiltonian. By training the meta-VQE with a few data points, it delivers an initial circuit parametrization that can be used to compute the ground state energy of any parametrization of the Hamiltonian within a certain trust region. We test this algorithm with a XXZ spin chain, an electronic H$_{4}$ Hamiltonian and a single-transmon quantum simulation. In all cases, the meta-VQE is able to learn the shape of the energy functional and, in some cases, resulted in improved accuracy in comparison to individual VQE optimization. The meta-VQE algorithm introduces both a gain in efficiency for parametrized Hamiltonians, in terms of the number of optimizations, and a good starting point for the quantum circuit parameters for individual optimizations. The proposed algorithm proposal can be readily mixed with other improvements in the field of variational algorithms to shorten the distance between the current state-of-the-art and applications with quantum advantage. ","The Meta-Variational Quantum Eigensolver (Meta-VQE): Learning energy
  profiles of parameterized Hamiltonians for quantum simulation"
18,1311121966686973952,109255123,Danny Caballero 🇲🇽,['Check out this new paper by our @PERLatMSU team written by Nils Mikkelsen and @NickYoungPER on the factors that predict cutoff scores on the Physics GRE. <LINK> #GRExit'],http://arxiv.org/abs/2009.14027,"Despite limiting access to applicants from underrepresented racial and ethnic groups, the practice of using hard or soft GRE cut-off scores in physics graduate program admissions is still a popular method for reducing the pool of applicants. The present study considers whether the undergraduate institutions of applicants have any influence on the admissions process by modelling a physics GRE cut-off score with application data from admissions offices of five universities. Two distinct approaches based on inferential and predictive modelling are conducted. While there is some disagreement regarding the relative importance between features, the two approaches largely agree that including institutional information significantly aids the analysis. Both models identify cases where the institutional effects are comparable to factors of known importance such as gender and undergraduate GPA. As the results are stable across many cut-off scores, we advocate against the practice of employing physics GRE cut-off scores in admissions. ","Investigating institutional influence on graduate program admissions by
  modelling physics GRE cut-off scores"
19,1311114599421407234,727177363344105474,Q. Liu | 刘启民,"['NEW PAPER  accepted at @SpringerNature Computer Science! Co-authored with Dr. Fang Liu, “Selective Cascade of Residual ExtraTrees” (SCORE) is a machine learning algorithm we developed based on neural networks and decision trees. See post-print @ <LINK> <LINK>']",https://arxiv.org/abs/2009.14138,"We propose a novel tree-based ensemble method named Selective Cascade of Residual ExtraTrees (SCORE). SCORE draws inspiration from representation learning, incorporates regularized regression with variable selection features, and utilizes boosting to improve prediction and reduce generalization errors. We also develop a variable importance measure to increase the explainability of SCORE. Our computer experiments show that SCORE provides comparable or superior performance in prediction against ExtraTrees, random forest, gradient boosting machine, and neural networks; and the proposed variable importance measure for SCORE is comparable to studied benchmark methods. Finally, the predictive performance of SCORE remains stable across hyper-parameter values, suggesting potential robustness to hyperparameter specification. ",Selective Cascade of Residual ExtraTrees
20,1311109507334852608,1178921691252023296,Takahiro IINO (飯野孝浩),"['Our new paper appears in arXiv! \n""A belt-like distribution of gaseous hydrogen cyanide on Neptune\'s equatorial stratosphere detected by ALMA""\nTakahiro Iino, Hideo Sagawa, Takashi Tsukagoshi, Satonori Nozawa\n<LINK>']",https://arxiv.org/abs/2009.14072,"We present a spatially resolved map of integrated-intensity and abundance of Neptune's stratospheric hydrogen cyanide (HCN). The analyzed data were obtained from the archived 2016 observation of the Atacama Large Millimeter/submillimeter Array. A 0.42 $\times$ 0.39 arcseconds synthesized beam, which is equivalent to a latitudinal resolution of $\sim$20 degrees at the disk center, was fine enough to resolve Neptune's 2.24 arcseconds diameter disk. After correcting the effect of different optical path lengths, a spatial distribution of HCN emissions is derived over Neptune's disk, and it clearly shows a band-like HCN enhancement at the equator. Radiative transfer analysis indicates that the HCN volume mixing ratio measured at the equator was 1.92 ppb above the 10$^{-3}$ bar pressure level, which is 40$\%$ higher than that measured at the southern middle and high latitudes. The spatial distribution of HCN can be interpreted as either the effect of the transportation of N$_{2}$ from the troposphere by meridional atmospheric circulation, or an external supply such as cometary collisions (or both of these reasons). From the meridional circulation point of view, the observed HCN enhancement on both the equator and the pole can be explained by the production and accumulation of HCN at the downward branches of the previously suggested two-cell meridional circulation models. However, the HCN-depleted latitude of 60 S does not match with the location of the upward branch of the two-cell circulation models. ","A belt-like distribution of gaseous hydrogen cyanide on Neptune's
  equatorial stratosphere detected by ALMA"
21,1311068545720016897,1062160582369959936,ewin,"['new paper, in which I learn that SGD is good: <LINK> with András Gilyén and @realZhaoSong', 'held off on tweeting it until now because i noticed after pushing to arxiv that the paper had a bug. thankfully it was not too sick but I took some time to nurse it back to health; hopefully should be all fine now']",http://arxiv.org/abs/2009.07268,"We give a classical algorithm for linear regression analogous to the quantum matrix inversion algorithm [Harrow, Hassidim, and Lloyd, Physical Review Letters'09, arXiv:0811.3171] for low-rank matrices [Wossnig, Zhao, and Prakash, Physical Review Letters'18, arXiv:1704.06174], when the input matrix $A$ is stored in a data structure applicable for QRAM-based state preparation. Namely, suppose we are given an $A \in \mathbb{C}^{m\times n}$ with minimum non-zero singular value $\sigma$ which supports certain efficient $\ell_2$-norm importance sampling queries, along with a $b \in \mathbb{C}^m$. Then, for some $x \in \mathbb{C}^n$ satisfying $\|x - A^+b\| \leq \varepsilon\|A^+b\|$, we can output a measurement of $|x\rangle$ in the computational basis and output an entry of $x$ with classical algorithms that run in $\tilde{\mathcal{O}}\big(\frac{\|A\|_{\mathrm{F}}^6\|A\|^6}{\sigma^{12}\varepsilon^4}\big)$ and $\tilde{\mathcal{O}}\big(\frac{\|A\|_{\mathrm{F}}^6\|A\|^2}{\sigma^8\varepsilon^4}\big)$ time, respectively. This improves on previous ""quantum-inspired"" algorithms in this line of research by at least a factor of $\frac{\|A\|^{16}}{\sigma^{16}\varepsilon^2}$ [Chia, Gily\'en, Li, Lin, Tang and Wang, STOC'20, arXiv:1910.06151]. As a consequence, we show that quantum computers can achieve at most a factor-of-12 speedup for linear regression in this QRAM data structure setting and related settings. Our work applies techniques from sketching algorithms and optimization to the quantum-inspired literature. Unlike earlier works, this is a promising avenue that could lead to feasible implementations of classical regression in a quantum-inspired settings, for comparison against future quantum computers. ",An improved quantum-inspired algorithm for linear regression
22,1310975420271980551,836002087397801985,Tim Sainburg,"['New paper ""Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning"" with @leland_mcinnes and @TqGentner! 1/\n<LINK> <LINK>', 'UMAP consists of two steps: 1. Compute a graphical representation of a dataset (fuzzy simplicial complex) and 2. Optimize a low-dimensional embedding of the graph.  Here, we replace the 2nd step with a neural net that learns a parametric relationship between data and embedding 2/ https://t.co/TWQPpgqmAH', 'This allows us to extend UMAP to other deep learning architectures and applications, like autoencoders and classifiers. 3/ https://t.co/7Tp0zIzMbo', ""One application is semisupervised learning by training a network jointly on UMAP loss over unlabeled data, and classifier loss over labeled data. Here's an example with the moons dataset: 4/ https://t.co/oLTsAnT1Yr"", 'Experiments: First, we performed experiments looking at embedding quality over a bunch of related algorithms and datasets. Here are those algorithms/datasets projected in 2D. 5/ https://t.co/sDZQ5223P8', ""Speed: The learned parametric relationship speeds up embedding and reconstruction by orders of magnitude (as expected). Training Parametric UMAP is also relatively fast. Here's a plot of UMAP loss vs training time: 6/ https://t.co/QSFoU79Rdn"", 'We also looked at learned latent features in Parametric UMAP latent space (as well as combining UMAP with an autoencoder loss). Here you can see that, like VAEs and GANs, we find linear complex features in UMAP latent space. 7/ https://t.co/5ZBrxnrMY2', 'Finally, semisupervised learning. We tried a couple of different approaches. First, the most naive: jointly train a network on UMAP loss with unlabeled data and classifier loss with labeled data. This does well on structured datasets like MNIST and F-MNIST, but not CIFAR10. 7/ https://t.co/M0GhChIjkr', 'Next we added data augmentation and consistency regularization over UMAP projections. Consistency regularization usually tries to produce the same predictions across unlabeled and labeled data. Instead of predictions, here we tried to produce the same latent UMAP structure. 8/ https://t.co/T04U2mGI3s', ""You can see some improvements over baseline, but again, not in CIFAR10. It's not surprising that UMAP confers no improvement for CIFAR10, because UMAP using pixelwise Euclidean distance in CIFAR doesn't capture very much categorically relevant structure. 9/"", ""So the final thing we tried is using the labeled data/classifier to 'learn' a categorically relevant metric, then train the semisupervised network / UMAP with that learned categorically relevant metric rather than Euclidean distance over pixels. Here's how those projections look: https://t.co/yHn5pOjkKX"", 'Using the learned metric alone, no improvement. But with learned metric + consistency regularization, we do see some improvement in CIFAR10. Likely because consistency regularization with a learned distance metric is acting more like classic consistency regularization here. 10/ https://t.co/5RNkK1f72t', 'So, in summary, we created a parametric version of UMAP. It can extends to deep learning applications, and can be used in experimental paradigms where real-time analysis and experimental control are needed. It also can be used for SSL. 11/', 'There is also a big section of the introduction dedicated to understanding how UMAP works algorithmically, comparing it to t-SNE, and why UMAP in particular translates so well to parametric, neural network based embedding (hint: it has to do with normalization). 12/', 'Parametric UMAP is part of UMAP v0.5, which will hopefully be out soon. If you want to try it early, install the github branch. The code for these experiments is also on github: https://t.co/3xr5XttpUn\n13/', 'Thanks to everyone for reading, my collaborators, and my lab for support!', ""Final note: if there is related work you think I missed deserves to be discussed/mentioned in this paper, send me a dm/email. ML is a big field and I'm sure I missed some important and relevant work!"", ""Here's a colab notebooks walking you through the algorithm: https://t.co/K5zGZRMMFb"", '@hippopedoid @leland_mcinnes @TqGentner Good question! Yes CNN works better, see the notebook examples for fully connected vs CNN with MNIST: https://t.co/NLcmgDaOz4. See the loss as well. https://t.co/2H9iDmqWw2', ""@hippopedoid @leland_mcinnes @TqGentner Parametric UMAP isn't limited to Euclidean distance over pixels though. E.g. the birdsong example uses DTW distance + an LSTM and the SSL approach at the end uses classifier distance in a convnet."", ""@AlekRossi Thanks! I've fixed it for the next version."", ""@hippopedoid @leland_mcinnes @TqGentner That's my intuition too. The convnet is more efficient given the structure of the data, but a large enough fully connected network should perform equally or better."", '@hippopedoid @leland_mcinnes @TqGentner Good questions.\nAligned embeddings were done with Procustes alignment. \nInit for non-parametric was left at defaults (so it should be PCA for t-SNE and spectral embedding for UMAP). For parametric, the initial projections are based upon the initialization of the network', ""@hippopedoid @leland_mcinnes @TqGentner We used large batch size for speed (1000ex iirc), but that doesn't effect embeddings, just speed. You should be able to make batches as small as 1 edge (in fact, that's what non-parametric UMAP is doing). This is because of the negative sampling, which is a main benefit over tsne"", '@hippopedoid @leland_mcinnes @TqGentner I see what you mean. We are iterating over edges, not points. So everything is an edge. The repulsion is then applied over negative samples, randomly sampled from the dataset (which are usually not going to be edges).', ""@hippopedoid @leland_mcinnes @TqGentner For speed, negative samples are taken from the batch, so that we don't have to pass additional samples through the encoder. So in my implementation you will need a batch size probably &gt; 10."", ""@hippopedoid @leland_mcinnes @TqGentner But that isn't an inherent constraint of the algorithm, you could negative sample from the full dataset (like non-parametric UMAP), it would just be less efficient.""]",https://arxiv.org/abs/2009.12981,"UMAP is a non-parametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) Compute a graphical representation of a dataset (fuzzy simplicial complex), and (2) Through stochastic gradient descent, optimize a low-dimensional embedding of the graph. Here, we extend the second step of UMAP to a parametric optimization over neural network weights, learning a parametric relationship between data and embedding. We first demonstrate that Parametric UMAP performs comparably to its non-parametric counterpart while conferring the benefit of a learned parametric mapping (e.g. fast online embeddings for new data). We then explore UMAP as a regularization, constraining the latent distribution of autoencoders, parametrically varying global structure preservation, and improving classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. Google Colab walkthrough: this https URL ","Parametric UMAP embeddings for representation and semi-supervised
  learning"
23,1310954052554027009,1133672112546340864,Ting-Yun Cheng (Sunny),"[""Beyond the Hubble Sequence -- Exploring Galaxy Morphology with Unsupervised Machine Learning\n\n<LINK>\n\nMy new paper's on arxiv!\n\nWe unsupervisedly separate SDSS galaxies into 27 classes based on galaxy structure and shapes. Check it out if you are interested! :-D <LINK>""]",https://arxiv.org/abs/2009.11932,"We explore unsupervised machine learning for galaxy morphology analyses using a combination of feature extraction with a vector-quantised variational autoencoder (VQ-VAE) and hierarchical clustering (HC). We propose a new methodology that includes: (1) consideration of the clustering performance simultaneously when learning features from images; (2) allowing for various distance thresholds within the HC algorithm; (3) using the galaxy orientation to determine the number of clusters. This setup provides 27 clusters created with this unsupervised learning which we show are well separated based on galaxy shape and structure (e.g., S\'ersic index, concentration, asymmetry, Gini coefficient). These resulting clusters also correlate well with physical properties such as the colour-magnitude diagram, and span the range of scaling-relations such as mass vs. size amongst the different machine-defined clusters. When we merge these multiple clusters into two large preliminary clusters to provide a binary classification, an accuracy of $\sim87\%$ is reached using an imbalanced dataset, matching real galaxy distributions, which includes 22.7\% early-type galaxies and 77.3\% late-type galaxies. Comparing the given clusters with classic Hubble types (ellipticals, lenticulars, early spirals, late spirals, and irregulars), we show that there is an intrinsic vagueness in visual classification systems, in particular galaxies with transitional features such as lenticulars and early spirals. Based on this, the main result in this work is not how well our unsupervised method matches visual classifications and physical properties, but that the method provides an independent classification that may be more physically meaningful than any visually based ones. ","Beyond the Hubble Sequence -- Exploring Galaxy Morphology with
  Unsupervised Machine Learning"
24,1310934735053623302,1082828960302592005,Driscoll Physics Lab,"['Check out our new work on gel rupture, done in collaboration with the Szczepanski lab.  We are especially proud that the co-first authors of this paper are undergraduate researchers!\n<LINK>\n@CarolineSzcz <LINK>']",https://arxiv.org/abs/2009.12396,"Hydrogels have had a profound impact in the fields of tissue engineering, drug delivery, and materials science as a whole. Due to the network architecture of these materials, imbibement with water often results in uniform swelling and isotropic expansion which scales with the degree of cross-linking. However, the development of internal stresses during swelling can have dramatic consequences, leading to surface instabilities as well as rupture or bursting events. To better understand hydrogel behavior, macroscopic mechanical characterization techniques (e.g.\ tensile testing, rheometry) are often used, however most commonly these techniques are employed on samples that are in two distinct states: (1) unswollen and without any solvent, or (2) in an equilibrium swelling state where the maximum amount of water has been imbibed. Rarely is the dynamic process of swelling studied, especially in samples where rupture or failure events are observed. To address this gap, here we focus on rupture events in poly(ethylene glycol)-based networks that occur in response to swelling with water. Rupture events were visualized using high-speed imaging, and the influence of swelling on material properties was characterized using dynamic mechanical analysis. We find that rupture events follow a three-stage process that includes a waiting period, a slow fracture period, and a final stage in which a rapid increase in the velocity of crack propagation is observed. We describe this fracture behavior based on changes in material properties that occur during swelling, and highlight how this rupture behavior can be controlled by straight-forward modifications to the hydrogel network structure. ",Gel rupture in a dynamic environment
25,1310912164748644353,1603589724,Taiji Suzuki,"['(Our new arXiv paper)\n""Estimation error analysis of deep learning on the regression problem on the variable exponent Besov space"",\nby Kazuma Tsuji and Taiji Suzuki.\n<LINK>']",https://arxiv.org/abs/2009.11285,"Deep learning has achieved notable success in various fields, including image and speech recognition. One of the factors in the successful performance of deep learning is its high feature extraction ability. In this study, we focus on the adaptivity of deep learning; consequently, we treat the variable exponent Besov space, which has a different smoothness depending on the input location $x$. In other words, the difficulty of the estimation is not uniform within the domain. We analyze the general approximation error of the variable exponent Besov space and the approximation and estimation errors of deep learning. We note that the improvement based on adaptivity is remarkable when the region upon which the target function has less smoothness is small and the dimension is large. Moreover, the superiority to linear estimators is shown with respect to the convergence rate of the estimation error. ","Estimation error analysis of deep learning on the regression problem on
  the variable exponent Besov space"
26,1310877005072736257,205969794,Léa Steinacker,"[""In our new paper on the spread of COVID-19 misinformation on 🇩🇪-Twitter we show that  \n\n🔸partisan accounts contribute relatively more to conspiratorial narratives\n\n🔸bots don't significantly influence the spread of misinformation in 🇩🇪-Twitter (1.31%)\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2009.12905,"In late 2019, the gravest pandemic in a century began spreading across the world. A state of uncertainty related to what has become known as SARS-CoV-2 has since fueled conspiracy narratives on social media about the origin, transmission and medical treatment of and vaccination against the resulting disease, COVID-19. Using social media intelligence to monitor and understand the proliferation of conspiracy narratives is one way to analyze the distribution of misinformation on the pandemic. We analyzed more than 9.5M German language tweets about COVID-19. The results show that only about 0.6% of all those tweets deal with conspiracy theory narratives. We also found that the political orientation of users correlates with the volume of content users contribute to the dissemination of conspiracy narratives, implying that partisan communicators have a higher motivation to take part in conspiratorial discussions on Twitter. Finally, we showed that contrary to other studies, automated accounts do not significantly influence the spread of misinformation in the German speaking Twitter sphere. They only represent about 1.31% of all conspiracy-related activities in our database. ","COVID-19's (mis)information ecosystem on Twitter: How partisanship
  boosts the spread of conspiracy narratives on German speaking Twitter"
27,1310741599924576256,1106481910853701632,Kazuyuki Sekizawa,"['Our new paper has been uploaded to arXiv :)\n\nIn this paper we put the first step toward the fully microscopic description of kinetic energy dissipation and fluctuations in low-energy heavy-ion reactions!\n\nS. Ayik and K. Sekizawa, arXiv:2009.11978 [nucl-th]\n<LINK>']",https://arxiv.org/abs/2009.11978,"Background: Microscopic mean-field approaches have been successful in describing the most probable reaction outcomes in low-energy heavy-ion reactions. However, those approaches are known to severely underestimate dispersions of observables around the average values that has limited their applicability. Recently it has been shown that a quantal transport approach based on the stochastic mean-field (SMF) theory significantly improves the description, while its application has been limited so far to fragment mass and charge dispersions. Purpose: In this work, we extend the quantal transport approach based on the SMF theory for relative kinetic energy dissipation and angular momentum transfer in low-energy heavy-ion reactions. Results: As the first application of the proposed formalism, we consider the radial linear momentum dispersion, neglecting the coupling between radial and angular momenta. We analyze the total kinetic energy (TKE) distribution of binary reaction products in the $^{136}$Xe+$^{208}$Pb reaction at $E_\mathrm{c.m.}=526$ MeV and compare with experimental data. From time evolution of single-particle orbitals in TDHF, the radial diffusion coefficient is computed on a microscopic basis, while a phenomenological treatment is introduced for the radial friction coefficient. By solving the quantal diffusion equation for the radial linear momentum, the dispersion of the radial linear momentum is obtained, from which one can construct the TKE distribution. We find that the calculations provide a good description of the TKE distribution for large values of energy losses, TKEL $\gtrsim$ 150 MeV. However, the calculations underestimate the TKE distribution for smaller energy losses. Further studies are needed to improve the technical details of calculations. (Shortened due to the word limit) ","Kinetic energy dissipation and fluctuations in strongly-damped heavy-ion
  collisions within the stochastic mean-field approach"
28,1310501283133091841,149526852,Miguel Zumalacarregui,"['New paper with @jmezquiagabravo, new framework, new GW effects &amp; new GR tests: <LINK>\n\nWe find that a gravitational lens can split a GW signal in theories beyond GR, much like birefringence in some materials. \n\nLet me walk you through our main results [1/n] <LINK>', 'A gravitational lens spontaneously breaks symmetries, allowing couplings between GWs and scalar fields forbidden in homogeneous space.\n\nAround the lens +,x &amp; scalar polarizations combine into propagation eigenstates, which evolve independently and may have different speed [2/n] https://t.co/QQdU0dgrO7', 'The different speed leads to a time delay between the different states. In many cases the two metric polarizations h_+, h_x (slightly mixed w/ scalar) can pick a different speed.\n\nThis splits a signal (e.g. a BH merger) into separate echoes [3/n] https://t.co/n2i7dVuPKm', 'If the time delay is small enough, the two polarizations interfere with each other in the detector, scrambling the waveform.\n\nThis type of test does not require an electromagnetic counterpart to the event. Any LIGO signal is sensitive at the ~ms level [4/n] https://t.co/x8Cjv5jioh', 'GW lensing tests will become much more effective with growing number of GW detections (higher chance of good source-lens orientation).\n\nOur forecast suggest a factor ~10 improvement with @LIGO design sensitivity and ~100 with 3rd generation detectors [5/n] https://t.co/2Eh2xp1UQX', 'GW birefringence allows detection in lensing set-ups that do not contribute in ""traditional"" lensing.\n\nIn particular, the metric (Shapiro) and geometric (deflection angle) contributions to the time delay can dominate for different lens redshift or mass (here example th.) [6/n] https://t.co/mjbFhnUG6h', 'After discussing the framework in general we applied it to a specific theory of gravity in the Horndeski class. \n\nGW lensing tests via birefringence can go much deeper into the parameter space of the theory than limits from GW170817 (average difference in GW vs EM speed)\n[7/n] https://t.co/0uinswAANX', 'These constraints would be achieved by a binary merger in a very dense environment, such as the vicinity of a super-massive black hole. Events like #GW190521 could be a smoking gun for such environments (e.g. EM counterparts, hierarchical formation, multi-band observation) [8/n] https://t.co/GWCiHzPSDE', 'Many more details on the paper itself [9/n]\n\nhttps://t.co/sfZkc1Q6ev https://t.co/WMo9CeS46D', 'or on our recent talks  in remote conferences:\n\nhttps://t.co/rF9LyJ5pYs\nhttps://t.co/dYJqngBXhb\n\nand of course, questions comments are most welcome! [10/n]']",https://arxiv.org/abs/2009.12187,"Gravitational waves (GW), as light, are gravitationally lensed by intervening matter, deflecting their trajectories, delaying their arrival and occasionally producing multiple images. In theories beyond general relativity (GR), new gravitational degrees of freedom add an extra layer of complexity and richness to GW lensing. We develop a formalism to compute GW propagation beyond GR over general space-times, including kinetic interactions with new fields. Our framework relies on identifying the dynamical propagation eigenstates (linear combinations of the metric and additional fields) at leading order in a short-wave expansion. We determine these eigenstates and the conditions under which they acquire a different propagation speed around a lens. Differences in speed between eigenstates cause birefringence phenomena, including time delays between the metric polarizations (orthogonal superpositions of $h_+,h_\times$) observable without an electromagnetic counterpart. In particular, GW echoes are produced when the accumulated delay is larger than the signal's duration, while shorter time delays produce a scrambling of the wave-form. We also describe the formation of GW shadows as non-propagating metric components are sourced by the background of the additional fields around the lens. As an example, we apply our methodology to quartic Horndeski theories with Vainshtein screening and show that birefringence effects probe a region of the parameter space complementary to the constraints from the multi-messenger event GW170817. In the future, identified strongly lensed GWs and binary black holes merging near dense environments, such as active galactic nuclei, will fulfill the potential of these novel tests of gravity. ","Gravitational wave lensing beyond general relativity: birefringence,
  echoes and shadows"
29,1309595798334832645,2932062039,Micah Goldblum,['Check out our new paper on generalizing across unseen domain shifts.  Adversarial training on perturbed batch norm statistics robustifies both classifiers and segmentation models: <LINK>'],https://arxiv.org/abs/2009.08965,"Adversarial training is the industry standard for producing models that are robust to small adversarial perturbations. However, machine learning practitioners need models that are robust to other kinds of changes that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by directly perturbing feature statistics, rather than image pixels, to produce models that are robust to various unseen distributional shifts. We explore the relationship between these perturbations and distributional shifts by visualizing adversarial features. Our proposed method, Adversarial Batch Normalization (AdvBN), is a single network layer that generates worst-case feature perturbations during training. By fine-tuning neural networks on adversarial feature distributions, we observe improved robustness of networks to various unseen distributional shifts, including style variations and image corruptions. In addition, we show that our proposed adversarial feature perturbation can be complementary to existing image space data augmentation methods, leading to improved performance. The source code and pre-trained models are released at \url{this https URL}. ",Encoding Robustness to Image Style via Adversarial Feature Perturbations
30,1309522446651985924,288648073,Riley Chien,['In a new paper out today from the Whitfield group @Dartmouth <LINK> we leverage prior work by Setia in our group and @IBMResearch to present a framework for designing customizable encodings for fermions on quantum computers. <LINK>'],http://arxiv.org/abs/2009.11860,"Simulating a fermionic system on a quantum computer requires encoding the anti-commuting fermionic variables into the operators acting on the qubit Hilbert space. The most familiar of which, the Jordan-Wigner transformation, encodes fermionic operators into non-local qubit operators. As non-local operators lead to a slower quantum simulation, recent works have proposed ways of encoding fermionic systems locally. In this work, we show that locality may in fact be too strict of a condition and the size of operators can be reduced by encoding the system quasi-locally. We give examples relevant to lattice models of condensed matter and systems relevant to quantum gravity such as SYK models. Further, we provide a general construction for designing codes to suit the problem and resources at hand and show how one particular class of quasi-local encodings can be thought of as arising from truncating the state preparation circuit of a local encoding. We end with a discussion of designing codes in the presence of device connectivity constraints. ",Custom fermionic codes for quantum simulation
31,1309522438032486402,768092862,Thomas G. Dietterich,"['New paper: <LINK> \n@lukasruff led our team in this attempt to unify various perspectives on deep anomaly detection within a probabilistic framework. Jacob Kauffmann, @robvdm\n, Grégoire Montavon, @WojciechSamek\n, @KloftMarius\n, and Klaus-Robert Müller.', '""A Unifying Review of Deep and Shallow Anomaly Detection"" https://t.co/BO6xu6C1uT https://t.co/dpTL77UPIG']",https://arxiv.org/abs/2009.11732,"Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection. ",A Unifying Review of Deep and Shallow Anomaly Detection
32,1309519664016318464,1390296240,Jeffrey Hazboun,"['New paper with @Chengcheng_xin and @Dr_CMingarelli investigating candidate super massive black hole binaries in CRTS, PTF and @PanSTARRS1 using current &amp; forecast pulsar timing arrays. #PTAs will have a lot to say in the next 15 years. \n\n<LINK> <LINK>', 'In addition to @nanograv 11yr, this was a good excuse to use https://t.co/BLlVgqAH5r, to not only build a near future IPTA, but also a realistic @SKA_telescope PTA. Rather than a straight line sensitivity, we see what it will look like to add SKA pulsars to the IPTA. https://t.co/xTwiSHIRoa', 'The EM candidates allow us to take advantage of full sky sensitivity maps to see which sources were in the most sensitive parts of the sky. And come up with ideas to help see them sooner. https://t.co/lRo9XCDRJD', 'Anyone interested in using the sensitivity curves or sky maps can find the versions from the paper (or make your own!) by going to https://t.co/Z8MhXfqijQ.\n\n[Bonus: @GoogleColab link] https://t.co/SHjnO1LnWB', ""Soon we'll be able to use real #pta data and targeted EM searches (like @CaitlinWitt332's work in https://t.co/9HBowoeehN) to say even more about these candidates. In the meantime these sky maps provide a fun sandbox for understanding what the #nHz sky will hold."", 'Great working with @Dr_CMingarelli as always! And looking forward to seeing a lot more great work from @chengcheng_xin!']",http://arxiv.org/abs/2009.11865,"Supermassive black hole binary systems (SMBHBs) emitting gravitational waves may be traced by periodic light curves. We assembled a catalog of 149 such periodic light curves, and using their masses, distances, and periods, predicted the gravitational-wave strain and detectability of each binary candidate using all-sky detection maps. We found that the International Pulsar Timing Array (IPTA) provides almost uniform sky coverage -- a unique ability of the IPTA -- and by 2025 will improve NANOGrav's current minimum detectable strain by a factor of 6, and its volume by a factor of 216. Moreover, IPTA will reach detection sensitivities for three candidates by 2025, and 13 by the end of the decade, enabling us to constrain the underlying empirical relations used to estimate SMBH masses. We find that we can in fact already constrain the mass of a binary in Mrk 504 to $M<3.3\times 10^9~M_\odot$. We also identify 24 high-mass high-redshift galaxies which, according to our models, should not be able to host SMBHBs. Importantly the GW detection of even one of these candidates would be an essentially eternal multimessenger system, and identifying common false positive signals from non-detections will be useful to filter the data from future large-scale surveys such as LSST. ","Multimessenger pulsar timing array constraints on supermassive black
  hole binaries traced by periodic light curves"
33,1309479525122621440,869876111336914944,Nana Liu,"['Excited to share our new paper on finding optimal robustness bounds for quantum classification using tools from quantum hypothesis testing! Many thanks to my fabulous collaborators for all the fun we had together! <LINK> 😀🤗', '@amaan_sid Thanks!! 😄', '@jj_xyz He he, health and safety always first']",https://arxiv.org/abs/2009.10064,"Quantum machine learning models have the potential to offer speedups and better predictive accuracy compared to their classical counterparts. However, these quantum algorithms, like their classical counterparts, have been shown to also be vulnerable to input perturbations, in particular for classification problems. These can arise either from noisy implementations or, as a worst-case type of noise, adversarial attacks. In order to develop defence mechanisms and to better understand the reliability of these algorithms, it is crucial to understand their robustness properties in presence of natural noise sources or adversarial manipulation. From the observation that measurements involved in quantum classification algorithms are naturally probabilistic, we uncover and formalize a fundamental link between binary quantum hypothesis testing and provably robust quantum classification. This link leads to a tight robustness condition which puts constraints on the amount of noise a classifier can tolerate, independent of whether the noise source is natural or adversarial. Based on this result, we develop practical protocols to optimally certify robustness. Finally, since this is a robustness condition against worst-case types of noise, our result naturally extends to scenarios where the noise source is known. Thus, we also provide a framework to study the reliability of quantum classification protocols beyond the adversarial, worst-case noise scenarios. ","Optimal Provable Robustness of Quantum Classification via Quantum
  Hypothesis Testing"
34,1309471457962065921,1140222123006472194,Kasper Elm Heintz,"['In case you’re not completely done with getting awesome #FastRadioBurst news this week, check out our new paper on @arxiv_org today: <LINK>  \nHere, we challenge the magnetar intepretation of FRBs, which has otherwise gained a lot of traction recently. <LINK>', 'For this work, we use the recently published stellar population properties of FRB host galaxies  (here the SFR CDF is shown) to show that it is unlikely that FRBs track star formation, as would be expected in the prompt magnetar scenario. Check it out! https://t.co/CtG5j7aRir']",https://arxiv.org/abs/2009.11735,"We explore the prompt magnetar progenitor scenario in the context of fast radio burst (FRB) host galaxies demographics and offset distributions. Magnetars are neutron stars with strong magnetic fields on the order of $10^{15}$ G with a short decay lifetime of less than $10^4$ years. Due to their extremely short lifetimes, magnetars should follow the demographics of galaxies according to their current star-formation rate (SFR). Moreover, we hypothesize that magnetars should follow the SFR profile within galaxies, which we assume to follow an exponential profile. We construct a simple model for the host galaxies of magnetars assuming these events track SFR in all galaxies and compare it to observed properties from a sample of \nsecure\ secure FRB hosts. We find the distribution of observed SFRs is inconsistent with the model at $>95\%$ c.l. The offset distribution is consistent with this scenario; however, this could be due to the limited sample size and the seeing limited estimates for the effective radii of the FRB host galaxies. Despite the recent association of an FRB with a magnetar in the Milky Way, magnetars may not be the only source of FRBs in the universe, yet any other successful model must account for the demographics of the FRB host in SFR and their observed galactocentric offsets. ","Confronting the Magnetar Interpretation of Fast Radio Bursts Through
  Their Host Galaxy Demographics"
35,1309464728972951553,256513537,Dr Chiara Mingarelli,"['New paper! @Chengcheng_xin, @JeffreyHazboun and me see which candidate supermassive black hole binaries in CRTS, PTF, and @PanSTARRS1 could be detected by @NANOGrav, IPTA, @SKA_telescope, and on what timescale this will happen. Spoiler alert: 3 by 2025.  <LINK> <LINK>', ""A few other key points: we show that the international pulsar timing array (IPTA) gives almost uniform sky coverage over individual PTAs, and that by 2025 IPTA will improve @NANOGrav NANOGrav's current minimum detectable strain by a factor of 6, and its volume by a factor of 216!"", 'IPTA will reach detection sensitivities for three candidates by 2025, and 13 by the end of the decade, enabling us not only to do multimessenger astrophysics, but also to constrain the underlying empirical relations used to estimate SMBH masses!', 'We also found that we can already constrain the mass of a binary in Mrk 504 to M &lt; 3.3e9 solar masses. We believe this is its first direct BH mass constraint (tell us if you know of another one!). It also has signs of x-ray periodicity (Saade+2020) in addition to optical! https://t.co/mKt5YFo3b8', 'We also identify 24 high-mass high-z galaxies which should not be able to host SMBHBs. We internally called these the “impossible binaries”. What it could really mean is that our models are too simple and that lots of gas and eccentricity are helping these high-z SMBHBs to merge!', 'GW detection of even one of these candidates would be an essentially eternal multimessenger system, and identifying common false positive signals from non-detections will be useful to filter the data from future large-scale surveys such as LSST. https://t.co/jLWnxsOG31', ""It was a pleasure to work with very talented collaborators: @Columbia's @chengcheg_xin (new to twitter!) who just started graduate school, and @NANOGrav postdoc @JeffreyHazboun !"", '@IBJIYONGI thanks for the signal boost!', '@IBJIYONGI ... and of course for your congratulations and support!!']",https://arxiv.org/abs/2009.11865,"Supermassive black hole binary systems (SMBHBs) emitting gravitational waves may be traced by periodic light curves. We assembled a catalog of 149 such periodic light curves, and using their masses, distances, and periods, predicted the gravitational-wave strain and detectability of each binary candidate using all-sky detection maps. We found that the International Pulsar Timing Array (IPTA) provides almost uniform sky coverage -- a unique ability of the IPTA -- and by 2025 will improve NANOGrav's current minimum detectable strain by a factor of 6, and its volume by a factor of 216. Moreover, IPTA will reach detection sensitivities for three candidates by 2025, and 13 by the end of the decade, enabling us to constrain the underlying empirical relations used to estimate SMBH masses. We find that we can in fact already constrain the mass of a binary in Mrk 504 to $M<3.3\times 10^9~M_\odot$. We also identify 24 high-mass high-redshift galaxies which, according to our models, should not be able to host SMBHBs. Importantly the GW detection of even one of these candidates would be an essentially eternal multimessenger system, and identifying common false positive signals from non-detections will be useful to filter the data from future large-scale surveys such as LSST. ","Multimessenger pulsar timing array constraints on supermassive black
  hole binaries traced by periodic light curves"
36,1309417647403139074,2886658437,Sean Raymond,"[""Born Eccentric!\n\nOur new paper -- led by Matt Clement -- shows that Jupiter and Saturn's orbits may have been non-circular from the start. \n\nThe evidence is indirect: simulations of the Solar System's instability more easily match present-day orbits.\n\nSee <LINK>"", ""This image explains the broader context\n\n(I'm planning on hanging a high-res version of this poster in our kitchen) https://t.co/VMRT3T3TER"", 'Authors: Matt Clement, Nate Kaib, Rogerio Deienno, John Chambers, Andre Izidoro and myself \n\n(I might have got the order wrong, but Matt Clement is the driver of this train)', 'Let me motivate our paper a little more clearly.\n\nThe gaseous disk phase (which only lasts a few million years) was pretty busy: all of the giant planets formed and migrated around.\n\nAt the end of this phase they were probably in a chain of orbital resonances.', 'To date it has generally been thought that Jupiter and Saturn were trapped in 3:2 resonance when the gas disk dissipated. \n\nThe next big event for the giant planets was their big dynamical instability (the ""Nice model"").', ""Starting with Jup and Sat in 3:2 resonance, the instability only matches Jup's current orbit a small fraction of the time (Nesvorny &amp; Morbidelli 2012)\n\nBUT: hydro simulations find that Jup and Sat could have been trapped in 2:1 resonance during the gas disk phase, not 3:2"", 'Pierens et al (2014) even found that Jup and Sat in 2:1 resonance end up with eccentric orbits when the gas disk goes away....\n\n(FYI Jupiter and Saturn can still follow a Grand Tack migration in the 2:1 resonance)\n\nhttps://t.co/ivO4mpSmxV', 'Finally, the punchline: \n\nAnd now, we (Clement et al) find that the odds of the planets ending up on their current orbits after the instability are quite a big higher if Jup and Sat started in the 2:1 resonance with modestly-eccentric orbits.\n\nhttps://t.co/t5fQYSOXts']",https://arxiv.org/abs/2009.11323,"An episode of dynamical instability is thought to have sculpted the orbital structure of the outer solar system. When modeling this instability, a key constraint comes from Jupiter's fifth eccentric mode (quantified by its amplitude M55), which is an important driver of the solar system's secular evolution. Starting from commonly-assumed near-circular orbits, the present-day giant planets' architecture lies at the limit of numerically generated systems, and M55 is rarely excited to its true value. Here we perform a dynamical analysis of a large batch of artificially triggered instabilities, and test a variety of configurations for the giant planets' primordial orbits. In addition to more standard setups, and motivated by the results of modern hydrodynamical simulations of the giant planets' evolution within the primordial gaseous disk, we consider the possibility that Jupiter and Saturn emerged from the nebular gas locked in 2:1 resonance with non-zero eccentricities. We show that, in such a scenario, the modern Jupiter-Saturn system represents a typical simulation outcome, and M55 is commonly matched. Furthermore, we show that Uranus and Neptune's final orbits are determined by a combination of the mass in the primordial Kuiper belt and that of an ejected ice giant. ","Born eccentric: constraints on Jupiter and Saturn's pre-instability
  orbits"
37,1309341858879827969,911474423412219904,Julien Tierny,"['Check out our new #ieeevis 2020 paper ""Localized topological simplification of scalar data"". Pre-simplify your data in seconds instead of minutes: up to x36 speedups!\n#TopologicalDataAnalysis #visualization #imageprocessing #datascience #persistenthomology\n<LINK> <LINK>']",https://arxiv.org/abs/2009.00083,"This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field f and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to f and only exhibits the selected set of extrema. Specifically, sub- and superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed -- from a combinatorial perspective -- to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to x36. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging. ",Localized Topological Simplification of Scalar Data
38,1309298726670753795,369569444,Takahiro TERADA (寺田 隆広),['Our new paper on an explanation for NANOGrav 12.5-yr excess by solar-mass PBH formation. \n<LINK>\nTestable with the GWs from mergers of the solar-mass PBH binaries. <LINK>'],https://arxiv.org/abs/2009.11853,"The NANOGrav collaboration for the pulsar timing array (PTA) observation recently announced evidence of an isotropic stochastic process, which may be the first detection of the stochastic gravitational-wave (GW) background. We discuss the possibility that the signal is caused by the second-order GWs associated with the formation of solar-mass primordial black holes (PBHs). This possibility can be tested by future interferometer-type GW observations targeting the stochastic GWs from merger events of solar-mass PBHs as well as by updates of PTA observations. ","Solar-Mass Primordial Black Holes Explain NANOGrav Hint of Gravitational
  Waves"
39,1309293075215167489,2337598033,Geraint F. Lewis,"['A new paper with PhD student, Florian List, and Pascal Elahi - \n\n""Lux ex tenebris: The imprint of annihilating dark matter on the intergalactic medium during Cosmic Dawn”\n\n<LINK> <LINK>']",https://arxiv.org/abs/2009.11298,"Upcoming measurements of the highly redshifted 21cm line with next-generation radio telescopes such as HERA and SKA will provide the intriguing opportunity to probe dark matter (DM) physics during the Epoch of Reionization (EoR), Cosmic Dawn, and the Dark Ages. With HERA already under construction, there is a pressing need to thoroughly understand the impact of DM physics on the intergalactic medium (IGM) during these epochs. We present first results of a hydrodynamic simulation suite with $2 \times 512^3$ particles in a $(100 \ h^{-1} \ \text{Mpc})^3$ box with DM annihilation and baryonic cooling physics. We focus on redshift $z \sim 11$, just before reionization starts in our simulations, and discuss the imprint of DM annihilation on the IGM and on structure formation. We find that whereas structure formation is not affected by thermal WIMPs heavier than $m_\chi \gtrsim 100 \ \text{MeV}$, heating from $\mathcal{O}$(GeV) DM particles may leave a significant imprint on the IGM that alters the 21cm signal. Cold gas in low density regions is particularly susceptible to the effects of DM heating. We note, however, that delayed energy deposition is not currently accounted for in our simulations. ","Lux ex tenebris: The imprint of annihilating dark matter on the
  intergalactic medium during Cosmic Dawn"
40,1309263951780352000,5025111,Will Whitney,"['New paper! We propose to measure the quality of learned representations using the complexity of finding a nearly-optimal predictor on a downstream task.\n\nBlog: <LINK>\nPaper: <LINK>\nLibrary: <LINK>', 'The methods that people currently use for evaluating learned representations are related: fix an amount of evaluation data, then look at a point estimate or integral of the loss. https://t.co/4eLAQhjXhc', ""Results that depend on the amount of eval data are odd:\n1. Guessing how much data is needed to solve a task is hard.\n2. Choosing a repr with n points is misleading IRL when data comes in all the time.\n\nShouldn't representation quality only depend on the task &amp; learning algorithm?"", ""We introduce a measure called surplus description length (SDL): the number of extra bits required to encode an infinite stream of data using a learning algorithm instead of the optimal code. Unlike existing measures, it's not a function of the dataset size. https://t.co/C5khbJ22a3"", 'SDL measures something fundamental: the information an alg needs to gain about the optimal predictor in order to perform well (i.e. expected loss ≤ ε). We also suggest ε sample complexity, which measures the same thing in a coarser way. Both are computed from loss-data curves.', 'Naively computing loss-data curves is slow: train ~100 small neural nets back to back. We use tools from JAX to batch the update step *across neural nets*, letting you train them all in parallel on one GPU. Reduces time from 30m to 2m.', ""Fast batched training was tricky to implement, but it's now easy to use in our representation eval library, Reprieve: https://t.co/oO3pfGIUYn\n\nEvaluation is too important to do ad-hoc. Reprieve is a stable, framework-agnostic tool to enable reproducibility &amp; fair comparisons."", ""It's been a pleasure working with my insightful, hardworking co-authors Min Jae Song, David Brandfonbrener, @thejaan, @kchonyc on these ideas. This work would not be where it is without them."", 'Thanks also to @SingularMattrix @avitaloliver @anselmlevskaya and lots of other JAXers who were unfailingly helpful when I bothered them with questions both trivial and arcane!', 'Whoops missed that Min Jae is on Twitter: @mj_theory!']",https://arxiv.org/abs/2009.07368,"We consider the problem of evaluating representations of data for use in solving a downstream task. We propose to measure the quality of a representation by the complexity of learning a predictor on top of the representation that achieves low loss on a task of interest, and introduce two methods, surplus description length (SDL) and $\varepsilon$ sample complexity ($\varepsilon$SC). In contrast to prior methods, which measure the amount of information about the optimal predictor that is present in a specific amount of data, our methods measure the amount of information needed from the data to recover an approximation of the optimal predictor up to a specified tolerance. We present a framework to compare these methods based on plotting the validation loss versus evaluation dataset size (the ""loss-data"" curve). Existing measures, such as mutual information and minimum description length probes, correspond to slices and integrals along the data axis of the loss-data curve, while ours correspond to slices and integrals along the loss axis. We provide experiments on real data to compare the behavior of each of these methods over datasets of varying size along with a high performance open source library for representation evaluation at this https URL ","Evaluating representations by the complexity of learning low-loss
  predictors"
41,1309234057327378438,951034206393716736,Yannig Goude,['New paper on adaptive load forecasting during lockdown with David Obst and Joseph de Vilmarest <LINK>'],https://arxiv.org/abs/2009.06527,"The coronavirus disease 2019 (COVID-19) pandemic has urged many governments in the world to enforce a strict lockdown where all nonessential businesses are closed and citizens are ordered to stay at home. One of the consequences of this policy is a significant change in electricity consumption patterns. Since load forecasting models rely on calendar or meteorological information and are trained on historical data, they fail to capture the significant break caused by the lockdown and have exhibited poor performances since the beginning of the pandemic. This makes the scheduling of the electricity production challenging, and has a high cost for both electricity producers and grid operators. In this paper we introduce adaptive generalized additive models using Kalman filters and fine-tuning to adjust to new electricity consumption patterns. Additionally, knowledge from the lockdown in Italy is transferred to anticipate the change of behavior in France. The proposed methods are applied to forecast the electricity demand during the French lockdown period, where they demonstrate their ability to significantly reduce prediction errors compared to traditional models. Finally expert aggregation is used to leverage the specificities of each predictions and enhance results even further. ","Adaptive Methods for Short-Term Electricity Load Forecasting During
  COVID-19 Lockdown in France"
42,1309224864276975616,4365927557,Dr. Jake Turner 🌅,"['****New Paper Alert** \n\nToday my group at @Cornell @CSInst (@AstroAndrew123, @DrRayJay, &amp; I) published a new paper using @NASA_TESS data \n\n""TESS Observations of the Hot Jupiter Exoplanet XO-6b: No Evidence of Transit Timing Variations"" \n\n<LINK>\n\nTHREAD 1/8 <LINK>', 'XO-6b is a typical hot Jupiter that orbits a F5V-type star.\n\nPrevious ground-based observations by Garai et al. 2020 (https://t.co/hHQWrbQTM1) find transit timing variations (TTvs) with an amplitude of 14 min &amp; period of  450 days\n\n2/8 https://t.co/6BSYRupp2u', ""Inspired by the possible TTVs from XO-6b, we looked at the system with NASA's TESS (@NASA_TESS) mission. \n\nTESS is perfect for this study because it has really good photometric and timing precision. \n\nMore on the timing verification from TESS: https://t.co/fmyIMPQCK6 2/8 https://t.co/8qLcDXeK3w"", 'The @NASA_TESS light curves of XO-6b were exquisite in precision allowing for us to characterize the system with much greater detail than ever before.  3/8 https://t.co/XTeqpqkY3G', 'We fit the individual and combined light curves with EXOMOP, a transit fitting code I developed in my PhD. All the fits individual fits were consistent with each other. \n\nMore details on code: https://t.co/P1dJrGCzS3\n 4/8 https://t.co/g9FkJPU7ac', 'The main result of our paper: \n- We find no evidence for TTVs: we can rule out TTVs &gt; 2.5 minutes at the 3σ level. \n- We rule out the previous claim of TTVs by 10σ \n5/8 https://t.co/cX5fQ4CEnG', 'The cause of the tension between our results &amp; those of Garai et al. (2020) is not clear but it may be due to unknown timing errors in their ground-based data. \n\n- A few of the smaller TTVs could be related to barycentric corrections, the larger ones must have other causes.\n\n6/8 https://t.co/QXXIeSKx9r', 'Careful absolute telescope clock calibrations are important to adequately schedule future atmospheric characterization observations on JWST, etc..\n\nOur study shows we need to be careful cause ground-based telescopes will definitely play a role (see https://t.co/0YheFgnuDd)\n7/8', ""In conclusion: \nOur findings highlight @NASA_TESS's capabilities for robust follow-up, and confirm that TTVs are rarely seen in hot Jupiters, unlike is the case with small planets. \n\nYou can find the paper free here: https://t.co/E1gDd0vzxm 8/8""]",https://arxiv.org/abs/2009.10781,"From previous ground-based observations, the hot Jupiter exoplanet XO-6b was reported to exhibit apparently periodic transit timing variations (TTVs), with a semi-amplitude of 14 minutes and a period of about 450 days. These variations were interpreted as being due to a resonant perturbation between XO-6b and a hitherto unknown low-mass planet orbiting the same star. To understand this enigmatic planetary system better, we analysed three sectors of data, spanning over seven months, from the Transiting Exoplanet Survey Satellite (TESS), which produces high-quality light curves that are well suited to characterizing exoplanets and searching for TTVs. Here we present an updated orbital period of 3.7649893 $\pm$ 0.0000037 days and a transit epoch of 2456652.7157 $\pm$ 0.0022 BJD$_{TDB}$. The planetary parameters we report, while consistent with their discovery values, have greatly improved precision. Notably, we find no evidence for TTVs: we can rule out TTVs $\gtrsim$ 2.5 minutes at the 3$\sigma$ level. Therefore, the TESS data have sufficient precision and time baseline to reveal readily the previously reported TTVs of approximately 10 minutes. Our findings highlight TESS's capabilities for robust follow-up, and confirm that TTVs are rarely seen in hot Jupiters, unlike is the case with small planets. ","TESS Observations of the Hot Jupiter Exoplanet XO-6b: No Evidence of
  Transit Timing Variations"
43,1309217889568854016,1152296594,Swabha Swayamdipta,"['As datasets have grown larger, data exploration has become increasingly challenging. Our new work on Dataset Cartography, at @emnlp2020 with @royschwartz02, @NickLourie, @yizhongwyz, @HannaHajishirzi, @nlpnoah, @YejinChoinka offers a solution 🗺️\nPaper: <LINK> 1/n <LINK>', 'Our data maps for datasets, wrt a given model, plots individual instances, contextualized based on how the model trains on them. The coordinates📍for the instances are given by two intuitive measures - model confidence and model variability, over a few epochs of training. 2/n', 'This helps us find regions in the data that might be the most ambiguous 😕 to the model but valuable for training, regardless. We also automatically discover instances easiest / hardest to learn for the model, the latter might also be the ones which are labeled incorrectly.  3/n', 'Finally, data maps can help us automatically compare different datasets, and ultimately build better datasets. Code and final camera ready version coming soon ⏰. Let me know 👇if you have questions, or DM me! n/n']",http://arxiv.org/abs/2009.10795,"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ""ambiguous"" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are ""easy to learn"" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds ""hard to learn""; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization. ","Dataset Cartography: Mapping and Diagnosing Datasets with Training
  Dynamics"
44,1309189265662849024,1092693586263457792,Greg Yang,"['1/ In a neural network, activation vectors depend on the weight matrices in really complex, nonlinear ways. New paper <LINK>: the activations are ""independent"" from the weights in a randomly initialized wide NN of any architecture! WTF!! <LINK>', '2/ The notion of ""independence"" here is of *random matrices*, called ""asymptotic free independence.""  Recall that the regular notion of independence implies two random variables cannot conspire to fluctuate in the same direction.', '3/ As a result, the expectation of their products is predictable from their individual expectations. Likewise, asymptotic freeness of two random matrices, intuitively, implies that their respective eigenvectors and eigenvalues lie in general position to each other.', '4/ Consequently, their spectral distributions (ie distribution of their eigenvalues) would combine in predictable ways were they to be summed or multiplied (as matrices).', '5/ The precise formulation of the independence result is: the diagonal matrices with activation vectors as diagonals are ""asymptotically free independent"" from the weights.', '6/ In fact, this ""free independence phenomenon"" implies the ""gradient independence phenomenon"" https://t.co/j9QnCRkZDe, where, during the first backpropagation, the gradient is the same whether you backprop using the forward propagation weights or a new set of iid weights.']",https://arxiv.org/abs/2009.10685,"In a neural network (NN), *weight matrices* linearly transform inputs into *preactivations* that are then transformed nonlinearly into *activations*. A typical NN interleaves multitudes of such linear and nonlinear transforms to express complex functions. Thus, the (pre-)activations depend on the weights in an intricate manner. We show that, surprisingly, (pre-)activations of a randomly initialized NN become *independent* from the weights as the NN's widths tend to infinity, in the sense of asymptotic freeness in random matrix theory. We call this the Free Independence Principle (FIP), which has these consequences: 1) It rigorously justifies the calculation of asymptotic Jacobian singular value distribution of an NN in Pennington et al. [36,37], essential for training ultra-deep NNs [48]. 2) It gives a new justification of gradient independence assumption used for calculating the Neural Tangent Kernel of a neural network. FIP and these results hold for any neural architecture. We show FIP by proving a Master Theorem for any Tensor Program, as introduced in Yang [50,51], generalizing the Master Theorems proved in those works. As warmup demonstrations of this new Master Theorem, we give new proofs of the semicircle and Marchenko-Pastur laws, which benchmarks our framework against these fundamental mathematical results. ",Tensor Programs III: Neural Matrix Laws
45,1309147235339448320,979379437069271043,Pedro Machado,"['New paper today with Guillermo, Ivan, @yuberfpg, Darío and Salva! Awesome team!\nWe look at what physics a Skipper CCD detector near a nuclear reactor could do and how things like backgrounds, uncertainties, quenching, ..., would affect the sensitivity.\n<LINK> <LINK>']",https://arxiv.org/abs/2009.10741,"We analyze in detail the physics potential of an experiment like the one recently proposed by the vIOLETA collaboration: a kilogram-scale Skipper CCD detector deployed 12 meters away from a commercial nuclear reactor core. This experiment would be able to detect coherent elastic neutrino nucleus scattering from reactor neutrinos, capitalizing on the exceptionally low ionization energy threshold of Skipper CCDs. To estimate the physics reach, we elect the measurement of the weak mixing angle as a case study. We choose a realistic benchmark experimental setup and perform variations on this benchmark to understand the role of quenching factor and its systematic uncertainties,background rate and spectral shape, total exposure, and reactor antineutrino flux uncertainty. We take full advantage of the reactor flux measurement of the Daya Bay collaboration to perform a data driven analysis which is, up to a certain extent, independent of the theoretical uncertainties on the reactor antineutrino flux. We show that, under reasonable assumptions, this experimental setup may provide a competitive measurement of the weak mixing angle at few MeV scale with neutrino-nucleus scattering. ","The physics potential of a reactor neutrino experiment with Skipper
  CCDs: Measuring the weak mixing angle"
46,1309129229037121544,382961853,Jo Dunkley,"[""New @ACT_Pol paper (and data) today with more than 4000 galaxy clusters detected via the Sunyaev-Zel'dovich effect, reaching billions of light yrs away!  - Hilton et al <LINK>, with redshifts from optical data including @theDESurvey, HSC, KiDS and @sdssurveys.""]",https://arxiv.org/abs/2009.11043,"We present a catalog of 4195 optically confirmed Sunyaev-Zel'dovich (SZ) selected galaxy clusters detected with signal-to-noise > 4 in 13,211 deg$^2$ of sky surveyed by the Atacama Cosmology Telescope (ACT). Cluster candidates were selected by applying a multi-frequency matched filter to 98 and 150 GHz maps constructed from ACT observations obtained from 2008-2018, and confirmed using deep, wide-area optical surveys. The clusters span the redshift range 0.04 < z < 1.91 (median z = 0.52). The catalog contains 222 z > 1 clusters, and a total of 868 systems are new discoveries. Assuming an SZ-signal vs. mass scaling relation calibrated from X-ray observations, the sample has a 90% completeness mass limit of M500c > 3.8 x 10$^{14}$ MSun, evaluated at z = 0.5, for clusters detected at signal-to-noise ratio > 5 in maps filtered at an angular scale of 2.4'. The survey has a large overlap with deep optical weak-lensing surveys that are being used to calibrate the SZ-signal mass-scaling relation, such as the Dark Energy Survey (4566 deg$^2$), the Hyper Suprime-Cam Subaru Strategic Program (469 deg$^2$), and the Kilo Degree Survey (825 deg$^2$). We highlight some noteworthy objects in the sample, including potentially projected systems; clusters with strong lensing features; clusters with active central galaxies or star formation; and systems of multiple clusters that may be physically associated. The cluster catalog will be a useful resource for future cosmological analyses, and studying the evolution of the intracluster medium and galaxies in massive clusters over the past 10 Gyr. ","The Atacama Cosmology Telescope: A Catalog of &gt; 4000 Sunyaev-Zel'dovich
  Galaxy Clusters"
47,1309100517935796224,1140222123006472194,Kasper Elm Heintz,"['Paper day! 🤩 and today is a big one — in our new paper on @arxiv_org today, we present comprehensive analyses of *five* new FRB host galaxies in addition to a careful re-examination of previous hosts published in the literature. Check it out! \n<LINK>', 'Before going detail of what we actually found in the paper, I want to mention that this work is the result of a wonderful collaboration between the team members of F^4 (https://t.co/q7MOrrWq1J) and the CRAFT and realfast teams.', 'So, the five new hosts studied in this work are shown here, and as is clearly visible, most of them are located far away from the host galaxy center in regions with seemingly no light from star formation! https://t.co/0bYjx9ShMS', 'For these new host galaxies and all previously published hosts from the literature, we inferred their stellar population properties and one of the interesting results we found was that most of the hosts appear to be less star-forming than typical main-sequence galaxies! https://t.co/a5bdNOZmpa', 'Moreover, their line emission are mostly indicative of harder ionization fields than what you’d expect from star formation alone — we found most of the FRB hosts to be located in the “LINER region” of the BPT diagram: https://t.co/tZYDgojNr9', 'We then digged a bit into what the implications of the host galaxy properties and especially the offsets of FRBs from the host centers were for the most likely progenitor channels by comparing to other, more well-known transients.', 'It turns out, the projected offsets are consistent with that observed for short GRBs, and core-collapse and Type Ia supernovae! The more centrally located long GRBs are excluded, based on both the offset and stellar mass distributions. https://t.co/Yn8PZ64FmG', 'All in all, #FastRadioBurst host galaxies are a weird bunch, but they hold the clues to the most likely progenitor channels of FRBs! Finally, all data are publicly available here: https://t.co/FBbiTLF3sS and on the dedicated FRB repo GitHub repo.']",https://arxiv.org/abs/2009.10747,"We present observations and detailed characterizations of five new host galaxies of fast radio bursts (FRBs) discovered with the Australian Square Kilometre Array Pathfinder (ASKAP) and localized to $\lesssim 1''$. Combining these galaxies with FRB hosts from the literature, we introduce criteria based on the probability of chance coincidence to define a sub-sample of 10 highly-confident associations (at $z=0.03-0.52$), three of which correspond to known repeating FRBs. Overall, the FRB host galaxies exhibit a broad, continuous range of color ($M_u-M_r = 0.9 - 2.0$), stellar mass ($M_\star = 10^{8} - 6\times 10^{10}\,M_{\odot}$), and star-formation rate (${\rm SFR} = 0.05 - 10\,M_{\odot}\,{\rm yr}^{-1}$) spanning the full parameter space occupied by $z<0.5$ galaxies. However, they do not track the color-magnitude, SFR-$M_\star$, nor BPT diagrams of field galaxies surveyed at similar redshifts. There is an excess of ""green valley"" galaxies and an excess of emission-line ratios indicative of a harder radiation field than that generated by star-formation alone. From the observed stellar mass distribution, we rule out the hypothesis that FRBs strictly track stellar mass in galaxies ($>99\%$ c.l.). We measure a median offset of 3.3 kpc from the FRB to the estimated center of the host galaxies and compare the host-burst offset distribution and other properties with the distributions of long- and short-duration gamma-ray bursts (LGRBs and SGRBs), core-collapse supernovae (CC-SNe), and Type Ia SNe. This analysis rules out galaxies hosting LGRBs (faint, star-forming galaxies) as common hosts for FRBs ($>95\%$ c.l.). Other transient channels (SGRBs, CC- and Type Ia SNe) have host galaxy properties and offsets consistent with the FRB distributions. All of the data and derived quantities are made publicly available on a dedicated website and repository. ","Host Galaxy Properties and Offset Distributions of Fast Radio Bursts:
  Implications for their Progenitors"
48,1309057171473260551,301992639,roeeaharoni,"['Turns out that using multilingual entity linking, we can automatically evaluate machine translation without any references! New paper with Zorik Gekhman, Genady Beryozkin, Markus Freitag and Wolfgang Macherey, to appear in Findings of EMNLP: <LINK> @GoogleAI <LINK>']",https://arxiv.org/abs/2009.11027,"We propose a simple and effective method for machine translation evaluation which does not require reference translations. Our approach is based on (1) grounding the entity mentions found in each source sentence and candidate translation against a large-scale multilingual knowledge base, and (2) measuring the recall of the grounded entities found in the candidate vs. those found in the source. Our approach achieves the highest correlation with human judgements on 9 out of the 18 language pairs from the WMT19 benchmark for evaluation without references, which is the largest number of wins for a single evaluation method on this task. On 4 language pairs, we also achieve higher correlation with human judgements than BLEU. To foster further research, we release a dataset containing 1.8 million grounded entity mentions across 18 language pairs from the WMT19 metrics track data. ",KoBE: Knowledge-Based Machine Translation Evaluation
49,1309045227844505601,481539448,Richard Alexander,"[""Another new paper (busy week 😀), led by @PhysicsUoL's Enrico Ragusa (+@JoshCalcino, Kieran Hirsh &amp; @danprice_astro). Enrico investigated how binary stars interact with their discs, focusing on the growth of disc eccentricity, and the inner cavity size.\n\n<LINK> <LINK>"", 'This is an old and very complex problem, and Enrico used a large suite of 3-D simulations (using @DiRAC_HPC) to explore the physics in detail. We find that the discs around stellar binaries always become eccentric, causing asymmetry in the disc even when the binary is circular.', 'Enrico explored the underlying physics in detail, and discovered a new link between the cavity size and the disc eccentricity. We also expect particle trapping in these asymmetric structures, which has important consequences for observations (e.g., with ALMA).', ""There's much more in the paper than I can summarize here though - full details can be found on the arXiv link at the top of the thread.""]",https://arxiv.org/abs/2009.10738,"We study the mutual evolution of the orbital properties of high mass ratio, circular, co-planar binaries and their surrounding discs, using 3D Smoothed Particle Hydrodynamics simulations. We investigate the evolution of binary and disc eccentricity, cavity structure and the formation of orbiting azimuthal over-dense features in the disc. Even with circular initial conditions, all discs with mass ratios $q>0.05$ develop eccentricity. We find that disc eccentricity grows abruptly after a relatively long time-scale ($\sim 400\textrm{--}700$ binary orbits), and is associated with a very small increase in the binary eccentricity. When disc eccentricity grows, the cavity semi-major axis reaches values $a_{\rm cav}\approx 3.5\, a_{\rm bin}$. We also find that the disc eccentricity correlates linearly with the cavity size. Viscosity and orbit crossing, appear to be responsible for halting the disc eccentricity growth -- eccentricity at the cavity edge in the range $e_{\rm cav}\sim 0.05\textrm{--} 0.35$. Our analysis shows that the current theoretical framework cannot fully explain the origin of these evolutionary features when the binary is almost circular ($e_{\rm bin}\lesssim 0.01$); we speculate about alternative explanations. As previously observed, we find that the disc develops an azimuthal over-dense feature in Keplerian motion at the edge of the cavity. A low contrast over-density still co-moves with the flow after 2000 binary orbits; such an over-density can in principle cause significant dust trapping, with important consequences for protoplanetary disc observations. ","The evolution of large cavities and disc eccentricity in circumbinary
  discs"
50,1309041527738568705,976037939292594177,Alessandro Ignesti,"['#paperday\nNew accepted paper on the arxiv <LINK> . We present a brand-new LOFAR observation of the Kite radio source in Abell 2626.  Due to its symmetric arcs, the origin of this source has been a puzzle for years. <LINK>', 'The new data revealed that the arcs lead in large, steep-spectrum plumes, similar to what we observe in X-shaped radio galaxies.  We conclude that this amazing source could be the result of complex interactions between a relic radio galaxy and the motions of the ICM.', 'We also present the first low-frequency observation of a jellyfish galaxy, JW100, in the same cluster. We found evidence of radio emission extended both in front and behind the stellar disk. We are going to further investigate this object, so stay tuned for updates! https://t.co/aKblRrB8YD']",https://arxiv.org/abs/2009.11210,"The radio source at the center of the galaxy cluster Abell 2626, also known as the Kite, stands out for its unique morphology composed of four, symmetric arcs. Previous studies have probed the properties of this source at different frequencies and its interplay with the surrounding thermal plasma, but the puzzle of its origin is still unsolved. We use new LOw Frequency ARray (LOFAR) observation from the LOFAR Two-meter Sky Survey at 144 MHz to investigate the origin of the Kite.} We present a detailed analysis of the new radio data which we combined with archival radio and X-ray observations. We have produced a new, resolved spectral index map of the source with a resolution of 7$''$ and we studied the spatial correlation of radio and X-ray emission to investigate the interplay between thermal and non-thermal plasma. The new LOFAR data have changed our view of the Kite by discovering two steep-spectrum ($\alpha<-1.5$) plumes of emission connected to the arcs. The spectral analysis shows, for the first time, a spatial trend of the spectrum along the arcs with evidence of curved synchrotron spectra and a spatial correlation with the X-ray surface brightness. On the basis of our results, we propose that the Kite was originally an X-shaped radio galaxy whose fossil radio plasma, after the end of the activity of the central active galactic nucleus, has been compressed due to motions of the thermal plasma in which it is encompassed. The interplay between the compression and advection of the fossil plasma, with the restarting of the nuclear activity of the central galaxy, could have enhanced the radio emission of the fossil plasma producing the arcs of the Kite. We present also the first, low-frequency observation of a jellyfish galaxy in the same field, in which we detect extended, low-frequency emission without a counterpart at higher frequencies. ","The great Kite in the sky: a LOFAR observation of the radio source in
  Abell 2626"
51,1308951548979011585,887992016,Luke Metz,"['We have a new paper on learned optimizers! We used thousands of tasks (and a lot of compute 😬) to train general purpose learned optimizers that perform well on never-before-seen tasks, and can even train new versions of themselves.\n<LINK>\n1/8', 'In the same way learned features took over computer vision, we believe ML algorithms will be replaced with learned components.\n\nWe shift away from hand designed optimizers (SGD, Adam) to learned optimizers parameterized by neural nets and trained to optimize neural nets.\n2/8', 'We explore a new learned optimizer architecture: a hierarchical LSTM. It has access to both training loss and validation loss of the target task, which allows for dynamic regularization. \n3/8 https://t.co/r5fwOrzpeF', 'We find the number of tasks we train the learned optimizer on to be critical. More tasks leads to better optimizers and we ultimately train on a dataset of ~6k tasks.\n4/8 https://t.co/fB5wQfriyt', 'The resulting learned optimizer, which requires no hyper parameter tuning, outperforms modestly tuned hand design methods on the majority of our tasks.\n5/8 https://t.co/iRzQ7VprDY', 'On larger scale tasks, these optimizers have comparable performance to learning rate tuned adam/momentum despite never seeing similar tasks at outer-training time. For example, below is a small ResNet on CIFAR-10.\n6/8 https://t.co/EZQ4felFxU', 'In my favorite experiment, we show how general these methods are by using them to train new versions of themselves!\n\n(This is similar to self-hosting compiles -- compilers which are written in the language that they compile.)\n7/8 https://t.co/TDgk9fwK8A', 'Thanks to my wonderful collaborators: @niru_m , @bucketofkets , @poolio, @jaschasd 🙏\n8/8', '@TheGradient In this work we focus on the mean test loss over the course of training. \n\nOne cool thing about learned optimizers though is we are free to pick which measure of success we care about and train an optimizer to specifically target it.', '@AIActorCritic @pabbeel It should perform pretty well as there are actually MNIST auto-encoders in the distribution of tasks that we train on.\n\nI agree that exploring how these learned methods work on more interesting loss surfaces is something that interests me greatly!', '@nikoliazekter Understanding how these optimizers work is of great interest to us! We have explored a bit more than whats in the paper but found the behavior of these optimizers are complex and hard to pin down as there are so many moving pieces. More soon hopefully!', '@alth0u Yep.\n\nOur learned optimizer can adaptively output the size of the steps it takes, so yes it could implement something like LR schedules.\n\nOur baselines all have tunable learning rates and Adam8p + opt_list have tunable learning rate schedules.', ""@niru_m @MilesCranmer @bucketofkets @poolio @jaschasd Thanks for the summary!\n\nSymbolic regression is quite interesting to me. Particularly when done in a way that doesn't require a combinatorial search like yours. Not only is it good for understanding, but also for deployment / inference speed.""]",https://arxiv.org/abs/2009.11243,"Much as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch. ","Tasks, stability, architecture, and compute: Training more effective
  learned optimizers, and using them to train themselves"
52,1308947577019281414,1087183776,Jordan Ellenberg,"['New paper!  With Daniel Corey and @wanlinbunny.  ""The Ceresa class:  tropical, topological, and algebraic.""  <LINK>', '@wanlinbunny The Ceresa class is a really handy algebraic cycle attached to a curve: in some sense the simplest cycle ""beyond the Jacobian.""', '@wanlinbunny We had wondered for a while whether there was a good analogue for a tropical curve (i.e. a metric graph).  Turns out there is!', ""@wanlinbunny What's more, it's computable in terms of commutators in the mapping class group; much of the main work of the paper is in that language."", '@wanlinbunny So if you like, the paper is about tropical curves, but also about the restriction of the Morita cocycle to groups generated by commuting Dehn twists.', '@wanlinbunny And the invariant ends up agreeing with the usual Ceresa class for curves/C((t)) whose tropicalization is the tropical curve in question.', ""@wanlinbunny What kind of invariant is this? It lives in a finite abelian group attached to the curve (not Pic^0, but related.)  In particular, it's torsion."", '@wanlinbunny The key point for that turns out to be an old theorem of @AndyPutmanMath -- thanks, Andy!', '@wanlinbunny @AndyPutmanMath I really like this definition; we have lots of questions about it, and in this paper just a few answers.']",https://arxiv.org/abs/2009.10824,"The Ceresa cycle is an algebraic cycle attached to a smooth algebraic curve with a marked point, which is trivial when the curve is hyperelliptic with a marked Weierstrass point. The image of the Ceresa cycle under a certain cycle class map provides a class in \'etale cohomology called the Ceresa class. Describing the Ceresa class explicitly for non-hyperelliptic curves is in general not easy. We present a ""combinatorialization"" of this problem, explaining how to define a Ceresa class for a tropical algebraic curve, and also for a topological surface endowed with a multiset of commuting Dehn twists (where it is related to the Morita cocycle on the mapping class group). We explain how these are related to the Ceresa class of a smooth algebraic curve over $\mathbb{C}(\!(t)\!)$, and show that the Ceresa class in each of these settings is torsion. ","The Ceresa class: tropical, topological, and algebraic"
53,1308903515864076294,1238925341747343361,Aaron Barth,"[""Strange things are afoot in IC 5063. @SpaceGeck discovered this last year with a close look at a Hubble snapshot, and today there's a new paper led by @StellarBones:\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2009.10153,"On Earth near sunset, the sun may cast ""crepuscular rays"" such that clouds near the horizon obscure the origin of light scattered in bright rays. In principle, AGN should be able to produce similar effects. Using new Hubble Space Telescope (HST) near-infrared and optical observations, we show that the active galaxy IC 5063 contains broad radial rays extending to $\gtrsim$11 kpc from the nucleus. We argue that the bright rays may arise from dusty scattering of continuum emission from the active nucleus, while the dark rays are due to shadowing near the nucleus, possibly by a warped torus. We also consider alternative AGN-related and stellar origins for the extended light. ","Crepuscular Rays from the Highly Inclined Active Galactic Nucleus in IC
  5063"
54,1308776984089157647,3433220662,Anthony Bonato,"[""New survey paper of mine on graph burning up on arXiv. Since we introduced it in 2016, researchers wrote more than two dozen papers on the topic. I'd joke about it being a hot topic but that would be cliche.\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2009.10642,"Graph burning is a deterministic, discrete-time process that models how influence or contagion spreads in a graph. Associated to each graph is its burning number, which is a parameter that quantifies how quickly the influence spreads. We survey results on graph burning, focusing on bounds, conjectures, and algorithms related to the burning number. We will discuss state-of-the-art results on the burning number conjecture, burning numbers of graph classes, and algorithmic complexity. We include a list of conjectures, variants, and open problems on graph burning. ",A survey of graph burning
55,1308698391388528640,2369154074,Philipp Koch,"['New preprint working paper online: <LINK>\n\n""Economic Complexity and Growth: Can value-added exports better explain the link?""\n\nA short summary thread 1/4\n\n#EconTwitter @WU_econ @Eco_Austria', 'Economic #complexity is typically approximated using a country’s gross export structure. In times of integrated global value chains, gross exports may convey an inaccurate image of a country’s economic performance (foreign value-added, double-counted exports). 2/4', 'Extending this approach, I propose to approximate complexity using the value-added export structure. Complexity rankings differ compared to established indices, and the explanatory power of GDP growth is higher for a sample of 40 lower-middle- to high-income-countries. 3/4', 'All this was inspired by the seminal work on economic complexity of @ricardo_hausman, @cesifoti, A. Tacchella and many others. Before checking out my paper, you should read theirs first!\nThe working paper is based on my thesis @WU_econ. 4/4']",https://arxiv.org/abs/2009.07599,"In economic literature, economic complexity is typically approximated on the basis of an economy's gross export structure. However, in times of ever increasingly integrated global value chains, gross exports may convey an inaccurate image of a country's economic performance since they also incorporate foreign value-added and double-counted exports. Thus, I introduce a new empirical approach approximating economic complexity based on a country's value-added export structure. This approach leads to substantially different complexity rankings compared to established metrics. Moreover, the explanatory power of GDP per capita growth rates for a sample of 40 lower-middle- to high-income countries is considerably higher, even if controlling for typical growth regression covariates. ","Economic Complexity and Growth: Can value-added exports better explain
  the link?"
56,1308585626997002240,2819715191,Antonella Palmese,['GW190521 could have been a merger of central black holes in dwarf galaxies\nNew fun paper with @conselice !\n<LINK> <LINK>'],https://arxiv.org/abs/2009.10688,"We present an alternative formation scenario for the gravitational wave event GW190521, that can be explained as the merger of central black holes from two ultra-dwarf galaxies of stellar mass $\sim 10^5-10^6 ~M_\odot$, which had themselves previously undergone a merger. The GW190521 components' masses of $85^{+21}_{-14}M_\odot$ and $66^{+17}_{-18}M_\odot$ challenge standard stellar evolution models, as they fall in the so-called mass gap. We demonstrate that the merger history of ultra-dwarf galaxies at high redshifts ($1\lesssim z \lesssim 2$) matches well the LIGO/Virgo inferred merger rate for black holes within the mass range of the GW190521 components, resulting in a likely time delay of $\lesssim 4$ Gyr considering the redshift of this event. We further demonstrate that the predicted time-scales are consistent with expectations for central black hole mergers, although with large uncertainties due to the lack of high-resolution simulations in low-mass dwarf galaxies. Our findings show that this black hole production and merging channel is viable and extremely interesting as a new way to explore galaxies' black hole seeds and galaxy formation. We recommend this scenario be investigated in detail with simulations and observations. ",GW190521 from the Merger of Ultra-Dwarf Galaxies
57,1308578349208481800,781766535660515328,Takahiro Morishita🔭🌏,"['Our new paper is out! Long story short: We reduced many HST images taken previously. Found 3 point sources likely at z~8 from ~300 sightlines. One of them may be an extreme starburst, inferred from its SED! <LINK> @thashimotoooo']",https://arxiv.org/abs/2009.10078,"To extend the search for quasars in the epoch of reionization beyond the tip of the luminosity function, we explore point source candidates at redshift $z\sim8$ in SuperBoRG, a compilation of $\sim$0.4deg$^2$ archival medium-deep ($m_{\rm F160W}\sim 26.5$ABmag, 5$\sigma$) parallel IR images taken with the Hubble Space Telescope (HST). Initial candidates are selected by using the Lyman-break technique. We then carefully analyze source morphology, and robustly identify 3 point sources at $z\sim8$. Photometric redshift analysis reveals that they are preferentially fit by extra-galactic templates, and we conclude that they are unlikely to be low-$z$ interlopers, including brown dwarfs. A clear IRAC ch2 flux excess is seen in one of the point sources, which is expected if the source has strong H$\beta$+[O III] emission with rest-frame equivalent width of $\sim3000$AA. Deep spectroscopic data taken with Keck/MOSFIRE, however, do not reveal Ly$\alpha$ emission from the object. In combination with the estimated H$\beta$+[O III] equivalent width, we place an upper limit on its Ly$\alpha$ escape fraction $f_{\rm esc, Ly\alpha}< 2 \%$. We estimate the number density of these point sources $\sim1\times10^{-6}$Mpc$^{-3}$mag$^{-1}$ at $M_{\rm UV}\sim-23$mag. The final interpretation of our results remains inconclusive: extrapolation from low-$z$ studies of $faint$ quasars suggests that $>100\times$ survey volume may be required to find one of this luminosity. The James Webb Space Telescope will be able to conclusively determine the nature of our luminous point source candidates, while the Roman Space Telescope will probe $\sim 200$ times the area of the sky with the same observing time considered in this HST study. ","SuperBoRG: Exploration of point sources at $z\sim8$ in HST parallel
  fields"
58,1308532869837524992,708096751161311232,Priyamvada Natarajan,"[""For my astro peeps who might have missed my paper on the arXiv today <LINK>\nHere's the gist of this new channel to make IMBHs *throughout cosmic time* in dense, gas-rich nuclear star clusters (NSCs) via accretion (1/n) <LINK>"", 'stellar mass seed BH ~ 5 -10 solar mass forms in the NSC that has ~ 10^4-5 stars. Seed BH behaves like a test particle scatters off stars &amp; gas, inevitably wandering randomly, small excursions from the center, growing very rapidly via wind-fed supra-exponential accretion (2/n) https://t.co/9xYc53lpIb', 'In this early growth phase can model motion as a simple harmonic oscillator and compute how much it can grow. As it grows motion gets damped due to drag from gas and dynamical friction, motion transits to that of a damped harmonic oscillator, continues growing but...(3/n)', 'Growth could get disrupted..can estimate final BH mass if prematurely terminated (25 - 100 solar masses). If it continues then it reaches the center, settles down to being a stationary BH at this point it would be between 100 - 1000 solar masses (4/n)', 'Of course, depending on the availability of gas could keep growing till gas supply is exhausted which in typical NSCs can easily bring it 10^4-5 solar masses. So here is a schematic of the possible outcomes -- (5/n) https://t.co/i9QNCMhWvi', 'What the model offers is a continual formation mechanism for IMBHs as NSCs seen in a large fraction of galaxies, they are huddled in the center of galaxies but some are off-center too. Therefore, NSCs can act as incubators for the formation of IMBHs - channel can explain....(6/n)', '(i) central &amp; off-center IMBHs are now detected in low-mass dwarf galaxies (ii) premature termination can account for formation of BHs in the mass gap 50 - 150 solar masses, sources like GW190521. In this model IMBHs grow by accretion not mergers (previous work) in NSCs. (7/n)', '@harshalhb_india model not yet set up to do demographic predictions like the merger scenarios..just worked out the physics at the moment - but yes plan is to look at all observables...']",https://arxiv.org/abs/2009.09156,"While the formation of the first black holes at high redshift is reasonably well understood though debated, massive black hole formation at later cosmic epochs has not been adequately explored. We present a gas accretion driven mechanism that can build up black hole masses rapidly in dense, gas-rich nuclear star clusters (NSCs). Wind-fed supra-exponential accretion of an initially wandering black hole in NSCs can lead to extremely fast growth, scaling stellar mass remnant seed black holes up to intermediate mass black holes (IMBHs). Operating throughout cosmic time, growth via this new channel is modulated by the gas supply, and premature termination results in the formation of lower mass black holes with masses in the range of 50 - few 100 solar masses, filling in the so-called mass gap. However, in most gas-rich NSCs, growth is unimpeded, inevitably leading to the formation of IMBHs with masses ranging from 100 - 100,000 solar masses. A spate of new detection spanning the full range of the IMBH mass function - from the LIGO-VIRGO source GW190521 to the emerging population of 10^5 solar mass black holes harbored in low-mass dwarf galaxies - are revealing this elusive population. Naturally accounting for the detected presence of off-center IMBHs in low-mass dwarfs, this new pathway also predicts the existence of an extensive population of wandering non-central black holes in more massive galaxies would be detectable via tidal disruption events and as GW sources. Gas-rich NSCs serve as incubators for the continual formation of black holes over a wide range in mass throughout cosmic time. ",A new channel to form IMBHs throughout cosmic time
59,1308474653820608512,15095910,Nazneen Rajani,"['New preprint alert! 🚨\nIntroducing GeDi (pronounced Jedi): A Powerful New Method for Controlling Language Models. \nPaper: <LINK>\nCode: <LINK>\nBlog: <LINK>\nThis paper has a bunch of really cool results. Here are a few. <LINK>', '2/n Our goal is to use smaller language models as generative classifiers to guide generation from larger LMs. \nGeDi uses Bayes rule to guide generation from other language models. Think of it as a filter for next word prediction from other LMs.', '3/n This allows us to control and make the underlying LM safer and helps with debiasing. We were able to detoxify GPT2 and make it friendlier without affecting the quality of generation. Here is a sample from the detoxification experiment. We all know what GPT2 would pred :) https://t.co/AxbutQuEy2', 'The other cool thing about GeDi is that it can generalize and generate positive text in domains outside of movie reviews that it was trained on. This is because we use contrasting control codes which lead to cancelation of domain-specific tokens. https://t.co/MUKMBbrrzF', '4/n Finally, we show that GeDi is able to generalize to new domains beyond the 4 topics it was trained on. https://t.co/O18RmwGo5E', 'n/n Work done with collaborators @benwkrause @AkhileshGotmare @BMarcusMcCann @StrongDuality @JotyShafiq @RichardSocher']",https://arxiv.org/abs/2009.06367,"While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed. ",GeDi: Generative Discriminator Guided Sequence Generation
60,1308357094136053760,1288452767376445440,Patrick Kidger,"['New paper, with @RickyTQChen!\n\n""Hey, that\'s not an ODE"": Faster ODE Adjoints with 12 Lines of Code\n<LINK>\n<LINK>\n\nWe roughly double the training speed of neural ODEs.\n\n1/ <LINK>', 'We show how to make the backward (adjoint) pass much cheaper.\n\nWe see a median of 40% fewer function evaluations (NFEs) across experiments on multiple domains: time series problems, generative modelling, and physical control. On some problems we see as much as 62% fewer NFEs.\n\n2/', 'The best bit is, we do this with just one easy change - the ""12 lines of code"" from the title.\n(A number that includes visual whitespace, incidentally :D )\n\nThis makes it an easy thing to add to any existing project.\n\n3/', 'The idea is that when solving the adjoint equations, typical adaptive-step differential equation solvers will be overzealous about rejecting steps. By exploiting the particular structure of the adjoint equations, things get much things cheaper/faster.\n\n4/', 'Questions? Comments? Let me or @RickyTQChen know!\n\n5/5', ""@MichaelPoli6 @RickyTQChen Haha :D Yup, it's available in torchdiffeq. We even include a short code example in the paper you can just copy-paste.""]",https://arxiv.org/abs/2009.09457,"Neural differential equations may be trained by backpropagating gradients via the adjoint method, which is another differential equation typically solved using an adaptive-step-size numerical differential equation solver. A proposed step is accepted if its error, \emph{relative to some norm}, is sufficiently small; else it is rejected, the step is shrunk, and the process is repeated. Here, we demonstrate that the particular structure of the adjoint equations makes the usual choices of norm (such as $L^2$) unnecessarily stringent. By replacing it with a more appropriate (semi)norm, fewer steps are unnecessarily rejected and the backpropagation is made faster. This requires only minor code modifications. Experiments on a wide range of tasks -- including time series, generative modeling, and physical control -- demonstrate a median improvement of 40% fewer function evaluations. On some problems we see as much as 62% fewer function evaluations, so that the overall training time is roughly halved. ","""Hey, that's not an ODE"": Faster ODE Adjoints via Seminorms"
61,1308303505384288256,50901426,Rafael Alves Batista,"['In this new paper, we show that the maximum gamma-ray energy of TXS 0506+056 is highly sensitive to propagation effects, in particular intergalactic magnetic fields.\n<LINK> <LINK>']",https://arxiv.org/abs/2009.09772,"The recent observation of high-energy neutrinos from the 2017 flare of the blazar TXS~0506+056, together with counterparts across the whole electromagnetic spectrum, opens up new possibilities for investigating the properties of this class of objects as well as the traversed medium. Propagation effects such as the attenuation of the very-high-energy gamma-ray component by the extragalactic background light are well known, and usually taken into account when fitting spectral energy distributions of objects. Other effects such as those of intergalactic magnetic fields are, however, often neglected. In this work, we present a comprehensive study of the influence of these fields and the extragalactic background light on the determination of the intrinsic gamma-ray spectrum of this blazar. ","The Intrinsic Gamma-Ray Spectrum of TXS 0506+056: Intergalactic
  Propagation Effects"
62,1308292340109381634,326864126,Shota Notsu,"['The preprint of our new paper (accepted for publication in MNRAS a few days ago) just came out on astro-ph today.\n\n""The composition of hot Jupiter atmospheres assembled within chemically evolved protoplanetary discs""\n<LINK>']",https://arxiv.org/abs/2009.09444,"The radial-dependent positions of snowlines of abundant oxygen- and carbon-bearing molecules in protoplanetary discs will result in systematic radial variations in the C/O ratios in the gas and ice. This variation is proposed as a tracer of the formation location of gas-giant planets. However, disc chemistry can affect the C/O ratios in the gas and ice, thus potentially erasing the chemical fingerprint of snowlines in gas-giant atmospheres. We calculate the molecular composition of hot Jupiter atmospheres using elemental abundances extracted from a chemical kinetics model of a disc midplane where we have varied the initial abundances and ionization rates. The models predict a wider diversity of possible atmospheres than those predicted using elemental ratios from snowlines only. As found in previous work, as the C/O ratio exceeds the solar value, the mixing ratio of CH$_{4}$ increases in the lower atmosphere, and those of C$_{2}$H$_{2}$ and HCN increase mainly in the upper atmosphere. The mixing ratio of H$_{2}$O correspondingly decreases. We find that hot Jupiters with C/O$>1$ can only form between the CO$_{2}$ and CH$_{4}$ snowlines. Moreover, they can only form in a disc which has fully inherited interstellar abundances, and where negligible chemistry has occurred. Hence, carbon-rich planets are likely rare, unless efficient transport of hydrocarbon-rich ices via pebble drift to within the CH$_{4}$ snowline is a common phenomenon. We predict combinations of C/O ratios and elemental abundances that can constrain gas-giant planet formation locations relative to snowline positions, and that can provide insight into the disc chemical history. ","The composition of hot Jupiter atmospheres assembled within chemically
  evolved protoplanetary discs"
63,1308132826387107841,2239670346,Jonathan Frankle,"['Several methods have recently been proposed for pruning neural networks at initialization. In our new paper (@KDziugaite, @roydanroy, @mcarbin), we rigorously study these methods to determine why they ""miss the mark"" and underperform pruning after training <LINK> <LINK>', 'TLDR: We find that these methods extract useful layerwise pruning rates rather than determining which individual connections to prune. This undermines the claimed justifications for the methods and suggests broader challenges with the methods, pruning at initialization, or both.', ""Main story: we've pruned to lower inference costs since the 80s, but we've only recently used pruning to lower training costs. Our lottery ticket work showed it may be possible to prune early in training; SNIP, GraSP, and SynFlow are efficient proposals to prune at initialization https://t.co/ctM0sMJdin"", ""These methods make some progress: they outperform random pruning. However, they remain far below pruning after training. Importantly, no single method is SOTA: it is possible to find a sparsity/network where each method (including naive magnitude pruning) looks like it's the best https://t.co/Ns0cc8I2Ou"", 'To understand why, we ran a series of ablations: randomly shuffling which weights are pruned per layer, reinitializing the unpruned weights, and ""inverting"" (pruning the most important weights). Our goal was to figure out which signals these methods actually use when pruning. https://t.co/gPx81vkS0x', 'We find that you can randomly shuffle or reinitialize the pruned networks and maintain (or, for SynFlow, *improve*) accuracy. That is, the useful information these methods extract are the per-layer proportions in which to prune. This is *not* the case when pruning after training. https://t.co/IsBp3RZ6ao', 'In the case of GraSP, you can actually prune the *most important* weights and reach the same accuracy. We actually find that pruning weights with the lowest magnitude GraSP scores leads to better performance. https://t.co/atZWswLJgv', ""Takeaway 1: Although these methods use very different pruning heuristics, they have surprisingly similar behaviors that are distinct from pruning after training. This may suggest that it's inherent difficulty to prune at initialization (e.g., https://t.co/ObD659dYS8 @utkuevci)."", 'Takeaway 2: The ablations (esp. shuffling) undermine the claimed rationales for the methods, which are said to ""identify important connections"" (SNIP), ""remove weights that will not reduce gradient flow,"" and ""tak[e] the inter-layer interactions of params into account"" (SynFlow).', 'Takeaway 3: If the problem is pruning at initialization, why not prune after some training? We tried pruning at many points during training, but accuracy improved only gradually. If we want to prune early in training, we will need new heuristics designed specifically to do so. https://t.co/VPRdHnhCeL', ""Summary: Proposals for pruning at initialization currently miss the mark: they underperform pruning after training, and you don't need to prune specific weights to match their accuracy. Is this due to the heuristics themselves, or is it inherent to pruning at initialization? https://t.co/rHqN18yMdz"", 'P.S. Why does SynFlow sometimes *improve* when shuffling? We believe that it is because SynFlow prunes entire neurons at far lower sparsities than other methods. This ""neuron collapse"" reduces the capacity of the network and may inhibit accuracy. Shuffling restores many neurons. https://t.co/lshIhnoDKu', '@adityakusupati My big takeaway is that, given such different heuristics (magnitude, gradient, hessian, with/without data) have the same pathology, there may be inherent challenges to pruning at initialization. The next question is whether we can find new heuristics that work *early* in training', ""@adityakusupati I'm glad folks are starting to look at ImageNet. ResNet-50 on ImageNet and ResNet-20 on CIFAR-10 behave very very differently from the overparameterized wide ResNets (and ImageNet ResNets being used on CIFAR-10) that are common in these papers."", ""@adityakusupati To be fair, very few people have those sorts of resources. Getting the ImageNet data for this paper was time-consuming and expensive and we still weren't able to get it for all experiments. And the TinyImageNet situation is (1) a mess and (2) unrepresentative of ImageNet."", '@adityakusupati We need a better intermediate tasks on the scale of TinyImageNet but with the same behavior as ResNet-20 on CIFAR-10 and ResNet-50 on ImageNet. Add that to the list of things to do...', ""@jxbz It's definitely something worth looking into as follow-up to our work. However, only SynFlow has issues with neuron collapse (likely due to a pathology where it punishes weights whose neurons have already been pruned), so I'm not sure that it needs addressing outside of SynFlow.""]",https://arxiv.org/abs/2009.08576,"Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both. ",Pruning Neural Networks at Initialization: Why are We Missing the Mark?
64,1308102399769411586,916346317709938690,Aurelie Herbelot,"['New pre-print with Katrin Erk: \n\n""How to marry a star? Probabilistic constraints for meaning in context."" \n\nA theoretical paper on evocation: how people ‘imagine\' an entire situation from a few words, and how meaning is contextualised in the process.\n\n <LINK> /1', ""#lightreading summary ⬇️\n\nWhen you hear 'the batter ran to the ball', what do you imagine? A batter? A ball? But perhaps also a pitch, an audience, the sun and a cap on the head of the batter.\n\nWhat do you imagine when you hear 'the duchess drove to the ball'? /2"", ""We propose a computational model which takes a (simple) sentence and builds a conceptual description for it. In the process of doing so, it captures  appropriate word senses or other lexical meaning variations. E.g. that the batter's ball is not a dancing event. /3"", ""Simple example: hearing 'bat' in the context of 'vampire' should make you much more likely to understand that the speaker is talking about animals: /4 https://t.co/WRC4M08NQK"", ""Hearing 'the vampire is eating' should activate the concept of an object (*what* is the vampire eating?) and that object might be more likely to be a blood orange than another vampire or a castle: /5 https://t.co/QkdcKOFLnJ"", 'Sometimes, different interpretations compete with each other. Consider the sentence:\n\n""The astronomer married the star.""\n\nWhat comes to your mind? The Hollywood star or the celestial object? Both? /6', 'To take care of this, the model generates not a single situation description but many, each one with its own probability. For some speaker, the description with the Hollywood star might be more likely than the one where the astronomer married Betelgeuse. Or vice-versa. /7', 'A situation description consists of:\n* scenarios (at-the-restaurant, gothic-novel) \n* concepts (champagne, vampire)\n* individuals (this bottle of champagne, Dracula)\n* features of individuals (having a cork, being pale)\n* roles (being the agent of a drinking event) /8', 'Like this: /9 https://t.co/6qXU7Fg1fn', 'Technically speaking, the account is implemented as a probabilistic generative model. It takes the logical form of a sentence: \n\n∃x,y [astronomer(x)∧star(y)∧marry(x,y)]\n\nand generates the conceptual descriptions most likely to account for that logical form. /9', 'For those interested, please check out the pre-print 🙂https://t.co/klklQU9vJ3\n\nA jupyter notebook with working examples: https://t.co/2Dxx7vlz2w\n /10']",https://arxiv.org/abs/2009.07936,"In this paper, we derive a notion of 'word meaning in context' which accounts for the wide range of lexical shifts and ambiguities observed in utterance interpretation. We characterize the lexical comprehension process as a combination of cognitive semantics and Discourse Representation Theory, formalized as a 'situation description system': a probabilistic model which takes utterance understanding to be the mental process of describing one or more situations that would account for an observed utterance. Our model uses insights from different types of generative models to capture the interplay of local and global contexts and their joint influence upon the lexical representation of sentence constituents. We implement the system using a directed graphical model, and apply it to examples containing various contextualisation phenomena. ",How to marry a star: probabilistic constraints for meaning in context
65,1308043002816933888,759028003976470528,Dr. Kevin Cooke,['Our new paper using @SOFIAtelescope data to model a rare cold quasar (an AGN hosting galaxy that still has its cold dust component) is now accepted and out on arXiv!  Check it out to learn how galaxies and their supermassive black holes grow in tandem!\n\n<LINK>'],http://arxiv.org/abs/2009.08465,"Cold quasars are a rare subpopulation observed to host unobscured, X-ray luminous active galactic nuclei (AGN) while also retaining a cold gas supply fueling high star formation rates. These objects are interpreted as AGN early in their evolution. We present new SOFIA HAWC+ far-infrared observations, FUV-FIR photometry, and optical spectroscopy to characterize the accretion and star formation behavior in a cold quasar at z ~ 0.405 (CQ 4479). CQ 4479 is a starburst galaxy with a predominantly young stellar population and a high gas mass fraction of ~50-70%. The AGN component has yet to become the dominant component of the FIR emission. We also find AGN bolometric luminosity that varies as a function of observation method and AGN region probed. Finally, we identify a candidate outflow feature corroborating the hypothesis that cold quasars have energetic feedback. This object presents an intriguing look into the early stages of AGN feedback and probes the rare phase where an AGN and cold gaseous component co-exist. ",Dying of the Light: An X-ray Fading Cold Quasar at z ~ 0.405
66,1307950186724032513,21902101,Jim Geach,['New paper led by @DrAshleyNova on arXiv this morning: AstroVaDEr: Astronomical Variational Deep Embedder for Unsupervised Morphological Classification of Galaxies and Synthetic Image Generation \n\n<LINK>'],https://arxiv.org/abs/2009.08470,"We present AstroVaDEr, a variational autoencoder designed to perform unsupervised clustering and synthetic image generation using astronomical imaging catalogues. The model is a convolutional neural network that learns to embed images into a low dimensional latent space, and simultaneously optimises a Gaussian Mixture Model (GMM) on the embedded vectors to cluster the training data. By utilising variational inference, we are able to use the learned GMM as a statistical prior on the latent space to facilitate random sampling and generation of synthetic images. We demonstrate AstroVaDEr's capabilities by training it on gray-scaled \textit{gri} images from the Sloan Digital Sky Survey, using a sample of galaxies that are classified by Galaxy Zoo 2. An unsupervised clustering model is found which separates galaxies based on learned morphological features such as axis ratio, surface brightness profile, orientation and the presence of companions. We use the learned mixture model to generate synthetic images of galaxies based on the morphological profiles of the Gaussian components. AstroVaDEr succeeds in producing a morphological classification scheme from unlabelled data, but unexpectedly places high importance on the presence of companion objects---demonstrating the importance of human interpretation. The network is scalable and flexible, allowing for larger datasets to be classified, or different kinds of imaging data. We also demonstrate the generative properties of the model, which allow for realistic synthetic images of galaxies to be sampled from the learned classification scheme. These can be used to create synthetic image catalogs or to perform image processing tasks such as deblending. ","AstroVaDEr: Astronomical Variational Deep Embedder for Unsupervised
  Morphological Classification of Galaxies and Synthetic Image Generation"
67,1307913358411264003,766065198721564672,Tim Langen,['Always wanted to know what rotons look like? Check out this and many other insights in our new paper on fluctuations across the superfluid to #supersolid phase transition in dipolar #quantum gases!\n\n<LINK>\n\n@Uni_Stuttgart @AvHStiftung @IQSTpress <LINK>'],https://arxiv.org/abs/2009.08910,"Phase transitions share the universal feature of enhanced fluctuations near the transition point. Here we show that density fluctuations reveal how a Bose-Einstein condensate of dipolar atoms spontaneously breaks its translation symmetry and enters the supersolid state of matter -- a phase that combines superfluidity with crystalline order. We report on the first direct in situ measurement of density fluctuations across the superfluid-supersolid phase transition. This allows us to introduce a general and straightforward way to extract the static structure factor, estimate the spectrum of elementary excitations and image the dominant fluctuation patterns. We observe a strong response in the static structure factor and infer a distinct roton minimum in the dispersion relation. Furthermore, we show that the characteristic fluctuations correspond to elementary excitations such as the roton modes, which have been theoretically predicted to be dominant at the quantum critical point, and that the supersolid state supports both superfluid as well as crystal phonons. ","Density Fluctuations across the Superfluid-Supersolid Phase Transition
  in a Dipolar Quantum Gas"
68,1307701024220753921,156804540,Francisco Rodrigues,['Our new paper on @arxiv_org  is out: Discovering causal factors of drought in Ethiopia\n<LINK>\nWe employ causal inference to predict causal links between climate variables in order to improve the drought forecasting.  @WarwickComplex @icmc_usp <LINK>'],https://arxiv.org/abs/2009.07955,"Drought is a costly natural hazard, many aspects of which remain poorly understood. It has many contributory factors, driving its outset, duration, and severity, including land surface, anthropogenic activities, and, most importantly, meteorological anomalies. Prediction plays a crucial role in drought preparedness and risk mitigation. However, this is a challenging task at socio-economically critical lead times (1-2 years), because meteorological anomalies operate at a wide range of temporal and spatial scales. Among them, past studies have shown a correlation between the Sea Surface Temperature (SST) anomaly and the amount of precipitation in various locations in Africa. In its Eastern part, the cooling phase of El Nino-Southern Oscillation (ENSO) and SST anomaly in the Indian ocean are correlated with the lack of rainfall. Given the intrinsic shortcomings of correlation coefficients, we investigate the association among SST modes of variability and the monthly fraction of grid points in Ethiopia, which are in drought conditions in terms of causality. Using the empirical extreme quantiles of precipitation distribution as a proxy for drought, We show that the level of SST second mode of variability in the prior year influences the occurrence of drought in Ethiopia. The causal link between these two variables has a negative coefficient that verifies the conclusion of past studies that rainfall deficiency in the Horn of Africa is associated with ENSO's cooling phase. ",Discovering causal factors of drought in Ethiopia
69,1306934928765079554,1282679296843288577,Jonathan Ullman,"['Really pleased with this new paper with my PhD student Albert Cheu.  We prove strong lower bounds for two ""intermediate models"" of differential privacy: the shuffle model and the pan-private model. 1/3\n\n<LINK>', ""Our work builds in an essential way on Albert's awesome paper with Victor Balcer, @mgtjoseph, and Jieming Mao. 2/3\n\nhttps://t.co/F1jA6pN8KZ"", ""It's awesome to have such a productive and independent PhD student!  I can't believe he's graduating in the Spring.  I'm jealous of whoever gets to be his next boss and/or post-doc advisor! *wink wink* 3/3""]",https://arxiv.org/abs/2009.08000,"There has been a recent wave of interest in intermediate trust models for differential privacy that eliminate the need for a fully trusted central data collector, but overcome the limitations of local differential privacy. This interest has led to the introduction of the shuffle model (Cheu et al., EUROCRYPT 2019; Erlingsson et al., SODA 2019) and revisiting the pan-private model (Dwork et al., ITCS 2010). The message of this line of work is that, for a variety of low-dimensional problems -- such as counts, means, and histograms -- these intermediate models offer nearly as much power as central differential privacy. However, there has been considerably less success using these models for high-dimensional learning and estimation problems. In this work, we show that, for a variety of high-dimensional learning and estimation problems, both the shuffle model and the pan-private model inherently incur an exponential price in sample complexity relative to the central model. For example, we show that, private agnostic learning of parity functions over $d$ bits requires $\Omega(2^{d/2})$ samples in these models, and privately selecting the most common attribute from a set of $d$ choices requires $\Omega(d^{1/2})$ samples, both of which are exponential separations from the central model. Our work gives the first non-trivial lower bounds for these problems for both the pan-private model and the general multi-message shuffle model. ","The Limits of Pan Privacy and Shuffle Privacy for Learning and
  Estimation"
70,1306913556923912197,24371728,Richard G. Clegg,"['New paper on arxiv with @narnolddd @miratepuffin @imanehafnus @felixcuadrado analysing the structure of the interaction network from the alt-right Gab network. We look at the role of community and elites and the bursty nature of network growth. <LINK>', 'Our Raphtory tool https://t.co/1Cd6JmsIsL lets us look at a large data set over different time scales. We could see slow growth over years but rapid growth and contraction over days related to events of interest to the alt-right (e.g. Charlottesville)', ""For me the most interesting result was the network large connected component which encompasses most of the network in peak hours but fragments into isolated user pairs and small communities when US and Europe are asleep. I don't think this behaviour has been observed before."", '@PhilTeeNet Let us hope the referees agree (conference submission). The connected component result I am really happy with.', '@PhilTeeNet Paper submission? Accepted?']",https://arxiv.org/abs/2009.08322,"Gab is an online social network often associated with the alt-right political movement and users barred from other networks. It presents an interesting opportunity for research because near-complete data is available from day one of the network's creation. In this paper, we investigate the evolution of the user interaction graph, that is the graph where a link represents a user interacting with another user at a given time. We view this graph both at different times and at different timescales. The latter is achieved by using sliding windows on the graph which gives a novel perspective on social network data. The Gab network is relatively slowly growing over the period of months but subject to large bursts of arrivals over hours and days. We identify plausible events that are of interest to the Gab community associated with the most obvious such bursts. The network is characterised by interactions between `strangers' rather than by reinforcing links between `friends'. Gab usage follows the diurnal cycle of the predominantly US and Europe based users. At off-peak hours the Gab interaction network fragments into sub-networks with absolutely no interaction between them. A small group of users are highly influential across larger timescales, but a substantial number of users gain influence for short periods of time. Temporal analysis at different timescales gives new insights above and beyond what could be found on static graphs. ","Moving with the Times: Investigating the Alt-Right Network Gab with
  Temporal Interaction Graphs"
71,1306880525026635781,1288452767376445440,Patrick Kidger,"['New paper:\n""Neural CDEs for Long Time-Series via the Log-ODE Method""\nGitHub: <LINK>\narXiv: <LINK>\nReddit: <LINK>\n\nWe process very long time series of length up to 17k!\n\n1/ <LINK>', 'First if you\'re not familiar, then a neural CDE is a ""continuous-time RNN"". This means you get robustness to irregular data, memory-efficient backpropagation, and state-of-the-art performance.\nhttps://t.co/tN7mOr4i4i\nhttps://t.co/1L1JrVYQjk\nhttps://t.co/WkOgPypqac\n\n2/', 'Here, we show how to solve a neural CDE using a particular numerical solver from stochastic analysis, namely the ""log-ODE method"". This takes steps over *multiple data points at once*.\n\n3/', ""In machine learning terms we then reinterpret this quite straighforwardly: it's a very particular choice of binning strategy.\n\nThe bin statistics are carefully chosen to extract precisely what's most important for a Neural CDE.\n\n4/"", ""Finally, we've also got an implementation over in torchcde:\nhttps://t.co/WkOgPypqac\nthat allows you to use it easily.\n\nQuestions? Comments? Let me know.\n\n5/5"", '@unsorsodicorda Haha, no worries! We agree that learning rough analysis would a huge ask. With the original NCDE paper, we were really careful not to use rough analysis.\n\n1/', ""@unsorsodicorda It's definitely still inspired by rough analysis, but I think knowledge of that shouldn't be necessary to do more research on it, or to use it. I think (hope) it should be as easy to understand as the ODE-RNN paper.\n\n2/"", ""@unsorsodicorda For your next point about competitiveness - we found that NCDEs and RNNs are competitive with each other, despite ResNets usually beating NODEs. I think the reason for this is quite simple: there's a lot of tricks that work in ResNets that don't carry over to \n\n3/"", '@unsorsodicorda NODEs _or_ RNNs, so RNNs are an easier baseline to beat. :D\n\n4/', '@unsorsodicorda For example getting batch norm working in either NODEs or RNNs is a lot trickier than ResNets. Or with ResNets you can change weights and sizes easily between layers, but with NODEs and RNNs these things tend to be fixed.\n\n5/', '@unsorsodicorda You can try and find ways of fixing these things of course, and a few things have been used or proposed, but nothing really seems to have achieved broad acceptance.\n\n6/6', ""@unsorsodicorda Yep, that's the one!""]",https://arxiv.org/abs/2009.08295,"Neural controlled differential equations (CDEs) are the continuous-time analogue of recurrent neural networks, as Neural ODEs are to residual networks, and offer a memory-efficient continuous-time way to model functions of potentially irregular time series. Existing methods for computing the forward pass of a Neural CDE involve embedding the incoming time series into path space, often via interpolation, and using evaluations of this path to drive the hidden state. Here, we use rough path theory to extend this formulation. Instead of directly embedding into path space, we instead represent the input signal over small time intervals through its \textit{log-signature}, which are statistics describing how the signal drives a CDE. This is the approach for solving \textit{rough differential equations} (RDEs), and correspondingly we describe our main contribution as the introduction of Neural RDEs. This extension has a purpose: by generalising the Neural CDE approach to a broader class of driving signals, we demonstrate particular advantages for tackling long time series. In this regime, we demonstrate efficacy on problems of length up to 17k observations and observe significant training speed-ups, improvements in model performance, and reduced memory requirements compared to existing approaches. ",Neural Rough Differential Equations for Long Time Series
72,1306808417193918464,1033824122818555904,Michael Tucker,"['New paper posted to the arxiv about SN2019yvq, a verified weirdo: <LINK>\n\nnow accepting theories for 2019yvq that also disprove modern physics, bonus points if it also resolves H0 tension\n\nfeatures @BenShappee, @AstronomerPat, @styrofoamplates']",http://arxiv.org/abs/2009.07856,"We present new photometric and spectroscopic observations of SN 2019yvq, a Type Ia supernova (SN Ia) exhibiting several peculiar properties including an excess of UV/optical flux within days of explosion, a high SiII velocity, and a low peak luminosity. Photometry near the time of first light places new constraints on the rapid rise of the UV/optical flux excess. A near-infrared spectrum at $+173$ days after maximum light places strict limits on the presence of H or He emission, effectively excluding the presence of a nearby non-degenerate star at the time of explosion. New optical spectra, acquired at +128 and +150 days after maximum light, confirm the presence of CaII$\lambda 7300~$\r{A} and persistent CaII NIR triplet emission as SN 2019yvq transitions into the nebular phase. The lack of [OI]$\lambda 6300~$\r{A} emission disfavors the violent merger of two C/O white dwarfs (WDs) but the merger of a C/O WD with a He WD cannot be excluded. We compare our findings with several models in the literature postulated to explain the early flux excess including double-detonation explosions, $^{56}$Ni mixing into the outer ejecta during ignition, and interaction with H- and He-deficient circumstellar material. Each model may be able to explain both the early flux excess and the nebular [CaII] emission, but none of the models can reconcile the high photospheric velocities with the low peak luminosity without introducing new discrepancies. ",SN2019yvq Does Not Conform to SN Ia Explosion Models
73,1306790160152121344,1069244826,Preetum Nakkiran,"['New paper with @whybansal:\n""Distributional Generalization: A New Kind of Generalization""\n<LINK>\n\nThread 1/n\n\nHere are some quizzes that motivate our results (vote in thread!)\nQUIZ 1: <LINK>', 'QUIZ 1:\nConsider the binary-CIFAR setup above. What happens?\n(1) No sub-class affected much by noise (since only 3% total noise)\n(2) Test error across all animals (since no explicit cat label provided)\n(3) Test error only on cats (errors localized)\n\n2/n', 'bonus: what if you early-stop? Or train an MLP? 3/n', 'QUIZ 2:\nTrain two independent interpolating classifiers (e.g. ResNets) -- identically, but on disjoint subsets of CIFAR-10.\nSay their individual test accuracy is ~50% = Pr[ f1(x) = y ]\n\nWhat is the probability they agree with *each other* on the test set? Pr[ f1(x) = f2(x) ]\n\n4/n', 'One of the most interesting parts of this work (imo) is that for some distributions, Distributional Generalization is *incompatible* with classical generalization. And yet,  standard classifiers actually seem to distributionally generalize instead of classically generalizing (!)', ""@tmlabonte @whybansal Thanks! I gave a talk at the MPI seminar; video should be up at some point: https://t.co/5Lpbbcafss\n\nI'm also open to talk invites anytime."", '@_sam_sinha_ It’s in the paper! (specifically Experiment 1 in the Introduction)']",https://arxiv.org/abs/2009.08092,"We introduce a new notion of generalization -- Distributional Generalization -- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. Our formal conjectures, which are much more general than this example, characterize the form of distributional generalization that can be expected in terms of problem parameters: model architecture, training procedure, number of samples, and data distribution. We give empirical evidence for these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. Our results thus advance our empirical understanding of interpolating classifiers. ",Distributional Generalization: A New Kind of Generalization
74,1306779645287763970,1674028088,Misha Laskin 🇺🇦,"['New paper led by @astooke w/ @kimin_le2  &amp; @pabbeel - Decoupling Representation Learning from RL. First time RL trained on unsupervised features matches (or beats) end-to-end RL!\nPaper: <LINK>\nCode: <LINK>\nSite: <LINK>\n[1/N] <LINK>', 'Today, deep RL algos are trained by propagating the (supervised) reward signal through the entire network. This results in deep nets that learn task-specific features and limits the capabilities of RL. Sparse rewards -&gt; no features; Multi-task -&gt; train separate nets.\n[2/N]', ""Ideally, we would first extract reward-agnostic features with unsupervised learning (UL), and then train control algos on the features. This is how it's done in computer vision -&gt; SimClr, MoCo, CPC. First train UL, then train classifier on top.\n[3/N]"", 'But even the best UL for RL methods today (CURL, Dreamer, SLAC, UNREAL) are trained jointly end-to-end. The reward signal is still propagated through the *entire* network. The reason is that decoupling UL from RL results in less effective policies. \n[4/N]', 'We introduce Augmented Temporal Contrast (ATC). ATC trains the CNN encoder with a UL loss, and then learns an RL policy on top. ATC applies (a) data aug on the observations (b) a contrastive loss across temporally separated frames (c) data aug on the latent vectors.\n[5/N]', 'Results: (1) ATC is the *first* RL algorithm that matches or beats end-to-end RL on most DMControl, DMLab, and Atari envs tested.\n[6/N] https://t.co/RYMQwtqBzx', 'Results: (2) We benchmark different UL algos for RL -&gt; (a) collect expert data (b) pre-train the CNN with UL (c) freeze CNN encoder and train RL on top of the learned features. Result -&gt; ATC is *the SOTA* UL algorithm for RL on most envs across all 3 domains tested.\n[7/N] https://t.co/XPp93w6MCz', 'Results: (3) To see if ATC features can be transferred, we pre-train one encoder with ATC on multiple envs. Then we train RL on top of the features and evaluate performance across train and test envs. ATC is learns  *single encoder* to solve both train AND test envs!\n[8/N] https://t.co/bCOXsQZjW8', 'Qualitatively visualizing the activation of the CNN filters, we see that ATC learns features relevant to the task, such as the paddle and ball in Breakout and enemy sprites in Laser Tag. \n[9/N] https://t.co/Sqa6sBhsRP', 'Our code is built on top of rlpyt and open-sourced here: https://t.co/XAjhQRYC7Y\n[10/N]', 'To recap, ATC is the first algo to decouple UL from RL that matches or beats end-to-end RL, bringing UL for RL closer to the paradigm in computer vision where generic task-agnostic features are first learned, then evaluated on downstream tasks.\n[11/N]', 'EDIT: led by @stookemon']",https://arxiv.org/abs/2009.08319,"In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at this https URL ",Decoupling Representation Learning from Reinforcement Learning
75,1306777007527616512,3094610676,Pranav Rajpurkar,"['Excited to share our new work (paper+dataset) on predicting survival with cancer (DLBCL) using histology sections 🔬💭\n\nDLBCL-Morph <LINK>\n\nWith @dvrabac @AkshaySmit, Sebastian Fernandez-Pol\n\nRebecca Rojansky, @yaso_natkunam, Ranjana Advani, @AndrewYNg \n\n1/n <LINK>', 'First, some background: Diffuse Large B-Cell Lymphoma (DLBCL) is the most common non-Hodgkin lymphoma. Though histologically DLBCL\nshows varying morphologies, no morphologic features have been consistently demonstrated to correlate with prognosis!\n\n2/n', 'With Drs. Sebastian Fernandez-Pol,  Rebecca Rojansky, @yaso_natkunam, Ranjana H. Advani of @StanfordPath, we brainstormed in Jan 2020 whether AI methods may be used to potentially identify novel, prognostically significant morphological or immunohistochemical biomarkers.\n\n3/n', 'Here, we release a dataset of 209 DLBCL cases with histology sections and associated clinical and cytogenetic data. Tissue microarrays (TMAs) stained with H&amp;E and for expression of the 5 oncogenes: CD10, BCL6, MUM1, BCL2, and MYC.\n\nAnd annotated! How? Coming up next...\n\n4/n https://t.co/d10NPDVc5W', 'For TMAs, we include ROI annotations from to highlight the core regions which represent DLBCL accurately.\n\nAnnotating+organizing data of this nature is painstakingly tough. Huge effort by @dvrabac @AkshaySmit in coordination w/ Drs. Sebastian Fernandez-Pol &amp; Rebecca Rojansky\n\n5/n', 'Now the fun AI part! We used HoVer-Net to segment every tumor cell inside each of the patches from H&amp;E stained TMAs. No retraining on the data, just direct application, and worked. Sweet! \n\nRead more about HoVer-Net here: https://t.co/szjK18GqBY\n\n6/n https://t.co/ChaQv4ZB2P', 'We used the per-nucleus binary segmentation images to compute several geometric features for each tumor cell nucleus. While DL models may not require such hand-crafted features, prognostic models which use these features can give more explainable results!\n\nFun shape math.\n\n7/n https://t.co/guETDiI1mK', ""Wouldn't it be interesting if we could find relations between these geometric (cell nuclei) features and the survival outcome in our cohort?\n\nWe did! A Cox proportional-hazards model gets a c-index (95% CI) of 0.635 (0.574, 0.691) using only these features.\n\n8/n"", 'A dataset without documentation is hard to use. We have done our best of making the dataset accessible and easy to use, including code for the analysis. No login access required for data download. \n\nhttps://t.co/sBzMyuXNbd\n\n9/n https://t.co/0CtsD3PIna', 'Great effort by first authors @AkshaySmit @dvrabac 🥳🥳 on this paper\n\nThank you to Dr. Sebastian Fernandez-Pol @StanfordPath for amazing insights and collaboration throughout\n\nA great team w/ Rebecca Rojansky and mentors Drs. @yaso_natkunam, Ranjana Advani, @AndrewYNg 🙏\n\n10/n', ""The (freely available) dataset is called DLBCL-Morph, and I'm hopeful that this may help w/ many important research questions surrounding DLBCL and survival. Our paper (with link to data+code) is available here https://t.co/gBqJLFlwmV\n\n11/n"", '@xhluu Looking into it, hang tight!']",https://arxiv.org/abs/2009.08123,"Diffuse Large B-Cell Lymphoma (DLBCL) is the most common non-Hodgkin lymphoma. Though histologically DLBCL shows varying morphologies, no morphologic features have been consistently demonstrated to correlate with prognosis. We present a morphologic analysis of histology sections from 209 DLBCL cases with associated clinical and cytogenetic data. Duplicate tissue core sections were arranged in tissue microarrays (TMAs), and replicate sections were stained with H&E and immunohistochemical stains for CD10, BCL6, MUM1, BCL2, and MYC. The TMAs are accompanied by pathologist-annotated regions-of-interest (ROIs) that identify areas of tissue representative of DLBCL. We used a deep learning model to segment all tumor nuclei in the ROIs, and computed several geometric features for each segmented nucleus. We fit a Cox proportional hazards model to demonstrate the utility of these geometric features in predicting survival outcome, and found that it achieved a C-index (95% CI) of 0.635 (0.574,0.691). Our finding suggests that geometric features computed from tumor nuclei are of prognostic importance, and should be validated in prospective studies. ","DLBCL-Morph: Morphological features computed using deep learning for an
  annotated digital DLBCL image set"
76,1306775408080162818,2894733166,Amelia Fraser-McKelvie,"[""It's new paper day! SDSS-IV MaNGA: The link between bars and the early cessation of star formation in spiral galaxies. Or as I like to call it- Live fast and die young: the rock n' roll lives of barred galaxies. 🎸<LINK>"", 'With the fantastic help of @AstroMikeMerri @tompeterken @KarenLMasters  @astro_francesca @drbecky_ @NickyBfudd and others.', 'We know that barred galaxies are generally redder than the overall galaxy population, and we perform full spectral fitting on MaNGA data to confirm that the reason for this is not because they are dustier...', '...but that their stellar populations are older, more metal rich, and their star formation rates are lower. https://t.co/oI8qWfHnDQ', 'But your comparison sample is important here! We were careful to select non-barred galaxies of the same stellar mass and morphological features as our barred galaxies (including the same number of spiral arms, tightness of arm winding, and bulge size). Thank you @galaxyzoo!', 'Here are some pretty @sdssurveys images of the @MaNGASurvey galaxies used in this work (retrieved with @Marvin_SDSS): https://t.co/1r02oGb4x2', 'The cool thing about full spectral fitting is you can track the entire star formation history of a galaxy (not just what we can see now). We traced back the star formation activity and mass growth of our sample of barred galaxies...', '...and found that they built up their mass on average 400 Million years before the non-barred comparison sample. Their star formation activity peaked 1.7 Billion years earlier!', 'So, what does this all mean? Do bars kill galaxies? Well, we’re not sure yet. Option #1 is that bars act to speed up gas usage in galaxies by funnelling gas into the central galactic regions where it forms stars at a faster rate. There is plenty of observational evidence for this', 'Option #2 (and favoured by simulators) is that bars simply form more easily in passive disks. Without a method to age-date the bar, this is hard to determine. In essence, it is a ‘chicken or the egg’ problem. What came first: the passive galaxy, or the bar? 🐔🍳', 'In conclusion, there is a strong link between a galaxy having a lower star formation rate today and the presence of a bar. Which caused the other we don’t know. What we DO know is that barred galaxies lived faster and died younger than their non-barred counterparts!']",https://arxiv.org/abs/2009.07859,"Bars are common in low-redshift disk galaxies, and hence quantifying their influence on their host is of importance to the field of galaxy evolution. We determine the stellar populations and star formation histories of 245 barred galaxies from the MaNGA galaxy survey, and compare them to a mass- and morphology-matched comparison sample of unbarred galaxies. At fixed stellar mass and morphology, barred galaxies are optically redder than their unbarred counterparts. From stellar population analysis using the full spectral fitting code Starlight, we attribute this difference to both older and more metal-rich stellar populations. Dust attenuation however, is lower in the barred sample. The star formation histories of barred galaxies peak earlier than their non-barred counterparts, and the galaxies build up their mass at earlier times. We can detect no significant differences in the local environment of barred and un-barred galaxies in this sample, but find that the HI gas mass fraction is significantly lower in high-mass ($\rm{M}_{\star} > 10^{10}~\rm{M}_{\odot}$) barred galaxies than their non-barred counterparts. We speculate on the mechanisms that have allowed barred galaxies to be older, more metal-rich and more gas-poor today, including the efficient redistribution of galactic fountain byproducts, and a runaway bar formation scenario in gas-poor disks. While it is not possible to fully determine the effect of the bar on galaxy quenching, we conclude that the presence of a bar and the early cessation of star formation within a galaxy are intimately linked. ","SDSS-IV MaNGA: The link between bars and the early cessation of star
  formation in spiral galaxies"
77,1306711032119701504,36653441,Johan Ugander,"['Long new paper, ""Randomized Graph Cluster Randomization"", with Hao Yin. <LINK> Building on earlier work on graph cluster randomization (GCR) fo causal inference on networks, we propose and study designs that use a randomized graph clustering instead! 🧵 1/n <LINK>', 'The global average treatment effect (GATE) estimand asks for the difference between when a network is all-treated vs. all-control. Under SUTVA, the GATE is the ATE. Without SUTVA, we use ""exposure models"" (assumptions about when someone is ""as if"" all-treated).  2/n', ""Aronow &amp; @cdsamii's 2017 AOAS paper is a great starting point for reading up on beyond-SUTVA causal inference on networks: https://t.co/T0LwtDs3bf A pre-print from ~2012 had a strong influence on me during my PhD! 3/n"", 'GCR creates a single fixed clustering of the interference graph and then randomizes treatment/control at the cluster level. Developed in this KDD13 paper https://t.co/TjV2TcPU8X and JCI17 paper: https://t.co/DFPz3T1pQI 4/n', 'Randomized GCR (RGCR) started as an attempt to solve the following problem: in GCR with a fixed clustering, sometimes nodes get unlucky. Mixing up the clustering ""smooths out"" the exposure probability by smoothing out the luck. 5/n https://t.co/K6nsW4136R', ""One of the positive results from the GCR work was that, under a restricted growth condition on the network (ball sizes don't grow too fast), the variance of a GATE estimate could be upper bounded by a quantity polynomial in the max degree (iid design is exponential in it). 6/n https://t.co/kPJaqnikkm"", ""Under RGCR, that bound becomes polynomial in max degree _and_ in the growth parameter! That's great, because I've known for a while that the growth assumption only holds for very large kappa. So, huge difference! 7/n https://t.co/70eZrwxkUw"", 'The paper has a whole appendix (A) that, to my knowledge, is the first detailed look at the empirics of how ball sizes grow in empirical social networks. The FB100 networks are small enough (and Hao a 31337 enough coder) that we were pretty easily able to crank these out. 8/n https://t.co/HuI4gEhro0', 'Growth conditions have been showing up more in the network experimentation literature recently, so it feels \nimportant to understand how reasonable they are for varied interference networks (above, friendship networks). E.g: https://t.co/snSsTB6azI &amp; https://t.co/6HtQf7ViM3 9/n', 'For RGCR, we propose and analyze two simple algorithms: randomized 3-net and a new-ish algorithm we call 1-hop-max. Aaron Sidford helped us spot that 1-hop-max is closely related to the CKR and FRT algorithms from the lit on metric approximation. Neat! 10/n https://t.co/J1H0wovldV', 'Theorems are nice, but what happens in practice? Well, GCR hides some had constants (p^{-kappa^6}), and when you start randomizing GCR (K&gt;1 in plot below), variance goes way way down. Notice y-axes scale. 11/n https://t.co/x40gQhXDap', 'Weighted variations on our randomized algorithms do slightly better still. One weighting (""spectral"") assigns weights to balance out exposure probabilities as part of an optimization heuristic that has a nice spectral solution. Fun! 12/n https://t.co/VRAVCi0CGM', 'Returning to the first plot of the 🧵, we study both HT and Hajek estimators of the GATE. The bias (of Hajek) and variance (of both) depend on network size/properties. RGCR w/ Hajek really shines when networks get large (here a growing family of ""small world++"" networks). 13/n https://t.co/Xs0YuTRvVe', 'Our simulations try to incorporate all the important challenges we see in network experimentation. Heavy-tailed degree distribution, response correlated with degree, response homophily (shown below), etc. ""As simple as possible, but not simpler""  is still complicated! 14/n https://t.co/pvbWLaVEFq', 'There are also a lot of open directions. Come up with new (or import known) randomized clustering algos that reduce variance well (in theory and/or practice). Expand on our study of the ball-structure of empirical networks to capture the important. Could go on! 15/n', ""This paper has been a labor of love. Hao presented a preliminary version at CODE@MIT last year. We had a draft with most of the pieces then, but it took 10 months to actually finish it. We'd would love feedback from anyone who takes a look! 16/n"", ""To end, a hat-tip to the late Stephen Fienberg. While finishing this paper, I was reminded of the workshop he organized at CMU in 2013 and this review essay of his: https://t.co/KKHffjeQXd Network experimentation is hard. We've come a long way, but still lots to do. RIP. 17/17 https://t.co/kOpmVOgZkd""]",https://arxiv.org/abs/2009.02297,"The global average treatment effect (GATE) is a primary quantity of interest in the study of causal inference under network interference. With a correctly specified exposure model of the interference, the Horvitz-Thompson (HT) and H\'ajek estimators of the GATE are unbiased and consistent, respectively, yet known to exhibit extreme variance under many designs and in many settings of interest. With a fixed clustering of the interference graph, graph cluster randomization (GCR) designs have been shown to greatly reduce variance compared to node-level random assignment, but even so the variance is still often prohibitively large. In this work we propose a randomized version of the GCR design, descriptively named randomized graph cluster randomization (RGCR), which uses a random clustering rather than a single fixed clustering. By considering an ensemble of many different cluster assignments, this design avoids a key problem with GCR where a given node is sometimes ""lucky"" or ""unlucky"" in a given clustering. We propose two randomized graph decomposition algorithms for use with RGCR, randomized 3-net and 1-hop-max, adapted from prior work on multiway graph cut problems. When integrating over their own randomness, these algorithms furnish network exposure probabilities that can be estimated efficiently. We develop upper bounds on the variance of the HT estimator of the GATE under assumptions on the metric structure of the interference graph. Where the best known variance upper bound for the HT estimator under a GCR design is exponential in the parameters of the metric structure, we give a comparable variance upper bound under RGCR that is instead polynomial in the same parameters. We provide extensive simulations comparing RGCR and GCR designs, observing substantial reductions in the mean squared error for both HT and H\'ajek estimators of the GATE in a variety of settings. ",Randomized Graph Cluster Randomization
78,1306690324215996416,1008944276431036416,Boris Ivanovic,"['New paper up on arXiv with Amine Elhafsi, Guy Rosman, @adnothing, @MarcoPavoneSU!! In it, we propose a new multi-agent trajectory forecasting output representation that is much more amenable to downstream planning and control algorithms. Check it out at <LINK>! <LINK>']",https://arxiv.org/abs/2009.07517,"Reasoning about human motion is a core component of modern human-robot interactive systems. In particular, one of the main uses of behavior prediction in autonomous systems is to inform robot motion planning and control. However, a majority of planning and control algorithms reason about system dynamics rather than the predicted agent tracklets (i.e., ordered sets of waypoints) that are commonly output by trajectory forecasting methods, which can hinder their integration. Towards this end, we propose Mixtures of Affine Time-varying Systems (MATS) as an output representation for trajectory forecasting that is more amenable to downstream planning and control use. Our approach leverages successful ideas from probabilistic trajectory forecasting works to learn dynamical system representations that are well-studied in the planning and control literature. We integrate our predictions with a proposed multimodal planning methodology and demonstrate significant computational efficiency improvements on a large-scale autonomous driving dataset. ","MATS: An Interpretable Trajectory Forecasting Representation for
  Planning and Control"
79,1306626047383924737,82497649,Moin Nadeem,"['New AACL paper!\n\nSampling from a language model is a crucial task for generation. While many sampling algorithms exist, what properties are desirable in a good sampling algorithm? 🧐\n\nJoint work w/ @TianxingH, @kchonyc, Jim Glass, and I\nPaper: <LINK>\nThread 👇', 'What makes the current sampling algorithms (top-k, nucleus, tempered) perform well? \n\nWe inspected them and extracted three shared properties. All algs reduce entropy of the distribution, preserve the relative order of the logits, and preserve the “slope” of the distribution. https://t.co/vMct1pdl8s', 'We design two algorithms that satisfy these properties, and three algorithms that violate these properties. \n\nMost interestingly, we find that we can design new sampling algorithms that satisfy these properties, and obtain competitive performance 😲 https://t.co/FsuhW7eXV0', 'Conversely, we find that two of the three algorithms that violate these properties yields significant performance degradation. \n\nFor our random masked algorithm, we were surprised to see that randomly masking logits (other than the first) yields similar performance! 🤔 https://t.co/0ZoMOl6d4G', 'We acknowledge the empirical limitations of our study, and emphasize that it is entirely possible for some crucial property that we have not discovered to exist!\n\nWe are hopeful that this study may help guide the development of novel sampling algorithms in the future.']",https://arxiv.org/abs/2009.07243,"This work studies the widely adopted ancestral sampling algorithms for auto-regressive language models, which is not widely studied in the literature. We use the quality-diversity (Q-D) trade-off to investigate three popular sampling algorithms (top-k, nucleus and tempered sampling). We focus on the task of open-ended language generation. We first show that the existing sampling algorithms have similar performance. After carefully inspecting the transformations defined by different sampling algorithms, we identify three key properties that are shared among them: entropy reduction, order preservation, and slope preservation. To validate the importance of the identified properties, we design two sets of new sampling algorithms: one set in which each algorithm satisfies all three properties, and one set in which each algorithm violates at least one of the properties. We compare their performance with existing sampling algorithms, and find that violating the identified properties could lead to drastic performance degradation, as measured by the Q-D trade-off. On the other hand, we find that the set of sampling algorithms that satisfies these properties performs on par with the existing sampling algorithms. Our data and code are available at this https URL ","A Systematic Characterization of Sampling Algorithms for Open-ended
  Language Generation"
80,1306623010275696641,257287707,Victor Zhong,"['Our #emnlp2020 paper Grounded Adaptation for Zero-shot Semantic Parsing proposes GAZP, a framework for zero-shot language-to-SQL parsing by synthesizing data in new DBs after reading their schema. 💡\n\nPaper 📰 <LINK>\nThread 👇1/9\n\n#NLProc #MachineLearning <LINK>', 'Most semantic parsers are learned for a single target DB. How do we create parsers that generalize to new databases without training data? 🧐 2/9', 'Suppose we create a system that produces SQL queries given user requests and train it on an academic DB about students and schools 🏫. How do we make the same system perform well when deployed to a new production sales DB 🤑 for which we have no training data? 3/9', 'In GAZP, given a training DB and its supervised data, we train a schema-reading forward parser ⏩ that generates SQL given an utterance and a schema-reading backward utterance generator ⏪ that generates an utterance given SQL. 4/9 https://t.co/NuWsvqEn4B', 'Next, given a new inference DB, we sample SQL queries according to its content and generate corresponding utterances using the backward generator ⏪. 5/9 https://t.co/X2hKCYXT5L', 'Then, we parse ⏩ the generated utterance and keep the synthesized (utterance, SQL) pair if the parsed and originally sampled SQL are consistent (e.g. executes to the same result).  Finally, we adapt the parser to the new DB by training on consistent, synthesized data. 6/9', 'Using this method, we improves results on the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks (shoutout to @taoyds and @ryanzhumich for their hard work on these datasets). 7/9 https://t.co/5CHU9W4sRs', 'Moreover, we find that \n1. grounded adaptation to inference DB beats data-augmentation on training DB (e.g. synthesizing data in the training DB)\n2. performance improves with more synthesized adaptation data\n3. verifying cycle-consistency is crucial to successful adaptation 8/9 https://t.co/Ui7SSIma3T', 'This was a long time coming! Thanks to my awesome collaborators @ml_perception @sidawxyz @LukeZettlemoyer, as well as wonderful @emnlp2020 reviewers + chairs who were engaging and provided constructive feedback. Also thanks to @_julianmichael_ for very helpful discussions! 9/9']",https://arxiv.org/abs/2009.07396,"We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation. ",Grounded Adaptation for Zero-shot Executable Semantic Parsing
81,1306572504887111683,169369247,Dripto Debroy,"['New paper! \n\nIn this work we create flag-style gadgets which can detect errors on NISQ algorithms, allowing for improved performance under postselection. These low-overhead ideas may be helpful for algorithms being run on machines too small for QEC.\n\n<LINK> <LINK>', ""I'm particularly excited because this is my first project where the core idea was one I came up. I scribbled Fig. 2b on one of the scratch pads at the QEC19 poster session after a lunch with our group and @JarrodMcclean. Joint work w/ @kenbrownquantum""]",https://arxiv.org/abs/2009.07752,"Flag verification techniques are useful in quantum error correction for detecting critical faults. Here we present an application of flag verification techniques to improving post-selected performance of near-term algorithms. We extend the definition of what constitutes a flag by creating error-detection gadgets based on known transformations of unitary operators. In the case of Clifford or near-Clifford circuits, these unitary operators can be chosen to be controlled Pauli gates, leading to gadgets which require only a small number of additional Clifford gates. We show that such flags can improve circuit fidelities by up to a factor of 2 after post selection, and demonstrate their effectiveness over error models featuring single-qubit depolarizing noise, crosstalk, and two-qubit coherent overrotation. ",Extended flag gadgets for low-overhead circuit verification
82,1306541068901588993,261658198,Juan Cruz-Benito,"['New paper from our #IBMQuantum Cloud Team c/ Sanjay Vishwakarma, @pacomartinfdez, and @ismaelfaro\n\n<LINK>', '👆We compare different neural network architectures like AWD-LSTMs, AWD-QRNNs and Transformer, while using transfer learning, and different tokenizations to see how they behave in building language models using a Python dataset for automated code generation and filling mask tasks']",https://arxiv.org/abs/2009.07740,"In recent years, the use of deep learning in language models gained much attention. Some research projects claim that they can generate text that can be interpreted as human-writing, enabling new possibilities in many application areas. Among the different areas related to language processing, one of the most notable in applying this type of modeling is programming languages. For years, the Machine Learning community has been researching this software engineering area, pursuing goals like applying different approaches to auto-complete, generate, fix, or evaluate code programmed by humans. Considering the increasing popularity of the Deep-Learning-enabled language models approach, we detected a lack of empirical papers that compare different deep learning architectures to create and use language models based on programming code. This paper compares different neural network architectures like AWD-LSTMs, AWD-QRNNs, and Transformer while using transfer learning and different tokenizations to see how they behave in building language models using a Python dataset for code generation and filling mask tasks. Considering the results, we discuss each approach's different strengths and weaknesses and what gaps we find to evaluate the language models or apply them in a real programming context. ","Automated Source Code Generation and Auto-completion Using Deep
  Learning: Comparing and Discussing Current Language-Model-Related Approaches"
83,1306540733407604737,631114017,Knud Jahnke,"[""Paper day by @mpi_astro's Irham Taufik Andika (@irhamta, incl. myself): <LINK>\n\nIrham found two new high redshift #quasars at z&gt;6, one of them (PSO J083.8371+11.8482) at z=6.34, which is a very special #QSO (even though it's looking inconspicuously):\n\n1/14 <LINK>"", ""It's a so-called #weakline #quasar, meaning that the CIV and Lyα broad emission lines are nearly absent. Only the lower ionisation MgII line can be actually seen.\n\nHere the black line is the measured quasar spectrum, the blue line a comparison to a typical z&gt;5.7 composite.\n\n2/ https://t.co/L303JAO63P"", 'Weak-line quasars deviate from the standard picture of #accretion onto #supermassive #blackholes since some of their broad-line emission region seems to be a) not fully formed yet, or b) has not been radiated by the central accretion disk.\n\n3/', 'So weak-line quasars could tell us about special, possibly young phases of quasar formation or episodic accretion. What is interesting is that PSO J083 has a seemingly young accretion age as an analysis of the ""proximity zone"" shortward of Lyα suggests:\n\n4/ https://t.co/6xRsMMtcfk', ""The proximity zone at high redshifts is the region where the quasar's own radiation has ionized hydrogen, which in turn will not absorb as strongly as the neutral hydrogen further away. Hence the size of the proximity zone can be used as an upper limit of quasar age.\n\n5/"", 'See e.g. https://t.co/GEXByCkgUW\n\nEven though there are caveats (to be explored in a future paper using @ESO #MUSE data) at face value PSO J083 would be a very young quasar, only 10³ to 10⁴ years old:\n\n6/ https://t.co/ollZvX1SHX', 'There is no model for a 10⁹ M_sun black hole to form in this short time (central conundrum in supermassive black hole formation), so what is going on? Could the young age be connected to an incompletely formed broad line region, i.e. the weak-line quasar nature?\n\n7/', '.@irhamta also checked whether any of the derived black hole mass and accretion rate values could be biased by #gravitationallensing, using @NASAHubble (WFC3 NIR and ACS narrow band). But that showed negligible impact:\n\n8/ https://t.co/URxsNCIFxn', 'And then aside from Magellan, Gemini, and HST @irhamta also managed to get @almaobs data to look at dust continuum (left) and [CII] (integrated, right):\n\n9/ https://t.co/Jm88LU3Anr', 'And [CII] is amazingly bright, is extended, and shows a velocity gradient (left):\n\n10/ https://t.co/3PxTiFasGr', 'Both dust and [CII] point to an extreme star formation rate, 2000-2500 M_sun/yr, and the total infrared is &gt;10¹³ L_sun, making this a ""HyLIRG"" and extreme even for the early Universe:\n\n11/ https://t.co/WiaYekM3Kx', 'The discussion comes to a conclusion that possibly young age, extreme star formation and weak lines could be related. Favoured interpretation is that the broad line region is not yet fully formed around a youngish-ly accreting supermassive black hole\n\n12/', 'So this could be a situation that we could encounter more often once we find earlier and earlier supermassive black holes. Check out the full 28 (!) page paper on the @arxiv and congratulations to @irhamta and the team for this complex analysis!\n\nhttps://t.co/fW5zXD4hI3\n\n13/', 'Team:\n@irhamta\n@knudjahnke\nMasafusa Onoue\nEduardo Bañados\n@ChiaraMazzucch2\nMladen Novak\nAnna-Christina Eilers\nBram Venemans\nJan-Torge Schindler\nFabian Walter\nMarcel Neeleman\nRob Simcoe\n@deky740\nEmanuele Farina\nVictor Marian,\nAntonio Pensabene\nThomas M. Cooper\nAlejandra Rojas\n\nEnd']",https://arxiv.org/abs/2009.07784,"We present the discovery of PSO J083.8371+11.8482, a weak emission line quasar with extreme star formation rate at $z=6.3401$. This quasar was selected from Pan-STARRS1, UHS, and unWISE photometric data. Gemini/GNIRS spectroscopy follow-up indicates a MgII-based black hole mass of $M_\mathrm{BH}=\left(2.0^{+0.7}_{-0.4}\right)\times10^9~M_\odot$ and an Eddington ratio of $L_\mathrm{bol}/L_\mathrm{Edd}=0.5^{+0.1}_{-0.2}$, in line with actively accreting supermassive black hole (SMBH) at $z\gtrsim6$. HST imaging sets strong constraint on lens-boosting, showing no relevant effect on the apparent emission. The quasar is also observed as a pure point-source with no additional emission component. The broad line region (BLR) emission is intrinsically weak and not likely caused by an intervening absorber. We found rest-frame equivalent widths of EW(Ly$\alpha$+NV) $=5.7\pm0.7$ Angstrom, EW(CIV) $\leq5.8$ Angstrom (3-sigma upper limit), and EW(MgII) $=8.7\pm0.7$ Angstrom. A small proximity zone size ($R_\mathrm{p}=1.2\pm0.4$ pMpc) indicates a lifetime of only $t_\mathrm{Q}=10^{3.4\pm0.7}$ years from the last quasar phase ignition. ALMA shows extended [CII] emission with a mild velocity gradient. The inferred far-infrared luminosity ($L_\mathrm{FIR}=(1.2\pm0.1)\times10^{13}\,L_\odot$) is one of the highest among all known quasar hosts at $z\gtrsim6$. Dust and [CII] emissions put a constraint on the star formation rate of SFR $=900-4900~M_\odot\,\mathrm{yr^{-1}}$, similar to that of hyper-luminous infrared galaxy. Considering the observed quasar lifetime and BLR formation timescale, the weak-line profile in the quasar spectrum is most likely caused by a BLR which is not yet fully formed rather than continuum boosting by gravitational lensing or a soft continuum due to super-Eddington accretion. ","Probing the Nature of High Redshift Weak Emission Line Quasars: A Young
  Quasar with a Starburst Host Galaxy"
84,1306512695672406021,23340278,Alexander Koller,"['Check out our new paper on fast (1-2k tokens/sec on CPU, 10k/sec on GPU) and accurate semantic parsing into graphs! Includes nontrivial, but provably dead-end-free transition system. #emnlp2020\n\nPaper: <LINK> \n\nOnline demo: <LINK>']",https://arxiv.org/abs/2009.07365,"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy. ",Fast semantic parsing with well-typedness guarantees
85,1306286933614624769,764979674,Jean-Francois Rajotte,['My first paper with the @UBCDSI in my new field (#SyntheticData #privacy #FederatedLearning) is out <LINK> to be presented at @PriSEM_WS. It is a decentralized application of PrivGAN by @SMukherjee89 @BDataScientist Yixi Xu and @anurive'],https://arxiv.org/abs/2009.06764,"More data is almost always beneficial for analysis and machine learning tasks. In many realistic situations however, an enterprise cannot share its data, either to keep a competitive advantage or to protect the privacy of the data sources, the enterprise's clients for example. We propose a method for data owners to share synthetic or fake versions of their data without sharing the actual data, nor the parameters of models that have direct access to the data. The method proposed is based on the privGAN architecture where local GANs are trained on their respective data subsets with an extra penalty from a central discriminator aiming to discriminate the origin of a given fake sample. We demonstrate that this approach, when applied to subsets of various sizes, leads to better utility for the owners than the utility from their real small datasets. The only shared pieces of information are the parameter updates of the central discriminator. The privacy is demonstrated with white-box attacks on the most vulnerable elments of the architecture and the results are close to random guessing. This method would apply naturally in a federated learning setting. ","Private data sharing between decentralized users through the privGAN
  architecture"
86,1306250030047731713,811646204635287552,Jacob Buckman,"['You\'ve heard about the ""optimism principle"" in RL, right? Well, turns out that it has a sibling: the *pessimism* principle. Find out more in our new paper (w/ @carlesgelada, @marcgbellemare): <LINK>\n\nMini-thread below! 1/ <LINK>', 'Optimism helps us *explore*. An optimistic agent will do things that it is uncertain about, helping it to learn more. On the other hand, pessimism helps us *exploit*. In order to guarantee good performance, stick to what we know. 2/', 'In RL, one must balance exploration and exploitation. Our paper is focused on a setting that can be viewed as *pure exploitation*: the fixed-dataset policy optimization setting. (This is also known as ""batch RL"" or ""offline RL"", although I think those names are bad.) 3/', 'In our paper, we show why non-pessimistic algorithms fail in this setting. Intuition: there are a ton of possible policies, so we will almost-certainly overestimate at least one. When we do, we will pick it, and it will turn out to be bad. Thus, we will pick a poor policy. 4/', 'Pessimism fixes this: by intentionally under-estimating our predictions, we can avoid the problematic overestimations and wind up with better performance. We show this mathematically by proving guarantees on the suboptimality of both naive and pessimistic algorithms. 5/', 'We show how existing algorithms already leverage the pessimism principle. There is a whole family of ""proximal algorithms"", which implement pessimism by constraining the learned policy to be nearby to the empirical policy. Turns out, almost every existing algorithm does this! 6/', ""Also, we show that there is another family of pessimistic algorithms which are strictly superior. However, they aren't implementable (yet), because they require us to estimate the epistemic uncertainty of a neural network -- something nobody knows how to do. 7/"", 'This work definitively poses ""principled NN uncertainty estimation"" as one of the key fundamental questions of our time. Uncertainty is the key to good performance in decision problems. Without it, we are doomed to doing essentially imitation learning. 8/', 'I am very proud of this paper, which took us almost two years to put together. I believe that it paints a full conceptual and mathematical picture of all the important ideas in this problem setting. 9/', 'One of my primary goals was to write a theory paper that is useful and insightful for the DRL community. I think that this paper is a must-read for anyone working on deep learning approaches to offline RL/batch RL/FDPO. \nTheoreticians may also find it interesting. 10/', 'Would love to hear any thoughts or feedback! 11/11', ""@recurseparadox I hypothesize that ensembles don't work -- they only work better than the other non-rigorous hacks people have tried. A principled uncertainty estimation technique will blow ensembles out of the water."", ""@recurseparadox This is obviously an unfalsifiable claim, so it's fine if you are not persuaded. But just look at the guarantees we can get from concentration inequalities. That's the level of statistical power I anticipate from principled uncertainties. Ensembles are nowhere close."", '@recurseparadox Challenging, certainly! All the more reason to answer it properly.\n\nIMO, characterizing ""the SGD posterior"" is not needed. We should focus on the result of th optimization, not the trajectory. (Similar to what we did in this paper!) Simply: what guarantees does a trained NN have?', '@recurseparadox SGD is one way to find these objects, and it is the way we currently know how to do. But the uncertainty guarantees should not be reliant on SGD.']",https://arxiv.org/abs/2009.06799,"We study worst-case guarantees on the expected return of fixed-dataset policy optimization algorithms. Our core contribution is a unified conceptual and mathematical framework for the study of algorithms in this regime. This analysis reveals that for naive approaches, the possibility of erroneous value overestimation leads to a difficult-to-satisfy requirement: in order to guarantee that we select a policy which is near-optimal, we may need the dataset to be informative of the value of every policy. To avoid this, algorithms can follow the pessimism principle, which states that we should choose the policy which acts optimally in the worst possible world. We show why pessimistic algorithms can achieve good performance even when the dataset is not informative of every policy, and derive families of algorithms which follow this principle. These theoretical findings are validated by experiments on a tabular gridworld, and deep learning experiments on four MinAtar environments. ",The Importance of Pessimism in Fixed-Dataset Policy Optimization
87,1306244485928620032,389221031,Pavel Mancera Piña,"['Paper day! I am super excited to share with everyone my new paper <LINK>. Once again, surrounded by amazing collaborators, and written and developed in the fantastic environment of the Kapteyn Institute. <LINK>', 'Long story short, we complemented the SPARC sample with a number of dwarfs with high quality rotation curves to study the baryonic specific angular momentum of disc galaxies, arguably one of the most important parameters in galaxy formation and evolution. About the sample: https://t.co/h2ope7HncR', 'We paid a lot of attention trying to deriving everything as robustly as possible. Among other things, we only use galaxies with a convergent specific angular momentum cumulative profile, and we correct the rotation curves for asymmetric drift (both gas and stellar).', 'And now, straight to the point, these is how the mass-specific angular momentum (j-M) relations look like! For stars, cold gas and baryons. The three are unbroken power laws, with massive and dwarf galaxies lying along the same sequences (opposite to some previous findings). https://t.co/fYkJr8OOKf', 'We provide the best-fitting parameters for the power laws, as well as the individual measurements for all our sample (157 galaxies). In the paper we also look for systematic residuals in the j-M laws. We find that at fixed M* galaxies with high j* have a larger disc scale length.', 'Also, at fixed Mbar, gas-poor galaxies have lower jbar than expected. We briefly discuss why we think this is happening, and we also show the three j-M laws adding the disc scale length and the gas fraction as third parameters.', 'Finally, we use our measurements to study the ""retained fraction of angular momentum"", fj, which is just the ratio between the baryonic specific angular momentum of the galaxies and that of their dark matter halo, fj = jbar/jDM.', 'We find that a constant value of fj = 70% for all discs is enough to reproduce the observed baryonic j-M law in a LCDM context, assuming a linear stellar-to-halo mass relation (see work by L. Posti). This is, imho, quite remarkable, and you can see it in this figure: https://t.co/IGf7srDYNm', 'As an extra, if you are looking for robust HI kinematics of dwarf galaxies besides, for instance, LITTLE THINGS, we derive 3D kinematic models for a sample of dwarfs from the WHISP, VLA-ANGST and LVHIS samples! https://t.co/aSY8wljlxV', ""And I think that's it! I had lots of fun working in this project, building up in the nice work by L. Posti (and others of course) on the stellar j-M laws. I surely learn a lot and I hope you will find this as cool and interesting as I find it! Please have a look!""]",https://arxiv.org/abs/2009.06645,"(Abridged) Specific angular momentum is one of the key parameters that control the evolution of galaxies. We derive the baryonic specific angular momentum of disc galaxies and study its relation with the dark matter specific angular momentum. Using a combination of high-quality HI rotation curves and HI/near-IR surface densities, we homogeneously measure the stellar ($j_{\rm *}$) and gas ($j_{\rm gas}$) specific angular momenta for a large sample of local disc galaxies. This allows us to determine the baryonic specific angular momentum ($j_{\rm bar}$) with high accuracy and across a very wide range of masses. The $j_{\ast}-M_\ast$ relation is an unbroken power-law from $7 \lesssim$ log($M_\ast$/$M_\odot) \lesssim 11.5$, with slope $0.54 \pm 0.02$. For the gas component, we find that the $j_{\rm gas}-M_{\rm gas}$ relation is also an unbroken power-law from $6 \lesssim$ log($M_{\rm gas}$/$M_\odot) \lesssim 11$, with a steeper slope of $1.02 \pm 0.04$. Regarding the baryonic relation, our data support a correlation characterized by single power-law with slope $0.60 \pm 0.02$. Our most massive spirals and smallest dwarfs lie along the same $j_{\rm bar}-M_{\rm bar}$ sequence. While the relations are tight and unbroken, we find internal correlations inside them: At fixed $M_\ast$, galaxies with larger $j_\ast$ have larger disc scale lengths, and at fixed $M_{\rm bar}$, gas-poor galaxies have lower $j_{\rm bar}$ than expected. We estimate the retained fraction of baryonic specific angular momentum, finding it constant across our entire mass range with a value of $\sim 0.6$, indicating that the $j_{\rm bar}$ of present-day disc galaxies is comparable to the initial specific angular momentum of their dark matter haloes. These results set important constraints for hydrodynamical simulations and semi-analytical models aiming to reproduce galaxies with realistic specific angular momenta. ",The baryonic specific angular momentum of disc galaxies
88,1306242706222518272,1258771704995815424,Beren Millidge,"['Excited to announce a new preprint: ""Activation Relaxation: A Local, Dynamical Approximation to Backprop in the Brain. ""\n\npaper: <LINK>\ncode: <LINK>\n\nwith @a_tschantz , @anilkseth, and @drclbuckley', 'We derive a novel learning algorithm which can converge to exact backprop gradients using only local and Hebbian learning rules, and we demonstrate performance equivalent with backprop when training deep neural networks.', 'The algorithm is very straightforward (approx 20 LOC, see code) and is based on the idea of conceptualising the true backprop gradients as the equilibrium point of a dynamical system.', 'Finally, we show how some of the remaining biological implausibilities in this model (such as the weight transport problem) can be ameliorated without undue harm to learning performance.']",https://arxiv.org/abs/2009.05359,"The backpropagation of error algorithm (backprop) has been instrumental in the recent success of deep learning. However, a key question remains as to whether backprop can be formulated in a manner suitable for implementation in neural circuitry. The primary challenge is to ensure that any candidate formulation uses only local information, rather than relying on global signals as in standard backprop. Recently several algorithms for approximating backprop using only local signals have been proposed. However, these algorithms typically impose other requirements which challenge biological plausibility: for example, requiring complex and precise connectivity schemes, or multiple sequential backwards phases with information being stored across phases. Here, we propose a novel algorithm, Activation Relaxation (AR), which is motivated by constructing the backpropagation gradient as the equilibrium point of a dynamical system. Our algorithm converges rapidly and robustly to the correct backpropagation gradients, requires only a single type of computational unit, utilises only a single parallel backwards relaxation phase, and can operate on arbitrary computation graphs. We illustrate these properties by training deep neural networks on visual classification tasks, and describe simplifications to the algorithm which remove further obstacles to neurobiological implementation (for example, the weight-transport problem, and the use of nonlinear derivatives), while preserving performance. ","Activation Relaxation: A Local Dynamical Approximation to
  Backpropagation in the Brain"
89,1306233895323656192,1215310334,Timo Schick,"[""🎉 New paper 🎉 We show that language models are few-shot learners even if they have far less than 175B parameters. Our method performs similar to @OpenAI's GPT-3 on SuperGLUE after training on 32 examples with just 0.1% of its parameter count: <LINK> #NLProc <LINK>"", 'This is achieved by combining PET (https://t.co/YCrBnggof7) with pretrained ALBERT. Key factors for strong performance include concurrently using multiple ""task descriptions"" and using labeled data to perform actual parameter updates.']",https://arxiv.org/abs/2009.07118,"When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much ""greener"" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models. ","It's Not Just Size That Matters: Small Language Models Are Also Few-Shot
  Learners"
90,1306151341224595456,1058369978,Ceccarelli Francesco,"['Check out our new paper on arxiv! \n\nRoom-temperature #singlephoton detection with #spad arrays: red-enhanced sensitivity, low timing jitter and multichannel acquisition!\n\n<LINK>']",https://arxiv.org/abs/2009.06728,"Single-photon detection is an invaluable tool for many applications ranging from basic research to consumer electronics. In this respect, the Single Photon Avalanche Diode (SPAD) plays a key role in enabling a broad diffusion of these techniques thanks to its remarkable performance, room-temperature operation, and scalability. In this paper we present a silicon technology that allows the fabrication of SPAD-arrays with an unprecedented combination of low timing jitter (95 ps FWHM) and high detection efficiency at red and near infrared wavelengths (peak of 70% at 650 nm, 45% at 800 nm). We discuss the device structure, the fabrication process, and we present a thorough experimental characterization of the fabricated detectors. We think that these long-awaited results can pave the way to new exciting developments in many fields, ranging from quantum optics to single molecule spectroscopy ","Custom silicon technology for SPAD-arrays with red-enhanced sensitivity
  and low timing jitter"
91,1306119018873606146,127070843,Michael Sentef,"['New preprint with the PSI group: <LINK>\n\nTaSe3 is a ""hot"" material with recent reports of topological phase transitions and possible topological superconductivity. This paper is about something else, namely mysterious sidebands.']",https://arxiv.org/abs/2009.07157,"Charge neutrality and their expected itinerant nature makes excitons potential transmitters of information. However, exciton mobility remains inaccessible to traditional optical experiments that only create and detect excitons with negligible momentum. Here, using angle-resolved photoemission spectroscopy, we detect dispersing excitons in the quasi-one-dimensional metallic trichalcogenide, TaSe3. The low density of conduction electrons and the low dimensionality in TaSe3 combined with a polaronic renormalization of the conduction band and the poorly screened interaction between these polarons and photo-induced valence holes leads to various excitonic bound states that we interpret as intrachain and interchain excitons, and possibly trions. The thresholds for the formation of a photo-hole together with an exciton appear as side valence bands with dispersions nearly parallel to the main valence band, but shifted to lower excitation energies. The energy separation between side and main valence bands can be controlled by surface doping, enabling the tuning of certain exciton properties. ","Multiple mobile excitons manifested as sidebands in
  quasi-one-dimensional metallic TaSe3"
92,1305927082971602949,1070579504379121664,Karl Cobbe,"['Excited to share our recent work on Phasic Policy Gradient, a new RL algorithm which improves sample efficiency by performing policy optimization and auxiliary optimization in two alternating phases. Check out the paper and code! <LINK>']",https://arxiv.org/abs/2009.04416,"We introduce Phasic Policy Gradient (PPG), a reinforcement learning framework which modifies traditional on-policy actor-critic methods by separating policy and value function training into distinct phases. In prior methods, one must choose between using a shared network or separate networks to represent the policy and value function. Using separate networks avoids interference between objectives, while using a shared network allows useful features to be shared. PPG is able to achieve the best of both worlds by splitting optimization into two phases, one that advances training and one that distills features. PPG also enables the value function to be more aggressively optimized with a higher level of sample reuse. Compared to PPO, we find that PPG significantly improves sample efficiency on the challenging Procgen Benchmark. ",Phasic Policy Gradient
93,1305862048400715776,106962512,Juan Felipe Carrasquilla Álvarez,"['Here is a new paper where we use a probabilistic formulation of quantum mechanics to study open quantum system dynamics. Work by Di Luo @Illinois_Alma, who interned at @VectorInst last year and has since produced a lot of interesting results. <LINK>']",https://arxiv.org/abs/2009.05580,"The theory of open quantum systems lays the foundations of a substantial part of modern research in quantum science and engineering. Rooted in the dimensionality of their extended Hilbert spaces, the high computational complexity of simulating open quantum systems calls for the development of strategies to approximate their dynamics. In this paper, we present an approach for tackling open quantum system dynamics. We simulate the dynamics of the Liouvillian superoperator using a forward-backward trapezoid method and find the steady-state via a variational formulation. We make use of a probabilistic formulation of quantum physics based on a positive operator-valued measure (POVM) in combination with autoregressive neural networks, which bring significant algorithmic flexibility due to their efficient sampling and tractable density. We introduce improved ansatzs, String States, which partially restore the symmetry of the autoregressive neural network and improve the description of local correlations. We benchmark our approaches on prototypical one and two-dimensional systems, finding results which closely track the exact solution and achieve higher accuracy in comparison to the recently proposed approach based on restricted Boltzmann machines. We anticipate this approach will be widely applicable to evolving density matrices in various contexts. ","Autoregressive Neural Network for Simulating Open Quantum Systems via a
  Probabilistic Formulation"
94,1305856278158245889,1003652696723873792,Max Gaspari,"['New paper with a great young postdoc, Denis Wittor! We dissect the key components of the chaotic turbulent weather generated via feedback due to supermassive black holes in the hot halos of galaxies.\n<LINK>\n#BlackHoleWeather #astrophysics #simulations #turbulence <LINK>']",https://arxiv.org/abs/2009.03344,"Turbulence in the intracluster, intragroup, and circumgalactic medium plays a crucial role in the self-regulated feeding and feedback loop of central supermassive black holes. We dissect the three-dimensional turbulent `weather' in a high-resolution Eulerian simulation of active galactic nucleus (AGN) feedback, shown to be consistent with multiple multi-wavelength observables of massive galaxies. We carry out post-processing simulations of Lagrangian tracers to track the evolution of enstrophy, a proxy of turbulence, and its related sinks and sources. This allows us to isolate in depth the physical processes that determine the evolution of turbulence during the recurring strong and weak AGN feedback events, which repeat self-similarly over the Gyr evolution. We find that the evolution of enstrophy/turbulence in the gaseous halo is highly dynamic and variable over small temporal and spatial scales, similar to the chaotic weather processes on Earth. We observe major correlations between the enstrophy amplification and recurrent AGN activity, especially via its kinetic power. While advective and baroclinc motions are always sub-dominant, stretching motions are the key sources of the amplification of enstrophy, in particular along the jet/cocoon, while rarefactions decrease it throughout the bulk of the volume. This natural self-regulation is able to preserve, as ensemble, the typically-observed subsonic turbulence during cosmic time, superposed by recurrent spikes via impulsive anisotropic AGN features (wide outflows, bubbles, cocoon shocks). This study facilitates the preparation and interpretation of the thermo-kinematical observations enabled by new revolutionary X-ray IFU telescopes, such as XRISM and Athena. ",Dissecting the turbulent weather driven by mechanical AGN feedback
95,1305795625225781248,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"['New paper on arXiv today, experiments looking at the role fluctuations can play in guiding quantum annealing <LINK> work performed @jqcDurNew\n and @DurhamQlm']",https://arxiv.org/abs/2009.06335,"Quantum annealing has great promise in leveraging quantum mechanics to solve combinatorial optimisation problems. However, to realize this promise to it's fullest extent we must appropriately leverage the underlying physics. In this spirit, I examine how the well known tendency of quantum annealers to seek solutions where more quantum fluctuations are allowed can be used to trade off optimality of the solution to a synthetic problem for the ability to have a more flexible solution, where some variables can be changed at little or no cost. I demonstrate this tradeoff experimentally using the reverse annealing feature a D-Wave Systems QPU for both problems composed of all binary variables, and those containing some higher-than-binary discrete variables. I further demonstrate how local controls on the qubits can be used to control the levels of fluctuations and guide the search. I discuss places where leveraging this tradeoff could be practically important, namely in hybrid algorithms where some penalties cannot be directly implemented on the annealer and provide some proof-of-concept evidence of how these algorithms could work. ",Fluctuation guided search in quantum annealing
96,1305742832351797254,1134375290581524480,Kai Schmitz,"['<LINK> Yay, new paper on the arXiv: We chase the NANOGrav ambulance and demonstrate that their signal might be due to gravitational waves emitted by cosmic strings in the early Universe! 🥳 <LINK>', '@spinrath Timing! ;-) https://t.co/Mt63GMbGV8']",https://arxiv.org/abs/2009.06607,The North American Nanohertz Observatory for Gravitational Waves (NANOGrav) has recently reported strong evidence for a stochastic common-spectrum process affecting the pulsar timing residuals in its 12.5-year data set. We demonstrate that this process admits an interpretation in terms of a stochastic gravitational-wave background emitted by a cosmic-string network in the early Universe. We study stable Nambu-Goto strings in dependence of their tension $G\mu$ and loop size $\alpha$ and show that the entire viable parameter space will be probed by an array of future experiments. ,Has NANOGrav found first evidence for cosmic strings?
97,1305575982800265221,861012544584110082,Mike Teodorescu,"['New working paper on fairness in machine learning with Jerry Kane, Lily Morse, and Yazeed Awwad, best paper finalist at SBE 2020:\nA Framework for Fairer Machine Learning in Organizations\n<LINK>']",https://arxiv.org/abs/2009.04661,"With the increase in adoption of machine learning tools by organizations risks of unfairness abound, especially when human decision processes in outcomes of socio-economic importance such as hiring, housing, lending, and admissions are automated. We reveal sources of unfair machine learning, review fairness criteria, and provide a framework which, if implemented, would enable an organization to both avoid implementing an unfair machine learning model, but also to avoid the common situation that as an algorithm learns with more data it can become unfair over time. Issues of behavioral ethics in machine learning implementations by organizations have not been thoroughly addressed in the literature, because many of the necessary concepts are dispersed across three literatures: ethics, machine learning, and management. Further, tradeoffs between fairness criteria in machine learning have not been addressed with regards to organizations. We advance the research by introducing an organizing framework for selecting and implementing fair algorithms in organizations. ",A Framework for Fairer Machine Learning in Organizations
98,1305555847599316992,792445487056052224,Sonia Conesa Boj,['New paper! Charting electron energy loss spectroscopy with machine learning: <LINK> When high-energy physics (@_nikhef ) meets material science great things can happen. Great collaboration with @JuanRojoC and more is yet to come! <LINK>'],https://arxiv.org/abs/2009.05050,"Exploiting the information provided by electron energy-loss spectroscopy (EELS) requires reliable access to the low-loss region where the zero-loss peak (ZLP) often overwhelms the contributions associated to inelastic scatterings off the specimen. Here we deploy machine learning techniques developed in particle physics to realise a model-independent, multidimensional determination of the ZLP with a faithful uncertainty estimate. This novel method is then applied to subtract the ZLP for EEL spectra acquired in flower-like WS$_2$ nanostructures characterised by a 2H/3R mixed polytypism. From the resulting subtracted spectra we determine the nature and value of the bandgap of polytypic WS$_2$, finding $E_{\rm BG} = 1.6_{-0.2}^{+0.3}\,{\rm eV}$ with a clear preference for an indirect bandgap. Further, we demonstrate how this method enables us to robustly identify excitonic transitions down to very small energy losses. Our approach has been implemented and made available in an open source Python package dubbed EELSfitter. ","Charting the low-loss region in Electron Energy Loss Spectroscopy with
  machine learning"
99,1305528876215545860,2423179856,Edward Raff,"['Automatic Yara Rule Generation Using Biclustering, new paper accepted to #AISec with @rjzak  @drhyrum @filar  &amp; others too cool for twitter. This paper has been a long time coming! \nPaper📝: <LINK> \nCode👨\u200d💻👩\u200d💻: <LINK> <LINK>', '#Yara rules are ubiquitous among reverse engineers and malware analysts. Its a language for writing signatures designed for the task. You can do regexes and conditional logic, but how to pick what you use? Thats part of what makes writing Yara rules a slow &amp; error prone. https://t.co/Idsla2Hpi4', 'Our idea, lets use large byte n-grams ( KiloGrams https://t.co/gt5dPFkR7W ) to be the features in a rule and biclustering (bic) to create the rule logic! Bic organizes rows &amp; cols to exposure structure. So lets ""and"" together everything in a bic, ""or"" together the different bics. https://t.co/ToPLuwJkdr', ""We've made some modifications to Spectral Co-clustering algo to deal with unknown number of biclusters. Then we use bloom filters to help us filter out bad n-grams that are too frequent. Then we iterate over different n-gram sizes to build a new Yara rule. https://t.co/S3NqVHtxN2"", 'Using AVClass https://t.co/VZVefrmgWI we built a large scale test corpus to compare, and found AutoYara had good results - especially when only a few samples are available. F_beta = 0.001 for desired 0.1% FPR. Why does AutoYara score better? https://t.co/aFNhKpCTVg', 'If we look at TPR for any given FPR, there is a specific region where YarGen works well. Notice large bubbles, that means YarGen needs a lot of samples to make good rules. AutoYara produces lots of rules with no FPs! That was ~400k test files. https://t.co/MI6UKqsILJ', 'How low does the FP rate go? We used samples from @0xffff0800 and @virustotal RetroHunt to test over 90 million! Compared to VxSig ( https://t.co/Zw0NVld6Jw ) we get more TPs and less FP in most cases. Often 0 FPs over all of RetroHunt! https://t.co/WCTlUBb56E', 'Still incredulous? @elastic had a production need for 24 families. Data as it happens in real life with 2 professional analyst, and a 3rd we provided. AutoYara can compete with humans, sometimes better. Could save 44%-100% of time! Let humans tackle the hard things AutoYara cant https://t.co/1q0m6dYBeY', 'AutoYara signatures may look a little funny, but not hard to edit. With just a few minutes of work signatures for 6 of the families could be improved. Figure Human comparison and Human+Machine teaming for signatures? https://t.co/GWR2fmaAai', 'We also look at what AutoYara is keying off of. Turns out, surprisingly similar to what a human analyst uses! Also helps understand why VxSig does poorly in many cases. It relies on 100% code, but when good sigs come from non-code regions, nothing VxSig can do to recover. https://t.co/G5n4Y6C0Vy', ""After several rejections I am glad to finally get this work out! A huge effort by lots of people who have all made this research better and stronger. Also, don't throw away VxSig/YarGen yet! YarGen still useful for analysts, and VxSig still powerful in hard cases.""]",https://arxiv.org/abs/2009.03779,"Yara rules are a ubiquitous tool among cybersecurity practitioners and analysts. Developing high-quality Yara rules to detect a malware family of interest can be labor- and time-intensive, even for expert users. Few tools exist and relatively little work has been done on how to automate the generation of Yara rules for specific families. In this paper, we leverage large n-grams ($n \geq 8$) combined with a new biclustering algorithm to construct simple Yara rules more effectively than currently available software. Our method, AutoYara, is fast, allowing for deployment on low-resource equipment for teams that deploy to remote networks. Our results demonstrate that AutoYara can help reduce analyst workload by producing rules with useful true-positive rates while maintaining low false-positive rates, sometimes matching or even outperforming human analysts. In addition, real-world testing by malware analysts indicates AutoYara could reduce analyst time spent constructing Yara rules by 44-86%, allowing them to spend their time on the more advanced malware that current tools can't handle. Code will be made available at this https URL . ",Automatic Yara Rule Generation Using Biclustering
100,1305516871941271552,4639078397,John Wise,"['New paper day led by grad students Will Hicks and Azton Wells (UCSD)! We found that minihalos are primarily enriched from nearby supernovae but do not form stars until they become more massive. Co-authors M. Norman, @aBrittonSmith @bwoshea  <LINK> <LINK>']",https://arxiv.org/abs/2009.05499,"Recent high-resolution simulations of early structure formation have shown that externally enriched halos may form some of the first metal enriched stars. This study utilizes a 1 comoving Mpc$^3$ high-resolution simulation to study the enrichment process of metal-enriched halos down to $z=9.3$. Our simulation uniquely tracks the metals ejected from Population III stars, and we use this information to identify the origin of metals within metal-enriched halos. These halos show a wide range of metallicities, but we find that the source of metals for $\gtrsim$ 50\% of metal-enriched halos is supernova explosions of Population III stars occuring outside their virial radii. The results presented here indicate that external enrichment by metal-free stars dominates the enrichment process of halos with virial mass below $10^{6}\,M_\odot$ down to $z=9.3$. Despite the prevalence of external enrichment in low mass halos, Pop II stars forming due to external enrichment are rare because of the small contribution of low-mass halos to the global star formation rate combined with low metallicities towards the center of these halos resulting from metal ejecta from external sources mixing from the outside-in. The enriched stars that do form through this process have absolute metallicities below $10^{-3}\,Z_\odot$. We also find that the fraction of externally enriched halos increases with time, $\sim 90\%$ of halos that are externally enriched have $M_\mathrm{vir} < 10^6\,M_\odot$, and that pair-instability supernovae contribute the most to the enrichment of the IGM as a whole and are thus are the predominant supernova type contributing to the external enrichment of halos. ",External Enrichment of Minihalos by the First Supernovae
101,1305508367083991043,76711005,"Antonio Pedro Ramos, PhD","['New working paper: Explaining the Decline of Child Mortality in 44 Developing Countries: A Bayesian Extension of Oaxaca Decomposition Methods <LINK>', 'We investigate the decline of infant mortality in 42 low and middle income countries (LMIC) using detailed micro data from 84 Demographic and Health Surveys. We estimate infant mortality risk for each infant in our data and develop a novel extension of Oaxaca decomposition +', 'to understand the sources of these changes. We find that the decline in infant mortality is due to a declining propensity for parents with given characteristics to experience the death of an infant rather than due to changes in the distributions of these characteristics +', 'over time. Our results suggest that technical progress and policy health interventions in the form of public goods are the main drivers of the the recent decline in infant mortality in LMIC.', '#OaxacaDecomposition #epitwitter #poptwitter #DemographicsMatter #Stats']",https://arxiv.org/abs/2009.05417,We investigate the decline of infant mortality in 42 low and middle income countries (LMIC) using detailed micro data from 84 Demographic and Health Surveys. We estimate infant mortality risk for each infant in our data and develop a novel extension of Oaxaca decomposition to understand the sources of these changes. We find that the decline in infant mortality is due to a declining propensity for parents with given characteristics to experience the death of an infant rather than due to changes in the distributions of these characteristics over time. Our results suggest that technical progress and policy health interventions in the form of public goods are the main drivers of the the recent decline in infant mortality in LMIC. ,"Explaining the Decline of Child Mortality in 44 Developing Countries: A
  Bayesian Extension of Oaxaca Decomposition Methods"
102,1305408326642143233,1123610968221728773,Christoph Studer,['New paper on spatial equalization for mmWave/THz communication with processing-in-memory (or should we call it through-memory processing?) at IEEE TCAS-II <LINK> \n\nOur accelerator saves up to 2x in area and 3x in power compared to conventional spatial equalizers!'],https://arxiv.org/abs/2009.03874,"All-digital basestation (BS) architectures enable superior spectral efficiency compared to hybrid solutions in massive multi-user MIMO systems. However, supporting large bandwidths with all-digital architectures at mmWave frequencies is challenging as traditional baseband processing would result in excessively high power consumption and large silicon area. The recently-proposed concept of finite-alphabet equalization is able to address both of these issues by using equalization matrices that contain low-resolution entries to lower the power and complexity of high-throughput matrix-vector products in hardware. In this paper, we explore two different finite-alphabet equalization hardware implementations that tightly integrate the memory and processing elements: (i) a parallel array of multiply-accumulate (MAC) units and (ii) a bit-serial processing-in-memory (PIM) architecture. Our all-digital VLSI implementation results in 28nm CMOS show that the bit-serial PIM architecture reduces the area and power consumption up to a factor of 2x and 3x, respectively, when compared to a parallel MAC array that operates at the same throughput. ","High-Bandwidth Spatial Equalization for mmWave Massive MU-MIMO with
  Processing-In-Memory"
103,1305303455481966592,839820726777544705,Alexia Jolicoeur-Martineau,"['New paper on adversarial😠 score matching  and an alternative to Langevin Sampling for better generative models! 😸 We show how we can obtain results better than SOTA GANs. 😻\n\nBlog: <LINK>\nPaper: <LINK>\nCode: <LINK>', ""@theshawwn Papers looking at high-resolution images should eventually come out soon. It's just that it requires a lot of compute so industry is more suited to do it. Try big batches + BigGAN adversarial variant on the DDPM architecture.""]",https://arxiv.org/abs/2009.05475,"Denoising Score Matching with Annealed Langevin Sampling (DSM-ALS) has recently found success in generative modeling. The approach works by first training a neural network to estimate the score of a distribution, and then using Langevin dynamics to sample from the data distribution assumed by the score network. Despite the convincing visual quality of samples, this method appears to perform worse than Generative Adversarial Networks (GANs) under the Fr\'echet Inception Distance, a standard metric for generative models. We show that this apparent gap vanishes when denoising the final Langevin samples using the score network. In addition, we propose two improvements to DSM-ALS: 1) Consistent Annealed Sampling as a more stable alternative to Annealed Langevin Sampling, and 2) a hybrid training formulation, composed of both Denoising Score Matching and adversarial objectives. By combining these two techniques and exploring different network architectures, we elevate score matching methods and obtain results competitive with state-of-the-art image generation on CIFAR-10. ",Adversarial score matching and improved sampling for image generation
104,1305071057020297216,712960453,Prashant Saxena,"['In this new paper on growing twisting rods using Cosserat theory, @SatyaPr93137324 presents a possible mechanism for generation of curls in naturally growing filaments such as hair, plant tendrils, carbon nanotubes...\n\n<LINK>', 'We used ideas of growth presented by @AlainGoriely and rods presented by Tim Healey ( https://t.co/8EegyrhdkS ). \nThis is second paper by @SatyaPr93137324 from his undergraduate research work!!', '@snape_aritra @AlainGoriely @SatyaPr93137324 Thanks, would love to get some feedback.']",https://arxiv.org/abs/2009.02037,"We present a growth model for special Cosserat rods that allows for induced rotation of cross-sections. The growth law considers two controls, one for lengthwise growth and other for rotations. This is explored in greater detail for straight rods with helical and hemitropic material symmetries by introduction of a symmetry preserving growth to account for the microstructure. The example of a guided-guided rod possessing a chiral microstructure is considered to study its deformation due to growth. We show the occurrence of growth induced out-of-plane buckling in such rods. ",Buckling of chiral rods due to coupled axial and rotational growth
105,1304417556921540608,150295570,Ovidiu Racorean,['The spacetime emerges from quantum entanglement...but not in the way you may think. My new paper...\n\n<LINK>'],https://arxiv.org/abs/2009.04990,"We argue, in the context of Ads/CFT correspondence, that the degree of entanglement on the CFTs side determines the orientation of space and time on the dual global spacetime. That is, the global spacetime dual to entangled copies of field theory is non-orientable, while the product state of the CFTs results in an orientable spacetime. As a result, disentangling the degrees of freedom between two copies of CFT implies, on the gravity side, the transition from a non-orientable spacetime to a spacetime having a definite orientation of space and time, thus an orientable spacetime. We conclude showing that topology change induced by decreasing the entanglement between two sets of degrees of freedom corresponds to a topological blow down operation. ",Quantum entanglement and the non-orientability of spacetime
106,1304363725563088898,49344793,Thomas Winters,['GITTA is a new grammar induction method for learning generative grammars using a template-focused approach\n\n📜 Paper: <LINK> (short paper at @iccc_conf)\n💽 Code: <LINK>\n▶️ Demo: <LINK> <LINK>'],https://arxiv.org/abs/2009.04530,"Natural language generation provides designers with methods for automatically generating text, e.g. for creating summaries, chatbots and game content. In practise, text generators are often either learned and hard to interpret, or created by hand using techniques such as grammars and templates. In this paper, we introduce a novel grammar induction algorithm for learning interpretable grammars for generative purposes, called Gitta. We also introduce the novel notion of template trees to discover latent templates in corpora to derive these generative grammars. By using existing human-created grammars, we found that the algorithm can reasonably approximate these grammars using only a few examples. These results indicate that Gitta could be used to automatically learn interpretable and easily modifiable grammars, and thus provide a stepping stone for human-machine co-creation of generative models. ","Discovering Textual Structures: Generative Grammar Induction using
  Template Trees"
107,1304307731482935297,1392935011,Ole-Chr. Granmo,"['New CAIR-paper: ""Massively Parallel and Asynchronous #TsetlinMachine Architecture Supporting Almost Constant-Time Scaling"". Congratulations to my brilliant colleagues Darshana, Bimal, Morten, Saeed, Lei, Rupsa and Rohan. What a great team-work!  <LINK>  #ML #AI <LINK>']",https://arxiv.org/abs/2009.04861,"Using logical clauses to represent patterns, Tsetlin Machines (TMs) have recently obtained competitive performance in terms of accuracy, memory footprint, energy, and learning speed on several benchmarks. Each TM clause votes for or against a particular class, with classification resolved using a majority vote. While the evaluation of clauses is fast, being based on binary operators, the voting makes it necessary to synchronize the clause evaluation, impeding parallelization. In this paper, we propose a novel scheme for desynchronizing the evaluation of clauses, eliminating the voting bottleneck. In brief, every clause runs in its own thread for massive native parallelism. For each training example, we keep track of the class votes obtained from the clauses in local voting tallies. The local voting tallies allow us to detach the processing of each clause from the rest of the clauses, supporting decentralized learning. This means that the TM most of the time will operate on outdated voting tallies. We evaluated the proposed parallelization across diverse learning tasks and it turns out that our decentralized TM learning algorithm copes well with working on outdated data, resulting in no significant loss in learning accuracy. Furthermore, we show that the proposed approach provides up to 50 times faster learning. Finally, learning time is almost constant for reasonable clause amounts (employing from 20 to 7,000 clauses on a Tesla V100 GPU). For sufficiently large clause numbers, computation time increases approximately proportionally. Our parallel and asynchronous architecture thus allows processing of massive datasets and operating with more clauses for higher accuracy. ","Massively Parallel and Asynchronous Tsetlin Machine Architecture
  Supporting Almost Constant-Time Scaling"
108,1304226115205046272,1148910974218321920,Dr. Isobel Romero-Shaw,"['Excited to release this! In my new paper (<LINK>) with @LaskyPaul, @EHThrane and @juan__cb, we find evidence that GW190521 may have come from an *eccentric* binary! This supports the hypothesis that it and other @LIGO @ego_virgo mergers formed *dynamically*!']",http://arxiv.org/abs/2009.04771,"Pair instability supernovae are thought to restrict the formation of black holes in the mass range ~50 - 135 solar masses. However, black holes with masses within this ""high mass gap"" are expected to form as the remnants of binary black hole mergers. These remnants can merge again dynamically in densely populated environments such as globular clusters. The hypothesis that the binary black hole merger GW190521 formed dynamically is supported by its high mass. Orbital eccentricity can also be a signature of dynamical formation, since a binary that merges quickly after becoming bound may not circularize before merger. In this work, we measure the orbital eccentricity of GW190521. We find that the data prefer a signal with eccentricity $e \geq 0.1$ at 10 Hz to a non-precessing, quasi-circular signal, with a log Bayes factor $\ln{\cal B}=5.0$. When compared to precessing, quasi-circular analyses, the data prefer a non-precessing, $e \geq 0.1$ signal, with log Bayes factors $\ln{\cal B}\approx2$. Using injection studies, we find that a non-spinning, moderately eccentric ($e = 0.13$) GW190521-like binary can be mistaken for a quasi-circular, precessing binary. Conversely, a quasi-circular binary with spin-induced precession may be mistaken for an eccentric binary. We therefore cannot confidently determine whether GW190521 was precessing or eccentric. Nevertheless, since both of these properties support the dynamical formation hypothesis, our findings support the hypothesis that GW190521 formed dynamically. ","GW190521: orbital eccentricity and signatures of dynamical formation in
  a binary black hole merger signal"
109,1304213205657423873,2411906239,Duncan Galloway,"[""In some neutron star binaries that show thermonuclear bursts, we also see minutes-long periodic X-ray variations. @Monash_Science PhD student Ka Ho Tse reports a new example in his just-submitted paper <LINK>. We don't know the cause of these variations..."", '... but the explanation to date has been that they come from thermonuclear burning *right* at the boundary of stability, where bursts should cease. This makes no sense! Ka Ho also compared models with observations, and find the observed amplitudes hard to explain. More to come!']",https://arxiv.org/abs/2009.01536,"Millihertz quasi-periodic oscillations (mHz QPOs) observed in neutron-star low-mass X-ray binaries (NS LMXBs) are generally explained as marginally stable thermonuclear burning on the neutron star surface. We report the discovery of mHz QPOs in an XMM-Newton observation of the transient 1RXS J180408.9$-$342058, during a regular bursting phase of its 2015 outburst. We found significant periodic signals in the March observation, with frequencies in the range $5-8\,\mathrm{mHz}$, superimposed on a strong $\sim1/f$ power-law noise continuum. Neither the QPO signals nor the power-law noise were present during the April observation, which exhibited a $2.5\times$ higher luminosity and had correspondingly more frequent bursts. When present, the QPO signal power decreases during bursts and disappears afterwards, similar to the behaviour in other sources. 1RXS J180408.9$-$342058 is the eighth source known to date that exhibits such QPOs driven by thermonuclear burning. We examine the range of properties of the QPO signals in different sources. Whereas the observed oscillation profile is similar to that predicted by numerical models, the amplitudes are significantly higher, challenging their explanation as originating from marginally stable burning. ","Detection of Millihertz Quasi-Periodic Oscillations in the X-Ray Binary
  1RXS J180408.9$-$342058"
110,1304207633868402688,29251447,Tuan Do,"['Where did the millions of stars at the Galactic center come from? In a new paper, we find evidence that some of the stars probably came from a globular cluster or dwarf galaxy that fell in and got trapped. \n\n1/3\n\n<LINK>', 'How do we know this? We find that stars with low metal abundance move differently around the black hole than the stars with more metals! It is very unusual if they formed in the same place, but could be explained if the stars fell in. https://t.co/R0MEgRKntF', 'In a companion paper, my theory colleagues used computer simulations throwing star clusters and small galaxies into the center of our galaxy to explore this process. The simulations show that this infall might have happened over 3 billion years ago.  \n\nhttps://t.co/zA05uMjVRS', '@ProfAnnikaPeter It was fun to be able to do some dynamical modeling again!']",https://arxiv.org/abs/2009.02335,"The Milky Way nuclear star cluster (MW NSC) has been used as a template to understand the origin and evolution of galactic nuclei and the interaction of nuclear star clusters with supermassive black holes. It is the only nuclear star cluster with a supermassive black hole where we can resolve individual stars to measure their kinematics and metal abundance to reconstruct its formation history. Here, we present results of the first chemo-dynamical model of the inner 1 pc of the MW NSC using metallicity and radial velocity data from the KMOS spectrograph on the Very Large Telescope. We find evidence for two kinematically and chemically distinct components in this region. The majority of the stars belong to a previously known super-solar metallicity component with a rotation axis perpendicular to the Galactic plane. However, we identify a new kinematically distinct sub-solar metallicity component which contains about 7\% of the stars and appears to be rotating faster than the main component with a rotation axis that may be misaligned. This second component may be evidence for an infalling star cluster or remnants of a dwarf galaxy, merging with the MW NSC. These measurements show that the combination of chemical abundances with kinematics is a promising method to directly study the MW NSC's origin and evolution. ","Revealing the Formation of the Milky Way Nuclear Star Cluster via
  Chemo-Dynamical Modeling"
111,1304080373819867136,827263859517857793,Sophia Waddell,"['New paper day - my second lead-author paper (with my supervisor, Luigi Gallo) is out on arxiv:\n\n<LINK>\n\nBased on work I did for my honours thesis at SMU, where we studied X-ray data for 69 type-1 active galaxies observed with Suzaku. 1/4\n\n@smuhalifax @SMUScience', 'These were divided into two groups: narrow-line (NLS1) and broad-line (BLS1), with the goal of finding differences between the two groups. Not only were we able to re-confirm previous results (NLS1s are accreting at a higher rate, and have steeper spectra), we found more! 2/4', ""NLS1 galaxies in general also have more complex spectra - weaker narrow iron lines, and stronger soft and hard excesses (extra photons at the lowest and highest X-ray energies). What's more, the soft and hard excesses are highly correlated for NLS1s - not so for BLS1s. 3/4"", 'This, along with other work presented in the paper, might point to a different physical origin of the soft and hard excesses in NLS1s vs. in BLS1s. More work is needed to study why NLS1s have such unique X-ray properties, but I had a lot of fun trying to figure it out! :) 4/4']",https://arxiv.org/abs/2009.04378,"A sample of narrow-line (NLS1) and broad-line Seyfert 1 (BLS1) galaxies observed with Suzaku is presented. The final sample consists of 22 NLS1s and 47 BLS1s, for a total of 69 AGN that are all at low redshift (z<0.5) and exhibit low host galaxy column densities (<10^22 cm^-2). The average spectrum for each object is fit with a toy model to characterise important parameters, including the photon index, soft excess, Compton hump (or hard excess), narrow iron line strength, luminosity and X-ray Eddington ratio (L_x/L_Edd). We confirm previous findings that NLS1s have steeper power laws and higher X-ray Eddington ratios, but also find that NLS1 galaxies have stronger soft and hard excesses than their BLS1 counterparts. Studying the correlations between parameters shows that the soft and hard excesses are correlated for NLS1 galaxies, while no such correlation is observed for BLS1s. Performing a principal component analysis (PCA) on the measured X-ray parameters shows that while the X-ray Eddington ratio is the main source of variations within our sample (PC1), variations in the soft and hard excesses form the second principal component (PC2) and it is dominated by the NLS1s. The correlation between the soft and hard excess in NLS1 galaxies may suggest a common origin for the two components, such as a blurred reflection model. The presented Suzaku sample of Seyfert 1 galaxies is a useful tool for analysis of the X-ray properties of AGN, and for the study of the soft and hard excesses observed in AGN. ","A Suzaku sample of unabsorbed narrow-line and broad-line Seyfert 1
  galaxies: I. X-ray spectral properties"
112,1304063195909427200,3236251346,Mikel Sanz,"['Glad to share a new preprint: Coplanar Antenna Design for Microwave Entangled Signals Propagating in Open Air (<LINK>) this paper started with a very simple and pragmatic question: are commercial antennae adequate for open air experiments with quantum microwaves? <LINK>', 'The answer turned out to be very interesting and somehow unexpected. We‘ve obtained the optimal shape to preserve the entanglement in a two-mode squeezed state and proven high sensitivity of the correlations with respect to errors in the optimal shape (1% deviations destroy them)']",https://arxiv.org/abs/2009.03021,"Open-air microwave quantum communication and metrology protocols must be able to transfer quantum resources from a fridge, where they are created, into an environment dominated by thermal noise. Indeed, the states that carry such quantum resources are generated in a cryostat at $T_\text{in} \simeq 10^{-2} $~K and with $Z_\text{in} = 50 \, \Omega$ intrinsic impedance, and require an antenna-like device to transfer them into the open air, characterized by an intrinsic impedance of $Z_\text{out} = 377 \, \Omega$ and a temperature of $T_\text{out} \simeq 300$ K, with minimal losses. This device accomplishes a smooth impedance matching between the cryostat and the open air. Here, we study the transmission of two-mode squeezed thermal states, developing a technique to design the optimal shape of a coplanar antenna to preserve the entanglement. Based on a numerical optimization procedure we find the optimal shape of the impedance is exponential, and we adjust this shape to an analytical function. Additionally, this study reveals that losses are very sensitive to this shape, and small changes dramatically affect the outcoming entanglement, which could have been a limitation in previous experiments employing commercial antennae. This work will impact the fields of quantum sensing and quantum metrology, as well as any open-air microwave quantum communication protocol, with special application to the development of the quantum radar. ","Coplanar Antenna Design for Microwave Entangled Signals Propagating in
  Open Air"
113,1304059554360111105,1123977306979041288,Sylvia Biscoveanu,"['Excited to share my new paper (<LINK>) with @ColmMTalbot @EHThrane @SuperSymmetric1 because it means I get to do a 🧵 on stochastic gravitational-wave backgrounds, which have the potential to reveal information about the early Universe! 1/n', 'Primordial gravitational-wave backgrounds arise from cosmological processes in the early Universe like inflation and phase transitions but are obscured by a much louder foreground of gravitational-waves from compact binary mergers 2/n', 'Some fraction of this foreground is resolvable, like the GW events we have already detected with @LIGO and @ego_virgo, but most sources are too faint to be individually detectable, and they instead overlap to create a non-Gaussian foreground.', 'The current methods for dealing with the foreground rely on subtraction - removing the power from individual compact binaries to reveal the primordial background - but this only works if all the compact binaries are loud enough to detect individually! 4/n', 'In our paper, we develop a fully Bayesian method to detect the primordial background and astrophysical foreground simultaneously, bypassing the subtraction problem 5/n', 'As an added bonus, because we take into account the non-Gaussianity of the foreground, ours is the statistically optimal method for the simultaneous detection of mixed Gaussian and non-Gaussian stochastic backgrounds! #bayesforthewin 6/6']",https://arxiv.org/abs/2009.04418,"Primordial gravitational waves are expected to create a stochastic background encoding information about the early Universe that may not be accessible by other means. However, the primordial background is obscured by an astrophysical foreground consisting of gravitational waves from compact binaries. We demonstrate a Bayesian method for estimating the primordial background in the presence of an astrophysical foreground. Since the background and foreground signal parameters are estimated simultaneously, there is no subtraction step, and therefore we avoid astrophysical contamination of the primordial measurement, sometimes referred to as ""residuals"". Additionally, since we include the non-Gaussianity of the astrophysical foreground in our model, this method represents the statistically optimal approach to the simultaneous detection of a multi-component stochastic background. ","Measuring the primordial gravitational-wave background in the presence
  of astrophysical foregrounds"
114,1304056420841725952,1710697381,Diego F. Torres,"['Presenting a collaborative review paper we wrote on the impact of @ESA_Integral on understanding High-Mass X-ray Binaries, for New Astronomy Reviews. Read it at <LINK> <LINK>']",https://arxiv.org/abs/2009.03244,"High mass X-ray binaries are among the brightest X-ray sources in the Milky Way, as well as in nearby Galaxies. Thanks to their highly variable emissions and complex phenomenology, they have attracted the interest of the high energy astrophysical community since the dawn of X-ray Astronomy. In more recent years, they have challenged our comprehension of physical processes in many more energy bands, ranging from the infrared to very high energies. In this review, we provide a broad but concise summary of the physical processes dominating the emission from high mass X-ray binaries across virtually the whole electromagnetic spectrum. These comprise the interaction of stellar winds with the high gravitational and magnetic fields of compact objects, the behaviour of matter under extreme magnetic and gravity conditions, and the perturbation of the massive star evolutionary processes by presence in a binary system. We highlight the role of the INTEGRAL mission in the discovery of many of the most interesting objects in the high mass X-ray binary class and its contribution in reviving the interest for these sources over the past two decades. We show how the INTEGRAL discoveries have not only contributed to significantly increase the number of high mass X-ray binaries known, thus advancing our understanding of the population as a whole, but also have opened new windows of investigation that stimulated the multi-wavelength approach nowadays common in most astrophysical research fields. We conclude the review by providing an overview of future facilities being planned from the X-ray to the very high energy domain that will hopefully help us in finding an answer to the many questions left open after more than 18 years of INTEGRAL scientific observations. ","Advances in Understanding High-Mass X-ray Binaries with INTEGRAL and
  Future Directions"
115,1303950246809137152,713841397552586752,Paola Gori-Giorgi,"['New paper by @kjdaas, @JuriGrossi Stefan Vuckovic, Ziad Musslimani, @DerkKooi, Klaas Giesbertz, Mike Seidl, where we look at the strong-coupling limit of the adiabatic connection that has the MP series as weak-coupling expansion.  #compchem\n<LINK>', 'Main findings: we start from the H atom (yes, the H atom, but polarised and unpolarised), which we can solve analytically or anyhow very accurately. Then we find that the results can be generalised to any closed-shell system.', 'We thus show that the first 3 leading terms of the strong-coupling limit are given by functionals of the Hartree-Fock density, for which we provide variational estimates. We also analyse the uniform electron gas.', 'Why are we looking at this? Because interpolations between weak coupling (MP2) and strong coupling give surprisingly good results for non-covalent interactions once we correct the size-consistency error, which has zero cost in this case https://t.co/i7ZOSeOr4a', 'Thus, these interpolations give double-hybrid functionals that do not need any dispersion correction to describe non-covalent interactions, because we use 100% of Ex and MP2. The interpolations decide how to mix them in. We will use our new results to further improve them.', 'In collaboration with Stefan Vuckovic, Eduardo Fabiano and Fabio Della Sala, improving the interpolations is the present research project of Tim Daas (@kjdaas ), a great PhD student in our group, who is also the first author of this paper.', 'But besides this, I suggest the paper to whoever likes strange adiabatic connections and exotic limits... (which was the real motivation..)']",https://arxiv.org/abs/2009.04326,"We study in detail the first three leading terms of the large coupling-strength limit of the adiabatic connection that has as weak-interaction expansion the M{\o}ller-Plesset perturbation theory. We first focus on the H atom, both in the spin-polarized and the spin-unpolarized case, reporting numerical and analytical results. In particular, we derive an asymptotic equation that turns out to have simple analytical solutions for certain channels. The asymptotic H atom solution for the spin-unpolarized case is then shown to be variationally optimal for the many-electron spin-restricted closed-shell case, providing expressions for the large coupling-strength density functionals up to the third leading order. We also analyze the H2 molecule and the uniform electron gas. ","Large coupling-strength expansion of the M{\o}ller-Plesset adiabatic
  connection: From paradigmatic cases to variational expressions for the
  leading terms"
116,1303871529319632896,970481802308653056,Max Lipton,"['My new paper on electrostatic knot theory is online! This paper, along with my previous result, articulates the relationship between the knot type and the topologies of their level potential surfaces, in a cute snippet of data I call the “Morse code.”\n\n<LINK> <LINK>', '@JSEllenberg @stevenstrogatz The Morse code can be “rearranged” via the Morse Replacement Lemma. The critical set and its behavior remains fixed, but the critical values can be rearranged.', '@JSEllenberg @stevenstrogatz However, the new function will not necessarily be harmonic, and therefore not necessarily representative of the electric potential of a charge distribution.']",https://arxiv.org/abs/2009.03958,"Consider a knot $K$ in $S^3$ with uniformly distributed electric charge. From the standpoint of both physics and knot theory, it is natural to try to understand the critical points of the electric potential and their behavior. When the knot is sufficiently close to a planar projection, we prove a lower bound on the size of the critical set based on the projection's crossings, improving a 2019 result of the author. Next, we show that critical points of index $1$ correspond to increases in the genus of the equipotential surfaces as one increases the value of the potential, whilst critical points of index $2$ correspond to decreases. We conclude with a Cerf-theoretic description of the bifurcation of critical points under generic knot isotopies. Our theorems are proven with Morse theory and techniques from geometric topology. keywords: Physical knot theory, electrostatics, Morse theory, dynamical systems, geometric topology, Cerf theory ","Critical points and equipotential surfaces of knotted electric charge
  distributions"
117,1303797756352434179,31049199,Drew Dimmery,"['Do you care about generalizing your experiments to a broader population? Of course you do! \n\nIn a new paper with My Phan, @darbour26 and Anup Rao, we discuss experimental design for generalization. \n\ntl;dr: ensure reweighted balance during design. <LINK> <LINK>']",https://arxiv.org/abs/2009.03860,"We consider the problem of designing a randomized experiment on a source population to estimate the Average Treatment Effect (ATE) on a target population. We propose a novel approach which explicitly considers the target when designing the experiment on the source. Under the covariate shift assumption, we design an unbiased importance-weighted estimator for the target population's ATE. To reduce the variance of our estimator, we design a covariate balance condition (Target Balance) between the treatment and control groups based on the target population. We show that Target Balance achieves a higher variance reduction asymptotically than methods that do not consider the target population during the design phase. Our experiments illustrate that Target Balance reduces the variance even for small sample sizes. ",Designing Transportable Experiments
118,1303659711414272002,1123610968221728773,Christoph Studer,['New paper on centimeter-accuracy positioning using data fusion of channel state information from conventional WLAN access points: <LINK>'],https://arxiv.org/abs/2009.02798,"Channel state information (CSI)-based fingerprinting via neural networks (NNs) is a promising approach to enable accurate indoor and outdoor positioning of user equipments (UEs), even under challenging propagation conditions. In this paper, we propose a positioning pipeline for wireless LAN MIMO-OFDM systems which uses uplink CSI measurements obtained from one or more unsynchronized access points (APs). For each AP receiver, novel features are first extracted from the CSI that are robust to system impairments arising in real-world transceivers. These features are the inputs to a NN that extracts a probability map indicating the likelihood of a UE being at a given grid point. The NN output is then fused across multiple APs to provide a final position estimate. We provide experimental results with real-world indoor measurements under line-of-sight (LoS) and non-LoS propagation conditions for an 80MHz bandwidth IEEE 802.11ac system using a two-antenna transmit UE and two AP receivers each with four antennas. Our approach is shown to achieve centimeter-level median distance error, an order of magnitude improvement over a conventional baseline. ","CSI-Based Multi-Antenna and Multi-Point Indoor Positioning Using
  Probability Fusion"
119,1303625256729563136,276597679,Markus Pössel,"[""My new paper has just come up on arXiv! It's a derivation of the full Schwarzschild metric using only symmetry arguments and the simplified version of Einstein's equation given by @johncarlosbaez and Ted Bunn. <LINK> #GenRelEdu 1/"", 'The basic recipe is simple: Argue in the usual way why, given spherical symmetry and staticity, the metric has the standard form with two remaining unknown functions that depend only on r. One is in front of dt^2, the other in front of dr^2. Then, introduce a family of 2/', 'infalling observers (which those in the know recognize as the infalling observers that occur in the Gullstrand-Painlevé form of the metric; Matt Visser had used those in a heuristic approach to the Schwarzschild metric, https://t.co/zeEIELSF4U ). 3/', ""Next, look at a small dust ball falling along with one of those observers. Choose the dust particles to be initially at rest, at some moment. The simple Baez-Bunn form of Einstein's vacuum equation then states that the second derivative of the dust ball volume w.r.t. proper 4/"", ""time is, initially, zero. (You can also read this as a statement about the Newtonian limit of general relativity, extending Einstein's equivalence principle to the simplest tidal forces. So even if you don't know about Einstein's equations, it's a plausible move.) 5/"", ""This equation fixes the unknown functions of r. Note that the differential equation that you need to solve is of a very simple type. Separate variables (a standard technique, after all) and you're done. 6/"", ""Last step: Use the Newtonian limit (from far away, the gravitational attraction of that spherical mass should be described by Newtonian gravity) to fix the value of the remaining integration constant. (This is where the usual mass M comes into play.) And then you're done. 7/"", 'The derivation builds on pre-existing work, of course. The Baez-Bunn form of the Einstein equation is a gift that keeps on giving, https://t.co/r5i5gwQqhL. I first learned about the Gullstrand-Painlevé observers in this article by Hamilton and Lisle: https://t.co/FZ7kXsTowM 8/', 'When I was incubating the idea of bringing the two together, I eagerly searched the literature to see if someone had scooped me. I found that Wolfgang Kassner had applied the dust ball model to a derivation in the usual Schwarzschild coordinates https://t.co/1ROQrpDsRN 9/', ""But that is not quite what I did, and it turns out that using the infalling observers (and thus arriving at the Gullstrand-Painlevé form first) makes things even simpler than in Kassner's derivation. So yes, the combination of those existing pieces of knowledge makes 10/"", 'a difference. The result will hopefully be of interest to those teaching ""physics first"" versions of general relativity. The derivation only makes use of elements one would introduce anyway (the meaning of the spacetime metric, the meaning of symmetries of solutions, 11/', 'the meaning of the Newtonian limit, and of tidal forces in that limit), and the math required in addition to the physics parts of the derivation is minimal. So if you teach ""physics first"", and think that a simple derivation of the Schwarzschild metric before you start 12/', 'exploring its consequences with your students might be a good addition, please have a look: https://t.co/SWG6CBHyWv 13/', 'P.S. 1: If you are wondering why I am tweeting about this today, while the eprint says 14 August - the arXiv team were apparently undecided about the classification, and the paper only ended up published on arXiv today. 14/', 'P.S. 2: There are of course other, sometimes very elegant simple derivations of the Schwarzschild metric out there, e.g. this one by Stanley Deser https://t.co/pnlwLIWrAB - but those typically require familiarity with more advanced tools of general relativity. 15/15', 'Full disclosure: I just received first referee reports on this, and one of the referees claims that the coordinate simplifications leading to the infalling-observer form is missing an important step. Eprint-readers, beware!', '@VergaraLautaro @johncarlosbaez Again, treat with caution. One of the referees raised serious concerns, and I will need to see if I can resolve them.', '@VergaraLautaro @johncarlosbaez Thank you!', '@atdotde @johncarlosbaez You would have the non-vacuum Baez-Bunn version (the second derivative of V would not be zero any more), and different boundary conditions at large r, of course.']",https://arxiv.org/abs/2009.03738,"The Schwarzschild metric is derived in a manner that does not require familiarity with the formalism of differential geometry beyond the ability to interpret a general spacetime metric. As such, the derivation is suitable for an undergraduate course on general relativity. The derivation uses infalling coordinates that are particularly well adapted to the situation, as well as Einstein's equation in the simple form introduced by Baez and Bunn. That version of the vacuum Einstein equations corresponds to requiring a particular local Newtonian limit: that, to first order, the deformation of a ""test ball"" of freely falling, initially-at-rest test particles is governed by the tidal forces of Newtonian gravity. ",Deriving the Schwarzschild solution from a local Newtonian limit
120,1303528950589603840,1067301932610478081,Ayaka Usui,['Our another new arXiv paper: Bayesian parameter estimation using Gaussian states and measurements - <LINK>\n\nThis provides a comprehensive investigation of Bayesian parameter estimation with single-mode Gaussian states and suitable Gaussian measurements.'],https://arxiv.org/abs/2009.03709,"Bayesian analysis is a framework for parameter estimation that applies even in uncertainty regimes where the commonly used local (frequentist) analysis based on the Cram\'er-Rao bound is not well defined. In particular, it applies when no initial information about the parameter value is available, e.g., when few measurements are performed. Here, we consider three paradigmatic estimation schemes in continuous-variable quantum metrology (estimation of displacements, phases, and squeezing strengths) and analyse them from the Bayesian perspective. For each of these scenarios, we investigate the precision achievable with single-mode Gaussian states under homodyne and heterodyne detection. This allows us to identify Bayesian estimation strategies that combine good performance with the potential for straightforward experimental realization in terms of Gaussian states and measurements. Our results provide practical solutions for reaching uncertainties where local estimation techniques apply, thus bridging the gap to regimes where asymptotically optimal strategies can be employed. ",Bayesian parameter estimation using Gaussian states and measurements
121,1303527872632467459,1067301932610478081,Ayaka Usui,['Our new arXiv paper: Simplifying multi-level thermal machines using virtual qubits - <LINK>\n\nThis provides an approach to simplify the master equation for a system coupled with several thermal machines!'],https://arxiv.org/abs/2009.03832,"Quantum thermodynamics often deals with the dynamics of small quantum machines interfacing with a large and complex environment. Virtual qubits, collisional models and reset master equations have become highly useful tools for predicting the qualitative behaviour of two-dimensional target systems coupled to few-qubit machines and a thermal environment. While few successes in matching the simplified model parameters for all possible physical systems are known, the qualitative predictions still allow for a general design of quantum machines irrespective of the implementation. We generalise these tools by introducing multiple competing virtual qubits for modelling multi-dimensional systems coupled to larger and more complex machines. By simulating the full physical dynamics for targets with three dimensions, we uncover general properties of reset models that can be used as `dials' to correctly predict the qualitative features of physical changes in a realistic setup and thus design autonomous quantum machines beyond a few qubits. We then present a general analytic solution of the reset model for arbitrary-dimensional systems coupled to multi-qubit machines. Finally, we showcase an improved three-level laser as an exemplary application of our results. ","Simplifying the design of multilevel thermal machines using virtual
  qubits"
122,1303359090454913025,1505457282,Pauline Ollitrault,['How to rapidly scale up electronic structure calculations on quantum computers to larger molecules? Check out our new paper on HF and DFT embedding schemes for VQE on arXiv: <LINK> ! \n@IBMResearch  @qiskit'],https://arxiv.org/abs/2009.01872,"In the near future, material and drug design may be aided by quantum computer assisted simulations. These have the potential to target chemical systems intractable by the most powerful classical computers. However, the resources offered by contemporary quantum computers are still limited, restricting the chemical simulations to very simple molecules. In order to rapidly scale up to more interesting molecular systems, we propose the embedding of the quantum electronic structure calculation into a classically computed environment obtained at the Hartree-Fock (HF) or Density Functional Theory (DFT) level of theory. We achieve this by constructing an effective Hamiltonian that incorporates a mean field potential describing the action of the inactive electrons on a selected Active Space (AS). The ground state of the AS Hamiltonian is determined by means of the Variational Quantum Eigensolver (VQE) algorithm. With the proposed iterative DFT embedding scheme we are able to obtain energy correction terms for a single pyridine molecule that outperform the Complete Active Space Self Consistent Field (CASSCF) results regardless of the chosen AS. ","Quantum HF/DFT-Embedding Algorithms for Electronic Structure
  Calculations: Scaling up to Complex Molecular Systems"
123,1303254365793333249,1133276730595192832,Raphael Sonabend,"['{distr6} introduces a novel approach to handling probability distributions in #rstats. In this paper we introduce {distr6}, compare different OOP paradigms in R, discuss new &amp; efficient math notation for distributions.\n\n<LINK>\n\n#datascience #statistics #coding']",https://arxiv.org/abs/2009.02993,"distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibility and scalability of R6, and the speed and efficiency of Rcpp. Over 50 probability distributions are currently implemented in the package with `core' methods including density, distribution, and generating functions, and more `exotic' ones including hazards and distribution function anti-derivatives. In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and product distributions. This paper presents the core functionality of the package and demonstrates examples for key use-cases. In addition this paper provides a critical review of the object-oriented programming paradigms in R and describes some novel implementations for design patterns and core object-oriented features introduced by the package for supporting distr6 components. ",distr6: R6 Object-Oriented Probability Distributions Interface in R
124,1303136309276487681,926430448556957696,David Gosset,"['New paper with Daniel Grier, Alex Kerzner, and Luke Schaeffer gives a fast classical algorithm to simulate multi-qubit measurement on planar graph states, with applications to Clifford circuit simulation.\n\n<LINK>']",https://arxiv.org/abs/2009.03218,"A general quantum circuit can be simulated classically in exponential time. If it has a planar layout, then a tensor-network contraction algorithm due to Markov and Shi has a runtime exponential in the square root of its size, or more generally exponential in the treewidth of the underlying graph. Separately, Gottesman and Knill showed that if all gates are restricted to be Clifford, then there is a polynomial time simulation. We combine these two ideas and show that treewidth and planarity can be exploited to improve Clifford circuit simulation. Our main result is a classical algorithm with runtime scaling asymptotically as $n^{\omega/2}<n^{1.19}$ which samples from the output distribution obtained by measuring all $n$ qubits of a planar graph state in given Pauli bases. Here $\omega$ is the matrix multiplication exponent. We also provide a classical algorithm with the same asymptotic runtime which samples from the output distribution of any constant-depth Clifford circuit in a planar geometry. Our work improves known classical algorithms with cubic runtime. A key ingredient is a mapping which, given a tree decomposition of some graph $G$, produces a Clifford circuit with a structure that mirrors the tree decomposition and which emulates measurement of the corresponding graph state. We provide a classical simulation of this circuit with the runtime stated above for planar graphs and otherwise $nt^{\omega-1}$ where $t$ is the width of the tree decomposition. Our algorithm incorporates two subroutines which may be of independent interest. The first is a matrix-multiplication-time version of the Gottesman-Knill simulation of multi-qubit measurement on stabilizer states. The second is a new classical algorithm for solving symmetric linear systems over $\mathbb{F}_2$ in a planar geometry, extending previous works which only applied to non-singular linear systems in the analogous setting. ",Fast simulation of planar Clifford circuits
125,1302997801673072640,10856622,Henrik Melin,"[""We really are in the infancy of understanding the upper atmosphere's of Uranus and Neptune - read all about it in my new review paper: \n\n<LINK>\n\n#Uranus #Neptune #IceGiants <LINK>""]",https://arxiv.org/abs/2009.02071,"We review the current understanding of the upper atmospheres of Uranus and Neptune, and explore the upcoming opportunities available to study these exciting planets. The ice giants are the least understood planets in the solar system, having been only visited by a single spacecraft, in 1986 and 1989, respectively. The upper atmosphere plays a critical role in connecting the atmosphere to the forces and processes contained within the magnetic field. For example, auroral current systems can drive charged particles into the atmosphere, heating it by way of Joule heating. Ground-based observations of H$_3^+$ provides a powerful remote diagnostic of the physical properties and processes that occur within the upper atmosphere, and a rich data set exists for Uranus. These observations span almost three decades and have revealed that the upper atmosphere has continuously cooled between 1992 and 2018 at about 8 K/year, from $\sim$750 K to $\sim$500 K. The reason for this trend remain unclear, but could be related to seasonally driven changes in the Joule heating rates due to the tilted and offset magnetic field, or could be related to changing vertical distributions of hydrocarbons. H$_3^+$ has not yet been detected at Neptune, but this discovery provides low-hanging fruit for upcoming facilities such as the James Webb Space Telescope (JWST) and the next generation of 30 metre telescopes. Detecting H$_3^+$ at Neptune would enable the characterisation of its upper atmosphere for the first time since 1989. To fully understand the ice giants we need dedicated orbital missions, in the same way the Cassini spacecraft explored Saturn. Only by combining in-situ observations of the magnetic field with in-orbit remote sensing can we get the complete picture of how energy moves between the atmosphere and the magnetic field. ",The upper atmospheres of Uranus and Neptune
126,1302993041926619138,1175368802458120193,Andreas Sander,"[""In case you haven't visited arXiv today, here is my newest paper with @jorick73 about the nature of massive He star mass loss: <LINK> \nAs often in #astrophysics and in science, it provides quite some new insights, while at the same time being only the beginning.""]",https://arxiv.org/abs/2009.01849,"The mass-loss rates of massive helium stars are one of the major uncertainties in modern astrophysics. Regardless of whether they were stripped by a binary companion or managed to peel off their outer layers by themselves, the influence and final fate of helium stars -- in particular the resulting black hole mass -- highly depends on their wind mass loss as stripped-envelope objects. While empirical mass-loss constraints for massive helium stars have improved over the last decades, the resulting recipes are limited to metallicities with the observational ability to sufficiently resolve individual stars. Yet, theoretical efforts have been hampered by the complexity of Wolf-Rayet (WR) winds arising from the more massive helium stars. In an unprecedented effort, we calculate next-generation stellar atmosphere models resembling massive helium main sequence stars with Fe-bump driven winds up to $500\,M_\odot$ over a wide metallicity range between $2.0$ and $0.02\,Z_\odot$. We uncover a complex $\Gamma_\text{e}$-dependency of WR-type winds and their metallicity-dependent breakdown. The latter can be related to the onset of multiple scattering, requiring higher $L/M$-ratios at lower metallicity. Based on our findings, we derive the first ever theoretically-motivated mass-loss recipe for massive helium stars. We also provide estimates for LyC and He II ionizing fluxes, finding stripped helium stars to contribute considerably at low metallicity. In sharp contrast to OB-star winds, the mass loss for helium stars scales with the terminal velocity. While limited to the helium main sequence, our study marks a major step towards a better theoretical understanding of helium star evolution. ",On the nature of massive helium star winds and Wolf-Rayet-type mass loss
127,1302990280807845888,4866589137,Dr. Nathan Adams,"[""New paper up on arxiv today features @peterhatfield's work on improving ML based photometric redshifts using gaussian processes and two deep fields that share the same broadband filters but probe colour space to different levels. Very cool stuff!\n\n<LINK>""]",https://arxiv.org/abs/2009.01952,"Wide-area imaging surveys are one of the key ways of advancing our understanding of cosmology, galaxy formation physics, and the large-scale structure of the Universe in the coming years. These surveys typically require calculating redshifts for huge numbers (hundreds of millions to billions) of galaxies - almost all of which must be derived from photometry rather than spectroscopy. In this paper we investigate how using statistical models to understand the populations that make up the colour-magnitude distribution of galaxies can be combined with machine learning photometric redshift codes to improve redshift estimates. In particular we combine the use of Gaussian Mixture Models with the high performing machine learning photo-z algorithm GPz and show that modelling and accounting for the different colour-magnitude distributions of training and test data separately can give improved redshift estimates, reduce the bias on estimates by up to a half, and speed up the run-time of the algorithm. These methods are illustrated using data from deep optical and near infrared data in two separate deep fields, where training and test data of different colour-magnitude distributions are constructed from the galaxies with known spectroscopic redshifts, derived from several heterogeneous surveys. ","Augmenting machine learning photometric redshifts with Gaussian mixture
  models"
128,1302869151959179270,101064332,Thomas Hueber,"['New paper: ""What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS"" preprint: <LINK>, accepted for presentation at #interspeech2020.', 'First paper of B. Stephenson as a PhD candidate (congrats!).  Very happy of this collaboration between @GipsaLab and @LIGLab (with @laurent_besacie), in the context of the @MIAI_UGA AI institute.', 'Abstract: In incremental text to speech synthesis (iTTS), the synthesizer produces an audio output before it has access to the entire input sentence.', 'We study the behavior of a neural seq2seq TTS system (@Tacotron2) when used in an incremental mode, i.e. when generating speech output for token n, the system has access to n + k tokens from the text sequence.', 'We first analyze the impact of this incremental policy on the evolution of the encoder representations of token n for different values of k (the lookahead parameter).', 'We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. We finally evaluate the effects of lookahead k at the decoder level, using a MUSHRA listening test.']",https://arxiv.org/abs/2009.02035,"In incremental text to speech synthesis (iTTS), the synthesizer produces an audio output before it has access to the entire input sentence. In this paper, we study the behavior of a neural sequence-to-sequence TTS system when used in an incremental mode, i.e. when generating speech output for token n, the system has access to n + k tokens from the text sequence. We first analyze the impact of this incremental policy on the evolution of the encoder representations of token n for different values of k (the lookahead parameter). The results show that, on average, tokens travel 88% of the way to their full context representation with a one-word lookahead and 94% after 2 words. We then investigate which text features are the most influential on the evolution towards the final representation using a random forest analysis. The results show that the most salient factors are related to token length. We finally evaluate the effects of lookahead k at the decoder level, using a MUSHRA listening test. This test shows results that contrast with the above high figures: speech synthesis quality obtained with 2 word-lookahead is significantly lower than the one obtained with the full sentence. ","What the Future Brings: Investigating the Impact of Lookahead for
  Incremental Neural TTS"
129,1302808523903045635,1105632244092321793,Dr. Jiayi Sun,"['New paper is out on Labor Day!\n\n<LINK>\n\nUsing CO maps from the PHANGS-ALMA survey, we present an unprecedented sample of &gt;100k molecular cloud measurements (which likely outnumber all previously observed giant molecular clouds combined) across 70 nearby galaxies.', 'With the HUGE sample, we quantify the statistical distribution of key molecular cloud properties (surface density, velocity dispersion, virial parameter, internal pressure) in typical star-forming environments thorough the Local Universe. The entire dataset is available online.', 'We found that the cloud properties clearly correlate with global properties of the host galaxy (e.g., stellar mass, SFR) and locations in each galaxy (e.g., in center/bar/arms). It suggests that molecular cloud populations are fundamentally coupled to their natal environments.', 'This paper represents a major science outcome from our PHANGS collaboration (https://t.co/74ol9Y7FOP). It is also a significant improvement over my paper in 2018, which presented ~30k measurements in 15 galaxies:\n\nhttps://t.co/k2UFR0JVKL', '@SuperASASSN 😁', '@dassanskriti1 Thx Sanskrit😁']",http://arxiv.org/abs/2009.01842,"Using the PHANGS-ALMA CO (2-1) survey, we characterize molecular gas properties on ${\sim}$100 pc scales across 102,778 independent sightlines in 70 nearby galaxies. This yields the best synthetic view of molecular gas properties on cloud scales across the local star-forming galaxy population obtained to date. Consistent with previous studies, we observe a wide range of molecular gas surface densities (3.4 dex), velocity dispersions (1.7 dex), and turbulent pressures (6.5 dex) across the galaxies in our sample. Under simplifying assumptions about sub-resolution gas structure, the inferred virial parameters suggest that the kinetic energy of the molecular gas typically exceeds its self-gravitational binding energy at ${\sim}$100 pc scales by a modest factor (1.3 on average). We find that the cloud-scale surface density, velocity dispersion, and turbulent pressure (1) increase towards the inner parts of galaxies, (2) are exceptionally high in the centers of barred galaxies (where the gas also appears less gravitationally bound), and (3) are moderately higher in spiral arms than in inter-arm regions. The galaxy-wide averages of these gas properties also correlate with the integrated stellar mass, star formation rate, and offset from the star-forming main sequence of the host galaxies. These correlations persist even when we exclude regions with extraordinary gas properties in galaxy centers, which contribute significantly to the inter-galaxy variations. Our results provide key empirical constraints on the physical link between molecular cloud populations and their galactic environment. ","Molecular Gas Properties on Cloud Scales Across the Local Star-forming
  Galaxy Population"
130,1302363872351969286,725121185952976896,Ilenna Jones,"['What are nonlinear, dendrite-complete neurons computationally capable of? Check out our new paper, “Can Single Neurons Solve MNIST? The computational power of biological dendritic trees” <LINK> <LINK>', 'Single point, dendrite-less neuron models are of much use in neuroscience and for deep learning neural networks (composed of many point neurons!) But this abstraction assumes that dendrites are linear, but accumulated evidence shows they are incredibly nonlinear. https://t.co/jaRJlxjCDP', 'So what can a nonlinear, dendrite-complete neuron capable do? We treat our hypothetical neuron model as an input/output device to ask what the neuron is computationally capable of. We ask this by using an abstracted neuron model made out of a deep learning network.', 'The deep learning network is modified to have a dendritic morphology constraint: a binary tree structure. We use deep learning methods to easily train a neuron-like model, called a 1-tree. We suspect that this sparse binary tree structure will have an impact on model performance. https://t.co/3ThFfWYGP2', 'Assuming that neurons have a binary output (spike or no spike), the model was trained on binary image classification tasks. 2 classes were chosen by determining the least discriminable pair. https://t.co/kNS4w4G8rb', 'The 1-tree performance on these tasks was compared to a linear classifier and a 2 layer fully connected neural network (FCNN). Largely, nonlinearities seems to improve 1-tree performance above the linear classifier, but tree sparsity lowers 1-tree performance below the FCNN. https://t.co/3Yo5XeM2Zf', 'What other biological, architecture-like properties could impact performance? Did you know a neuron can receive on average 4 repeated inputs from the same presynaptic neuron? Let’s try that in our model and call the “1”-tree a “k”-tree, k for the number of repeats. https://t.co/fAaLuhovJ2', 'The k-tree performance improves with increased “k”, and even meets the performance of a comparable FCNN for Fashion MNIST, E-MNIST, and USPS. https://t.co/mzwlsDIo34', 'Takeaways are that dendritic tree structure constraints can limit neuron model performance, but increased input repeats can expand model performance. Lastly, this task-based approach to neuron modeling brings neuroscience and deep learning closer together. Thanks for listening! https://t.co/EksDDSFcO8', ""If you want to see my talks on this, check out @Numenta's video and @SIGNNeuro's videos on this paper!: https://t.co/nRCvUcOwdx and https://t.co/yGinJFktcm"", 'Lastly, find the code here! https://t.co/0l4ZQBRgCb']",https://arxiv.org/abs/2009.01269,"Physiological experiments have highlighted how the dendrites of biological neurons can nonlinearly process distributed synaptic inputs. This is in stark contrast to units in artificial neural networks that are generally linear apart from an output nonlinearity. If dendritic trees can be nonlinear, biological neurons may have far more computational power than their artificial counterparts. Here we use a simple model where the dendrite is implemented as a sequence of thresholded linear units. We find that such dendrites can readily solve machine learning problems, such as MNIST or CIFAR-10, and that they benefit from having the same input onto several branches of the dendritic tree. This dendrite model is a special case of sparse network. This work suggests that popular neuron models may severely underestimate the computational power enabled by the biological fact of nonlinear dendrites and multiple synapses per pair of neurons. The next generation of artificial neural networks may significantly benefit from these biologically inspired dendritic architectures. ","Can Single Neurons Solve MNIST? The Computational Power of Biological
  Dendritic Trees"
131,1302185366611472384,1037250350561091584,Joris Guerin,"['Using person re-identification in practical applications?\n\nIn our #PRLetters paper, the monitoring agents are an inherent part of the ReID pipeline, leading to new evaluation metrics for the full frame setting. Check it out: <LINK>!\n\n@ufrn @uff_br <LINK>', 'With @OliverSumari, @josemiki93,  @LuigyMa and Esteban Clua']",https://arxiv.org/abs/2009.01377,"With the major adoption of automation for cities security, person re-identification (Re-ID) has been extensively studied recently. In this paper, we argue that the current way of studying person re-identification, i.e. by trying to re-identify a person within already detected and pre-cropped images of people, is not sufficient to implement practical security applications, where the inputs to the system are the full frames of the video streams. To support this claim, we introduce the Full Frame Person Re-ID setting (FF-PRID) and define specific metrics to evaluate FF-PRID implementations. To improve robustness, we also formalize the hybrid human-machine collaboration framework, which is inherent to any Re-ID security applications. To demonstrate the importance of considering the FF-PRID setting, we build an experiment showing that combining a good people detection network with a good Re-ID model does not necessarily produce good results for the final application. This underlines a failure of the current formulation in assessing the quality of a Re-ID model and justifies the use of different metrics. We hope that this work will motivate the research community to consider the full problem in order to develop algorithms that are better suited to real-world scenarios. ","Towards Practical Implementations of Person Re-Identification from Full
  Video Frames"
132,1301904169230508038,60893773,James Bullock,['Folks following #lgsymp2020 may find new paper by @AstronoMerc_ interesting: predicted AND observed relationship between stellar metallicity gradients in dwarf galaxies and age: younger galaxies have flatter gradients.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2009.01241,"We explore the origin of stellar metallicity gradients in simulated and observed dwarf galaxies. We use FIRE-2 cosmological baryonic zoom-in simulations of 26 isolated galaxies as well as existing observational data for 10 Local Group dwarf galaxies. Our simulated galaxies have stellar masses between $10^{5.5}$ and $10^{8.6} \msun$. Whilst gas-phase metallicty gradients are generally weak in our simulated galaxies, we find that stellar metallicity gradients are common, with central regions tending to be more metal-rich than the outer parts. The strength of the gradient is correlated with galaxy-wide median stellar age, such that galaxies with younger stellar populations have flatter gradients. Stellar metallicty gradients are set by two competing processes: (1) the steady ""puffing"" of old, metal-poor stars by feedback-driven potential fluctuations, and (2) the accretion of extended, metal-rich gas at late times, which fuels late-time metal-rich star formation. If recent star formation dominates, then extended, metal-rich star formation washes out pre-existing gradients from the ""puffing"" process. We use published results from ten Local Group dwarf galaxies to show that a similar relationship between age and stellar metallicity-gradient strength exists among real dwarfs. This suggests that observed stellar metallicity gradients may be driven largely by the baryon/feedback cycle rather than by external environmental effects. ","A Relationship Between Stellar Metallicity Gradients and Galaxy Age in
  Dwarf Galaxies"
133,1301903449982992389,373267070,Andrew Zeitlin,"['Should developing countries use training or cash grants to combat youth unemployment?\nCraig McIntosh &amp; I have a new paper out today helping answer that question, working in Rwanda w/ @poverty_action, @EDCtweets, @GiveDirectly, &amp; @USAID\n\nWorking paper: <LINK>. <LINK>', ""This paper is unusual in that it has been through the Journal of Development Economics's pre-results review process. The paper benefited greatly from their comments. This is a nice model for projects like this, where you want to get things right up front for policy conversations."", ""Why benchmark job training against cash? Such programs attract huge amounts of funding, even though impacts are very mixed (pic summarizes @dmckenzie001's WBRO 2017 review). The predecessor to the program we evaluate had positive impacts based on RCT evidence (Alcid 2014). https://t.co/sxziC7obJD"", 'Working with @EDCtweets and @GiveDirectly, we evaluate the Huguka Dukore job training program against control, both comparably sized and substantially larger cash transfers, and a combined arm providing training + cash; randomization is at the individual level.', '18 months after baseline, we find neither program moved overall employment rates, but both training and mid-sized transfers increase hours worked. Interestingly, this masks the fact that training increased non-agric wage labor and cash moved people into non-agric microenterprise. https://t.co/K9WkqLdg6j', ""A cash transfer equivalent to the cost of the training nearly doubles income and quadruples productive assets (compared to control). Training also boosts these -- but not as much -- striking considering there's no transfer in the training program!"", 'We find no evidence of complementarities:  impacts in the Combined arm look remarkably similar to the sum of cash + training component impacts.  So no sign here of compounding effects from multiple constraints.', 'We find no evidence of heterogeneity on pre-specified subgroups: by gender, risk aversion, or baseline consumption, and we test for interference using variation in saturation within villages and find no evidence.', 'Taking these results together, cash ""wins"" a cost-equivalent comparison here, which we find by fitting a regression line to the impacts of cash (upper shaded circles) and comparing it with training (diamond) at a value that would have cost the same to deliver. https://t.co/ryj05Tfj45', ""We can also compare benefit-cost ratios for different transfer values, shown as slopes below. For many outcomes, impacts of larger transfers don't rise in proportion to costs, relative to small &amp; mid-sized transfers. Better to give mid amounts to more people. https://t.co/jrLpkRjXKD"", ""Still, these effects are relatively short term (cf @cblatts, Fiala, Martinez 2020), so part of the hope for training is greater persistence. Our 36-month follow-up will begin to speak to this, albeit through the lens of resilience to the pandemic's economic shock."", 'Finally, thanks to partners @poverty_action, @EDCtweets, @GiveDirectly, and @USAID. These multi-party comparative studies take a village to pull off, and we were lucky to work with people &amp; institutions so interested in generating evidence.', '...  and these great reflections from @jcarbiv  speak better than I possibly could to all that is involved in moving these conversations on evidence in large institutions like USAID: https://t.co/mIvbGi1fCf']",https://arxiv.org/abs/2009.01749,"We use a randomized experiment to compare a workforce training program to cash transfers in Rwanda. Conducted in a sample of poor and underemployed youth, this study measures the impact of the training program not only relative to a control group but relative to the counterfactual of simply disbursing the cost of the program directly to beneficiaries. While the training program was successful in improving a number of core outcomes (productive hours, assets, savings, and subjective well-being), cost-equivalent cash transfers move all these outcomes as well as consumption, income, and wealth. In the head-to-head costing comparison cash proves superior across a number of economic outcomes, while training outperforms cash only in the production of business knowledge. We find little evidence of complementarity between human and physical capital interventions, and no signs of heterogeneity or spillover effects. ","Using Household Grants to Benchmark the Cost Effectiveness of a USAID
  Workforce Readiness Program"
134,1301878132266409984,19011452,Peter Brown,"['New paper on high precision optical measurements of meteors\n<LINK>\nMain result: mm-sized meteoroids have tensile strengths of a few kPa, about the same as a snow flake whether on cometary or asteroidal type orbits. @WesternU #meteor @BadAstronomer @ProfBrianCox <LINK>']",https://arxiv.org/abs/2009.01372,"Context. The mirror tracking system of the Canadian Automated Meteor Observatory (CAMO) can track meteors in real time, providing an effective angular resolution of 1 arc second and a temporal resolution of 100 frames per second. Aims. We describe the upgraded hardware and give details of the data calibration and reduction pipeline. We investigate the influence of meteor morphology on radiant and velocity measurement precision, and use direct observations of meteoroid fragmentation to constrain their compressive strengths. Methods. On July 21, 2017, CAMO observed a ~4 second meteor on a JFC orbit. It had a shallow entry angle ~8 deg and 12 fragments were visible in the narrow-field video. The event was manually reduced and the exact moment of fragmentation was determined. The aerodynamic ram pressure at the moment of fragmentation was used as a proxy for compressive strength, and strengths of an additional 19 fragmenting meteoroids were measured in the same way. The uncertainty in the atmosphere mass density was estimated to be +/-25% using NAVGEM-HA data. Results. We find that meteor trajectory accuracy significantly depends on meteor morphology. The CAMO radiant and initial velocity precision for non-fragmenting meteors with short wakes is ~0.5' and 1 m/s, while that for meteors with fragments or long wakes is similar to non-tracking, moderate field of view optical systems (5', ~50 m/s). Measured compressive strengths of 20 fragmenting meteoroids (with less precise radiants due to their morphology) was in the range of 1-4 kPa, which is in excellent accord with Rosetta in-situ measurements of 67P. Fragmentation type and strength do not appear to be dependent on orbit. The mass index of the 12 fragments in the July 21 meteoroid was very high (s = 2.8), indicating possible progressive fragmentation. ","High precision meteor observations with the Canadian Automated Meteor
  Observatory -- Data reduction pipeline and application to meteoroid
  mechanical strength measurements"
135,1301840198926508032,872072000906424321,Martin Mundt,"['Been quiet for a while, so I\'m happy to share a progress update. \n\nOur new big paper: ""A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning"" just came online at <LINK> 1/3 <LINK>', 'We identify overlooked lessons from older literature in the adjacent fields of active learning and open set recognition. With these lessons, we propose a coherent framework to bridge current CL with active learning and learning in an open world beyond static benchmark sets 2/3', ""Read if you are interested in the paradigm's synergies and:\n* principled core set approximations of a dataset\n* active queries that avoid uninformative/redundant samples\n* automated task order selection to improve performance\n* robust CL &amp; AL in the presence of corrupted data\n3/3""]",https://arxiv.org/abs/2009.01797,"Current deep learning research is dominated by benchmark evaluation. A method is regarded as favorable if it empirically performs well on the dedicated test set. This mentality is seamlessly reflected in the resurfacing area of continual learning, where consecutively arriving sets of benchmark data are investigated. The core challenge is framed as protecting previously acquired representations from being catastrophically forgotten due to the iterative parameter updates. However, comparison of individual methods is nevertheless treated in isolation from real world application and typically judged by monitoring accumulated test set performance. The closed world assumption remains predominant. It is assumed that during deployment a model is guaranteed to encounter data that stems from the same distribution as used for training. This poses a massive challenge as neural networks are well known to provide overconfident false predictions on unknown instances and break down in the face of corrupted data. In this work we argue that notable lessons from open set recognition, the identification of statistically deviating data outside of the observed dataset, and the adjacent field of active learning, where data is incrementally queried such that the expected performance gain is maximized, are frequently overlooked in the deep learning era. Based on these forgotten lessons, we propose a consolidated view to bridge continual learning, active learning and open set recognition in deep neural networks. Our results show that this not only benefits each individual paradigm, but highlights the natural synergies in a common framework. We empirically demonstrate improvements when alleviating catastrophic forgetting, querying data in active learning, selecting task orders, while exhibiting robust open world application where previously proposed methods fail. ","A Wholistic View of Continual Learning with Deep Neural Networks:
  Forgotten Lessons and the Bridge to Active and Open World Learning"
136,1301792837214797827,561899047,Aki Vehtari,"['Akash Dhaka, @AleexCatalina, @Michael_riis, @MansMeg, @jhhhuggins, and I have a new paper ""Robust, Accurate Stochastic Optimization for Variational Inference"" <LINK> <LINK>', 'tl;dr We combine Polyak–Ruppert averaging with MCMC convergence diagnostics to make stochastic optimization in variational inference more robust or get a warning when it performs badly. These help to make automated use of VI safer in probabilistic programming frameworks. https://t.co/3RUKPPdzfs', 'Many VI methods use stochastic optimization either due to using random mini-batches of data or Monte Carlo to estimate expectations of the divergences. For example. For example, autodiff VI (ADVI) in Stan has stochastic target and gradients due to the latter.', 'To fulfill Robbins-Monroe condition of reaching eventually the optimum, the step size is usually gradually decreased. Although this guarantees asymptotic convergence, it may take unfeasible amount of time and the last iteration in finite time can be far from the optimum.', 'Under certain conditions, stochastic optimization with a fixed step size converges to a finite variance stationary process around the optimum. Average of the iterations converges towards the optimum faster. This is an old but under-used idea, aka Polyak–Ruppert averaging.', 'Recently iterate averaging has been used also with name stochastic weight averaging (SWA) in context of deep learning.', 'What we add is 1) a diagnostic for detecting when we have reached stationarity and can start averaging, and 2) a standard error estimate to decide when we can stop averaging or give up if the standard error is not decreasing (indicating violation of conditions).', 'The diagnostics are familiar from MCMC convergence literature (e.g. Rhat, MCSE, and autocorrelation) and VI diagnostics literature (e.g. Pareto k).', '@lauretig Oops, forgot that. The repo will be public next week.']",https://arxiv.org/abs/2009.00666,"We consider the problem of fitting variational posterior approximations using stochastic optimization methods. The performance of these approximations depends on (1) how well the variational family matches the true posterior distribution,(2) the choice of divergence, and (3) the optimization of the variational objective. We show that even in the best-case scenario when the exact posterior belongs to the assumed variational family, common stochastic optimization methods lead to poor variational approximations if the problem dimension is moderately large. We also demonstrate that these methods are not robust across diverse model types. Motivated by these findings, we develop a more robust and accurate stochastic optimization framework by viewing the underlying optimization algorithm as producing a Markov chain. Our approach is theoretically motivated and includes a diagnostic for convergence and a novel stopping rule, both of which are robust to noisy evaluations of the objective function. We show empirically that the proposed framework works well on a diverse set of models: it can automatically detect stochastic optimization failure or inaccurate variational approximation ","Robust, Accurate Stochastic Optimization for Variational Inference"
137,1301684882347757569,20926161,Michael Ekstrand,"['🎺 new preprint, @RajAmifa\'s first paper: ""Comparing Fair Ranking Metrics"", to be presented at FAccTrec #recsys2020. @BSUGradColl @ComputingPhD @boisestatecs @PIReTship <LINK>', ""@RajAmifa @BSUGradColl @ComputingPhD @boisestatecs @PIReTship This work is #NSFfunded, through my CAREER award funding Amifa, and an REU supplement that paid for Ananda and Connor to spend the summer working with us. And Connor's staying on for the academic year.""]",https://arxiv.org/abs/2009.01311,"Ranked lists are frequently used by information retrieval (IR) systems to present results believed to be relevant to the users information need. Fairness is a relatively new but important aspect of these rankings to measure, joining a rich set of metrics that go beyond traditional accuracy or utility constructs to provide a more holistic understanding of IR system behavior. In the last few years, several metrics have been proposed to quantify the (un)fairness of rankings, particularly with respect to particular group(s) of content providers, but comparative analyses of these metrics -- particularly for IR -- is lacking. There is limited guidance, therefore, to decide what fairness metrics are applicable to a specific scenario, or assessment of the extent to which metrics agree or disagree applied to real data. In this paper, we describe several fair ranking metrics from existing literature in a common notation, enabling direct comparison of their assumptions, goals, and design choices; we then empirically compare them on multiple data sets covering both search and recommendation tasks. ",Comparing Fair Ranking Metrics
138,1301535421558644738,2902687319,Mike Hudson,['New paper led by Kyle Oman on HI stripping and star formation quenching timescales. Stripping before pericentre in massive clusters but very inefficient in poor (LG-like) groups. Quenching times are long: the clock below is 0 when sat @ pericentre. <LINK> <LINK>'],https://arxiv.org/abs/2009.00667,"We combine orbital information from N-body simulations with an analytic model for star formation quenching and SDSS observations to infer the differential effect of the group/cluster environment on star formation in satellite galaxies. We also consider a model for gas stripping, using the same input supplemented with HI fluxes from the ALFALFA survey. The models are motivated by and tested on the Hydrangea cosmological hydrodynamical simulation suite. We recover the characteristic times when satellite galaxies are stripped and quenched. Stripping in massive ($M_\mathrm{ vir}\sim 10^{14.5}\,\mathrm{M}_\odot$) clusters typically occurs at or just before the first pericentric passage. Lower mass ($\sim10^{13.5}\,\mathrm{M}_\odot$) groups strip their satellites on a significantly longer (by $\sim3\,\mathrm{Gyr}$) timescale. Quenching occurs later: Balmer emission lines typically fade $\sim3.5\,\mathrm{Gyr}$ ($5.5\,\mathrm{Gyr}$) after first pericentre in clusters (groups), followed a few hundred $\mathrm{Myr}$ later by reddenning in $(g-r)$ colour. These `delay timescales' are remarkably constant across the entire satellite stellar mass range probed ($\sim10^{9.5}-10^{11}\,\mathrm{M}_\odot$), a feature closely tied to our treatment of `group pre-processing'. The lowest mass groups in our sample ($\sim10^{12.5}\,\mathrm{M}_\odot$) strip and quench their satellites extremely inefficiently: typical timescales may approach the age of the Universe. Our measurements are qualitatively consistent with the `delayed-then-rapid' quenching scenario advocated for by several other studies, but we find significantly longer delay times. Our combination of a homogeneous analysis and input catalogues yields new insight into the sequence of events leading to quenching across wide intervals in host and satellite mass. ","A homogeneous measurement of the delay between the onsets of gas
  stripping and star formation quenching in satellite galaxies of groups and
  clusters"
139,1301120932149329922,730429184,Andy King,"['In his new @MICCAI2020 #STACOM paper, Bram Ruijsink shows how results of downstream image analysis tasks can be used to boost training data for #cardiac #MRI #segmentation - <LINK> @KingsImaging @SmartHeartUK #AI #MachineLearning']",https://arxiv.org/abs/2009.00584,"One of the challenges in developing deep learning algorithms for medical image segmentation is the scarcity of annotated training data. To overcome this limitation, data augmentation and semi-supervised learning (SSL) methods have been developed. However, these methods have limited effectiveness as they either exploit the existing data set only (data augmentation) or risk negative impact by adding poor training examples (SSL). Segmentations are rarely the final product of medical image analysis - they are typically used in downstream tasks to infer higher-order patterns to evaluate diseases. Clinicians take into account a wealth of prior knowledge on biophysics and physiology when evaluating image analysis results. We have used these clinical assessments in previous works to create robust quality-control (QC) classifiers for automated cardiac magnetic resonance (CMR) analysis. In this paper, we propose a novel scheme that uses QC of the downstream task to identify high quality outputs of CMR segmentation networks, that are subsequently utilised for further network training. In essence, this provides quality-aware augmentation of training data in a variant of SSL for segmentation networks (semiQCSeg). We evaluate our approach in two CMR segmentation tasks (aortic and short axis cardiac volume segmentation) using UK Biobank data and two commonly used network architectures (U-net and a Fully Convolutional Network) and compare against supervised and SSL strategies. We show that semiQCSeg improves training of the segmentation networks. It decreases the need for labelled data, while outperforming the other methods in terms of Dice and clinical metrics. SemiQCSeg can be an efficient approach for training segmentation networks for medical image data when labelled datasets are scarce. ",Quality-aware semi-supervised learning for CMR segmentation
140,1301112675754221570,131879500,John Ilee,"['Excited to announce our new paper out today.  We examine how the Square Kilometre Array (SKA, @SKA_telescope) will be able to observe and characterise planet-hosting young discs: <LINK> <LINK>', 'We show that SKA1-MID will be able to observe the emission of cm-sized pebbles in such a disc, and particularly any gap/ring structure that is carved by planets forming. This is thanks to its extremely high resolution (and sensitivity). \n\nJust look at that uv coverage... 🤤 https://t.co/vUq0gr1Q0S', 'ALMA has made amazing progress at this over the past few years, but it is becoming clear that (sub)mm observations may not trace the bulk of the disc material.  Moving to the cm overcomes this, and SKA will be key in determining accurate disc properties (watch this space).', ""Huge thanks to @cassidentprone, @cwalshastrochem, Izaskun Jiménez-Serra, Christophe Pinte, Jason Terry, Tyler Bourke, and Melvin Hoare for helping pull this together (and of course @SKA_telescope!).  We're planning a few more papers in this series, so keep an eye out for those."", ""Finally, my curiosity got the better of me. I decided to see what something like HL Tau might look like with the SKA.  Result: It's VERY cool (though a little artistic freedom applies here).  The late 2020's will be exciting! https://t.co/8MrRzfoDUv""]",https://arxiv.org/abs/2009.00562,"High angular resolution observations of discs at mm wavelengths (on scales of a few au) are now commonplace, but there is a current lack of a comparable angular resolution for observations at cm wavelengths. This presents a significant barrier to improving our understanding of planet formation, in particular how dust grains grow from mm to cm sizes. In this paper, we examine the ability of the Square Kilometre Array (SKA) to observe dust substructure in a young, planet-forming disc at cm wavelengths. We use dusty hydrodynamics and continuum radiative transfer to predict the distribution and emission of 1 cm dust grains (or pebbles) within the disc, and simulate continuum observations with the current SKA1-MID design baseline at frequencies of 12.5 GHz (Band 5b, ~2.4 cm) on 5-10 au scales. The SKA will provide high-fidelity observations of the cm dust emission substructure in discs for integration times totalling 100's of hours. Radial structure can be obtained at a sufficient resolution and S/N from shorter (10's of hours) integration times by azimuthal averaging in the image plane. By modelling the intensity distribution directly in the visibility plane, it is possible to recover a similar level of (axisymmetric) structural detail from observations with integration times 1-2 orders of magnitude lower than required for high-fidelity imaging. Our results demonstrate that SKA1-MID will provide crucial constraints on the distribution and morphology of the raw material for building planets, the pebbles in protoplanetary discs. ","Observing protoplanetary discs with the Square Kilometre Array -- I.
  Characterising pebble substructure caused by forming planets"
141,1301074690811756544,348979225,Alec MacKinnon,"[""with Sergio, @guiguesp and Jordi, here's our new paper on interpreting the gamma-ray spectrum of the most energetic solar flares <LINK> #solarflares #gammarays #pions""]",https://arxiv.org/abs/2009.00414,"Gamma-ray continuum at > 10 MeV photon energy yields information on > 0.2 - 0.3 GeV/nucleon ions at the Sun. We use the general-purpose Monte Carlo code FLUKA (FLUktuierende KAskade) to model the transport of ions injected into thick and thin target sources, the nuclear processes that give rise to pions and other secondaries and the escape of the resulting photons from the atmosphere. We give examples of photon spectra calculated with a range of different assumptions about the primary ion velocity distribution and the source region. We show that FLUKA gives results for pion decay photon emissivity in agreement with previous treatments. Through the directionality of secondary products, as well as Compton scattering and pair production of photons prior to escaping the Sun, the predicted spectrum depends significantly on the viewing angle. Details of the photon spectrum in the 100 MeV range may constrain the angular distribution of primary ions and the depths at which they interact. We display a set of thick-target spectra produced making various assumptions about the incident ion energy and angular distribution and the viewing angle. If ions are very strongly beamed downward, or ion energies do not extend much above 1 GeV/nucleon, the photon spectrum is highly insensitive to details of the ion distribution. Under the simplest assumptions, flares observed near disc centre should not display significant radiation above 1 GeV photon energy. We give an example application to Fermi Large Area Telescope data from the flare of 12 June 2010. ","FLUKA Simulations of Pion Decay Gamma-radiation from Energetic Flare
  Ions"
142,1301013157398245376,1611666830,Arne Løhre Grimsmo 🧡,"['New paper with Thomas Smith, @maja_cassidy, @Prof_D_Reilly  and @BartlettQuantum on readout of Majorana qubits: <LINK>. Main takeaways:\n\n1/3', '1. Dispersive readout looks very promising.\n\n2. But, there is some fine-print: The ""QNDness"" depends on the details of the Majorana qubit design and the readout protocol. In some cases the measured Majorana parity is manifestly conserved by the light-matter interaction.\n\n2/3', 'This leads to a stronger notion of QND readout than for conventional superconducting qubits. \n\n3. The ""longitudinal readout"" scheme we came up with in 2019 also looks extremely promising. Manifestly QND and might be faster and higher fidelity than dispersive.\n\n3/3', ""@tahantech @maja_cassidy @Prof_D_Reilly @BartlettQuantum Yeah I believe that's true, the paper is about gates but he proposes longitudinal readout too in the last two sentences of his paper""]",https://arxiv.org/abs/2009.00027,"We analyze a readout scheme for Majorana qubits based on dispersive coupling to a resonator. We consider two variants of Majorana qubits: the Majorana transmon and the Majorana box qubit. In both cases, the qubit-resonator interaction can produce sizeable dispersive shifts in the MHz range for reasonable system parameters, allowing for submicrosecond readout with high fidelity. For Majorana transmons, the light-matter interaction used for readout manifestly conserves Majorana parity, which leads to a notion of quantum nondemolition (QND) readout that is stronger than for conventional charge qubits. In contrast, Majorana box qubits only recover an approximately QND readout mechanism in the dispersive limit where the resonator detuning is large. We also compare dispersive readout to longitudinal readout for the Majorana box qubit. We show that the latter gives faster and higher fidelity readout for reasonable parameters, while having the additional advantage of being manifestly QND, and so may prove to be a better readout mechanism for these systems. ",Dispersive readout of Majorana qubits
143,1313365714397323265,2377407248,Daniel Whiteson,"['New paper! (with my great UCI ML colleagues)\n\n<LINK>\n\nFast simulation for particle physics... without GANs.  \n\nWe used an auto-regressive model and explicitly built it to handle the sparseness of the data. <LINK>', 'It generates the pixels one at a time. https://t.co/zxof9MVdrN', 'And it does pretty well! https://t.co/OUkBAOQGwH', 'The best test is whether a network trained on our fast-sim images can learn to distinguish images from the full simulation.  It can! https://t.co/XFfOwO2gq9', 'Great work by Yadong Lu, Julian Collado and Pierre Baldi.', '@martin_trapp @heghbalz Yes, will be posted to the UCI ML repo after paper is reviewed.']",https://arxiv.org/abs/2009.14017,"Generation of simulated data is essential for data analysis in particle physics, but current Monte Carlo methods are very computationally expensive. Deep-learning-based generative models have successfully generated simulated data at lower cost, but struggle when the data are very sparse. We introduce a novel deep sparse autoregressive model (SARM) that explicitly learns the sparseness of the data with a tractable likelihood, making it more stable and interpretable when compared to Generative Adversarial Networks (GANs) and other methods. In two case studies, we compare SARM to a GAN model and a non-sparse autoregressive model. As a quantitative measure of performance, we compute the Wasserstein distance ($W_p$) between the distributions of physical quantities calculated on the generated images and on the training images. In the first study, featuring images of jets in which 90% of the pixels are zero-valued, SARM produces images with $W_p$ scores that are 24-52% better than the scores obtained with other state-of-the-art generative models. In the second study, on calorimeter images in the vicinity of muons where 98% of the pixels are zero-valued, SARM produces images with $W_p$ scores that are 66-68% better. Similar observations made with other metrics confirm the usefulness of SARM for sparse data in particle physics. Original data and software will be made available upon acceptance of the manuscript from the UCI Machine Learning in Physics web portal at: this http URL ","SARM: Sparse Autoregressive Model for Scalable Generation of Sparse
  Images in Particle Physics"
144,1312560990467186688,450466066,Soujanya Poria,['Read our new paper that sheds light on the role of context in utterance-level dialogue understanding using several context-perturbation techniques.\n \nPaper: <LINK>\nCode: <LINK>\n\n#NLProc <LINK>'],https://arxiv.org/abs/2009.13902,"The recent abundance of conversational data on the Web and elsewhere calls for effective NLP systems for dialog understanding. Complete utterance-level understanding often requires context understanding, defined by nearby utterances. In recent years, a number of approaches have been proposed for various utterance-level dialogue understanding tasks. Most of these approaches account for the context for effective understanding. In this paper, we explore and quantify the role of context for different aspects of a dialogue, namely emotion, intent, and dialogue act identification, using state-of-the-art dialog understanding methods as baselines. Specifically, we employ various perturbations to distort the context of a given utterance and study its impact on the different tasks and baselines. This provides us with insights into the fundamental contextual controlling factors of different aspects of a dialogue. Such insights can inspire more effective dialogue understanding models, and provide support for future text generation approaches. The implementation pertaining to this work is available at this https URL ",Utterance-level Dialogue Understanding: An Empirical Study
145,1311344875321913347,1002014071,Frank Wilczek,['New paper on chromatic interferometry <LINK> will appear in Optics Express @OpticalSociety .  Opening the doors of perception a crack wider ... <LINK>'],https://arxiv.org/abs/2009.08217,"By developing a `two-crystal' method for color erasure, we can broaden the scope of chromatic interferometry to include optical photons whose frequency difference falls outside of the 400 nm to 4500 nm wavelength range, which is the passband of a PPLN crystal. We demonstrate this possibility experimentally, by observing interference patterns between sources at 1064.4 nm and 1063.6 nm, corresponding to a frequency difference of about 200 GHz. ",Chromatic interferometry with small frequency differences
146,1311321005348929536,3319563187,Xiaohui Fan,"['Paper day: with no observing runs, we had the time to finish this massive paper on IGM optical depth at the epoch of reionization with a new z&gt;6.3 quasar sample. <LINK>. Congrats Jinyi Yang, @feigewang, @joe_hennawi, @freddavies and others. <LINK>', '@brant_robertson @feigewang @joe_hennawi @freddavies Thanks Brant! I have seen the paper. Will study it carefully.']",https://arxiv.org/abs/2009.13544,"We report new measurements of the intergalactic medium (IGM) Ly$\alpha$ and Ly$\beta$ effective optical depth at $5.3<z<6.5$, using a new sample of quasar sightlines including 32 quasars at $6.308\le z\le7.00$. These quasars provide a large statistical sample to measure the IGM evolution during the transition phase of the reionization epoch. We construct a data set of deep optical spectra of these quasars using VLT, Keck, Gemini, LBT, and MMT. We measure the Ly$\alpha$ effective optical depth at $5.36<z<6.57$ using the Ly$\alpha$ forests of both individual spectra and the stacked spectrum. The large scatter of individual measurements is consistent with previous work, suggesting an inhomogeneous reionization process. Combining our new measurements and previous results, we obtain a best-fit for the Ly$\alpha$ effective optical depth evolution at $z>5.3$, $\tau\propto(1+z)^{8.6\pm1.0}$. We then estimate the observed Ly$\beta$ effective optical depth using Ly$\beta$ forests and convert them to Ly$\alpha$ optical depth for comparison, which provides additional constraints on the evolution of the IGM optical depth. The Ly$\beta$-based measurements are generally in agreement with the best-fit evolution obtained from Ly$\alpha$ forests. Using this new sample, we identify 389 Ly$\alpha$ and 50 Ly$\beta$ transmission spikes at $5.5<z<6.3$. The upper limits of Ly$\alpha$ optical depth estimated using transmission spikes are well consistent with our best-fit evolution. The evolution in number density of these high-redshift transmission spikes suggests a rapid transition phase at the end of the reionization. Comparison of our optical depth measurements with hydrodynamical simulations indicates a IGM neutral hydrogen fraction $\langle f_{\rm HI}\rangle\gtrsim10^{-4}$ at $z=6$. ","Measurements of the z ~ 6 Intergalactic Medium Optical Depth and
  Transmission Spikes Using a New z &gt; 6.3 Quasar Sample"
147,1311320413050490881,1550676954,Jordan S Read (he/him),"[""We've made some excellent progress on process-guided #MachineLearning for the prediction of temperature and flow in rivers. See new paper in ArXiv led by Xiaowei Jia: <LINK>. Recurrent graph networks provide information flow to mimic the natural system, &amp; more... <LINK>""]",https://arxiv.org/abs/2009.12575,"This paper proposes a physics-guided machine learning approach that combines advanced machine learning models and physics-based models to improve the prediction of water flow and temperature in river networks. We first build a recurrent graph network model to capture the interactions among multiple segments in the river network. Then we present a pre-training technique which transfers knowledge from physics-based models to initialize the machine learning model and learn the physics of streamflow and thermodynamics. We also propose a new loss function that balances the performance over different river segments. We demonstrate the effectiveness of the proposed method in predicting temperature and streamflow in a subset of the Delaware River Basin. In particular, we show that the proposed method brings a 33\%/14\% improvement over the state-of-the-art physics-based model and 24\%/14\% over traditional machine learning models (e.g., Long-Short Term Memory Neural Network) in temperature/streamflow prediction using very sparse (0.1\%) observation data for training. The proposed method has also been shown to produce better performance when generalized to different seasons or river segments with different streamflow ranges. ","Physics-Guided Recurrent Graph Networks for Predicting Flow and
  Temperature in River Networks"
148,1310977581626339328,1297174706622205955,Andrea Palermo,"['New paper on the arXiv <LINK>! This time we enjoyed the collaboration of G. Prokhorov. The paper proposes a new observable sensitive to local parity violation in the quark-gluon plasma (QGP), using polarization and helicity of baryons.\n\n[Thread] <LINK>', 'Effects of local parity violations in the QGP are investigated experimentally by looking for evidence of the ""chiral magnetic effect"". The high uncertainties on the magnetic fields involved in the collision, however, make it hard to measure this effect precisely.', 'We studied parity violation assuming the presence of an axial chemical potential in the local equilibrium density operator by which we approximate the state of the plasma at some time. Now we ask, how would this affect the polarization of hyperons? https://t.co/Nw0CmiMN83', 'Within the validity of some approximation, such as the assumption of slowly varying thermodynamic quantities, we are able to compute the contribution of the axial chemical potential to polarization,  but a model-independent observable follows from the symmetries of the system. https://t.co/lMWSgx7aPz', 'Indeed, expanding helicity in multipoles, we have that a non-vanishing scalar component of helicity would be a manifestation of local parity violation. A similar effect might be probed measuring a pedestal in helicity correlators of baryons produced in the same event. https://t.co/aozkNXwNhw']",https://arxiv.org/abs/2009.13449,"We show that local parity violation due to chirality imbalance in relativistic nuclear collisions can be revealed by measuring the projection of the polarization vector onto the momentum, i.e. the helicity, of final state baryons. The proposed method does not require a coupling to the electromagnetic field, like in the Chiral Magnetic Effect. By using linear response theory, we show that, in the presence of a chiral imbalance, the spin 1/2 baryons and anti-baryons receive an additional contribution to the polarization along their momentum and proportional to the axial chemical potential. The additional, parity-breaking, contribution to helicity can be detected by studying helicity-helicity azimuthal angular correlation. ",Polarization as a signature of local parity violation in hot QCD matter
149,1310847369404182528,45191927,Dr Jake Taylor,"['My new paper is out today! We show that scattering *needs* to be taken into account in our retrieval models. If not, the abundance of molecules and thermal structure of the planet can be inferred incorrectly. Paper: <LINK>\nVideo summary: <LINK>', 'We first show how scattering can mimic molecular emission features. This even causes isothermal atmospheres to have features! This is because the cloud emissivity is changing as a function of wavelength. Example: increasing scat. cloud abundance from top (cloud free) to bottom. https://t.co/rseRbYWWK9', 'We explore the degeneracies which can arise due to scattering. These include the brightness temperature of planets being inferred to be *smaller* than the atmospheric temperature. Could explain the 1000K nightside temp in Beatty et al 2019 and Keating et al 2019. https://t.co/Qrdxw87DUB', 'We demonstrate its impact by modelling a hot Jupiter *with scattering* and retrieving on it without scattering models. We find the incorrect inferred abundances and a fake temperature inverse, as the scattering of the cloud has produced emission features. https://t.co/slRSu1fpt7', 'To account for this, we develop a novel technique to fit for the single scattering albedo spectrum of the cloud. This is an agnostic approach and does not assume anything about the clouds chemical and physical properties. Previous studies would decide the cloud species a priori.', 'With this technique, we are able to retrieve the general shape of the clouds single scattering albedo spectra which can be compared to forward models to infer what the clouds are made of. Here I show the retrieved spectra (purple) and forward model (green). https://t.co/BfO87nG4D2', 'We then test to see if it is possible to retrieve on a more complex cloud model generated by CHIMERA (using the Ackerman &amp; Marley model). We find that we can retrieve the ""averaged"" general shape of the single scattering albedo where the photosphere is (green and red lines). https://t.co/AXg60gXcl0', ""Modelling clouds is very hard! This is an initial look and attempt at accounting for the scattering, but it's not perfect and more work needs to be done! I hope this will be a stepping stone to get people thinking more about scattering and its impact!"", 'I thank my co-authors: @V_Parmentier, Mike Line (who it not on twitter), @GleeAstro, @PatrickIrwin1 and @AirborneGrain for all their help. This was a difficult paper to get finished, I had a really hard time in lockdown. Their support was invaluable and helped keep my spirits up!', '@DrRyanGarland Let me get my post doc funding first ;)', 'I would like to add that I am very happy to receive feedback. If I forgot to cite your paper, please let me know. The literature is so vast and ever expanding, it is hard to cite everyone. It ends up being a wall of citations! :P', '@cfisher94 Haha I decided to kill 2 birds with 1 stone. But I totally agree! I’ve always thought about doing it, but been too lazy. I think it’s a good way to promote our work for more accessibility !', '@cfisher94 We’ve gotta start a new trend :)', ""@astronomerslc25 Yep ! This is applicable to irradiated BDs as well. There is lots to investigate :) as an example in @DrRyanGarland's thesis he uses the real/imaginary index spectra of Na2S for cloudy BDs, but can't rule out H2O clouds.. I wonder what an agnostic approach would be like :P"", '@LeighFletcher Thanks Leigh. It has been such a rollercoaster of a year. I thought the acknowledgement part of the paper was the best way to immortalise the help and support that I received, and how we all supported and looked after each other when times got really hard.']",https://arxiv.org/abs/2009.12411,"Observational studies of exoplanets are suggestive of a ubiquitous presence of clouds. The current modelling techniques used in emission to account for the clouds tend to require prior knowledge of the cloud condensing species and often do not consider the scattering effects of the cloud. We explore the effects that thermal scattering has on the emission spectra by modelling a suite of hot Jupiter atmospheres with varying cloud single-scattering albedos (SSAs) and temperature profiles. We examine cases ranging from simple isothermal conditions to more complex structures and physically driven cloud modelling. We show that scattering from nightside clouds would lead to brightness temperatures that are cooler than the real atmospheric temperature if scattering is unaccounted for. We show that scattering can produce spectral signatures in the emission spectrum even for isothermal atmospheres. We identify the retrieval degeneracies and biases that arise in the context of simulated JWST spectra when the scattering from the clouds dominates the spectral shape. Finally, we propose a novel method of fitting the SSA spectrum of the cloud in emission retrievals, using a technique that does not require any prior knowledge of the cloud chemical or physical properties. ","How Does Thermal Scattering Shape the Infrared Spectra of Cloudy
  Exoplanets? A Theoretical Framework and Consequences for Atmospheric
  Retrievals in the JWST era"
150,1309500042118660096,3609913993,Jeff Cain,"['Check out my latest paper on @arxiv, where we use CNTs as “nano reactors” to make materials that shouldn’t exist! We synthesize new 1D members of the transition metal trichalcogenides (TMT), which do not exist in bulk - only when stabilized by CNTs! <LINK> <LINK>', 'TMTs are canonical examples of superconductors and CDW materials. Some members of this family have never been synthesized(eg TiTe3, NbTe3), and energetically can’t exist in bulk. We solve this by synthesizing the materials in CNTs to protect &amp; stabilize them.']",https://arxiv.org/abs/2009.10869,"The structure of MX3 transition metal trichalcogenides (TMTs, with M a transition metal and X a chalcogen) is typified by one-dimensional (1D) chains weakly bound together via van der Waals interactions. This structural motif is common across a range of M and X atoms (e.g. NbSe3, HfTe3, TaS3), but not all M and X combinations are stable. We report here that three new members of the MX3 family which are not stable in bulk, specifically NbTe3, VTe3, and TiTe3, can be synthesized in the few- to single-chain limit via nano-confined growth within the stabilizing cavity of multi-walled carbon nanotubes. Transmission electron microscopy (TEM) and atomic-resolution scanning transmission electron microscopy (STEM) reveal the chain-like nature and the detailed atomic structure. The synthesized materials exhibit behavior unique to few-chain quasi-1D structures, such as multi-chain spiraling and a trigonal anti-prismatic rocking distortion in the single-chain limit. Density functional theory (DFT) calculations provide insight into the crystal structure and stability of the materials, as well as their electronic structure. ","Stabilization of NbTe3, VTe3, and TiTe3 via Nanotube Encapsulation"
151,1309300896208171009,734677275216470016,Guodong Zhang,"['New paper alert: <LINK>\n\nWe provide a unified and automated method to analyze first-order methods for smooth &amp; strongly-monotone games. The convergence rate for any first-order method can be obtained via a mechanical procedure of deriving and solving an SDP. <LINK>', 'Using this framework, we are able to recover or even improve known convergence bounds for a variety of algorithms. For example, we can recover the rate bound of the gradient method, proximal point method, and optimistic gradient method.', 'We can also gain new insights and derive new results that were previously unknown. For example, we for the first time provide the global convergence result of negative momentum, which is difficult to obtain using existing approaches.', 'Finally, we are able to extend this framework to a stochastic setting with multiplicative noise. See more details in the paper.', 'Joint work with @XuchanB, @LaurentLessard and @RogerGrosse .']",https://arxiv.org/abs/2009.11359,"The theory of integral quadratic constraints (IQCs) allows the certification of exponential convergence of interconnected systems containing nonlinear or uncertain elements. In this work, we adapt the IQC theory to study first-order methods for smooth and strongly-monotone games and show how to design tailored quadratic constraints to get tight upper bounds of convergence rates. Using this framework, we recover the existing bound for the gradient method~(GD), derive sharper bounds for the proximal point method~(PPM) and optimistic gradient method~(OG), and provide \emph{for the first time} a global convergence rate for the negative momentum method~(NM) with an iteration complexity $\mathcal{O}(\kappa^{1.5})$, which matches its known lower bound. In addition, for time-varying systems, we prove that the gradient method with optimal step size achieves the fastest provable worst-case convergence rate with quadratic Lyapunov functions. Finally, we further extend our analysis to stochastic games and study the impact of multiplicative noise on different algorithms. We show that it is impossible for an algorithm with one step of memory to achieve acceleration if it only queries the gradient once per batch (in contrast with the stochastic strongly-convex optimization setting, where such acceleration has been demonstrated). However, we exhibit an algorithm which achieves acceleration with two gradient queries per batch. ","A Unified Analysis of First-Order Methods for Smooth Games via Integral
  Quadratic Constraints"
152,1309170590901829635,1116002690604130305,Juliette Becker,"['The K2-266 system has a unique geometry, in which an ultra-short period (USP) planet resides significantly misaligned to a system of tightly packed inner planets (STIP). In our new paper (<LINK>), we present two ways this geometry can form. <LINK>', 'Although we don’t know which hypothesis is correct for K2-266, they’re both possibilities for this system and other systems with similar geometries!', 'One way for this to happen is to have an additional unseen planet in the system. Depending on its exact orbital parameters, such a companion can cause an initially coplanar USP-STIP system to become misaligned. https://t.co/9IpUxkNa8D', 'A second option to get this geometry is for the system to assemble while the star is still somewhat young and has a significant quadrupole moment. This, combined with a slight stellar obliquity with respect to the planet-forming disk, can also cause the USP-STIP misalignment.', 'For more details, please check out either the paper (https://t.co/5JE6tkcy44) or a short summary I wrote on my website (https://t.co/976xrQ8ZRU). \nThanks to all my coauthors on this work, @kbatygin, Dan Fabrycky, Fred Adams, @astroplum, Andrew Vanderburg, and @Astro_JRod', ""@ExoCytherean @kbatygin @astroplum @Astro_JRod Thanks! I'll be interested to hear what you think!""]",https://arxiv.org/abs/2009.10745,"Ultra-short period planets provide a window into the inner edge of the parameter space occupied by planetary orbits. In one particularly intriguing class of multi-planet systems, the ultra-short period planet is flanked by short-period companions, and the outer planets occupy a discernibly distinct dynamical state. In the observational database, this phenomenon is represented by a small number of stars hosting systems of tightly packed co-planar planets as well as an ultra-short period planet, whose orbit of is misaligned relative to the mutual plane of the former. In this work, we explore two different mechanisms that can produce an ultra-short period planet that is misaligned with the rest of its compact planetary system: natural decoupling between the inner and outer system via the stellar quadrupole moment, and decoupling forced by an external companion with finely-tuned orbital parameters. These two processes operate with different timescales, and can thus occur simultaneously. In this work, we use the K2-266 system as an illustrative example to elucidate the dynamics of these two processes, and highlight the types of constraints that may arise regarding the dynamical histories of systems hosting ultra-short period planets. ","The Origin of Systems of Tightly Packed Inner Planets with Misaligned,
  Ultra-Short-Period Companions"
153,1308767903370547200,762359343656361984,James Zou,"['Our new #emnlp paper shows how to teach ML via natural language explanation of contrasts between concepts (eg "" difference between COVID and flu is ..."").\n\nIt\'s much more efficient than using labeled examples. Excited for more human-like learning! <LINK> <LINK>', 'Great job by Weixin and @Zhou_Yu_AI!']",https://arxiv.org/abs/2009.10259,"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides several bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points. ",ALICE: Active Learning with Contrastive Natural Language Explanations
154,1308664432671756288,460489687,Juan Mateos Garcia,"['A narrowing of AI research? \n\nOur new paper about the evolution of thematic diversity in AI research is live in arXiv now. I will post a thread about it sometime soon. Comments welcome!\n\n<LINK> <LINK>', '@ArhoSuominen Interesting! It would be good to discuss this in further detail when I am back to work from parental leave mid-October.']",https://arxiv.org/abs/2009.10385,"The arrival of deep learning techniques able to infer patterns from large datasets has dramatically improved the performance of Artificial Intelligence (AI) systems. Deep learning's rapid development and adoption, in great part led by large technology companies, has however created concerns about a premature narrowing in the technological trajectory of AI research despite its weaknesses, which include lack of robustness, high environmental costs, and potentially unfair outcomes. We seek to improve the evidence base with a semantic analysis of AI research in arXiv, a popular pre-prints database. We study the evolution of the thematic diversity of AI research, compare the thematic diversity of AI research in academia and the private sector and measure the influence of private companies in AI research through the citations they receive and their collaborations with other institutions. Our results suggest that diversity in AI research has stagnated in recent years, and that AI research involving the private sector tends to be less diverse and more influential than research in academia. We also find that private sector AI researchers tend to specialise in data-hungry and computationally intensive deep learning methods at the expense of research involving other AI methods, research that considers the societal and ethical implications of AI, and applications in sectors like health. Our results provide a rationale for policy action to prevent a premature narrowing of AI research that could constrain its societal benefits, but we note the informational, incentive and scale hurdles standing in the way of such interventions. ",A narrowing of AI research?
155,1308320583055282177,597775255,Dr Adam Finley,"['New paper on arXiv: <LINK> We discuss the implications of the high tangential wind speeds observed by Parker Solar Probe during its first two orbits, on our understanding of the solar wind angular momentum-loss rate. @AWESoMeStarsERC @mathewjowens @RuipSol  (1/4) <LINK>', 'The angular momentum flux in the solar wind appears to be highly variable. By averaging the observations during each orbit, instead of using only the perihelion values, we find an angular momentum-loss rate similar to previous estimates using the Helios and Wind spacecraft. (2/4)', 'It is hard to say what the global angular momentum-loss rate is based on the average value PSP found in the solar equator. More measurements are needed, including observations at higher latitudes. These will become available as @ESASolarOrbiter leaves the ecliptic plane. (3/4)', 'The value of the solar angular momentum-loss rate has implications for modelling the rotation evolution of Sun-like stars. Whose magnetic activity is linked to their rotation rates, which likely impacts the habitability of exoplanets. @NASASun @UoE_Astro @UKSolarPhysics  (4/4)']",https://arxiv.org/abs/2009.08991,"The long-term evolution of the Sun's rotation period cannot be directly observed, and is instead inferred from trends in the measured rotation periods of other Sun-like stars. Assuming the Sun spins-down as it ages, following rotation rate $\propto$ age$^{-1/2}$, requires the current solar angular momentum-loss rate to be around $6\times 10^{30}$erg. Magnetohydrodynamic models, and previous observations of the solar wind (from the Helios and Wind spacecraft), generally predict a values closer to $1\times 10^{30}$erg or $3\times 10^{30}$erg, respectively. Recently, the Parker Solar Probe (PSP) observed tangential solar wind speeds as high as $\sim50$km/s in a localized region of the inner heliosphere. If such rotational flows were prevalent throughout the corona, it would imply that the solar wind angular momentum-loss rate is an order of magnitude larger than all of those previous estimations. In this letter, we evaluate the angular momentum flux in the solar wind, using data from the first two orbits of PSP. The solar wind is observed to contain both large positive (as seen during perihelion), and negative angular momentum fluxes. We analyse two solar wind streams that were repeatedly traversed by PSP; the first is a slow wind stream whose average angular momentum flux fluctuates between positive to negative, and the second is an intermediate speed stream containing a positive angular momentum flux (more consistent with a constant flow of angular momentum). When the data from PSP is evaluated holistically, the average equatorial angular momentum flux implies a global angular momentum-loss rate of around $2.6-4.2\times 10^{30}$ erg (which is more consistent with observations from previous spacecraft). ",The Solar Wind Angular Momentum Flux as Observed by Parker Solar Probe
156,1306956675786571778,1086908209271582721,Ashish Sharma,"['Our #EMNLP2020 paper presents a new computational approach to understanding how empathy is expressed in text-based mental health support! \n\nJoint work with Adam Miner, Dave Atkins, and @timalthoff\n\nPreprint: <LINK>\n\nSummary 👇 1/7', 'Interacting empathically is fundamental to successful mental health support. In text-based support systems like @TalkLifeApp, empathy must be expressed through text alone, as opposed to face-to-face therapy where one can leverage several audio-visual signals like pitch.  2/7', 'What does it mean to be empathic in textual, peer-to-peer support conversations and how can we measure it computationally? How empathic are users in peer-to-peer support systems? How can we make them express more empathy towards people in distress? 3/7', 'To answer these, we develop EPITOME, a new conceptual framework consisting of three empathy communication mechanisms -- *Emotional Reactions*, *Interpretations*, and *Explorations*. 4/7 https://t.co/GwyJLYZCGv', 'EPITOME is based on prominent empathy scales from psychotherapy research and incorporates both emotional (Emo. Reactions) and cognitive (Interpretations, Explorations) aspects of empathy that have never been studied computationally in text-based, asynchronous conversations. 5/7 https://t.co/O3doILAjiX', 'Using EPITOME, we collect a new corpus of 10k (post, response) pairs, and develop &amp; train a new multi-task RoBERTa-based bi-encoder model. A key aspect of our computational approach is the underlying supportive evidence, *rationales from input text*, for empathy predictions. 6/7 https://t.co/XwLJmO2DLI', 'Our analysis over 235k interactions suggests that most peer supporters express empathy rarely and this does not improve over time. This points to critical opportunities for feedback and training of peer supporters to increase the effectiveness of mental health support. 7/7 https://t.co/BgsGGAKo3h']",https://arxiv.org/abs/2009.08441,"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback. ","A Computational Approach to Understanding Empathy Expressed in
  Text-Based Mental Health Support"
157,1306410949792858114,4666231375,Konstantin Batygin,"['The unraveling of the sol system has fascinated mathematicians for centuries. Newton himself believed the Jupiter-Saturn ""great inequality"" (5:2 near-resonance) held the key to the sol system\'s demise. In a new paper led by @jonKzink, we show he was right: <LINK> <LINK>']",https://arxiv.org/abs/2009.07296,"Using an ensemble of N-body simulations, this paper considers the fate of the outer gas giants (Jupiter, Saturn, Uranus, and Neptune) after the Sun leaves the main sequence and completes its stellar evolution. Due to solar mass-loss -- which is expected to remove roughly half of the star's mass -- the orbits of the giant planets expand. This adiabatic process maintains the orbital period ratios, but the mutual interactions between planets and the width of mean-motion resonances (MMR) increase, leading to the capture of Jupiter and Saturn into a stable 5:2 resonant configuration. The expanded orbits, coupled with the large-amplitude librations of the critical MMR angle, make the system more susceptible to perturbations from stellar flyby interactions. Accordingly, within about 30 Gyr, stellar encounters perturb the planets onto the chaotic sub-domain of the 5:2 resonance, triggering a large-scale instability, which culminates in the ejections of all but one planet over the subsequent $\sim10$ Gyr. After an additional $\sim50$ Gyr, a close stellar encounter (with a perihelion distance less than $\sim200$ AU) liberates the final planet. Through this sequence of events, the characteristic timescale over which the solar system will be completely dissolved is roughly 100 Gyr. Our analysis thus indicates that the expected dynamical lifetime of the solar system is much longer than the current age of the universe, but is significantly shorter than previous estimates. ","The Great Inequality and the Dynamical Disintegration of the Outer Solar
  System"
158,1306225509681033217,1056626853652426752,Jonathan Herzig,"['1/ Does a span-based parser perform better than seq2seq models in terms of compositional generalization in semantic parsing?\nSeems like it!\nCheck out our new paper: <LINK>, Joint work with @JonathanBerant <LINK>', '2/ We present SpanBasedSP, a parser that predicts a span tree over the input utterance, explicitly encoding how\npartial programs compose over spans in the input.\n\nThis form of inductive bias helps the model in dealing with new structures, unseen during training time.', '3/ To be comparable with seq2seq models, we train from programs, without access to gold trees, treating trees\nas latent variables. We also parse a common class of non-projective trees through an extension to standard CKY. https://t.co/anGR2VTUi3', '4/  SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization. \nFrom 69.8 → 95.3 average accuracy on SCAN, CLOSURE and GeoQuery. https://t.co/EqeDSnYK0K', '@jayantkrish @JonathanBerant I agree it depends on the data. I think our method could help in iid splits if there are indeed many structures in the test set that are unseen during training time. For most popular datasets it seems that this is not the case though, and this is why seq2seq models do well there.']",https://arxiv.org/abs/2009.06040,"Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training. In this work, we posit that a span-based parser should lead to better compositional generalization. we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input. SpanBasedSP extends Pasupat et al. (2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY. On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from $61.0 \rightarrow 88.9$ average accuracy. ",Span-based Semantic Parsing for Compositional Generalization
159,1305896401440661504,1282067403183058944,Victor Reis,"[""New paper: a version of Spencer's theorem for polynomials, where the ℓ_∞ norm becomes ||p||_∞ := sup |p(x)| over |x| &lt;= 1\n\nKey idea: ||p||_∞ = max |p(x)| evaluated at deg(p) special points, up to a factor of five\n\nI guess this is math-CA? I put cs-DM: <LINK>""]",https://arxiv.org/abs/2009.05692,"Given $n$ polynomials $p_1, \dots, p_n$ of degree at most $n$ with $\|p_i\|_\infty \le 1$ for $i \in [n]$, we show there exist signs $x_1, \dots, x_n \in \{-1,1\}$ so that \[\Big\|\sum_{i=1}^n x_i p_i\Big\|_\infty < 30\sqrt{n}, \] where $\|p\|_\infty := \sup_{|x| \le 1} |p(x)|$. This result extends the Rudin-Shapiro sequence, which gives an upper bound of $O(\sqrt{n})$ for the Chebyshev polynomials $T_1, \dots, T_n$, and can be seen as a polynomial analogue of Spencer's ""six standard deviations"" theorem. ",Balancing Polynomials in the Chebyshev Norm
160,1305303215894913025,282227303,Dane Wilburne,['Here is a new paper I wrote with Lily Silverstein and Jay Yang on the asymptotic degree of random monomial ideals.  <LINK>'],https://arxiv.org/abs/2009.05174,"One of the fundamental invariants connecting algebra and geometry is the degree of an ideal. In this paper we derive the probabilistic behavior of degree with respect to the versatile Erd\H{o}s-R\'enyi-type model for random monomial ideals defined in \cite{rmi}. We study the staircase structure associated to a monomial ideal, and show that in the random case the shape of the staircase diagram is approximately hyperbolic, and this behavior is robust across several random models. Since the discrete volume under this staircase is related to the summatory higher-order divisor function studied in number theory, we use this connection and our control over the shape of the staircase diagram to derive the asymptotic degree of a random monomial ideal. Another way to compute the degree of a monomial ideal is with a standard pair decomposition. This paper derives bounds on the number of standard pairs of a random monomial ideal indexed by any subset of the ring variables. The standard pairs indexed by maximal subsets give a count of degree, as well as being a more nuanced invariant of the random monomial ideal. ",Asymptotic Degree of Random Monomial Ideals
161,1302971371161612288,261865146,Dr Sofia Qvarfort,"['New preprint out! Noise affects all quantum systems, including optomechanical ones. \n\nIn this paper: <LINK>, we provide a general solution for optical loss in nonlinear optomechanical systems.\n\nHow did we do it and what does it mean? Some highlights. 👇 (1/4) <LINK>', 'Basically, we combined a bunch of methods to solve an equation that describes the noisy system. \n\nWe used\n1. A Lie algebra solution of the unitary dynamics\n2. A partition of the evolution similar to the interaction picture\n3. A vectorisation of the master equation\n\n(2/4)', 'The result in an analytic expression for the non-unitary evolution.  \n\nTo show how the solutions can be used, we computed (i) the average (decaying) photon number ☀️, (ii) the intra-cavity optical quadratures, and (iii) the fidelity for generating optical cat-states 🐱. \n\n(3/4)', ""Ultimately, having access to this tool means we can now model a lot more scenarios than we could before, which is exciting! \n\nIf you have any questions or comments for us, or cool ideas for how these solutions could be used, please don't hesitate to reach out!\n\n(4/4).""]",https://arxiv.org/abs/2009.02295,"Open-system dynamics play a key role in the experimental and theoretical study of cavity optomechanical systems. In many cases, the quantum Langevin equations have enabled excellent models for optical decoherence, yet a master-equation approach to the fully nonlinear optomechanical Hamiltonian has thus far proven more elusive. To address this outstanding question and broaden the mathematical tool set available, we derive a solution to the Lindblad master equation that models optical decoherence for a system evolving with the nonlinear optomechanical Hamiltonian. The method combines a Lie-algebra solution to the unitary dynamics with a vectorization of the Lindblad equation, and we demonstrate its applicability by considering the preparation of optical cat states via the optomechanical nonlinearity in the presence of optical loss. Our results provide a direct way of analytically assessing the impact of optical decoherence on the optomechanical intracavity state. ","Master-equation treatment of nonlinear optomechanical systems with
  optical loss"
162,1301895743595188224,247800333,Ahmad طه,"[""Absolutely pumped to share our new paper (with @ShenWang9 and @ahmedabokifa) on control of water quality in drinking water networks (DWN):\n\nHow Effective is Model Predictive Control in\nReal-Time Water Quality Regulation?\n\n<LINK> \n\nHere's why it's significant. 1/n"", 'Time- and space-varying chlorine concentrations in DWNs are a proxy/indicator for water quality. It is hence intuitive to model chlorine\xa0concentrations in water networks as a dynamic system. But this has never been done before.\xa02/n', 'That is, before this paper, no actual state-space model has been derived to model water quality dynamics. Sounds too good to be true? Perhaps.  But the model is too complex to be derived. But the mighty @ShenWang9 did it. And he did it with style. 3/n', 'The control-theoretic model confirms simulations of water quality dynamics via black-box toolboxes and EPANET.\xa0\nThis work is significant because you can use it to do control via any control method of your choosing (MPC, H-2, H-Infinity, L-Infinity, and so on). 4/n', 'We choose MPC in this paper (because scalability, state/input constraint satisfaction, and more) and show how this controller can react swiftly to contamination events via dispatching optimal chlorine into the network. 5/n', 'As always, the complete codes are on Github. https://t.co/af3TkLTdev\n6/n', ""If you have been doing research in control theory for so long, and feel tired/bored of classical applications (in power, transportation, robotics), and you're interested in something new as water quality, then this paper can help you learn something new---and exciting. 7/n=7"", ""@theEnergyMads The chlorine booster stations (that inject chlorine into the network), I presume, consume little energy. We don't do optimal  water pump control (that consume a lot of power) here---we assume fixed pump schedules since water quality dynamics are much faster than hydraulics.""]",https://arxiv.org/abs/2009.01298,"Real-time water quality control (WQC) in water distribution networks (WDN), the problem of regulating disinfectant levels, is challenging due to lack of (i) a proper control-oriented modeling considering complicated components (junctions, reservoirs, tanks, pipes, pumps, and valves) for water quality modeling in WDN and (ii) a corresponding scalable control algorithm that performs real-time water quality regulation. In this paper, we solve the WQC problem by (a) proposing a novel state-space representation of the WQC problem that provides an explicit relationship between inputs (chlorine dosage at booster stations) and states/outputs (chlorine concentrations in the entire network) and (b) designing a highly scalable model predictive control (MPC) algorithm that showcases fast response time and resilience against some sources of uncertainty. ","How Effective is Model Predictive Control in Real-Time Water Quality
  Regulation? State-Space Modeling and Scalable Control"
163,1316260901784375297,3865005196,Olivia Guest · Ολίβια Γκεστ,"[""So true! I'm proud of how we can find each other. This (<LINK>) w/@kirstie_j &amp; this w/@Abebab (<LINK>) are collaborations, friendships that started &amp; we're cultivated on Twitter.\n\nIn fact this w/@andrea_e_martin also did! <LINK> <LINK>""]",https://arxiv.org/abs/2009.14258,"This article sets out our perspective on how to begin the journey of decolonising computational fields, such as data and cognitive sciences. We see this struggle as requiring two basic steps: a) realisation that the present-day system has inherited, and still enacts, hostile, conservative, and oppressive behaviours and principles towards women of colour (WoC); and b) rejection of the idea that centering individual people is a solution to system-level problems. The longer we ignore these two steps, the more ""our"" academic system maintains its toxic structure, excludes, and harms Black women and other minoritised groups. This also keeps the door open to discredited pseudoscience, like eugenics and physiognomy. We propose that grappling with our fields' histories and heritage holds the key to avoiding mistakes of the past. For example, initiatives such as ""diversity boards"" can still be harmful because they superficially appear reformatory but nonetheless center whiteness and maintain the status quo. Building on the shoulders of many WoC's work, who have been paving the way, we hope to advance the dialogue required to build both a grass-roots and a top-down re-imagining of computational sciences -- including but not limited to psychology, neuroscience, cognitive science, computer science, data science, statistics, machine learning, and artificial intelligence. We aspire for these fields to progress away from their stagnant, sexist, and racist shared past into carving and maintaining an ecosystem where both a diverse demographics of researchers and scientific ideas that critically challenge the status quo are welcomed. ",Towards decolonising computational sciences
164,1316173556368076800,1208217542843518982,Keyulu Xu,"['How do neural networks extrapolate, i.e., predict outside the training distribution? We study MLPs and Graph Neural Networks trained by gradient descent, and show how a good representation and architecture can help extrapolation.\n\n<LINK> <LINK>', 'Joint work with @SimonShaoleiDu  Ken-ichi Kawarabayashi, Stefanie Jegelka, @mozhi_zhang @jingling_li']",https://arxiv.org/abs/2009.11848,"We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) -- structured networks with MLP modules -- have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufficiently ""diverse"". Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings. ","How Neural Networks Extrapolate: From Feedforward to Graph Neural
  Networks"
165,1311588057783635969,366131267,Nisha katyal,"['Sharing our most recent work connecting the planetary interiors and  atmospheres:\n<LINK>\nWe study the influence of mantle redox state on the atmospheric development and provides useful insights to inform the future missions looking for magma ocean planets.', 'The paper is recently accepted to A&amp;A', '@tonhingm Thanks for being a really helpful co-author']",http://arxiv.org/abs/2009.14599,"The magma ocean period was a critical phase determining how Earth atmosphere developed into habitability. However there are major uncertainties in the role of key processes such as outgassing from the planetary interior and escape of species to space that play a major role in determining the atmosphere of early Earth. We investigate the influence of outgassing of various species and escape of H$_2$ for different mantle redox states upon the composition and evolution of the atmosphere for the magma ocean period. We include an important new atmosphere-interior coupling mechanism namely the redox evolution of the mantle which strongly affects the outgassing of species. We simulate the volatile outgassing and chemical speciation at the surface for various redox states of the mantle by employing a C-H-O based chemical speciation model combined with an interior outgassing model. We then apply a line-by-line radiative transfer model to study the remote appearance of the planet in terms of the infrared emission and transmission. Finally, we use a parameterized diffusion-limited and XUV energy-driven atmospheric escape model to calculate the loss of H$_2$ to space. We have simulated the thermal emission and transmission spectra for reduced or oxidized atmospheres present during the magma ocean period of Earth. Reduced or thin atmospheres consisting of H$_2$ in abundance emit more radiation to space and have larger effective height as compared to oxidized or thick atmospheres which are abundant in H$_2$O and CO$_2$. We obtain the outgassing rates of H2 from the mantle into the atmosphere to be a factor of ten times larger than the rates of diffusion-limited escape to space. Our work presents useful insight into the development of Earth atmosphere during the magma ocean period as well as input to guide future studies discussing exoplanetary interior compositions. ","Effect of mantle oxidation state and escape upon the evolution of
  Earth's magma ocean atmosphere"
166,1311500085549821952,2448534947,Chris Pal,"['Can we improve the ability of Transformer based language models to perform logical inferences, generalize learned reasoning, and generate proofs using natural language? Check out the arXiv version of our NeurIPS 2020 paper to find out what we learned: <LINK> <LINK>']",https://arxiv.org/abs/2009.14786,"We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logical reasoning task in natural language, which involves reasoning over relationships between entities grounded in first-order logical proofs. Specifically, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs. We test the generated proofs for logical consistency, along with the accuracy of the final inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their generalization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they find it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs. This suggests that Transformers have efficient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies. ","Measuring Systematic Generalization in Neural Proof Generation with
  Transformers"
167,1310743763086536704,742475738977308674,Rachit Agarwal,['Our new study in collaboration with @abhibane on quantifying Risk of getting infection when meeting others is out as a preprint on arxiv <LINK>. Here we focus on COVID-19 as a use case.'],http://arxiv.org/abs/2009.12588,"A wide range of approaches have been applied to manage the spread of global pandemic events such as COVID-19, which have met with varying degrees of success. Given the large-scale social and economic impact coupled with the increasing time span of the pandemic, it is important to not only manage the spread of the disease but also put extra efforts on measures that expedite resumption of social and economic life. It is therefore important to identify situations that carry high risk, and act early whenever such situations are identified. While a large number of mobile applications have been developed, they are aimed at obtaining information that can be used for contact tracing, but not at estimating the risk of social situations. In this paper, we introduce an infection risk score that provides an estimate of the infection risk arising from human contacts. Using a real-world human contact dataset, we show that the proposed risk score can provide a realistic estimate of the level of risk in the population. We also describe how the proposed infection risk score can be implemented on smartphones. Finally, we identify representative use cases that can leverage the risk score to minimize infection propagation. ","Infection Risk Score: Identifying the risk of infection propagation
  based on human contact"
168,1310501283133091841,149526852,Miguel Zumalacarregui,"['New paper with @jmezquiagabravo, new framework, new GW effects &amp; new GR tests: <LINK>\n\nWe find that a gravitational lens can split a GW signal in theories beyond GR, much like birefringence in some materials. \n\nLet me walk you through our main results [1/n] <LINK>', 'A gravitational lens spontaneously breaks symmetries, allowing couplings between GWs and scalar fields forbidden in homogeneous space.\n\nAround the lens +,x &amp; scalar polarizations combine into propagation eigenstates, which evolve independently and may have different speed [2/n] https://t.co/QQdU0dgrO7', 'The different speed leads to a time delay between the different states. In many cases the two metric polarizations h_+, h_x (slightly mixed w/ scalar) can pick a different speed.\n\nThis splits a signal (e.g. a BH merger) into separate echoes [3/n] https://t.co/n2i7dVuPKm', 'If the time delay is small enough, the two polarizations interfere with each other in the detector, scrambling the waveform.\n\nThis type of test does not require an electromagnetic counterpart to the event. Any LIGO signal is sensitive at the ~ms level [4/n] https://t.co/x8Cjv5jioh', 'GW lensing tests will become much more effective with growing number of GW detections (higher chance of good source-lens orientation).\n\nOur forecast suggest a factor ~10 improvement with @LIGO design sensitivity and ~100 with 3rd generation detectors [5/n] https://t.co/2Eh2xp1UQX', 'GW birefringence allows detection in lensing set-ups that do not contribute in ""traditional"" lensing.\n\nIn particular, the metric (Shapiro) and geometric (deflection angle) contributions to the time delay can dominate for different lens redshift or mass (here example th.) [6/n] https://t.co/mjbFhnUG6h', 'After discussing the framework in general we applied it to a specific theory of gravity in the Horndeski class. \n\nGW lensing tests via birefringence can go much deeper into the parameter space of the theory than limits from GW170817 (average difference in GW vs EM speed)\n[7/n] https://t.co/0uinswAANX', 'These constraints would be achieved by a binary merger in a very dense environment, such as the vicinity of a super-massive black hole. Events like #GW190521 could be a smoking gun for such environments (e.g. EM counterparts, hierarchical formation, multi-band observation) [8/n] https://t.co/GWCiHzPSDE', 'Many more details on the paper itself [9/n]\n\nhttps://t.co/sfZkc1Q6ev https://t.co/WMo9CeS46D', 'or on our recent talks  in remote conferences:\n\nhttps://t.co/rF9LyJ5pYs\nhttps://t.co/dYJqngBXhb\n\nand of course, questions comments are most welcome! [10/n]']",https://arxiv.org/abs/2009.12187,"Gravitational waves (GW), as light, are gravitationally lensed by intervening matter, deflecting their trajectories, delaying their arrival and occasionally producing multiple images. In theories beyond general relativity (GR), new gravitational degrees of freedom add an extra layer of complexity and richness to GW lensing. We develop a formalism to compute GW propagation beyond GR over general space-times, including kinetic interactions with new fields. Our framework relies on identifying the dynamical propagation eigenstates (linear combinations of the metric and additional fields) at leading order in a short-wave expansion. We determine these eigenstates and the conditions under which they acquire a different propagation speed around a lens. Differences in speed between eigenstates cause birefringence phenomena, including time delays between the metric polarizations (orthogonal superpositions of $h_+,h_\times$) observable without an electromagnetic counterpart. In particular, GW echoes are produced when the accumulated delay is larger than the signal's duration, while shorter time delays produce a scrambling of the wave-form. We also describe the formation of GW shadows as non-propagating metric components are sourced by the background of the additional fields around the lens. As an example, we apply our methodology to quartic Horndeski theories with Vainshtein screening and show that birefringence effects probe a region of the parameter space complementary to the constraints from the multi-messenger event GW170817. In the future, identified strongly lensed GWs and binary black holes merging near dense environments, such as active galactic nuclei, will fulfill the potential of these novel tests of gravity. ","Gravitational wave lensing beyond general relativity: birefringence,
  echoes and shadows"
169,1310480386607198208,50901426,Rafael Alves Batista,"['In <LINK> we propose a novel method for constraining intergalactic magnetic fields (IGMFs) using multi-messenger observations of neutrinos + gamma rays.\nApplied to TXS 0506+056, we derive the first bounds on the magnetic field *AND* coherence length of IGMFs. <LINK>']",https://arxiv.org/abs/2009.12161,"The origin of magnetic fields in the universe is an open problem. Seed magnetic fields possibly produced in early times may have survived up to the present day close to their original form, providing an untapped window to the primeval universe. The recent observations of high-energy neutrinos from the blazar TXS 0506+056 in association with an electromagnetic counterpart in a broad range of wavelengths can be used to probe intergalactic magnetic fields via the time delay between the neutrinos and gamma rays as well as the time dependence of the gamma-ray fluxes. Using extensive three-dimensional Monte Carlo simulations, we present a novel method to constrain these fields. We apply it to TXS 0506+056 and, for the first time, derive constraints on both the magnetic-field strength and its coherence length, considering six orders of magnitude for each. ","Multimessenger Constraints on Intergalactic Magnetic Fields from the
  Flare of TXS 0506+056"
170,1309817174739963905,871295109421080576,Hitesh Kishore Das,"[""My first paper 😀 came on arXiv yesterday. We (I, @prakritipc, Prateek Sharma) study how temperature &amp; metallicity affect linear &amp; nonlinear evolution of thermal instability. I'll try to give a brief overview of our work.\n<LINK>\n#firstpaper #Astrophysics"", 'For a short 3min summary, you can watch this video on youtube:\nhttps://t.co/0sJMucfXC8\n\n#galaxies #galaxyevolution #galaxycluster\n#hydrodynamicSimulations\n\n2/9', 'What is thermal instability (TI)? If you start with uniformly distributed hot medium and some density perturbations, the regions with slightly higher density will cool more. Due to this the density increases more in dense regions and you have a runaway process in your hands.\n3/9 https://t.co/AgN9HQ2N6t', 'Why is it important? There are multiple phases in the last figure-Cold dense gas, Hot diffused gas and warm intermediate gas. Such coexistence of multiple phases in gaseous medium, called multiphase gas, has been seen in observations of Intracluster and Circumgalactic medium.\n4/9', 'This makes TI important in multiphase gas formation. Cooling rate is dependent on temperature &amp; metallicity (Z), but studies have mostly concentrated on temperature dependence. We examine effects of Z variation and find that TI growth rates have no explicit Z dependence.\n5/9', 'The picture in the 3rd tweet is true for small-scale perturbations. The evolution of large-scale perturbations is still a debated question. We find that the nonlinear behavior of large-scale overdensity (cloud) depends on the linear stability of large-scale perturbations.\n6/9', 'If large-scale perturbations are stable, small-scale perturbations take over and large clouds give rise to closely-spaced small clouds. Otherwise if they are unstable, large clouds shrink monolithically. This figure illustrates our proposed evolution of large-scale clouds.\n7/9 https://t.co/aVUDWvplUG', 'We also find that the characteristic scale of min(cs*tcool) is followed by the clouds only in a transient phase and the clouds can be larger at later times as they merge and relax to background pressure.\n8/9', 'So, this summarizes the main conclusions of our paper. We hope that this paper helps to further understand the multiphase gas formation is Intracluster and Circumgalactic medium. Thanks 😀 for reading this long thread.\n\n #CircumgalacticMedium #IntraclusterMedium\n\n9/9']",https://arxiv.org/abs/2009.11317,"We test how metallicity variation (a background gradient and fluctuations) affects the physics of local thermal instability using analytical calculations and idealized, high-resolution 1D hydrodynamic simulations. Although the cooling function ($\Lambda[T,Z]$) and the cooling time ($t_{\rm cool}$) depend on gas temperature and metallicity, we find that the growth rate of thermal instability is explicitly dependent only on the derivative of the cooling function relative to temperature ($\partial \ln \Lambda/\partial \ln T$) and not on the metallicity derivative ($\partial \ln \Lambda/ \partial \ln Z$). For most of $10^4~{\rm K} \lesssim T \lesssim 10^7~{\rm K}$, both the isobaric and isochoric modes (occurring at scales smaller and larger than the sonic length covered in a cooling time [$c_s t_{\rm cool}$], respectively) grow linearly, and at higher temperatures ($\gtrsim 10^7~{\rm K}$) the isochoric modes are stable. We show that even the nonlinear evolution depends on whether the isochoric modes are linearly stable or unstable. For the stable isochoric modes, we observe the growth of small-scale isobaric modes but this is distinct from the nonlinear fragmentation of a dense cooling region. For unstable isochoric perturbations we do not observe large density perturbations at small scales. While very small clouds ($\sim {\rm min}[c_st_{\rm cool}]$) form in the transient state of nonlinear evolution of the stable isochoric thermal instability, most of them merge eventually. ","Shatter or not: role of temperature and metallicity in the evolution of
  thermal instability"
171,1309263951780352000,5025111,Will Whitney,"['New paper! We propose to measure the quality of learned representations using the complexity of finding a nearly-optimal predictor on a downstream task.\n\nBlog: <LINK>\nPaper: <LINK>\nLibrary: <LINK>', 'The methods that people currently use for evaluating learned representations are related: fix an amount of evaluation data, then look at a point estimate or integral of the loss. https://t.co/4eLAQhjXhc', ""Results that depend on the amount of eval data are odd:\n1. Guessing how much data is needed to solve a task is hard.\n2. Choosing a repr with n points is misleading IRL when data comes in all the time.\n\nShouldn't representation quality only depend on the task &amp; learning algorithm?"", ""We introduce a measure called surplus description length (SDL): the number of extra bits required to encode an infinite stream of data using a learning algorithm instead of the optimal code. Unlike existing measures, it's not a function of the dataset size. https://t.co/C5khbJ22a3"", 'SDL measures something fundamental: the information an alg needs to gain about the optimal predictor in order to perform well (i.e. expected loss ≤ ε). We also suggest ε sample complexity, which measures the same thing in a coarser way. Both are computed from loss-data curves.', 'Naively computing loss-data curves is slow: train ~100 small neural nets back to back. We use tools from JAX to batch the update step *across neural nets*, letting you train them all in parallel on one GPU. Reduces time from 30m to 2m.', ""Fast batched training was tricky to implement, but it's now easy to use in our representation eval library, Reprieve: https://t.co/oO3pfGIUYn\n\nEvaluation is too important to do ad-hoc. Reprieve is a stable, framework-agnostic tool to enable reproducibility &amp; fair comparisons."", ""It's been a pleasure working with my insightful, hardworking co-authors Min Jae Song, David Brandfonbrener, @thejaan, @kchonyc on these ideas. This work would not be where it is without them."", 'Thanks also to @SingularMattrix @avitaloliver @anselmlevskaya and lots of other JAXers who were unfailingly helpful when I bothered them with questions both trivial and arcane!', 'Whoops missed that Min Jae is on Twitter: @mj_theory!']",https://arxiv.org/abs/2009.07368,"We consider the problem of evaluating representations of data for use in solving a downstream task. We propose to measure the quality of a representation by the complexity of learning a predictor on top of the representation that achieves low loss on a task of interest, and introduce two methods, surplus description length (SDL) and $\varepsilon$ sample complexity ($\varepsilon$SC). In contrast to prior methods, which measure the amount of information about the optimal predictor that is present in a specific amount of data, our methods measure the amount of information needed from the data to recover an approximation of the optimal predictor up to a specified tolerance. We present a framework to compare these methods based on plotting the validation loss versus evaluation dataset size (the ""loss-data"" curve). Existing measures, such as mutual information and minimum description length probes, correspond to slices and integrals along the data axis of the loss-data curve, while ours correspond to slices and integrals along the loss axis. We provide experiments on real data to compare the behavior of each of these methods over datasets of varying size along with a high performance open source library for representation evaluation at this https URL ","Evaluating representations by the complexity of learning low-loss
  predictors"
172,1308938327735713793,3022633752,Tianle Cai,"['Do existing pruning methods really exploit the info from data? Are the architectures of the pruned networks really matter for the performance? We propose sanity checks on pruning methods and find a great part of existing methods does not rely on these! <LINK> <LINK>', 'TL;DR: We sanity-check several existing pruning methods and find the performance of a large group of methods only rely on the *pruning ratio of each layer*. This finding inspires us to design an efficient data-independent, training-free pruning method as a byproduct.', 'Joint work with my fantastic collaborators Jingtong, Yihang, Tianhao, @SparkyTruck, Liwei, and @jasondeanlee 🔥🔥🔥', 'We sanity-check pruning methods by (1) corrupting the data used in the pruning process, or (2) destroy the structure of pruned networks layer-wisely. Results show that the performance of a group of methods which we called “initial tickets” does not drop under these attacks! https://t.co/U5oLMkROI8', 'This finding indicates that only *the pruning ratio of each layer* matters for the final performance of initial tickets! Following this idea, we develop a new kind of data-independent and (pre)training-free pruning method called random ticket which achieve good performance.', 'We also find some smarter pruning methods using certain kinds of rewinding can pass our sanity checks. We hybridize the idea of random tickets with these methods and obtain further improvement upon the rewinding methods.', ""@namhoonlee09 Hi Namhoon, we didn't try it on MLPs since they cannot achieve promising performance on larger datasets. But that's really a good question whether some components of the NN will change the result of sanity checks and deserves further exploration! I'm always open to chat : )"", ""@namhoonlee09 Yeah, your visualizations show some patterns of weights of fc layer. Then it's interesting to see how about conv layer. If we can figure out why convs cannot fully exploit the signal, we may be able to get a better pruning at init method. The reason may be complicated though.""]",https://arxiv.org/abs/2009.11094,"Network pruning is a method for reducing test-time computational resource requirements with minimal performance degradation. Conventional wisdom of pruning algorithms suggests that: (1) Pruning methods exploit information from training data to find good subnetworks; (2) The architecture of the pruned network is crucial for good performance. In this paper, we conduct sanity checks for the above beliefs on several recent unstructured pruning methods and surprisingly find that: (1) A set of methods which aims to find good subnetworks of the randomly-initialized network (which we call ""initial tickets""), hardly exploits any information from the training data; (2) For the pruned networks obtained by these methods, randomly changing the preserved weights in each layer, while keeping the total number of preserved weights unchanged per layer, does not affect the final performance. These findings inspire us to choose a series of simple \emph{data-independent} prune ratios for each layer, and randomly prune each layer accordingly to get a subnetwork (which we call ""random tickets""). Experimental results show that our zero-shot random tickets outperform or attain a similar performance compared to existing ""initial tickets"". In addition, we identify one existing pruning method that passes our sanity checks. We hybridize the ratios in our random ticket with this method and propose a new method called ""hybrid tickets"", which achieves further improvement. (Our code is publicly available at this https URL) ",Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot
173,1308764410245246982,314014164,Adrian Price-Whelan,"['Paper this week led by @monica_valluri: We find that long, outer halo stellar streams (e.g., Sagittarius) should be very sensitive to dark matter halo figure rotation <LINK>']",https://arxiv.org/abs/2009.09004,"The dark matter halos that surround Milky Way-like galaxies in cosmological simulations are, to first order, triaxial. Nearly 30 years ago it was predicted that such triaxial dark matter halos should exhibit steady figure rotation or tumbling motions for durations of several gigayears. The angular frequency of figure rotation predicted by cosmological simulations is described by a log-normal distribution of pattern speed with a median value 0.15hkm/s/kpc (~ 0.15h rad/Gyr ~ 9h deg/Gyr) and a width of 0.83km/s/kpc. These pattern speeds are so small that they have generally been considered both unimportant and undetectable. In this work we show that even this extremely slow figure rotation can significantly alter the structure of extended stellar streams produced by the tidal disruption of satellites in the Milky Way halo. We simulate the behavior of a Sagittarius-like polar tidal stream in triaxial dark matter halos with different shapes, when the halos are rotated about the three principal axes. For pattern speeds typical of cosmological halos we demonstrate, for the first time, that a Sagittarius-like tidal stream would be altered to a degree that is detectable even with current observations. This discovery will potentially allow for a future measurement of figure rotation of the Milky Way's dark halo, and perhaps enabling the first evidence of this relatively unexplored prediction of LambdaCDM. ",Detecting the Figure Rotation of Dark Matter Halos with Tidal Streams
174,1308736274807820288,19501739,colin gray,"['Happy to announce a preprint of a study with rock star collaborators @Cristianapt,@nataliabielova, Michael Toth, and @damicli! We evaluate consent banner requirements through  the lens of #darkpatterns, #design, #privacylaw, and #dataprivacy. <LINK>']",https://arxiv.org/abs/2009.10194,"User engagement with data privacy and security through consent banners has become a ubiquitous part of interacting with internet services. While previous work has addressed consent banners from either interaction design, legal, and ethics-focused perspectives, little research addresses the connections among multiple disciplinary approaches, including tensions and opportunities that transcend disciplinary boundaries. In this paper, we draw together perspectives and commentary from HCI, design, privacy and data protection, and legal research communities, using the language and strategies of ""dark patterns"" to perform an interaction criticism reading of three different types of consent banners. Our analysis builds upon designer, interface, user, and social context lenses to raise tensions and synergies that arise together in complex, contingent, and conflicting ways in the act of designing consent banners. We conclude with opportunities for transdisciplinary dialogue across legal, ethical, computer science, and interactive systems scholarship to translate matters of ethical concern into public policy. ","Dark Patterns and the Legal Requirements of Consent Banners: An
  Interaction Criticism Perspective"
175,1308697467144335360,2829453443,Jamie O'Hare,"[""📰 @Lynsay and I's #BugBounty paper is now live on arXiv. \n\nWe propose a possible bug bounty solution appropriate for resource and economically limited organisations, such as Universities. \n\nLink 👉 <LINK>\n\nFind out more information in the thread below 👇 <LINK>"", ""The main gist of the proposal is the additional crowd sourcing of the report verification process, as seen below.\n\nThis process also allows those who can't find vulnerabilities on BBPs (reportedly &gt;90%) can learn from others through practical reproduction. https://t.co/RPFqG7jP8x"", 'Obviously, this introduces massive amounts of risk as you are providing confidential information to external hackers.\n\nWe suggest these external hackers should infact be internal individuals where trust is already established. (Ethical Hacking students at University for example)', 'However, with increased participants comes an increase demand for rewards.\n\nThis is where gamification elements can help out.  Substituting some of the rewards, and restructuring payout scale to reflect this may incentivise participation, keep participants while keeping costs low', 'There is more in the paper than I can possibly write on Twitter. \n\nAny feedback is welcomed, be it over twitter or via emails found in the pdf.\n\nEnjoy 🤓', '@TheCyberJoe @Lynsay Thanks Joe! Yeah, the next big test is see if we can get something like this up and running. \n\n3rd party systems will be a major stumbling block, however, with the increased reproduction by verifiers and incentivisation for detailed reports. It should be simple as passing on info', '@TheCyberJoe @Lynsay I know some universities operate their own 3rd party systems without support (like Moodle), however,will these 3rd party systems EULAs allow for bug bounty like activities?\n\nHopefully a followup paper with all this and more in about 1.5-2 years time.😎']",https://arxiv.org/abs/2009.10158,"Despite significant popularity, the bug bounty process has remained broadly unchanged since its inception, with limited implementation of gamification aspects. Existing literature recognises that current methods generate intensive resource demands, and can encounter issues impacting program effectiveness. This paper proposes a novel bug bounty process aiming to alleviate resource demands and mitigate inherent issues. Through the additional crowdsourcing of report verification where fellow hackers perform vulnerability verification and reproduction, the client organisation can reduce overheads at the cost of rewarding more participants. The incorporation of gamification elements provides a substitute for monetary rewards, as well as presenting possible mitigation of bug bounty program effectiveness issues. Collectively, traits of the proposed process appear appropriate for resource and budget-constrained organisations - such Higher Education institutions. ",Proposal of a Novel Bug Bounty Implementation Using Gamification
176,1308449116318396422,3025082120,Shayne Longpre,"['QA models are surprisingly accurate on partial, OOD inputs. Yi, @chrisdubois, and I ask what factors affect this? Perplexingly we find this phenomenon invariant to random seed, architecture, pretraining, even training domain!\n \nSee our short preprint [1/5]\n<LINK>', 'Looking at @MRQA2019 data we see models trained on domain A still perform well on Minimal Prediction Preserving Inputs (MPPI) from domain B. In other words, successful OOD inputs transfer well between datasets with very different crowdsourcing and annotation procedures. [2/5] https://t.co/3yao902fhe', 'Further, regularizing models that perform well on MPPIs forces them to attend to longer, more human interpretable inputs, but does not noticeably improve their generalization or adversarial robustness. [3/5] https://t.co/ISMcGhILvS', 'With these findings we hope to refine previous hypotheses on out-of-distribution performance, including poor posterior calibration of neural models, lack of pre-training, and “dataset bias” (where a model learns spurious, non-generalizable cues in the training data). [4/5]', 'We would like to thank @Eric_Wallace_, @ihsgnef, @boydgraber, Christopher Clark, @ml_drew, @kanitw, @lao_ni, and Charlie Maalouf for their guiding insights and helpful discussion! [5/5]', '@NafiseSadat Thank you for sharing this Nafise! It looks super interesting and relevant.\n\nWe absolutely agree there are domain-specific biases in QA. Our work instead suggests some types of partial inputs (MPPIs) are not necessarily as reflective of “bias” as human interpretation indicates.', '@NafiseSadat Please let us know if any sections were ambiguous in that sense, or might be phrased better to reflect that :)']",https://arxiv.org/abs/2009.08070,"Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and ""dataset bias"" (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples. ","On the Transferability of Minimal Prediction Preserving Inputs in
  Question Answering"
177,1308132826387107841,2239670346,Jonathan Frankle,"['Several methods have recently been proposed for pruning neural networks at initialization. In our new paper (@KDziugaite, @roydanroy, @mcarbin), we rigorously study these methods to determine why they ""miss the mark"" and underperform pruning after training <LINK> <LINK>', 'TLDR: We find that these methods extract useful layerwise pruning rates rather than determining which individual connections to prune. This undermines the claimed justifications for the methods and suggests broader challenges with the methods, pruning at initialization, or both.', ""Main story: we've pruned to lower inference costs since the 80s, but we've only recently used pruning to lower training costs. Our lottery ticket work showed it may be possible to prune early in training; SNIP, GraSP, and SynFlow are efficient proposals to prune at initialization https://t.co/ctM0sMJdin"", ""These methods make some progress: they outperform random pruning. However, they remain far below pruning after training. Importantly, no single method is SOTA: it is possible to find a sparsity/network where each method (including naive magnitude pruning) looks like it's the best https://t.co/Ns0cc8I2Ou"", 'To understand why, we ran a series of ablations: randomly shuffling which weights are pruned per layer, reinitializing the unpruned weights, and ""inverting"" (pruning the most important weights). Our goal was to figure out which signals these methods actually use when pruning. https://t.co/gPx81vkS0x', 'We find that you can randomly shuffle or reinitialize the pruned networks and maintain (or, for SynFlow, *improve*) accuracy. That is, the useful information these methods extract are the per-layer proportions in which to prune. This is *not* the case when pruning after training. https://t.co/IsBp3RZ6ao', 'In the case of GraSP, you can actually prune the *most important* weights and reach the same accuracy. We actually find that pruning weights with the lowest magnitude GraSP scores leads to better performance. https://t.co/atZWswLJgv', ""Takeaway 1: Although these methods use very different pruning heuristics, they have surprisingly similar behaviors that are distinct from pruning after training. This may suggest that it's inherent difficulty to prune at initialization (e.g., https://t.co/ObD659dYS8 @utkuevci)."", 'Takeaway 2: The ablations (esp. shuffling) undermine the claimed rationales for the methods, which are said to ""identify important connections"" (SNIP), ""remove weights that will not reduce gradient flow,"" and ""tak[e] the inter-layer interactions of params into account"" (SynFlow).', 'Takeaway 3: If the problem is pruning at initialization, why not prune after some training? We tried pruning at many points during training, but accuracy improved only gradually. If we want to prune early in training, we will need new heuristics designed specifically to do so. https://t.co/VPRdHnhCeL', ""Summary: Proposals for pruning at initialization currently miss the mark: they underperform pruning after training, and you don't need to prune specific weights to match their accuracy. Is this due to the heuristics themselves, or is it inherent to pruning at initialization? https://t.co/rHqN18yMdz"", 'P.S. Why does SynFlow sometimes *improve* when shuffling? We believe that it is because SynFlow prunes entire neurons at far lower sparsities than other methods. This ""neuron collapse"" reduces the capacity of the network and may inhibit accuracy. Shuffling restores many neurons. https://t.co/lshIhnoDKu', '@adityakusupati My big takeaway is that, given such different heuristics (magnitude, gradient, hessian, with/without data) have the same pathology, there may be inherent challenges to pruning at initialization. The next question is whether we can find new heuristics that work *early* in training', ""@adityakusupati I'm glad folks are starting to look at ImageNet. ResNet-50 on ImageNet and ResNet-20 on CIFAR-10 behave very very differently from the overparameterized wide ResNets (and ImageNet ResNets being used on CIFAR-10) that are common in these papers."", ""@adityakusupati To be fair, very few people have those sorts of resources. Getting the ImageNet data for this paper was time-consuming and expensive and we still weren't able to get it for all experiments. And the TinyImageNet situation is (1) a mess and (2) unrepresentative of ImageNet."", '@adityakusupati We need a better intermediate tasks on the scale of TinyImageNet but with the same behavior as ResNet-20 on CIFAR-10 and ResNet-50 on ImageNet. Add that to the list of things to do...', ""@jxbz It's definitely something worth looking into as follow-up to our work. However, only SynFlow has issues with neuron collapse (likely due to a pathology where it punishes weights whose neurons have already been pruned), so I'm not sure that it needs addressing outside of SynFlow.""]",https://arxiv.org/abs/2009.08576,"Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both. ",Pruning Neural Networks at Initialization: Why are We Missing the Mark?
178,1307878215843799040,1183882197528436736,Miguel Martinez,"[""Very happy to finally be able to present my first paper at @NUCIERA I've been working on for the past year as a post-bac! In this study, we look at mergers from BH triple systems in GCs and their implications. Certainly very different from exomoons...\n\n<LINK>""]",http://arxiv.org/abs/2009.08468,"Hierarchical triples are expected to be produced by the frequent binary-mediated interactions in the cores of globular clusters. In some of these triples, the tertiary companion can drive the inner binary to merger following large eccentricity oscillations, as a result of the eccentric Kozai-Lidov mechanism. In this paper, we study the dynamics and merger rates of black hole (BH) hierarchical triples, formed via binary--binary encounters in the CMC Cluster Catalog, a suite of cluster simulations with present-day properties representative of the Milky Way's globular clusters. We compare the properties of the mergers from triples to the other merger channels in dense star clusters, and show that triple systems do not produce significant differences in terms of mass and effective spin distribution. However, they represent an important pathway for forming eccentric mergers, which could be detected by LIGO--Virgo/KAGRA (LVK), and future missions such as LISA and DECIGO. We derive a conservative lower limit for the merger rate from this channel of $0.35$ Gpc$^{-3}$yr$^{-1}$ in the local Universe and up to $\sim9\%$ of these events may have a detectable eccentricity at LVK design sensitivity. Additionally, we find that triple systems could play an important role in retaining second-generation BHs, which can later merge again in the core of the host cluster. ",Black Hole Mergers from Hierarchical Triples in Dense Star Clusters
179,1307846058286837761,4827295586,Kevin Wagner,"['Check out this ~20-30 Jupiter-mass companion to a young star that we recently imaged with VLT/SPHERE! You can find the paper here: <LINK> and some of my favorite aspects about this system in the thread below. 1/4 @danielapai @AstroThayne @SPHERE_outreach <LINK>', 'At just 0.125 arcsec, this detection was quite challenging, and took us some time and effort to pull out. For comparison, the habitable zones of nearby sun-like stars are at separations of ~0.1 arcsec, and with continued progress we may image even smaller (rocky) planets. 2/4', ""With a mass ratio of ~0.01, HIP 75056Ab likely formed via gravitational disk instability, although we can't rule out a planet-like core accretion origin. Uncovering how the companion likely formed will be one of the interesting future possibilities. 3/4"", 'Finally, with a temperature over 2000K, the atmosphere of HIP 75056Ab is as hot as those of some cool (M-type) stars, which makes it an interesting comparison to older sub-stellar objects with colder temperatures. Check out the paper for more details! https://t.co/ETr5Djh6ge']",https://arxiv.org/abs/2009.08537,"We present the discovery and spectroscopy of HIP 75056Ab, a companion directly imaged at a very small separation of 0.125 arcsec to an A2V star in the Scorpius-Centaurus OB2 association. Our observations utilized VLT/SPHERE between 2015$-$2019, enabling low-resolution spectroscopy (0.95$-$1.65 $\mu m$), dual-band imaging (2.1$-$2.25 $\mu m$), and relative astrometry over a four-year baseline. HIP 75056Ab is consistent with spectral types in the range of M6$-$L2 and $T_{\rm eff}\sim$ 2000$-$2600 K. A comparison of the companion's brightness to evolutionary tracks suggests a mass of $\sim$20$-$30 M$_{Jup}$. The astrometric measurements are consistent with an orbital semi-major axis of $\sim$15$-$45 au and an inclination close to face-on (i$\lesssim$35$^o$). In this range of mass and orbital separation, HIP 75056Ab is likely at the low-mass end of the distribution of companions formed via disk instability, although a formation of the companion via core accretion cannot be excluded. The orbital constraints are consistent with the modest eccentricity values predicted by disk instability, a scenario that can be confirmed by further astrometric monitoring. HIP 75056Ab may be utilized as a low-mass atmospheric comparison to older, higher-mass brown dwarfs, and also to young giant planets. Finally, the detection of HIP 75056Ab at 0.125 arcsec represents a milestone in detecting low-mass companions at separations corresponding to the habitable zones of nearby Sun-like stars. ",Direct Imaging Discovery of a Young Brown Dwarf Companion to an A2V Star
180,1306945727843762176,1093604920262037505,Andrea Gokus,"[""#papertime!\nToday on #arXiv you can find our new approach on how to search for high-redshift blazars with the gamma-ray satellite Fermi-LAT, by Michael Kreter et al.: <LINK>\n\n'Why do we care about these sources?' and 'What is your new approach?', you might ask... <LINK>"", ""Of all blazars seen by Fermi-LAT, only a few are high-z sources! That's mainly due to their usually low flux, which cannot reach beyond the LAT's sensitivity, however distant blazars can still be detected:\nWhen showing flaring behaviour on monthly or, rarely, daily timescales! https://t.co/kZyxFH28Lg"", 'In this paper we present a new method to search for undetected high-z gamma-ray emitting blazars.\nWe chose 176 blazars from the BZCAT and the SHAO catalogs, which have not been previously reported in the 3FGL catalog and computed monthly-binned gamma-ray light curves. https://t.co/R69bK7geqA', 'The new thing about our work is that we are not only looking for single, high significant (TS &gt; 25 ~ 5 Sigma) detections, but also for medium significant (TS &gt; 9 ~ 3 Sigma) detections, if they arise several times for the same source. Sure, we need to account for possible ... https://t.co/ZAKRjDkvRX', '... false-positive detections and we do so by computing a sample of light curves at random sky positions without any known gamma-ray source close-by.\n\nThis new &amp; cool approach led to us finding 6 previously unknown high-z blazars, with the most distant one at redshift z = 4.72! https://t.co/Z7hPDtOgpG']",https://arxiv.org/abs/2009.07945,"High-$z$ blazars (z $\geq 2.5$) are the most powerful class of persistent $\gamma$-ray sources in the Universe. These objects possess the highest jet powers and luminosities and have black hole masses often in excess of $10^9$ solar masses. In addition, high-$z$ blazars are important cosmological probes and serve as test objects for blazar evolution models. Due to their large distance, their high-energy emission typically peaks below the GeV range, which makes them difficult to study with Fermi/LAT. Therefore, only the very brightest objects are detectable and, to date, only a small number of high-z blazars have been detected with Fermi/LAT. In this work, we studied the monthly binned long-term $\gamma$-ray emission of a sample of 176 radio and optically detected blazars that have not been reported as known $\gamma$-ray sources in the 3FGL catalog. In order to account for false-positive detections, we calculated monthly Fermi/LAT light curves for a large sample of blank sky positions and derived the number of random fluctuations that we expect at various test statistic (TS) levels. For a given blazar, a detection of TS > 9 in at least one month is expected $\sim 15\%$ of the time. Although this rate is too high to secure detection of an individual source, half of our sample shows such single-month $\gamma$-ray activity, indicating a population of high-energy blazars at distances of up to z=5.2. Multiple TS > 9 monthly detections are unlikely to happen by chance, and we have detected several individual new sources in this way, including the most distant $\gamma$-ray blazar, BZQ J1430+4204 (z = 4.72). Finally, two new $\gamma$-ray blazars at redshifts of z = 3.63 and z = 3.11 are unambiguously detected via very significant (TS > 25) flares in individual monthly time bins. ",Search for high-redshift blazars with Fermi/LAT
181,1306940953681371137,822198179282227200,Carlo Sparaciari,"['New preprint out today on the arXiv! We study resource distillation in general convex Gaussian resource theories, where classical randomness and conditional operations are allowed. Distillation becomes possible, albeit in a limited fashion!  \n\n<LINK> <LINK>', 'Thanks to @HaileyJee and Mario Berta for yet another great collaboration.']",https://arxiv.org/abs/2009.08434,"It is known that distillation in continuous variable resource theories is impossible when restricted to Gaussian states and operations. To overcome this limitation, we enlarge the theories to include convex mixtures of Gaussian states and operations. This extension is operationally well-motivated since classical randomness is easily accessible. We find that resource distillation becomes possible for convex Gaussian resource theories-albeit in a limited fashion. We derive this limitation by studying the convex roof extension of a Gaussian resource measure and then go on to show that our bound is tight by means of example protocols for the distillation of squeezing and entanglement. ",Resource distillation in convex Gaussian resource theories
182,1306902651012034560,361971478,Saad Aloteibi,['Pre-print of our #CIKM2020 paper “Learning to personalize for web search sessions”. We study the use of pre-computed user models to personalize search results at the session level. A joint work with Stephen Clark.\n<LINK>'],http://arxiv.org/abs/2009.08206,"The task of session search focuses on using interaction data to improve relevance for the user's next query at the session level. In this paper, we formulate session search as a personalization task under the framework of learning to rank. Personalization approaches re-rank results to match a user model. Such user models are usually accumulated over time based on the user's browsing behaviour. We use a pre-computed and transparent set of user models based on concepts from the social science literature. Interaction data are used to map each session to these user models. Novel features are then estimated based on such models as well as sessions' interaction data. Extensive experiments on test collections from the TREC session track show statistically significant improvements over current session search algorithms. ",Learning to Personalize for Web Search Sessions
183,1306757838258286592,983857052840636417,Rob Corless,"['I am delighted to announce that our paper on Integrals of Functions Containing Parameters has been accepted to the Mathematical Gazette. You may find a copy (sans an enlightening cartoon which we have license to publish in a journal but not online) at <LINK>', 'You may find the cartoon at https://t.co/hdTSRJgGA2', ""The paper addresses an annoying difficulty with integrals of continuous functions containing parameters: unless you're careful, the integral will not be correct for all possible values of the parameters. We have a suggested method... It's not difficult, but it does go against"", 'how we teach Calculus today. I hope that you enjoy it!']",http://arxiv.org/abs/2009.08431,"This paper offers what seems at first to be a minor technical correction to the current practice of computing indefinite integrals, and introduces the idea of a ""Kahanian constant of integration"". However, the total impact of this minor correction is potentially large because the current practice is taught early at the university level and to very many students---most of whom do not go on to become mathematics majors. Moreover, computer algebra systems have become widespread, including good free ones, some of which are available for smartphones. Most current computer algebra systems apply current textbook rules and amplify the effects of fundamental ""minor"" errors such as the error in continuity that we address in this article. So in practice, the correction we present is important. ",Integrals of functions containing parameters
184,1306711032119701504,36653441,Johan Ugander,"['Long new paper, ""Randomized Graph Cluster Randomization"", with Hao Yin. <LINK> Building on earlier work on graph cluster randomization (GCR) fo causal inference on networks, we propose and study designs that use a randomized graph clustering instead! 🧵 1/n <LINK>', 'The global average treatment effect (GATE) estimand asks for the difference between when a network is all-treated vs. all-control. Under SUTVA, the GATE is the ATE. Without SUTVA, we use ""exposure models"" (assumptions about when someone is ""as if"" all-treated).  2/n', ""Aronow &amp; @cdsamii's 2017 AOAS paper is a great starting point for reading up on beyond-SUTVA causal inference on networks: https://t.co/T0LwtDs3bf A pre-print from ~2012 had a strong influence on me during my PhD! 3/n"", 'GCR creates a single fixed clustering of the interference graph and then randomizes treatment/control at the cluster level. Developed in this KDD13 paper https://t.co/TjV2TcPU8X and JCI17 paper: https://t.co/DFPz3T1pQI 4/n', 'Randomized GCR (RGCR) started as an attempt to solve the following problem: in GCR with a fixed clustering, sometimes nodes get unlucky. Mixing up the clustering ""smooths out"" the exposure probability by smoothing out the luck. 5/n https://t.co/K6nsW4136R', ""One of the positive results from the GCR work was that, under a restricted growth condition on the network (ball sizes don't grow too fast), the variance of a GATE estimate could be upper bounded by a quantity polynomial in the max degree (iid design is exponential in it). 6/n https://t.co/kPJaqnikkm"", ""Under RGCR, that bound becomes polynomial in max degree _and_ in the growth parameter! That's great, because I've known for a while that the growth assumption only holds for very large kappa. So, huge difference! 7/n https://t.co/70eZrwxkUw"", 'The paper has a whole appendix (A) that, to my knowledge, is the first detailed look at the empirics of how ball sizes grow in empirical social networks. The FB100 networks are small enough (and Hao a 31337 enough coder) that we were pretty easily able to crank these out. 8/n https://t.co/HuI4gEhro0', 'Growth conditions have been showing up more in the network experimentation literature recently, so it feels \nimportant to understand how reasonable they are for varied interference networks (above, friendship networks). E.g: https://t.co/snSsTB6azI &amp; https://t.co/6HtQf7ViM3 9/n', 'For RGCR, we propose and analyze two simple algorithms: randomized 3-net and a new-ish algorithm we call 1-hop-max. Aaron Sidford helped us spot that 1-hop-max is closely related to the CKR and FRT algorithms from the lit on metric approximation. Neat! 10/n https://t.co/J1H0wovldV', 'Theorems are nice, but what happens in practice? Well, GCR hides some had constants (p^{-kappa^6}), and when you start randomizing GCR (K&gt;1 in plot below), variance goes way way down. Notice y-axes scale. 11/n https://t.co/x40gQhXDap', 'Weighted variations on our randomized algorithms do slightly better still. One weighting (""spectral"") assigns weights to balance out exposure probabilities as part of an optimization heuristic that has a nice spectral solution. Fun! 12/n https://t.co/VRAVCi0CGM', 'Returning to the first plot of the 🧵, we study both HT and Hajek estimators of the GATE. The bias (of Hajek) and variance (of both) depend on network size/properties. RGCR w/ Hajek really shines when networks get large (here a growing family of ""small world++"" networks). 13/n https://t.co/Xs0YuTRvVe', 'Our simulations try to incorporate all the important challenges we see in network experimentation. Heavy-tailed degree distribution, response correlated with degree, response homophily (shown below), etc. ""As simple as possible, but not simpler""  is still complicated! 14/n https://t.co/pvbWLaVEFq', 'There are also a lot of open directions. Come up with new (or import known) randomized clustering algos that reduce variance well (in theory and/or practice). Expand on our study of the ball-structure of empirical networks to capture the important. Could go on! 15/n', ""This paper has been a labor of love. Hao presented a preliminary version at CODE@MIT last year. We had a draft with most of the pieces then, but it took 10 months to actually finish it. We'd would love feedback from anyone who takes a look! 16/n"", ""To end, a hat-tip to the late Stephen Fienberg. While finishing this paper, I was reminded of the workshop he organized at CMU in 2013 and this review essay of his: https://t.co/KKHffjeQXd Network experimentation is hard. We've come a long way, but still lots to do. RIP. 17/17 https://t.co/kOpmVOgZkd""]",https://arxiv.org/abs/2009.02297,"The global average treatment effect (GATE) is a primary quantity of interest in the study of causal inference under network interference. With a correctly specified exposure model of the interference, the Horvitz-Thompson (HT) and H\'ajek estimators of the GATE are unbiased and consistent, respectively, yet known to exhibit extreme variance under many designs and in many settings of interest. With a fixed clustering of the interference graph, graph cluster randomization (GCR) designs have been shown to greatly reduce variance compared to node-level random assignment, but even so the variance is still often prohibitively large. In this work we propose a randomized version of the GCR design, descriptively named randomized graph cluster randomization (RGCR), which uses a random clustering rather than a single fixed clustering. By considering an ensemble of many different cluster assignments, this design avoids a key problem with GCR where a given node is sometimes ""lucky"" or ""unlucky"" in a given clustering. We propose two randomized graph decomposition algorithms for use with RGCR, randomized 3-net and 1-hop-max, adapted from prior work on multiway graph cut problems. When integrating over their own randomness, these algorithms furnish network exposure probabilities that can be estimated efficiently. We develop upper bounds on the variance of the HT estimator of the GATE under assumptions on the metric structure of the interference graph. Where the best known variance upper bound for the HT estimator under a GCR design is exponential in the parameters of the metric structure, we give a comparable variance upper bound under RGCR that is instead polynomial in the same parameters. We provide extensive simulations comparing RGCR and GCR designs, observing substantial reductions in the mean squared error for both HT and H\'ajek estimators of the GATE in a variety of settings. ",Randomized Graph Cluster Randomization
185,1306690324215996416,1008944276431036416,Boris Ivanovic,"['New paper up on arXiv with Amine Elhafsi, Guy Rosman, @adnothing, @MarcoPavoneSU!! In it, we propose a new multi-agent trajectory forecasting output representation that is much more amenable to downstream planning and control algorithms. Check it out at <LINK>! <LINK>']",https://arxiv.org/abs/2009.07517,"Reasoning about human motion is a core component of modern human-robot interactive systems. In particular, one of the main uses of behavior prediction in autonomous systems is to inform robot motion planning and control. However, a majority of planning and control algorithms reason about system dynamics rather than the predicted agent tracklets (i.e., ordered sets of waypoints) that are commonly output by trajectory forecasting methods, which can hinder their integration. Towards this end, we propose Mixtures of Affine Time-varying Systems (MATS) as an output representation for trajectory forecasting that is more amenable to downstream planning and control use. Our approach leverages successful ideas from probabilistic trajectory forecasting works to learn dynamical system representations that are well-studied in the planning and control literature. We integrate our predictions with a proposed multimodal planning methodology and demonstrate significant computational efficiency improvements on a large-scale autonomous driving dataset. ","MATS: An Interpretable Trajectory Forecasting Representation for
  Planning and Control"
186,1306536228288954368,426509606,Yamir Moreno,"['Our last paper ""Dynamics of heuristics selection for cooperative behavior"" is now out in the arXiv (<LINK>). Here, we study the emergence and fixation of cooperative strategies through a model of heuristics selection based on evolutionary algorithms. <LINK>']",https://arxiv.org/abs/2009.07791,"Situations involving cooperative behaviour are widespread among animals and humans alike. Game theory and evolutionary dynamics have provided the theoretical and computational grounds to understand what are the mechanisms that allow for such cooperation. Studies in this area usually take into consideration different behavioural strategies and investigate how they can be fixed in the population under evolving rules. However, how those strategies emerged from basic evolutionary mechanisms continues to be not fully understood. To address this issue, here we study the emergence of cooperative strategies through a model of heuristics selection based on evolutionary algorithms. In the proposed model, agents interact with other players according to a heuristic specified by their genetic code and reproduce -- at a longer time scale -- proportionally to their fitness. We show that the system can evolve to cooperative regimes for low mutation rates through heuristics selection while increasing the mutation decreases the level of cooperation. Our analysis of possible strategies shows that reciprocity and punishment are the main ingredients for cooperation to emerge, being conditional cooperation the more frequent strategy. Additionally, we show that if in addition to behavioural rules, genetic relatedness is included, then kinship plays a relevant role. Our results illustrate that our evolutionary heuristics model is a generic and powerful tool to study the evolution of cooperative behaviour. ",Dynamics of heuristics selection for cooperative behaviour
187,1306284340154900480,1290298254643650560,Erik Kool,"['Today on arXiv a study led by Sheng Yang from @theOKC of SN 2020faa. This SN appears to be a carbon copy of iPTF 14hls, a peculiar SN II that showed 5 episodes of rebrightening. As of now SN 2020faa has traced the first bump, how many more can we expect? <LINK> <LINK>']",https://arxiv.org/abs/2009.07270,"We present observations of SN 2020faa. This Type II supernova displays a luminous light curve that started to rebrighten from an initial decline. We investigate this in relation to the famous supernova iPTF14hls, which received a lot of attention and multiple interpretations in the literature, however whose nature and source of energy still remains unknown. We demonstrate the great similarity between SN 2020faa and iPTF14hls during the first 6 months, and use this comparison both to forecast the evolution of SN 2020faa and to reflect on the less well observed early evolution of iPTF14hls. We present and analyse our observational data, consisting mainly of optical light curves from the Zwicky Transient Facility in the gri bands as well as a sequence of optical spectra. We construct colour curves, a bolometric light curve, compare ejecta-velocity and Black-body radius evolutions for the two supernovae, as well as for more typical Type II supernovae. The light curves show a great similarity with those of iPTF14hls over the first 6 months, in luminosity, timescale and colours. Also the spectral evolution of SN 2020faa is that of a Type II supernova, although it probes earlier epochs than those available for iPTF14hls. The similar light curve behaviour is suggestive of SN 2020faa being a new iPTF14hls. We present these observations now to advocate follow-up observations, since most of the more striking evolution of supernova iPTF14hls came later, with light curve undulations and a spectacular longevity. On the other hand, for SN 2020faa we have better constraints on the explosion epoch than we had for iPTF14hls, and we have been able to spectroscopically monitor it from earlier phases than was done for the more famous sibling. ",Supernova SN 2020faa -- an iPTF14hls look-alike?
188,1306034093411495939,67043272,Rafael Martínez Galarza,"[""We have submitted a paper to MNRAS in which we explore how to combine anomaly detection methods to:\n1) Identify interesting astrophysical light curves, \n2) Find analogs to those LCs. \nIt's a story of Waldo &amp; co.  Find it here <LINK> <LINK>"", 'Kudos to students Dennis Crake, Kushal Tirumala, and Daniel Giles, the students who did most of the work.', 'Comments are welcome and appreciated.']",https://arxiv.org/abs/2009.06760,"Our understanding of the Universe has profited from deliberate, targeted studies of known phenomena, as well as from serendipitous, unexpected discoveries, such as the discovery of a complex variability pattern in the direction of KIC 8462852 (Boyajian's star). Upcoming surveys, such as the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST), will explore the parameter space of astrophysical transients at all time scales, and offer the opportunity to discover even more extreme examples of unexpected phenomena. We investigate strategies to identify novel objects and to contextualize them within large time-series data sets in order to facilitate the discovery of new classes of objects, as well as the physical interpretation of their anomalous nature. We develop a method that combines tree-based and manifold-learning algorithms for anomaly detection in order to perform two tasks: 1) identify and rank anomalous objects in a time-domain dataset; and 2) group those anomalies according to their similarity in order to identify analogs. We achieve the latter by combining an anomaly score from a tree-based method with a dimensionality manifold-learning reduction strategy. Clustering in the reduced space allows for the successful identification of anomalies and analogs. We also assess the impact of pre-processing and feature engineering schemes and investigate the astrophysical nature of the objects that our models identify as anomalous by augmenting the Kepler data with Gaia color and luminosity information. We find that multiple models, used in combination, are a promising strategy to identify novel light curves and light curve families. ","A method for finding anomalous astronomical light curves and their
  analogs"
189,1305862048400715776,106962512,Juan Felipe Carrasquilla Álvarez,"['Here is a new paper where we use a probabilistic formulation of quantum mechanics to study open quantum system dynamics. Work by Di Luo @Illinois_Alma, who interned at @VectorInst last year and has since produced a lot of interesting results. <LINK>']",https://arxiv.org/abs/2009.05580,"The theory of open quantum systems lays the foundations of a substantial part of modern research in quantum science and engineering. Rooted in the dimensionality of their extended Hilbert spaces, the high computational complexity of simulating open quantum systems calls for the development of strategies to approximate their dynamics. In this paper, we present an approach for tackling open quantum system dynamics. We simulate the dynamics of the Liouvillian superoperator using a forward-backward trapezoid method and find the steady-state via a variational formulation. We make use of a probabilistic formulation of quantum physics based on a positive operator-valued measure (POVM) in combination with autoregressive neural networks, which bring significant algorithmic flexibility due to their efficient sampling and tractable density. We introduce improved ansatzs, String States, which partially restore the symmetry of the autoregressive neural network and improve the description of local correlations. We benchmark our approaches on prototypical one and two-dimensional systems, finding results which closely track the exact solution and achieve higher accuracy in comparison to the recently proposed approach based on restricted Boltzmann machines. We anticipate this approach will be widely applicable to evolving density matrices in various contexts. ","Autoregressive Neural Network for Simulating Open Quantum Systems via a
  Probabilistic Formulation"
190,1305374612356845568,422672164,Dr Michael Reidinger,"['A gravitational-wave limit on the Chandrasekhar mass of dark matter\n\n""We explore a new paradigm to study dissipative dark matter models using gravitational-wave observations.""\n<LINK>']",https://arxiv.org/abs/2009.05209,"We explore a new paradigm to study dissipative dark matter models using gravitational-wave observations. We consider a dark atomic model which predicts the formation of binary black holes such as GW190425 while obeying constraints from large-scale structure, and improving on the missing satellite problem. Using LIGO and Virgo gravitational-wave data from 12th September 2015 to 1st October 2019, we show that interpreting GW190425 as a dark matter black-hole binary limits the Chandrasekhar mass for dark matter to be below 1.4 $M_\odot$ at $> 99.9\%$ confidence implying that the dark proton is heavier than 0.95 GeV, while also suggesting that the molecular energy-level spacing of dark molecules lies near $10^{-3}$ eV and constraining the cooling rate of dark matter at low temperatures. ",A gravitational-wave limit on the Chandrasekhar mass of dark matter
191,1304802684982243331,927915414151147521,Antonis Papasavva,"['📢 In our latest work, \\w the usual suspects @iDRAMALab @emilianoucl @gianluca_string @jhblackb @zsavvas90, we perform a preliminary analysis of the QAnon movement on <LINK>. \nFind our preprint here 📰👇👇👇<LINK>', 'We find that the submissions in v/GreatAwakening tend to get approximately 57 upvotes and only 1.7 downvotes. On average, the /v/GreatAwakening submissions tend to be positively voted with the final vote (sum) being 59, and the median sum being ~32. https://t.co/hHlCsIBtNd', 'Alarmingly, the audience of /v/GreatAwakening consumes content from a handful of users. The top submitter is responsible for 31.47% (1.36K) submissions. Excluding the top 15 submitters, all other users only posted 31.14% (1.34K) submissions. 📊 https://t.co/9GYmSHYCRk', 'Interestingly, we observe that about 26% (932 users) of the users in our dataset registered a new account on\nVoat in September 2018: the month Reddit banned many QAnon related subreddits. Our results show that user migration is apparent when other communities get banned.📊📈🚫 https://t.co/a40wCzbWBM', 'Last, we show that QAnon related discussions are hateful and racist. The term ""jew"" is closely related to holocaust associated words, and the terms ""masters,"" and ""puppet,"" showing that users in this community blame people of Jewish religion/descent for leading the ""deep state""🤬']",https://arxiv.org/abs/2009.04885,"Online fringe communities offer fertile grounds for users seeking and sharing ideas fueling suspicion of mainstream news and conspiracy theories. Among these, the QAnon conspiracy theory emerged in 2017 on 4chan, broadly supporting the idea that powerful politicians, aristocrats, and celebrities are closely engaged in a global pedophile ring. Simultaneously, governments are thought to be controlled by ""puppet masters,"" as democratically elected officials serve as a fake showroom of democracy. This paper provides an empirical exploratory analysis of the QAnon community on Voat.co, a Reddit-esque news aggregator, which has captured the interest of the press for its toxicity and for providing a platform to QAnon followers. More precisely, we analyze a large dataset from /v/GreatAwakening, the most popular QAnon-related subverse (the Voat equivalent of a subreddit), to characterize activity and user engagement. To further understand the discourse around QAnon, we study the most popular named entities mentioned in the posts, along with the most prominent topics of discussion, which focus on US politics, Donald Trump, and world events. We also use word embeddings to identify narratives around QAnon-specific keywords. Our graph visualization shows that some of the QAnon-related ones are closely related to those from the Pizzagate conspiracy theory and so-called drops by ""Q."" Finally, we analyze content toxicity, finding that discussions on /v/GreatAwakening are less toxic than in the broad Voat community. ","""Is it a Qoincidence?"": An Exploratory Study of QAnon on Voat"
192,1304226115205046272,1148910974218321920,Dr. Isobel Romero-Shaw,"['Excited to release this! In my new paper (<LINK>) with @LaskyPaul, @EHThrane and @juan__cb, we find evidence that GW190521 may have come from an *eccentric* binary! This supports the hypothesis that it and other @LIGO @ego_virgo mergers formed *dynamically*!']",http://arxiv.org/abs/2009.04771,"Pair instability supernovae are thought to restrict the formation of black holes in the mass range ~50 - 135 solar masses. However, black holes with masses within this ""high mass gap"" are expected to form as the remnants of binary black hole mergers. These remnants can merge again dynamically in densely populated environments such as globular clusters. The hypothesis that the binary black hole merger GW190521 formed dynamically is supported by its high mass. Orbital eccentricity can also be a signature of dynamical formation, since a binary that merges quickly after becoming bound may not circularize before merger. In this work, we measure the orbital eccentricity of GW190521. We find that the data prefer a signal with eccentricity $e \geq 0.1$ at 10 Hz to a non-precessing, quasi-circular signal, with a log Bayes factor $\ln{\cal B}=5.0$. When compared to precessing, quasi-circular analyses, the data prefer a non-precessing, $e \geq 0.1$ signal, with log Bayes factors $\ln{\cal B}\approx2$. Using injection studies, we find that a non-spinning, moderately eccentric ($e = 0.13$) GW190521-like binary can be mistaken for a quasi-circular, precessing binary. Conversely, a quasi-circular binary with spin-induced precession may be mistaken for an eccentric binary. We therefore cannot confidently determine whether GW190521 was precessing or eccentric. Nevertheless, since both of these properties support the dynamical formation hypothesis, our findings support the hypothesis that GW190521 formed dynamically. ","GW190521: orbital eccentricity and signatures of dynamical formation in
  a binary black hole merger signal"
193,1304207633868402688,29251447,Tuan Do,"['Where did the millions of stars at the Galactic center come from? In a new paper, we find evidence that some of the stars probably came from a globular cluster or dwarf galaxy that fell in and got trapped. \n\n1/3\n\n<LINK>', 'How do we know this? We find that stars with low metal abundance move differently around the black hole than the stars with more metals! It is very unusual if they formed in the same place, but could be explained if the stars fell in. https://t.co/R0MEgRKntF', 'In a companion paper, my theory colleagues used computer simulations throwing star clusters and small galaxies into the center of our galaxy to explore this process. The simulations show that this infall might have happened over 3 billion years ago.  \n\nhttps://t.co/zA05uMjVRS', '@ProfAnnikaPeter It was fun to be able to do some dynamical modeling again!']",https://arxiv.org/abs/2009.02335,"The Milky Way nuclear star cluster (MW NSC) has been used as a template to understand the origin and evolution of galactic nuclei and the interaction of nuclear star clusters with supermassive black holes. It is the only nuclear star cluster with a supermassive black hole where we can resolve individual stars to measure their kinematics and metal abundance to reconstruct its formation history. Here, we present results of the first chemo-dynamical model of the inner 1 pc of the MW NSC using metallicity and radial velocity data from the KMOS spectrograph on the Very Large Telescope. We find evidence for two kinematically and chemically distinct components in this region. The majority of the stars belong to a previously known super-solar metallicity component with a rotation axis perpendicular to the Galactic plane. However, we identify a new kinematically distinct sub-solar metallicity component which contains about 7\% of the stars and appears to be rotating faster than the main component with a rotation axis that may be misaligned. This second component may be evidence for an infalling star cluster or remnants of a dwarf galaxy, merging with the MW NSC. These measurements show that the combination of chemical abundances with kinematics is a promising method to directly study the MW NSC's origin and evolution. ","Revealing the Formation of the Milky Way Nuclear Star Cluster via
  Chemo-Dynamical Modeling"
194,1304080373819867136,827263859517857793,Sophia Waddell,"['New paper day - my second lead-author paper (with my supervisor, Luigi Gallo) is out on arxiv:\n\n<LINK>\n\nBased on work I did for my honours thesis at SMU, where we studied X-ray data for 69 type-1 active galaxies observed with Suzaku. 1/4\n\n@smuhalifax @SMUScience', 'These were divided into two groups: narrow-line (NLS1) and broad-line (BLS1), with the goal of finding differences between the two groups. Not only were we able to re-confirm previous results (NLS1s are accreting at a higher rate, and have steeper spectra), we found more! 2/4', ""NLS1 galaxies in general also have more complex spectra - weaker narrow iron lines, and stronger soft and hard excesses (extra photons at the lowest and highest X-ray energies). What's more, the soft and hard excesses are highly correlated for NLS1s - not so for BLS1s. 3/4"", 'This, along with other work presented in the paper, might point to a different physical origin of the soft and hard excesses in NLS1s vs. in BLS1s. More work is needed to study why NLS1s have such unique X-ray properties, but I had a lot of fun trying to figure it out! :) 4/4']",https://arxiv.org/abs/2009.04378,"A sample of narrow-line (NLS1) and broad-line Seyfert 1 (BLS1) galaxies observed with Suzaku is presented. The final sample consists of 22 NLS1s and 47 BLS1s, for a total of 69 AGN that are all at low redshift (z<0.5) and exhibit low host galaxy column densities (<10^22 cm^-2). The average spectrum for each object is fit with a toy model to characterise important parameters, including the photon index, soft excess, Compton hump (or hard excess), narrow iron line strength, luminosity and X-ray Eddington ratio (L_x/L_Edd). We confirm previous findings that NLS1s have steeper power laws and higher X-ray Eddington ratios, but also find that NLS1 galaxies have stronger soft and hard excesses than their BLS1 counterparts. Studying the correlations between parameters shows that the soft and hard excesses are correlated for NLS1 galaxies, while no such correlation is observed for BLS1s. Performing a principal component analysis (PCA) on the measured X-ray parameters shows that while the X-ray Eddington ratio is the main source of variations within our sample (PC1), variations in the soft and hard excesses form the second principal component (PC2) and it is dominated by the NLS1s. The correlation between the soft and hard excess in NLS1 galaxies may suggest a common origin for the two components, such as a blurred reflection model. The presented Suzaku sample of Seyfert 1 galaxies is a useful tool for analysis of the X-ray properties of AGN, and for the study of the soft and hard excesses observed in AGN. ","A Suzaku sample of unabsorbed narrow-line and broad-line Seyfert 1
  galaxies: I. X-ray spectral properties"
195,1301903294604992515,12131042,Jeremy Blackburn,"[""What do you do when social media companies ban you from commenting? a) stop being an asshole or b) make your own browser that let's you comment on things even though you've been banned? In our (@gigaryte and @BadCksum ) #IMC2020 paper, we study Dissenter. <LINK> <LINK>"", ""Dissenter is Gab's forked version of the Brave browser, and it includes a built in commenting system that provides a sort of hidden comment section for any Web site. https://t.co/2fJf5ZGtye"", 'Among other things, we find that quality of comments on Dissenter tend to be of lower quality and more toxic than baseline sites, and the existence of a small cluster of active and highly toxic users. https://t.co/c89bbStsBn']",https://arxiv.org/abs/2009.01772,"Efforts by content creators and social networks to enforce legal and policy-based norms, e.g. blocking hate speech and users, has driven the rise of unrestricted communication platforms. One such recent effort is Dissenter, a browser and web application that provides a conversational overlay for any web page. These conversations hide in plain sight - users of Dissenter can see and participate in this conversation, whereas visitors using other browsers are oblivious to their existence. Further, the website and content owners have no power over the conversation as it resides in an overlay outside their control. In this work, we obtain a history of Dissenter comments, users, and the websites being discussed, from the initial release of Dissenter in Feb. 2019 through Apr. 2020 (14 months). Our corpus consists of approximately 1.68M comments made by 101k users commenting on 588k distinct URLs. We first analyze macro characteristics of the network, including the user-base, comment distribution, and growth. We then use toxicity dictionaries, Perspective API, and a Natural Language Processing model to understand the nature of the comments and measure the propensity of particular websites and content to elicit hateful and offensive Dissenter comments. Using curated rankings of media bias, we examine the conditional probability of hateful comments given left and right-leaning content. Finally, we study Dissenter as a social network, and identify a core group of users with high comment toxicity. ",Reading In-Between the Lines: An Analysis of Dissenter
196,1301887145573928961,1261860960895004672,Jorge Moreno,"['It’s paper day! Using the FIRE simulations and Local Group observations, we find that stellar metallicity gradients correlate your ages. Tip of the hat to the lead author, Francisco Javier Mercado! 🤟🏽🤟🏽🤟🏽 <LINK>', '@AstronoMerc_ @jbprime @MBKplus @AndrewWetzel @PFHopkins_Astro']",https://arxiv.org/abs/2009.01241?fbclid=IwAR18Zb9pHH5WwJ9G8exlL-Ww7Et9i42paHp4mAenBuHtJ0XVjP5lxBLDdcE,"We explore the origin of stellar metallicity gradients in simulated and observed dwarf galaxies. We use FIRE-2 cosmological baryonic zoom-in simulations of 26 isolated galaxies as well as existing observational data for 10 Local Group dwarf galaxies. Our simulated galaxies have stellar masses between $10^{5.5}$ and $10^{8.6} \msun$. Whilst gas-phase metallicty gradients are generally weak in our simulated galaxies, we find that stellar metallicity gradients are common, with central regions tending to be more metal-rich than the outer parts. The strength of the gradient is correlated with galaxy-wide median stellar age, such that galaxies with younger stellar populations have flatter gradients. Stellar metallicty gradients are set by two competing processes: (1) the steady ""puffing"" of old, metal-poor stars by feedback-driven potential fluctuations, and (2) the accretion of extended, metal-rich gas at late times, which fuels late-time metal-rich star formation. If recent star formation dominates, then extended, metal-rich star formation washes out pre-existing gradients from the ""puffing"" process. We use published results from ten Local Group dwarf galaxies to show that a similar relationship between age and stellar metallicity-gradient strength exists among real dwarfs. This suggests that observed stellar metallicity gradients may be driven largely by the baryon/feedback cycle rather than by external environmental effects. ","A Relationship Between Stellar Metallicity Gradients and Galaxy Age in
  Dwarf Galaxies"
197,1301869927410794496,882307001451069440,Francisco J. Mercado,"[""SO SO excited bc it's my first paper day!! We use a suite of isolated dwarf galaxy (FIRE) simulations to study stellar metallicity gradients and their origins: <LINK> <LINK>"", 'We predict that dwarf galaxies follow a gradient-strength-galaxy-age relationship such that galaxies with older stellar populations tend to have stronger (more negative) stellar metallicity gradients. https://t.co/rtvwu3QmxY', 'We also use published results for 10 existing Local Group dwarf galaxies to show that they, too, follow a VERY similar gradient-strength-galaxy-age relationship. https://t.co/oRmWjqyGCR', 'Interestingly, most of these observed galaxies are satellites of the MW while our simulated galaxies are ISOLATED systems... Yet they follow this very similar relationship!', 'This suggests that the environment of a dwarf galaxy likely plays a secondary role in shaping stellar metallicity gradients. Check the paper out to learn about what drives these gradients! https://t.co/2wLirxBaNF', ""I dedicate this paper to José A. Flores Velázquez and Perla Maritza Mercado. I wish they were both still here to celebrate with me but I know they're both proud ❤️❤️"", ""It's important to remember that none of this would be possible w/o support from the community around me. My thanks go out to my co-authors especially @jbprime and @jorgito__moreno. And thanks to @DarthLazar for always being willing to answer my endless onslaught of questions!"", '@j_tharindu thanks Tharindu!', '@astrochicana Muchísimas gracias!! 😊', '@astroarianna 💜💜💜', '@8minutesold Thank you!! 😃', '@ynxmonica Thanks Monica!! 😃', '@Naj_Astro Thanks Najmeh!!']",https://arxiv.org/abs/2009.01241,"We explore the origin of stellar metallicity gradients in simulated and observed dwarf galaxies. We use FIRE-2 cosmological baryonic zoom-in simulations of 26 isolated galaxies as well as existing observational data for 10 Local Group dwarf galaxies. Our simulated galaxies have stellar masses between $10^{5.5}$ and $10^{8.6} \msun$. Whilst gas-phase metallicty gradients are generally weak in our simulated galaxies, we find that stellar metallicity gradients are common, with central regions tending to be more metal-rich than the outer parts. The strength of the gradient is correlated with galaxy-wide median stellar age, such that galaxies with younger stellar populations have flatter gradients. Stellar metallicty gradients are set by two competing processes: (1) the steady ""puffing"" of old, metal-poor stars by feedback-driven potential fluctuations, and (2) the accretion of extended, metal-rich gas at late times, which fuels late-time metal-rich star formation. If recent star formation dominates, then extended, metal-rich star formation washes out pre-existing gradients from the ""puffing"" process. We use published results from ten Local Group dwarf galaxies to show that a similar relationship between age and stellar metallicity-gradient strength exists among real dwarfs. This suggests that observed stellar metallicity gradients may be driven largely by the baryon/feedback cycle rather than by external environmental effects. ","A Relationship Between Stellar Metallicity Gradients and Galaxy Age in
  Dwarf Galaxies"
198,1301767040953462785,1121482511576653824,Samuel Pawel,"['New preprint ""The sceptical Bayes factor for the assessment of replication success"" (<LINK>) together with @HeldLeonhard.\nWe propose a new method for the analysis of replication studies that combines Bayesian hypothesis testing with reverse-Bayes analysis.']",https://arxiv.org/abs/2009.01520,"Replication studies are increasingly conducted but there is no established statistical criterion for replication success. We propose a novel approach combining reverse-Bayes analysis with Bayesian hypothesis testing: a sceptical prior is determined for the effect size such that the original finding is no longer convincing in terms of a Bayes factor. This prior is then contrasted to an advocacy prior (the reference posterior of the effect size based on the original study), and replication success is declared if the replication data favour the advocacy over the sceptical prior at a higher level than the original data favoured the sceptical prior over the null hypothesis. The sceptical Bayes factor is the highest level where replication success can be declared. A comparison to existing methods reveals that the sceptical Bayes factor combines several notions of replicability: it ensures that both studies show sufficient evidence against the null and penalises incompatibility of their effect estimates. Analysis of asymptotic properties and error rates, as well as case studies from the Social Sciences Replication Project show the advantages of the method for the assessment of replicability. ",The sceptical Bayes factor for the assessment of replication success
199,1301383661489618945,1177063549606203394,Tommi Tenkanen,"['My last paper is now out! Together with my collaborator Catarina Cosme we studied a scenario where the observed #DarkMatter is produced in the early universe by amplification of quantum fluctuations of a scalar field during cosmic inflation. <LINK> 1/3', 'In particular, we studied how the (free or self-interacting) dark matter abundance and its perturbation spectrum change if the early universe was not purely radiation dominated but there was a period of e.g. slow reheating after inflation, as is indeed possible. 2/3', ""We also discussed how the scenario could be further tested through primordial dark matter isocurvature and non-Gaussianity. It's not hopeless to test even purely gravitationally-interacting #DarkMatter! 3/3""]",https://arxiv.org/abs/2009.01149,"It has been shown that the observed dark matter (DM) abundance can be produced by amplification of quantum fluctuations of an energetically subdominant scalar field during inflation. In this paper, we study the robustness of this ""spectator dark matter"" scenario to changes in the expansion rate of the early Universe. Compared to the standard radiation-dominated (RD) scenario, two aspects will change: the DM energy density evolves differently as a function of time, and also the DM isocurvature perturbation spectrum will be different from the result in the RD case. These can impose sizeable changes to the values of model parameters which allow the field to constitute all DM while simultaneously satisfying all observational constraints. We study both free and self-interacting DM in scenarios with non-standard expansion and quantify the changes to the cases with a standard cosmological history. We also discuss testability of the scenario through primordial DM isocurvature and non-Gaussianity. ",Spectator dark matter in non-standard cosmologies
200,1301382109148266496,801743,Neil Ernst,"['Together with @JeffCarver32, @mendezfe and @mtorchiano we have written up a study on peer review in software engineering. We looked at how people conduct reviews and what qualities reviewers look for in a paper. Paper: <LINK> Replication: <LINK>', '@siccegge @zacchiro @JeffCarver32 @mendezfe @mtorchiano Seeing you are in crypto, I would guess that the validation is easier to parse than a proof. But variance is quite high in the estimates.']",https://arxiv.org/abs/2009.01209,"Peer review is a key activity intended to preserve the quality and integrity of scientific publications. However, in practice it is far from perfect. We aim at understanding how reviewers, including those who have won awards for reviewing, perform their reviews of software engineering papers to identify both what makes a good reviewing approach and what makes a good paper. We first conducted a series of in-person interviews with well-respected reviewers in the software engineering field. Then, we used the results of those interviews to develop a questionnaire used in an online survey and sent out to reviewers from well-respected venues covering a number of software engineering disciplines, some of whom had won awards for their reviewing efforts. We analyzed the responses from the interviews and from 175 reviewers who completed the online survey (including both reviewers who had won awards and those who had not). We report on several descriptive results, including: 45% of award-winners are reviewing 20+ conference papers a year, while 28% of non-award winners conduct that many. 88% of reviewers are taking more than two hours on journal reviews. We also report on qualitative results. To write a good review, the important criteria were it should be factual and helpful, ranked above others such as being detailed or kind. The most important features of papers that result in positive reviews are clear and supported validation, an interesting problem, and novelty. Conversely, negative reviews tend to result from papers that have a mismatch between the method and the claims and from those with overly grandiose claims. The main recommendation for authors is to make the contribution of the work very clear in their paper. In addition, reviewers viewed data availability and its consistency as being important. ",Understanding Peer Review of Software Engineering Papers
201,1300972587615285248,293552287,Hoan Tran,['We formulate and propose the Universal Approximation Property of Quantum Feature Map.\n<LINK>\nOur research is independent but in the same line with the recent work by Xanadu to understand the expressive power of the quantum model.\n<LINK> <LINK>'],https://arxiv.org/abs/2009.00298,"Encoding classical data into quantum states is considered a quantum feature map to map classical data into a quantum Hilbert space. This feature map provides opportunities to incorporate quantum advantages into machine learning algorithms to be performed on near-term intermediate-scale quantum computers. The crucial idea is using the quantum Hilbert space as a quantum-enhanced feature space in machine learning models. While the quantum feature map has demonstrated its capability when combined with linear classification models in some specific applications, its expressive power from the theoretical perspective remains unknown. We prove that the machine learning models induced from the quantum-enhanced feature space are universal approximators of continuous functions under typical quantum feature maps. We also study the capability of quantum feature maps in the classification of disjoint regions. Our work enables an important theoretical analysis to ensure that machine learning algorithms based on quantum feature maps can handle a broad class of machine learning tasks. In light of this, one can design a quantum machine learning model with more powerful expressivity. ","Universal Approximation Property of Quantum Machine Learning Models in
  Quantum-Enhanced Feature Spaces"
202,1315036352702935040,135061425,Charu Sharma,['We propose two novel self-supervised pre-training tasks that encode a hierarchical partitioning of the point clouds using a cover-tree in our #NeurIPS2020 paper. @ManuKaul1\n Arxiv: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/2009.14168,"The increased availability of massive point clouds coupled with their utility in a wide variety of applications such as robotics, shape synthesis, and self-driving cars has attracted increased attention from both industry and academia. Recently, deep neural networks operating on labeled point clouds have shown promising results on supervised learning tasks like classification and segmentation. However, supervised learning leads to the cumbersome task of annotating the point clouds. To combat this problem, we propose two novel self-supervised pre-training tasks that encode a hierarchical partitioning of the point clouds using a cover-tree, where point cloud subsets lie within balls of varying radii at each level of the cover-tree. Furthermore, our self-supervised learning network is restricted to pre-train on the support set (comprising of scarce training examples) used to train the downstream network in a few-shot learning (FSL) setting. Finally, the fully-trained self-supervised network's point embeddings are input to the downstream task's network. We present a comprehensive empirical evaluation of our method on both downstream classification and segmentation tasks and show that supervised methods pre-trained with our self-supervised learning method significantly improve the accuracy of state-of-the-art methods. Additionally, our method also outperforms previous unsupervised methods in downstream classification tasks. ",Self-Supervised Few-Shot Learning on Point Clouds
203,1312109075765760000,15389063,Saliya Ekanayake,"[""If you'd like to read how we bring the power of parallel and distributed computing to find similar proteins super fast, the SC20 paper is available as pre-print at <LINK>\n\nWant to try it out? Check out the code in GitHub <LINK> <LINK>""]",https://arxiv.org/abs/2009.14467,"Identifying similar protein sequences is a core step in many computational biology pipelines such as detection of homologous protein sequences, generation of similarity protein graphs for downstream analysis, functional annotation and gene location. Performance and scalability of protein similarity searches have proven to be a bottleneck in many bioinformatics pipelines due to increases in cheap and abundant sequencing data. This work presents a new distributed-memory software, PASTIS. PASTIS relies on sparse matrix computations for efficient identification of possibly similar proteins. We use distributed sparse matrices for scalability and show that the sparse matrix infrastructure is a great fit for protein similarity searches when coupled with a fully-distributed dictionary of sequences that allows remote sequence requests to be fulfilled. Our algorithm incorporates the unique bias in amino acid sequence substitution in searches without altering the basic sparse matrix model, and in turn, achieves ideal scaling up to millions of protein sequences. ","Distributed Many-to-Many Protein Sequence Alignment using Sparse
  Matrices"
204,1311596375088521217,1279154780431167488,Chantal Amrhein,['Very happy to have my first PhD paper accepted at Findings of EMNLP! ✨\n\nWe study the trade-off between greater vocab overlap and information loss when using romanization for transfer between scripts in NMT.\n \nMany thanks to @RicoSennrich for advising me! <LINK> <LINK>'],https://arxiv.org/abs/2009.14824,"Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model. ","On Romanization for Model Transfer Between Scripts in Neural Machine
  Translation"
205,1311050874400112640,51169895,Gianluca Stringhini,"['In our latest paper we investigate the effect of sharing fauxtography (manipulated or misleading images) on social network engagement. We find that including fauxtography in posts increases the number of shares on both Twitter and Reddit <LINK> 1/ <LINK>', 'These results confirm anecdotal observations that tweets containing fauxtography attract more retweets and likes than the avg tweets by those same accounts. Some recent examples:\n\nhttps://t.co/7lDH54m28C\nhttps://t.co/0oZ85ACHhB\nhttps://t.co/Tsil6vtjE3\nhttps://t.co/95VrRZyulM 2/', 'This provides another piece of the puzzle in understanding online disinformation. In particular, it highlights that simple measures like miscaptioning a picture can go great lengths in conveying false narratives, without having to employ sophisticated techniques like deepfakes 3/', 'Work led by my student @YupingWang90 with great collaborators from @binghamtonu @uclcs @maxplanckpress @Illinois_Alma 4/4']",https://arxiv.org/abs/2009.11792,"Despite the influence that image-based communication has on online discourse, the role played by images in disinformation is still not well understood. In this paper, we present the first large-scale study of fauxtography, analyzing the use of manipulated or misleading images in news discussion on online communities. First, we develop a computational pipeline geared to detect fauxtography, and identify over 61k instances of fauxtography discussed on Twitter, 4chan, and Reddit. Then, we study how posting fauxtography affects engagement of posts on social media, finding that posts containing it receive more interactions in the form of re-shares, likes, and comments. Finally, we show that fauxtography images are often turned into memes by Web communities. Our findings show that effective mitigation against disinformation need to take images into account, and highlight a number of challenges in dealing with image-based disinformation. ",Understanding the Use of Fauxtography on Social Media
206,1310484108573843456,2369154074,Philipp Koch,"['Our working paper ""A test for Heckscher-Ohlin using value-added exports"" (with Clemens Fessler) is now online: <LINK> We find evidence in favor of Heckscher-Ohlin, if using the ratio of factor compensations as intensity/endowment #EconTwitter #trade @Eco_Austria']",https://arxiv.org/abs/2009.11743,"Empirical evidence for the Heckscher-Ohlin model has been inconclusive. We test whether the predictions of the Heckscher-Ohlin Theorem with respect to labor and capital find support in value-added trade. Defining labor-capital intensities and endowments as the ratio of hours worked to the nominal capital stock, we find evidence against Heckscher-Ohlin. However, taking the ratio of total factor compensations, and thus accounting for differences in technologies, we find strong support for it. That is, labor-abundant countries tend to export value-added in goods of labor-intensive industries. Moreover, differentiating between broad industries, we find support for nine out of twelve industries. ",A test for Heckscher-Ohlin using value-added exports
207,1309546589061607428,87807273,David Barrett,"['Gradient descent can be surprisingly good at optimising deep neural networks without overfitting &amp; without explicit regularisation. \n \nWe have found a hidden form of regularisation - Implicit Gradient Regularisation - that helps explain this.\n\n<LINK> @bdml17 1/5 <LINK>', 'We observe that gradient descent doesn’t actually follow the steepest trajectory down the loss surface gradient, but instead, it takes discrete steps so each step actually moves off the steepest path, onto a shallower path ... 2/5', '… and this gradient descent path is closer to an exact flow along a modified loss surface consisting of the original loss E and a regularisation term that penalizes large loss gradients. 3/5', 'We calculate this Implicit Gradient Regularization using backward error analysis from geometric numerical integration theory. 4/5', 'This explains in part how gradient descent is biased toward flat minima, where solutions are more robust to noisy parameter perturbations and where test errors tend to be lower. 5/5', 'This was a joint collaboration with Benoit Dherin @bdml17 from Google Dublin, and our first DeepMind - Google Dublin paper since I returned to Ireland last year after 12 years abroad!', 'Thanks to Samuel smith @sohamde_  @MihaelaCRosca\n@rpascanu @shakir_za and many other @DeepMind and @GoogleAI for feedback and support during this work.']",https://arxiv.org/abs/2009.11162,"Gradient descent can be surprisingly good at optimizing deep neural networks without overfitting and without explicit regularization. We find that the discrete steps of gradient descent implicitly regularize models by penalizing gradient descent trajectories that have large loss gradients. We call this Implicit Gradient Regularization (IGR) and we use backward error analysis to calculate the size of this regularization. We confirm empirically that implicit gradient regularization biases gradient descent toward flat minima, where test errors are small and solutions are robust to noisy parameter perturbations. Furthermore, we demonstrate that the implicit gradient regularization term can be used as an explicit regularizer, allowing us to control this gradient regularization directly. More broadly, our work indicates that backward error analysis is a useful theoretical approach to the perennial question of how learning rate, model size, and parameter regularization interact to determine the properties of overparameterized models optimized with gradient descent. ",Implicit Gradient Regularization
208,1308805309368954881,48007712,Nathan Ng,"['New work with @kchonyc and @MarzyehGhassemi!\n\narxiv: <LINK>\ncode: <LINK> \n\nWe propose a novel data augmentation scheme based on using a pair of corruption and reconstruction functions to generate new examples along an underlying data manifold. 1/ <LINK>', 'Typical data augmentation methods improve classifiers by making them invariant to local perturbations in the data space, thereby improving generalization. However, in domains like NLP, finding augmentations that preserve meaning and semantics is difficult. 2/ https://t.co/Rf9WAnnPfh', 'We propose a novel data augmentation method that generates examples without the need for any domain knowledge or dataset fine-tuning! A corruption function moves examples off the data manifold, and a reconstruction function projects them back on. 3/ https://t.co/98HzaIh0mw', 'On NLP tasks we apply MLM training noise to move examples off the data manifold and use BERT to project them back on. New examples are pseudo-labelled using the original label or with a teacher model trained on the original training set. 4/ https://t.co/KDioT502p1', 'We test our method’s ability to improve in-domain performance and out-of-domain generalizability across 3 tasks: sentiment analysis, NLI, and MT tasks. Across all 9 datasets and 4 model types, we see consistent performance boosts on ID and OOD data. 5/ https://t.co/78IfpbGqom', 'SSMBA is ready to use out of the box for any supervised NLP task with no additional fine-tuning. Check out the code here https://t.co/th47UxaxLT and give it a try in your next project! 6/6', ""@bricksdont @kchonyc @MarzyehGhassemi ah good catch! I'll get that fixed for the camera ready...""]",https://arxiv.org/abs/2009.10195,"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English. ","SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving
  Out-of-Domain Robustness"
209,1308800639523155970,1127801884830396416,Nick Choksi,"['Meteorites are filled to the brim with enigmatic mm-sized igneous rocks (“chondrules”).To quote @vorlon “chondrules would not be predicted to exist if they did not exist”. In my latest, we propose a model for their formation in planetesimal collisions <LINK> 1/', 'And to quote @vorlon again: “one of the best ways to make friends within meteoritics, and one of the best ways to make enemies in meteoritics, is to propose a new mechanism for the formation of chondrules.” So this should be interesting… 2/2']",https://arxiv.org/abs/2009.10093,"We assess whether chondrules, once-molten mm-sized spheres filling the oldest meteorites, could have formed from super-km/s collisions between planetesimals in the solar nebula. High-velocity collisions release hot and dense clouds of silicate vapor which entrain and heat chondrule precursors. Thermal histories of CB chondrules are reproduced for colliding bodies $\sim$10--100 km in radius. The slower cooling rates of non-CB, porphyritic chondrules point to colliders with radii $\gtrsim$ 500 km. How chondrules, collisionally dispersed into the nebula, agglomerated into meteorite parent bodies remains a mystery. The same orbital eccentricities and inclinations that enable energetic collisions prevent planetesimals from re-accreting chondrules efficiently and without damage; thus the sedimentary laminations of the CB/CH chondrite Isheyevo are hard to explain by direct fallback of collisional ejecta. At the same time, planetesimal surfaces may be littered with the shattered remains of chondrules. The micron-sized igneous particles recovered from comet 81P/Wild-2 may have originated from in-situ collisions and subsequent accretion in the proto-Kuiper belt, obviating the need to transport igneous solids across the nebula. Asteroid sample returns from Hayabusa2 and OSIRIS-REx may similarly contain chondrule fragments. ","Chondrules from high-velocity collisions: thermal histories and the
  agglomeration problem"
210,1308492596285779969,1140066312380567553,Bryan Wilder,"[""I'm excited to share a project long in the making. We designed algorithms to find influential nodes in social networks, applied to HIV prevention for homeless youth. A trial with 713 youth over 2 years showed significant benefits. Paper just posted, <LINK> (1/9)"", 'Homeless youth have up to 10x HIV prevalence vs general population. One intervention is to recruit peer leaders from the youth to promote protective behaviors. But how to choose the most influential peer leaders? (2/9)', 'There\'s tons of computer science work on finding influential nodes in a social network (""influence maximization""). But, mostly targeted at advertising/online social networks...not easily applicable to community health. (3/9)', ""What are the new challenges? In a word, data. Who's connected to who? How will information diffuse? None of this is known. Gathering network structure = time consuming, face to face interviews with youth. (4/9)"", 'We developed algorithms to efficiently subsample the network, only requiring about 20% of the effort in data collection. Then, we designed a robust optimization algorithm to identify influential nodes even under uncertainty. (5/9)', 'It worked in simulation but what about reality? We ran a clinical trial at centers for homeless youth in LA. Trial compared three arms: interventions with our algorithm, selecting highest-degree youth (standard baseline), and no intervention. 713 youth total over 2 years. (6/9)', 'The results just out: in the algorithm arm, statistically significant reduction in key outcome, condomless anal sex (OR = 0.69). No significant change for the other arms. AI helped! (7/9)', 'Key takeaways in the paper (https://t.co/yNK15YDmJ5): simple, robust, data-efficient algorithms are critical for public health domains. Beyond the algorithm though, always requires community trust. (8/9)', 'It was truly amazing to work with this close-knit team of social work/AI researchers: @MilindTambe_AI, @EricRicePhD, @onasch_vera, Graham Diguiseppe, @AmulyaYadav19 and many more at @CAIS_USC and @HCRCS. (9/9)']",https://arxiv.org/abs/2009.09559,"Youth experiencing homelessness (YEH) are subject to substantially greater risk of HIV infection, compounded both by their lack of access to stable housing and the disproportionate representation of youth of marginalized racial, ethnic, and gender identity groups among YEH. A key goal for health equity is to improve adoption of protective behaviors in this population. One promising strategy for intervention is to recruit peer leaders from the population of YEH to promote behaviors such as condom usage and regular HIV testing to their social contacts. This raises a computational question: which youth should be selected as peer leaders to maximize the overall impact of the intervention? We developed an artificial intelligence system to optimize such social network interventions in a community health setting. We conducted a clinical trial enrolling 713 YEH at drop-in centers in a large US city. The clinical trial compared interventions planned with the algorithm to those where the highest-degree nodes in the youths' social network were recruited as peer leaders (the standard method in public health) and to an observation-only control group. Results from the clinical trial show that youth in the AI group experience statistically significant reductions in key risk behaviors for HIV transmission, while those in the other groups do not. This provides, to our knowledge, the first empirical validation of the usage of AI methods to optimize social network interventions for health. We conclude by discussing lessons learned over the course of the project which may inform future attempts to use AI in community-level interventions. ","Clinical trial of an AI-augmented intervention for HIV prevention in
  youth experiencing homelessness"
211,1306036621301682178,762294664116535296,Jason Lee,"['New work: ""Iterative Refinement in the Continuous Space\nfor Non-Autoregressive Neural Machine Translation"" w/ @raphaelshu @kchonyc \n\n<LINK>\n\nWe propose an inference procedure for non-autoregressive NMT that refines translations *purely in the continuous space.*', 'Given a latent variable NMT model, we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable (and the source sentence) as input. (1/n)', 'At inference time, we (1) initialize the latent variable, e.g. as the mean of the prior, and (2) follow the gradients estimated by the inference network.\n\nInference is efficient as each refinement step only involves computation in the continuous space. (2/n)', 'Compared to our baseline EM-like inference procedure that refines in both token/continuous spaces (Shu et al., 2020), the proposed approach (1) is twice as fast and (2) results in higher BLEU scores and marginal probabilities (3/n)', 'Visualization of the gradients and the optimization trajectory reveals that local optima estimated by the learned inference network do not necessarily coincide with the approximate posterior mean (4/n) https://t.co/5O4TZx8mCz', 'Refining in the continuous space (by following the estimated gradients) results in non-trivial, non-local revisions to the original sentence. (5/n) https://t.co/yArkvKaLV9', 'Summary: refining in the continuous space works for discrete output also! 😃\n\naccepted to EMNLP 2020 (n/n)']",https://arxiv.org/abs/2009.07177,"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT'14 En-De, WMT'16 Ro-En and IWSLT'16 De-En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT'14 En-De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU). ","Iterative Refinement in the Continuous Space for Non-Autoregressive
  Neural Machine Translation"
212,1305450552336228352,77712285,Andrea Baronchelli,"['When do neutral markers (hair style, slang, etc.) result in new groups and associated norms (heavy metal ppl, cool ppl at school, etc)? \n\nWe explore how markers affect coordination in a model based on reinf. learning, and find a rich landscape of answers.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2009.05354,"Observable social traits determine how we interact in society and remain pervasive even in our globalized world. While a popular hypothesis states that they may help promote cooperation, the alternative explanation that they facilitate coordination has gained ground in recent years. Here we explore this framework and present a model that investigates the role of ethnic markers in coordination games. We consider fixed markers characterizing agents that use reinforcement learning to update their strategies in the game. For a wide range of parameters, we observe the emergence of a collective equilibrium in which markers play an assorting role. However, if individuals are too conformists or greedy, markers fail to shape social interactions. These results extend and complement previous work focused on agent imitation and show that reinforcement learning is a good candidate to explain many instances of ethnic markers. ","The emergence of segregation: from observable markers to group specific
  norms"
213,1303332260318457857,68538286,Dan Hendrycks,"['How multipurpose is #GPT3? We gave it questions about elementary math, history, law, and more. We found that GPT-3 is now better than random chance across many tasks, but for all 57 tasks it still has wide room for improvement.\n\n<LINK>\n<LINK> <LINK>', '@colinraffel UnifiedQA-3b worked better on average. For example, on PIQA AllenAI uses 3b on the public leaderboard.']",https://arxiv.org/abs/2009.03300,"We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings. ",Measuring Massive Multitask Language Understanding
214,1303151638522277894,1012125662117851136,Edward Kennedy,"['So excited to share new work w/star @CMU_Stats PhD student Alan Mishler (<LINK>):\n\n<LINK>\n\nWe show how to post-process arbitrary risk predictors so they satisfy more meaningful *counterfactual* fairness constraints, &amp; study statistical properties <LINK>']",https://arxiv.org/abs/2009.02841,"In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed. The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention. However, no methods currently exist to satisfy such fairness criteria. In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous post-processing approaches to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data. ","Fairness in Risk Assessment Instruments: Post-Processing to Achieve
  Counterfactual Equalized Odds"
