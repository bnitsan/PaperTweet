,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1423755203451392003,101171811,LIGO Hanford,"[""Have had a couple of new papers released over the last week.\nHere's a @LIGO | @ego_virgo | @KAGRA_PR paper on an all-sky search for Long-Duration #GravitationalWave bursts from O3 (Observing Run #3)!\nPaper: <LINK>\nScience Summary: <LINK> <LINK>""]",https://arxiv.org/abs/2107.13796,"After the detection of gravitational waves from compact binary coalescences, the search for transient gravitational-wave signals with less well-defined waveforms for which matched filtering is not well-suited is one of the frontiers for gravitational-wave astronomy. Broadly classified into ""short"" $ \lesssim 1~$\,s and ""long"" $ \gtrsim 1~$\,s duration signals, these signals are expected from a variety of astrophysical processes, including non-axisymmetric deformations in magnetars or eccentric binary black hole coalescences. In this work, we present a search for long-duration gravitational-wave transients from Advanced LIGO and Advanced Virgo's third observing run from April 2019 to March 2020. For this search, we use minimal assumptions for the sky location, event time, waveform morphology, and duration of the source. The search covers the range of $2~\text{--}~ 500$~s in duration and a frequency band of $24 - 2048$ Hz. We find no significant triggers within this parameter space; we report sensitivity limits on the signal strength of gravitational waves characterized by the root-sum-square amplitude $h_{\mathrm{rss}}$ as a function of waveform morphology. These $h_{\mathrm{rss}}$ limits improve upon the results from the second observing run by an average factor of 1.8. ","All-sky search for long-duration gravitational-wave bursts in the third
  Advanced LIGO and Advanced Virgo run"
1,1423297428351070218,4796888732,Glen Berseth,['New work on fully autonomous real-world robotic learning with mobile manipulators. One of the exciting parts of this work is how we can enable robots to train in the real world without any human support or resets.\n\nwebsite: <LINK>\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2107.13545,"We study how robots can autonomously learn skills that require a combination of navigation and grasping. While reinforcement learning in principle provides for automated robotic skill learning, in practice reinforcement learning in the real world is challenging and often requires extensive instrumentation and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Our proposed system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where manipulation policy uncertainty drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically, in around 40 hours of autonomous real-world training. ","Fully Autonomous Real-World Reinforcement Learning with Applications to
  Mobile Manipulation"
2,1422833898493513730,214530054,Umberto Lupo,"['New paper: <LINK>\nand repository:\n<LINK>\n\nWe build on the best ideas in the literature for speeding up the computation of Vietoris-Rips barcodes, and provide a Python interface that will feel familiar if you have used @scikit_tda and #giotto_tda. <LINK>', 'Massive thanks to my HPC collaborators at HEIG-VD, who did most of the work.\n\nAll feedback welcome!']",https://arxiv.org/abs/2107.05412,"We introduce giotto-ph, a high-performance, open-source software package for the computation of Vietoris-Rips barcodes. giotto-ph is based on Morozov and Nigmetov's lockfree (multicore) implementation of Ulrich Bauer's Ripser package. It also contains a re-working of the GUDHI library's implementation of Boissonnat and Pritam's Edge Collapser, which can be used as a pre-processing step to dramatically reduce overall run-times in certain scenarios. Our contribution is twofold: on the one hand, we integrate existing state-of-the-art ideas coherently in a single library and provide Python bindings to the C++ code. On the other hand, we increase parallelization opportunities and improve overall performance by adopting more efficient data structures. Our persistent homology backend establishes a new state of the art, surpassing even GPU-accelerated implementations such as Ripser++ when using as few as 5-10 CPU cores. Furthermore, our implementation of Edge Collapser has fewer software dependencies and improved run-times relative to GUDHI's original implementation. ","giotto-ph: A Python Library for High-Performance Computation of
  Persistent Homology of Vietoris-Rips Filtrations"
3,1422751230464516096,1214215979200172033,Leonard Wong,"['New paper with Jun Zhang @UMich. We show that Rényi entropy and divergence and the q-exponential family are naturally compatible with a generalized convex duality. It also comes with a logarithmic divergence which generalizes the Bregman divergence.\n\n<LINK>', 'See https://t.co/8zWm5GWEpt by @brekelmaniac who nicely summarizes some of the results.']",https://arxiv.org/abs/2107.11925,"Tsallis and R\'{e}nyi entropies, which are monotone transformations of each other, are deformations of the celebrated Shannon entropy. Maximization of these deformed entropies, under suitable constraints, leads to the $q$-exponential family which has applications in non-extensive statistical physics, information theory and statistics. In previous information-geometric studies, the $q$-exponential family was analyzed using classical convex duality and Bregman divergence. In this paper, we show that a generalized $\lambda$-duality, where $\lambda = 1 - q$ is the constant information-geometric curvature, leads to a generalized exponential family which is essentially equivalent to the $q$-exponential family and has deep connections with R\'{e}nyi entropy and optimal transport. Using this generalized convex duality and its associated logarithmic divergence, we show that our $\lambda$-exponential family satisfies properties that parallel and generalize those of the exponential family. Under our framework, the R\'{e}nyi entropy and divergence arise naturally, and we give a new proof of the Tsallis/R\'{e}nyi entropy maximizing property of the $q$-exponential family. We also introduce a $\lambda$-mixture family which may be regarded as the dual of the $\lambda$-exponential family, and connect it with other mixture-type families. Finally, we discuss a duality between the $\lambda$-exponential family and the $\lambda$-logarithmic divergence, and study its statistical consequences. ",Tsallis and R\'{e}nyi deformations linked via a new $\lambda$-duality
4,1422598093682450436,4334246657,Drew Jaegle,"['Perceiver IO: a new &amp; improved Perceiver for all of your input and output needs.\n\nCheck out the paper (<LINK>) &amp; code (<LINK>) for more on masked &amp; multi-task language modeling, optical flow, ImageNet, StarCraft, video+audio+label autoencoding! <LINK>']",https://arxiv.org/abs/2107.14795,"A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain & task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence. ",Perceiver IO: A General Architecture for Structured Inputs & Outputs
5,1422253017642856448,793288660040896512,Shengjia Zhao,"['🚨[New Paper] Should you trust calibrated predictions for high-stakes decisions? Check out <LINK> to see what calibration actually means for decision-making, and a new decision-tailored calibration notion. With @mikekimbackward, Roshni, @tengyuma, @StefanoErmon <LINK>']",https://arxiv.org/abs/2107.05719,"When facing uncertainty, decision-makers want predictions they can trust. A machine learning provider can convey confidence to decision-makers by guaranteeing their predictions are distribution calibrated -- amongst the inputs that receive a predicted class probabilities vector $q$, the actual distribution over classes is $q$. For multi-class prediction problems, however, achieving distribution calibration tends to be infeasible, requiring sample complexity exponential in the number of classes $C$. In this work, we introduce a new notion -- \emph{decision calibration} -- that requires the predicted distribution and true distribution to be ``indistinguishable'' to a set of downstream decision-makers. When all possible decision makers are under consideration, decision calibration is the same as distribution calibration. However, when we only consider decision makers choosing between a bounded number of actions (e.g. polynomial in $C$), our main result shows that decisions calibration becomes feasible -- we design a recalibration algorithm that requires sample complexity polynomial in the number of actions and the number of classes. We validate our recalibration algorithm empirically: compared to existing methods, decision calibration improves decision-making on skin lesion and ImageNet classification with modern neural network predictors. ","Calibrating Predictions to Decisions: A Novel Approach to Multi-Class
  Calibration"
6,1422223542872125448,891833750643933184,Jen-hwa Chu,['<LINK> New paper with @kaneplusplus and new PhD graduate Wenlan Zang from @ysphbiostat : A Scalable Approach to Estimating the Rank of High-Dimensional Data'],https://arxiv.org/abs/2107.14426,"A key challenge to performing effective analyses of high-dimensional data is finding a signal-rich, low-dimensional representation. For linear subspaces, this is generally performed by decomposing a design matrix (via eigenvalue or singular value decomposition) into orthogonal components, and then retaining those components with sufficient variations. This is equivalent to estimating the rank of the matrix and deciding which components to retain is generally carried out using heuristic or ad-hoc approaches such as plotting the decreasing sequence of the eigenvalues and looking for the ""elbow"" in the plot. While these approaches have been shown to be effective, a poorly calibrated or misjudged elbow location can result in an overabundance of noise or an under-abundance of signal in the low-dimensional representation, making subsequent modeling difficult. In this article, we propose a latent-space-construction procedure to estimate the rank of the detectable signal space of a matrix by retaining components whose variations are significantly greater than random matrices, of which eigenvalues follow a universal March\u{e}nko-Pastur (MP) distribution. ",A Scalable Approach to Estimating the Rank of High-Dimensional Data
7,1422223254090027011,384900803,Shantanu Basu,"['New paper from our research group, authored with three of my graduate students: Bino, Das, and Sharkawi. A new method to calculate hourglass magnetic fields, that are inherent to #starformation #astronomy @westernuPhysAst @westernuMath @westernuScience <LINK> <LINK>']",https://arxiv.org/abs/2107.14679,"Modelling the magnetic field in prestellar cores can serve as a useful tool for studying the initial conditions of star formation. The analytic hourglass model of Ewertowski and Basu (2013) provides a means to fit observed polarimetry measurements and extract useful information. The original model does not specify any radial distribution of the electric current density. Here, we perform a survey of possible centrally-peaked radial distributions of the current density, and numerically derive the full hourglass patterns. Since the vertical distribution is also specified in the original model, we can study the effect of different ratios of vertical to radial scale length on the overall hourglass pattern. Different values of this ratio may correspond to different formation scenarios for prestellar cores. We demonstrate the flexibility of our model and how it can be applied to a variety of magnetic field patterns. ",Hourglass Magnetic Field from a Survey of Current Density Profiles
8,1422202800050683915,2377341628,Patrick Diehl,['New preprint of our short paper; Peridynamics for Quasistatic Fracture Modeling\n\n<LINK>\n\nA preprint with the numerical details will follow with the next week. A theory paper is in preparation.\n\n#peridynamic #Engineering #simulation <LINK>'],https://arxiv.org/abs/2107.14665,"Fracture involves interaction across large and small length scales. With the application of enough stress or strain to a brittle material, atomistic scale bonds will break, leading to fracture of the macroscopic specimen. From the perspective of mechanics fracture should appear as an emergent phenomena generated by a continuum field theory eliminating the need for a supplemental kinetic relation describing crack growth. We develop a new fast method for modeling quasi-static fracture using peridynamics. We apply fixed point theory and model stable crack evolution for hard and soft loading. For soft loading we recover unstable fracture. For hard loading we recover stable crack growth. We show existence of quasistatic fracture solutions in the neighborhood of stable critical points for appropriately defined energies. The numerical method uses an analytic stiffness matrix for fast numerical implementation. A rigorous mathematical analysis shows that the method converges for load paths associated with soft and hard loading. For soft loading the crack becomes unstable shortly after the stress at the tip of the pre-crack reaches the material strength. ",Peridynamics for Quasistatic Fracture Modeling
9,1422157614306234368,192826908,Jorge Lillo-Box,"['Paper day! The @KeplerGO mission still hides many interesting discoveries. We have confirmed new planets detected with Kepler that are NOT transiting... how can that be? 👇\n<LINK>', 'In Millholland &amp; Laughlin (2017), my paper mates discovered a bunch of light curve modulations that could potentially be attributed to planets inducing three effects: 1) Reflexion of stellar light, 2) ellipsoidal modulations (due to tides), and 3) Doppler Beaming https://t.co/1IiDm3Vfg6', 'We used CAFE spectrograph from @CalarAltoObs and HERMES from Mercator Telescope @OOCC_IAC to confirm three of these planets with radial velocity and validate other three. https://t.co/ASrO5BFagi', 'We also used AstraLux high-spatial resolution camera to discard possible close contaminant sources. https://t.co/9UACFXLu7r', 'I would like to thank my two paper mates Sarah Millholland and Greg Laughlin for their help and support in developing this work. This new planet detection technique is very promising for future missions like @ESA_Plato and @ArielTelescope !!']",https://arxiv.org/abs/2107.14621,"The direct detection of new extrasolar planets from high-precision photometry data is commonly based on the observation of the transit signal of the planet as it passes in front of its star. Close-in planets, however, leave additional imprints in the light curve even if they do not transit. These are the so-called phase curve variations that include ellipsoidal, reflection and beaming effects. In Millholland & Laughlin (2017), the authors scrutinized the Kepler database looking for these phase variations from non-transiting planets. They found 60 candidates whose signals were compatible with planetary companions. In this paper, we perform a ground-based follow-up of a sub-sample of these systems with the aim of confirming and characterizing these planets and thus validating the detection technique. We used the CAFE and HERMES instruments to monitor the radial velocity of ten non-transiting planet candidates along their orbits. We additionally used AstraLux to obtain high-resolution images of some of these candidates to discard blended binaries that contaminate the Kepler light curves by mimicking planetary signals. Among the ten systems, we confirm three new hot-Jupiters (KIC8121913 b, KIC10068024 b, and KIC5479689 b) with masses in the range 0.5-2 M$_{\rm Jup}$ and set mass constraints within the planetary regime for the other three candidates (KIC8026887b, KIC5878307 b, and KIC11362225 b), thus strongly suggestive of their planetary nature. For the first time, we validate the technique of detecting non-transiting planets via their phase curve variations. We present the new planetary systems and their properties. We find good agreement between the RV-derived masses and the photometric masses in all cases except KIC8121913 b, which shows a significantly lower mass derived from the ellipsoidal modulations than from beaming and radial velocity data. ","Follow-up of non-transiting planets detected by Kepler. Confirmation of
  three hot-Jupiters and validation of three other planets"
10,1422137431093137411,944291984675614721,Tobias de Jong,"['New paper on #moire pattern deformation in #twisted bilayer #graphene using Low Energy Electron Microscopy now on @arxiv: <LINK> 😁😁 <LINK>', 'Contrary to our earlier efforts https://t.co/K9tKHUAitf, it turns out aberration corrected LEEM *can* directly image the moiré pattern and it\'s deformations. We ""only"" need to push the microscope (very close to) the #worldrecord resolution. https://t.co/b0ixgAest7', 'Using Geometric Phase Analysis (implemented in #python https://t.co/o6f7tlPNKc WIP), we extract local twist angle and local strain of the graphene layers from the deformation in the moiré pattern. https://t.co/7qMYugGf2T', 'What is more, we can track deformations over time, and show the moiré pattern actually moves at a timescale of seconds (at 500C). The cool thing for me personally is that these movements corresponds to atomic displacements of about 70 picometer😎 (picometers observed in a LEEM!) https://t.co/vkv77oW3qS', 'As a final observation, we see stable atomic edge dislocations in the moiré lattice, a #topological defect which could have interesting properties. https://t.co/KzAwpqhs2C']",https://arxiv.org/abs/2107.14716,"In twisted bilayer graphene (TBG) a moir\'e pattern forms that introduces a new length scale to the material. At the 'magic' twist angle of 1.1{\deg}, this causes a flat band to form, yielding emergent properties such as correlated insulator behavior and superconductivity [1-4]. In general, the moir\'e structure in TBG varies spatially, influencing the local electronic properties [5-9] and hence the outcome of macroscopic charge transport experiments. In particular, to understand the wide variety observed in the phase diagrams and critical temperatures, a more detailed understanding of the local moir\'e variation is needed [10]. Here, we study spatial and temporal variations of the moir\'e pattern in TBG using aberration-corrected Low Energy Electron Microscopy (AC-LEEM) [11,12]. The spatial variation we find is lower than reported previously. At 500{\deg}C, we observe thermal fluctuations of the moir\'e lattice, corresponding to collective atomic displacements of less than 70pm on a time scale of seconds [13], homogenizing the sample. Despite previous concerns, no untwisting of the layers is found, even at temperatures as high as 600{\deg}C [14,15]. From these observations, we conclude that thermal annealing can be used to decrease the local disorder in TBG samples. Finally, we report the existence of individual edge dislocations in the atomic and moir\'e lattice. These topological defects break translation symmetry and are anticipated to exhibit unique local electronic properties. ",Imaging moir\'e deformation and dynamics in twisted bilayer graphene
11,1421914523842269197,13000042,Kenny Smith 🌌🐀,"['Sidestepping Newton’s first law, quantum computers solving problems and technologies we don’t have yet, launching an altogether new age. \n\nIf this holds up, and if it is replicable ... \n\nGoogle’s time stone paper is in preprint: <LINK> <LINK>']",https://arxiv.org/abs/2107.13571,"Quantum many-body systems display rich phase structure in their low-temperature equilibrium states. However, much of nature is not in thermal equilibrium. Remarkably, it was recently predicted that out-of-equilibrium systems can exhibit novel dynamical phases that may otherwise be forbidden by equilibrium thermodynamics, a paradigmatic example being the discrete time crystal (DTC). Concretely, dynamical phases can be defined in periodically driven many-body localized systems via the concept of eigenstate order. In eigenstate-ordered phases, the entire many-body spectrum exhibits quantum correlations and long-range order, with characteristic signatures in late-time dynamics from all initial states. It is, however, challenging to experimentally distinguish such stable phases from transient phenomena, wherein few select states can mask typical behavior. Here we implement a continuous family of tunable CPHASE gates on an array of superconducting qubits to experimentally observe an eigenstate-ordered DTC. We demonstrate the characteristic spatiotemporal response of a DTC for generic initial states. Our work employs a time-reversal protocol that discriminates external decoherence from intrinsic thermalization, and leverages quantum typicality to circumvent the exponential cost of densely sampling the eigenspectrum. In addition, we locate the phase transition out of the DTC with an experimental finite-size analysis. These results establish a scalable approach to study non-equilibrium phases of matter on current quantum processors. ",Observation of Time-Crystalline Eigenstate Order on a Quantum Processor
12,1421235564502601734,2905792802,Xiaoman Luo,['Excited to announce our new paper! <LINK> <LINK>'],https://arxiv.org/abs/2107.13737,"We propose a new estimator for the average causal effects of a binary treatment with panel data in settings with general treatment patterns. Our approach augments the two-way-fixed-effects specification with the unit-specific weights that arise from a model for the assignment mechanism. We show how to construct these weights in various settings, including situations where units opt into the treatment sequentially. The resulting estimator converges to an average (over units and time) treatment effect under the correct specification of the assignment model. We show that our estimator is more robust than the conventional two-way estimator: it remains consistent if either the assignment mechanism or the two-way regression model is correctly specified and performs better than the two-way-fixed-effect estimator if both are locally misspecified. This strong double robustness property quantifies the benefits from modeling the assignment process and motivates using our estimator in practice. ",Double-Robust Two-Way-Fixed-Effects Regression For Panel Data
13,1421234894462590979,928301283034914817,Lihua Lei,"['🚨New working paper🚨w/ @ArkhangelskyD, Guido Imbens, and @xiaoman_luo. We propose the RIPW estimator that makes TWFE regression robust to treatment effect heterogeneity under general designs (incl. staggered adoption)\n\n<LINK>\n\nI’ll walk through some details. 1/n <LINK>', 'Recently, it is found that the TWFE estimator is not robust to effect heterogeneity (over units and time).\n\nWe show that if the assignment model is known, our RIPW estimator—a weighted TWFE estimator—resolves this issue in the static case w/o any version of parallel trend.\n\n2/n', 'RIPW estimator reweighs the TWFE objective by the inversed (generalized) propensity weights, modified by a *reshaped distribution* Π on the support of assignments.\n\nIt is tempting to set Π uniform, as in the cross-sectional IPW. Surprisingly, it *doesn’t work* in general.\n\n3/n https://t.co/XvSf64bbjq', 'First, we need to define what we mean by “work”. Our estimand is the DATE (doubly average treatment effect), an average of unit-time specific effects with *user-specified* weights, instead of just a convex combination. It is in a similar spirit to the cross-sectional ATE. \n\n4/n https://t.co/IJHzavW3rc', 'We show that the RIPW estimator converges to DATE if the reshaped distribution satisfies the *DATE equation* -- a “mysterious” quadratic system that only depends on the design (types of treatment assignments) and the weights for DATE.\n\n5/n https://t.co/3E27bquXm0', 'The DATE equation has closed-form solutions for many designs. The most intriguing one is for *staggered adoption* designs where assignments are of form (0, .., 0, 1, .., 1). We show that the uniform dist. is *not* a solution of the DATE equation for equally-weighted DATE. \n\n6/n', 'For example, if there are three periods, the solutions of DATE equation are given by:\n\nΠ(0, 0, 0), Π(0, 0, 1), Π(0, 1, 1), Π(1, 1, 1)\n= a convex combination of (2/9,1/3,0,4/9) &amp; (4/9,0,1/3,2/9).\n\nMagical numbers! Simulation results confirm that they are derived correctly.\n\n7/n https://t.co/rdug4Kjyiu', 'When the generalized propensity scores are known (e.g., RCT), we prove that the RIPW estimator is consistent for DATE, \n\n(1) w/o any version of parallel trends,\n\n(2) w/ unrestricted effect heterogeneity over both units and time.\n\nThe result is purely design-based.\n\n8/n', 'We further show that the RIPW estimator is double-robust: it is consistent for DATE if *either* the TWFE model *or* the assignment model is approximately correct. \n\nWe also provide double-robust confidence intervals under mild additional assumptions! \n\n9/n', 'The double robustness is surprising. \n\nFor cross-sectional data, DR requires a consistent estimate of E[PO|X] when the outcome model is correct. \n\nHowever, for panels with a bounded number of periods, it is impossible because the unit FEs can’t be consistently estimated.\n\n10/n', 'This has strong implications that AIPW-type estimators fail to be double-robust in this case.\n\nBy contrast, our RIPW estimator is a better approach to handle *incidental parameter* problems caused by the large number of FEs. \n\n11/n https://t.co/fLToy8iJ4b', 'In practice, we often have knowledge on the assignment process. For example, the policy adoption time can be modeled via duration models. \n\nWe reanalyzed the data in @Andrew___Baker’s wonderful blog, using the Cox model for the time the law is passed\nhttps://t.co/u8z3RAphjR\n\n12/n', 'We present another example analyzing the short-term effect of state of emergency on the dine-in rate during the early COVID-19 pandemic in the U.S., using the daily data released by OpenTable. \nThis is a case where assignments are (arguably) easier to model than outcomes.\n\n13/n', 'Summary:\n\nTheoretically, the RIPW estimator is robust to unrestricted effect heterogeneity.\n\nPractically, the RIPW estimator can leverage the additional information from the assignment process.\n\nPaper: https://t.co/HO6x9uRxq9\nCode: https://t.co/jmFBYmtqy1\n\n14/n (n=14)']",https://arxiv.org/abs/2107.13737,"We propose a new estimator for the average causal effects of a binary treatment with panel data in settings with general treatment patterns. Our approach augments the two-way-fixed-effects specification with the unit-specific weights that arise from a model for the assignment mechanism. We show how to construct these weights in various settings, including situations where units opt into the treatment sequentially. The resulting estimator converges to an average (over units and time) treatment effect under the correct specification of the assignment model. We show that our estimator is more robust than the conventional two-way estimator: it remains consistent if either the assignment mechanism or the two-way regression model is correctly specified and performs better than the two-way-fixed-effect estimator if both are locally misspecified. This strong double robustness property quantifies the benefits from modeling the assignment process and motivates using our estimator in practice. ",Double-Robust Two-Way-Fixed-Effects Regression For Panel Data
14,1421185613567381504,1865522274,Dmitry Arkhangelsky,"['1/5  (Reweighted) two-way regression strikes back in our new paper with Guido Imbens, @lihua_lei_stat, and Xiaoman Luo (<LINK>)! Can (and should) be used whenever data allows us to meaningfully discuss the model for the treatment assignment.', '2/5 The two-way regression model is a good approximation (captures a lot of variation) but has undesirable properties whenever treatment varies substantially over units and time. However, precisely this variation provides information about the assignment process.', '3/5 We show how to use this information to augment the two-way regression model with unit-specific weights.  These are constructed using the assignment process but are not equal to the standard inverse propensities.', '4/5 The resulting estimator does not suffer from the bad properties of TWFE and converges to the average treatment effect (over units and time). Limitation: do not allow for dynamic treatment effects but have unrestricted heterogeneity in contemporaneous ones.', 'If the assignment model is correctly specified (and estimable), then the estimator is consistent without any version of parallel trends. More generally, the estimator is double-robust: close to ATE if either the two-way model or the assignment model is close to the truth.']",https://arxiv.org/abs/2107.13737,"We propose a new estimator for the average causal effects of a binary treatment with panel data in settings with general treatment patterns. Our approach augments the two-way-fixed-effects specification with the unit-specific weights that arise from a model for the assignment mechanism. We show how to construct these weights in various settings, including situations where units opt into the treatment sequentially. The resulting estimator converges to an average (over units and time) treatment effect under the correct specification of the assignment model. We show that our estimator is more robust than the conventional two-way estimator: it remains consistent if either the assignment mechanism or the two-way regression model is correctly specified and performs better than the two-way-fixed-effect estimator if both are locally misspecified. This strong double robustness property quantifies the benefits from modeling the assignment process and motivates using our estimator in practice. ",Double-Robust Two-Way-Fixed-Effects Regression For Panel Data
15,1421086670401810433,3077991249,Stephan Mandt,"['New compression paper: in ""Insights from Generative Modeling for Neural Video Compression"" (<LINK>), we view current neural video compression architectures through the lens of generative modeling. (1/5)', 'Our main findings: (i) many recently proposed neural video coding architectures are instances of a ""master model"" that parameterizes autoregressive transforms with additional latent variables. (2/5)', '(ii) The generalized view makes it simple to improve current architectures. e.g., we can add a scale variable that acts as a gate between the next-frame prediction and the additive residual noise in analogy to how RealNVP improves over NICE. (3/5)', '(iii) Results can be furthermore improved by introducing structured priors that capture dependencies between optical flow fields and the residual noise and/or temporal priors. (4/5)', '(iv) Additional contributions include variable bitrate versions of the models, various ablations, and a general discussion on the suitability of flows for compression. Joint work with Ruihan Yang, Yibo Yang, and @jl_marino. (5/5)']",https://arxiv.org/abs/2107.13136,"While recent machine learning research has revealed connections between deep generative models such as VAEs and rate-distortion losses used in learned compression, most of this work has focused on images. In a similar spirit, we view recently proposed neural video coding algorithms through the lens of deep autoregressive and latent variable modeling. We present recent neural video codecs as instances of a generalized stochastic temporal autoregressive transform, and propose new avenues for further improvements inspired by normalizing flows and structured priors. We propose several architectures that yield state-of-the-art video compression performance on full-resolution video and discuss their tradeoffs and ablations. In particular, we propose (i) improved temporal autoregressive transforms, (ii) improved entropy models with structured and temporal dependencies, and (iii) variable bitrate versions of our algorithms. Since our improvements are compatible with a large class of existing models, we provide further evidence that the generative modeling viewpoint can advance the neural video coding field. ",Insights from Generative Modeling for Neural Video Compression
16,1421044847826325508,1169535083579092992,Ana Martin,"['We have a new member on the DAQC family 🥳. In this last paper we study the effect of noise sources in digital and digital-analog quantum computing and compare the performance of both paradigms. Thanks to @Paula_G_Phys, @qmisanz for this great teamwork 👏🏼\n\n<LINK>']",http://arxiv.org/abs/2107.12969,"Quantum computing makes use of quantum resources provided by the underlying quantum nature of matter to enhance classical computation. However, current Noisy Intermediate-Scale Quantum (NISQ) era in quantum computing is characterized by the use of quantum processors comprising from a few tens to, at most, few hundreds of physical qubits without implementing quantum error correction techniques. This limits the scalability in the implementation of quantum algorithms. Digital-analog quantum computing (DAQC) has been proposed as a more resilient alternative quantum computing paradigm to outperform digital quantum computation within the NISQ era framework. It arises from adding the flexibility provided by fast single-qubit gates to the robustness of analog quantum simulations. Here, we perform a careful comparison between digital and digital-analog paradigms under the presence of noise sources. The comparison is illustrated by comparing the performance of the quantum Fourier transform algorithm under a wide range of single- and two-qubit noise sources. Indeed, we obtain that, when the different noise channels usually present in superconducting quantum processors are considered, the fidelity of the QFT algorithm for the digital-analog paradigm outperforms the one obtained for the digital approach. Additionally, this difference grows when the size of the processor scales up, constituting consequently a sensible alternative paradigm in the NISQ era. Finally, we show how the DAQC paradigm can be adapted to quantum error mitigation techniques for canceling different noise sources, including the bang error. ",Noise in Digital and Digital-Analog Quantum Computation
17,1421041868909985793,561899047,Aki Vehtari,"['New paper ""Detecting and diagnosing prior and likelihood sensitivity with power-scaling"" with Noa Kallioinen, @PaananenTopi, and @paulbuerkner <LINK>, with code <LINK> and priorsense R package <LINK> <LINK>', 'We diagnose how much the posterior changes when prior or likelihood is power-scaled, i.e., exponentiated with some alpha&gt;0. This corresponds to scaling how informative the prior or the likelihood is. https://t.co/c89fO7Vv8x', 'Using importance sampling, we can easily compute arbitrary quantities measuring how much the posterior changes. As the default choice, we use the symmetrised metric version of cumulative Jensen-Shannon divergence. https://t.co/Sb4AlRZTJ0', 'For some quantities like mean and variance, we can also derive local analytic sensitivity at alpha=1.', 'Ideally, the posterior is likelihood sensitive, i.e., the likelihood is informative. If the posterior is also prior sensitive, there might be a prior-likelihood conflict. If the posterior is only prior sensitive, there might non-informative likelihood. https://t.co/2DaaEq3G5U', 'We can provide the diagnostic for all parameters and interpretation of the diagnostic. All this is available in the priorsense R package. https://t.co/Y1F6fbx5xR', 'The presence of prior sensitivity or the absence of likelihood sensitivity is not always a problem. Sometimes the prior can be intentionally strongly informative, and then a prior sensitivity can be desired.', ""Power-scaling prior and likelihood sensitivity diagnostic is not perfect and can miss some things, but as it's easily automated it will help quickly catch many typical issues and thus has a natural role in the bigger Bayesian workflow."", ""This was Noa's first paper at Aalto, and I'm very happy he is staying with us for a few more years.""]",https://arxiv.org/abs/2107.14054,"Determining the sensitivity of the posterior to perturbations of the prior and likelihood is an important part of the Bayesian workflow. We introduce a practical and computationally efficient sensitivity analysis approach using importance sampling to estimate properties of posteriors resulting from power-scaling the prior or likelihood. On this basis, we suggest a diagnostic that can indicate the presence of prior-data conflict or likelihood noninformativity and discuss limitations to the power-scaling approach. The approach can be easily included in Bayesian workflows with minimal effort by the model builder and we present an implementation in our new R package \texttt{priorsense}. We further demonstrate the workflow on case studies of real data using models varying in complexity from simple linear models to Gaussian process models. ","Detecting and diagnosing prior and likelihood sensitivity with
  power-scaling"
18,1420999082236162049,1656175616,Daniel Lerch,"['Preprint of our new paper: ""Subsequent embedding in image steganalysis: Theoretical framework and practical applications"" on arXiv <LINK>\n#steganography, #steganalysis']",https://arxiv.org/abs/2107.13862,"Steganalysis is a collection of techniques used to detect whether secret information is embedded in a carrier using steganography. Most of the existing steganalytic methods are based on machine learning, which typically requires training a classifier with ""laboratory"" data. However, applying machine-learning classification to a new source of data is challenging, since there is typically a mismatch between the training and the testing sets. In addition, other sources of uncertainty affect the steganlytic process, including the mismatch between the targeted and the true steganographic algorithms, unknown parameters -- such as the message length -- and even having a mixture of several algorithms and parameters, which would constitute a realistic scenario. This paper presents subsequent embedding as a valuable strategy that can be incorporated into modern steganalysis. Although this solution has been applied in previous works, a theoretical basis for this strategy was missing. Here, we cover this research gap by introducing the ""directionality"" property of features with respect to data embedding. Once this strategy is sustained by a consistent theoretical framework, new practical applications are also described and tested against standard steganography, moving steganalysis closer to real-world conditions. ","Subsequent embedding in targeted image steganalysis: Theoretical
  framework and practical applications"
19,1420972752249360385,1015053310603284480,Stephen Kane,"['Congratulations to @michelle_hill63 for her incredible work in leading a new paper on the iota Draconis system, available here: <LINK>', 'iota Draconis is a bright (V~3) giant star that is known to host a planet in a highly eccentric orbit (e~0.7). We\'ve been following this star for over a decade with Lick and APF data. The system had a known RV trend which we\'ve been waiting to ""turn around"", which it finally did.', 'Furthermore, the star was observed by TESS and is an ideal asteroseismology target, and indeed signatures were detected. These allowed us to constrain the stellar radius and mass to 2% and 6%, respectively.', 'Using these revised stellar properties, we improved the parameters of the known planet and extracted the properties for the new companion, which has a mass of ~15.6 Jupiter masses and an orbital period of ~68 years.', 'As an added bonus, we analyzed the combined RV+Gaia/Hipparcos data which verified the orbital solution for the new companion. This system provides a fascinating case-study for planetary orbital evolution around evolved stars.', '@michelle_hill63 and I are especially grateful to Tiago Campante, @zhexingli, @Paul_Dalba, Timothy Brandt, Timothy White, @fringetracker, Keivan Stassun, BJ Fulton, and numerous others who contributed to this effort!']",https://arxiv.org/abs/2107.13583,"Giant stars as known exoplanet hosts are relatively rare due to the potential challenges in acquiring precision radial velocities and the small predicted transit depths. However, these giant host stars are also some of the brightest in the sky and so enable high signal-to-noise follow-up measurements. Here we report on new observations of the bright (V ~ 3.3) giant star $\iota$ Draconis ($\iota$ Dra), known to host a planet in a highly eccentric ~511 day period orbit. TESS observations of the star over 137 days reveal asteroseismic signatures, allowing us to constrain the stellar radius, mass, and age to ~2%, ~6%, and ~28%, respectively. We present the results of continued radial velocity monitoring of the star using the Automated Planet Finder over several orbits of the planet. We provide more precise planet parameters of the known planet and, through the combination of our radial velocity measurements with Hipparcos and Gaia astrometry, we discover an additional long-period companion with an orbital period of ~$68^{+60}_{-36}$ years. Mass predictions from our analysis place this sub-stellar companion on the border of the planet and brown dwarf regimes. The bright nature of the star combined with the revised orbital architecture of the system provides an opportunity to study planetary orbital dynamics that evolve as the star moves into the giant phase of its evolution. ","Asteroseismology of iota Draconis and Discovery of an Additional
  Long-Period Companion"
20,1420910060721512448,746440524052082688,Nicolas Delfosse,"['Check out our new paper on using soft information to improve quantum error correction with Chris Pattison, Michael Beverland, @themarcusps.\nThat was great to have Chris as an intern in our group last Fall!\n\n<LINK>', 'Why should you read this paper?\n1/ You want better quantum error correction.\n2/ Your measurement outcomes look continuous and not only +1 or -1 like in most quantum error correction papers.\n3/ You want to understand how to to optimize the measurement time of your physical qubits.', 'I am especially excited about our study of the optimal measurement time. Have a look at Figure 12.']",https://arxiv.org/abs/2107.13589,"The typical model for measurement noise in quantum error correction is to randomly flip the binary measurement outcome. In experiments, measurements yield much richer information - e.g., continuous current values, discrete photon counts - which is then mapped into binary outcomes by discarding some of this information. In this work, we consider methods to incorporate all of this richer information, typically called soft information, into the decoding of quantum error correction codes, and in particular the surface code. We describe how to modify both the Minimum Weight Perfect Matching and Union-Find decoders to leverage soft information, and demonstrate these soft decoders outperform the standard (hard) decoders that can only access the binary measurement outcomes. Moreover, we observe that the soft decoder achieves a threshold 25\% higher than any hard decoder for phenomenological noise with Gaussian soft measurement outcomes. We also introduce a soft measurement error model with amplitude damping, in which measurement time leads to a trade-off between measurement resolution and additional disturbance of the qubits. Under this model we observe that the performance of the surface code is very sensitive to the choice of the measurement time - for a distance-19 surface code, a five-fold increase in measurement time can lead to a thousand-fold increase in logical error rate. Moreover, the measurement time that minimizes the physical error rate is distinct from the one that minimizes the logical performance, pointing to the benefits of jointly optimizing the physical and quantum error correction layers. ",Improved quantum error correction using soft information
21,1420910037774438403,990478024188485633,Yukei Murakami,"['New paper day!! \nWe release PIPS — a new, fast, state-of-the-art Python platform for the period detection in photometric time-series. A fully automated detection algorithm can help you discover variable stars and exoplanets! Check it out on arxiv👇\n<LINK>']",https://arxiv.org/abs/2107.14223,"We describe the \texttt{Period detection and Identification Pipeline Suite} (PIPS) -- a new, fast, and user-friendly platform for period detection and analysis of astrophysical time-series data. PIPS is an open-source Python package that provides various pre-implemented methods and a customisable framework for automated, robust period measurements with principled uncertainties and statistical significance calculations. In addition to detailing the general algorithm that underlies PIPS, this paper discusses one of PIPS' central and novel features, the Fourier-likelihood periodogram, and compares its performance to existing methods. The resulting improved performance implies that one can construct deeper, larger, and more reliable sets of derived properties from various observations, including all-sky surveys. We present a comprehensive validation of PIPS against artificially generated data, which demonstrates the reliable performance of our algorithm for a class of periodic variable stars (RR Lyrae stars). We further showcase an application to recently obtained data of variable stars in the globular cluster M15. ","PIPS, an advanced platform for period detection in time series -- I.
  Fourier-likelihood periodogram and application to RR Lyrae Stars"
22,1420842878796926976,990433714948661250,Sergey Levine,"['When RL learns in the real world, it can\'t push a ""reset button"" to try again, but must do lifelong reset-free learning. Maybe harder, but also easier, because it gives an opportunity to automatically build a curriculum! We leverage this in our new paper: <LINK> <LINK>']",https://arxiv.org/abs/2107.12931,"Reinforcement learning (RL) promises to enable autonomous acquisition of complex behaviors for diverse agents. However, the success of current reinforcement learning algorithms is predicated on an often under-emphasised requirement -- each trial needs to start from a fixed initial state distribution. Unfortunately, resetting the environment to its initial state after each trial requires substantial amount of human supervision and extensive instrumentation of the environment which defeats the goal of autonomous acquisition of complex behaviors. In this work, we propose Value-accelerated Persistent Reinforcement Learning (VaPRL), which generates a curriculum of initial states such that the agent can bootstrap on the success of easier tasks to efficiently learn harder tasks. The agent also learns to reach the initial states proposed by the curriculum, minimizing the reliance on human interventions into the learning. We observe that VaPRL reduces the interventions required by three orders of magnitude compared to episodic RL while outperforming prior state-of-the art methods for reset-free RL both in terms of sample efficiency and asymptotic performance on a variety of simulated robotics problems. ",Autonomous Reinforcement Learning via Subgoal Curricula
23,1420804708629766146,888216099757490176,Maithra Raghu,"['Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization\n\nWe introduce a rich family of tasks with a  pointer-value rule, to study mechanisms NN of generalization, from memorization to reasoning. \n\nPaper: <LINK> <LINK>', 'Generalization to unseen instances is central to the success of neural networks, but how does this happen?\n\nAre neural networks reliant on seeing very similar training examples?\n\nHow much do they generalize through abstract reasoning?\n\nDisentangling these methods is challenging!', 'We introduce Pointer Value Retrieval (PVR), a family of tasks to study these aspects of neural network generalization.\n\nPVR tasks vary in input type and difficulty, but in all of them, one position of the input acts as a pointer to a different input position (the value). https://t.co/NbrKQuUOE9', 'The interplay between positions, values and the pointer rule allows us to systematically vary difficulty, from introducing distribution shift to increasing functional complexity. \n\nThis in turn helps us explore different types of generalization! https://t.co/2qqBmDRjR0', 'We study a warmup task of exploring generalization in a visual PVR task.\n\nWe find generalization here relies on nearest-neighbor styled approaches and uncover a subtle spurious correlation causing poor performance on distribution shift. https://t.co/Kb6KWjSx7c', 'We then explore PVR tasks of increasing functional complexity. While train accuracy remains high throughout, test accuracy is very variable, and highly dependent on data size, suggesting some reliance on memorization\n\nWe show connections to noise sensitivity in boolean functions https://t.co/B19mniAztB', 'We also investigate performance variations across different model architectures, including MLPs, and the MLP-Mixer. Better architectural inductive bias shows clear benefits to data efficiency and generalization. https://t.co/vO45VzVTfq', ""Finally, to test reasoning, we study a PVR task with distribution shift &amp; functional complexity. Importantly, memorization alone can't solve this task\n\nWe find a surprising partial success: training is unstable, but in successful runs, the model learns the PVR reasoning rule! https://t.co/1IPyDzTHkR"", 'These results suggest many exciting open questions to study with PVR. Are there structural inductive biases that can prevent some cases of spurious correlations? What happens in training dynamics to determine whether reasoning is learned? We hope these can be explored in future! https://t.co/Qdrl6qPugo', 'This work is with wonderful (and entirely Twitterless!) collaborators Chiyuan Zhang, Jon Kleinberg and Samy Bengio', '@yieldthought Both the Transformer and the MLP-mixer did better than the naive MLP (even when we scaled the latter up), and the MLP-mixer was (surprisingly!) a bit better than the Transformer: higher accuracy and more data efficient!']",https://arxiv.org/abs/2107.12580,"Central to the success of artificial neural networks is their ability to generalize. But does neural network generalization primarily rely on seeing highly similar training examples (memorization)? Or are neural networks capable of human-intelligence styled reasoning, and if so, to what extent? These remain fundamental open questions on artificial neural networks. In this paper, as steps towards answering these questions, we introduce a new benchmark, Pointer Value Retrieval (PVR) to study the limits of neural network reasoning. The PVR suite of tasks is based on reasoning about indirection, a hallmark of human intelligence, where a first stage (task) contains instructions for solving a second stage (task). In PVR, this is done by having one part of the task input act as a pointer, giving instructions on a different input location, which forms the output. We show this simple rule can be applied to create a diverse set of tasks across different input modalities and configurations. Importantly, this use of indirection enables systematically varying task difficulty through distribution shifts and increasing functional complexity. We conduct a detailed empirical study of different PVR tasks, discovering large variations in performance across dataset sizes, neural network architectures and task complexity. Further, by incorporating distribution shift and increased functional complexity, we develop nuanced tests for reasoning, revealing subtle failures and surprising successes, suggesting many promising directions of exploration on this benchmark. ","Pointer Value Retrieval: A new benchmark for understanding the limits of
  neural network generalization"
24,1420763274795122702,91420905,Alex Smith,['New paper by András Kovács on measuring the ISW signal at high redshifts from supervoids in the eBOSS quasar sample <LINK>'],https://arxiv.org/abs/2107.13038,"The late-time integrated Sachs-Wolfe (ISW) imprint of $R\gtrsim 100~h^{-1}{\rm Mpc}$ super-structures is sourced by evolving large-scale potentials due to a dominant dark energy component in the $\Lambda$CDM model. The aspect that makes the ISW effect distinctly interesting is the repeated observation of stronger-than-expected imprints from supervoids at $z\lesssim0.9$. Here we analyze the un-probed key redshift range $0.8<z<2.2$ where the ISW signal is expected to fade in $\Lambda$CDM, due to a weakening dark energy component, and eventually become consistent with zero in the matter dominated epoch. On the contrary, alternative cosmological models, proposed to explain the excess low-$z$ ISW signals, predicted a sign-change in the ISW effect at $z\approx1.5$ due to the possible growth of large-scale potentials that is absent in the standard model. To discriminate, we estimated the high-$z$ $\Lambda$CDM ISW signal using the Millennium XXL mock catalogue, and compared it to our measurements from about 800 supervoids identified in the eBOSS DR16 quasar catalogue. At $0.8<z<1.2$, we found an excess ISW signal with $A_\mathrm{ ISW}\approx3.6\pm2.1$ amplitude. The signal is then consistent with the $\Lambda$CDM expectation ($A_\mathrm{ ISW}=1$) at $1.2<z<1.5$ where the standard and alternative models predict similar amplitudes. Most interestingly, we also detected an opposite-sign ISW signal at $1.5<z<2.2$ that is in $2.7\sigma$ tension with the $\Lambda$CDM prediction. Taken at face value, these moderately significant detections of ISW anomalies suggest an alternative growth rate of structure in low-density environments at $\sim100~h^{-1}{\rm Mpc}$ scales. ","Evidence for a high-z ISW signal from supervoids in the distribution of
  eBOSS quasars"
25,1420676251984011268,1138762581164855298,Christoph Ternes,"['New paper, <LINK> . We carefully examine the sensitivity to measure the neutrino mass ordering at JUNO, discussing the impact of several of the experimental features and systematic uncertainties.']",https://arxiv.org/abs/2107.12410,"The flagship measurement of the JUNO experiment is the determination of the neutrino mass ordering. Here we revisit its prospects to make this determination by 2030, using the current global knowledge of the relevant neutrino parameters as well as current information on the reactor configuration and the critical parameters of the JUNO detector. We pay particular attention to the non-linear detector energy response. Using the measurement of $\theta_{13}$ from Daya Bay, but without information from other experiments, we estimate the probability of JUNO determining the neutrino mass ordering at $\ge$ 3$\sigma$ to be 31% by 2030. As this probability is particularly sensitive to the true values of the oscillation parameters, especially $\Delta m^2_{21}$, JUNO's improved measurements of $\sin^2 \theta_{12}$, $\Delta m^2_{21}$ and $|\Delta m^2_{ee}|$, obtained after a couple of years of operation, will allow an updated estimate of the probability that JUNO alone can determine the neutrino mass ordering by the end of the decade. Combining JUNO's measurement of $|\Delta m^2_{ee}|$ with other experiments in a global fit will most likely lead to an earlier determination of the mass ordering. ",JUNO's prospects for determining the neutrino mass ordering
26,1420631583799795713,1329803308450582531,Emmanuel Frion,"[""New paper with Leo Giani and @taysmandrade! If you ever wondered how the expansion of the Universe would affect the apparent angular diameter of a black hole's shadow and its photon rings' amplitude and frequency, we answered that today! \n<LINK>""]",http://arxiv.org/abs/2107.13536,"The apparent angular size of the shadow of a black hole in an expanding Universe is redshift-dependent. Since cosmological redshifts change with time - known as the redshift drift - all redshift-dependent quantities acquire a time-dependence, and a fortiori so do black hole shadows. We find a mathematical description of the black hole shadow drift and show that the amplitude of this effect is of order $10^{-16}$ per day for M87$^{\star}$. While this effect is small, we argue that its non-detection can be used to constrain the accretion rate around supermassive black holes, as well as a novel probe of the equivalence principle. If general relativity is assumed, we infer from the data obtained by the Event Horizon Telescope for M87$^{\star}$ a maximum accretion rate of $|\dot{M}/{M}| \leq 10^5 M_{\odot}$ per year. On the other hand, in the case of an effective gravitation coupling, we derive a constraint of $|\dot{G}/G| \leq 10^{-3}-10^{-4}$ per year. The effect of redshift drift on the visibility amplitude and frequency of the universal interferometric signatures of photon rings is also discussed, which we show to be very similar to the shadow drift. This is of particular interest for future experiments involving spectroscopic and interferometric techniques, which could make observations of photon rings and their frequency drifts viable. ",Black Hole Shadow Drift and Photon Ring Frequency Drift
27,1420554520539680768,901266828655284225,Brian Metzger,"['<LINK>  New paper with my HUJI colleagues on an explanation for the recently discovered ""quasi periodic eruptions"" (QPE: ~hour long X-ray flares that occur ~periodically every few hours to a day from otherwise quiescent Galactic nuclei hosting low-mass SMBHs). <LINK>', 'QPEs were discovered just a few years ago, with the two most recent systems discovered with eROSITA in work led by @ArcodiaRiccardo .  For example, this remarkable NICER observations shows 14 such bursts over 11 days. https://t.co/h4UjrpH9ap', 'A natural mechanism to generate periodic X-ray emission is to periodically feed the SMBH with gas on scales of ~ AU.  An ordinary star like the Sun overflows its Roche Lobe at an orbital period of hours, so a mass-transferring ~circular stellar EMRI is a reasonable guess.', 'The problem for a single star: its orbital evolution driven by gravitational waves alone cannot steadily feed the SMBH at the rate to explain QPE.  OTOH, if the mass transfer is unstable (e.g., as in TDE) the short system lifetime make the required EMRI rates prohibitive.', 'A promising alternative, as we pointed out in 2017 w/ N. Stone, is to consider *two* consecutive stellar EMRIs.  The first EMRI stably transfers mass onto the SMBH for ~Myrs, before the second EMRI spirals in and begins to interact with it, periodically stripping mass.', 'Our original model considered the (seemingly) general case of misaligned orbital planes, in which the probably (~frequency) of close flybys is low.  However, after short-period QPEs were discovered, we realized a modified scenario of *coplanar* stellar orbits works beautifully.', 'Coplanar EMRIs (to within ~1% inclination) seems like a special configuration, until you realize that SMBH-spin induced precession on these radial scales will bring even initially misaligned EMRI orbital planes into alignment.', 'Among several nice features of the model, the period of the QPE is directly related to the mean density of the RLOFing star.  The first 4 measured QPE periods are consistent with a range of stellar masses/evolutionary states. https://t.co/dMJbNgFMI8', 'A prediction of this scenario is that the QPEing will not last forever.  As a result of the precession of the stellar orbits by the SMBH spin, we expect that within a few years the EMRI orbits will come out of alignment and these sources will temporarily ""turn off"" as QPEs.']",https://arxiv.org/abs/2107.13015,"A star that approaches a supermassive black hole (SMBH) on a circular extreme mass ratio inspiral (EMRI) can undergo Roche lobe overflow (RLOF), resulting in a phase of long-lived mass-transfer onto the SMBH. If the interval separating consecutive EMRIs is less than the mass-transfer timescale driven by gravitational wave emission (typically ~1-10 Myr), the semi-major axes of the two stars will approach each another on scales of <~ hundreds to thousands of gravitational radii. Close flybys tidally strip gas from one or both RLOFing stars, briefly enhancing the mass-transfer rate onto the SMBH and giving rise to a flare of transient X-ray emission. If both stars reside in an common orbital plane, these close interactions will repeat on a timescale as short as hours, generating a periodic series of flares with properties (amplitudes, timescales, sources lifetimes) remarkably similar to the ""quasi-periodic eruptions"" (QPEs) recently observed from galactic nuclei hosting low-mass SMBHs. A cessation of QPE activity is predicted on a timescale of months to years, due to nodal precession of the EMRI orbits out of alignment by the SMBH spin. Channels for generating the requisite coplanar EMRIs include the tidal separation of binaries (Hills mechanism) or Type I inwards migration through a gaseous AGN disk. Alternative scenarios for QPEs, that invoke single stellar EMRIs on an eccentric orbit undergoing a runaway sequence of RLOF events, are strongly disfavored by formation rate constraints. ","Interacting Stellar EMRIs as Sources of Quasi-Periodic Eruptions in
  Galactic Nuclei"
28,1420463898122100737,263867946,Emily,"['My first academic paper is now public!\n\nTaikoNation: Patterning-focused Chart Generation for Rhythm Action Games\n\n""We establish a new approach for chart generation that produces charts with more congruent, human-like patterning than seen in prior work.""\n\n<LINK> <LINK>', '@droombs 🥺', '@Xavy_osu for sure, feel free to either email me at ehalina(at)https://t.co/0sPntdlFiD or DM me on Discord @ Kaifin#7132']",https://arxiv.org/abs/2107.12506,"Generating rhythm game charts from songs via machine learning has been a problem of increasing interest in recent years. However, all existing systems struggle to replicate human-like patterning: the placement of game objects in relation to each other to form congruent patterns based on events in the song. Patterning is a key identifier of high quality rhythm game content, seen as a necessary component in human rankings. We establish a new approach for chart generation that produces charts with more congruent, human-like patterning than seen in prior work. ",TaikoNation: Patterning-focused Chart Generation for Rhythm Action Games
29,1420291675461758976,1317002454375059456,Paula García Molina,['New work on Noise in Digital and Digital-Analog Quantum Computation <LINK> In this paper in collaboration with @qmisanz and @quantum_ana we study the effect of noise sources in digital and digital-analog quantum computing and error mitigation techniques.'],https://arxiv.org/abs/2107.12969,"Quantum computing makes use of quantum resources provided by the underlying quantum nature of matter to enhance classical computation. However, current Noisy Intermediate-Scale Quantum (NISQ) era in quantum computing is characterized by the use of quantum processors comprising from a few tens to, at most, few hundreds of physical qubits without implementing quantum error correction techniques. This limits the scalability in the implementation of quantum algorithms. Digital-analog quantum computing (DAQC) has been proposed as a more resilient alternative quantum computing paradigm to outperform digital quantum computation within the NISQ era framework. It arises from adding the flexibility provided by fast single-qubit gates to the robustness of analog quantum simulations. Here, we perform a careful comparison between digital and digital-analog paradigms under the presence of noise sources. The comparison is illustrated by comparing the performance of the quantum Fourier transform algorithm under a wide range of single- and two-qubit noise sources. Indeed, we obtain that, when the different noise channels usually present in superconducting quantum processors are considered, the fidelity of the QFT algorithm for the digital-analog paradigm outperforms the one obtained for the digital approach. Additionally, this difference grows when the size of the processor scales up, constituting consequently a sensible alternative paradigm in the NISQ era. Finally, we show how the DAQC paradigm can be adapted to quantum error mitigation techniques for canceling different noise sources, including the bang error. ",Noise in Digital and Digital-Analog Quantum Computation
30,1420285323502886912,1339508444764786694,Gregor Kasieczka,"['New paper on arXiv: ""Unsupervised Hadronic SUEP at the LHC"" (<LINK>) with @drc83, including more friends from Toronto and Heidelberg 🍜🍜🍜 (1/4)', 'SUEP = Soft Unclustered Energy Patterns is a tricky signature for new physics. Often, we search for a small number of highly energetic particles (e.g. muons). This has its own challenges, but usually, expected signatures contain some handle that helps to identify signals (2/4).', 'SUEP is expected to produce a large number of low-energy particles and thus is much harder to find. We propose a new observable that could be used to find SUEP and compare it to supervised and unsupervised #ML (3/4) https://t.co/sFPSDBzaGI', 'The allowed SUEP phase-space is still relatively unexplored, so we do not want to make strong model assumptions or rely on supervised learning. Anomaly detection trained on data offers a way out, and we show how a relatively simple autoencoder could help discover SUEP. (4/4) https://t.co/rUBMdT5S4P']",http://arxiv.org/abs/2107.12379,"Confining dark sectors with pseudo-conformal dynamics produce SUEP, or Soft Unclustered Energy Patterns, at colliders: isotropic dark hadrons with soft and democratic energies. We target the experimental nightmare scenario, SUEPs in exotic Higgs decays, where all dark hadrons decay promptly to SM hadrons. First, we identify three promising observables, the charged particle multiplicity, the event ring isotropy, and the matrix of geometric distances between charged tracks. Their patterns can be exploited through a cut-and-count search, supervised machine learning, or an unsupervised autoencoder. We find that the HL-LHC will probe exotic Higgs branching ratios at the per-cent level, even without a detailed knowledge of the signal features. Our techniques can be applied to other SUEP searches, especially the unsupervised strategy, which is independent of overly specific model assumptions and the corresponding precision simulations. ",Unsupervised Hadronic SUEP at the LHC
31,1420283313797881856,3236251346,Mikel Sanz,"['New quantum paper today on Noise in Digital and Digital-Analog Quantum Computing (<LINK>) with two brilliant scientists @Paula_G_Phys and @quantum_ana. We compare the performance of digital and digital-analog quantum computing under different noise sources (1/2)', 'Additionally, we show that not only standard quantum error mitigation techniques can be applied to DAQC, but also how to extend them to suppress bang errors. @Ikerbasque @upvehu @OpenSuperQ (2/2) https://t.co/lgO7YAcRT5']",https://arxiv.org/abs/2107.12969,"Quantum computing makes use of quantum resources provided by the underlying quantum nature of matter to enhance classical computation. However, current Noisy Intermediate-Scale Quantum (NISQ) era in quantum computing is characterized by the use of quantum processors comprising from a few tens to, at most, few hundreds of physical qubits without implementing quantum error correction techniques. This limits the scalability in the implementation of quantum algorithms. Digital-analog quantum computing (DAQC) has been proposed as a more resilient alternative quantum computing paradigm to outperform digital quantum computation within the NISQ era framework. It arises from adding the flexibility provided by fast single-qubit gates to the robustness of analog quantum simulations. Here, we perform a careful comparison between digital and digital-analog paradigms under the presence of noise sources. The comparison is illustrated by comparing the performance of the quantum Fourier transform algorithm under a wide range of single- and two-qubit noise sources. Indeed, we obtain that, when the different noise channels usually present in superconducting quantum processors are considered, the fidelity of the QFT algorithm for the digital-analog paradigm outperforms the one obtained for the digital approach. Additionally, this difference grows when the size of the processor scales up, constituting consequently a sensible alternative paradigm in the NISQ era. Finally, we show how the DAQC paradigm can be adapted to quantum error mitigation techniques for canceling different noise sources, including the bang error. ",Noise in Digital and Digital-Analog Quantum Computation
32,1420279825646899200,1352309163779641344,Javier Rivera-Dean,['New paper out! High photon number entangled states and coherent state superposition from the extreme-ultraviolet to the far infrared.\n\nLink arXiv: <LINK>'],https://arxiv.org/abs/2107.12887,"We present a theoretical demonstration on the generation of entangled coherent states and of coherent state superpositions, with photon numbers and frequencies orders of magnitude higher than those provided by the current technology. This is achieved by utilizing a quantum mechanical multimode description of the single- and two-color intense laser field driven process of high harmonic generation in atoms. It is found that all field modes involved in the high harmonic generation process are entangled, and upon performing a quantum operation, leads to the generation of high photon number optical cat states spanning from the far infrared to the extreme-ultraviolet spectral region. This provides direct insights into the quantum mechanical properties of the optical field in intense laser matter interaction. Finally, these states can be considered as a new resource for fundamental tests of quantum theory, quantum information processing or sensing with non-classical states of light. ","High photon number entangled states and coherent state superposition
  from the extreme-ultraviolet to the far infrared"
33,1420265793338609664,1352309163779641344,Javier Rivera-Dean,['New paper out! New schemes for generating Schrödinger cat states using strong laser fields.\n\nLink arXiv: <LINK>\nLink Zenodo (code): <LINK>\nImage: Wigner functions for superpositions of three coherent states generated with one of the approaches <LINK>'],https://arxiv.org/abs/2107.12811,"Recently, using conditioning approaches on the high-harmonic generation process induced by intense laser-atom interactions, we have developed a new method for the generation of optical Schr\""odinger cat states (M. Lewenstein et al., arXiv:2008.10221 (2020)). These quantum optical states have been proven to be very manageable as, by modifying the conditions under which harmonics are generated, one can interplay between $\textit{kitten}$ and $\textit{genuine cat}$ states. Here, we demonstrate that this method can also be used for the development of new schemes towards the creation of optical Schr\""odinger cat states, consisting of the superposition of three distinct coherent states. Apart from the interest these kind of states have on their own, we additionally propose a scheme for using them towards the generation of large cat states involving the sum of two different coherent states. The quantum properties of the obtained superpositions aim to significantly increase the applicability of optical Schr\""odinger cat states for quantum technology and quantum information processing. ","New schemes for creating large optical Schrodinger cat states using
  strong laser fields"
34,1420111695364640768,1305574724743974912,Neeraja Gupta,"[""📢New Working Paper📢\n(w. @AlistairEcon &amp; @rigotti_luca)\nSocial scientists now have multiple populations from which they can collect data: which one should they use? The answer depends on cost, convenience, and data quality: The Experimenter's Dilemma.\n<LINK>\n1/N"", ""Main idea: Fix a budget and form the experimenter's  preference (stat. power) over: (i) observation cost; (ii) attenuation of effect size. Online populations offer low costs, but noise can attenuate treatment effects. Lab more expensive, but potentially higher quality data.\n2/N https://t.co/ryNvNX84Vn"", 'Fixing an experimental budget on each population and using games with no effective tension to measure  noise, we find Prolific should be dominant. However, even at 60% noise MTurk is still potentially better than the lab due to very cheap observations.\n3/N https://t.co/oL4WK1kc8Z', ""However, moving to a static over two prisoner's dilemma games the lab actually performs better: both Prolific and MTurk exhibit almost zero elasticity of response here.\n4/N https://t.co/aCKbjL3D4B"", 'Conclusions: MTurk is dominated by Prolific, where the higher noise is not worth the reduced cost. However, for our more subtle social dilemma comparison, the physical lab has an important role despite high observation costs, with greater sensitivity to induced payoffs.\n5/5', '@Danielf_Parra @m_serra_garcia @AlistairEcon @rigotti_luca Thanks Daniel! We are pretty excited about it too :)']",https://arxiv.org/abs/2107.05064,"We compare three populations commonly used in experiments by economists and other social scientists: undergraduate students at a physical location (lab), Amazon's Mechanical Turk (MTurk), and Prolific. The comparison is made along three dimensions: the noise in the data due to inattention, the cost per observation, and the elasticity of response. We draw samples from each population, examining decisions in four one-shot games with varying tensions between the individual and socially efficient choices. When there is no tension, where individual and pro-social incentives coincide, noisy behavior accounts for 60% of the observations on MTurk, 19% on Prolific, and 14% for the lab. Taking costs into account, if noisy data is the only concern Prolific dominates from an inferential power point of view, combining relatively low noise with a cost per observation one fifth of the lab's. However, because the lab population is more sensitive to treatment, across our main PD game comparison the lab still outperforms both Prolific and MTurk. ",The Experimenters' Dilemma: Inferential Preferences over Populations
35,1420088205165142016,1114037118,Chris Heunen,"['New paper with @Manchegobaby, about a well-founded programming (meta)language that compiles any irreversible quantum computation into reversible circuits: <LINK>. \n(If you ask nicely, he may do a #memeabstract!)']",https://arxiv.org/abs/2107.12144,"We study the two dual quantum information effects to manipulate the amount of information in quantum computation: hiding and allocation. The resulting type-and-effect system is fully expressive for irreversible quantum computing, including measurement. We provide universal categorical constructions that semantically interpret this arrow metalanguage with choice, starting with any rig groupoid interpreting the reversible base language. Several properties of quantum measurement follow in general, and we translate (noniterative) quantum flow charts into our language. The semantic constructions turn the category of unitaries between Hilbert spaces into the category of completely positive trace-preserving maps, and they turn the category of bijections between finite sets into the category of functions with chosen garbage. Thus they capture the fundamental theorems of classical and quantum reversible computing of Toffoli and Stinespring. ",Quantum Information Effects
36,1420053715348819973,2640960864,J. Andrew Casey-Clyde,"[""My first paper is on the @arxiv! We explore a new, quasar-based model of supermassive black hole binaries (SMBHBs) and predict their local number density and the volume of the nanohertz gravitational-wave background (GWB) using @NANOGrav's 12.5-yr results.\n<LINK> <LINK>"", 'We actually find a GWB like the signal hint in the NANOGrav results might imply five times more nearby SMBHBs than some more conservative estimates using galactic major merger rates. That could be great news for searches for individual SMBHBs! https://t.co/oDd4t0v84E', 'Heavier SMBHBs can also lead to a louder GWB, so we asked, ""what if there aren\'t more SMBHBs, and they\'re just more massive?"" It turns out that these SMBHBs merge faster, which leads to a smaller # of SMBHBs emitting. So a loud GWB might need more than just extra massive SMBHBs! https://t.co/ICrhRfle3U', 'Our model also lets us tie together SMBHB and quasar population fractions, i.e., how often we can expect SMBHBs with associated quasars. We found that at most only about 25% of SMBHBs should have associated quasar activity! https://t.co/8CEkEZ77fh', 'Getting a handle on these and other population fractions is actually one of the most exciting things about this model for us, so stay tuned for more updates on that front!', 'And thanks to @Dr_CMingarelli @astropardo and all our other collaborators for all your help and feedback!']",https://arxiv.org/abs/2107.11390,"The nanohertz gravitational wave background (GWB) is believed to be dominated by GW emission from supermassive black hole binaries (SMBHBs). Observations of several dual active galactic nuclei (AGN) strongly suggest a link between AGN and SMBHBs, given that these dual AGN systems will eventually form bound binary pairs. Here we develop an exploratory SMBHB population model based on empirically constrained quasar populations, allowing us to decompose the GWB amplitude into an underlying distribution of SMBH masses, SMBHB number density, and volume enclosing the GWB. Our approach also allows us to self-consistently predict the number of local SMBHB systems from the GWB amplitude. Interestingly, we find the local number density of SMBHBs implied by the common-process signal in the NANOGrav 12.5-yr dataset to be roughly five times larger than previously predicted by other models. We also find that at most $\sim 25 \%$ of SMBHBs can be associated with quasars. Furthermore, our quasar-based approach predicts $\gtrsim 95\%$ of the GWB signal comes from $z \lesssim 2.5$, and that SMBHBs contributing to the GWB have masses $\gtrsim 10^8 M_\odot$. We also explore how different empirical galaxy-black hole scaling relations affect the local number density of GW sources, and find that relations predicting more massive black holes decrease the local number density of SMBHBs. Overall, our results point to the important role that a measurement of the GWB will play in directly constraining the cosmic population of SMBHBs, as well as their connections to quasars and galaxy mergers. ","A quasar-based supermassive black hole binary population model:
  implications for the gravitational-wave background"
37,1420023208007421959,22148802,Leo C. Stein 🦁,"['🎉New paper day!🎉\n""Surprises in a classic boundary-layer problem""\nco-authored with W. Clark, M. Gomes, @Arnaldo_AGITF, yours truly, and the master: @stevenstrogatz.\n<LINK>\nSuper fun to work on! And for me it started right here on twitter. Intrigued?\n🧵 1/', 'This paper really needed a big toolbox of applied math methods: a nonlinear boundary-value problem, singular perturbations, exponential asymptotics, the phase plane, arbitrary precision and 32 orders of magnitude of agreement between analytics and numerics, and a pitchfork!\n2/ https://t.co/NTF0SYmKPB', 'My involvement in the calculation started here on Twitter, when @stevenstrogatz posted about his course on Asymptotics and perturbation methods, on YouTube last term (https://t.co/YFVBGLoHN4) because of the pandemic. For Steve, it started years ago!\n3/', 'We talk about the history and detective story in the paper. Steve talks about the problem in Lecture 16 (https://t.co/ijWe5YwGMy). Before I heard about this problem, previous students and postdocs William Clark, Mario Gomes, and @Arnaldo_AGITF already pitched in. 4/', ""The problem at hand is to solve a differential equation as a boundary value problem. Sounds simple, but it has incredibly rich phenomenology. There are narrow layers of width ≅ ε, and there are *three* possible solutions! Here's what they look like:\n5/ https://t.co/7AC0qZSzhw"", 'One way of solve a boundary value problem is to ""shoot"" for a solution, turning it into an initial value problem. So, what are the right initial slopes? Well, we need to use singular perturbation theory. One of the initial slopes blows up like ~A/ε. But the other two?\n6/ https://t.co/ys0cXldQ5I', 'Well, the other two are exponentially close to 1. Their dependence on ε involves\n   (A/ε) exp(-B/ε),\nwhich is a function that is ""beyond all orders"" of perturbation theory! So how could we find those slopes?\n7/ https://t.co/3KbW6AN0aM', 'Initially it involved some guess work... but then we *really* cracked the problem open once we realized what we could do in the ""phase plane"" (y, y\') of the problem. The phase-plane trajectories are integrable!\n8/ https://t.co/ZXl3VFgbex', 'This integrability allowed us to teleport information from trustworthy places on the curves, where perturbation theory works well, to those places where we get these beyond-all-orders results. Solving this implicit equation involves a special function.\n9/ https://t.co/cpPwPtLATj', ""(That special function is called the Lambert W function, and I'm a big fan, but no expert. The real expert around here is @corless_rob. Actually he answered one of my questions about this particular argument to W on this  bird app some weeks ago!)\n10/ https://t.co/OzVkdmTZOj"", ""One last cool feature of this problem. Here's the plot of (1 - initial slope) for the three solutions (over 32 orders of magnitude! This is why we need arbitrary precision numerics).  They look like they're approaching each other as ε grows, right? Right!\n11/ https://t.co/rp1mJ7ehlN"", 'In fact, the three possible solutions merge into *one* unique solution when ε=0.2159869288903... We were able to find this number by using our knowledge of the phase plane trajectories and symmetries between the solutions. Where they meet, we have a pitchfork bifurcation:\n12/ https://t.co/gxABxkkHhx', ""If any of this stuff sounds cool to you, please check out the preprint at https://t.co/CLyLE4wzyB. But wait, here's a bonus! We also have a github repository with a (python) jupyter notebook and a Mathematica notebook, so you can reproduce these calculations too!\n13/"", ""I'm a big believer in reproducible computing, and numerics were crucial to making progress and our understanding of this problem. So, I want all this code to be public (and I hope more people make their computer code public, too). Check it out at https://t.co/1iaHpxpOkQ.\n14/"", 'And of course feel free to ask any questions! Discussing on twitter is how I got involved in this whole thing. I hope our paper is educational for interested students 😁\n15/15', '@Hao_and_Y Yes, I definitely will!']",https://arxiv.org/abs/2107.11624,"We revisit a textbook example of a singularly perturbed nonlinear boundary-value problem. Unexpectedly, it shows a wealth of phenomena that seem to have been overlooked previously, including a pitchfork bifurcation in the number of solutions as one varies the small parameter, and transcendentally small terms in the initial conditions that can be calculated by elementary means. Based on our own classroom experience, we believe this problem could provide an enjoyable workout for students in courses on perturbation methods, applied dynamical systems, or numerical analysis. ",Surprises in a classic boundary-layer problem
38,1419979080120901663,983982423175258114,Fabian Mentzer,"['📢📢📢 New paper: ""Towards Generative Video Compression"". We present a GAN-based neural video compression system that is comparable to HEVC visually, and outperforms previous work that does not use GANs. Check it out on arxiv: <LINK> <LINK>', 'We propose a technique to mitigate temporal error-accumulation when unrolling via randomized shifting, motivated by spectral analysis. Furthermore, we release all the user study data to facilitate future research into perceptual video metrics. https://t.co/SDihTZBpAq', 'Joint work with @etagust, Johannes Ballé, David Minnen, @nmjohn, and @george_toderici.']",https://arxiv.org/abs/2107.12038,"We present the first neural video compression method based on generative adversarial networks (GANs). Our approach significantly outperforms previous neural and non-neural video compression methods in a user study, setting a new state-of-the-art in visual quality for neural methods. We show that the GAN loss is crucial to obtain this high visual quality. Two components make the GAN loss effective: we i) synthesize detail by conditioning the generator on a latent extracted from the warped previous reconstruction to then ii) propagate this detail with high-quality flow. We find that user studies are required to compare methods, i.e., none of our quantitative metrics were able to predict all studies. We present the network design choices in detail, and ablate them with user studies. ",Neural Video Compression using GANs for Detail Synthesis and Propagation
39,1419953782721495060,956539964795301889,Jacopo Bertolotti,"['New paper on @arXiv! <LINK>\nThis work was done by a former PhD student in my group (Alba Paniagua-Diaz, currently working with @pablo_artal), but while it was a chapter in her thesis, we never found the time to put it into a paper shape. Until now.\n🧵 1/', 'This work marks the point where Alba moved from ""I am doing what my supervisor tells me to do"" to ""I have these ideas I want to try out"", which is probably the most important moment in a PhD 🙂\n\nSo, what is it about?\n2/', '""Wavefront shaping"": by controlling the input wavefront, you can control the output even when the beam is completely scrambled by multiple scattering. How good is your control over the beam determines how good is your control over the output\nhttps://t.co/9HYk85ZKUc\n3/', 'In this paper we look at the problem from the opposite direction: what if our input beam is already completely scrambled (i.e. it looks like a speckle pattern)? Can we use wavefront shaping to recover a ""good"" beam?\n4/', 'If you had perfect control over the beam, and you could change its amplitude and phase however you wanted, the answer would be a trivial ""yes"". But you don\'t have perfect control over the beam.\n5/', 'Apart from technical limitations, you just can\'t take a point in the speckle pattern where there is no intensity, and multiply it by infinity to get a finite one. It just doesn\'t work.\nSo the question becomes: how well can you do it ""in practice""?\n6/', 'But first: who cares?\nWe care because there are many cases where we would like to refocus the light coming out of a multimode fibre (e.g. because we have a high-power fibre laser), and the output of a multimode fibre is at best a speckle pattern.\n7/', 'So in this paper we study how well we can take the output of a multimode fibre and form a nice Gaussian focus with it, with particular attention to the various possible limiting factors.\nIt is a very ""practical"" paper, which hopefully will be useful to people 🙂\n8/8', ""@sylvaingigan @arxiv @pablo_artal Let's see what the referees have to say 😉\n\n(One big problem with very delayed papers, is that if referees start insisting in new measurements, things get tricky)""]",https://arxiv.org/abs/2107.10601,"A perfectly collimated beam can be spread out by multiple scattering, creating a speckle pattern and increasing the etendue of the system. Standard optical systems conserve etendue, and thus are unable to reverse the process by transforming a speckle pattern into a collimated beam or, equivalently, into a sharp focus. Wavefront shaping is a technique that is able to manipulate the amplitude and/or phase of a light beam, thus controlling its propagation through such media. Wavefront shaping can thus break the conservation of etendue and, in principle, reduce it. In this work we study how much of the energy contained in a fully developed speckle pattern can be converted into a high quality (low M2) beam, and discuss the advantages and limitations of this approach, with special attention given to the inherent variability in the quality of the output due to the multiple scattering. ","Wavefront shaping to improve beam quality: converting a speckle pattern
  into a Gaussian spot"
40,1419902117758881792,10666172,Sabine Hossenfelder,"['New paper, comments welcome <LINK> <LINK>', '@anothernoki Recht auf körperliche Unversehrtheit. Wenn Du Dich nicht impfen lassen willst weil es Dir egal ist ob Du an COVID stirbst, ist das deine Sache. Wenn Du andere Leute damit in Gefahr bringst nicht.', '@anothernoki was redest Du für einen Schrott über ""medikamentösen Eingriff ohne Einverständnis des Patienten""? Verwechselst Du mich mit jemand oder kannst Du nur nicht lesen?', '@anothernoki @mcMuck Kompletter Quatsch, keiner wirft Leute, die aus medizinischen Gründen nicht geimpft werden können zusammen mit denen, die andere Leute (zB, die, die man nicht Impfen kann...) absichtlich in Gefahr bringen, weil sie sich nicht Impfen lassen wollen. Totales Straw-man Argument.']",https://arxiv.org/abs/2107.11497,"Cosmology relies on a coarse-grained description of the universe, assumed to be valid on large length scales. However, the nonlinearity of general relativity makes coarse-graining extremely difficult. We here address this problem by extending the Mori-Zwanzig projection operator formalism, a highly successful coarse-graining method from statistical mechanics, towards general relativity. Using the Buchert equations, we derive a new dynamic equation for the Hubble parameter which captures the effects of averaging through a memory function. This gives an empirical prediction for the cosmic jerk. ","Mori-Zwanzig formalism for general relativity: a new approach to the
  averaging problem"
41,1419848836109856769,96779364,Arnab Bhattacharyya,"['New paper: <LINK> (Efficient inference of interventional distributions) with Sutanu Gayen, @Saravanan_CU, Vedant Raval, and N.V. Vinodchandran. \n\nTL;DR: Get observational samples, learn interventional distribution, with guarantees for poly # samples and runtime.', 'Continuing a line of work we initiated earlier (https://t.co/ESygJG9Wkl), we give finite sample guarantees for estimating causal effects, given knowledge of a causal graph and access to observational samples. All variables are discrete, from a finite-sized alphabet (e.g, binary).', 'Seminal work by @yudapearl and Shpitser 15+ years ago had pinpointed conditions under which interventions are identifiable in this setting. They gave an algorithm which can require an unbounded number of samples.\n\nOur work makes their result algorithmic and quantitative.', ""We also prove a hardness result showing that marginals of Bayes nets (no interventions) are hard to learn. That is, we can't expect a poly time algorithm that outputs a circuit which implements an approximate probability mass function for the marginal on n/2 nodes."", '#causality #causalinference #Statistics #epitwitter']",https://arxiv.org/abs/2107.11712,"We consider the problem of efficiently inferring interventional distributions in a causal Bayesian network from a finite number of observations. Let $\mathcal{P}$ be a causal model on a set $\mathbf{V}$ of observable variables on a given causal graph $G$. For sets $\mathbf{X},\mathbf{Y}\subseteq \mathbf{V}$, and setting ${\bf x}$ to $\mathbf{X}$, let $P_{\bf x}(\mathbf{Y})$ denote the interventional distribution on $\mathbf{Y}$ with respect to an intervention ${\bf x}$ to variables ${\bf x}$. Shpitser and Pearl (AAAI 2006), building on the work of Tian and Pearl (AAAI 2001), gave an exact characterization of the class of causal graphs for which the interventional distribution $P_{\bf x}({\mathbf{Y}})$ can be uniquely determined. We give the first efficient version of the Shpitser-Pearl algorithm. In particular, under natural assumptions, we give a polynomial-time algorithm that on input a causal graph $G$ on observable variables $\mathbf{V}$, a setting ${\bf x}$ of a set $\mathbf{X} \subseteq \mathbf{V}$ of bounded size, outputs succinct descriptions of both an evaluator and a generator for a distribution $\hat{P}$ that is $\varepsilon$-close (in total variation distance) to $P_{\bf x}({\mathbf{Y}})$ where $Y=\mathbf{V}\setminus \mathbf{X}$, if $P_{\bf x}(\mathbf{Y})$ is identifiable. We also show that when $\mathbf{Y}$ is an arbitrary set, there is no efficient algorithm that outputs an evaluator of a distribution that is $\varepsilon$-close to $P_{\bf x}({\mathbf{Y}})$ unless all problems that have statistical zero-knowledge proofs, including the Graph Isomorphism problem, have efficient randomized algorithms. ",Efficient inference of interventional distributions
42,1419717086214041604,2179114005,Ronald Richman,"['Excited to present a new paper with Mario Wüthrich on an explainable deep learning model for tabular data, LocalGLMNet:\n\n<LINK>\n\nUsually, DL models for tabular data do not have interpretable outputs, so post-hoc methods such as SHAP must be applied. 1/N', 'Here, we present a new model that learns a local generalized linear model for each observation, preserving the advantages of representation learning offered by DL, while resulting in an interpretable model with explainable variable contributions.\n\n2/N https://t.co/IQPDnUGqfE', 'Since the model is differentiable, we are able to test for  interactions using the model gradients, which can either explicitly be incorporated into the model, or used for enhanced model understanding.\n\n3/N https://t.co/Jf7FZY1FYM', 'Along the way, we develop a new way of testing for variable significance, by adding a random covariate to the model, and calibrating a confidence band to the network outputs for this variable.\n\n4/N https://t.co/Gywzi3iIVk', 'We get contextualized embeddings for categorical data, in the spirit of the TabTransformer model, where the embedding value varies with the other covariates.\n\n5/n https://t.co/ihBkrD9saX', ""Finally, we derive a variable importance plot that doesn't depend on the usual assumption of independence between covariates (for example in SHAP).\n\n6/N https://t.co/gmpWH1pzwl"", 'While we present this model in the context of insurance modelling, it should be applicable in many other contexts requiring explainability.\n\n7/7', 'ps Thanks to @rasbt for answering my query on ML journals!']",https://arxiv.org/abs/2107.11059,"Deep learning models have gained great popularity in statistical modeling because they lead to very competitive regression models, often outperforming classical statistical models such as generalized linear models. The disadvantage of deep learning models is that their solutions are difficult to interpret and explain, and variable selection is not easily possible because deep learning models solve feature engineering and variable selection internally in a nontransparent way. Inspired by the appealing structure of generalized linear models, we propose a new network architecture that shares similar features as generalized linear models, but provides superior predictive power benefiting from the art of representation learning. This new architecture allows for variable selection of tabular data and for interpretation of the calibrated deep learning model, in fact, our approach provides an additive decomposition in the spirit of Shapley values and integrated gradients. ",LocalGLMnet: interpretable deep learning for tabular data
43,1419645116638183424,1035496901800587264,Edoardo Ponti,"['In our new paper, @KreutzerJulia @licwu @sivareddyg   and I present a method to enhance translation-based cross-lingual transfer (gains up to 2.7 per task and 5.6 per language). Pdf: <LINK>. Code: <LINK> @Mila_Quebec @CambridgeLTL @GoogleAI', 'While often achieving SOTA results, translation-based transfer suffers from some limitations: 1) errors accumulate along the pipeline and cannot be corrected; 2) only the maximum-likelihood translation is generated, which may not suffice for the downstream task.', 'Instead, we integrate both translator and classifier into a single model, by treating the intermediate translations as a latent random variable. By initialising both components with pre-trained models, our method is suitable for few-shot learning. https://t.co/4dVuYAYkV5', 'By performing inference under this model, 1) we fine-tune the translator end-to-end or via Minimum Risk Training according to the downstream task loss; 2) we draw multiple translation samples to perform ensemble prediction in the downstream task. https://t.co/qACQsAByBx', 'We evaluate our model on several classification tasks (XCOPA, XNLI, PAWS-X) and report gains especially for resource-poor languages like Haitian Creole. In fact, our model yields larger gains for languages whose BLEU scores are lower. https://t.co/Rab4uIevBJ', 'We also systematically compare a large set of NMT models wrt their effect on cross-lingual transfer. We find that 1) their BLUE scores vary to a large extent across models and languages; 2) lower-ranked translations do not degrade the downstream performance. https://t.co/bf7Nx1T7uD']",http://arxiv.org/abs/2107.11353,"While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation. To remedy this, we propose a new technique that integrates both steps of the traditional pipeline (translation and classification) into a single model, by treating the intermediate translations as a latent random variable. As a result, 1) the neural machine translation system can be fine-tuned with a variant of Minimum Risk Training where the reward is the accuracy of the downstream task classifier. Moreover, 2) multiple samples can be drawn to approximate the expected loss across all possible translations during inference. We evaluate our novel latent translation-based model on a series of multilingual NLU tasks, including commonsense reasoning, paraphrase identification, and natural language inference. We report gains for both zero-shot and few-shot learning setups, up to 2.7 accuracy points on average, which are even more prominent for low-resource languages (e.g., Haitian Creole). Finally, we carry out in-depth analyses comparing different underlying NMT models and assessing the impact of alternative translations on the downstream performance. ",Modelling Latent Translations for Cross-Lingual Transfer
44,1419570029423734787,147315191,Valeria Pettorino,"['Paper Adv: <LINK> Constraints on: \n1) a new Early Dark Energy (EDE) parameterization and on \n2) tomographic EDE, binned in redshift. We looked at the impact on the Hubble tension and how constraints change when the sound speed is allowed to vary.']",https://arxiv.org/abs/2107.11065,"Many quintessence models possess scaling or attractor solutions where the fraction of dark energy follows the dominant component in previous epochs of the expansion, or phase transitions may happen close to matter-radiation equality time. A non-negligible early dark energy (EDE) fraction around matter-radiation equality could contribute to alleviate the $H_0$ tension. We constrain the EDE fraction using two approaches: first, we use a fluid parameterization that mimics the plateaux of the dominant components in the past. An alternative tomographic approach constrains the EDE density in binned redshift intervals. This allows us to reconstruct $\Omega_{de}(z)$ before and after the decoupling of the CMB photons. We have employed Planck data 2018, the Pantheon supernovae of Type Ia (SNIa), galaxy clustering data, the prior on the absolute magnitude of SNIa by SH0ES, and weak lensing (WL) data from KiDS+VIKING-450 and DES-Y1. When we use a minimal parameterization mimicking the background plateaux, EDE has only a small impact on current cosmological tensions. The constraints on the EDE fraction weaken considerably when its sound speed is allowed to vary. By means of our binned analysis we put very tight constraints on the EDE fraction around the CMB decoupling time, $\lesssim 0.4\%$ at $2\sigma$ c.l. We confirm previous results that a significant EDE fraction in the radiation-dominated epoch (RDE) loosens the $H_0$ tension, but tends to worsen the $\sigma_8$ one. The presence of EDE in the matter-dominated era helps to alleviate this issue. When the SH0ES prior and WL data are considered in the fitting analysis in combination with data from CMB, SNIa and baryon acoustic oscillations, the EDE fractions are constrained to be $\lesssim 2.6\%$ in the RDE epoch and $\lesssim 1.5\%$ in the redshift range $z\in (100,1000)$ at $2\sigma$ c.l. The tensions remain at $\sim 2-3\sigma$ c.l. ",Early dark energy in the pre- and post-recombination epochs
45,1418513279190769664,1402765872943570944,Guillermo Franco Abellan,"['Our new paper is out! \nWe compare sixteen different models that have been proposed to resolve the H0 tension against a vast array of data, and quantify the relative success of each using a series of metrics.\nGo check it out!\n<LINK> <LINK>']",https://arxiv.org/abs/2107.10291,"Despite the remarkable success of the $\Lambda$Cold Dark Matter ($\Lambda$CDM) cosmological model, a growing discrepancy has emerged (currently measured at the level of $\sim 4-6 \sigma$) between the value of the Hubble constant $H_0$ measured using the local distance ladder and the value inferred using the cosmic microwave background and galaxy surveys. While a vast array of $\Lambda$CDM extensions have been proposed to explain these discordant observations, understanding the (relative) success of these models in resolving the tension has proven difficult -- this is a direct consequence of the fact that each model has been subjected to differing, and typically incomplete, compilations of cosmological data. In this review, we attempt to make a systematic comparison of sixteen different models which have been proposed to resolve the $H_0$ tension (spanning both early- and late-Universe solutions), and quantify the relative success of each using a series of metrics and a vast array of data combinations. Owing to the timely appearance of this article, we refer to this contest as the ''$H_0$ Olympics''; the goal being to identify which of the proposed solutions, and more broadly which underlying mechanisms, are most likely to be responsible for explaining the observed discrepancy (should unaccounted for systematics not be the culprit). This work also establishes a foundation of tests which will allow the success of novel proposals to be meaningful ''benchmarked''. ",The $H_0$ Olympics: A fair ranking of proposed models
46,1418496159061090305,870360681245179904,Daniel Cordier,"['Our new paper about @TitanSaturnMoon alkanofer is on @arxiv at <LINK>, data available on @ZENODO_ORG at <LINK> and PC-SAFT eos program can be downloaded from @github at <LINK>\nHave fun! <LINK>']",https://arxiv.org/abs/2107.06348,"According to clues left by the Cassini mission, Titan, one of the two Solar System bodies with a hydrologic cycle, may harbor liquid hydrocarbon-based analogs of our terrestrial aquifers, referred to as ""alkanofers"". On the Earth, petroleum and natural gas reservoirs show a vertical gradient in chemical composition, established over geological timescales. In this work, we aim to investigate the conditions under which Titan's processes could lead to similar situations. We built numerical models including barodiffusion and thermodiffusion (Soret's effect) in N_2+CH_4+C_2H_6 liquid mixtures, which are relevant for Titan's possible alkanofers. Our main assumption is the existence of reservoirs of liquids trapped in a porous matrix with low permeability. Due to the small size of the molecule, nitrogen seems to be more sensitive to gravity than ethane, even if the latter has a slightly larger mass. This behavior, noticed for an isothermal crust, is reinforced by the presence of a geothermal gradient. Vertical composition gradients, formed over timescales of between a fraction of a mega-year to several tens of mega-years, are not influenced by molecular diffusion coefficients. We find that ethane does not accumulate at the bottom of the alkanofers under diffusion, leaving the question of why ethane is not observed on Titan's surface unresolved. If the alkanofer liquid was in contact with water-ice, we checked that N_2 did not, in general, impede the clathration of C_2H_6, except in some layers. Interestingly, we found that noble gases could easily accumulate at the bottom of an alkanofer. ","Vertical compositional variations of liquid hydrocarbons in Titan's
  alkanofers"
47,1418491800264744960,2915749124,Dhiraj Hazra,"[""Our new paper 'Dark Twilight Joined with the Light of Dawn to Unveil the Reionization History' with Daniela Paoletti, Fabio Finelli and @georgesmoot — <LINK> — an extended analysis of the reionization history based on recent cosmological and astrophysical data.""]",https://arxiv.org/abs/2107.10693,"Improved measurement of the Cosmic Microwave Background polarization from Planck allows a detailed study of reionization beyond the average optical depth. The lower value of the optical depth disfavours an early onset and an early completion of reionization in favour of a redsfhit range where different astrophysical probes provide sensible information on the sources of reionization and the status of the intergalactic medium. In this work we extend our previous study in which we constrained reionization by combining three different probes - CMB, UV luminosity density and neutral hydrogen fraction data - in both treatment and data: we first allow variation in the UV source term varying the product of the efficiency of conversion of UV luminosity into ionizing photons and the escape fraction together with the reionization and cosmological parameters, and then we investigate the impact of a less conservative cut for the UV luminosity function. We find that the estimate for the efficiency is consistent within 95% C.L. with the fixed value we considered in our previous results and is mostly constrained by the QHII data. We find that allowing the efficiency to vary does not affect significantly our results for the average optical depth for monotonic reionization histories, recovering $\tau=0.0519_{-0.0008}^{+0.0010}$ at 68% CL , consistent with our previous studies. Using a less conservative cut for the UV luminosity function, we find $\tau=0.0541_{-0.0016}^{+0.0013}$ at 68% CL, due to the faint end of the luminosity function in the data we use, that also prefers a larger contribution from higher redshifts. ","Dark Twilight Joined with the Light of Dawn to Unveil the Reionization
  History"
48,1418460487445094402,387358708,Salva Centelles,"['(1/4) Exciting new paper today with my awesome collaborators Christian Döring, Manfred Lindner, Björn Malte Schaefer and Matthias Bartelmann! Take a look at <LINK> <LINK>', '(2/4) The target of the paper is to compute the effect of gravitational waves produced by a first order phase transition in the galactic structure formation. The PT must be late (t* &gt;~ 10^6s) , long (beta &lt;~ 7 H*) and strong (alpha &gt;~1) in order for this effect to be sizeable. https://t.co/GVD1BRGe5P', '(3/4) We perform a second order perturbation analysis in the 1+3 covariant framework to derive a wave equation in which adiabatic density perturbations of the photon-baryon fluid are sourced by the GW energy density during radiation domination on sub-horizon scales. https://t.co/OfnhwkAbY9', '(4/4) We then evolve the perturbations from the end of the PT until today and use the cosmic variance bound to constraint the phase transition parameters: alpha, beta and t*, which in turn constraint the energy density and frequency of gravitational waves today. https://t.co/zK8AmsQjzz']",https://arxiv.org/abs/2107.10283,"We study the impact of gravitational waves originating from a first order phase transition on structure formation. To do so, we perform a second order perturbation analysis in the $1+3$ covariant framework and derive a wave equation in which second order, adiabatic density perturbations of the photon-baryon fluid are sourced by the gravitational wave energy density during radiation domination and on sub-horizon scales. The scale on which such waves affect the energy density perturbation spectrum is found to be proportional to the horizon size at the time of the phase transition times its inverse duration. Consequently, structure of the size of galaxies and bigger can only be affected in this way by relatively late phase transitions at $\ge 10^{6}\,\text{s}$. Using cosmic variance as a bound we derive limits on the strength $\alpha$ and the relative duration $(\beta/H_*)^{-1}$ of phase transitions as functions of the time of their occurrence which results in a new exclusion region for the energy density in gravitational waves today. We find that the cosmic variance bound forbids only relative long lasting phase transitions, e.g. $\beta/H_*\lesssim 6.8$ for $t_*\approx 5\times10^{11}\,\text{s}$, which exhibit a substantial amount of supercooling $\alpha>20$ to affect the matter power spectrum. ",Gravitational wave induced baryon acoustic oscillations
49,1418410337695506432,96779364,Arnab Bhattacharyya,"['New paper: <LINK> (Learning Sparse Fixed-Structure Gaussian Bayesian Networks) with Davin Choo, @rrgajjala, Sutanu Gayen, and @Yohanna49592977.', 'We look at a basic model used to specify causal dependencies among continuous variables. You have n variables that are ordered in some way, and each variable is generated as a linear combination of the previous variables plus an independent gaussian noise. Simple, right? E.g.: https://t.co/MPLTH7fugA', 'These are called Gaussian Bayes nets. The dependency structure of the variables is naturally encoded by a DAG. For the example above: https://t.co/hO6gWyXvtX', 'Suppose you have a distribution P generated as a Gaussian Bayes net over a DAG G. \n\nThe distribution learning problem is: given samples from P, infer parameters of a distribution Q such that TV(P,Q)&lt;ε with good enough probability.', 'There are actually two problems here. The first is the ""structure learning"" problem where G is not known (but maybe you only know that it is sparse). This problem is quite hard, and there are essentially no general algorithmic results.', 'In this paper, we look at the easier ""fixed-structure"" problem where G is already given. Amazingly to us, we could say something new about this basic problem!', 'The obvious thing to try is to learn the coefficients of each equation by lin regression at each node. If you run least squares with O~(n/ε) equations at each node, then you learn a Bayes net with KL div ε from P.', 'But this isn\'t the only option! At each node, you can run several batches of least squares, where each batch is a ""small"" system of equations. Each batch solution gives you an estimate of the coefficients at that node, and then you can take the average across batch solutions.', 'In the extreme case, if a node has p parents, you can solve several batches of pxp systems (with gaussian elim). Here, we show that each batch solution is distributed as Cauchy (!), not gaussian. It then makes more sense to take the median of the solutions rather than average.', ""The advantage of these other algorithms is that they allow each batch to be processed parallelly. Also, in experiments (https://t.co/tZZoMYXnCE), they perform better when there's noise or the DAG is mis-specified."", '#causalinference #Statistics #MachineLearning #Algorithms']",https://arxiv.org/abs/2107.10450,"Gaussian Bayesian networks (a.k.a. linear Gaussian structural equation models) are widely used to model causal interactions among continuous variables. In this work, we study the problem of learning a fixed-structure Gaussian Bayesian network up to a bounded error in total variation distance. We analyze the commonly used node-wise least squares regression (LeastSquares) and prove that it has a near-optimal sample complexity. We also study a couple of new algorithms for the problem: - BatchAvgLeastSquares takes the average of several batches of least squares solutions at each node, so that one can interpolate between the batch size and the number of batches. We show that BatchAvgLeastSquares also has near-optimal sample complexity. - CauchyEst takes the median of solutions to several batches of linear systems at each node. We show that the algorithm specialized to polytrees, CauchyEstTree, has near-optimal sample complexity. Experimentally, we show that for uncontaminated, realizable data, the LeastSquares algorithm performs best, but in the presence of contamination or DAG misspecification, CauchyEst/CauchyEstTree and BatchAvgLeastSquares respectively perform better. ",Learning Sparse Fixed-Structure Gaussian Bayesian Networks
50,1418399407792443392,923231130383536128,Eduardo Fonseca,"['New paper! We evaluate two pooling methods to improve shift invariance in CNNs, obtaining SOTA on FSD50K. Methods are based on low-pass filtering &amp; adaptive sampling of feature maps. They increase robustness to time/freq shifts in the input! w/ @andrebola_ <LINK> <LINK>']",https://arxiv.org/abs/2107.00623,"Recent studies have put into question the commonly assumed shift invariance property of convolutional networks, showing that small shifts in the input can affect the output predictions substantially. In this paper, we analyze the benefits of addressing lack of shift invariance in CNN-based sound event classification. Specifically, we evaluate two pooling methods to improve shift invariance in CNNs, based on low-pass filtering and adaptive sampling of incoming feature maps. These methods are implemented via small architectural modifications inserted into the pooling layers of CNNs. We evaluate the effect of these architectural changes on the FSD50K dataset using models of different capacity and in presence of strong regularization. We show that these modifications consistently improve sound event classification in all cases considered. We also demonstrate empirically that the proposed pooling methods increase shift invariance in the network, making it more robust against time/frequency shifts in input spectrograms. This is achieved by adding a negligible amount of trainable parameters, which makes these methods an appealing alternative to conventional pooling layers. The outcome is a new state-of-the-art mAP of 0.541 on the FSD50K classification benchmark. ","Improving Sound Event Classification by Increasing Shift Invariance in
  Convolutional Neural Networks"
51,1418316329002704896,705445424711266304,Sébastien Lachapelle,['New paper out!\nDiscovering Latent Causal Variables via Mechanism Sparsity: A New Principle for Nonlinear ICA\n<LINK>\n\nPoster session this Friday (July 23) 12-1pm (ET time) @ Workshop on the Neglected Assumptions in Causal Inference at ICML 2021'],https://arxiv.org/abs/2107.10098,"This work introduces a novel principle we call disentanglement via mechanism sparsity regularization, which can be applied when the latent factors of interest depend sparsely on past latent factors and/or observed auxiliary variables. We propose a representation learning method that induces disentanglement by simultaneously learning the latent factors and the sparse causal graphical model that relates them. We develop a rigorous identifiability theory, building on recent nonlinear independent component analysis (ICA) results, that formalizes this principle and shows how the latent variables can be recovered up to permutation if one regularizes the latent mechanisms to be sparse and if some graph connectivity criterion is satisfied by the data generating process. As a special case of our framework, we show how one can leverage unknown-target interventions on the latent factors to disentangle them, thereby drawing further connections between ICA and causality. We propose a VAE-based method in which the latent mechanisms are learned and regularized via binary masks, and validate our theory by showing it learns disentangled representations in simulations. ","Disentanglement via Mechanism Sparsity Regularization: A New Principle
  for Nonlinear ICA"
52,1418222057704263680,1199153585013055489,Katiana Kontolati,"['Interested in constructing surrogates for very high-dimensional models? 👉Check out our new paper on arXiv: <LINK>\n\n#AcademicTwitter #AcademicChatter #JohnsHopkins <LINK>', 'We construct polynomial chaos expansion surrogates on subspace manifolds by utilizing manifold learning techniques and propose an encoder-decoder framework that leads to a significant acceleration of UQ tasks in complex systems.\n👉Github repo: https://t.co/kFqg7dPHp8', '@mistrigr Thank you! 🙏☺️', '@KaustavBera11 Thank you, Kaustav! 🙂']",https://arxiv.org/abs/2107.09814,"In this work we introduce a manifold learning-based method for uncertainty quantification (UQ) in systems describing complex spatiotemporal processes. Our first objective is to identify the embedding of a set of high-dimensional data representing quantities of interest of the computational or analytical model. For this purpose, we employ Grassmannian diffusion maps, a two-step nonlinear dimension reduction technique which allows us to reduce the dimensionality of the data and identify meaningful geometric descriptions in a parsimonious and inexpensive manner. Polynomial chaos expansion is then used to construct a mapping between the stochastic input parameters and the diffusion coordinates of the reduced space. An adaptive clustering technique is proposed to identify an optimal number of clusters of points in the latent space. The similarity of points allows us to construct a number of geometric harmonic emulators which are finally utilized as a set of inexpensive pre-trained models to perform an inverse map of realizations of latent features to the ambient space and thus perform accurate out-of-sample predictions. Thus, the proposed method acts as an encoder-decoder system which is able to automatically handle very high-dimensional data while simultaneously operating successfully in the small-data regime. The method is demonstrated on two benchmark problems and on a system of advection-diffusion-reaction equations which model a first-order chemical reaction between two species. In all test cases, the proposed method is able to achieve highly accurate approximations which ultimately lead to the significant acceleration of UQ tasks. ","Manifold learning-based polynomial chaos expansions for high-dimensional
  surrogate models"
53,1418137710569697281,23000769,Christopher Conselice,['New paper out today led by @AstroSunnyC on the largest galaxy morphological catalog to date with 20 million objects from the @theDESurvey of mostly lower redshift systems.  Classifications are done using Deep Learning CNN - catalog to be made public soon.\n\n<LINK>'],https://arxiv.org/abs/2107.10210,"We present in this paper one of the largest galaxy morphological classification catalogues to date, including over 20 million of galaxies, using the Dark Energy Survey (DES) Year 3 data based on Convolutional Neural Networks (CNN). Monochromatic $i$-band DES images with linear, logarithmic, and gradient scales, matched with debiased visual classifications from the Galaxy Zoo 1 (GZ1) catalogue, are used to train our CNN models. With a training set including bright galaxies ($16\le{i}<18$) at low redshift ($z<0.25$), we furthermore investigate the limit of the accuracy of our predictions applied to galaxies at fainter magnitude and at higher redshifts. Our final catalogue covers magnitudes $16\le{i}<21$, and redshifts $z<1.0$, and provides predicted probabilities to two galaxy types -- Ellipticals and Spirals (disk galaxies). Our CNN classifications reveal an accuracy of over 99\% for bright galaxies when comparing with the GZ1 classifications ($i<18$). For fainter galaxies, the visual classification carried out by three of the co-authors shows that the CNN classifier correctly categorises disky galaxies with rounder and blurred features, which humans often incorrectly visually classify as Ellipticals. As a part of the validation, we carry out one of the largest examination of non-parametric methods, including $\sim$100,000 galaxies with the same coverage of magnitude and redshift as the training set from our catalogue. We find that the Gini coefficient is the best single parameter discriminator between Ellipticals and Spirals for this data set. ","Galaxy Morphological Classification Catalogue of the Dark Energy Survey
  Year 3 data with Convolutional Neural Networks"
54,1418134599281090561,92580220,Thomas Spriggs,"['New paper day! 🎉🎉🎉 Introducing the Planetary Nebula Catalogue of the Fornax3D ETGs, along with independent distance estimates! This paper has been accepted in A&amp;A, and is now on Arxiv /ADS: <LINK>\nPlots and overview to follow', 'Introducing our catalogue of 1350 PNe from across 21 ETGs of the Fornax cluster, observed using MUSE as part of the Fornax3D survey. We also include PNLF derived distances to each galaxy. https://t.co/wVQHW44MmR', 'Those distances are in very good agreement with the Surface Brightness Fluctuation (SBF) within the literature, and our average distance to the cluster: D=19.86 +/- 0.32 Mpc, agrees well with other cluster average distances. https://t.co/YgtM94D8HL', 'Now, back to writing the thesis. #PhDlife #Fornax3D #Astrophysics', '@I_Hert_Space']",https://arxiv.org/abs/2107.09680,"Extragalactic planetary nebulae (PNe) offer a way to determine the distance to their host galaxies thanks to the nearly universal shape of the planetary nebulae luminosity function (PNLF). Accurate PNe distance measurements rely on obtaining well-sampled PNLFs and the number of observed PNe scales with the encompassed stellar mass. This means either disposing of wide-field observations or focusing on the bright central regions of galaxies. In this work we take this second approach and conduct a census of the PNe population in the central regions of galaxies in the Fornax cluster, using VLT/MUSE data for the early-type galaxies observed over the course of the Fornax3D survey. Using such integral-field spectroscopic observations to carefully separate the nebular emission from the stellar continuum, we isolated [OIII] 5007 {\AA} sources of interest, filtered out unresolved impostor sources or kinematic outliers, and present a catalogue of 1350 unique PNe sources across 21 early-type galaxies, which includes their positions, [OIII] 5007 {\AA} line magnitudes, and line-of-sight velocities. Using the PNe catalogued within each galaxy, we present independently derived distance estimates based on the fit to the entire observed PNLF observed while carefully accounting for the PNe detection incompleteness. With these individual measurements, we arrive at an average distance to the Fornax cluster itself of 19.86 $\pm$ 0.32 Mpc ($\mu_{PNLF}$ = 31.49 $\pm$ 0.04 mag). Our PNLF distance measurements agree well with previous distances based on surface brightness fluctuations, finding no significant systematic offsets between the two methods as otherwise reported in previous studies. ","The Fornax3D project: Planetary nebulae catalogue and independent
  distance measurements to Fornax cluster galaxies"
55,1418038418882416642,618128128,Shuiwang Ji,['New paper on SSL for Graphs:\nGroup Contrastive Self-Supervised Learning on Graphs\n\nPaper link: <LINK>'],https://arxiv.org/abs/2107.09787,"We study self-supervised learning on graphs using contrastive methods. A general scheme of prior methods is to optimize two-view representations of input graphs. In many studies, a single graph-level representation is computed as one of the contrastive objectives, capturing limited characteristics of graphs. We argue that contrasting graphs in multiple subspaces enables graph encoders to capture more abundant characteristics. To this end, we propose a group contrastive learning framework in this work. Our framework embeds the given graph into multiple subspaces, of which each representation is prompted to encode specific characteristics of graphs. To learn diverse and informative representations, we develop principled objectives that enable us to capture the relations among both intra-space and inter-space representations in groups. Under the proposed framework, we further develop an attention-based representor function to compute representations that capture different substructures of a given graph. Built upon our framework, we extend two current methods into GroupCL and GroupIG, equipped with the proposed objective. Comprehensive experimental results show our framework achieves a promising boost in performance on a variety of datasets. In addition, our qualitative results show that features generated from our representor successfully capture various specific characteristics of graphs. ",Group Contrastive Self-Supervised Learning on Graphs
56,1417924206889603072,1235566258222817280,Andrea Antoni,['Excited to share our new paper out today on the collapse of convective red supergiants in failed supernovae <LINK>'],https://arxiv.org/abs/2107.09068,"During the core collapse of massive stars that do not undergo a canonical energetic explosion, some of the hydrogen envelope of a red supergiant (RSG) progenitor may infall onto the newborn black hole (BH). Within the Athena++ framework, we perform three-dimensional, hydrodynamical simulations of idealized models of supergiant convection and collapse in order to assess whether the infall of the convective envelope can give rise to rotationally-supported material, even if the star has zero angular momentum overall. Our dimensionless, polytropic models are applicable to the optically-thick hydrogen envelope of non-rotating RSGs and cover a factor of 20 in stellar radius. At all radii, the specific angular momentum due to random convective flows implies associated circularization radii of 10 - 1500 times the innermost stable circular orbit of the BH. During collapse, the angular momentum vector of the convective flows is approximately conserved and is slowly varying on the timescale relevant to forming disks at small radii. Our results indicate that otherwise failed explosions of RSGs lead to the formation of rotationally-supported flows that are capable of driving outflows to large radii and powering observable transients. When the BH is able to accrete most of the hydrogen envelope, the final BH spin parameter is $\sim$ 0.5, even though the star is non-rotating. For fractional accretion of the envelope, the spin parameter is generally lower and never exceeds 0.8. We discuss the implications of our results for transients produced by RSG collapse to a black hole. ","Numerical Simulations of the Random Angular Momentum in Convection:
  Implications for Supergiant Collapse to Form Black Holes"
57,1417874378067308544,10471882,matt brehmer,"[""Last year, @eagereyes and I interviewed people about live presentations of data + 📊 in their organizations. \n\nOur #ieeevis '21 paper documents our findings with a musical performance metaphor, along with some new ideas for presenting data. 🎶 \n\npre-print: <LINK> <LINK>""]",https://arxiv.org/abs/2107.09042,"Prior research on communicating with visualization has focused on public presentation and asynchronous individual consumption, such as in the domain of journalism. The visualization research community knows comparatively little about synchronous and multimodal communication around data within organizations, from team meetings to executive briefings. We conducted two qualitative interview studies with individuals who prepare and deliver presentations about data to audiences in organizations. In contrast to prior work, we did not limit our interviews to those who self-identify as data analysts or data scientists. Both studies examined aspects of speaking about data with visual aids such as charts, dashboards, and tables. One study was a retrospective examination of current practices and difficulties, from which we identified three scenarios involving presentations of data. We describe these scenarios using an analogy to musical performance: small collaborative team meetings are akin to jam session, while more structured presentations can range from semi-improvisational performances among peers to formal recitals given to executives or customers. In our second study, we grounded the discussion around three design probes, each examining a different aspect of presenting data: the progressive reveal of visualization to direct attention and advance a narrative, visualization presentation controls that are hidden from the audience's view, and the coordination of a presenter's video with interactive visualization. Our distillation of interviewees' responses surfaced twelve themes, from ways of authoring presentations to creating accessible and engaging audience experiences. ","From Jam Session to Recital: Synchronous Communication and Collaboration
  Around Data in Organizations"
58,1417870039378583554,1360892702,Ekin Dogus Cubuk,"['New paper on ML &amp; physics at ICML!\n \nLearn2Hop: Learned Optimization on Rough Landscapes\nWith Applications to Atomic Structural Optimization\n \nWe adapt learned optimizers for atomic structural optimization, and compare to baselines from physics. \n\nabs: <LINK> <LINK>', ""Finding the low-energy minima of atomic energy landscapes efficiently is one of the most important challenges in materials science.\n \nML has become ubiquitous in physics as a curve-fitting tool, but hasn't been utilized as much for optimization."", 'On the well-studied benchmark of LJ clusters, learned optimizers generalize well across system sizes. They increase the success rate on simple landscapes like LJ13, and lower the final energy on glassy landscapes such as LJ75. https://t.co/dlCIUqORmm', 'Learned optimizers outperform baselines such as Adam and basin-hopping on more realistic landscapes such as ""glassy"" gold clusters and amorphous silicon. https://t.co/UqGOaHPKrw', 'Learned optimizers also show some generalization across the periodic table: e.g. when trained only on gold-silver clusters, they can outperform baselines on palladium-platinum clusters.\n \nThey do better however if also trained on clusters containing palladium or platinum. https://t.co/ksByIKyZJh', 'Still too challenging to train these optimizers on more expensive potentials such as DFT, but we are excited about the future of this direction for involving ML in atomic structural optimization!\n \nwith @Luke_Metz and @sschoenholz, led by the incredible resident @amilmerchant.', 'Check out our presentation at ICML for more details: \n\nhttps://t.co/kFyNd7rYYe']",http://arxiv.org/abs/2107.09661,"Optimization of non-convex loss surfaces containing many local minima remains a critical problem in a variety of domains, including operations research, informatics, and material design. Yet, current techniques either require extremely high iteration counts or a large number of random restarts for good performance. In this work, we propose adapting recent developments in meta-learning to these many-minima problems by learning the optimization algorithm for various loss landscapes. We focus on problems from atomic structural optimization--finding low energy configurations of many-atom systems--including widely studied models such as bimetallic clusters and disordered silicon. We find that our optimizer learns a 'hopping' behavior which enables efficient exploration and improves the rate of low energy minima discovery. Finally, our learned optimizers show promising generalization with efficiency gains on never before seen tasks (e.g. new elements or compositions). Code will be made available shortly. ",Learn2Hop: Learned Optimization on Rough Landscapes
59,1417860509102186499,1050093076159422464,Sina Fazelpour,"['🧵Happy to share the pre-print of our new paper (w/ @mariadearteaga), “Diversity in Sociotechnical Machine Learning Systems”, where we map &amp; explicate different sociocultural diversity-related considerations throughout the machine learning pipeline\n<LINK> <LINK>', 'Research on sociocultural diversity in ML systems falls into two broad lines: One concerns diversity of teams &amp; collectives whose choices shape a system (e.g., designers, developers, labelers). Here, diversity is viewed as a potential organizational response to algorithmic bias.', 'The other research line pertains to the composition of items at some stage of the data-processing pipeline. Here, sociocultural diversity is seen as a design desideratum in curating input datasets or in subset selection of algorithmic output (e.g., in hiring, image search).', ""But currently there's a gap between these works on potential benefits and measures of diversity in ML systems, on the one hand, and broader research on sociocultural diversity in humanities &amp; organizational and social sciences, on the other."", 'Why does it matter? Because as that broader research shows, diversity is not a monolithic concept, but has varied meanings. Different diversity concepts are based on distinct epistemic, ethical &amp; political rationales that should inform how we measure diversity in a context &amp; why.', 'So, without grounding proposed measures in appropriate concepts, we face challenges like disconnects between our motivations in attending to sociocultural diversity and our implementation; or misunderstanding the relation between diversity &amp; other desiderata (e.g., fairness).', 'Similarly, when making claims about the benefits of diversity for collective performance, diversity researchers have emphasized the need for being specific about the precise pathway through which this benefit is supposed to happen (&amp; pathway-specific effect modifiers).', ""Without connecting the discussion of diversity's benefits to specific pathways &amp; relevant effect modifiers, we can end up with uninformative (or false) generalities, flawed experimental designs, or erroneous interpretations of &amp; inferences from empirical findings."", 'To address these issues, in the paper we draw on research on sociocultural diversity in philosophy, psychology, and organizational &amp; social sciences to provide taxonomies of diversity concepts and pathways of potential benefit that we take to be relevant to the ML context.', 'We situate these taxonomies (of concepts and pathways) in the lifecycle of ML systems and make a case for their usefulness in fair and accountable ML by showing how they help clarify existing findings and disagreements, and point towards new directions of research.', ""I'm grateful for having @mariadearteaga as my collaborator on this. The paper is a revised and expanded version of our translation tutorial at #FAccT21. You can watch the video for that tutorial here: https://t.co/gp2qcTRgvl""]",https://arxiv.org/abs/2107.09163,"There has been a surge of recent interest in sociocultural diversity in machine learning (ML) research, with researchers (i) examining the benefits of diversity as an organizational solution for alleviating problems with algorithmic bias, and (ii) proposing measures and methods for implementing diversity as a design desideratum in the construction of predictive algorithms. Currently, however, there is a gap between discussions of measures and benefits of diversity in ML, on the one hand, and the broader research on the underlying concepts of diversity and the precise mechanisms of its functional benefits, on the other. This gap is problematic because diversity is not a monolithic concept. Rather, different concepts of diversity are based on distinct rationales that should inform how we measure diversity in a given context. Similarly, the lack of specificity about the precise mechanisms underpinning diversity's potential benefits can result in uninformative generalities, invalid experimental designs, and illicit interpretations of findings. In this work, we draw on research in philosophy, psychology, and social and organizational sciences to make three contributions: First, we introduce a taxonomy of different diversity concepts from philosophy of science, and explicate the distinct epistemic and political rationales underlying these concepts. Second, we provide an overview of mechanisms by which diversity can benefit group performance. Third, we situate these taxonomies--of concepts and mechanisms--in the lifecycle of sociotechnical ML systems and make a case for their usefulness in fair and accountable ML. We do so by illustrating how they clarify the discourse around diversity in the context of ML systems, promote the formulation of more precise research questions about diversity's impact, and provide conceptual tools to further advance research and practice. ",Diversity in Sociotechnical Machine Learning Systems
60,1417793989231747078,3351977373,Alex Clark,"['Ever wondered if you can find full photon temporal wavepacket indistinguishability from a cw excited emitter? See our new arXiv paper which compares pulsed and cw interference of photons from a DBT molecule! <LINK> @QSUMproject @ImperialPhysics @QETLabsBristol', '@igordownunder @QSUMproject @ImperialPhysics @QETLabsBristol In cw, the length will just shift the side dips in the histogram to different times. The only requirement is that they are far enough away from the central dip. There are photons at all times, and the histogram shows those that arrive simultaneously in the central dip.']",https://arxiv.org/abs/2107.09434,"The indistinguishability of successively generated photons from a single quantum emitter is most commonly measured using two-photon interference at a beam splitter. Whilst for sources excited in the pulsed regime the measured bunching of photons reflects the full wavepacket indistinguishability of the emitted photons, for continuous wave (cw) excitation the inevitable dependence on detector timing resolution and driving strength obscures the underlying photon interference process. Here we derive a method to extract the photon indistinguishability from cw measurements by considering the relevant correlation functions. The equivalence of both methods is experimentally verified through comparison of cw and pulsed excitation of an archetypal source of photons, a single molecule. ","Photon indistinguishability measurements under pulsed and continuous
  excitation"
61,1417712570647998473,145500767,Enrique Lopez Rodriguez,"['New paper w/ @asborlaff @suzIQUV from our Legacy Program on extragalactic magnetism: The gas streams of NGC1097 follow the B-field, which feeds the black hole with matter from the host galaxy. We used @ehtelescope techniques to estimate B-field modes. \n<LINK> <LINK>']",https://arxiv.org/abs/2107.09063,"Galactic bars are frequent in disk galaxies and they may support the transfer of matter towards the central engine of active nuclei. The barred galaxy NGC 1097 has magnetic forces controlling the gas flow at several kpc scales, which suggest that magnetic fields (B-fields) are dynamically important along the bar and nuclear ring. However, the effect of the B-field on the gas flows in the central kpc scale has not been characterized. Using thermal polarized emission at $89$ $\mu$m with HAWC+/SOFIA, here, we measure that the polarized flux is spatially located at the contact regions of the outer-bar with the starburst ring. The linear polarization decomposition analysis shows that the $89$ $\mu$m and radio ($3.5$ and $6.2$ cm) polarization traces two different modes, $m$, of the B-field: a constant B-field orientation and dominated by $m=0$ at $89$ $\mu$m, and a spiral B-field dominated by $m=2$ at radio. We show that the B-field at 89 $\mu$m is concentrated in the warmest region of a shock driven by the galactic-bar dynamics in the contact regions between the outer-bar with the starburst ring. Radio polarization traces a superposition of the spiral B-field outside and within the starburst ring. According to Faraday rotation measures between $3.5$ and $6.2$ cm, the radial component of the B-field along the contact regions points toward the galaxy's center on both sides. We conclude that gas streams outside and within the starburst ring follow the B-field, which feeds the black hole with matter from the host galaxy. ","Extragalactic magnetism with SOFIA (Legacy Program) -- II: A
  magnetically-driven flow in the starburst ring of NGC 1097"
62,1417658290553585665,370378404,Takehiko Yasuda,"['In my new paper, I discuss the isomorphism problem for projective schemes. A subject different from ones that I worked on previously.\n<LINK>']",https://arxiv.org/abs/2107.09277,"We discuss the isomorphism problem of projective schemes; given two projective schemes, can we algorithmically decide whether they are isomorphic? We give affirmative answers in the case of one-dimensional projective schemes, the case of smooth irreducible varieties with a big canonical sheaf or a big anti-canonical sheaf, and the case of K3 surfaces with a finite automorphism group. As related algorithmic problems, we also discuss decidability of positivity properties of invertible sheaves, and approximation of the nef cone and the pseudo-effective cone. ","The isomorphism problem of projective schemes and related algorithmic
  problems"
63,1417647947018379272,32965031,Sami Douba,"['New paper 🙃 Together with work of @agolian, Przytycki–Wise, Liu, and others, the main result implies that a closed aspherical 3-manifold admits a nonpositively curved Riemannian metric if and only if its fundamental group embeds in a compact Lie group. <LINK>']",https://arxiv.org/abs/2107.09490,"Let $\Gamma$ be a finitely generated group of matrices over $\mathbb{C}$. We construct an isometric action of $\Gamma$ on a complete CAT(0) space $X$ such that the restriction of this action to any subgroup of $\Gamma$ containing no nontrivial unipotent elements is well behaved. As an application, we show that if $M$ is a graph manifold that does not admit a nonpositively curved Riemannian metric, then any finite-dimensional $\mathbb{C}$-linear representation of $\pi_1(M)$ maps a nontrivial element of $\pi_1(M)$ to a unipotent matrix. In particular, the fundamental groups of such 3-manifolds do not admit any faithful finite-dimensional unitary representations. ",Proper CAT(0) actions of unipotent-free linear groups
64,1417638452602892291,11778512,Mason Porter,"['We have an exciting new methods paper on arXiv: <LINK>\n\nTitle: Analysis of Spatiotemporal Anomalies Using Persistent Homology: Case Studies with COVID-19 Data\n\nAuthors: Abigail Hickok, Deanna Needell, Mason A. Porter <LINK>']",https://arxiv.org/abs/2107.09188,"We develop a method for analyzing spatial and spatiotemporal anomalies in geospatial data using topological data analysis (TDA). To do this, we use persistent homology (PH), which allows one to algorithmically detect geometric voids in a data set and quantify the persistence of such voids. We construct an efficient filtered simplicial complex (FSC) such that the voids in our FSC are in one-to-one correspondence with the anomalies. Our approach goes beyond simply identifying anomalies; it also encodes information about the relationships between anomalies. We use vineyards, which one can interpret as time-varying persistence diagrams (which are an approach for visualizing PH), to track how the locations of the anomalies change with time. We conduct two case studies using spatially heterogeneous COVID-19 data. First, we examine vaccination rates in New York City by zip code at a single point in time. Second, we study a year-long data set of COVID-19 case rates in neighborhoods of the city of Los Angeles. ","Analysis of Spatial and Spatiotemporal Anomalies Using Persistent
  Homology: Case Studies with COVID-19 Data"
65,1417592232010469377,1093725312368762886,Caleb Miles,"['🚨 New arXiv paper 🚨\n<LINK>\n\nWe address a longstanding problem in testing for a mediated effect. Traditional tests of indirect effects are conservative and underpowered when the effects of exposure on mediator and mediator on outcome are both small-to-moderate.', ""Under ass'ns, the indirect effect is IDed by the product of regression coefs, so the H_0 of no indirect effect is delta_x*delta_y=0. This null gives rise to some unusual behavior. hat{delta}_x*hat{delta}_y doesn't converge uniformly even when their joint convergence is uniform: https://t.co/nFbBFKzCHO"", 'Additionally, the best performing traditional test rejects when the two tests of delta_x=0 and delta_y=0 each reject, but this is underpowered since the non-rejection regions overlap.\n\nWe instead propose two tests optimizing power wrt minimax and Bayes risk optimality criteria. https://t.co/oTmAPSM0Ez', ""1st: A minimax optimal test, which achieves its nominal type 1 error alpha exactly everywhere in H_0. Its power is then never &lt; alpha in the alternative space. It may look funny, but we show it's the unique test w this property in a set generated by a certain class of functions. https://t.co/UBEiDpOn5D"", '2nd: A Bayes risk optimal test. We adapt the sparse linear programming approach proposed in Rosenblum et al. (https://t.co/JtszDztjQ6) to our setting to get an approx. optimal test for the 0-1 loss. Another funny-looking rejection region! https://t.co/1r8jllfFVS', 'How do these tests compare in terms of performance? Almost identically as it turns out, but both dominate the joint significance test (the one that rejects both delta_x=0 and delta_y=0), especially for small-to-moderate effect sizes wrt sd. https://t.co/xbcfLwmxR3', ""Other fun things:\n✅ Allowance for exposure-mediator interaction\n✅ Nonstandard (&amp; general) def'n of p-values applied to these tests\n✅ Large-scale hyp. testing of indirect effects\n✅ Tests of products of &gt;2 coefficients based on Latin squares and (maybe?) hypercubes https://t.co/vxZLJTU2vX"", ""Lastly, software! We have an R package implementing the minimax optimal test (Bayes risk test to come). It's a bit of a work in progress, so there may still be some bugs to work out, but we're very excited about its potential for widespread practical use! https://t.co/UzmswyNPAE""]",https://arxiv.org/abs/2107.07575,"The indirect effect of an exposure on an outcome through an intermediate variable can be identified by a product of regression coefficients under certain causal and regression modeling assumptions. Thus, the null hypothesis of no indirect effect is a composite null hypothesis, as the null holds if either regression coefficient is zero. A consequence is that existing hypothesis tests are either severely underpowered near the origin (i.e., when both coefficients are small with respect to standard errors) or do not preserve type 1 error uniformly over the null hypothesis space. We propose hypothesis tests that (i) preserve level alpha type 1 error, (ii) meaningfully improve power when both true underlying effects are small relative to sample size, and (iii) preserve power when at least one is not. One approach gives a closed-form test that is minimax optimal with respect to local power over the alternative parameter space. Another uses sparse linear programming to produce an approximately optimal test for a Bayes risk criterion. We provide an R package that implements the minimax optimal test. ","Optimal tests of the composite null hypothesis arising in mediation
  analysis"
66,1417515887024689155,10471882,matt brehmer,"[""I'm pleased to introduce Diatoms, our new approach for generating #visualization design inspiration.\n\n📃 Check out our #ieeevis '21 paper (by myself, @eagereyes, and Carmen Hull): <LINK>\n\n🎬 explainer video: <LINK> <LINK>"", ""@JanWillemTulp @eagereyes Thanks! \n\nSeveral of the information design students who participated in our study did share and discuss their final glyph designs with us. \n\nWe didn't have space in the paper to show these, but with their permission I'd like to show some of them in the upcoming VIS talk :)"", '@eagereyes @antarcticdesign Thanks for the question, Eamonn! \n\nWe see our approach as being applicable to the early and divergent phases of vis design, being complementary to the later process of establishing a visual hierarchy, such as in your taxonomic approach (which was an important influence on us!)', ""@JanWillemTulp @eagereyes I'd be interested in that too. We've talked about doing more class-based activities w/ students. We also know that pro designers need more control over the initial palettes (beyond the examples used in the paper), so more work is needed before this technique is used in production"", '@JanWillemTulp @eagereyes and since VIS is virtual again this year, the talk will be publicly available online in October']",https://arxiv.org/abs/2107.09015,"We introduce Diatoms, a technique that generates design inspiration for glyphs by sampling from palettes of mark shapes, encoding channels, and glyph scaffold shapes. Diatoms allows for a degree of randomness while respecting constraints imposed by columns in a data table: their data types and domains as well as semantic associations between columns as specified by the designer. We pair this generative design process with two forms of interactive design externalization that enable comparison and critique of the design alternatives. First, we incorporate a familiar small multiples configuration in which every data point is drawn according to a single glyph design, coupled with the ability to page between alternative glyph designs. Second, we propose a small permutables design gallery, in which a single data point is drawn according to each alternative glyph design, coupled with the ability to page between data points. We demonstrate an implementation of our technique as an extension to Tableau featuring three example palettes, and to better understand how Diatoms could fit into existing design workflows, we conducted interviews and chauffeured demos with 12 designers. Finally, we reflect on our process and the designers' reactions, discussing the potential of our technique in the context of visualization authoring systems. Ultimately, our approach to glyph design and comparison can kickstart and inspire visualization design, allowing for the serendipitous discovery of shape and channel combinations that would have otherwise been overlooked. ",Generative Design Inspiration for Glyphs with Diatoms
67,1417514163438424068,1025939776401158144,Alexander J. Sutherland,"[""Hey folks, I'm excited to say that I have a new paper on the arXiv today (<LINK>), so here's a 🧵! \n\nResolvent degree is an invariant that measures certain types of complexity (for field extensions, [branched] covers of varieties, groups, etc.) | 1/12"", 'When you see RD(n), you should think ""this is how hard it is to solve a generic degree n polynomial."" \n\nThe tools I use come from algebraic geometry. I think they are quite fun and are relatively easy to understand! | 2/12', 'For more on the basics of resolvent degree, check out this introductory thread I wrote last week!\n\nAs a bonus at the end of thread, I include a link to a Quanta article about resolvent degree for a general mathematical audience | 3/12\n\nhttps://t.co/TrttO8FjaT', 'RD(n) is very difficult to compute. By definition, RD(n)≥1, but there are currently no non-trivial lower bounds (it is possible that RD(n)=1 for all n).\n\nBut there has been a history of determining upper bounds!\n\nIn 1945, Segre asked about the large n behavior of RD(n). | 4/12', 'The previous best upper bounds on RD(n) were determined by Wolfson (in  https://t.co/ohzSKKKs5W), where he constructs a function F(m) such that RD(n)≤n-m when n≥F(m).\n\nI construct a similar function G(m) whose key properties are summarized in the following theorem: | 5/12 https://t.co/J3cJp1eg2K', 'The construction of G is split into two methods. For large n (the general case), I improve upon the algorithm of Wolfson by using a result of Debarre and Manivel.\n\nFor the remainder of the thread, I want to briefly talk about how I do the small n cases. | 6/12', 'Given a degree d hypersurface H ⊆ ℙ_K^r (with K a finite-type ℂ-algebra) and a point P ∈ H(K), we can compute d-1 other hypersurfaces (known as the polars of H at P).\n\nFact: The intersection of H and these polars is a cone contained in H with vertex P! | 7/12', 'So, I call that intersection the polar cone of H at P.\n\nImportantly, any point Q≠P in the polar cone determines a line on H!\n\nThis fact was known to Segre, who in fact gives a book of Bertini as a reference, but is largely unknown today! | 8/12', 'The only modern reference I know which talks about polars is https://t.co/x5wq3k4MmT. \n\nI expand this construction to iterated polar cones, which allow one to construct not just lines on hypersurfaces, but k-planes on intersections of hypersurfaces! | 9/12', 'In particular, using these methods, I am able to prove RD(n)≤n-6 for n≥21, which addresses a gap in an argument given by Chebotarev in 1954.\n\nSimilarly, these methods can prove geometrically that RD(n)≤n-5 for n≥9, which fixes a gap in an argument by Wiman in 1927.* | 10/12', 'As part of this project, I translated the respective articles of Wiman and Chebotarev from German (resp. Russian) to English and with more modern terminology.\n\nI have put these translations on the arXiv as well (https://t.co/GQsp96FCeA, resp. https://t.co/d7siZ0aTAY). | 11/12', '*Dixmier gave a purely algebraic proof of this fact in 1993, in the appendix of https://t.co/z4OVK0taBP\n\nThanks for taking the time to read this thread and feel free to leave any comments / questions below! | 12/12']",https://arxiv.org/abs/2107.08139,"For each $n$, let $\text{RD}(n)$ denote the minimum $d$ for which there exists a formula for the general polynomial of degree $n$ in algebraic functions of at most $d$ variables. In 1945, Segre called for a better understanding of the large $n$ behavior of $\text{RD}(n)$. In this paper, we provide improved thresholds for upper bounds on $\text{RD}(n)$. Our techniques build upon classical algebraic geometry to provide new upper bounds for small $n$ and, in doing so, fix gaps in the proofs of A. Wiman and G.N. Chebotarev in [Wim1927] and [Che1954]. ",Upper Bounds on Resolvent Degree and Its Growth Rate
68,1417496293602205707,905075448798998530,Nikola Milojević-Dupont,"['Check out our new preprint:\n\nwe gathered and analyzed open geospatial data on buildings worldwide trying to understand current practices and how useful is current data for urban sustainability research.\n\nPaper: <LINK>\n\nWebsite: <LINK> <LINK>', 'Why? Maps of buildings footprint, age, type, height.. are primordial to transform cities to improve social and environmental sustainability, resilience to climate change, etc. But current data are fragmented, making it difficult to identify what is available &amp; where.', 'We have identified and benchmarked more than 140 government releases from 28 countries containing above 100 million buildings, based on five dimensions:\n\naccessibility, richness, data quality, harmonization, and relationships with other actors\n\nSummary table - end of manuscript👇 https://t.co/lUpKuYhfKT', 'TL;DR:\n\n1. data on buildings are increasingly available but mostly limited to the global north\n\n2. many datasets are useful for sustainability research but broader adoption of best practices is needed for higher relevance to most timely issues (urbanization, decarbonization,...)', '3. the heterogeneity in current practices generates difficulties to access and use the data even for advanced users\n    \n4. OSM and government data complement each other in many regions: there is a large potential from further integration https://t.co/m50sO0TUxq']",https://arxiv.org/abs/2107.04023,"As buildings are central to the social and environmental sustainability of human settlements, high-quality geospatial data are necessary to support their management and planning. Authorities around the world are increasingly collecting and releasing such data openly, but these are mostly disconnected initiatives, making it challenging for users to fully leverage their potential for urban sustainability. We conduct a global study of 2D geospatial data on buildings that are released by governments for free access, ranging from individual cities to whole countries. We identify and benchmark more than 140 releases from 28 countries containing above 100 million buildings, based on five dimensions: accessibility, richness, data quality, harmonisation, and relationships with other actors. We find that much building data released by governments is valuable for spatial analyses, but there are large disparities among them and not all instances are of high quality, harmonised, and rich in descriptive information. Our study also compares authoritative data to OpenStreetMap, a crowdsourced counterpart, suggesting a mutually beneficial and complementary relationship. ","Open government geospatial data on buildings for planning sustainable
  and resilient cities"
69,1417418545105887232,1655032190,Enrico Corsaro,['New paper out today about estimating a proper Rossby number using asteroseismology!\n<LINK>'],https://arxiv.org/abs/2107.08551,"Stellar activity and rotation are tightly related in a dynamo process. Our understanding of this mechanism is mainly limited by our capability of inferring the properties of stellar turbulent convection. In particular, the convective turnover time is a key ingredient through the estimation of the stellar Rossby number, which is the ratio of the rotation period and the convective turnover time. In this work we propose a new calibration of the $(B-V)$ color index dependence of the convective turnover time, hence of the stellar Rossby number. Our new calibration is based on the stellar structure properties inferred through the detailed modeling of solar-like pulsators using asteroseismic observables. We show the impact of this calibration in a stellar activity -- Rossby number diagram by applying it to a sample of about 40,000 stars observed with Kepler and for which photometric activity proxy $S_\mathrm{\!ph}$ and surface rotation periods are available. Additionally, we provide a new calibration of the convective turnover time as function of the $(G_\mathrm{BP}-G_\mathrm{RP})$ color index for allowing applicability in the ESA Gaia photometric passbands. ",A calibration of the Rossby number from asteroseismology
70,1417412598245609489,1153677180842467333,Roman Kolcun,"['In our new paper, we revisit popular ML models for IoT device identification and show how their accuracy degrades over time. Paper will appear in TMA 2021. Joined work with @diana_popescu_ Vadim Safronov @pooyadav @mort___ @ammandalari and @realhamed \n<LINK>']",https://arxiv.org/abs/2107.07818,"Internet-of-Things (IoT) devices are known to be the source of many security problems, and as such, they would greatly benefit from automated management. This requires robustly identifying devices so that appropriate network security policies can be applied. We address this challenge by exploring how to accurately identify IoT devices based on their network behavior, while leveraging approaches previously proposed by other researchers. We compare the accuracy of four different previously proposed machine learning models (tree-based and neural network-based) for identifying IoT devices. We use packet trace data collected over a period of six months from a large IoT test-bed. We show that, while all models achieve high accuracy when evaluated on the same dataset as they were trained on, their accuracy degrades over time, when evaluated on data collected outside the training set. We show that on average the models' accuracy degrades after a couple of weeks by up to 40 percentage points (on average between 12 and 21 percentage points). We argue that, in order to keep the models' accuracy at a high level, these need to be continuously updated. ",Revisiting IoT Device Identification
71,1417387736550490112,1345856121660178433,Annabelle Bohrdt,['Our paper on cold atom realizations of the Fermi-Hubbard model — part review 📚 part proposal for exciting new directions 🗺 🧭— is finally out! <LINK> <LINK>'],https://arxiv.org/abs/2107.08043,"In the last decade, quantum simulators, and in particular cold atoms in optical lattices, have emerged as a valuable tool to study strongly correlated quantum matter. These experiments are now reaching regimes that are numerically difficult or impossible to access. In particular they have started to fulfill a promise which has contributed significantly to defining and shaping the field of cold atom quantum simulations, namely the exploration of doped and frustrated quantum magnets and the search for the origins of high-temperature superconductivity in the fermionic Hubbard model. Despite many future challenges lying ahead, such as the need to further lower the experimentally accessible temperatures, remarkable studies have already emerged. Among them, spin-charge separation in one-dimensional systems has been demonstrated, extended-range antiferromagnetism in two-dimensional systems has been observed, connections to modern day large-scale numerical simulations were made, and unprecedented comparisons with microscopic trial wavefunctions have been carried out at finite doping. In many regards, the field has acquired new realms, putting old ideas to a new test and producing new insights and inspiration for the next generation of physicists. In the first part of this paper, we review the results achieved in cold atom realizations of the Fermi-Hubbard model in recent years. In the second part of this paper, with the stage set and the current state of the field in mind, we propose a new direction for cold atoms to explore: namely mixed-dimensional bilayer systems, where the charge motion is restricted to individual layers which remain coupled through spin-exchange. We propose a novel, strong pairing mechanism in these systems, which puts the formation of hole pairs at experimentally accessible, elevated temperatures within reach. ",Exploration of doped quantum magnets with ultracold atoms
72,1417307799931949089,1015190157845258240,Yuji Matsumoto,"['Our new paper <LINK> !\nTakahashi &amp; Muto (2018) showed that the dust ring forms due to magnetically driven disk winds.\nWe found that in wind-driven disks, inner crystalline dust particles move outward and the dust ring is composed of crystalline dust.']",https://arxiv.org/abs/2107.08370,"We present a novel mechanism for the outward transport of crystalline dust particles: the outward radial drift of pebbles. The dust ring structure is frequently observed in protoplanetary disks. One of the plausible mechanisms of the formation of dust rings is the accumulation of pebbles around the pressure maximum, which is formed by the mass loss due to magnetically driven disk winds. In evolving protoplanetary disks due to magnetically driven disk winds, dust particles can migrate outwardly from the crystallization front to the pressure maximum by radial drift. We found that the outward radial drift process can transport crystalline dust particles efficiently when the radial drift timescale is shorter than the advection timescale. Our model predicts that the crystallinity of silicate dust particles could be as high as 100% inside the dust ring position. ","On the crystallinity of silicate dust in evolving protoplanetary disks
  due to magnetically driven disk winds"
73,1417171486423588873,3344838844,Erini Lambrides,"['New paper up on the arxiv (<LINK>) and I am defending tomorrow! <LINK>', ""If you'd like to attend my defense virtually, shoot me a dm and I'll send ya the zoom link! Title below!\n\nBlinded by the Light: Connecting the Growth of Super-Massive Black Holes and Galaxy Evolution""]",https://arxiv.org/abs/2107.07533,"For over 60 years, the scientific community has studied actively growing central super-massive black holes (active galactic nuclei -- AGN) but fundamental questions on their genesis remain unanswered. Numerical simulations and theoretical arguments show that black hole growth occurs during short-lived periods ($\sim$ 10$^{7}$ -10$^{8}$ yr) of powerful accretion. Major mergers are commonly invoked as the most likely dissipative process to trigger the rapid fueling of AGN. If the AGN-merger paradigm is true, we expect galaxy mergers to coincide with black hole accretion during a heavily obscured AGN phase (N$_H$ $ > 10^{23}$ cm$^{-2}$). Starting from one of the largest samples of obscured AGN at 0.5 $<$ $z$ $<$ 3.1, we select 40 non-starbursting lower-luminosity obscured AGN. We then construct a one-to-one matched redshift- and near-IR magnitude-matched non-starbursting inactive galaxy control sample. Combining deep color \textit{Hubble Space Telescope} imaging and a novel method of human classification, we test the merger-AGN paradigm prediction that heavily obscured AGN are strongly associated with galaxies undergoing a major merger. On the total sample of 80 galaxies, we estimate each individual classifier's accuracy at identifying merging galaxies/post-merging systems and isolated galaxies. We calculate the probability of each galaxy being in either a major merger or isolated system, given the accuracy of the human classifiers and the individual classifications of each galaxy. We do not find statistically significant evidence that obscured AGN at cosmic noon are predominately found in systems with evidence of significant merging/post-merging features. ","Lower-Luminosity Obscured AGN Host Galaxies are Not Predominantly in
  Major-Merging Systems at Cosmic Noon"
74,1417135351592800256,724917570705461248,Jan van Roestel,"['On ArXiv today; our paper on 5 new eclipsing AM CVn systems discovered with @ZTFsurvey <LINK> \nAM CVn systems are a rare type of accreting white dwarf with a degenerate donor.', '@ztfsurvey We found these 5 systems by analysing the ZTF lightcurves of all white dwarfs (identified using Gaia). We specifically searched for dimming events that occurred with a period of &lt;70 minutes. We estimate that, as ZTF continues to observe, 1-4 more can be found. https://t.co/H41J3vNAd0', 'Eclipsing systems are very valuable because they allow the mass and radius of both components to be measured (mostly) independent of models. This shows that the white dwarf are more massive than typical single white dwarfs (~0.8 solar mass instead of ~0.5).', 'Even more interesting, the donor stars with a mass of just 1-4% of a solar mass seem to be inflated and large. Comparison with models suggests that they had Helium stars as a progenitor, but the long period systems are still large compared to these models. https://t.co/fwAP8m2NMp', 'They are spectroscopically also very interesting because we are seeing the elements from the core-remnant of a star! Besides helium, a lot of metal lines including potassium and zinc which have not been seen before in AM CVn systems https://t.co/gCV1J1j0mT']",https://arxiv.org/abs/2107.07573,"AM CVn systems are ultra-compact, helium-rich, accreting binaries with degenerate or semi-degenerate donors. We report the discovery of five new eclipsing AM CVn systems with orbital periods of 61.5, 55.5, 53.3, 37.4, and 35.4 minutes. These systems were discovered by searching for deep eclipses in the Zwicky Transient Facility (ZTF) lightcurves of white dwarfs selected using Gaia parallaxes. We obtained phase-resolved spectroscopy to confirm that all systems are AM CVn binaries, and we obtained high-speed photometry to confirm the eclipse and characterize the systems. The spectra of two long-period systems (61.5 and 53.3 minutes) show many emission and absorption lines, indicating the presence of N, O, Na, Mg, Si, and Ca, and also the K and Zn, elements which have never been detected in AM CVn systems before. By modelling the high-speed photometry, we measured the mass and radius of the donor star, potentially constraining the evolutionary channel that formed these AM CVn systems. We determined that the average mass of the accreting white dwarf is $\approx0.8$$\mathrm{M_{\odot}}$, and that the white dwarfs in long-period systems are hotter than predicted by recently updated theoretical models. The donors have a high entropy and are a factor of $\approx$ 2 more massive compared to zero-entropy donors at the same orbital period. The large donor radius is most consistent with He-star progenitors, although the observed spectral features seem to contradict this. The discovery of 5 new eclipsing AM~CVn systems is consistent with the known observed AM CVn space density and estimated ZTF recovery efficiency. Based on this estimate, we expect to find another 1--4 eclipsing AM CVn systems as ZTF continues to obtain data. This will further increase our understanding of the population, but will require high precision data to better characterize these 5 systems and any new discoveries. ",Discovery and characterization of five new eclipsing AM CVn systems
75,1417128129198530561,1358707016,Will Barnes,"['☀️ New paper alert ☀️\n\nOur latest paper on active region heating has been accepted to ApJ! The preprint is availlable on arXiv now: <LINK>. All of the relevant code and data needed to reproduce the paper is available here: <LINK> <LINK>', 'In this paper, we trained a machine learning model using model predictions from our earlier paper to classify the frequency of energy release based on observables from AIA. This allowed us to map the heating frequency across an observed active region! (see picture above)', 'This represents an important step in doing model-data comparisons. Understanding the underlying physics of coronal heating means simultaneously satisfying multiple observable constraints. Forward modeling + ML provide a framework for doing this systematically and at scale!', 'Perhaps most importantly, this means the last chapter of my thesis is finally published (and only 2 years too late! 😅). You can read more about the forward modeling side of this study in our first paper from 2019: https://t.co/0bj6aakyIO.']",https://arxiv.org/abs/2107.07612,"Constraining the frequency of energy deposition in magnetically-closed active region cores requires sophisticated hydrodynamic simulations of the coronal plasma and detailed forward modeling of the optically-thin line-of-sight integrated emission. However, understanding which set of model inputs best matches a set of observations is complicated by the need for any proposed heating model to simultaneously satisfy multiple observable constraints. In this paper, we train a random forest classification model on a set of forward-modeled observable quantities, namely the emission measure slope, the peak temperature of the emission measure distribution, and the time lag and maximum cross-correlation between multiple pairs of AIA channels. We then use our trained model to classify the heating frequency in every pixel of active region NOAA 1158 using the observed emission measure slopes, peak temperatures, time lags, and maximum cross-correlations and are able to map the heating frequency across the entire active region. We find that high-frequency heating dominates in the inner core of the active region while intermediate frequency dominates closer to the periphery of the active region. Additionally, we assess the importance of each observed quantity in our trained classification model and find that the emission measure slope is the dominant feature in deciding with which heating frequency a given pixel is most consistent. The technique presented here offers a very promising and widely applicable method for assessing observations in terms of detailed forward models given an arbitrary number of observable constraints. ","Understanding Heating in Active Region Cores through Machine Learning
  II. Classifying Observations"
76,1417047877881417728,995032097806082049,Tarje NM,"['New preprint by PhD student Ben Moseley on ""Finite-basis physics-informed neural networks"" (FBPINNs); a novel framework for  solving a variety of differential equations with a focus on scalability for multi-/large- problems. Paper and code to follow soon!\n\n<LINK>']",https://arxiv.org/abs/2107.07871,"Recently, physics-informed neural networks (PINNs) have offered a powerful new paradigm for solving problems relating to differential equations. Compared to classical numerical methods PINNs have several advantages, for example their ability to provide mesh-free solutions of differential equations and their ability to carry out forward and inverse modelling within the same optimisation problem. Whilst promising, a key limitation to date is that PINNs have struggled to accurately and efficiently solve problems with large domains and/or multi-scale solutions, which is crucial for their real-world application. Multiple significant and related factors contribute to this issue, including the increasing complexity of the underlying PINN optimisation problem as the problem size grows and the spectral bias of neural networks. In this work we propose a new, scalable approach for solving large problems relating to differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are inspired by classical finite element methods, where the solution of the differential equation is expressed as the sum of a finite set of basis functions with compact support. In FBPINNs neural networks are used to learn these basis functions, which are defined over small, overlapping subdomains. FBINNs are designed to address the spectral bias of neural networks by using separate input normalisation over each subdomain, and reduce the complexity of the underlying optimisation problem by using many smaller neural networks in a parallel divide-and-conquer approach. Our numerical experiments show that FBPINNs are effective in solving both small and larger, multi-scale problems, outperforming standard PINNs in both accuracy and computational resources required, potentially paving the way to the application of PINNs on large, real-world problems. ","Finite Basis Physics-Informed Neural Networks (FBPINNs): a scalable
  domain decomposition approach for solving differential equations"
77,1417040427694366720,1031884432079241218,Varignon Julien,"['Our new paper on band gaps in 3d perovskite oxides is available on ArXiv. If you want to know how magnetism affects the band gap amplitude, have a look here : <LINK>']",https://arxiv.org/abs/2107.07794,"Understanding the controlling principles of band gaps trends in d electron perovskites is needed both for gauging metal-insulator transitions, as well as their application in catalysis and doping. The magnitude of this band gap is rather different for different magnetic spin configurations. We find via electronic structure theory that the factors that connect gapping magnitudes to magnetism depend on the nature of the band edge orbital character (BEOC) and surprisingly scale with the number of antiferromagnetic contacts z$_i$ between neighboring transition metal ions. The dependence is weak when the BEOC are (d,d)-like (""Mott insulators""), whereas this dependence is rather strong in (p,d)-like (""charge transfer"" insulators). These unexpected rules are traced to the reduced orbital interactions through the increase in the number of antiferromagnetic contacts between transition metal ions. The impact of magnetic order is not limited to the band gap magnitude and includes also the magnitude of lattice distortions connected to the electronic structure. These results highlight the importance of establishing in electronic structure theory of gap-related phenomena (doping, transport, metal-insulator transitions, conductive interfaces) the appropriate magnetic order. ",Dependence of band gaps in d-electron perovskite oxides on magnetism
78,1417032787488616450,301426952,Arttu Rajantie 🇪🇺 🇫🇮 #FBPE,"['New paper ""Stochastic isocurvature constraints for axion dark matter with high-scale inflation"" with Liina Jukko, based on her MSc dissertation\n<LINK>', 'Axions and axion-like particles are promising candidates for dark matter. They are constrained by primordial isocurvature perturbations. We show that for high inflationary Hubble rates (&gt;10^12 GeV), standard perturbative methods fail. We use the stochastic method instead.']",https://arxiv.org/abs/2107.07948,"Axions are among the best motivated dark matter candidates. Their production in the early Universe by the vacuum misalignment mechanism gives rise to isocurvature perturbations, which are constrained by cosmic microwave background measurements. In this paper, we compute the axion isocurvature power spectrum using spectral expansion in the stochastic Starobinsky-Yokoyama formalism, which captures non-linear effects in the axion dynamics. In contrast to most of the existing literature, we focus on high inflationary Hubble rates of order $10^{13}~{\rm GeV}$, and demonstrate that there is a significant window in which axions can account for all or part of the dark matter abundance without violating the isocurvature bounds or tensor mode bounds. Crucially, we find that the isocurvature spectrum is dominated by non-perturbative contributions in a large part of this window. Therefore the commonly used linear approximation is not reliable in this region, making the stochastic approach essential. ","Stochastic isocurvature constraints for axion dark matter with
  high-scale inflation"
79,1417024456040411143,4716962310,Li Junnan,"['Excited to release Align before Fuse (ALBEF), a new vision-language pre-training method with SoTA performance on multiple V+L tasks! We provide code and models for pre-training and finetuning: <LINK>\nBlog: <LINK>\nPaper: <LINK>']",https://arxiv.org/abs/2107.07651,"Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. In order to improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR$^2$, ALBEF achieves absolute improvements of 2.37% and 3.84% compared to the state-of-the-art, while enjoying faster inference speed. Code and pre-trained models are available at this https URL ","Align before Fuse: Vision and Language Representation Learning with
  Momentum Distillation"
80,1416965721020526596,1176335023818997760,Madeleine Zurowski,"['New paper on the arxiv today! It’s based around assessing the model independent test of DAMA’s results. We look at how the target mass and background of an experiment will influence its ability to observe the apparent annual modulation 1/2 <LINK>', 'And find that even though detectors with the same target material (NaI) are sensitive to the same DM interactions, a higher background will strongly constrain your ability to distinguish a modulating signal, even with a larger target mass. Bigger doesn’t always mean better!', '(I said /2 but I lied) This result might seem kind of obvious and trivial, but given COSINE and ANAIS have both reported no obvious modulation, I think it’s important to understand how rock solid that observation (or lack thereof) is as DM rejection before digging DAMA’s grave']",https://arxiv.org/abs/2107.07674,"We present here the model dependent and independent sensitivity studies for NaI detectors designed to test the DAMA result, and compare the predicted limits from SABRE with the present performance of both ANAIS and COSINE. We find that the strongest discovery and exclusion limits are set by a detector with the lowest background (assuming equal run times), and also note that our method correctly computes the present exclusion limits previously published by ANAIS and COSINE. In particular, with a target mass of 50 kg and background rate of 0.36 cpd/kg/keV (after veto), SABRE will be able to exclude the DAMA signal with 3$\sigma$ confidence or `discover' it with 5$\sigma$ confidence within 2 years. This strongly motivates the quest for ever lower backgrounds in NaI detectors. ",Influence of NaI background and mass on testing the DAMA modulation
81,1416961562674286593,1055973579467059200,Rajsekhar Mohapatra,['We have a new paper out on arxiv today! We have conducted high-resolution simulations on the multiphase intracluster medium in galaxy halos. \nCheck it out at:  <LINK>\nSimulation movies at: <LINK>.'],https://arxiv.org/abs/2107.07722,"Turbulence in the intracluster medium (ICM) is driven by active galactic nuclei (AGNs) jets, by mergers, and in the wakes of infalling galaxies. It not only governs gas motion but also plays a key role in the ICM thermodynamics. Turbulence can help seed thermal instability by generating density fluctuations, and mix the hot and cold phases together to produce intermediate temperature gas ($10^4$--$10^7$ $\mathrm{K}$) with short cooling times. We conduct high resolution ($384^3$--$768^3$ resolution elements) idealised simulations of the multiphase ICM and study the effects of turbulence strength, characterised by $f_{\mathrm{turb}}$ ($0.001$--$1.0$), the ratio of turbulent forcing power to the net radiative cooling rate. We analyse density and temperature distribution, amplitude and nature of gas perturbations, and probability of transitions across the temperature phases. We also study the effects of mass and volume-weighted thermal heating and weak ICM magnetic fields. For low $f_{\mathrm{turb}}$, the gas is distribution is bimodal between the hot and cold phases. The mixing between different phases becomes more efficient with increasing $f_{\mathrm{turb}}$, producing larger amounts of the intermediate temperature gas. Strong turbulence ($f_{\mathrm{turb}}\geq0.5$) generates larger density fluctuations and faster cooling, The rms logarithmic pressure fluctuation scaling with Mach number $\sigma_{\ln{\bar{P}}}^2\approx\ln(1+b^2\gamma^2\mathcal{M}^4)$ is unaffected by thermal instability and is the same as in hydro turbulence. In contrast, the density fluctuations characterised by $\sigma_s^2$ are much larger, especially for $\mathcal{M}\lesssim0.5$. In magnetohydrodynamic runs, magnetic fields provide significant pressure support in the cold phase but do not have any strong effects on the diffuse gas distribution, and nature and amplitude of fluctuations. ","Characterising the turbulent multiphase halos with periodic box
  simulations"
82,1416951323954647042,1079172757009514496,Kristina Wicke,['New paper on the complexity of optimising variants of phylogenetic diversity on phylogenetic networks. Joint work with Magnus Bordewich &amp; Charles Semple. \n\n<LINK>'],https://arxiv.org/abs/2107.07834,"Phylogenetic Diversity (PD) is a prominent quantitative measure of the biodiversity of a collection of present-day species (taxa). This measure is based on the evolutionary distance among the species in the collection. Loosely speaking, if $\mathcal{T}$ is a rooted phylogenetic tree whose leaf set $X$ represents a set of species and whose edges have real-valued lengths (weights), then the PD score of a subset $S$ of $X$ is the sum of the weights of the edges of the minimal subtree of $\mathcal{T}$ connecting the species in $S$. In this paper, we define several natural variants of the PD score for a subset of taxa which are related by a known rooted phylogenetic network. Under these variants, we explore, for a positive integer $k$, the computational complexity of determining the maximum PD score over all subsets of taxa of size $k$ when the input is restricted to different classes of rooted phylogenetic networks ","On the Complexity of Optimising Variants of Phylogenetic Diversity on
  Phylogenetic Networks"
83,1416126820898983942,1318712202241736704,Saeedehparsaeefard,['Our new paper on transfer learning + 6G <LINK>'],https://arxiv.org/abs/2107.05728,"6G networks will greatly expand the support for data-oriented, autonomous applications for over the top (OTT) and networking use cases. The success of these use cases will depend on the availability of big data sets which is not practical in many real scenarios due to the highly dynamic behavior of systems and the cost of data collection procedures. Transfer learning (TL) is a promising approach to deal with these challenges through the sharing of knowledge among diverse learning algorithms. with TL, the learning rate and learning accuracy can be considerably improved. However, there are implementation challenges to efficiently deploy and utilize TL in 6G. In this paper, we initiate this discussion by providing some performance metrics to measure the TL success. Then, we show how infrastructure, application, management, and training planes of 6G can be adapted to handle TL. We provide examples of TL in 6G and highlight the spatio-temporal features of data in 6G that can lead to efficient TL. By simulation results, we demonstrate how transferring the quantized neural network weights between two use cases can make a trade-off between overheads and performance and attain more efficient TL in 6G. We also provide a list of future research directions in TL for 6G. ",Toward Efficient Transfer Learning in 6G
84,1416087577220816897,40285266,Stanislav Fort at EAGx Prague ¬(🔥📎🔥📎),['Our new paper <LINK> led by @_BrettLarsen is out on arXiv! <LINK>'],https://arxiv.org/abs/2107.05802,"A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We analyze this phenomenon for random subspaces by first examining the success probability of hitting a training loss sub-level set when training within a random subspace of a given training dimensionality. We find a sharp phase transition in the success probability from $0$ to $1$ as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of properties of the high-dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sub-level set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total parameters, implying by our theory that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sub-level sets is very large. Moreover, we compare this threshold training dimension to more sophisticated ways of reducing training degrees of freedom, including lottery tickets as well as a new, analogous method: lottery subspaces. Code is available at this https URL ","How many degrees of freedom do we need to train deep networks: a loss
  landscape perspective"
85,1416078313446166531,34488432,Joel Z Leibo,['Very excited to announce Melting Pot: a large suite for testing how well agent populations trained by multi-agent RL can generalize to new social situations. \n\ngithub: <LINK>\nblog: <LINK>\npaper: <LINK>\n\n#ICML2021 <LINK>'],https://arxiv.org/abs/2107.06857,"Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone. ","Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting
  Pot"
86,1416064788577259529,34078684,Dipanjan Das,"['End-to-end dialogue models often generate responses about the world that are not ""faithful"" to evidence in grounding corpora.  We present new work on controlling these responses to be attributable to such evidence.\n\nPaper: <LINK>\nAbs: <LINK>\n\n1/', 'Here is an example dialogue from Wizard of Wikipedia that switches back and forth between personal experience and a response pertaining to the real world.  For the latter, we believe it is important for the response to be supported by a given grounding document. 2/ https://t.co/FD569NnRgm', 'Our approach is straightforward:  we add features to control for support given the grounding document, and present a resampling technique that satisfies faithfulness metrics, avoiding content ""hallucination"" as much as possible. 3/ https://t.co/e7e9vZjzQy', 'On human evaluations on WoW, we obtain strong results across all pertinent metrics. 4/ https://t.co/84di4au7bU', ""I am tweeting this on behalf of my co-authors @hjrashkin, @davidswelt and @gtomar_google (who are much less active than I am).  This was Hannah's first project after she joined Google last year -- the paper will appear at ACL 2021 shortly.  5/"", 'Stay tuned for more work in this area in the upcoming months and years.  6/6']",https://arxiv.org/abs/2107.06963,"Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. At training time, additional inputs based on these evaluation measures are given to the dialogue model. At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. We also investigate the usage of additional controls at decoding time using resampling techniques. In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems. ","Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable
  Features"
87,1416056286081343494,3860113272,Ron Belmont,"['We have new paper out with a friendly reminder that non-flow subtraction breaks closure badly at RHIC energies <LINK>', '@profdvp Thank you so much! It’s been a long road…']",https://arxiv.org/abs/2107.07287,"Recently the PHENIX Collaboration has made available two-particle correlation Fourier coefficients for multiple detector combinations in minimum bias p+p and 0-5% central p+Au, d+Au, 3He+Au collisions at 200 GeV [1]. Using these coefficients for three sets of two-particle correlations, azimuthal anisotropy coefficients $v_2$ and $v_3$ are extracted for midrapidity charged hadrons as a function of transverse momentum. In this paper, we use the available coefficients to explore various non-flow hypotheses as well as compare the results with theoretical model calculations. The non-flow methods fail basic closure tests with AMPT and PYTHIA/ANGANTYR, particularly when including correlations with particles in the low multiplicity light-projectile going direction. In data, the non-flow adjusted $v_2$ results are modestly lower in p+Au and the adjusted $v_3$ results are more significantly higher in p+Au and d+Au. However, the resulting higher values for the ratio $v_3/v_2$ in p+Au at RHIC compared to p+Pb at the LHC is additional evidence for a significant over-correction. Incorporating these additional checks, the conclusion that these flow coefficients are dominated by initial geometry coupled with final-state interactions (e.g.~hydrodynamic expansion of quark-gluon plasma) remains true, and explanations based on initial-state glasma are ruled out. The detailed balance between intrinsic and fluctuation-driven geometry and the exact role of weakly versus strongly-coupled pre-hydrodynamic evolution remains an open question for triangular flow, requiring further theoretical and experimental investigation. ","Checking Non-Flow Assumptions and Results via PHENIX Published
  Correlations in $p$$+$$p$, $p$$+$Au, $d$$+$Au, $^3$He$+$Au at $\sqrt{s_{NN}}$
  = 200 GeV"
88,1416049118028390400,2550133394,Mostafa Dehghani,"['1. Benchmarks are fundamental to track progress in empirical machine learning. In our new paper, we study how benchmarking may affect the long term research direction and pace of progress in ML and put forward the notion of a ""benchmark lottery"":\n<LINK>', '2. This\xa0is a join work with\xa0@ytay017, @agritsenko, Zhe Zhao, @neilhoulsby, @841io, @metzlerd, and @OriolVinyalsML.', '3. We start by studying the ""task selection bias"" in multiple established benchmarks. We show how relative performance of algorithms is affected by the task selection process and discuss how merely ranking models based on an aggregated score can lead to suboptimal conclusions.', '4. Next we take a people perspective and share our opinion on how community bias contributes to the benchmark lottery, e.g.  via the review process by demanding SOTA on specific benchmarks or undermining advantage of new ideas from different aspects, like efficiency and fairness.', '5. The main concern with respect to the community bias is that research is becoming too incremental and biased toward the common expectations, since a completely new approach will initially have a hard time competing against established and carefully fine-tuned methods.', '6. In our paper, we also argue that benchmarks are stateful entities and that participation in a benchmark differs vastly depending upon its state.  For instance, getting top score in shared tasks at a later stage is vastly different from the time of its inception.', '7. This is because usually as time passes, the landscape of research with respect to a specific benchmark fills with tricks and complicated and specialized strategies to secure good scores.', '8. These adapted recipes are not always universal and may be applicable only to narrow tasks or setups, which might kill the potential of scoring high for an ""out of hype"" idea. We also bring some discussions on potential problems of the continual re-use of the same benchmark.', '9. We also talk about ""rigging the lottery"", the issue that some communities face, where the lack of well-established community driven sets of benchmarks or clear guidelines may inadvertently enable researchers to fit benchmarks to models.', '10. We highlight cases reinforcing this situation like high computational costs of proper evaluation on some benchmarks, or when the root cause is of behavioral nature, where researchers prefer to showcase only what their method shines at - oftentimes to avoid negative reviews.', '11. Finally, we present suggestions for improving the idea benchmarking process in ways that make it less of a lottery, like investing in making clear guidelines for making and using benchmarks (e.g. regulating hyper-parameter tuning budget) as well as the review process.', '12. Moreover, we talk about how using statistical significance testing, analysing sources of variance and going beyond single train/test splits can help.', '13. We also argue that measuring progress can sometimes be chasing a moving target since the meaning of progress might change as the research landscape evolves. As a potential solution we discuss promoting living benchmarks that also helps avoiding “creeping overfitting”.', '14. If a benchmark constantly evolves, for instance, adds new examples, adds new tasks, deprecates older data, and fixes labeling mistakes, it is less prone to “tricks” and highly robust models would find themselves consistently doing well across versions of the benchmark.', '15. In the end, there are many reasons to be excited about the future - the community is continuously taking positive delta changes that contribute to fixing issues with measuring progress in the empirical machinelearning :)']",https://arxiv.org/abs/2107.07002,"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning. ",The Benchmark Lottery
89,1416025077112614913,1187419731126575106,SLDS / Bernd Bischl,"['We just pushed a tutorial paper on hyperparameter optimization on Arxiv (also currently under review). Particularly interesting for new users of HPO, who are looking for a principled overview. <LINK> \n#MachineLearning #AutoML #HPO', ""Also: It's still a working paper, we are already receiving some comments by mail and might change some parts. But we mainly want to stick to the basics and points of general importance. --&gt; Feel free to message us.""]",https://arxiv.org/abs/2107.05847,"Most machine learning algorithms are configured by one or several hyperparameters that must be carefully chosen and often considerably impact performance. To avoid a time consuming and unreproducible manual trial-and-error process to find well-performing hyperparameter configurations, various automatic hyperparameter optimization (HPO) methods, e.g., based on resampling error estimation for supervised machine learning, can be employed. After introducing HPO from a general perspective, this paper reviews important HPO methods such as grid or random search, evolutionary algorithms, Bayesian optimization, Hyperband and racing. It gives practical recommendations regarding important choices to be made when conducting HPO, including the HPO algorithms themselves, performance evaluation, how to combine HPO with ML pipelines, runtime improvements, and parallelization. This work is accompanied by an appendix that contains information on specific software packages in R and Python, as well as information and recommended hyperparameter search spaces for specific learning algorithms. We also provide notebooks that demonstrate concepts from this work as supplementary files. ","Hyperparameter Optimization: Foundations, Algorithms, Best Practices and
  Open Challenges"
90,1416015286839435265,410743045,Joel Miller,['Exciting new paper out on arXiv today 👀\n\n<LINK>'],https://arxiv.org/abs/2107.07307,"Local variations in the intergalactic medium (IGM) neutral hydrogen fraction will affect the Ly-$\alpha$ absorption signature of protoclusters identified in tomographic surveys. Using the IllustrisTNG simulations, we investigate how the AGN proximity effect and hot, collisionally ionised gas arising from gravitational infall and black hole feedback changes the Ly-$\alpha$ absorption associated with $M_{z=0}\simeq10^{14}\,M_\odot$ protoclusters at $z\simeq2.4$. We find that protocluster galaxy overdensities exhibit a weak anti-correlation with Ly-$\alpha$ transmission in IGM transmission maps, but local HI ionisation enhancements due to hot $T>10^{6}\rm\,K$ gas or nearby AGN can disrupt this relationship within individual protoclusters. On average, however, we find that strong reductions in the IGM neutral fraction are limited to within $\lesssim 5h^{-1}\,\textrm{cMpc}$ of the dark matter haloes. Local ionisation enhancements will therefore have a minimal impact on the completeness of protocluster identification in tomographic surveys if smoothing Ly-$\alpha$ transmission maps over scales of $\sim4 h^{-1}\,\textrm{cMpc}$, as is typically done in observations. However, if calibrating the relationship between the matter density and Ly-$\alpha$ transmission in tomographic maps using simple analytical models for the Ly-$\alpha$ forest opacity, the presence of hot gas around haloes can still result in systematically lower estimates of $M_{z=0}$ for the most massive protoclusters. ","Searching for the shadows of giants II: the effect of local ionisation
  on the Lyman-$\alpha$ absorption signatures of protoclusters at redshift
  $z\sim2.4$"
91,1415953349875273728,2736986775,Felix Kuebler,['New paper with R. Malhotra and H. Polemarchakis\n<LINK>'],http://arxiv.org/abs/2107.07294,"We develop conditions under which individual choices and Walrasian equilibrium prices and allocations can be exactly inferred from finite market data. First, we consider market data that consist of individual demands as prices and incomes change. Second, we show that finitely many observations of individual endowments and associated Walrasian equilibrium prices, and only prices, suffice to identify individual demands and, as a consequence, equilibrium comparative statics. ",Exact inference from finite market data
92,1415928833488785409,1515424688,Armen Aghajanyan,"[""I'm excited to announce our new pre-training paper: HTLM: Hyper-Text Pre-Training and Prompting of Language Models (<LINK>) where we unlock new ways of priming and automatically generating prompts by pre-training on simplified HTML."", 'Modeling HTML has a ton of advantages: it is easily gathered at scale, it provides rich end-task like supervision (i.e. id attributes encode category information) and allows for structured prompting that follows semantics of HTML, i.e. zero-shot summarization by infilling &lt;title&gt;', 'We train a BART-like model on over 20T of simplified HTML with stochastic size hints over the masks to allow for more fine-grained control during prompting. We call this model HTLM (Hyper-Text Language Model).', 'Turns out that we can represent a good amount of NLP tasks as HTML prompts. A prototypical example is doing summarization by infilling a &lt;title&gt;. Below we ask HTLM to do exactly this with a hint that our generated mask should be roughly 12 tokens. https://t.co/XC7IPmWdmd', 'Furthermore, by doing clever masking and asking HTLM to generate the most likely hyper-text formatting for any available training data, we can automatically generate valid prompts from very few examples. https://t.co/JebH5Z1kfa', 'We measure the zero-shot performance of manually found prompts and auto-generated prompts (with and without size hints) and discover that we’re able to consistently outperform previous SOTA zero-shot summarization. https://t.co/dRmClKphg6', 'Size hints turn out to be quite important when doing manual prompt tuning as traditionally we would have to find a prompt that both communicates the semantics of the task as well as the length of the generated output, whereas size-hints allow you to focus only on the former. https://t.co/rNQSp4M6xQ', 'We also get competitive results on zero-shot classification results. https://t.co/QImoolZ4DD', 'Furthermore we are able to do k=1-shot table-to-text generation by directly representing the tables in HTML. We do well on fine-tuning HTLM on these tasks as well. https://t.co/pBjtwDgPO4', 'We follow up by doing an analysis from Scao and Rush, “How Many Data Points is a Prompt for HTLM and find that HTML prompts are generally worth more to HTLM than NL prompts to other pre-trained models. https://t.co/JKIzuje5x5', 'This was work done with amazing co-authors @diametralis @ml_perception @mandarjoshi_ @Hu_Hsu Gargi Ghosh and @LukeZettlemoyer']",https://arxiv.org/abs/2107.06955,"We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research. ",HTLM: Hyper-Text Pre-Training and Prompting of Language Models
93,1415876247779889158,1185977761032110080,Kazumasa Ohno (大野 和正),"['Let me introduce new exoplanetary aerosol paper! ""Grain Growth in Escaping Atmospheres: Implications for the Radius Inflation of Super-Puffs""\n<LINK>', 'We studied how aerosols, such as cloud and haze, grow in escaping atmospheres and influence transit observations of exoplanets, with a special focus on extremely low density planets, called super-puffs.', ""Small planets called super-Earth and sub-Neptune are common, but do you know the presence of several strange planets? They have size comparable to gas giants but mass smaller than Neptune. The density is only ~0.1 g/cc or even below, so very puffy (that's why super-puff)."", 'Physically, such planets are possible if the planet has a massive atmosphere (&gt;10 wt% by planet mass). But several studies suggested that such massive atmosphere cannot be sustainable against to atmospheric escape until today.', 'Some recent studies suggested that their large radii are caused by other reasons, such as the presence of circumplanetary ring. One promising idea is that aerosols in the atmosphere elevates the pressure level where transit observation probe, enlarging the apparent planet size.', 'Especially, Wang &amp; Dai (2019) suggested that an atmosphere escaping from the planet can transport abundant dust (=aerosols) and explain the large size of super-puffs. We examine how much dust can be transported based on microphysics, which were lacked in the previous study.', 'I note that @PlanetaryGao published a very nice paper about the same topic in the last year. The main difference is that we parameterize the dust formation altitude and rate to discuss what kind of aerosol works better. We also studied some complication factor, such as porosity.', 'The formed dust quickly grow into large size via coagulation. While some small particles are transported by outflow, large particles settle down to the planet. How much the outflow transport dust depends on dust formation altitude, amount, and strength of the escape. https://t.co/PbOHkY1eOd', 'The dust abundance in the outflow as a function of dust production rate. When dust is formed at relatively deep atmosphere (high P0), almost no dust can be transported even if the production rate is extremely high. One conclusion is that high-altitude dust is necessary. https://t.co/tMYmFpqADu', 'To generalize the result, we also conducted an analytical argument to predict the dust abundance in the outflow. We found that the abundance cannot exceed the Mach number at dust production altitude. The analytical model is the dashed lines in above. The fit looks not bad.', 'The transit radius with dusty outflow divided by dust-free atmosphere. As seen, the radius enhancement can be significant only for small P0. Thus, the large apparent radius may imply the presence of high-altitude dust, such as haze. This supports the idea by Gao &amp; Zhang (2020) https://t.co/C4wDDL7aQB', 'Here is the transmission spectra of Kepler-51b like super-puffs. The spectra become almost completely featureless in visible to NIR, consistent with observations. Also, the depth drastically decreases with wavelength, that would be tested by JWST or ARIEL. https://t.co/7eOzdRjQs3', 'I also want to note that the observations at long wavelength may be able to see the absorption feature of aerosol itself. For example, meteoric dust is also high-altitude aerosol, while they produce silicate feature at ~10 µm. This helps to constrain the aerosol composition.', 'Ok, the dusty outflow may work out, but how can we distinguish this from other hypothesis, such as the circumplanetary ring? We are thinking that the transmission spectrum is also useful for this purpose.', 'We computed the spectrum of planets with circumplanetary ring. While transit radius drastically decrease for aerosol scenario, this is not the case for the ring scenario. This is because the dusty outflow consists of small particles, while the ring consists of large particles. https://t.co/uUHIDyWYPR', 'One may think ""Why do only a few planets become super-puffs?"". Indeed, recent studies suggest the ubiquity of haze on exoplanets, which might look contradicting to the rarity of super-puffs.', 'We provided one reason for this question. This figure shows the mass-radius relation of low-mass planets. Solid lines show the radii at 1e-2 bar, while the dashed lines show those at 1e-8 bar. Actually, the difference is pretty small for the majority of mass range. https://t.co/samW3Qk70Q', 'The radius enhancement by elevated pressure level works only for small mass range, but such range is vulnerable to atmospheric escape. The reason is actually simple: the enhancement by elevated pressure level could work only when the scale height is comparable to planet radius.', 'The ratio of 1e-8 bar to 1e-2 bar radii. White region is unstable space in which the mass loss timescale is &lt;0.1 Gyr. The drastic enhancement by aerosol works only in the narrow mass range that is verge on the total atmospheric loss. https://t.co/B67Yb2MBUQ', 'We roughly estimated that such mass range is ~2--5 MEarth. If we plot the low-density planets compiled by Chachan et al. (2020), the majority of them do exist around such mass range. Thus, the strict condition needed for the enhancement may explain the rarity of super-puffs.', 'We studied a number of topics, but I now stop by this topic. We examined the effect of porosity evolution of dust aggregates, but the basic conclusions are unchanged (dust abundance is limited by growth and settling). https://t.co/5lIVAqCRra', 'But we observed that the fractal dimension of aerosols is different from those in static atmosphere (typically ~2). It would be interesting to see how the particle morphology differ for different environment of exoplanets, which would be future studies.', ""That's it! I started this study about 2 years ago with @_yukitanaka. The progress was slow due to thesis defense, other papers, pandemic...\nI'm happy that I can finally post it on arXiv!""]",https://arxiv.org/abs/2107.07027,"Super-puffs -- low-mass exoplanets with extremely low bulk density -- are attractive targets for exploring their atmospheres and formation processes. Recent studies suggested that the large radii of super-puffs may be caused by atmospheric dust entrained in the escaping atmospheres. In this study, we investigate how the dust grows in escaping atmospheres and influence the transit radii using a microphysical model of grain growth. Collision growth is efficient in many cases, leading to hinder the upward transport of dust via enhanced gravitational settling. We find that dust abundance in the outflow hardly exceeds the Mach number at the dust production region. Thus, dust formed at upper atmospheres, say $P\lesssim{10}^{-5}$ bar, are needed to launch a dusty outflow with high dust abundance. With sufficiently high dust production altitudes and rates, the dusty outflow can enhance the observable radius by a factor of $\sim$2 or even more. We suggest that photochemical haze is a promising candidate of high-altitude dust that can be entrained in the outflow. We also compute the synthetic transmission spectra of super-puff atmospheres and demonstrate that the dusty outflow produces a broad spectral slope and obscures molecular features, in agreement with recently reported featureless spectra. Lastly, using an interior structure model, we suggest that the atmospheric dust could drastically enhance the observable radius only for planets in a narrow mass range of $\sim2$--$5M_{\rm \oplus}$, in which the boil-off tends to cause total atmospheric loss. This may explain why super-puffs are uncommon despite the suggested universality of photochemical hazes. ","Grain Growth in Escaping Atmospheres: Implications for the Radius
  Inflation of Super-Puffs"
94,1415849320973324290,799950129113550852,Shengwu Li,"[""Econ perspective: People make choices to satisfy their preferences.\n\nPsych perspective: People adapt their preferences to justify their choices.\n\nHere's a theory that combines them.  (A new working paper joint with Erik Eyster and Sarah Ridout.) <LINK>🧵 <LINK>"", 'First, a parable from @R_Thaler (1980): Bob pays $100 for a ticket to a basketball game to be played 60 miles from his home. On the day of the game there is a snowstorm. He decides to go anyway. If the ticket had been free-of-charge, he would have stayed home.', 'Bob falls for the sunk-cost fallacy. Buying the tickets was a mistake, ex post. But if he attends the game, he can avoid conceding the error by exaggerating his enthusiasm.', ""The key ingredients for Bob's behavior are\n1. an ex post mistake,\n2. plausible preferences that he could adopt to rationalize his choices."", 'In the model, our agent chooses from a menu A_1, then learns the state of the world s, then chooses from a menu A_2 that can depend on his first action.', 'The model primitives are a material utility function u, a set of plausible utility functions V (rationales), and a weight parameter gamma.\n\nThe agent, having chosen action a_1 and learned that the state is s, chooses her second action a_2 and a rationale v to maximize this: https://t.co/RBzIGTe0Jk', ""The first term is material utility.  The second term measures how optimal the agent's actions were, ex post, under the chosen rationale."", 'After making a mistake, the agent can raise the second term by adopting rationales that justify his first action. By construction, these distort his second action compared to the classical benchmark. https://t.co/WVJkR5p6rO', ""The theory explains Bob's behavior (details in paper).  It predicts the sunk-cost effect, as well as a new 'unsunk-benefits'  effect, and has tractable comparative statics for supermodular decision problems."", ""But how can we ever know the agent's rationales, if she fluidly changes them ex post? It turns out we can take a revealed-preference approach to rationales. We prove that the rationales (and other primitives) are fully identified from choice behavior."", ""Here's a link to the paper.  I'll be giving a 30 minute talk about this on 22 July, 10 am Eastern, at the World Congress of the Game Theory Society. https://t.co/hScOc6rEaB"", '@matt_blackwell @maya_sen Cool! Thanks for sending it on.', '@mlkovach1 Thank you! Glad to hear it from someone who actually knows decision theory. 😅', '@flesherdan Here’s the conference website: https://t.co/KISmQkau00 I’m also giving a talk on the 28th of July. https://t.co/NjYmLFppSt', '@itaisher You’re a fast reader.', '@rithvikra0 Thank you! It’s been a long while in the works.', '@angusarmstrong8 As I understand it, the classical benchmark at least requires time consistency of preferences, which our agent does not have. (Your morning self and your evening self should agree about a beer scheduled for 8 pm.)', '@angusarmstrong8 I’m trying to understand - is your position that Bob’s behavior is consistent with the classical model? For his choice to attend the game to depend on sunk costs?', '@diegozamoradiaz Here you go!\n\nhttps://t.co/pvjjgMYx4H']",https://arxiv.org/abs/2107.07491,"People rationalize their past choices, even those that were mistakes in hindsight. We propose a formal theory of this behavior. The theory predicts that sunk costs affect later choices. Its model primitives are identified by choice behavior and it yields tractable comparative statics. ",A Theory of Ex Post Rationalization
95,1415841720831410178,370378404,Takehiko Yasuda,['Wrote open problems in the wild McKay correspondence in this new paper.  There are yet a lot of things to be done. I hope that more people would join us in exploring this theme.\n<LINK>'],https://arxiv.org/abs/2107.07073,"The wild McKay correspondence is a form of McKay correspondence in terms of stringy invariants that is generalized to arbitrary characteristics. It gives rise to an interesting connection between the geometry of wild quotient varieties and arithmetic on extensions of local fields. The principal purpose of this article is to collect open problems on the wild McKay correspondence, as well as those in related fields that the author believes are interesting or important. It also serves as a survey on the present state of these fields. ",Open problems in the wild McKay correspondence and related fields
96,1415813134959349767,1263773978604126208,"池田思朗, Shiro Ikeda","['Our new paper accepted for publication in Astronomical Journal. A sparse matrix decomposition method is applied to denoising of submillimeter spectroscopy with single-dish telescopes. Conguratulations, Taniguchi-san. <LINK>']",https://arxiv.org/abs/2107.06290,"For submillimeter spectroscopy with ground-based single-dish telescopes, removing noise contribution from the Earth's atmosphere and the instrument is essential. For this purpose, here we propose a new method based on a data-scientific approach. The key technique is statistical matrix decomposition that automatically separates the signals of astronomical emission lines from the drift noise components in the fast-sampled (1--10 Hz) time-series spectra obtained by a position-switching (PSW) observation. Because the proposed method does not apply subtraction between two sets of noisy data (i.e., on-source and off-source spectra), it improves the observation sensitivity by a factor of $\sqrt{2}$. It also reduces artificial signals such as baseline ripples on a spectrum, which may also help to improve the effective sensitivity. We demonstrate this improvement by using the spectroscopic data of emission lines toward a high-redshift galaxy observed with a 2-mm receiver on the 50-m Large Millimeter Telescope (LMT). Since the proposed method is carried out offline and no additional measurements are required, it offers an instant improvement on the spectra reduced so far with the conventional method. It also enables efficient deep spectroscopy driven by the future 50-m class large submillimeter single-dish telescopes, where fast PSW observations by mechanical antenna or mirror drive are difficult to achieve. ","A data-scientific noise-removal method for efficient submillimeter
  spectroscopy with single-dish telescopes"
97,1415691812736540672,1329142899129290752,Shion Guha,"['&lt;🎺New paper announcement🎺&gt;\nI am very proud to announce what I think is the best paper I have ever written on the tenure track road. \n\nWe present possibly the first theoretical framework for algorithmic decision-making in the public sector (ADMAPS) 1/n\n \n<LINK> <LINK>', 'This was led by @dev7saxena along with the indomitable @pamwis and @kbadillou.  \n\nADMAPS stands on the shoulders of prior giants and is validated through a deep ethnographic study of how child welfare workers in Wisconsin make algorithmic decisions about children. Some notes:\n2/n', ""If we've chatted recently, you know I have been very frustrated with the current state of affairs @sig_chi @ACM_CSCW @FAccTConference where inferences are made about critical public sector domains like criminal justice/child welfare through artificial, crowdsourced studies. 3/n"", 'Regular, street-level bureaucrats make high stakes decisions about actual, vulnerable people daily through biased risk assessment instruments. *How* they make decisions are just as important as *what* those decisions are. 4/n', 'But, as a field (and I include me here as well), we have been quite lazy about studying these actual communities from multiple stakeholder perspectives. Social media and MTurk are not substitutes for doing the hard work to gain the trust and collaborate with communities. 5/n', 'As @dev7saxena will certify, he has spent ~2 years in deep ethnographic work and we have spent 3 years in collaboration with the relevant Wisconsin agencies to publish this one paper. This is out of cadence with what is expected of a HCI/data science junior TT professor. 6/n', ""I have been asked many times in conferences why I don't substitute community engagement with social media data and simulated, scenario based, crowdsourced experiments. Apparently, this would enable me to publish more stuff sooner. Meh? 7/n"", ""ADMAPS finds 3 broad dimensions to this decision-making process - human discretion, policies and regulations and decision-making under uncertainties. We didn't come up with these magically - they are grounded in prior work by excellent folks. 8/n"", ""Some of these folks who've been instrumental in laying this groundwork are @_alialkhatib on human discretion, @melaniesage on policies and regulations and @najaholten on decision-making processes. This list is not exhaustive. 9/n"", 'This paper has been accepted to @ACM_CSCW but we wanted to get a pre-print out asap. If your work touches on algorithms and the public sector, you might find this paper very interesting and @dev7saxena and I would love to have a conversation/collaborate with you anytime. 10/n', ""Coda: We've seen validation of this work beyond Wisconsin to other states recently and as part of my recent move to @UofTInfoFaculty , I am attempting to bring this appreciation to Canada which needs its own reckoning with child welfare systems. Happy to chat more about all this!"", '@rumichunara Thanks! Happy to chat anytime!', '@sharoz @UofTInfoFaculty None of these findings would have been found with controlled experiments. These are rich human nuances in behavior that can be described but not measured. There is actually a lot of room for controlled experiments here that compares assessments that we are also working on.', ""@ruchowdh Thank you! That means a lot - really it's all @dev7saxena (he's on the job market soon!)""]",https://arxiv.org/abs/2107.03487,"Algorithms have permeated throughout civil government and society, where they are being used to make high-stakes decisions about human lives. In this paper, we first develop a cohesive framework of algorithmic decision-making adapted for the public sector (ADMAPS) that reflects the complex socio-technical interactions between \textit{human discretion}, \textit{bureaucratic processes}, and \textit{algorithmic decision-making} by synthesizing disparate bodies of work in the fields of Human-Computer Interaction (HCI), Science and Technology Studies (STS), and Public Administration (PA). We then applied the ADMAPS framework to conduct a qualitative analysis of an in-depth, eight-month ethnographic case study of the algorithms in daily use within a child-welfare agency that serves approximately 900 families and 1300 children in the mid-western United States. Overall, we found there is a need to focus on strength-based algorithmic outcomes centered in social ecological frameworks. In addition, algorithmic systems need to support existing bureaucratic processes and augment human discretion, rather than replace it. Finally, collective buy-in in algorithmic systems requires trust in the target outcomes at both the practitioner and bureaucratic levels. As a result of our study, we propose guidelines for the design of high-stakes algorithmic decision-making tools in the child-welfare system, and more generally, in the public sector. We empirically validate the theoretically derived ADMAPS framework to demonstrate how it can be useful for systematically making pragmatic decisions about the design of algorithms for the public sector. ","A Framework of High-Stakes Algorithmic Decision-Making for the Public
  Sector Developed through a Case Study of Child-Welfare"
98,1415608291091263488,1355196636440494080,Jonathan Kriewald,"['paper day :)\n\nwe explore interference effects due to new leptonic CPV phases in the presence of TeV-scale sterile neutrinos in cLFV observables, leading in extreme cases to a total cancellation of the rates\n\ncheck it out on arXiv: \n\n<LINK> <LINK>']",https://arxiv.org/abs/2107.06313,"In extensions of the Standard Model by Majorana fermions, the presence of additional CP violating phases has been shown to play a crucial role in lepton number violating processes. In this work we show that (Dirac and Majorana) CP violating phases can also lead to important effects in charged lepton flavour violating (cLFV) transitions and decays, in some cases with a significant impact for the predicted rates of cLFV observables. We conduct a thorough exploration of these effects in several cLFV observables, and discuss the implications for future observation. We emphasise how the presence of leptonic CP violating phases might lead to modified cLFV rates, and to a possible loss of correlation between cLFV observables. ",On the role of leptonic CPV phases in cLFV observables
99,1415572068192690177,1910301474,MartinWeides,"[""new paper 'Frequency fluctuations of ferromagnetic resonances at milliKelvin temperatures' online, how stable are FMR resonances in YIG, and how do they relate to the observed linewidth? \n<LINK>\n@TimWolz @UofGSciEng @UofG_ENE <LINK>""]",https://arxiv.org/abs/2107.06531,"Unwanted fluctuations over time, in short, noise, are detrimental to device performance, especially for quantum coherent circuits. Recent efforts have demonstrated routes to utilizing magnon systems for quantum technologies, which are based on interfacing single magnons to superconducting qubits. However, the coupling of several components often introduces additional noise to the system, degrading its coherence. Researching the temporal behavior can help to identify the underlying noise sources, which is a vital step in increasing coherence times and the hybrid device performance. Yet, the frequency noise of the ferromagnetic resonance (FMR) has so far been unexplored. Here, we investigate such FMR frequency fluctuations of a YIG sphere down to mK-temperatures, and find them independent of temperature and drive power. This suggests that the measured frequency noise in YIG is dominated by so far undetermined noise sources, which properties are not consistent with the conventional model of two-level systems, despite their effect on the sample linewidth. Moreover, the functional form of the FMR frequency noise power spectral density (PSD) cannot be described by a simple power law. By employing time-series analysis, we find a closed function for the PSD that fits our observations. Our results underline the necessity of coherence improvements to magnon systems for useful applications in quantum magnonics. ","Frequency fluctuations of ferromagnetic resonances at milliKelvin
  temperatures"
100,1415569152916533251,1146521605457272832,Jorinde van de Vis,"['A gravitational wave signal from a first order phase transition might be characterized by two typical length scales: bubble size &amp; sound shell thickness. Can LISA measure both scales?\nNew paper out! \n<LINK>\nwith Felix Giese @Flxgie and Thomas Konstandin <LINK>', ""@Flxgie We had a lot of fun with statistics (I still don't know whether I'm a Bayesian or a frequentist) and @Flxgie made some very beautiful plots 🤩""]",https://arxiv.org/abs/2107.06275,"We study to what extend LISA can observe features of gravitational wave spectra originating from cosmological first-order phase transitions. We focus on spectra which are of the form of double-broken power laws. These spectra are predicted by hydrodynamic simulations and also analytical models such as the sound shell model. We argue that the ratio of the two break frequencies is an interesting observable since it can be related to the wall velocity while overall amplitude and frequency range are often degenerate for the numerous characteristics of the phase transition. Our analysis uses mock data obtained from the power spectra predicted by the simplified simulations and the sound shell model and analyzes the detection prospects using $\chi^2$-minimization and likelihood sampling. We point out that the prospects of observing two break frequencies from the electroweak phase transition is hindered by a shift of the spectrum to smaller frequencies for strong phase transitions. Finally, we also highlight some differences between signals from the sound shell model compared to simulations. ",Finding sound shells in LISA mock data using likelihood sampling
101,1415554652804947974,2377407248,Daniel Whiteson,"['New paper!\n\nMeasuring how well a smartphone camera can detect cosmic muons!\n\n<LINK>\n\nLed by Jeff Swaney and Mike Mulhearn, with @cshimmin', 'What? Your phone can see particles?\n\nWhen a muon passed through your phone camera, it frees up electrons, just like when a photon does. So the camera sees that pixel as on. If you cover the lens and put the phone in a muon beam, presto, you see tracks! https://t.co/l8AwyMsQlp', 'We wanted to do much more: to turn the network of smartphones into a world-wide detector for cosmic particles. https://t.co/SFtUZQwVdy', 'To do that, we needed to measure how often the phone sees or misses a particle. So we put some phones between two scintillators: https://t.co/W1FCNyvUu9', 'And measured how often we spotted the muon in the phone.\n\nAlong the way, we had to reverse engineer how the phone turns electrons into digitized values, so we could measure the pure response: https://t.co/cMOPXF6drO', 'This will help us understand how well a network of phones can act as a global detector (https://t.co/FWCchA1MX5)', 'TLDR: smartphones are about 70-80% efficient at detecting muons!', ""@y0b1byte Yes, if the flux is very high, but that's not a concern for cosmic rays.  More of an issue is that the performance degrades if phone is kept at high temperature for too long."", '@Antony_Clements @SeamusBlackley @cshimmin Maybe!  A lot of it is noise from badly-behaving pixels. We had to filter out the hot pixels to get a reliable muon signal.']",https://arxiv.org/abs/2107.06332,"A measurement of the efficiency of CMOS sensors in smartphone cameras to cosmic ray muons is presented. A coincidence in external scintillators indicates the passage of a cosmic ray muon, allowing the measurement of the efficiency of the CMOS sensor. The observed flux is consistent with well-established values, and efficiencies are presented as a function of the number of photo-electrons collected from the CMOS silicon photodiode pixels. These efficiencies are vital to understanding the feasibility of large-scale smartphone networks operating as air-shower observatories. ",Measurement of Smartphone Sensor Efficiency to Cosmic Ray Muons
102,1415495592701304837,131742305,Anthony Bosman,['Spent the summer math-ing with four awesome @AndrewsUniv undergrads. They proved new things about knots/links. Check out their paper! 🪢\n\n<LINK>'],https://arxiv.org/abs/2107.06791,"It is known that algebraically split links (links with vanishing pairwise linking number) can be transformed into the trivial link by a series of local moves on the link diagram called delta-moves; we define the delta-unlinking number to be the minimum number of such moves needed. This generalizes the notion of delta-unknotting number, defined to be the minimum number of delta-moves needed to move a knot into the unknot. While the delta-unknotting number has been well-studied and calculated for prime knots, no prior such analysis has been conducted for the delta-unlinking number. We prove a number of lower and upper bounds on the delta-unlinking number, relating it to classical link invariants including unlinking number, 4-genus, and Arf invariant. This allows us to determine the precise value of the delta-unlinking number for algebraically split prime links with up to 9 crossings as well as determine the 4-genus for most of these links. ",The delta-unlinking number of algebraically split links
103,1415483853192404998,279821183,Ben Montet,"['Happy to announce a new paper out of our group led by Elsa Palumbo, a @Caltech (\'23) undergrad working with us through the @CaltechSURF program! ""Evidence for Centrifugal Breakout around the Young M Dwarf TIC 234284556,"" <LINK>; submitted to AAS journals', ""As a part of an investigation into variability of young stars with @NASA_TESS, Elsa found a rapidly rotating star with strange transit-looking features. They had the same period as the star's rotation, but also changed in depth and duration! https://t.co/79PdpPRaNy"", ""With additional data coming in from TESS during Sector 3, we took bets on what a new month of data would bring. What we weren't expecting was over ~24 hours the dips would completely disappear, but that's what happened! https://t.co/vlfWERZo0m"", 'A dip into the literature led Elsa to theories of centrifugal breakout, in which a magnetically-trapped cloud of nearly co-rotating material drags the magnetic field lines until the drag becomes too strong, then the lines snap and reconnect, quickly dispersing the material.', 'The theory is &gt;15 years old, but mostly applied in the context of high-mass stars, with no direct evidence of this signal to date. We think this is the best evidence for the centrifugal breakout model ever found!', 'Additional spectroscopy with the Veloce spectrograph at the AAT observatory in Australia shows the hydrogen features are fairly constant (no real shape changes, flux variations consistent with starspots) suggesting that it is clouds rather than plasma causing the dips. https://t.co/HrGHDGwkbD', ""It's a fun paper! Read it at https://t.co/1I5s3PV2LJ and read Elsa's summary of what she found at https://t.co/OVzAz5c3Zf. \n\nElsa will be applying for PhD programs in ~18 months and would make a great addition to any astronomy program! ☺️""]",https://arxiv.org/abs/2107.05649,"Magnetospheric clouds have been proposed as explanations for depth-varying dips in the phased light curves of young, magnetically active stars such as $\sigma$ Ori E and RIK-210. However, the stellar theory that first predicted magnetospheric clouds also anticipated an associated mass-loss mechanism known as centrifugal breakout for which there has been limited empirical evidence. In this paper, we present data from TESS, LCO, ASAS-SN, and Veloce on the 45 Myr M3.5 star TIC 234284556, and propose that it is a candidate for the direct detection of centrifugal breakout. In assessing this hypothesis, we examine the sudden ($\sim$1-day timescale) disappearance of a previously stable ($\sim$1-month timescale) transit-like event. We also interpret the presence of an anomalous brightening event that precedes the disappearance of the signal, analyze rotational amplitudes and optical flaring as a proxy for magnetic activity, and estimate the mass of gas and dust present immediately prior to the potential breakout event. After demonstrating that our spectral and photometric data support a magnetospheric clouds and centrifugal breakout model and disfavor alternate scenarios, we discuss the possibility of a coronal mass ejection (CME) or stellar wind origin of the corotating material and we introduce a reionization mechanism as a potential explanation for more gradual variations in eclipse parameters. Finally, after comparing TIC 234284556 with previously identified ""flux-dip"" stars, we argue that TIC 234284556 may be an archetypal representative of a whole class of young, magnetically active stars. ",Evidence for Centrifugal Breakout around the Young M Dwarf TIC 234284556
104,1415475602832781315,901142962758758400,Hang-Hyun Jo,"['Our new paper with Yohsuke Murase @yohm13, Janos Török, Janos Kertesz @janos_kertesz, and Kimmo Kaski @kimmokaski is out in the arXiv: ""Deep learning based parameter search for an agent based social network model"", see <LINK>.', ""Since 2014 we have studied the social network modeling based on Kumpula et al.'s model (https://t.co/xqUt51F5UB). Our new paper is the synthesis of (almost) all of our previous works: https://t.co/U8pqFp4Ud8, https://t.co/r1y1x0mr7n, and https://t.co/QjMrGNnkBf."", ""I omitted the figure in the paper. By the way, you can watch the simulation video for the original Kumpula et al.'s model https://t.co/54UspkHS6N and its multilayer version (our work) https://t.co/54UspkHS6N, both of which are made by @yohm13. https://t.co/20cvI9QCe5""]",https://arxiv.org/abs/2107.06507,"Interactions between humans give rise to complex social networks that are characterized by heterogeneous degree distribution, weight-topology relation, overlapping community structure, and dynamics of links. Understanding such networks is a primary goal of science due to serving as the scaffold for many emergent social phenomena from disease spreading to political movements. An appropriate tool for studying them is agent-based modeling, in which nodes, representing persons, make decisions about creating and deleting links, thus yielding various macroscopic behavioral patterns. Here we focus on studying a generalization of the weighted social network model, being one of the most fundamental agent-based models for describing the formation of social ties and social networks. This Generalized Weighted Social Network (GWSN) model incorporates triadic closure, homophilic interactions, and various link termination mechanisms, which have been studied separately in the previous works. Accordingly, the GWSN model has an increased number of input parameters and the model behavior gets excessively complex, making it challenging to clarify the model behavior. We have executed massive simulations with a supercomputer and using the results as the training data for deep neural networks to conduct regression analysis for predicting the properties of the generated networks from the input parameters. The obtained regression model was also used for global sensitivity analysis to identify which parameters are influential or insignificant. We believe that this methodology is applicable for a large class of complex network models, thus opening the way for more realistic quantitative agent-based modeling. ","Deep learning based parameter search for an agent based social network
  model"
105,1415371562870530048,1143885357760602117,Jad Rahme,"['Generalization in RL is hard. Limited interaction with the environment turns a fully observed MDP into an implicitly partially observed one we call the ""Epistemic POMDP"".  This viewpoint helps! \n\nIf you want to learn more, check out our new paper: <LINK> <LINK>', 'A very fun collaboration with @its_dibya @aviral_kumar2 @yayitsamyzhang @ryan_p_adams and @svlevine :)']",http://arxiv.org/abs/2107.06277,"Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite. ","Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit
  Partial Observability"
106,1415364606445318155,990433714948661250,Sergey Levine,"['Empirical studies observed that generalization in RL is hard. Why? In a new paper, we provide a partial answer: generalization in RL induces partial observability, even for fully observed MDPs! This makes standard RL methods suboptimal.\n<LINK>\n\nA thread:', 'Take a look at this example: the agent has a multi-step ""guessing game"" to label an image (not a bandit -- you get multiple guesses until you get it right!). We know in MDPs there is an optimal deterministic policy, so RL will learn a deterministic policy here. https://t.co/zih0Jp5h6l', ""Of course, this is a bad idea -- if it guesses wrong on the first try, it should not guess the same label again. But this task *is* fully observed -- there is a unique mapping from image pixels to labels, the problem is that we just don't know what it is from training data!"", 'The guessing game is a MDP, but learning to guess from finite data becomes (implicitly) a POMDP -- what we call the epistemic POMDP, because it emerges from epistemic uncertainty. This is not unique to guessing, the same holds eg for mazes in ProcGen, robotic grasping, etc.', 'This leads to some counterintuitive things. Look at the ""zookeeper example"" below: the optimal MDP strategy is to look at the map (which is fully observed) and go to the otters, but peeking through windows generalizes much better (is never optimal in training). https://t.co/2lLv65hTee', 'What is happening here is that generalization in RL requires taking epistemic (information-gathering) actions at test time, just like we would in a POMDP, but this is never optimal to do at training-time. Hence, MDP methods will not generalize as well as POMDP methods.', 'Based on this idea, we developed a new algorithm, LEEP, that utilizes epistemic POMDP ideas to get better generalization. LEEP actually does *worse* on the training environments, but much better on test environments, as we would expect. https://t.co/l5AxpWI1AQ', 'Unfortunately, solving (or even estimating) the epistemic POMDP is very hard, and LEEP makes some very crude approximations. Lots more research is needed to utilize the epistemic POMDP, in which case I think we can all make lots of progress on generalization!', 'This was a really fun collaboration with @its_dibya, @jrahme0, @aviral_kumar2, @yayitsamyzhang, @ryan_p_adams -- a really fun group to work with on generalization🙂', '@qcjefftqc That is a really good question! I do think it might provide part of the explanation, though such methods do not (in principle) capture the right ""type"" of uncertainty. But it seems likely they ""accidentally"" get some epistemic uncertainty, esp. with regularization/early stopping.']",https://arxiv.org/abs/2107.06277,"Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves significant gains in generalization over current methods on the Procgen benchmark suite. ","Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit
  Partial Observability"
107,1415343549843968009,959009455475175424,Ian Mason,"['New paper at <LINK>. We explore how we can leverage per-unit activation distributions to adapt to unlabelled data in new domains. \n\nWith @CianEastwood, Chris Williams and @bschoelkopf. More here ⬇️. <LINK>']",https://arxiv.org/abs/2107.05446,"Source-free domain adaptation (SFDA) aims to adapt a model trained on labelled data in a source domain to unlabelled data in a target domain without access to the source-domain data during adaptation. Existing methods for SFDA leverage entropy-minimization techniques which: (i) apply only to classification; (ii) destroy model calibration; and (iii) rely on the source model achieving a good level of feature-space class-separation in the target domain. We address these issues for a particularly pervasive type of domain shift called measurement shift which can be resolved by restoring the source features rather than extracting new ones. In particular, we propose Feature Restoration (FR) wherein we: (i) store a lightweight and flexible approximation of the feature distribution under the source data; and (ii) adapt the feature-extractor such that the approximate feature distribution under the target data realigns with that saved on the source. We additionally propose a bottom-up training scheme which boosts performance, which we call Bottom-Up Feature Restoration (BUFR). On real and synthetic data, we demonstrate that BUFR outperforms existing SFDA methods in terms of accuracy, calibration, and data efficiency, while being less reliant on the performance of the source model in the target domain. ","Source-Free Adaptation to Measurement Shift via Bottom-Up Feature
  Restoration"
108,1415252809633632258,2427184074,Christopher Berry,"[""Congratulations to @ynxmonica on her new paper! She's been working very hard to investigate how modelling mass transfer and common envelope evolution impact the predictions for binary black holes. This physics is super important <LINK> <LINK>"", 'In this study, @ynxmonica carefully compared results for the evolution of black hole+hydrogen-rich stars using MESA (with the full stellar structure) and the quick prescriptions used in COSMIC. Stable mass transfer is much more common using the detailed simulation https://t.co/9bwPkQglm7', 'Binary black holes formed after common envelope evolution is rare using the MESA simulations (I wanted to call the paper ""Successful common envelope evolution is uncommon""). Thus, overall, we find few merging binary black hole systems', 'Not only do the number of binaries that will form merging binary black holes change, but the delay times too. table mass transfer leads to wider orbits than common envelope evolution, hence the black holes take longer to merge. This could have a big impact on predicted rates! https://t.co/ADfczC7QzB', 'If you want to catch up on this binary black hole study, @ynxmonica discussed her results at @EAS_meeting https://t.co/8CP4F2Os4O']",https://arxiv.org/abs/2107.05702,"Rapid binary population synthesis codes are often used to investigate the evolution of compact-object binaries. They typically rely on analytical fits of single-star evolutionary tracks and parameterized models for interactive phases of evolution (e.g., mass-transfer on thermal timescale, determination of dynamical instability, and common envelope) that are crucial to predict the fate of binaries. These processes can be more carefully implemented in stellar structure and evolution codes such as MESA. To assess the impact of such improvements, we compare binary black hole mergers as predicted in models with the rapid binary population synthesis code COSMIC to models ran with MESA simulations through mass transfer and common-envelope treatment. We find that results significantly differ in terms of formation paths, the orbital periods and mass ratios of merging binary black holes, and consequently merger rates. While common-envelope evolution is the dominant formation channel in COSMIC, stable mass transfer dominates in our MESA models. Depending upon the black hole donor mass, and mass-transfer and common-envelope physics, at sub-solar metallicity COSMIC overproduces the number of binary black hole mergers by factors of 2--35 with a significant fraction of them having merger times orders of magnitude shorter than the binary black holes formed when using detailed MESA models. Therefore we find that some binary black hole merger rate predictions from rapid population syntheses of isolated binaries may be overestimated by factors of ~5--500. We conclude that the interpretation of gravitational-wave observations requires the use of detailed treatment of these interactive binary phases. ","Binary Black Hole Formation with Detailed Modeling: Stable Mass Transfer
  Leads to Lower Merger Rates"
109,1414969834115411968,869532833764831232,Thibaut Vidal,"['New exponential-size neighborhoods for the pickup-and-delivery problem. Optimal solutions are attained for most problems with up to 30 nodes in a *single* local-search descent, giving 1,000x speedup over previous algorithms! Check our paper here:\n<LINK>\n#ORMS <LINK>', 'work with Toni Pacheco, Rafael Martinelli (@rafaelmartinel8), Tulio Toffolo (@tuliotoffolo) and Anand Subramanian (@Subjectto_)', '👉@pacheco_toni']",https://arxiv.org/abs/2107.05189,"Neighborhood search is a cornerstone of state-of-the-art traveling salesman and vehicle routing metaheuristics. While neighborhood exploration procedures are well developed for problems with individual services, their counterparts for one-to-one pickup-and-delivery problems have been more scarcely studied. A direct extension of classic neighborhoods is often inefficient or complex due to the necessity of jointly considering service pairs. To circumvent these issues, we introduce major improvements to existing neighborhood searches for the pickup-and-delivery traveling salesman problem and new large neighborhoods. We show that the classical Relocate-Pair neighborhood can be fully explored in $O(n^2)$ instead of $O(n^3)$ time. We adapt the 4-Opt and Balas-Simonetti neighborhoods to consider precedence constraints. Moreover, we introduce an exponential-size neighborhood called 2k-Opt, which includes all solutions generated by multiple nested 2-Opts and can be searched in $O(n^2)$ time using dynamic programming. We conduct extensive computational experiments, highlighting the significant contribution of these new neighborhoods and speed-up strategies within two classical metaheuristics. Notably, our approach permits to repeatedly solve small pickup-and-delivery problem instances to optimality or near-optimality within milliseconds, and therefore it represents a valuable tool for time-critical applications such as meal delivery or mobility of demand. ","Exponential-Size Neighborhoods for the Pickup-and-Delivery Traveling
  Salesman Problem"
110,1414781294253285386,16079444,Ying-Jer Kao,"['New paper with Yu-Hsueh, Jozef, and Yong-Baek where we demonstrate single-particle excitations of the spin-1 Kitaev model is bosonic.\n<LINK>']",https://arxiv.org/abs/2107.04730,"We study the excitation spectrum of the spin-1 Kitaev model using the symmetric tensor network. By evaluating the virtual order parameters defined on the virtual Hilbert space in the tensor network formalism, we confirm the ground state is in a $\mathbb{Z}_2$ spin liquid phase. Using the correspondence between the transfer matrix spectrum and low-lying excitations, we find that contrary to the dispersive Majorana excitation in the spin-1/2 case, the isotropic spin-1 Kitaev model has a dispersive charge anyon excitation. Bottom of the gapped single-particle charge excitations are found at $\mathbf{K}, \mathbf{K}'=(\pm2\pi/3, \mp 2\pi/3)$, with a corresponding correlation length of $\xi \approx 6.7$ unit cells. The lower edge of the two-particle continuum, which is closely related to the dynamical structure factor measured in inelastic neutron scattering experiments, is obtained by extracting the excitations in the vacuum superselection sector in the anyon theory language ",Excitation spectrum of spin-1 Kitaev spin liquids
111,1414769780209471489,4438354094,Tom Wong,"['New paper with Josh Lockhart @AbacusBanana @UCLQuantum! We show that out of all 1,018,689,568 simple, connected, irregular graphs with 11 vertices or less, there are 64 on which Laplacian and adjacency (continuous-time) quantum walks are equivalent. <LINK> <LINK>', 'This tweet from last December was me simulating all these quantum walks. https://t.co/jHXysYcXcG', ""Here's a picture from January. I had already found all 64 graphs. I printed them so that I could rearrange them and look for patterns. I found eight infinite families of graphs supporting these equivalent walks. https://t.co/nwqzkq3ekT"", ""@octonion @AbacusBanana @UCLQuantum Yes, that's right.""]",https://arxiv.org/abs/2107.05580,"The continuous-time quantum walk is a particle evolving by Schr\""odinger's equation in discrete space. Encoding the space as a graph of vertices and edges, the Hamiltonian is proportional to the discrete Laplacian. In some physical systems, however, the Hamiltonian is proportional to the adjacency matrix instead. It is well-known that these quantum walks are equivalent when the graph is regular, i.e., when each vertex has the same number of neighbors. If the graph is irregular, however, the quantum walks evolve differently. In this paper, we show that for some irregular graphs, if the particle is initially localized at a certain vertex, the probability distributions of the two quantum walks are identical, even though the amplitudes differ. We analytically prove this for a graph with five vertices and a graph with six vertices. By simulating the walks on all 1,018,689,568 simple, connected, irregular graphs with eleven vertices or less, we found sixty-four graphs with this notion of equivalence. We also give eight infinite families of graphs supporting these equivalent walks. ",Equivalent Laplacian and Adjacency Quantum Walks on Irregular Graphs
112,1414660683740979201,1001049754787368960,Dr. Yu-Dai Tsai,"['Fun fact #1 about asteroids, celebrating our new paper ""Asteroid g-2 Experiments"", <LINK>:\nProfessor Moriarty has a book on ""The Dynamics of an Asteroid"" #Sherlock \n@NASAExoplanets @esa @ESAGaia @lucavisinelli @SunnyVagnozzi @exploreplanets  \n@AsteroidWatch', ""We did not cite it because we don't know which asteroid it is (probably Icarus?)""]",https://arxiv.org/abs/2107.04038,"We study for the first time the possibility of probing long-range fifth forces utilizing asteroid astrometric data, via the fifth force-induced orbital precession. We examine nine Near-Earth Object (NEO) asteroids whose orbital trajectories are accurately determined via optical and radar astrometry. Focusing on a Yukawa-type potential mediated by a new gauge field (dark photon) or a baryon-coupled scalar, we estimate the sensitivity reach for the fifth-force coupling strength and mediator mass in the mass range $m \simeq 10^{-21}-10^{-15}\,{\rm eV}$. Our estimated sensitivity is comparable to leading limits from torsion balance experiments, potentially exceeding these in a specific mass range. The fifth forced-induced precession increases with the orbital semi-major axis in the small $m$ limit, motivating the study of objects further away from the Sun. We discuss future exciting prospects for extending our study to more than a million asteroids (including NEOs, main-belt asteroids, Hildas, and Jupiter Trojans), as well as trans-Neptunian objects and exoplanets. ",Asteroid astrometry as a fifth-force and ultralight dark sector probe
113,1414566474266693633,816198577517133824,Benedikt Sabass,"['New preprint where we systematically study a physical mechanism for force-dependent growth of focal adhesions. Any comments for improving the paper? <LINK>', 'https://t.co/uQ6ELwvkW2']",https://arxiv.org/abs/2107.03714,"Mechanical loading generally weakens adhesive structures and eventually leads to their rupture. However, biological systems can adapt to loads by strengthening adhesions, which is essential for maintaining the integrity of tissue and whole organisms. Inspired by cellular focal adhesions, we suggest here a generic, molecular mechanism that allows adhesion systems to harness applied loads for self-stabilization under non-equilibrium conditions -- without any active feedback involved. The mechanism is based on conformation changes of adhesion molecules that are dynamically exchanged with a reservoir. Tangential loading drives the occupation of some stretched conformation states out of equilibrium, which, for thermodynamic reasons, leads to association of further molecules with the adhesion cluster. Self-stabilization robustly increases adhesion lifetimes in broad parameter ranges. Unlike for catch-bonds, bond dissociation rates do not decrease with force. The self-stabilization principle can be realized in many ways in complex adhesion-state networks; we show how it naturally occurs in cellular adhesions involving the adaptor proteins talin and vinculin. ","A generic self-stabilization mechanism for biomolecular adhesions under
  load"
114,1414553858588696577,972555245179064320,Jordy de Vries,"['New paper out: <LINK> .\n\nThe strong CP problem is the problem that the theta angle is very small (&lt;10^-10) as indicated by measurements of the electric dipole moment of the neutron and mercury atom. It is not clear why this angle is so small. 1/n', 'The Standard Model has a remarkable property that while the electroweak sector violates CP, this does not transfer into the strong sector. That is, electroweak loops renormalize the theta term at a minuscule level well below 10^-10. 2/n', 'So once you pick theta small in the SM at any scale, it remains small. This is not true in generic new physics models iff they have more sources of CP violation, which is often the case and also expected because of the universal missing antimatter. 3/n', 'If so, theta does get large corrections. It is not sufficient to pick theta small at one scale as it will become large again due to loops. In such models, you need to solve the strong CP problem at low energies, in the infrared, and the only known game in town are axions. 4/n', 'Remarkably the low-energy pattern of CP violation contains hints whether the strong CP problem was solved in the ultraviolet (for instance due to explicit symmetries) or the infrared (axions). This pattern can be seen by ratios of Electric Dipole Moments of various systems. 5/n', 'For instance, the EDM of atomic Ra over the neutron EDM is very telling. In a ultraviolet solution of strong CP we predict the gray band. Whereas in a model with extra CP violation, (left-right model as example), where an axion mechanism is required we obtain the red band. 6/n https://t.co/jLnbUfz1fZ', 'In this way, one can find indirect evidence for a Peccei-Quinn mechanism and the associated axions without ever measuring an axion directly. One only studies electric dipole moments of various systems. The paper discusses all details and more examples. 7/7.']",https://arxiv.org/abs/2107.04046,"Effective field theory arguments suggest that if BSM sectors contain new sources of CP-violation that couple to QCD, these sources will renormalize the $\theta$ term and frustrate ultraviolet solutions to the strong CP problem. Simultaneously, they will generate distinctive patterns of low-energy electric dipole moments in hadronic, nuclear, atomic, and molecular systems. Observing such patterns thus provides evidence that strong CP is solved by an infrared relaxation mechanism. We illustrate the renormalization of $\theta$ and the collections of EDMs generated in a several models of BSM physics, confirming effective field theory expectations, and demonstrate that measurements of ratios of electric dipole moments at planned experiments can provide valuable input on the resolution of the strong CP problem. ",Uncovering an Axion Mechanism with the EDM Portfolio
115,1414529762656145412,1253004130441928704,Francis DiTraglia,['Hot off the presses: a new working paper with co-authors in biostats at Cambridge &amp; Imperial extending my past work on instrument selection and applying it to mendelian randomization:\n<LINK>\nFun to learn about new (to me) applications of IV!'],https://arxiv.org/abs/2107.01513,"Mendelian randomization (MR) is a widely-used method to identify causal links between a risk factor and disease. A fundamental part of any MR analysis is to choose appropriate genetic variants as instrumental variables. Current practice usually involves selecting only those genetic variants that are deemed to satisfy certain exclusion restrictions, in a bid to remove bias from unobserved confounding. Many more genetic variants may violate these exclusion restrictions due to unknown pleiotropic effects (i.e. direct effects on the outcome not via the exposure), but their inclusion could increase the precision of causal effect estimates at the cost of allowing some bias. We explore how to optimally tackle this bias-variance trade-off by carefully choosing from many weak and locally invalid instruments. Specifically, we study a focused instrument selection approach for publicly available two-sample summary data on genetic associations, whereby genetic variants are selected on the basis of how they impact the asymptotic mean square error of causal effect estimates. We show how different restrictions on the nature of pleiotropic effects have important implications for the quality of post-selection inferences. In particular, a focused selection approach under systematic pleiotropy allows for consistent model selection, but in practice can be susceptible to winner's curse biases. Whereas a more general form of idiosyncratic pleiotropy allows only conservative model selection, but offers uniformly valid confidence intervals. We propose a novel method to tighten honest confidence intervals through support restrictions on pleiotropy. We apply our results to several real data examples which suggest that the optimal selection of instruments does not only involve biologically-justified valid instruments, but additionally hundreds of potentially pleiotropic variants. ","Selection of invalid instruments can improve estimation in Mendelian
  randomization"
116,1414507433905508353,1134375290581524480,Kai Schmitz,"['New paper on the arXiv: <LINK>. We consider metastable cosmic strings, which are predicted by many GUT models, and compute for the first time the GW spectrum from string loops and segments. More details in my talk today at this workshop: <LINK> <LINK>']",https://arxiv.org/abs/2107.04578,"A metastable cosmic-string network is a generic consequence of many grand unified theories (GUTs) when combined with cosmic inflation. Metastable cosmic strings are not topologically stable, but decay on cosmic time scales due to pair production of GUT monopoles. This leads to a network consisting of metastable long strings on superhorizon scales as well as of string loops and segments on subhorizon scales. We compute for the first time the complete stochastic gravitational-wave background (SGWB) arising from all these network constituents, including several technical improvements to both the derivation of the loop and segment contributions. We find that the gravitational waves emitted by string loops provide the main contribution to the gravitational-wave spectrum in the relevant parameter space. The resulting spectrum is consistent with the tentative signal observed by the NANOGrav and Parkes pulsar timing collaborations for a string tension of G\mu ~ 10^-11...-7 and has ample discovery space for ground- and space-based detectors. For GUT-scale string tensions, G\mu ~ 10^-8...-7, metastable strings predict a SGWB in the LIGO-Virgo-KAGRA band that could be discovered in the near future. ",Stochastic gravitational-wave background from metastable cosmic strings
117,1414498922584711171,927837253,Emtiyaz Khan,"['Our new paper on ""The Bayesian Learning Rule"" is now on arXiv, where we provide a common learning-principle behind a variety of learning algorithms (optimization, deep learning, and graphical models). \n<LINK>\nGuess what, the principle is Bayesian. A very long🧵 <LINK>', 'The main message is that many algorithm, that have ""stood-the-test-of-time"", can all be seen as solving a two-step scheme where\n1) A Bayesian objective is optimized to find posterior approximations\n2) Natural-gradient descent is used for optimization (Bayesian learning rule)\n2/ https://t.co/DB4JMGtO3d', 'An important point of the paper is that ""natural-gradients"" are a *must* here, be it Bayesian/non-Bayesian. For Bayes, natural-gradients (or some kind of Fisher scaling) are always part of the solution. It is very unfortunate that this point is not appreciated in ML (yet). 3/ https://t.co/I00sNLZmAO', 'Natural-gradients arise due to the Entropy, which is also the reason for exponential-weighting in Bayes rule (or in maxEntropy principle). \n\nThe rest of the paper is really showing the importance of the entropy, in automatically determining the *complexity* of the algorithm. 4/ https://t.co/PHN9LbvIa6', 'How is this done? The main point is very simple: natural gradients assign ""essential"" higher-order information of the loss to the appropriate natural parameters. \n\nThe paper demonstrates the important of this single equation, through a series of examples. 5/ https://t.co/L5Ci0cPmyU', 'My favorite (and simplest) example is the Ridge Regression, where the 2nd-derivative of the loss is assinged to inverse-covariance (S), and a mix of 1st and 2nd deriv to defined the mean. 6/ https://t.co/SPDGfRgS9f', 'The Newton\'s method example clearly shows the mix of 1st and 2nd derivative in the ""first"" natural-gradient, and how they are ultimately assigned to the mean and covariance via a Newton-like update. 7/ https://t.co/jxDRiwsxDf', ""This is the message that's repeated throughout the paper. \n\nYou need to make two decisions: first, the form of the post. approx. and second, the natgrad approx. \n\nWith this construction the Bayesian Learning Rule reduces to your favorite algorithm. See the table 👇🏿8/ https://t.co/25yj7opbIE"", 'This work is very similar to Yann Olivier et al. Information Geometric Optimization (although they focus on black-box functions, and we derive learning algorithms). Here we argue to *always* optimize the same Bayesian objective, which is a(n important) fundamental difference.9/ https://t.co/fEIDtdagZA', 'If you are from the optimization field, I will encourage reading Section 3, where we show that the second order optimality condition is a special case of the Bayes with Gaussian approx.\n\nWe then show that using Gauss-mixture enables you to do multimodal optimization. 10/ https://t.co/Ut1e4pfztq', 'Multimodal optimization is an interesting one. Here the entropy of mixture automatically enables exploration, where each component is encouraged to take responsibility of a different region.11/ https://t.co/Hm8dC5tbgm', 'The responsibility function appears because of the (guess what) the entropy of the mixture.12/ https://t.co/uZxLSewAA0', ""Another of my favorite example is that Dropout for Newton's method, which also pops out with a spike-and-slab mixture. This is extending @yaringal and @ZoubinGhahrama1 MC dropout to more flexible choices of distribution (the entropy is again the key here) 13/ https://t.co/DKBFesfEot"", 'The paper contains a series of example from deep learning, but probabilistic graphical models is my favorite one where we show all exp-family conjugate models, EM, SVI, VMP, etc as special cases. The paper ends with our original CVI algorithm. https://t.co/nMMXFqj8qb 14/ https://t.co/2Gs9Q8uRD9', ""I really hope that the non-conjugate VI community will pay attention. I have seen so many papers in the last 4-5 years that could have done so much better with natural-gradients, but still ended up using some ad-hoc methods (and didn't even know about our work). 15/"", ""We also have a small comment about connections to online learning. Bayes and online learning historically have had similar proposals, but somehow the two community do not benefit from each others' work as much as they should. 16/ https://t.co/Nrmc42in8y"", 'The thread is getting too long, so I will end it here with a bit of history. \n\nOriginally, we came up with the main idea in 2016 with Wu Lin, when we wrote this paper. https://t.co/nMMXFqj8qb. At the time, I thought I was done. 17/', 'Over the years, I realized that the paper is very difficult to understand for the community (too dense). So I wrote a follow up with @nielsen_didrik \nhttps://t.co/Uy7huQA5Gj\nBut this also got a bit lost. 18/', ""I realized that people don't see that whenever they are solving a VI problem, they are in fact computing natural-gradients. This became a struggle then to convince people, that almost everything that you need for exp-family is all in those paper. 19/"", 'So I wrote a few more papers, focusing only on the Gaussian case. https://t.co/G3o5yKXqc6 \n\nAnd then I sort of gave up. It was too tiring to convince the community. Nobody seemed to see the point. It was a bit depressing point in my life. 20/', 'In 2019, Haavard Rue came to visit me, and we kept talking about this and we came up with this story to communicate the importance of natural-gradients. This is how this current paper is written. 21/', 'Soon after, some people saw value in my work and invited me to give a NeurIPS tutorial, and I decided to entirely focus on ""Bayesian principles"", which I felt in the end some people appreciated, and I was relieved. 22/\n\nhttps://t.co/KhaDTO882T', 'Since Feb 2020, until now, we worked on the writing of this paper. Overall, this work took 4 years of work and 1.5 years of writing. I enjoyed it thoroughly.  And now I hope now the community will benefit from this work. \n\n23/', 'I must thank many people who worked with me over the years, and also my current group. Without their help and support, I would have given up a long time ago. \n\nCheck them out here https://t.co/ahUTEdJzKF\n\n/end']",https://arxiv.org/abs/2107.04562,"We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones. ",The Bayesian Learning Rule
118,1414446001935593478,976155561522794497,Juliano César Silva Neves,['My new paper discusses 5-dimensional regular black holes! Our world conceived of as a 4d brane immersed into a 5-dimensional spacetime. #BlackHole \n<LINK>'],https://arxiv.org/abs/2107.04072,"Following a recent approach, complete and analytic solutions (brane and bulk) of regular black holes are shown in a brane context. The metrics are regular both on the four-dimensional brane and in the five-dimensional bulk. Like many brane world scenarios, the bulk spacetime is asymptotically anti-de Sitter. On the other hand, a de Sitter core on the brane avoids the singularity inside the event horizon, providing then well-known regular black holes on the brane. From the bulk perspective, the regular black holes are five-dimensional objects, with the event horizon extending to the extra dimension, but the de Sitter core is entirely on the four-dimensional brane. ",Five-dimensional regular black holes in a brane world
119,1413857819280003076,1077995761487568896,Jon Miller,"['New paper!  Paul Draghis @paul_andrei97 measured the spin of the black hole in XTE J1908+094 with @NASANuSTAR , and the orientation of the inner disk.  The spin is intermediate &amp; the orientation reveals some difficulties in obtaining values from jets.\n<LINK>']",https://arxiv.org/abs/2107.02810,"NuSTAR observed the black hole candidate XTE J1908$+$094 during its 2013 and 2019 outbursts. We use relativistic reflection to measure the spin of the black hole through 19 different assumptions of relxill flavors and parameter combinations. The most favored model in terms of Deviance Information Criterion (DIC) measures the spin of the black hole to be $a = 0.55^{+0.29}_{-0.45}$, and an inclination of $\theta=27^{+2}_{-3}$ degrees ($1\sigma$ statistical errors). We look at the effects of coronal geometry assumptions and density of the accretion disk on the spin prediction. All 19 tested models provide consistent spin estimates. We discuss the evolution of spin measurement techniques using relativistic reflection in X-ray binaries and discuss the implications of this spin measurement in reconciling the distributions of stellar mass black hole spin measurements made through X-ray and gravitational wave observations. ",The Spin and Orientation of the Black Hole in XTE J1908$+$094
120,1413581695991914501,1172283476919422977,Ugur Kayas,['Check out our new paper on approaching dynamic programming algorithms with fundamental algebraic theory\n<LINK> <LINK>'],https://arxiv.org/abs/2107.01752,"Dynamic programming (DP) is a broadly applicable algorithmic design paradigm for the efficient, exact solution of otherwise intractable, combinatorial problems. However, the design of such algorithms is often presented informally in an ad-hoc manner, and as a result is often difficult to apply correctly. In this paper, we present a rigorous algebraic formalism for systematically deriving novel DP algorithms, either from existing DP algorithms or from simple functional recurrences. These derivations lead to algorithms which are provably correct and polymorphic over any semiring, which means that they can be applied to the full scope of combinatorial problems expressible in terms of semirings. This includes, for example: optimization, optimal probability and Viterbi decoding, probabilistic marginalization, logical inference, fuzzy sets, differentiable softmax, and relational and provenance queries. The approach, building on many ideas from the existing literature on constructive algorithmics, exploits generic properties of (semiring) polymorphic functions, tupling and formal sums (lifting), and algebraic simplifications arising from constraint algebras. We demonstrate the effectiveness of this formalism for some example applications arising in signal processing, bioinformatics and reliability engineering. ",Polymorphic dynamic programming by algebraic shortcut fusion
121,1413560370002755586,1324170692401639424,Robert McGehee,['New DM paper! We built a minimal dark sector to add (heavier) ADM to any of your favorite baryogenesis scenarios. Comes with direct detection and visibly decaying dark photons at no extra charge! \n\n<LINK>'],https://arxiv.org/abs/2107.03398,"It is often said that asymmetric dark matter is light compared to typical weakly interacting massive particles. Here we point out a simple scheme with a neutrino portal and $\mathcal{O}(60 \text{ GeV})$ asymmetric dark matter which may be ''added'' to any standard baryogenesis scenario. The dark sector contains a copy of the Standard Model gauge group, as well as (at least) one matter family, Higgs, and right-handed neutrino. After baryogenesis, some lepton asymmetry is transferred to the dark sector through the neutrino portal where dark sphalerons convert it into a dark baryon asymmetry. Dark hadrons form asymmetric dark matter and may be directly detected due to the vector portal. Surprisingly, even dark anti-neutrons may be directly detected if they have a sizeable electric dipole moment. The dark photons visibly decay at current and future experiments which probe complementary parameter space to dark matter direct detection searches. Exotic Higgs decays are excellent signals at future $e^+ e^-$ Higgs factories. ",Asymmetric Dark Matter May Not Be Light
122,1413419040639524867,1171357907574824961,Łukasz Tychoniec,"['There it goes! Our new paper presenting an overview of the chemical tracers in protostellar systems is out! Thanks to amazing ALMA observatory @almaobs  we can map stellar nurseries at Solar System scales. \n<LINK> <LINK>', 'I like to think about it as a map that we will use as a reference when observing those systems with JWST at comparable resolution. @JWSTObserver  Many of sources covered in this paper are in a queue for the Cycle 1 observations!']",https://arxiv.org/abs/2107.03696,"The physical and chemical conditions in Class 0/I protostars are fundamental in unlocking the protostellar accretion process and its impact on planet formation. The aim is to determine which physical components are traced by different molecules at sub-arcsecond scales (100 - 400 au). We use a suite of Atacama Large Millimeter/submillimeter Array (ALMA) datasets in Band 6 (1 mm), Band 5 (1.8 mm) and Band 3 (3 mm) at spatial resolutions 0.5 - 3"" for 16 protostellar sources. The protostellar envelope is well traced by C$^{18}$O, DCO$^+$ and N$_2$D$^+$, with the freeze-out of CO governing the chemistry at envelope scales. Molecular outflows are seen in classical shock tracers like SiO and SO, but ice-mantle products such as CH$_3$OH and HNCO released with the shock are also observed. The molecular jet is prominent not only in SiO and SO but also occasionally in H$_2$CO. The cavity walls show tracers of UV-irradiation such as C$_2$H c-C$_3$H$_2$ and CN. The hot inner envelope, apart from showing emission from complex organic molecules (COMs), also presents compact emission from small molecules like H$_2$S, SO, OCS and H$^{13}$CN, most likely related to ice sublimation and high-temperature chemistry. Sub-arcsecond millimeter-wave observations allow to identify those (simple) molecules that best trace each of the physical components of a protostellar system. COMs are found both in the hot inner envelope (high excitation lines) and in the outflows (lower-excitation lines) with comparable abundances. COMs can coexist with hydrocarbons in the same protostellar sources, but they trace different components. In the near future, mid-IR observations with JWST-MIRI will provide complementary information about the hottest gas and the ice mantle content, at unprecedented sensitivity and at resolutions comparable to ALMA for the same sources. ",Which molecule traces what: chemical diagnostics of protostellar sources
123,1413346374893117448,937150008050245632,Baoyu Zhou,['New paper out!!\n\n“Inexact Sequential Quadratic Optimization for Minimizing a Stochastic Objective Function Subject to Deterministic Nonlinear Equality Constraints” (coauthors @frank_e_curtis and Daniel Robinson)\n\n<LINK>'],http://arxiv.org/abs/2107.03512,"An algorithm is proposed, analyzed, and tested experimentally for solving stochastic optimization problems in which the decision variables are constrained to satisfy equations defined by deterministic, smooth, and nonlinear functions. It is assumed that constraint function and derivative values can be computed, but that only stochastic approximations are available for the objective function and its derivatives. The algorithm is of the sequential quadratic optimization variety. A distinguishing feature of the algorithm is that it allows inexact subproblem solutions to be employed, which is particularly useful in large-scale settings when the matrices defining the subproblems are too large to form and/or factorize. Conditions are imposed on the inexact subproblem solutions that account for the fact that only stochastic objective gradient estimates are available. Convergence results in expectation are established for the method. Numerical experiments show that it outperforms an alternative algorithm that employs highly accurate subproblem solutions in every iteration. ","Inexact Sequential Quadratic Optimization for Minimizing a Stochastic
  Objective Function Subject to Deterministic Nonlinear Equality Constraints"
124,1413227670222131200,1004716949346152448,Patrick David,['Here’s the GitHub CoPilot paper just released! 👇👇@OpenAI \n\nInterestingly it also includes HumanEval a new evaluation set.\n\n<LINK>\n\n<LINK>\n\n#copilot #NLProc #gpt3 #Transformers #deeplearning'],https://arxiv.org/abs/2107.03374,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. ",Evaluating Large Language Models Trained on Code
125,1413200452993261573,2364317023,Dorsa Sadigh,"['Making assistive teleoperation intuitive, easy to operate, and precise! The journal version of our work on the framework of learned latent actions + shared autonomy + personalization is out.  We also have new studies with users with disability.\nPaper: <LINK> <LINK>', 'This will appear at Journal of Autonomous Robots. \nIn collaboration with @loseydp, Hong Jun Jeon, @MengxiLi3, @krishpopdesu, @AjayMandlekar, @animesh_garg, @leto__jean']",https://arxiv.org/abs/2107.02907,"Assistive robot arms enable people with disabilities to conduct everyday tasks on their own. These arms are dexterous and high-dimensional; however, the interfaces people must use to control their robots are low-dimensional. Consider teleoperating a 7-DoF robot arm with a 2-DoF joystick. The robot is helping you eat dinner, and currently you want to cut a piece of tofu. Today's robots assume a pre-defined mapping between joystick inputs and robot actions: in one mode the joystick controls the robot's motion in the x-y plane, in another mode the joystick controls the robot's z-yaw motion, and so on. But this mapping misses out on the task you are trying to perform! Ideally, one joystick axis should control how the robot stabs the tofu and the other axis should control different cutting motions. Our insight is that we can achieve intuitive, user-friendly control of assistive robots by embedding the robot's high-dimensional actions into low-dimensional and human-controllable latent actions. We divide this process into three parts. First, we explore models for learning latent actions from offline task demonstrations, and formalize the properties that latent actions should satisfy. Next, we combine learned latent actions with autonomous robot assistance to help the user reach and maintain their high-level goals. Finally, we learn a personalized alignment model between joystick inputs and latent actions. We evaluate our resulting approach in four user studies where non-disabled participants reach marshmallows, cook apple pie, cut tofu, and assemble dessert. We then test our approach with two disabled adults who leverage assistive devices on a daily basis. ",Learning Latent Actions to Control Assistive Robots
126,1413067820426940420,2298327384,Sourav Garg,"['Not all ""keypoints"" are created equal!\nCheck out our new #IEEE #RAL &amp; #IROS2021 paper: <LINK> for estimating relative weighting of #keypoints to enable better #placerecognition/#localization with reduced #storage/#compute <LINK>', 'Led by @Nik__V__ from @IITISM_DHANBAD #India and @QUTRobotics #Australia @maththrills']",https://arxiv.org/abs/2107.02440,"Visual Place Recognition (VPR) approaches have typically attempted to match places by identifying visual cues, image regions or landmarks that have high ``utility'' in identifying a specific place. But this concept of utility is not singular - rather it can take a range of forms. In this paper, we present a novel approach to deduce two key types of utility for VPR: the utility of visual cues `specific' to an environment, and to a particular place. We employ contrastive learning principles to estimate both the environment- and place-specific utility of Vector of Locally Aggregated Descriptors (VLAD) clusters in an unsupervised manner, which is then used to guide local feature matching through keypoint selection. By combining these two utility measures, our approach achieves state-of-the-art performance on three challenging benchmark datasets, while simultaneously reducing the required storage and compute time. We provide further analysis demonstrating that unsupervised cluster selection results in semantically meaningful results, that finer grained categorization often has higher utility for VPR than high level semantic categorization (e.g. building, road), and characterise how these two utility measures vary across different places and environments. Source code is made publicly available at this https URL ","A Hierarchical Dual Model of Environment- and Place-Specific Utility for
  Visual Place Recognition"
127,1412771746290229248,772809603046334464,Jonathan Mackey,['New @hesstelescopes and @HAWC_Obs paper comparing sources in the Galactic plane as observed by the two instruments: <LINK>\n@davit_zargaryan @DIASAstronomy #DIASdiscovers'],https://arxiv.org/abs/2107.01425,"The High Altitude Water Cherenkov (HAWC) observatory and the High Energy Stereoscopic System (H.E.S.S.) are two leading instruments in the ground-based very-high-energy gamma-ray domain. HAWC employs the water Cherenkov detection (WCD) technique, while H.E.S.S. is an array of Imaging Atmospheric Cherenkov Telescopes (IACTs). The two facilities therefore differ in multiple aspects, including their observation strategy, the size of their field of view and their angular resolution, leading to different analysis approaches. Until now, it has been unclear if the results of observations by both types of instruments are consistent: several of the recently discovered HAWC sources have been followed up by IACTs, resulting in a confirmed detection only in a minority of cases. With this paper, we go further and try to resolve the tensions between previous results by performing a new analysis of the H.E.S.S. Galactic plane survey data, applying an analysis technique comparable between H.E.S.S. and HAWC. Events above 1 TeV are selected for both datasets, the point spread function of H.E.S.S. is broadened to approach that of HAWC, and a similar background estimation method is used. This is the first detailed comparison of the Galactic plane observed by both instruments. H.E.S.S. can confirm the gamma-ray emission of four HAWC sources among seven previously undetected by IACTs, while the three others have measured fluxes below the sensitivity of the H.E.S.S. dataset. Remaining differences in the overall gamma-ray flux can be explained by the systematic uncertainties. Therefore, we confirm a consistent view of the gamma-ray sky between WCD and IACT techniques. ",TeV emission of Galactic plane sources with HAWC and H.E.S.S
128,1412693788208869377,51700215,Phil Bull,"['New paper: After a serendipitous discovery, and a mini rollercoaster of improvements and setbacks, Mel Irfan and I report that Kernel PCA is a pretty decent method for removing foregrounds from autocorrelation 21cm intensity maps! <LINK>', ""Kernel PCA is a non-linear extension of Principal Component Analysis. It *implicitly* takes non-linear transformations of the data, resulting in a higher-dimensional space where it's easier to separate different kinds of signal using a regular PCA decomposition."", ""We never have to explicitly work in the higher-dimensional space because of the 'kernel trick', which means we can get away with only needing to evaluate some scalar products by plugging the original data vectors into some choice of kernel (hence the name, Kernel PCA)."", 'The non-linear transformation is actually implicitly defined by the kernel, so this choice (and the hyperparameters of the kernel) are responsible for determining the performance of the component separation. Some kernels might pick out smooth vs localised structure for example.', 'We tried a few different kernels on some simulations with different foreground models, a realistic beam, masks etc. and found that KPCA tends to outperform PCA in most (but not all) scenarios.', 'I think it will be useful for comparison -- when applying foreground removal to real, complicated data, all sorts of extra junk (systematics) could be partially filtered, leaving complex residuals that are hard to characterise. If two very different FG removal methods agree...', ""...you can feel more confident that your residuals are under control. As we show, KPCA and PCA behave quite differently, so if they recover consistent signals, it's a strong test."", 'Code available here! It does the cosmo + foreground simulations, instrumental effects, and FG filtering https://t.co/xGIaZaQiKC', ""@WesselValk I think there must be some similarities -- they're both kernel methods, but I guess KPCA is more similar to normal PCA in that it's doing an eigendecomposition of the covariance rather than fitting the data.""]",https://arxiv.org/abs/2107.02267,"The high dynamic range between contaminating foreground emission and the fluctuating 21cm brightness temperature field is one of the most problematic characteristics of 21cm intensity mapping data. While these components would ordinarily have distinctive frequency spectra, making it relatively easy to separate them, instrumental effects and calibration errors further complicate matters by modulating and mixing them together. A popular class of foreground cleaning method are unsupervised techniques related to Principal Component Analysis (PCA), which exploit the different shapes and amplitudes of each component's contribution to the covariance of the data in order to segregate the signals. These methods have been shown to be effective at removing foregrounds, while also unavoidably filtering out some of the 21cm signal too. In this paper we examine, for the first time in the context of 21cm intensity mapping, a generalised method called Kernel PCA, which instead operates on the covariance of non-linear transformations of the data. This allows more flexible functional bases to be constructed, in principle allowing a cleaner separation between foregrounds and the 21cm signal to be found. We show that Kernel PCA is effective when applied to simulated single-dish (autocorrelation) 21cm data under a variety of assumptions about foregrounds models, instrumental effects etc. It presents a different set of behaviours to PCA, e.g. in terms of sensitivity to the data resolution and smoothing scale, outperforming it on intermediate to large scales in most scenarios. ","Cleaning foregrounds from single-dish 21cm intensity maps with Kernel
  Principal Component Analysis"
129,1412619664157446144,1076577039669489664,Andreas Blattmann,"['Have you ever looked at a picture and wished you could bring it to life? If so, you should definitely check out our new paper ‘iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis’.\n\narxiv: <LINK>\nProject page: <LINK> <LINK>', 'We present iPOKE, a model for locally controlled, stochastic video synthesis based on poking a single pixel in a static scene, that enables users to animate still images only with simple mouse drags.\n\nJ/w @timoMil, @mdorkenw and B.Ommer.\ncode: https://t.co/eRARsZmtXv https://t.co/ScrOrrEdcj']",https://arxiv.org/abs/2107.02790,"How would a static scene react to a local poke? What are the effects on other parts of an object if you could locally push it? There will be distinctive movement, despite evident variations caused by the stochastic nature of our world. These outcomes are governed by the characteristic kinematics of objects that dictate their overall motion caused by a local interaction. Conversely, the movement of an object provides crucial information about its underlying distinctive kinematics and the interdependencies between its parts. This two-way relation motivates learning a bijective mapping between object kinematics and plausible future image sequences. Therefore, we propose iPOKE -- invertible Prediction of Object Kinematics -- that, conditioned on an initial frame and a local poke, allows to sample object kinematics and establishes a one-to-one correspondence to the corresponding plausible videos, thereby providing a controlled stochastic video synthesis. In contrast to previous works, we do not generate arbitrary realistic videos, but provide efficient control of movements, while still capturing the stochastic nature of our environment and the diversity of plausible outcomes it entails. Moreover, our approach can transfer kinematics onto novel object instances and is not confined to particular object classes. Our project page is available at this https URL ",iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis
130,1412478535013257218,25263396,Durk Kingma,"['New paper: Variational Diffusion Models (VDMs)!\n<LINK>\n \n✅ New general insights into diffusion models\n✅ Simple objective\n✅ Fast optimization &amp; anytime synthesis\n✅ SotA likelihoods &amp; lossless compression\n\nWork with @TimSalimans @poolio @hojonathanho \n(1/n) <LINK>', '1⃣👀 For a model whose parameters and hyper-parameters are purely optimized towards the maximum likelihood objective (= compression), image quality is pretty great:\n(2/n) https://t.co/uho8RQKCOX', '2⃣The proposed methods can also be used to optimize a *weighted* likelihood bound, similar to earlier diffusion models, which can greatly improve perceptual quality (FID 7.4 -&gt; 4.0 or better) at the expensive of raw likelihood.\n(3/n)', '3⃣We show a simple way to optimize the noise schedule jointly with the rest of the model, turning the model into an (infinitely deep) VAE, but with fast optimization and any-time synthesis.\n(4/n)', '4⃣We develop new (and useful!) insights:\n▶️ We prove that the variational lower bound (VLB) of diff. models simplifies to a very short expression.\n▶️ We prove that VLB is almost completely invariant to the noise schedule (but can minimize its variance!)\n▶️ More in paper.\n(5/n)', '5⃣We show that Fourier Features can greatly improve diffusion model likelihoods. Conversely, they did not help PixelCNN++ model, an autoregressive model we tried it on.\n(6/n) https://t.co/IMqZdIULMH', '6⃣We show how to turn the model into a lossless compression scheme, resulting in great net compression rates. (Something to explore further!) \n7⃣Comparison with previous likelihood-based models, in terms of bits per dimension:\n(7/n) https://t.co/YzGKxC4dSV', '8⃣Training is pretty fast! Our CIFAR-10 model surpasses 2.80 bpd in 2.5 hrs w/ 8 GPUs; versus 2 days for the best published transformer with equivalent hardware. Synthesis is quite a bit slower though; something to tackle in future work.\n(8/n)', ""9⃣We're working on publishable code 🖥️🐒. Basic implementation of the model is actually quite simple, hope to release soon!\n(9/n)"", 'P.S. Lots of progress in diffusion-based generative models lately; please check our paper for a brief discussion of the excellent earlier and concurrent work in this area. Still lots of open ends in this area of research though!\n(10/10)', '@yassersouri @TimSalimans @poolio @hojonathanho Great question! The continuous-time model is trained with T=∞, but we can synthesize with any finite discretization T, where lower T gives worse samples but is also faster.']",https://arxiv.org/abs/2107.00630,"Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. ",Variational Diffusion Models
131,1412452760000380933,960527787428900864,Michael Albergo,['We have a new paper led by Dan Hackett that tackles multimodal sampling with flows and composite HMC/flow algorithms in lattice field theory! Check it out: <LINK> <LINK>'],http://arxiv.org/abs/2107.00734,"Recent results have demonstrated that samplers constructed with flow-based generative models are a promising new approach for configuration generation in lattice field theory. In this paper, we present a set of methods to construct flow models for targets with multiple separated modes (i.e. theories with multiple vacua). We demonstrate the application of these methods to modeling two-dimensional real scalar field theory in its symmetry-broken phase. In this context we investigate the performance of different flow-based sampling algorithms, including a composite sampling algorithm where flow-based proposals are occasionally augmented by applying updates using traditional algorithms like HMC. ",Flow-based sampling for multimodal distributions in lattice field theory
132,1412435996344467463,1019760963569049601,Almog Yalinewich,"['1/3 New paper on the arxiv with my former student Andrey Remorov. We report a novel, universal conservation law for strong explosions in steep atmospheric density gradients\n<LINK>', '2/3 We find a simple power law relation between swept up mass and shock velocity. Shown below is a map of the power law index vs adiabatic index and density profile https://t.co/SCKm5469wL', '3/3 This conservation law reproduces experiments and numerical simulations, and we show how it can be used to model many astrophysical problems, such as bolides (e.g. Chelyabinsk event), impact craters, asteroid impact avoidance and hypervelocity white dwarfs']",https://arxiv.org/abs/2107.01701,"In this work we present a mathematical model for the propagation of the shock waves that occur in graded density profiles. These waves can occur in a wide range of astrophysical events, such as collisions in planetary and stellar atmospheres, common envelope explosions and peculiar type Ia supernovae. The behaviour of the shock wave and its evolution can be modelled using type II self similar solutions. In such solutions the evolution of the shock wave is determined by boundary conditions at the shock front and a singular point in the shocked region. We show how the evolution can be determined for different equations of state and density profiles, and compare these results to numerical simulations. These findings are also applied to a variety of astrophysical phenomena to further test their validity. ",The Propagation of Strong Shocks into Planetary and Stellar Atmospheres
133,1412423135215947779,69202541,Jonathan Le Roux,"['New paper out, accepted @INTERSPEECH2021: ""Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech Recognition,"" by Niko Moritz, Takaaki Hori, JL.\nDCN allows to stack layers without overall context growing beyond that of a single layer. (1/3)\n<LINK>', 'DCN maintains two separate sequences of causal and non-causal frames that carefully interact so that the look-ahead size does not increase for a growing number of consecutive DCN layers, in contrast with the delay aggregation of restricted SA architectures. (2/3) https://t.co/biefjiKhPF', 'We combine DCN-based encoder models with triggered attention for streaming end-to-end ASR using transformer- and conformer-based neural network architectures, obtaining state-of-the-art results on the LibriSpeech, HKUST, and Switchboard ASR tasks. (3/3) https://t.co/C6X46AVWZ4']",https://arxiv.org/abs/2107.01269,"Attention-based end-to-end automatic speech recognition (ASR) systems have recently demonstrated state-of-the-art results for numerous tasks. However, the application of self-attention and attention-based encoder-decoder models remains challenging for streaming ASR, where each word must be recognized shortly after it was spoken. In this work, we present the dual causal/non-causal self-attention (DCN) architecture, which in contrast to restricted self-attention prevents the overall context to grow beyond the look-ahead of a single layer when used in a deep architecture. DCN is compared to chunk-based and restricted self-attention using streaming transformer and conformer architectures, showing improved ASR performance over restricted self-attention and competitive ASR results compared to chunk-based self-attention, while providing the advantage of frame-synchronous processing. Combined with triggered attention, the proposed streaming end-to-end ASR systems obtained state-of-the-art results on the LibriSpeech, HKUST, and Switchboard ASR tasks. ","Dual Causal/Non-Causal Self-Attention for Streaming End-to-End Speech
  Recognition"
134,1412322888850497536,791705191175360512,Niels Warburton,"['New paper day! By pushing self-force calculations to second-order in the mass ratio we find it can accurately model the GW flux from black hole binaries with mass ratios as large as 10:1 .  <LINK>. 1/n <LINK>', 'In the above example we compare our self-force (SF) result to NR fluxes from an @SXSProject simulation with q=10. Up to 5 cycles from the merger the relative error in the second-order SF result (2SF) is less than 2e-3.   2/n', 'Closer to the ISCO our two-timescale approximation (https://t.co/7IubudQ7ho) breaks down which gives the downward tick you see in the 2SF flux. In the future attaching a transition to plunge will improve the result in this region. 3/n', 'We can also really abuse perturbation theory and compare fluxes at q=1. Surprisingly our 2SF result does really well even for equal mass binaries! Up to 5 cycles before merger the relative error in the (2,2)-mode flux (compared with NR) is less than 2.5e-3. https://t.co/OkWhC5fcIL', 'We can also look at the subdominant modes. Here the comparison is not quite as good. From examining the PN series we see this is expected as in subdominant modes the nu^4 term (what would be 3SF) enters earlier in the PN series than for the (2,2)-mode.  5/n https://t.co/1USLQljHXr', 'Nonetheless, the agreement between SF and NR at q=10 for the (3,3)-mode is ~1% (relative) 5 cycles before the waveform peak. With a simple resummation that adds information from just the leading-order PN term for a given mode the relative error drops to 4.5e-3. 6/n', 'As the (2,2)-flux dominants the total flux the agreement between SF and NR for the *total* flux remains excellent at 2.5e-3 (relative) up to 5 cycles before the peak. 7/n', 'As a further check we can compare our results to NR as a function of the symmetric mass ratio, nu. After subtracting the SF and NR fluxes we expect the residual to go as nu^4 and indeed this what we observe.  8/n https://t.co/4qQinW2aD5', 'The comparison with the (2,2)-mode is less clean than the (3,3)-mode. This is likely because after subtracting the 2SF fluxes the residual encroaches on the scale of the oscillations in the NR data due to residual eccentricity and the motion of the centre of mass. 9/n', 'In this work we perturb around a non-spinning (Schwarzschild) black hole which mean we can also model binaries where the primary is slowly spinning. We can also add the contribution from spin on to the secondary using results from, e.g., https://t.co/osRAKI9pjt. 10/n', 'When both components are slowly spinning we again find good agreement with @SXSProject simulations with q=8. Here the spin magnitude of both black holes is around 0.1. 11/n https://t.co/1BOH5lr3vD', '@SXSProject For a spinning secondary (such as a black hole) there is no restriction on the spin we can model. Here we show a comparison with an @SXSProject simulation with an almost non-spinning primary and a secondary with spin =~-0.8. Again the agreement is good, even at q=~6.3 https://t.co/PfrqzdXa0F', '@SXSProject We also agreement in the weak field with all the known 3.5PN term. Here we show the residuals after subtracting successive PN terms. The final residual (purple, upside down triangles) should follow the currently unknown 4PN contribution to the (2,2)-mode flux. 13/n https://t.co/omRxwDg6cS', ""This work was completed with Adam Pound, Barry Wardell, Jeremy Miller and Leanne Durkan. What's next? Yesterday at the Marcel Grossman meeting @barry_wardell presented our first results for the waveform. At q=10 the comparison with NR looks amazing. Watch this space!""]",https://arxiv.org/abs/2107.01298,"Within the framework of self-force theory, we compute the gravitational-wave energy flux through second order in the mass ratio for compact binaries in quasicircular orbits. Our results are consistent with post-Newtonian calculations in the weak field and they agree remarkably well with numerical-relativity simulations of comparable-mass binaries in the strong field. We also find good agreement for binaries with a spinning secondary or a slowly spinning primary. Our results are key for accurately modelling extreme-mass-ratio inspirals and will be useful in modelling intermediate-mass-ratio systems. ","Gravitational-wave energy flux for compact binaries through second order
  in the mass ratio"
135,1412319880083394562,1002538331039846400,Nikolaos Syrrakos,['New paper on the arXiv today! Having fun with pentagon Feynman integrals. <LINK>'],https://arxiv.org/abs/2107.02106,"We study several multiscale one-loop five-point families of Feynman integrals. More specifically, we employ the Simplified Differential Equations approach to obtain results in terms of Goncharov polylogarithms of up to transcendental weight four for families with two and three massive external legs and massless propagators, as well as with one massive internal line and up to two massive external legs. This is the first time this computational approach is applied to cases involving internal masses. ","One-loop Feynman integrals for $2\to3$ scattering involving many scales
  including internal masses"
136,1412265221016768513,2596589880,Nikolaus Kriegeskorte,"['New paper led by Andrew Zaharia, with @potnisanish and @alexmwalther: \n“Visualizing the geometry of labeled high-dimensional data with spheres”\n<LINK>', 'Imagine 2 distributions, one big, one small, each uniform within a hypersphere in 200 dimensions. The big one entirely contains the small one &amp; their bounding hyperspheres touch in a point. How do 100 samples from each of these look when visualized with PCA, MDS, or t-SNE in 2D? https://t.co/sWDj5knG9I', 'What if we wanted a visualization that reveals the relationships between the enclosing hyperspheres of multiple distributions that we have labeled data for?', 'Enter hypersphere2sphere (H2S), in which we estimate the enclosing hyperspheres in the original high-dimensional space, then visualize their center separations, radii, and overlaps using spheres in 3D or circles in 2D. https://t.co/bU4GWyBdB7', 'For up to 4 distributions visualized in 3D (or 3 in 2D), the separations, radii, &amp; overlaps (measured as distances along the center connection line) of the high-dim hyperspheres can be expressed exactly in the language of spheres (all centers lie in a 3D or 2D linear subspace). https://t.co/V9cPECJByY', 'When there are more than n+1 labels (where n is the dimensionality of the visualization space, 2 or 3), we minimize the sum of squared errors with which the center separations, radii, and overlaps are expressed in the visualization. https://t.co/Wqm89j6Ldp', 'H2S robustly reveals the relationships among the enclosing hyperspheres, insensitive (unlike PCA, MDS, &amp; t-SNE) to imbalances in the # of samples available for different distributions (e, 100 red, 20 blue) &amp; to changes of the dimensionality of the original data (left, right). https://t.co/h1Cm8RTzFP', 'For the human brain, H2S reveals the disentangling of categories of visual images along stages of representation in the ventral visual pathway (faces: red, bodies: green, objects: blue, scenes: gray), measured here with blood-oxygen-level-dependent functional MRI. https://t.co/fsXbr1NhIN', 'For representations inside a deep neural network model trained to recognize handwritten digits, H2S reveals the disentangling of the representational territories corresponding to different digits. https://t.co/sEqVCjbaDu', 'Note PCA, MDS, &amp; t-SNE do not reveal stagewise disengagement. PCA &amp; MDS suggest overlapping territories up to the penultimate layer 5. t-SNE correctly reveals the distributions to be nonoverlapping already in the input images, but gives no sense of their degree of entanglement. https://t.co/VpmSA3py8a', 'Hinton diagrams show (1) the hypersphere radii, separations, and overlaps along with their visualized values (to reveal distortions), (2) significant separations and overlaps (or margins), and (3) significant differences among separations, among radii, and among overlaps. https://t.co/rr0qqK89Ts', 'Enclosing hyperspheres give a sense of the territories occupied by distributions, even if the distributions have complex shapes.', 'H2S is especially attractive when we do not have enough samples per class for complex distributional models as in many of our studies in visual neuroscience.', 'Overall, hypersphere2sphere (H2S) expresses the global relationships among enclosing hyperspheres in the language of spheres, providing a simple &amp; useful addition to our toolbox of visualization methods for high-dimensional distributions.', '""disengagement"" should be ""disentanglement""']",https://arxiv.org/abs/2107.00731,"Data visualizations summarize high-dimensional distributions in two or three dimensions. Dimensionality reduction entails a loss of information, and what is preserved differs between methods. Existing methods preserve the local or the global geometry of the points, and most techniques do not consider labels. Here we introduce ""hypersphere2sphere"" (H2S), a new method that aims to visualize not the points, but the relationships between the labeled distributions. H2S fits a hypersphere to each labeled set of points in a high-dimensional space and visualizes each hypersphere as a sphere in 3D (or circle in 2D). H2S perfectly captures the geometry of up to 4 hyperspheres in 3D (or 3 in 2D), and approximates the geometry for larger numbers of distributions, matching the sizes (radii), and the pairwise separations (between-center distances) and overlaps (along the center-connection line). The resulting visualizations are robust to sampling imbalances. Leveraging labels and the sphere as the simplest geometrical primitive, H2S provides an important addition to the toolbox of visualization techniques. ",Visualizing the geometry of labeled high-dimensional data with spheres
137,1412148429397057543,226061622,aahlad puli,"['New paper out! (with @lilyhzhang , @ekoermann , and Rajesh Ranganath) <LINK> . TL;DR: We build predictive models in the presence of spurious correlations induced by the relationship between the label and some nuisance variable in the data generating process. <LINK>', 'A classic example occurs in natural images due to the correlation between label and background (nuisance): cows appear on grass and penguins on snow. This relationship is not guaranteed to hold in general. https://t.co/VsrO2b5pqA', 'Such nuisance-induced spurious correlations also occur in safety-critical applications like X-ray classification (see work by @johnrzech et al. https://t.co/7dRVwuQ8IT) with physiological traits outside the lung predicting lung-conditions like pneumonia. https://t.co/wiLZWvfBCo', 'Even when training data has independent nuisance and label, predictive models can perform worse than random prediction on the test distribution.', 'We develop a representation learning algorithm called Nuisance-Randomized Distillation (NURD) to build predictive models that have performance guarantees on any test data (that differs from the training data only in the nuisance-label relationship).', 'NURD builds off a property we identify: conditional independence of the label and the nuisance given the representation guarantees performance for predictive models built on said representation, under a broken nuisance-label relationship', 'NURD has two implementations to break the nuisance-label relationship (generative and reweighting). With non-lung patches as the nuisance, NURD predicts pneumonia in the presence of strong spurious correlations: https://t.co/E9OcGnpyk1']",https://arxiv.org/abs/2107.00520,"In many prediction problems, spurious correlations are induced by a changing relationship between the label and a nuisance variable that is also correlated with the covariates. For example, in classifying animals in natural images, the background, which is a nuisance, can predict the type of animal. This nuisance-label relationship does not always hold, and the performance of a model trained under one such relationship may be poor on data with a different nuisance-label relationship. To build predictive models that perform well regardless of the nuisance-label relationship, we develop Nuisance-Randomized Distillation (NURD). We introduce the nuisance-randomized distribution, a distribution where the nuisance and the label are independent. Under this distribution, we define the set of representations such that conditioning on any member, the nuisance and the label remain independent. We prove that the representations in this set always perform better than chance, while representations outside of this set may not. NURD finds a representation from this set that is most informative of the label under the nuisance-randomized distribution, and we prove that this representation achieves the highest performance regardless of the nuisance-label relationship. We evaluate NURD on several tasks including chest X-ray classification where, using non-lung patches as the nuisance, NURD produces models that predict pneumonia under strong spurious correlations. ","Out-of-distribution Generalization in the Presence of Nuisance-Induced
  Spurious Correlations"
138,1412105767545716742,1556664198,Kyle Cranmer,"[""New paper, it's a beast! Huge effort led by Dan Hackett. We investigate different ways of training flows (reverse &amp; forwards KL) for multi-modal distributions (eg. scalar field theory), combining them with MCMC samplers, &amp; pros/cons of performance metrics\n<LINK> <LINK>""]",https://arxiv.org/abs/2107.00734,"Recent results have demonstrated that samplers constructed with flow-based generative models are a promising new approach for configuration generation in lattice field theory. In this paper, we present a set of methods to construct flow models for targets with multiple separated modes (i.e. theories with multiple vacua). We demonstrate the application of these methods to modeling two-dimensional real scalar field theory in its symmetry-broken phase. In this context we investigate the performance of different flow-based sampling algorithms, including a composite sampling algorithm where flow-based proposals are occasionally augmented by applying updates using traditional algorithms like HMC. ",Flow-based sampling for multimodal distributions in lattice field theory
139,1412032730964049920,894875488094760960,Andrea Dittadi,"['Excited to share our new work on generalization in object-centric learning!\nWe trained popular unsupervised object discovery methods on common multi-object datasets and investigated robustness to a diverse range of distribution shifts.\n\nPaper: <LINK>\n\n[1/5] <LINK>', 'Some highlights:\n\n1) ARI is the only segmentation metric (among those considered in recent literature) that consistently correlates with downstream object property prediction.\n\n2) The reconstruction error appears useful for model selection when masks are not available.\n\n[2/5] https://t.co/Bix2rQ2PZV', '3) If a single object is out-of-distribution, the overall segmentation performance and the downstream prediction of the in-distribution object are not strongly impacted.\n\n[3/5] https://t.co/pUM8SOj9Bn', '4) When global properties of the scene are altered, the segmentation performance can be more significantly impacted, but this depends on the distribution shift. On the other hand, the downstream prediction performance seems to decrease more consistently.\n\n[4/5] https://t.co/MMfvnqeoC2', 'Big thanks to my great collaborators! @oneapra, Michele De Vita, @OleWinther1 at @DTUtweet, and @bschoelkopf &amp; @FrancescoLocat8 at @MPI_IS &amp; @AmazonScience\n\n[5/5]']",http://arxiv.org/abs/2107.00637,"The idea behind object-centric representation learning is that natural scenes can better be modeled as compositions of objects and their relations as opposed to distributed representations. This inductive bias can be injected into neural networks to potentially improve systematic generalization and learning efficiency of downstream tasks in scenes with multiple objects. In this paper, we train state-of-the-art unsupervised models on five common multi-object datasets and evaluate segmentation accuracy and downstream object property prediction. In addition, we study systematic generalization and robustness by investigating the settings where either single objects are out-of-distribution -- e.g., having unseen colors, textures, and shapes -- or global properties of the scene are altered -- e.g., by occlusions, cropping, or increasing the number of objects. From our experimental study, we find object-centric representations to be generally useful for downstream tasks and robust to shifts in the data distribution, especially if shifts affect single objects. ",Generalization and Robustness Implications in Object-Centric Learning
140,1411961279066083330,902992672209633280,Joe Randall,"['Excited to present our latest paper on the arXiv! \n\nWe developed a new quantum simulator based on interacting carbon-13 nuclear spins in diamond, and used it to observe the signatures of a many-body-localized discrete time crystal. Check it out here:\n\n<LINK>', '@cebradley @abobeih @QuTech_news']",https://arxiv.org/abs/2107.00736,"The discrete time crystal (DTC) is a recently discovered phase of matter that spontaneously breaks time-translation symmetry. Disorder-induced many-body-localization is required to stabilize a DTC to arbitrary times, yet an experimental investigation of this localized regime has proven elusive. Here, we observe the hallmark signatures of a many-body-localized DTC using a novel quantum simulation platform based on individually controllable $^{13}$C nuclear spins in diamond. We demonstrate the characteristic long-lived spatiotemporal order and confirm that it is robust for generic initial states. Our results are consistent with the realization of an out-of-equilibrium Floquet phase of matter and establish a programmable quantum simulator based on solid-state spins for exploring many-body physics. ","Observation of a many-body-localized discrete time crystal with a
  programmable spin-based quantum simulator"
141,1411937406954225673,1339508444764786694,Gregor Kasieczka,"['New on arXiv: ""Shared Data and Algorithms for Deep Learning in Fundamental Physics"" (<LINK>). Short thread why I am rather happy about this #OpenScience paper (1/6)', 'Deep learning is gaining incredible traction in physics. Great! But given the diverse nature of our data (images, 4-vectors, graph, time series, hexagons,...) we spend a lot of time trying to find suitable network architectures. (2/6)', 'The publication + pd4ml repo (https://t.co/JUpV1eJS5n) collects datasets from particle physics, astroparticle physics, and hadron- and nuclear physics with diverse representations to allow the easy study of overarching solutions for our fields (more data welcome! ping me) (3/6)', ""We also provide a simple graph-based model (implementation as well in https://t.co/JUpV1eJS5n) that does reasonably well on all of them. So if you have complex physics data and don't want to spend time thinking about an architecture: This could be a great starting point (4/6)"", 'For the future, it will be interesting if we can come up with a generic architecture that even outperforms the established reference on all datasets; collect more data; and combine with AutoML methods (via @Punch4NFDI). (5/6)', 'This cross-disciplinary work would not have been possible without IDT-UM (https://t.co/U9vXWPykBC) funded by @BMBF_Bund + collaborators from @RWTH, @LMU_Muenchen, @fias_science, and @uniheidelberg. Many thanks to LisaB, @WilliamKorcari, @ErikBuh from @UHHMIN / @quunihh (6/6)', 'PS: See the presentation during the datasets session at #ML4Jets this week (https://t.co/N4xCUvxthe). Or just read the paper. (7/6)']",https://arxiv.org/abs/2107.00656,"We introduce a Python package that provides simply and unified access to a collection of datasets from fundamental physics research - including particle physics, astroparticle physics, and hadron- and nuclear physics - for supervised machine learning studies. The datasets contain hadronic top quarks, cosmic-ray induced air showers, phase transitions in hadronic matter, and generator-level histories. While public datasets from multiple fundamental physics disciplines already exist, the common interface and provided reference models simplify future work on cross-disciplinary machine learning and transfer learning in fundamental physics. We discuss the design and structure and line out how additional datasets can be submitted for inclusion. As showcase application, we present a simple yet flexible graph-based neural network architecture that can easily be applied to a wide range of supervised learning tasks. We show that our approach reaches performance close to dedicated methods on all datasets. To simplify adaptation for various problems, we provide easy-to-follow instructions on how graph-based representations of data structures, relevant for fundamental physics, can be constructed and provide code implementations for several of them. Implementations are also provided for our proposed method and all reference algorithms. ",Shared Data and Algorithms for Deep Learning in Fundamental Physics
142,1411929142824636416,460069521,Andrew Francis,"['New paper out on the arXiv!\n\nJoint with @KristinaWicke and Mareike Fischer:\n""Phylogenetic Diversity Rankings in the Face of Extinctions”.\n\n<LINK>']",https://arxiv.org/abs/2107.00748,"Planning for the protection of species often involves difficult choices about which species to prioritize, given constrained resources. One way of prioritizing species is to consider their ""evolutionary distinctiveness"", i.e. their relative evolutionary isolation on a phylogenetic tree. Several evolutionary isolation metrics or phylogenetic diversity indices have been introduced in the literature, among them the so-called Fair Proportion index (also known as the ""evolutionary distinctiveness"" score). This index apportions the total diversity of a tree among all leaves, thereby providing a simple prioritization criterion for conservation. Here, we focus on the prioritization order obtained from the Fair Proportion index and analyze the effects of species extinction on this ranking. More precisely, we analyze the extent to which the ranking order may change when some species go extinct and the Fair Proportion index is re-computed for the remaining taxa. We show that for each phylogenetic tree, there are edge lengths such that the extinction of one leaf per cherry completely reverses the ranking. Moreover, we show that even if only the lowest ranked species goes extinct, the ranking order may drastically change. We end by analyzing the effects of these two extinction scenarios (extinction of the lowest ranked species and extinction of one leaf per cherry) for a collection of empirical and simulated trees. In both cases, we can observe significant changes in the prioritization orders, highlighting the empirical relevance of our theoretical findings. ","Phylogenetic Diversity Rankings in the Face of Extinctions: the
  Robustness of the Fair Proportion Index"
143,1411337514145792002,102076366,'(masataro asai . (˙ᗡ˙ɥԀ)),"['New paper of Latplan (under review @ JAIR), which extends 4 years of work with A.Fukunaga, @azaazarashi, @cjmuise. We generate a symbolic model from raw observations with unsupervised NNs, allowing symbolic planners to act on raw perception. 1/9 <LINK> <LINK>', 'Compared to our IJCAI2020 paper with @cjmuise , it performs a much better precondition learning with Bidirectional Cube-Space AE, resulting in spectacular improvement in producing optimal plans (AMA4+, bottom), meaning that the learned PDDL model is more accurate 2/9 https://t.co/V10f6e0giY', 'The system is a fully end-to-end neural network, unlike existing systems which train separate shallow / rule-based learners (e.g., Konidaris et al. 2014) 3/9 https://t.co/7tR33WC0AP', 'We formalize the entire architecture as a proper generative model, with optimization objective theoretically justified as a lower bound of the log likelihood 4/9 https://t.co/IEhw0E5uG5', 'To revise Cube-Space AE (IJCAI2020) as a proper generative model, the Back-to-Logit effect learning mechanism is given a new random variable e (for effect) which ensures the conformance to STRIPS formalism 5/9 https://t.co/dNvrm36Gch', 'Cube-Space AE is ALSO analyzed from the graph coloring perspective. This provides ""double"" theoretical justifications from both the probabilistic (statistical) and the graph-theoretic (symbolic) angles! 6/9 https://t.co/kUxfOJzowv', 'Take-home message for the ML folks: What are those weird unfashionable ""PDDL"" thing that symbolic planning people always talk about? It\'s indeed a well-crafted generative model! Now what are the remaining ""symbolic priors"" that are already used in symbolic AI and not in NNs? 7/9', 'Message for the symbolic folks: We tried our best to demystify statistical jargons, to describe our neural network with sanity. If you still hesitate to use one, the paper shows you how it is actually similar to SAT/MILP modeling - defining variables and constraints. 8/9 https://t.co/Qdh1RuGk4O', 'Finally: Definition of symbols, and Symbol Stability Problem as a subproblem of Symbol Grounding. What happens if learned symbols are ""meaningless""? Avoiding this is a key to an effective symbol grounding system. We avoid this with a clever closed-world assumption prior. 9/9 https://t.co/gKOLifEtmm', '@followML_ @azaazarashi @cjmuise bad bot']",https://arxiv.org/abs/2107.00110,"Current domain-independent, classical planners require symbolic models of the problem domain and instance as input, resulting in a knowledge acquisition bottleneck. Meanwhile, although deep learning has achieved significant success in many fields, the knowledge is encoded in a subsymbolic representation which is incompatible with symbolic systems such as planners. We propose Latplan, an unsupervised architecture combining deep learning and classical planning. Given only an unlabeled set of image pairs showing a subset of transitions allowed in the environment (training inputs), Latplan learns a complete propositional PDDL action model of the environment. Later, when a pair of images representing the initial and the goal states (planning inputs) is given, Latplan finds a plan to the goal state in a symbolic latent space and returns a visualized plan execution. We evaluate Latplan using image-based versions of 6 planning domains: 8-puzzle, 15-Puzzle, Blocksworld, Sokoban and Two variations of LightsOut. ",Classical Planning in Deep Latent Space
144,1411043189125955586,1227540343177928704,Mayee Chen,"['New paper appearing in #ICML2021! Mandoline: Model Evaluation under Distribution Shift:\n\nPaper: <LINK>\nCode: <LINK>\n\nwork done w/ equal contribution from @krandiash and @nimit_sohoni , as well as @faitpoms, @kayvonf, and @HazyResearch 1/6 <LINK>', 'ML models are often deployed on unlabeled data that is very different from the data it was trained on. In our paper, we explore how to utilize domain knowledge and side information to efficiently evaluate models under distribution shift. 2/6', 'We introduce Mandoline, an importance weighting (IW) framework that allows us to estimate model performance on an unlabeled target deployment dataset using just a labeled source dataset. 3/6', 'Typically, IW based on the covariates suffers from support shift (e.g. when there are previously unseen points in the target dataset) and also struggles when the feature space is complex and high-dimensional. 4/6', 'Instead, Mandoline performs IW based on user-defined/programmatic ""slices"" that aim to capture relevant axes of distribution shift. 5/6', 'Theoretically, such slices are able to correct for distribution shift while mitigating the above problems of standard IW. Empirically, we do up to 3x better than baselines of standard IW in the raw feature space. 6/6']",https://arxiv.org/abs/2107.00643,"Machine learning models are often deployed in different settings than they were trained and validated on, posing a challenge to practitioners who wish to predict how well the deployed model will perform on a target distribution. If an unlabeled sample from the target distribution is available, along with a labeled sample from a possibly different source distribution, standard approaches such as importance weighting can be applied to estimate performance on the target. However, importance weighting struggles when the source and target distributions have non-overlapping support or are high-dimensional. Taking inspiration from fields such as epidemiology and polling, we develop Mandoline, a new evaluation framework that mitigates these issues. Our key insight is that practitioners may have prior knowledge about the ways in which the distribution shifts, which we can use to better guide the importance weighting procedure. Specifically, users write simple ""slicing functions"" - noisy, potentially correlated binary functions intended to capture possible axes of distribution shift - to compute reweighted performance estimates. We further describe a density ratio estimation framework for the slices and show how its estimation error scales with slice quality and dataset size. Empirical validation on NLP and vision tasks shows that Mandoline can estimate performance on the target distribution up to 3x more accurately compared to standard baselines. ",Mandoline: Model Evaluation under Distribution Shift
145,1411032855082270720,3327515352,M.P. Ross,"[""New paper out today. First time we've used gravity to calibrate a LIGO observatory! Took just over a year and half to go from a sketch at a conference to installed hardware. Then the uncertainty and systematics analysis only required another 18 months. \n<LINK>"", ""Don't miss the big G measurement in the appendix! First measurement with a gravitational wave observatory but no where close to as precise as torsion balance experiments."", ""@GEddolls Yep. The rotor applies ~0.2 - 0.3 pNm of torque on the TM depending on multipole moment. And it ends up showing up in the strain measurements because the interferometer beam isn't perfectly centered. We had to take a few percent correction to account for it."", ""@GEddolls If you're really interested, we wrote up a technical note about the torque coupling:\nhttps://t.co/VUkl5LRZH9""]",https://arxiv.org/abs/2107.00141,The precise calibration of the strain readout of the LIGO gravitational wave observatories is paramount to the accurate interpretation of gravitational wave events. This calibration is traditionally done by imparting a known force on the test masses of the observatory via radiation pressure. Here we describe the implementation of an alternative calibration scheme: the Newtonian Calibrator. This system uses a rotor consisting of both quadrupole and hexapole mass distributions to apply a time-varying gravitational force on one of the observatory's test masses. The force produced by this rotor can be predicted to $<1\%$ relative uncertainty and is well-resolved in the readout of the observatory. This system currently acts as a cross-check of the existing absolute calibration system. ,Initial Results from the LIGO Newtonian Calibrator
146,1410986063598870528,19391874,Naren Ramakrishnan,"['New paper on AntiPatterns in MLOps: \n<LINK>\nJoint work with Hays ""Skip"" McCormick, Prakash Arunachalam, Yu Yu et al. of @BNYMellon  and @nikhilm_1, @sathappanspm, @hbar of @SanghaniCtrVT.', 'I have learnt a lot about antipatterns from Skip esp. via his book: \nhttps://t.co/6IYvZd7eHx\nOur paper provides a vocabulary to describe defective ML practices, esp. in financial applications like forecasting treasury settlement failures &amp; balance prediction.', 'See this very accessible blog post from @BNYMellon about their role as a clearing provider processing more than $8.6 trillion in Fed-eligible securities daily and how ML is used in their pipeline.\nhttps://t.co/M1XkMyLlLZ', 'As ML matures and gets viewed through a software engineering lens (see @isbellHFh keynote), patterns and antipatterns will pop up everywhere. This paper to appear in KDD workshop on ML in Finance (https://t.co/Nl51dNuRc5).']",https://arxiv.org/abs/2107.00079,"We describe lessons learned from developing and deploying machine learning models at scale across the enterprise in a range of financial analytics applications. These lessons are presented in the form of antipatterns. Just as design patterns codify best software engineering practices, antipatterns provide a vocabulary to describe defective practices and methodologies. Here we catalog and document numerous antipatterns in financial ML operations (MLOps). Some antipatterns are due to technical errors, while others are due to not having sufficient knowledge of the surrounding context in which ML results are used. By providing a common vocabulary to discuss these situations, our intent is that antipatterns will support better documentation of issues, rapid communication between stakeholders, and faster resolution of problems. In addition to cataloging antipatterns, we describe solutions, best practices, and future directions toward MLOps maturity. ",Using AntiPatterns to avoid MLOps Mistakes
147,1410980023360757762,27526402,Sam Quinn,"['Quick thread on a fun new paper characterizing a long-period substellar object observed by Kepler. <LINK> The single transit, identified by amateur astronomers, lasts nearly two days, and at 0.91 Rjup, is consistent with a planet, a brown dwarf, or a star. 1/5 <LINK>', 'RVs over three seasons show little variation but help rule out low mass stars. Most of the remaining mass posterior have radii inconsistent with the observed radius, so we use rejection sampling to arrive at a bimodal posterior indicating a Saturn or brown dwarf mass. 2/5 https://t.co/4k1jUCtu1v', 'Fitting a circular orbit, we would conclude that the companion is likely a planet with an 8.2 yr period. But with an eccentric fit, we cannot distinguish between planetary and brown dwarf companions, and the true period is likely longer than a decade. 3/5 https://t.co/X1KO5Ca8I2', 'We hope to obtain additional RVs. If the companion is a brown dwarf, we expect to detect the RV variation and obtain more precise measurements of the mass and orbit, which could yield a valuable benchmark cool brown dwarf. 4/5 https://t.co/aRbKIDmBqM', 'The star is 1 kpc away, but we expect TESS to observe similar nearby systems, for which imaging and astrometry may be possible. Given the excellent ongoing work being done to identify and characterize TESS single transits, it’s only a matter of time! 5/5', '@david_kipping Yes, MR drives the bimodality. I think much of the R variance comes from inflation for hot and/or young objects. But these all make the planet larger than theory predicts. So it’s hard (*for me, an observer) to imagine an intermediate mass brown dwarf so small.', '@david_kipping But it’ll be fun to see how the empirical relation evolves as we confirm more long period companions from TESS!', '@david_kipping (and strike “brown dwarf” from that reply)']",https://arxiv.org/abs/2107.00027,"We report the detection of a single transit-like signal in the Kepler data of the slightly evolved F star KIC4918810. The transit duration is ~45 hours, and while the orbital period ($P\sim10$ years) is not well constrained, it is one of the longest among companions known to transit. We calculate the size of the transiting object to be $R_P = 0.910$ $R_J$. Objects of this size vary by orders of magnitude in their densities, encompassing masses between that of Saturn ($0.3$ $M_J$) and stars above the hydrogen-burning limit (~80 $M_J$). Radial-velocity observations reveal that the companion is unlikely to be a star. The mass posterior is bimodal, indicating a mass of either ~0.24 $M_J$ or ~26 $M_J$. Continued spectroscopic monitoring should either constrain the mass to be planetary or detect the orbital motion, the latter of which would yield a benchmark long-period brown dwarf with a measured mass, radius, and age. ",A long-period substellar object exhibiting a single transit in Kepler
148,1410956135507582976,1200141005233836032,Pedro Fernandes,"['<LINK>\nCheck out my new paper with @dmulryne, @PedroCarrilho11 and Timothy Clifton. We discuss black holes on a Galileon theory of gravity resultant from a dimensional regularization of the Gauss-Bonnet term. We find (under reasonable assumptions) that\n\n1/2', ""the theory has a unique black hole solution, a result reminiscent of Birkhoff's theorem. We also find that evaporation leads to stable relics with a size directly related to the coupling constant of the theory. We speculate on the role of these relics as dark matter candidates.""]",https://arxiv.org/abs/2107.00046,"In this work we study static black holes in the regularized 4D Einstein-Gauss-Bonnet theory of gravity; a shift-symmetric scalar-tensor theory that belongs to the Horndeski class. This theory features a simple black hole solution that can be written in closed form, and which we show is the unique static, spherically-symmetric and asymptotically-flat black hole vacuum solution of the theory. We further show that no asymptotically-flat, time-dependent, spherically-symmetric perturbations to this geometry are allowed, which suggests that it may be the only spherically-symmetric vacuum solution that this theory admits (a result analogous to Birkhoff's theorem). Finally, we consider the thermodynamic properties of these black holes, and find that their final state after evaporation is a remnant with a size determined by the coupling constant of the theory. We speculate that remnants of this kind from primordial black holes could act as dark matter, and we constrain the parameter space for their formation mass, as well as the coupling constant of the theory. ","Black Holes in the Scalar-Tensor Formulation of 4D Einstein-Gauss-Bonnet
  Gravity: Uniqueness of Solutions, and a New Candidate for Dark Matter"
149,1410932507227328517,750411947661811712,Dr. Sarah Pearson,"['Paper day! I\'m very excited to announce that my new paper with co-authors @suzIQUV @kvj_astro @melissakness et al. is out today. We develop a new code, the ""Hough Stream Spotter"" (HSS) and use it to search for globular cluster stellar streams in M31!\xa0<LINK>', 'Through injections of synthetic stellar streams (teal),\xa0\xa0we show that our code can easily recover positions, orientations and curvatures of stellar streams (purple) in the PAndAS data of Andromeda’s stellar halo. https://t.co/uaEnoL6e8X', 'The HSS is complete to recover streams with 100%,75%,50% of the surface density of a 10xM_Pal 5-like stream in M31\'s stellar halo. ""+"": fully recovered streams, triangle: partially recovered streams, ""-"": not recovered. Darker color: more significant detection. https://t.co/t8mAyfxv4I', 'We find 27 GC stream candidates (purple), 48 data artifacts due to the CHFT 1x1 deg pointings (salmon), and 78 streams near known dwarf debris features (pink). “+” shows the location of outer halo GCs, transparent circles show the most significant GC stream candidates. https://t.co/B2EjSqaYZr', 'Analyses of the CMDs show that the 5 most significant GC candidate streams have a stronger signal along the red giant branch (upper left) than all detections (upper right), than the data artifacts (lower left), and the known dwarfs (lower right). https://t.co/mSYn2KsQb2', 'With @NASARoman\xa0and the HSS, we can place PAndAS-quality constraints on GC streams for any halo within 10 Mpc, and streams with 50%(right)-100%(left) of the surface density of Pal 5 can be detected in a wealth of galaxies in the near future. https://t.co/SYE99XzHMx', 'This has the potential to open up a new discovery space for GC stream studies, GC stream gap searches, and for GC stream-based constraints on the nature of dark matter. The Hough Stream Spotter (HSS) code is available here:\xa0https://t.co/0URrgscfRW.']",https://arxiv.org/abs/2107.00017,"Stellar streams from globular clusters (GCs) offer constraints on the nature of dark matter and have been used to explore the dark matter halo structure and substructure of our Galaxy. Detection of GC streams in other galaxies would broaden this endeavor to a cosmological context, yet no such streams have been detected to date. To enable such exploration, we develop the Hough Stream Spotter (HSS), and apply it to the Pan-Andromeda Archaeological Survey (PAndAS) photometric data of resolved stars in M31's stellar halo. We first demonstrate that our code can re-discover known dwarf streams in M31. We then use the HSS to blindly identify 27 linear GC stream-like structures in the PAndAS data. For each HSS GC stream candidate, we investigate the morphologies of the streams and the colors and magnitudes of all stars in the candidate streams. We find that the five most significant detections show a stronger signal along the red giant branch in color-magnitude diagrams (CMDs) than spurious non-stream detections. Lastly, we demonstrate that the HSS will easily detect globular cluster streams in future Nancy Grace Roman Space Telescope data of nearby galaxies. This has the potential to open up a new discovery space for GC stream studies, GC stream gap searches, and for GC stream-based constraints on the nature of dark matter. ","The Hough Stream Spotter: A New Method for Detecting Linear Structure in
  Resolved Stars and Application to the Stellar Halo of M31"
150,1410928787433431041,280083723,Yoh Tanimoto,['new paper~ we computed the modular operator w.r.t vacuum of null cut algebras in free fields and proved that inclusions of null cuts are half-sided modular inclusions~ <LINK>'],https://arxiv.org/abs/2107.00039,"We consider the algebras generated by observables in quantum field theory localized in regions in the null plane. For a scalar free field theory, we show that the one-particle structure can be decomposed into a continuous direct integral of lightlike fibres, and the modular operator decomposes accordingly. This implies that a certain form of QNEC is valid in free fields involving the causal completions of half-spaces on the null plane (null cuts). We also compute the relative entropy of null cut algebras with respect to the vacuum and some coherent states. ",Modular operator for null plane algebras in free fields
151,1410865774609186817,1323926701177507841,Pablo Rivière,"['A new paper lead by my PhD (co-supervision with Asunfión Fuente) is out today, a study of deuteration in starless cores in the Taurus Molecular Cloud. Enjoy it!\n\n<LINK>']",https://arxiv.org/abs/2107.00423,"The chemical and physical evolution of starless and pre-stellar cores are of paramount importance to understanding the process of star formation. The Taurus Molecular Cloud cores TMC 1-C and TMC 1-CP share similar initial conditions and provide an excellent opportunity to understand the evolution of the pre-stellar core phase. We investigated the evolutionary stage of starless cores based on observations towards the prototypical dark cores TMC 1-C and TMC 1-CP, mapping them in the CS $3\rightarrow 2$, C$^{34}$S $3\rightarrow 2$, $^{13}$CS $2\rightarrow 1$, DCN $1\rightarrow 0$, DCN $2\rightarrow 1$, DNC $1\rightarrow 0$, DNC $2\rightarrow 1$, DN$^{13}$C $1\rightarrow 0$, DN$^{13}$C $2\rightarrow 1$, N$_2$H$^+$ $1\rightarrow 0$, and N$_2$D$^+$ $1\rightarrow 0$ transitions. We performed a multi-transitional study of CS and its isotopologs, DCN, and DNC lines to characterize the physical and chemical properties of these cores. We studied their chemistry using the state-of-the-art gas-grain chemical code Nautilus and pseudo time-dependent models to determine their evolutionary stage. Observational diagnostics seem to indicate that TMC 1-C is in a later evolutionary stage than TMC 1-CP, with a chemical age $\sim$1 Myr. TMC 1-C shows signs of being an evolved core at the onset of star formation, while TMC 1-CP appears to be in an earlier evolutionary stage due to a more recent formation or, alternatively, a collapse slowed down by a magnetic support. ","Evolutionary view through the starless cores in Taurus: deuteration in
  TMC 1-C and TMC 1-CP"
152,1410843177058127875,96253726,Navin Sridhar,"['New paper!\n<LINK>\nThe coronal plasmas of black holes are subject to inverse-Compton cooling by the soft ~blackbody photons from the accretion disk. What powers the hard, non-thermal X-rays from BH coronae despite this radiative cooling is an unsolved mystery...', 'In this work, we show that the bulk motion of plasmoid chains—resulting from the reconnection of magnetic field lines anchored onto the accretion disk—can Compton up-scatter the soft disk photons into a hard, non-thermal spectrum. *This works despite a cooled-down coronal plasma* https://t.co/jQy5UDZsT1', 'We demonstrate this using first-principle PIC simulations of relativistic reconnection in pair plasmas for different levels of magnetization (σ) and seed photon density. The spectrum of bulk motions resembles a ~100 keV Maxwellian, and barely depends on the above two parameters.', 'We perform Monte-Carlo calculations of the radiative transfer of seed photons through the reconnection layer laden with particles, whose momenta are obtained from PIC simulations: the σ=10 model describes the BH ""hard state"" spectrum remarkably well, across 3 decades in energy! https://t.co/T1OGjAq48Q', 'We welcome you to our paper—available on arXiv (https://t.co/8Z3diNKhOF)—for a detailed dive into our calculations, assumptions, and results. Stay tuned to the second part of this series, where we will be modifying the composition of the corona! All comments appreciated! https://t.co/6qkp5SaFcp']",https://arxiv.org/abs/2107.00263,"We perform two-dimensional particle-in-cell simulations of reconnection in magnetically dominated electron-positron plasmas subject to strong Compton cooling. We vary the magnetization $\sigma\gg1$, defined as the ratio of magnetic tension to plasma inertia, and the strength of cooling losses. Magnetic reconnection under such conditions can operate in magnetically dominated coronae around accreting black holes, which produce hard X-rays through Comptonization of seed soft photons. We find that the particle energy spectrum is dominated by a peak at mildly relativistic energies, which results from bulk motions of cooled plasmoids. The peak has a quasi-Maxwellian shape with an effective temperature of $\sim 100$ keV, which depends only weakly on the flow magnetization and the strength of radiative cooling. The mean bulk energy of the reconnected plasma is roughly independent of $\sigma$, whereas the variance is larger for higher magnetizations. The spectra also display a high-energy tail, which receives $\sim 25$% of the dissipated reconnection power for $\sigma=10$ and $\sim 40$% for $\sigma=40$. We complement our particle-in-cell studies with a Monte-Carlo simulation of the transfer of seed soft photons through the reconnection layer, and find the escaping X-ray spectrum. The simulation demonstrates that Comptonization is dominated by the bulk motions in the chain of Compton-cooled plasmoids and, for $\sigma\sim 10$, yields a spectrum consistent with the typical hard state of accreting black holes. ","Comptonization by Reconnection Plasmoids in Black Hole Coronae I:
  Magnetically Dominated Pair Plasma"
153,1423088875778367488,495550336,Abhishek Gupta,"['New work on learning how to grasp and navigate with mobile robots using RL. What I find very exciting is the ability of the system to be trained for &gt; 60 hrs with minimal intervention, learning in diverse scenarios.\nPaper: <LINK> \nWebsite: <LINK> <LINK>']",https://arxiv.org/abs/2107.13545,"We study how robots can autonomously learn skills that require a combination of navigation and grasping. While reinforcement learning in principle provides for automated robotic skill learning, in practice reinforcement learning in the real world is challenging and often requires extensive instrumentation and supervision. Our aim is to devise a robotic reinforcement learning system for learning navigation and manipulation together, in an autonomous way without human intervention, enabling continual learning under realistic assumptions. Our proposed system, ReLMM, can learn continuously on a real-world platform without any environment instrumentation, without human intervention, and without access to privileged information, such as maps, objects positions, or a global view of the environment. Our method employs a modularized policy with components for manipulation and navigation, where manipulation policy uncertainty drives exploration for the navigation controller, and the manipulation module provides rewards for navigation. We evaluate our method on a room cleanup task, where the robot must navigate to and pick up items scattered on the floor. After a grasp curriculum training phase, ReLMM can learn navigation and grasping together fully automatically, in around 40 hours of autonomous real-world training. ","Fully Autonomous Real-World Reinforcement Learning with Applications to
  Mobile Manipulation"
154,1421104907340353538,1002014071,Frank Wilczek,"['New paper posted on the arXiv today: <LINK> .\nI think it shows a way to do ""digital quantum mechanics"" that will prove very fruitful. <LINK>']",https://arxiv.org/abs/2107.13593,"I extend, apply, and generalize a model of a quantum radiator proposed by Griffiths to construct models of radiation fields that exhibit high entropy for long periods of time but approach pure states asymptotically. The models, which are fully consistent with the basic principles of quantum theory, provide coarse-grained models of both realistic physical systems and exotic space-times including black and white holes and baby and prodigal universes. Their analysis suggests experimental probes of some basic but subtle implications of quantum theory including interference between a particle and its own past, influence of quantum statistical entanglement on entropy flow, and residual entanglement connecting distant radiation with a degenerate source. ",Models of Hidden Purity
155,1420991888757116931,2904119824,Lenore Blum,"['Our new paper, ""A Theory of Consciousness from a Theoretical Computer Science Perspective: Insights from the Conscious Turing Machine"", is now on arxiv!\n<LINK>\n#consciousness #AI #ML \n@cstheory <LINK>']",https://arxiv.org/abs/2107.13704,"The quest to understand consciousness, once the purview of philosophers and theologians, is now actively pursued by scientists of many stripes. We examine consciousness from the perspective of theoretical computer science (TCS), a branch of mathematics concerned with understanding the underlying principles of computation and complexity, including the implications and surprising consequences of resource limitations. In the spirit of Alan Turing's simple yet powerful definition of a computer, the Turing Machine (TM), and perspective of computational complexity theory, we formalize a modified version of the Global Workspace Theory (GWT) of consciousness originated by cognitive neuroscientist Bernard Baars and further developed by him, Stanislas Dehaene, Jean-Pierre Changeaux and others. We are not looking for a complex model of the brain nor of cognition, but for a simple computational model of (the admittedly complex concept of) consciousness. We do this by defining the Conscious Turing Machine (CTM), also called a conscious AI, and then we define consciousness and related notions in the CTM. While these are only mathematical (TCS) definitions, we suggest why the CTM has the feeling of consciousness. The TCS perspective provides a simple formal framework to employ tools from computational complexity theory and machine learning to help us understand consciousness and related concepts. Previously we explored high level explanations for the feelings of pain and pleasure in the CTM. Here we consider three examples related to vision (blindsight, inattentional blindness, and change blindness), followed by discussions of dreams, free will, and altered states of consciousness. ","A Theory of Consciousness from a Theoretical Computer Science
  Perspective: Insights from the Conscious Turing Machine"
156,1420423598137921538,11409082,Antonio Lieto,"['Check out our new #paper where we present a #persuasiverobot integrating (via the ACT-R #cognitivearchitecture) #storytelling strategies, #ethical stances, and #rhetoricaltechniques and test its persuasive strength in dialogues on #COVID19 related issues: <LINK> <LINK>']",https://arxiv.org/abs/2107.12845,"We present a storytelling robot, controlled via the ACT-R cognitive architecture, able to adopt different persuasive techniques and ethical stances while conversing about some topics concerning COVID-19. The main contribution of the paper consists in the proposal of a needs-driven model that guides and evaluates, during the dialogue, the use (if any) of persuasive techniques available in the agent procedural memory. The portfolio of persuasive techniques tested in such a model ranges from the use of storytelling, to framing techniques and rhetorical-based arguments. To the best of our knowledge, this represents the first attempt of building a persuasive agent able to integrate a mix of explicitly grounded cognitive assumptions about dialogue management, storytelling and persuasive techniques as well as ethical attitudes. The paper presents the results of an exploratory evaluation of the system on 63 participants ","A Storytelling Robot managing Persuasive and Ethical Stances via ACT-R:
  an Exploratory Study"
157,1418554825441255435,1150877949500874752,Georgia T Papadakis,"[""Here's a new theory paper on thermophotovoltaics: <LINK>. We analytically explain how luminescence efficiency is enhanced in the near-field, and how this plays a role in the overall TPV performance. And other things!"", 'Thanks @n_maccaferri for inviting me to Meta 21, it was a pleasure to present in one of your sessions! Hope you find this paper useful! :)']",https://arxiv.org/abs/2107.10705,"We evaluate near-field thermophotovoltaic (TPV) energy conversion systems focusing in particular on their open-circuit voltage (Voc). Unlike previous analyses based largely on numerical simulations with fluctuational electrodynamics, here, we develop an analytic model that captures the physics of near-field TPV systems and can predict their performance metrics. Using our model, we identify two important opportunities of TPV systems operating in the near-field. First, we show analytically that enhancement of radiative recombination is a natural consequence of operating in the near-field. Second, we note that, owing to photon recycling and minimal radiation leakage in near-field operation, the PV cell used in near-field TPV systems can be much thinner compared to those used in solar PV systems. Since non-radiative recombination is a volumetric effect, use of a thinner cell reduces non-radiative losses per unit area. The combination of these two opportunities leads to increasingly large values of Voc as the TPV vacuum gap decreases. Hence, although operation in the near-field was previously perceived to be beneficial for electrical power density enhancement, here, we emphasize that thin-film near-field TPVs are also significantly advantageous in terms of Voc and consequently conversion efficiency as well as power density. We provide numerical results for an InAs-based thin-film TPV that exhibits efficiency > 50% at an emitter temperature as low as 1100 K. ",Thermodynamics of light management in near-field thermophotovoltaics
158,1418477046762778626,1005395495949406208,Francesco Locatello,"[""Is this the biggest heart you've ever seen? Extrapolating factors of variation from training is hard and even theoretically identifiable methods don't do well. Check out our new paper @awscloud @bethgelab @MPI_IS \n\npaper: <LINK>\ncode: <LINK> <LINK>"", ""The problem is simple: Let's say a neural network has seen small-to-medium sized hearts. Can it predict that that a heart is bigger than anything seen in the training set? Turns out, predicting it's a heart is easy. Predicting it's bigger is not. https://t.co/dDNM0yBQzo"", ""TL;DR: Interpolation and composition don't work perfectly but are not as challenging as extrapolation. Pre-training on more data seem the best solution as different architectural biases and supervision don't make much difference."", 'Work led by @schott_lukas during his internship at @awscloud with @JKugelgen, @f_traeuble, @pegehler, @c_russl, @MatthiasBethge, @bschoelkopf, and @wielandbr']",https://arxiv.org/abs/2107.08221,"An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization. ","Visual Representation Learning Does Not Generalize Strongly Within the
  Same Domain"
159,1418249278816808963,1191386359707029505,Animesh Mukherjee,"['Releasing our new ACM Hypertext 2021 paper and the dataset.\n\nDebiasing Multilingual Word Embeddings: A Case Study of Three Indian Languages with Srijan Bansal, Vishal Garimella Ayush Suhane\nPaper: <LINK>\nCode and dataset: <LINK>']",https://arxiv.org/abs/2107.10181,"In this paper, we advance the current state-of-the-art method for debiasing monolingual word embeddings so as to generalize well in a multilingual setting. We consider different methods to quantify bias and different debiasing approaches for monolingual as well as multilingual settings. We demonstrate the significance of our bias-mitigation approach on downstream NLP applications. Our proposed methods establish the state-of-the-art performance for debiasing multilingual embeddings for three Indian languages - Hindi, Bengali, and Telugu in addition to English. We believe that our work will open up new opportunities in building unbiased downstream NLP applications that are inherently dependent on the quality of the word embeddings used. ","Debiasing Multilingual Word Embeddings: A Case Study of Three Indian
  Languages"
160,1417104936324763651,1410599522468732943,Elena Lacchin,"['New paper on Globular Clusters today! \nCheck it out on <LINK> 🚀 <LINK>', ""@R_Amadio Next time we meet, I'll explain you😜""]",https://arxiv.org/abs/2107.07962,"By means of 3D hydrodynamic simulations, we study how Type Ia supernovae (SNe) explosions affect the star formation history and the chemical properties of second generation (SG) stars in globular clusters (GC). SG stars are assumed to form once first generation asymptotic giant branch (AGB) stars start releasing their ejecta; during this phase, external gas is accreted by the system and SNe Ia begin exploding, carving hot and tenuous bubbles. Given the large uncertainty on SNe Ia explosion times, we test two different values for the 'delay time'. We run two different models for the external gas density: in the low-density scenario with short delay time, the explosions start at the beginning of the SG star formation, halting it in its earliest phases. The external gas hardly penetrates the system, therefore most SG stars present extreme helium abundances (Y > 0.33). The low-density model with delayed SN explosions has a more extended SG star formation epoch and includes SG stars with modest helium enrichment. On the contrary, the high-density model is weakly affected by SN explosions, with a final SG mass similar to the one obtained without SNe Ia. Most of the stars form from a mix of AGB ejecta and pristine gas and have a modest helium enrichment. We show that gas from SNe Ia may produce an iron spread of $\sim 0.14$ dex, consistent with the spread found in about 20% of Galactic GCs, suggesting that SNe Ia might have played a key role in the formation of this sub-sample of GCs. ","On the role of Type Ia supernovae in the second generation star
  formation in globular clusters"
161,1416064788577259529,34078684,Dipanjan Das,"['End-to-end dialogue models often generate responses about the world that are not ""faithful"" to evidence in grounding corpora.  We present new work on controlling these responses to be attributable to such evidence.\n\nPaper: <LINK>\nAbs: <LINK>\n\n1/', 'Here is an example dialogue from Wizard of Wikipedia that switches back and forth between personal experience and a response pertaining to the real world.  For the latter, we believe it is important for the response to be supported by a given grounding document. 2/ https://t.co/FD569NnRgm', 'Our approach is straightforward:  we add features to control for support given the grounding document, and present a resampling technique that satisfies faithfulness metrics, avoiding content ""hallucination"" as much as possible. 3/ https://t.co/e7e9vZjzQy', 'On human evaluations on WoW, we obtain strong results across all pertinent metrics. 4/ https://t.co/84di4au7bU', ""I am tweeting this on behalf of my co-authors @hjrashkin, @davidswelt and @gtomar_google (who are much less active than I am).  This was Hannah's first project after she joined Google last year -- the paper will appear at ACL 2021 shortly.  5/"", 'Stay tuned for more work in this area in the upcoming months and years.  6/6']",https://arxiv.org/abs/2107.06963,"Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text. We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence. Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses. We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity. At training time, additional inputs based on these evaluation measures are given to the dialogue model. At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence. We also investigate the usage of additional controls at decoding time using resampling techniques. In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems. ","Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable
  Features"
162,1415915759960690696,602152164,Pedro H. C. Sant'Anna,"['🦶🦶🚨New DiD Paper🚨🦶🦶\n\nWhat if treatment is continuous not binary? What parameters can you estimate under what assumptions? What about two-way fixed effects regressions? What about staggered timing?\n\nBrant Callaway, @agoodmanbacon and I dive in here:\n<LINK> <LINK>', 'We’ve noticed that, um, a few people have been asking about this:\n\n2/n https://t.co/TCf6HArJ7j', 'And no wonder! \n\nContinuous treatments are common and important. Variation in treatment intensity might be your only (or best) research design. You may care about the “dose-response” effects per se, or want to use them to strengthen a causal claim. \n\n3/N', 'We want you to remember 3 things\n1. Parameters: what kind of causal parameter am I talking about?\n2. ""Bias"": what do I have to assume to make the comparison group a good counterfactual? (and what’s the counterfactual?)\n3. TWFE: how does it handle ""bias"" and weight parameters?\n4/N', 'PARAMETERS\n\nWith a binary treatment each unit just has “a” treatment effect. People discuss averages of them, all the time (ATE/ATT/LATE/MTE/etc).\n\nWith a cont. treatment each unit has a treatment effect *function*: the causal effect of dose d relative to no treatment (D=0).\n5/N', 'We can ask things like “what’s the effect of dose D=d for units that got dose D=a?” Call that an avg treatment effect on the treated: ATT(d|a)\n\nd=dose whose effect we consider\na=group we consider it for\n\nThe LEVEL of the curvy lines show ATT(d|.) for two groups (D=a and D=b)\n6/N https://t.co/zOREuDCLYT', 'BUT another difference with a cont. treatment is: “no treatment” is not the only counterfactual! It’s as important—or more!—to ask “what was the causal effect of moving a tiny bit from d’ to d?”\n\nThat, however, is an “average causal response” (a la Angrist and Imbens 1995)\n\n7/N', 'We call the average causal response at dose D=d for units with dose D=a, ACRT(d|a). \n\nIt is the SLOPE of the ATT function at dose d for group a. This asks “what was the effect of the dth dose increment for units treated with D=a?”\n\nHere it is for cont. and discrete doses:\n\n8/N https://t.co/dvFd8vOqLG', 'Note that the dose and group can differ. It’s OK to ask:\n“For those who took 3 aspirin, what would have been the effect on their headache of taking 2?” That’s ATT(2|3)\n\nor “what’s the effect of just crossing the lead exposure limit on kids w/ 0 exposure?"" That’s ACRT(limit|0)\n9/N', 'So now we have all the building blocks to understand parameters in DiD with a continuous treatment:\n\nAre you interested in ATT or ACRT? At what dose? For which group? All units? Just those treated with a spec. dose?\n\nThese matter *both* for the math and for interpretation!\n\n10/N', 'BIAS\n\nHow do we identify all these causal parameters?\n\nATTs aren’t so hard. With untreated units, a traditional parallel trends assumption on untreated potential outcomes lets you estimate the ATT(d|d) as in a binary DiD. Just compare each dose group to the untreated group.\n\n11/N', 'But learning about slopes is harder. Why?\n\nImagine comparing ATT(b|b) to ATT(a|a). You want ACRT(b|b), the slope of ATT(d|b) at b\n\nBut ATT(b|b) differs from ATT(a|a) both because the dose is higher (thats ACRT(b|b)!) AND bc you jump from one line to the other (thats ""bias""). \n12/ https://t.co/lHbWP6c7LV', 'Look at it like this:\n\nATT(b|b) – ATT(a|a)=ATT(b|b) – ATT(a|b) + [ATT(a|b) – ATT(a|a)]\n\nThe 1st part is ACRT(b|b)—😀! The 2nd part is heterogeneity in ATT at dose a—😭!\n\nFor cont. d, take ∂ATT(d|d)/∂d. For ACRT you *only* want the first “d” to change. In reality, both do.\n\n13/N', 'IOW to get ACRs we compare dose groups. The low-dose group is supposed to tell us what the high-dose group would look like at the low dose. \n\nFor that to work it can’t just be that the groups look alike with NO treatment. They have to look alike at the low dose too!\n\n14/N', 'TWFE\n\nSo…what about regressions? Advice has been to run: \n\nreg y i.unit i.time D*POST\n\nBut does it work? (Meaning: does it yield a causal parameter under a “traditional” parallel trends assumption on untreated outcomes?)\n\nAs with staggered timing the answer is…No.\n\n15/N', 'To see why, look at what TWFE does in the 2 period case with no treatment in period 1. \n\nIt turns out TWFE weights together comparisons of outcome changes (the ΔYt) between adjacent doses (the derivatives).\n\n16/N https://t.co/q4VBBjs14E', 'If trad. parallel trends holds, then comparing dose groups will not pick up differential trends in Y(0). \n\nBut we just showed that it DOES pick up diffs in the ATT at the lower dose. Without stronger assumptions, this is “selection bias” in a continuous DiD TWFE estimate.\n\n17/N https://t.co/0bM9IpsOZP', 'Here’s an example of how bad that can be. We make a dataset where units with higher doses have systematically larger treatment effect functions, but at each group’s true dose the ACR is always 1. \n\nIdeally the TWFE coefficient would be 1. It is not. It is 2.23!!\n\n18/N https://t.co/bQqUOgbp2e', 'So if you are running this you have to ask yourself: \n\nis my dose-response relationship capturing the effect of additional doses, or differences in the treatment effects across groups who really did get different doses? \n\n19/N', 'On the upside, this shows how to interpret TWFE in terms of ACR parameters. \n\nIf we assume “strong” parallel trends (which says that all groups would have had the same outcomes had they received *any* dose) then bias terms go away and TWFE is an average ACRs across doses.\n\n20/N https://t.co/qy8vKt1rLK', 'But strong PT is so strong that it means that causal effects for one group aren’t “local” anymore. \n\nThe effect of the “dth” dose is the same for every group. So under that assumption we are in a world of ACRs not ACRTs, and ATEs not ATTs. That’s pretty strong.\n\n21/N', 'This might be the most disheartening conclusion in the paper.\n\nUnlike in staggered timing, where you can change your estimator and get back to world where PT is sufficient for identification, that’s not the case here. You need a stronger assumption. \n\n22/N', 'It’s also true that the TWFE weights are very weird. \n\nThey are hump-shaped and centered on E[D]. \n\nSo ACRs around the mean dose get the most weight even if the distribution of D is not centered on E[D]. This is what those weights look like in the scatter example above.\n\n23/N https://t.co/e0d3T9ilNR', 'In Section 4 of the paper, we discuss staggered timing *and* a continuous treatment!! \n\nProblems add up! \n\nBut this is getting too long, and I only have 1 more tweet as threads are capped at 25 tweets.\n\nSo the question is: What should I remember?\n24/N https://t.co/XRcwZeVv1R', 'Although we make a lot of specific points, overall msg is that you should think about how your design and estimation strategy work, and what assumptions they require to deliver the kind of parameter you care.\n\nIf you know that, state it clearly, and justify it. \n\nThanks!\n\n25/25', ""Ah, and forgot to say: Stay tuned as we'll add an application, alternative estimators to TWFE that do not have the problems of weird weighting, and, of course, have a fully functional package to make life easier.\n\nWe're just so excited that we couldn't wait to share this draft.""]",https://arxiv.org/abs/2107.02637,"This paper analyzes difference-in-differences setups with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to treatment effect heterogeneity. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular two-way fixed-effect specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation strategies that do not suffer from these drawbacks. Our results also cover cases where (i) there is no available untreated comparison group and (ii) there are multiple periods and variation in treatment timing, which are both common in empirical work. ",Difference-in-Differences with a Continuous Treatment
163,1415858194212167682,1198393614478516224,Vaikuntanathan Lab,"['Check out new paper by Agnish: It shows how an ultrasensitive switch can help circadian clocks function robustly even in suboptimal conditions. <LINK>', 'Also included: a simple phenomenological picture to explain how circadian oscillation timescales can be stably maintained.']",https://arxiv.org/abs/2107.06954,"Biochemical circadian rhythm oscillations play an important role in many signalling mechanisms. In this work, we explore some of the biophysical mechanisms responsible for sustaining robust oscillations by constructing a minimal but analytically tractable model of the circadian oscillations in the KaiABC protein system found in the cyanobacteria $S. \ elongatus$. In particular, our minimal model explicitly accounts for two experimentally characterized biophysical features of the KaiABC protein system, namely, a differential binding affinity and an ultrasensitive response. Our analytical work shows how these mechanisms might be crucial for promoting robust oscillations even in sub optimal nutrient conditions. Our analytical and numerical work also identifies mechanisms by which biological clocks can stably maintain a constant time period under a variety of nutrient conditions. Finally, our work also explores the thermodynamic costs associated with the generation of robust sustained oscillations and shows that the net rate of entropy production alone might not be a good figure of merit to asses the quality of oscillations. ","A mechanism for the generation of robust circadian oscillations through
  ultransensitivity and differential binding affinity"
164,1415299109733519360,1077995761487568896,Jon Miller,"['The paper linked below is new and interesting work on neutron star accretion by Jakob van den Eijnden.  Accretion-powered pulsars are not ""supposed"" to have radio jets as the inner disk might be disrupted ... but they do.  \n<LINK>']",https://arxiv.org/abs/2107.05286,"We report new radio observations of a sample of thirty-six neutron star (NS) X-ray binaries, more than doubling the sample in the literature observed at current-day sensitivities. These sources include thirteen weakly-magnetised ($B<10^{10}$ G) and twenty-three strongly-magnetised ($B\geq10^{10}$ G) NSs. Sixteen of the latter category reside in high-mass X-ray binaries, of which only two systems were radio-detected previously. We detect four weakly and nine strongly-magnetised NSs; the latter are systematically radio fainter than the former and do not exceed $L_R \approx 3\times10^{28}$ erg/s. In turn, we confirm the earlier finding that the weakly-magnetized NSs are typically radio fainter than accreting stellar-mass black holes. While an unambiguous identification of the origin of radio emission in high-mass X-ray binaries is challenging, we find that in all but two detected sources (Vela X-1 and 4U 1700-37) the radio emission appears more likely attributable to a jet than the donor star wind. The strongly-magnetised NS sample does not reveal a global correlation between X-ray and radio luminosity, which may be a result of sensitivity limits. Furthermore, we discuss the effect of NS spin and magnetic field on radio luminosity and jet power in our sample. No current model can account for all observed properties, necessitating the development and refinement of NS jet models to include magnetic field strengths up to $10^{13}$ G. Finally, we discuss jet quenching in soft states of NS low-mass X-ray binaries, the radio non-detections of all observed very-faint X-ray binaries in our sample, and future radio campaigns of accreting NSs. ",A new radio census of neutron star X-ray binaries
165,1414756122704027648,21985608,Ryan Giordano,"['We have a new arxiv paper up, ""Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian Nonparametrics,"" with fantastic co-authors Runjing Liu, Mike Jordan, and Tamara Broderick.\n\nI think Figure 3 is pretty cool.\n\n<LINK>']",https://arxiv.org/abs/2107.03584,"Bayesian models based on the Dirichlet process and other stick-breaking priors have been proposed as core ingredients for clustering, topic modeling, and other unsupervised learning tasks. Prior specification is, however, relatively difficult for such models, given that their flexibility implies that the consequences of prior choices are often relatively opaque. Moreover, these choices can have a substantial effect on posterior inferences. Thus, considerations of robustness need to go hand in hand with nonparametric modeling. In the current paper, we tackle this challenge by exploiting the fact that variational Bayesian methods, in addition to having computational advantages in fitting complex nonparametric models, also yield sensitivities with respect to parametric and nonparametric aspects of Bayesian models. In particular, we demonstrate how to assess the sensitivity of conclusions to the choice of concentration parameter and stick-breaking distribution for inferences under Dirichlet process mixtures and related mixture models. We provide both theoretical and empirical support for our variational approach to Bayesian sensitivity analysis. ","Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian
  Nonparametrics"
166,1414681787666665477,1220444384921104385,Amirhossein Nouranizadeh,"['Hi everyone! Please check out the preprint of our new paper ""Maximum Entropy Weighted Independent Set Pooling for Graph Neural Networks"": \n<LINK> \n#information_theory #machine_learning #graph_neural_networks #GNN #graph_classification #graphs #infomax <LINK>', 'We designed a graph pooling strategy within the graph neural architectures based on the concepts of the Shannon capacity of graphs and the Infomax principle.', 'We also proposed an unsupervised neural implementation for solving the combinatorial optimization problem of the maximum (weight) independent set problem defined over the graphs.', '#mutual_information #maximum_mutual_information #shannon_capacity #pooling #graph_pooling #combinatorial_optimization #maximum_independent_set #maximum_weight_independent_set']",https://arxiv.org/abs/2107.01410,"In this paper, we propose a novel pooling layer for graph neural networks based on maximizing the mutual information between the pooled graph and the input graph. Since the maximum mutual information is difficult to compute, we employ the Shannon capacity of a graph as an inductive bias to our pooling method. More precisely, we show that the input graph to the pooling layer can be viewed as a representation of a noisy communication channel. For such a channel, sending the symbols belonging to an independent set of the graph yields a reliable and error-free transmission of information. We show that reaching the maximum mutual information is equivalent to finding a maximum weight independent set of the graph where the weights convey entropy contents. Through this communication theoretic standpoint, we provide a distinct perspective for posing the problem of graph pooling as maximizing the information transmission rate across a noisy communication channel, implemented by a graph neural network. We evaluate our method, referred to as Maximum Entropy Weighted Independent Set Pooling (MEWISPool), on graph classification tasks and the combinatorial optimization problem of the maximum independent set. Empirical results demonstrate that our method achieves the state-of-the-art and competitive results on graph classification tasks and the maximum independent set problem in several benchmark datasets. ","Maximum Entropy Weighted Independent Set Pooling for Graph Neural
  Networks"
167,1414375806885842948,1001049754787368960,Dr. Yu-Dai Tsai,"['Our new paper is out. ""Asteroid g-2 Experiments!""\n<LINK>\nThis is, to our knowledge, the first time asteroid astronomy was used to study new physics. Comments and suggestions are more than welcome. This is the beginning of a new research direction!', '@lucavisinelli @SunnyVagnozzi']",https://arxiv.org/abs/2107.04038,"We study for the first time the possibility of probing long-range fifth forces utilizing asteroid astrometric data, via the fifth force-induced orbital precession. We examine nine Near-Earth Object (NEO) asteroids whose orbital trajectories are accurately determined via optical and radar astrometry. Focusing on a Yukawa-type potential mediated by a new gauge field (dark photon) or a baryon-coupled scalar, we estimate the sensitivity reach for the fifth-force coupling strength and mediator mass in the mass range $m \simeq 10^{-21}-10^{-15}\,{\rm eV}$. Our estimated sensitivity is comparable to leading limits from torsion balance experiments, potentially exceeding these in a specific mass range. The fifth forced-induced precession increases with the orbital semi-major axis in the small $m$ limit, motivating the study of objects further away from the Sun. We discuss future exciting prospects for extending our study to more than a million asteroids (including NEOs, main-belt asteroids, Hildas, and Jupiter Trojans), as well as trans-Neptunian objects and exoplanets. ",Asteroid astrometry as a fifth-force and ultralight dark sector probe
168,1413290165922041863,51570722,Carl Carrie (@🏠),"['New Paper on Codex, a GPT language model fine tuned on publicly available code from GitHub, and it’s Python code-writing capabilities.\n\n<LINK>\n\n#NLP #NLG #OpenAI #AI <LINK>']",https://arxiv.org/abs/2107.03374,"We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. ",Evaluating Large Language Models Trained on Code
169,1412712836661485568,1364749022,Haitham Bou Ammar,['Excited to share and present a new method for data completion using normalising flows. Our method works with many flow types such as iResNet and others. We can complete data even when about 90% missing rates.\n@VincentMoens \nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2107.02474,"We present a method for conditional sampling for pre-trained normalizing flows when only part of an observation is available. We derive a lower bound to the conditioning variable log-probability using Schur complement properties in the spirit of Gaussian conditional sampling. Our derivation relies on partitioning flow's domain in such a way that the flow restrictions to subdomains remain bijective, which is crucial for the Schur complement application. Simulation from the variational conditional flow then amends to solving an equality constraint. Our contribution is three-fold: a) we provide detailed insights on the choice of variational distributions; b) we discuss how to partition the input space of the flow to preserve bijectivity property; c) we propose a set of methods to optimise the variational distribution. Our numerical results indicate that our sampling method can be successfully applied to invertible residual networks for inference and classification. ","Viscos Flows: Variational Schur Conditional Sampling With Normalizing
  Flows"
170,1412693915787055104,1277975431723900936,Laura Rogers,"['New Co-Author paper: ""Infrared Excesses around Bright White Dwarfs from Gaia and unWISE. II"" \n<LINK>', 'Studies identifying and characterising infrared radiation around white dwarf stars can reveal valuable insights into white dwarf planetary systems and stellar companions. In paper I (S. Xu, 2020) 188 infrared excess white dwarf candidates were discovered using Gaia and unWISE', 'In paper II, follow up photometric observations of these candidates were taken with Spitzer (3.6 and 4.5 micron) and Gemini (JHK) to confirm the infrared excess and the nature of the excess', 'The infrared excess is confirmed for 61 white dwarfs, and it is likely 10 of which are stellar companions. Check out table A2 (appendix) for all confirmed infrared excess white dwarfs in this study']",https://arxiv.org/abs/2107.01221,"Infrared excesses around white dwarf stars indicate the presence of various astrophysical objects of interest, including companions and debris disks. In this second paper of a series, we present follow-up observations of infrared excess candidates from Gaia and unWISE discussed in the first paper, Paper I. We report space-based infrared photometry at 3.6 and 4.5 micron for 174 white dwarfs from the Spitzer Space Telescope and ground-based near-infrared J, H, and K photometry of 235 white dwarfs from Gemini Observatory with significant overlap between Spitzer and Gemini observations. This data is used to confirm or rule-out the observed unWISE infrared excess. From the unWISE-selected candidate sample, the most promising infrared excess sample comes from both colour and flux excess, which has a Spitzer confirmation rate of 95%. We also discuss a method to distinguish infrared excess caused by stellar or sub-stellar companions from potential dust disks. In total, we confirm the infrared excess around 62 white dwarfs, 10 of which are likely to be stellar companions. The remaining 52 bright white dwarf with infrared excess beyond two microns has the potential to double the known sample of white dwarfs with dusty exoplanetary debris disks. Follow-up high-resolution spectroscopic studies of a fraction of confirmed excess white dwarfs in this sample have discovered emission from gaseous dust disks. Additional investigations will be able to expand the parameter space from which dust disks around white dwarfs are found. ",Infrared Excesses around Bright White Dwarfs from Gaia and unWISE. II
171,1410999635968106500,4666231375,Konstantin Batygin,"['In a new paper led by Katee Schultz and @ChrisCrossSci, we examine the onset of early dynamical instabilities in short-period Super-Earth systems due to a diminishing stellar J2 as a driving mechanism for planetary inclinations. Details here: <LINK> <LINK>']",https://arxiv.org/abs/2107.00044,"A large proportion of transiting planetary systems appear to possess only a single planet as opposed to multiple transiting planets. This excess of singles is indicative of significant mutual inclinations existing within a large number of planetary systems, but the origin of these misalignments is unclear. Moreover, recent observational characterization reveals that mutual inclinations tend to increase with proximity to the host star. These trends are both consistent with the dynamical influence of a strong quadrupolar potential arising from the host star during its early phase of rapid rotation, coupled with a non-zero stellar obliquity. Here, we simulate a population of planetary systems subject to the secular perturbation arising from a tilted, oblate host star as it contracts and spins down subsequent to planet formation. We demonstrate that this mechanism can reproduce the general increase in planet-planet mutual inclinations with proximity to the host star, and delineate a parameter space wherein the host star can drive dynamical instabilities. We suggest that approximately 5-10\% of low-mass Kepler systems are susceptible to this instability mechanism, suggesting that a significant number of single-transiting planets may truly be intrinsically single. We also report a novel connection between instability and stellar obliquity reduction and make predictions that can be tested within upcoming TESS observations. ","The distribution of mutual inclinations arising from the stellar
  quadrupole moment"
172,1426137381233700865,1357327723539095553,Fokko de Vries,['Our latest study on gate-defined nanodevices in twisted graphene is now available on ArXiV! <LINK> \nWe show how to use gate-defined interfaces to form Fabry Perot cavities and quantum dots in this exciting material. \n@EliasPortoles @ETH_physics <LINK>'],http://arxiv.org/abs/2107.14299,"The rich and electrostatically tunable phase diagram exhibited by moir\'e materials has made them a suitable platform for hosting single material multi-purpose devices. To engineer such devices, understanding electronic transport and localization across electrostatically defined interfaces is of fundamental importance. Little is known, however, about how the interplay between the band structure originating from the moir\'e lattice and electric potential gradients affects electronic confinement. Here, we electrostatically define a cavity across a twisted double bilayer graphene sample. We observe two kinds of Fabry-P\'erot oscillations. The first, independent of charge polarity, stems from confinement of electrons between dispersive-band/flat-band interfaces. The second arises from junctions between regions tuned into different flat bands. When tuning the out-of-plane electric field across the device, we observe Coulomb blockade resonances in transport, an indication of strong electronic confinement. From the gate, magnetic field and source-drain voltage dependence of the resonances, we conclude that quantum dots form at the interfaces of the Fabry-P\'erot cavity. Our results constitute a first step towards better understanding interfacial phenomena in single crystal moir\'e devices. ","Fabry-P\'erot cavities and quantum dot formation at gate-defined
  interfaces in twisted double bilayer graphene"
173,1422165986707390466,22812127,Christian Steinmetz,['We propose a domain-inspired architecture to synthesize a room impulse response given only a short speech recording from with the target room.\n\n📜 <LINK>\n🔊 <LINK>\n\nWork from an internship at @FBRealityLabs with @Paul_Calamia and Vamsi K. Ithapu. <LINK>'],https://arxiv.org/abs/2107.07503,"Deep learning approaches have emerged that aim to transform an audio signal so that it sounds as if it was recorded in the same room as a reference recording, with applications both in audio post-production and augmented reality. In this work, we propose FiNS, a Filtered Noise Shaping network that directly estimates the time domain room impulse response (RIR) from reverberant speech. Our domain-inspired architecture features a time domain encoder and a filtered noise shaping decoder that models the RIR as a summation of decaying filtered noise signals, along with direct sound and early reflection components. Previous methods for acoustic matching utilize either large models to transform audio to match the target room or predict parameters for algorithmic reverberators. Instead, blind estimation of the RIR enables efficient and realistic transformation with a single convolution. An evaluation demonstrates our model not only synthesizes RIRs that match parameters of the target room, such as the $T_{60}$ and DRR, but also more accurately reproduces perceptual characteristics of the target room, as shown in a listening test when compared to deep learning baselines. ","Filtered Noise Shaping for Time Domain Room Impulse Response Estimation
  From Reverberant Speech"
174,1422159482763759616,739069936019083265,Idan Attias,"['Recently, there is a growing interest in streaming algorithms in an adaptive setting. Existing frameworks suggest how to tackle adaptiveness: via differential privacy and difference estimators. We propose a hybrid framework that gains from both approaches:\n<LINK>', 'Previous results:\nhttps://t.co/3V7y2qE6j4 (PODS2020),\nhttps://t.co/Gjxzbg3Fik (NeurIPS2020),\nhttps://t.co/3clAydZ21F.']",https://arxiv.org/abs/2107.14527,"Streaming algorithms are algorithms for processing large data streams, using only a limited amount of memory. Classical streaming algorithms operate under the assumption that the input stream is fixed in advance. Recently, there is a growing interest in studying streaming algorithms that provide provable guarantees even when the input stream is chosen by an adaptive adversary. Such streaming algorithms are said to be {\em adversarially-robust}. We propose a novel framework for adversarial streaming that hybrids two recently suggested frameworks by Hassidim et al. (2020) and by Woodruff and Zhou (2021). These recently suggested frameworks rely on very different ideas, each with its own strengths and weaknesses. We combine these two frameworks (in a non-trivial way) into a single hybrid framework that gains from both approaches to obtain superior performances for turnstile streams. ","A Framework for Adversarial Streaming via Differential Privacy and
  Difference Estimators"
175,1421681752196714500,1400911697452453893,Chandar Lab,"['[1/6] With regard to the results highlighting insensitivity to word order of neural models in NLU tasks, we study it further with newly proposed metrics - DND and IDC.\nWork by Louis Clouâtre, @prasannapartha @amalzouaq @apsarathchandar\narxiv: <LINK> <LINK>', '[2/6]  Two major takeaways: (1) Local ordering of a sequence is more important than its global ordering. We observed that the metric quantifying distortion to local ordering of tokens, DND, correlates with the (in)sensitivity to perturbation of different models ! https://t.co/CKJamx8EYz', '[3/6 ]  We observed the correlation between DND and downstream performance of a model on perturbed sample is consistent across BiLSTM, ConvNet, Transformers and with different tokenization schemes (sub-word, Character level). https://t.co/JFtgGSU2Yl', '[4/6] (2) Our analysis on existing word-level perturbations suggests that models mostly rely on local organization of characters that are seldom perturbed by the commonly used word-level perturbations and hence explaining their insensitivity to such perturbations. https://t.co/20RYZAfL0B', '[5/6] We find that the lack of correlation between performance of Non-pretrained Transformers and IDC metric (global ordering of tokens) helps identify when models fail to make use of the positional information present in text, hence defaulting to bag-of-word models. https://t.co/VsA2UmcQFu', '[6/6] We observe the effect of perturbation to be similar across the different architectures. The weaker models have smaller drop in performance but still explained by the metric. https://t.co/ttgb4cor8B']",https://arxiv.org/abs/2107.13955,"Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations has shown that neural models are surprisingly insensitive to the order of words. In this paper, we investigate this phenomenon by developing order-altering perturbations on the order of words, subwords, and characters to analyze their effect on neural models' performance on language understanding tasks. We experiment with measuring the impact of perturbations to the local neighborhood of characters and global position of characters in the perturbed texts and observe that perturbation functions found in prior literature only affect the global ordering while the local ordering remains relatively unperturbed. We empirically show that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure. ",Local Structure Matters Most: Perturbation Study in NLU
176,1421234894462590979,928301283034914817,Lihua Lei,"['🚨New working paper🚨w/ @ArkhangelskyD, Guido Imbens, and @xiaoman_luo. We propose the RIPW estimator that makes TWFE regression robust to treatment effect heterogeneity under general designs (incl. staggered adoption)\n\n<LINK>\n\nI’ll walk through some details. 1/n <LINK>', 'Recently, it is found that the TWFE estimator is not robust to effect heterogeneity (over units and time).\n\nWe show that if the assignment model is known, our RIPW estimator—a weighted TWFE estimator—resolves this issue in the static case w/o any version of parallel trend.\n\n2/n', 'RIPW estimator reweighs the TWFE objective by the inversed (generalized) propensity weights, modified by a *reshaped distribution* Π on the support of assignments.\n\nIt is tempting to set Π uniform, as in the cross-sectional IPW. Surprisingly, it *doesn’t work* in general.\n\n3/n https://t.co/XvSf64bbjq', 'First, we need to define what we mean by “work”. Our estimand is the DATE (doubly average treatment effect), an average of unit-time specific effects with *user-specified* weights, instead of just a convex combination. It is in a similar spirit to the cross-sectional ATE. \n\n4/n https://t.co/IJHzavW3rc', 'We show that the RIPW estimator converges to DATE if the reshaped distribution satisfies the *DATE equation* -- a “mysterious” quadratic system that only depends on the design (types of treatment assignments) and the weights for DATE.\n\n5/n https://t.co/3E27bquXm0', 'The DATE equation has closed-form solutions for many designs. The most intriguing one is for *staggered adoption* designs where assignments are of form (0, .., 0, 1, .., 1). We show that the uniform dist. is *not* a solution of the DATE equation for equally-weighted DATE. \n\n6/n', 'For example, if there are three periods, the solutions of DATE equation are given by:\n\nΠ(0, 0, 0), Π(0, 0, 1), Π(0, 1, 1), Π(1, 1, 1)\n= a convex combination of (2/9,1/3,0,4/9) &amp; (4/9,0,1/3,2/9).\n\nMagical numbers! Simulation results confirm that they are derived correctly.\n\n7/n https://t.co/rdug4Kjyiu', 'When the generalized propensity scores are known (e.g., RCT), we prove that the RIPW estimator is consistent for DATE, \n\n(1) w/o any version of parallel trends,\n\n(2) w/ unrestricted effect heterogeneity over both units and time.\n\nThe result is purely design-based.\n\n8/n', 'We further show that the RIPW estimator is double-robust: it is consistent for DATE if *either* the TWFE model *or* the assignment model is approximately correct. \n\nWe also provide double-robust confidence intervals under mild additional assumptions! \n\n9/n', 'The double robustness is surprising. \n\nFor cross-sectional data, DR requires a consistent estimate of E[PO|X] when the outcome model is correct. \n\nHowever, for panels with a bounded number of periods, it is impossible because the unit FEs can’t be consistently estimated.\n\n10/n', 'This has strong implications that AIPW-type estimators fail to be double-robust in this case.\n\nBy contrast, our RIPW estimator is a better approach to handle *incidental parameter* problems caused by the large number of FEs. \n\n11/n https://t.co/fLToy8iJ4b', 'In practice, we often have knowledge on the assignment process. For example, the policy adoption time can be modeled via duration models. \n\nWe reanalyzed the data in @Andrew___Baker’s wonderful blog, using the Cox model for the time the law is passed\nhttps://t.co/u8z3RAphjR\n\n12/n', 'We present another example analyzing the short-term effect of state of emergency on the dine-in rate during the early COVID-19 pandemic in the U.S., using the daily data released by OpenTable. \nThis is a case where assignments are (arguably) easier to model than outcomes.\n\n13/n', 'Summary:\n\nTheoretically, the RIPW estimator is robust to unrestricted effect heterogeneity.\n\nPractically, the RIPW estimator can leverage the additional information from the assignment process.\n\nPaper: https://t.co/HO6x9uRxq9\nCode: https://t.co/jmFBYmtqy1\n\n14/n (n=14)']",https://arxiv.org/abs/2107.13737,"We propose a new estimator for the average causal effects of a binary treatment with panel data in settings with general treatment patterns. Our approach augments the two-way-fixed-effects specification with the unit-specific weights that arise from a model for the assignment mechanism. We show how to construct these weights in various settings, including situations where units opt into the treatment sequentially. The resulting estimator converges to an average (over units and time) treatment effect under the correct specification of the assignment model. We show that our estimator is more robust than the conventional two-way estimator: it remains consistent if either the assignment mechanism or the two-way regression model is correctly specified and performs better than the two-way-fixed-effect estimator if both are locally misspecified. This strong double robustness property quantifies the benefits from modeling the assignment process and motivates using our estimator in practice. ",Double-Robust Two-Way-Fixed-Effects Regression For Panel Data
177,1421104067057704966,1047554408542814208,Matteo Bruno,"['Our latest work with @__Sarawalk__ and @RenaudLambiotte is out today on Arxiv! We investigate how bots acted on Twitter during the 2019 UK election, focusing on the #Brexit discussion. We find a large injection of #bots one week before the election... 1/4\n<LINK>', '...and we are find differences in the behaviour of bots, suspended users and genuine users, quantifying their activity and highlighting different patterns, additionally showing that they occupy different positions in retweet networks. 2/4 https://t.co/nSyPTrc5Ke', ""By considering their interactions with verified users, we are able to quantify the presence of bots in different groups. We find that they are spread across all groups, but a large amount of bots interact with the pro-Brexit side and many are close to Trump's 2020 campaign. 3/4 https://t.co/IzqeoUdMMS"", 'Furthermore, we analyze the use of hashtags by automated accounts, finding group of coordinated users retweeting popular and populist hashtags in order to boost the relative discussions. Check out the paper for more details! 4/4 https://t.co/N3vpwj9dFd']",http://arxiv.org/abs/2107.14155,"Online Social Networks represent a novel opportunity for political campaigns, revolutionising the paradigm of political communication. Nevertheless, many studies uncovered the presence of d/misinformation campaigns or of malicious activities by genuine or automated users, putting at severe risk the credibility of online platforms. This phenomenon is particularly evident during crucial political events, as political elections. In the present paper, we provide a comprehensive description of the structure of the networks of interactions among users and bots during the UK elections of 2019. In particular, we focus on the polarised discussion about Brexit on Twitter analysing a data set made of more than 10 million tweets posted for over a month. We found that the presence of automated accounts fostered the debate particularly in the days before the UK national elections, in which we find a steep increase of bots in the discussion; in the days after the election day, their incidence returned to values similar to the ones observed few weeks before the elections. On the other hand, we found that the number of suspended users (i.e. accounts that were removed by the platform for some violation of the Twitter policy) remained constant until the election day, after which it reached significantly higher values. Remarkably, after the TV debate between Boris Johnson and Jeremy Corbyn, we observed the injection of a large number of novel bots whose behaviour is markedly different from that of pre-existing ones. Finally, we explored the bots' stance, finding that their activity is spread across the whole political spectrum, although in different proportions, and we studied the different usage of hashtags by automated accounts and suspended users, thus targeting the formation of common narratives in different sides of the debate. ","Brexit and bots: characterizing the behaviour of automated accounts on
  Twitter during the UK election"
178,1421044847826325508,1169535083579092992,Ana Martin,"['We have a new member on the DAQC family 🥳. In this last paper we study the effect of noise sources in digital and digital-analog quantum computing and compare the performance of both paradigms. Thanks to @Paula_G_Phys, @qmisanz for this great teamwork 👏🏼\n\n<LINK>']",http://arxiv.org/abs/2107.12969,"Quantum computing makes use of quantum resources provided by the underlying quantum nature of matter to enhance classical computation. However, current Noisy Intermediate-Scale Quantum (NISQ) era in quantum computing is characterized by the use of quantum processors comprising from a few tens to, at most, few hundreds of physical qubits without implementing quantum error correction techniques. This limits the scalability in the implementation of quantum algorithms. Digital-analog quantum computing (DAQC) has been proposed as a more resilient alternative quantum computing paradigm to outperform digital quantum computation within the NISQ era framework. It arises from adding the flexibility provided by fast single-qubit gates to the robustness of analog quantum simulations. Here, we perform a careful comparison between digital and digital-analog paradigms under the presence of noise sources. The comparison is illustrated by comparing the performance of the quantum Fourier transform algorithm under a wide range of single- and two-qubit noise sources. Indeed, we obtain that, when the different noise channels usually present in superconducting quantum processors are considered, the fidelity of the QFT algorithm for the digital-analog paradigm outperforms the one obtained for the digital approach. Additionally, this difference grows when the size of the processor scales up, constituting consequently a sensible alternative paradigm in the NISQ era. Finally, we show how the DAQC paradigm can be adapted to quantum error mitigation techniques for canceling different noise sources, including the bang error. ",Noise in Digital and Digital-Analog Quantum Computation
179,1420984125129854977,1205696717137444865,Pedram Roushan,"['Much of nature is not in equilibrium. It is challenging to distinguish dynamical phases from transient phenomena.  We observe time crystalline phase, establishing a scalable approach to study nonequilibrium phases of matter with quantum processors.\n<LINK> <LINK>', '@KJSatz @JarrodMcclean @nick_c_rubin @mharrigan52 @balopat @ofer_naaman']",https://arxiv.org/abs/2107.13571,"Quantum many-body systems display rich phase structure in their low-temperature equilibrium states. However, much of nature is not in thermal equilibrium. Remarkably, it was recently predicted that out-of-equilibrium systems can exhibit novel dynamical phases that may otherwise be forbidden by equilibrium thermodynamics, a paradigmatic example being the discrete time crystal (DTC). Concretely, dynamical phases can be defined in periodically driven many-body localized systems via the concept of eigenstate order. In eigenstate-ordered phases, the entire many-body spectrum exhibits quantum correlations and long-range order, with characteristic signatures in late-time dynamics from all initial states. It is, however, challenging to experimentally distinguish such stable phases from transient phenomena, wherein few select states can mask typical behavior. Here we implement a continuous family of tunable CPHASE gates on an array of superconducting qubits to experimentally observe an eigenstate-ordered DTC. We demonstrate the characteristic spatiotemporal response of a DTC for generic initial states. Our work employs a time-reversal protocol that discriminates external decoherence from intrinsic thermalization, and leverages quantum typicality to circumvent the exponential cost of densely sampling the eigenspectrum. In addition, we locate the phase transition out of the DTC with an experimental finite-size analysis. These results establish a scalable approach to study non-equilibrium phases of matter on current quantum processors. ",Observation of Time-Crystalline Eigenstate Order on a Quantum Processor
180,1420828066868301825,2806161057,Pedro Tsividis,"['Excited to share our preprint, in which we propose a strong form of model-based RL: Theory-Based Reinforcement Learning. An agent endowed with common sense concepts learns as quickly as humans in a challenging suite of video games: <LINK>', 'Our game playing agent, EMPA (the Exploring, Modeling, Planning Agent), performs Bayesian inference to learn probabilistic generative models expressed as programs for a game-engine simulator, and runs internal simulations to support efficient exploration and heuristic planning.', 'EMPA closely matches human learning efficiency on a suite of 90 challenging video games, learning new games in just minutes of game play and generalizing robustly to new game situations and new levels. https://t.co/OkJgLfjqcm', ""The model also captures fine-grained structure in people's exploration trajectories and learning dynamics. https://t.co/QMt6XiAtA5"", 'Many thanks to my co-authors Joshua Tenenbaum, @gershbrain, @JoaoLoula, Jake Burga, Nate Foss, Andres Campero, and Thomas Pouncy. Also big thanks to @juanprdzlv and @ajaysub110 for ongoing work. @MITCoCoSci']",https://arxiv.org/abs/2107.12544v1,"Reinforcement learning (RL) studies how an agent comes to achieve reward in an environment through interactions over time. Recent advances in machine RL have surpassed human expertise at the world's oldest board games and many classic video games, but they require vast quantities of experience to learn successfully -- none of today's algorithms account for the human ability to learn so many different tasks, so quickly. Here we propose a new approach to this challenge based on a particularly strong form of model-based RL which we call Theory-Based Reinforcement Learning, because it uses human-like intuitive theories -- rich, abstract, causal models of physical objects, intentional agents, and their interactions -- to explore and model an environment, and plan effectively to achieve task goals. We instantiate the approach in a video game playing agent called EMPA (the Exploring, Modeling, and Planning Agent), which performs Bayesian inference to learn probabilistic generative models expressed as programs for a game-engine simulator, and runs internal simulations over these models to support efficient object-based, relational exploration and heuristic planning. EMPA closely matches human learning efficiency on a suite of 90 challenging Atari-style video games, learning new games in just minutes of game play and generalizing robustly to new game situations and new levels. The model also captures fine-grained structure in people's exploration trajectories and learning dynamics. Its design and behavior suggest a way forward for building more general human-like AI systems. ","] Human-Level Reinforcement Learning through Theory-Based Modeling,
  Exploration, and Planning"
181,1420804708629766146,888216099757490176,Maithra Raghu,"['Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization\n\nWe introduce a rich family of tasks with a  pointer-value rule, to study mechanisms NN of generalization, from memorization to reasoning. \n\nPaper: <LINK> <LINK>', 'Generalization to unseen instances is central to the success of neural networks, but how does this happen?\n\nAre neural networks reliant on seeing very similar training examples?\n\nHow much do they generalize through abstract reasoning?\n\nDisentangling these methods is challenging!', 'We introduce Pointer Value Retrieval (PVR), a family of tasks to study these aspects of neural network generalization.\n\nPVR tasks vary in input type and difficulty, but in all of them, one position of the input acts as a pointer to a different input position (the value). https://t.co/NbrKQuUOE9', 'The interplay between positions, values and the pointer rule allows us to systematically vary difficulty, from introducing distribution shift to increasing functional complexity. \n\nThis in turn helps us explore different types of generalization! https://t.co/2qqBmDRjR0', 'We study a warmup task of exploring generalization in a visual PVR task.\n\nWe find generalization here relies on nearest-neighbor styled approaches and uncover a subtle spurious correlation causing poor performance on distribution shift. https://t.co/Kb6KWjSx7c', 'We then explore PVR tasks of increasing functional complexity. While train accuracy remains high throughout, test accuracy is very variable, and highly dependent on data size, suggesting some reliance on memorization\n\nWe show connections to noise sensitivity in boolean functions https://t.co/B19mniAztB', 'We also investigate performance variations across different model architectures, including MLPs, and the MLP-Mixer. Better architectural inductive bias shows clear benefits to data efficiency and generalization. https://t.co/vO45VzVTfq', ""Finally, to test reasoning, we study a PVR task with distribution shift &amp; functional complexity. Importantly, memorization alone can't solve this task\n\nWe find a surprising partial success: training is unstable, but in successful runs, the model learns the PVR reasoning rule! https://t.co/1IPyDzTHkR"", 'These results suggest many exciting open questions to study with PVR. Are there structural inductive biases that can prevent some cases of spurious correlations? What happens in training dynamics to determine whether reasoning is learned? We hope these can be explored in future! https://t.co/Qdrl6qPugo', 'This work is with wonderful (and entirely Twitterless!) collaborators Chiyuan Zhang, Jon Kleinberg and Samy Bengio', '@yieldthought Both the Transformer and the MLP-mixer did better than the naive MLP (even when we scaled the latter up), and the MLP-mixer was (surprisingly!) a bit better than the Transformer: higher accuracy and more data efficient!']",https://arxiv.org/abs/2107.12580,"Central to the success of artificial neural networks is their ability to generalize. But does neural network generalization primarily rely on seeing highly similar training examples (memorization)? Or are neural networks capable of human-intelligence styled reasoning, and if so, to what extent? These remain fundamental open questions on artificial neural networks. In this paper, as steps towards answering these questions, we introduce a new benchmark, Pointer Value Retrieval (PVR) to study the limits of neural network reasoning. The PVR suite of tasks is based on reasoning about indirection, a hallmark of human intelligence, where a first stage (task) contains instructions for solving a second stage (task). In PVR, this is done by having one part of the task input act as a pointer, giving instructions on a different input location, which forms the output. We show this simple rule can be applied to create a diverse set of tasks across different input modalities and configurations. Importantly, this use of indirection enables systematically varying task difficulty through distribution shifts and increasing functional complexity. We conduct a detailed empirical study of different PVR tasks, discovering large variations in performance across dataset sizes, neural network architectures and task complexity. Further, by incorporating distribution shift and increased functional complexity, we develop nuanced tests for reasoning, revealing subtle failures and surprising successes, suggesting many promising directions of exploration on this benchmark. ","Pointer Value Retrieval: A new benchmark for understanding the limits of
  neural network generalization"
182,1420291675461758976,1317002454375059456,Paula García Molina,['New work on Noise in Digital and Digital-Analog Quantum Computation <LINK> In this paper in collaboration with @qmisanz and @quantum_ana we study the effect of noise sources in digital and digital-analog quantum computing and error mitigation techniques.'],https://arxiv.org/abs/2107.12969,"Quantum computing makes use of quantum resources provided by the underlying quantum nature of matter to enhance classical computation. However, current Noisy Intermediate-Scale Quantum (NISQ) era in quantum computing is characterized by the use of quantum processors comprising from a few tens to, at most, few hundreds of physical qubits without implementing quantum error correction techniques. This limits the scalability in the implementation of quantum algorithms. Digital-analog quantum computing (DAQC) has been proposed as a more resilient alternative quantum computing paradigm to outperform digital quantum computation within the NISQ era framework. It arises from adding the flexibility provided by fast single-qubit gates to the robustness of analog quantum simulations. Here, we perform a careful comparison between digital and digital-analog paradigms under the presence of noise sources. The comparison is illustrated by comparing the performance of the quantum Fourier transform algorithm under a wide range of single- and two-qubit noise sources. Indeed, we obtain that, when the different noise channels usually present in superconducting quantum processors are considered, the fidelity of the QFT algorithm for the digital-analog paradigm outperforms the one obtained for the digital approach. Additionally, this difference grows when the size of the processor scales up, constituting consequently a sensible alternative paradigm in the NISQ era. Finally, we show how the DAQC paradigm can be adapted to quantum error mitigation techniques for canceling different noise sources, including the bang error. ",Noise in Digital and Digital-Analog Quantum Computation
183,1420287742068539394,801793180899360768,Hendrik Schuff,"['""Looks like a worm ... oh wait, I think it\'s a snake.""\nHow can we give machine learning models the capability of a 2nd, 3rd and k-th thought?\nWe propose a method to do so in our latest preprint and show that it can improve performance.\n<LINK> <LINK>']",https://arxiv.org/abs/2107.12220,"When humans solve complex problems, they rarely come up with a decision right-away. Instead, they start with an intuitive decision, reflect upon it, spot mistakes, resolve contradictions and jump between different hypotheses. Thus, they create a sequence of ideas and follow a train of thought that ultimately reaches a conclusive decision. Contrary to this, today's neural classification models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. We take inspiration from Hegel's dialectics and propose a method that turns an existing classifier's class prediction (such as the image class forest) into a sequence of predictions (such as forest $\rightarrow$ tree $\rightarrow$ mushroom). Concretely, we propose a correction module that is trained to estimate the model's correctness as well as an iterative prediction update based on the prediction's gradient. Our approach results in a dynamic system over class probability distributions $\unicode{x2014}$ the thought flow. We evaluate our method on diverse datasets and tasks from computer vision and natural language processing. We observe surprisingly complex but intuitive behavior and demonstrate that our method (i) can correct misclassifications, (ii) strengthens model performance, (iii) is robust to high levels of adversarial attacks, (iv) can increase accuracy up to 4% in a label-distribution-shift setting and (iv) provides a tool for model interpretability that uncovers model knowledge which otherwise remains invisible in a single distribution prediction. ",Thought Flow Nets: From Single Predictions to Trains of Model Thought
184,1420032056151613440,1219708000790876161,Datta Lab,"['Moving fronts are ubiquitous in nature. One of the most basic characteristics of a front is its morphological stability: Do shape perturbations decay or grow over time? In <LINK>, we studied this for chemotactic fronts of collectively-migrating cells. (1/7) <LINK>', 'This question is well-studied for diverse other classes of fronts (e.g., the classic Saffman-Taylor and Mullins-Sekerka instabilities that give viscous fingering &amp; dendritic growth during crystallization, respectively). But the answer is less clear for chemotactic fronts. (2/7)', ""Inspired by some of our lab's previous experiments (https://t.co/vybq3LRrQX), we worked with @RicardAlert to theoretically establish the conditions for the stability of chemotactic fronts -- bringing chemotaxis into the classic framework of interfacial instabilities. (3/7) https://t.co/WFiQS4fARZ"", 'Using a linear stability analysis, we established the conditions for front stability. Surprisingly, limitations in the ability of individual cells to sense chemical stimuli at different concentrations control the entire population-scale morphology of a chemotactic front! (4/7)', ""Guided by these findings, we examined experimental data on bacterial chemotaxis and found that cells seem to consistently be positioned in the regime for stable fronts -- suggesting that the cells' sensory machinery *might* have evolved to ensure stable front propagation. (5/7)"", 'Also, these findings may be more general: as sensing of any stimuli is necessarily limited in living and active matter in general, the principle of sensing-induced stability may operate in other types of directed migration such as durotaxis, electrotaxis, and phototaxis. (6/7)', 'Thank you @RicardAlert for a wonderful collaboration, and thanks to the @NSF, @HFSP, and @EPrinceton for funding. As usual, feedback/questions are always welcome! (7/7)']",https://arxiv.org/abs/2107.11702,"In contexts ranging from embryonic development to bacterial ecology, cell populations migrate chemotactically along self-generated chemical gradients, often forming a propagating front. Here, we theoretically show that the stability of such chemotactic fronts to morphological perturbations is determined by limitations in the ability of individual cells to sense and thereby respond to the chemical gradient. Specifically, cells at bulging parts of a front are exposed to a smaller gradient, which slows them down and promotes stability, but they also respond more strongly to the gradient, which speeds them up and promotes instability. We predict that this competition leads to chemotactic fingering when sensing is limited at too low chemical concentrations. Guided by this finding and by experimental data on E. coli chemotaxis, we suggest that the cells' sensory machinery might have evolved to avoid these limitations and ensure stable front propagation. Finally, as sensing of any stimuli is necessarily limited in living and active matter in general, the principle of sensing-induced stability may operate in other types of directed migration such as durotaxis, electrotaxis, and phototaxis. ",Cellular Sensing Governs the Stability of Chemotactic Fronts
185,1419978974650933261,1168263794906324992,Ricard Alert Zenón,"['👉 How do chemotactic cells manage to migrate together in flat fronts? Can they experience a fingering instability? Check out the answers in our preprint with @TheSquishyLab, where we propose a theory for the stability of chemotactic fronts. 😀\n\n<LINK> <LINK>', ""We show that fingering occurs if the ability of cells to sense chemical stimuli is limited at too low chemoattractant concentrations. We then suggest that the cells' sensory machinery might have evolved to avoid these limitations and ensure robust collective chemotaxis! https://t.co/jPE6yJo1WX""]",http://arxiv.org/abs/2107.11702,"In contexts ranging from embryonic development to bacterial ecology, cell populations migrate chemotactically along self-generated chemical gradients, often forming a propagating front. Here, we theoretically show that the stability of such chemotactic fronts to morphological perturbations is determined by limitations in the ability of individual cells to sense and thereby respond to the chemical gradient. Specifically, cells at bulging parts of a front are exposed to a smaller gradient, which slows them down and promotes stability, but they also respond more strongly to the gradient, which speeds them up and promotes instability. We predict that this competition leads to chemotactic fingering when sensing is limited at too low chemical concentrations. Guided by this finding and by experimental data on E. coli chemotaxis, we suggest that the cells' sensory machinery might have evolved to avoid these limitations and ensure stable front propagation. Finally, as sensing of any stimuli is necessarily limited in living and active matter in general, the principle of sensing-induced stability may operate in other types of directed migration such as durotaxis, electrotaxis, and phototaxis. ",Cellular Sensing Governs the Stability of Chemotactic Fronts
186,1419918173701230600,725230886170431488,Sangwoo Mo,"[""Many abstract reasoning problems (e.g., IQ test) can be interpreted as a MAXSAT optimization! Inspired by this, we propose a structured generative model which can construct the answers of Raven's Progressive Matrices.\n\n<LINK>\n1/4 <LINK>"", 'Our key idea is to encode the problem (e.g., 8 context images) as propositional variables via VQVAE and predict the solution via a differentiable SATNet layer.\n\nHeavily inspired by amazing works by @_powei @brandondamos @zicokolter!\nCheck also SATNet: https://t.co/O0kuYiAs6o\n2/4', 'Our method creates more visually/logically plausible answers than the black-box generative models (e.g., pix2pix) and outperforms previous discriminative approaches, even though we do not assume the predefined candidate solutions.\n3/4 https://t.co/H0sPAWfwp5', 'Led by a brilliant 1st year PhD student @sihyun_yu and collaborated with @sungsoo_ahn_ @jinwoos0417 \n\nThis work is also presented as a Spotlight Talk at ICML 2021 Workshop: Self-Supervised Learning for Reasoning and Perception 😀\n4/4']",https://arxiv.org/abs/2107.10493,"Abstract reasoning, i.e., inferring complicated patterns from given observations, is a central building block of artificial general intelligence. While humans find the answer by either eliminating wrong candidates or first constructing the answer, prior deep neural network (DNN)-based methods focus on the former discriminative approach. This paper aims to design a framework for the latter approach and bridge the gap between artificial and human intelligence. To this end, we propose logic-guided generation (LoGe), a novel generative DNN framework that reduces abstract reasoning as an optimization problem in propositional logic. LoGe is composed of three steps: extract propositional variables from images, reason the answer variables with a logic layer, and reconstruct the answer image from the variables. We demonstrate that LoGe outperforms the black box DNN frameworks for generative abstract reasoning under the RAVEN benchmark, i.e., reconstructing answers based on capturing correct rules of various attributes from observations. ",Abstract Reasoning via Logic-guided Generation
187,1419660662716895233,14383237,Jeroen van der Ham 💉💉🦠💉,"['You can dynamically detect malware on Android mobiles using machine learning, even without privileged access. Great study by Sebastian Panman-de Wit, Doina Bucur and myself, where we used real world data. <LINK>']",https://arxiv.org/abs/2107.11167,"Mobile malware are malicious programs that target mobile devices. They are an increasing problem, as seen in the rise of detected mobile malware samples per year. The number of active smartphone users is expected to grow, stressing the importance of research on the detection of mobile malware. Detection methods for mobile malware exist but are still limited. In this paper, we provide an overview of the performance of machine learning (ML) techniques to detect malware on Android, without using privileged access. The ML-classifiers use device information such as the CPU usage, battery usage, and memory usage for the detection of 10 subtypes of Mobile Trojans on the Android Operating System (OS). We use a real-life dataset containing device and malware data from 47 users for a year (2016). We examine which features, i.e. aspects, of a device, are most important to monitor to detect (subtypes of) Mobile Trojans. The focus of this paper is on dynamic hardware features. Using these dynamic features we apply state-of-the-art machine learning classifiers: Random Forest, K-Nearest Neighbour, and AdaBoost. We show classification results on different feature sets, making a distinction between global device features, and specific app features. None of the measured feature sets require privileged access. Our results show that the Random Forest classifier performs best as a general malware classifier: across 10 subtypes of Mobile Trojans, it achieves an F1 score of 0.73 with a False Positive Rate (FPR) of 0.009 and a False Negative Rate (FNR) of 0.380. The Random Forest, K-Nearest Neighbours, and AdaBoost classifiers achieve F1 scores above 0.72, an FPR below 0.02 and, an FNR below 0.33, when trained separately to detect each subtype of Mobile Trojans. ","Dynamic detection of mobile malware using smartphone data and machine
  learning"
188,1419647955666051074,3272519155,Bihan Wen,"['Low-light image enhancement (LLIE) is challenging, due to varied degradations and quality assessment for different samples. We propose the first deep reinforcement learning model for customized LLIE. Our ACM MM paper: <LINK>. Code: <LINK>']",https://arxiv.org/abs/2107.05830,"Low-light image enhancement (LLIE) is a pervasive yet challenging problem, since: 1) low-light measurements may vary due to different imaging conditions in practice; 2) images can be enlightened subjectively according to diverse preferences by each individual. To tackle these two challenges, this paper presents a novel deep reinforcement learning based method, dubbed ReLLIE, for customized low-light enhancement. ReLLIE models LLIE as a markov decision process, i.e., estimating the pixel-wise image-specific curves sequentially and recurrently. Given the reward computed from a set of carefully crafted non-reference loss functions, a lightweight network is proposed to estimate the curves for enlightening of a low-light image input. As ReLLIE learns a policy instead of one-one image translation, it can handle various low-light measurements and provide customized enhanced outputs by flexibly applying the policy different times. Furthermore, ReLLIE can enhance real-world images with hybrid corruptions, e.g., noise, by using a plug-and-play denoiser easily. Extensive experiments on various benchmarks demonstrate the advantages of ReLLIE, comparing to the state-of-the-art methods. ","ReLLIE: Deep Reinforcement Learning for Customized Low-Light Image
  Enhancement"
189,1418996397274173445,882180067870351360,Holcman,"['To better understand the ionic flux between channels, we study  the diffusion approximation in \n""Modeling and asymptotic analysis of the concentration difference in a nanoregion between an influx and outflux diffusion across narrow windows""\n<LINK>']",https://arxiv.org/abs/2107.09476,"When a flux of Brownian particles is injected in a narrow window located on the surface of a bounded domain, these particles diffuse and can eventually escape through a cluster of narrow windows. At steady-state, we compute asymptotically the distribution of concentration between the different windows. The solution is obtained by solving Laplace's equation using Green's function techniques and second order asymptotic analysis, and depends on the influx amplitude, the diffusion properties as well as the geometrical organization of all the windows, such as their distances and the mean curvature. We explore the range of validity of the present asymptotic expansions using numerical simulations of the mixed boundary value problem. Finally, we introduce a length scale to estimate how deep inside a domain a local diffusion current can spread. We discuss some applications in biophysics. ","Modeling and asymptotic analysis of the concentration difference in a
  nanoregion between an influx and outflux diffusion across narrow windows"
190,1418369014326341633,22604597,Rohan Alexander,"[""Annie Collins and I look at the reproducibility of COVID-19-related pre-prints and find that it's not great. We download a lot of pre-prints, look for markers of open code/data, and are unable to find them for roughly 3/4. \nDetails: <LINK>\nComments welcome!"", ""Annie is an undergrad at @UofTStatSci and this started as a class paper. Thank you @CANSSIOntario and @monjalexander for providing financial support. I'm not sure what she's doing when she graduates, but if you're looking for someone brilliant then you couldn't do better.""]",https://arxiv.org/abs/2107.10724,"To examine the reproducibility of COVID-19 research, we create a dataset of pre-prints posted to arXiv, bioRxiv, and medRxiv between 28 January 2020 and 30 June 2021 that are related to COVID-19. We extract the text from these pre-prints and parse them looking for keyword markers signaling the availability of the data and code underpinning the pre-print. For the pre-prints that are in our sample, we are unable to find markers of either open data or open code for 75 per cent of those on arXiv, 67 per cent of those on bioRxiv, and 79 per cent of those on medRxiv. ",Reproducibility of COVID-19 pre-prints
191,1418276159456825348,333069274,sukanya chakrabarti,['congratulations to my grad student Peter Craig on his first-author paper on the Magellanic Stream! (also w Stefi Baum and @tc1415 ) we find a direct correlation between the mass of the Milky Way and the stream length <LINK>'],https://arxiv.org/abs/2107.09791,"We present a model for the formation of the Magellanic Stream (MS) due to ram pressure stripping. We model the history of the Small and Large Magellanic Clouds in the recent cosmological past in a static Milky Way potential with diffuse halo gas, using observationally motivated orbits for the Magellanic Clouds derived from HST proper motions within the potential of the Milky Way. This model is able to reproduce the trailing arm but does not reproduce the leading arm feature, which is common for models of the stream formation that include ram pressure stripping effects. Our model produces a good match to observations (including the densities and line-of-sight velocities of the stream, as well as the positions and velocities of the satellites at present day) when we include a diffuse halo component for the Milky Way. From analyzing our grid of models, we find that there is a direct correlation between the observed stream length in our simulations and the mass of the Milky Way. For the observed MS length, the inferred Milky Way mass is $1.5 \pm 0.3 \times 10^{12}$ $M_\odot$, which agrees closely with other independent measures of the Milky Way mass. We also discuss the MS in the context of HI streams in galaxy clusters, and find that the MS lies on the low-mass end of a continuum from Hickson groups to the Virgo cluster. As a tracer of the dynamical mass in the outer halo, the MS is a particularly valuable probe of the Milky Way's potential. ",A Dynamical Mass Estimate from the Magellanic Stream
192,1418255159503769612,2867454883,Mar Giralt-Miron,"[""We're really happy! After 3 years of work with Marcel Guardia and Inma Baldomá (@DynSysBCN) our results are online!🎉\n\n<LINK>\n<LINK>\n\nIn these papers we study the breakdown of the stable and unstable manifolds of L3 in the RPC3BP ☀️🪐☄️\n\n(1/9)"", 'We study the Restricted Planar Circular 3-Body problem. ⬇️\n\n(2/9) https://t.co/lFXbzEUvtX', 'When taking a system of coordinates that rotates with the primaries, its position gets fixed.\n\nThis system can be modeled by a 2-dof Hamiltonian, that has five critical points, called Lagrange points (L1, L2, L3, L4 and L5).\n\nWe study L3.\n\n(3/9) https://t.co/QgBGQUyvBP', 'L3 is a saddle-center critical point. Therefore, it has a 1-dim stable and unstable manifold in a 4-dim phase space.\n\n(The picture is a projection into the position-plane, it is missing a representation of the momenta)\n\n(4/9) https://t.co/X2qzDvB3D5', 'The aim of our papers has been to obtain an asymptotic formula for the distance between stable and unstable manifolds when the mass ratio μ is small. ⬇️\n\n(5/9) https://t.co/33mZyFEnWZ', ""Notice that the distance between manifolds is exponentially small with respect to μ!\n\nThis is a beyond all orders phenomenon, which means that the difference can't be detected by expanding into a power series\n\n As a result, classical perturbative methods can't be applied😓\n\n(6/9)"", 'This happens because the hyperbolic eigenvalues of L3 are smaller than the elliptic ones.\n\nThen, the rapidly rotating dynamics cause this phenomenon, usually known as exponentially small splitting of separatrices.\n\n(7/9) https://t.co/cCoPaLbqWr', 'To compute the first term of the asymptotic formula we had to analyse the complex singularities of the homoclinic of a certain averaged equation and the associated inner equation.\n\nCheck our complete result ⬇️\n\n(8/9) https://t.co/BaQ5MhIyCd', 'We expect this result to be a first step to prove the existence of chaotic motions “exponentially close” to L3 and its invariant manifolds. \n\nBut this is work for the next paper!\n\nThanks for reading this far. We hope you liked it! 😊 \n\n(9/9)']",https://arxiv.org/abs/2107.09942,"The Restricted 3-Body Problem models the motion of a body of negligible mass under the gravitational influence of two massive bodies, called the primaries. If the primaries perform circular motions and the massless body is coplanar with them, one has the Restricted Planar Circular 3-Body Problem (RPC3BP). In synodic coordinates, it is a two degrees of freedom Hamiltonian system with five critical points, L1,..,L5, called the Lagrange points. The Lagrange point L3 is a saddle-center critical point which is collinear with the primaries and is located beyond the largest of the two. In this paper and its sequel, we provide an asymptotic formula for the distance between the one dimensional stable and unstable invariant manifolds of L3 when the ratio between the masses of the primaries $\mu$ is small. It implies that L3 cannot have one-round homoclinic orbits. If the mass ratio $\mu$ is small, the hyperbolic eigenvalues are weaker than the elliptic ones by factor of order $\sqrt{\mu}$. This implies that the distance between the invariant manifolds is exponentially small with respect to $\mu$ and, therefore, the classical Poincar\'e--Melnikov method cannot be applied. In this first paper, we approximate the RPC3BP by an averaged integrable Hamiltonian system which possesses a saddle center with a homoclinic orbit and we analyze the complex singularities of its time parameterization. We also derive and study the inner equation associated to the original perturbed problem. The difference between certain solutions of the inner equation gives the leading term of the distance between the stable and unstable manifolds of L3. In the sequel we complete the proof of the asymptotic formula for the distance between the invariant manifolds. ","Breakdown of homoclinic orbits to L3 in the RPC3BP (I). Complex
  singularities and the inner equation"
193,1418228961050435584,1186248407939244033,Uri Shaham,"['What do you get when you cross beam search and nucleus sampling?\n\nWe explore two variants of ""nucleus search"" that integrate top-p pruning into deterministic search.\nCan they find some gems that beam search misses?👇\n<LINK>\nwith @omerlevy_']",https://arxiv.org/abs/2107.09729,"We combine beam search with the probabilistic pruning technique of nucleus sampling to create two deterministic nucleus search algorithms for natural language generation. The first algorithm, p-exact search, locally prunes the next-token distribution and performs an exact search over the remaining space. The second algorithm, dynamic beam search, shrinks and expands the beam size according to the entropy of the candidate's probability distribution. Despite the probabilistic intuition behind nucleus search, experiments on machine translation and summarization benchmarks show that both algorithms reach the same performance levels as standard beam search. ",What Do You Get When You Cross Beam Search with Nucleus Sampling?
194,1417856009134018561,1256206943938584576,KavehDelfanazari,"['In this pre-print, we propose voltage-controlled photonic integrated circuits with hybrid graphene-superconductor resonator arrays: active control of electromagnetic induced transparency:\n<LINK>\n@KDelfanazari @UofG_ENE @cambridge_ee @JoyceGroup_ @TheHofmannGroup']",https://arxiv.org/abs/2107.03677,"Metamaterial photonic integrated circuits with arrays of hybrid graphene-superconductor coupled split-ring resonators (SRR) capable of modulating and slowing down terahertz (THz) light are introduced and proposed. The hybrid device optical responses, such as electromagnetic induced transparency (EIT) and group delay, can be modulated in several ways. First, it is modulated electrically by changing the conductivity and carrier concentrations in graphene. Alternatively, the optical response can be modified by acting on the device temperature sensitivity, by switching Nb from a lossy normal phase to a low-loss quantum mechanical phase below the transition temperature (Tc) of Nb. Maximum modulation depths of 57.3 % and 97.61 % are achieved for EIT and group delay at the THz transmission window, respectively. A comparison is carried out between the Nb-graphene-Nb coupled SRR-based devices with those of Au-graphene-Au SRRs and a significant enhancement of the THz transmission, group delay, and EIT responses are observed when Nb is in the quantum mechanical phase. Such hybrid devices with their reasonably large and tunable slow light bandwidth pave the way for the realization of active optoelectronic modulators, filters, phase shifters, and slow light devices for applications in chip-scale quantum communication and quantum processing. ","Active terahertz modulator and slow light metamaterial devices with
  hybrid graphene-superconductor photonic integrated circuits"
195,1417794633791426568,1322479795662397442,Constantinos Siettos,"['We address a computational model bridging the micro-scale of neuronal modulation and the cognitive scale for a simple antisaccade task, thus capturing the major hypotheses in #Schizophrenia . Find out more in our new preprint at: <LINK> !']",https://arxiv.org/abs/2107.09360,We address a biophysical network dynamical model to study how the modulation of dopamine (DA) activity and related N-methyl-d-aspartate (NMDA) glutamate receptor activity as well as the emerging Pre-Frontal Cortex (PFC) functional connectivity network (FCN) affect inhibitory cognitive function in schizophrenia in an antisaccade task. The values of the model parameters and the topology of the PFC-FCN were estimated by minimizing the differences between simulations and the observed distributions of reaction times (RT) during the performance of the antisaccade task in 30 patients with schizophrenia and 30 healthy controls. We show that the proposed model approximates remarkably well the predicted prefrontal cortical DA hypo-activity and the related NMDA receptor hypo-function as well as the FCN dysconnection pattern that are considered as the major etio-pathological hypotheses to explain cognitive deficits in schizophrenia. ,"A biophysical network model reveals the link between deficient
  inhibitory cognitive control and major neurotransmitter and neural
  connectivity hypotheses in schizophrenia"
196,1417646731777302532,988606024625131520,Kyungjoo Noh,"['Excited to share my first last-author paper: <LINK>. We propose an interesting way to suppress leakage in Kerr cat qubits using colored single-photon loss (hence ""colored Kerr cat qubits""). It\'s been a fun project!']",https://arxiv.org/abs/2107.09198,"Protected qubits such as the 0-$\pi$ qubit, and bosonic qubits including cat qubits and GKP qubits offer advantages for fault-tolerance. Some of these protected qubits (e.g., 0-$\pi$ qubit and Kerr cat qubit) are stabilized by Hamiltonians which have (near-)degenerate ground state manifolds with large energy-gaps to the excited state manifolds. Without dissipative stabilization mechanisms the performance of such energy-gap-protected qubits can be limited by leakage to excited states. Here, we propose a scheme for dissipatively stabilizing an energy-gap-protected qubit using colored (i.e., frequency-selective) dissipation without inducing errors in the ground state manifold. Concretely we apply our colored dissipation technique to Kerr cat qubits and propose colored Kerr cat qubits which are protected by an engineered colored single-photon loss. When applied to the Kerr cat qubits our scheme significantly suppresses leakage-induced bit-flip errors (which we show are a limiting error mechanism) while only using linear interactions. Beyond the benefits to the Kerr cat qubit we also show that our frequency-selective loss technique can be applied to a broader class of protected qubits. ",Stabilizing a Bosonic Qubit using Colored Dissipation
197,1417577420601823232,954436574468624384,Karolina Dziugaite,"['1/ Welcome, Twitter, to my 1st tweet (!) a🧵on new work ""Deep Learning on a Data Diet: Finding Important Examples Early in Training"" <LINK> We find that, at init, you can identify and prune a large % of DATA with NO effect on accuracy.  w/@mansiege @SuryaGanguli <LINK>', '2/ We observe that, on standard vision benchmarks, the initial loss gradient norm of individual training examples, averaged over several weight initializations, (the ""GRaND"" score) can be used to identify a subset of training data that suffices to train to high accuracy. But...', '3/ after only a few epochs of training, the information in our GRaND score is reflected in the normed error (L2 distance between the predicted probabilities and one hot labels) which can be used to efficiently prune a significant % of data without sacrificing test accuracy. https://t.co/K5QZOQIBmZ', '4/ Based on these findings, we propose data pruning methods that use only local information early in training, and connect them to work by Toneva et al. (2018) that tracks the # of times through training an example transitions from being correctly classified to misclassified.', '5/ Toneva et al. find that some examples are rarely forgotten, while others are forgotten repeatedly. They show one can prune rarely forgotten examples. Their ""forget"" score is usually computed after training: we observe that it works after a few epochs, but not at init.', ""6/ Our work sheds light on how the underlying data distribution shapes training dynamics: our scores rank examples based on importance for generalization, detect noisy examples, and identify subspaces of the model's data representation that are relatively stable over training."", '7/ We find that the mode SGD converges to is determined earlier in training for “prunable” examples (up to linear mode connectivity). In contrast, the optimization landscape evaluated on important-for-training examples is sensitive to SGD noise throughout training. https://t.co/0PkmV4ivhu', ""8/ There are many more questions to be answered! Feel free to reach out after you've read the paper!"", '@nsaphra @kchonyc We confirmed this finding on several standard architectures trained on vision datasets, for all of which rewinding to initialization does *not* work (one needs to rewind to some point early in training).', ""@davelewisdotir @mansiege @SuryaGanguli Thanks for the reference, David. I was not aware of this work. The approach and details seem similar to coreset approaches, which we discuss, but even that literature doesn't cite this work, interestingly. We'll make sure readers know about it."", '@botian_ @mansiege @SuryaGanguli I am curious too :) In our experiments, going from CIFAR10 to CIFAR100 the amount of data we need (in percentage) doubled.']",https://arxiv.org/abs/2107.07075,"The recent success of deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, on standard vision benchmarks, the initial loss gradient norm of individual training examples, averaged over several weight initializations, can be used to identify a smaller set of training data that is important for generalization. Furthermore, after only a few epochs of training, the information in gradient norms is reflected in the normed error--L2 distance between the predicted probabilities and one hot labels--which can be used to prune a significant fraction of the dataset without sacrificing test accuracy. Based on this, we propose data pruning methods which use only local information early in training, and connect them to recent work that prunes data by discarding examples that are rarely forgotten over the course of training. Our methods also shed light on how the underlying data distribution shapes the training dynamics: they rank examples based on their importance for generalization, detect noisy examples and identify subspaces of the model's data representation that are relatively stable over training. ","Deep Learning on a Data Diet: Finding Important Examples Early in
  Training"
198,1416179459216928772,1123400922921615360,Akshay Rao,"['Coastal communities have access to immense amounts of #energy in ocean waves and are in a prime location for seawater #desalination. \n\nWe propose a novel wave powered desalination system that achieves high #efficiency and long-term cost competitiveness. \n\n<LINK> <LINK>', 'This new process couples a batch desalination process with a fully mechanical wave-energy converter. \n\nThis reduces energy conversion losses and minimizes the need for electricity or battery storage.', ""The project also won 1st place at this year's @NREL Marine Energy competition!""]",https://arxiv.org/abs/2107.07137,"Ocean waves provide a consistent, reliable source of clean energy making them a viable energy source for desalination. Ocean wave energy is useful to coastal communities, especially island nations. However, large capital costs render current wave-powered desalination technologies economically infeasible. This work presents a high efficiency configuration for ocean wave energy powering batch reverse osmosis. The proposed system uses seawater as the working fluid in a hydro-mechanical wave energy converter and replaces the reverse osmosis high-pressure pump with a hydraulic converter for direct-drive coupling. This allows for minimal intermediary power conversions, fewer components, and higher efficiencies. The concept was analyzed with MATLAB to model the transient energy dynamics of the wave energy converter, power take-off system, and desalination load. The fully hydro-mechanical coupling, incorporating energy recovery, could achieve an SEC and LCOW as low as 2.30 kWh/m3 and $1.96, respectively, for different sea states. The results were validated at the sub-system level against existing literature on wave energy models and previous work completed on batch reverse osmosis models, as this system was the first to combine these two technologies. SEC and LCOW values were validated by comparing to known and predicted values for various types of RO systems. ",Direct-drive ocean wave-powered batch reverse osmosis
199,1416049118028390400,2550133394,Mostafa Dehghani,"['1. Benchmarks are fundamental to track progress in empirical machine learning. In our new paper, we study how benchmarking may affect the long term research direction and pace of progress in ML and put forward the notion of a ""benchmark lottery"":\n<LINK>', '2. This\xa0is a join work with\xa0@ytay017, @agritsenko, Zhe Zhao, @neilhoulsby, @841io, @metzlerd, and @OriolVinyalsML.', '3. We start by studying the ""task selection bias"" in multiple established benchmarks. We show how relative performance of algorithms is affected by the task selection process and discuss how merely ranking models based on an aggregated score can lead to suboptimal conclusions.', '4. Next we take a people perspective and share our opinion on how community bias contributes to the benchmark lottery, e.g.  via the review process by demanding SOTA on specific benchmarks or undermining advantage of new ideas from different aspects, like efficiency and fairness.', '5. The main concern with respect to the community bias is that research is becoming too incremental and biased toward the common expectations, since a completely new approach will initially have a hard time competing against established and carefully fine-tuned methods.', '6. In our paper, we also argue that benchmarks are stateful entities and that participation in a benchmark differs vastly depending upon its state.  For instance, getting top score in shared tasks at a later stage is vastly different from the time of its inception.', '7. This is because usually as time passes, the landscape of research with respect to a specific benchmark fills with tricks and complicated and specialized strategies to secure good scores.', '8. These adapted recipes are not always universal and may be applicable only to narrow tasks or setups, which might kill the potential of scoring high for an ""out of hype"" idea. We also bring some discussions on potential problems of the continual re-use of the same benchmark.', '9. We also talk about ""rigging the lottery"", the issue that some communities face, where the lack of well-established community driven sets of benchmarks or clear guidelines may inadvertently enable researchers to fit benchmarks to models.', '10. We highlight cases reinforcing this situation like high computational costs of proper evaluation on some benchmarks, or when the root cause is of behavioral nature, where researchers prefer to showcase only what their method shines at - oftentimes to avoid negative reviews.', '11. Finally, we present suggestions for improving the idea benchmarking process in ways that make it less of a lottery, like investing in making clear guidelines for making and using benchmarks (e.g. regulating hyper-parameter tuning budget) as well as the review process.', '12. Moreover, we talk about how using statistical significance testing, analysing sources of variance and going beyond single train/test splits can help.', '13. We also argue that measuring progress can sometimes be chasing a moving target since the meaning of progress might change as the research landscape evolves. As a potential solution we discuss promoting living benchmarks that also helps avoiding “creeping overfitting”.', '14. If a benchmark constantly evolves, for instance, adds new examples, adds new tasks, deprecates older data, and fixes labeling mistakes, it is less prone to “tricks” and highly robust models would find themselves consistently doing well across versions of the benchmark.', '15. In the end, there are many reasons to be excited about the future - the community is continuously taking positive delta changes that contribute to fixing issues with measuring progress in the empirical machinelearning :)']",https://arxiv.org/abs/2107.07002,"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of ""a benchmark lottery"" that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss the implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems, and reinforcement learning. ",The Benchmark Lottery
200,1415841186112172038,338353269,Pengsheng Guo,"['Check out our @Apple research paper ""Fast and Explicit Neural View Synthesis"" <LINK>\n\nWe propose a simple yet effective approach that obtains comparable or even better novel view synthesis quality to SOTA while increasing rendering speed by over 400x. <LINK>', 'In contrast to continuous radiance fields, our approach enjoys these benefits: 1) the scene representation is fast to obtain, and 2) it is efficient to render. We show a promising path towards explicit representation, and the potential to disentangle rendering from reconstruction']",https://arxiv.org/abs/2107.05775v1,"We study the problem of novel view synthesis of a scene comprised of 3D objects. We propose a simple yet effective approach that is neither continuous nor implicit, challenging recent trends on view synthesis. We demonstrate that although continuous radiance field representations have gained a lot of attention due to their expressive power, our simple approach obtains comparable or even better novel view reconstruction quality comparing with state-of-the-art baselines while increasing rendering speed by over 400x. Our model is trained in a category-agnostic manner and does not require scene-specific optimization. Therefore, it is able to generalize novel view synthesis to object categories not seen during training. In addition, we show that with our simple formulation, we can use view synthesis as a self-supervision signal for efficient learning of 3D geometry without explicit 3D supervision. ",] Fast and Explicit Neural View Synthesis
201,1415605121187188740,1321069260945444864,Frederik Träuble,"[""What's the role of pre-trained representations for RL and how important is this for OOD generalization? We attempted to find some answers on that!\n\n“Representation Learning for Out-of-Distribution Generalization in Reinforcement Learning”\n<LINK>\n\n[1/5] <LINK>"", 'Learning data representations that are useful for various downstream tasks is a cornerstone of artificial intelligence, but how useful are they in challenging RL downstream control tasks such as reaching or pushing objects? [2/5]', 'We pre-trained 200+ representations from simulated camera observations covering a wide range of properties, then trained multiple downstream policies on each of these representations and systematically evaluated their performance across many different OOD scenarios. [3/5]', 'We evaluated generalization to OOD object properties and even to the real world! One insight: A policies’ OOD performance is closely linked with a related generalization score of its representation backbone. If we got you excited, our paper awaits with many more results! [4/5] https://t.co/Y0PKCRNWUK', 'Thanks to all the amazing collaborators on that project!\n@andrea_dittadi (equal contrib.), @manuelwuethrich, Felix Widmaier, @pegehler, @OleWinther1, @FrancescoLocat8, @OlivierBachem, @bschoelkopf, Stefan Bauer \n\n@MPI_IS @DTUtweet @AmazonScience @GoogleAI\n[5/5]']",http://arxiv.org/abs/2107.05686,"Building sample-efficient agents that generalize out-of-distribution (OOD) in real-world settings remains a fundamental unsolved problem on the path towards achieving higher-level cognition. One particularly promising approach is to begin with low-dimensional, pretrained representations of our world, which should facilitate efficient downstream learning and generalization. By training 240 representations and over 10,000 reinforcement learning (RL) policies on a simulated robotic setup, we evaluate to what extent different properties of pretrained VAE-based representations affect the OOD generalization of downstream agents. We observe that many agents are surprisingly robust to realistic distribution shifts, including the challenging sim-to-real case. In addition, we find that the generalization performance of a simple downstream proxy task reliably predicts the generalization performance of our RL agents under a wide range of OOD settings. Such proxy tasks can thus be used to select pretrained representations that will lead to agents that generalize. ","The Role of Pretrained Representations for the OOD Generalization of
  Reinforcement Learning Agents"
202,1415355752361906179,933338958850801664,Hejie Cui,"['Existing scene graph generation (SGG) frameworks fail in modeling zero-shot triplets. \nIn our #ECMLPKDD2021 paper, we propose COACHER to integrate commonsense from semantic similar neighbors and paths for SGG.\nPaper: <LINK> \n#GNN #KnowledgeGrap <LINK>']",https://arxiv.org/abs/2107.05080,"Relation prediction among entities in images is an important step in scene graph generation (SGG), which further impacts various visual understanding and reasoning tasks. Existing SGG frameworks, however, require heavy training yet are incapable of modeling unseen (i.e.,zero-shot) triplets. In this work, we stress that such incapability is due to the lack of commonsense reasoning,i.e., the ability to associate similar entities and infer similar relations based on general understanding of the world. To fill this gap, we propose CommOnsense-integrAted sCenegrapHrElation pRediction (COACHER), a framework to integrate commonsense knowledge for SGG, especially for zero-shot relation prediction. Specifically, we develop novel graph mining pipelines to model the neighborhoods and paths around entities in an external commonsense knowledge graph, and integrate them on top of state-of-the-art SGG frameworks. Extensive quantitative evaluations and qualitative case studies on both original and manipulated datasets from Visual Genome demonstrate the effectiveness of our proposed approach. ","Zero-Shot Scene Graph Relation Prediction through Commonsense Knowledge
  Integration"
203,1414886163429699606,1352655415063040001,Paul Wiecki,"['INS on UTe2 finds fluctuations at incommensurate AFM wavevectors. But from an NMR perspective, this thing looks exactly like a ferromagnet above the Curie temperature. We also find a thermodynamic anomaly which behaves like a Curie temperature with field. <LINK>']",https://arxiv.org/abs/2107.02706,"The normal-state out of which unconventional superconductivity in UTe$_2$ emerges is studied in detail using a variety of thermodynamic and transport probes. Clear evidence for a broad Schottky-like anomaly with roughly R ln 2 entropy around $T^{*} \approx 12$K is observed in all measured quantities. Comparison with high magnetic field transport data allows the construction of an $H\text{-}T$ phase diagram resembling that of the ferromagnetic superconductor URhGe. The low field electronic Gr\""uneisen parameter of $T^{*}$ and that of the metamagnetic transition at $H_m \approx 35$T are comparable pointing to a common origin of both phenomena. Enhanced Wilson and Korringa ratios suggests that the existence of short range ferromagnetic fluctuations cannot be ruled out. ",Thermodynamic signatures of short-range magnetic correlations in UTe$_2$
204,1414640317245046794,789002976,Mark Dekker,"['New preprint alert!\n\n<LINK>\n\nThe pandemic has made the importance of superspreaders evident. We propose a measure to identify them: Contact Sequence Centrality, and apply it to human interaction data.\n\n#Networks\nWith @fdabl @tfblanken @BorsboomDenny @jiamin_ou']",https://arxiv.org/abs/2107.01443,"Human social behavior plays a crucial role in how pathogens like SARS-CoV-2 or fake news spread in a population. Social interactions determine the contact network among individuals, while spreading, requiring individual-to-individual transmission, takes place on top of the network. Studying the topological aspects of a contact network, therefore, not only has the potential of leading to valuable insights into how the behavior of individuals impacts spreading phenomena, but it may also open up possibilities for devising effective behavioral interventions. Because of the temporal nature of interactions - since the topology of the network, containing who is in contact with whom, when, for how long, and in which precise sequence, varies (rapidly) in time - analyzing them requires developing network methods and metrics that respect temporal variability, in contrast to those developed for static (i.e., time-invariant) networks. Here, by means of event mapping, we propose a method to quantify how quickly agents mingle by transforming temporal network data of agent contacts. We define a novel measure called 'contact sequence centrality', which quantifies the impact of an individual on the contact sequences, reflecting the individual's behavioral potential for spreading. Comparing contact sequence centrality across agents allows for ranking the impact of agents and identifying potential 'behavioral super-spreaders'. The method is applied to social interaction data collected at an art fair in Amsterdam. We relate the measure to the existing network metrics, both temporal and static, and find that (mostly at longer time scales) traditional metrics lose their resemblance to contact sequence centrality. Our work highlights the importance of accounting for the sequential nature of contacts when analyzing social interactions. ",Quantifying agent impacts on contact sequences in social interactions
205,1414566474266693633,816198577517133824,Benedikt Sabass,"['New preprint where we systematically study a physical mechanism for force-dependent growth of focal adhesions. Any comments for improving the paper? <LINK>', 'https://t.co/uQ6ELwvkW2']",https://arxiv.org/abs/2107.03714,"Mechanical loading generally weakens adhesive structures and eventually leads to their rupture. However, biological systems can adapt to loads by strengthening adhesions, which is essential for maintaining the integrity of tissue and whole organisms. Inspired by cellular focal adhesions, we suggest here a generic, molecular mechanism that allows adhesion systems to harness applied loads for self-stabilization under non-equilibrium conditions -- without any active feedback involved. The mechanism is based on conformation changes of adhesion molecules that are dynamically exchanged with a reservoir. Tangential loading drives the occupation of some stretched conformation states out of equilibrium, which, for thermodynamic reasons, leads to association of further molecules with the adhesion cluster. Self-stabilization robustly increases adhesion lifetimes in broad parameter ranges. Unlike for catch-bonds, bond dissociation rates do not decrease with force. The self-stabilization principle can be realized in many ways in complex adhesion-state networks; we show how it naturally occurs in cellular adhesions involving the adaptor proteins talin and vinculin. ","A generic self-stabilization mechanism for biomolecular adhesions under
  load"
206,1413262016723173383,2366239772,Giacomo Bartoli,['💣💣 So happy to share this achievement. We recently proposed a new architecture for Quantum Computing based on the latest Cloud technologies.\n\nFind it our here:\n\n<LINK>'],https://arxiv.org/abs/2107.02007,"Starting from the idea of Quantum Computing which is a concept that dates back to 80s, we come to the present day where we can perform calculations on real quantum computers. This sudden development of technology opens up new scenarios that quickly lead to the desire and the real possibility of integrating this technology into current software architectures. The usage of frameworks that allow computation to be performed directly on quantum hardware poses a series of challenges. This document describes a an architectural framework that addresses the problems of integrating an API exposed Quantum provider in an existing Enterprise architecture and it provides a minimum viable product (MVP) solution that really merges classical quantum computers on a basic scenario with reusable code on GitHub repository. The solution leverages a web-based frontend where user can build and select applications/use cases and simply execute it without any further complication. Every triggered run leverages on multiple backend options, that include a scheduler managing the queuing mechanism to correctly schedule jobs and final results retrieval. The proposed solution uses the up-to-date cloud native technologies (e.g. Cloud Functions, Containers, Microservices) and serves as a general framework to develop multiple applications on the same infrastructure. ",A Serverless Cloud Integration For Quantum Computing
207,1412703697616969728,712960453,Prashant Saxena,"[""New #preprint from @sumitmehta1992's PhD work on instabilities in a pressurised constrained compressible soft cylinder.\n<LINK>\n\nWe study pattern formation along the axis and circumference and propose a way to switch between either of them by tuning the anisotropy <LINK>"", '@sumitmehta1992 It is an interesting research topic for me. Have worked a great deal on incompressible materials, but there are some finer points that need to be addressed while working on compressible solids.', ""I'm also starting to love the compound matrix method that we used in this paper to solve ODEs. It's much faster and can deal with rapidly varying solutions quite nicely.""]",https://arxiv.org/abs/2107.01375,"Pressurised cylindrical channels made of soft materials are ubiquitous in biological systems, soft robotics, and metamaterial designs. In this paper, we study large deformation of a long, thick-walled, and compressible hyperelastic cylindrical channel under internal pressure. The applied pressure can lead to elastic bifurcations along the axial or circumferential direction. Incremental theory is used to derive the partial differential equations that govern the bifurcation behaviour of the cylindrical channel. Two cases of boundary conditions on the outer surface of the cylinder, namely, free and constrained are studied to understand their influence on the buckling behaviour. The derived equations are solved numerically using the compound matrix method to evaluate the critical pressure. The effects of the thickness of the cylinder and the compressibility of the material on the critical pressure are investigated for both the boundary conditions. The results reveal that for an isotropic material, the bifurcation occurs along the axial direction of the cylinder at lower critical pressure compared to the circumferential direction for all cases considered. Finally, we demonstrate the tailorability of bifurcation behaviour of the cylinder by adding reinforcements along the length of cylinder. The anisotropic hyperelastic material behaviour for triggering the bifurcation in the circumferential direction is studied by varying the material parameters. ","Instabilities in a compressible hyperelastic cylindrical channel due to
  internal pressure and external constraints"
208,1412662575435636736,217771939,Michael Szell,"['🚨New Preprint! Growing Urban Bicycle Networks\n<LINK>\nExplore at <LINK>\n\nWe study the limitations of growing 🚲🕸️ \nMain finding: Cities must invest 1) with right growth strategy and 2) *persistently*, to overcome a critical mass. 🧵 <LINK>', 'Most cities on the planet have no infrastructure for safe cycling. Therefore we grow bike networks from scratch, using only 1) the street network, 2) arbitrary points of interest. Minimal data needed so this works everywhere. https://t.co/EhhokHlunS', 'We find a ""phase transition"" at a critical threshold: During growth, some quality metrics *decrease* until a critical point is reached. This point depends on the growth strategy. https://t.co/xNPCI1221E', 'Unfortunately, cities follow the *worst* growth strategy: random. It wastes investments with bad connectedness. Also invites objections: ""We already built many bike tracks but nobody is using them, so why build more?"" \nIt\'s not a network\'s length that matters but how you grow it. https://t.co/TDrRRMMj5G', 'Comparing our synthetic growth with the well developed Copenhagen network reveals 80% overlap - we recreate reality! This could find ""missing links"" (in future work..) However, as of now, this is NOT concrete recommendation for new infrastructure - only after refinements. https://t.co/0wZisFHKmA', 'How does this bike network growth affect the car network? Mostly by decreasing its directness. \nIs this good or bad? -How we decide such questions will determine our quality of life in cities, and generally the fate of the planet...\n\n(img by @TUMInitiative) https://t.co/k12nCn3wBk', 'Research with S. Mimar, T. Perlman, @Ghoshal_G, @robysinatra. \nPaper: https://t.co/GYXk4CrXyZ\nCode: https://t.co/x1TLn1lgud\n\nhttps://t.co/xOIplgCcUO developed by my awesome MSc students Cecilia L. Kolding Andersen and Morten Lynghede. Explore 62 cities &amp; Download 1000+ videos! https://t.co/6EbOZvRuMu', 'happy@feedback! @StreetsblogUSA @gboeing @CyclingEmbassy @giulio_mattioli @EuCyclistsFed @TheWarOnCars @robinlovelace @martikagv @andershartmann @cyklistforbund @BaldwinMatthew_ @npalomin \n@CyclingScience1 @Sust_Mobility @VCOE_AT @NACTO @openstreetmap @LitmanVTPI @altafieldnotes']",https://arxiv.org/abs/2107.02185,"Cycling is a promising solution to unsustainable urban transport systems. However, prevailing bicycle network development follows a slow and piecewise process, without taking into account the structural complexity of transportation networks. Here we explore systematically the topological limitations of urban bicycle network development. For 62 cities we study different variations of growing a synthetic bicycle network between an arbitrary set of points routed on the urban street network. We find initially decreasing returns on investment until a critical threshold, posing fundamental consequences to sustainable urban planning: Cities must invest into bicycle networks with the right growth strategy, and persistently, to surpass a critical mass. We also find pronounced overlaps of synthetically grown networks in cities with well-developed existing bicycle networks, showing that our model reflects reality. Growing networks from scratch makes our approach a generally applicable starting point for sustainable urban bicycle network planning with minimal data requirements. ",Growing Urban Bicycle Networks
209,1412400769094098945,272074813,Thomas O'Connor,"['We have a new preprint up on Arxiv at <LINK> - It explores the complex entanglement networks formed by blends long ring and linear polymers. We study the statistics of these ""composite networks"" in equilibrium and how elongation flows drive them to ""unthread"". <LINK>', '@mjahore Thanks! The configurations are made from the melts by doing a primitive path analysis. I did the rendering in Ovito with the Osprey renderer. I played a lot with the ""overhead"" light to try and highlight some of the network geometry. Red are linear chains and green are rings.']",http://arxiv.org/abs/2107.01491,"Extensive molecular simulations are applied to characterize the equilibrium dynamics, entanglement topology, and nonlinear extensional rheology of symmetric ring-linear polymer blends with systematically varied ring fraction $\phi_R$. Chains with degree of entanglement $Z\approx14$ mixed to produce 10 well-entangled systems with $\phi_R$ varying from neat linear to neat ring melts. Primitive path analysis are used to visualize and quantify the structure of the composite ring-linear entanglement network. We directly measure the quantity of ring-linear threading and linear-linear entanglement as a function of $\phi_R$, and identify with simple arguments a ring fraction $\phi_R\approx0.4$ where the topological constraints of the entanglement network are maximized. These topological analyses are used to rationalize the $\phi_R$-dependence of ring and linear chain dynamics, conformations, and blend viscosity. Complimentary simulations of startup uniaxial elongation flows demonstrate the extensional stress overshoot observed in recent filament stretching experiments, and characterize how it depends on the blend composition and entanglement topology. The overshoot is driven by an overstretching and recoil of ring polymer conformations that is caused by the convective unthreading of rings from linear chains. This produces significant changes in the entanglement structure of blends that we directly visualize and quantify with primitive path analyses during flow. ","Composite Entanglement Topology and Extensional Rheology of Symmetric
  Ring-Linear Polymer Blends"
210,1412329771757932544,963873866392186882,Philipp Schindler,['Our preprint on using variational cricuits in an ion trap system for optimal metrology is online: <LINK> We show how we can use low-depth quantum circuits to find the optimal measurement in the presence of experimental imperfections. @uniinnsbruck @AQTION_eu <LINK>'],https://arxiv.org/abs/2107.01860,"Quantum sensors are an established technology that has created new opportunities for precision sensing across the breadth of science. Using entanglement for quantum-enhancement will allow us to construct the next generation of sensors that can approach the fundamental limits of precision allowed by quantum physics. However, determining how state-of-the-art sensing platforms may be used to converge to these ultimate limits is an outstanding challenge. In this work we merge concepts from the field of quantum information processing with metrology, and successfully implement experimentally a *programmable quantum sensor* operating close to the fundamental limits imposed by the laws of quantum mechanics. We achieve this by using low-depth, parametrized quantum circuits implementing optimal input states and measurement operators for a sensing task on a trapped ion experiment. With 26 ions, we approach the fundamental sensing limit up to a factor of 1.45(1), outperforming conventional spin-squeezing with a factor of 1.87(3). Our approach reduces the number of averages to reach a given Allan deviation by a factor of 1.59(6) compared to traditional methods not employing entanglement-enabled protocols. We further perform on-device quantum-classical feedback optimization to `self-calibrate' the programmable quantum sensor with comparable performance. This ability illustrates that this next generation of quantum sensor can be employed without prior knowledge of the device or its noise environment. ",Optimal metrology with programmable quantum sensors
211,1412322888850497536,791705191175360512,Niels Warburton,"['New paper day! By pushing self-force calculations to second-order in the mass ratio we find it can accurately model the GW flux from black hole binaries with mass ratios as large as 10:1 .  <LINK>. 1/n <LINK>', 'In the above example we compare our self-force (SF) result to NR fluxes from an @SXSProject simulation with q=10. Up to 5 cycles from the merger the relative error in the second-order SF result (2SF) is less than 2e-3.   2/n', 'Closer to the ISCO our two-timescale approximation (https://t.co/7IubudQ7ho) breaks down which gives the downward tick you see in the 2SF flux. In the future attaching a transition to plunge will improve the result in this region. 3/n', 'We can also really abuse perturbation theory and compare fluxes at q=1. Surprisingly our 2SF result does really well even for equal mass binaries! Up to 5 cycles before merger the relative error in the (2,2)-mode flux (compared with NR) is less than 2.5e-3. https://t.co/OkWhC5fcIL', 'We can also look at the subdominant modes. Here the comparison is not quite as good. From examining the PN series we see this is expected as in subdominant modes the nu^4 term (what would be 3SF) enters earlier in the PN series than for the (2,2)-mode.  5/n https://t.co/1USLQljHXr', 'Nonetheless, the agreement between SF and NR at q=10 for the (3,3)-mode is ~1% (relative) 5 cycles before the waveform peak. With a simple resummation that adds information from just the leading-order PN term for a given mode the relative error drops to 4.5e-3. 6/n', 'As the (2,2)-flux dominants the total flux the agreement between SF and NR for the *total* flux remains excellent at 2.5e-3 (relative) up to 5 cycles before the peak. 7/n', 'As a further check we can compare our results to NR as a function of the symmetric mass ratio, nu. After subtracting the SF and NR fluxes we expect the residual to go as nu^4 and indeed this what we observe.  8/n https://t.co/4qQinW2aD5', 'The comparison with the (2,2)-mode is less clean than the (3,3)-mode. This is likely because after subtracting the 2SF fluxes the residual encroaches on the scale of the oscillations in the NR data due to residual eccentricity and the motion of the centre of mass. 9/n', 'In this work we perturb around a non-spinning (Schwarzschild) black hole which mean we can also model binaries where the primary is slowly spinning. We can also add the contribution from spin on to the secondary using results from, e.g., https://t.co/osRAKI9pjt. 10/n', 'When both components are slowly spinning we again find good agreement with @SXSProject simulations with q=8. Here the spin magnitude of both black holes is around 0.1. 11/n https://t.co/1BOH5lr3vD', '@SXSProject For a spinning secondary (such as a black hole) there is no restriction on the spin we can model. Here we show a comparison with an @SXSProject simulation with an almost non-spinning primary and a secondary with spin =~-0.8. Again the agreement is good, even at q=~6.3 https://t.co/PfrqzdXa0F', '@SXSProject We also agreement in the weak field with all the known 3.5PN term. Here we show the residuals after subtracting successive PN terms. The final residual (purple, upside down triangles) should follow the currently unknown 4PN contribution to the (2,2)-mode flux. 13/n https://t.co/omRxwDg6cS', ""This work was completed with Adam Pound, Barry Wardell, Jeremy Miller and Leanne Durkan. What's next? Yesterday at the Marcel Grossman meeting @barry_wardell presented our first results for the waveform. At q=10 the comparison with NR looks amazing. Watch this space!""]",https://arxiv.org/abs/2107.01298,"Within the framework of self-force theory, we compute the gravitational-wave energy flux through second order in the mass ratio for compact binaries in quasicircular orbits. Our results are consistent with post-Newtonian calculations in the weak field and they agree remarkably well with numerical-relativity simulations of comparable-mass binaries in the strong field. We also find good agreement for binaries with a spinning secondary or a slowly spinning primary. Our results are key for accurately modelling extreme-mass-ratio inspirals and will be useful in modelling intermediate-mass-ratio systems. ","Gravitational-wave energy flux for compact binaries through second order
  in the mass ratio"
212,1412296511027879937,1318562163875786756,Ory Schnitzer,['New arXiv preprint with Matias Ruiz @EdinUniMaths:\n\n<LINK>\n\nWe develop a “slender-body theory” to study how geometry can be used to tune the plasmon-resonance properties of nanometallic structures formed of one of more slender rings. \n\n#plasmonics'],https://arxiv.org/abs/2107.01716,"We develop an approximate quasi-static theory describing the low-frequency plasmonic resonances of slender nanometallic rings and configurations thereof. First, we use asymptotic arguments to reduce the plasmonic eigenvalue problem governing the geometric (material- and frequency-independent) modes of a given ring structure to a 1D-periodic integro-differential problem in which the eigenfunctions are represented by azimuthal voltage and polarization-charge profiles associated with each ring. Second, we obtain closed-form solutions to the reduced eigenvalue problem for azimuthally invariant rings (including torus-shaped rings but also allowing for non-circular cross-sectional shapes), as well as coaxial dimers and chains of such rings. For more general geometries, involving azimuthally non-uniform rings and non-coaxial structures, we solve the reduced eigenvalue problem using a semi-analytical scheme based on Fourier expansions of the reduced eigenfunctions. Third, we used the asymptotically approximated modes, in conjunction with the quasi-static spectral theory of plasmonic resonance, to study and interpret the frequency response of a wide range of nanometallic slender-ring structures under plane-wave illumination. ",Plasmonic resonances of slender nanometallic rings
213,1411958029419954177,2449351549,Suryanarayana Maddu,['Our preprint for training neural networks to solve multi-scale PDEs is out. <LINK>\n\nWe propose novel strategies for multi-objective optimization that fixes learning pathologies in physics-informed neural networks @MOSAICgroup1 @microbionaut @CASUSscience <LINK>'],http://arxiv.org/abs/2107.00940,"We characterize and remedy a failure mode that may arise from multi-scale dynamics with scale imbalances during training of deep neural networks, such as Physics Informed Neural Networks (PINNs). PINNs are popular machine-learning templates that allow for seamless integration of physical equation models with data. Their training amounts to solving an optimization problem over a weighted sum of data-fidelity and equation-fidelity objectives. Conflicts between objectives can arise from scale imbalances, heteroscedasticity in the data, stiffness of the physical equation, or from catastrophic interference during sequential training. We explain the training pathology arising from this and propose a simple yet effective inverse-Dirichlet weighting strategy to alleviate the issue. We compare with Sobolev training of neural networks, providing the baseline of analytically $\boldsymbol{\epsilon}$-optimal training. We demonstrate the effectiveness of inverse-Dirichlet weighting in various applications, including a multi-scale model of active turbulence, where we show orders of magnitude improvement in accuracy and convergence over conventional PINN training. For inverse modeling using sequential training, we find that inverse-Dirichlet weighting protects a PINN against catastrophic forgetting. ","Inverse-Dirichlet Weighting Enables Reliable Training of Physics
  Informed Neural Networks"
214,1411928301455695874,802543221943439360,Andrea Caputo,['Paper day! <LINK>\nWe continue exploring the linear dynamics of an electromagnetic field propagating in curved spacetime with plasma. In particular we study the Kerr metric and show that plasma-driven modes become superradiantly unstable at the linear level. <LINK>'],https://arxiv.org/abs/2107.01174,"Motivated by electromagnetic-field confinement due to plasma near accreting black holes, we continue our exploration of the linear dynamics of an electromagnetic field propagating in curved spacetime in the presence of plasma by including three effects that were neglected in our previous analysis: collisions in the plasma, thermal corrections, and the angular momentum of the background black-hole spacetime. We show that: (i) the plasma-driven long-lived modes survive in a collisional plasma except when the collision timescale is unrealistically small; (ii) thermal effects, which might be relevant for accretion disks around black holes, do not affect the axial long-lived modes; (iii) in the case of a spinning black hole the plasma-driven modes become superradiantly unstable at the linear level; (iv) the polar sector in the small-frequency regime admits a reflection point due to the resonant properties of the plasma. Dissipative effects such as absorption, formation of plasma waves, and nonlinear dynamics play a crucial role in the vicinity of this resonant point. ","Plasma-photon interaction in curved spacetime II: collisions, thermal
  corrections, and superradiant instabilities"
215,1410956135507582976,1200141005233836032,Pedro Fernandes,"['<LINK>\nCheck out my new paper with @dmulryne, @PedroCarrilho11 and Timothy Clifton. We discuss black holes on a Galileon theory of gravity resultant from a dimensional regularization of the Gauss-Bonnet term. We find (under reasonable assumptions) that\n\n1/2', ""the theory has a unique black hole solution, a result reminiscent of Birkhoff's theorem. We also find that evaporation leads to stable relics with a size directly related to the coupling constant of the theory. We speculate on the role of these relics as dark matter candidates.""]",https://arxiv.org/abs/2107.00046,"In this work we study static black holes in the regularized 4D Einstein-Gauss-Bonnet theory of gravity; a shift-symmetric scalar-tensor theory that belongs to the Horndeski class. This theory features a simple black hole solution that can be written in closed form, and which we show is the unique static, spherically-symmetric and asymptotically-flat black hole vacuum solution of the theory. We further show that no asymptotically-flat, time-dependent, spherically-symmetric perturbations to this geometry are allowed, which suggests that it may be the only spherically-symmetric vacuum solution that this theory admits (a result analogous to Birkhoff's theorem). Finally, we consider the thermodynamic properties of these black holes, and find that their final state after evaporation is a remnant with a size determined by the coupling constant of the theory. We speculate that remnants of this kind from primordial black holes could act as dark matter, and we constrain the parameter space for their formation mass, as well as the coupling constant of the theory. ","Black Holes in the Scalar-Tensor Formulation of 4D Einstein-Gauss-Bonnet
  Gravity: Uniqueness of Solutions, and a New Candidate for Dark Matter"
216,1421024263834791938,2838141805,Jason Xue,"[""We recently study the variant attack of membership inference, which we called it Membership Collisions. It examines whether the synthetic data has a collision of actual training data. The paper has been accepted to ACM CCS'21 @acm_ccs (see preprint at <LINK>).""]",https://arxiv.org/abs/2107.13190,"Generative Adversarial Networks (GAN)-synthesized table publishing lets people privately learn insights without access to the private table. However, existing studies on Membership Inference (MI) Attacks show promising results on disclosing membership of training datasets of GAN-synthesized tables. Different from those works focusing on discovering membership of a given data point, in this paper, we propose a novel Membership Collision Attack against GANs (TableGAN-MCA), which allows an adversary given only synthetic entries randomly sampled from a black-box generator to recover partial GAN training data. Namely, a GAN-synthesized table immune to state-of-the-art MI attacks is vulnerable to the TableGAN-MCA. The success of TableGAN-MCA is boosted by an observation that GAN-synthesized tables potentially collide with the training data of the generator. Our experimental evaluations on TableGAN-MCA have five main findings. First, TableGAN-MCA has a satisfying training data recovery rate on three commonly used real-world datasets against four generative models. Second, factors, including the size of GAN training data, GAN training epochs and the number of synthetic samples available to the adversary, are positively correlated to the success of TableGAN-MCA. Third, highly frequent data points have high risks of being recovered by TableGAN-MCA. Fourth, some unique data are exposed to unexpected high recovery risks in TableGAN-MCA, which may attribute to GAN's generalization. Fifth, as expected, differential privacy, without the consideration of the correlations between features, does not show commendable mitigation effect against the TableGAN-MCA. Finally, we propose two mitigation methods and show promising privacy and utility trade-offs when protecting against TableGAN-MCA. ","TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized
  Tabular Data Releasing"
217,1417869427538898946,3466980795,Wieland Brendel,"['Can #NeuralNetworks generalize to factors of variation present in their training data?\nWe investigated this with a large-scale study and showed limited generalization capabilities. \nBenchmark: <LINK>  \nPaper: <LINK> \n\n#MachineLearning #AI \n[1/6] <LINK>', 'For several datasets (dSprites, Shapes3D and MPI3D), we introduce various systematic splits such as interpolation, extrapolation or composition. For instance, for extrapolation, we only have small objects during training but large objects during test time. [2/6] https://t.co/4X8w1OMrOf', 'We trained 2000+ models from 17 different representation learning approaches and found that they mostly struggle to generalize to our benchmark regardless of the supervision signal and architecture. This is most apparent on the challenging dataset MPI3D. [3/6] https://t.co/reGhd5ZFBs', 'We further find a prototypical behavior across models on out-of-distribution factors: As soon as a factor of variation is outside the training distribution, models consistently tend to predict a value in the previously observed range. [4/6]', 'Despite the limited out-of-distribution performance, these models can be fairly modular: \nEven though some factors are out-of-distribution, the prediction of in-distribution factors remains accurate. [5/6]', 'To foster progress on the generalization along factors, we release our evaluation code and datasets splits.\nCode: https://t.co/gzGy2vKMMd  \n \nThanks to @schott_lukas, @JKugelgen, @f_traeuble, @pegehler, @c_russl, @MatthiasBethge, @bschoelkopf, @FrancescoLocat8. \n[6/6]']",https://arxiv.org/abs/2107.08221,"An important component for generalization in machine learning is to uncover underlying latent factors of variation as well as the mechanism through which each factor acts in the world. In this paper, we test whether 17 unsupervised, weakly supervised, and fully supervised representation learning approaches correctly infer the generative factors of variation in simple datasets (dSprites, Shapes3D, MPI3D) from controlled environments, and on our contributed CelebGlow dataset. In contrast to prior robustness work that introduces novel factors of variation during test time, such as blur or other (un)structured noise, we here recompose, interpolate, or extrapolate only existing factors of variation from the training data set (e.g., small and medium-sized objects during training and large objects during testing). Models that learn the correct mechanism should be able to generalize to this benchmark. In total, we train and test 2000+ models and observe that all of them struggle to learn the underlying mechanism regardless of supervision signal and architectural bias. Moreover, the generalization capabilities of all tested models drop significantly as we move from artificial datasets towards more realistic real-world datasets. Despite their inability to identify the correct mechanism, the models are quite modular as their ability to infer other in-distribution factors remains fairly stable, providing only a single factor is out-of-distribution. These results point to an important yet understudied problem of learning mechanistic models of observations that can facilitate generalization. ","Visual Representation Learning Does Not Generalize Strongly Within the
  Same Domain"
218,1417851297764626432,356676252,John Regan,"['On the @arxiv  today (<LINK>) @Fabio_Pacucci , @marmezcua_astro and I looked at the active fraction of MBHs in dwarf galaxies. We find, using a physical model of MBH accretion, that the active fraction is between 20% and 30% for the most massive dwarfs,decreasing <LINK>', 'as the stellar mass decreases. Note that these are active fractions - the occupation fraction will be (much) higher! We also give a prediction for the probability of detecting a MBH in a given dwarf galaxy based on the stellar mass and angular momentum of gas within the galaxy!', '@royalsociety @sfi @MUTheorPhys   @MaynoothUni']",https://arxiv.org/abs/2107.09069,"The population of massive black holes (MBHs) in dwarf galaxies is elusive, but fundamentally important to understand the coevolution of black holes with their hosts and the formation of the first collapsed objects in the Universe. While some progress was made in determining the X-ray detected fraction of MBHs in dwarfs, with typical values ranging from $0\%$ to $6\%$, their overall active fraction, ${\cal A}$, is still largely unconstrained. Here, we develop a theoretical model to predict the multiwavelength active fraction of MBHs in dwarf galaxies starting from first principles and based on the physical properties of the host, namely, its stellar mass and angular momentum content. We find multiwavelength active fractions for MBHs, accreting at typically low rates, ranging from $5\%$ to $22\%$, and increasing with the stellar mass of the host as ${\cal A} \sim(\log_{10}M_{\star})^{4.5}$. If dwarfs are characterized by low-metallicity environments, the active fraction may reach $\sim 30\%$ for the most massive hosts. For galaxies with stellar mass in the range $10^7<M_{\star} [M_{\odot}]<10^{10}$, our predictions are in agreement with occupation fractions derived from simulations and semi-analytical models. Additionally, we provide a fitting formula to predict the probability of finding an active MBH in a dwarf galaxy from observationally derived data. This model will be instrumental to guide future observational efforts to find MBHs in dwarfs. The James Webb Space Telescope, in particular, will play a crucial role in detecting MBHs in dwarfs, possibly uncovering active fractions $\sim 3$ times larger than current X-ray surveys. ",The Active Fraction of Massive Black Holes in Dwarf Galaxies
219,1417541136386768896,1001049754787368960,Dr. Yu-Dai Tsai,"['Another interesting but not-so-fun fact about our recent project is that one of the asteroids we study is potentially hazardous to the Earth (see 1999 KW4/Moshup, <LINK>)! \n1/4', 'We need to study new physics properly, to determine if the tiny perturbation it creates would affect the asteroid trajectories. Other perturbations from gravitational objects or solar thermal effects are much larger, but there is already significant effort to study them. 2/4', 'Our paper provides an estimation of the new physics effect of a specific model, but we are planning to collaborate with asteroid experts to include new physics contributions to the detailed simulations. 3/4', 'Although very very unlikely, our project may have some real-life consequences 🙂\nThanks again to my awesome collaborators, Youjia Wu, @Sunny, and @Luca, for this interesting work. 4/4']",https://arxiv.org/abs/2107.04038,"We study for the first time the possibility of probing long-range fifth forces utilizing asteroid astrometric data, via the fifth force-induced orbital precession. We examine nine Near-Earth Object (NEO) asteroids whose orbital trajectories are accurately determined via optical and radar astrometry. Focusing on a Yukawa-type potential mediated by a new gauge field (dark photon) or a baryon-coupled scalar, we estimate the sensitivity reach for the fifth-force coupling strength and mediator mass in the mass range $m \simeq 10^{-21}-10^{-15}\,{\rm eV}$. Our estimated sensitivity is comparable to leading limits from torsion balance experiments, potentially exceeding these in a specific mass range. The fifth forced-induced precession increases with the orbital semi-major axis in the small $m$ limit, motivating the study of objects further away from the Sun. We discuss future exciting prospects for extending our study to more than a million asteroids (including NEOs, main-belt asteroids, Hildas, and Jupiter Trojans), as well as trans-Neptunian objects and exoplanets. ",Asteroid astrometry as a fifth-force and ultralight dark sector probe
220,1417089077015465990,77712285,Andrea Baronchelli,"['New: ""The COVID-19 infodemic does not affect vaccine acceptance"". <LINK>\n\nSix European countries, 2021. We found that social media responded vigorously to vaccine news (e.g., EMA suspending AZ). Yet overall vaccine acceptance was largely flat.\n\nSee also below. <LINK>']",https://arxiv.org/abs/2107.07946,"How information consumption affects behaviour is an open and widely debated research question. A popular hypothesis states that the so-called infodemic has a substantial impact on orienting individual decisions. A competing hypothesis stresses that exposure to vast amounts of even contradictory information has little effect on personal choices. The COVID-19 pandemic offered an opportunity to investigate this relationship, analysing the interplay between COVID-19 related information circulation and the propensity of users to get vaccinated. We analyse the vaccine infodemics on Twitter and Facebook by looking at 146M contents produced by 20M accounts between 1 January 2020 and 30 April 2021. We find that vaccine-related news triggered huge interest through social media, affecting attention patterns and the modality in which information was spreading. However, we observe that such a tumultuous information landscape translated only in minimal variations in overall vaccine acceptance as measured by Facebook's daily COVID-19 Trends and Impact Survey (previously known as COVID-19 World Symptoms Survey) on a sample of 1.6M users. Notably, the observation period includes the European Medicines Agency (EMA) investigations over blood clots cases potentially related to vaccinations, a series of events that could have eroded trust in vaccination campaigns. We conclude the paper by investigating the numerical correlation between various infodemics indices and vaccine acceptance, observing strong compatibility with a null model. This finding supports the hypothesis that altered information consumption patterns are not a reliable predictor of collective behavioural change. Instead, wider attention on social media seems to resolve in polarisation, with the vaccine-prone and the vaccine-hesitant maintaining their positions. ","Lack of evidence for correlation between COVID-19 infodemic and vaccine
  acceptance"
221,1417013842417274881,468764094,W. Quattrociocchi,['Our last work about the interplay between content consumation and behavior. We find overabundance of information about Vaccines and a flat Vaccine hesitancy trend. @a_baronca @zollofab @matteo_cinelli @valensic_ @DeveloperGale @gbrtte_ @nadin @matt55nado\n\n<LINK> <LINK>'],https://arxiv.org/abs/2107.07946,"How information consumption affects behaviour is an open and widely debated research question. A popular hypothesis states that the so-called infodemic has a substantial impact on orienting individual decisions. A competing hypothesis stresses that exposure to vast amounts of even contradictory information has little effect on personal choices. The COVID-19 pandemic offered an opportunity to investigate this relationship, analysing the interplay between COVID-19 related information circulation and the propensity of users to get vaccinated. We analyse the vaccine infodemics on Twitter and Facebook by looking at 146M contents produced by 20M accounts between 1 January 2020 and 30 April 2021. We find that vaccine-related news triggered huge interest through social media, affecting attention patterns and the modality in which information was spreading. However, we observe that such a tumultuous information landscape translated only in minimal variations in overall vaccine acceptance as measured by Facebook's daily COVID-19 Trends and Impact Survey (previously known as COVID-19 World Symptoms Survey) on a sample of 1.6M users. Notably, the observation period includes the European Medicines Agency (EMA) investigations over blood clots cases potentially related to vaccinations, a series of events that could have eroded trust in vaccination campaigns. We conclude the paper by investigating the numerical correlation between various infodemics indices and vaccine acceptance, observing strong compatibility with a null model. This finding supports the hypothesis that altered information consumption patterns are not a reliable predictor of collective behavioural change. Instead, wider attention on social media seems to resolve in polarisation, with the vaccine-prone and the vaccine-hesitant maintaining their positions. ","Lack of evidence for correlation between COVID-19 infodemic and vaccine
  acceptance"
222,1416066786047500289,1207131443643797504,Ethan Manilow,['Stoked to share our paper led by @hugggof &amp; (undergrad!!!) Aldo Aguilar has been accepted to @ismir2021!! 🎉\n\nWe propose a simple yet effective hierarchical extension to prototypical nets for few-shot instrument recognition: <LINK> <LINK>'],https://arxiv.org/abs/2107.07029,"Deep learning work on musical instrument recognition has generally focused on instrument classes for which we have abundant data. In this work, we exploit hierarchical relationships between instruments in a few-shot learning setup to enable classification of a wider set of musical instruments, given a few examples at inference. We apply a hierarchical loss function to the training of prototypical networks, combined with a method to aggregate prototypes hierarchically, mirroring the structure of a predefined musical instrument hierarchy. These extensions require no changes to the network architecture and new levels can be easily added or removed. Compared to a non-hierarchical few-shot baseline, our method leads to a significant increase in classification accuracy and significant decrease mistake severity on instrument classes unseen in training. ","Leveraging Hierarchical Structures for Few-Shot Musical Instrument
  Recognition"
223,1415711039862288386,2940404451,Peter Xenopoulos,"['For most sports, but especially esports, we analyze the game by watching film, which makes finding specific player setups time-consuming. In ggViz, we introduce a system to query a large CSGO dataset to find player setups. Link to paper: <LINK> <LINK>', 'First, we start by parsing thousands of CSGO demofiles into precise player locations using the csgo parser in Python (https://t.co/TyPAVPtnDK). Player locations can also be described by their discrete location on the map (either by color or yellow box). https://t.co/yvzQx5Y0iE', 'We want users to be able to sketch their scenario of interest, and receive a list of relevant situations. We can encode a game frame through a token, generated by how many players from a given side exist in a discrete location. https://t.co/bVL36Kiybf', 'These tokens are quick to generate and can be stored easily in a database. To retrieve similar plays, we can search for plays that have the same token. Retrieval on 100GB of game data returns results in just a few seconds. https://t.co/P5GlzbKJK6', 'We developed the following interface for users to sketch scenarios of interest (A), select particular game frames (B), and play the demos in 2D, marking important events like kills and grenades (C). https://t.co/BHKfYLNv5W', 'We tested the system with analysts, coaches, and managers from four top CSGO teams. Each found the system useful not only for demo review but also tactic discovery. We show a case study of analyzing retakes, such as a 3-on-2 retake for Inferno B bombsite. https://t.co/1p1OB0SASt', 'Stay tuned for more CSGO analytics content, including two accepted papers at IJCAI’s Workshop on Sports Analytics, and more preprints. If you’re interested in getting your own CSGO data, take a look at the csgo parser at https://t.co/TyPAVPtnDK', '@Zach_Kamran Thank you! A lot to expand on, including ranking the states in the result set, using the tokens as features in prediction models, and so on']",https://arxiv.org/abs/2107.06495,"Game review is crucial for teams, players and media staff in sports. Despite its importance, game review is work-intensive and hard to scale. Recent advances in sports data collection have introduced systems that couple video with clustering techniques to allow for users to query sports situations of interest through sketching. However, due to data limitations, as well as differences in the sport itself, esports has seen a dearth of such systems. In this paper, we leverage emerging data for Counter-Strike: Global Offensive (CSGO) to develop ggViz, a novel visual analytics system that allows users to query a large esports data set for similar plays by drawing situations of interest. Along with ggViz, we also present a performant retrieval algorithm that can easily scale to hundreds of millions of game situations. We demonstrate ggViz's utility through detailed cases studies and interviews with staff from professional esports teams. ",ggViz: Accelerating Large-Scale Esports Game Analysis
224,1415593113582768129,903175505377198080,alessioferrari,"['After three years and three iterations of search and code, we have finalised our mapping study on #formal methods in #railways with M. ter Beek. Time for holiday now.\nHere is the preprint: <LINK>\n\nPart of @S2R_ASTRail and @4securail projects by @Shift2Rail_JU <LINK>']",https://arxiv.org/abs/2107.05413,"Formal methods are mathematically-based techniques for the rigorous development of software-intensive systems. The railway signaling domain is a field in which formal methods have traditionally been applied, with several success stories. This article reports on a mapping study that surveys the landscape of research on applications of formal methods to the development of railway systems. Our main results are as follows: (i) we identify a total of 328 primary studies relevant to our scope published between 1989 and 2020, of which 44% published during the last 5 years and 24% involving industry; (ii) the majority of studies are evaluated through Examples (41%) and Experience Reports (38%), while full-fledged Case Studies are limited (1.5%); (iii) Model checking is the most commonly adopted technique (47%), followed by simulation (27%) and theorem proving (19.5%); (iv) the dominant languages are UML (18%) and B (15%), while frequently used tools are ProB (9%), NuSMV (8%) and UPPAAL (7%); however, a diverse landscape of languages and tools is employed; (v) the majority of systems are interlocking products (40%), followed by models of high-level control logic (27%); (vi) most of the studies focus on the Architecture (66%) and Detailed Design (45%) development phases. Based on these findings, we highlight current research gaps and expected actions. In particular, the need to focus on more empirically sound research methods, such as Case Studies and Controlled Experiments, and to lower the degree of abstraction, by applying formal methods and tools to development phases that are closer to software development. Our study contributes with an empirically based perspective on the future of research and practice in formal methods applications for railways. ",Formal Methods in Railways: a Systematic Mapping Study
225,1413163692485713923,1374512842536194056,Pasquale Cascarano,['We are very proud to share our DeepCEL0 algorithm for 2D single molecule localization in fluorescence microscopy @ColombaComes @sedaboni1 \n\nHere you find the link to our preprint\n<LINK> <LINK>'],https://arxiv.org/abs/2107.02281,"In fluorescence microscopy, Single Molecule Localization Microscopy (SMLM) techniques aim at localizing with high precision high density fluorescent molecules by stochastically activating and imaging small subsets of blinking emitters. Super Resolution (SR) plays an important role in this field since it allows to go beyond the intrinsic light diffraction limit. In this work, we propose a deep learning-based algorithm for precise molecule localization of high density frames acquired by SMLM techniques whose $\ell_{2}$-based loss function is regularized by positivity and $\ell_{0}$-based constraints. The $\ell_{0}$ is relaxed through its Continuous Exact $\ell_{0}$ (CEL0) counterpart. The arising approach, named DeepCEL0, is parameter-free, more flexible, faster and provides more precise molecule localization maps if compared to the other state-of-the-art methods. We validate our approach on both simulated and real fluorescence microscopy data. ",DeepCEL0 for 2D Single Molecule Localization in Fluorescence Microscopy
