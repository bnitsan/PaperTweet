,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1468618801134592012,837133583558987776,Colin Raffel,"['Announcing a new research focus in my lab: Developing tools to enable collaboratively-built and continually-improved models.\n\nBlog post: <LINK>\nPaper on model ""patches"": <LINK>\nPaper on ""merging"" models: <LINK>\nThread ⬇️ (1/11) <LINK>', 'Pre-trained models are a vital part of the model ML ecosystem. But large-scale pre-training is expensive, so most popular pre-trained models were developed by small isolated teams within large resource-rich companies. (2/11) https://t.co/l78gfkomnU', 'This means the majority of the research community has limited control over the design and creation of pre-trained models. Furthermore, pre-trained models are almost never updated - after release, they are reused until being replaced by a better model. (3/11)', 'Contrast this with open-source software development, where a distributed community of developers collaboratively build and continually improve a piece of software. Many of the most widely-used pieces of software were built through open-source software development. (4/11) https://t.co/Z7GKuWxtOu', 'Open-source software development is made possible through a mature set of tools, including version control, continuous integration, forking, merging, package management, and more. Can we enable a similar process for developing machine learning models? (5/11) https://t.co/u8TEWT9FYE', 'First, we need a way to ""patch"" models - i.e., propose changes to a model that are cheap to communicate and store. Gradient descent updates all of a model’s parameters at every training iteration, which makes tracking every change to a model infeasible. (6/11)', 'In our recent NeurIPS paper (https://t.co/qtwlyS1Gys), we demonstrate that it’s possible to choose a small subset of parameters to update over many iterations of training. The subset is chosen according to which parameters have the largest Fisher information. (7/11) https://t.co/I3KIolK94E', 'Next, we need to be able to ""merge"" updates from different contributors who have been training the model in parallel. A standard approach (in federated learning and elsewhere) is to simply average together the updates from each contributor. (8/11)', 'In recent work (https://t.co/Qw6nF5Nl42), we show improved results by using each parameter’s Fisher information to weight its contribution in the average. ""Fisher merging"" is competitive with sequential training and enables new paths for knowledge transfer. (9/11) https://t.co/wFRuDZavw2', 'Beyond patching and merging, there is much additional work to be done - we need ways to rapidly vet contributions, enable architecture changes, ensure backward compatibility, and more. (10/11)', 'These problems motivate interesting research that builds on existing work in continual learning, optimization, multitask learning, federated learning, and beyond. I’m excited to carry out this work with my group and would love to get feedback on this research direction! (11/11)']",https://arxiv.org/abs/2111.09839,"During typical gradient-based training of deep neural networks, all of the model's parameters are updated at each iteration. Recent work has shown that it is possible to update only a small subset of the model's parameters during training, which can alleviate storage and communication requirements. In this paper, we show that it is possible to induce a fixed sparse mask on the model's parameters that selects a subset to update over many iterations. Our method constructs the mask out of the $k$ parameters with the largest Fisher information as a simple approximation as to which parameters are most important for the task at hand. In experiments on parameter-efficient transfer learning and distributed training, we show that our approach matches or exceeds the performance of other methods for training with sparse updates while being more efficient in terms of memory usage and communication costs. We release our code publicly to promote further applications of our approach. ",Training Neural Networks with Fixed Sparse Masks
1,1467899198716596231,1149154781195206657,Rahul Jayaraman,"[""Excited to share a recent paper in which we provide evidence for a new main-sequence+subdwarf B binary (it's even more exciting since the sdB pulsates)! <LINK>"", ""Subdwarf B (sdB) stars represent one of the later stages of stellar evolution -- they're stars about half the mass of our Sun with a helium core and a thin hydrogen shell. They are often found in binaries, and form when the progenitor star's outer atmosphere is stripped of H."", 'While we have good models for binary sdB formation channels from the seminal papers (Han 2002 and 2003), we still need observational evidence to verify these, especially the predictions for longer-period binaries. And this work provides exactly that!', 'Moreover, the Han papers tend to gloss over the case of ""medium period"" sdB binaries. However, if such a system is found, there has to be a corresponding model to explain its formation. Our work provides exactly this: a *possible* medium-period binary, and a model to explain it!', 'We used a Fourier transform of @NASA_TESS 20-sec cadence data (this plot) to identify this star -- we observed a high-amplitude pulsation at 524 c/day, which was unexpected, as the nominal target was a ""typical"" A star. \n\nThe pulsations remained stable across Sectors 29 and 42. https://t.co/SVXufzip3S', 'We found a UV excess in the SED and then fit it to stellar atmospheric models, finding that an unresolved subdwarf B (sdB) component can probably explain this pulsation. Hot stars (T &gt; 25000 K) are known to pulsate at incredibly high frequencies, with periods of &lt; ~ 200s!', '""But what about white dwarfs?"" Good question! We calculated that a white dwarf would have to pulsate with a 35% amplitude to match the observed pulsation amplitude in the FFT -- which is much less than the ~2% amplitude that WDs typically pulsate with.', 'We obtained RV measurements to constrain the mass and period, and found that our data excluded sdB-mass stars below a period of ~35 days. There is an ""island"" of allowed periods between 35 and 80 days, and then the highest likelihood is a period above 150 days.', 'As mentioned earlier, this intermediate-period range is interesting, as no stars have been found in this region. We did some modeling using MESA and found that stable, non-conservative mass transfer via Roche lobe overflow could explain the medium period case.', '(Roche lobe overflow occurs when a star expands and it can\'t hold on to all of its gas/plasma. The short-period sdB binaries are *not* formed by this; rather, they are formed via ""common envelope"" evolution, in which both stars end up sharing an atmosphere in a peanut shape.)', ""We've plotted a potential evolutionary track on the Hertzsprung-Russell diagram for both the sdB progenitor (black) and its companion (green) that could match the observed configuration. This star will steadily move leftward on the diagram until it becomes a white dwarf. https://t.co/p6MhFxPn4O"", 'To further verify our model, we are still taking periodic RV measurements. These new measurements aim to break the degeneracy that currently exists between the intermediate- and long-period cases. \n\nAdditional spectra could also show signatures of the sdB star (e.g. helium lines)', ""This work also shows that the @NASA_TESS 20-second data is invaluable for asteroseismology of hot stars! Before Kepler and TESS, it was very difficult to tease out high-frequency pulsations from ground-based data. Now, it's as easy as calling np.fft.fft on a light curve :)""]",https://arxiv.org/abs/2111.12098,"Using the novel TESS 20-sec cadence data, we have discovered an unusual combination of pulsating stars in what we infer to be a binary system. The primary is a standard $\delta$ Scuti star with pulsations over the range 32-41 d$^{-1}$; this is in an inferred wide orbit with a hot subdwarf B (sdB) secondary, which itself has a large-amplitude p-mode pulsation at 524 d$^{-1}$. We establish constraints on the period of the putative binary by using radial velocity measurements of the $\delta$ Scuti star and show that any sdB-mass companion star must have an orbital period greater than $\sim 35$ d. Our identification of this sdB binary serves as an important addition to the relatively small amount of sdB binaries known to have a period longer than a few days. We show that such a binary can be formed through stable, nonconservative mass transfer, without undergoing a common envelope phase. ","TIC 5724661: A Long-Period Binary with a Pulsating sdB Star and $\delta$
  Scuti Variable"
2,1467658466776870912,2932678322,Keaton Bell,['Check out this great new white dwarf asteroseismology paper led by @CorsicoAlej on @NASA_TESS observations of arguably the best pulsating white dwarf. Just look at that rich pulsation spectrum!  <LINK> <LINK>'],https://arxiv.org/abs/2111.15551,"The collection of high-quality photometric data by space telescopes is revolutionizing the area of white-dwarf asteroseismology. Among the different kinds of pulsating white dwarfs, there are those that have He-rich atmospheres, and they are called DBVs or V777 Her variable stars. The archetype of these pulsating white dwarfs, GD~358, is the focus of the present paper. We report a thorough asteroseismological analysis of the DBV star GD~358 (TIC~219074038) based on new high-precision photometric data gathered by the {\it TESS} space mission combined with data taken from the Earth. In total, we detected 26 periodicities from the {\it TESS} light curve of this DBV star using a standard pre-whitening. The oscillation frequencies are associated with nonradial $g$(gravity)-mode pulsations with periods from $\sim 422$ s to $\sim 1087$ s. Moreover, we detected 8 combination frequencies between $\sim 543$ s and $\sim 295$ s. We combined these data with a huge amount of observations from the ground. We found a constant period spacing of $39.25\pm0.17$ s, which helped us to infer its mass ($M_{\star}= 0.588\pm0.024 M_{\sun}$) and constrain the harmonic degree $\ell$ of the modes. We carried out a period-fit analysis on GD~358, and we were successful in finding an asteroseismological model with a stellar mass ($M_{\star}= 0.584^{+0.025}_{-0.019} M_{\sun}$), in line with the spectroscopic mass ($M_{\star}= 0.560\pm0.028 M_{\sun}$). We found that the frequency splittings vary according to the radial order of the modes, suggesting differential rotation. Obtaining a seismological made it possible to estimate the seismological distance ($d_{\rm seis}= 42.85\pm 0.73$ pc) of GD~358, which is in very good accordance with the precise astrometric distance measured by {\it GAIA} EDR3 ($\pi= 23.244\pm 0.024, d_{\rm GAIA}= 43.02\pm 0.04$~pc). ","Pulsating hydrogen-deficient white dwarfs and pre-white dwarfs observed
  with {\it TESS}: III. Asteroseismology of the DBV star GD 358"
3,1467512397401235458,977016438178435072,Tábita Hünemeier 🏳️‍🌈,"['Check out our new paper! Here we propose a method for directly tackles the issue of identification of the homozygosity islands at the population level, without the need of analyzing single individuals and then combining the results.\n\n<LINK>']",https://arxiv.org/abs/2111.10187,"In this paper, we propose a new method for offline change-point detection on some parameters of the distribution of a random vector. We introduce a penalized maximum likelihood approach that can be efficiently computed by a dynamic programming algorithm or approximated by a fast greedy binary splitting algorithm. We prove both algorithms converge almost surely to the set of change-points under very general assumptions on the distribution and independent sampling of the random vector. In particular, we show the assumptions leading to the consistency of the algorithms are satisfied by categorical and Gaussian random variables. This new approach is motivated by the problem of identifying homozygosity islands on the genome of individuals in a population. Our method directly tackles the issue of identification of the homozygosity islands at the population level, without the need of analyzing single individuals and then combining the results, as is made nowadays in state-of-the-art approaches. ","Population based change-point detection for the identification of
  homozygosity islands"
4,1466813039139864577,1159548392839753729,Hugo Yeche,"['Happy to share ""HiRID-ICU-Benchmark"" (HiB), a new benchmark for patient monitoring tasks in the ICU.\n\nFrom the HiRID database, we define 6 diverse and clinically relevant tasks for +40K patients and +15M time-points.\n\npaper: <LINK>\ncode: <LINK> <LINK>', 'We provide an easy-to-use pipeline for data processing and labels extraction. This shared and reproducible pipeline will improve the comparison of future works. https://t.co/eWhF3cHT7P', 'This benchmark compares methods on a diverse set of clinically relevant tasks as summarized below. https://t.co/HlAXrr5GEL', 'Finally, we compared recent sequence DL architectures with conventional ML algorithms. In line with the original HiRID paper, we observe that lightGBM with hand-extracted features outperforms all DL methods. https://t.co/X55CIg5hqK', 'This work is the result of a great collaboration with Rita Kuznetsova, Marc Zimmermann, @mhueser_, @xinruilyu under Martin Faltys, and @gxr supervision. \n\nFeel free to reach out during NeurIPS Datasets and Benchmarks poster session 4']",https://arxiv.org/abs/2111.08536,"The recent success of machine learning methods applied to time series collected from Intensive Care Units (ICU) exposes the lack of standardized machine learning benchmarks for developing and comparing such methods. While raw datasets, such as MIMIC-IV or eICU, can be freely accessed on Physionet, the choice of tasks and pre-processing is often chosen ad-hoc for each publication, limiting comparability across publications. In this work, we aim to improve this situation by providing a benchmark covering a large spectrum of ICU-related tasks. Using the HiRID dataset, we define multiple clinically relevant tasks in collaboration with clinicians. In addition, we provide a reproducible end-to-end pipeline to construct both data and labels. Finally, we provide an in-depth analysis of current state-of-the-art sequence modeling methods, highlighting some limitations of deep learning approaches for this type of data. With this benchmark, we hope to give the research community the possibility of a fair comparison of their work. ","HiRID-ICU-Benchmark -- A Comprehensive Machine Learning Benchmark on
  High-resolution ICU Data"
5,1466573172388425731,1153651078744834048,Hayata Yamasaki,['Paper submitted. We construct a new noise-robust algorithm for optimizing parameterized quantum circuits in variational quantum algorithms at a significantly smaller cost of measurement shots than the state-of-the-art optimization algorithms. <LINK>'],https://arxiv.org/abs/2111.07952,"Optimization of parameterized quantum circuits is indispensable for applications of near-term quantum devices to computational tasks with variational quantum algorithms (VQAs). However, the existing optimization algorithms for VQAs require an excessive number of quantum-measurement shots in estimating expectation values of observables or iterating updates of circuit parameters, whose cost has been a crucial obstacle for practical use. To address this problem, we develop an efficient framework, \textit{stochastic gradient line Bayesian optimization} (SGLBO), for the circuit optimization with fewer measurement shots. The SGLBO reduces the cost of measurement shots by estimating an appropriate direction of updating the parameters based on stochastic gradient descent (SGD) and further by utilizing Bayesian optimization (BO) to estimate the optimal step size in each iteration of the SGD. We formulate an adaptive measurement-shot strategy to achieve the optimization feasibly without relying on precise expectation-value estimation and many iterations; moreover, we show that a technique of suffix averaging can significantly reduce the effect of statistical and hardware noise in the optimization for the VQAs. Our numerical simulation demonstrates that the SGLBO augmented with these techniques can drastically reduce the required number of measurement shots, improve the accuracy in the optimization, and enhance the robustness against the noise compared to other state-of-art optimizers in representative tasks for the VQAs. These results establish a framework of quantum-circuit optimizers integrating two different optimization approaches, SGD and BO, to reduce the cost of measurement shots significantly. ","Stochastic Gradient Line Bayesian Optimization: Reducing Measurement
  Shots in Optimizing Parameterized Quantum Circuits"
6,1466494724144046085,1572894624,Brian Yan,"['🚨New paper on Joint Modeling of Monolingual and Code-switched ASR: <LINK>\n\nAs a 2nd gen Chinese-American, I speak sporadically in English, Mandarin, or both with code-switching.\n\nIn this work, we build ASR models that work even on my convos with my parents! 1/N', 'The basis of our approach stems from probabilistic graphical models (PGM) of the bilingual ASR problem. \n\nFirst consider as a baseline some *direct* approaches. Problem here: combining 2 unrelated languages makes the task complex. https://t.co/6iM0TAKM5l', 'Now consider as another baseline some *divide-and-conquer* approaches. These are better due to the simpler monolingual sub-tasks. But the trade-off is that we depend on another random variable responsible for modeling the division. https://t.co/uSweZmv9yK', 'Instead, what we propose is to use label-to-frame synchronization to remove the aforementioned dependency in our PGM.\n\nOur approach is a *conditional* factorization where the final bilingual output, which may or may not be code-switched, depends only on our monolingual models! https://t.co/SMYBHowcTi', 'One neural network instance following this conditional framework uses CTC monolingual modules to condition a RNN-T bilingual module (named as Conditional RNN-T).\n\nWe found that compared to baselines, this network outperforms on both monolingual and code-switched tasks. https://t.co/ZCVng97tei', ""Here's a visual of how the label-to-frame synchronization in the monolingual modules allow for the neat conditional factorization.\n\nThe monolingual modules fit together like puzzle pieces. We don't need any other information to figure out what the code-switched utterance is. https://t.co/yOxHDVevEM"", 'More results, ablations, and analyses can be found in our full paper along with details about the method.\n\nThank you to my co-authors at the Tencent AI Labs and at @LTIatCMU (@shinjiw_at_cmu @siddalmia05 @DanBerrebbi @WavLab)', '@jefflai108 I believe that the code-switched prediction can be obtained given only the monolingual information and nothing else. PGM helps to capture this idea and contrast it w other (effective) works, namely MoE. I was also inspired by speaker-conditional chain works by Jing Shi et al.', '@ajaydiv Thanks for the reply! It’s a great direction to explore. Conceptually it seems that yes, this scales to more languages. It’s a good future study :)']",https://arxiv.org/abs/2111.15016,"Conversational bilingual speech encompasses three types of utterances: two purely monolingual types and one intra-sententially code-switched type. In this work, we propose a general framework to jointly model the likelihoods of the monolingual and code-switch sub-tasks that comprise bilingual speech recognition. By defining the monolingual sub-tasks with label-to-frame synchronization, our joint modeling framework can be conditionally factorized such that the final bilingual output, which may or may not be code-switched, is obtained given only monolingual information. We show that this conditionally factorized joint framework can be modeled by an end-to-end differentiable neural network. We demonstrate the efficacy of our proposed model on bilingual Mandarin-English speech recognition across both monolingual and code-switched corpora. ","Joint Modeling of Code-Switched and Monolingual ASR via Conditional
  Factorization"
7,1466491878916956161,2737324542,Philip Muirhead,"[""New paper from the RIT-BU-UToronto PCEB collab. Analyzed K2 and HST data on V471 Tau, an eclipsing dK+WD in Hyades. Recovered the transit of the WD with lensing(!). We find the dK isn't inflated, but the WD is still weird (too hot/massive for Hyades): <LINK>""]",https://arxiv.org/abs/2111.06905,"V471 Tau is a post-common-envelope binary consisting of an eclipsing DA white dwarf and a K-type main-sequence star in the Hyades star cluster. We analyzed publicly available photometry and spectroscopy of V471 Tau to revise the stellar and orbital parameters of the system. We used archival K2 photometry, archival Hubble Space Telescope spectroscopy, and published radial-velocity measurements of the K-type star. Employing Gaussian processes to fit for rotational modulation of the system flux by the main-sequence star, we recovered the transits of the white dwarf in front of the main-sequence star for the first time. The transits are shallower than would be expected from purely geometric occultations owing to gravitational microlensing during transit, which places an additional constraint on the white-dwarf mass. Our revised mass and radius for the main-sequence star is consistent with single-star evolutionary models given the age and metallicity of the Hyades. However, as noted previously in the literature, the white dwarf is too massive and too hot to be the result of single-star evolution given the age of the Hyades, and may be the product of a merger scenario. We independently estimate the conditions of the system at the time of common envelope that would result in the measured orbital parameters today. ","Revised Stellar Parameters for V471 Tau, A Post-common Envelope Binary
  in the Hyades"
8,1466452287770005509,131248250,Scott Gaudi,"['Excited to share a new paper led by @UCB_Astronomy graduate student Keming Zhang @AstroKeming (along with me and his advisor Josh Bloom @profjsb) that discusses a newly uncovered ubiquitous unifying degeneracy in planetary microlensing events. (1/16) \n🧵👇\n<LINK> <LINK>', 'Gravitational microlensing is a weird and wonderful way to find planets orbiting other stars, especially cold, low-mass planets.  It works by using gravity as nature’s magnifying glass to detect planets orbiting distant host stars. (2/16) https://t.co/JAQTBn68qn', 'Microlensing has already been used to discover over 100 exoplanets.  But the true potential of microlensing will be realized by NASA’s Nancy Grace Roman Space Telescope @NASARoman. (3/16) https://t.co/5D652Wn8Gd', 'Roman is predicted to detect ~1500 planets, with sensitivity to analogs of all the solar system planets except for Mercury, planets (and satellites!) with masses down to roughly that of the moon, and free-floating planets with masses as low as that of Mars. (4/16) https://t.co/4DCkjuffKJ', 'But! microlensing light curves can sometimes be tricky to interpret.  In some cases, an observed light curve can be almost perfectly well explained by two or more distinct physical models, meaning that the interpretation of that light curve is ambiguous or ‘degenerate’. https://t.co/z8mmVKGSdy', 'For planetary microlensing events, several such so-called ‘degeneracies’ have been predicted and/or identified, with names such as the ‘close-wide’, ‘inner-outer’, ‘close-resonant’, and ‘wide-resonant’ degeneracies. (6/16)', 'It was generally thought that these degeneracies were distinct, or, at best, the relationship between them was murky. (7/16)', 'Here’s where Keming comes in.  Keming developed a state-of-the-art Machine Learning (ML) algorithm that uses likelihood-free-inference (LFI) to infer the parameters of an observed microlensing event.  (8/16)', 'The key to the LFI approach is the Neural Density Estimator (NDE), which is a particular type of neural network capable of learning conditional distributions that are complex and multi-modal, or degenerate. (9/16) https://t.co/Xi6z7FK4JR', 'By training on 691,257 simulated events like that expected with the Roman Space Telescope, Keming’s algorithm can determine the parameters of any microlensing event effectively in real time. (10/16)', 'Applying this algorithm, Keming noticed degeneracies that hadn’t appeared in the literature before, as well as some that had.  A systematic analysis of these cases revealed that they are the result of the same unifying degeneracy, which we dubbed the ‘offset degeneracy’. (11/16)', 'Simply put, we found that for a large fraction of planetary microlensing events, there always exists two models with distinct separations between the planet and host star that can nearly exactly reproduce the observations. (12/16) https://t.co/8U3CXN7qiO', 'Somewhat befuddled by this result, we then went back and analyzed previously published events to see why they didn’t observe this degeneracy.  It turns out they did, but just didn’t realize it. (13/16) https://t.co/SPq6T54Toy', 'The discovery of this degeneracy suggests a deeper symmetry in the mathematics of planetary microlenses than has previously been recognized.  It’ll be interesting to see what we learn from studying it in depth. (14/16)', 'Perhaps more importantly, this discovery represents a new milestone for the growing utility of ML in the physical sciences. The discovery of the offset degeneracy appears to be the first example where ML directly yields new mathematical insight in astrophysics. (15/16)', 'All hail our future machine overlords.  (16/16)']",https://arxiv.org/abs/2111.13696,"While gravitational microlensing by planetary systems provides unique vistas on the properties of exoplanets, observations of a given 2-body microlensing event can often be interpreted with multiple distinct physical configurations. Such ambiguities are typically attributed to the close-wide and inner-outer types of degeneracies that arise from transformation invariances and symmetries of microlensing caustics. However, there remain unexplained inconsistencies between aforementioned theories and observations. Here, leveraging a fast machine learning inference framework, we present the discovery of the offset degeneracy, which concerns a magnification-matching behaviour on the lens-axis and is formulated independent of caustics. This offset degeneracy unifies the close-wide and inner-outer degeneracies, generalises to resonant topologies, and upon reanalysis, not only appears ubiquitous in previously published planetary events with 2-fold degenerate solutions, but also resolves prior inconsistencies. Our analysis demonstrates that degenerate caustics do not strictly result in degenerate magnifications and that the commonly invoked close-wide degeneracy essentially never arises in actual events. Moreover, it is shown that parameters in offset degenerate configurations are related by a simple expression. This suggests the existence of a deeper symmetry in the equations governing 2-body lenses than previously recognised. ",A Ubiquitous Unifying Degeneracy in Two-Body Microlensing Systems
9,1466382624658804742,1001388471125270528,Ali Sunyaev,"['our new paper about the influence of differential privacy on neural network design is going to be presented at ""Privacy in Machine Learning"" (NeurIPS 2021 Workshop) -&gt; \n<LINK>\n@NeurIPSConf @KASTEL_SRL \n<LINK>\n<LINK>']",https://arxiv.org/abs/2111.14924,"One barrier to more widespread adoption of differentially private neural networks is the entailed accuracy loss. To address this issue, the relationship between neural network architectures and model accuracy under differential privacy constraints needs to be better understood. As a first step, we test whether extant knowledge on architecture design also holds in the differentially private setting. Our findings show that it does not; architectures that perform well without differential privacy, do not necessarily do so with differential privacy. Consequently, extant knowledge on neural network architecture design cannot be seamlessly translated into the differential privacy context. Future research is required to better understand the relationship between neural network architectures and model accuracy to enable better architecture design choices under differential privacy constraints. ","Architecture Matters: Investigating the Influence of Differential
  Privacy on Neural Network Design"
10,1466191974927679490,3379371999,Sania Heba,"['New paper alert! In <LINK> —Freezing-in a hot bath: resonances, medium effects and phase transitions— we formulate freeze-in in a way that accounts for various medium corrections such as thermal masses and phase space suppression/enhancement factors <LINK>', 'We show that for a simple scalar singlet DM model, these effects can account for a 30% difference in relic density calculations in parts of the parameter space!', 'We also consider the case of small reheating temperatures where the necessary freeze-in couplings turn out to actually be large enough to be testable at colliders', 'Maybe I’ll do a longer thread later, but do keep an eye out for the new release of DarkSUSY over the next few days, which will have brand new freeze-in capabilities including all of these effects!']",https://arxiv.org/abs/2111.14871,"Relic density calculations of dark matter freezing out from the primordial plasma have reached a high level of sophistication, with several numerical tools readily available that match the observationally required accuracy. Dark matter production via the freeze-in mechanism, on the other hand, is sensitive to much higher temperatures than in the freeze-out case, implying both technical and computational difficulties when aiming for the same level of precision. We revisit the formulation of freeze-in production in a way that facilitates the inclusion of in-medium corrections like plasma effects and the spin statistics of relativistic quantum gases, as well as the temperature dependence of dark matter production rates induced by the electroweak and strong phase transitions, and we discuss in detail the additional complications arising in the presence of $s$-channel resonances. We illustrate our approach in the context of Higgs portal models, and provide the most accurate calculation to date of the freeze-in abundance of Scalar Singlet dark matter. We explore in particular the case of small reheating temperatures, for which the couplings implied by the freeze-in mechanism may be testable at the LHC. Together with this article we present a major update 6.3 of DarkSUSY with the added capability of performing general freeze-in calculations, including all complications mentioned above. ","Freezing-in a hot bath: resonances, medium effects and phase transitions"
11,1466081824925442059,41446149,Nenad Tomasev,"['Happy to announce our new paper ""Advancing mathematics by guiding human intuition with AI"" <LINK> that was just published in Nature, and one of the associated maths papers ""The signature and cusp geometry of hyperbolic knots""\xa0<LINK>', 'One of the greatest opportunities for AI in the coming years will be in accelerating scientific discovery, and we need to find useful ways of incorporating machine learning in the scientific process across different fields, having it complement human brilliance', 'It is a mutually beneficial iterative process through which AI can help guide the intuition of human domain experts, and where the domain experts can in turn help steer the lens of AI.', 'We hope that these early breakthroughs are merely one of the first steps on an exciting journey of using AI for helping accelerate future discoveries in mathematics.', ""This work wouldn't have been possible without @ADaviesAI, @PetarV_93, Lars Buesing, Sam Blackwell, Daniel Zheng, @weballergy, Richard Tanburn, @PeterWBattaglia, @BlundellCharles, @demishassabis,  @pushmeet, Geordie Williamson, Marc Lackenby &amp; Andras Juhasz.""]",https://arxiv.org/abs/2111.15323,"We introduce a new real-valued invariant called the natural slope of a hyperbolic knot in the 3-sphere, which is defined in terms of its cusp geometry. We show that twice the knot signature and the natural slope differ by at most a constant times the hyperbolic volume divided by the cube of the injectivity radius. This inequality was discovered using machine learning to detect relationships between various knot invariants. It has applications to Dehn surgery and to 4-ball genus. We also show a refined version of the inequality where the upper bound is a linear function of the volume, and the slope is corrected by terms corresponding to short geodesics that link the knot an odd number of times. ",The signature and cusp geometry of hyperbolic knots
12,1466078168570163200,1327100802,Supranta Sarma Boruah,"['Check out our new paper with @MikeHudsonAstro and Guilhem Lavaux where we use Bayesian forward modelling to reconstruct the dark matter distribution of the Universe with peculiar velocity data. <LINK> \n\nA thread on some of the main results.', 'Bayesian forward modelling is a very promising approach to analyze cosmological data. However, it is hard to get unbiased results. In particular, dealing with galaxy bias can be difficult.', 'Adding peculiar velocity data into the mix can break this degeneracy since the velocities are not affected by the galaxy bias and will improve the reconstruction. In this paper, we use the peculiar velocity data directly to perform the reconstruction.', 'However, this analysis is affected by what is known as the inhomogeneous Malmquist bias. This can significantly bias the inferred power spectrum if we do not correct for it. Here is an example of this bias. https://t.co/pP0XyaXSLV', 'We correct for this bias and show that we can get unbiased reconstruction with our approach. We also apply our method to reconstruct the velocity field of our Universe. https://t.co/XEmcdpquTT', 'Furthermore, even though we do not explicitly fit for a bulk flow, the large-scale velocities in our reconstruction agrees very well with other results in the literature. https://t.co/uS1NUhNQPd', 'It is an exciting time for forward-modelled analyses in cosmology and in the future we will be hopefully be able to combine multiple cosmological data sets in these analyses for improved results.']",https://arxiv.org/abs/2111.15535,"We present a forward-modelled velocity field reconstruction algorithm that performs the reconstruction of the mass density field using only peculiar velocity data. Our method consistently accounts for the inhomogeneous Malmquist bias using analytic integration along the line-of-sight. By testing our method on a simulation, we show that our method gives an unbiased reconstruction of the velocity field. We show that not accounting for the inhomogeneous Malmquist bias can lead to significant biases in the forward-modelled reconstructions. We applied our method to a peculiar velocity data set consisting of the SFI++ and 2MTF Tully-Fisher catalogues and the A2 supernovae compilation, thus obtaining a novel velocity reconstruction in the local Universe. Our velocity reconstructions have a cosmological power spectrum consistent with the theoretical expectation. Furthermore, we obtain a full description of the uncertainties on reconstruction through samples of the posterior distribution. We validate our velocity reconstruction of the local Universe by comparing it to an independent reconstruction using the 2M++ galaxy catalogue, obtaining good agreement between the two reconstructions. Using Bayesian model comparison, we find that our velocity model performs better than the adaptive kernel smoothed velocity with the same peculiar velocity data. However, our velocity model does not perform as well as the velocity reconstruction from the 2M++ galaxy catalogue, due to the sparse and noisy nature of the peculiar velocity tracer samples. The method presented here provides a way to include peculiar velocity data in initial condition reconstruction frameworks. ","Reconstructing dark matter distribution with peculiar velocities:
  Bayesian forward modelling with corrections for inhomogeneous Malmquist bias"
13,1466026311957241856,1041309478157672450,James Alvey,"['Can we really detect the Cosmic Neutrino Background❓ (second 🧵 of the week!)\n\nNew paper together with the excellent Nash Sabti (@NashIn0044), Miguel Escudero (@MEscuderoAbenza) and Thomas Schwetz!\n\nLink: <LINK>', 'CNB Detection: An existing proposal to detect the cosmic neutrino background is via their capture onto beta-decaying nuclei, such as tritium - this is the approach of the proposed PTOLEMY experiment.', 'Sensitivity: Crucial parameters for this type of measurement are the absolute neutrino mass and the local neutrino number density. The neutrino mass controls the separation of the signal from the beta-decay background, while the number density controls the overall signal rate. https://t.co/yOawipQM8H', 'Cosmological Problems: The issue is that within the standard model (LCDM), the sum of neutrino masses is strongly constrained to be less than 0.12 eV by CMB + BAO data. This makes it extremely challenging to resolve the signal from the beta-decay background (left panel above).', 'Large Neutrino Masses: As we saw on Monday, however, there are cosmologies where neutrinos can have much larger masses that are still fully compatible with current data. What are the prospects for detection in these cases?\n\nhttps://t.co/17kTdlB8ik', 'Prospects: Whilst there is a trade off between larger neutrino masses and smaller cosmological number densities, the clustering of neutrinos in the Milky Way can significantly enhance their local abundance. The detection prospects at PTOLEMY in these cases are then much improved! https://t.co/1j2LLQCmUF', 'Other Experiments: We also explore the intriguing links between CNB detection at PTOLEMY, neutrino mass measurements at KATRIN, neutrinoless double-beta decay searches, and upcoming surveys by DESI/EUCLID - see the paper for more details!', 'Code: Alongside the paper, we publicly release a code that lets you compute the sensitivity of PTOLEMY for a given experimental and cosmological setup, including the clustering enhancement. Find it at the link below!\n\nGithub: https://t.co/bxLDWAVqPT']",https://arxiv.org/abs/2111.14870,"The Cosmic Neutrino Background (CNB) is a definite prediction of the standard cosmological model and its direct discovery would represent a milestone in cosmology and neutrino physics. In this work, we consider the capture of relic neutrinos on a tritium target as a possible way to detect the CNB, as aimed for by the PTOLEMY project. Crucial parameters for this measurement are the absolute neutrino mass $m_\nu$ and the local neutrino number density $n_\nu^{\rm loc}$. Within the $\Lambda$CDM model, cosmology provides a stringent upper limit on the sum of neutrino masses of $\sum m_\nu < 0.12\,{\rm eV}$, with further improvements expected soon from galaxy surveys by DESI and EUCLID. This makes the prospects for a CNB detection and a neutrino mass measurement in the laboratory very difficult. In this context, we consider a set of non-standard cosmological models that allow for large neutrino masses ($m_\nu \sim 1\,{\rm eV}$), potentially in reach of the KATRIN neutrino mass experiment or upcoming neutrinoless double-beta decay searches. We show that the CNB detection prospects could be much higher in some of these models compared to those in $\Lambda$CDM, and discuss the potential for such a detection to discriminate between cosmological scenarios. Moreover, we provide a simple rule to estimate the required values of energy resolution, exposure, and background rate for a PTOLEMY-like experiment to cover a certain region in the $(m_\nu,\, n_\nu^{\rm loc})$ parameter space. Alongside this paper, we publicly release a code to calculate the CNB sensitivity in a given cosmological model. ",Cosmic Neutrino Background Detection in Large-Neutrino-Mass Cosmologies
14,1466003548144082944,456819625,Lawrence Bull,"['Our new paper - using a mixture of interpretable #GaussianProcesses to model overlapping power trends in wind farm data\n\n<LINK>\n<LINK>\n\nRather than removing data, we automatically model different power relationships\n#windenergy #machinelearning <LINK>', 'Thanks to everyone involved! Those on Twitter: @drgTim @dervilisTheDRG @lizzyintheDRG']",http://arxiv.org/abs/2111.15496,"Power curves capture the relationship between wind speed and output power for a specific wind turbine. Accurate regression models of this function prove useful in monitoring, maintenance, design, and planning. In practice, however, the measurements do not always correspond to the ideal curve: power curtailments will appear as (additional) functional components. Such multivalued relationships cannot be modelled by conventional regression, and the associated data are usually removed during pre-processing. The current work suggests an alternative method to infer multivalued relationships in curtailed power data. Using a population-based approach, an overlapping mixture of probabilistic regression models is applied to signals recorded from turbines within an operational wind farm. The model is shown to provide an accurate representation of practical power data across the population. ","Bayesian Modelling of Multivalued Power Curves from an Operational Wind
  Farm"
15,1465871359427309569,460069521,Andrew Francis,"['New paper out on the arXiv!\n\n""Brauer and partition diagram models for phylogenetic trees and forests”, with Peter Jarvis from U Tasmania.\n\n<LINK>']",https://arxiv.org/abs/2111.15225,"We introduce a correspondence between phylogenetic trees and Brauer diagrams, inspired by links between binary trees and matchings described by Diaconis and Holmes (1998). This correspondence gives rise to a range of semigroup structures on the set of phylogenetic trees, and opens the prospect of many applications. We furthermore extend the Diaconis-Holmes correspondence from binary trees to non-binary trees and to forests, showing for instance that the set of all forests is in bijection with the set of partitions of finite sets. ",Brauer and partition diagram models for phylogenetic trees and forests
16,1465862367233949702,1465098219847831562,Pablo Villanueva Domingo,"['I’m happy to announce our new paper, where we apply novel deep learning methods based on graph neural networks to predict the mass of our galaxy and its neighbor, Andromeda. To our knowledge, this is the first time that AI is applied to this task!\n<LINK>', 'The graph neural networks models were developed in our previous work, training on CAMELS simulations @camels_project to infer the mass of dark matter halos from galactic properties\nhttps://t.co/BMCKfiDmlq', '@paco_astro\n@DavidSpergel\n@desikanarayanan']",https://arxiv.org/abs/2111.14874,"We present new constraints on the masses of the halos hosting the Milky Way and Andromeda galaxies derived using graph neural networks. Our models, trained on thousands of state-of-the-art hydrodynamic simulations of the CAMELS project, only make use of the positions, velocities and stellar masses of the galaxies belonging to the halos, and are able to perform likelihood-free inference on halo masses while accounting for both cosmological and astrophysical uncertainties. Our constraints are in agreement with estimates from other traditional methods. ",Weighing the Milky Way and Andromeda with Artificial Intelligence
17,1465859042174144523,1093387119148462081,Daniel Green,"['CMB enthusiasts, we have a new tool for you! \n(thanks to Selim Hotinli, Joel Meyers, Cynthia Trendafilova &amp; @ajvengelen).\n\nNow you can calculate spectra and forecast using delensed CMB spectra\n\n<LINK>\n<LINK>\n\npaper: <LINK> <LINK>', 'The code expands on our previous work (https://t.co/zEZ7IVh9ON) to include full sky iterative delensing of TTTEEEBB\n\nThe paper includes more discussion of the benefits of delensing, how it adds to the amount of information in the CMB spectra and removes non-Gaussian covariances https://t.co/dUgAFBnPwL', 'We also discuss how delensing can remove possible biases, eg from baryon feedback (also discussed by https://t.co/U7faXaUpYL).  This turned out to be more complicated than I expected and I learned a lot from my collaborators (who deserve all the credit for this work). https://t.co/VD9iPKcJXn']",https://arxiv.org/abs/2111.15036,"The effects of gravitational lensing of the cosmic microwave background (CMB) have been measured at high significance with existing data and will be measured even more precisely in future surveys. Reversing the effects of lensing on the observed CMB temperature and polarization maps provides a variety of benefits. Delensed CMB spectra have sharper acoustic peaks and more prominent damping tails, allowing for improved inferences of cosmological parameters that impact those features. Delensing reduces $B$-mode power, aiding the search for primordial gravitational waves and allowing for lower variance reconstruction of lensing and other sources of secondary CMB anisotropies. Lensing-induced power spectrum covariances are reduced by delensing, simplifying analyses and improving constraints on primordial non-Gaussianities. Biases that result from incorrectly modeling nonlinear and baryonic feedback effects on the lensing power spectrum are mitigated by delensing. All of these benefits are possible without any changes to experimental or survey design. We develop a self-consistent, iterative, all-orders treatment of CMB delensing on the curved sky and demonstrate the impact that delensing will have with future surveys. ",The Benefits of CMB Delensing
18,1465710964892786693,1215310334,Timo Schick,"[""🎉 New paper 🎉 We show that prompt-based learners like PET excel in true few-shot settings (@EthanJPerez) if correctly configured: On @oughtinc's RAFT, PET performs close to non-expert humans for 7/11 tasks without any tuning on a dev set. #NLProc\n\n📝: <LINK> <LINK>""]",http://arxiv.org/abs/2111.13440,"Prompt-based approaches are strong at few-shot learning. However, Perez et al. (2021) have recently cast doubt on their performance because they had difficulty getting good results in a ""true"" few-shot setting in which prompts and hyperparameters cannot be tuned on a dev set. In view of this, we conduct an extensive study of PET, a method that combines textual instructions with example-based finetuning. We show that, if correctly configured, PET performs strongly in a true few-shot setting, i.e., without a dev set. Crucial for this strong performance is PET's ability to intelligently handle multiple prompts. We then put our findings to a real-world test by running PET on RAFT, a benchmark of tasks taken directly from realistic NLP applications for which no labeled dev or test sets are available. PET achieves a new state of the art on RAFT and performs close to non-expert humans for 7 out of 11 tasks. These results demonstrate that prompt-based learners like PET excel at true few-shot learning and underpin our belief that learning from instructions will play an important role on the path towards human-like few-shot learning capabilities. ",True Few-Shot Learning with Prompts -- A Real-World Perspective
19,1465705680355033093,97707247,Gautam Kamath,"['New paper on arXiv: ""Efficient Mean Estimation with Pure Differential Privacy via a Sum-of-Squares Exponential Mechanism,"" with @Samuel_BKH and @mahbodm_. \n\nFinally resolves an open problem on my mind since April 2019 (~2.5 years ago). \n\n<LINK>\n🧵Thread ⬇️ 1/n <LINK>', ""We give the 1st algorithm for mean estimation which is simultaneously:\n-(ε,0)-differentially private\n-O(d) sample complexity\n-poly time\nThe fact we didn't have such an algorithm before indicates something was missing in our understanding of multivariate private estimation. 2/n"", ""This algorithm is an instance of a broader framework which employs Sum-of-Squares for private estimation. This is the first application of SoS for DP that I am aware of. We apply this framework for two sub-problems, I'm sure there are more applications lurking. 3/n"", 'Besides the fact that this solves a problem near and dear to my heart, I am also excited because this is another triumph of a recent direction related to efficient multivariate statistics. It has succeeded in achieving robustness and heavy tails, and now privacy. 4/n', 'Problem is very simple: estimate the mean of a distribution with covariance Σ &lt;= 1, under pure DP. Sounds easy! But the standard techniques are all deficient in terms of sample complexity, the privacy guarantee, or running time. All other techniques have the same drawbacks! 5/n https://t.co/obfbL45kNZ', 'To arrive at our method, let\'s look at how you\'d (naively) run the exponential mechanism.\n1. Define a set of candidate means. Use a cover of the space for this purpose (size ~2^d).\n2. Score function for a candidate: ""Does the candidate \'look correct\' in every 1D projection""? 6/n', 'Both parts are inefficient. To deal with the first, recall that the exp mech is a sampling-based procedure. If the score function is concave, then we can employ tools for efficient log-concave sampling (e.g., Bassily-Smith-Thakurta https://t.co/mj31g0Hz3e).  7/n', 'See also recent improvements by Mangoubi-@NisheethVishnoi (https://t.co/q2vJ6IW76V). \nWe further need the score function to be Lipschitz, and to switch to a continuous analogue of the exponential mechanism. 8/n', 'To address the second issue: we turn to recent advances in high-dimensional robust estimation. These excel at ""finding interesting directions"" efficiently. Our method resembles a class of estimators introduced by @lugosi_gabor-Mendelson (https://t.co/Kb03tlaYOb), which was... 9/n', 'made efficient by @Samuel_BKH (https://t.co/4WI2vJ47Vl). Our algo can be viewed as a private adaptation of Cherapanamjeri-Flammarion-Bartlett (https://t.co/HlcaUUnors). From some starting pt:\n1. Use SDP to find an ""interesting"" direction.\n2. Choose stepsize.\n3. Step, repeat. 10/n', 'Our method, from some starting pt:\n1. *Privately* find an interesting direction\n2. *Privately* choose step size\n3. Step, repeat.\nStep 1 is hiding a lot. It invokes the exponential mechanism, which itself runs the log-concave sampler where the function value is an SDP! 11/n', 'Taking a step back, the general recipe is something like the following. If you can define a score function with ""nice properties,"" then you can efficiently run the exp mech to produce a high utility point privately. 12/n https://t.co/JYInJLiJU9', 'To be a bit more precise, we also have a meta-theorem for SoS score functions. This is kind of a mouthful. There is nothing inherently special about SoS score functions though (to my knowledge), but they seem to be convenient for establishing the properties we desire. 13/n https://t.co/9JcLYUYD9c', ""But back to the specific problem of private mean estimation. Here's our main theorem in all its glory. Wow, I almost forgot to mention: we get robustness and sub-Gaussian tails for free! It's an all-in-one estimator. Recall the problem was open even without these desiderata. 14/n https://t.co/Y5IVH56BrV"", 'Anyway, if any of this interests you, please check out the paper (https://t.co/K8xWukFO6O). This is a very exciting time for private statistics! \n\nIf you prefer talks over threads, you can also watch my talk from @BIRS_Math yesterday. https://t.co/RCBhBagFlW\n\n15/15', '@realhamed @Samuel_BKH @mahbodm_ Sorry but you probably have to also read https://t.co/aoFKX6havh 🙂. CC @sewoong79', ""@lreyzin @Samuel_BKH @mahbodm_ Don't hesitate to ask if you have questions!""]",https://arxiv.org/abs/2111.12981,"We give the first polynomial-time algorithm to estimate the mean of a $d$-variate probability distribution with bounded covariance from $\tilde{O}(d)$ independent samples subject to pure differential privacy. Prior algorithms for this problem either incur exponential running time, require $\Omega(d^{1.5})$ samples, or satisfy only the weaker concentrated or approximate differential privacy conditions. In particular, all prior polynomial-time algorithms require $d^{1+\Omega(1)}$ samples to guarantee small privacy loss with ""cryptographically"" high probability, $1-2^{-d^{\Omega(1)}}$, while our algorithm retains $\tilde{O}(d)$ sample complexity even in this stringent setting. Our main technique is a new approach to use the powerful Sum of Squares method (SoS) to design differentially private algorithms. SoS proofs to algorithms is a key theme in numerous recent works in high-dimensional algorithmic statistics -- estimators which apparently require exponential running time but whose analysis can be captured by low-degree Sum of Squares proofs can be automatically turned into polynomial-time algorithms with the same provable guarantees. We demonstrate a similar proofs to private algorithms phenomenon: instances of the workhorse exponential mechanism which apparently require exponential time but which can be analyzed with low-degree SoS proofs can be automatically turned into polynomial-time differentially private algorithms. We prove a meta-theorem capturing this phenomenon, which we expect to be of broad use in private algorithm design. Our techniques also draw new connections between differentially private and robust statistics in high dimensions. In particular, viewed through our proofs-to-private-algorithms lens, several well-studied SoS proofs from recent works in algorithmic robust statistics directly yield key components of our differentially private mean estimation algorithm. ","Efficient Mean Estimation with Pure Differential Privacy via a
  Sum-of-Squares Exponential Mechanism"
20,1465699935693668354,35881581,Giorgos Zervas,"['1/4 We have a new paper out with @BoraOzaltun and @econ_greg where we develop a maximum likelihood estimator for BLP.\n\nWe show that MLE works better than GMM when the model is correctly specified and is nearly always as bad as GMM when the model is wrong.\n\n<LINK> <LINK>', '2/4 While MLE imposes more assumptions than GMM, it also frees you from having to find valid instruments.', ""3/4 As an example, we use ML to estimate BLP'95 and compare our results with state-of-the-art methods like PyBLP (labelled CG in the table below.) \n\nMLE produces considerably tighter standard errors. https://t.co/Krb67pN8vn"", '4/4 Soon, we will publish our code (built with PyTorch) so you too can experience the pain and joy of estimating BLP using maximum likelihood.', ""@yangyang2000 Depends on your reference point. If you have analytical derivatives of your objective, it does increase speed a bit (you can do things like inverting many markets in parallel.) If you don't, it does a lot (and increases accuracy too) because of autograd."", ""@steventberry That's a very nice way to put it."", '@steventberry I wonder how Google figured out you might be interested in the paper so fast. Recommendation algorithms have gotten eerily good!']",http://arxiv.org/abs/2111.12397,"We discuss estimation of the differentiated products demand system of Berry et al (1995) (BLP) by maximum likelihood estimation (MLE). We derive the maximum likelihood estimator in the case where prices are endogenously generated by firms that set prices in Bertrand-Nash equilibrium. In Monte Carlo simulations the MLE estimator outperforms the best-practice GMM estimator on both bias and mean squared error when the model is correctly specified. This remains true under some forms of misspecification. In our simulations, the coverage of the ML estimator is close to its nominal level, whereas the GMM estimator tends to under-cover. We conclude the paper by estimating BLP on the car data used in the original Berry et al (1995) paper, obtaining similar estimates with considerably tighter standard errors. ",Maximum Likelihood Estimation of Differentiated Products Demand Systems
21,1465681771962503174,14242308,evgenya shkolnik,"[""Also, in #SPARCSwillfly news, we have a new paper on the arXiv about how best to measure stellar flares with SPARCS; creative and adaptable new software led by @SESEASU's Dr. Tahina Ramiaramanantsoa. #PI_Daily <LINK>""]",https://arxiv.org/abs/2111.10322,"The Star-Planet Activity Research CubeSat (SPARCS) is a 6U CubeSat under development to monitor the flaring and chromospheric activity of M dwarfs at near-ultraviolet (NUV) and far-ultraviolet (FUV) wavelengths. The spacecraft hosts two UV-optimized delta-doped charge-coupled devices fed by a 9-cm telescope and a dichroic beam splitter. A dedicated science payload processor performs near real-time onboard science image processing to dynamically change detector integration times and gains to reduce the occurrence of pixel saturation during strong M dwarf flaring events and provide adequate flare light curve structure resolution while enabling the detection of low-amplitude rotational modulation. The processor independently controls the NUV and FUV detectors. For each detector, it derives control updates from the most recent completed exposure and applies them to the next exposure. The detection of a flare event in the NUV channel resets the exposure in the FUV channel with new exposure parameters. Implementation testing of the control algorithm using simulated light curves and full-frame images demonstrates a robust response to the quiescent and flaring levels expected for the stars to be monitored by the mission. The SPARCS onboard autonomous exposure control algorithm is adaptable for operation in future point source-targeting space-based and ground-based observatories geared towards the monitoring of extreme transient astrophysics phenomena. ","Onboard Dynamic Image Exposure Control for the Star-Planet Activity
  Research CubeSat (SPARCS)"
22,1465659868283654146,745831886,Stephane Werner,"[""My new paper was accepted by MNRAS, and it's on arxiv! We found interesting things about galaxies falling in clusters at z~1. The take-home message is: satellite quenching was not important at redshift 1, most galaxies quenched in the infall region. \n\n<LINK>""]",http://arxiv.org/abs/2111.14624,"We quantify the relative importance of environmental quenching versus pre-processing in $z\sim1$ clusters by analysing the infalling galaxy population in the outskirts of 15 galaxy clusters at $0.8<z<1.4$ drawn from the GOGREEN and GCLASS surveys. We find significant differences between the infalling galaxies and a control sample; in particular, an excess of massive quiescent galaxies in the infalling region. These massive infalling galaxies likely reside in larger dark matter haloes than similar-mass control galaxies because they have twice as many satellite galaxies. Furthermore, these satellite galaxies are distributed in an NFW profile with a larger scale radius compared to the satellites of the control galaxies. Based on these findings, we conclude that it may not be appropriate to use 'field' galaxies as a substitute for infalling pre-cluster galaxies when calculating the efficiency and mass dependency of environmental quenching in high redshift clusters. By comparing the quiescent fraction of infalling galaxies at $1<R/R_{200}<3$ to the cluster sample ($R/R_{200}<1$) we find that almost all quiescent galaxies with masses $>10^{11}M_{\odot}$ were quenched prior to infall, whilst up to half of lower mass galaxies were environmentally quenched after passing the virial radius. This means most of the massive quiescent galaxies in $z\sim1$ clusters were self-quenched or pre-processed prior to infall. ","Satellite quenching was not important for z$\sim$1 clusters: most
  quenching occurred during infall"
23,1465521697021440019,1915852868,Anıl Zenginoğlu,"['New arXiv paper!\n\n""A Null Infinity Layer for Wave Scattering""\n\nHere is a one-line summary: To solve the Helmholtz equation on an unbounded domain, scale-out the oscillatory decay and compactify!\n\nThread 🧵\n\n<LINK>', 'What problem does this paper solve?\n\nThe Helmholtz equation describes time-harmonic waves oscillating with a single frequency in time. \n\nSometimes you need to solve the Helmholtz equation numerically on unbounded domains.\nhttps://t.co/sFVxD2ED54', 'Examples are seismic waves across long distances, radar waves reflected by aircraft, gravitational waves from black holes thousands of light-years away.\n\nWhenever the waves propagate for many wavelengths, the mathematical idealization of the problem domain is infinitely large.', ""In such cases, the Helmholtz equation doesn't have unique solutions. \n\nSommerfeld noticed in 1912 that he needed to enforce an outgoing radiation condition at infinity\xa0to guarantee uniqueness, eliminating incoming waves from infinity, allowing only outgoing waves to infinity."", 'How do we solve\xa0the Helmholtz equation numerically with the Sommerfeld condition at infinity? \n\nPapers and textbooks may tell you that computers have finite resources, so we have to truncate the infinite domain somehow and impose boundary conditions on that truncated domain.', ""But the finite resource argument is wrong. \n\nYou could map the infinite domain to a finite domain and add the point at infinity. This procedure, called compactification, is well-known.\n\nAnd it doesn't work.\n\nWhy not?"", 'Here\'s Grosch and Orszag from 1977: compactifications ""are very useful if the solution being sought behaves in a simple way at infinity; otherwise, they are not particularly helpful. Solutions that vanish rapidly or approach a constant at infinity are readily treated by mapping,', 'but solutions that oscillate out to infinity are not so amenable to these techniques.""\n\nThis result was part of the motivation for early work on artificial boundary conditions in the 80s.\n\nBut it doesn\'t have to be that way because we know the asymptotic behavior of the solution.', 'Just transform the variable such that the solutions do not oscillate out to infinity. Instead, they approach a constant at infinity as Grosch and Orszag would like to have it.\n\nThis idea is the essence of the paper: Scale out the oscillatory decay and compactify.', ""What are the advantages? \n\nFirst, you don't need to deal with boundary conditions at all. The Sommerfeld radiation condition is a consequence of the equation, not a separate condition added by hand.\n\nThis is a big deal. Geometry inserts the radiation condition into the equation."", ""Second, you get the solution on the entire domain, all the way to infinity. \n\nThis is important if you're interested in asymptotic observables, such as the echo area in acoustics, the radar cross-section in electromagnetics, or the gravitational waveform in general relativity."", ""Third, the flexibility of the transformation allows for efficient computations of (unidirectional) high-frequency wave propagation. \n\nWe can also restrict the transformation to a layer of finite thickness, with the outer boundary at null infinity (hence the paper's title)."", ""Scale out the oscillatory decay and compactify!\n\nThis simple directive solves a decades-old problem in wave scattering. \n\nIt's an elegant solution to Sommerfeld's original quest, related to Penrose compactification, time transformations, conformal and hyperbolic geometry."", ""Feel free to challenge this idea. I'd love to hear from knowledgeable skeptics.\n\nI'm also looking into exploring this method further, so reach out if you're interested."", ""@quarantinedmind Not naive at all. I should have said that you don't need to deal with BC numerically. \n\nYou need to know which waves you want to eliminate (outgoing or incoming) and how they look mathematically. Then you massage it into the equation through scaling and compactification."", '@quarantinedmind After the procedure, no numerical BC treatment is needed because the condition is not a separate property added by hand, but a consequence of the transformed equation itself.']",https://arxiv.org/abs/2111.14217,"We show how to solve time-harmonic wave scattering problems on unbounded domains without truncation. The technique, first developed in numerical relativity for time-domain wave equations, maps the unbounded domain to a bounded domain and scales out the known oscillatory decay towards infinity. We design a null infinity layer that corresponds to the infinite exterior domain and restricts the transformations to an annular domain. The method does not require the local Green function. Therefore we can use it to solve Helmholtz equations with variable coefficients and certain nonlinear source terms. The method's main advantages are the exact treatment of the local boundary and access to radiative fields at infinity. The freedom in the transformations allows us to choose parameters adapted to high-frequency wave propagation in the exterior domain. We demonstrate the efficiency of the technique in one- and two-dimensional numerical examples. ",A null infinity layer for wave scattering
24,1465373151752245251,1324285243784912896,Carla R. Almeida,['New paper with @deniscr77.\n\n<LINK>'],https://arxiv.org/abs/2111.13575,"Quantum gravity is effective in domains where both quantum effects and gravity are important, such as in the vicinity of space-time singularities. This paper will investigate the quantization of a black-hole gravity, particularly the region surrounding the singularity hidden inside the event horizon. Describing the system with a Hamiltonian formalism, we apply the covariant integral quantization method to find the Wheeler-DeWitt equation of the model. We find that the quantized system has a discrete energy spectrum, and the time coefficient $g_{00}$ provides the dynamics for this model in a non-trivial way. Different configurations for the phase space of a Schwarzschild black hole are obtained in a semi-classical analysis. ","Quantization of a Black-Hole Gravity: geometrodynamics and the quantum
  phase-space formalism"
25,1465358653452652545,1123977306979041288,Sylvia Biscoveanu,"['🚨New paper on the arXiv today with @ColmMTalbot and @sasomao about how you can get the binary neutron star mass distribution wrong with gravitational wave measurements if you mismodel the spin: <LINK>', ""If you assume neutron stars have small dimensionless spin, but some systems in the population have larger spins than allowed by your prior, you'll overestimate the maximum mass and underestimate the minimum mass. https://t.co/RyxnrOdN0s"", 'This will lead to false inferences for the nuclear equation of state, supernova physics, and the dark matter contribution from primordial sub-solar mass black holes! To be safe use a prior allowing for high precessing spin or fit the mass and spin distributions simultaneously.']",https://arxiv.org/abs/2111.13619,"The binary neutron star (BNS) mass distribution measured with gravitational-wave observations has the potential to reveal information about the dense matter equation of state, supernova physics, the expansion rate of the universe, and tests of General Relativity. As most current gravitational-wave analyses measuring the BNS mass distribution do not simultaneously fit the spin distribution, the implied population-level spin distribution is the same as the spin prior applied when analyzing individual sources. In this work, we demonstrate that introducing a mismatch between the implied and true BNS spin distributions can lead to biases in the inferred mass distribution. This is due to the strong correlations between the measurements of the mass ratio and spin components aligned with the orbital angular momentum for individual sources. We find that applying a low-spin prior which excludes the true spin magnitudes of some sources in the population leads to significantly overestimating the maximum neutron star mass and underestimating the minimum neutron star mass at the population level with as few as six BNS detections. The safest choice of spin prior that does not lead to biases in the inferred mass distribution is one which allows for high spin magnitudes and tilts misaligned with the orbital angular momentum. ","The effect of spin mismodeling on gravitational-wave measurements of the
  binary neutron star mass distribution"
26,1465284939260186624,4866997574,Daniel Price,"['1/3 New paper time! Led by PhD student Elli Borchert at Monash we explain how stars like FU Orionis go into outburst so quickly <LINK>', '2/3 since Bonnell &amp; Bastien first proposed binary star interactions as the origin of FU Ori events in 1992, the mystery here was always how you could get a star to go into outburst in 1-2 years if you hit its surrounding disc from the outside (where orbital timescales are slow).', ""3/3 Our answer: you're thinking about the wrong star! Recent observations show FU Ori as a 1.2Msun-0.6 Sun binary, but where the low mass star is 100 x brighter... so it's the little guy that matters! We get this naturally in sims when the secondary flys through the primary disc""]",https://arxiv.org/abs/2111.12723,"We examine whether stellar flyby simulations can initiate FU Orionis outbursts using 3D hydrodynamics simulations coupled to live Monte Carlo radiative transfer. We find that a flyby where the secondary penetrates the circumprimary disc triggers a 1-2 year rise in the mass accretion rate to $10^{-4}~{\rm M_{\odot}~ yr^{-1}}$ that remains high ($\gtrsim 10^{-5}~{\rm M_{\odot}~yr^{-1}}$) for more than a hundred years, similar to the outburst observed in FU Ori. Importantly, we find that the less massive star becomes the dominant accretor, as observed. ",On the rise times in FU Orionis events
27,1465258987604086788,1283150444,Maurizio Pierini,"['New paper on the New Physics Learning Machine method: how to include systematic uncertainties. <LINK>', 'While trying to fit deviations in data wrt a reference sample (standard model expectation) one has to take into account the fact that the reference description comes with uncertainties. We did so combining the previously developed method to a “Likelihood Free Inference” spin-off', 'We first model the dependence of the reference on the nuisance with a network learning the Taylor expansion (as done in standard analyses with analytic methods). We freeze this taylor expansion network in the following steps https://t.co/UATqvaVB5E', 'We break down the problem (learning the lhc test statistics t as a data vs reference classification) in two likelihood ratio learnings: tau = H1 (signal hypothesis) with nuisance vs nominal-nuisance H0 (no-signal hypothesis) and Delta = H0 with nuisance vs nominal-nuisance H0 https://t.co/kMcxl9ssER', 'The lhc test statistics is learned by the difference of the two. For a tuned (on bkg-only toys) choice of hyper-parameters, the cancellation gives back an expected chisq distribution, equivalent to the asymptotic formula of the LHC statistical procedure https://t.co/snlOFNyUl8', 'The method is less sensitive than a classic analysis on a specific signal. But its signal-agnostic nature is crucial to extend the reach of the LHC search program. The effect of nuisance parameters reduces the sensitivity (as it should) but signal sensitivity is retained', 'The modeling of the nuisance has the important role of removing the risk of false claims, due to the imperfect knoll of the reference. With this crucial step, we are now ready to move this approach to real data.']",https://arxiv.org/abs/2111.13633,"We show how to deal with uncertainties on the Standard Model predictions in an agnostic new physics search strategy that exploits artificial neural networks. Our approach builds directly on the specific Maximum Likelihood ratio treatment of uncertainties as nuisance parameters for hypothesis testing that is routinely employed in high-energy physics. After presenting the conceptual foundations of our method, we first illustrate all aspects of its implementation and extensively study its performances on a toy one-dimensional problem. We then show how to implement it in a multivariate setup by studying the impact of two typical sources of experimental uncertainties in two-body final states at the LHC. ",Learning New Physics from an Imperfect Machine
28,1465248184167456773,107180349,Marco Di Liberto,['New paper on arXiv. We show how to generate chiral states by combining two ingredients: interacting bosons and pi-flux plaquettes. <LINK>\n\n(with @GoldmanNathan2)'],https://arxiv.org/abs/2111.13572,"Higher Bloch bands provide a remarkable setting for realizing many-body states that spontaneously break time-reversal symmetry, offering a promising path towards the realization of interacting topological phases. Here, we propose a different approach by which chiral orbital order effectively emerges in the low-energy physics of interacting bosons moving on a square plaquette pierced by a $\pi$-flux. We analyze the low-energy excitations of the condensate in terms of two orbital degrees of freedom and identify a gapped collective mode corresponding to the out-of-phase oscillations of the relative density and phase of the two orbitals. We further highlight the chiral nature of the ground state by revealing the cyclotron-like dynamics of the density upon quenching an impurity potential on a single site. Our single-plaquette results can be used as building blocks for extended dimerized lattices, as we exemplify using the BBH model of higher-order topological insulators. Our results provide a distinct direction to realize interacting orbital-like models that spontaneously break time-reversal symmetry, without resorting to higher bands nor to external drives. ",Orbital order and chiral currents of interacting bosons with $\pi$-flux
29,1465168906893541380,976155561522794497,Juliano César Silva Neves,"['My new paper is all about violation of an important symmetry in physics, the Lorentz symmetry. A particular field, which breaks that symmetry, could increase the expansion of the universe in a given direction (fingerprints from a quantum gravity).\n<LINK>']",https://arxiv.org/abs/2111.13165,"An effect of the Lorentz symmetry breaking is pointed out in the cosmological context. Using a Bianchi I geometry coupled to the Kalb-Ramond field, a consequence of the Lorentz symmetry violation is indicated by a different rate of expansion in a given spatial direction. This article focuses on the coupling constant $\xi_1$, which generates, from the Kalb-Ramond field, all three coefficients that give rise to the Lorentz violation in the gravity sector of the minimal Standard Model Extension. The coupling constant $\xi_1$ increases the rate of expansion of the universe in a given direction during a dark energy era. As a consequence, a range of validity of that coupling constant is also obtained. ",Bianchi type I cosmology with a Kalb-Ramond background field
30,1465143575809777666,338552252,Rodolfo 🏳️‍🌈,['new paper just dropped 🔥🔥🔥\n\n<LINK>'],https://arxiv.org/abs/2111.13638,"Let $X \in \mathcal{H}(2)$ be a Veech surface of discriminant $D$ and let $G(X)$ be the permutation group induced by the affine group of $X$ on the set of Weierstrass points of $X$. We show that $G(X) \cong \mathrm{Dih}_4$ if $D \equiv_{4} 0$, $G(X) \cong \mathrm{Dih}_5$ if $D \equiv_{8} 5$, and $G(X) \cong \mathrm{Dih}_6$ if $D \equiv_{8} 1$, where $\mathrm{Dih}_n$ is the dihedral group of order $2n$. Thus, $G(X)$ is a weak invariant, as it can distinguish the residue class of $D$ mod $8$, but it cannot tell different spin invariants apart when $D \equiv_{8} 1$. Moreover, we show that the same groups arise when we only consider the action of the parabolic elements of the Veech group of $X$ on the Weierstrass points. We prove a similar result in the Weierstrass Prym locus in genus three. Indeed, we show that the permutation group induced by the affine group on the set of fixed points for the Prym involution of a Weierstrass Prym eigenform of discriminant $D$ is isomorphic to $\mathrm{Sym}_2$ when $D$ is an even quadratic residue modulo $16$, and it is isomorphic to $\mathrm{Sym}_3$ otherwise. In genus three, the same group is also obtained when we consider only parabolic elements. Since the permutation action of the affine group on Prym fixed points is trivial in genus four and no Weierstrass Prym eigenforms exist for genus five or larger, we completely classify this action for all Weierstrass Prym eigenforms. ",Permutations on Weierstrass Prym eigenforms
31,1463733877021700096,1237589672609476611,Zhimei Ren,"['Excited to introduce our new paper on sensitivity analysis of individual treatment effects: <LINK>. \nPredictive inference of counterfactuals and ITEs, robust to unmeasured confounding.  Joint work with @YingJin531  and Emmanuel Candès. (1/n)', '@YingJin531 2/ \nPrevious work (@lihua_lei_stat ) provides calibrated predictive inference of counterfactuals and ITEs with experimental/unconfounded data. What happens when there is unmeasured confounding?', '@YingJin531 3/\nThe distributional shift from observations to counterfactuals becomes unidentifiable, but it is bounded (partially identifiable) under the sensitivity model!', '@YingJin531 4/\nWe propose robust conformal inference procedures when the test distribution differs from the training distribution by a limited amount; it wraps around any ML algorithm to produce valid prediction sets and works for a general class of robust prediction problems.', '@YingJin531 5/ \nHow do we know whether a specific individual benefits from a treatment? How robust is our conclusion against unmeasured confounding? The procedure outputs a numerical value to quantify the robustness of causal conclusion on an ITE against unmeasured confounding.']",https://arxiv.org/abs/2111.12161,"We propose a model-free framework for sensitivity analysis of individual treatment effects (ITEs), building upon ideas from conformal inference. For any unit, our procedure reports the $\Gamma$-value, a number which quantifies the minimum strength of confounding needed to explain away the evidence for ITE. Our approach rests on the reliable predictive inference of counterfactuals and ITEs in situations where the training data is confounded. Under the marginal sensitivity model of Tan (2006), we characterize the shift between the distribution of the observations and that of the counterfactuals. We first develop a general method for predictive inference of test samples from a shifted distribution; we then leverage this to construct covariate-dependent prediction sets for counterfactuals. No matter the value of the shift, these prediction sets (resp. approximately) achieve marginal coverage if the propensity score is known exactly (resp. estimated). We describe a distinct procedure also attaining coverage, however, conditional on the training data. In the latter case, we prove a sharpness result showing that for certain classes of prediction problems, the prediction intervals cannot possibly be tightened. We verify the validity and performance of the new methods via simulation studies and apply them to analyze real datasets. ","Sensitivity Analysis of Individual Treatment Effects: A Robust Conformal
  Inference Approach"
32,1463703166185213954,1345200990438375427,Dominic Berry,['We have a new paper out today showing how to use a quantum computer to estimate forces on nuclei in molecules.\n<LINK> <LINK>'],https://arxiv.org/abs/2111.12437,"While most work on the quantum simulation of chemistry has focused on computing energy surfaces, a similarly important application requiring subtly different algorithms is the computation of energy derivatives. Almost all molecular properties can be expressed an energy derivative, including molecular forces, which are essential for applications such as molecular dynamics simulations. Here, we introduce new quantum algorithms for computing molecular energy derivatives with significantly lower complexity than prior methods. Under cost models appropriate for noisy-intermediate scale quantum devices we demonstrate how low rank factorizations and other tomography schemes can be optimized for energy derivative calculations. We perform numerics revealing that our techniques reduce the number of circuit repetitions required by many orders of magnitude for even modest systems. In the context of fault-tolerant algorithms, we develop new methods of estimating energy derivatives with Heisenberg limited scaling incorporating state-of-the-art techniques for block encoding fermionic operators. Our results suggest that the calculation of forces on a single nucleus may be of similar cost to estimating energies of chemical systems, but that further developments are needed for quantum computers to meaningfully assist with molecular dynamics simulations. ","Efficient quantum computation of molecular forces and other energy
  gradients"
33,1463595777498955782,79968415,Rahul Kashyap,"['What happens when two neutron stars merge?  There remains either a neutron star (NS) or a black hole (BH)...eventually. But if the system collapse to form a BH on a short timescale (~1-2 milliseconds), we call them prompt collapse event. Our new paper\n<LINK> (1)', 'https://t.co/g0fWqQoerK  \nWe put several constraints and combination of different datasets in a single plot. Here is a video to describe it sequentially --', 'Please read ahead to know what this plot is about. https://t.co/7UcRAHqvq4', 'The prompt collapse happens only when the combined mass of two NS goes above a certain limit, called threshold mass, otherwise the matter can withstand the crushing force of gravity, at least for a little while, and remain as hypermassive NS. (2)', 'Now the question comes -- Can we learn about the properties of NS matter from it? Since the properties of matter (commonly known as the equation of state) is unknown, the game is to use different theories about it and see which one matches the observations the best. (3)', 'We recently finished a work showing that it can be used to constrain the maximum mass of NS in a narrow range which will be extremely useful for our understanding of NS matter. (4)', 'The crucial assumption that undergoes here is that the ratio of threshold mass (Mth) and maximum mass (Mmax) of NS for a given candidate equation of state is proportional to the compactness of maximum mass NS (Cmax). (5)', ""Amazing! 'cause it connects threshold mass to Mmax and radius of maximum mass NS. On the other hand, there exists an upper and lower bound on Cmax using all (or as much as!) possible EOSs i.e. pressure-density curves to obtain the limits of radii (Rmax) of maximum masses. (6)"", 'We use this information to obtain the limits on threshold masses because we can write a function -- Mth(Mmax,Rmax). These limits as a function of Mmax further show that, just like GW170817, we can use future delayed collapse event to constrain the lower limit of maximum mass. (7)', ""One can use a future prompt collapse event to constrain the upper limit of maximum mass. It's easy to distinguish between a prompt and delayed collapse using their gravitational wave -- prompt collapse events don't have post-merger signal while the delayed collapse events do.(8)"", ""It is not exceedingly difficult to know for a good BNS detection whether it's a prompt or delayed collapse in future GW detectors. We are excited to obtain the limits on maximum mass using our methodology. \nPlease follow the video and paper to know about it in more detail. (9)""]",https://arxiv.org/abs/2111.05183,"We determine the threshold mass for prompt (no bounce) black hole formation in equal-mass neutron star (NS) mergers using a new set of 227 numerical relativity simulations. We consider 23 phenomenological and microphysical finite temperature equations of state (EOS), including models with hyperons and first-order phase transitions to deconfined quarks. We confirm the existence of EOS-insensitive relations between the threshold mass, the binary tidal parameter at the threshold ($\Lambda_{th}$), the maximum mass of nonrotating NSs, and the radii of reference mass NSs. We correct the systematic errors in previously reported fitting coefficients that were obtained with approximate general-relativity simulations. We combine the EOS-insensitive relations, phenomenological constraints on NS properties and observational data from GW170817 to derive an improved lower limit on radii of maximum mass and 1.6 M$_\odot$ NS of 9.81 km and 10.90 km, respectively. We also constrain the radius and quadrupolar tidal deformability ($\Lambda$) of a 1.4 $M_\odot$ NS to be larger than 10.74 km and 172, respectively. We consider uncertainties in all independent parameters -- fitting coefficients as well as GW170817 masses while reporting the range of radii constraints. We introduce new methods to constrain the upper as well as lower limit of NS maximum mass using future BNS detections and their identification as prompt or delayed collapse. With future observations it will be possible to derive even tighter constraints on the properties of matter at and above nuclear density using the method proposed in this work. ","Numerical relativity simulations of prompt collapse mergers: threshold
  mass and phenomenological constraints on neutron star properties after
  GW170817"
34,1463460046214475779,1263870728870469632,Enrico Ronca,['Our new paper about the first orbital theory for electronic systems in QED environments is now out on ArXiv.\n\n<LINK>'],https://arxiv.org/abs/2111.11829,"Coupling between molecules and vacuum photon fields inside an optical cavity has proven to be an effective way to engineer molecular properties, in particular reactivity. To ease the rationalization of cavity induced effects we introduce an ab initio method leading to the first fully consistent molecular orbital theory for quantum electrodynamics environments. Our framework is non-perturbative and explains modifications of the electronic structure due to the interaction with the photon field. We show that the newly developed orbital theory can be used to predict cavity induced modifications of molecular reactivity and pinpoint classes of systems with significant cavity effects. We also investigate cavity-induced modifications of molecular reactivity in the vibrational strong coupling regime. ",Molecular orbital theory in cavity QED environments
35,1463366831503147008,4807828837,Vishal Upendran,"['☀️🚨New paper alert🚨☀️\nWork with @dktripathi  on understanding the chromospheric and transition region dynamics, in Coronal holes and Quiet Sun, was accepted for publication in ApJ yesterday.  It is put up on arxiv: <LINK>, but read on to know more!  (1/9)', 'As a Season - 2 to https://t.co/roG30C3GE5, we study the intensity and velocities of the Mg II, C II and Si IV lines  in CHs and QS for &gt;100,000 spectra, as a function of the photospheric magnetic flux density (|B|). (2/9)', 'The formation height in atmosph. follows Mg II peaks&lt;core~CII&lt;Si IV if height~temperature. Largely, Mg II results follow those from C II: reduced intensity, excess up and downflows in CHs over QS for identical |B|. But the excess downflows are not seen in Si IV! (3/9)', 'We find Si IV downflows are strongly correlated with chromospheric downflows, with excess deceleration in QS. Also, Si IV upflows are well correlated with both up and downflows in chromosphere (indicating presence of bidirectional flows), with excess upflows in CHs. (4/9)', 'Thus upflows = accelerated in CHs, downflows = decelerated in QS. With our knowledge of line profiles of C II and non thermal width of Si IV, we find the observ. are best explained by impulsive heating occuring at various heights in different topological settings. (5/9)', 'i.e, predominant interchange reconnection in CH, and closed loop recn in QS. Then, stratified atmosphere + mass flux conserving flows can cause these relations, along with waves (maybe slow mode shocks from reconnection?) (6/9)', 'The outflowing plasma (CHs) may become the solar wind, while the flows in QS are loop filling and give rise to local heating. Thus, solar wind emergence &amp; acceleration, and QS heating seem to arise from the same underlying cause occurring in different topological settings. (7/9)', 'Also, the kinked fields resulting from interchange recn may also result in switchback generation=&gt; our flow measurements serve as constraints to the models attributing corona/transition region as origin for switchbacks! (8/9)', ""I presented this paper @hinode14iris11 meeting last month. I have taken the liberty to record my talk, and upload on youtube (as unlisted - won't show up in search) here: https://t.co/ed7nJCmpk4\n\nHope you have as much fun reading it as we had doing/writing it! (9/9)"", '@louiseharra @realDavidBrooks @helen_hm11 @vmpillet @yeimyrivera_sol @do_you_kerr @markcheung @JustinCKasper @clara_froment  the full paper is possibly of interest to you?']",https://arxiv.org/abs/2111.11668,"The solar coronal heating in quiet Sun (QS) and coronal holes (CH), including solar wind formation, are intimately tied by magnetic field dynamics. Thus, a detailed comparative study of these regions is needed to understand the underlying physical processes. CHs are known to have subdued intensity and larger blueshifts in the corona. This work investigates the similarities and differences between CHs and QS in the chromosphere using the Mg II h & k, C II lines, and transition region using Si IV line, for regions with identical absolute magnetic flux density (|B|). We find CHs to have subdued intensity in all the ines, with the difference increasing with line formation height and |B|. The chromospheric lines show excess upflows and downflows in CH, while Si IV shows excess upflows (downflows) in CHs (QS), where the flows increase with |B|. We further demonstrate that the upflows (downflows) in Si IV are correlated with both upflows and downflows (only downflows) in the chromospheric lines. CHs (QS) show larger Si IV upflows (downflows) for similar flows in the chromosphere, suggesting a common origin to these flows. These observations may be explained due to impulsive heating via interchange (closed-loop) reconnection in CHs (QS), resulting in bidirectional flows at different heights, due to differences in magnetic field topologies. Finally, the kinked field lines from interchange reconnection may be carried away as magnetic field rotations and observed as switchbacks. Thus, our results suggest a unified picture of solar wind emergence, coronal heating, and near-Sun switchback formation. ","On the formation of solar wind & switchbacks, and quiet Sun heating"
36,1463210620061376519,1219296192229773313,Giulio Falcioni,"['New paper out today!\nIt was a long journey through scattering amplitudes up to four loops in QCD, which showed some beautiful, universal structure.\n\n<LINK>']",https://arxiv.org/abs/2111.10664,"Using rapidity evolution equations we study two-to-two gauge-theory scattering amplitudes in the Regge limit. We carry out explicit computations at next-to-next-to-leading logarithmic accuracy through four loops and present new results for both infrared-singular and finite contributions to the amplitude. New techniques are devised in order to derive the colour structure stemming from three-Reggeon exchange diagrams in terms of commutators of channel operators, obtaining results that are valid for any gauge group, and apply to scattered particles in any colour representation. We also elucidate the separation between contributions to the Regge cut and Regge pole in the real part of the amplitude to all loop orders. We show that planar contributions due to multiple-Reggeon exchange diagrams can be factorised as a Regge pole along with the single-Reggeon exchange, and when this is done, the singular part of the gluon Regge trajectory is directly determined by the cusp anomalous dimension. We explicitly compute the Regge cut component of the amplitude through four loops and show that it is non-planar. From a different perspective, the new results provide important information on soft singularities in general kinematics beyond the planar limit: by comparing the computed corrections to the general form of the four-loop soft anomalous dimension we derive powerful constraints on its kinematic dependence, opening the way for a bootstrap-based determination. ","Scattering amplitudes in the Regge limit and the soft anomalous
  dimension through four loops"
37,1463167704177823754,1312362807665491970,Theo Anton,"['New paper (my first 🥳) with Tim Clifton. We extend the parameterised post-Newtonian cosmology formalism, which is a theory-independent framework for testing gravity in cosmology, by deriving a momentum constraint equation [thread]\n\n<LINK>', 'The momentum constraint is important because it relates the gravitational potentials (the two usual scalars plus a frame-dragging divergenceless vector) to the inhomogeneous matter distribution and matter velocities.', 'The equation is valid for any metric theory of gravity that fits into the classical parameterised post-Newtonian (PPN) formalism of Will &amp; Nordtvedt etc, and for arbitrarily large density contrast on small scales.', 'In particular, we find that it holds for both conservative (e.g. scalar-tensor) and non-conservative gravity theories. We show how the formalism works for some simple example theories: a quintessence model, the Brans-Dicke theory, and a vector-tensor theory.']",https://arxiv.org/abs/2111.10860,"We derive a theory-independent version of the momentum constraint equation for use in cosmology, as a part of the Parameterised Post-Newtonian Cosmology (PPNC) framework. Our equations are constructed by adapting the corresponding quantities from formalisms devised for testing and constraining gravity in isolated astrophysical systems, thereby extending the domain of applicability of these approaches up to cosmological scales. Our parameterised equations include both scalar and divergenceless-vector gravitational potentials, and can be applied to both conservative and non-conservative theories of gravity. They can also be used to describe the gravitational fields of both non-linear structures and super-horizon perturbations. We apply the parameterised equations we propose to quintessence models of dark energy, as well as to scalar-tensor and vector-tensor theories of gravity. We find them to work well in each case. Our equations are highly compact, and are intended to be useful for constraining gravity in a theory-independent fashion in cosmology. ","The momentum constraint equation in Parameterised Post-Newtonian
  Cosmology"
38,1463112286495674369,117917587,Tatsuro KAWAMOTO,"['New paper on arXiv:\xa0<LINK>\n“Sequential locality of graphs and its hypothesis testing”\nIn short, we formulated statistical tests for a matrix reordering problem (envelope reduction). 1/4 <LINK>', 'Whereas the reordering problem as an optimization only focuses on the best ordering, to assess the statistical significance of the sequences/graphs, we need to evaluate all possible orderings/(distinguishable) graph instances, which requires lots of combinatorial calculations.2/4', 'Tech. pt 1: We introduced a systematic approach to calculate combinatorial quantities using integrals. Although this type of trick has been often used by physicists as an approx. method in asymptotic theories, it is also useful for exact calculations of finite-size problems. 3/4', 'Tech. pt 2: We introduced a stat model called ordered random graph model (ORGM). This is a block model that partly overlaps with the stochastic block model. We use the ORGM for a power analysis &amp; as a null hypothesis. https://t.co/xENwVpCPr4']",https://arxiv.org/abs/2111.11267,"Adjacency matrix is the most fundamental and intuitive object in graph analysis that is useful not only mathematically but also for visualizing the structures of graphs. Because the appearance of an adjacency matrix is critically affected by the ordering of rows and columns, or vertex ordering, statistical assessment of graphs together with their vertex sequences is important in identifying the characteristic structures of graphs. In this study, we propose a hypothesis testing framework that assesses how locally vertices are connected to each other along a specified vertex sequence, which provides a statistical foundation for an optimization problem called envelope reduction. The proposed tests are formulated based on a combinatorial approach and a block model with intrinsic vertex ordering. This work offers a novel perspective to a wide range of graph data obtained through experiments in different fields of science and helps researchers to conclude their findings with statistical guarantees. ",Sequential locality of graphs and its hypothesis testing
39,1463102123487473668,1093924185427062784,Maureen Cohen,"[""New paper on the ArXiv! We used the Unified Model to study oscillations in the atmosphere of an Earth-like tidally locked #exoplanet. You've never seen oscillations like these before! 😱🤯🪐Detailed blog post to come. <LINK> @exoclimatology""]",https://arxiv.org/abs/2111.11281,"Using a three-dimensional general circulation model, we show that the atmospheric dynamics on a tidally locked Earth-like exoplanet, simulated with the planetary and orbital parameters of Proxima Centauri b, support a longitudinally asymmetric stratospheric wind oscillation (LASO), analogous to Earth's quasi-biennial oscillation (QBO). In our simulations, the LASO has a vertical extent of 35--55 km, a period of 5--6.5 months, and a peak-to-peak wind speed amplitude of -70 to +130 m/s with a maximum at an altitude of 41 km. Unlike the QBO, the LASO displays longitudinal asymmetries related to the asymmetric thermal forcing of the planet and to interactions with the resulting stationary Rossby waves. The equatorial gravity wave sources driving the LASO are localised in the deep convection region at the substellar point and in a jet exit region near the western terminator, unlike the QBO, for which these sources are distributed uniformly around the planet. Longitudinally, the western terminator experiences the highest wind speeds and undergoes reversals earlier than other longitudes. The antistellar point only experiences a weak oscillation with a very brief, low-speed westward phase. The QBO on Earth is associated with fluctuations in the abundances of water vapour and trace gases such as ozone which are also likely to occur on exoplanets if these gases are present. Strong fluctuations in temperature and the abundances of atmospheric species at the terminators will need to be considered when interpreting atmospheric observations of tidally locked exoplanets. ","Longitudinally asymmetric stratospheric oscillation on a tidally locked
  exoplanet"
40,1462967188483051521,841858076193955841,Karan Desai (KD),"[""📢New dataset!📢 RedCaps: 12M image-text pairs from Reddit for vision and vision-and-language applications.\nWebsite: <LINK>\nPaper: <LINK>\n\nCheck out captions from a RedCaps-trained model!⬇️\nTry more here: <LINK>\nWhat's new?🧵1/8 <LINK>"", 'Conversational flavor of data: RedCaps data is created with a specific intent of human interaction on social media. Reddit users have an incentive (upvotes) to upload high-quality data — sometimes witty or emotional, unlike HTML alt-text. 2/8 https://t.co/vBu8ZpMt86', 'Subreddits: We collect data from 350 manually chosen subreddits. Largest subreddits show that Reddit users like to share pets, hobbies, photography! These subreddits let us steer data distribution, and provide image labels even when captions don’t mention objects in image. 3/8 https://t.co/8lZ4RImW8M', 'Dataset size: One of the largest public image-text datasets in 2021! RedCaps contains data from the past 13 years (2008–2020). However, it is not static by design! It will continue to grow in the future as more data gets uploaded to Reddit. 4/8 https://t.co/pbAHhmJBIz', 'Vision pre-training: In controlled settings, models trained on RedCaps outperform those trained on existing public image-text datasets (SBU, CC-3M) on 10/11 downstream tasks. See zero-shot classification⬇️, more in the paper. 5/8 https://t.co/uKmP88JtX7', 'Image captioning: We showed captions predicted by models trained on RedCaps vs CC-3M (alt-text dataset) to human evaluators. They preferred captions from RedCaps-trained model (underlined ⬇️) for 633/1000 images! 6/8 https://t.co/PESWUUMsKv', 'Subreddit-controlled captioning: Since we trained models with subreddits, we can *prompt* them with different subreddits at test-time. This gives linguistically diverse and amusing captions! Try out our demo: https://t.co/ftUciwTXJm\nTweet your captions at us with #redcaps! 7/8 https://t.co/cjBUwWXA1J', 'Dataset available at https://t.co/u6XC75uFzl!\nPre-trained models coming soon.\n\n(w/ @gauravkaul7, @zubinaysola, @jcjohnss)\nTo appear at NeurIPS 2021 datasets and benchmarks.\n8/8 Fin.', 'To add a bit more: I want to give a special shoutout to (1) our anonymous reviewers, who provided constructive feedback and thoroughly engaged with us — see discussion at https://t.co/Vt0kX6Zexk — peer reviewing at its finest! And (2) &gt;&gt; 9/11', '(2) Authors of https://t.co/CqVHs15Shw (@Abebab, @vinayprabhu) — this paper appeared on arxiv ~1 month after I started working on this project. The paper uncovers problematic trends in large image datasets, which I would not have accounted for, had I not read this paper. &gt;&gt; 10/11', '(1,2) have affected our design choices for the better. We avoided subreddits having lot of people images, filtered data using NSFW/face detectors, and added a form on https://t.co/jMGzVuztQX for anyone to request image removal from RedCaps. Not perfect, but a positive step. 11/11', '@TheRealPomax @gauravkaul7 @zubinaysola @jcjohnss We used the official Reddit API for collection — as per API terms (https://t.co/WfdRQBdS2O) we can copy ad display this information in our ""application"" by minimal formatting for display, see clause below. Images on https://t.co/u6XC75MgXV are static URLs not hosted by us. (1/2) https://t.co/B3bOgjFH3k', '@TheRealPomax @gauravkaul7 @zubinaysola @jcjohnss We distribute images in annotations as URLs; dataset users need to download them separately (we provide https://t.co/U0WAvmziI3). This distribution is similar to Pushshift Reddit database (https://t.co/AVgNt0usBR)\nSharing URLs also gives full data control to Reddit users. (2/2)', '@_kamalb @TheRealPomax @gauravkaul7 @zubinaysola @jcjohnss We agree but we think it\'s the right design choice as Reddit user privacy comes first. This ""dataset decay"" issue exists with existing datasets (SBU, CC — see orange, green curves https://t.co/4Ibe4x01ix)', '@_kamalb @TheRealPomax @gauravkaul7 @zubinaysola @jcjohnss One reviewer shared this concern, in our reviewing discussions we actually checked the rate of dataset decay. See details here — https://t.co/ZR7O8qQLs1\n\nTL: DR; deletion is less likely after &gt; 6mo of posting. Reddit users lose the gained post karma so less incentive to delete!']",https://arxiv.org/abs/2111.11431,"Large datasets of paired images and text have become increasingly popular for learning generic representations for vision and vision-and-language tasks. Such datasets have been built by querying search engines or collecting HTML alt-text -- since web data is noisy, they require complex filtering pipelines to maintain quality. We explore alternate data sources to collect high quality data with minimal filtering. We introduce RedCaps -- a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. We collect data from a manually curated set of subreddits, which give coarse image labels and allow us to steer the dataset composition without labeling individual instances. We show that captioning models trained on RedCaps produce rich and varied captions preferred by humans, and learn visual representations that transfer to many downstream tasks. ","RedCaps: web-curated image-text data created by the people, for the
  people"
41,1462862673520087047,831571293392732162,Thomas Fel,['We are pleased to present our work: an efficient attribution method based on Sobol indices for explainability!\n\nOur new NeurIPS paper is available at <LINK>.\n\n<LINK> for the implementation of the method🎉 <LINK>'],https://arxiv.org/abs/2111.04138,"We describe a novel attribution method which is grounded in Sensitivity Analysis and uses Sobol indices. Beyond modeling the individual contributions of image regions, Sobol indices provide an efficient way to capture higher-order interactions between image regions and their contributions to a neural network's prediction through the lens of variance. We describe an approach that makes the computation of these indices efficient for high-dimensional problems by using perturbation masks coupled with efficient estimators to handle the high dimensionality of images. Importantly, we show that the proposed method leads to favorable scores on standard benchmarks for vision (and language models) while drastically reducing the computing time compared to other black-box methods -- even surpassing the accuracy of state-of-the-art white-box methods which require access to internal representations. Our code is freely available: this https URL ","Look at the Variance! Efficient Black-box Explanations with Sobol-based
  Sensitivity Analysis"
42,1462838200167186437,724609851884724225,Ana Belen Sainz,['New paper out! \n<LINK>\n\n@ictqt  @mattyhoban'],https://arxiv.org/abs/2111.10244,"Einstein-Podolsky-Rosen (EPR) steering is often (implicitly or explicitly) taken to be evidence for spooky action-at-a-distance. An alternative perspective on steering - endorsed by EPR themselves - is that Alice has no causal influence on the physical state of Bob's system; rather, Alice merely updates her knowledge of the state of Bob's system by performing a measurement on a system correlated with his. In this work, we elaborate on this perspective (from which the very term `steering' is seen to be inappropriate), and we are led to a resource-theoretic treatment of correlations in EPR scenarios. For both bipartite and multipartite scenarios, we develop the resulting resource theory, wherein the free operations are local operations and shared randomness (LOSR). We show that resource conversion under free operations in this paradigm can be evaluated with a single instance of a semidefinite program, making the problem numerically tractable. Moreover, we find that the structure of the pre-order of resources features interesting properties, such as infinite families of incomparable resources. In showing this, we derive new EPR resource monotones. We also discuss advantages of our approach over a pre-existing proposal for a resource theory of `steering', and discuss how our approach sheds light on basic questions, such as which multipartite assemblages are classically explainable. ","Quantifying EPR: the resource theory of nonclassicality of common-cause
  assemblages"
43,1462822795830829058,1106252206112735232,Dorota M Grabowska,"['New #QuantumComputing paper! We introduce a resource-efficient formulation of U(1) gauge theories, geared for simulations on digital quantum computers. Work done as part of #Cern QTI (@CERNquantum), with Christian Bauer from @LBNLphysics .\n\nPaper here: <LINK>', ""The goal was to satisfy Gauss' Law while faithfully represent the undigitized theory at all values of (bare) coupling.\n\nI gave a Theory Colloquium on this and other quantum computing topics last week. Slides and a recording of the talk can be found here: https://t.co/96FmhAnW1t"", 'I\'ll post a (mostly) jargon-free explanation of this work later this week. I just have to run it by the family first, as they still refer to my lattice QCD work as ""sałata i krutąsy"" (lettuce and croutons).']",https://arxiv.org/abs/2111.08015,"We derive a representation for a lattice U(1) gauge theory with exponential convergence in the number of states used to represent each lattice site that is applicable at all values of the coupling. At large coupling, this representation is equivalent to the Kogut-Susskind electric representation, which is known to provide a good description in this region. At small coupling, our approach adjusts the maximum magnetic field that is represented in the digitization as in this regime the low-lying eigenstates become strongly peaked around zero magnetic field. Additionally, we choose a representation of the electric component of the Hamiltonian that gives minimal violation of the canonical commutation relation when acting upon low-lying eigenstates, motivated by the Nyquist-Shannon sampling theorem. For (2+1) dimensions with 4 lattice sites the expectation value of the plaquette operator can be calculated with only 7 states per lattice site with per-mille level accuracy for all values of the coupling constant. ","Efficient Representation for Simulating U(1) Gauge Theories on Digital
  Quantum Computers at All Values of the Coupling"
44,1462787672175517696,1004365363574902784,Kevin J. Kelly,"['Very, very excited about our new paper out today, <LINK>, along with @carguelles314, @iesteban_ph, @HostertMatheus, @joachimkopp, @PedroANMachado, @ivanj_ms, and @yuberfpg!\n\nA thread on what we found 😀', 'We explored the latest @Microboone results and looked at them in two different ways:\n\n1) Is there a model-independent way of ruling out the famed MiniBooNE ""low-energy excess""?\n\nand\n\n2) What do these results say about eV-scale sterile neutrinos?', 'The MiniBooNE low-energy excess is one of the most puzzling anomalies in particle physics. Along with the ""LSND Anomaly"", it stands as an indicator for new physics, like additional neutrinos beyond the 3 we know of. These are often called ""sterile neutrinos.""', ""I won't go into it on this thread, but there are plenty of other challenges if you believe such a sterile neutrino exists, especially from cosmology and other types of lab-based neutrino measurements."", 'MicroBooNE, along with the upcoming SBND and ICARUS experiments, was designed to thoroughly test this low-energy excess and determine whether or not it comes from one of these light sterile neutrinos.', 'Last month, MicroBooNE presented their first electron-related results (as well as photon-based ones), and claimed not to see an excess consistent with what MiniBooNE observed, at high significance!\n\nhttps://t.co/nny1BcElLq', ""Here's the challenge, though. MiniBooNE's observed excess is in addition to a sizeable expected distribution of background events, from a number of sources. Moreover, MiniBooNE took so much data that systematic uncertainties are *significant* in understanding their results."", ""So, in light of that, it's important to ask what *exactly* is the MiniBooNE low-energy excess. While MicroBooNE has started to present some very impressive results, we need to make sure that this low-energy excess can be explored as comprehensively as possible."", 'This comes back to question #1 -- how can MicroBooNE explore this low-energy excess in a model-independent way?\n\nWe addressed this question by means of constructing ""templates"" -- spectra that could explain the MiniBooNE excess well enough, statistically speaking.', 'This figure shows this idea: the ""nominal"" template is what MicroBooNE set out to constrain, and did so very well. The pink and blue templates, we found, have significantly different shapes, but fit the MiniBooNE data with p-values above 80%. https://t.co/BKDp5fNx7H', 'Because these templates have very different shapes, when we determine what MicroBooNE should see in light of one of these templates, the results vary significantly: https://t.co/Id3lX5gdNP', 'So, we take these templates that fit MiniBooNE well, and determine just how well MicroBooNE can exclude these different spectral hypotheses.', ""We find that, while they exclude this nominal template at high significance, there are plenty of templates (which fit MiniBooNE well) that have yet to be firmly excluded by MicroBooNE. With more data, we'll see what happens! https://t.co/IaQQBCJybc"", 'With that ""wrapped up"", we turn to the vaunted sterile-neutrino explanation of the MiniBooNE low-energy excess. The first way we attack this is in a simple, ""two-neutrino"" framework, where additional neutrino oscillations are dictated by a new massive neutrino with a mixing angle', 'Many experiments have tested this scenario, but it turns out that MicroBooNE\'s latest results (especially its ""inclusive analysis"", https://t.co/TC6Gc28knr) provide some of the strongest constraints.', 'In this ""mixing angle vs. mass-splitting"" parameter space, we find that MicroBooNE has begun to constrain, at high significance, a large fraction of the space that the MiniBooNE anomaly purports to place these parameters. https://t.co/WjgRN3KJou', 'Part of this comes from data fluctuations compared to expectations -- an excess of muon-type events and a deficit of electron-type events leading to a stronger-than-expected constraint. Nevertheless, this is exciting progress!', 'However, to thoroughly test this, the full glory of the SBN program (SBND + MicroBooNE + ICARUS) is required. With that combined analysis, we can robustly test this anomaly once and for all and arrive at a consistent answer.', ""This thread has already gone *very* long, so I'll just mention some of the other things we looked at:"", 'a) what about a ""complete"" 3+1 neutrino framework, where oscillations of background neutrinos matter? This is the only consistent way to analyze rich datasets like that of MicroBooNE and future experiments of similar types.', 'b) can MicroBooNE say anything about electron-neutrino or muon-neutrino disappearance, using their ""background"" samples. It turns out that statements of this type are difficult to make, due in large part to large systematic uncertainties.', 'Regardless, these results from @Microboone are both exciting and impressive, and very important for our field as we move into the next generation of precision neutrino physics. A firm understanding of what they observe is necessary for *many* interpretations of data to come.', 'Big thanks to the MicroBooNE collaboration for providing thorough data releases, without which our analyses would be significantly more difficult to carry out. Hopefully releases like this are the norm going forward for neutrino experiments!', 'And again, big thanks to my collaborators (all on twitter!), @carguelles314, @iesteban_ph, @HostertMatheus, @joachimkopp, @PedroANMachado, @ivanj_ms, and @yuberfpg, for a great team-up! We put this together fairly quickly, and I had more fun than can be expected in doing so.']",https://arxiv.org/abs/2111.10359,"A new generation of neutrino experiments is testing the $4.7\sigma$ anomalous excess of electron-like events observed in MiniBooNE. This is of huge importance for particle physics, astrophysics, and cosmology, not only because of the potential discovery of physics beyond the Standard Model, but also because the lessons we will learn about neutrino-nucleus interactions will be crucial for the worldwide neutrino program. MicroBooNE has recently released results that appear to disfavor several explanations of the MiniBooNE anomaly. Here, we show quantitatively that MicroBooNE results, while a promising start, unquestionably do not probe the full parameter space of sterile neutrino models hinted at by MiniBooNE and other data, nor do they probe the $\nu_e$ interpretation of the MiniBooNE excess in a model-independent way. ","MicroBooNE and the $\nu_e$ Interpretation of the MiniBooNE Low-Energy
  Excess"
45,1462760780181475335,717162062837719040,Phil Armitage,"['New paper! In work led by Benoît Tabone, we develop an analytic (self-similar) model for protoplanetary disk evolution that includes both viscous and MHD-wind evolution terms. It should be useful for population-level disk analyses. (1/9)\n\n<LINK> <LINK>', ""The new work generalizes the self-similar solution of Lynden-Bell and Pringle, which includes only a viscous term. Physically, this represents turbulent redistribution of disk angular momentum. It's known to be  incomplete. (2/9) \n\nhttps://t.co/MSbQQPAUZ7"", 'One reason is that the late-time evolution under pure viscous evolution is a power-law decline in quantities such as the stellar accretion rate. This is NOT what is inferred observationally. Disks disperse more sharply, a property known as ""two-timescale evolution"". (3/9)', ""Two-timescale evolution could be a consequence of photoevaporation. X-ray or UV radiation (from the central, or external stars) heats the disk surface, and the hot unbound gas escapes in a wind. It's observed in Orion (HST image: Bally, Throop, O'Dell). (4/9) https://t.co/UBU49LTQXI"", 'Cathie Clarke and collaborators showed that viscous + photoevaporative models show two-timescale behavior. The physics has been simulated in depth by groups led by @r_d_alexander, Barbara Ercolano, Uma Gorti, James Owen, and others. It happens. (5/9)\n\nhttps://t.co/WTit28moOQ https://t.co/ql06GptMCP', 'Photoevaporation, however, is probably only part of the story. Disks likely have significant net magnetic flux, which allows for mass AND angular momentum loss to occur via MHD winds. Thermal effects still matter, as in this simulation by Lile Wang. (6/9)\n\nhttps://t.co/RDdH8tLAmU https://t.co/vNLlUCO6II', ""With @jbsimon_astro and Rebecca Martin, we found previously that MHD wind-driven dispersal shows the observationally favored two-timescale behavior. But this, and most other groups' work, was numerical, and not trivial to use in data analysis. (7/9)\n\nhttps://t.co/vz8kaFGLZo"", ""Benoît's new solution does not encompass every physically possible wind (that would be hard, as MHD winds involve several weakly-constrained functions). But it's quite flexible. e.g. we discuss cases where the net flux is fixed, or decays with time. (8/9) https://t.co/j06dJ463qB"", 'We hope that the model will be useful for population-level analyses of protoplanetary disk data, which are potentially powerful with high completeness from ALMA surveys. Such an analysis led by Benoît is coming, soon, we hope! (9/9)']",https://arxiv.org/abs/2111.10145,"The evolution of protoplanetary discs and the related process of planet formation is regulated by angular momentum transport and mass-loss processes. Over the past decade, the paradigm of viscosity has been challenged and MHD disc winds appear as a compelling scenario to account for disc accretion. In this work, we aim to construct the equivalent of the widely used analytical description of viscous evolution for the MHD wind case. The transport of angular momentum and mass induced by the wind is parameterized by an $\alpha$-like parameter and by the magnetic lever arm parameter $\lambda$. Extensions of the paradigmatic Lynden-Bell and Pringle similarity solutions to the wind case are presented. We show that wind-driven accretion leads to a steeper decrease in the disc mass and accretion rate than in viscous models due to the absence of disc spreading. If the decline of the magnetic field strength is slower than that of the gas surface density, the disc is dispersed after a finite time. The evolution of the disc in the $\dot{M}_*-M_D$ plane is sensitive to the wind and turbulence parameters. A disc population evolving under the action of winds can exhibit a correlation between $\dot{M}_*$ and $M_D$ depending on the initial conditions. The simplified framework proposed in this work opens to a new avenue to test the effectiveness of wind-driven accretion from the observed disc demographics and constitutes an important step to include wind-driven accretion in planet population synthesis models. ","Secular evolution of MHD wind-driven discs: analytical solutions in the
  expanded {\alpha}-framework"
46,1461770260785229829,14328706,Weizhu Chen,"['Please check DeBERTaV3. We introduce a new gradient disentangled embedding sharing to improve pretraining efficiency and downstream tasks performance. Here is the paper: <LINK> and the models and code are available to improve your task.', 'https://t.co/iTuO83Mrxi', 'This is a joint work with @Pengcheng2020 and @JianfengGao0217.', 'This is a joint work with @Pengcheng2020 and @JianfengGao0217']",https://arxiv.org/abs/2111.09543,"This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the ""tug-of-war"" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at this https URL ","DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with
  Gradient-Disentangled Embedding Sharing"
47,1461640649795465220,234398193,Hal Tasaki,"['Hal Tasaki ""Rigorous Index Theory for One-Dimensional Interacting Topological Insulators""\nMy new paper.  Although limited to one symmetry class (class D) in 1 dim, we recover the desired Z_2 classification rigorously.  The proof is quite elementary.\n<LINK>']",https://arxiv.org/abs/2111.07335,We present a rigorous but elementary index theory for a class of one-dimensional systems of interacting fermions that includes the Su-Schrieffer-Heeger (SSH) model as a special case. We prove that the sign of the expectation value of the local twist operator gives a topological $\mathbb{Z}_2$ index for a unique gapped ground state on the infinite chain. This establishes that any path of interacting disordered models (in the class) that connects the two extreme cases of the SSH model must go through a phase transition. We also prove that any unique gapped ground state in the class is accompanied by a gapless edge mode when defined on a suitable half-infinite chain. ,"Rigorous Index Theory for One-Dimensional Interacting Topological
  Insulators"
48,1461609663187890180,40754053,stefano maria iacus,['Did #COVID19 equally affect men and women at work? New paper with S. Grubanov-Boskovic @spiros2 @FrancescoSermi U.Minora <LINK>'],https://arxiv.org/abs/2111.09442,"The COVID-19 pandemic has created a sudden need for a wider uptake of home-based telework as means of sustaining the production. Generally, teleworking arrangements impacts directly worker's efficiency and motivation. The direction of this impact, however, depends on the balance between positive effects of teleworking (e.g. increased flexibility and autonomy) and its downsides (e.g. blurring boundaries between private and work life). Moreover, these effects of teleworking can be amplified in case of vulnerable groups of workers, such as women. The first step in understanding the implications of teleworking on women is to have timely information on the extent of teleworking by age and gender. In the absence of timely official statistics, in this paper we propose a method for nowcasting the teleworking trends by age and gender for 20 Italian regions using mobile network operators (MNO) data. The method is developed and validated using MNO data together with the Italian quarterly Labour Force Survey. Our results confirm that the MNO data have the potential to be used as a tool for monitoring gender and age differences in teleworking patterns. This tool becomes even more important today as it could support the adequate gender mainstreaming in the ``Next Generation EU'' recovery plan and help to manage related social impacts of COVID-19 through policymaking. ","Monitoring COVID-19-induced gender differences in teleworking rates
  using Mobile Network Data"
49,1461602402868641796,10666172,Sabine Hossenfelder,"['We have a new paper, enjoy :) \n\n<LINK>', ""@Matzan481_ excellent question. I don't have an answer but we're working on another paper..."", '@PoppySarafan Excellent!']",https://arxiv.org/abs/2111.09347,"The Delayed-Choice Quantum Eraser experiment is commonly interpreted as implying that in quantum mechanics a choice made at one time can influence an earlier event. We here suggest an extension of the experiment that results in a paradox when interpreted using a local realist interpretation combined with backward causation (""retrocausality""). We argue that resolving the paradox requires giving up the idea that, in quantum mechanics, a choice can influence the past, and that it instead requires a violation of Statistical Independence without retrocausality. We speculate what the outcome of the experiment would be. ",The Quantum Eraser Paradox
50,1461570189619707905,1093715503762165760,Ilya Valmianski,"['Natural language generation in medical dialogue is challenging because of strict requirements for controllability and appropriateness. Check out our new paper on doing controlled generation for history taking. #GPT, #ML4H2021\n\n<LINK>']",https://arxiv.org/abs/2111.09381,"We present MEDCOD, a Medically-Accurate, Emotive, Diverse, and Controllable Dialog system with a unique approach to the natural language generator module. MEDCOD has been developed and evaluated specifically for the history taking task. It integrates the advantage of a traditional modular approach to incorporate (medical) domain knowledge with modern deep learning techniques to generate flexible, human-like natural language expressions. Two key aspects of MEDCOD's natural language output are described in detail. First, the generated sentences are emotive and empathetic, similar to how a doctor would communicate to the patient. Second, the generated sentence structures and phrasings are varied and diverse while maintaining medical consistency with the desired medical concept (provided by the dialogue manager module of MEDCOD). Experimental results demonstrate the effectiveness of our approach in creating a human-like medical dialogue system. Relevant code is available at this https URL ","MEDCOD: A Medically-Accurate, Emotive, Diverse, and Controllable Dialog
  System"
51,1461566108201365511,1070545175942959105,Tom McCoy,"['*NEW PREPRINT*\n\nNeural-network language models (e.g., GPT-2) can generate high-quality text. Are they simply copying text they have seen before, or do they have generalizable linguistic abilities?\n\nAnswer: Some of both!\n\nPaper: <LINK> \n\n1/n <LINK>', 'Work done with @tallinzen, Paul Smolensky, @JianfengGao0217, &amp; @real_asli.\n \nWe generate text from language models and then analyze whether the text is novel or duplicated from the training set. We analyze novelty for sequential structure (n-grams) and syntactic structure.\n \n2/n', 'In model-generated text, very few bigrams and trigrams are novel - i.e., most of them appear in the training set. But for 5-grams and larger, the majority are novel!\n\n3/n https://t.co/ees58DEh7Y', 'Models occasionally do copy very long passages. E.g., sometimes GPT-2 copies passages from the training set that are over 1,000 words long.\n\n4/n', 'Models also display syntactic novelty: the majority of generated sentences have a syntactic structure that no training sentence has.\n\n5/n https://t.co/a4EAVn9lMe', 'Across both n-grams and syntactic structures, we find that models are rarely novel at the small scale (small n-grams/individual dependencies), but are very novel at the medium-to-large scale (larger n-grams/overall sentence structure).\n\n6/n', 'We also conduct extensive manual analysis of the novel words that GPT-2 coins.\n\n7/n', 'Most of GPT-2’s novel words are morphologically well-formed; e.g., when forming novel plurals, it almost always chooses the correct suffix (-s vs. -es). But it does make occasional word-formation errors: novel acronyms often don’t quite match what they stand for.\n\n8/n https://t.co/HxAcfF5Hsx', 'Most novel words are used in syntactically-valid contexts. Left: When the subject of a sentence is a novel plural noun, the verb is also almost always plural. Right: GPT-2 also correctly shows other consequences of plurality, such as having coreferential pronouns be plural.\n\n9/n https://t.co/K01EEoAjML', 'Despite this success with morphology and syntax, novel words are often used in ways that do not make sense semantically (left), though there are some cases where GPT-2 essentially provides a clear and accurate definition of the word (right).\n\n10/n https://t.co/9B98u3AcJC', 'Our main message is that novelty has not received the attention it deserves in evaluation of text generation. See the paper for many other analyses and results! (And for an explanation of why the project is called RAVEN!) https://t.co/Ig6tbeHP6N \n\n11/n', 'A huge thanks to @OpenAI  for granting us access to the WebText dataset, to @huggingface for making it so easy to use Transformers, to @marcc_systems for computing resources, and to the community at @MSFTResearch, where this project got started as my internship project.\n\n12/n', 'Finally, thank you to @real_asli, who has been a fantastic mentor throughout this whole process: both during the internship where the project got started, and for many months after. If you get a chance to work with her, you should definitely take it!\n\nn/n', '@LChoshen Yes, exactly! That\'s what ""Baseline: Test set"" is in each plot.', '@LChoshen I think that the way you phrased it is better in fact! (""less/more novel than we expect"")', '@begusgasper @real_asli Exactly!', '@m_guerini @serrasinem @helena_bonaldi Sounds awesome! Do you have a write-up of this that you can share?', ""@m_guerini @serrasinem @helena_bonaldi Thank you, I'll check it out!"", ""@alethioguy @LukeZettlemoyer @yuvalpi I'm a big fan of that paper! We briefly talk about blends in the appendix: https://t.co/HB61FqaQz0"", ""@BlancheMinerva Not available yet, but hopefully soon! (Just need to get approval from some of the authors' employers before we can release)."", ""@BlancheMinerva The amount of work varies by the analysis. Assuming you already have code to generate text, then:\n- The n-gram analyses (Section 5) are easy to rerun, but can be time-consuming (2-3 days) if your model's training data is huge. You can save a lot of time by skipping some analyses"", ""@BlancheMinerva - The syntax analyses (Section 6) require you to parse your training data, which we provide code for but is time-consuming. Once you have parses, it's a similar time commitment as the n-gram analyses."", ""@BlancheMinerva - The manual analyses in Section 7 are time-consuming since they're entirely manual."", '@BlancheMinerva For a bit more context: Analyzing models trained on Wikitext-103 (100 million words) takes a few hours. Analyzing models trained on WebText (8 billion words) takes 2-3 days, all done on CPUs. The slowness is due to the need to iterate over the training sets.']",https://arxiv.org/abs/2111.09509,"Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure - e.g., individual dependencies - model-generated text is substantially less novel than our baseline of human-generated text from each model's test set. For larger-scale structure - e.g., overall sentence structure - model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory). ","How much do language models copy from their training data? Evaluating
  linguistic novelty in text generation using RAVEN"
52,1461513687127134209,1093387119148462081,Daniel Green,"['New paper with Tim Cohen and Akhil Premukar, A Tail of Eternal Inflation:\n\n<LINK>\n\nWe find the phase transition to eternal inflation is incalculable for seemingly very weak non-Gaussianty.  This has implications for Stochastic Inflation and the de Sitter Entropy <LINK>', 'We calculate the non-Gaussian (higher derivative) corrections to Stochastic Inflation and find it naively predicts eternal inflation could happen in our universe.\n\nWe interpret this as a breakdown of Stochastic Inflation when calculating the tail of the probability distribution', 'This was a surprising result to me.  I had expected weak non-Gaussianity at horizon crossing would mean these corrections are small.  It turns out the tail tests physics inside the horizon where the EFT of Inflation breaks down.  This might also be important for PBH formation.']",https://arxiv.org/abs/2111.09332,"Non-trivial inflaton self-interactions can yield calculable signatures of primordial non-Gaussianity that are measurable in cosmic surveys. Surprisingly, we find that the phase transition to slow-roll eternal inflation is often incalculable in the same models. Instead, this transition is sensitive to the non-Gaussian tail of the distribution of scalar fluctuations, which probes physics inside the horizon, potentially beyond the cutoff scale of the Effective Field Theory of Inflation. We demonstrate this fact directly by calculating non-Gaussian corrections to Stochastic Inflation within the framework of Soft de Sitter Effective Theory, from which we derive the associated probability distribution for the scalar fluctuations. We find parameter space consistent with current observations and weak coupling at horizon crossing in which the large fluctuations relevant for eternal inflation can only be determined by appealing to a UV completion. We also show this breakdown of the perturbative description is required for the de Sitter entropy to reflect the number of de Sitter microstates. ",A Tail of Eternal Inflation
53,1461377877018963968,1319131066259521536,Michael Auli,"['XLS-R is our new model for cross-lingual self-supervised speech learning trained on 436K hours of public speech in 128 languages and with up to 2B params.\n\nPaper: <LINK>\nBlog: <LINK>\nCode/models: \n<LINK>\n<LINK> <LINK>', ""On speech translation it improves prior work on CoVoST-2 by 7.4 BLEU on average for 21 translation directions into English. The largest improvements are on low-resource language directions. See Hugging Face's speech translation demo here: https://t.co/sBfa8RDxdJ"", 'And on speech recognition, the model reduces error rates of comparable prior work by between 14-34% relative on average - across four ASR benchmarks and 37 languages. There are improvements of up to 6.8 WER on some of the BABEL languages we experimented on. https://t.co/r3onOduuUJ', 'Arun Babu @ChanghanWang Andros Tjandra @hikushalhere @QiantongX  @NamanGoyal21 Kritika Singh @PatrickPlaten Yatharth Saraf @juanmiguelpino  @ZloiAlexei @alex_conneau']",https://arxiv.org/abs/2111.09296,"This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world. ","XLS-R: Self-supervised Cross-lingual Speech Representation Learning at
  Scale"
54,1461363725361811459,773137323907223552,katie breivik,"['New software paper from @tomjwagg, myself, and Selma de Mink on arXiv today: <LINK>!\nIf you’ve ever tried to calculate the average LISA signal to noise ratio for binary systems, you know there are A LOT of opportunities for missed factors of 2 or 10/3 or 98/5. <LINK>', 'We ran into this and rederived the SNR so many times that we said ENOUGH! Enter LEGWORK. A python package that does the legwork (😎) for calculating lowest order post-Newtonian gravitational wave evolution of binary populations and SNRs for GW detectors like @LISACommunity.', '@tomjwagg is an absolute visionary when it comes to visualization, demos, and tutorials; so please do check them out here https://t.co/iNAxUGqYEE\nAnd of course, let us know if you run into any trouble!', 'Finally, keep an eye out for a couple of papers in the next few days on the projects that inspired us to put LEGWORK together. \n👀👀👀 (spoiler: get hype for some compact binary populations y’all!!)', '@AstrophysicalAC @tomjwagg So glad you think so! I have definitely found myself SO frustrated chasing derivations that I figured it was worth the package just for that!', '@LISACommunity We are heavily stellar-origin binary biased so we really only do the lowest-order PN stuff (think sine waves with an amplitude modulation).', '@LISACommunity BUT! we optimized the code so that you can compute literally TENS-OF-MILLIONS of SNRS in just a couple of mins! So you win some you lose some (a lot of) LISA sources 🙃', ""OMG I also forgot my absolute FAVORITE part of the paper itself:\nWe used @rodluger's showyourwork package (https://t.co/4LNGL6q85T), so you can see *exactly* what code was used to compile the figures in the paper. You can see the paper source code here: https://t.co/zhG48TziuS"", '@duetosymmetry @tomjwagg Hahahaha I’m so glad you caught my Easter egg joke! It does prove the point!!', '@duetosymmetry @tomjwagg You (and everyone else!) can do:\n&gt; pip install legwork\n\nCongratulations!! 🥳', '@AstroVivi @exoplaneteer @tomjwagg Thanks Vivi!!']",https://arxiv.org/abs/2111.08717,"We present LEGWORK (LISA Evolution and Gravitational Wave Orbit Kit), an open-source Python package for making predictions about stellar-origin gravitational wave sources and their detectability in LISA or other space-based gravitational wave detectors. LEGWORK can be used to evolve the orbits of sources due to gravitational wave emission, calculate gravitational wave strains (using post-Newtonian approximations), compute signal-to-noise ratios and visualise the results. It can be applied to a variety of potential sources, including binaries consisting of white dwarfs, neutron stars and black holes. Although we focus on double compact objects, in principle LEGWORK can be used for any system with a user-specified orbital evolution, such as those affected by a third object or gas drag. We optimised the package to make it efficient for use in population studies which can contain tens-of-millions of sources. This paper describes the package and presents several potential use cases. We explain in detail the derivations of the expressions behind the package as well as identify and clarify some discrepancies currently present in the literature. We hope that LEGWORK will enable and accelerate future studies triggered by the rapidly growing interest in gravitational wave sources. ","LEGWORK: A python package for computing the evolution and detectability
  of stellar-origin gravitational-wave sources with space-based detectors"
55,1461362891899105282,502318662,Emmanuel Bengio,"['New paper laying down the foundations of GFlowNet, our new DNN-driven probabilistic framework. By Yoshua Bengio, @TristanDeleu, @edwardjhu, Salem Lahou, @mo_tiwari, &amp; myself\n<LINK>\nEven though this framework emerged from our desire to sample novel drugs /1', 'it turns out that we can do much more with it: general probabilistic operations on sets &amp; graphs, such as otherwise intractable marginalizations, estimating partition functions and free energies, conditional probabilities of superset given subsets, estimating entropy or 2/', 'mutual information, and more. As Yoshua puts it, it ""may become a key ingredient for system-2 deep learning enabling causal discovery and reasoning"".\nOf course much of these properties need to be shown to work empirically, although we\'re pretty excited by existing results, 3/', ""in particular, our NeurIPS paper, https://t.co/rYflzGPnbm, has shown pretty amazing generative capabilities in a small-drug-molecule setting.\nI'm excited for what's to come! 🎉 4/4""]",https://arxiv.org/abs/2111.09266,"Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of GFlowNets. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. GFlowNets amortize the work typically done by computationally expensive MCMC methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions. ",GFlowNet Foundations
56,1461295552767922177,1295978398557319168,Tom McGrath,"[""I've been looking forward to sharing this for a while - we've just put up a new paper on the arxiv looking at the human concepts that AlphaZero learns during training: <LINK>"", 'We can probe for all sorts of chess concepts at each layer and throughout training, mapping out when and where things get learned https://t.co/oXND7HLfVL', 'We also take a look at how AlphaZero learns opening theory and compare it to human history - some surprising similarities and differences! https://t.co/q5SjlRz4g9', ""We also took a look more closely at the network's internals - you can see a whole range of interesting things getting computed block by block. All the factors are online here: https://t.co/yP2IzSPArA - would be interesting to see what others can find! https://t.co/IOL9XIpLSd""]",https://arxiv.org/abs/2111.09259,"What is learned by sophisticated neural network agents such as AlphaZero? This question is of both scientific and practical interest. If the representations of strong neural networks bear no resemblance to human concepts, our ability to understand faithful explanations of their decisions will be restricted, ultimately limiting what we can achieve with neural network interpretability. In this work we provide evidence that human knowledge is acquired by the AlphaZero neural network as it trains on the game of chess. By probing for a broad range of human chess concepts we show when and where these concepts are represented in the AlphaZero network. We also provide a behavioural analysis focusing on opening play, including qualitative analysis from chess Grandmaster Vladimir Kramnik. Finally, we carry out a preliminary investigation looking at the low-level details of AlphaZero's representations, and make the resulting behavioural and representational analyses available online. ",Acquisition of Chess Knowledge in AlphaZero
57,1461271030757044226,3306943245,Konstantin Klemmer,['New paper out in @itssieee Intelligent Transportation Systems🤖🚘🚴\u200d♀️\n\nWe propose a simulation environment for finding optimal deployment plans for shared e-mobility systems using multi-agent deep neural search. \n\nCheck the paper here: <LINK> <LINK>'],https://arxiv.org/abs/2111.02149,"Shared e-mobility services have been widely tested and piloted in cities across the globe, and already woven into the fabric of modern urban planning. This paper studies a practical yet important problem in those systems: how to deploy and manage their infrastructure across space and time, so that the services are ubiquitous to the users while sustainable in profitability. However, in real-world systems evaluating the performance of different deployment strategies and then finding the optimal plan is prohibitively expensive, as it is often infeasible to conduct many iterations of trial-and-error. We tackle this by designing a high-fidelity simulation environment, which abstracts the key operation details of the shared e-mobility systems at fine-granularity, and is calibrated using data collected from the real-world. This allows us to try out arbitrary deployment plans to learn the optimal given specific context, before actually implementing any in the real-world systems. In particular, we propose a novel multi-agent neural search approach, in which we design a hierarchical controller to produce tentative deployment plans. The generated deployment plans are then tested using a multi-simulation paradigm, i.e., evaluated in parallel, where the results are used to train the controller with deep reinforcement learning. With this closed loop, the controller can be steered to have higher probability of generating better deployment plans in future iterations. The proposed approach has been evaluated extensively in our simulation environment, and experimental results show that it outperforms baselines e.g., human knowledge, and state-of-the-art heuristic-based optimization approaches in both service coverage and net revenue. ","Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search"
58,1461254223614648324,1224994747846098944,Diego Calderón,['New paper led by @pejco3am on observational signatures of a supernova hitting a wind-collision slab in a binary system. \nThe animation shows the hydro simulation of a stellar wind collision of stars with unequal winds \n<LINK> <LINK> <LINK>'],https://arxiv.org/abs/2111.08039,"When a core-collapse supernova explodes in a binary star system, the ejecta might encounter an overdense shell, where the stellar winds of the two stars previously collided. In this work, we investigate effects of such interactions on supernova light curves on time-scales from the early flash ionization signatures to approximately one year after the explosion. We construct a model of the colliding-wind shell in an orbiting binary star system and we provide an analytical expression for the shell thickness and density, which we calibrate with three-dimensional adaptive mesh refinement hydrodynamical simulations probing different ratios of wind momenta and different regimes of radiative cooling efficiency. We model the angle-dependent interaction of supernova ejecta with the circumstellar medium and estimate the shock radiative efficiency with a realistic cooling function. We find that the radiated shock power exceeds typical Type IIP supernova luminosity only for double red supergiant binaries with mass ratios $q \gtrsim 0.9$, wind mass-loss rates $\dot{M} \gtrsim 10^{-4} M_\odot\,\text{yr}^{-1}$, and separations between about 50 and 1500 AU. The required $\dot{M}$ increases for binaries with smaller $q$ or primaries with faster wind. We estimate that $\ll 1\%$ of all collapsing massive stars satisfy the conditions on binary mass ratio and separation. Recombination luminosities due to colliding wind shells are at most a factor of 10 higher than for an otherwise unperturbed constant-velocity wind, but higher densities associated with wind acceleration close to the star provide much stronger signal. ","Supernovae in colliding-wind binaries: observational signatures in the
  first year"
59,1461250685979267078,998517830235643904,Riccardo Barbieri,"['New paper out! \n\nIf you like breaking things, check it out here <LINK>\n\nAnd if you like pretty astro plots, check out this thread by Becca and her youtube channel for even prettier videos! <LINK>']",https://arxiv.org/abs/2111.08065,"The inspiral of supermassive black-hole binaries in gas-rich environment is driven by the presence of an accretion disc and viscous interactions tend to align the spin of the black holes with the orbital angular momentum of the disc. Recent work introduced a new iterative approach to describe the alignment process and the resulting non-linear evolution of the surrounding warped accretion disc. Their model predicted that black-hole spins reach either full alignment or a critical obliquity where solutions to the warp equations cease to exist. In this paper, we show that this critical region corresponds to the disc breaking phenomenon, where the disc is disrupted into two or more discrete sections. We use 3D hydrodynamical simulations to (i) recover the predictions of the semi-analytic model and (ii) unveil a richer phenomenology where the disc exhibits either unsuccessful, single and multiple breaks. We additionally identify hydrodynamic effects such as spiral arms that are able to stabilise the disc against breaking beyond criticality. Our results show that when disc breaking occurs, the ability of black holes and disc to align is compromised and in some cases even prevented as the binary inspirals. ","The Bardeen-Petterson effect in accreting supermassive black-hole
  binaries: disc breaking and critical obliquity"
60,1461198274166071302,1191386359707029505,Animesh Mukherjee,['New paper: Two-Face: Adversarial Audit of Commercial Face Recognition Systems. @ICWSM 2022.\n\nThis work extends the #gendershades paper by @jovialjoy and @timnitGebru to noisy facial images. Disparities increase manifolds!\n\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2111.09137,"Computer vision applications like automated face detection are used for a variety of purposes ranging from unlocking smart devices to tracking potential persons of interest for surveillance. Audits of these applications have revealed that they tend to be biased against minority groups which result in unfair and concerning societal and political outcomes. Despite multiple studies over time, these biases have not been mitigated completely and have in fact increased for certain tasks like age prediction. While such systems are audited over benchmark datasets, it becomes necessary to evaluate their robustness for adversarial inputs. In this work, we perform an extensive adversarial audit on multiple systems and datasets, making a number of concerning observations - there has been a drop in accuracy for some tasks on CELEBSET dataset since a previous audit. While there still exists a bias in accuracy against individuals from minority groups for multiple datasets, a more worrying observation is that these biases tend to get exorbitantly pronounced with adversarial inputs toward the minority group. We conclude with a discussion on the broader societal impacts in light of these observations and a few suggestions on how to collectively deal with this issue. ",Two-Face: Adversarial Audit of Commercial Face Recognition Systems
61,1461165819895828480,978189226146607104,Drew A Hudson,"[""Excited to introduce the GANformer2 model for Recurrent and Compositional Scene Generation!\n\nOur new NeurIPS paper is available at <LINK> \n\nAnd stay tuned for the model's implementation at <LINK> 🎉 <LINK>""]",https://arxiv.org/abs/2111.08960,"We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See this https URL for model implementation. ",Compositional Transformers for Scene Generation
62,1461106184082046980,1425048932506345482,Clecio R. Bom,['Our new arxiv paper on Hubble constant constraint using standard sirens from Gravitational Waves and Legacy survey is out! <LINK>'],https://arxiv.org/abs/2111.06445,"We present a new constraint on the Hubble constant $H_0$ using a sample of well-localized gravitational wave (GW) events detected during the first three LIGO/Virgo observing runs as dark standard sirens. In the case of dark standard sirens, a unique host galaxy is not identified, and the redshift information comes from the distribution of potential host galaxies. From the third LIGO/Virgo observing run detections, we add the asymmetric-mass binary black hole GW190412, the high-confidence GW candidates S191204r, S200129m, and S200311bg to the sample of dark standard sirens analyzed. Our sample contains the top $20\%$ (based on localization) GW events and candidates to date with significant coverage by the Dark Energy Spectroscopic Instrument (DESI) Legacy Survey. We combine the $H_0$ posterior for eight dark siren events, finding $H_0 = 79.8^{+19.1}_{-12.8}~{\rm km~s^{-1}~Mpc^{-1}}$ ($68\%$ Highest Density Interval) for a prior in $H_0$ uniform between $[20,140]~{\rm km~s^{-1}~Mpc^{-1}}$. This result shows that a combination of 8 well-localized dark sirens combined with an appropriate galaxy catalog is able to provide an $H_0$ constraint that is competitive ($\sim 20\%$ versus $18\%$ precision) with a single bright standard siren analysis (i.e. assuming the electromagnetic counterpart) using GW170817. When combining the posterior with that from GW170817, we obtain $H_0 = 72.77^{+11.0}_{-7.55}~{\rm km~s^{-1}~Mpc^{-1}}$. This result is broadly consistent with recent $H_0$ estimates from both the Cosmic Microwave Background and Supernovae. ","A standard siren measurement of the Hubble constant using gravitational
  wave events from the first three LIGO/Virgo observing runs and the DESI
  Legacy Survey"
63,1460977336023719937,824515986472652800,Vinitra Swamy,"['Excited to be presenting our new paper on ""Interpreting Language Models Through Knowledge Graph Acquisition"" at the NeurIPS inaugural eXplainable AI Workshop this year! This work is a collaboration with Angelika Romanou and Martin Jaggi.\n<LINK>\n@aggr0m @ICepfl']",http://arxiv.org/abs/2111.08546,"Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze ""fill-in-the-blank"" statements at various stages of RoBERTa's early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models' behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance. ",Interpreting Language Models Through Knowledge Graph Extraction
64,1460902680495280133,51700215,Phil Bull,['New paper! In which Mel Irfan has heroically pruned the junk out of some pathfinder intensity mapping data from MeerKAT in order to measure the Galactic synchrotron SED over several decades in frequency  <LINK>'],https://arxiv.org/abs/2111.08517,"21cm intensity mapping experiments are bringing an influx of high spectral resolution observational data in the $\sim100$ MHz $- 1$ GHz regime. We use pilot $971-1075$ MHz data from MeerKAT in single-dish mode, recently used to test the calibration and data reduction scheme of the upcoming MeerKLASS survey, to probe the spectral index of diffuse synchrotron emission below 1 GHz within $145^{\circ} < \alpha < 180^{\circ}$, $-1^{\circ} < \delta < 8^{\circ}$. Through comparisons with data from the OVRO Long Wavelength Array and the Maipu and MU surveys, we find an average spectral index of $-2.75 < \beta < -2.71$ between 45 and 1055 MHz. By fitting for spectral curvature with a spectral index of the form $\beta + c \, {\rm{ln}}(\nu / 73~{\rm MHz})$, we measure $\beta = -2.55 \pm 0.13$ and $c = -0.12 \pm 0.05$ within our target field. Our results are in good agreement (within $1\sigma$) with existing measurements from experiments such as ARCADE2 and EDGES. These results show the calibration accuracy of current data and demonstrate that MeerKLASS will also be capable of achieving a secondary science goal of probing the interstellar medium. ","Measurements of the diffuse Galactic synchrotron spectral index and
  curvature from MeerKLASS pilot data"
65,1460790714137149444,321794593,José G. Fernández-Trincado,['Our new accepted paper tonight on ArXiv 👉🏻 “Inspection of 19 globular cluster candidates in the Galactic bulge with the VVV survey” by @ElisaGarro1 👉🏻 <LINK>'],https://arxiv.org/abs/2111.08317,"The census of the globular clusters (GCs) in the Milky Way (MW) is still a work in progress. We explore the nature of 19 new GC candidates in the Galactic bulge, based on the analysis of their colour-magnitude diagrams (CMDs) in the near-IR, using the VISTA Variables in the Via L\'actea Survey (VVV) database. We estimate their main astrophysical parameters: reddening and extinction, distance, total luminosity, mean cluster proper motions (PMs), metallicity and age. We obtain the cluster catalogues including the likely cluster members by applying a decontamination procedure on the observed CMDs, based upon the vector PM diagrams from VIRAC2. We estimate a wide reddening range of the $0.25 \leqslant E(J-K_s) \leqslant 2.0$ mag and extinction $0.11 \leqslant A_{Ks} \leqslant 0.86$ mag for the sample clusters as expected in the bulge regions. The range of heliocentric distances is $6.8\leqslant D\leqslant 11.4$ kpc. This allows us to place these clusters between 0.56 and 3.25 kpc from the Galactic centre, assuming $R_{\odot}=8.2$ kpc. Also, their PMs are kinematically similar to the typical motion of the Galactic bulge, apart from VVV-CL160, which shows different PMs. We also derive their metallicities and ages, finding $-1.40 \leqslant$ [Fe/H] $\leqslant 0.0$ dex and $t\approx 8-13$ Gyr respectively. The luminosities are calculated both in $K_{s}-$ and V-bands, recovering $-3.4 \leqslant M_V \leqslant -7.5$. We also examine the possible RR Lyrae members found in the cluster fields. Based on their positions, kinematics, metallicities and ages and comparing our results with the literature, we conclude that 9 candidates are real GCs, 7 need more observations to be fully confirmed as GCs, whereas 3 candidates are discarded for being younger open clusters. ","Inspection of 19 globular cluster candidates in the Galactic bulge with
  the VVV survey"
66,1460774809114136577,1254948077825294336,Mahdi Haghifam,"[""🔥 New paper 🔥on @shortstein and @zakynthinou's CMI framework, demonstrating its unifying nature for obtaining optimal or near-optimal bounds for the expected excess risk in the realizable setting.\n<LINK>\nWill be a spotlight at NeurIPS’21! <LINK>"", 'Different frameworks for proving generalization are not compatible. For instance, the minmax learnability of thresholds cannot be established using input-output MI, PAC-Bayes bounds, and privacy frameworks. How expressive is CMI in terms of explaining generalization?', 'Some results\n1- The CMI framework yields optimal risk bound for sample compression schemes. Covers many algorithms, including SVMs.\n2- A broad class of proper ERMs achieves CMI of order O(1) (without suboptimal log n factor) for learning VC classes with finite star number.', '3. Steinke and Zakynthinou introduced a variant of CMI called “evaluated"" CMI (eCMI). We show that, for every interpolating algorithm and data distribution, the expected risk vanishes as the number of samples (n) diverges if and only if its eCMI has sublinear growth with n.', 'Joint work with @kdziugaite, Shay Moran, and @roydanroy.', '@lzamparo @shortstein @zakynthinou Thanks!']",https://arxiv.org/abs/2111.05275,"In this work, we investigate the expressiveness of the ""conditional mutual information"" (CMI) framework of Steinke and Zakynthinou (2020) and the prospect of using it to provide a unified framework for proving generalization bounds in the realizable setting. We first demonstrate that one can use this framework to express non-trivial (but sub-optimal) bounds for any learning algorithm that outputs hypotheses from a class of bounded VC dimension. We prove that the CMI framework yields the optimal bound on the expected risk of Support Vector Machines (SVMs) for learning halfspaces. This result is an application of our general result showing that stable compression schemes Bousquet al. (2020) of size $k$ have uniformly bounded CMI of order $O(k)$. We further show that an inherent limitation of proper learning of VC classes contradicts the existence of a proper learner with constant CMI, and it implies a negative resolution to an open problem of Steinke and Zakynthinou (2020). We further study the CMI of empirical risk minimizers (ERMs) of class $H$ and show that it is possible to output all consistent classifiers (version space) with bounded CMI if and only if $H$ has a bounded star number (Hanneke and Yang (2015)). Moreover, we prove a general reduction showing that ""leave-one-out"" analysis is expressible via the CMI framework. As a corollary we investigate the CMI of the one-inclusion-graph algorithm proposed by Haussler et al. (1994). More generally, we show that the CMI framework is universal in the sense that for every consistent algorithm and data distribution, the expected risk vanishes as the number of samples diverges if and only if its evaluated CMI has sublinear growth with the number of samples. ",Towards a Unified Information-Theoretic Framework for Generalization
67,1460690694687772672,191645855,Steve Yadlowsky,"['New paper on evaluating treatment prioritization rules w/ @_scott_fleming_ @drnigam @EmmaBrunskill and Stefan Wager. We define a class of ⛳️RATE metrics⛳️, measuring how well the rule ranks according to treatment effect, and quantify statistical behavior. <LINK> <LINK>', 'RATE metrics generalize a number of existing metrics for evaluating CATE estimators, such as the Qini coefficient. However, we emphasize that any prioritization rule can be evaluated with this metric, including baseline risk-based models.', 'We show how RATE metrics allow one to find higher power metrics for evaluation than the Qini coefficient, and how we can reduce the variance of estimation using AIPW scores, even in randomized trials.', ""Finally, we use these metrics in a few applications where treatment prioritization is important (personalized medicine and digital advertising), to demonstrate how to evaluate CATE-based and baseline risk-based models' ability to predict treatment effect heterogeneity.""]",https://arxiv.org/abs/2111.07966,"There are a number of available methods that can be used for choosing whom to prioritize treatment, including ones based on treatment effect estimation, risk scoring, and hand-crafted rules. We propose rank-weighted average treatment effect (RATE) metrics as a simple and general family of metrics for comparing treatment prioritization rules on a level playing field. RATEs are agnostic as to how the prioritization rules were derived, and only assesses them based on how well they succeed in identifying units that benefit the most from treatment. We define a family of RATE estimators and prove a central limit theorem that enables asymptotically exact inference in a wide variety of randomized and observational study settings. We provide justification for the use of bootstrapped confidence intervals and a framework for testing hypotheses about heterogeneity in treatment effectiveness correlated with the prioritization rule. Our definition of the RATE nests a number of existing metrics, including the Qini coefficient, and our analysis directly yields inference methods for these metrics. We demonstrate our approach in examples drawn from both personalized medicine and marketing. In the medical setting, using data from the SPRINT and ACCORD-BP randomized control trials, we find no significant evidence of heterogeneous treatment effects. On the other hand, in a large marketing trial, we find robust evidence of heterogeneity in the treatment effects of some digital advertising campaigns and demonstrate how RATEs can be used to compare targeting rules that prioritize estimated risk vs. those that prioritize estimated treatment benefit. ","Evaluating Treatment Prioritization Rules via Rank-Weighted Average
  Treatment Effects"
68,1460609190569861121,1637831695,Aaron Vincent,"['New paper out - this one has been quite a journey. <LINK> 1/n', 'If weakly interacting particles can get stuck in a star (by its gravitational field), they can transport heat thanks to their long mean free path. Actually modelling this is a pain: a 6d integro-differential equation, aka the Boltzmann Equation', ""To solve the BE you need to approximate. Two schemes were developed in the 80's to try and understand how these dark matter particles transport heat. They both work in different regimes and are very incompatible https://t.co/RuwRCNMkES"", 'The isothermal (weak coupling) formalism was developed first, but later shown to overestimate heat transport. The local (LTE) formalism was developed in 1990, and with some little corrections, has been considered the ""correct"" approach ever since. https://t.co/9ezWGsffh5', 'Both approaches break down when you need them most: at the ""Knudsen transition"", when heat transport is optimal. https://t.co/GLExOEATvu', 'So, how to solve this conundrum? Monte Carlo! https://t.co/YnyobTXhNm', 'Our awesome students Hannah and Siyam developed a simulation of heat transport in stars, looking at half a dozen different types of particle interactions, across many dark matter masses and coupling strengths. Thousands of CPU hours later...', ""The isothermal approach is wrong... in a very predictable way. The 'correct' approach works well... until you get weak or non-standard interactions, then fails spectacularly. The solution? A very simple  'corrected' isothermal model (blue) https://t.co/0JWFifCfMM"", '...which works across all cross sections, interaction types and masses. Peak luminosities from simulations shown here, with lines showing the predictions from our new phenomenological approach. Legends show the scaling of the cross section due to eg a different mediator particle. https://t.co/IahyC1039F', 'So the original approach turns out to be much more robust, given a correction so that it scales correctly with the mean free path. https://t.co/dlhyRBeM7r', '...and an extra factor of 2, which was also noticed in 1990. I have no idea where that factor of 2 is from. It will haunt me forever.', 'Thanks to my wonderful collaborators, especially to the brilliant Hannah Banks, who wrote and then ran ALL THE SIMULATIONS.']",https://arxiv.org/abs/2111.06895,"Asymmetric dark matter (ADM) that is captured in stars can act as an efficient conductor of heat. Small ADM-induced changes in a star's temperature gradient are known to alter neutrino fluxes and asteroseismological signatures, erase convective cores and modify a star's main sequence lifetime. The Sun's proximity to us makes it an ideal laboratory for studying these effects. However, the two formalisms commonly used to parametrize such heat transport were developed over 30 years ago, and calibrated with a single set of simulations. What's more, both are based on assumptions that break down at the Knudsen transition, where heat transport is maximized. We construct a Monte Carlo simulation to exactly solve the Boltzmann collision equation, determining the steady-state distribution and luminosity carried in stars by ADM with cross sections that depend on velocity and momentum. We find that, although the established (Gould and Raffelt) formalism based on local thermal equilibrium does well for constant cross sections, the isothermal (Spergel and Press) method actually performs better across all models with a simple, universal rescaling function. Based on simulation results, we provide recommendations on the parametrization of DM heat transport in stellar evolution models. ",Simulation of energy transport by dark matter scattering in stars
69,1460588022525792257,952949678533849088,Kareem El-Badry,"['New paper! We show that an 11 solar-mass black hole discovered in the LMC last week is not a black hole at all, but an undermassive stripped star orbiting a rapidly rotating star. <LINK> 1/n <LINK>', 'Deja vu? Yes, this is the same scenario that (we argue)  explains two other recently demoted BH candidates, LB-1 and HR 6819. 2/ https://t.co/DLR3dnrISL', 'The argument is very simple. The visible star in this system is tidally distorted. That, together with the known orbital period, tell us its density. We know its radius (roughly) , so this tells us its mass. 3/ https://t.co/5WROCyuefN', 'The mass ends up being about 1 solar mass, *much* less than expected for a normal star of this radius and temperature (5 solar masses, as assumed in the discovery paper). This is the stripped core of a star, not a normal star. 4/ https://t.co/uqdRAHw8g3', ""With the visible star's mass reduced, the implied companion mass drops, too. A normal-star companion becomes fully consistent with the data. 5/ https://t.co/07oOFjAS0V"", ""These stripped-star binaries are neat tests of binary evolution models, but they're nefarious black hole imposters. People hunting for black holes should be on the lookout for them going forward! 6/6""]",https://arxiv.org/abs/2111.07925,"We show that the radial velocity-variable star in the black hole candidate NGC 1850 BH1 cannot be a normal $\approx 5\,M_{\odot}$ subgiant, as was proposed, but is an overluminous stripped-envelope star with mass $\approx 1 M_{\odot}$. The result follows directly from the star's observed radius and the orbital period -- density relation for Roche lobe-filling stars: the star's density, as constrained by the observed ellipsoidal variability, is too low for its mass to exceed $\approx 1.5\,M_{\odot}$. This lower mass significantly reduces the implied mass of the unseen companion and qualitative interpretation of the system, such that a normal main-sequence companion with mass $(2.5-5)\,M_{\odot}$ is fully consistent with the data. We explore evolutionary scenarios that could produce the binary using MESA and find that its properties can be matched by models in which a $\sim5\,M_{\odot}$ primary loses most of its envelope to a companion and is observed in a bloated state before contracting to become a core helium burning sdOB star. This is similar to the scenario proposed to explain the binaries LB-1 and HR 6819. Though it likely does not contain a black hole, NGC 1850 BH1 provides an interesting test case for binary evolution models, particularly given its membership in a cluster of known age. ","NGC 1850 BH1 is another stripped-star binary masquerading as a black
  hole"
70,1460558815963885570,3236251346,Mikel Sanz,"['New #ArXiv paper on quantum algorithms to upload functions <LINK> Inspired by Grover-Rudolph, we first show that any function with a bounded second logarithmic derivative can be uploaded up to a controllable error in the fidelity with a constant number of gates', 'This is a nice example of the power of approximate quantum algorithms, i.e. algorithms performing a task up to certain controllable errors. Additionally, based on this result, we also propose a variational ansatz to upload function beyond the regularity conditions of the theorem https://t.co/NgO1GOFxvG', 'This means with zeros and singularities. We provide an ansatz and the minimal number of hyperparameters capturing the main characteristics of the function. More, GR provided suitable initial training values allowing for a fast convergence avoiding local minima and barren plateaus https://t.co/bSQyY53eGh', 'This is the brilliant Master thesis of Gabriel Marín and the great work of Javier González-Conde (@Conzavin). Congrats, it’s a pleasure to work with you!']",http://arxiv.org/abs/2111.07933,"Loading classical data into quantum computers represents an essential stage in many relevant quantum algorithms, especially in the field of quantum machine learning. Therefore, the inefficiency of this loading process means a major bottleneck for the application of these algorithms. Here, we introduce two approximate quantum-state preparation methods inspired by the Grover-Rudolph algorithm, which partially solve the problem of loading real functions. Indeed, by allowing for an infidelity $\epsilon$ and under certain smoothness conditions, we prove that the complexity of Grover-Rudolph algorithm can be reduced from $\mathcal{O}(2^{n})$ to $\mathcal{O}(2^{k_0(\epsilon)})$, with $n$ the number of qubits and $k_0(\epsilon)$ asymptotically independent of $n$. This leads to a dramatic reduction in the number of required two-qubit gates. Aroused by this result, we also propose a variational algorithm capable of loading functions beyond the aforementioned smoothness conditions. Our variational ansatz is explicitly tailored to the landscape of the function, leading to a quasi-optimized number of hyperparameters. This allows us to achieve high fidelity in the loaded state with high speed convergence for the studied examples. ",Quantum algorithms for approximate function loading
71,1460442818120273924,594183280,Johnny Dorigo Jones,"['Click here to see my new paper about the brightest flashlights in space: <LINK>', 'Thanks to @SeanDJohnson4 for guiding me through my post-bacc stage and for thinking through the proximity effect with me a hundred times']",https://arxiv.org/abs/2111.06927,"Blazars are some of the brightest UV and X-ray sources in the sky and are valuable probes of the elusive warm-hot intergalactic medium (WHIM; $T{\simeq} 10^5-10^7$ K). However, many of the brightest blazars$-$called BL Lac objects such as 1ES1553+113$-$have quasi-featureless spectra and poorly constrained redshifts. Here, we significantly improve the precision of indirect redshift constraints for blazars based on the edge of the $\rm{H\,I}$ Ly$\alpha$ forest observed in their UV spectra. We develop a robust technique to constrain the redshift of a $z<0.5$ AGN or blazar with a $1\sigma$ uncertainty of ${\approx}0.01$ using only the position of its highest-redshift Ly$\alpha$ absorber with $\log N_{\rm{H\,I}}/{\rm cm^{-2}} > 12.6$. We use a large sample of 192 AGN/QSOs at $0.01\lesssim z\lesssim0.45$ that have high-quality COS FUV spectra to characterize the intrinsic scatter in the gap between the AGN redshift and the edge of their Ly$\alpha$ forest. We present new COS NUV data for 1ES1553+113 and confirm its redshift of $z=0.433$ using our technique. We apply our Ly$\alpha$-forest-based redshift estimation technique to nine additional blazars with archival ${\it HST}$ UV spectra, most of which are key targets for future X-ray missions. Our inferred redshift constraints improve estimates for two BL Lacs (1ES1118+424 and S50716+714) and are consistent with previous estimates for the rest. Our results emphasize the need to obtain further UV spectra of bright blazars, of which many have uncertain redshifts, in order to maximize the scientific value of future X-ray WHIM observations that will improve our understanding of galaxy evolution. ","Improving blazar redshift constraints with the edge of the Ly$\alpha$
  forest: 1ES 1553+113 and implications for observations of the WHIM"
72,1460210418014511105,2836516877,Benjamin Shaw,['Excited to see our latest @UoM_Pulsars paper on the ArXiv today in which we present 106 new pulsar glitches detected and measured over the last ~decade with the telescopes @jodrellbank.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2111.06835,"Pulsar glitches are rapid spin-up events that occur in the rotation of neutron stars, providing a valuable probe into the physics of the interiors of these objects. Long-term monitoring of a large number of pulsars facilitates the detection of glitches and the robust measurements of their parameters. The Jodrell Bank pulsar timing programme regularly monitors more than 800 radio pulsars and has accrued, in some cases, over 50 years of timing history on individual objects. In this paper we present 106 new glitches in 70 radio pulsars as observed up to the end of 2018. For 70% of these pulsars, the event we report is its only known glitch. For each new glitch we provide measurements of its epoch, amplitude and any detected changes to the spin-down rate of the star. Combining these new glitches with those listed in the Jodrell Bank glitch catalogue we analyse a total sample of 543 glitches in 178 pulsars. We model the distribution of glitch amplitudes and spin-down rate changes using a mixture of two Gaussian components. We corroborate the known dependence of glitch rate and activity on pulsar spin-down rates and characteristic ages, and show that younger pulsars tend to exhibit larger glitches. Pulsars whose spin-down rates between $10^{-14}$ Hz s$^{-1}$ and $10^{-10.5}$ Hz s$^{-1}$ show a mean reversal of 1.8% of their spin-down as a consequence of glitches. Our results are qualitatively consistent with the superfluid vortex unpinning models of pulsar glitches. ","The Jodrell Bank Glitch Catalogue: 106 new rotational glitches in 70
  pulsars"
73,1459206865867390983,14242308,evgenya shkolnik,"['A thought-provoking new paper on what Lyman-alpha transits are teaching us about the mass-loss of exoplanet atmospheres, by James Owen et al. <LINK>']",https://arxiv.org/abs/2111.06094,"Lyman-$\alpha$ transits have been detected from a handful of nearby exoplanets and are one of our best insights into the atmospheric escape process. However, the fact interstellar absorption often renders the line-core unusable means we typically only observe the transit signature in the blue-wing, and they have been challenging to interpret. This has been recently highlighted by non-detections from planets thought to be undergoing vigorous escape. Pioneering 3D simulations have shown that escaping hydrogen is shaped into a cometary tail receding from the planet by tidal forces and interactions with the circumstellar environment. Motivated by this work, we develop the fundamental physical framework in which to interpret Lyman-$\alpha$ transits. We consider how this tail of gas is photoionized, and radially accelerated to high velocities. Using this framework, we show that the transit depth is often controlled by the properties of the stellar tidal field rather than details of the escape process. Instead, it is the transit duration that encodes details of the escape processes. Somewhat counterintuitively, we show that higher irradiation levels, which are expected to drive more powerful outflows, produce weaker, shorter Lyman-$\alpha$ transits. This result arises because the fundamental controlling physics is not the mass-loss rate but the distance a neutral hydrogen atom can travel before it's photoionized. Thus, Lyman-$\alpha$ transits do not primarily probe the mass-loss rates, but instead, they inform us about the velocity at which the escape mechanism is ejecting material from the planet, providing a clean test of predictions from atmospheric escape models. Ultimately, a detectable Lyman-$\alpha$ transit requires the escaping planetary gas to be radially accelerated to velocities of $\sim 100$ km~s$^{-1}$ before it becomes too ionized. ",The fundamentals of Lyman-alpha exoplanet transits
74,1459145225675448343,1163899384796459008,Wilfrid Somogyi,"['As David Bowie once said: ""Is there oxygen on exoplanets?""\n\nNew @ExoMol paper with @TroveMaster just dropped on arXiv  <LINK> <LINK>']",https://arxiv.org/abs/2111.05840,"We present a unified variational treatment of the electric quadrupole (E2) matrix elements, Einstein coefficients, and line strengths for general open-shell diatomic molecules in the general purpose diatomic code \Duo. Transformation relations between the Cartesian representation (typically used in electronic structure calculations) to the tensorial representation (required for spectroscopic applications) of the electric quadrupole moment components are derived. The implementation has been validated against accurate theoretical calculations and experimental measurements of quadrupole intensities of $^1\text{H}_2$ available in the literature. We also present accurate electronic structure calculations of the electric quadrupole moment functions for the $X^1\Sigma^+$ electronic states of $\text{CO}$ and $\text{HF}$ at the CCSD(T) and MRCI levels of theory, respectively, as well for the $a^1\Delta_g$ -- $b^1\Sigma_g^+$ quadrupole transition moment of $\text{O}_2$ with MRCI level of theory. Accurate infrared E2 line lists for $^{12}\text{C}^{16}\text{O}$ and $^1\text{H}^{19}\text{F}$ are provided. A demonstration of spectroscopic applications is presented by simulating E2 spectra for $^{12}\text{C}^{16}\text{O}$, $\text{H}^{19}\text{F}$ and $^{16}\text{O}_2$ (Noxon $a^1\Delta_g$ -- $b^1\Sigma_g^+$ band). ","Calculation of Electric Quadrupole Linestrengths for Diatomic Molecules:
  Application to the H2, CO, HF and O2 Molecules"
75,1459114254863265793,54789976,Sharath Adavanne,"[""📢 New paper: A multi-source localization and tracking approach for dynamic sound scenes with varying numbers of sources. \n\n'Differentiable Tracking-Based Training of Deep Learning Sound Source Localizers' with Archontis Politis and @TuomasVirt at WASPAA\n\n<LINK>"", 'DNN-based sound source localization models are commonly set as classification or regression tasks. Although the classification approach has shown good results, it is challenging to scale it to higher resolution, multi-source localization, and tracking. https://t.co/d4o7tNguhb', 'The multi-output regression-based approach is an alternative solution, however, training it is challenging. Especially, when the number of sources is varying.', 'We solve this multi-output regression problem by employing our novel deep-learning-based Hungarian network that solves the assignment problem between the varying numbers of predicted and reference locations. Code available here: https://t.co/Z4XrP3s6ge', 'We further formulate the popular tracking-based metrics of CLEAR-MOT as a differential objective function and optimise the multi source localizer directly on it.', 'The proposed training procedure results in large improvements in localization error, detection metrics, and tracking capabilities. Code available here:  https://t.co/hYAoUY1J3y']",https://arxiv.org/abs/2111.00030,"Data-based and learning-based sound source localization (SSL) has shown promising results in challenging conditions, and is commonly set as a classification or a regression problem. Regression-based approaches have certain advantages over classification-based, such as continuous direction-of-arrival estimation of static and moving sources. However, multi-source scenarios require multiple regressors without a clear training strategy up-to-date, that does not rely on auxiliary information such as simultaneous sound classification. We investigate end-to-end training of such methods with a technique recently proposed for video object detectors, adapted to the SSL setting. A differentiable network is constructed that can be plugged to the output of the localizer to solve the optimal assignment between predictions and references, optimizing directly the popular CLEAR-MOT tracking metrics. Results indicate large improvements over directly optimizing mean squared errors, in terms of localization error, detection metrics, and tracking capabilities. ","Differentiable Tracking-Based Training of Deep Learning Sound Source
  Localizers"
76,1459064228309090334,1318520798378889219,Michael Wemyss,"['I have a new joint paper on the ArXiv this morning! Without wishing to be too sentimental, it is fair to say that this paper contains a small part of my soul. A short thread on the importance of tenure follows. <LINK>', 'A bit of background. Myself and Gavin started working on the problem in 2014. That is seven years ago! What came out of our first 4 years? Nothing really, other than a bucket load of data, and a nagging feeling of only scratching the surface.', ""It should be noted that what the paper does could have been formulated (and proved) 100 years ago. The problem is very elementary to state, and the technology we build on to prove it hasn't changed much since the 1970s."", 'What has changed is the power of computer algebra to give intiution to abstract problems. This has driven many parts of our analysis, and kept us going through our dark days in the theoretical wilderness. In the process, it has fundamentally changed my whole view on mathematics.', 'Our main breakthrough is probably being crazy enough to believe that there was a reasonable and beautiful answer. But such a belief can only be sustained if it is backed up by data. And having no pressure to publish very partial, incomplete, results.', 'There is something to be said here about patience. But what this thread is really about: if you have tenure, use it! Take on those high risk projects! Sometimes they even work. Being tenured should not make us more risk-adverse 😀', ""When I was an ECR, what frustrated me most was the drive to take on risky projects was somewhat sapped by the reality of short-term contracts. ECRs simply don't have the luxury of time. But science often needs time."", 'I have been very privileged to have had this benefit of time over the past 7 years. Without it, this paper would simply not have been possible. Tenure makes things happen.', 'Final point: none of our code made it into the paper! It became fully theroetical after the main conceptual breakthrough. Special thanks and ❤️ to @magma_maths and Singular for providing such awesome software. We cited you heavily, of course!']",https://arxiv.org/abs/2111.05900,"This article describes local normal forms of functions in noncommuting variables, up to equivalence generated by isomorphism of noncommutative Jacobi algebras, extending singularity theory in the style of Arnold's commutative local normal forms into the noncommutative realm. This generalisation unveils many new phenomena, including an ADE classification when the Jacobi ring has dimension zero and, by suitably taking limits, a further ADE classification in dimension one. These are natural generalisations of the simple singularities and those with infinite multiplicity in Arnold's classification. We obtain normal forms away from some exceptional Type E cases. Remarkably these normal forms have no moduli, and the key new feature is that the noncommutative world affords larger families, and there are many more examples of each type. The first application of noncommutative singularity theory is to the birational geometry of 3-folds. We prove that all local normal forms of Type A and D are geometric, in the sense that each one gives rise to the contraction algebra of some smooth 3-fold flop or divisor-to-curve contraction. The general elephant of the corresponding contraction has matching type, and so this fully classifies contraction algebras of flops of length one and two. In the process, we describe the first and conjecturally only infinite family of length two crepant divisor-to-curve contractions. A further consequence is the classification of Gopakumar-Vafa invariants for length two flops, with noncommutative obstructions forcing gaps in the invariants that can arise. ",Local Normal Forms of Noncommutative Functions
77,1459053752896438275,335941225,Saeed Jahromi,['Check out our new paper on the phase diagram of the antiferromagnetic Kitaev model in magnetic field for general spins.\nA nice collaboration between @DIPCehu and our colleagues from @UniFAU \n\n<LINK> <LINK>'],https://arxiv.org/abs/2111.06132,"We combine tensor-network approaches and high-order linked-cluster expansions to investigate the quantum phase diagram of the antiferromagnetic Kitaev's honeycomb model in a magnetic field for general spin values. For the pure Kitaev model, tensor network calculations confirm the absence of fluxes and spin-spin correlations beyond nearest neighbor in the ground state, but signal a breaking of the discrete orientational symmetry for $S\in\{1,3/2,2\}$ inline with the semiclassical limit. An intermediate region between Kitaev phases and the high-field polarized phase is demonstrated for all considered spin values. The analysis of the high-field gap and the associated spectral weight of the polarized phase for general spin S is consistent with an unconventional quantum critical breakdown of the high-field polarized phase in accordance with the presence of exotic physics at intermediate Kitaev couplings. ","Kitaev honeycomb antiferromagnet in a field: quantum phase diagram for
  general spin"
78,1458989379305672706,141440459,Rod Van Meter 🌻,"['New paper dance!\nThe present and (near) future of discrete logarithm problems on quantum computers.\n#cryptography #QuantumComputing \n<LINK>', 'If you SciRate:\nhttps://t.co/QBa20zzF6N', '@dabacon Thanks, YouTube. https://t.co/ejvCLOxRe8']",https://arxiv.org/abs/2111.06102,"The discrete logarithm problem (DLP) is the basis for several cryptographic primitives. Since Shor's work, it has been known that the DLP can be solved by combining a polynomial-size quantum circuit and a polynomial-time classical post-processing algorithm. Evaluating and predicting the instance size that quantum devices can solve is an emerging research topic. In this paper, we propose a quantitative measure based on the success probability of the post-processing algorithm to determine whether an experiment on a quantum device (or a classical simulator) succeeded. We also propose a procedure to modify bit strings observed from a Shor circuit to increase the success probability of a lattice-based post-processing algorithm. We report preliminary experiments conducted on IBM-Quantum quantum computers and near-future predictions based on noisy-device simulations. We conducted our experiments with the ibm_kawasaki device and discovered that the simplest circuit (7 qubits) from a 2-bit DLP instance achieves a sufficiently high success probability to proclaim the experiment successful. Experiments on another circuit from a slightly harder 2-bit DLP instance, on the other hand, did not succeed, and we determined that reducing the noise level by half is required to achieve a successful experiment. Finally, we give a near-term prediction based on required noise levels to solve some selected small DLP and integer factoring instances. ","The Present and Future of Discrete Logarithm Problems on Noisy Quantum
  Computers"
79,1458767676453629953,136327374,Claudio Paganini 🇺🇦🇺🇦🇺🇦,['This one has been in the making for quite some time. If you want to know what the theory of Causal Fermion Systems has to say on Baryogenesis(/Fermiogenesis) check out our new paper: <LINK>'],https://arxiv.org/abs/2111.05556,It is shown that the theory of causal fermion systems gives rise to a novel mechanism of baryogenesis. This mechanism is worked out computationally in globally hyperbolic spacetimes in a way which enables the quantitative study in concrete cosmological situations. ,A Mechanism of Baryogenesis for Causal Fermion Systems
80,1458708964242345985,1163747241586298880,Alexey Vishnyakov,['Check out our new #rop paper!\nMAJORCA: Multi-Architecture JOP and ROP Chain Assembler <LINK>\nRelated survey: <LINK>'],http://arxiv.org/abs/2111.05781,"Nowadays, exploits often rely on a code-reuse approach. Short pieces of code called gadgets are chained together to execute some payload. Code-reuse attacks can exploit vulnerabilities in the presence of operating system protection that prohibits data memory execution. The ROP chain construction task is the code generation for the virtual machine defined by an exploited executable. It is crucial to understand how powerful ROP attacks can be. Such knowledge can be used to improve software security. We implement MAJORCA that generates ROP and JOP payloads in an architecture agnostic manner and thoroughly consider restricted symbols such as null bytes that terminate data copying via strcpy. The paper covers the whole code-reuse payloads construction pipeline: cataloging gadgets, chaining them in DAG, scheduling, linearizing to the ready-to-run payload. MAJORCA automatically generates both ROP and JOP payloads for x86 and MIPS. MAJORCA constructs payloads respecting restricted symbols both in gadget addresses and data. We evaluate MAJORCA performance and accuracy with rop-benchmark and compare it with open-source compilers. We show that MAJORCA outperforms open-source tools. We propose a ROP chaining metric and use it to estimate the probabilities of successful ROP chaining for different operating systems with MAJORCA as well as other ROP compilers to show that ROP chaining is still feasible. This metric can estimate the efficiency of OS defences. ",MAJORCA: Multi-Architecture JOP and ROP Chain Assembler
81,1458707483128741894,926852003925581824,Leonard Adolphs,"['New paper on arXiv: ""Reason ﬁrst, then respond: Modular Generation for Knowledge-infused Dialogue"" 🤔→💬  \n\nWe propose a modular two-step model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents. \n\n<LINK>\n\n(1/6) <LINK>', 'K2R consists of\n\n1. A seq2seq knowledge model that maps from context to knowledge.\n\n2. A seq2seq response model that generates the conversational response given the predicted knowledge and the context.\n\n(2/6)', 'K2R outperforms its seq2seq counterpart on open-domain dialogue while being more interpretable due to the explicit intermediate knowledge step. It has no problem blending in seemingly unrelated **injected** knowledge in the response and fitting the dialogue context (img).\n\n(3/6) https://t.co/JmEGo51n3v', 'K2R can turn short-span QA results into conversational responses that fit the previous context → making pretrained QA models usable in conversational agents without the need for retraining!\n\n(4/6) https://t.co/qVjyha9lIO', 'In knowledge-grounded dialogue (Wizard of Wikipedia), it is shown to hallucinate less and have a larger overlap with the ground-truth knowledge.\n\n(5/6) https://t.co/bKKuIPvvOi', 'This is joint work with @shtruk, @JackUrbs, Arthur Szlam, and @jaseweston from @AIatMeta!\n\n(6/6)']",https://arxiv.org/abs/2111.05204,"Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this ""reasoning step"", the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. ","Reason first, then respond: Modular Generation for Knowledge-infused
  Dialogue"
82,1458661090326286336,887992016,Luke Metz,"['New paper: when to use gradients\n\n<LINK>\n\nDL researchers often compute derivatives though just about everything (physics simulators, optimization procedures, renderers). Sometimes these gradients are useful, other times they are not.\n\nWe explore why.\n\n1/7', ""We show that when computing a gradient through an iterative system, we need to compute terms which consist of a product of the state transition Jacobian. This product is what causes issues.\n\nIf the Jacobian's eigenvalues are &gt; 1, gradients explode. &lt; 1, gradients vanish 😱\n\n2/7 https://t.co/3JM3xwYKMm"", 'We demonstrate exploding gradients in physics simulation, molecular dynamics, and learned optimization.\n\nIn the absence of noise, the loss surface can be high curvature, causing large gradients. While averaging smooths the loss,  the grad variance still grows exponentially.\n\n3/7 https://t.co/OYMQCmIWna', 'Looking at the Jacobian of 2 different initializations, we see that the stable initializations have smaller eigenvalues, and thus more controlled gradients.\n\n4/7 https://t.co/1FkALZuxAb', 'So what to do about this? A few approaches. One is to use truncated backprop through time. While this somewhat works, it’s finicky and less performant than simply training *without* gradients.\n\n5/7 https://t.co/LXgN0ru7T1', 'Getting rid of gradients leads to faster training?!?\n\nWhile this seems counterintuitive, consider trying to optimize a wiggly function convolved with a Gaussian. The more wiggly the function, the higher the grad variance.\n\nWith blackbox/evolution, variance remains constant.\n\n6/7 https://t.co/gqOPEb694l', 'Thanks Daniel Freeman(@bucketofkets ), @sschoenholz  and @TalKachman for the great collaboration. This was a really fun project to work on!\n\nKey takeaway: take gradients with care. Just because you can backprop doesn’t mean you should!\n\n7/7']",https://arxiv.org/abs/2111.05803,"Differentiable programming techniques are widely used in the community and are responsible for the machine learning renaissance of the past several decades. While these methods are powerful, they have limits. In this short report, we discuss a common chaos based failure mode which appears in a variety of differentiable circumstances, ranging from recurrent neural networks and numerical physics simulation to training learned optimizers. We trace this failure to the spectrum of the Jacobian of the system under study, and provide criteria for when a practitioner might expect this failure to spoil their differentiation based optimization algorithms. ",Gradients are Not All You Need
83,1458647022878138374,142783289,Jon Barron,"['Our new survey paper on neural rendering came together really nicely. I\'m particularly happy with this positioning of ""3D neural rendering"" (NeRF etc) vs ""2D neural rendering"". <LINK> <LINK>']",https://arxiv.org/abs/2111.05849,"Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects... ",Advances in Neural Rendering
84,1458608539879055363,20703003,Peter B Denton,"['New paper out on (weak) evidence for a sterile neutrino at @MicroBooNE @Fermilab!\n\n<LINK>\n\nI swear I thought of this paper *after* this tweet <LINK>\n\nAnd yes, there will be Orange.\n\n1/15 <LINK>', ""The main goal of MicroBooNE is to test whether MiniBooNE's low energy excess is really neutrinos or not, and thus test if there's a new oscillation frequency indicating nu_mu-&gt;nu_e appearance. This channel requires both nu_e-&gt;nu_e disappearance (which we might be seeing)...\n\n2/15"", '... and nu_mu-&gt;nu_mu disappearance, which we aren\'t seeing. It is also in tension with cosmology.\n\nBut, unlike MiniBooNE, the ""backgrounds"" for the nu_mu-&gt;nu_e search at MicroBooNE is mostly real nu_e\'s produced in the beam! So one can check for nu_e disappearance!\n\n3/15', 'This point seems to have been largely overlooked in the literature, although the short baseline neutrino program proposal seems to allude to this possibility: https://t.co/nxo149Qt3e. If there are other papers discussing this let me know!\n\nAnyway, on to the analysis...\n\n4/15', ""Most of the analyses presented by MicroBooNE have a deficit of electron neutrinos compared to the expectation and such a deficit suggests anomalous nu_e disappearance.\n\n(None of the known oscillations from atmospheric or solar Delta m^2's are expected to do anything here)\n\n5/15 https://t.co/itsfFJyIbo"", ""But a rate measurement isn't always compelling, nor is something at the edge of the spectrum (e.g. LSND, MiniBooNE), we want something in the middle of the spectrum! See these great plots from Daya Bay and KamLand showing atmospheric and solar oscillations.\n\n6/15 https://t.co/LkFzIySLkl"", ""Do such dips exist in MicroBooNE's disappearance data?\n\nNot at high significance.\n\nWhat about 2.2sig?\n\nYES. And in today's landscape lacking lots of great evidence for new physics, I'll take it and run with it at least a little way\n\n7/15 https://t.co/T4sqr3WuYF"", ""But Peter, can this be tested elsewhere?\n\nWhy, I'm so glad you asked. In fact, the preferred value agrees almost *exactly* with the latest gallium data from BEST!\n\nAre there other constraints?\n\nShhhh.....\n\n8/15 https://t.co/yiZoRdHQNc"", ""The frequency is the same as reactor data which is cool but the amplitude is 10x larger\n\nSolar also disfavors gallium &amp; MicroBooNE's preferred region\n\nAnd the mass and mixings are Large for cosmology. Possible solutions to this could include the Hubble tension but it's hard\n\n9/15"", ""... but the situation there isn't particularly worse than before, and we don't have MINOS+ and IceCube causing problems since we're not focusing on appearance, so progress!?\n\n10/15"", ""The above sterile search is based on the most sensitive of MicroBooNE's analysis dubbed Wire-Cell. I also looked at the other three channels presented and they all sort of see a dip in the same range at significances of 1.1sig, 1.4sig, and 2.1sig.\n\n11/15 https://t.co/o4mLtQHLKN"", ""These can't be simply added as there is overlap in the events, but there is some independence. Moreover, there should be many shared systematics, so some theory uncertainty cancellations could help since they seem to sort of agree in sterile parameter space.\n\n12/15 https://t.co/lzClUwzwBS"", 'In the future MicroBooNE can do better (obv) and could reach &gt;3sig. In fact, with existing data they could be up to 2.8sig already!\n\nThe best way to know for sure, of course, is with the whole short baseline program at Fermilab, which is partially already online!\n\n13/15 https://t.co/eHCuwtr3y5', ""One MAJOR caveat to all this is that these significances all assume Wilks' theorem and don't include Feldman-Cousins corrections. Nor do the sensitivity estimates do Asimov trials. Also, I did the best I could with available data. We should wait for MicroBooNE to be sure\n\n14/15"", 'Summary: no nu_mu-&gt;nu_e appearance at MicroBooNE, but maybe nu_e -&gt;nu_e disappearance? Parameters agree well with gallium, but in tension with others.\n\nIt is premature to conclude that MicroBooNE has\nkilled light sterile neutrinos when their data is\npointing towards them.\n\n15/15', ""@GKaragiorgi Yeah, it's ~8% with sizable uncertainties. But one thing to keep in mind is that unitarity fits (Stephen&amp;Mark, Sebastian,Kevin,Shirley, Zhuojun,Jiajie,Jian,TseChun refs 76-78 in my paper) ignore hints of UV from e.g. gallium, RAA, LSND, MiniBooNE. So I'm not sure they apply""]",https://arxiv.org/abs/2111.05793,"A sterile neutrino is a well motivated minimal new physics model that leave an imprint in neutrino oscillations. Over the last two decades, a number of hints pointing to a sterile neutrino have emerged, many of which are pointing near $m_4\sim1$ eV. Here we show how MicroBooNE data can be used to search for electron neutrino disappearance using each of their four analysis channels. We find a tantalizing hint for oscillations with the highest single channel significance of $2.2\sigma$ (assuming Wilks' theorem) coming from the Wire-Cell analysis which prefers $\sin^2(2\theta_{14})=0.30$ and $\Delta m^2_{41}=1.42$ eV$^2$; the other partially independent channels have compatible hints. This region of parameter space is in good agreement with existing hints from source experiments, is at a similar frequency but higher mixing than indicated by reactor anti-neutrinos, and is at the edge of the region allowed by solar neutrino data. Existing unanalyzed data from MicroBooNE could increase the sensitivity to $2.8\sigma$ and future data can reach $>3\sigma$ with existing systematics, assuming Wilks' theorem. ","Sterile Neutrino Searches with MicroBooNE: Electron Neutrino
  Disappearance"
85,1458466543441006595,2766925212,Andrew Childs,"['In a new paper with Qi Zhao, You Zhou, Alex Shaw, and @tongyang93, we show how to accelerate quantum simulation when the input state is chosen at random from a 1-design. <LINK>', 'Coincidentally, Chi-Fang Chen &amp; @fgslbrandao independently studied closely related issues in another paper out today: https://t.co/f977NwsVH9']",http://arxiv.org/abs/2111.04773,"The algorithmic error of digital quantum simulations is usually explored in terms of the spectral norm distance between the actual and ideal evolution operators. In practice, this worst-case error analysis may be unnecessarily pessimistic. To address this, we develop a theory of average-case performance of Hamiltonian simulation with random initial states. We relate the average-case error to the Frobenius norm of the multiplicative error and give upper bounds for the product formula (PF) and truncated Taylor series methods. As applications, we estimate average-case error for digital Hamiltonian simulation of general lattice Hamiltonians and $k$-local Hamiltonians. In particular, for the nearest-neighbor Heisenberg chain with $n$ spins, the error is quadratically reduced from $\mathcal O(n)$ in the worst case to $\mathcal O(\sqrt{n})$ on average for both the PF method and the Taylor series method. Numerical evidence suggests that this theory accurately characterizes the average error for concrete models. We also apply our results to error analysis in the simulation of quantum scrambling. ",Hamiltonian simulation with random inputs
86,1458418545902112775,935631082036359168,Matthias C. Caro,"[""Today, it's time for me to tell you about a new paper on the theory of quantum machine learning (QML) that I'm particularly proud of: <LINK>"", ""If you're doing (Q)ML, you want to ensure that your model generalizes well: Good performance on training data should imply good performance on new examples! Otherwise, you're in danger of overfitting the training data... https://t.co/y4CaLyokKZ"", 'Here, we prove strong generalization guarantees for variational QML models. Take-home message: \nIf your model has T trainable local gates, training data of size ~T will suffice for good generalization. So, poly-size QML models generalize well from poly-size training data! https://t.co/jYNBTPTvG7', ""But that's not all: Basically, no matter what QML architecture you're using, we've got you covered! Our results work for gate-sharing, multiple copies, variable architecture, and even take the optimization procedure into account. https://t.co/hSxDcOhAzh"", 'In addition to our theory, we demonstrate two numerical applications. First, we use quantum convolutional neural networks to classify quantum phases, with training data size growing at worst polylogarithmically with the system size. https://t.co/MGPkyLJcuV', 'Second, we use variational QML to compile an efficiently implementable unitary into elementary gates. Here, our theory guarantees that only polynomially many training points are needed for good generalization. So, we can scale this up to quite large numbers of qubits. https://t.co/mGQcmBihGd', ""And to showcase the importance of the optimization to generalization: When our QML model for compiling is favorably initialized, we don't need to optimize too much and generalize well already from effectively constant-size training data. https://t.co/ftIHPA78MR"", ""For the few who made it this far in the thread: Of course, the mathematician in me can't help himself telling you also about our proofs. ;) So here's an overview over what's happening there: https://t.co/orhRNnSCRG"", 'I am immensely grateful to my amazing colleagues @RobertHuangHY, @MvsCerezo, @kunal_phy, @sornborg, @LCincio, and @ColesQuantum for this @TUM_Mathematics-@MCQST_cluster-@IQIM_Caltech-@JointQuICS-@LosAlamosNatLab collaboration!', ""PS: If you want to hear more about this work, please stop by my #QTML2021 talk tomorrow! You can livestream it via @riken_en's Youtube channel at https://t.co/Zjns1tJCGA.""]",https://arxiv.org/abs/2111.05292,"Modern quantum machine learning (QML) methods involve variationally optimizing a parameterized quantum circuit on a training data set, and subsequently making predictions on a testing data set (i.e., generalizing). In this work, we provide a comprehensive study of generalization performance in QML after training on a limited number $N$ of training data points. We show that the generalization error of a quantum machine learning model with $T$ trainable gates scales at worst as $\sqrt{T/N}$. When only $K \ll T$ gates have undergone substantial change in the optimization process, we prove that the generalization error improves to $\sqrt{K / N}$. Our results imply that the compiling of unitaries into a polynomial number of native gates, a crucial application for the quantum computing industry that typically uses exponential-size training data, can be sped up significantly. We also show that classification of quantum states across a phase transition with a quantum convolutional neural network requires only a very small training data set. Other potential applications include learning quantum error correcting codes or quantum dynamical simulation. Our work injects new hope into the field of QML, as good generalization is guaranteed from few training data. ",Generalization in quantum machine learning from few training data
87,1458402064493654018,1077995761487568896,Jon Miller,"['New paper by grad student @NicolasTrueba:  \nWhen winds are launched within ~1000 GM/c^2, the central engine cannot be taken to be a point source, and simple geometry then constrains its size.  \nImportant for @chandraxray, XRISM, @AthenaXIFU, @ArcusXray.\n<LINK> <LINK>']",https://arxiv.org/abs/2111.04764,"Analyses of absorption from disk winds and atmospheres in accreting compact objects typically treat the central emitting regions in these systems as point sources relative to the absorber. This assumption breaks down if the absorbing gas is located within $few \times 1000\cdot GM/{c}^{2}$, in which case a small component of the absorber's Keplerian motion contributes to the velocity-width of absorption lines. Here, we demonstrate how this velocity-broadening effect can be used to constrain the sizes of central engines in accreting compact objects via a simple geometric relationship, and develop a method for modeling this effect. We apply this method on the Chandra/HETG spectra of three ultra-compact and short period neutron star X-ray binaries in which evidence of gravitationally redshifted absorption, owing to an inner-disk atmosphere, has recently been reported. The significance of the redshift is above $5\sigma$ for XTE J1710$-$281 (this work) and 4U 1916$-$053, and is inconsistent with various estimates of the relative radial velocity of each binary. For our most sensitive spectrum (XTE J1710$-$281), we obtain a 1$\sigma$ upper bound of 310 $\text{km}$ $\text{s}^{-1}$ on the magnitude of this geometric effect and a central engine of size ${R}_{CE} < 60 ~ GM/{c}^{2}$ (or, $< 90 ~ GM/{c}^{2}$ at the $3\sigma$ level). These initial constraints compare favorably to those obtained via microlensing in quasars and approach the sensitivity of constraints via relativistic reflection in neutron stars. This sensitivity will increase with further exposures, as well as the launch of future microcalorimeter and grating missions. ","A Spectroscopic Angle on Central Engine Size Scales in Accreting Neutron
  Stars"
88,1458362696320045056,976037939292594177,Alessandro Ignesti,['#paperday Check out the new paper by A. Franchetto. By investigating the metallicity gradient along the tail of jellyfish galaxies we found evidence of ISM-ICM mixing <LINK> <LINK>'],https://arxiv.org/abs/2111.04755,"Hydrodynamical simulations show that the ram-pressure stripping in galaxy clusters fosters a strong interaction between stripped interstellar medium (ISM) and the surrounding medium, with the possibility of intracluster medium (ICM) cooling into cold gas clouds. Exploiting the MUSE observation of three jellyfish galaxies from the GAs Stripping Phenomena in galaxies with MUSE (GASP) survey, we explore the gas metallicity of star-forming clumps in their gas tails. We find that the oxygen abundance of the stripped gas decreases as a function of the distance from the parent galaxy disk; the observed metallicity profiles indicate that more than 40% of the most metal-poor stripped clouds are constituted by cooled ICM, in qualitative agreement with simulations that predict mixing between the metal-rich ISM and the metal-poor ICM. ","Evidence for mixing between ICM and stripped ISM by the analysis of the
  gas metallicity in the tails of jellyfish galaxies"
89,1458253300487368705,1114611290352361472,"Nicolas Trueba, PhD","['1/3 - New Paper Day:<LINK>!\nWe developed a new method for constraining the size of central engines in accreting neutron stars by measuring the velocity width of absorption lines. <LINK>', '2/3 We applied this technique on a handful of neutron star ultra-compact X-ray binaries - we are able to set a 60 GM/c^2 upper limit on the central engine of XTE J1710-281. https://t.co/RE0MEApRtW', '3/3 We also build on the evidence of gravitationally redshifted absorption in these systems. Gravitational redshift helps us estimate the orbital radius of the absorber independent of mass or luminosity, greatly simplifying our central engine constraints.']",https://arxiv.org/abs/2111.04764,"Analyses of absorption from disk winds and atmospheres in accreting compact objects typically treat the central emitting regions in these systems as point sources relative to the absorber. This assumption breaks down if the absorbing gas is located within $few \times 1000\cdot GM/{c}^{2}$, in which case a small component of the absorber's Keplerian motion contributes to the velocity-width of absorption lines. Here, we demonstrate how this velocity-broadening effect can be used to constrain the sizes of central engines in accreting compact objects via a simple geometric relationship, and develop a method for modeling this effect. We apply this method on the Chandra/HETG spectra of three ultra-compact and short period neutron star X-ray binaries in which evidence of gravitationally redshifted absorption, owing to an inner-disk atmosphere, has recently been reported. The significance of the redshift is above $5\sigma$ for XTE J1710$-$281 (this work) and 4U 1916$-$053, and is inconsistent with various estimates of the relative radial velocity of each binary. For our most sensitive spectrum (XTE J1710$-$281), we obtain a 1$\sigma$ upper bound of 310 $\text{km}$ $\text{s}^{-1}$ on the magnitude of this geometric effect and a central engine of size ${R}_{CE} < 60 ~ GM/{c}^{2}$ (or, $< 90 ~ GM/{c}^{2}$ at the $3\sigma$ level). These initial constraints compare favorably to those obtained via microlensing in quasars and approach the sensitivity of constraints via relativistic reflection in neutron stars. This sensitivity will increase with further exposures, as well as the launch of future microcalorimeter and grating missions. ","A Spectroscopic Angle on Central Engine Size Scales in Accreting Neutron
  Stars"
90,1458089531454992404,22402106,David Tsang,"[""New paper day! <LINK>\n\nI'm very proud of PhD student Duncan Neill, from my group at Bath, who led this new work exploring the electromagnetic emissions from Resonant Shattering Flares from Black Hole-Neutron Star and Binary Neutron Star mergers. 1/11"", 'Resonant Shattering Flares (RSFs) are gamma-ray flares during inspiral that occur because of tidal resonance - the gravitational tugging of one star on the other occurs just at the right frequency to cause it to shatter, like an opera singer with a wine glass. 2/11 https://t.co/V8mkpaEuHg', 'Using coincident gravitational-wave detection and gamma-ray emission we had previously shown how this could help measure the properties of neutron star matter - matter so dense that understanding it can help us understand nuclear physics. https://t.co/21D4ciL2gY 3/11', 'In this paper, we showed how multiple shatterings during the resonant window of ~0.1 seconds could lead to colliding blast waves, giving a single (non-thermal) flare similar to ones seen in precursors to short Gamma Ray Bursts. 4/11', 'We also discussed how other types of electromagnetic counterparts to gravitational-wave mergers, such as SGRBs and kilonovae, require disruption of a neutron star, either through collision or tidal disruption. This does not happen for most BHNS mergers. 5/11 https://t.co/Uxl5daBsHg', 'The detectability of RSFs depends crucially on the average surface magnetic field strength of a neutron star. Too weak of a field and not much energy can be extracted to power a pair-fireball. 6/11', ""If some SGRB precursors are RSFs, as we suspect, this implies that at least some neutron stars must have long-lived strong surface fields, which indicates it may be supported by flux frozen into the neutron star's superconducting core (crust-only B-fields decay very quickly) 7/11"", 'We can use our emission models, and and the fantastic BPASS pop synth code from @astro_jje in order to estimate the rates of detectable RSFs based on current detectors, and find that we can get up to ~5 and ~25 RSFs per year for BHNS and NSNS systems respectively. 8/11 https://t.co/QyNmsZLvFk', 'We also found that we could put constraints on the progenitor B-fields for the NSNS merger observed with GW170817. We find that the surface B-fields must have been less than ~10^13 G since no RSF was observed above the background prior to the merger. 9/11 https://t.co/nRks845sU0', 'RSF prompt emission and afterglow make them look like weak SGRBs, but the prompt occurs -before- the merger. This makes them easy to identify as a multimessenger signature. 10/11 https://t.co/WCsSLPIbfl', 'Their presence or absence can give us constraints on the surface B-field and therefore the magnetic field evolution of the NS population, and a multimessenger detection can give us strong constraints on nuclear physics - competitive with current collider experiments!  11/11', 'I forgot to add here that, in contrast to tidal disruption, tidal resonance IS reached for almost all BHNS systems, so RSFs may in fact be the most common EM counterpart to a BHNS merger! 5.5/11', '@SchnittGetsReal Oops! I knew I forgot something!!']",https://arxiv.org/abs/2111.03686,"Multi-messenger probes of BHNS and NSNS mergers are important for our understanding of the properties of these compact objects and the extreme physical domain they inhabit. While SGRBs and kilonovae are the most considered counterpart to these mergers, both require disruption of the NS, which may not occur in BHNS systems. We investigate the properties of an EM counterpart that does not require tidal disruption: resonant shattering flares (RSFs). We find that RSFs are short (duration $\sim 0.1 \text{ s}$) non-thermal flares with luminosities up to a few $\times10^{48}\text{ erg/s}$ that are strongly dependent on the magnetic field strength at the surface of the NS. These flares are a result of multiple colliding relativistic shells launched during the resonance window, leading to a prompt non-thermal gamma-ray emission, as well as broad-band afterglow emission. We compute the expected rates of detectable RSFs using the BPASS population synthesis code, with different assumptions about the evolution of surface magnetic field strengths before merger. We find the rate of detectable RSFs to be $\sim 0.0001-5$ per year for BHNS mergers and $\sim 0.0005-25$ per year for NSNS mergers, with the lower bound corresponding to surface field decay consistent with magneto-thermal evolution in purely crustal fields, while the upper bounds are for systems which have longer-lived surface magnetic fields supported by flux frozen into the core. If some of the observed SGRB precursor flares are indeed RSFs, this suggests the presence of a longer-lived surface field for some fraction of the neutron star population, and that we should expect RSFs to be the most common detectable EM counterpart to GW detections of BHNS mergers. The non-detection of a RSF prior to GRB170817A provides an upper bound for the magnetic fields for the progenitor NSs of $B_{\rm surf}~\sim 10^{13} {\rm G}$. ","Resonant Shattering Flares in Black Hole-Neutron Star and Binary Neutron
  Star Mergers"
91,1458073562984632321,1134375290581524480,Kai Schmitz,"['New paper: <LINK> with Eduard Gorbar, Oleksandr Sobol, and Stanislav Vilchinskii. We generalize ""hypermagnetogenesis"" during axion inflation to the full SM case and present estimates of its efficiency that account for the Schwinger pair production of SM fermions. <LINK>']",https://arxiv.org/abs/2111.04712,"Axion inflation coupled to the Standard Model (SM) hypercharge gauge sector represents an attractive scenario for the generation of primordial hypermagnetic fields. The description of this scenario is, however, complicated by the Schwinger effect, which gives rise to highly nonlinear dynamics. Hypermagnetogenesis during axion inflation in the absence of nonlinear effects is well studied and known to result in a hypermagnetic energy density that scales like $H^4\,e^{2\pi\xi}/\xi^5$, where $\xi$ is proportional to the time derivative of the axion-vector coupling in units of the Hubble rate $H$. In this paper, we generalize this result to the full SM case by consistently taking into account the Schwinger pair production of all SM fermions. To this end, we employ the novel gradient-expansion formalism that we recently developed in [2109.01651], and which is based on a set of vacuum expectation values for bilinear hyperelectromagnetic functions in position space. We parametrize the numerical output of our formalism in terms of three parameters ($\xi$, $H$, and $\Delta$, where the latter accounts for the damping of subhorizon gauge-field modes because of the finite conductivity of the medium) and work out semianalytical fit functions that describe our numerical results with high accuracy. Finally, we validate our results by comparing them to existing estimates in the literature as well as to the explicit numerical results in a specific inflationary model, which leads to good overall agreement. We conclude that the systematic uncertainties in the description of hypermagnetogenesis during axion inflation, which previously spanned up to several orders of magnitude, are now reduced to typically less than 1 order of magnitude, which paves the way for further phenomenological studies. ",Hypermagnetogenesis from axion inflation: Model-independent estimates
92,1458040585470758914,20326827,Prof. Dr. Steffen Wendzel,"['Making covert channel countermeasures adaptive is a way that should provide certain benefits, especially in sophisticated scenarios. New paper: <LINK> #steganography']",http://arxiv.org/abs/2111.03310,"The detection and elimination of covert channels are performed by a network node, known as a warden. Especially if faced with adaptive covert communication parties, a regular warden equipped with a static set of normalization rules is ineffective compared to a dynamic warden. However, dynamic wardens rely on periodically changing rule sets and have their own limitations, since they do not consider traffic specifics. We propose a novel adaptive warden strategy, capable of selecting active normalization rules by taking into account the characteristics of the observed network traffic. Our goal is to disturb the covert channel and provoke the covert peers to expose themselves more by increasing the number of packets required to perform a successful covert data transfer. Our evaluation revealed that the adaptive warden has better efficiency and effectiveness when compared to the dynamic warden because of its adaptive selection of normalization rules. ",Adaptive Warden Strategy for Countering Network Covert Storage Channels
93,1458037734753374212,1034025041300803585,Jonathan Foldager,"['New paper out! \n<LINK>\n\nTogether with @artix41 and @Tweetteresearch, we propose a new variational quantum algorithm for thermal state preparation, namely one where we parameterize the (depolarization) noise. 👇', 'We derive a closed-form approximation for the free-energy and use it as a cost function for our variational algorithm. We find that it is indeed possible to get high fidelities in multiple temperatures and hamiltonians, but we also show the performance depends on the temperature.', 'We hope this work will inspire future research on exploiting noise in variational algorithms!']",https://arxiv.org/abs/2111.03935,"Preparing thermal states on a quantum computer can have a variety of applications, from simulating many-body quantum systems to training machine learning models. Variational circuits have been proposed for this task on near-term quantum computers, but several challenges remain, such as finding a scalable cost-function, avoiding the need of purification, and mitigating noise effects. We propose a new algorithm for thermal state preparation that tackles those three challenges by exploiting the noise of quantum circuits. We consider a variational architecture containing a depolarizing channel after each unitary layer, with the ability to directly control the level of noise. We derive a closed-form approximation for the free-energy of such circuit and use it as a cost function for our variational algorithm. By evaluating our method on a variety of Hamiltonians and system sizes, we find several systems for which the thermal state can be approximated with a high fidelity. However, we also show that the ability for our algorithm to learn the thermal state strongly depends on the temperature: while a high fidelity can be obtained for high and low temperatures, we identify a specific range for which the problem becomes more challenging. We hope that this first study on noise-assisted thermal state preparation will inspire future research on exploiting noise in variational algorithms. ",Noise-Assisted Variational Quantum Thermalization
94,1458037456692039682,251604578,Gernot Maier,['New paper published: significantly improved throughput calibration for the @VeritasGammaRay telescopes. Calibration is actually fun! <LINK>'],https://arxiv.org/abs/2111.04676,"Context. The response of imaging atmospheric Cherenkov telescopes to incident {\gamma}-ray-initiated showers in the atmosphere changes as the telescopes age due to exposure to light and weather. These aging processes affect the reconstructed energies of the events and {\gamma}-ray fluxes. Aims. This work discusses the implementation of signal calibration methods for the Very Energetic Radiation Imaging Telescope Array System (VERITAS) to account for changes in the optical throughput and detector performance over time. Methods. The total throughput of a Cherenkov telescope is the product of camera-dependent factors, such as the photomultiplier tube gains and their quantum efficiencies, and the mirror reflectivity and Winston cone response to incoming radiation. This document summarizes different methods to determine how the camera gains and mirror reflectivity have evolved over time and how we can calibrate this changing throughput in reconstruction pipelines for imaging atmospheric Cherenkov telescopes. The implementation is validated against seven years of observations with the VERITAS telescopes of the Crab Nebula, which is a reference object in very-high-energy astronomy. Results. Regular optical throughput monitoring and the corresponding signal calibrations are found to be critical for the reconstruction of extensive air shower images. The proposed implementation is applied as a correction to the signals of the photomultiplier tubes in the telescope simulation to produce fine-tuned instrument response functions. This method is shown to be effective for calibrating the acquired {\gamma}-ray data and for recovering the correct energy of the events and photon fluxes. At the same time, it keeps the computational effort of generating Monte Carlo simulations for instrument response functions affordably low. ",The throughput calibration of the VERITAS telescopes
95,1457909342351630344,468023312,Malintha Fernando,['A new paper: “CoCo Games: Graphical Game-Theoretic Swarm Control for Communication-Aware Coverage”. #Robotics #gametheory \n<LINK>'],https://arxiv.org/abs/2111.04576,"We propose a novel framework for real-time communication-aware coverage control in networked robot swarms. Our framework unifies the robot dynamics with network-level message-routing to reach consensus on swarm formations in the presence of communication uncertainties by leveraging local information. Specifically, we formulate the communication-aware coverage as a cooperative graphical game, and use variational inference to reach mixed strategy Nash equilibria of the stage games. We experimentally validate the proposed approach in a mobile ad-hoc wireless network scenario using teams of aerial vehicles and terrestrial user equipment (UE) operating over a large geographic region of interest. We show that our approach can provide wireless coverage to stationary and mobile UEs under realistic network conditions. ","CoCo Games: Graphical Game-Theoretic Swarm Control for
  Communication-Aware Coverage"
96,1457882989782650885,969190164764372993,Xudong Sun,"['Paper Monday! I’m pleased to share our new work “Torus-stable zone above starspots”, an attempt to explain the “missing stellar CME conundrum”. It is accepted to MNRAS and now out on arXiv (<LINK>). A 🧵 (1/11) <LINK>', 'Super flares are frequently observed on cool stars, but stellar CMEs are rarely reported. This is surprising, because the flare-CME association rate increases with flare energy on the Sun. Almost all X-class solar flares are accompanied by a CME. (2/11)', 'What’s causing the lack of stellar CME detection? In Oct 2014, the giant solar active region 12192 produced 6 X-flares but no CME. Analyses show that confinement by strong overlying coronal magnetic field plays an important role. (3/11)', 'AR 12192 was proposed as a “solar analog” to the non-eruptive stellar flares by @cosmodrake and @rachelosten. The efficacy of the magnetic confinement has been demonstrated in a series of nice simulations led by @AstroRaikoh. (4/11)', 'In this work, we explore the role of “torus instability” (TI), a leading mechanism for solar CMEs. For a twisted magnetic flux rope, if the background magnetic field decreases sufficiently slowly with height, the TI will be suppressed, (5/11)', 'so no CME will occur due to TI. Ongoing eruption via other mechanisms may also fail. Here, the profile of the background coronal field is the key. The critical height, below which the TI is suppressed, defines the extent of a “torus stable zone” (TSZ). (6/11)', 'We evaluate stellar TSZ using a PFSS model comprising a bipole as a pair of starspots and an axial dipole, which provide the local and global scale confinement. There are 3 parameters: starspot size, dipole strength, and “source surface” radius. (7/11)', 'We find that TSZ is positively correlated with all 3 parameters. The detailed values depend on the interplay of the spots’ and the dipole fields. For solar-like spots (5 deg) and intermediate dipole (100 G), an interesting, secondary TSZ arises… (8/11)', 'at higher altitude, which is particularly amiable for “failed eruptions”. Nascent solar flux ropes are generally “flat”. Because cool stars often have larger spots (&gt;20 deg), stronger dipole (&gt;1 kG), and higher source surface (&gt;10 Rs) compared to the Sun, … (9/11)', 'these low-lying flux ropes will safely reside within an extended TSZ. TI onset will be difficult; failed eruptions become more likely. The simple model thus helps explain the “missing stellar CMEs”. (10/11)', ""Mahalo to my wonderful coauthors Tibor Török and @MarcDeRosa_, who scrutinized over every word of the manuscript. Mahalo also goes to Bernhard Kliem and @StellarTayar for helpful comments. Also @mathewjowens: the paper starts with a 'whilst'! (11/11)"", '@SethInSpace Thx Seth!', '@louiseharra Thank you Louise!']",http://arxiv.org/abs/2111.03665,"Whilst intense solar flares are almost always accompanied by a coronal mass ejection (CME), reports on stellar CMEs are rare, despite the frequent detection of stellar 'super flares'. The torus instability of magnetic flux ropes is believed to be one of the main driving mechanisms of solar CMEs. Suppression of the torus instability, due to a confining background coronal magnetic field that decreases sufficiently slowly with height, may contribute to the lack of stellar CME detection. Here we use the solar magnetic field as a template to estimate the vertical extent of this 'torus-stable zone' (TSZ) above a stellar active region. For an idealised potential field model comprising the fields of a local bipole (mimicking a pair of starspots) and a global dipole, we show that the upper bound of the TSZ increases with the bipole size, the dipole strength, and the source surface radius where the coronal field becomes radial. The boundaries of the TSZ depend on the interplay between the spots' and the dipole's magnetic fields, which provide the local- and global-scale confinement, respectively. They range from about half the bipole size to a significant fraction of the stellar radius. For smaller spots and an intermediate dipole field, a secondary TSZ arises at a higher altitude, which may increase the likelihood of 'failed eruptions'. Our results suggest that the low apparent CME occurrence rate on cool stars is, at least partially, due to the presence of extended TSZs. ",Torus-Stable Zone Above Starspots
97,1457695585369219075,1134375290581524480,Kai Schmitz,"['New paper <LINK> We present ""leptoflavorgenesis"": a new possibility to create the baryon asymmetry of the Universe and an alternative to ""baryogenesis"" and ""leptogenesis"". No right-handed neutrinos, no B-L violation, but closely related to future searches for LFV <LINK>']",https://arxiv.org/abs/2111.03082,"Charged-lepton flavor violation (CLFV) is a smoking-gun signature of physics beyond the Standard Model. The discovery of CLFV in upcoming experiments would indicate that CLFV processes must have been efficient in the early Universe at relatively low temperatures. In this letter, we point out that such efficient CLFV interactions open up new ways of creating the baryon asymmetry of the Universe. First, we revisit the one-loop corrections from charged-lepton Yukawa interactions to the chemical transport in the Standard Model plasma, which imply that nonzero lepton flavor asymmetries summing up to $B-L = 0$ are enough to generate the baryon asymmetry. Then, we describe two scenarios of what we call {\it leptoflavorgenesis}, where efficient CLFV processes are responsible for the generation of primordial lepton flavor asymmetries that are subsequently converted to a baryon asymmetry by weak sphaleron processes. Here, the conversion factor from lepton flavor asymmetry to baryon asymmetry is suppressed by charged-lepton Yukawa couplings squared, which provides a natural explanation for the smallness of the observed baryon-to-photon ratio. ","Leptoflavorgenesis: baryon asymmetry of the Universe from lepton flavor
  violation"
98,1457669678596300803,958374804,Jonathan Gorard,"['<LINK>\nNew paper (with Xerxes) on the homotopic foundations of @wolframphysics , addressing the fundamental question of *how* and *why* geometrical structures in physics (such as spacetime, Hilbert space, etc.) emerge from discrete ""pregeometric"" data! (1/5) <LINK>', 'The basic idea is that, starting from a multiway system, which is really a monoidal 1-category, one can perform “completions” by introducing higher cells (and thus higher homotopies) until one obtains the full ""rulial"" multiway system, which is really an infinity-groupoid. (2/5)', ""The “rulial” multiway system can thus be interpreted as a homotopy type via Grothendieck's hypothesis, and the “multiverse” of all such “rulial” multiway systems thus carries the structure of a cohesive (infinity, 1)-topos, with cohesivity being preserved under fibration. (3/5)"", 'Treating “rulial space” as a fibration, one can then obtain the usual discrete structures (hypergraphs, causal graphs, multiway systems, etc.) as global sections of rulial space, which inherit spatial/geometrical structure functorially from the aforementioned homotopy type. (4/5)', ""It's perhaps one of the most philosophically interesting papers I've ever (co-)written, since it begins to get at the foundational question of *why* the laws of physics have the structure that they do. With potential applications to TQFT, (higher) gauge field theory, etc. (5/5)"", '@mattecapu @wolframphysics Cheers Matteo! Since ultimately we were concerned with obtaining homotopy n-types, it made sense to work with n-fold groupoids (since in that case it doesn’t matter). We could always collapse the faces in the cells to obtain the corresponding n-categories if we wanted to…']",https://arxiv.org/abs/2111.03460,"How do spaces emerge from pregeometric discrete building blocks governed by computational rules? To address this, we investigate non-deterministic rewriting systems (multiway systems) of the Wolfram model. We express these rewriting systems as homotopy types. Using this new formulation, we outline how spatial structures can be functorially inherited from pregeometric type-theoretic constructions. We show how higher homotopy types are constructed from rewriting rules. These correspond to morphisms of an $n$-fold category. Subsequently, the $n \to \infty$ limit of the Wolfram model rulial multiway system is identified as an $\infty$-groupoid, with the latter being relevant given Grothendieck's homotopy hypothesis. We then go on to show how this construction extends to the classifying space of rulial multiway systems, which forms a multiverse of multiway systems and carries the formal structure of an ${\left(\infty, 1\right)}$-topos. This correspondence to higher categorical structures offers a new way to understand how spaces relevant to physics may arise from pregeometric combinatorial models. A key issue we have addressed here is to relate abstract non-deterministic rewriting systems to higher homotopy spaces. A consequence of constructing spaces and geometry synthetically is that it eliminates ad hoc assumptions about geometric attributes of a model such as an a priori background or pre-assigned geometric data. Instead, geometry is inherited functorially by higher structures. This is relevant for formally justifying different choices of underlying spacetime discretization adopted by models of quantum gravity. We conclude with comments on how our framework of higher category-theoretic combinatorial constructions, corroborates with other approaches investigating higher categorical structures relevant to the foundations of physics. ","Pregeometric Spaces from Wolfram Model Rewriting Systems as Homotopy
  Types"
99,1457648288585424896,962876421268914177,Hanlin Ren,"['<LINK>\nA new paper with Ran Duan! We show that in an undirected graph, we can design an oracle that maintains 𝒆𝒙𝒂𝒄𝒕 distances under any number of edge failures. For d edge failures, our distance oracle has space complexity O(dn^4) and query time d^{O(d)}.', 'This is the first fault-tolerant distance oracle that supports an arbitrary number of failures and maintains exact distances. Previously, there are good oracles that maintain (1+eps)-approximate distances, but exact distances remained elusive.', 'Indeed, the previous best exact distance oracle needed roughly n^d space (which is barely non-trivial).', 'One drawback: although using only O(dn^4) space, our oracle seems to need n^{O(d)} time to preprocess. Can we improve the preprocessing time?']",https://arxiv.org/abs/2111.03360,"We present the first compact distance oracle that tolerates multiple failures and maintains exact distances. Given an undirected weighted graph $G = (V, E)$ and an arbitrarily large constant $d$, we construct an oracle that given vertices $u, v \in V$ and a set of $d$ edge failures $D$, outputs the exact distance between $u$ and $v$ in $G - D$ (that is, $G$ with edges in $D$ removed). Our oracle has space complexity $O(d n^4)$ and query time $d^{O(d)}$. Previously, there were compact approximate distance oracles under multiple failures [Chechik, Cohen, Fiat, and Kaplan, SODA'17; Duan, Gu, and Ren, SODA'21], but the best exact distance oracles under $d$ failures require essentially $\Omega(n^d)$ space [Duan and Pettie, SODA'09]. Our distance oracle seems to require $n^{\Omega(d)}$ time to preprocess; we leave it as an open question to improve this preprocessing time. ",Maintaining Exact Distances under Multiple Edge Failures
100,1457645345597386760,938807681929924609,Samuel Jackson,['New preprint out! A lovely new collaboration with Australian researchers using deep learning to enhance our understanding of multiscale systems and fluid flow! Check out the paper and open data/code - <LINK>'],https://arxiv.org/abs/2111.01270#,"Field-of-view and resolution trade-offs in X-Ray micro-computed tomography (micro-CT) imaging limit the characterization, analysis and model development of multi-scale porous systems. To this end, we developed an applied methodology utilising deep learning to enhance low resolution images over large sample sizes and create multi-scale models capable of accurately simulating experimental fluid dynamics from the pore (microns) to continuum (centimetres) scale. We develop a 3D Enhanced Deep Super Resolution (EDSR) convolutional neural network to create super resolution (SR) images from low resolution images, which alleviates common micro-CT hardware/reconstruction defects in high-resolution (HR) images. When paired with pore-network simulations and parallel computation, we can create large 3D continuum-scale models with spatially varying flow & material properties. We quantitatively validate the workflow at various scales using direct HR/SR image similarity, pore-scale material/flow simulations and continuum scale multiphase flow experiments (drainage immiscible flow pressures and 3D fluid volume fractions). The SR images and models are comparable to the HR ground truth, and generally accurate to within experimental uncertainty at the continuum scale across a range of flow rates. They are found to be significantly more accurate than their LR counterparts, especially in cases where a wide distribution of pore-sizes are encountered. The applied methodology opens up the possibility to image, model and analyse truly multi-scale heterogeneous systems that are otherwise intractable. ","Deep learning of multi-resolution X-Ray micro-CT images for multi-scale
  modelling"
101,1457635673616302085,2427184074,Christopher Berry,"[""The latest @GravtitationWave catalogue is out today #GWTC3\nPaper <LINK>\nData <LINK>\nThis completed our observations from @LIGO &amp; @ego_virgo's third observing run\n\nI would normally write a thread on the new science, but today I want to talk people <LINK>"", 'Parpes from @LIGO @ego_virgo &amp; @KAGRA_PR are well known for having long author lists (10 pages for #GWTC3). Building, operating and maintaining the detectors, keeping the computing infrastructure working, and analysing the data takes lots of people. We are a global village', 'However, typically, a paper from @LIGO @ego_virgo &amp; @KAGRA_PR is *directly* the work of a smaller group. Perhaps a dozen people doing the analysis and writing the paper, and a similar number reviewing the results, with a few more people contributing bits and pieces', 'Catalogue papers are different. They really do draw upon the work of hundreds of people. There are so many stages of analysis, so many things that need to be carefully checked. With roughly 100 candidates, you needs lots of person power!', 'One thing I really love about working in @LIGO @ego_virgo @KAGRA_PR is that if you need some help, you can literally email the world expert and they\'ll reply ""Sure, I\'ll get that to you for Tuesday"". With such wide range of expertise in the Collaboration, we can do amazing things', ""However, collaboration has been much more difficult recently. The #GWTC3 team have been working together for 16 months, but we've never met in person. During that time there's been a pandemic, political unrest, a hurricane hitting Louisiana and more I must have forgotten"", 'There have been happy interruptions to work too. Two of the members of the @GWTC3 editorial team have taken parental leave (Robin, 1.89e−30 solar masses, and Lydia, 1.95e−30 solar masses) were born about a week apart. Several other analysts and reviewers have become parents too', 'With so much going on in the world, organising remote teaching and juggling homeschooling, stress and anxiety over issues at home and across the world, I think that it is remarkable that we have achieved all that we have: the most comprehensive gravitational-wave analysis', ""Gravitational-wave astronomy is cool, but it is not the most important thing in the world. I'm extremely grateful to everyone who contributed their time and effort into getting this work done. We achieved a lot, and got it out on time. I think there is a lot to be proud of"", '@cdmpatra @LIGO @ego_virgo @KAGRA_PR @NUCIERA Here is a plot showing stellar lifetime as a function of mass (from https://t.co/KFrpAph91s). this get a bit more complicated at higher masses, and much more complicated when you have binary interactions. The general rule is more massive stars live shorter lives https://t.co/X9Krz8Qa7E', 'Of course, one of the most important results coming from #GWTC3 are these amazing stickers https://t.co/imJjFtFbx1', '@cdmpatra @LIGO @ego_virgo @KAGRA_PR @NUCIERA The stellar graveyard plot does, as it is based upon the results of the gravitational-wave analysis', ""@DrDa5id I've been thinking of naming them either Gwen or Gwary"", '@DrDa5id Too soon']",https://arxiv.org/abs/2111.03606,"The third Gravitational-wave Transient Catalog (GWTC-3) describes signals detected with Advanced LIGO and Advanced Virgo up to the end of their third observing run. Updating the previous GWTC-2.1, we present candidate gravitational waves from compact binary coalescences during the second half of the third observing run (O3b) between 1 November 2019, 15:00 UTC and 27 March 2020, 17:00 UTC. There are 35 compact binary coalescence candidates identified by at least one of our search algorithms with a probability of astrophysical origin $p_\mathrm{astro} > 0.5$. Of these, 18 were previously reported as low-latency public alerts, and 17 are reported here for the first time. Based upon estimates for the component masses, our O3b candidates with $p_\mathrm{astro} > 0.5$ are consistent with gravitational-wave signals from binary black holes or neutron star-black hole binaries, and we identify none from binary neutron stars. However, from the gravitational-wave data alone, we are not able to measure matter effects that distinguish whether the binary components are neutron stars or black holes. The range of inferred component masses is similar to that found with previous catalogs, but the O3b candidates include the first confident observations of neutron star-black hole binaries. Including the 35 candidates from O3b in addition to those from GWTC-2.1, GWTC-3 contains 90 candidates found by our analysis with $p_\mathrm{astro} > 0.5$ across the first three observing runs. These observations of compact binary coalescences present an unprecedented view of the properties of black holes and neutron stars. ","GWTC-3: Compact Binary Coalescences Observed by LIGO and Virgo During
  the Second Part of the Third Observing Run"
102,1457535272678166528,60893773,James Bullock,"['New paper led by @UCIPhysAstro PhD student @danphysic uses FIRE simulations to predict J-factors for DM-indirect detection @ Galactic Center ➡️Velocity-dependent p &amp; d-wave annihilation amplified compared to dark-matter-only expectations @UCIrvineGD <LINK> <LINK>', ""For p-wave annihilation ~(v/c)^2, standard expectation is these models not detectable because v&lt;&lt;c in Milky Way.  We find signal is ~10 times higher than you'd expect from dark-matter-only =&gt; p-wave models w/ correct thermal abundance may be within the range of detection! https://t.co/QAT5V6A4jF"", 'The reason for enhancement is that dark-matter particle velocities are significantly enhanced in the Galactic Center compared to dark-matter-only simulations - a result of galaxy formation https://t.co/cjtJ9xRyDs', 'What fantastic work by Daniel McKeown @danphysic and wonderful collaborators @AstronoMerc_ @ZachHafen @MBKplus @AndrewWetzel @linoush95  @PFHopkins_Astro @AstroBananna - Congratulations! https://t.co/IzgRLdyiwO', '@TillSawala @UCIPhysAstro @danphysic @UCIrvineGD For p &amp; d-wave, yes. Dwarfs harder. Clusters easier though.', '@ProfTimTait @danphysic @AstronoMerc_ @ZachHafen @MBKplus @AndrewWetzel @linoush95 @PFHopkins_Astro @AstroBananna Yes!  Simona was on @danphysic advancement so has a heads up!']",https://arxiv.org/abs/2111.03076,"We use FIRE-2 zoom cosmological simulations of Milky Way size galaxy halos to calculate astrophysical J-factors for dark matter annihilation and indirect detection studies. In addition to velocity-independent (s-wave) annihilation cross sections $\sigma_v$, we also calculate effective J-factors for velocity-dependent models, where the annihilation cross section is either either p-wave ($\propto v^2/c^2$) or d-wave ($\propto v^4/c^4$). We use 12 pairs of simulations, each run with dark-matter-only (DMO) physics and FIRE-2 physics. We observe FIRE runs produce central dark matter velocity dispersions that are systematically larger than in DMO runs by factors of $\sim 2.5-4$. They also have a larger range of central ($\sim 400$ pc) dark matter densities than the DMO runs ($\rho_{\rm FIRE}/\rho_{\rm DMO} \simeq 0.5 - 3$) owing to the competing effects of baryonic contraction and feedback. At 3 degrees from the Galactic Center, FIRE J-factors are $5-50$ (p-wave) and $15-500$ (d-wave) times higher than in the DMO runs. The change in s-wave signal at 3 degrees is more modest and can be higher or lower ($\sim 0.3-6$), though the shape of the emission profile is flatter (less peaked towards the Galactic Center) and more circular on the sky in FIRE runs. Our results for s-wave are broadly consistent with the range of assumptions in most indirect detection studies. We observe p-wave J-factors that are significantly enhanced compared to most past estimates. We find that thermal models with p-wave annihilation may be within range of detection in the near future. ","Amplified J-factors in the Galactic Center for velocity-dependent
  darkmatter annihilation in FIRE simulations"
103,1457531860779667467,187324749,Yjan Gordon,"['New paper led by @JonnyPierce1 on the role of galaxy mergers in AGN triggering <LINK>\nWe show that the excess of AGN in merging systems (relative to a control sample) is correlated with [O III] luminosity - no correlation is found wrt radio power. <LINK>', 'We also show that radio AGN tend to be post-coalescence while optical AGN are pre-coalescence. Together these results suggest that: \na) AGN in merging systems are generally more radiatively efficient, and\nb) may become radio loud as the merger evolves. https://t.co/liIYDpPgIe']",https://arxiv.org/abs/2111.03075,"Investigation of the triggering mechanisms of radio AGN is important for improving our general understanding of galaxy evolution. In the first paper in this series, detailed morphological analysis of high-excitation radio galaxies (HERGs) with intermediate radio powers suggested that the importance of triggering via galaxy mergers and interactions increases strongly with AGN radio power and weakly with optical emission-line luminosity. Here, we use an online classification interface to expand our morphological analysis to a much larger sample of 155 active galaxies (3CR radio galaxies, radio-intermediate HERGs and Type 2 quasars) that covers a broad range in both 1.4 GHz radio power and [OIII]$\lambda$5007 emission-line luminosity. All active galaxy samples are found to exhibit excesses in their rates of morphological disturbance relative to 378 stellar-mass- and redshift-matched non-active control galaxies classified randomly and blindly alongside them. These excesses are highest for the 3CR HERGs (4.7$\sigma$) and Type 2 quasar hosts (3.7$\sigma$), supporting the idea that galaxy mergers provide the dominant triggering mechanism for these subgroups. When the full active galaxy sample is considered, there is clear evidence to suggest that the enhancement in the rate of disturbance relative to the controls increases strongly with [OIII]$\lambda$5007 emission-line luminosity but not with 1.4 GHz radio power. Evidence that the dominant AGN host types change from early-type galaxies at high radio powers to late-type galaxies at low radio powers is also found, suggesting that triggering by secular, disk-based processes holds more importance for lower-power radio AGN. ","Do AGN triggering mechanisms vary with radio power? II. The importance
  of mergers as a function of radio power and optical luminosity"
104,1457525905711042570,901266828655284225,Brian Metzger,"['<LINK>  Perfect timing for new paper on a new type of transient. Punchline: while kilonovae signal the birth of light black holes (from NS mergers), ""Super-kilonovae"" may accompany the birth of the most massive (stellar mass) LIGO BHs.  @astroVAV @amanagarawal20', ""Basic idea: scale up collapsars (progenitors of long GRBs) to extremely massive stars above the pair-instability mass gap.  When trying to feed a newly formed BH at such high rates, much of the in-falling star doesn't make it in, instead being ejected in accretion disk outflows."", 'We estimate these neutron-rich outflows generate r-process elements, with  yields ~10s of Msun (~100 times higher than in neutron star mergers like GW170817 and ~10 times higher than ""ordinary"" low-mass collapsars).', 'This results in a lower final mass BH than one would predict from the progenitor He core, allowing to fill-in the pair-instability mass-gap ""from above,"" and providing a speculative channel for generating massive BBH like the components GW190521. https://t.co/txzEAL7dEe', 'This large mass ejection powers a radioactively-powered transient much brighter and longer-lasting than ordinary kilonovae, similar to SNe but much redder (near-IR peak); hence ""Super-kilonova"".  Roman Space Telescope could potentially discover these transients out to z ~ 1. https://t.co/omoLHbqK11', 'The nominal BH accretion rates achieved are also significantly higher than ordinary collapsars, allowing to power particularly energetic gamma-ray bursts.  SuperKN should be searched for following the most energetic GRBs by JWST.', 'Gravitational instabilities in these massive disks could also generate GW emission accessible to 3G detectors; unlike in the ""chirp"" in CO mergers, the GW signal decreases in frequency with time as the disk grows in radius (we term ""sad trombone"", to borrow a term from the FRBs). https://t.co/QSmoC79fML', 'Overall a fun ""ideas"" paper to work out, which puts together some ideas already germinating out there in the community, and completed by a talented group of young researchers, most of which overlapped here at Columbia/CCA over past few years.']",https://arxiv.org/abs/2111.03094,"The core collapse of rapidly rotating massive ~10 Msun stars (""collapsars""), and resulting formation of hyper-accreting black holes, are a leading model for the central engines of long-duration gamma-ray bursts (GRB) and promising sources of r-process nucleosynthesis. Here, we explore the signatures of collapsars from progenitors with extremely massive helium cores >130 Msun above the pair-instability mass gap. While rapid collapse to a black hole likely precludes a prompt explosion in these systems, we demonstrate that disk outflows can generate a large quantity (up to >50 Msun) of ejecta, comprised of >5-10 Msun in r-process elements and ~0.1-1 Msun of $^{56}$Ni, expanding at velocities ~0.1c. Radioactive heating of the disk-wind ejecta powers an optical/infrared transient, with a characteristic luminosity $\sim 10^{42}$ erg s$^{-1}$ and spectral peak in the near-infrared (due to the high optical/UV opacities of lanthanide elements) similar to kilonovae from neutron star mergers, but with longer durations $\gtrsim$ 1 month. These ""super-kilonovae"" (superKNe) herald the birth of massive black holes >60 Msun, which, as a result of disk wind mass-loss, can populate the pair-instability mass gap 'from above' and could potentially create the binary components of GW190521. SuperKNe could be discovered via wide-field surveys such as those planned with the Roman Space Telescope or via late-time infrared follow-up observations of extremely energetic GRBs. Gravitational waves of frequency ~0.1-50 Hz from non-axisymmetric instabilities in self-gravitating massive collapsar disks are potentially detectable by proposed third-generation intermediate and high-frequency observatories at distances up to hundreds of Mpc; in contrast to the ""chirp"" from binary mergers, the collapsar gravitational-wave signal decreases in frequency as the disk radius grows (""sad trombone""). ","""Super-Kilonovae"" from Massive Collapsars as Signatures of Black-Hole
  Birth in the Pair-instability Mass Gap"
105,1456967470347079681,1003652696723873792,Max Gaspari,"['This new paper is a great example of how to link the 3 major scales (micro/meso/macro) of BH feeding &amp; feedback, leveraging both multiphase observations and simulations, as suggested in the #BlackHoleWeather framework:\n<LINK>\n<LINK>\n#astrophysics <LINK>']",https://arxiv.org/abs/2111.02683,"Supermassive black holes and supernovae explosions at the centres of active galaxies power cycles of outflowing and inflowing gas that affect galactic evolution and the overall structure of the Universe. While simulations and observations show that this must be the case, the range of physical scales (over ten orders of magnitude) and paucity of available tracers, make both the simulation and observation of these effects difficult. By serendipity, there lies an active galaxy, Centaurus A (NGC 5128), at such a close proximity as to allow its observation over this entire range of scales and across the entire electromagnetic spectrum. In the radio band, however, details on scales of 10-100 kpc from the supermassive black hole have so far been obscured by instrumental limitations. Here we report low-frequency radio observations that overcome these limitations and show evidence for a broad, bipolar outflow with velocity 1100 km per s and mass outflow rate of 2.9 solar masses per year on these scales. We combine our data with the plethora of multi-scale, multi-wavelength historical observations of Centaurus A to probe a unified view of feeding and feedback, which we show to be consistent with the Chaotic Cold Accretion self-regulation scenario. ",Multi-scale feedback and feeding in the closest radio galaxy Centaurus A
106,1456695666814636036,1003652696723873792,Max Gaspari,"['New paper on multiscale BH #feeding &amp; #feedback in the beautiful CenA galaxy (with B. McKinley), majorly contributing to the theory part. Another strong case supporting the #ChaoticColdAccretion unified scenario and pillar for #BlackHoleWeather.\n<LINK>\n#astronomy <LINK>']",https://arxiv.org/abs/2111.02683,"Supermassive black holes and supernovae explosions at the centres of active galaxies power cycles of outflowing and inflowing gas that affect galactic evolution and the overall structure of the Universe. While simulations and observations show that this must be the case, the range of physical scales (over ten orders of magnitude) and paucity of available tracers, make both the simulation and observation of these effects difficult. By serendipity, there lies an active galaxy, Centaurus A (NGC 5128), at such a close proximity as to allow its observation over this entire range of scales and across the entire electromagnetic spectrum. In the radio band, however, details on scales of 10-100 kpc from the supermassive black hole have so far been obscured by instrumental limitations. Here we report low-frequency radio observations that overcome these limitations and show evidence for a broad, bipolar outflow with velocity 1100 km per s and mass outflow rate of 2.9 solar masses per year on these scales. We combine our data with the plethora of multi-scale, multi-wavelength historical observations of Centaurus A to probe a unified view of feeding and feedback, which we show to be consistent with the Chaotic Cold Accretion self-regulation scenario. ",Multi-scale feedback and feeding in the closest radio galaxy Centaurus A
107,1456646000034996224,1365703285521477640,Collin Christy,"['My first first-author paper is now up on the arxiv! <LINK> \n \nWe present the first results of @AssasnCitizen, a citizen science project dedicated to the classification of variable star candidates present in ASAS-SN’s new g-band data. Here’s what we found!', 'Citizen ASAS-SN is a citizen science project hosted by @the_zooniverse, the world’s largest hub for people-powered research. This project tasks users to classify variable candidates based on their phased light curves. Below is an example of the workflow we present to users. https://t.co/mAzqnXd1MJ', 'To start, we looked at ~7 million stars located near the south celestial pole, and through simple cuts in the light curve statistics, we whittled down this dataset to a sample of ~40,000 variable candidates. https://t.co/Neo9oYq1GY', 'Using the previously identified variables in our sample, we found that our users classified eclipsing and pulsating systems quite well but struggled to correctly classify rotators. https://t.co/fyRIgRiTOT', 'Here are a few examples of light curves our users had an easy time classifying! They are quite pretty :) https://t.co/d28FAys3tA', 'After removing the previously cataloged variables and candidates voted as “Junk”, our users helped us discover over 10,000 new variable sources!! Below we show the projection of these new sources around the south pole along with our candidate, non-Junk, and crossmatched sample. https://t.co/8hVeSZalIo', 'Our users were able to classify light curves with spurious variability (aka ""Junk"") very well! The concentric ring pattern in the upper left map is caused by aliased periods in some of the light curves where our fields overlap. Once we remove these light curves, this goes away!', 'We retrained the previous V-band machine learning classifier using features from our new g-band data and archival photometry (incl. Gaia ED3). The ASAS-SN g-band catalog of variable stars will include classifications from both the g-band classifier and our citizen scientists! https://t.co/yfnaY3qQNI', 'We found that our citizen scientists often pointed out weird and unusual light curves on our projects Talk forum. The identification and study of these oddball stars are of great importance as they can lead to new astrophysical insights! https://t.co/WukO1CjLWK', 'We have many more light curves waiting to be classified and more unusual variables to be found!\n\nCitizen ASAS-SN: https://t.co/qW8QRMThtJ', 'I Would also encourage you all to check out the new Citizen ASAS-SN tab on the ASAS-SN webpage, where you can access the new g-band catalog of variable stars, their light curves, and a list of all of those who contributed! \n\nhttps://t.co/rdwlzER6Tt', 'Lastly, I would like to thank @j_tharindu, @SuperASASSN, Chris Kochanek, @ZachWay96, @JosePrietoK, @BenShappee, Tom Holoien, @chargedcurrent, @dad_ders, and all of our volunteers on Citizen ASAS-SN. Without you all, this work would not have been possible!', 'For those of you who are interested in the field of variable stars, I heavily suggest you check out our new variable star atlas. This was made possible thanks to the hard work by @j_tharindu and @dad_ders!\n\nhttps://t.co/XMq6SSsfFu']",https://arxiv.org/abs/2111.02415,"We present the first results from Citizen ASAS-SN, a citizen science project for the All-Sky Automated Survey for Supernovae (ASAS-SN) hosted on the Zooniverse platform. Citizen ASAS-SN utilizes the newer, deeper, higher cadence ASAS-SN $g$-band data and tasks volunteers to classify periodic variable star candidates based on their phased light curves. We started from 40,640 new variable candidates from an input list of ${\sim} 7.4$ million stars with $\delta < -60^\circ$ and the volunteers identified 10,420 new discoveries which they classified as 4,234 pulsating variables, 3,132 rotational variables, 2,923 eclipsing binaries, and 131 variables flagged as Unknown. They classified known variable stars with an accuracy of 89% for pulsating variables, 81% for eclipsing binaries, and 49% for rotational variables. We examine user performance, agreement between users, and compare the citizen science classifications with our machine learning classifier updated for the $g$-band light curves. In general, user activity correlates with higher classification accuracy and higher user agreement. We used the user's ""Junk"" classifications to develop an effective machine learning classifier to separate real from false variables, and there is a clear path for using this ""Junk"" training set to significantly improve our primary machine learning classifier. We also illustrate the value of Citizen ASAS-SN for identifying unusual variables with several examples. ","Citizen ASAS-SN Data Release I: Variable Star Classification Using
  Citizen Science"
108,1456615646003859456,215220352,Nikos Stylianou,"['New paper with @ivlahavas: CoreLM: Coreference-aware Language Model Fine-Tuning. It is set to appear in Fourth Workshop on Computational Models of Reference, Anaphora and Coreference (CRAC) at #EMNLP2021. \n\nPaper: <LINK>\n🧵1/6', 'In this work we investigate the use of Coreference Resolution during Fine-Tuning as a better strategy to Fine-Tuning Language Models, beyond just simple Fine-Tuning on new data.  For that reason we propose the CoreLM framework, which wraps around pretrained LMs. 2/6 https://t.co/CS5dpEuVYb', 'The framework is comprised by an Entity-Gating layer and an Entity handling mechanism that allow us to use Coreference annotations at a word level to create explicit connections between words that refer to the same real world entity. 3/6 https://t.co/LDMwxgBVrW', 'We use this connections to infuse information past the context window of the models and evaluate their performance compared to a Fine-Tuned version of the same model in Zero-Shot Language Modeling and Cloze Testing. 4/6 https://t.co/e4Ckzc5eaE', 'We discovered that Fine-Tuning using predicted Coreference clusters improved performance in word categories that were involved in the clusters, either directly or indirectly. However, errors in the predictions resulted in wrong word predictions in cloze tasks. 5/6 https://t.co/gC3iCqUp85', 'We conclude that Coreference Resolution can be used to augment the Fine-Tuning process and achieve better results than simple Fine-Tuning, in this setting. We will extensively evaluate the proposed methods in more tasks and with different base LMs to explore further. 6/6']",https://arxiv.org/abs/2111.02687,"Language Models are the underpin of all modern Natural Language Processing (NLP) tasks. The introduction of the Transformers architecture has contributed significantly into making Language Modeling very effective across many NLP task, leading to significant advancements in the field. However, Transformers come with a big computational cost, which grows quadratically with respect to the input length. This presents a challenge as to understand long texts requires a lot of context. In this paper, we propose a Fine-Tuning framework, named CoreLM, that extends the architecture of current Pretrained Language Models so that they incorporate explicit entity information. By introducing entity representations, we make available information outside the contextual space of the model, which results in a better Language Model for a fraction of the computational cost. We implement our approach using GPT2 and compare the fine-tuned model to the original. Our proposed model achieves a lower Perplexity in GUMBY and LAMBDADA datasets when compared to GPT2 and a fine-tuned version of GPT2 without any changes. We also compare the models' performance in terms of Accuracy in LAMBADA and Children's Book Test, with and without the use of model-created coreference annotations. ",CoreLM: Coreference-aware Language Model Fine-Tuning
109,1456601775444725761,780011537738174464,Dennis Soemers,"['(1/5) Our new paper on ""Optimised Playout Implementations for the Ludii General Game System"" is now available on arXiv! <LINK> <LINK>', '(2/5) Based on the game descriptions in the game description language of @LudiiGames, we automatically detect whether games fit in any of a few broad classes of games that permit significant optimisations for (semi-)random playouts / rollouts (as used in e.g. MCTS)', '(3/5) One example is in games like Chess, where legal + illegal (due to some postcondition) moves (blue + red arrows) are cheap to compute, but filtering this down to only the legal (blue) moves is expensive. https://t.co/su59U36SFW', '(4/5) In rollouts, we sample moves (uniformly or otherwise) from the complete set, and only evaluate the expensive postconditions for moves that were sampled (rejecting them and resampling if postconditions are not satisfied). The same idea applies to liberty tests in Go.', '(5/5) This work will be presented at ACG 2021 in about two and a half weeks. Register for free at https://t.co/hPWDBa893J!\n\n@LudiiGames @archaeoludology @UM_DKE @ERC_Research']",https://arxiv.org/abs/2111.02839,"This paper describes three different optimised implementations of playouts, as commonly used by game-playing algorithms such as Monte-Carlo Tree Search. Each of the optimised implementations is applicable only to specific sets of games, based on their rules. The Ludii general game system can automatically infer, based on a game's description in its general game description language, whether any optimised implementations are applicable. An empirical evaluation demonstrates major speedups over a standard implementation, with a median result of running playouts 5.08 times as fast, over 145 different games in Ludii for which one of the optimised implementations is applicable. ",Optimised Playout Implementations for the Ludii General Game System
110,1456576016466563073,1655709104,Phil Barbeau,"['Our new @COHERENT_NUS quenching factor paper is out. This was a long time coming, but a real tour de force to understand many small effects. It seems silly to get excited about a calibration, but in this case, it makes so much physics possible. <LINK>']",https://arxiv.org/abs/2111.02477#,"We present results of several measurements of CsI[Na] scintillation response to 3-60 keV energy nuclear recoils performed by the COHERENT collaboration using tagged neutron elastic scattering experiments and an endpoint technique. Earlier results, used to estimate the coherent elastic neutrino-nucleus scattering (CEvNS) event rate for the first observation of this process achieved by COHERENT at the Spallation Neutron Source (SNS), have been reassessed. We discuss corrections for the identified systematic effects and update the respective uncertainty values. The impact of updated results on future precision tests of CEvNS is estimated. We scrutinize potential systematic effects that could affect each measurement. In particular we confirm the response of the H11934-200 Hamamatsu photomultiplier tube (PMT) used for the measurements presented in this study to be linear in the relevant signal scale region. ","Measurement of scintillation response of CsI[Na] to low-energy nuclear
  recoils by COHERENT"
111,1456538097185923102,1238481001304686594,Pablo Martínez-Miravé,"['New paper! With @MariamTortola and Susana Molina Sedgwick\n\n""Non-standard interactions from the future neutrino solar sector""👇\n\n<LINK>\n\nWe study the pontential of a combined analysis of JUNO and Hyper-Kamiokande in the presence of non-standard interactions(NSI)', 'We show that strong constraints on NSI can be derived in that case, while ensuring an accurate determination of the oscillation parameters.\n\nWe also illustrate the nice complementarity between both experiments 😊 https://t.co/BaAC8JE7NP']",https://arxiv.org/abs/2111.03031,"The next-generation neutrino experiment JUNO will determine the solar oscillation parameters - $\sin^2 \theta_{12}$ and $\Delta m^2_{21}$ - with great accuracy, in addition to measuring $\sin^2\theta_{13}$, $\Delta m^2_{31}$, and the mass ordering. In parallel, the continued study of solar neutrinos at Hyper-Kamiokande will provide complementary measurements in the solar sector. In this paper, we address the expected sensitivity to non-universal and flavour-changing non-standard interactions (NSI) with $d$-type quarks from the combination of these two future neutrino experiments. We also show the robustness of their measurements of the solar parameters $\sin^2 \theta_{12}$ and $\Delta m^2_{21}$ in the presence of NSI. We study the impact of the exact experimental configuration of the Hyper-Kamiokande detector, and conclude it is of little relevance in this scenario. Finally, we find that the LMA-D solution is expected to be present if no additional input from non-oscillation experiments is considered. ",Non-standard interactions from the future neutrino solar sector
112,1456438500127952899,781250438725308416,Nicholas Galitzki,"[""As if today wasn't great enough, new paper alert!!!🚨🚨🚨 @SimonsObs galactic science paper (or should I say 'science of the local cosmic ecosystem'?) is on the arxiv: <LINK> Perhaps a nice intermezzo between #Astro2020 chapters?""]",https://arxiv.org/abs/2111.02425#,"Observing in six frequency bands from 27 to 280 GHz over a large sky area, the Simons Observatory (SO) is poised to address many questions in Galactic astrophysics in addition to its principal cosmological goals. In this work, we provide quantitative forecasts on astrophysical parameters of interest for a range of Galactic science cases. We find that SO can: constrain the frequency spectrum of polarized dust emission at a level of $\Delta\beta_d \lesssim 0.01$ and thus test models of dust composition that predict that $\beta_d$ in polarization differs from that measured in total intensity; measure the correlation coefficient between polarized dust and synchrotron emission with a factor of two greater precision than current constraints; exclude the non-existence of exo-Oort clouds at roughly 2.9$\sigma$ if the true fraction is similar to the detection rate of giant planets; map more than 850 molecular clouds with at least 50 independent polarization measurements at 1 pc resolution; detect or place upper limits on the polarization fractions of CO(2-1) emission and anomalous microwave emission at the 0.1% level in select regions; and measure the correlation coefficient between optical starlight polarization and microwave polarized dust emission in $1^\circ$ patches for all lines of sight with $N_{\rm H} \gtrsim 2\times10^{20}$ cm$^{-2}$. The goals and forecasts outlined here provide a roadmap for other microwave polarization experiments to expand their scientific scope via Milky Way astrophysics. ",The Simons Observatory: Galactic Science Goals and Forecasts
113,1456427057513041924,1159960736162095104,Robert Edward Grant,['New Galactic Spiral / Polygonal Right Triangle / Modular Spirals Paper just published on Arxiv .org! \n\nA Novel Geometric Model of Natural Spirals Based on Right Triangle Polygonal Modular Formations <LINK>'],https://arxiv.org/abs/2111.02895,"We propose a novel class of spirals that are based on perfect polygonal formations. The spirals are defined by a fractal of right triangles that delineate their geometry and determine their progression rates and modes. We show how these spirals match naturally occurring ones, including hurricanes and galaxies, much better than already proposed models, allowing for more accurate categorization of these phenomena and better prediction of their physical properties. ","A Novel Geometric Model of Natural Spirals Based on Right Triangle
  Polygonal Modular Formations"
114,1456318555599687682,955103299,Marina Radulaski,['The 4th figure of #Rlab new perspective paper is a masterpiece assembled by @sridhar_majety reviewing 3 generations of Quantum Repeaters. Silicon carbide color centers have superior coherence and spectral properties for these key quantum network elements \n\n<LINK> <LINK> <LINK>'],https://arxiv.org/abs/2111.00136,"Color centers in wide band gap semiconductors are prominent candidates for solid-state quantum technologies due to their attractive properties including optical interfacing, long coherence times, spin-photon and spin-spin entanglement, as well as the potential for scalability. Silicon carbide color centers integrated into photonic devices span a wide range of applications in quantum information processing, in a material platform with quantum-grade wafer availability and advanced processing capabilities. Recent progress in emitter generation and characterization, nanofabrication, device design, and quantum optical studies have amplified the scientific interest in this platform. We provide a conceptual and quantitative analysis of the role of silicon carbide integrated photonics in three key application areas: quantum networking, simulation, and computing. ",Quantum Information Processing With Integrated Silicon Carbide Photonics
115,1456230190510653447,269085275,Andrew Jesson 🇺🇦,"['📢 New #NeurIPS2021 paper with @ptigas 📢\n\nCausal-BALD: Deep Bayesian Active Learning of Outcomes to Infer Treatment-Effects from Observational Data\n\n📄 <LINK>\n🖥️ <LINK>\n\nWith @joost_v_amersf @BlackHC @ShalitUri @yaringal @OATML_Oxford\n\n🧵1/10 <LINK>', '▪️The Motive▪️\n\nKnowing the effect of a cancer treatment depends on knowing the cancer status of a new lesion\n\nBut, certainty about cancer status requires performing an invasive biopsy\n\nHow do we maximize knowledge about the treatment effect, while minimizing biopsy harm?\n\n🧵2/10 https://t.co/8ieJH3Dt14', '▪️The Framework▪️\n\nWe explore the efficient acquisition of labeled outcomes from high-dimensional, large-sample observational data consisting of controlled and treated examples in order to learn about personal-level treatment-effects\n\n📄 https://t.co/mX3HFmnTvM\n\n🧵3/10 https://t.co/34wZYhX2Ai', '▪️The Challenge▪️\n\nLimited overlap between the control and treated populations challenges data efficiency\n\nThere is non-overlapping support in regions 1 and 5, so we cannot know the treatment effect there\n\nLabel acquisition should focus on regions 2, 3, and 4\n\n🧵4/10 https://t.co/F8UE0iKqdE', '🚩Random Acquisition is Biased to the Mode🚩\n\nRandom acquisition gives an unbiased sample of the pool data, but is it data-efficient?\n\nNo, it over-samples from the mode, resulting in confidence and accuracy for high-frequency data, at the expense of under-represented data\n\n🧵5/10 https://t.co/a3ffyM0QvC', '🚩Naive BALD is Biased to Regions with Non-Overlapping Support🚩\n\nIs acquisition based on uncertainty in the treatment-effect data-efficient?\n\nNo, it acquires data with high uncertainty about the counterfactual, even if no counterfactual example exists!\n\n🧵6/10 https://t.co/ncZDSHoFDy', '▪️The Remedy▪️\n\nIn our #NeurIPS2021 paper, we introduce several causal BALD objectives that efficiently acquire labels for both high-frequency and under-represented data with identifiable treatment effects\n\n📄 https://t.co/mX3HFmnTvM\n\n🧵7/10 https://t.co/d77AMHG62g', '▪️The Results▪️\n\nWe theoretically justify our causal acquisition functions and show significant improvement over baselines on synthetic and semi-synthetic datasets\n\nClone our github repo to reproduce these results!\n\n🖥️ https://t.co/0YmQlBdHZG\n\n🧵8/10 https://t.co/WPAprWB3mz', '▪️The Giants▪️\n\nWe recommend recent works by Qin, @TianZuoWang1 and Zhou https://t.co/SSjXEScRIF and @iirisSun, @pschulam, et al. https://t.co/IP5J4FLLg2\n\nFoundational work by Deng, Pineau, and @SusanMurphylab1 https://t.co/4emoZixI2C\n\nPlus @alexdamour et al. on overlap\n\n🧵9/10', '▪️The Feedback▪️\n\nDMs are open to myself and @ptigas if you have any questions or concerns about this work\n\n🧵10/10', '@JakobSchwerter @ShalitUri @ptigas @joost_v_amersf @BlackHC @yaringal @OATML_Oxford Thanks! Not yet, no']",https://arxiv.org/abs/2111.02275,"Estimating personalized treatment effects from high-dimensional observational data is essential in situations where experimental designs are infeasible, unethical, or expensive. Existing approaches rely on fitting deep models on outcomes observed for treated and control populations. However, when measuring individual outcomes is costly, as is the case of a tumor biopsy, a sample-efficient strategy for acquiring each result is required. Deep Bayesian active learning provides a framework for efficient data acquisition by selecting points with high uncertainty. However, existing methods bias training data acquisition towards regions of non-overlapping support between the treated and control populations. These are not sample-efficient because the treatment effect is not identifiable in such regions. We introduce causal, Bayesian acquisition functions grounded in information theory that bias data acquisition towards regions with overlapping support to maximize sample efficiency for learning personalized treatment effects. We demonstrate the performance of the proposed acquisition strategies on synthetic and semi-synthetic datasets IHDP and CMNIST and their extensions, which aim to simulate common dataset biases and pathologies. ","Causal-BALD: Deep Bayesian Active Learning of Outcomes to Infer
  Treatment-Effects from Observational Data"
116,1456212359375110147,891766035250122757,Rosie Talbot,"['My new paper is up on arXiv today 💫 We used simulations to explore the diverse range of large-scale outflows that can be produced by AGN jets. Have a read here: <LINK>', 'We considered different black hole and environmental properties and found outflows that range from light, hot and collimated structures to highly mass-loaded, multiphase, bipolar winds. https://t.co/dkyERLzdXI', 'As always, a massive massive thanks to @matplotlib 💫⭐️', '@SophiLilleengen Thank you ☺️', '@Rohan_Naidu Thanks!! 😊', '@MarculewiczM Thanks!! That’s very kind ☺️']",https://arxiv.org/abs/2111.01801,"Recent observations of Seyfert galaxies indicate that low power, misaligned jets can undergo significant interaction with the gas in the galactic disc and may be able to drive large-scale, multiphase outflows. We apply our novel sub-grid model for Blandford-Znajek jets to simulations of the central regions of Seyferts, in which a black hole is embedded in a dense, circumnuclear disc (CND) and surrounded by a dilute circumgalactic medium (CGM). We find that the variability of the accretion flow is highly sensitive both to the jet power and to the CND thermodynamics and, ultimately, is determined by the complex interplay between jet-driven outflows and backflows. Even at moderate Eddington ratios, AGN jets are able to significantly alter the thermodynamics and kinematics of CNDs and entrain up to 10% of their mass in the outflow. Mass outflow rates and kinetic powers of the warm outflowing component are in agreement with recent observations for black holes with similar bolometric luminosities, with outflow velocities that are able to reach 500 km/s. Depending on their power and direction, jets are able to drive a wide variety of large-scale outflows, ranging from light, hot and collimated structures to highly mass-loaded, multiphase, bipolar winds. This diversity of jet-driven outflows highlights the importance of applying physically motivated models of AGN feedback to realistic galaxy formation contexts. Such simulations will play a crucial role in accurately interpreting the wealth of data that next generation facilities such as JWST, SKA and Athena will provide. ","Blandford-Znajek jets in galaxy formation simulations: exploring the
  diversity of outflows produced by spin-driven AGN jets"
117,1456183903811620872,595554321,Jason D Lotay,"['New paper on Flows of G2-structures on contact Calabi-Yau 7-manifolds, with Henrique Sa Earp and his recently completed PhD student Julieth Saavadra from Unicamp @unicampoficial in Brazil, is now online: <LINK>. Supported by @royalsociety and @SimonsFdn.']",http://arxiv.org/abs/2111.01841,"We study the Laplacian flow and coflow on contact Calabi-Yau $7$-manifolds. We show that the natural initial condition leads to an ancient solution of the Laplacian flow with a finite time Type I singularity which is not a soliton, whereas it produces an immortal (though not eternal and not self-similar) solution of the Laplacian coflow which has an infinite time singularity, that is Type IIb unless the transverse Calabi--Yau geometry is flat. The flows in each case collapse (after normalising the volume) to a lower-dimensional limit, which is either $\mathbb{R}$ for the Laplacian flow or standard $\mathbb{C}^3$ for the Laplacian coflow. We also study the Hitchin flow in this setting, which we show coincides with the Laplacian coflow up to reparametrisation of time, and defines an (incomplete) Calabi--Yau structure on the spacetime track of the flow. ",Flows of $\mathrm{G}_2$-structures on contact Calabi--Yau $7$-manifolds
118,1456175144683048961,805477439648440321,Rob Kavanagh,"['Delighted to have our new paper accepted by @RAS_Journals! We explored the variability of CMEs and associated radio emission with the changing surface magnetic field of Epsilon Eridani. Congrats to @dualta93 for leading this great work!\n\n<LINK> <LINK>', '@RAS_Journals @dualta93 Force of habit, I meant @AAS_Publishing! 🤠']",https://arxiv.org/abs/2111.02284,"We simulate possible stellar coronal mass ejection (CME) scenarios over the magnetic cycle of $\epsilon$ Eridani (18 Eridani; HD 22049). We use three separate epochs from 2008, 2011, and 2013, and estimate the radio emission frequencies associated with these events. These stellar eruptions have proven to be elusive, although a promising approach to detect and characterise these phenomena are low-frequency radio observations of potential type II bursts as CME induced shocks propagate through the stellar corona. Stellar type II radio bursts are expected to emit below 450 MHz, similarly to their solar counterparts. We show that the length of time these events remain above the ionospheric cutoff is not necessarily dependent on the stellar magnetic cycle, but more on the eruption location relative to the stellar magnetic field. We find that these type II bursts would remain within the frequency range of LOFAR for a maximum of 20-30 minutes post-eruption for the polar CMEs, (50 minutes for 2nd harmonics). We find evidence of slower equatorial CMEs, which result in slightly longer observable windows for the 2008 and 2013 simulations. Stellar magnetic geometry and strength has a significant effect on the detectability of these events. We place the CMEs in the context of the stellar mass-loss rate (27-48 $\times$ solar mass-loss rate), showing that they can amount to 3-50% of the stellar wind mass-loss rate for $\epsilon$ Eridani. Continuous monitoring of likely stellar CME candidates with low-frequency radio telescopes will be required to detect these transient events. ","Coronal mass ejections and type II radio emission variability during a
  magnetic cycle on the solar-type star $\epsilon$ Eridani"
119,1456167325732904968,1107605372905377792,Aku Venhola🇺🇦,"['New paper in ArXiv today about low surface brightness dwarfs in the Fornax cluster:\n<LINK>\nIn this paper, we applied MTO to find new galaxies from the FDS data. This way we were able to extend our previous FDS dwarf catalog to 821 dwarfs. Below some key findings:', '- Massive dwarfs (log(M*)&gt;7) show correlation between axis-ratio and surface brightness, which can be corrected by inclination correction -&gt; this is not the case for log(M*)&lt;7 dwarfs. Interpretation: disks vs. not disks?', '-Surface brightness of dwarfs become fainter toward the cluster center. This is partly due to the ageing of their stellar populations but we do find evidence for tidal interactions as well: more disturbed morphology, lower axis-ratios and lower surface density toward the center.', '-We found the low-luminosity end slope of the galaxy luminosity function to be -1.4, which is consistent with findings in other environments studied with similar data. However, the match is not that great with simulations (resolution issues?).', 'Conclusion: the Fornax galaxy cluster makes dwarfs fainter by stopping their star formation and by lowering their density via tidal forces.']",https://arxiv.org/abs/2111.01855,"In this work we use Max-Tree Objects, (MTO) on the FDS data in order to detect previously undetected Low surface brightness (LSB) galaxies. After extending the existing Fornax dwarf galaxy catalogs with this sample, our goal is to understand the evolution of LSB dwarfs in the cluster. We also study the contribution of the newly detected galaxies to the faint end of the luminosity function. We test the detection completeness and parameter extraction accuracy of MTO. We then apply MTO to the FDS images to identify LSB candidates. The identified objects are fitted with 2D S\'ersic models using GALFIT and classified based on their morphological appearance, colors, and structure. With MTO, we are able to increase the completeness of our earlier FDS dwarf catalog (FDSDC) 0.5-1 mag deeper in terms of total magnitude and surface brightness. Due to the increased accuracy in measuring sizes of the detected objects, we also add many small galaxies to the catalog that were previously excluded as their outer parts had been missed in detection. We detect 265 new LSB dwarf galaxies in the Fornax cluster, which increases the total number of known dwarfs in Fornax to 821. Using the extended catalog, we show that the luminosity function has a faint-end slope of -1.38+/-0.02. We compare the obtained luminosity function with different environments studied earlier using deep data but do not find any significant differences. On the other hand, the Fornax-like simulated clusters in the IllustrisTNG cosmological simulation have shallower slopes than found in the observational data. We also find several trends in the galaxy colors, structure, and morphology that support the idea that the number of LSB galaxies is higher in the cluster center due to tidal forces and the age dimming of the stellar populations. The same result also holds for the subgroup of large LSB galaxies, so-called ultra-diffuse galaxies. ","The Fornax Deep Survey (FDS) with VST XII: Low surface brightness dwarf
  galaxies in the Fornax cluster"
120,1456078464847532032,1015053310603284480,Stephen Kane,"['While waiting for #Astro2020 to drop, take a look at this new paper regarding the planetary habitability lessons that can be learned from nearby, young systems, such as AU Mic. <LINK>', 'The system is ~20 Myrs old and the luminosity of the star is rapidly evolving. That means the incident flux and atmospheric loss processes are also rapidly changing. At the same time, degassing rates of the planet vary, dependent on initial volatile inventories.', 'We also examined orbital stability within the evolving Habitable Zone, showing that terrestrial planets can retain long-term stability. Such a young, nearby planet would be incredibly valuable for studying the earliest stages of planetary evolution.', 'My sincere thanks go to my wonderful co-authors: Brad Foley, @michelle_hill63, @itsatrappist, @mrtommyb, Bryson Cale, @EmDwarf, @PlavchanPeter, and Justin Wittrock. Many thanks to the referee for all of their great feedback. And of course, thank you all for reading this!']",https://arxiv.org/abs/2111.01816,"The diversity of planetary systems that have been discovered are revealing the plethora of possible architectures, providing insights into planet formation and evolution. They also increase our understanding of system parameters that may affect planetary habitability, and how such conditions are influenced by initial conditions. The AU~Mic system is unique among known planetary systems in that it is a nearby, young, multi-planet transiting system. Such a young and well characterized system provides an opportunity to study orbital dynamical and habitability studies for planets in the very early stages of their evolution. Here, we calculate the evolution of the Habitable Zone of the system through time, including the pre-main sequence phase that the system currently resides in. We discuss the planetary atmospheric processes occurring for an Earth-mass planet during this transitionary period, and provide calculations of the climate state convergence age for both volatile rich and poor initial conditions. We present results of an orbital dynamical analysis of the AU~Mic system that demonstrate the rapid eccentricity evolution of the known planets, and show that terrestrial planets within the Habitable Zone of the system can retain long-term stability. Finally, we discuss follow-up observation prospects, detectability of possible Habitable Zone planets, and how the AU Mic system may be used as a template for studies of planetary habitability evolution. ","Orbital Dynamics and the Evolution of Planetary Habitability in the AU
  Mic System"
121,1456076469772378114,1653127615,Chris Monahan,"['Our new paper just arrived on the arXiv <LINK>! We study how the momentum carried by quarks inside a neutron is related to their spin (relative to the neutron), directly from the theory of the strong nuclear force (which holds the proton together). #physics #QCD', ""@ZhongboK The neutron is very special! :) Yes, you are completely right, in the paper we discuss nucleons, but I thought maybe on twitter more people would have heard of a neutron than a nucleon. Of course that doesn't account for the fact that my followers are basically all physicists...""]",https://arxiv.org/abs/2111.01808,"We present a determination of the non-singlet transversity parton distribution function (PDF) of the nucleon, normalized with respect to the tensor charge at $\mu^2=2$ GeV$^2$ from lattice quantum chromodynamics. We apply the pseudo-distribution approach, using a gauge ensemble with a lattice spacing of 0.094 fm and the light quark mass tuned to a pion mass of 358 MeV. We extract the transversity PDF from the analysis of the short-distance behavior of the Ioffe-time pseudo-distribution using the leading-twist next-to-leading order (NLO) matching coefficients calculated for transversity. We reconstruct the $x$-dependence of the transversity PDF through an expansion in a basis of Jacobi polynomials in order to reduce the PDF ansatz dependence. Within the limitations imposed by a heavier-than-physical pion mass and a fixed lattice spacing, we present a comparison of our estimate for the valence transversity PDF with the recent global fit results based on single transverse spin asymmetry. We find the intrinsic nucleon sea to be isospin symmetric with respect to transversity. ","The transversity parton distribution function of the nucleon using the
  pseudo-distribution approach"
122,1456067109264449541,1108100809596813314,Ming Sun,['Our new paper on an edge-on spiral undergoing edge-on stripping    <LINK> <LINK>'],https://arxiv.org/abs/2111.01821,"Ram pressure stripping (RPS) is an important mechanism for galaxy evolution. In this work, we present results from HST and APEX observations of one RPS galaxy, ESO 137-002 in the closest rich cluster Abell 3627. The galaxy is known to host prominent X-ray and H$\alpha$ tails. The HST data reveal significant features indicative of RPS in the galaxy, including asymmetric distribution of dust in the galaxy, dust filaments and dust clouds in ablation generally aligned with the direction of ram pressure, and young star clusters immediately upstream of the residual dust clouds that suggest star formation (SF) triggered by RPS. The distribution of the molecular gas is asymmetric in the galaxy, with no CO upstream and abundant CO downstream and in the inner tail region. A total amount of $\sim 5.5 \times 10^{9}$ M$_\odot$ of molecular gas is detected in the galaxy and its tail. On the other hand, we do not detect any active SF in the X-ray and H$\alpha$ tails of ESO 137-002 with the HST data and place a limit on the SF efficiency in the tail. Hence, if selected by SF behind the galaxy in the optical or UV (e.g., surveys like GASP or using the Galex data), ESO 137-002 will not be considered a ``jellyfish'' galaxy. Thus, galaxies like ESO 137-002 are important for our comprehensive understanding of RPS galaxies and the evolution of the stripped material. ESO 137-002 also presents a great example of an edge-on galaxy experiencing a nearly edge-on RPS wind. ","ESO 137-002: a large spiral undergoing edge-on ram-pressure stripping
  with little star formation in the tail"
123,1455990822537764872,69202541,Jonathan Le Roux,"['New paper out: ""Sequence Transduction with Graph-based Supervision"", w/ N. Moritz, T. Hori, and S. Watanabe @shinjiw_at_cmu. We use our GTC loss to explore variations on RNN-T alignment rules &amp; show a CTC-like transducer system outperforms standard RNN-T.\n<LINK>']",https://arxiv.org/abs/2111.01272,"The recurrent neural network transducer (RNN-T) objective plays a major role in building today's best automatic speech recognition (ASR) systems for production. Similarly to the connectionist temporal classification (CTC) objective, the RNN-T loss uses specific rules that define how a set of alignments is generated to form a lattice for the full-sum training. However, it is yet largely unknown if these rules are optimal and do lead to the best possible ASR results. In this work, we present a new transducer objective function that generalizes the RNN-T loss to accept a graph representation of the labels, thus providing a flexible and efficient framework to manipulate training lattices, e.g., for studying different transition rules, implementing different transducer losses, or restricting alignments. We demonstrate that transducer-based ASR with CTC-like lattice achieves better results compared to standard RNN-T, while also ensuring a strictly monotonic alignment, which will allow better optimization of the decoding procedure. For example, the proposed CTC-like transducer achieves an improvement of 4.8% on the test-other condition of LibriSpeech relative to an equivalent RNN-T based system. ",Sequence Transduction with Graph-based Supervision
124,1455961909572685828,834684487334129665,Melody Huang,"[""Excited to share that a paper I've been working on with Naoki Egami, Erin Hartman, and Luke Miratrix (@LMiratrix) is on ArXiv now! \n<LINK> \nWe develop a new method to improve the generalization of experimental results (1/2)"", 'The method allows researchers to leverage population-level data to more efficiently generalize their causal effects, while guaranteeing consistency. We also provide a diagnostic measure that checks if using the method will result in improvements! (2/2)', '@LMiratrix Thanks Luke! Likewise! I definitely learned a ton from all of your comments and feedback!']",https://arxiv.org/abs/2111.01357,"Generalizing causal estimates in randomized experiments to a broader target population is essential for guiding decisions by policymakers and practitioners in the social and biomedical sciences. While recent papers developed various weighting estimators for the population average treatment effect (PATE), many of these methods result in large variance because the experimental sample often differs substantially from the target population, and estimated sampling weights are extreme. To improve efficiency in practice, we propose post-residualized weighting in which we use the outcome measured in the observational population data to build a flexible predictive model (e.g., machine learning methods) and residualize the outcome in the experimental data before using conventional weighting methods. We show that the proposed PATE estimator is consistent under the same assumptions required for existing weighting methods, importantly without assuming the correct specification of the predictive model. We demonstrate the efficiency gains from this approach through simulations and our application based on a set of job training experiments. ","Leveraging Population Outcomes to Improve the Generalization of
  Experimental Results"
125,1455943300679180298,877641073350369281,Aharon Brodutch,['New paper with the @QuantumAephraim lab.  Congrats to Noah and @YBYilmazQO on getting this experiment out and getting a lot of experimental work done under covid restrictions. A pleasure to work with @nicoleyh11 and @DavidArvShu for the first time.\n<LINK>'],https://arxiv.org/abs/2111.01194,"Operator noncommutation, a hallmark of quantum theory, limits measurement precision, according to uncertainty principles. Wielded correctly, though, noncommutation can boost precision. A recent foundational result relates a metrological advantage with negative quasiprobabilities -- quantum extensions of probabilities -- engendered by noncommuting operators. We crystallize the relationship in an equation that we prove theoretically and observe experimentally. Our proof-of-principle optical experiment features a filtering technique that we term partially postselected amplification (PPA). Using PPA, we measure a waveplate's birefringent phase. PPA amplifies, by over two orders of magnitude, the information obtained about the phase per detected photon. In principle, PPA can boost the information obtained from the average filtered photon by an arbitrarily large factor. The filter's amplification of systematic errors, we find, bounds the theoretically unlimited advantage in practice. PPA can facilitate any phase measurement and mitigates challenges that scale with trial number, such as proportional noise and detector saturation. By quantifying PPA's metrological advantage with quasiprobabilities, we reveal deep connections between quantum foundations and precision measurement. ","Negative quasiprobabilities enhance phase estimation in quantum-optics
  experiment"
126,1455940276309172227,939946148,Davide Meloni,['New paper! <LINK>'],https://arxiv.org/abs/2111.00329#,"Our knowledge on the active three-neutrino mixing angles ($\theta_{12}$, $\theta_{13}$, and $\theta_{23}$) and the Dirac CP phase $\delta_{CP}$ is becoming accurate day-by-day enabling us to test the unitarity of the PMNS matrix with high precision. Future long-baseline experiments will play an important role in this direction. In this work, we study the impact of possible non-unitary neutrino mixing (NUNM) in the context of next-generation long-baseline experiments DUNE and T2HKK/JD+KD having one detector in Japan (T2HK/JD) and a second detector in Korea (KD). We estimate the sensitivities of these setups to place model-independent constraints on various NUNM parameters. We demonstrate the possible correlations between NUNM parameters and oscillations parameters $\theta_{23}$ and $\delta_{CP}$. Our numerical results, supported by approximate analytical expressions of oscillation probabilities in matter, reveal that JD+KD has better sensitivities for $|\alpha_{21}|$ and $\alpha_{22}$ than DUNE, due to its larger statistics in appearance channel and less systematic uncertainties in disappearance channel, respectively. For $|\alpha_{31}|$, $|\alpha_{32}|$, and $\alpha_{33}$, DUNE gives better constraints than JD+KD, due to its larger matter effect and wider neutrino energy spectrum. For $\alpha_{11}$, both DUNE and JD+KD give similar bounds. We also show how much the bounds on the NUNM parameters can be improved by combining the prospective data from DUNE and JD+KD setups. Due to zero-distance effects, the near detectors in both DUNE and JD+KD, can further improve the limits on $|\alpha_{21}|$ considerably, while giving slightly better constraints for $\alpha_{11}$ and $\alpha_{22}$. Finally, we observe that $\nu_\tau$ events in DUNE can improve the constraints on $|\alpha_{32}|$ and $\alpha_{33}$. ","Model-Independent Constraints on Non-Unitary Neutrino Mixing from
  High-Precision Long-Baseline Experiments"
127,1455940119895158785,1192152664412475393,Fulvio Gesmundo,"['Chiara Meroni and I just posted a new paper in convex algebraic geometry: The Geometry of Discotopes\n\n<LINK>\n\nThese are semialgebraic convex bodies arising as Minkowski sum of discs.', 'A very exciting project: we used tools from real and complex algebraic geometry to characterize many geometric properties of these convex bodies. \n\nCheck it out and let us know if you have any comment!']",https://arxiv.org/abs/2111.01241,"We study a class of semialgebraic convex bodies called discotopes. These are instances of zonoids, objects of interest in real algebraic geometry and random geometry. We focus on the face structure and on the boundary hypersurface of discotopes, highlighting interesting birational properties which may be investigated using tools from algebraic geometry. When a discotope is the Minkowski sum of two-dimensional discs, the Zariski closure of its set of extreme points is an irreducible hypersurface. In this case, we provide an upper bound for the degree of the hypersurface, drawing connections to the theory of classical determinantal varieties. ",The Geometry of Discotopes
128,1455808495224229891,21902101,Jim Geach,"['Check out our new paper on arXiv today ""Realistic galaxy image simulation via score-based generative models""\n\n<LINK>', 'GANs get a lot of glory, but score-based generative methods can produce superior results compared to GANs. Here we use a denoising diffusion probabilistic model to generate realistic images of galaxies.', 'Half of the galaxies in this image are not real https://t.co/6ZpzfglxDX', 'We also show that the emergent properties, such as flux, colour, size, are re-produced by the DDPM. The DDPM is trained by slowly adding noise to an image via a Markov Chain, and then the model tries to reverse this process. https://t.co/dAj5w3m7iz', 'You can use the model to generate realistic images, but it also has other applications, like in-filling missing data (e.g. satellite trails 😬) https://t.co/x61Xu2gDqq', 'And performing domain transfer - here we ""DESI-fy"" some cartoon sketches. You could use this for image or sketch-based searches. https://t.co/7e0mi33G4R', 'Finally, as an aside, we trained a DDPM on the entire @apod archive to generate fake APODs. The images contain slightly surreal landscapes, galaxies, aurorae, etc. You can follow @ThisIsNotAnApod and see more at https://t.co/MExNCLtPwI https://t.co/3SeDSaQwSm', 'This work was led by Mike Smith, a PhD student at @UniofHerts']",https://arxiv.org/abs/2111.01713,"We show that a Denoising Diffusion Probabalistic Model (DDPM), a class of score-based generative model, can be used to produce realistic mock images that mimic observations of galaxies. Our method is tested with Dark Energy Spectroscopic Instrument (DESI) grz imaging of galaxies from the Photometry and Rotation curve OBservations from Extragalactic Surveys (PROBES) sample and galaxies selected from the Sloan Digital Sky Survey. Subjectively, the generated galaxies are highly realistic when compared with samples from the real dataset. We quantify the similarity by borrowing from the deep generative learning literature, using the `Fr\'echet Inception Distance' to test for subjective and morphological similarity. We also introduce the `Synthetic Galaxy Distance' metric to compare the emergent physical properties (such as total magnitude, colour and half light radius) of a ground truth parent and synthesised child dataset. We argue that the DDPM approach produces sharper and more realistic images than other generative methods such as Adversarial Networks (with the downside of more costly inference), and could be used to produce large samples of synthetic observations tailored to a specific imaging survey. We demonstrate two potential uses of the DDPM: (1) accurate in-painting of occluded data, such as satellite trails, and (2) domain transfer, where new input images can be processed to mimic the properties of the DDPM training set. Here we `DESI-fy' cartoon images as a proof of concept for domain transfer. Finally, we suggest potential applications for score-based approaches that could motivate further research on this topic within the astronomical community. ",Realistic galaxy image simulation via score-based generative models
129,1455699982615396357,1208788043198394368,Y. Asada,['Happy to announce our new paper has been accepted for publication in ApJ!! The accepted version is now available on arXiv.\n<LINK>'],https://arxiv.org/abs/2111.01447,"We search for H$\alpha$ emitters at $z\sim7.8$ in four gravitationally lensed fields observed in the Hubble Frontier Fields program. We use the Lyman break method to select galaxies at the target redshift, and make the photometry in {\it Spitzer}/IRAC 5.8 $\mu$m band to detect the H$\alpha$ emission from the candidate galaxies. We find no significant detection of counterparts in the IRAC 5.8 $\mu$m band, and this gives a constraint on the H$\alpha$ luminosity function (LF) at $z\sim7.8$. We compare the constraint with previous studies on rest-frame UV and FIR observation using the correlation between the H$\alpha$ luminosity and the star formation rate. Additionally, we convert the constraint on the H$\alpha$ LF into an upper limit for the star formation rate density (SFRD) at this epoch assuming the shape of the LF. We examine two types of parameterization of the LF, and obtain an upper limit for the SFRD of $\log_{10}(\rho_{\rm SFR}\ [M_\odot\ \mathrm{yr^{-1}\ Mpc^{-3}}])\lesssim-1.1$ at $z\sim7.8$. With this constraint on the SFRD, we give an independent probe into the total star formation activity including the dust-obscured and unobscured star formation at the Epoch of Reionization. ","Search for H$\alpha$ Emitters at $z\sim7.8$: A Constraint on the
  H$\alpha$-based Star Formation Rate Density"
130,1455602326194974720,777776718928941056,Daniel Muthukrishna,"['New paper on the Arxiv today on the real-time detection of anomalies in astronomical time series. <LINK>\n\nWe compare a deep neural network and a parametric approach. The fact that NNs generalize too well to different types of data obscures anomaly detection', 'Excited to finally have this paper out with Michelle Lochner, @Doctor_Lobster, @SaraWebbScience, and @gsnarayan']",https://arxiv.org/abs/2111.00036,"New time-domain surveys, such as the Rubin Observatory Legacy Survey of Space and Time (LSST), will observe millions of transient alerts each night, making standard approaches of visually identifying new and interesting transients infeasible. We present two novel methods of automatically detecting anomalous transient light curves in real-time. Both methods are based on the simple idea that if the light curves from a known population of transients can be accurately modelled, any deviations from model predictions are likely anomalies. The first modelling approach is a probabilistic neural network built using Temporal Convolutional Networks (TCNs) and the second is an interpretable Bayesian parametric model of a transient. We demonstrate our methods' ability to provide anomaly scores as a function of time on light curves from the Zwicky Transient Facility. We show that the flexibility of neural networks, the attribute that makes them such a powerful tool for many regression tasks, is what makes them less suitable for anomaly detection when compared with our parametric model. The parametric model is able to identify anomalies with respect to common supernova classes with low false anomaly rates and high true anomaly rates achieving Area Under the Receive Operating Characteristic (ROC) Curve (AUC) scores above 0.8 for most rare classes such as kilonovae, tidal disruption events, intermediate luminosity transients, and pair-instability supernovae. Our ability to identify anomalies improves over the lifetime of the light curves. Our framework, used in conjunction with transient classifiers, will enable fast and prioritised follow-up of unusual transients from new large-scale surveys. ",Real-time detection of anomalies in large-scale transient surveys
131,1455577881304698881,134363945,dheeraj rajagopal,"['Excited to share our new work!  Our work explores whether pretrained seq2seq models are adept at reasoning across domains without finetuning on individual domains. \nPaper: <LINK> [1/4]', 'We present a method to prompt seq2seq models to extract cross-domain knowledge via templates (optionally) with explanations in an use case of reasoning across commonsense and health domains [2/4]', 'We find that seq2seq models are reasonable at this, and show potential future directions for improvement via analysis. Please feel free to reach out for any questions/comments/suggestions. [3/4]', 'joint work with some great collaborators: @vkhetan_iit, @esbogdan, @fanoan, Anatole Gershman, Eduard Hovy [4/4]']",http://arxiv.org/abs/2111.00539,"In this paper, we explore the ability of sequence to sequence models to perform cross-domain reasoning. Towards this, we present a prompt-template-filling approach to enable sequence to sequence models to perform cross-domain reasoning. We also present a case-study with commonsense and health and well-being domains, where we study how prompt-template-filling enables pretrained sequence to sequence models across domains. Our experiments across several pretrained encoder-decoder models show that cross-domain reasoning is challenging for current models. We also show an in-depth error analysis and avenues for future research for reasoning across domains ",Cross-Domain Reasoning via Template Filling
132,1455557838579904512,1288452767376445440,Patrick Kidger,"['⚡️ New paper! ⚡️\n\n""Equinox: neural networks in JAX via callable PyTrees and filtered transformations""\n\nAccepted @diffprogramming #NeurIPS2021 !\n\nElegant and simple neural networks in #JAX.\n\nPaper:\n<LINK>\n\nLibrary:\n<LINK>\n\n1/n🧵', ""2/ What does programming with Equinox look like?\n\nHere's a toy example. Specifying a model is easy: here we have a parameter and a forward pass.\n\n(You can optionally also add a custom __init__ if you want to customise that.) https://t.co/q74QI9zgoN"", ""3/ Then the forward pass works exactly as you expect it to.\n\nThe message so far being: it's really simple and elegant to build your models. https://t.co/Vt8RmMDhSH"", '4/ Now this is where things get cool: Equinox works directly with standard jax.jit, jax.grad etc.\n\nNo custom wrappers (e.g. `flax.linen.jit`) or class-to-JAX transforms (e.g. `haiku.transform`) required! https://t.co/TjilwBngCN', '5/ And the result -- that is, the gradient on `adder` -- is just another instance of `Adder`.\n\nThis works because, under the hood, subclassing `equinox.Module` just means registering `Adder` as a PyTree. And JAX intrinsically understands how to use PyTrees.\n\nSimple! https://t.co/3tiXzlJIYh', ""6/ That's the key idea: models are PyTrees...\n\n...and if you know JAX you already know PyTrees...\n\n... which means that Equinox introduces no new abstractions.\n\nNo edge cases.\nThe library is easy to use.\nYour code is easy to reason about.\n🌷"", ""7/ I've found Equinox to be particularly useful on complicated projects, without getting tangled in the details."", ""8/ To finish up!\n\nEquinox is available on GitHub:\nhttps://t.co/qm6MbWyXLu\n\nInstallable via pip:\n`pip install equinox`\n\nFor more details check out the paper on arXiv:\nhttps://t.co/WZvqOMnBHR\n\nAnd if you're history buff, check out the original announcement:\nhttps://t.co/0Qd098NQ31""]",https://arxiv.org/abs/2111.00254,"JAX and PyTorch are two popular Python autodifferentiation frameworks. JAX is based around pure functions and functional programming. PyTorch has popularised the use of an object-oriented (OO) class-based syntax for defining parameterised functions, such as neural networks. That this seems like a fundamental difference means current libraries for building parameterised functions in JAX have either rejected the OO approach entirely (Stax) or have introduced OO-to-functional transformations, multiple new abstractions, and been limited in the extent to which they integrate with JAX (Flax, Haiku, Objax). Either way this OO/functional difference has been a source of tension. Here, we introduce `Equinox', a small neural network library showing how a PyTorch-like class-based approach may be admitted without sacrificing JAX-like functional programming. We provide two main ideas. One: parameterised functions are themselves represented as `PyTrees', which means that the parameterisation of a function is transparent to the JAX framework. Two: we filter a PyTree to isolate just those components that should be treated when transforming (`jit', `grad' or `vmap'-ing) a higher-order function of a parameterised function -- such as a loss function applied to a model. Overall Equinox resolves the above tension without introducing any new programmatic abstractions: only PyTrees and transformations, just as with regular JAX. Equinox is available at \url{this https URL}. ","Equinox: neural networks in JAX via callable PyTrees and filtered
  transformations"
133,1455459307424677889,1299630459605970944,Tousif Islam,"[""New Paper here! It performs the 'higher modes consistentcy test' on GW190814 to shed light on the 'no-hair theorem'. \narXiv link: <LINK> <LINK>""]",https://arxiv.org/abs/2111.00111,"As one of the consequences of the black-hole ""no-hair"" theorem in general relativity (GR), the multipolar structure of the radiation (i.e. different spherical harmonic modes) from a merging quasi-circular binary black hole (BBH) is fully determined by the intrinsic parameters (i.e. the masses and spins of the companion black holes). In Refs. [1,2], we have formulated an efficient test named `higher-modes consistency test' to check for the consistency of the observed gravitational-wave (GW) signal with the expected multipolar structure of radiation from BBHs in GR. Detection of the high-mass-ratio merger of GW190814 enables the observation of spherical harmonic modes beyond the dominant $(\ell, m) = (2,\pm2)$ mode; thereby providing a unique opportunity to perform the `higher-modes consistency test'. Using different state-of-art waveform models (IMRPhenomXPHM, IMRPhenomXHM, IMRPhenomHM and SEONNRv4HM_ROM), we show that GW190814 strongly favors the ""no-hair"" hypothesis in GR over a hypothesis that assumes generic deviation from the multipolar structure of the radiation by a Bayes factor of $\rm log_{e}\mathcal{B}\sim8$. We further investigate any potential systematic errors arising as a result of different waveform modeling choices as well as due to neglecting many higher order modes. We find that, if the waveform model includes only $(\ell, m) = (3,\pm3)$ mode in its higher harmonics, the same event may reject the `no-hair' hypothesis in GR when the detector sensitivity improves by a factor of $12.5$. Our analysis, therefore, provides motivation to include as many higher order modes as possible in future waveform models. ","Applying higher-modes consistency test on GW190814 : lessons on no-hair
  theorem, nature of the secondary compact object and waveform modeling"
134,1455332154930466820,955103299,Marina Radulaski,"['#Rlab updates: check out our new perspective paper on Quantum Information Processing With Integrated Silicon Carbide Photonics. @sridhar_majety, @PrantaSaha01 and @VictoriaANorman put a considerable effort to explain and visualize future devices\n\n<LINK>\n\n1/2 <LINK>', 'I will be tweeting a figure per day this week. Above, @sridhar_majety rendered an integrated Hong-Ou-Mandel experiment in silicon carbide with color centers integrated into photonic crystal cavities and waveguides overlayed superconducting nanowire single photon detectors\n\n2/2', '@bencbartlett @sridhar_majety @PrantaSaha01 @VictoriaANorman Thanks! Yes, the 3D figures were done in Blender.']",https://arxiv.org/abs/2111.00136,"Color centers in wide band gap semiconductors are prominent candidates for solid-state quantum technologies due to their attractive properties including optical interfacing, long coherence times, spin-photon and spin-spin entanglement, as well as the potential for scalability. Silicon carbide color centers integrated into photonic devices span a wide range of applications in quantum information processing, in a material platform with quantum-grade wafer availability and advanced processing capabilities. Recent progress in emitter generation and characterization, nanofabrication, device design, and quantum optical studies have amplified the scientific interest in this platform. We provide a conceptual and quantitative analysis of the role of silicon carbide integrated photonics in three key application areas: quantum networking, simulation, and computing. ",Quantum Information Processing With Integrated Silicon Carbide Photonics
135,1467934898728148995,2894532745,Priya L. Donti,"['In our new #NeurIPS2021 paper, we provide one of the first approaches to address N-k SCOPF, a core problem for the operation of power grids, at realistic scale.\n \nPaper: <LINK>\n\nJoint work with Aayushya Agarwal, @neerajssp2, @LarryPileggi, and @zicokolter\n\n1/ <LINK>', ""N-k SCOPF aims to schedule power generation in a way that is robust to k potential equipment failures. While it's become increasingly important to solve (see, e.g., recent blackout events in the UK and Texas), N-k SCOPF is in practice prohibitively expensive to solve at scale. 2/"", 'We propose a heuristic approach to address N-k SCOPF at scale. Our approach entails rewriting N-k SCOPF as a continuous minimax (attacker-defender) optimization problem, and solving it efficiently using insights from adversarial robustness and implicit layers in deep learning. 3/ https://t.co/ijpTFX8hcg', 'We use our approach to address N-3 SCOPF on a realistic-size (4622 bus) system, and show that it significantly reduces the number of feasibility violations (by a factor of 3-4x) compared to state-of-the-art baselines, while taking only 21 minutes to run on a standard laptop. 4/ https://t.co/nfCtGUoOSi', 'If you’re interested in chatting further about this work, Aayushya and I will be presenting our poster at #NeurIPS2021 on Tue, Dec 7 from 11:30am-1pm Eastern. Hope to see you there! \n\nhttps://t.co/ocgvWwJSTj \n\n5/5 https://t.co/pT3pgCiX1s', ""@nandofioretto @neerajssp2 @LarryPileggi @zicokolter Thanks! And yes, exactly :) It's a bit of a play on the name of our method, which is CAN∂Y""]",https://arxiv.org/abs/2111.06961,"In recent years, the ML community has seen surges of interest in both adversarially robust learning and implicit layers, but connections between these two areas have seldom been explored. In this work, we combine innovations from these areas to tackle the problem of N-k security-constrained optimal power flow (SCOPF). N-k SCOPF is a core problem for the operation of electrical grids, and aims to schedule power generation in a manner that is robust to potentially k simultaneous equipment outages. Inspired by methods in adversarially robust training, we frame N-k SCOPF as a minimax optimization problem - viewing power generation settings as adjustable parameters and equipment outages as (adversarial) attacks - and solve this problem via gradient-based techniques. The loss function of this minimax problem involves resolving implicit equations representing grid physics and operational decisions, which we differentiate through via the implicit function theorem. We demonstrate the efficacy of our framework in solving N-3 SCOPF, which has traditionally been considered as prohibitively expensive to solve given that the problem size depends combinatorially on the number of potential outages. ","Adversarially Robust Learning for Security-Constrained Optimal Power
  Flow"
136,1467513010784714758,808042683793113088,Robert Lieck,"['A context-free grammar with continuous non-terminals? A Kalman filter with unknown, infinitely nested, hierarchical dependencies?\n\nRecursive Bayesian Networks cover both (and more), see our new #NeurIPS2021 paper: <LINK> <LINK>', 'Recursive Bayesian Networks (RBNs) unify and generalise probabilistic context-free grammars and dynamic Bayesian networks, allowing for both continuous latent variables and an unknown, infinitely nested, hierarchical dependency structure. https://t.co/52ycVa2gBV', 'They support highly-structured, mixed discrete-continuous variables and can be compactly specified using the familiar and expressive graphical notation for Bayesian networks. https://t.co/Hzac0VtTWc', 'For the Gaussian case, we derive a closed-form approximation of the full marginal likelihood and posterior distributions that allow for efficient parameter training and Bayesian inference.', ""In structure analysis of piano preludes from JS Bach's WTK I &amp; II, Gaussian RBNs capture fundamental properties of tonality and harmonic modulations that are characteristic for Baroque music. https://t.co/H1GTCn87dH""]",https://arxiv.org/abs/2111.01853,"Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are widely used sequence models with complementary strengths and limitations. While PCFGs allow for nested hierarchical dependencies (tree structures), their latent variables (non-terminal symbols) have to be discrete. In contrast, DBNs allow for continuous latent variables, but the dependencies are strictly sequential (chain structure). Therefore, neither can be applied if the latent variables are assumed to be continuous and also to have a nested hierarchical dependency structure. In this paper, we present Recursive Bayesian Networks (RBNs), which generalise and unify PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. We provide two solutions: 1) For arbitrary RBNs, we generalise inside and outside probabilities from PCFGs to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) For Gaussian RBNs, we additionally derive an analytic approximation, allowing for robust parameter optimisation and Bayesian inference. The capacity and diverse applications of RBNs are illustrated on two examples: In a quantitative evaluation on synthetic data, we demonstrate and discuss the advantage of RBNs for segmentation and tree induction from noisy sequences, compared to change point detection and hierarchical clustering. In an application to musical data, we approach the unsolved problem of hierarchical music analysis from the raw note level and compare our results to expert annotations. ","Recursive Bayesian Networks: Generalising and Unifying Probabilistic
  Context-Free Grammars and Dynamic Bayesian Networks"
137,1463482581178064902,839948365622300672,Juri Smirnov 🌻,"['WIMPs Without Weakness! \n<LINK>\n\nNew paper, with my amazing collaborators Tracy Slatyer and Pouya Asadi.\n\nI also deeply thank @ProfJohnBeacom for extremely helpful discussions, and the idea for this perfect title! \n\nIn a nutshell, a 🧵: \n1/N', 'It appears that a very simple scenario with two relics that freezeout in the early universe, fundamentally changes our expectations of dark matter target mass and annihilation rate. \nWhile one relic is stable, the other decays to SM particles after it thermally decouples. \n2/N', 'Dilution by entropy injection is known, so what’s the deal?  This Fig. sows that the abundance of the dark matter only depends on the ratio of annihilation rates of those two particles. In one swoop this removes any anchoring to a particular cross section or DM mass scale. \n\n3/N https://t.co/pMidRU2QxR', 'Traditionally, WIMP freezeout = electroweak scale, with an upper mass bound around 100 TeV. This believe drives our searches (even plots are cut at 100 TeV). This Fig. shows, however, the mass window can in fact be much broader, with masses up to 10^7 TeV or 10^9 TeV. \n\n4/N https://t.co/ojUT1ncDse', 'Implications? Annihilation products, such as photons, can behave drastically different here. Interacting with the CMB photons their spectra will completely change at such energies. Really good news for neutrinos though, as they uniquely probe deeply into the GC and beyond. \n\n5/N', 'Finally, as @trlinden  et. al pointed out, the DM distribution in scenarios such as ours is affected. Overdensities, and mini-halos, will make even the very heavy relics accessible to annihilation searches. More reason to turn our lamppost into a searchlight for ultra heavy DM!']",https://arxiv.org/abs/2111.11444,"We study general freeze-out scenarios where an arbitrary number of initial and final dark matter particles participate in the number-changing freeze-out interaction. We consider a simple sector with two particle species undergoing such a thermal freeze-out; one of the relics is stable and gives rise to the dark matter today, while the other one decays to the Standard Model, injecting significant entropy into the thermal bath that dilutes the dark matter abundance. We show that this setup can lead to a stable relic population that reproduces the observed dark matter abundance without requiring weak scale masses or couplings. The final dark matter abundance is estimated analytically. We carry out this calculation for arbitrary temperature dependence in the freeze-out process and identify the viable dark matter mass and cross section ranges that explain the observed dark matter abundance. This setup can be used to open parameter space for both heavy (above the unitarity bound) or light (sub-GeV) dark matter candidates. We point out that the best strategy for probing most parts of our parameter space is to look for signatures of an early matter-dominant epoch. ",WIMPs Without Weakness: Generalized Mass Window with Entropy Injection
138,1462056656561483778,58439890,Thomas Lancaster,"['A bit technical, but interesting new paper here showing that automated writing systems generate content that could be considered as plagiarism (in some cases, more than 1,000 words of identical phrasing) #academicintegrity #artificialintelligence <LINK>']",https://arxiv.org/abs/2111.09509,"Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure - e.g., individual dependencies - model-generated text is substantially less novel than our baseline of human-generated text from each model's test set. For larger-scale structure - e.g., overall sentence structure - model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory). ","How much do language models copy from their training data? Evaluating
  linguistic novelty in text generation using RAVEN"
139,1458082810779029511,1192265252575956994,Paul R Griffin,"[""We have a new paper on quantum machine learning we’ve just released on the arxiv preprint link coming from a research project we're currently working on. Great work by Davit and Nikos.\n<LINK> \n\nCheers""]",https://arxiv.org/abs/2111.03372,"One of the most promising areas of research to obtain practical advantage is Quantum Machine Learning which was born as a result of cross-fertilisation of ideas between Quantum Computing and Classical Machine Learning. In this paper, we apply Quantum Machine Learning (QML) frameworks to improve binary classification models for noisy datasets which are prevalent in financial datasets. The metric we use for assessing the performance of our quantum classifiers is the area under the receiver operating characteristic curve (ROC/AUC). By combining such approaches as hybrid-neural networks, parametric circuits, and data re-uploading we create QML inspired architectures and utilise them for the classification of non-convex 2 and 3-dimensional figures. An extensive benchmarking of our new FULL HYBRID classifiers against existing quantum and classical classifier models, reveals that our novel models exhibit better learning characteristics to asymmetrical Gaussian noise in the dataset compared to known quantum classifiers and performs equally well for existing classical classifiers, with a slight improvement over classical results in the region of the high noise. ","Binary classifiers for noisy datasets: a comparative study of existing
  quantum machine learning frameworks and some new approaches"
140,1457979181539659776,1381477369903513601,Ilker Birbil @ UvA,"['Our new work on optimization with constraint learning is on arXiv. This paper comes with a software package OptiCL, which is also available on GitHub.\n\nPaper: <LINK>\nGitHub: <LINK>\n\nThe real heroes: @holly_wiberg &amp; @MaragnoDonato <LINK>']",https://arxiv.org/abs/2111.04469,"We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization-representability of many machine learning methods, including linear models, decision trees, ensembles, and multi-layer perceptrons. The consideration of multiple methods allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also characterize a decision trust region using the convex hull of the observations, to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and clustering. In combination with domain-driven constraints and objective terms, the embedded models and trust region define a mixed-integer optimization problem for prescription generation. We implement this framework as a Python package (OptiCL) for practitioners. We demonstrate the method in both chemotherapy optimization and World Food Programme planning. The case studies illustrate the benefit of the framework in generating high-quality prescriptions, the value added by the trust region, the incorporation of multiple machine learning methods, and the inclusion of multiple learned constraints. ",Mixed-Integer Optimization with Constraint Learning
141,1457668698798497792,608020439,Dr. Gareth Cabourn Davies,"['Lots of work from lots of wonderful people in the @LIGO, @ego_virgo @KAGRA_PR collaborations has gone into this, and we get some pretty exciting results!\n\nNew possible NSBH systems and loads more binary black holes!\n\nRead the paper here:\n<LINK>\n\n🧵👇 <LINK>', 'Its an excellent paper, but I may be biased as part of the team that led the writing of the paper. This team was excellently led by @cplberry of @UofGravity, and with colleagues from around the globe 2/', ""Its 82 pages and a large percentage of that is the author list to highlight all our amazing colleagues\n\nThis paper really is a collaborative effort, and everyone contributed to these exciting results\n\nBUT I'm going to indulge myself and highlight things I've directly worked on 3/"", 'The @UoPCosmology institute at @portsmouthuni has been vital for this catalog, not just in scientific content, where ICG researchers developed many of the search and detector characterisation methods, but also in organisation and leadership in the collaboration 4/', 'Me and colleagues at @UoPCosmology organised and performed analyses of the gravitational-wave data using PyCBC searches, and there are quite a few events which would not have been found without our efforts. 5/', 'The PyCBC searches used analysis developed in my time at @IGFAE_HEP in https://t.co/lJ309SFDQa with @TomD_Santiago, @UoPCosmology colleagues and @alexandernitz 6/', 'The O3b data is now public as well, so if you want to discover huge astrophysical objects crashing into one another as well, give it a go!\n\nData is available through GWOSC, and open source gravitational-wave search and parameter estimation software is available from PyCBC 7/', 'We love efforts both inside and outside the collaboration to squeeze as much science out of the data as possible, so I look forward to even more results coming out from this data in the coming months 8/', 'Any volunteers to update https://t.co/3dUVD39f2z ?\n\nEND']",https://arxiv.org/abs/2111.03606,"The third Gravitational-wave Transient Catalog (GWTC-3) describes signals detected with Advanced LIGO and Advanced Virgo up to the end of their third observing run. Updating the previous GWTC-2.1, we present candidate gravitational waves from compact binary coalescences during the second half of the third observing run (O3b) between 1 November 2019, 15:00 UTC and 27 March 2020, 17:00 UTC. There are 35 compact binary coalescence candidates identified by at least one of our search algorithms with a probability of astrophysical origin $p_\mathrm{astro} > 0.5$. Of these, 18 were previously reported as low-latency public alerts, and 17 are reported here for the first time. Based upon estimates for the component masses, our O3b candidates with $p_\mathrm{astro} > 0.5$ are consistent with gravitational-wave signals from binary black holes or neutron star-black hole binaries, and we identify none from binary neutron stars. However, from the gravitational-wave data alone, we are not able to measure matter effects that distinguish whether the binary components are neutron stars or black holes. The range of inferred component masses is similar to that found with previous catalogs, but the O3b candidates include the first confident observations of neutron star-black hole binaries. Including the 35 candidates from O3b in addition to those from GWTC-2.1, GWTC-3 contains 90 candidates found by our analysis with $p_\mathrm{astro} > 0.5$ across the first three observing runs. These observations of compact binary coalescences present an unprecedented view of the properties of black holes and neutron stars. ","GWTC-3: Compact Binary Coalescences Observed by LIGO and Virgo During
  the Second Part of the Third Observing Run"
142,1456977009112858633,955103299,Marina Radulaski,"['In the final figure in #Rlab new perspective paper, we compile a recipe for deploying integrated silicon carbide color center photonics for measurement based quantum computing. Prepared in an insightful illustration by @PrantaSaha01 \n\n<LINK> <LINK> <LINK>']",https://arxiv.org/abs/2111.00136,"Color centers in wide band gap semiconductors are prominent candidates for solid-state quantum technologies due to their attractive properties including optical interfacing, long coherence times, spin-photon and spin-spin entanglement, as well as the potential for scalability. Silicon carbide color centers integrated into photonic devices span a wide range of applications in quantum information processing, in a material platform with quantum-grade wafer availability and advanced processing capabilities. Recent progress in emitter generation and characterization, nanofabrication, device design, and quantum optical studies have amplified the scientific interest in this platform. We provide a conceptual and quantitative analysis of the role of silicon carbide integrated photonics in three key application areas: quantum networking, simulation, and computing. ",Quantum Information Processing With Integrated Silicon Carbide Photonics
143,1456196565350092803,989510983,Antonio Fernández,"['New paper accepted ""Chirotonia: A Scalable and Secure e-Voting Framework based on Blockchains and Linkable Ring Signatures"" with Antonio Russo, @mbelcrypt_vasco and @spromano \n\nSystem tested in 3 elections at university departments!\n\n<LINK>']",https://arxiv.org/abs/2111.02257,"In this paper we propose a comprehensive and scalable framework to build secure-by-design e-voting systems. Decentralization, transparency, determinism, and untamperability of votes are granted by dedicated smart contracts on a blockchain, while voter authenticity and anonymity are achieved through (provable secure) linkable ring signatures. These, in combination with suitable smart contract constraints, also grant protection from double voting. Our design is presented in detail, focusing on its security guarantees and the design choices that allow it to scale to a large number of voters. Finally, we present a proof-of-concept implementation of the proposed framework, made available as open source. ","Chirotonia: A Scalable and Secure e-Voting Framework based on
  Blockchains and Linkable Ring Signatures"
144,1456180048222998528,569063423,Heino Falcke,"['New official paper by @ehtelescope led by @azstewobs group. Low variability we find on large interferometry baseline triangles is well explained if the ring we see in M87* ist indeed gravitational and not due to plasma effects. Technical but  promising. <LINK> <LINK>', 'Followed by investigation of how general we can relate size of the black hole shadow to the bright ring we see, even if the theory of gravity deviates from general relativity. Turns out the shadow is a robust measure of spacetime properties of black holes https://t.co/ItzuROxg7T https://t.co/hlAnqrKYEa', 'Maybe useful to say that we introduced the concept of a black hole shadow in a paper in 2000 with @AgolEric and Melia and you can find a more intuitive description in this recent paper with @thomasbronzwaer https://t.co/8cQgkm1uKs https://t.co/niJSXhWWLt']",https://arxiv.org/abs/2111.01317,"The black-hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5-61 days) is comparable to the 6-day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure phase measurements on all six linearly independent non-trivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of $\sim3-5^\circ$. The only triangles that exhibit substantially higher variability ($\sim90-180^\circ$) are the ones with baselines that cross visibility amplitude minima on the $u-v$ plane, as expected from theoretical modeling. We used two sets of General Relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black-hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black-hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas. ","The Variability of the Black-Hole Image in M87 at the Dynamical Time
  Scale"
145,1468665127402876928,1230525790636232715,Dongyue Oliver Li,"['Excited to present our paper at #NeurIPS2021 poster session 8 (Friday 8:30 PT)! We propose regularization methods to mitigate over-fitting during fine-tuning, w/ @HongyangZhang. Please stop by! \n\nPoster: <LINK>\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2111.04578,"A widely used algorithm for transfer learning is fine-tuning, where a pre-trained model is fine-tuned on a target task with a small amount of labeled data. When the capacity of the pre-trained model is much larger than the size of the target data set, fine-tuning is prone to overfitting and ""memorizing"" the training labels. Hence, an important question is to regularize fine-tuning and ensure its robustness to noise. To address this question, we begin by analyzing the generalization properties of fine-tuning. We present a PAC-Bayes generalization bound that depends on the distance traveled in each layer during fine-tuning and the noise stability of the fine-tuned model. We empirically measure these quantities. Based on the analysis, we propose regularized self-labeling -- the interpolation between regularization and self-labeling methods, including (i) layer-wise regularization to constrain the distance traveled in each layer; (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is confident) and reweight less confident data points. We validate our approach on an extensive collection of image and text data sets using multiple pre-trained model architectures. Our approach improves baseline methods by 1.76% (on average) for seven image classification tasks and 0.75% for a few-shot classification task. When the target data set includes noisy labels, our approach outperforms baseline methods by 3.56% on average in two noisy settings. ","Improved Regularization and Robustness for Fine-tuning in Neural
  Networks"
146,1468393927963123712,846245360,Alex Tamkin,"['DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning\n\nSSL is a promising technology, but current methods are field-specific. Can we find general algorithms that can be applied to any domain?\n\n🌐: <LINK>\n📄: <LINK>\n\n🧵👇 #NeurIPS2021\n\n1/ <LINK>', 'Self-supervised learning (SSL) algorithms can drastically reduce the need for labeling by pretraining on unlabeled data\n\nBut designing SSL methods is hard and can require lots of domain-specific intuition and trial and error \n\n2/', 'We designed DABS to drive progress in domain-agnostic SSL\n\nOur benchmark addresses three core modeling components in SSL algorithms: \n\n(1) architectures\n(2) pretraining objectives\n(3) transfer methods\n\n3/', '1) Architectures:\n\nMost models are designed for particular modalities (e.g. ResNets for images)\n\nBut Transformers have recently been applied to many settings, and Perceivers are even more general\n\nWhat architectures are general, efficient, and learn the best representations?\n\n4/', '2) Pretraining objectives:\n\nWe currently have domain-specific ways to extract signal from unlabeled data\n\nLanguage modeling prevails in NLP, while contrastive learning is more common in vision\n\nCan we uncover unifying principles and methods that work well on any domain?\n\n5/', ""3) Transfer learning\n\nFull finetuning, linear evaluation, p/prompt/prefix tuning… there's a whole range of techniques to adapt models to downstream tasks.\n\nDo these work equally well across domains? What are the tradeoffs, and do better methods exist?\n\n6/"", 'Datasets &amp; Domains\n\nDABS is organized into 7 domains: natural images, speech, English-language text, multilingual text, wearable sensors, chest x-rays, and images w/ text descriptions. \n\nEach domain has an unlabeled dataset for pretraining and downstream datasets for transfer\n\n7/ https://t.co/3qK0dtMBUg', ""The goal is to find a *single* SSL algorithm that performs well across all of these domains\n\nWe kick off the challenge with two new baselines using transformers, where the pretraining objectives are based on the input embeddings. There's a lot of headroom left!\n\n8/ https://t.co/RphhkTkHSN"", ""To assess real-world generalization, DABS is a *living benchmark*—\n\nWe'll be adding additional domains focusing on scientific and other real-world applications\n\nProposed algorithms will be tested on these new domains to see how well they hold up\n\n9/"", 'We hope DABS helps yield new insights about why / when SSL works, and helps make it a more mature technology that can be used off-the-shelf in scientific, medical, and other high-impact fields\n\n10/', ""Also—If you're a domain expert interested in adding a domain for your field (unlabeled dataset + labeled downstream tasks), please reach out!\n\n11/"", 'This is joint work w/ Vincent Liu, Rongfei Lu, Daniel Fein, Colin Schultz, and Noah Goodman! @StanfordAILab @stanfordnlp \n\nStop by our #NeurIPS2021 poster on Friday, 8:30–10am PST 👋\nhttps://t.co/9XIJNRouyw\n\n12/', 'Website: https://t.co/eKDERghKSF\nPaper: https://t.co/CM6DCyLgT7\nCode: https://t.co/q3tMsBtlnx\n\n🌅13/13']",https://arxiv.org/abs/2111.12062,"Self-supervised learning algorithms, including BERT and SimCLR, have enabled significant strides in fields like natural language processing, computer vision, and speech processing. However, these algorithms are domain-specific, meaning that new self-supervised learning algorithms must be developed for each new setting, including myriad healthcare, scientific, and multimodal domains. To catalyze progress toward domain-agnostic methods, we introduce DABS: a Domain-Agnostic Benchmark for Self-supervised learning. To perform well on DABS, an algorithm is evaluated on seven diverse domains: natural images, multichannel sensor data, English text, speech recordings, multilingual text, chest x-rays, and images with text descriptions. Each domain contains an unlabeled dataset for pretraining; the model is then is scored based on its downstream performance on a set of labeled tasks in the domain. We also present e-Mix and ShED: two baseline domain-agnostic algorithms; their relatively modest performance demonstrates that significant progress is needed before self-supervised learning is an out-of-the-box solution for arbitrary domains. Code for benchmark datasets and baseline algorithms is available at this https URL ",DABS: A Domain-Agnostic Benchmark for Self-Supervised Learning
147,1467512397401235458,977016438178435072,Tábita Hünemeier 🏳️‍🌈,"['Check out our new paper! Here we propose a method for directly tackles the issue of identification of the homozygosity islands at the population level, without the need of analyzing single individuals and then combining the results.\n\n<LINK>']",https://arxiv.org/abs/2111.10187,"In this paper, we propose a new method for offline change-point detection on some parameters of the distribution of a random vector. We introduce a penalized maximum likelihood approach that can be efficiently computed by a dynamic programming algorithm or approximated by a fast greedy binary splitting algorithm. We prove both algorithms converge almost surely to the set of change-points under very general assumptions on the distribution and independent sampling of the random vector. In particular, we show the assumptions leading to the consistency of the algorithms are satisfied by categorical and Gaussian random variables. This new approach is motivated by the problem of identifying homozygosity islands on the genome of individuals in a population. Our method directly tackles the issue of identification of the homozygosity islands at the population level, without the need of analyzing single individuals and then combining the results, as is made nowadays in state-of-the-art approaches. ","Population based change-point detection for the identification of
  homozygosity islands"
148,1466491878916956161,2737324542,Philip Muirhead,"[""New paper from the RIT-BU-UToronto PCEB collab. Analyzed K2 and HST data on V471 Tau, an eclipsing dK+WD in Hyades. Recovered the transit of the WD with lensing(!). We find the dK isn't inflated, but the WD is still weird (too hot/massive for Hyades): <LINK>""]",https://arxiv.org/abs/2111.06905,"V471 Tau is a post-common-envelope binary consisting of an eclipsing DA white dwarf and a K-type main-sequence star in the Hyades star cluster. We analyzed publicly available photometry and spectroscopy of V471 Tau to revise the stellar and orbital parameters of the system. We used archival K2 photometry, archival Hubble Space Telescope spectroscopy, and published radial-velocity measurements of the K-type star. Employing Gaussian processes to fit for rotational modulation of the system flux by the main-sequence star, we recovered the transits of the white dwarf in front of the main-sequence star for the first time. The transits are shallower than would be expected from purely geometric occultations owing to gravitational microlensing during transit, which places an additional constraint on the white-dwarf mass. Our revised mass and radius for the main-sequence star is consistent with single-star evolutionary models given the age and metallicity of the Hyades. However, as noted previously in the literature, the white dwarf is too massive and too hot to be the result of single-star evolution given the age of the Hyades, and may be the product of a merger scenario. We independently estimate the conditions of the system at the time of common envelope that would result in the measured orbital parameters today. ","Revised Stellar Parameters for V471 Tau, A Post-common Envelope Binary
  in the Hyades"
149,1466062068000301056,840339066340167680,Shany Danieli,"['A picture is worth a thousand words!\nWe studied NGC5846-UDG1, an extremely globular cluster-rich ultra-diffuse galaxy or the ""globular cluster-galaxy"". \nUsing this new image, taken with the Hubble Space Telescope, we find some pretty interesting results.\n<LINK> <LINK>', 'UDG1 is made up of a *very* diffuse layer of ""field stars"", overlooked by a vast population of globular clusters associated with the galaxy. The field stars component is so low surface brightness that at a first glance UDG1 looks like a cluster of clusters!', 'With these new HST images, we measure the cluster-to-field stars light ratio, i.e., how many stars are in globular clusters in the present day.', 'We find that UDG1 hosts 54+-9 (!) globular clusters and that the clusters currently comprise ~13% of the total light. That is x10-100 higher than any previously known galaxy.', 'What does it all mean? UDG1 is the first known galaxy with a present-day globular cluster-to-field stars mass ratio that is higher than 10% (with 90% confidence).', 'It is thought that on long enough time scales, clusters across all masses evolve dynamically, including complete dissolution of low-mass clusters. That is, globular cluster systems could lose up to ∼ 80−90% of their stars over their lifetime.', 'Modeling the globular cluster mass function, we infer that 65% of the stars were formed in bound clusters. That is, most of the star formation in UDG1 happened in extremely dense cluster-forming gas clumps, with unusually little low-density (i.e. unbound) star formation. https://t.co/M3N4Vb6Vm7', 'We speculate that the high globular cluster mass fraction is a result of early-collapsing dark matter halos due to their elevated gas surface densities at high redshift (as first suggested in Trujillo-Gomez+2021). Such conditions give rise to high cluster formation efficiencies.', 'The high clustering of the supernova feedback sources within the clusters could increase the mass loading of gas outflows, which would lead to significant expansion of the stars and dark matter compared to galaxies with typical halo collapse times and globular cluster populations', 'UDG1 may be a case where star formation began like in many other galaxies but then failed to form stars in a low-density mode at later epochs. It is likely not alone and we think that more clump-only star formation in galaxies may be quite common.', 'Many thanks to my AMAZING collaborators: @DokkumPieter, Sebastian Trujillo Gomez, Diederik Kruijssen, Aaron Romanowsky, Scott Carlsten, @ZiliShen, @AstroJacobLi, @roberto_abraham, Jean Brodie, Charlie Conroy, Jonah Gannon, and @johnnypgreco.', '@karlglazebrook ~8-11 Gyr, from this cool paper by @VoltarCH using MUSE. You can see the ages of the 11 spectroscopically confirmed clusters and of the field stars (first row). https://t.co/abRM1uAUvz']",https://arxiv.org/abs/2111.14851,"It has been shown that ultra-diffuse galaxies (UDGs) have higher specific frequencies of globular clusters on average than other dwarf galaxies with similar luminosities. The UDG NGC5846-UDG1 is among the most extreme examples of globular cluster-rich galaxies found so far. Here we present new Hubble Space Telescope (HST) observations and analysis of this galaxy and its globular cluster system. We find that NGC5846-UDG1 hosts $54 \pm 9$ globular clusters, three to four times more than any previously known galaxy with a similar luminosity, and higher than reported in previous studies. With a galaxy luminosity of $L_{V,\mathrm{gal}} \approx 6 \times 10^7\,{\rm L}_{\odot}$ ($M_\star \approx 1.2 \times 10^8\,{\rm M}_\odot$) and a total globular cluster luminosity of $L_{V,\mathrm{GCs}} \approx 7.6 \times 10^6\,{\rm L}_{\odot}$, we find that the clusters currently comprise $\sim 13 \%$ of the total light. Taking into account the effects of mass loss from clusters during their formation and throughout their lifetime, we infer that most of the stars in the galaxy likely formed in globular clusters, and very little to no ""normal"" low-density star formation occurred. This result implies that the most extreme conditions during early galaxy formation promoted star formation in massive and dense clumps, in contrast to the dispersed star formation observed in galaxies today. ","NGC5846-UDG1: A galaxy formed mostly by star formation in massive,
  extremely dense clumps of gas"
150,1465968600825671681,422672164,Dr Michael Reidinger,"['Effects on the local dark matter distribution due to the Large Magellanic Cloud\n\n""We study the local dark matter distribution in two models for the Milky Way (MW)-Large Magellanic Cloud (LMC) interaction.""\n<LINK>']",https://arxiv.org/abs/2111.15440,"We study the local dark matter distribution in two models for the Milky Way (MW)-Large Magellanic Cloud (LMC) interaction. The effect of the LMC on the local dark matter distribution is dependent on the evolution of the MW-LMC system, such that a static model is insufficient to accurately model the dark matter velocity distribution in the solar neighbourhood. An evolved model boosts local LMC dark matter particle velocities by nearly 50%, to a median value of $\approx750$km/s. MW dark matter particles also experience a velocity boost, which we identify as being caused by reflex motion owing to the infall of the LMC. We study the implications of LMC particles in the solar neighbourhood for dark matter detection experiments. Specifically, the directionality of LMC particles is distinguishable from the MW particles, with a difference in the apparent origin centroid location between the MW and LMC particles of $26\pm6 ^\circ$. This unique identifier, along with their high velocities, can be utilised by directional detectors to search for dark matter particles originating in the LMC. ","Effects on the local dark matter distribution due to the Large
  Magellanic Cloud"
151,1465763863589056514,956622601,Piotr Piecuch,"['Thank you @TitouLoos! Indeed, we deposited our comprehensive (and long-overdue) study of ACP methods for strongly correlated electrons (as in metal-insulator transitions), which also provides a historical overview of these kinds of approaches, on arXiv: <LINK>. <LINK>', '@cortogantese @TitouLoos Thank you for your kind words! We started working on this paper in late 2020, when Ilias was still at @michiganstateu (this was one of his PhD projects). I am happy that I finally found time in recent few weeks to finalize our writing. It is wonderful to hear that you like it!']",https://arxiv.org/abs/2111.13787,"When the number of strongly correlated electrons becomes larger, the single-reference coupled-cluster (CC) CCSD, CCSDT, etc. hierarchy displays an erratic behavior, while traditional multi-reference approaches may no longer be applicable due to enormous dimensionalities of the underlying model spaces. These difficulties can be alleviated by the approximate coupled-pair (ACP) theories, in which selected $(T_2)^2$ diagrams in the CCSD amplitude equations are removed, but there is no generally accepted and robust way of incorporating connected triply excited ($T_3$) clusters within the ACP framework. It is also not clear if the specific combinations of $(T_2)^2$ diagrams that work well for strongly correlated minimum-basis-set model systems are optimum when larger basis sets are employed. This study explores these topics by considering a few novel ACP schemes with the active-space and full treatments of $T_3$ correlations and schemes that scale selected $(T_2)^2$ diagrams by factors depending on the numbers of occupied and unoccupied orbitals. The performance of the proposed ACP approaches is illustrated by examining the symmetric dissociations of the $\text{H}_6$ and $\text{H}_{10}$ rings using basis sets of the triple- and double-$\zeta$ quality and the $\text{H}_{50}$ linear chain treated with a minimum basis, for which the conventional CCSD and CCSDT methods fail. ","Addressing Strong Correlation by Approximate Coupled-Pair Methods with
  Active-Space and Full Treatments of Three-Body Clusters"
152,1465527382475874305,1236622778175909888,Jingtao Zhan,"['📢 Happy to share our new work, ""Interpreting Dense Retrieval as Mixture of Topics"", which tries to disclose the secrets of DR\'s success. \nWe find DR pays attention to different aspects of input and extracts different high-level topic representations. See <LINK> <LINK>']",https://arxiv.org/abs/2111.13957,"Dense Retrieval (DR) reaches state-of-the-art results in first-stage retrieval, but little is known about the mechanisms that contribute to its success. Therefore, in this work, we conduct an interpretation study of recently proposed DR models. Specifically, we first discretize the embeddings output by the document and query encoders. Based on the discrete representations, we analyze the attribution of input tokens. Both qualitative and quantitative experiments are carried out on public test collections. Results suggest that DR models pay attention to different aspects of input and extract various high-level topic representations. Therefore, we can regard the representations learned by DR models as a mixture of high-level topics. ",Interpreting Dense Retrieval as Mixture of Topics
153,1465268011993022468,3034383171,Diana Khoromskaia,"['Our new preprint is online! We explore numerically the ""morphospace"" of epithelia as active nematic materials.\n@GSalbreux @TheCrick @unige_en @sciences_UNIGE @ArxivBiophys\n--&gt; find it here: <LINK>\n#morphogenesis #activematter #nematics #organoids <LINK>']",https://arxiv.org/abs/2111.12820,"Shape transformations of epithelial tissues in three dimensions, which are crucial for embryonic development or in vitro organoid growth, can result from active forces generated within the cytoskeleton of the epithelial cells. How the interplay of local differential tensions with tissue geometry and with external forces results in tissue-scale morphogenesis remains an open question. Here, we describe epithelial sheets as active viscoelastic surfaces and study their deformation under patterned internal tensions and bending moments. In addition to isotropic effects, we take into account nematic alignment in the plane of the tissue, which gives rise to shape-dependent, anisotropic active tensions and bending moments. We present phase diagrams of the mechanical equilibrium shapes of pre-patterned closed shells and explore their dynamical deformations. Our results show that a combination of nematic alignment and gradients in internal tensions and bending moments is sufficient to reproduce basic building blocks of epithelial morphogenesis, including fold formation, budding, neck formation, flattening, and tubulation. ",Active morphogenesis of patterned epithelial shells
154,1465018106485563393,186037471,Josh Tong,"['Excited to share the shapes of melting ice we find, now on the arxiv!\n<LINK>']",https://arxiv.org/abs/2111.09937,"We report on the shape dynamics of ice suspended in cold fresh water and subject to the natural convective flows generated during melting. Experiments reveal shape motifs for increasing far-field temperature: Sharp pinnacles directed downward at low temperatures, scalloped waves for intermediate temperatures between 5 and $7^\circ$C, and upward pointing pinnacles at higher temperatures. Phase-field simulations reproduce these morphologies, which are closely linked to the anomalous density-temperature profile of liquid water. Boundary layer flows yield pinnacles that sharpen with accelerating growth of tip curvature while scallops emerge from a Kelvin-Helmholtz-like instability caused by counterflowing currents that roll up to form vortex arrays. By linking the molecular-scale effects underlying water's density anomaly to the macro-scale flows that imprint the surface, these results show that the morphology of melted ice is a sensitive indicator of ambient temperature. ",Anomalous convective flows carve pinnacles and scallops in melting ice
155,1463892400200294416,1243777631436193793,Wenhao Wang,"['We are happy to announce that in the Facebook AI Image Similarity Challenge, we rank first in Matching Track and third in Descriptor Track.\nYou can find the solutions from <LINK> and\n<LINK>.\nThe links to code are available in the papers.']",https://arxiv.org/abs/2111.07090,"Image copy detection is of great importance in real-life social media. In this paper, a data-driven and local-verification (D$^2$LV) approach is proposed to compete for Image Similarity Challenge: Matching Track at NeurIPS'21. In D$^2$LV, unsupervised pre-training substitutes the commonly-used supervised one. When training, we design a set of basic and six advanced transformations, and a simple but effective baseline learns robust representation. During testing, a global-local and local-global matching strategy is proposed. The strategy performs local-verification between reference and query images. Experiments demonstrate that the proposed method is effective. The proposed approach ranks first out of 1,103 participants on the Facebook AI Image Similarity Challenge: Matching Track. The code and trained models are available at this https URL ","D$^2$LV: A Data-Driven and Local-Verification Approach for Image Copy
  Detection"
156,1463682412056125446,805911864123211776,Keshav Motwani,"['I am very excited to share my first statistics paper, with @aaron_molstad and @rbacher! We propose a method to fit a classification model using multiple datasets with response categories at different resolutions, as is common in single-cell datasets <LINK> <LINK>', 'The process of manual cell type annotation in single-cell genomics is time-intensive and subjective, which has led to different studies describing cell types with labels of varying degrees of resolution.', 'For example, if a cell is truly of cell type “effector memory CD4+ T cell”, it is equally valid, though less informative, to call it a “memory CD4+ T cell”, a “CD4+ T cell”, or a “T cell.”', 'Depending on the investigator’s specific research interests and availability of external data (e.g. protein expression), it is common to see all of these labels across published datasets.', 'Despite the existence of hundreds of publicly available datasets with expertly annotated cell types, existing methods are limited in their ability to integrate a wide-array of datasets due to varying label resolution.', 'In our paper, we propose a new classification method which will allow investigators to use many datasets jointly to train a unified classification model without loss of information by utilizing the known relationships between labels.', 'Our method allows one to make cell type predictions/annotations at the finest resolution labels allowed by the union of all datasets’ labels.', 'We applied the method to ten single-cell genomics datasets from peripheral blood annotated at varying resolutions, as depicted in this figure. https://t.co/VbCtqeKziu', 'This method could be used in other contexts as well. For example, in one dataset, we may know whether a patient is a control, disease subtype A, or disease subtype B, whereas in another dataset, we only know if a patient is a control or has the disease.', 'We may want to combine both datasets for fitting the model for increased sample size, but still be able to predict subtypes on a new data point. Our method would allow for this.', 'Check out the software here: https://t.co/VWQyJ6YRIB and a user guide here: https://t.co/0OU4tLGZ65']",https://arxiv.org/abs/2111.12149,"Categorizing individual cells into one of many known cell type categories, also known as cell type annotation, is a critical step in the analysis of single-cell genomics data. The current process of annotation is time-intensive and subjective, which has led to different studies describing cell types with labels of varying degrees of resolution. While supervised learning approaches have provided automated solutions to annotation, there remains a significant challenge in fitting a unified model for multiple datasets with inconsistent labels. In this article, we propose a new multinomial logistic regression estimator which can be used to model cell type probabilities by integrating multiple datasets with labels of varying resolution. To compute our estimator, we solve a nonconvex optimization problem using a blockwise proximal gradient descent algorithm. We show through simulation studies that our approach estimates cell type probabilities more accurately than competitors in a wide variety of scenarios. We apply our method to ten single-cell RNA-seq datasets and demonstrate its utility in predicting fine resolution cell type labels on unlabeled data as well as refining cell type labels on data with existing coarse resolution annotations. An R package implementing the method is available at this https URL and the collection of datasets we analyze is available at this https URL ","Binned multinomial logistic regression for integrative cell type
  annotation"
157,1463503414588059649,129550400,Giona Casiraghi,"['The downside of heterogeneity: In our last article, we study the lock-in effect in a network of task assignments and link our findings to the problem of resilience and observations in social systems.\nCheck out the preprint here: <LINK>']",https://arxiv.org/abs/2111.10648,"We study the lock-in effect in a network of task assignments. Agents have a heterogeneous fitness for solving tasks and can redistribute unfinished tasks to other agents. They learn over time to whom to reassign tasks and preferably choose agents with higher fitness. A lock-in occurs if reassignments can no longer adapt. Agents overwhelmed with tasks then fail, leading to failure cascades. We find that the probability for lock-ins and systemic failures increase with the heterogeneity in fitness values. To study this dependence, we use the Shannon entropy of the network of task assignments. A detailed discussion links our findings to the problem of resilience and observations in social systems. ","The downside of heterogeneity: How established relations counteract
  systemic adaptivity in tasks assignments"
158,1463425906249289728,12745042,RJB Goudie,['New preprint by @hhau_stats +me @MRC_BSU\n\nHave Bayesian submodels linked in a “chain-like” structure for several data sources? ie neighbouring submodels share parameters\n\nWe propose “chained Markov melding” for forming a suitable encompassing Bayes model\n\n<LINK> <LINK>'],https://arxiv.org/abs/2111.11566,"A challenge for practitioners of Bayesian inference is specifying a model that incorporates multiple relevant, heterogeneous data. It may be easier to instead specify distinct submodels for each source of data, then join the submodels together. We consider chains of submodels, where submodels directly relate to their neighbours via common quantities which may be parameters or deterministic functions thereof. We propose chained Markov melding, an extension of Markov melding, a generic method to combine chains of submodels into a joint model. One challenge we address is appropriately capturing the prior dependence between common quantities within a submodel, whilst also reconciling differences in priors for the same common quantity between two adjacent submodels. Estimating the posterior of the resulting overall joint model is also challenging, so we describe a sampler that uses the chain structure to incorporate information contained in the submodels in multiple stages, possibly in parallel. We demonstrate our methodology using two examples. The first example considers an ecological integrated population model, where multiple data are required to accurately estimate population immigration and reproduction rates. We also consider a joint longitudinal and time-to-event model with uncertain, submodel-derived event times. Chained Markov melding is a conceptually appealing approach to integrating submodels in these settings. ",Combining chains of Bayesian models with Markov melding
159,1463214828122742794,314395154,Tengyu Ma,"['Recent algorithms for offline RL perform well; do they still work in the multi-agent setting? Surprisingly, they may underperform due to challenges in multi-agent optimization. We propose OMAR that improves action opt. with the 0th-order approach: <LINK>. 1/n <LINK>', 'We found that conservative Q-learning in continuous-action multi-agent settings may create bad local maxima in the Q-functions’ landscape (see Fig. 1d). This means any first-order opt. of the policy may get stuck (regardless of actor learning rate and # of actor update steps) 2/n https://t.co/5W7o6wgGga', 'Motivated by the analysis, we use a fast zeroth-order optimizer to compute the best action for the current Q-function and use it as additional supervision for the policy updates. \nOMAR can improve the performance in both online/offline, single-agent/multi-agent settings. 3/n', 'However, OMAR gives the biggest improvements in offline multi-agent settings due to the large action space from multiple agents, the nature of requiring coordinated actions from multiple agents, and the challenge of learning without explorations. 4/n https://t.co/aE8jEL6M7F', 'The figure above: OMAR outperforms various baselines, including TD3+BC and CQL, in the multi-particle environment (MPE), multi-agent locomotion (MA-MuJoCo), multi-agent discrete action video games (SMAC), and also Maze2D navigation from D4RL.  5/n', 'Arxiv: https://t.co/ABDQ8iWqbO\nJoint work with @PennyLingPan (first author), @longbo_huang, and @HarryXu12 (last author)']",http://arxiv.org/abs/2111.11188,"Conservatism has led to significant progress in offline reinforcement learning (RL) where an agent learns from pre-collected datasets. However, as many real-world scenarios involve interaction among multiple agents, it is important to resolve offline RL in the multi-agent setting. Given the recent success of transferring online RL algorithms to the multi-agent setting, one may expect that offline RL algorithms will also transfer to multi-agent settings directly. Surprisingly, we empirically observe that conservative offline RL algorithms do not work well in the multi-agent setting -- the performance degrades significantly with an increasing number of agents. Towards mitigating the degradation, we identify a key issue that non-concavity of the value function makes the policy gradient improvements prone to local optima. Multiple agents exacerbate the problem severely, since the suboptimal policy by any agent can lead to uncoordinated global failure. Following this intuition, we propose a simple yet effective method, Offline Multi-Agent RL with Actor Rectification (OMAR), which combines the first-order policy gradients and zeroth-order optimization methods to better optimize the conservative value functions over the actor parameters. Despite the simplicity, OMAR achieves state-of-the-art results in a variety of multi-agent control tasks. ","Plan Better Amid Conservatism: Offline Multi-Agent Reinforcement
  Learning with Actor Rectification"
160,1463161177412976660,708639362653749248,Christoph Weisser,['We propose a novel approach on how to integrate topic structures of hashtag graphs into the estimation of topic models by connecting graph-based community detection and semi-supervised NMF.\n\n<LINK>'],https://arxiv.org/abs/2111.10401,"Extracting topics from large collections of unstructured text-documents has become a central task in current NLP applications and algorithms like NMF, LDA as well as their generalizations are the well-established current state of the art. However, especially when it comes to short text documents like Tweets, these approaches often lead to unsatisfying results due to the sparsity of the document-feature matrices. Even though, several approaches have been proposed to overcome this sparsity by taking additional information into account, these are merely focused on the aggregation of similar documents and the estimation of word-co-occurrences. This ultimately completely neglects the fact that a lot of topical-information can be actually retrieved from so-called hashtag-graphs by applying common community detection algorithms. Therefore, this paper outlines a novel approach on how to integrate topic structures of hashtag graphs into the estimation of topic models by connecting graph-based community detection and semi-supervised NMF. By applying this approach on recently streamed Twitter data it will be seen that this procedure actually leads to more intuitive and humanly interpretable topics. ","Community-Detection via Hashtag-Graphs for Semi-Supervised NMF Topic
  Models"
161,1463113463258091522,1313016221781110784,Alexandros Efstratiou,"['New preprint out with @tristanc ! In this study, we examine whether scientific consensus around COVID-19 issues is misrepresented on Twitter.\n\n<LINK> (1/6)', 'We use Nobel Laureates in Medicine and Physiology to create a proxy of scientific consensus on COVID-19. Then, we collect tweets mentioning these laureates in the context of COVID-19 vaccines (15.8K) or COVID-19 generally (208K). (2/6)', 'We find a 1:24 ratio of scientific consensus on issues such as vaccines or virus origins. However, the contrarian laureate is mentioned 426 times more than expected based on true consensus on vaccines and 43 times more on COVID-19 generally. (3/6)', 'This amplification is driven by less popular accounts, which however receive higher relative engagement. Our SNA also reveals that discussions of the contrarian scientist are segmented across several geographical regions. (4/6) https://t.co/Azxn9XyZxa', 'Overall, a minority of contrarian scientists may be amplified on social media. This can make unsupported information seem more credible than it is and assist in its spread and uptake. (5/6)', 'Further work is needed on a) other platforms and b) with a wider set of scientists to make up scientific consensus. (6/6)']",https://arxiv.org/abs/2111.10594,"The COVID-19 pandemic has resulted in a slew of misinformation, often described as an ""infodemic"". Whereas previous research has focused on the propagation of unreliable sources as a main vehicle of misinformation, the present study focuses on exploring the role of scientists whose views oppose the scientific consensus. Using Nobelists in Physiology and Medicine as a proxy for scientific consensus, we analyze two separate datasets: 15.8K tweets by 13.1K unique users on COVID-19 vaccines specifically, and 208K tweets by 151K unique users on COVID-19 broadly which mention the Nobelist names. Our analyses reveal that dissenting scientists are amplified by a factor of 426 relative to true scientific consensus in the context of COVID-19 vaccines, and by a factor of 43 in the context of COVID-19 generally. Although more popular accounts tend to mention consensus-abiding scientists more, our results suggest that this false consensus is driven by higher engagement with dissent-mentioning tweets. Furthermore, false consensus mostly occurs due to traffic spikes following highly popularized statements of dissenting scientists. We find that dissenting voices are mainly discussed in French, English-speaking, Turkish, Brazilian, Argentine, Indian, and Japanese misinformation clusters. This research suggests that social media platforms should prioritize the exposure of consensus-abiding scientists as a vehicle of reversing false consensus and addressing misinformation stemming from seemingly credible sources. ","Misrepresenting Scientific Consensus on COVID-19: The Amplification of
  Dissenting Scientists on Twitter"
162,1463107657745715205,1250924296056102913,Mainak Singha (he/him),['My 2nd paper is on arxiv !!\nWe find that supermassive black hole (AGN) driven outflows are mostly compact and their accretion properties may not play a role in launching these outflows.\n<LINK> <LINK>'],https://arxiv.org/abs/2111.10418,"[Abridged]The strong asymmetry in the optical [O III]$\lambda$5007 emission line is one of the best signatures of AGN-driven warm (~10$^4$ K) ionized gas outflows on host galaxy scales. While large spectroscopic surveys like SDSS have characterized the kinematics of [O III] for large samples of AGN, estimating the associated energetics requires spatially resolving these outflows with, for example, IFU studies. As part of CARS we obtained spatially-resolved IFU spectroscopy for a representative sample of 39 luminous type 1 AGN at 0.01<z<0.06 with MUSE and VIMOS IFUs at the VLT to infer the spatial location of the ionized gas outflows. We compare the light distributions of the [O III] wing to that of the H$\beta$ broad emission line region, a classical point source (PSF). We then use the PSF to distinguish between the unresolved and resolved [O III] wing emission. We further determine its location using spectro-astrometry for the point-like sources. The [O III] wing is spatially unresolved in 23 out of the 36 AGN with >80 % of the flux associated with a point-like source. We measure <100 pc offsets in the spatial location of the outflow from the AGN nucleus using the spectro-astrometry technique for these sources. For the other 13 AGN, the [O III] wing emission is resolved and possibly extended on kpc scale. We conclude that [O III] wing emission can be compact or extended in an unbiased luminous AGN sample, where both cases are likely to appear. Electron density in the compact [O III] wing regions (median $n_e$~1900 cm$^{-3}$) is nearly a magnitude higher than in the extended ones (median $n_e$~500 cm$^{-3}$). The presence of spatially extended and compact [O III] wing emission is unrelated to the AGN bolometric luminosity and to inclination effects, which means other features such as time delays, or mechanical feedback/radio jets may shape the ionized gas outflow properties. ","The Close AGN Reference Survey (CARS): Locating the [O III] wing
  component in luminous local Type 1 AGN"
163,1463102123487473668,1093924185427062784,Maureen Cohen,"[""New paper on the ArXiv! We used the Unified Model to study oscillations in the atmosphere of an Earth-like tidally locked #exoplanet. You've never seen oscillations like these before! 😱🤯🪐Detailed blog post to come. <LINK> @exoclimatology""]",https://arxiv.org/abs/2111.11281,"Using a three-dimensional general circulation model, we show that the atmospheric dynamics on a tidally locked Earth-like exoplanet, simulated with the planetary and orbital parameters of Proxima Centauri b, support a longitudinally asymmetric stratospheric wind oscillation (LASO), analogous to Earth's quasi-biennial oscillation (QBO). In our simulations, the LASO has a vertical extent of 35--55 km, a period of 5--6.5 months, and a peak-to-peak wind speed amplitude of -70 to +130 m/s with a maximum at an altitude of 41 km. Unlike the QBO, the LASO displays longitudinal asymmetries related to the asymmetric thermal forcing of the planet and to interactions with the resulting stationary Rossby waves. The equatorial gravity wave sources driving the LASO are localised in the deep convection region at the substellar point and in a jet exit region near the western terminator, unlike the QBO, for which these sources are distributed uniformly around the planet. Longitudinally, the western terminator experiences the highest wind speeds and undergoes reversals earlier than other longitudes. The antistellar point only experiences a weak oscillation with a very brief, low-speed westward phase. The QBO on Earth is associated with fluctuations in the abundances of water vapour and trace gases such as ozone which are also likely to occur on exoplanets if these gases are present. Strong fluctuations in temperature and the abundances of atmospheric species at the terminators will need to be considered when interpreting atmospheric observations of tidally locked exoplanets. ","Longitudinally asymmetric stratospheric oscillation on a tidally locked
  exoplanet"
164,1462979534085443589,916864821122830336,MinKai Lin,"['With my assistant Chun-Yen Hsu, we find new flavors of the streaming instability for planetesimal formation and explore how magnetic fields affect the OG streaming instability. Chun-Yen wrote a dusty MHD code from scratch to check results! <LINK> @epo_asiaa <LINK>']",https://arxiv.org/abs/2111.10381,"The streaming instability is one of the most promising pathways to the formation of planetesimals from pebbles. Understanding how this instability operates under realistic conditions expected in protoplanetary disks is therefore crucial to assess the efficiency of planet formation. Contemporary models of protoplanetary disks show that magnetic fields are key to driving gas accretion through large-scale, laminar magnetic stresses. However, the effect of such magnetic fields on the streaming instability has not been examined in detail. To this end, we study the stability of dusty, magnetized gas in a protoplanetary disk. We find the streaming instability can be enhanced by passive magnetic torques and even persist in the absence of a global radial pressure gradient. In this case, instability is attributed to the azimuthal drift between dust and gas, unlike the classical streaming instability, which is driven by radial drift. This suggests that the streaming instability can remain effective inside dust-trapping pressure bumps in accreting disks. When a live vertical field is considered, we find the magneto-rotational instability can be damped by dust feedback, while the classic streaming instability can be stabilized by magnetic perturbations. We also find that Alfv\'en waves can be destabilized by dust-gas drift, but this instability requires nearly ideal conditions. We discuss the possible implications of these results for dust dynamics and planetesimal formation in protoplanetary disks. ","Streaming instabilities in accreting and magnetized laminar
  protoplanetary disks"
165,1462800523871100933,979379437069271043,Pedro Machado,"['Great team work with excellent folks!\n<LINK> \n\nTL;DR: \n1) Possibly, we propose a method ;-)\n\n2) @Microboone cuts in the MiniBooNE sterile neutrino region. No final answer. Still lots of work to do at SBN with SBND and ICARUS!\n\n@Fermilab @FNALNeutrinos <LINK>']",https://arxiv.org/abs/2111.10359,"A new generation of neutrino experiments is testing the $4.7\sigma$ anomalous excess of electron-like events observed in MiniBooNE. This is of huge importance for particle physics, astrophysics, and cosmology, not only because of the potential discovery of physics beyond the Standard Model, but also because the lessons we will learn about neutrino-nucleus interactions will be crucial for the worldwide neutrino program. MicroBooNE has recently released results that appear to disfavor several explanations of the MiniBooNE anomaly. Here, we show quantitatively that MicroBooNE results, while a promising start, unquestionably do not probe the full parameter space of sterile neutrino models hinted at by MiniBooNE and other data, nor do they probe the $\nu_e$ interpretation of the MiniBooNE excess in a model-independent way. ","MicroBooNE and the $\nu_e$ Interpretation of the MiniBooNE Low-Energy
  Excess"
166,1461624034802688006,1310552063999438849,Hauke Group,['We propose a very feasible Rydberg-atom experiment to dynamically induce a stable gauge theory with disorder-free localization in the quench dynamics of a translation-invariant gauge-sector superposition domain-wall state: <LINK>\n@JCHalimeh @QManyBody @UniTrento <LINK>'],https://arxiv.org/abs/2111.08715,"Disorder-free localization is a recently discovered phenomenon of nonergodicity that can emerge in quantum many-body systems hosting gauge symmetries when the initial state is prepared in a superposition of gauge superselection sectors. Thermalization is then prevented up to all accessible evolution times despite the model being nonintegrable and translation-invariant. In a recent work [Halimeh, Zhao, Hauke, and Knolle, arXiv:2111.02427], it has been shown that terms linear in the gauge-symmetry generator stabilize disorder-free localization in $\mathrm{U}(1)$ gauge theories against gauge errors that couple different superselection sectors. Here, we show in the case of $\mathbb{Z}_2$ gauge theories that disorder-free localization can not only be stabilized, but also \textit{enhanced} by the addition of translation-invariant terms linear in a local $\mathbb{Z}_2$ \textit{pseudogenerator} that acts identically to the full generator in a single superselection sector, but not necessarily outside of it. We show analytically and numerically how this leads through the quantum Zeno effect to the dynamical emergence of a renormalized gauge theory with an enhanced local symmetry, which contains the $\mathbb{Z}_2$ gauge symmetry of the ideal model, associated with the $\mathbb{Z}_2$ pseudogenerator. The resulting proliferation of superselection sectors due to this dynamically emergent gauge theory creates an effective disorder greater than that in the original model, thereby enhancing disorder-free localization. We demonstrate the experimental feasibility of the $\mathbb{Z}_2$ pseudogenerator by providing a detailed readily implementable experimental proposal for the observation of disorder-free localization in a Rydberg setup. ","Enhancing disorder-free localization through dynamically emergent local
  symmetries"
167,1461513687127134209,1093387119148462081,Daniel Green,"['New paper with Tim Cohen and Akhil Premukar, A Tail of Eternal Inflation:\n\n<LINK>\n\nWe find the phase transition to eternal inflation is incalculable for seemingly very weak non-Gaussianty.  This has implications for Stochastic Inflation and the de Sitter Entropy <LINK>', 'We calculate the non-Gaussian (higher derivative) corrections to Stochastic Inflation and find it naively predicts eternal inflation could happen in our universe.\n\nWe interpret this as a breakdown of Stochastic Inflation when calculating the tail of the probability distribution', 'This was a surprising result to me.  I had expected weak non-Gaussianity at horizon crossing would mean these corrections are small.  It turns out the tail tests physics inside the horizon where the EFT of Inflation breaks down.  This might also be important for PBH formation.']",https://arxiv.org/abs/2111.09332,"Non-trivial inflaton self-interactions can yield calculable signatures of primordial non-Gaussianity that are measurable in cosmic surveys. Surprisingly, we find that the phase transition to slow-roll eternal inflation is often incalculable in the same models. Instead, this transition is sensitive to the non-Gaussian tail of the distribution of scalar fluctuations, which probes physics inside the horizon, potentially beyond the cutoff scale of the Effective Field Theory of Inflation. We demonstrate this fact directly by calculating non-Gaussian corrections to Stochastic Inflation within the framework of Soft de Sitter Effective Theory, from which we derive the associated probability distribution for the scalar fluctuations. We find parameter space consistent with current observations and weak coupling at horizon crossing in which the large fluctuations relevant for eternal inflation can only be determined by appealing to a UV completion. We also show this breakdown of the perturbative description is required for the de Sitter entropy to reflect the number of de Sitter microstates. ",A Tail of Eternal Inflation
168,1461271030757044226,3306943245,Konstantin Klemmer,['New paper out in @itssieee Intelligent Transportation Systems🤖🚘🚴\u200d♀️\n\nWe propose a simulation environment for finding optimal deployment plans for shared e-mobility systems using multi-agent deep neural search. \n\nCheck the paper here: <LINK> <LINK>'],https://arxiv.org/abs/2111.02149,"Shared e-mobility services have been widely tested and piloted in cities across the globe, and already woven into the fabric of modern urban planning. This paper studies a practical yet important problem in those systems: how to deploy and manage their infrastructure across space and time, so that the services are ubiquitous to the users while sustainable in profitability. However, in real-world systems evaluating the performance of different deployment strategies and then finding the optimal plan is prohibitively expensive, as it is often infeasible to conduct many iterations of trial-and-error. We tackle this by designing a high-fidelity simulation environment, which abstracts the key operation details of the shared e-mobility systems at fine-granularity, and is calibrated using data collected from the real-world. This allows us to try out arbitrary deployment plans to learn the optimal given specific context, before actually implementing any in the real-world systems. In particular, we propose a novel multi-agent neural search approach, in which we design a hierarchical controller to produce tentative deployment plans. The generated deployment plans are then tested using a multi-simulation paradigm, i.e., evaluated in parallel, where the results are used to train the controller with deep reinforcement learning. With this closed loop, the controller can be steered to have higher probability of generating better deployment plans in future iterations. The proposed approach has been evaluated extensively in our simulation environment, and experimental results show that it outperforms baselines e.g., human knowledge, and state-of-the-art heuristic-based optimization approaches in both service coverage and net revenue. ","Deployment Optimization for Shared e-Mobility Systems with Multi-agent
  Deep Neural Search"
169,1460895138461458436,1156374257523462150,Elisa Garro,['#Paperday\nWe continue with the characterization of new candidate globular clusters in order to complete de census of the globular clusters in the Milky Way. Here we study 19 more: <LINK> <LINK>'],http://arxiv.org/abs/2111.08317,"The census of the globular clusters (GCs) in the Milky Way (MW) is still a work in progress. We explore the nature of 19 new GC candidates in the Galactic bulge, based on the analysis of their colour-magnitude diagrams (CMDs) in the near-IR, using the VISTA Variables in the Via L\'actea Survey (VVV) database. We estimate their main astrophysical parameters: reddening and extinction, distance, total luminosity, mean cluster proper motions (PMs), metallicity and age. We obtain the cluster catalogues including the likely cluster members by applying a decontamination procedure on the observed CMDs, based upon the vector PM diagrams from VIRAC2. We estimate a wide reddening range of the $0.25 \leqslant E(J-K_s) \leqslant 2.0$ mag and extinction $0.11 \leqslant A_{Ks} \leqslant 0.86$ mag for the sample clusters as expected in the bulge regions. The range of heliocentric distances is $6.8\leqslant D\leqslant 11.4$ kpc. This allows us to place these clusters between 0.56 and 3.25 kpc from the Galactic centre, assuming $R_{\odot}=8.2$ kpc. Also, their PMs are kinematically similar to the typical motion of the Galactic bulge, apart from VVV-CL160, which shows different PMs. We also derive their metallicities and ages, finding $-1.40 \leqslant$ [Fe/H] $\leqslant 0.0$ dex and $t\approx 8-13$ Gyr respectively. The luminosities are calculated both in $K_{s}-$ and V-bands, recovering $-3.4 \leqslant M_V \leqslant -7.5$. We also examine the possible RR Lyrae members found in the cluster fields. Based on their positions, kinematics, metallicities and ages and comparing our results with the literature, we conclude that 9 candidates are real GCs, 7 need more observations to be fully confirmed as GCs, whereas 3 candidates are discarded for being younger open clusters. ","Inspection of 19 globular cluster candidates in the Galactic bulge with
  the VVV survey"
170,1460857303465283589,1093715503762165760,Ilya Valmianski,"['1/2 Check out our new extended abstract accepted to #ML4H2021 “Adding more data does not always help: A study in medical conversation summarization with PEGASUS”\n\n<LINK>\n\nWe show empirical limits of PEGASUS fine-tuned to summarize medical dialogue.', '2/2 we find that performance saturates as the dataset is scaled to ~1200 examples, while still far below theoretical maximum performance. We also look at several pseudo labeling/active labeling approaches but find that the raw number of examples is the most important factor.']",https://arxiv.org/abs/2111.07564,"Medical conversation summarization is integral in capturing information gathered during interactions between patients and physicians. Summarized conversations are used to facilitate patient hand-offs between physicians, and as part of providing care in the future. Summaries, however, can be time-consuming to produce and require domain expertise. Modern pre-trained NLP models such as PEGASUS have emerged as capable alternatives to human summarization, reaching state-of-the-art performance on many summarization benchmarks. However, many downstream tasks still require at least moderately sized datasets to achieve satisfactory performance. In this work we (1) explore the effect of dataset size on transfer learning medical conversation summarization using PEGASUS and (2) evaluate various iterative labeling strategies in the low-data regime, following their success in the classification setting. We find that model performance saturates with increase in dataset size and that the various active-learning strategies evaluated all show equivalent performance consistent with simple dataset size increase. We also find that naive iterative pseudo-labeling is on-par or slightly worse than no pseudo-labeling. Our work sheds light on the successes and challenges of translating low-data regime techniques in classification to medical conversation summarization and helps guides future work in this space. Relevant code available at \url{this https URL}. ","Adding more data does not always help: A study in medical conversation
  summarization with PEGASUS"
171,1460531790188322816,344361113,Manuel Gomez-Rodriguez,"['Check out our paper on Counterfactual Temporal Point Processes (TPPs) (<LINK>), the first paper led by Kimia Noorbakhsh. This paper was her 3-month internship project! What we propose in this paper is best explained by an example relevant for COVID-19 (1/n)', 'Assume that, during a pandemic, a government decides\nto implement business restrictions every time the weekly incidence—the (relative) number of new cases—is larger than certain threshold but unfortunately the incidence nevertheless spirals out of control (sounds familiar?) (2/n)', 'Counterfactual TPPs could help the government understand retrospectively to what extent the incidence would have grown had a lower threshold been implemented. This is in contrast with existing epidemiological models, also those developed during COVID-19 (3/n)', 'Existing epidemiological models cannot answer counterfactual questions, only predict what the future may look like under interventions given the past. Our methodology is general and applies to many types of temporal point processes, not only those found in epidemiology (4/n)', 'A catch? Counterfactual TPPs lie within level three in the ""ladder of causation"" of @yudapearl—we cannot validate our counterfactual predictions using observational nor interventional experiments. However, our model satisfies monotonicity, an intuitive assumption about... (5/n)', '...the causal mechanism of the world, which specifies how changes on the intensity function of a temporal point process may have lead to particular outcomes while holding ""every-thing else"" fixed. This assumption also helps avoiding non identifiability issues (6/n)', 'We have also released an open source implementation of counterfactual TPPs and a SIR network-based epidemiological model fitted using data from an Ebola outbreak in West Africa: https://t.co/1K1rsMonNo (Thanks to @WilliamTrouleau for helping us with the Ebola dataset!) (n/n)']",https://arxiv.org/abs/2111.07603,"Machine learning models based on temporal point processes are the state of the art in a wide variety of applications involving discrete events in continuous time. However, these models lack the ability to answer counterfactual questions, which are increasingly relevant as these models are being used to inform targeted interventions. In this work, our goal is to fill this gap. To this end, we first develop a causal model of thinning for temporal point processes that builds upon the Gumbel-Max structural causal model. This model satisfies a desirable counterfactual monotonicity condition, which is sufficient to identify counterfactual dynamics in the process of thinning. Then, given an observed realization of a temporal point process with a given intensity function, we develop a sampling algorithm that uses the above causal model of thinning and the superposition theorem to simulate counterfactual realizations of the temporal point process under a given alternative intensity function. Simulation experiments using synthetic and real epidemiological data show that the counterfactual realizations provided by our algorithm may give valuable insights to enhance targeted interventions. ",Counterfactual Temporal Point Processes
172,1460232756793950221,91585736,Johan Triana,"['New Preprint with @faherreraur : ""Ultrafast modulation of vibrational polaritons for controlling the quantum field statistics at mid-infrared frequencies""\n\nWe study the quantum field statistics of confined light by modulating the cavity frequency.\n\n<LINK>', 'The idea is supported by changing the cavity resonance frequency through a modulation of the dielectric response of the cavity materials using femtosecond UV pulses. \n\nBy a previous knowledge of molecular system, both super and sub Poissonian states could be detected.']",https://arxiv.org/abs/2111.06729,"Controlling the quantum field statistics of confined light is a long-standing goal in integrated photonics. We show that by coupling molecular vibrations with a confined mid-infrared cavity vacuum, the photocount and quadrature field statistics of the cavity field can be reversibly manipulated over sub-picosecond timescales. The mechanism involves changing the cavity resonance frequency through a modulation of the dielectric response of the cavity materials using femtosecond UV pulses. For a single anharmonic molecular vibration in an infrared cavity under ultrastrong coupling conditions, the pulsed modulation of the cavity frequency can adiabatically produce mid-infrared light that is simultaneously sub-Poissonian and quadrature squeezed, depending on the dipolar behavior of the vibrational mode. For a vibration-cavity system in strong coupling, non-adiabatic polariton excitations can be produced after the frequency modulation pulse is over, when the system is initially prepared in the lower polariton state. We propose design principles for the generation of mid-infrared quantum light by analyzing the dependence of the cavity field statistics on the shape of the electric dipole function of the molecule, the cavity detuning at the modulation peak and the anharmonicity of the Morse potential. Feasible experimental implementations of the modulation scheme are suggested. This work paves the way for the development of molecule-based mid-infrared quantum optical devices at room temperature. ","Ultrafast modulation of vibrational polaritons for controlling the
  quantum field statistics at mid-infrared frequencies"
173,1458707483128741894,926852003925581824,Leonard Adolphs,"['New paper on arXiv: ""Reason ﬁrst, then respond: Modular Generation for Knowledge-infused Dialogue"" 🤔→💬  \n\nWe propose a modular two-step model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents. \n\n<LINK>\n\n(1/6) <LINK>', 'K2R consists of\n\n1. A seq2seq knowledge model that maps from context to knowledge.\n\n2. A seq2seq response model that generates the conversational response given the predicted knowledge and the context.\n\n(2/6)', 'K2R outperforms its seq2seq counterpart on open-domain dialogue while being more interpretable due to the explicit intermediate knowledge step. It has no problem blending in seemingly unrelated **injected** knowledge in the response and fitting the dialogue context (img).\n\n(3/6) https://t.co/JmEGo51n3v', 'K2R can turn short-span QA results into conversational responses that fit the previous context → making pretrained QA models usable in conversational agents without the need for retraining!\n\n(4/6) https://t.co/qVjyha9lIO', 'In knowledge-grounded dialogue (Wizard of Wikipedia), it is shown to hallucinate less and have a larger overlap with the ground-truth knowledge.\n\n(5/6) https://t.co/bKKuIPvvOi', 'This is joint work with @shtruk, @JackUrbs, Arthur Szlam, and @jaseweston from @AIatMeta!\n\n(6/6)']",https://arxiv.org/abs/2111.05204,"Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this ""reasoning step"", the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting. ","Reason first, then respond: Modular Generation for Knowledge-infused
  Dialogue"
174,1458483019757535237,1262311297187819520,Tom S. Seifert,"['How far do spins travel in metals with strong spin orbit coupling? Apparently, the answer depends on how fast you look at them. In our preprint, we find 4 times shorter spin propagation lengths at THz than at GHz rates: <LINK> @PhysikFUBerlin #THz #spintronics <LINK>']",https://arxiv.org/abs/2111.04191,"Control over spin transport in antiferromagnetic systems is essential for future spintronic applications with operational speeds extending to ultrafast time scales. Here, we study the transition from the gigahertz (GHz) to terahertz (THz) regime of spin transport and spin-to-charge current conversion (S2C) in the prototypical antiferromagnet IrMn by employing spin pumping and THz spectroscopy techniques. We reveal a factor of 4 shorter characteristic propagation lengths of the spin current at THz frequencies (~ 0.5 nm) as compared to the GHz regime (~ 2 nm) which may be attributed to the ballistic and diffusive nature of electronic spin transport, respectively. The conclusion is supported by an extraction of sub-picosecond temporal dynamics of the THz spin current. We also report on a significant impact of the S2C originating from the IrMn/non-magnetic metal interface which is much more pronounced in the THz regime and opens the door for optimization of the spin control at ultrafast time scales. ","Impact of gigahertz and terahertz transport regimes on spin propagation
  and conversion in the antiferromagnet IrMn"
175,1458280561286397957,1065179437568712704,Michael Girard,"['Interestingly, the central retinal vessel trunk (and its branches) appears to be a stronger biomarker for #glaucoma than RNFL thickness! We propose a paradigm where the major retinal vessels may act as a protective skeleton for the optic disc. @arxiv: <LINK>. <LINK>']",https://arxiv.org/abs/2111.03997,"Purpose: To assess whether the three-dimensional (3D) structural configuration of the central retinal vessel trunk and its branches (CRVT&B) could be used as a diagnostic marker for glaucoma. Method: We trained a deep learning network to automatically segment the CRVT&B from the B-scans of the optical coherence tomography (OCT) volume of the optic nerve head (ONH). Subsequently, two different approaches were used for glaucoma diagnosis using the structural configuration of the CRVT&B as extracted from the OCT volumes. In the first approach, we aimed to provide a diagnosis using only 3D CNN and the 3D structure of the CRVT&B. For the second approach, we projected the 3D structure of the CRVT&B orthographically onto three planes to obtain 2D images, and then a 2D CNN was used for diagnosis. The segmentation accuracy was evaluated using the Dice coefficient, whereas the diagnostic accuracy was assessed using the area under the receiver operating characteristic curves (AUC). The diagnostic performance of the CRVT&B was also compared with that of retinal nerve fiber layer (RNFL) thickness. Results: Our segmentation network was able to efficiently segment retinal blood vessels from OCT scans. On a test set, we achieved a Dice coefficient of 0.81\pm0.07. The 3D and 2D diagnostic networks were able to differentiate glaucoma from non-glaucoma subjects with accuracies of 82.7% and 83.3%, respectively. The corresponding AUCs for CRVT&B were 0.89 and 0.90, higher than those obtained with RNFL thickness alone. Conclusions: Our work demonstrated that the diagnostic power of the CRVT&B is superior to that of a gold-standard glaucoma parameter, i.e., RNFL thickness. Our work also suggested that the major retinal blood vessels form a skeleton -- the configuration of which may be representative of major ONH structural changes as typically observed with the development and progression of glaucoma. ","The Three-Dimensional Structural Configuration of the Central Retinal
  Vessel Trunk and Branches as a Glaucoma Biomarker"
176,1458172614019633169,726232195707318273,⟨ Maicol | A | Ochoa ⟩,"['We study the emergence of the molecule-plasmon strong-coupling regime with semiclassical models, deriving forms for the coupling strength, damping rates, and conditions for observing the Rabi splitting in terms of geometric parameters. #physics #Chemistry\n<LINK>']",https://arxiv.org/abs/2111.03730,"The interaction between excited states of a molecule and excited states of metal nanostructure (e.g. plasmons) leads to hybrid states with modified optical properties. When plasmon resonance is swept through molecular transition frequency an avoided crossing may be observed, which is often regarded as a signature of strong coupling between plasmons and molecules. Such strong coupling is expected to be realized when $2|U|/{\hbar\Gamma}>1$, where $U$ and ${\Gamma}$ are the molecule-plasmon coupling and the spectral width of the optical transition respectively. Because both $U$ and ${\Gamma}$ strongly increase with decreasing distance between a molecule and a plasmonic structure it is not obvious that this condition can be satisfied for any molecule-metal surface distance. In this work we investigate the behavior of $U$ and ${\Gamma}$ for several geometries. Surprisingly, we find that if the only contributions to ${\Gamma}$ are lifetime broadenings associated with the radiative and nonradiative relaxation of a single molecular vibronic transition, including effects on molecular radiative and nonradiative lifetimes induced by the metal, the criterion $2|U|/{\hbar\Gamma}>1$ is easily satisfied by many configurations irrespective of the metal-molecule distance. This implies that the Rabi splitting can be observed in such structures if other sources of broadening are suppressed. Additionally, when the molecule-metal surface distance is varied keeping all other molecular and metal parameters constant, this behavior is mitigated due to the spectral shift associated with the same molecule-plasmon interaction, making the observation of Rabi splitting more challenging. ","Coupling, lifetimes and ""strong coupling"" maps for single molecules at
  plasmonic interfaces"
177,1458037734753374212,1034025041300803585,Jonathan Foldager,"['New paper out! \n<LINK>\n\nTogether with @artix41 and @Tweetteresearch, we propose a new variational quantum algorithm for thermal state preparation, namely one where we parameterize the (depolarization) noise. 👇', 'We derive a closed-form approximation for the free-energy and use it as a cost function for our variational algorithm. We find that it is indeed possible to get high fidelities in multiple temperatures and hamiltonians, but we also show the performance depends on the temperature.', 'We hope this work will inspire future research on exploiting noise in variational algorithms!']",https://arxiv.org/abs/2111.03935,"Preparing thermal states on a quantum computer can have a variety of applications, from simulating many-body quantum systems to training machine learning models. Variational circuits have been proposed for this task on near-term quantum computers, but several challenges remain, such as finding a scalable cost-function, avoiding the need of purification, and mitigating noise effects. We propose a new algorithm for thermal state preparation that tackles those three challenges by exploiting the noise of quantum circuits. We consider a variational architecture containing a depolarizing channel after each unitary layer, with the ability to directly control the level of noise. We derive a closed-form approximation for the free-energy of such circuit and use it as a cost function for our variational algorithm. By evaluating our method on a variety of Hamiltonians and system sizes, we find several systems for which the thermal state can be approximated with a high fidelity. However, we also show that the ability for our algorithm to learn the thermal state strongly depends on the temperature: while a high fidelity can be obtained for high and low temperatures, we identify a specific range for which the problem becomes more challenging. We hope that this first study on noise-assisted thermal state preparation will inspire future research on exploiting noise in variational algorithms. ",Noise-Assisted Variational Quantum Thermalization
178,1457998058655043586,55671329,Gerhard Johann Hagerer,['We are proud that our recent paper at KDIR 2021 was awarded with the best student paper award.\n\nA Case Study and Qualitative Analysis of Simple Cross-lingual Opinion Mining \n\nPaper: <LINK>\nPresentation: <LINK>\nConference: <LINK>'],https://arxiv.org/abs/2111.02259,"User-generated content from social media is produced in many languages, making it technically challenging to compare the discussed themes from one domain across different cultures and regions. It is relevant for domains in a globalized world, such as market research, where people from two nations and markets might have different requirements for a product. We propose a simple, modern, and effective method for building a single topic model with sentiment analysis capable of covering multiple languages simultanteously, based on a pre-trained state-of-the-art deep neural network for natural language understanding. To demonstrate its feasibility, we apply the model to newspaper articles and user comments of a specific domain, i.e., organic food products and related consumption behavior. The themes match across languages. Additionally, we obtain an high proportion of stable and domain-relevant topics, a meaningful relation between topics and their respective textual contents, and an interpretable representation for social media documents. Marketing can potentially benefit from our method, since it provides an easy-to-use means of addressing specific customer interests from different market regions around the globe. For reproducibility, we provide the code, data, and results of our study. ","A Case Study and Qualitative Analysis of Simple Cross-Lingual Opinion
  Mining"
179,1457975451675250688,1445651856818991104,Donato Maragno,"['In our joint work with @MIT , we propose an end-to-end pipeline for optimization with #constraintlearning.\nWe also released the first version of #OptiCL, a python package for Optimization with Constraints Learning.\nCode: <LINK>\nPaper: <LINK>']",https://arxiv.org/abs/2111.04469,"We establish a broad methodological foundation for mixed-integer optimization with learned constraints. We propose an end-to-end pipeline for data-driven decision making in which constraints and objectives are directly learned from data using machine learning, and the trained models are embedded in an optimization formulation. We exploit the mixed-integer optimization-representability of many machine learning methods, including linear models, decision trees, ensembles, and multi-layer perceptrons. The consideration of multiple methods allows us to capture various underlying relationships between decisions, contextual variables, and outcomes. We also characterize a decision trust region using the convex hull of the observations, to ensure credible recommendations and avoid extrapolation. We efficiently incorporate this representation using column generation and clustering. In combination with domain-driven constraints and objective terms, the embedded models and trust region define a mixed-integer optimization problem for prescription generation. We implement this framework as a Python package (OptiCL) for practitioners. We demonstrate the method in both chemotherapy optimization and World Food Programme planning. The case studies illustrate the benefit of the framework in generating high-quality prescriptions, the value added by the trust region, the incorporation of multiple machine learning methods, and the inclusion of multiple learned constraints. ",Mixed-Integer Optimization with Constraint Learning
180,1457712808670470145,1111772429930676225,Austen Gabrielpillai,"['#paperday My first first-author paper is now up on arXiv! (<LINK>)\n\ntl;dr: we ran Rockstar and Consistent-Trees on IllustrisTNG, the Santa Cruz SAM on top of those merger trees, and used SubLink to find and compare bijectively matched subhalos. (1/5)', ""First, the SAM results after running it on TNG100-1-Dark and TNG300-1-Dark's Rockstar catalogs. We found that for well resolved subhalos (&gt;1000 dark matter particles) the convergence between the two runs is very good + the results agree well with the calibration data (2/5) https://t.co/EvSwmxLmr2"", 'Using TNG100, we compared scaling relationships between the two models (top row), their dispersions (bottom-left), as well as the log difference of a quantity between bijective matches (bottom-middle), and the correlation of matched residuals (bottom-right). (3/5) https://t.co/ocgUCTPTQz', 'We conducted this analysis for a variety of relationships (stellar mass halo mass, cold gas mass, etc.) and found that most residuals have no to very little correlation! This provides us with valuable insight for tuning the SAM in the future to better match TNG outputs. (4/5)', ""I'm especially excited for the work to come, as well as the eventual part II where we look at quantities through a time evolution lens.\n\nMany thanks to my co-authors for making this first dive into astronomy research a great one! (5/5)""]",https://arxiv.org/abs/2111.03077,"We present the first results from applying the Santa Cruz semi-analytic model (SAM) for galaxy formation on merger trees extracted from a dark matter only version of the IllustrisTNG (TNG) simulations. We carry out a statistical comparison between the predictions of the Santa Cruz SAM and TNG for a subset of central galaxy properties at z=0, with a focus on stellar mass, cold and hot gas mass, star formation rate (SFR), and black hole (BH) mass. We find fairly good agreement between the mean predictions of the two methods for stellar mass functions and the stellar mass vs. halo mass (SMHM) relation, and qualitatively good agreement between the SFR or cold gas mass vs. stellar mass relation and quenched fraction as a function of stellar mass. There are greater differences between the predictions for hot (circumgalactic) gas mass and BH mass as a function of halo mass. Going beyond the mean relations, we also compare the dispersion in the predicted scaling relations, and the correlation in residuals on a halo-by-halo basis between halo mass and galaxy property scaling relations. Intriguingly, we find similar correlations between residuals in SMHM in the SAM and in TNG, suggesting that these relations may be shaped by similar physical processes. Other scaling relations do not show significant correlations in the residuals, indicating that the physics implementations in the SAM and TNG are significantly different. ","Galaxy Formation in the Santa Cruz semi-analytic model compared with
  IllustrisTNG -- I. Galaxy scaling relations, dispersions, and residuals at
  z=0"
181,1457604159566499842,1179071880508116992,Antonio Mezzacapo,"['How to best use quantum computers for combinatorial problems is an open question. In this work, we formulate relaxations of MaxCut into quantum Hamiltonians and propose measurement-based rounding schemes, inspired by classical relaxation methods.\n<LINK> <LINK>', 'The relaxations are based on quantum random access codes, inheriting their memory compression with respect to classical encodings. We establish relations between the spectra of the relaxed Hamiltonians and original combinatorial problems, via quantum rounding protocols.', 'First rounding is stochastic, producing average cuts that approximate in average the optimal ones by a factor of least 0.555 or 0.625, if given access to a quantum state with energy between the optimal classical cut and the maximal relaxed energy.', 'The second rounding is deterministic, outperforming the stochastic one on average. We performed experiments on superconducting quantum devices, which have demonstrated approximation ratios of 0.905 for a 40-node MaxCut graph and 0.9626 on a 40-node weighted planar MaxCut problem.', 'We show that rounding from higher relaxed energy states leads to better solutions. This motivates to look into classes of graphs that show large separations between the relaxed spectrum and the optimal classical cut values. More sophisticated rounding procedure can be considered.', 'Existing methods for quantum many-body systems can be used to address combinatorial problem in this relaxed formulation. We have explored these ideas along with the research team at @Boeing.', 'Experiments performed on @IBMResearch Dublin quantum processor - quantum volume 64 - programmed with @qiskit']",https://arxiv.org/abs/2111.03167,"Combinatorial problems are formulated to find optimal designs within a fixed set of constraints. They are commonly found across diverse engineering and scientific domains. Understanding how to best use quantum computers for combinatorial optimization is to date an open problem. Here we propose new methods for producing approximate solutions for the maximum cut problem and its weighted version, which are based on relaxations to local quantum Hamiltonians. These relaxations are defined through commutative maps, which in turn are constructed borrowing ideas from quantum random access codes. We establish relations between the spectra of the relaxed Hamiltonians and optimal cuts of the original problems, via two quantum rounding protocols. The first one is based on projections to random magic states. It produces average cuts that approximate the optimal one by a factor of least 0.555 or 0.625, depending on the relaxation chosen, if given access to a quantum state with energy between the optimal classical cut and the maximal relaxed energy. The second rounding protocol is deterministic and it is based on estimation of Pauli observables. The proposed quantum relaxations inherit memory compression from quantum random access codes, which allowed us to test the performances of the methods presented for 3-regular random graphs and a design problem motivated by industry for sizes up to 40 nodes, on superconducting quantum processors. ",Approximate Solutions of Combinatorial Problems via Quantum Relaxations
182,1456538097185923102,1238481001304686594,Pablo Martínez-Miravé,"['New paper! With @MariamTortola and Susana Molina Sedgwick\n\n""Non-standard interactions from the future neutrino solar sector""👇\n\n<LINK>\n\nWe study the pontential of a combined analysis of JUNO and Hyper-Kamiokande in the presence of non-standard interactions(NSI)', 'We show that strong constraints on NSI can be derived in that case, while ensuring an accurate determination of the oscillation parameters.\n\nWe also illustrate the nice complementarity between both experiments 😊 https://t.co/BaAC8JE7NP']",https://arxiv.org/abs/2111.03031,"The next-generation neutrino experiment JUNO will determine the solar oscillation parameters - $\sin^2 \theta_{12}$ and $\Delta m^2_{21}$ - with great accuracy, in addition to measuring $\sin^2\theta_{13}$, $\Delta m^2_{31}$, and the mass ordering. In parallel, the continued study of solar neutrinos at Hyper-Kamiokande will provide complementary measurements in the solar sector. In this paper, we address the expected sensitivity to non-universal and flavour-changing non-standard interactions (NSI) with $d$-type quarks from the combination of these two future neutrino experiments. We also show the robustness of their measurements of the solar parameters $\sin^2 \theta_{12}$ and $\Delta m^2_{21}$ in the presence of NSI. We study the impact of the exact experimental configuration of the Hyper-Kamiokande detector, and conclude it is of little relevance in this scenario. Finally, we find that the LMA-D solution is expected to be present if no additional input from non-oscillation experiments is considered. ",Non-standard interactions from the future neutrino solar sector
183,1456356241496698892,51169895,Gianluca Stringhini,"['In our latest pre-print we present a computational pipeline to study online discussion surrounding conspiracy theories on Reddit and Twitter.\n\nWork let by my student @pujanpaudel14 \n\n📜<LINK>\n\n1/ <LINK>', 'We first train a learning to rank model to identify social media posts that discuss a set of 189 conspiracy theories debunked by @snopes \n\nWe identify 66,274 posts and\n288,721 comments on Reddit, and 379,708 tweets on Twitter about them posted between 2016 and 2021\n\n2/', 'We find that conspiracy theories are discussed for long periods of time, with 80% of them being discussed for over a year on social media\n\n3/', 'We also find that different communities discuss conspiracy theories differently. While conspiracy-focused subreddits dissect the elements of each conspiracy theory, general purpose ones just report about these conspiracies\n\n4/ https://t.co/3w9jBo2riU', 'Finally, we find that conspiracy-focused communities are mostly echo chambers and do not have a strong influence on other communities in spreading conspiracy theories, while general purpose communities do not go as deep in their discussion but have a stronger influence\n\n5/ https://t.co/iUv7Qflwhi', 'Our results raise important questions on how to moderate mis/disinformation. The communities that are devoted to conspiracy theories and are common targets of deplatforming actions are rather isolated, and removing them would likely not affect the conspiracy ecosystem much.\n\n6/', 'What is a good moderation strategy then? This needs to be the subject of further research by the community 👩\u200d🔬👨\u200d🔬\n\n7/7', 'What should we do then? More research is needed!\n\n7/7']",https://arxiv.org/abs/2111.02187,"This paper presents a multi-platform computational pipeline geared to identify social media posts discussing (known) conspiracy theories. We use 189 conspiracy claims collected by Snopes, and find 66k posts and 277k comments on Reddit, and 379k tweets discussing them. Then, we study how conspiracies are discussed on different Web communities and which ones are particularly influential in driving the discussion about them. Our analysis sheds light on how conspiracy theories are discussed and spread online, while highlighting multiple challenges in mitigating them. ","Soros, Child Sacrifices, and 5G: Understanding the Spread of Conspiracy
  Theories on Web Communities"
184,1456175139868053506,1308463968713924610,Dr Nishtha Srivastava,['🚨Pre-print alert. We propose a python module for Automatic Volcanic event detection and catalog consolidation. <LINK>🚨 #volcanoes #seismology #python #DeepLearning #MachineLearning'],https://arxiv.org/abs/2111.01513,"Many active volcanoes in the world exhibit Strombolian activity, which is typically characterized by relatively frequent mild events and also by rare and much more destructive major explosions and paroxysms. Detailed analyses of past major and minor events can help to understand the eruptive behavior of the volcano and the underlying physical and chemical processes. Catalogs of volcanic eruptions may be established using continuous seismic recordings at stations in the proximity of volcanoes. However, in many cases, the analysis of the recordings relies heavily on the manual picking of events by human experts. Recently developed Machine Learning-based approaches require large training data sets which may not be available a priori. Here, we propose an alternative automated approach: the Adaptive-Window Volcanic Event Selection Analysis Module (AWESAM). This process of creating event catalogs consists of three main steps: (i) identification of potential volcanic events based on squared ground-velocity amplitudes, an adaptive MaxFilter, and a prominence threshold. (ii) catalog consolidation by comparing and verification of the initial detections based on recordings from two different seismic stations. (iii) identification and exclusion of signals from regional tectonic earthquakes. The software package is applied to publicly accessible continuous seismic recordings from two almost equidistant stations at Stromboli volcano in Italy. We tested AWESAM by comparison with a hand-picked catalog and found that around 95 percent of the eruptions with a signal-to-noise ratio above three are detected. In a first application, we derive a new amplitude-frequency relationship from over 290.000 volcanic events at Stromboli during 2019-2020. The module allows for a straightforward generalization and application to other volcanoes worldwide. ","AWESAM: A Python Module for Automated Volcanic Event Detection Applied
  to Stromboli"
185,1456167325732904968,1107605372905377792,Aku Venhola🇺🇦,"['New paper in ArXiv today about low surface brightness dwarfs in the Fornax cluster:\n<LINK>\nIn this paper, we applied MTO to find new galaxies from the FDS data. This way we were able to extend our previous FDS dwarf catalog to 821 dwarfs. Below some key findings:', '- Massive dwarfs (log(M*)&gt;7) show correlation between axis-ratio and surface brightness, which can be corrected by inclination correction -&gt; this is not the case for log(M*)&lt;7 dwarfs. Interpretation: disks vs. not disks?', '-Surface brightness of dwarfs become fainter toward the cluster center. This is partly due to the ageing of their stellar populations but we do find evidence for tidal interactions as well: more disturbed morphology, lower axis-ratios and lower surface density toward the center.', '-We found the low-luminosity end slope of the galaxy luminosity function to be -1.4, which is consistent with findings in other environments studied with similar data. However, the match is not that great with simulations (resolution issues?).', 'Conclusion: the Fornax galaxy cluster makes dwarfs fainter by stopping their star formation and by lowering their density via tidal forces.']",https://arxiv.org/abs/2111.01855,"In this work we use Max-Tree Objects, (MTO) on the FDS data in order to detect previously undetected Low surface brightness (LSB) galaxies. After extending the existing Fornax dwarf galaxy catalogs with this sample, our goal is to understand the evolution of LSB dwarfs in the cluster. We also study the contribution of the newly detected galaxies to the faint end of the luminosity function. We test the detection completeness and parameter extraction accuracy of MTO. We then apply MTO to the FDS images to identify LSB candidates. The identified objects are fitted with 2D S\'ersic models using GALFIT and classified based on their morphological appearance, colors, and structure. With MTO, we are able to increase the completeness of our earlier FDS dwarf catalog (FDSDC) 0.5-1 mag deeper in terms of total magnitude and surface brightness. Due to the increased accuracy in measuring sizes of the detected objects, we also add many small galaxies to the catalog that were previously excluded as their outer parts had been missed in detection. We detect 265 new LSB dwarf galaxies in the Fornax cluster, which increases the total number of known dwarfs in Fornax to 821. Using the extended catalog, we show that the luminosity function has a faint-end slope of -1.38+/-0.02. We compare the obtained luminosity function with different environments studied earlier using deep data but do not find any significant differences. On the other hand, the Fornax-like simulated clusters in the IllustrisTNG cosmological simulation have shallower slopes than found in the observational data. We also find several trends in the galaxy colors, structure, and morphology that support the idea that the number of LSB galaxies is higher in the cluster center due to tidal forces and the age dimming of the stellar populations. The same result also holds for the subgroup of large LSB galaxies, so-called ultra-diffuse galaxies. ","The Fornax Deep Survey (FDS) with VST XII: Low surface brightness dwarf
  galaxies in the Fornax cluster"
186,1456076469772378114,1653127615,Chris Monahan,"['Our new paper just arrived on the arXiv <LINK>! We study how the momentum carried by quarks inside a neutron is related to their spin (relative to the neutron), directly from the theory of the strong nuclear force (which holds the proton together). #physics #QCD', ""@ZhongboK The neutron is very special! :) Yes, you are completely right, in the paper we discuss nucleons, but I thought maybe on twitter more people would have heard of a neutron than a nucleon. Of course that doesn't account for the fact that my followers are basically all physicists...""]",https://arxiv.org/abs/2111.01808,"We present a determination of the non-singlet transversity parton distribution function (PDF) of the nucleon, normalized with respect to the tensor charge at $\mu^2=2$ GeV$^2$ from lattice quantum chromodynamics. We apply the pseudo-distribution approach, using a gauge ensemble with a lattice spacing of 0.094 fm and the light quark mass tuned to a pion mass of 358 MeV. We extract the transversity PDF from the analysis of the short-distance behavior of the Ioffe-time pseudo-distribution using the leading-twist next-to-leading order (NLO) matching coefficients calculated for transversity. We reconstruct the $x$-dependence of the transversity PDF through an expansion in a basis of Jacobi polynomials in order to reduce the PDF ansatz dependence. Within the limitations imposed by a heavier-than-physical pion mass and a fixed lattice spacing, we present a comparison of our estimate for the valence transversity PDF with the recent global fit results based on single transverse spin asymmetry. We find the intrinsic nucleon sea to be isospin symmetric with respect to transversity. ","The transversity parton distribution function of the nucleon using the
  pseudo-distribution approach"
187,1455888282529718274,824550893853036545,Christian Herff,"['New preprint out with Jonas Kohler. \nIn this study, we tried to synthesize high-quality audio from stereotactic-EEG recordings using an Encoder-Decoder Framework similar to Tacotron 2.\n<LINK>', 'This recurrent architecture did indeed outperform our previously presented CNN and produced nice speech from the sparse sampling provided by sEEG. Looking at the results dependent on training set size, we see results still improve with more training data. https://t.co/nkEq8hdvmk', 'We also looked at the 33 most important electrode contacts (yellow) using variational feature dropout and found a wide-variety of cortical and white matter areas contributing: https://t.co/qPSXbb6SRD', 'In the attention matrix of the Encoder-Decoder, we see a nice diagonal, but also realize that the very beginning and very end of our long input sequences are not attended to at. The huge temporal embedding in sEEG (1.2 seconds to synthesize 400ms of audio) might not be necessary. https://t.co/1jvuyzVs6Q', ""Again, we're sharing all code:\nhttps://t.co/RiOiHFWXZa\nAnd data:\nhttps://t.co/AAyJl1jOwd\nWe're excited to see what you can do with it."", '@sergeydoestweet Thanks! Yes, the presentation includes this study.']",https://arxiv.org/abs/2111.01457,"Speech Neuroprostheses have the potential to enable communication for people with dysarthria or anarthria. Recent advances have demonstrated high-quality text decoding and speech synthesis from electrocorticographic grids placed on the cortical surface. Here, we investigate a less invasive measurement modality, namely stereotactic EEG (sEEG) that provides sparse sampling from multiple brain regions, including subcortical regions. To evaluate whether sEEG can also be used to synthesize high-quality audio from neural recordings, we employ a recurrent encoder-decoder framework based on modern deep learning methods. We demonstrate that high-quality speech can be reconstructed from these minimally invasive recordings, despite a limited amount of training data. Finally, we utilize variational feature dropout to successfully identify the most informative electrode contacts. ","Synthesizing Speech from Intracranial Depth Electrodes using an
  Encoder-Decoder Framework"
188,1455686391086698497,2742174478,Jes Frellsen,['Can an EBM be sandwiched? We (@conggeng94 Wang Gao @jesfrellsen Hauberg) propose a bidirectional bound on EBM LL and link our upper bound to gradient penalties. This stabilize training and gives HQ densities and samples. Enjoy the sandwich!\n#NeurIPS2021\n<LINK> <LINK>'],https://arxiv.org/abs/2111.00929,"Energy-based models (EBMs) provide an elegant framework for density estimation, but they are notoriously difficult to train. Recent work has established links to generative adversarial networks, where the EBM is trained through a minimax game with a variational value function. We propose a bidirectional bound on the EBM log-likelihood, such that we maximize a lower bound and minimize an upper bound when solving the minimax game. We link one bound to a gradient penalty that stabilizes training, thereby providing grounding for best engineering practice. To evaluate the bounds we develop a new and efficient estimator of the Jacobi-determinant of the EBM generator. We demonstrate that these developments significantly stabilize training and yield high-quality density estimation and sample generation. ","Bounds all around: training energy-based models with bidirectional
  bounds"
189,1455533679934164996,1658897460,Tim Davis,"['Today we released the VERTICO (The Virgo Environment Traced In CO) survey paper! Survey led by @DrTobyBrown using beautiful @almaobs CO data for ~50 Virgo Cluster galaxies to reveal how galaxies in dense environments evolve. <LINK>\n<LINK> <LINK>', 'The survey involves a lot of current and past @cardiffuni @cardiffPHYSX people, including myself, Nikki Zabel and @astroquokka']",https://arxiv.org/abs/2111.00937,"We present the Virgo Environment Traced in CO (VERTICO) survey, a new effort to map $^{12}$CO($2-1$), $^{13}$CO($2-1$), and C$^{18}$O($2-1$) in 51 Virgo Cluster galaxies with the Atacama Compact Array, part of the Atacama Large Millimeter/submillimeter Array (ALMA). The primary motivation of VERTICO is to understand the physical mechanisms that perturb molecular gas disks, and therefore star formation and galaxy evolution, in dense environments. This first paper contains an overview of VERTICO's design and sample selection, $^{12}$CO($2-1$) observations, and data reduction procedures. We characterize global $^{12}$CO($2-1$) fluxes and molecular gas masses for the 49 detected VERTICO galaxies, provide upper limits for the two non-detections, and produce resolved $^{12}$CO($2-1$) data products (median resolution $= 8^{\prime\prime} \approx 640~{\rm pc}$). Azimuthally averaged $^{12}$CO($2-1$) radial intensity profiles are presented along with derived molecular gas radii. We demonstrate the scientific power of VERTICO by comparing the molecular gas size--mass scaling relation for our galaxies with a control sample of field galaxies, highlighting the strong effect that radius definition has on this correlation. We discuss the drivers of the form and scatter in the size--mass relation and highlight areas for future work. VERTICO is an ideal resource for studying the fate of molecular gas in cluster galaxies and the physics of environment-driven processes that perturb the star formation cycle. Upon public release, the survey will provide a homogeneous legacy dataset for studying galaxy evolution in our closest cluster. ",VERTICO: The Virgo Environment Traced In CO Survey
190,1455463440802328582,255603507,Sara Papi,['Are we missing something in the evaluation of our SimulST systems? Check out our paper accepted at CLIC-it to find out. @fbk_mt  @Turchi_Marco @negri_teo \n<LINK>'],https://arxiv.org/abs/2111.00514,"Simultaneous speech translation (SimulST) is the task in which output generation has to be performed on partial, incremental speech input. In recent years, SimulST has become popular due to the spread of cross-lingual application scenarios, like international live conferences and streaming lectures, in which on-the-fly speech translation can facilitate users' access to audio-visual content. In this paper, we analyze the characteristics of the SimulST systems developed so far, discussing their strengths and weaknesses. We then concentrate on the evaluation framework required to properly assess systems' effectiveness. To this end, we raise the need for a broader performance analysis, also including the user experience standpoint. SimulST systems, indeed, should be evaluated not only in terms of quality/latency measures, but also via task-oriented metrics accounting, for instance, for the visualization strategy adopted. In light of this, we highlight which are the goals achieved by the community and what is still missing. ",Visualization: the missing factor in Simultaneous Speech Translation
191,1455373672311578629,1379356298714742784,SummarizedML,['We study automated intrusion prevention using reinforcement learning.\n📄 <LINK> <LINK>'],http://arxiv.org/abs/2111.00289v1,"We study automated intrusion prevention using reinforcement learning. Following a novel approach, we formulate the problem of intrusion prevention as an (optimal) multiple stopping problem. This formulation gives us insight into the structure of optimal policies, which we show to have threshold properties. For most practical cases, it is not feasible to obtain an optimal defender policy using dynamic programming. We therefore develop a reinforcement learning approach to approximate an optimal policy. Our method for learning and validating policies includes two systems: a simulation system where defender policies are incrementally learned and an emulation system where statistics are produced that drive simulation runs and where learned policies are evaluated. We show that our approach can produce effective defender policies for a practical IT infrastructure of limited size. Inspection of the learned policies confirms that they exhibit threshold properties. ",] Intrusion Prevention through Optimal Stopping
192,1465281600426496000,4417968213,Ani Nenkova,"['What we found about temporal effects on NLP system performance:\n\n1) system performance may not get worse over time; it can even improve over time; \n\nYET\n\n2) a system with better performance can be obtained by retraining on temporally more recent data\n\n<LINK>', 'By how much performance changes over time, and by how much it can be improved by temporal adaptation varies considerably depending on the language representation.\n\nRepresentations that yield an overall better system also afford less room for temporal adaptation', 'Experiments on named entity recognition, sentiment in product reviews and true casing consistently show the need to clearly distinguish between temporal model deterioration and the potential for temporal adaptation.', 'The overlap between training (fine-tuning) and testing data vocabulary steadily diminishes over time, and the smaller overlap is correlated with worse performance for a system without pre-trained representations.', 'Self-labeling is one successful approach to temporal domain adaptation without human labeling of more recent data. \n\nThe system produces labels for data from a more recent time period and is retrained on the combination of original and self-labeled data.', '@LChoshen Do you mean no human labeled new data? If so, indeed, no new data is now introduced.\n\nThere is unlabelled data, labeled by the system trained on older data. Then the initial and self-labeled data are combined, the model retrained and the results are better.', '@LChoshen As for why it works, I agree it is unintuitive and hopefully there will be a follow up paper to unpack the why.\n\nWe tried it because performance didn’t seem to deteriorate and checking if the labels are actually useful was an additional way to verify that.']",https://arxiv.org/abs/2111.12790,"Keeping the performance of language technologies optimal as time passes is of great practical interest. Here we survey prior work concerned with the effect of time on system performance, establishing more nuanced terminology for discussing the topic and proper experimental design to support solid conclusions about the observed phenomena. We present a set of experiments with systems powered by large neural pretrained representations for English to demonstrate that {\em temporal model deterioration} is not as big a concern, with some models in fact improving when tested on data drawn from a later time period. It is however the case that {\em temporal domain adaptation} is beneficial, with better performance for a given time period possible when the system is trained on temporally more recent data. Our experiments reveal that the distinctions between temporal model deterioration and temporal domain adaptation becomes salient for systems built upon pretrained representations. Finally we examine the efficacy of two approaches for temporal domain adaptation without human annotations on new data, with self-labeling proving to be superior to continual pre-training. Notably, for named entity recognition, self-labeling leads to better temporal adaptation than human annotation. ",Temporal Effects on Pre-trained Models for Language Processing Tasks
193,1463170559697641492,3366411995,Sona Najafi,"['Read about our recent work on Quantum reservoir computing using arrays of Rydberg atoms. In this work, we study the advantage of the 2D array of the Rydberg atoms as a quantum reservoir: \n<LINK>', 'We further show that our quantized RNN (qRNN) is indeed capable of replicating the learning of several cognitive tasks such as multitasking, decision making, and long-term memory by taking advantage of quantum many-body scars.']",https://arxiv.org/abs/2111.10956,"Quantum computing promises to provide machine learning with computational advantages. However, noisy intermediate-scale quantum (NISQ) devices pose engineering challenges to realizing quantum machine learning (QML) advantages. Recently, a series of QML computational models inspired by the noise-tolerant dynamics on the brain have emerged as a means to circumvent the hardware limitations of NISQ devices. In this article, we introduce a quantum version of a recurrent neural network (RNN), a well-known model for neural circuits in the brain. Our quantum RNN (qRNN) makes use of the natural Hamiltonian dynamics of an ensemble of interacting spin-1/2 particles as a means for computation. In the limit where the Hamiltonian is diagonal, the qRNN recovers the dynamics of the classical version. Beyond this limit, we observe that the quantum dynamics of the qRNN provide it quantum computational features that can aid it in computation. To this end, we study a qRNN based on arrays of Rydberg atoms, and show that the qRNN is indeed capable of replicating the learning of several cognitive tasks such as multitasking, decision making, and long-term memory by taking advantage of several key features of this platform such as interatomic species interactions, and quantum many-body scars. ",Quantum reservoir computing using arrays of Rydberg atoms
194,1458442467011878917,186701821,Aldo Pacchiano,"['(1/3 ) Happy to finally post this paper on arxiv! In this work we propose an algorithm with logarithmic instance dependent regret guarantees for the Multi-Player Multi-Armed bandit problem. \n<LINK>', '(2/3) Our results are based on two innovations. First, we show that a simple modification to a successive elimination strategy can be used by the players to estimate their suboptimality gaps, up to constant factors, in the absence of collisions.', '(3/3) Second, we leverage the first result to design a communication protocol that successfully uses the small reward of collisions to coordinate among players, while preserving meaningful instance-dependent logarithmic regret guarantees.']",https://arxiv.org/abs/2111.04873,"We study the problem of information sharing and cooperation in Multi-Player Multi-Armed bandits. We propose the first algorithm that achieves logarithmic regret for this problem. Our results are based on two innovations. First, we show that a simple modification to a successive elimination strategy can be used to allow the players to estimate their suboptimality gaps, up to constant factors, in the absence of collisions. Second, we leverage the first result to design a communication protocol that successfully uses the small reward of collisions to coordinate among players, while preserving meaningful instance-dependent logarithmic regret guarantees. ","An Instance-Dependent Analysis for the Cooperative Multi-Player
  Multi-Armed Bandit"
195,1456180048222998528,569063423,Heino Falcke,"['New official paper by @ehtelescope led by @azstewobs group. Low variability we find on large interferometry baseline triangles is well explained if the ring we see in M87* ist indeed gravitational and not due to plasma effects. Technical but  promising. <LINK> <LINK>', 'Followed by investigation of how general we can relate size of the black hole shadow to the bright ring we see, even if the theory of gravity deviates from general relativity. Turns out the shadow is a robust measure of spacetime properties of black holes https://t.co/ItzuROxg7T https://t.co/hlAnqrKYEa', 'Maybe useful to say that we introduced the concept of a black hole shadow in a paper in 2000 with @AgolEric and Melia and you can find a more intuitive description in this recent paper with @thomasbronzwaer https://t.co/8cQgkm1uKs https://t.co/niJSXhWWLt']",https://arxiv.org/abs/2111.01317,"The black-hole images obtained with the Event Horizon Telescope (EHT) are expected to be variable at the dynamical timescale near their horizons. For the black hole at the center of the M87 galaxy, this timescale (5-61 days) is comparable to the 6-day extent of the 2017 EHT observations. Closure phases along baseline triangles are robust interferometric observables that are sensitive to the expected structural changes of the images but are free of station-based atmospheric and instrumental errors. We explored the day-to-day variability in closure phase measurements on all six linearly independent non-trivial baseline triangles that can be formed from the 2017 observations. We showed that three triangles exhibit very low day-to-day variability, with a dispersion of $\sim3-5^\circ$. The only triangles that exhibit substantially higher variability ($\sim90-180^\circ$) are the ones with baselines that cross visibility amplitude minima on the $u-v$ plane, as expected from theoretical modeling. We used two sets of General Relativistic magnetohydrodynamic simulations to explore the dependence of the predicted variability on various black-hole and accretion-flow parameters. We found that changing the magnetic field configuration, electron temperature model, or black-hole spin has a marginal effect on the model consistency with the observed level of variability. On the other hand, the most discriminating image characteristic of models is the fractional width of the bright ring of emission. Models that best reproduce the observed small level of variability are characterized by thin ring-like images with structures dominated by gravitational lensing effects and thus least affected by turbulence in the accreting plasmas. ","The Variability of the Black-Hole Image in M87 at the Dynamical Time
  Scale"
