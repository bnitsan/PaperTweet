,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1525124399673884673,1705189098,Ani Eloyan,['New paper by now former student Dr. \n@KUNMENG2 on statistical inference of shapes via the Smooth Euler Characteristic Transform with \n@lorin_crawford.  We provide mathematical foundations for randomness of shapes and present methods for hypothesis testing <LINK> <LINK>'],https://arxiv.org/abs/2204.12699,"In this paper, we provide the mathematical foundations for the randomness of shapes and the distributions of smooth Euler characteristic transform. Based on these foundations, we propose an approach for testing hypotheses on random shapes. Simulation studies are provided to support our mathematical derivations and show the performance of our proposed hypothesis testing framework. Our discussions connect the following fields: algebraic and computational topology, probability theory and stochastic processes, Sobolev spaces and functional analysis, statistical inference, and medical imaging. ","Randomness and Statistical Inference of Shapes via the Smooth Euler
  Characteristic Transform"
1,1524315317178183682,701398560512851968,Loic Landrieu,"['[CVPR 2022 Oral] üì∏+‚òÅÔ∏è ""Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation""\nNew SOTA for S3DIS: 74.7 mIoU. Check out the paper <LINK> and code <LINK>\n#CVPR #MachineLearning #ComputerVision']",http://arxiv.org/abs/2204.07548,"Recent works on 3D semantic segmentation propose to exploit the synergy between images and point clouds by processing each modality with a dedicated network and projecting learned 2D features onto 3D points. Merging large-scale point clouds and images raises several challenges, such as constructing a mapping between points and pixels, and aggregating features between multiple views. Current methods require mesh reconstruction or specialized sensors to recover occlusions, and use heuristics to select and aggregate available images. In contrast, we propose an end-to-end trainable multi-view aggregation model leveraging the viewing conditions of 3D points to merge features from images taken at arbitrary positions. Our method can combine standard 2D and 3D networks and outperforms both 3D models operating on colorized point clouds and hybrid 2D/3D networks without requiring colorization, meshing, or true depth maps. We set a new state-of-the-art for large-scale indoor/outdoor semantic segmentation on S3DIS (74.7 mIoU 6-Fold) and on KITTI-360 (58.3 mIoU). Our full pipeline is accessible at this https URL, and only requires raw 3D scans and a set of images and poses. ","Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic
  Segmentation"
2,1524255968011321344,23024823,Yang Cai,"['Very excited about our new paper with @ArgyrisOikonom1 @WeiqiangZheng3.\n\nWe obtain the tight last-iterate convergence rates for the Extragradient (EG) and Optimistic Gradient (OG) algorithms, settling an open problem raised by @KonstDaskalakis.\n<LINK>', ""We study the convex-concave min-max optimization  (and more generally the monotone variation inequalities) in the constrained setting. The EG algorithm by Korpelevich '1976 and the OG algorithm by Popov '1980 are among the most classical and popular algorithms for such problems."", 'For both EG and OG, we know that the last-iterate asymptotically converges, but the rate was not known despite having been studied for a long time. We obtain tight last-iterate convergence rates for both EG and OG.', 'Our proof builds on a new natural potential function, whose monotonicity is established using a sum-of-squares programming based computer-aided proof.\n\nFor more details, see my talk at the Simons Institute for our result on EG.\nhttps://t.co/eMr8KJmdtT']",https://arxiv.org/abs/2204.09228,"The monotone variational inequality is a central problem in mathematical programming that unifies and generalizes many important settings such as smooth convex optimization, two-player zero-sum games, convex-concave saddle point problems, etc. The extragradient algorithm by Korpelevich [1976] and the optimistic gradient descent-ascent algorithm by Popov [1980] are arguably the two most classical and popular methods for solving monotone variational inequalities. Despite its long history, the following major problem remains open. What is the last-iterate convergence rate of the extragradient algorithm or the optimistic gradient descent-ascent algorithm for monotone and Lipschitz variational inequalities with constraints? We resolve this open problem by showing that both the extragradient algorithm and the optimistic gradient descent-ascent algorithm have a tight $O\left(\frac{1}{\sqrt{T}}\right)$ last-iterate convergence rate for arbitrary convex feasible sets, which matches the lower bound by Golowich et al. [2020a, b]. Our rate is measured in terms of the standard gap function. At the core of our results lies a new performance measure -- the tangent residual, which can be viewed as an adaptation of the norm of the operator that takes the local constraints into account. We use the tangent residual (or a slight variation of the tangent residual) as the performance measure in our analysis of the extragradient algorithm (or the optimistic gradient descent-ascent algorithm). To establish the monotonicity of these performance measures, we develop a new approach that combines the power of the sum-of-squares programming with the low dimensionality of the update rule of the extragradient or the optimistic gradient descent-ascent algorithm. We believe our approach has many additional applications in the analysis of iterative methods. ","Tight Last-Iterate Convergence of the Extragradient and the Optimistic
  Gradient Descent-Ascent Algorithm for Constrained Monotone Variational
  Inequalities"
3,1523591077646675969,916940326132224000,Virginie Do,"['New paper accepted to #SIGIR2022, which means I‚Äôm going to Madrid this summer for my first in-person conference! ü•≥üá™üá∏\n\n‚ÄúOptimizing generalized Gini indices for fairness in rankings‚Äù, with Nicolas Usunier.\n<LINK> \n\n@MetaAI @DauphineMiles @LAMSADEDauphine 1/4', 'Generalized Gini functions are a family of inequality indices in economics. They include the well-known Gini coefficient. We show that in fact, they cover a variety of existing (two-sided) fairness criteria for recommender systems. 2/4', 'Generalized Gini functions are non-differentiable, so optimizing them over rankings for large-scale recommendation is challenging. We propose an algorithm based on Frank-Wolfe methods for nonsmooth optimization, and show it is computationally efficient. 3/4', 'Final remark: a friend of mine suggested an alternative title for the paper - ‚ÄúFIRGINIE: Fairness In Rankings with GINI indicEs.‚Äù Missed opportunity?  ü§î 4/4']",https://arxiv.org/abs/2204.06521,"There is growing interest in designing recommender systems that aim at being fair towards item producers or their least satisfied users. Inspired by the domain of inequality measurement in economics, this paper explores the use of generalized Gini welfare functions (GGFs) as a means to specify the normative criterion that recommender systems should optimize for. GGFs weight individuals depending on their ranks in the population, giving more weight to worse-off individuals to promote equality. Depending on these weights, GGFs minimize the Gini index of item exposure to promote equality between items, or focus on the performance on specific quantiles of least satisfied users. GGFs for ranking are challenging to optimize because they are non-differentiable. We resolve this challenge by leveraging tools from non-smooth optimization and projection operators used in differentiable sorting. We present experiments using real datasets with up to 15k users and items, which show that our approach obtains better trade-offs than the baselines on a variety of recommendation tasks and fairness criteria. ",Optimizing generalized Gini indices for fairness in rankings
4,1522996992300904449,1070455434304200705,Aryaman Arora,"['Excited to announce a new paper published at ACL 2022:\n\nEstimating the Entropy of Linguistic Distributions\nw/ @clara__meister, @ryandcotterell\n<LINK>\n\nThis is work that began over my internship at ETH Z√ºrich last summer :) Brief overview:', 'Information theory has a long history of application to linguistics, including investigations into\n‚è∫Ô∏èthe optimality of the lexicon,\n‚è∫Ô∏èmorphological complexity,\n‚è∫Ô∏èreading time and surprisal\nand so on‚Ä¶ (2/n)', 'The workhorse of information theory is Shannon entropy, which is defined over a distribution. But in linguistic work, we do not have access to the distribution‚Äîonly samples from it, e.g. a finite text corpus. (3/n)', 'To calculate entropy, we normally use maximum likelihood estimation, which means just plugging sample probabilities into the equation.\n\nDO NOT DO THIS! (4/n)', ""MLE is negatively biased (in expectation), especially with a sample size smaller than the number of classes. But people working on statistical estimation theory have come up with better entropy estimators that just haven't been applied to computational linguistic work yet. (5/n)"", 'So, we applied 6 different entropy estimators to both data sampled from synthetic distributions (w/ known entropy), then linguistic data as well as a replication of 2 previous information-theoretic studies. (6/n)', 'We found that the Chao‚ÄìShen estimator (using probability re-weightings) and the Nemenman‚ÄìShafee‚ÄìBialek estimator (using a Bayesian approach) gave the best performance on synthetic data and estimating the entropy of the unigram distribution (figure below). (7/n) https://t.co/OzD0yrK4w4', 'Applying these to previous work on the association between gendered nouns and their modifying adjectives (Williams et al., 2021) and between gender partitions of basic vocab across related languages (McCarthy et al., 2020), we found effect sizes reduced by ¬Ω. (8/n) https://t.co/XZgAWMKaek', 'So, we strongly suggest to information-theoretic linguists to be wary about using maximum-likelihood estimates of entropy, and instead use an estimator like Chao‚ÄìShen or NSB.\n\nFor more about estimation theory, code, pretty graphs, and a buncha proofs, check out the paper. (n/n)']",https://arxiv.org/abs/2204.01469,"Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. Finally, we end our paper with concrete recommendations for entropy estimation depending on distribution type and data availability. ",Estimating the Entropy of Linguistic Distributions
5,1522884041971838976,2886658437,Sean Raymond,"['How might an advanced alien civilization sculpt a planetary system to serve as a beacon of their existence?\n\nNew paper by Matt Clement, Dimitri Veras, @david_kipping and myself\n\nA thread that turned out less-short-than-anticipated \n\n1/\n\n<LINK>', 'A SETI beacon produced by an alien civilization should:\n1. Be detectable from the outside\n2. Be ""non-natural""\n3. Be long-lived \n\n2/', 'The idea @david_kipping came up with is to use multi-resonant planetary systems to encode simple integer sequences. \n\nBut which ones?  2:1 resonances form commonly in nature (as in the HR 8799 exoplanet system and the Galilean moons)\n\n3/ https://t.co/jCUPOzwD13', ""We built resonant chains with orbital period ratios of:\n- Consecutive Integers (1-6)\n- Consecutive prime numbers (1, 2, 3, 5, 7, 11)\n- Fibonacci Sequence (1, 1, 2, 3, 5, 8, 13)\n- Lazy Caterer's sequence (1, 2, 4, 7, 11, 16)\n\n4/\n\nhttps://t.co/3dkl0REYo3 https://t.co/x9zBYk1aHO"", ""Matt Clement built each of the resonant chains and tested their stability for the 10 Gyr main-sequence lifetime of a Sun-like star.  Dimitri Veras then tested the next 10 Gyrs of the systems during their stars' post-main sequence evolution\n\n5/ https://t.co/MjGoi2HTGx"", 'Outcome: all sequences were stable for the full 20 Gyr (longer than the age of the Universe) -- except for the consecutive integers.\n\n6/ https://t.co/gDaR5xTCew', ""An example: as the host star loses mass, the planets' orbits expand.  BUT they do so together, and maintain their relative orbital period ratios.\n\n(The planets needed to be at least several au from their stars to avoid engulfment during red giant phase)\n\n7/ https://t.co/IEztJvPgx4"", ""A SETI beacon could be constructed from consecutive primes, the Fibonacci or Lazy Caterers' sequences.\n\nThose chains of resonances (each including different higher-order resonances) are unlikely to form naturally (as far as we understand from simulations of their formation)\n\n8/"", 'You might wonder: are there any well-characterized resonant chains with non-natural sequences in our current catalog?\n\nTrappist-1 follows 2,3,4,6,9,15,24 from the outside-in, but that is pretty obscure...\n\n9/\nhttps://t.co/eDOKiW4vHz', 'How might aliens shape a planetary system to their liking?  Perhaps by repeated scattering events of small bodies as described in this paper by Korycansky, Laughlin &amp; Adams (for the case of Earth)\n\n10/\nhttps://t.co/ecemb8sx2U', 'An article about our idea we wrote for @NautilusMag \n\n11/\n\nhttps://t.co/1LSWULwf6Y', 'Finally, our paper was motivated by this blog post on ""Cosmic Signposts"" I wrote a few months ago.  \n\n12/\n\nhttps://t.co/Kk8LaspOTy', 'Wrap-up: a really fun project (and my first SETI publication).  Thanks to my collaborators Matt Clement, Dimitri Veras and @david_kipping !\n\nhttps://t.co/Ixyw5SNhjQ']",https://arxiv.org/abs/2204.14259,"How might an advanced alien civilization manipulate the orbits within a planetary system to create a durable signpost that communicates its existence? While it is still debated whether such a purposeful advertisement would be prudent and wise, we propose that mean-motion resonances between neighboring planets -- with orbital periods that form integer ratios -- could in principle be used to encode simple sequences that one would not expect to form in nature. In this Letter we build four multi-resonant planetary systems and test their long-term orbital stability. The four systems each contain 6 or 7 planets and consist of: (i) consecutive integers from 1 to 6; (ii) prime numbers from 2 to 11; (iii) the Fibonacci sequence from 1 to 13; and (iv) the Lazy Caterer sequence from 1 to 16. We built each system using N-body simulations with artificial migration forces. We evaluated the stability of each system over the full 10 Gyr integration of the Sun's main sequence phase. We then tested the stability of these systems for an additional 10 Gyr, during and after post-main sequence evolution of the central stars (assumed to be Sun-like) to their final, white dwarf phase. The only system that was destabilized was the consecutive integer sequence (system i). The other three sequences therefore represent potential SETI beacons. ","Mathematical encoding within multi-resonant planetary systems as SETI
  beacons"
6,1522828099414634499,314171681,Laura Baudis,['New paper on the arXiv: everything you always wanted to know about Gator - a high-purity germanium counting facility dedicated to low-background gamma-ray spectrometry for dark matter and neutrinoless double beta decay searches <LINK> <LINK>'],https://arxiv.org/abs/2204.12478,"We describe the upgrade and performance of the high-purity germanium counting facility Gator, which is dedicated to low-background $\gamma$-ray spectrometry. Gator is operated at the Gran Sasso Underground Laboratory in Italy, at an average depth of 3600 meter water equivalent, and employed for material screening and selection in ultra-low background, rare-event search experiments in astroparticle physics. The detector is equipped with a passive shield made of layers of copper, lead and polyethylene, and the sample cavity is purged with gaseous nitrogen maintained at positive pressure for radon suppression. After upgrading its enclosure, the background rate is (82.0$\pm$0.7) counts/(kg$\cdot$day) in the energy region 100 keV to 2700 keV, a 20% reduction compared to the previously reported rate. We show the stability of various operation parameters as a function of time. We also summarize the sample analysis procedure, and demonstrate Gator's sensitivity by examining one material sample, a candidate photosensor for the DARWIN experiment. ","The upgraded low-background germanium counting facility Gator for
  high-sensitivity $\gamma$-ray spectrometry"
7,1522623411754835970,1235148346618261508,Taha Yassine,['Our new paper is accepted to SPAWC 2022. Check it out and learn about channel charting using deep neural networks.\n\n <LINK>\n\n#6G #MachineLearning #DeepLearning'],https://arxiv.org/abs/2204.13996,"Channel charting is an unsupervised learning method that aims at mapping wireless channels to a so-called chart, preserving as much as possible spatial neighborhoods. In this paper, a model-based deep learning approach to this problem is proposed. It builds on a physically motivated distance measure to structure and initialize a neural network that is subsequently trained using a triplet loss function. The proposed structure exhibits a low number of parameters and clever initialization leads to fast training. These two features make the proposed approach amenable to on-the-fly channel charting. The method is empirically assessed on realistic synthetic channels, yielding encouraging results. ","Leveraging triplet loss and nonlinear dimensionality reduction for
  on-the-fly channel charting"
8,1521759842767196160,1206030745,Luc Le Magoarou,['Can physical models be used to structure and initialize neural networks performing wireless channel charting?\n\nThis question is investigated in our new paper accepted to SPAWC 2022 (special session on wireless channel charting and‚Ä¶<LINK> <LINK>'],https://arxiv.org/abs/2204.13996,"Channel charting is an unsupervised learning method that aims at mapping wireless channels to a so-called chart, preserving as much as possible spatial neighborhoods. In this paper, a model-based deep learning approach to this problem is proposed. It builds on a physically motivated distance measure to structure and initialize a neural network that is subsequently trained using a triplet loss function. The proposed structure exhibits a low number of parameters and clever initialization leads to fast training. These two features make the proposed approach amenable to on-the-fly channel charting. The method is empirically assessed on realistic synthetic channels, yielding encouraging results. ","Leveraging triplet loss and nonlinear dimensionality reduction for
  on-the-fly channel charting"
9,1521541543676444672,232287209,Dallas Card,"[""In a new paper with Junshen Chen (@cab1n_) and Dan @Jurafsky (to appear in Findings of ACL), we focus on domain adaptation for off-the-shelf models (e.g., lexicons or cloud APIs), which often don't provide full access to training data or model parameters. <LINK>"", 'We suggest that even when data cannot be shared, model producers should try to facilitate adaptation by model consumers. How can this be done? We present two lightweight techniques for this purpose, both of which can be used by model consumers without additional training.', 'In brief, model producers can incorporate domain specific biases and/or domain specific feature normalization during training, with parameters which can then be easily replaced by model consumers when applying models in a new domain.', 'On four multi-domain text classification datasets, we find significant improvements in out-of-domain accuracy when used in combination with either linear or contextual embedding models. Experiments also estimate the typical drop in accuracy of base models across domains (3-10%). https://t.co/JRbT1UIrnn', 'Fine-tuning CE models to the new domain is still best where possible, but requires model access and sufficient computational resources (GPUs). Especially for CSS practitioners employing off-the-shelf models, lightweight adaptation may be preferable, and should be encouraged.', 'As an aside, we also compare against several sentiment lexicons, and find they are no better than a basic bag-of-words logistic regression model (trained on a handful of sentiment analysis datasets) in terms of out-of-domain accuracy, when adapting models to the new domain. https://t.co/fWh7gTFdqH', 'Replication code is available here: https://t.co/BTuemReUhW', 'And more details in this blog post: https://t.co/3FmmenRlfD']",https://arxiv.org/abs/2204.14213,"Off-the-shelf models are widely used by computational social science researchers to measure properties of text, such as sentiment. However, without access to source data it is difficult to account for domain shift, which represents a threat to validity. Here, we treat domain adaptation as a modular process that involves separate model producers and model consumers, and show how they can independently cooperate to facilitate more accurate measurements of text. We introduce two lightweight techniques for this scenario, and demonstrate that they reliably increase out-of-domain accuracy on four multi-domain text classification datasets when used with linear and contextual embedding models. We conclude with recommendations for model producers and consumers, and release models and replication code to accompany this paper. ",Modular Domain Adaptation
10,1521484608466006018,159526922,Christophe,"['New paper with a new message flow analysis for ROS 2!\nMessage Flow Analysis with Complex Causal Links for Distributed ROS 2 Systems\nwith @PYLajoie, @jumpjoe78, and Michel Dagenais\n\nPaper: <LINK>\nCode: <LINK>\n#goROS #ROS2 @OpenRoboticsOrg @rosorg']",https://arxiv.org/abs/2204.10208,"Distributed robotic systems rely heavily on publish-subscribe frameworks such as the Robot Operating System (ROS) to efficiently implement modular computation graphs. The ROS 2 executor, a high-level task scheduler which handles ROS 2 messages, is a performance bottleneck. We extend ros2_tracing, a framework with instrumentation and tools for real-time tracing of ROS 2, with the analysis and visualization of the flow of messages across distributed ROS 2 systems. Our method detects one-to-many and many-to-many causal links between input and output messages, including indirect causal links through simple user-level annotations. We validate our method on both synthetic and real robotic systems, and demonstrate its low runtime overhead. Moreover, the underlying intermediate execution representation database can be further leveraged to extract additional metrics and high-level results. This can provide valuable timing and scheduling information to further study and improve the ROS 2 executor as well as optimize any ROS 2 system. The source code is available at: this https URL ","Message Flow Analysis with Complex Causal Links for Distributed ROS 2
  Systems"
11,1521291232080760833,278791721,Maria De-Arteaga,"['üì£ New #facct2022 paper ‚ÄúJustice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms‚Äù.\nLed by 1st year @UTexasMcCombs PhD student @TerrenceNeumann (say hi to him in Seoul!), and joint with @sinafazelpur üßµ\n<LINK> <LINK>', 'As misinformation detection pipelines increasingly incorporate algorithms, justice of these systems become a central concern. Misinformation detection pipelines consist of varied tasks, each of which can give rise to many ethical concerns, and involve multiple stakeholders. 2/5 https://t.co/ZkeacaP5tS', 'Commonly, algorithmic fairness considers cases where each data instance pertains a single direct stakeholder that occupies one role: decision subject. In the case of informational items, in contrast, multiple stakeholders are directly implicated in each informational item. 3/5 https://t.co/NlgHksCQFa', 'We employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to misinformation detection systems, considering representation, participation, allocation, and credibility affecting different stakeholders. 4/5', 'Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. 5/5']",https://arxiv.org/abs/2204.13568,"Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain. ","Justice in Misinformation Detection Systems: An Analysis of Algorithms,
  Stakeholders, and Potential Harms"
12,1521259672782704640,1487935233215315974,Guy Emerson,"['What are truth conditions good for? Learning grounded semantics! New ACL paper with @YinhongLiu2: <LINK>\n\nTraining on the @VisualGenome dataset, and evaluating on four semantic datasets, we find that a truth-conditional model gives better performance. #acl2022nlp', ""On a more personal level, this is the first paper where I've supervised work that directly builds on my PhD thesis (as opposed to supervising work on a different topic).  Feels like quite a milestone!"", 'Also, shoutout to @ah__cl, @davidschlangen, and @anitaveroe, for exciting work in this space which is complementary to ours.', ""If you have questions, feel free to ask... but please be patient for my response, as I'm currently on parental leave!""]",https://arxiv.org/abs/2204.10624,"Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability. It models the meaning of a word as a binary classifier rather than a numerical vector. In this work, we propose a method to train a Functional Distributional Semantics model with grounded visual data. We train it on the Visual Genome dataset, which is closer to the kind of data encountered in human language acquisition than a large text corpus. On four external evaluation datasets, our model outperforms previous work on learning semantics from Visual Genome. ",Learning Functional Distributional Semantics with Visual Data
13,1521228025500737536,960527787428900864,Michael Albergo,"['Very excited to share a new paper with Peter Lunts and Michael Lindsey!\nWe worked to push HMC into state-of-the-art large lattices in condensed matter to study the quantum critical phenomena of the spin-fermion model: <LINK> <LINK>', 'Understanding quantum criticality in metals is an important problem very relevant to high-temperature superconductivity. https://t.co/mB9fWgJEtG', 'This is one of the hardest problems in theoretical condensed matter physics due to (a) a Fermi surface, (b) strong coupling of the fermions and collective bosonic modes, (c) diverging correlation length, and (d) lack of any small control parameter.', 'We developed a suite of HMC tuning tricks that enables us to study the universal scaling regime by simulating unprecedentedly large system sizes at a low enough temperature. All the way to V =  80 x 80 x 200! https://t.co/JwMPZvwoPy', 'We found a form of the critical susceptibility that (a) violates the so-far numerically observed Hertz-Millis form and (b) shows strong evidence of corroborating the fixed point found in an exactly solvable limit. https://t.co/RhpILbw4uf https://t.co/6WmJpOAOyZ', 'We are pretty psyched about the method. It can be applied to other models of quantum criticality, as well as electron-phonon models.', 'Big shout outs to Peter and Michael for teaching me a bunch of physics along the way! And to @KyleCranmer for guidance along the way, as well as Phiala Shanahan for HMC insights :)']",https://arxiv.org/abs/2204.14241,"We numerically study the $\mathrm{O}(3)$ spin-fermion model, a minimal model of the onset of antiferromagnetic spin-density wave (SDW) order in a two-dimensional metal. We employ a Hybrid Monte Carlo (HMC) algorithm with a novel auto-tuning procedure, which learns the optimal HMC hyperparameters in an initial warmup phase. This allows us to study unprecedentedly large systems, even at criticality. At the quantum critical point, we find a critical scaling of the dynamical spin susceptibility $\chi(\omega, \vec q)$ that strongly violates the Hertz-Millis form, which is the first demonstrated instance of such a phenomenon in this model. The form that we do observe provides strong evidence that the universal scaling is actually governed by the fixed point near perfect hot-spot nesting of Schlief, Lunts, and Lee [Phys. Rev. X 7, 021010 (2017)], even away from perfect nesting. Our work provides a concrete link between controlled calculations of SDW metallic criticality in the long-wavelength and small nesting angle limits and a microscopic finite-size model at realistic appreciable values of the nesting angle. Additionally, the HMC method we introduce is generic and can be used to study other fermionic models of quantum criticality, where there is a strong need to simulate large systems. ","Non-Hertz-Millis scaling of the antiferromagnetic quantum critical metal
  via scalable Hybrid Monte Carlo"
14,1521171306607087616,796539777130569728,Georgios Valogiannis,"['New paper out, with @CoraDvorkin ! We perform the first application of the Wavelet Scattering Transform on galaxy data (BOSS CMASS), finding great improvement in the values of the predicted 1œÉ errors compared to the regular power spectrum @harvardphysics \n<LINK>', '@CoraDvorkin @harvardphysics @iaifi_news']",https://arxiv.org/abs/2204.13717,"We perform the first application of the wavelet scattering transform (WST) on actual galaxy observations, through a WST analysis of the BOSS DR12 CMASS dataset. We lay out the detailed procedure on how to capture all necessary layers of realism for an application on data obtained from a spectroscopic survey, including the effects of redshift-space anisotropy, non-trivial survey geometry, the shortcomings of the dataset through a set of systematic weights and the Alcock-Paczynski distortion effect. In order to capture the cosmological dependence of the WST, we use galaxy mocks obtained from the state-of-the-art ABACUSSUMMIT simulations, tuned to match the anisotropic correlation function of the BOSS CMASS sample in the redshift range $0.46<z<0.60$. Using our theory model for the WST coefficients, as well as for the first 2 multipoles of the galaxy power spectrum, that we use as reference, we perform a likelihood analysis of the CMASS data and obtain the posterior probability distributions of 4 cosmological parameters, $\{\omega_b,\omega_c,n_s,\sigma_8\}$, as well as the Hubble constant, derived from a fixed value of the angular size of the sound horizon at last scattering measured by the Planck satellite, all of which are marginalized over the 7 nuisance parameters of the Halo Occupation Distribution model. The WST is found to deliver a substantial improvement in the values of the predicted $1\sigma$ errors compared to the regular power spectrum, which are tighter by a factor in the range $3-6$ in the case of flat and uninformative priors and by a factor of $4-28$, when a Big Bang Nucleosynthesis prior is applied on the value of $\omega_b$. Furthermore, in the latter case, we obtain a 0.6% measurement of the Hubble constant. Our results are investigative and subject to certain approximations in our analysis, that we discuss in the text. ","Going Beyond the Galaxy Power Spectrum: an Analysis of BOSS Data with
  Wavelet Scattering Transforms"
15,1521153089880870912,1465098219847831562,Pablo Villanueva Domingo,"['Happy to announce a new paper with @paco_astro !\n\nWe apply deep learning methods to infer clustering and cosmological parameters on galaxy catalogues from CAMELS simulations @camels_project. (1/4)\n\n<LINK> <LINK>', 'We use graph neural networks to perform likelihood-free inference at the galaxy-field level, accounting for baryonic uncertainties. These models can constrain the value of Omega_m with a 4%-13% accuracy using only ~1000 galaxies in a (25 Mpc/h)^3 volume. (2/4) https://t.co/AloIJJGb23', 'Our models can also be trained to predict the power spectrum of a galaxy catalogue with a 3% accuracy. This illustrates how graph neural networks can learn clustering information. (3/4) https://t.co/9qEh7B2x95', 'Graph neural networks become an ideal tool for these tasks, since they are specially suited to deal with sparse and irregular data, like galaxy catalogues, and they are not limited by a minimum scale.\n\nSee more info in the paper https://t.co/Vkmdg8ljal ! (4/4) https://t.co/Jz5Y0H1HYj']",https://arxiv.org/abs/2204.13713,"We train deep learning models on thousands of galaxy catalogues from the state-of-the-art hydrodynamic simulations of the CAMELS project to perform regression and inference. We employ Graph Neural Networks (GNNs), architectures designed to work with irregular and sparse data, like the distribution of galaxies in the Universe. We first show that GNNs can learn to compute the power spectrum of galaxy catalogues with a few percent accuracy. We then train GNNs to perform likelihood-free inference at the galaxy-field level. Our models are able to infer the value of $\Omega_{\rm m}$ with a $\sim12\%-13\%$ accuracy just from the positions of $\sim1000$ galaxies in a volume of $(25~h^{-1}{\rm Mpc})^3$ at $z=0$ while accounting for astrophysical uncertainties as modelled in CAMELS. Incorporating information from galaxy properties, such as stellar mass, stellar metallicity, and stellar radius, increases the accuracy to $4\%-8\%$. Our models are built to be translational and rotational invariant, and they can extract information from any scale larger than the minimum distance between two galaxies. However, our models are not completely robust: testing on simulations run with a different subgrid physics than the ones used for training does not yield as accurate results. ",Learning cosmology and clustering with cosmic graphs
16,1521137661104345088,328430286,Jad C. Halimeh,"['New paper <LINK> (2nd of 2 today). In a work led by Ana Hudomal of @theoryleeds, we study analytically and numerically the driven PXP model, and map out two distinct classes of optimal driving. \n@JDesaules\n@QManyBody\n@MCQST_cluster\n@ERC_Research\n@LeverhulmeTrust <LINK>']",https://arxiv.org/abs/2204.13718,"Periodic driving has been established as a powerful technique for engineering novel phases of matter and intrinsically out-of-equilibrium phenomena such as time crystals. Recent work by Bluvstein et al. [Science 371, 1355 (2021)] has demonstrated that periodic driving can also lead to a significant enhancement of quantum many-body scarring, whereby certain non-integrable systems can display persistent quantum revivals from special initial states. Nevertheless, the mechanisms behind driving-induced scar enhancement remain poorly understood. Here we report a detailed study of the effect of periodic driving on the PXP model describing Rydberg atoms in the presence of a strong Rydberg blockade - the canonical static model of quantum many-body scarring. We show that periodic modulation of the chemical potential gives rise to a rich phase diagram, with at least two distinct types of scarring regimes that we distinguish by examining their Floquet spectra. We formulate a toy model, based on a sequence of square pulses, that accurately captures the details of the scarred dynamics and allows for analytical treatment in the large-amplitude and high-frequency driving regimes. Finally, we point out that driving with a spatially inhomogeneous chemical potential allows to stabilize quantum revivals from arbitrary initial states in the PXP model, via a mechanism similar to prethermalization. ",Driving quantum many-body scars
17,1521118399748882435,143323383,Abe Handler,"['Excited to announce a new paper at the intersection of NLP and HCI, with @nargesmahyar and @brendan642 forthcoming in ACM TiiS <LINK>  \nExplaining our work on the ClioQuery text analytics system takes a few chars, so here‚Äôs a üßµ 1/13', 'Text analytics is a well-studied problem (see https://t.co/6bYqKzUuHB). Researchers have proposed many tools to help analysts find events, clusters and hierarchies in big collections of documents. 2/13', 'But when we set out to build an analytics system for historians, we found little evidence that historians actually review clusters or hierarchies in their work. Instead, we learned that historians often investigate specific query words in newspaper archives. 3/13', 'We designed the ClioQuery system around this specific need and this specific practice. 4/13', 'ClioQuery applies text simplification techniques from NLP to help historians quickly and comprehensively gather and analyze all occurrences of a query word across an archive. 5/13', ""It presents all occurrences of a user's query word across a corpus, in a rich visual format designed for skimming. It also pairs this visual summary with more traditional features like linked views and in-text highlighting, to help engender trust in summarization techniques. 6/13"", 'We evaluated ClioQuery through both (A) qualitative interviews with academic historians/librarians, and (B) quantitative tests of human performance based on real historical tasks. 7/13', 'We learned a lot from designing, building and evaluating the tool. Here are some highlights ‚Ä¶ 8/13', 'Early design ideas did not work. Like at all. The historians we spoke with were very suspicious of text summarization. ‚ÄúI need to know what is included and why,‚Äù one said. ‚ÄúI am wary of algorithms that choose for me what the important facts are.‚Äù (See Fig 13 and 14). 9/13', 'Through prototyping, we eventually found approaches that historians liked more. These designs emphasized transparency and interpretability in summarization, suggesting a role for further NLP research into more transparent summarization techniques. 10/13', 'On the HCI and VIS side, ClioQuery suggests new features and directions for interactive text analysis. Our work introduces new text analysis techniques for summarizing a query across a corpus 11/13', 'Our findings also reinforce prior studies from @nicole_sultanum et al. (https://t.co/tm3G7orlxG), suggesting the importance of showing actual text in text visualization. (They worked with doctors; we worked with historians.) 12/13', 'On a personal note, I learned a lot while working on this paper (e.g., what is a design study?), and have spoken to many people about the project over the past few years. I am excited to share our findings.  13/13']",https://arxiv.org/abs/2204.04694,"Historians and archivists often find and analyze the occurrences of query words in newspaper archives, to help answer fundamental questions about society. But much work in text analytics focuses on helping people investigate other textual units, such as events, clusters, ranked documents, entity relationships, or thematic hierarchies. Informed by a study into the needs of historians and archivists, we thus propose ClioQuery, a text analytics system uniquely organized around the analysis of query words in context. ClioQuery applies text simplification techniques from natural language processing to help historians quickly and comprehensively gather and analyze all occurrences of a query word across an archive. It also pairs these new NLP methods with more traditional features like linked views and in-text highlighting to help engender trust in summarization techniques. We evaluate ClioQuery with two separate user studies, in which historians explain how ClioQuery's novel text simplification features can help facilitate historical research. We also evaluate with a separate quantitative comparison study, which shows that ClioQuery helps crowdworkers find and remember historical information. Such results suggest possible new directions for text analytics in other query-oriented settings. ","ClioQuery: Interactive Query-Oriented Text Analytics for Comprehensive
  Investigation of Historical News Archives"
18,1521107086272303105,328430286,Jad C. Halimeh,"['New paper <LINK> (1 out of 2 today). We review our theoretical and experimental works on the stabilization of gauge theories in quantum simulators, along with the intriguing salient features they exhibit.\n@HaukeGroup  \n@QManyBody \n@MCQST_cluster \n@ERC_Research <LINK>']",https://arxiv.org/abs/2204.13709,"Quantum simulation is at the heart of the ongoing ""second"" quantum revolution, with various synthetic quantum matter platforms realizing evermore exotic condensed matter and particle physics phenomena at high levels of precision and control. The implementation of gauge theories on modern quantum simulators is especially appealing due to three main reasons: (i) it offers a new probe of high-energy physics on low-energy tabletop devices, (ii) it allows exploring condensed matter phenomena that are prominent in gauge theories even without a direct connection to high-energy physics, and (iii) it serves as a banner of experimental benchmarking given the plethora of local constraints arising from the gauge symmetry that need to be programmed and controlled. In order to faithfully model gauge-theory phenomena on a quantum simulator, stabilizing the underlying gauge symmetry is essential. In this brief review, we outline recently developed experimentally feasible methods introduced by us that have shown, in numerical and experimental benchmarks, reliable stabilization of quantum-simulator implementations of gauge theories. We explain the mechanism behind these \textit{linear gauge protection} schemes, and illustrate their power in protecting salient features such as gauge invariance, disorder-free localization, quantum many-body scars, and other phenomena of topical interest. We then discuss their application in experiments based on Rydberg atoms, superconducting qubits, and in particular ultracold neutral atoms in optical superlattices. We hope this review will illustrate some facets of the exciting progress in stabilization of gauge symmetry and in gauge-theory quantum simulation in general. ",Stabilizing Gauge Theories in Quantum Simulators: A Brief Review
19,1521055445024223236,3403213937,Paul Molli√®re,"['Hi all! Here a thread on my new paper (<LINK>). This project was born when we derived atmospheric C/Os of directly imaged planets with GRAVITY. What does that tell us about formation? Can we constrain formation locations, or modes?', ""One of our conclusions is that planet formation is so complex that it may be difficult to invert the process for a given planet, based on its composition. E.g., The plot below shows HR 8799e's inferred formation location when adding chemical evolution to the √ñberg+11 disk model. https://t.co/uD7tAnorqs"", ""What is more, quite a lot of different planet formation assumptions lead to basically identical compositional outcomes. As an example, we study the √ñberg disk, adding chemical evolution, or adding pebbles. They all do a good job at fitting the planets' C/O and [Fe/H], mostly. https://t.co/9TLDqndAl8"", 'What does this mean for the future? The power of atmospheric compositions lies in the trends that may emerge when analyzing the atmospheres of a large planet population. High-res spectra, JWST, and Ariel in the future will certainly help. Look at the wealth of visible species: https://t.co/pAcGXBPQVL', 'If many planets have a very high metallicity, this could be difficult to reproduce with pebbles. But high C/O and intermediate enrichment may point toward pebbles being important. The refractory content of an atmosphere could distinguish between planetesimals and pebbles, etc...', 'In summary: the more absorber species we probe for the planet population (carrying carbon, oxygen, nitrogen, refractory species), the more we will be able to say about the broad strokes of planet formation. Truly inverting formation for a single planet may stay a challenge!']",https://arxiv.org/abs/2204.13714,"Constraining planet formation based on the atmospheric composition of exoplanets is a fundamental goal of the exoplanet community. Existing studies commonly try to constrain atmospheric abundances, or to analyze what abundance patterns a given description of planet formation predicts. However, there is also a pressing need to develop methodologies that investigate how to transform atmospheric compositions into planetary formation inferences. In this study we summarize the complexities and uncertainties of state-of-the-art planet formation models and how they influence planetary atmospheric compositions. We introduce a methodology that explores the effect of different formation model assumptions when interpreting atmospheric compositions. We apply this framework to the directly imaged planet HR 8799e. Based on its atmospheric composition, this planet may have migrated significantly during its formation. We show that including the chemical evolution of the protoplanetary disk leads to a reduced need for migration. Moreover, we find that pebble accretion can reproduce the planet's composition, but some of our tested setups lead to too low atmospheric metallicities, even when considering that evaporating pebbles may enrich the disk gas. We conclude that the definitive inversion from atmospheric abundances to planet formation for a given planet may be challenging, but a qualitative understanding of the effects of different formation models is possible, opening up pathways for new investigations. ","Interpreting the atmospheric composition of exoplanets: sensitivity to
  planet formation assumptions"
20,1521043648556720130,1430890835902562313,Silvia Galli,"['New paper led by brilliant PhD student Etienne Camphuis! We studied the accuracy of analytical CMB covariance matrices in the case of observations of a small patch of the sky, like the one from the @SPTelescope.  #neucosmos @ERC_Research  <LINK> üßµ 1/', '1) For the first time (afaik) we found  tricks to calculate the matrices exactly (computationally expensive) 2) We used this exact calculation to check the accuracy of existing fast approximations. They are accurate at the ~5% level. 2/', '3) We then found a new approximation which is 4 times more accurate! 4) Finally, we worked out the details to calculate the covariance matrix for the specific case of the Polspice power spectrum estimator. 3/', 'All the original ideas of the paper come from Etienne (tricks for exact calculation, new approximation, Polspice adaptation) and great work was done by all our collaborators Karim Benabed, Eric Hivon and Marc Lilley! 4/', 'This work allows us to have a reliable reference for the covariance matrices we will build for the next release of the SPT-3G data. 5/']",https://arxiv.org/abs/2204.13721,"Accurate covariance matrices are required for a reliable estimation of cosmological parameters from pseudo-power spectrum estimators. In this work, we focus on the analytical calculation of covariance matrices. We consider the case of observations of the Cosmic Microwave Background in temperature and polarization on a small footprint such as in the SPT-3G experiment, which observes 4% of the sky. Power spectra evaluated on small footprints are expected to have large correlations between modes, and these need to be accurately modelled. We present, for the first time, an algorithm that allows an efficient (but computationally expensive) exact calculation of analytic covariance matrices. Using it as our reference, we test the accuracy of existing fast approximations of the covariance matrix. We find that, when the power spectrum is binned in wide bandpowers, current approaches are correct up to the 5% level on the SPT-3G small sky footprint. Furthermore, we propose a new approximation which improves over the previous ones reaching a precision of 1% in the wide bandpowers case and generally more than 4 times more accurate than current approaches. Finally, we derive the covariance matrices for mask-corrected power spectra estimated by the PolSpice code. In particular, we include, in the case of a small sky fraction, the effect of the apodization of the large scale modes. While we considered the specific case of the CMB, our results are applicable to any other cosmological probe which requires the calculation of pseudo-power spectrum covariance matrices. ",Accurate CMB covariance matrices: exact calculation and approximations
21,1521013918700912640,477030336,Leon Derczynski üè°üå±,"[""New paper! üìù\n\nText data can contain harmful things, but unlike other disciplines, we don't have protocols for handling or presenting harmful materialüõ¢Ô∏è. This paper with @hannahrosekirk @Abebab @bertievidgen presents the risks &amp; some solutions. ü•ºü•Ω\n\n<LINK>""]",https://arxiv.org/abs/2204.14256,"Textual data can pose a risk of serious harm. These harms can be categorised along three axes: (1) the harm type (e.g. misinformation, hate speech or racial stereotypes) (2) whether it is \textit{elicited} as a feature of the research design from directly studying harmful content (e.g. training a hate speech classifier or auditing unfiltered large-scale datasets) versus \textit{spuriously} invoked from working on unrelated problems (e.g. language generation or part of speech tagging) but with datasets that nonetheless contain harmful content, and (3) who it affects, from the humans (mis)represented in the data to those handling or labelling the data to readers and reviewers of publications produced from the data. It is an unsolved problem in NLP as to how textual harms should be handled, presented, and discussed; but, stopping work on content which poses a risk of harm is untenable. Accordingly, we provide practical advice and introduce \textsc{HarmCheck}, a resource for reflecting on research into textual harms. We hope our work encourages ethical, responsible, and respectful research in the NLP community. ",Handling and Presenting Harmful Text
22,1520807696487919617,395617787,Lauritz Thamsen,"['New work on making high-priority real-time tasks on IoT devices less susceptible to packet floods. @BehnkeIlja will present our full paper on May 18 at the 25th International Symposium on Real-Time Distributed Computing, <LINK>.\n\nPreprint: <LINK> <LINK>']",https://arxiv.org/abs/2204.08846,"When IP-packet processing is unconditionally carried out on behalf of an operating system kernel thread, processing systems can experience overload in high incoming traffic scenarios. This is especially worrying for embedded real-time devices controlling their physical environment in industrial IoT scenarios and automotive systems. We propose an embedded real-time aware IP stack adaption with an early demultiplexing scheme for incoming packets and subsequent per-flow aperiodic scheduling. By instrumenting existing embedded IP stacks, rigid prioritization with minimal latency is deployed without the need of further task resources. Simple mitigation techniques can be applied to individual flows, causing hardly measurable overhead while at the same time protecting the system from overload conditions. Our IP stack adaption is able to reduce the low-priority packet processing time by over 86% compared to an unmodified stack. The network subsystem can thereby remain active at a 7x higher general traffic load before disabling the receive IRQ as a last resort to assure deadlines. ","Differentiating Network Flows for Priority-Aware Scheduling of Incoming
  Packets in Real-Time IoT Systems"
23,1520696037492539392,4883662141,Roy Schwartz,"['Spurious correlations are a major problem in NLP. One way to address their shortcomings is to eliminate them altogether via balancing or filtering. Is this the right way to go? \nOur new #NAACL2022 Findings paper suggests otherwise\n<LINK> w/ @GabiStanovsky\n1/n <LINK>', 'We want to eliminate the side effect of spurious correlations. A possible solution is balance-out every spurious feature, either by augmenting the data with new instances or by filtering it (e.g., using adversarial filtering https://t.co/BVKrWhIPo5)\n2/n', 'But what are spurious correlations? Various definitions have been used in the field. A recent proposal by Gardner et al. (https://t.co/7LSupziCOX) suggests that every single word feature is spurious, as its meaning can appear in different contexts, and thus mislead a model\n3/n', 'But the same principle can be applied beyond unigrams; this dataset is balanced for unigrams but not for bigrams, and relying on the latter (e.g., ‚Äúvery good‚Äù) will make the model not generalize to certain contexts (e.g., ‚Äúnot very good‚Äù)\nüëâ balancing to little is not enough\n4/n https://t.co/lwnoMIexY8', 'Similarly, if we balance out every combination of features, we would be left with a dataset that contains no information. So balancing too much might prevent us from learning anything.\n5/n', 'So balancing is not really practical. But is it even desired? We argue that the idea of balancing is meant to prevent us from learning certain knowledge (e.g., that the word ""great"" is correlated with positive sentiment)\n6/n', 'But much of semantic information, e.g., world knowledge (Joe Biden is the US president) and common sense knowledge (people are happy when they receive a gift) is context and time dependent, and thus could be considered spurious. Do we want to eliminate this type of knowledge?\n7/n https://t.co/ngYqUokmJo', 'We conclude that dataset balancing is not the right way forward for mitigating spurious correlations\n\nSo what can we do instead? We promote 3 different alternatives for mitigating these side effects. None of them is new, but all seem more promising that dataset balancing\n8/n https://t.co/s0dMiLQrug', 'First, we can augment our data with richer contexts, such as negation, metaphors, humor, etc. Instead of unlearning that ‚Äúgreat‚Äù is correlated with positive sentiment, we should teach our model that ‚Äúnot great‚Äù is correlated with negative sentiment\n9/n', 'We could also build models that abstain or interact in cases of uncertainty\nInstead of always providing some output, which might reward models that rely on spurious correlations (as they have little to lose in cases of uncertainty), let‚Äôs build models that abstain/interact\n10/n https://t.co/SpumcneXjJ', 'Finally, perhaps we should consider moving from large-scale fine-tuning to few- and zero-shot settings. This would allow models to not be exposed to some of these correlations to begin with (though obviously pre-training can also include such correlations)\n11/n', 'Many more details are in the paper!\n#NAACL2022 Findings\nhttps://t.co/3sgR1drnU6 \nJoint work w/ @GabiStanovsky\n12/12']",http://arxiv.org/abs/2204.12708,"Recent work has shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization. To mitigate this problem, a common practice is to balance datasets by adding new instances or by filtering out ""easy"" instances (Sakaguchi et al., 2020), culminating in a recent proposal to eliminate single-word correlations altogether (Gardner et al., 2021). In this opinion paper, we identify that despite these efforts, increasingly-powerful models keep exploiting ever-smaller spurious correlations, and as a result even balancing all single-word features is insufficient for mitigating all of these correlations. In parallel, a truly balanced dataset may be bound to ""throw the baby out with the bathwater"" and miss important signal encoding common sense and world knowledge. We highlight several alternatives to dataset balancing, focusing on enhancing datasets with richer contexts, allowing models to abstain and interact with users, and turning from large-scale fine-tuning to zero- or few-shot setups. ","On the Limitations of Dataset Balancing: The Lost Battle Against
  Spurious Correlations"
24,1520435076194082816,947751551258583040,Yoav Levine,"['1/2 \nNew paper: ""Sub-Task Decomposition Enables Learning in Seq2seq Tasks"". We prove that unlearnable composite problems can be learned if the network is trained to first predict intermediate steps towards the solution, and only then the solution itself <LINK>', '2/2\nOur paper theoretically motivates trending approaches such as chain of thought prompting, scratchpads, self thought reasoners (and more), which tackle compounded problems in natural language via introducing intermediate supervision in a seq2seq manner.@noamwies @AmnonShashua']",https://arxiv.org/abs/2204.02892,"The field of Natural Language Processing (NLP) has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models (LMs). Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. A known mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks' labels are simply concatenated to the original input. In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, an unlearnable composite problem becomes learnable. We prove this for the notoriously unlearnable composite task of bit-subset parity, with the intermediate supervision being parity results of increasingly large bit-subsets. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing a case where learning is facilitated in the presence of intermediate supervision. ",Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks
25,1520246158664249344,990433714948661250,Sergey Levine,"['Representation learning via goal-conditioned bisim can enable ""analogies"" between different scenes. In our new work, we explore how to learn representations that provide analogies, then apply it to goal-conditioned RL. See @yayitsamyzhang\'s thread:\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2204.13060,"Building generalizable goal-conditioned agents from rich observations is a key to reinforcement learning (RL) solving real world problems. Traditionally in goal-conditioned RL, an agent is provided with the exact goal they intend to reach. However, it is often not realistic to know the configuration of the goal before performing a task. A more scalable framework would allow us to provide the agent with an example of an analogous task, and have the agent then infer what the goal should be for its current state. We propose a new form of state abstraction called goal-conditioned bisimulation that captures functional equivariance, allowing for the reuse of skills to achieve new goals. We learn this representation using a metric form of this abstraction, and show its ability to generalize to new goals in simulation manipulation tasks. Further, we prove that this learned representation is sufficient not only for goal conditioned tasks, but is amenable to any downstream task described by a state-only reward function. Videos can be found at this https URL ",Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning
26,1520105865365270529,789998140782882817,Amy Zhang,"['Building generalizable goal-conditioned agents from rich observations is a key to solving real world problems. We propose a new form of state abstraction, goal-conditioned bisimulation, that captures functional equivariance. 1/n\nPaper: <LINK> <LINK>', 'This allows for skill reuse to achieve new goals. Prior work has proposed representation learning objectives that induce invariance to irrelevant factors through the use of domain knowledge about the relevant state features for a task. 2/n', 'In goal-conditioned problems, which features are relevant depends on the goal. While there may be uncontrollable aspects of an environment that can always be ignored, knowledge of the task and current state is necessary to determine what is relevant to complete the task. 3/n', 'Rather than construct representations that ignore features that are never relevant, our goal is to construct functionally equivariant representations that only capture changes between state-goal pairs. 4/n https://t.co/quTNGLBZpw', 'If we can acquire representations that have these properties, then we can similarly specify goals for goal-conditioned policies in one setting, and have them carry out those goals in many diverse settings. 5/n', 'As shown in the gif above, this frees us from needing an exact goal image for achieving the same functional task in a new setting. Joint work with Philippe Hansen-Estruch, @ashvinair, Patrick Yin, and @svlevine.']",https://arxiv.org/abs/2204.13060,"Building generalizable goal-conditioned agents from rich observations is a key to reinforcement learning (RL) solving real world problems. Traditionally in goal-conditioned RL, an agent is provided with the exact goal they intend to reach. However, it is often not realistic to know the configuration of the goal before performing a task. A more scalable framework would allow us to provide the agent with an example of an analogous task, and have the agent then infer what the goal should be for its current state. We propose a new form of state abstraction called goal-conditioned bisimulation that captures functional equivariance, allowing for the reuse of skills to achieve new goals. We learn this representation using a metric form of this abstraction, and show its ability to generalize to new goals in simulation manipulation tasks. Further, we prove that this learned representation is sufficient not only for goal conditioned tasks, but is amenable to any downstream task described by a state-only reward function. Videos can be found at this https URL ",Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning
27,1520096390554144768,2289040153,Rina Friedberg,"['New arXiv paper from me, Stuart Ambler, and Guillaume Saint-Jacques on monitoring large-scale experimentation systems for impact on group representation! <LINK>', 'This paper grew out of the goal to proactively monitor A/B tests at LinkedIn for unintended consequences. We introduce statistics to quantify representation impact, give a hypothesis testing framework, and discuss implementation for hundreds of experiments per day.', 'On a related note, I would like to formally apologize to everyone who knew me while I was taking asymptotic statistics and complaining about how many delta method exercises we were assigned; turns out, the delta method is very useful, and I was very wrong.']",https://arxiv.org/abs/2204.12011,"As companies adopt increasingly experimentation-driven cultures, it is crucial to develop methods for understanding any potential unintended consequences of those experiments. We might have specific questions about those consequences (did a change increase or decrease gender representation equality among content creators?); we might also wonder whether if we have not yet considered the right question (that is, we don't know what we don't know). Hence we address the problem of unintended consequences in experimentation from two perspectives: namely, pre-specified vs. data-driven selection, of dimensions of interest. For a specified dimension, we introduce a statistic to measure deviation from equal representation (DER statistic), give its asymptotic distribution, and evaluate finite-sample performance. We explain how to use this statistic to search across large-scale experimentation systems to alert us to any extreme unintended consequences on group representation. We complement this methodology by discussing a search for heterogeneous treatment effects along a set of dimensions with causal trees, modified slightly for practicalities in our ecosystem, and used here as a way to dive deeper into experiments flagged by the DER statistic alerts. We introduce a method for simulating data that closely mimics observed data at LinkedIn, and evaluate the performance of DER statistics in simulations. Last, we give a case study from LinkedIn, and show how these methodologies empowered us to discover surprising and important insights about group representation. Code for replication is available in an appendix. ","Representation-Aware Experimentation: Group Inequality Analysis for A/B
  Testing and Alerting"
28,1519983832497332224,1266071268874432513,Ken Mimasu,"['New paper today! \n\nWith Xu Li, Kimiko Yamashita, Chengjie Yang, Shuang-Yong Zhou &amp; our dear departed Cen Zhang, to which we dedicate this work.\n\n""Moments for positivity: using Drell-Yan data to test positivity bounds and reverse-engineer new physics"" <LINK>']",https://arxiv.org/abs/2204.13121,"Moments of the leptonic angular distribution in the Drell-Yan process have recently been shown to be sensitive probes of a specific class of dimension-8, four-fermion operators in the Standard Model Effective Field Theory, involving a pair of quarks and leptons. The same operators are also subject to positivity bounds, when requiring the associated (unknown) UV completion to obey basic principles of quantum field theory. We perform a phenomenological study to quantify the sensitivity of the high-luminosity LHC to this set of operators and, by extension, the positivity bounds. We further extend the angular basis of moments and consider double differential information to improve the ability to disentangle the different operators, leading to a sensitivity to new physics scales up to 3 TeV. We use this information to explore the violation of positivity at the LHC as a way to test the underlying principles of quantum field theory. Finally, we present a case study which combines our results with information from other (current and prospective) experiments, as well as the positivity cone to infer the properties of possible tree-level UV completions. The data lead to robust, model-independent lower bounds on the $M/\sqrt{g}$ combination of the particle mass and coupling, for states that couple to right-handed leptons and/or up quarks. ","Moments for positivity: using Drell-Yan data to test positivity bounds
  and reverse-engineer new physics"
29,1519951458908749824,175028318,Andrea Oddo,"['#TeamBispectrum is back! Congrats to all members of my former group for this brand new paper, but double congratulations to Federico and Chiara, who recently both won the @EC_Euclid STAR Prize! Having co-authored a paper with both of them is a real honor!\n\n<LINK>']",https://arxiv.org/abs/2204.13628,"We present the analysis of the halo bispectrum in redshift-space in terms of its multipoles, monopole, quadrupole and hexadecapole, measured from a large set of simulations. We fit such measurements with a tree-level model in perturbation theory that depends on linear and nonlinear bias parameters as well as on the growth rate $f$ of density fluctuations. The likelihood analysis takes advantage of a very large set of mock catalogs, enabling a robust estimation of the covariance properties for all multipoles. We compare the numerical estimate of the covariance matrix to its Gaussian prediction finding discrepancies of 10% or less for all configurations with the sole exception of the squeezed triangles in the monopole case. We find the range of validity of the tree-level model, for the total simulation volume of about 1000 $h^{-3}\, {\rm Gpc}^3$, reaches a maximum wavenumber of $0.08 \, h \, {\rm Mpc}^{-1}$ for the monopole, while it is limited to $0.06$ and $0.045\, h \, \rm{Mpc}^{-1}$ respectively for quadrupole and hexadecapole. Despite this, the addition of the quadrupole to the analysis allows for significant improvements on the determination of the model parameters and specifically on $f$, similarly to the power spectrum case. Finally, we compare our numerical estimate for the full covariance with its theoretical prediction in the Gaussian approximation and find the latter to work remarkably well in the context of simulation boxes with periodic boundary condition. ",The Halo Bispectrum Multipoles in Redshift Space
30,1519942553679712257,41040382,Mark Chen,['Our new paper is accepted in Applied Energy. <LINK>'],https://arxiv.org/abs/2204.10770#,"The rapid expansion of Advanced Meter Infrastructure (AMI) has dramatically altered the energy information landscape. However, our ability to use this information to generate actionable insights about residential electricity demand remains limited. In this research, we propose and test a new framework for understanding residential electricity demand by using a dynamic energy lifestyles approach that is iterative and highly extensible. To obtain energy lifestyles, we develop a novel approach that applies Latent Dirichlet Allocation (LDA), a method commonly used for inferring the latent topical structure of text data, to extract a series of latent household energy attributes. By doing so, we provide a new perspective on household electricity consumption where each household is characterized by a mixture of energy attributes that form the building blocks for identifying a sparse collection of energy lifestyles. We examine this approach by running experiments on one year of hourly smart meter data from 60,000 households and we extract six energy attributes that describe general daily use patterns. We then use clustering techniques to derive six distinct energy lifestyle profiles from energy attribute proportions. Our lifestyle approach is also flexible to varying time interval lengths, and we test our lifestyle approach seasonally (Autumn, Winter, Spring, and Summer) to track energy lifestyle dynamics within and across households and find that around 73% of households manifest multiple lifestyles across a year. These energy lifestyles are then compared to different energy use characteristics, and we discuss their practical applications for demand response program design and lifestyle change analysis. ","Constructing dynamic residential energy lifestyles using Latent
  Dirichlet Allocation"
31,1519931924826910720,1191386359707029505,Animesh Mukherjee,"['Preprint of our new paper on ""Data Bootstrapping Approaches to Improve Low Resource Abusive Language Detection for Indic Languages"".\n<LINK>\n\n@ACMHT @nlproc @nohate_speech @WeCounterHate @cnerg #IndianLanguages @CCDHate @hopenothate @NoHateSpeech_IT']",https://arxiv.org/abs/2204.12543,"Abusive language is a growing concern in many social media platforms. Repeated exposure to abusive speech has created physiological effects on the target users. Thus, the problem of abusive language should be addressed in all forms for online peace and safety. While extensive research exists in abusive speech detection, most studies focus on English. Recently, many smearing incidents have occurred in India, which provoked diverse forms of abusive speech in online space in various languages based on the geographic location. Therefore it is essential to deal with such malicious content. In this paper, to bridge the gap, we demonstrate a large-scale analysis of multilingual abusive speech in Indic languages. We examine different interlingual transfer mechanisms and observe the performance of various multilingual models for abusive speech detection for eight different Indic languages. We also experiment to show how robust these models are on adversarial attacks. Finally, we conduct an in-depth error analysis by looking into the models' misclassified posts across various settings. We have made our code and models public for other researchers. ","Data Bootstrapping Approaches to Improve Low Resource Abusive Language
  Detection for Indic Languages"
32,1519892212183875584,1144194850746540032,Jung-Woo Ha,"['For anyone curious about the effects of corpus size and property on the emergence of in-context zero/few-shot learning capability in hyperscale language model (#LLM). please check out our new paper! \nThanks, @kchonyc  (#NAACL2022)\n\n<LINK> \n@ClovaAiLab']",https://arxiv.org/abs/2204.13509,"Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance. ","On the Effect of Pretraining Corpora on In-context Learning by a
  Large-scale Language Model"
33,1519892013071486978,145500767,Enrique Lopez Rodriguez,"['New SALSA paper (Paper III)!\nOur Survey on extragALactic magnetiSm with @SOFIAtelescope presents the data reduction steps to obtain the magnetic field maps of 14 galaxies.\nCheck the paper here: <LINK> <LINK>', '@SOFIAtelescope With the new observational method presented here, i.e. on-the-fly-mapping, we have improved the overheads and sensitivity by a factor 2.5! \nIt means that the cost of operations, number of papers, and science collected per hour byHAWC+/ @SOFIAtelescope has improved by that much!', '@SOFIAtelescope We have the magnetic fields in active galactic nuclei. https://t.co/P0Bsxq3Fwm', '@SOFIAtelescope We have the magnetic fields in starburst galaxies. https://t.co/Ny8qDe4k5f', '@SOFIAtelescope and in spiral galaxies. https://t.co/55TEMvSGpE']",https://arxiv.org/abs/2204.13611,"We describe the data processing of the Survey on extragALactic magnetiSm with SOFIA (SALSA Legacy Program). This first data release presents 33% (51.34h out of 155.7h, including overheads) of the total awarded time taken from January 2020 to December 2021. Our observations were performed using the newly implemented on-the-fly mapping (OTFMAP) technique in the polarimetric mode. We present the pipeline steps to obtain homogeneously reduced high-level data products of polarimetric maps of galaxies for use in scientific analysis. Our approach has a general design and can be applied to sources smaller than the field-of-view of the HAWC+ array in any given band. We estimate that the OTFMAP polarimetric mode offers a reduction of observing overheads by a factor 2.34, and an improvement in sensitivity by a factor 1.80 when compared to previously obtained polarimetric observations using the chopping and nodding mode. The OTFMAP is a significant optimization of the polarimetric mode of HAWC+ as it ultimately reduces the cost of operations of SOFIA/HAWC+ by increasing the science collected per hour of observation up to an overall factor of 2.49. The OTFMAP polarimetric mode is the standard observing strategy of SALSA. The results and quantitative analysis of this first data release are presented in Papers IV and V of the series. ","Extragalactic magnetism with SOFIA (SALSA Legacy Program) -- III: First
  data release and on-the-fly polarization mapping characterization"
34,1519788983852711937,886443925,Philipp Siedler,"['New paper on Multi-Agent #ReinforcementLearning (MARL) solving a wild-fire management resource distribution task utilising collaboration, auto-curricula training and an openended environment. Accepted at @GamificationMAS workshop @iclr_conf: <LINK> 1/2 <LINK>', 'Huge thanks to @serkancabi, @ptigas for guidance and sharpness, @drimgemp for igniting the context idea, @a_tacchetti and the Gamification and Multiagent solutions reviewers for their time and effort, and finally to @jasmin_ for supporting yet another hobby project of mine ü§ñ 2/2']",https://arxiv.org/abs/2204.11350,"Most real-world domains can be formulated as multi-agent (MA) systems. Intentionality sharing agents can solve more complex tasks by collaborating, possibly in less time. True cooperative actions are beneficial for egoistic and collective reasons. However, teaching individual agents to sacrifice egoistic benefits for a better collective performance seems challenging. We build on a recently proposed Multi-Agent Reinforcement Learning (MARL) mechanism with a Graph Neural Network (GNN) communication layer. Rarely chosen communication actions were marginally beneficial. Here we propose a MARL system in which agents can help collaborators perform better while risking low individual performance. We conduct our study in the context of resource distribution for wildfire management. Communicating environmental features and partially observable fire occurrence help the agent collective to pre-emptively distribute resources. Furthermore, we introduce a procedural training environment accommodating auto-curricula and open-endedness towards better generalizability. Our MA communication proposal outperforms a Greedy Heuristic Baseline and a Single-Agent (SA) setup. We further demonstrate how auto-curricula and openendedness improves generalizability of our MA proposal. ","Collaborative Auto-Curricula Multi-Agent Reinforcement Learning with
  Graph Neural Network Communication Layer for Open-ended Wildfire-Management
  Resource Distribution"
35,1519737243509817344,781766535660515328,Takahiro Morishitaüî≠üåè,"['New paper day! We investigated the host galaxy of a possible gravitational-wave recoiling black hole (!!) <LINK> <LINK>', 'With the previously known HUGE velocity offset (2000km/s), our new HST images revealed spatial offset between the QSO and its host, by ~11kpc. We also found that the host is old and not in an ongoing merger,  and concluded that the observed displacement is not temporary.', ""Now all observational evidence of 3c186 is in favor of the GW recoiling scenario! \nWe started this project back in 2020 and then entered the pandemic. I'm glad this is finally out and so grateful to all collaborators for their kind and strong support!!""]",https://arxiv.org/abs/2204.12499,"3C186, a radio-loud quasar at $z=1.0685$, was previously reported to have both velocity and spatial offsets from its host galaxy, and has been considered as a promising candidate for a gravitational wave recoiling black hole triggered by a black hole merger. Another possible scenario is that 3C186 is in an on-going galaxy merger, exhibiting a temporary displacement. In this study, we present analyses of new deep HST/WFC3-IR and ACS images, aiming to characterize the host galaxy and test this alternative scenario. We carefully measure the light-weighted center of the host and reveal a significant spatial offset from the quasar core ($11.1\pm0.1$kpc). The direction of the confirmed offset aligns almost perpendicularly to the radio jet. We do not find evidence of a recent merger, such as a young starburst in disturbed outskirts, but only marginal light concentration in F160W at $\sim30$kpc. The host consists of matured ($>200$Myr) stellar populations and one compact star-forming region. We compare with hydro-dynamical simulations and find that those observed features are consistently seen in late-stage merger remnants. Taken together, those pieces of evidence indicate that the system is not an on-going/young merger remnant, suggesting that the recoiling black hole scenario is still a plausible explanation for the puzzling nature of 3C186. ","The Host Galaxy of the Recoiling Black Hole Candidate in 3C 186: An Old
  Major Merger Remnant at the Center of a z=1 Cluster"
36,1519728054762942466,756230494422007808,Terra Blevins,"['Are any large-scale pretrained models truly monolingual? In new work with @LukeZettlemoyer, we find that automatic data collection methods leak millions of non-English tokens into popular pretraining corpora. (1/3)\n\n‚ú®Paper‚ú®: <LINK> <LINK>', 'Specifically, we see that although the overall percentages of non-English text in these corpora are small, this corresponds to millions of out-of-language tokens (2/3) https://t.co/usumROHmmf', 'Prior work has found that monolingual models transfer surprisingly well across languages -- we show that cross-lingual performance is strongly correlated with the amount of data leaked in the pretraining corpus. (3/3)', '@yanaiela I was at Meta/Facebook while working on this project!']",https://arxiv.org/abs/2204.08110,"English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate this phenomenon and find that common English pretraining corpora actually contain significant amounts of non-English text: even when less than 1% of data is not English (well within the error rate of strong language classifiers), this leads to hundreds of millions of foreign language tokens in large-scale datasets. We then demonstrate that even these small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining. In light of these findings, we argue that no model is truly monolingual when pretrained at scale, which should be considered when evaluating cross-lingual transfer. ","Language Contamination Explains the Cross-lingual Capabilities of
  English Pretrained Models"
37,1519710592973651970,1441424997516554242,Jorge Garc√≠a Rojas,['Check our new paper on the homogeneity of the local Galactic ISM... Our results are consistent with other metallicity tracers of the current chemical composition of the ISM. This is at odds with a paper published in @Nature some months ago. <LINK>'],https://arxiv.org/abs/2204.12258,"In this paper we discuss and confront recent results on metallicity variations in the local interstellar medium, obtained from observations of HII regions and neutral clouds of the Galactic thin disk, and compare them with recent high-quality metallicity determinations of other tracers of the chemical composition of the interstellar medium as B-type stars, classical Cepheids and young clusters. We find that the metallicity variations obtained for these last kinds of objects are consistent with each other and with that obtained for HII regions but significantly smaller than those obtained for neutral clouds. We also discuss the presence of a large population of low-metallicity clouds as the possible origin for large metallicity variations in the local Galactic thin disk. We find that such hypothesis does not seem compatible with: (a) what is predicted by theoretical studies of gas mixing in galactic disks, and (b) the models and observations on the metallicity of high-velocity clouds and its evolution as they mix with the surrounding medium in their fall onto the Galactic plane. We conclude that that most of the evidence favors that the chemical composition of the interstellar medium in the solar neighborhood is highly homogeneous. ",About Metallicity Variations in the Local Galactic Interstellar Medium
38,1519685299399450629,1074795788331569152,Hayley Beltz,"['Just in time for #Exo4, it\'s new paper day! If you\'ve ever thought to yourself, ""Gee, I wonder what the high resolution emission spectra of an ultrahot Jupiter looks like"" you\'re in luck! <LINK>', ""First, some background: ultrahot Jupiters (UHJs) have HUGE day-night temperature gradients meaning that we expect their spectra to vary strongly throughout the planet's orbit. (For more info about their atmospheric structure, see my last paper!) https://t.co/wC4FOMTi97"", 'In fact, depending on the phase, the spectra will show emission features, absorption features, or both. (3D effects!) Check out this little movie showing how the spectra changes as different parts of the planet come into view:\nhttps://t.co/vRY2DwznIp', 'We also explored the effects of our different magnetic drag treatments. When we look at the net Doppler shifts as a function of phase, we see that on the nightside our dragged models return slight net redshifts compared to the blueshifts seen in the drag-free model.', 'Now the exact values of the shifts are wavelength dependent, but this shows that our differences in the atmospheric structure of magnetic models can show up (and potentially be detected) in high resolution emission spectra.', 'Finally, we end the paper with a warning to those who want to use a single 1D model for their entire observation. Especially near the quadratures, where both day and nightsides are present, a dayside model can retrieve Doppler shifts many km/s away from the true value.', 'Anyways, I hope you check out the paper and I am more than happy to chat about it with anyone interested!', '@brettmor Thank you!!', '@_astronomay Thank you!!!!']",https://arxiv.org/abs/2204.12996,"Ultrahot Jupiters are ideal candidates to explore with high-resolution emission spectra. Detailed theoretical studies are necessary to investigate the range of spectra we can expect to see from these objects throughout their orbit, because of the extreme temperature and chemical longitudinal gradients that exist across day and nightside regions. Using previously published 3D GCM models of WASP-76b with different treatments of magnetic drag, we post-process the 3D atmospheres to generate high-resolution emission spectra for two wavelength ranges and throughout the planet's orbit. We find that the high-resolution emission spectra vary strongly as a function of phase, at times showing emission features, absorption features, or both, which are a direct result of the 3D structure of the planet. At phases exhibiting both emission and absorption features, the Doppler shift differs in direction between the two spectral features, making them differentiable instead of canceling each other out. Through the use of cross-correlation, we find different patterns in net Doppler shift for models with different treatments of drag: the nightside spectra show opposite signs in their Doppler shift, while the dayside phases have a reversal in the trend of net shift with phase. Finally, we caution researchers from using a single spectral template throughout the planet's orbit; this can bias the corresponding net Doppler shift returned, as it can pick up on a bright region on the edge of the planet disk that is highly red- or blue-shifted. ","Magnetic Drag and 3-D Effects in Theoretical High-Resolution Emission
  Spectra of Ultrahot Jupiters: the Case of WASP-76b"
39,1519655462773440513,1266475355453501440,Frank E. Curtis,"['Check out our new paper on an extension of TRACE that allows inexact subproblem solutions. The extension maintains optimal iteration complexity guarantees, and can improve worst-case computational complexity. Great work by @Qi76651451 !\n\n<LINK>']",https://arxiv.org/abs/2204.11322,"An algorithm for solving nonconvex smooth optimization problems is proposed, analyzed, and tested. The algorithm is an extension of the Trust Region Algorithm with Contractions and Expansions (TRACE) [Math. Prog. 162(1):132, 2017]. In particular, the extension allows the algorithm to use inexact solutions of the arising subproblems, which is an important feature for solving large-scale problems. Inexactness is allowed in a manner such that the optimal iteration complexity of ${\cal O}(\epsilon^{-3/2})$ for attaining an $\epsilon$-approximate first-order stationary point is maintained while the worst-case complexity in terms of Hessian-vector products may be significantly improved as compared to the original TRACE. Numerical experiments show the benefits of allowing inexact subproblem solutions and that the algorithm compares favorably to a state-of-the-art technique. ","Worst-Case Complexity of TRACE with Inexact Subproblem Solutions for
  Nonconvex Smooth Optimization"
40,1519576362226438147,2491211646,Matthieu Bethermin,"['Today, the paper presenting the new version of the SIDES empirical simulation is out. The simulation now includes far-IR/mm continuum together with CO, CI, and CII. The Python code is also released. The paper discuss extensively the [CII] intensity mapping <LINK>', 'With this new code, you can obtain very easily realistic simulation (catalogs, photometric maps, spectral cubes) of photometric or line surveys from 100 microns to 3 mm. Some products are released, but you can adapt the code to obtain exactly what you need\nhttps://t.co/IEdSRTWV1W', 'While the continuum was extensively tested in the 2017 paper. We now focus on the line emission and especially the large-scale signal probed by line intensity mapping. These are maps of 1 GHz spectral slices emitted by the various lines from the simulation. https://t.co/E7RNRcC10q', 'As a first test, we checked that the line luminosity functions measured by ALMA surveys are well reproduced. The simulation does a really decent job, especially considering that we did not try to tune it to reproduce them better. https://t.co/a4OLRMzZlN', 'We then compared the power spectrum (level of fluctuations at the various scales) of the [CII] emission of various models. There are 2 order of magnitudes of difference between the various approaches/models in the literature. This shows how uncertain is this signal. https://t.co/BLwLk7IFRP', 'Why is it important? [CII] intensity mapping directly probes the clustering of the high-z star formation and can constrain galaxy evolution. We hope that a new generation as CONCERTO will be able to detect the signal and eliminate some of the scenarios.\nhttps://t.co/Js17hrF4B5', 'It could sound easy, but the [CII] signal will have to be separated from the continuum (rather easy) and the CO (much more challenging). Here we see the expected signal at large-scale from the various components. https://t.co/rleGNZl1pO', 'Even at high-frequency, the CO+[CI] at lower z (black solid line) is brighter than our baseline [CII] model (in red) and it get worse when going to lower frequency (high redshift, the most interesting).', 'The grey line indicate the signal from CO+CI after masking the galaxies, which are massive enough to be included in OIR galaxy catalogs. This could help a lot. However, at z&gt;6.5, it will not be sufficient and we will have to be sneakier!', 'Finally, we checked if we reproduced the signal from the mmIME early CO intensity mapping experiment (Keating et al.) and we are agreeing in the 1 sigma range. https://t.co/6ybUaAId5d', 'The code is public, but do not hesitate to ask us if you need help and we are also very happy to start new collaborations to develop new extension of the simulation (radio for SKA, OIR for multi-lambda surveys, UV/IR balance...).']",https://arxiv.org/abs/2204.12827,"The intensity mapping of the [CII] 158um line redshifted to the sub-mm window is a promising probe of the z>4 star formation and its spatial distribution into the large-scale structure. To prepare the first-generation experiments (e.g., CONCERTO), we need realistic simulations of the sub-mm extragalactic sky in spectroscopy. We present a new version of the SIDES simulation including the main sub-mm lines around 1mm (CO, [CII], [CI]). This approach successfully reproduces the observed line luminosity functions. We then use our simulation to generate CONCERTO-like cubes (125-305GHz) and forecast the power spectra of the fluctuations caused by the various astrophysical components at those frequencies. Depending on our assumptions on the relation between star formation rate and [CII] luminosity, and the star formation history, our predictions of the z~6 [CII] power spectrum vary by two orders of magnitude. This highlights how uncertain the predictions are and how important future measurements will be to improve our understanding of this early epoch. SIDES can reproduce the CO shot noise recently measured at ~100 GHz by the mmIME experiment. Finally, we compare the contribution of the different astrophysical components at various redshift to the power spectra. The continuum is by far the brightest, by a factor of 3 to 100 depending on the frequency. At 300GHz, the CO foreground power spectrum is higher than the [CII] one for our base scenario. At lower frequency, the contrast between [CII] and extragalactic foregrounds is even worse. Masking the known galaxies from deep surveys should allow to reduce the foregrounds to 20% of the [CII] power spectrum up to z~6.5. However, this masking method will not be sufficient at higher redshifts. The code and the products of our simulation are released publicly and can be used for both intensity mapping experiments and sub-mm continuum and line surveys. ","CONCERTO: High-fidelity simulation of millimeter line emissions of
  galaxies and [CII] intensity mapping"
41,1519370293935894528,258979716,Cassidy Laidlaw,"['Come check out our poster (with @ancadianadragan) at @iclr_conf today on the Boltzmann Policy Distribution! We introduce a new model of human behavior that can let AI systems predict and cooperate with people with little or no human data. \n\nPaper: <LINK>\n(1/n) <LINK>', ""Prior human models are based either on imitation learning, which needs expensive human data and can't easily transfer OOD, or noisily-optimal behavior for a reward function. Models of the latter type like Boltzmann rationality struggle when people are *systematically suboptimal.*"", 'For instance, in this scenario from a cooking game, the optimal action is to move up but the person goes left‚Äî3 different times! Boltzmann rationality can only capture *random* deviations from optimality, and we find in this game it fails to predict real human actions. https://t.co/KPuXeEqbD0', 'Our key insight is that if we replace the distribution over *trajectories* in Boltzmann rationality with a distribution over *policies*, we can couple action choices over time and capture systematic suboptimality. https://t.co/qrICgJegYO', ""We can use the BPD as a prior over human policies and update it with Bayes' rule after observing some human behavior at test time. This means that with the BPD we don't need human data at train time but can still adapt at test time. https://t.co/yr6lWnVvQY"", ""In the collaborative cooking game Overcooked, the BPD is far more predictive of real human behavior than Boltzmann rationality. In fact, despite using no human data before evaluation, it's almost as good as data-intensive behavior cloning! https://t.co/UnK9jbHou1"", 'We can also use the BPD to train collaborative AI agents without human data. These perform much better with humanlike players than agents based on Boltzmann rational human models, and nearly as well as those using a lot of human data. https://t.co/q0VUgNApbQ', 'Building safe AI systems requires the ability to robustly collaborate with people and I hope the BPD is a step towards that goal. In the future, we also plan to use the BPD in reward learning and inverse RL as a replacement for Boltzmann rationality.', 'Recorded presentation: https://t.co/8NlpqktV0A\nPoster session: Wed, April 27 10:30-12:30 PT']",https://arxiv.org/abs/2204.10759,"Models of human behavior for prediction and collaboration tend to fall into two categories: ones that learn from large amounts of data via imitation learning, and ones that assume human behavior to be noisily-optimal for some reward function. The former are very useful, but only when it is possible to gather a lot of human data in the target environment and distribution. The advantage of the latter type, which includes Boltzmann rationality, is the ability to make accurate predictions in new environments without extensive data when humans are actually close to optimal. However, these models fail when humans exhibit systematic suboptimality, i.e. when their deviations from optimal behavior are not independent, but instead consistent over time. Our key insight is that systematic suboptimality can be modeled by predicting policies, which couple action choices over time, instead of trajectories. We introduce the Boltzmann policy distribution (BPD), which serves as a prior over human policies and adapts via Bayesian inference to capture systematic deviations by observing human actions during a single episode. The BPD is difficult to compute and represent because policies lie in a high-dimensional continuous space, but we leverage tools from generative and sequence models to enable efficient sampling and inference. We show that the BPD enables prediction of human behavior and human-AI collaboration equally as well as imitation learning-based human models while using far less data. ","The Boltzmann Policy Distribution: Accounting for Systematic
  Suboptimality in Human Models"
42,1519352654899691520,1158317142447706112,Stephanie Brandl,"['You can find our new paper ‚ÄúHow Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns‚Äù which we will present at @naaclmeeting now on arxiv: <LINK>. \nTL,DR: In this paper, we show that gender-neutral pronouns in Danish, English', 'and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance in NLI and coreference resolution.\nWe see a clear increase in perplexity for gender-neutral pronouns in comparison to gendered pronouns such', 'as she/he and a correlation between attention flow and perplexity, particularly in later layers. This suggests that there is some development across layers\nthat is stronger for gender-neutral pronouns than for gendered pronouns https://t.co/vJ77wJ8pFA', 'Furthermore, we observe a drastic drop in performance in pronoun resolution in English https://t.co/GG9xAofanl', 'as well as a smaller decrease for Danish where we applied the full coreference resolution https://t.co/ZWCKMNc4BI', 'We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved.\nThis is joint work with @ruixiangcui and Anders S√∏gaard @coastalcph']",https://arxiv.org/abs/2204.10281,"Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved. ","How Conservative are Language Models? Adapting to the Introduction of
  Gender-Neutral Pronouns"
43,1519321861577973763,1202165863895449600,Fran√ßois Charton,"['Our new paper on Symbolic Regression with @pa_kamienny @stephanedascoli @GuillaumeLample is now on Arxiv !\nWe achieve performance comparable to SOTA genetic algorithms on SRBench with Transformers, whose inference time is orders of magnitude lower!\n<LINK> 1/4 <LINK>', 'Given the value of y at a set of points x (black dots), can you recognize the function y = f(x) ? This is what Symbolic Regression is about ! You can play around with our model here to get a better idea : https://t.co/TEWHl4VlAf 2/4 https://t.co/sFeioENP9v', 'Symbolic Regression usually involves a two-step procedure: predicting an equation ""skeleton"", then fitting the constants with a solver such as BFGS.\nTraining a Transformer to predict the full expression end-to-end works better! Predicted constants can then be refined using BFGS. https://t.co/dk8sC0MhBI', 'Our model is trained on a vast dataset of synthetic examples, and scales to input dimensions up to ten. After several million examples, the attention maps start to reveal intricate mathematical analysis : in the example f(x)=sin(x)/x below, we see Fourier-like patterns. 4/4 https://t.co/EbO5Rk87LT']",https://arxiv.org/abs/2204.10532,"Symbolic regression, the task of predicting the mathematical expression of a function from the observation of its values, is a difficult task which usually involves a two-step procedure: predicting the ""skeleton"" of the expression up to the choice of numerical constants, then fitting the constants by optimizing a non-convex loss function. The dominant approach is genetic programming, which evolves candidates by iterating this subroutine a large number of times. Neural networks have recently been tasked to predict the correct skeleton in a single try, but remain much less powerful. In this paper, we challenge this two-step procedure, and task a Transformer to directly predict the full mathematical expression, constants included. One can subsequently refine the predicted constants by feeding them to the non-convex optimizer as an informed initialization. We present ablations to show that this end-to-end approach yields better results, sometimes even without the refinement step. We evaluate our model on problems from the SRBench benchmark and show that our model approaches the performance of state-of-the-art genetic programming with several orders of magnitude faster inference. ",End-to-end symbolic regression with transformers
44,1519313005242159105,724609851884724225,Ana Belen Sainz,"['New paper out! \n\n""An open-source linear program for testing nonclassicality""\n<LINK>\n\nInput the states and measurements you want to use, and the program tells you if you can generate nonclassical statistics in a prepare-measure experiment. \n\n@ictqt\n@JohnHSelby']",https://arxiv.org/abs/2204.11905,"The gold standard for demonstrating that an experiment resists any classical explanation is to show that its statistics violate generalized noncontextuality. We here provide an open-source linear program for testing whether or not any given prepare-measure experiment is classically-explainable in this sense. The input to the program is simply an arbitrary set of quantum states and an arbitrary set of quantum effects; the program then determines if the Born rule statistics generated by all pairs of these can be explained by a classical (noncontextual) model. If a classical model exists, it provides an explicit model. If it does not, then it computes the minimal amount of noise that must be added such that a model does exist, and then provides this model. We generalize all these results to arbitrary generalized probabilistic theories (and accessible fragments thereof) as well; indeed, our linear program is a test of simplex-embeddability. ",An open-source linear program for testing nonclassicality
45,1519264246865043459,4003274873,Perry Gibson üçç,"['Are you into reconfigurable DNN accelerators, and the @ApacheTVM deep learning compiler stack?\n\nWell, the preprint of our new paper ""Bifrost"" is now available, to appear at #ISPASS 2022\n\n<LINK>']",https://arxiv.org/abs/2204.12418,"Reconfigurable accelerators for deep neural networks (DNNs) promise to improve performance such as inference latency. STONNE is the first cycle-accurate simulator for reconfigurable DNN inference accelerators which allows for the exploration of accelerator designs and configuration space. However, preparing models for evaluation and exploring configuration space in STONNE is a manual developer-timeconsuming process, which is a barrier for research. This paper introduces Bifrost, an end-to-end framework for the evaluation and optimization of reconfigurable DNN inference accelerators. Bifrost operates as a frontend for STONNE and leverages the TVM deep learning compiler stack to parse models and automate offloading of accelerated computations. We discuss Bifrost's advantages over STONNE and other tools, and evaluate the MAERI and SIGMA architectures using Bifrost. Additionally, Bifrost introduces a module leveraging AutoTVM to efficiently explore accelerator designs and dataflow mapping space to optimize performance. This is demonstrated by tuning the MAERI architecture and generating efficient dataflow mappings for AlexNet, obtaining an average speedup of $50\times$ for the convolutional layers and $11\times$ for the fully connected layers. Our code is available at www.github.com/gicLAB/bifrost. ","Bifrost: End-to-End Evaluation and Optimization of Reconfigurable DNN
  Accelerators"
46,1519214435642097666,1490663255043452929,Risto Paatelainen,"['New paper out! Next step QCD :) @HIPhysics @KumpulaScience @ERC_Research \n\n<LINK>', 'and long companion paper\n\nhttps://t.co/W5IRYPmTVj']",https://arxiv.org/abs/2204.11893,"We determine the pressure of a cold and dense electron gas to a nearly complete next-to-next-to-next-to-leading order (N$^3$LO) in the fine-structure constant $\alpha_e$, utilizing a new result for the two-loop photon self-energy from a companion paper. Our result contains all infrared-sensitive contributions to the pressure at this order, including the coefficient of the $O(\alpha_e^3 \ln \alpha_e^{ })$ term, and leaves only a single coefficient associated with the contributions of unresummed hard momenta undetermined. Moreover, we explicitly demonstrate the complete cancellation of infrared divergences according to the effective field theory paradigm by determining part of the hard contributions at this order. Our calculation provides the first improvement to a 45-year-old milestone result and demonstrates the feasibility of the corresponding N$^3$LO calculation for cold and dense quark matter. ",Degenerate fermionic matter at N$^3$LO: Quantum Electrodynamics
47,1519212970953297920,1501847085037019141,H√©ctor Gil Mar√≠n,"['New paper on cosmology constraints using @sdssurveys data.\n\nWe apply ShapeFit on @eBOSSurvey data and perform a model-agnostic parameter inference on 0.2&lt;z&lt;2.2, and interpreted in the light of LCDM++ models.\n\n<LINK>\n\nSee the üßµ to find its main results üëá', 'The whole SDSS 0.2&lt;z&lt;2.2 range is compressed in 4 model-agnostic parameters per redshift bin: angular and radial BAO distances, the growth of structure times sigma8; and the novel ShapeFit parameter m.\n\nThis represents a compression of 1,723,267 galaxies in just 16 parameters! https://t.co/5gN9OOWyqp', 'We interpret this parametrization in the light of LCDM finding, \nOm = 0.2971 ¬± 0.0061;\ns8 = 0.857 ¬± 0.040;\n\nIf a BBN prior is added we find,\nŒ©m = 0.3001 ¬± 0.0057;\nH0 = 68.16 ¬± 0.67;\ns8 = 0.858 ¬± 0.036;\nThe results are a substantial improvement with respect to BAO and RSD analyses https://t.co/gWPKtQOmrf', 'Neutrinos!\nThanks to the shape information we are able to constrain the sum of the mass of neutrinos with LSS+BBN only information finding Œ£mŒΩ &lt; 0.40 eV (95%). When combined with Planck, we find Œ£mŒΩ &lt; 0.082 eV (95%) https://t.co/Le581MmhZ8', 'Curvature!\nWhen we relax the flatness condition we find the the results very consistent with a flat Universe, \nŒ©k = ‚àí0.022 (+0.032) (‚àí0.038) without using any other dataset; and, \nŒ©k = +0.0015 ¬± 0.0016 when data from Planck is added. https://t.co/NE2gXVlNXn', 'Dark Energy!\nWe constraint the dark energy equation of state parameter,\n\nw = ‚àí0.998 (+0.085) (‚àí0.073) without using any other dataset; and,\nw = ‚àí1.093(+0.048) (‚àí0.044) when data from Planck is added. https://t.co/lxoJgbMqSc', 'Tensions,We find that our results on sigma8 arein well agreement from those of Planck, \ns8_ShapeFit = 0.858 ¬± 0.036;\ns8_Planck = 0.8101 ¬± 0.0062;\n\nWe also find good internal s8 consistency,\ns8 = 0.820 ¬± 0.043 (LRGs 0.2&lt;z&lt;1.0)\ns8 = 0.993 ¬± 0.072 (QSOs, 0.8&lt;z&lt;2.2)', 'It has been a pleasure to collaborate with my PhD student @Cosmosamu (who will be defending his thesis soon) and @licia_verde. We also thank the effort of @sdssurveys team on producing such good quality data, which has made this work possible.', ""@AddisonGraeme @sdssurveys @eBOSSurvey Hi Graeme, we haven't quantify it, we simply were aware that in some of those papers that effect was not taken correctly into account. We believe it doesn't belong to us to quantify by how much sigma8 increases in their when this is properly accounted for."", ""@AddisonGraeme @sdssurveys @eBOSSurvey Having said that, we've recently seen some papers where this effect should be correctly taken into account, and that still have a low œÉ8. \nSee for e.g., this Monday arxiv,\nhttps://t.co/by1SLonWBA\nwhere they find œÉ8=0.707 ¬± 0.035.\nI don't understand the origin of this discrepancy.""]",https://arxiv.org/abs/2204.11868,"We present the first model-agnostic analysis of the complete set of Sloan Digital Sky Survey III (BOSS) and -IV (eBOSS) catalogues of luminous red galaxy and quasar clustering in the redshift range $0.2\leq z \leq 2.2$ (10 billion years of cosmic evolution), which consistently includes the baryon acoustic oscillations (BAO), redshift space distortions (RSD) and the shape of the transfer function signatures, from pre- and post-reconstructed catalogues in Fourier space. This approach complements the standard analyses techniques which only focus on the BAO and RSD signatures, and the full-modeling approaches which assume a specific underlying cosmology model to perform the analysis. These model-independent results can then easily be interpreted in the context of the cosmological model of choice. In particular, when combined with $z>2.1$ Ly-$\alpha$ BAO measurements, the clustering BAO, RSD and {\it Shape} parameters can be interpreted within a flat-$\Lambda$CDM model yielding $h=0.6816\pm0.0067$, $\Omega_{\rm m}=0.3001\pm0.0057$ and $10^{9}\times A_s= 2.43\pm0.20$ (or $\sigma_8=0.858\pm0.036$) with a Big Bang Nucleosynthesis prior on the baryon density. Without any external dataset, the BOSS and eBOSS data alone imply $\Omega_{\rm m}=0.2971\pm 0.0061$ and $10^{9}\times A_s=2.39^{+0.24}_{-0.43}$ (or $\sigma_8=0.857\pm0.040$). For models beyond $\Lambda$CDM, eBOSS data alone (in combination with Planck) constrain the sum of neutrino mass to be $\Sigma m_\nu< 0.40$ eV with a BBN prior ($\Sigma m_\nu <0.082$ eV) at 95\% CL, the curvature energy density to $\Omega_\mathrm{k} = -0.022_{-0.038}^{+0.032}$ ($\Omega_\mathrm{k} = 0.0015\pm 0.0016$) and the dark energy equation of state parameter to $w=-0.998_{-0.073}^{+0.085}$ ($w=-1.093_{-0.044}^{+0.048}$) at 68\% CL without a BBN prior. ","Model-agnostic interpretation of 10 billion years of cosmic evolution
  traced by BOSS and eBOSS data"
48,1519201215036510208,802543221943439360,Andrea Caputo,"['New paper out! <LINK>\n\nStarting from first principles, we study radiative transfer by new bosons. A particularly relevant conclusion is that in the strong trapping regime the Stefan-Boltzmann emission of bosons is a very good proxy of the true flux. <LINK>', 'This is a bit at odds with other works. The difference is that previous works adopted some approximations when dealing with the integral of the optical depth. In particular one usually finds a volume integral done with the optical depth considered only in the radial direction.', 'In the attached plot -- Primakoff process in SN -- you can see that this approximation (dashed blue curve) largely overestimates the true flux (solid blue curve).']",https://arxiv.org/abs/2204.11862,"Starting from first principles, we study radiative transfer by new feebly-interacting bosons (FIBs) such as axions, axion-like particles (ALPs), dark photons, and others. Our key simplification is to include only boson emission or absorption (including decay), but not scattering between different modes of the radiation field. Based on a given distribution of temperature and FIB absorption rate in a star, we derive explicit volume-integral expressions for the boson luminosity, reaching from the free-streaming to the strong-trapping limit. The latter is seen explicitly to correspond to quasi-thermal emission from a ""FIB sphere"" according to the Stefan-Boltzmann law. Our results supersede expressions and approximations found in the recent literature on FIB emission from a supernova core and, for radiatively unstable FIBs, provide explicit expressions for the nonlocal (""ballistic"") transfer of energy recently discussed in horizontal-branch stars. ",Radiative transfer in stars by feebly interacting bosons
49,1519150735849668609,3241924438,Akshansh Mishra,['Check out the new paper on the arXiv.\n<LINK>\n\n#NaturalLanguageProcessing #NLP #MachineLearning #Summarization #welding #arXiv <LINK>'],https://arxiv.org/abs/2204.12309,"Text summarization is a technique for condensing a big piece of text into a few key elements that give a general impression of the content. When someone requires a quick and precise summary of a large amount of information, it becomes vital. If done manually, summarizing text can be costly and time-consuming. Natural Language Processing (NLP) is the sub-division of Artificial Intelligence that narrows down the gap between technology and human cognition by extracting the relevant information from the pile of data. In the present work, scientific information regarding the Friction Stir Welding of Aluminum alloys was collected from the abstract of scholarly research papers. For extracting the relevant information from these research abstracts four Natural Language Processing based algorithms i.e. Latent Semantic Analysis (LSA), Luhn Algorithm, Lex Rank Algorithm, and KL-Algorithm were used. In order to evaluate the accuracy score of these algorithms, Recall-Oriented Understudy for Gisting Evaluation (ROUGE) was used. The results showed that the Luhn Algorithm resulted in the highest f1-Score of 0.413 in comparison to other algorithms. ","Information Retrieval in Friction Stir Welding of Aluminum Alloys by
  using Natural Language Processing based Algorithms"
50,1519119528835567617,107357817,daisukelab,"['Our new paper is out! We explored a simple masked patch modeling w/o augmentation to learn a latent that describes the input spectrogram as it is.\n‚ÄúMasked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation‚Äù\n<LINK>', 'We used MAE with spectrogram input, evaluated the HEAR 2021 benchmark, and visualized reconstructions and self-attention maps to understand learned features. (I like visualizations.‚ô•) https://t.co/aMlZp6QBfg']",https://arxiv.org/abs/2204.12260,"Recent general-purpose audio representations show state-of-the-art performance on various audio tasks. These representations are pre-trained by self-supervised learning methods that create training signals from the input. For example, typical audio contrastive learning uses temporal relationships among input sounds to create training signals, whereas some methods use a difference among input views created by data augmentations. However, these training signals do not provide information derived from the intact input sound, which we think is suboptimal for learning representation that describes the input as it is. In this paper, we seek to learn audio representations from the input itself as supervision using a pretext task of auto-encoding of masked spectrogram patches, Masked Spectrogram Modeling (MSM, a variant of Masked Image Modeling applied to audio spectrogram). To implement MSM, we use Masked Autoencoders (MAE), an image self-supervised learning method. MAE learns to efficiently encode the small number of visible patches into latent representations to carry essential information for reconstructing a large number of masked patches. While training, MAE minimizes the reconstruction error, which uses the input as training signal, consequently achieving our goal. We conducted experiments on our MSM using MAE (MSM-MAE) models under the evaluation benchmark of the HEAR 2021 NeurIPS Challenge. Our MSM-MAE models outperformed the HEAR 2021 Challenge results on seven out of 15 tasks (e.g., accuracies of 73.4% on CREMA-D and 85.8% on LibriCount), while showing top performance on other tasks where specialized models perform better. We also investigate how the design choices of MSM-MAE impact the performance and conduct qualitative analysis of visualization outcomes to gain an understanding of learned representations. We make our code available online. ","Masked Spectrogram Modeling using Masked Autoencoders for Learning
  General-purpose Audio Representation"
51,1519025647540224002,40299444,Alexey Petrov,"['A new paper! I propose a new state, the glueball molecule, which is a molecular state of a glueball and an ordinary meson. I also study phenomenology of such states. See it at <LINK> <LINK>', '@Jordy_de_Vries Hi Jordy. Yes, indeed, I cannot predict the absolute value for the binding energy because of that counter term. So, just like in deuteron, I assume that pi(1800) is a molecule and get \\lambda_R. But once I do, I can do things for other mesons‚Ä¶', '@Jordy_de_Vries So I use the formalism only to establish that there is a pole, I.e. a bound state', '@Jordy_de_Vries But I don‚Äôt think that this system is as fine-tuned as a deuteron or an X(3872)']",https://arxiv.org/abs/2204.11269,"Experimental searches for pure glueball states have proven challenging and so far yielded no results. This is believed to occur because glueballs mix with the ordinary $q\bar q$ states with the same quantum numbers. We will discuss an alternative mechanism, the formation of the glueball-meson molecular states. We will argue that the wave functions of already observed excited meson states may contain a significant part due to such molecular states. We discuss the phenomenology of glueball molecules and comment on a possible charmless component of the $XYZ$ states. ",Glueball molecules
52,1518976880556142592,1420495724366557185,Beryl Hovis-Afflerbach,"['I‚Äôm super excited that the paper I wrote with Dean Pesnell while working at NASA Goddard last year is published in Solar Physics and is on the arxiv today! In it, we present two new methods for studying Polar Faculae on the Sun. <LINK>', 'Polar faculae are bright points in the Sun‚Äôs photosphere where magnetic field intersects with the surface. Their number has been shown to correlate with the strength of the polar magnetic field and to be a predictor of the subsequent solar cycle', 'But due to their small &amp; transient nature, combined with different techniques &amp; observational factors, previous counts of faculae differ. Further, there were no scalable techniques to measure their statistical properties, such as lifetime &amp; how it varies with the solar cycle', 'Using data from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), we develop two new methods for tracking polar faculae and measuring their properties:', '1) We calculate the pixel-by-pixel standard deviation of all the HMI visible light images over one day, in which faculae show up as streaks due to the Sun‚Äôs rotation. You can see the original data in the top image and the standard deviation below https://t.co/3apkhEOvcg', 'Counting the streaks is a much faster way to count polar faculae, because we see all the faculae over the course of a single day. Our facular count agrees with previous counts in its variation with the solar cycle and polar magnetic field https://t.co/xrF68abgSX', 'Further, because we know how fast the Sun rotates, we can divide the length of each streak by the latitude-dependent rotation speed to determine the streak‚Äôs lifetime', 'When we applied this technique to 134 days of data over 11 years, we counted and measured 9291 faculae! We found that the mean lifetime of polar faculae is around 6 hours, but they can last up to 1 day https://t.co/V0rHzgZdpk', '2) In the second method, we make a progressive standard deviation movie: for each image of the Sun over 1 day, we calculate the standard deviation up to that point, then overlay the HMI magnetogram, which shows us what the magnetic field looks like at that point in time https://t.co/LUTOcUNKFE', 'Using this method, we can see how polar faculae evolve, that they participate in convective motions at the poles, and that they are associated with the same polarity magnetic field as that of the pole', 'The next step forward is to extend these methods to automate the measurement of faculae, which should allow for daily measurements over more than a solar cycle, since SDO‚Äôs launch in 2010!', 'If you‚Äôre interested in our results, please check out our paper, which is published in Solar Physics as part of the topical collection on Celebrating a Solar Cycle of Discovery with SDO! https://t.co/EvLq1oSNY5']",https://arxiv.org/abs/2204.10863,"Polar faculae (PFe) are the footpoints of magnetic field lines near the Sun's poles that are seen as bright regions along the edges of granules. The time variation in the number of PFe has been shown to correlate with the strength of the polar magnetic field and to be a predictor of the subsequent solar cycle. Due to the small size and transient nature of these features, combined with different techniques and observational factors, previous counts of PFe differ in magnitude. Further, there were no scalable techniques to measure the statistical properties of faculae. Using data from the Helioseismic and Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO), we present two new methods for tracking faculae and measuring their properties. In the first, we calculate the standard deviation of the HMI images over one day, visualizing the faculae as streaks. The facular lifetime is found by dividing the angular streak length by the latitude-dependent rotation rate. We apply this method to 134 days of data over 11 years. The distribution of facular lifetimes has a mean of 6.0 hours, a FWHM of 5.4 hours, and a skew towards longer lifetimes, with some lasting up to 1 day. In the second method, we overlay images of the progressive standard deviation with the HMI magnetogram to show the close relationship between facular candidates and the magnetic field. The results allow us to distinguish between motion due to the Sun's rotation and ""proper motion"" due to faculae moving across the Sun's surface, confirming that PFe participate in convective motions at the poles. Counts of PFe using both methods agree with previous counts in their variation with the solar cycle and the polar magnetic field. These methods can be extended to automate the identification and measurement of other properties of of PFe, which would allow for daily measurements of all faculae since SDO began operation in 2010. ",Two New Methods for Counting and Tracking the Evolution of Polar Faculae
53,1518959874385850369,324766904,Dave Radke,"['Should teammates fully optimize for a shared goal? How does this impact #AI reinforcement learning (#RL) agents?\n\nMy new paper explores this concept. I even mixed in a quote from @WayneGretzky while I was at it!\n\nAccepted to #ALA2022 at @aamas2022!\n\nüìÑ: <LINK>', '@WayneGretzky @aamas2022 For my hockey followers: I tend to think of the ""system"" in this paper as a hockey team and ""team"" as the D/Fwd blocks or lines.\n\nMain finding: selfish players lead to poor results (obviously), but system-focused players aren\'t the best either. Teams need role-based goals.']",https://arxiv.org/abs/2204.07471,"We propose a model for multi-objective optimization, a credo, for agents in a system that are configured into multiple groups (i.e., teams). Our model of credo regulates how agents optimize their behavior for the component groups they belong to. We evaluate credo in the context of challenging social dilemmas with reinforcement learning agents. Our results indicate that the interests of teammates, or the entire system, are not required to be fully aligned for globally beneficial outcomes. We identify two scenarios without full common interest that achieve high equality and significantly higher mean population rewards compared to when the interests of all agents are aligned. ",The Importance of Credo in Multiagent Learning
54,1518957736616239105,1593207031,Estelle Maeva Inack,['Entanglement and symmetries are the usual suspects when using NNs to probe condensed matter systems. We show that trainability should also matter. Check out our new paper with @sgmorawetz and @rgmelko <LINK>'],https://arxiv.org/abs/2204.11272,"Artificial neural networks have been widely adopted as ansatzes to study classical and quantum systems. However, some notably hard systems such as those exhibiting glassiness and frustration have mainly achieved unsatisfactory results despite their representational power and entanglement content, thus, suggesting a potential conservation of computational complexity in the learning process. We explore this possibility by implementing the neural annealing method with autoregressive neural networks on a model that exhibits glassy and fractal dynamics: the two-dimensional Newman-Moore model on a triangular lattice. We find that the annealing dynamics is globally unstable because of highly chaotic loss landscapes. Furthermore, even when the correct ground state energy is found, the neural network generally cannot find degenerate ground-state configurations due to mode collapse. These findings indicate that the glassy dynamics exhibited by the Newman-Moore model caused by the presence of fracton excitations in the configurational space likely manifests itself through trainability issues and mode collapse in the optimization landscape. ","Neural annealing and visualization of autoregressive neural networks in
  the Newman-Moore model"
55,1518943286098018305,724609851884724225,Ana Belen Sainz,"[""New paper out!\xa0\n\nYou take something for granted, and then you can't find a reference where it's proven... what do you do? You solve it yourself!\xa0\n\nSimulating all multipartite non-signalling in generalised probabilistic theories\n<LINK>\n\n@ictqt\n@JohnHSelby""]",https://arxiv.org/abs/2204.10639,"Non-signalling quantum channels -- relevant in, e.g., the study of Bell and Einstein-Podolsky-Rosen scenarios -- may be simulated via affine combinations of local operations in bipartite scenarios. Moreover, when these channels correspond to stochastic maps between classical variables, such simulation is possible even in multipartite scenarios. These two results have proven useful when studying the properties of these channels, such as their communication and information processing power, and even when defining measures of the non-classicality of physical phenomena (such as Bell non-classicality and steering). In this paper we show that such useful quasi-stochastic characterizations of channels may be unified and applied to the broader class of multipartite non-signalling channels. Moreover, we show that this holds for non-signalling channels in quantum theory, as well as in a larger family of generalised probabilistic theories. More precisely, we prove that non-signalling channels can always be simulated by affine combinations of corresponding local operations, provided that the underlying physical theory is locally tomographic -- a property that quantum theory satisfies. Our results then can be viewed as a generalisation of Refs.~[Phys. Rev. Lett. 111, 170403] and [Phys. Rev. A 88, 022318 (2013)] to the multipartite scenario for arbitrary tomographically local generalised probabilistic theories (including quantum theory). Our proof technique leverages Hardy's duotensor formalism, highlighting its utility in this line of research. ","Simulating all multipartite non-signalling channels via
  quasiprobabilistic mixtures of local channels in generalised probabilistic
  theories"
56,1518895284574638080,113753401,QShane,['New paper on the arXiv today presenting a graphical language for linear optical quantum circuits:\n<LINK>\n@Quandela_SAS <LINK>'],https://arxiv.org/abs/2204.11787,"We introduce the LOv-calculus, a graphical language for reasoning about linear optical quantum circuits with so-called vacuum state auxiliary inputs. We present the axiomatics of the language and prove its soundness and completeness: two LOv-circuits represent the same quantum process if and only if one can be transformed into the other with the rules of the LOv-calculus. We give a confluent and terminating rewrite system to rewrite any polarisation-preserving LOv-circuit into a unique triangular normal form, inspired by the universal decomposition of Reck et al. (1994) for linear optical quantum circuits. ",LOv-Calculus: A Graphical Language for Linear Optical Quantum Circuits
57,1518866543890870273,94558760,Richard Dufour,['üìùNew paper related to our participation with @LabrakYanis to the BioCreative VII LitCovid Track for COVID-19 literature topic annotations <LINK>'],https://arxiv.org/abs/2204.09781,"The COVID-19 pandemic has been severely impacting global society since December 2019. Massive research has been undertaken to understand the characteristics of the virus and design vaccines and drugs. The related findings have been reported in biomedical literature at a rate of about 10,000 articles on COVID-19 per month. Such rapid growth significantly challenges manual curation and interpretation. For instance, LitCovid is a literature database of COVID-19-related articles in PubMed, which has accumulated more than 200,000 articles with millions of accesses each month by users worldwide. One primary curation task is to assign up to eight topics (e.g., Diagnosis and Treatment) to the articles in LitCovid. Despite the continuing advances in biomedical text mining methods, few have been dedicated to topic annotations in COVID-19 literature. To close the gap, we organized the BioCreative LitCovid track to call for a community effort to tackle automated topic annotation for COVID-19 literature. The BioCreative LitCovid dataset, consisting of over 30,000 articles with manually reviewed topics, was created for training and testing. It is one of the largest multilabel classification datasets in biomedical scientific literature. 19 teams worldwide participated and made 80 submissions in total. Most teams used hybrid systems based on transformers. The highest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro F1-score, micro F1-score, and instance-based F1-score, respectively. The level of participation and results demonstrate a successful track and help close the gap between dataset curation and method development. The dataset is publicly available via this https URL for benchmarking and further development. ","Multi-label classification for biomedical literature: an overview of the
  BioCreative VII LitCovid Track for COVID-19 literature topic annotations"
58,1518761665847775233,1019760963569049601,Almog Yalinewich,"['New paper on the arxiv, in which we propose a model for fast radio bursts, in which fast spinning neutron stars with low magnetic fields are the progenitors, rather than magnetars (which are the progenitors in most other models)\n<LINK>', '@di_goldene_pave oops, should be equation 19. Thanks for catching it!\nThe emission mechanism for the counterpart is the same as for the radio emission, the only difference is that the shock radius is smaller, and the Lorentz factor is higher, so the photons get boosted to a higher energy.']",https://arxiv.org/abs/2204.11663,"Recent observations of coherent radiation from the Crab pulsar (Bij et al 2021) suggest the emission is driven by an ultra - relativistic ($\gamma \sim 10^4$), cold plasma flow. A relativistically expanding plasma shell can compress the ambient magnetic field, like a moving mirror, and thus produce coherent radiation whose wavelength is shorter than that of the ambient medium by $\gamma^2$. This mechanism has been studied in the past by Colgate and Noerdelinger (1971), in the context of radio loud supernova explosions. In this work we propose that a similar mechanism drives the coherent emission in fast radio bursts. The high Lorenz factors dramatically lower the implied energy and magnetic field requirements, allowing the spin down energy of regular (or even recycled), fast spinning pulsars, rather than slow spinning magnetars, to explain FRBs. We show that this model can explain the frequency and the time evolution of observed FRBs, as well as their duration, energetics and absence of panchromatic counterparts. We also predict that the peak frequency of sub pulses decline with observation time as $\omega_{\rm obs} \propto t_{\rm obs}^{-1/2}$. Unfortunately, with current capabilities it is not possible to constrain the shape of the curve $\omega_{\rm obs} \left(t_{\rm obs} \right)$. Finally, we find that a variation of this model can explain weaker radio transients, such as the one observed from a galactic magnetar. In this variant, the shock wave produces low frequency photons which are then Compton scattered to the GHz range. ",The Moving Mirror model for Fast Radio Bursts
59,1518618467598946304,1870088930,Nikhil Garg,"['New paper! ""Equity in Resident Crowdsourcing: Measuring Under-reporting without Ground Truth Data"" w/ @ZhiLiu724 \n\nWe develop method to quantify under-reporting in @nyc311-like systems. Applying it to NYC Parks data reveals big spatial disparities \n\n<LINK> <LINK>', ""Quantifying under-reporting is tricky. Why? The whole reason these systems exist is bc we don't know  when/that incidents happen. So, if something isn't reported: is it because it didn't happen, or because no one reported it?"", 'Standard approach: Try to estimate ground truth. This is hard (and harder to validate), for same missing data reasons! \n\nOur approach: Leverage that there are sometimes *multiple* reports about the same incident, to disentangle reporting rates from incident occurrence rates.', 'Why does this matter? If there are reporting disparities, that propagates to downstream disparities in what work government agencies do, and when they do it.\n\nWe‚Äôre now working directly with Parks to understand and improve the end-to-end reporting-to-work pipeline. https://t.co/0Bg4YXaTCJ', 'We think our method can be almost directly applied to other reporting systems, like software bugs and other agency/311 systems -- please reach out!\n\nPaper link: https://t.co/vjvoOm6Nep\n\n(Virtual) talk next week 5/5 at C3ai seminar: https://t.co/X20ePSUQRj', 'And (virtual) talk this Wednesday at TOC4Fairness!\n\nhttps://t.co/yV0Xf4Yez1']",https://arxiv.org/abs/2204.08620,"Modern city governance relies heavily on crowdsourcing (or ""co-production"") to identify problems such as downed trees and power-lines. A major concern in these systems is that residents do not report problems at the same rates, leading to an inequitable allocation of government resources. However, measuring such under-reporting is a difficult statistical task, as, almost by definition, we do not observe incidents that are not reported. Thus, distinguishing between low reporting rates and low ground-truth incident rates is challenging. We develop a method to identify (heterogeneous) reporting rates, without using external (proxy) ground truth data. Our insight is that rates on $\textit{duplicate}$ reports about the same incident can be leveraged, to turn the question into a standard Poisson rate estimation task -- even though the full incident reporting interval is also unobserved. We apply our method to over 100,000 resident reports made to the New York City Department of Parks and Recreation, finding that there are substantial spatial and socio-economic disparities in reporting rates, even after controlling for incident characteristics. ","Equity in Resident Crowdsourcing: Measuring Under-reporting without
  Ground Truth Data"
60,1518587777608265730,69202541,Jonathan Le Roux,"['New paper out w/ @ZhongqiuWang, G. Wichern, @shinjiw_at_cmu, ""STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency."" \n<LINK>', 'We combine a dual window size approach with DNN spectral mapping based enhancement and frame-online beamforming, reaching strong performance on a noisy reverberant SE task with algorithmic latency as low as 2 ms. https://t.co/8AoCQLz8Da']",https://arxiv.org/abs/2204.09911,"Deep learning based speech enhancement in the short-term Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window contains more samples and the frequency resolution can be higher for potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed based on the same 32 ms window size. To reduce this inherent latency, we adapt a conventional dual window size approach, where a regular input window size is used for STFT but a shorter output window is used for the overlap-add in the iSTFT, for STFT-domain deep learning based frame-online speech enhancement. Based on this STFT and iSTFT configuration, we employ single- or multi-microphone complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the RI components predicted by the DNN to conduct frame-online beamforming, the results of which are then used as extra features for a second DNN to perform frame-online post-filtering. The frequency-domain beamforming in between the two DNNs can be easily integrated with complex spectral mapping and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation results on a noisy-reverberant speech enhancement task demonstrate the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFT-domain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms. ",STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency
61,1518586230576979969,1145854474184904705,Xiang Fu,"['New paper: an end-to-end approach for learning to simulate coarse-grained molecular dynamics! \n\n- Multi-scale GNN\n- complex systems: polymers and batteries\n- very long stable rollouts\n- orders of magnitude faster than classical force fields!\n\n<LINK>\n\nüßµ1/n <LINK>', 'Molecular dynamics (MD) simulations are computationally expensive, especially for large systems with complex interactions. To simulate MD using a force field, a very short time-integration step must be used -- usually at femtosecond-level.\n\n-- speed bottlenecks!\n\n2/n https://t.co/NlCJek5mDY', 'We address the challenges of MD simulation by:\n\n- spatially, coarse-grain (CG) the physical system;\n- temporally, directly predict time-integrated acceleration without computing forces!\n\n3/n https://t.co/Q2JBFOG1k6', 'Spatially, graph clustering enables CG of any resolution!\nA single-chain polymer of ~1000 beads can be coarse-grained into 10 CG beads;\nA battery system of ~6000 atoms can be coarse-grained into ~800 CG beads.\nUseful properties are still preserved!\n\n4/n https://t.co/mWaVsXUW62', 'Temporally, each time step in our model can be as long as picosecond/nanosecond level -- &gt;10^3 steps in traditional MD!\nLong-time properties can be computed without a problem.\nThis is great! But what price do we pay?\n\n5/n', 'CG + large time integration make the dynamics uncertain and non-Markovian. Our model works with:\n\n- stochastic dynamics\n- incorporation of historical states\n- multi-scale modeling\n- post-prediction refinement using a diffusion model\n\n6/n https://t.co/0tzgyGLnKb', 'Our experiments are on realistic, practical, and complex systems. The single-chain CG polymer is a representative material design setup;\nwhile the battery system, solid polymer electrolyte, is a promising candidate in advancing Li-ion battery technology.\n\n7/n', 'The resulting model can learn from short training MD trajectories, and simulate novel testing systems for much longer stably!\n\nWe accurately recover important equilibrium and dynamical properties of the two complex systems considered, with much lower comp. cost!\n\n8/n https://t.co/E3syw6mIpj', 'ML for accelerating scientific computing has infinite potential. Tons of questions still remain:\n\nCG / large time integration not only accelerate MD but also enable learning, as they simplify the dynamics.\nThey necessarily lose information -- where is the sweet spot?\n\n9/n', 'What is the optimal way to coarse-grain?  For a given system and target quantity? How to learn a CG procedure?\n\nForce fields vs direct simulators?\n\nSee you at ICLR and MRS Spring meeting, and find out more about ML for CGMD in our paper!\n\nhttps://t.co/ltKsp1hUtJ\n\n10/n', 'Great collaboration with @xie_tian, Nathan Rebello, Bradley Olsen, and Tommi Jaakkola\n\n11/n, n=11.']",https://arxiv.org/abs/2204.10348,"Molecular dynamics (MD) simulation is the workhorse of various scientific domains but is limited by high computational cost. Learning-based force fields have made major progress in accelerating ab-initio MD simulation but are still not fast enough for many real-world applications that require long-time MD simulation. In this paper, we adopt a different machine learning approach where we coarse-grain a physical system using graph clustering, and model the system evolution with a very large time-integration step using graph neural networks. A novel score-based GNN refinement module resolves the long-standing challenge of long-time simulation instability. Despite only trained with short MD trajectory data, our learned simulator can generalize to unseen novel systems and simulate for much longer than the training trajectories. Properties requiring 10-100 ns level long-time dynamics can be accurately recovered at several-orders-of-magnitude higher speed than classical force fields. We demonstrate the effectiveness of our method on two realistic complex systems: (1) single-chain coarse-grained polymers in implicit solvent; (2) multi-component Li-ion polymer electrolyte systems. ","Simulate Time-integrated Coarse-grained Molecular Dynamics with
  Geometric Machine Learning"
62,1518390203366666241,9888672,Peter Rohde,['New paper with @YingkaiOuyang ‚Äî ‚ÄúA general framework for the composition of quantum homomorphic encryption &amp; quantum error correction‚Äù.\n\n<LINK>'],https://arxiv.org/abs/2204.10471,"Two essential primitives for universal, cloud-based quantum computation with security based on the laws of quantum mechanics, are quantum homomorphic encryption with information-theoretic security and quantum error correction. The former enables information-theoretic security of outsourced quantum computation, while the latter allows reliable and scalable quantum computations in the presence of errors. Previously these ingredients have been considered in isolation from one another. By establishing group-theoretic requirements that these two ingredients must satisfy, we provide a general framework for composing them. Namely, a quantum homomorphic encryption scheme enhanced with quantum error correction can directly inherit its properties from its constituent quantum homomorphic encryption and quantum error correction schemes. We apply our framework to both discrete- and continuous-variable models for quantum computation, such as Pauli-key and permutation-key encryptions in the qubit model, and displacement-key encryptions in a continuous-variable model based on Gottesman-Kitaev-Preskill codes. ","A general framework for the composition of quantum homomorphic
  encryption \& quantum error correction"
63,1517582191462469632,438512626,Jerry Li,"['Slightly belated, but happy to announce a new paper with @sitanch, Brice Huang, and Allen Liu. We give tight lower bounds for basic quantum property testing problems without quantum memory.\n\n<LINK>\n\ntl;dr Non-adaptivity is all you need, a thread ü§™ 1/n', 'We consider mixedness testing and state certification, which are the natural quantum analogues of uniformity and identity testing, respectively. For mixedness testing of a d-dim state, @BooleanAnalysis and John Wright showed that Œò(d) samples are sufficient and necessary. 2/n', ""But their tester needs heavily entangled measurements, which makes their tester impractical to implement on existing quantum devices.üò≠\n\nSo what if we only consider testers that don't use entangled measurements, i.e., only measure one copy of the state at a time? ü§î3/n"", 'It\'s ""folklore"" that if these measurements are specified ahead of time, that you need Œò(d^3/2) samples. More recently, @SebastienBubeck, @sitanch, and myself showed that even adaptive algorithms require Œ©(d^4/3) samples.\n\nBut this left the question: does adaptivity help? 4/n', 'We show that the answer is no: adaptive algorithms still need Œò(d^3/2) samples. We construct a new hard instance based on Gaussian perturbations. We do this because unlike previous instances, the likelihood ratio for this instance has a really elegant self-similar structure. 5/n https://t.co/XOcwJHahiy', 'This lets us boil the question down to controlling the behavior of a certain matrix martingale, which we can do using well-known techniques from matrix concentration inequalities. ü§Ø 6/n', 'For state certification, we generalize the prior instance-optimal bounds of @BooleanAnalysis, @sitanch, and myself, and show that they hold for arbitrary adaptive measurements, not just non-adaptive ones. Here too, you seem to get the same bounds with and without adaptivity. 7/n', ""It's pretty fascinating: even though proving lower bounds against adaptive algorithms is often really hard, I don't know of any natural quantum state learning setting where it actually gives any advantage. Hence, non-adaptivity is all you need????? ü§îü§îü§î 8/n"", ""Big shout-out to all my co-authors, they are all insanely talented. Fun fact: Brice and Allen didn't know any quantum at all, and within a month or so of working with them, we had a working proof of this lower bound, which had eluded @sitanch and me for nearly 2 years...üôÉ 9/9""]",https://arxiv.org/abs/2204.07155,"We consider the problem of quantum state certification, where we are given the description of a mixed state $\sigma \in \mathbb{C}^{d \times d}$, $n$ copies of a mixed state $\rho \in \mathbb{C}^{d \times d}$, and $\varepsilon > 0$, and we are asked to determine whether $\rho = \sigma$ or whether $\| \rho - \sigma \|_1 > \varepsilon$. When $\sigma$ is the maximally mixed state $\frac{1}{d} I_d$, this is known as mixedness testing. We focus on algorithms which use incoherent measurements, i.e. which only measure one copy of $\rho$ at a time. Unlike those that use entangled, multi-copy measurements, these can be implemented without persistent quantum memory and thus represent a large class of protocols that can be run on current or near-term devices. For mixedness testing, there is a folklore algorithm which uses incoherent measurements and only needs $O(d^{3/2} / \varepsilon^2)$ copies. The algorithm is non-adaptive, that is, its measurements are fixed ahead of time, and is known to be optimal for non-adaptive algorithms. However, when the algorithm can make arbitrary incoherent measurements, the best known lower bound is only $\Omega (d^{4/3} / \varepsilon^2)$ [Bubeck-Chen-Li '20], and it has been an outstanding open problem to close this polynomial gap. In this work, 1) we settle the copy complexity of mixedness testing with incoherent measurements and show that $\Omega (d^{3/2} / \varepsilon^2)$ copies are necessary, and 2) we show the instance-optimal bounds for state certification to general $\sigma$ first derived by [Chen-Li-O'Donnell '21] for non-adaptive measurements also hold for arbitrary incoherent measurements. Qualitatively, our results say that adaptivity does not help at all for these problems. Our results are based on new techniques that allow us to reduce the problem to understanding certain matrix martingales, which we believe may be of independent interest. ","Tight Bounds for Quantum State Certification with Incoherent
  Measurements"
64,1517544315681185792,740997024,Mike Byrne,"['New paper with colleague Phil Kortum &amp; students on BMD ballot verification:\n\n‚ÄúCan Voters Detect Errors on Their Printed Ballots? Absolutely.‚Äù\n\nSubmitted for peer review, preprint available on arXiv:\n\n<LINK>']",https://arxiv.org/abs/2204.09780,"There is still debate on whether voters can detect malicious changes in their printed ballot after making their selections on a Ballot Marking Device (BMD). In this study, we altered votes on a voter's ballot after they had made their selections on a BMD. We then required them to examine their ballots for any changes from the slate they used to vote. Overall accuracy was exceptionally high. Participants saw 1440 total contests, and of those 1440, there were a total of 4 errors, so total accuracy was 99.8%. Participants were able to perform with near-perfect accuracy regardless of ballot length, ballot type, number of altered races, and location of altered races. Detection performance was extremely robust. We conclude that with proper direction and resources, voters can be near-perfect detectors of ballot changes on printed paper ballots after voting with a BMD. This finding has significant implications for the voting community as BMD use continues to grow. Research should now focus on identifying administrative and behavioral methods that will prompt and encourage voters to check their BMD-generated ballots before they drop them in the ballot box. ",Can Voters Detect Errors on Their Printed Ballots? Absolutely
65,1517451185019572226,216729597,Marcel S. Pawlowski,"[""New paper on the arXiv, and it's an exciting one! \n\nWe studied a possible joint explanation for the formation of *both* planes of satellite galaxies in the Local Group, the one of the Milky Way and Andromeda's, via a past MW-M31 encounter ‚Ä¶ in MOND.\n\n<LINK> <LINK>"", ""You might've heard of the planes of satellite galaxies problem of cosmology. The finding that e.g. the Milky Way satellite galaxies are distributed, and preferentially rotate, in a flattened structure. A similar structure was found for Andromeda (M31). https://t.co/ZZcZNDFE31"", 'Such coherent arrangements are exceedingly rare for satellites in cosmological simulations based on the standard ŒõCDM model.\n\nA possible source of coherently orbiting satellites are tidal dwarf galaxies. Those should be free of DM in ŒõCDM, but would look like they had DM in MOND.', 'Prime candidates for the interacting galaxies producing tidal debris is a past fly-by encounter of the MW and M31. Using the Phantom of RAMSES code, such a collision was simulated in MOND, to see if the resulting debris are oriented similar to the observed satellite planes. https://t.co/W0B86uZlaz', ""Wait, what? A past MW-M31 encounter?\n\nIn MOND, since all mass is in baryons, you can calculate the MW-M31 orbit backwards (similar to the Timing Argument, but no freedom to add DM). Turns out that due to the stronger acceleration, there must've been a past pericenter; a fly-by. https://t.co/n5WbAyBHWZ"", ""The orbit that works best even turns out to have a MW-M31 proper motion consistent with observations, even though this wasn't taken as a constraint in building the model. A nice consistency giving more credibility to this approach. https://t.co/amun4TqJbX"", 'So, can the tidal debris reproduce the observed satellite galaxy coherence? Yes!\nThe orbital poles of the observed, brightest MW satellites (left) cluster, indicating that they co-orbit. The tidal debris in the simulation (right) cluster similarly, and in the same direction! https://t.co/ozlIPx72AZ', ""Similarly, there are tidal debris around the M31 analog of the simulation that also cluster where the expected orbital pole of Andromeda's satellite galaxy plane points (pink cross). https://t.co/b9siki79CH"", ""There's a lot more the paper talks about, but I'll leave it at this here and recommend you read it if interested in the details of the simulation and its parameters, the approach, MOND, or the resulting distribution, mass and angular momenta of the debris: https://t.co/MWObSrZuVs"", ""There's of course a lot still left to be done, for example forming actual tidal dwarfs from the debris, forming enough of them, or checking if they could show sufficiently old stellar populations and low metallicities to be consistent."", ""So, we have a possible explanation of the orientation of both satellite planes in the Local Group (if we live in a MOND universe). There was no reason this had to work, so showing it is possible is a big deal! Doesn't prove the scenario, but definitely encourages further work."", ""@maximetrebitsch @benfamaey Thanks! Since these are early galaxies, they should have a rather high gas fraction. And especially for the tidal debris it is important to model the disks  to large radii, where there's gas but no stars."", ""@maximetrebitsch @benfamaey Those gas disks might experience some hydro-specific effects during the encounter that a pure N-body model wouldn't catch ‚Ä¶  though I believe the results for an analog, purely stellar model should be similar at this stage."", ""@maximetrebitsch @benfamaey Of course a full model with cooling, SF, feedback would be better. That's also needed to potentially form actual tidal dwarfs. But one step at a time. ;-)"", ""@mschaller_ @benfamaey I'm afraid I'll have to disappoint you: there aren't any new galaxies formed in this simulation, so nothing to show for the MDAR. Except maybe the two initial disk galaxies, which however are set up to be stable in MOND, so they'll follow the MDAR by definition.""]",https://arxiv.org/abs/2204.09687,"The existence of mutually correlated thin and rotating planes of satellite galaxies around both the Milky Way (MW) and Andromeda (M31) calls for an explanation. Previous work in Milgromian dynamics (MOND) indicated that a past MW-M31 encounter might have led to the formation of these satellite planes. We perform the first-ever hydrodynamical MOND simulation of the Local Group using Phantom of RAMSES. We show that an MW-M31 encounter at $z \approx 1$, with a perigalactic distance of about 80 kpc, can yield two disc galaxies at $z=0$ oriented similarly to the observed galactic discs and separated similarly to the observed M31 distance. Importantly, the tidal debris are distributed in phase space similarly to the observed MW and M31 satellite planes, with the correct preferred orbital pole for both. The MW-M31 orbital geometry is consistent with the presently observed M31 proper motion despite this not being considered as a constraint when exploring the parameter space. The mass of the tidal debris around the MW and M31 at $z=0$ compare well with the mass observed in their satellite systems. The remnant discs of the two galaxies have realistic radial scale lengths and velocity dispersions, and the simulation naturally produces a much hotter stellar disc in M31 than in the MW. However, reconciling this scenario with the ages of stellar populations in satellite galaxies would require that a higher fraction of stars previously formed in the outskirts of the progenitors ended up within the tidal debris, or that the MW-M31 interaction occurred at $z>1$. ","3D hydrodynamic simulations for the formation of the Local Group
  satellite planes"
66,1517444801846812673,1416441264120160263,David Bethge,"['Happy to announce our new paper accepted at IEEE EMBC. \n\n""Exploiting Multiple EEG Data Domains with Adversarial Learning""\n\nJoint work with Philipp Hallgarten,  @oozdenizci , Albrecht Schmidt &amp; Ralf Mikut \n\nPaper preprintüìÑ: <LINK>\n\nCodeüë®\u200düíª: <LINK>', '[1/3] \nWe argue that multi-source learning via learning domain-invariant representations from multiple data-sources is a viable alternative, as the available data from different EEG data-source domains (e.g., subjects, sessions, experimental setups) grow massively.', '[2/3]\nWe propose an adversarial inference approach to learn data-source invariant representations, enabling multi-source learning for EEG-based brain-computer interfaces. We present a unifying EEG pre-processing framework for fusing different raw EEG time-series datasets. https://t.co/AaVlNksqLU', '[3/3]\nWe demonstrate the feasibility of our invariant representation learning approach in suppressing data- source-relevant information leakage by 35% while still achieving stable EEG-based emotion classification performance.']",https://arxiv.org/abs/2204.07777,"Electroencephalography (EEG) is shown to be a valuable data source for evaluating subjects' mental states. However, the interpretation of multi-modal EEG signals is challenging, as they suffer from poor signal-to-noise-ratio, are highly subject-dependent, and are bound to the equipment and experimental setup used, (i.e. domain). This leads to machine learning models often suffer from poor generalization ability, where they perform significantly worse on real-world data than on the exploited training data. Recent research heavily focuses on cross-subject and cross-session transfer learning frameworks to reduce domain calibration efforts for EEG signals. We argue that multi-source learning via learning domain-invariant representations from multiple data-sources is a viable alternative, as the available data from different EEG data-source domains (e.g., subjects, sessions, experimental setups) grow massively. We propose an adversarial inference approach to learn data-source invariant representations in this context, enabling multi-source learning for EEG-based brain-computer interfaces. We unify EEG recordings from different source domains (i.e., emotion recognition datasets SEED, SEED-IV, DEAP, DREAMER), and demonstrate the feasibility of our invariant representation learning approach in suppressing data-source-relevant information leakage by 35% while still achieving stable EEG-based emotion classification performance. ",Exploiting Multiple EEG Data Domains with Adversarial Learning
67,1517409532011372544,1271852576296906755,Ashley Chrimes,"['And here it is, another new paper on arxiv today! We compare the NIR photometry of Galactic magnetar counterparts with @astroBPASS expectations for NS companions. We find one (strong) candidate for a bound companion to a magnetar! Read more here - <LINK> <LINK>']",https://arxiv.org/abs/2204.09701,"It is well established that magnetars are neutron stars with extreme magnetic fields and young ages, but the evolutionary pathways to their creation are still uncertain. Since most massive stars are in binaries, if magnetars are a frequent result of core-collapse supernovae, some fraction are expected to have a bound companion at the time of observation. In this paper, we utilise literature constraints, including deep Hubble Space Telescope imaging, to search for bound stellar companions to magnetars. The magnitude and colour measurements are interpreted in the context of binary population synthesis predictions. We find two candidates for stellar companions associated with CXOU J171405.7-381031 and SGR 0755-2933, based on their J-H colours and H-band absolute magnitudes. Overall, the proportion of the Galactic magnetar population with a plausibly stellar near-infrared counterpart candidate, based on their magnitudes and colours, is between 5 and 10 per cent. This is consistent with a population synthesis prediction of 5 per cent, for the fraction of core-collapse neutron stars arising from primaries which remain bound to their companion after the supernova. These results are therefore consistent with magnetars being drawn in an unbiased way from the natal core-collapse neutron star population, but some contribution from alternative progenitor channels cannot be ruled out. ","Where are the magnetar binary companions? Candidates from a comparison
  with binary population synthesis predictions"
68,1517327046132834305,968896771,Kevin J. Black,['A new paper from my favorite computational astrophysicist\n<LINK>'],https://arxiv.org/abs/2204.10141,"Precision-era optical cluster cosmology calls for a precise definition of the red sequence (RS), consistent across redshift. To this end, we present the Red Dragon algorithm: an error-corrected multivariate Gaussian mixture model (GMM). Simultaneous use of multiple colors and smooth evolution of GMM parameters result in a continuous RS and blue cloud (BC) characterization across redshift, avoiding the discontinuities of red fraction inherent in swapping RS selection colors. Based on a mid-redshift spectroscopic sample of SDSS galaxies, a RS defined by Red Dragon selects quenched galaxies (low specific star formation rate) with a balanced accuracy of over 90%. This approach to galaxy population assignment gives more natural separations between RS and BC galaxies than hard cuts in color--magnitude or color--color spaces. The Red Dragon algorithm is publicly available at bitbucket.org/wkblack/red-dragon-gamma. ",Red Dragon: A Redshift-Evolving Gaussian Mixture Model for Galaxies
69,1517308234004058117,17993000,Zach Shahn,"['Sorry @KhoaVuUmn. In our new ""working paper"" (are non-economists allowed to have working papers #EconTwitter?), we show that Structural Nested Mean Models are identified under time-varying conditional parallel trends assumptions. (This is good!) üßµ  <LINK> <LINK>', 'Why does this matter? 1) SNMMs are models of time-varying treatment effect heterogeneity. So now we can learn how effects vary as a function of time-varying covariates under DiD type assumptions.', ""2) We only require parallel trends to hold conditional on time-varying covariates. So if there's a measured time-varying confounder of trends, you can adjust for it under our approach."", ""3) We can identify additional causal contrasts, eg the effect of one blip of treatment followed by no further treatment, beyond the ITT effect of an initial blip like @CdeChaisemartin and D'Haultfoeuille or the effect of sustained treatment like Callaway and @pedrohcgs"", '4) Our estimating equations are doubly robust and allow ML estimation of nuisance functions with cross-fitting', 'So this seems to be that mythical free lunch where you can estimate more stuff than the other time-varying DiD methods under weaker assumptions? (Except that in practice to reap some of these rewards you will prob need to correctly specify a parametric blip model.)', ""But we're interlopers in econometrics and the paper is still a work in progress, so I'm really hoping for both feedback and pushback from people like @pedrohcgs, @jmwooldridge, @CdeChaisemartin, @Susan_Athey, and others in what I understand to be the working paper tradition"", 'Will add simulations and real data analysis among other things soon, just had to put this out before one of you econ jackals took our idea! Also lots of directions for future work opened up by SNMMs. We mention some in the paper.', ""Very joint work with Jamie Robins, Oliver Dukes, David Richardson, and Eric Tchetgen Tchetgen. I don't think any of them are (publicly) on twitter though."", '@matt_blackwell @a_strezh @KhoaVuUmn Looking forward to reading it!']",https://arxiv.org/abs/2204.10291,"In this paper, we generalize methods in the Difference in Differences (DiD) literature by showing that both additive and multiplicative standard and coarse Structural Nested Mean Models (Robins, 1994, 1997, 1998, 2000, 2004; Lok and Degruttola, 2012; Vansteelandt and Joffe, 2014) are identified under parallel trends assumptions. Our methodology enables adjustment for time-varying covariates, identification of effect heterogeneity as a function of time-varying covariates, and estimation of treatment effects under a general class of treatment patterns (e.g. we do not restrict to the `staggered adoption' setting). We stress that these extensions come essentially for free, as our parallel trends assumption is not stronger than other parallel trends assumptions in the DiD literature. However, in contrast to much of the DiD literature, we only consider panel data, not repeated cross sectional data. ",Structural Nested Mean Models Under Parallel Trends Assumptions
70,1517135120821456899,2918290813,Dr./Prof. Sam Lawler,"['New paper thread!  @OSSOSurvey XXV (25?!! Wow) \n\nLed by now-MSc student Breanna Crompvoets, from work she did for her honours thesis with me last year.\n\nBasically, there are a LOT of TNOs in the very distant, very high-order resonances. Like, a LOT.\n\n<LINK>', '@OSSOSurvey Co-authors include @astrokiwi @kat_volk @Mikea1985 @jjkavelaars @StephenGwynCADC', 'We looked at the populations and orbital distributions of the really distant Neptune mean-motion resonances. Which are completely beautiful: https://t.co/EbP2aAaZOi', ""The @OSSOSurvey had such beautifully precise orbital measurements that we were able to securely measure resonance in ridiculously high order resonances like the 13:6 and 35:8 resonances (yes, 35:8, that's just crazy)"", 'And because the @OSSOSurvey was well-characterized (we know all our observation biases really well), we can effectively de-bias our measurements. \n\nWe found one TNO in the 35:8 resonance: how many TNOs need to be in that resonance for OSSOS to have a good chance of detecting one?', ""We have huge error bars on these population estimates, of course, because with only one TNO in many of these resonances, there's a lot of uncertainty. \n\nBut this is the very first time we've been able to measure most of this super-distant resonant population!"", ""We measure (with very large error bars) that there are about 100,000 TNOs bigger than 100km across in these distant resonances.  \n\nThat's about the same number of TNOs that are in the classical Kuiper Belt!  \n\nAnd that's...actually pretty weird."", 'How did all those TNOs get out there? The published population model that does the best job reproducing the resonant population ratios we measure is a careful analysis of scattering TNOs that temporary stick in the distant resonances: https://t.co/pg7IaqDeUs', 'BUT, they predict about an order of magnitude lower total population.  \n\nSo... I guess this is where we say to modellers: get to work!  Make more solar system models that match our measurements!', 'How did all these TNOs get way out into these distant resonances? \n\nThis new set of careful observations is telling us some important clue about the past history of our solar system, and now we need to build some theoretical models to match this new data.  Science!', 'But will we get to do additional precise measurements?\n\nEvery couple of weeks, more bright satellites are launched, making it even harder for everyone on Earth to access the night sky, including us research astronomers.\n\nMake the satellites fainter, and launch fewer, PLEASE. https://t.co/NAsQHMpwpf', ""@rappolee @kat_volk We didn't try to add in any additional planets for this paper, the distant resonances are neat enough on their own!""]",https://arxiv.org/abs/2204.09139,"There have been 77 TNOs discovered to be librating in the distant transneptunian resonances (beyond the 2:1 resonance, at semimajor axes greater than 47.7~AU) in four well-characterized surveys: the Outer Solar System Origins Survey (OSSOS) and three similar prior surveys. Here we use the OSSOS Survey Simulator to measure their intrinsic orbital distributions using an empirical parameterized model. Because many of the resonances had only one or very few detections, $j$:$k$ resonant objects were grouped by $k$ in order to have a better basis for comparison between models and reality. We also use the Survey Simulator to constrain their absolute populations, finding that they are much larger than predicted by any published Neptune migration model to date; we also find population ratios that are inconsistent with published models, presenting a challenge for future Kuiper Belt emplacement models. The estimated population ratios between these resonances are largely consistent with scattering-sticking predictions, though further discoveries of resonant TNOs with high-precision orbits will be needed to determine whether scattering-sticking can explain the entire distant resonant population or not. ","OSSOS XXV: Large Populations and Scattering-Sticking in the Distant
  Transneptunian Resonances"
71,1517039759071186951,547776192,Chris Lovell,"[""New paper on the arXiv today led by @stewilkins looking at 'the redshift frontier' (z &gt; 10) in FLARES ‚ú®\n\nWe compare to the few existing constraints, and make predictions for the properties of galaxies (intrinsic and observed) accessible to @NASAWebb \n\n<LINK>""]",https://arxiv.org/abs/2204.09431,"The James Webb Space Telescope (JWST) is set to transform many areas of astronomy, one of the most exciting is the expansion of the redshift frontier to $z>10$. In its first year alone JWST should discover hundreds of galaxies, dwarfing the handful currently known. To prepare for these powerful observational constraints, we use the First Light And Reionisation Epoch (FLARES) simulations to predict the physical and observational properties of the $z>10$ population of galaxies accessible to JWST. This is the first time such predictions have been made using a hydrodynamical model validated at low redshift. Our predictions at $z=10$ are broadly in agreement with current observational constraints on the far-UV luminosity function and UV continuum slope $\beta$, though the observational uncertainties are large. We note tension with recent constraints $z\sim 13$ from Harikane et al. 2022 - compared to these constraints, FLARES predicts objects with the same space density should have an order of magnitude lower luminosity, though this is mitigated slightly if dust attenuation is negligible in these systems. Our predictions suggest that in JWST's first cycle alone, around $600$ galaxies should be identified at $z>10$, with the first small samples available at $z>13$. ","First Light And Reionisation Epoch Simulations (FLARES) V: The redshift
  frontier"
72,1516915846584713217,1511475735378202627,Ren Hirayam,"[""new paper out of the oven! <LINK>\n\nThis is the (long overdue) paper based on my Master's thesis, where we developed a fast method to compute conserved charge cumulants in hydro simulations.""]",https://arxiv.org/abs/2204.07181,We introduce a fast and simple method of computing cumulants of net-proton or net-charge fluctuations in event-by-event hydrodynamic simulations of heavy-ion collisions. One evaluates the mean numbers of particles in every hydrodynamic event. Cumulants are then expressed as a function of these mean numbers. We implement the corrections due to global conservation laws. The method is tested using ideal hydrodynamic simulations of Au+Au collisions at $\sqrt{s_{NN}}=200$ AGeV with the NeXSPheRIO code. Results are in good agreement with experimental data on net-proton and net-charge fluctuations by the STAR collaboration. ,Cumulants of conserved charges in hydrodynamic simulations
73,1516791410380447749,2289913051,Linus Schumacher,"['New @arxiv preprint from our group: \n""Regulation of stem cell dynamics through volume exclusion""\nfirst PhD paper from @RorroAGarcia with me and @ramon_grima \n<LINK>\n\nLet us know what you think ‚Äì feedback welcome!']",https://arxiv.org/abs/2204.09004,"Maintenance and regeneration of adult tissues rely on the self-renewal of stem cells. Regeneration without over-proliferation requires precise regulation of the stem cell proliferation and differentiation rates. The nature of such regulatory mechanisms in different tissues, and how to incorporate them in models of stem cell population dynamics, is incompletely understood. The critical birth-death (CBD) process is widely used to model stem cell populations, capturing key phenomena, such as population asymmetry and scaling laws in clone size distributions. However, the CBD process neglects regulatory mechanisms. Here, we propose the birth-death process with volume exclusion (vBD), a variation of the birth-death process that takes into account crowding effects, such as may arise due to limited space in a stem cell niche. While the deterministic rate equations predict a single non-trivial attracting steady state, the master equation predicts extinction and a transient distribution of stem cell numbers that can be bimodal. We investigate the accuracy of the system-size expansion (including finite size corrections to the linear-noise approximation), the quasi-steady state approximation, and the WKB method to approximate the probability distribution solution of the vBD master equation, as well as the mean extinction time. Our study suggests that the size distribution of a stem cell population and its extinction dynamics bear signatures that may be useful to detect negative feedback mediated via volume exclusion. ",Regulation of stem cell dynamics through volume exclusion
74,1516688376480649218,737271507513188352,Boris Goncharov,['A new paper with Alex and Jan where we discuss problems in ground-based gravitational-wave astronomy in the next decade and solutions provided by the null stream of Einstein Telescope: <LINK>\nImage: illustration of the null stream formed by three interferometers. <LINK>'],https://arxiv.org/abs/2204.08533,"Among third-generation ground-based gravitational-wave detectors proposed for the next decade, Einstein Telescope provides a unique kind of null stream $\unicode{x2014}$ the signal-free linear combination of data $\unicode{x2014}$ that enables otherwise inaccessible tests of the noise models. We project and showcase challenges in modeling the noise in the 2030-s and how it will affect the performance of third-generation detectors. We find that the null stream of Einstein Telescope is capable of entirely eliminating transient detector glitches that are known to limit current gravitational-wave searches. The techniques we discuss are computationally efficient and do not require a-priori knowledge about glitch models. Furthermore, we show how the null stream can be used to provide an unbiased estimation of the noise power spectrum necessary for online and offline data analyses even with multiple loud signals in band. We overview other approaches to utilizing the null stream. Finally, we comment on the limitations and future challenges of null stream analyses for Einstein Telescope and arbitrary detector networks. ",Utilizing the null stream of Einstein Telescope
75,1516659673897877506,2742636202,Balazs Vedres,"[""Gender diversity contributes to team creativity, but only when female team members are included in the team's network. See our new paper with Orsolya Vasarhelyi: <LINK> <LINK>"", '@ulfaslak All three contribute to higher creativity, the middle one contributes most (bonding: stronger ties between male and female developers).']",https://arxiv.org/abs/2204.08505,"Diversity in teams can boost creativity, and gender diversity was shown to be a contributor to collective creativity. We show that gender diversity requires inclusion to lead to benefits in creativity by analyzing teams in 4011 video game projects. Recording data on the weighted network from past collaborations, we developed four measures of inclusion, depending on a lack of segregation, strong ties across genders, and the incorporation of women into the core of the team s network. We found that gender diversity without inclusion does not contribute to creativity, while with maximal inclusion one standard deviation change in diversity results in .04 to .09 standard deviation change in creativity, depending on the measure of inclusion. To reap creative benefits of diversity, developer firms need to include 23 percent or more female developers (as opposed to the 15 percent mean female proportion) and include them in the team along all dimensions. Inclusion at low diversity has a negative effect. By analyzing the sequences of diversity and inclusion across games within firms, we found that adding diversity first, and developing inclusion later can lead to higher diversity and inclusion, compared to adding female developers with already existing cross-gender ties to the team. ",Inclusion unlocks the creative potential of gender diversity in teams
76,1516635723520409600,1089941781586808832,Jemin hwangbo,"['Our RSS 2022 paper is on Arxiv now. In this paper, we introduce a new sample-efficient navigation planner, which is based on a learned model of a quadruped robot!\n\nVideo: \n<LINK>\n\nPaper: <LINK>']",http://arxiv.org/abs/2204.08647,"For autonomous quadruped robot navigation in various complex environments, a typical SOTA system is composed of four main modules -- mapper, global planner, local planner, and command-tracking controller -- in a hierarchical manner. In this paper, we build a robust and safe local planner which is designed to generate a velocity plan to track a coarsely planned path from the global planner. Previous works used waypoint-based methods (e.g. Proportional-Differential control and pure pursuit) which simplify the path tracking problem to local point-goal navigation. However, they suffer from frequent collisions in geometrically complex and narrow environments because of two reasons; the global planner uses a coarse and inaccurate model and the local planner is unable to track the global plan sufficiently well. Currently, deep learning methods are an appealing alternative because they can learn safety and path feasibility from experience more accurately. However, existing deep learning methods are not capable of planning for a long horizon. In this work, we propose a learning-based fully autonomous navigation framework composed of three innovative elements: a learned forward dynamics model (FDM), an online sampling-based model-predictive controller, and an informed trajectory sampler (ITS). Using our framework, a quadruped robot can autonomously navigate in various complex environments without a collision and generate a smoother command plan compared to the baseline method. Furthermore, our method can reactively handle unexpected obstacles on the planned path and avoid them. Project page this https URL ","Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe
  Quadruped Navigation"
77,1516635314420387845,4898302213,Chris Lindsay,"['Today (4/20) on the Arxiv: Our new paper detailing a modeling study investigating how the prescription and amount of convective overshoot beneath red giant convective envelopes noticeably changes asteroseismic observables.\n<LINK>', 'How do we probe the structure of heavenly bodies we can‚Äôt directly measure? More than a century ago, scientists used information contained in earthquake-induced sound waves propagating though the Earth to determine the basic interior structure of our planet. SEISMOLOGY! https://t.co/8NusWMfYyi', 'Convective motions in stars make them ring, just like how earthquakes generate sound waves which propagate through the Earth. Using data from @nasa‚Äôs Kepler or TESS missions, we can measure the frequencies and amplitudes of these waves to learn about stellar interior structure. https://t.co/CSwo9jJ55O', 'Red giant stars have thick, well-mixed, convective envelopes surrounding a thin Hydrogen burning shell encompassing a Helium core. The momentum of convective fluid blobs is expected to carry them past the expected convective boundaries in a process called convective overshoot. https://t.co/0JdoQQpSwj', 'Treatment of overshoot in stellar models is important since properties like age, derived for field stars, are model dependent. Envelope overshoot has also been shown to match modeled position of the red giant branch luminosity bump up with observed over densities in red giants. https://t.co/SP14b5OvQE', 'Through detailed stellar modeling, we seek to investigate how different prescriptions and amounts of overshoot change the seismic properties of red giants. We find that the stellar models‚Äô gravity mode phase offset parameter goes through strong variations on the red giant branch. https://t.co/8Y2wv7id7E', 'Incorporating envelope overshoot of varying amplitude changes the location of these strong gravity mode phase offset variations. Two red giant models with different overshoot amplitudes could be in the same location in the H-R diagram but have different internal structures.', 'We find that these strong variations, if observed in ensemble data of red giants, could be used to constrain the amount of envelope overshoot which should be incorporated into red giant stellar models!\nCheck out the paper for more asteroseismic goodness!', 'Ask @darthoctopus any questions about the math üòå']",https://arxiv.org/abs/2204.08485,"Most current models of low mass red giant stars do not reproduce the observed position of the red giant branch luminosity bump, a diagnostic of the maximum extent of the convective envelope during the first dredge up. Global asteroseismic parameters, the large frequency separation and frequency of maximum oscillation power, measured for large samples of red giants, show that modeling convective overshoot below the convective envelope helps match the modeled luminosity bump positions to observations. However, these global parameters cannot be used to probe envelope overshoot in a star-by-star manner. Red giant mixed modes, which behave like acoustic modes at the surface and like gravity modes in the core, contain important information about the interior structure of the star, especially near the convective boundary. Therefore, these modes may be used to probe interior processes, such as overshoot. Using a grid of red giant models with varying mass, metallicity, surface gravity, overshoot treatment, and amount of envelope overshoot, we find that changing the overshoot amplitude (and prescription) of overshoot below the convection zone in red giant stellar models results in significant differences in the evolution of the models' dipole mixed-mode oscillation frequencies, the average mixed mode period spacing, $\langle \Delta P \rangle$, and gravity mode phase offset term, $\epsilon_g$. ","Mixed Mode Asteroseismology of Red Giant Stars Through the Luminosity
  Bump"
78,1516547055631671304,1264296748668813317,Shakti Kumar,"[""Preprint of our work‚Äì GDC for Few-Shot Learning is now available at <LINK>\nPaper recording from our talk at @mcgillu can also be found at <LINK> Check out how we're setting new records in #FewShotLearning !\n#MachineLearning #Statistics""]",https://arxiv.org/abs/2204.05230,"Few shot learning is an important problem in machine learning as large labelled datasets take considerable time and effort to assemble. Most few-shot learning algorithms suffer from one of two limitations- they either require the design of sophisticated models and loss functions, thus hampering interpretability; or employ statistical techniques but make assumptions that may not hold across different datasets or features. Developing on recent work in extrapolating distributions of small sample classes from the most similar larger classes, we propose a Generalized sampling method that learns to estimate few-shot distributions for classification as weighted random variables of all large classes. We use a form of covariance shrinkage to provide robustness against singular covariances due to overparameterized features or small datasets. We show that our sampled points are close to few-shot classes even in cases when there are no similar large classes in the training set. Our method works with arbitrary off-the-shelf feature extractors and outperforms existing state-of-the-art on miniImagenet, CUB and Stanford Dogs datasets by 3% to 5% on 5way-1shot and 5way-5shot tasks and by 1% in challenging cross domain tasks. ",GDC- Generalized Distribution Calibration for Few-Shot Learning
79,1516483733427081225,2848291100,Megan Leszczynski,"['New preprint alert! üì£ \n\nHow do we improve long-tailed performance of entity retrieval? We use a supervised contrastive loss to *geometrically encode entity types* in representation space w/ bi-encoders. Check out our paper on TABi!\n\nüìú <LINK>\n\nDetailsüëá (1/n) <LINK>', 'We know types can improve the long-tailed performance of entity retrieval, but hard to achieve in an *open-domain* setting (e.g., Siri, QA). We show that encoding types w/ a supervised contrastive loss can improve long-tail perf, without sacrificing popular entity perf. (2/n) https://t.co/tDRB84lvVh', 'TABi:\na) improves top-1 retrieval over the tail by nearly 6 points on AmbER,\nb) outperforms SoTA models for retrieval on open-domain tasks in KILT, \nc) is robust to missing types or incorrect types (we need just 5% coverage, and can handle up to 75% noise)\n\n(3/n) https://t.co/1ugYWWjZSs', 'Check out our code on GitHub and our blog post to learn more! (4/n)\n\nüíª code: https://t.co/NA1x8wixA6 \nüåç blog: https://t.co/uHcVGBsjT3', 'This paper will appear in Findings of the ACL 2022. See you in Dublin! w/ @realDanFu, @MayeeChen, @HazyResearch. Huge thanks to @StanfordAILab and @StanfordHAI for the resources to make this happen! (5/5)', 'And check out @realDanFu‚Äôs thread on how this work fits into a broader line of work on advances in understanding, improving, and applying contrastive learning!\n(6/5)\nhttps://t.co/uCr5SCcwaH']",https://arxiv.org/abs/2204.08173,"Entity retrieval--retrieving information about entity mentions in a query--is a key step in open-domain tasks, such as question answering or fact checking. However, state-of-the-art entity retrievers struggle to retrieve rare entities for ambiguous mentions due to biases towards popular entities. Incorporating knowledge graph types during training could help overcome popularity biases, but there are several challenges: (1) existing type-based retrieval methods require mention boundaries as input, but open-domain tasks run on unstructured text, (2) type-based methods should not compromise overall performance, and (3) type-based methods should be robust to noisy and missing types. In this work, we introduce TABi, a method to jointly train bi-encoders on knowledge graph types and unstructured text for entity retrieval for open-domain tasks. TABi leverages a type-enforced contrastive loss to encourage entities and queries of similar types to be close in the embedding space. TABi improves retrieval of rare entities on the Ambiguous Entity Retrieval (AmbER) sets, while maintaining strong overall retrieval performance on open-domain tasks in the KILT benchmark compared to state-of-the-art retrievers. TABi is also robust to incomplete type systems, improving rare entity retrieval over baselines with only 5% type coverage of the training dataset. We make our code publicly available at this https URL ",TABi: Type-Aware Bi-Encoders for Open-Domain Entity Retrieval
80,1516271391787536386,714535792366981121,Chris,"['Hi folks, we have a new submitted paper! In short, we estimate scaleheights and ages of low-mass stars and brown dwarfs in deep fields. We find that M dwarfs are older than L dwarfs as a population, in agreement with models. <LINK> <LINK>', 'We also make predictions for a similar survey with JWST called PASSAGES. With the upcoming Nancy Grace Roman Telescope  and the Vera Rubin Observatory, we will be able to see these tiny ""stars"" throughout the Galaxy.', 'The goal is to start doing galactic archeology with brown dwarfs and low-mass stars and to learn something potentially new about how our galaxy formed and evolved.', 'There are significant systematic uncertainties in evolutionary models and age-velocity dispersion relations in addition to limitations due to small sample sizes, which will be solved, in part, by the next generation of surveys.', 'Major thanks to my co-authors for being awesome ! @ChihChunHsu @browndwarfs @philosicist @astro_daniella  @RobTejada42  + others']",https://arxiv.org/abs/2204.07621,"Ultracool dwarfs represent a significant proportion of stars in the Milky Way,and deep samples of these sources have the potential to constrain the formation history and evolution of low-mass objects in the Galaxy. Until recently, spectral samples have been limited to the local volume (d<100 pc). Here, we analyze a sample of 164 spectroscopically-characterized ultracool dwarfs identified by Aganze et al. (2022) in the Hubble Space Telescope WFC3 Infrared Spectroscopic Parallel (WISP) Survey and 3D-HST. We model the observed luminosity function using population simulations to place constraints on scaleheights, vertical velocity dispersions and population ages as a function of spectral type. Our star counts are consistent with a power-law mass function and constant star formation history for ultracool dwarfs, with vertical scaleheights 249$_{-61}^{+48}$ pc for late M dwarfs, 153$_{-30}^{+56}$ pc for L dwarfs, and 175$_{-56}^{+149}$ pc for T dwarfs. Using spatial and velocity dispersion relations, these scaleheights correspond to disk population ages of 3.6$_{-1.0}^{+0.8}$ for late M dwarfs, 2.1$_{-0.5}^{+0.9}$ Gyr for L dwarfs, and 2.4$_{-0.8}^{+2.4}$ Gyr for T dwarfs, which are consistent with prior simulations that predict that L-type dwarfs are on average a younger and less dispersed population. There is an additional 1-2 Gyr systematic uncertainty on these ages due to variances in age-velocity relations. We use our population simulations to predict the UCD yield in the JWST PASSAGES survey, a similar and deeper survey to WISPS and 3D-HST, and find that it will produce a comparably-sized UCD sample, albeit dominated by thick disk and halo sources. ","Beyond the Local Volume II: Population Scaleheights and Ages of
  Ultracool Dwarfs in Deep HST/WFC3 Parallel Fields"
81,1516248815279521795,976155561522794497,Juliano C√©sar Silva Neves,"['My new paper is about laws of nature from the perspective in which laws are just descriptions. Thus, laws of nature are valid just in the observable universe. Some conclusions: the theory of everything is doomed, and there are just two types of multiverse.\n<LINK>']",https://arxiv.org/abs/2204.08413,"The theory of regularity is a philosophical perspective in which laws of nature are just descriptions, that is to say, laws of nature do not govern the world. Moreover, according to the theory of regularity, the number of laws of nature might be infinite, thus any attempt towards the theory of everything is doomed. Here I propose a special or restricted theory of regularity. The main difference as to the well-known version of that theory is both the range of validity and the scale of the laws of nature. Laws of nature ought to be considered just inside the observable universe and within certain energy and length scales. Even so I apply the theory of regularity to the multiverse scenario. As a consequence, the special theory of regularity supports only two types of multiverses by comparison with our world: those ones with a different sequence of unique events and different laws of nature and those ones with the same sequence of unique events and the same laws of nature instanced by the unique events. The latter case is some sort of eternal recurrence or a parallel eternal recurrence. ",Special theory of regularity
82,1516226735087497222,223458855,Michael A. Fedderke,"['New paper ü•≥ <LINK>\nWe explore how you could possibly do astrometric detection of low-frequency gravitational waves using a stellar interferometer in space, using as stable targets a small number of photometrically stable, distant, hot white dwarfs. 1/2', 'This is an alternative optimization of this measurement as compared to existing work on this subject that uses large-N surveys (e.g., Gaia) to search for this well-known signal. 2/2']",https://arxiv.org/abs/2204.07677,"We evaluate the potential for gravitational-wave (GW) detection in the frequency band from 10 nHz to 1 $\mu$Hz using extremely high-precision astrometry of a small number of stars. In particular, we argue that non-magnetic, photometrically stable hot white dwarfs (WD) located at $\sim$ kpc distances may be optimal targets for this approach. Previous studies of astrometric GW detection have focused on the potential for less precise surveys of large numbers of stars; our work provides an alternative optimization approach to this problem. Interesting GW sources in this band are expected at characteristic strains around $h_c \sim 10^{-17} \times \left(\mu\text{Hz}/f_{\text{GW}}\right)$. The astrometric angular precision required to see these sources is $\Delta \theta \sim h_c$ after integrating for a time $T \sim 1/f_{\text{GW}}$. We show that jitter in the photometric center of WD of this type due to starspots is bounded to be small enough to permit this high-precision, small-$N$ approach. We discuss possible noise arising from stellar reflex motion induced by orbiting objects and show how it can be mitigated. The only plausible technology able to achieve the requisite astrometric precision is a space-based stellar interferometer. Such a future mission with few-meter-scale collecting dishes and baselines of $\mathcal{O}(100\text{ km})$ is sufficient to achieve the target precision. This collector size is broadly in line with the collectors proposed for some formation-flown, space-based astrometer or optical synthetic-aperature imaging-array concepts proposed for other science reasons. The proposed baseline is however somewhat larger than the km-scale baselines discussed for those concepts, but we see no fundamental technical obstacle to utilizing such baselines. A mission of this type thus also holds the promise of being one of the few ways to access interesting GW sources in this band. ",Astrometric Gravitational-Wave Detection via Stellar Interferometry
83,1516091529193934848,1250628810,Jessy Lin,"['How can agents infer what people want from what they say?\n\nIn our new paper at #acl2022nlp w/ @dan_fried, Dan Klein, and @ancadianadragan, we learn preferences from language by reasoning about how people communicate in context.\n\nPaper: <LINK>\n[1/n] <LINK>', '@dan_fried @ancadianadragan We‚Äôd like AI agents that not only follow our instructions (‚Äúbook this flight‚Äù), but learn to generalize to what to do in new contexts (know what flights I prefer from our past interactions and book on my behalf) ‚Äî i.e., learn *rewards* from language. [2/n]', '@dan_fried @ancadianadragan The challenge is that language only reveals partial, context-dependent information about our goals and preferences (when I tell a flight booking agent I want ‚Äúthe jetblue flight,‚Äù I don‚Äôt mean I always want a jetblue flight ‚Äî just in this particular case!). [3/n] https://t.co/6n3Lg4X4CR', '@dan_fried @ancadianadragan On the other hand, we have a lot of techniques in inverse reinforcement learning to go from actions -&gt; underlying rewards, but these methods will miss the fact that language naturally communicates *why* people want those actions. [4/n]', '@dan_fried @ancadianadragan To study this, we collect a dataset of natural language in a new task, FlightPref, where one player (the ‚Äúassistant‚Äù) has to infer the preferences of the ‚Äúuser"" while they book flights together. \n\nLots of rich, interesting phenomena in the data (to be released!): [5/n] https://t.co/iknlAjY3Tx', '@dan_fried @ancadianadragan We build a pragmatic model that reasons that language communicates what agents should do, and the *way* people describe what to do reveal the features they care about. Both enable agents to make more accurate inferences. [6/n] https://t.co/p3xOJlS8KS', '@dan_fried @ancadianadragan There‚Äôs many directions to take FlightPref / reward learning from language further: building agents that learn to ask questions based on uncertainty, studying adaptation to different humans, and seeing how these ideas extend to inferring real preferences in the wild! [7/n]', '@dan_fried @ancadianadragan More broadly, it‚Äôs an exciting time to be working on language + action/RL! A lot of work has been focused on e.g. language for generalization, but our work hints at how language humans use to *communicate* present distinct challenges for grounded agents (pragmatics, etc.!). [8/8]']",https://arxiv.org/abs/2204.02515,"In classic instruction following, language like ""I'd like the JetBlue flight"" maps to actions (e.g., selecting that flight). However, language also conveys information about a user's underlying reward function (e.g., a general preference for JetBlue), which can allow a model to carry out desirable actions in new contexts. We present a model that infers rewards from language pragmatically: reasoning about how speakers choose utterances not only to elicit desired actions, but also to reveal information about their preferences. On a new interactive flight-booking task with natural language, our model more accurately infers rewards and predicts optimal actions in unseen environments, in comparison to past work that first maps language to actions (instruction following) and then maps actions to rewards (inverse reinforcement learning). ",Inferring Rewards from Language in Context
84,1516066005449158661,761626039,Stefan Feuerriegel,['New working paper: Causal Transformer for Estimating Counterfactual Outcomes. This is state-of-the-art network for estimating treatment effects over time. (We currently integrate some additional feedback and will soon post a revised version) <LINK>'],https://arxiv.org/abs/2204.07258,"Estimating counterfactual outcomes over time from observational data is relevant for many applications (e.g., personalized medicine). Yet, state-of-the-art methods build upon simple long short-term memory (LSTM) networks, thus rendering inferences for complex, long-range dependencies challenging. In this paper, we develop a novel Causal Transformer for estimating counterfactual outcomes over time. Our model is specifically designed to capture complex, long-range dependencies among time-varying confounders. For this, we combine three transformer subnetworks with separate inputs for time-varying covariates, previous treatments, and previous outcomes into a joint network with in-between cross-attentions. We further develop a custom, end-to-end training procedure for our Causal Transformer. Specifically, we propose a novel counterfactual domain confusion loss to address confounding bias: it aims to learn adversarial balanced representations, so that they are predictive of the next outcome but non-predictive of the current treatment assignment. We evaluate our Causal Transformer based on synthetic and real-world datasets, where it achieves superior performance over current baselines. To the best of our knowledge, this is the first work proposing transformer-based architecture for estimating counterfactual outcomes from longitudinal data. ",Causal Transformer for Estimating Counterfactual Outcomes
85,1514876504014041089,1440653053321891843,Luis Alfredo Anchordoqui,['New paper on the arXiv working out the FPF sensitivity to anomalous U(1) gauge bosons living in the bulk of Little Sting Theory D-brane models <LINK>'],https://arxiv.org/abs/2204.06469,"We show that experiments at the Forward Physics Facility, planned to operate near the ATLAS interaction point during the LHC high-luminosity era, will be able to probe predictions of Little String Theory by searching for anomalous U(1) gauge bosons living in the bulk. The interaction of the abelian broken gauge symmetry with the Standard Model is generated at the one-loop level through kinetic mixing with the photon. Gauge invariant generation of mass for the U(1) gauge boson proceeds via the Higgs mechanism in spontaneous symmetry breaking, or else through anomaly-cancellation appealing to Stueckelberg-mass terms. We demonstrate that FASER2 will be able to probe string scales over roughly two orders of magnitude: 10^5 < M_s/TeV < 10^7. ","Anomalous U(1) Gauge Bosons and String Physics at the Forward Physics
  Facility"
86,1514868617468387335,328430286,Jad C. Halimeh,"[""New paper <LINK>. We propose a cold-atom quantum simulator to implement the topological Œ∏-angle in gauge theories. We study the effect of confinement due to this angle on Coleman's phase transition and scarring.\n@HaukeGroup\n@QManyBody\n@MCQST_cluster\n@ERC_Research <LINK>""]",https://arxiv.org/abs/2204.06570,"The topological $\theta$-angle in gauge theories engenders a series of fundamental phenomena, including violations of charge-parity (CP) symmetry, dynamical topological transitions, and confinement--deconfinement transitions. At the same time, it poses major challenges for theoretical studies, as it implies a sign problem in numerical simulations. Analog quantum simulators open the promising prospect of treating quantum many-body systems with such topological terms, but, contrary to their digital counterparts, they have not yet demonstrated the capacity to control the $\theta$-angle. Here, we demonstrate how a tunable topological $\theta$-term can be added to a prototype theory with $\mathrm{U}(1)$ gauge symmetry, a discretized version of quantum electrodynamics in one spatial dimension. As we show, the model can be realized experimentally in a single-species Bose--Hubbard model in an optical superlattice with three different spatial periods, thus requiring only standard experimental resources. Through numerical calculations obtained from the time-dependent density matrix renormalization group method and exact diagonalization, we benchmark the model system, and illustrate how salient effects due to the $\theta$-term can be observed. These include charge confinement, the weakening of quantum many-body scarring, as well as the disappearance of Coleman's phase transition due to explicit breaking of CP symmetry. This work opens the door towards studying the rich physics of topological gauge-theory terms in large-scale cold-atom quantum simulators. ","Tuning the Topological $\theta$-Angle in Cold-Atom Quantum Simulators of
  Gauge Theories"
87,1514862957238407176,2856538378,Karel Van Acoleyen,"['New paper out on the arxiv, with Daan Maertens and Nick Bultinck. Hawking radiation as quench dynamics from hopping Hamiltonians that interface modes with opposite chirality. (GR is beautiful, but you do not need it to understand the Hawking effect.)\n\n<LINK>']",https://arxiv.org/abs/2204.06583,"We construct two free fermion lattice models exhibiting Hawking pair creation. Specifically, we consider the simplest case of a d=1+1 massless Dirac fermion, for which the Hawking effect can be understood in terms of a quench of the uniform vacuum state with a non-uniform Hamiltonian that interfaces modes with opposite chirality. For both our models we find that additional modes arising from the lattice discretization play a crucial role, as they provide the bulk reservoir for the Hawking radiation: the Hawking pairs emerge from fermions deep inside the Fermi sea scattering off the effective black hole horizon. Our first model combines local hopping dynamics with a translation over one lattice site, and we find the resulting Floquet dynamics to realize a causal horizon, with fermions scattering from the region outside the horizon. For our second model, which relies on a purely local hopping Hamiltonian, we find the fermions to scatter from the inside. In both cases, for Hawking temperatures up to the inverse lattice spacing we numerically find the resulting Hawking spectrum to be in perfect agreement with the Fermi-Dirac quantum field theory prediction. ",Hawking radiation on the lattice as universal (Floquet) quench dynamics
88,1514857941068877824,586676613,Robert Insall,"['Our new paper on deep learning in pathology.  Enjoy!\n\nBrief description below.\n\n<LINK>', 'Generative adversarial networks are awesome. Absolutely staggering in their capabilities. Just look at that bane of social media, https://t.co/v0uWs2ozFX.\n\nBut in medicine they have a problem.', ""Their key goal is not accuracy, but undetectability. They aren't concerned with being right.  Instead they seek 'truthiness' (an appalling badly-synthesised word from US news media, but the precise one here).\n\nIf diagnosing a patient, you need to be correct, not convincing..."", ""One thing that helps is the marvellous CycleGAN.  Its architecture ensures that when it generates an image, it takes due account of the structure it's trying to reproduce. CycleGAN images aren't 'invented' but 'restyled' - the structure is still there but the details are new."", ""There's still a problem though. If you're trying to determine which parts of a sample are cancerous, it's not sufficient to invent the details against a background of genuine tissue. Identification of cancer and tissue both need to be right."", 'This paper - fabulous work by the excellent @cwalshai - addresses this issue, by ensuring the GAN considers the structure of the tissue AND the location of the cancer while it trains.\n\nAnd it succeeds!  Splendidly.\n\n\\end\\', 'https://t.co/HWC2Wfz3s0']",https://arxiv.org/abs/2204.06849,"Immunohistochemistry is a valuable diagnostic tool for cancer pathology. However, it requires specialist labs and equipment, is time-intensive, and is difficult to reproduce. Consequently, a long term aim is to provide a digital method of recreating physical immunohistochemical stains. Generative Adversarial Networks have become exceedingly advanced at mapping one image type to another and have shown promise at inferring immunostains from haematoxylin and eosin. However, they have a substantial weakness when used with pathology images as they can fabricate structures that are not present in the original data. CycleGANs can mitigate invented tissue structures in pathology image mapping but have a related disposition to generate areas of inaccurate staining. In this paper, we describe a modification to the loss function of a CycleGAN to improve its mapping ability for pathology images by enforcing realistic stain replication while retaining tissue structure. Our approach improves upon others by considering structure and staining during model training. We evaluated our network using the Fr\'echet Inception distance, coupled with a new technique that we propose to appraise the accuracy of virtual immunohistochemistry. This assesses the overlap between each stain component in the inferred and ground truth images through colour deconvolution, thresholding and the Sorensen-Dice coefficient. Our modified loss function resulted in a Dice coefficient for the virtual stain of 0.78 compared with the real AE1/AE3 slide. This was superior to the unaltered CycleGAN's score of 0.74. Additionally, our loss function improved the Fr\'echet Inception distance for the reconstruction to 74.54 from 76.47. We, therefore, describe an advance in virtual restaining that can extend to other immunostains and tumour types and deliver reproducible, fast and readily accessible immunohistochemistry worldwide. ","Ensuring accurate stain reproduction in deep generative networks for
  virtual immunohistochemistry"
89,1514773040608862217,928595497748541441,Ali Hassani,"['Neighborhood Attention localizes receptive fields dynamically, so you only pay attention to what you need!\n\nCheck out our new paper: Neighborhood Attention Transformer.\n<LINK> <LINK>']",https://arxiv.org/abs/2204.07143,"We present Neighborhood Attention Transformer (NAT), an efficient, accurate and scalable hierarchical transformer that works well on both image classification and downstream vision tasks. It is built upon Neighborhood Attention (NA), a simple and flexible attention mechanism that localizes the receptive field for each query to its nearest neighboring pixels. NA is a localization of self-attention, and approaches it as the receptive field size increases. It is also equivalent in FLOPs and memory usage to Swin Transformer's shifted window attention given the same receptive field size, while being less constrained. Furthermore, NA includes local inductive biases, which eliminate the need for extra operations such as pixel shifts. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20k. We will open-source our checkpoints, training script, configurations, and our CUDA kernel at: this https URL . ",Neighborhood Attention Transformer
90,1514702332889546760,1404001821060915205,Yerong,"['Our new paper on 1H 1934-063 appear on arxiv now (<LINK>)! We find a relatively low-ionization (logxi~1.6), low-speed (0.075c) ultra-fast outflow and explain the unknown 1 keV emission with a secondary reflection on the base of outflows.', 'This discovery may reveal the link between the reprocessing of the inner accretion flow photons and ejection. https://t.co/xjuP3Z7aUb']",http://arxiv.org/abs/2204.06075,"Accretion and ejection of matter in active galactic nuclei (AGN) are tightly connected phenomena and represent fundamental mechanisms regulating the growth of the central supermassive black hole and the evolution of the host galaxy. However, the exact physical processes involved are not yet fully understood. We present a high-resolution spectral analysis of a simultaneous \xmm\ and \nustar\ observation of the narrow line Seyfert 1 (NLS1) AGN 1H 1934-063, during which the X-ray flux dropped by a factor of $\sim6$ and subsequently recovered within 140 kiloseconds. By means of the time-resolved and flux-resolved X-ray spectroscopy, we discover a potentially variable warm absorber and a relatively stable ultra-fast outflow (UFO, $v_\mathrm{UFO}\sim-0.075\,c$) with a mild ionization state ($\log(\xi/\mathrm{erg\,cm\,s^{-1})}\sim1.6$). The detected emission lines (especially a strong and broad feature around 1\,keV) are of unknown origin and cannot be explained with emission from plasmas in photo- or collisional-ionization equilibrium. Such emission lines could be well described by a strongly blueshifted ($z\sim-0.3$) secondary reflection off the base of the equatorial outflows, which may reveal the link between the reprocessing of the inner accretion flow photons and the ejection. However, this scenario although being very promising is only tentative and will be tested with future observations. ",Ejection-accretion connection in NLS1 AGN 1H 1934-063
91,1514684483919925258,1148488566881705984,Allison Tam,"['New paper! Language and large foundation models come together to drive semantically meaningful exploration. This idea helps RL agents learn faster in 3D environments, even when language annotations are unavailable (<LINK>)\n\nRead on üîé‚¨áÔ∏è <LINK>', ""Humans naturally use language to communicate and highlight the most important parts of our environment. Helpful for novelty-based exploration, since language can help determine what's actually new and what's not. 2/9 https://t.co/6YXMNtvEhT"", 'Language gives agents a different perspective on the state space. It creates high-level abstractions that reflect the semantics of the environment. How does this work in practice? 3/9 https://t.co/1kbDRyCAOu', 'We extend existing exploration methods by using language-based state representations, in lieu of controllable states (Never Give Up; https://t.co/wZDfuAoVju) or random features (Random Network Distillation; https://t.co/gigaF5bBVq). 4/9 https://t.co/uH4bWvhxFh', ""Representations from CLIP-style models are particularly powerful. Language pretraining shapes these embeddings with semantics &amp; transfers knowledge from large-scale caption data. So language can improve exploration even in environments that don't provide annotations/captions. 5/9"", ""Agents are instructed to navigate a large-scale simulated City or interact with household objects in a virtual Playroom. Language-shaped explorers significantly outperform baselines. ‚ö°‚ö° Best part: our method doesn't rely on language annotations. 6/9 https://t.co/iUjUGNZod3"", ""Language structures human-learning in fascinating ways. Why shouldn't it do the same for agents? Vision-language foundation models bridge the two worlds together. 7/9"", ""Very recent work from Jesse Mu, et al (https://t.co/d9AblmFWcp) have also shown how language can benefit exploration in MiniHack and MiniGrid environments. We've now shown it can be done even without language annotations, and in 3-D! 8/9"", 'Thanks to my amazing collaborators: @NeilCRabinowitz, @AndrewLampinen, @scychan_brains, Nicholas Roy, @djstrouse, @janexwang, @AndreaBanino, @FelixHill84 9/9']",https://arxiv.org/abs/2204.05080,"Continuous first-person 3D environments pose unique exploration challenges to reinforcement learning (RL) agents because of their high-dimensional state and action spaces. These challenges can be ameliorated by using semantically meaningful state abstractions to define novelty for exploration. We propose that learned representations shaped by natural language provide exactly this form of abstraction. In particular, we show that vision-language representations, when pretrained on image captioning datasets sampled from the internet, can drive meaningful, task-relevant exploration and improve performance on 3D simulated environments. We also characterize why and how language provides useful abstractions for exploration by comparing the impacts of using representations from a pretrained model, a language oracle, and several ablations. We demonstrate the benefits of our approach in two very different task domains -- one that stresses the identification and manipulation of everyday objects, and one that requires navigational exploration in an expansive world -- as well as two popular deep RL algorithms: Impala and R2D2. Our results suggest that using language-shaped representations could improve exploration for various algorithms and agents in challenging environments. ","Semantic Exploration from Language Abstractions and Pretrained
  Representations"
92,1514606937949696003,355864483,Zhilei Xu,"['Paper day! Here is a new mapping method we developed, gearing up for 21cm precision cosmology!\n<LINK>']",https://arxiv.org/abs/2204.06021,"Motivated by the desire for wide-field images with well-defined statistical properties for 21cm cosmology, we implement an optimal mapping pipeline that computes a maximum likelihood estimator for the sky using the interferometric measurement equation. We demonstrate this direct optimal mapping with data from the Hydrogen Epoch of Reionization (HERA) Phase I observations. After validating the pipeline with simulated data, we develop a maximum likelihood figure-of-merit for comparing four sky models at 166MHz with a bandwidth of 100kHz. The HERA data agree with the GLEAM catalogs to $<$10%. After subtracting the GLEAM point sources, the HERA data discriminate between the different continuum sky models, providing most support for the model from Byrne et al. 2021. We report the computation cost for mapping the HERA Phase I data and project the computation for the HERA 320-antenna data, both are feasible with a modern server. The algorithm is broadly applicable to other interferometers, and is valid for wide-field and non-coplanar arrays. ","Direct Optimal Mapping for 21cm Cosmology: A Demonstration with the
  Hydrogen Epoch of Reionization Array"
93,1514562268683456517,220910543,Khuyagbaatar Batsuren üá∫üá¶,"['I am super excited to share this new LREC paper. \nLink: <LINK> \nData: <LINK>\nThis resource is about the semantic diversity of 699 languages in the kinship domain. Joint work of @TKhishigsuren, @amarsanaag, and many other awesome colleagues. (1/4) <LINK>', 'As you may know, languages are extremely diverse at the semantic level. Often, MT faces untranslatability (or lexical gap) issues. Today‚Äôs top MT systems, for instance, make consistent mistakes over simple sentences such as (2/4)', ""'My brother is younger than me' even across relatively high-resource languages: for example, the\nJapanese translation of the English sentence above is ÁßÅ„ÅÆÂÖÑ„ÅØÁßÅ„Çà„ÇäËã•„ÅÑ„Åß„Åô with a nonsensical meaning of My elder brother is younger than me. (3/4)"", 'The same mistake is observed in many other languages such as Mongolian or Hungarian. Our MT experiments showed that handling lexical gaps is indeed a weak point of current MT techniques (4/4) https://t.co/wzNT5x8cmR']",https://arxiv.org/abs/2204.05049,"This paper describes a method to enrich lexical resources with content relating to linguistic diversity, based on knowledge from the field of lexical typology. We capture the phenomenon of diversity through the notions of lexical gap and language-specific word and use a systematic method to infer gaps semi-automatically on a large scale. As a first result obtained for the domain of kinship terminology, known to be very diverse throughout the world, we publish a lexico-semantic resource consisting of 198 domain concepts, 1,911 words, and 37,370 gaps covering 699 languages. We see potential in the use of resources such as ours for the improvement of a variety of cross-lingual NLP tasks, which we demonstrate through a downstream application for the evaluation of machine translation systems. ","Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of
  Lexical Gaps in Kinship"
94,1514554294200147976,2176486874,Steven Thomson,"['New paper out today, with Ancel Larzul and @MarcoSchiro_ of @cdf1530. In this work, we ask the question ""Are fast scramblers good thermal baths?"". But what does this mean, and why is it an interesting question? ü§î Read on to find out...! üßµ\n\n<LINK>', ""First off, what even is a 'fast scrambler'? ü§î\n\nWhen most physical systems are prepared in a far-from-equilibrium state, they undergo a process known as 'thermalisation' where after some (often chaotic!) dynamics, they eventually reach an equilibrium steady state. https://t.co/beb4gcw9ki"", 'Some physical systems thermalise very slowly, and others not at all. Major examples of the latter include glasses (https://t.co/I5zBkVuYuM) and many-body localisation (https://t.co/sqCU1sbjA8). https://t.co/02IRQubvH5', ""But some systems thermalise extremely quickly - as fast as possible, in fact - and these are known as 'fast scramblers'. The model we study in this paper is one of those. It's called the Sachdev-Ye-Kitaev model (SYK for short). https://t.co/mo9vYZQQ8f https://t.co/lhQkIBqLY8"", ""(The term 'scrambling' comes from the idea that any information contained in the initial state of the system - or any 'memory' of the initial state - is very quickly lost as the system evolves over time in a chaotic manner.) https://t.co/pzj0m46psg"", ""The SYK model is interesting for a lot of reasons, partly because it's one of the fastest scramblers, partly because it's exactly solvable (at least in certain limits...!), and partly because it has deep connections to black holes and quantum gravity that I won't get into here. https://t.co/7cP6guyOMA"", ""But these aspects of the SYK model are old news. \n\nWe're interested in what happens if we connect an SYK model to another quantum system - does the fast scrambling property of the SYK model mean that it efficiently transfers heat to and from anything that it's connected to? ü§î https://t.co/cZmri094Ei"", ""In a flagrant violation of Betteridge's law, the answer to the question in our title turns out to be 'yes'!\n\nWell...with caveats. üòâ\n\nIt turns out that for weak couplings between the SYK model and the target system, the SYK model is indeed a highly effective thermal bath. üëç"", ""The main reason is that the SYK4 model has a huge peak in its spectral function close to zero frequency (green line in bottom plot), which means that even at weak coupling, it has a huge 'appetite' for low frequency excitations, which are rapidly absorbed by the SYK model. https://t.co/iMZyRxqyX5"", ""This translates into the SYK model being very efficient at absorbing energy from anything that it's coupled to, more so than a generic thermal bath (blue line in the above image). https://t.co/w3N8yUHD7p"", 'But when the system-bath coupling is strong, this advantage is negated somewhat, and it turns out that the SYK model is no better than any other thermal bath. Basically, given a strong enough coupling, the detailed spectral properties of the bath no longer make a big difference. https://t.co/Bunohykib0', ""We demonstrate this effect in the paper using lead author Ancel's wonderful numerical simulations and some fantastic insights from the quantum Boltzmann equation - but for more on that, it's best to read the paper. üòâ\n\nPlease do check it out and share! üëç https://t.co/XfCBMVhlhM"", ""PS: This work has its origins in something we spent a lot of time on a few years ago involving coupling SYK models to general thermal baths, but divergent terms in the self-energy caused problems. I'm really happy that something based on that has finally seen the light of day! üòÅ https://t.co/vdkMQ9iqu0"", ""@Sayan_Quantum @MarcoSchiro_ @cdf1530 Thanks for your interest! That's a really interesting question. I'd guess they probably perform badly, but I'm not aware of any situation where this has been studied - 'generic' baths are usually arrays of harmonic oscillators rather than locally interacting systems."", ""@Sayan_Quantum @MarcoSchiro_ @cdf1530 As for the link between scrambling and bath effectiveness, I think we'd need to explore a wider class of models to say for sure, as we only looked at two extremes (the very fast scrambler SYK4, and integrable SYK2). Checking intermediate cases could help establish this link. ü§î"", ""@Sayan_Quantum @MarcoSchiro_ @cdf1530 Ultimately the SYK4 was a good bath at weak coupling due to the enhanced spectral density at low frequencies as compared with SYK2, so the deeper question is whether this sort of spectral feature is directly indicative of scrambling rates, and I don't know the answer to this. ü§î"", ""@TanmayBhore It's very likely to be one of the entries, at least! üòÖ Although I have to say, there are quite a few really interesting papers on today's mailing list, so I might have to make some hard choices...!""]",https://arxiv.org/abs/2204.06434,"The Sachdev-Ye-Kitaev (SYK$_{4}$) model has attracted attention for its fast scrambling properties and its thermalization rate that is set only by the temperature. In this work we ask the question of whether the SYK$_{4}$ model is also a good thermal bath, in the sense that it allows a system coupled to it to thermalize. We address this question by considering the dynamics of a system of $N$ random non-interacting Majorana fermions coupled to an SYK$_{4}$ bath with $M$ Majorana fermions that we solve with Keldysh techniques in the limit of $M\gg N\gg 1$. We compare this nonequilibrium setting with a conventional bath made of free non-interacting degrees of freedom with a continous spectrum. We show that the SYK$_{4}$ bath is more efficient in thermalising the system at weak coupling, due to its enhanced density of states at low frequency, while at strong system-bath couplings both type of environments give rise to a similar time scale for thermalisation. ",Are fast scramblers good thermal baths?
95,1514538957660434432,1042121476151881728,Christian Henke,"['New paper on arXiv! Using a renormalised energy momentum tensor for space-time and matter, it is shown that the #LambdaCDM model is realised with a #MOND-like behaviour. \n\n<LINK>']",https://arxiv.org/abs/2204.05755,"It has been demonstrated that the difference between the Renormalised Brane World (RBW) model and the Lambda Cold Dark Matter ($\Lambda$CDM) model occurs only at sufficiently distant times. In this paper, it is shown that for spherically symmetric situations an analog deviation between the RBW model and Newton's theory occurs at large distances. More precisely, this deviation of the RBW model is nothing other than the explanation of Milgrom's hypothesis and follows from itself. Therefore, the results of this paper explains flat rotation curves of galaxies without dark matter. ",$\Lambda$CDM $\cup$ MOND
96,1514494383168577540,505670972,Cem Say,"['Our new paper, where we introduce the notion of ""energy complexity"" in automata theory, is out. Physicists, as well as computational complexity and implementation people, might enjoy this:\n<LINK> <LINK>']",https://arxiv.org/abs/2204.06025,"The erasure of each bit of information by a computing device has an intrinsic energy cost. Although any Turing machine can be rewritten to be thermodynamically reversible without changing the recognized language, finite automata that are restricted to scan their input once in ""real-time"" fashion can only recognize the members of a proper subset of the class of regular languages in this reversible manner. We use a general quantum finite automaton model to study the thermodynamic cost per step associated with the recognition of different regular languages. We show that zero-error quantum finite automata have no energy cost advantage over their classical deterministic counterparts, and prove an upper bound for the cost that holds for all regular languages. We also demonstrate languages for which ""error can be traded for energy"", i.e. whose zero-error recognition is associated with provably bigger energy cost per step when compared to their bounded-error recognition by real-time finite-memory quantum devices. ",Energy Complexity of Regular Language Recognition
97,1514410868817244167,1095022417028763649,Alicia Parrish,"['New paper &amp; dataset! We ask whether we can collect explanations for two *opposing* answer options to hard multiple-choice questions as a way to help humans correctly identify the answer, even in the presence of an unreliable system. (1/9) <LINK> <LINK>', 'We know that model-generated explanations can include false information, so we consider whether such answers can be more helpful to humans if the systems present explanations + evidence for *two candidate answers*. (2/9)', ""We want to know if humans who don't have access to the ground-truth can more accurately identify which of two answer options is correct when presented with the strongest possible argument for each option. (3/9)"", 'We collect a dataset of expert-written (counter)-explanations, with supporting evidence from the passage, for answers to questions about short stories. The writers have read the entire stories, but our judges have not. (4/9) https://t.co/2Q4K7470iO', ""The judges have only 90s to read the arguments + excerpts and to scan the passage to find the answer. The time limit encourages writers to write concise and easy-to-understand arguments, while also ensuring the judges can't just read the whole passage. (5/9)"", 'We find that presenting human-selected text snippets as evidence increases human accuracy; however, we do not observe that explanations improve human accuracy in this set-up. (6/9) https://t.co/COHsL47pLN', ""We still think it's reasonable that this kind of debate set-up will be useful in some settings. Afterall, difficult questions in the real world are often decided by carefully weighing multiple sides of an issue. (7/9)"", 'Our next step is looking into whether a *multi-turn* debate set-up would be more helpful to humans, where a second turn of arguments specifically address deficiencies in the original arguments. (8/9)', ""More analyses and thoughts in the paper! We'll be presenting a poster of this work at the @LNLSWorkshop and would love to hear your thoughts and feedback! (9/9)""]",https://arxiv.org/abs/2204.05212,"Current QA systems can generate reasonable-sounding yet false answers without explanation or evidence for the generated answer, which is especially problematic when humans cannot readily check the model's answers. This presents a challenge for building trust in machine learning systems. We take inspiration from real-world situations where difficult questions are answered by considering opposing sides (see Irving et al., 2018). For multiple-choice QA examples, we build a dataset of single arguments for both a correct and incorrect answer option in a debate-style set-up as an initial step in training models to produce explanations for two candidate answers. We use long contexts -- humans familiar with the context write convincing explanations for pre-selected correct and incorrect answers, and we test if those explanations allow humans who have not read the full context to more accurately determine the correct answer. We do not find that explanations in our set-up improve human accuracy, but a baseline condition shows that providing human-selected text snippets does improve accuracy. We use these findings to suggest ways of improving the debate set up for future data collection efforts. ","Single-Turn Debate Does Not Help Humans Answer Hard
  Reading-Comprehension Questions"
98,1514352308783624193,74846617,John Forbes,"[""New paper out today! I've been working on this one for *coughcough* years...\n\nWe tracked every bit of gas entering or leaving a given region of a simulated galaxy to understand its energy budget:\n\n <LINK> <LINK>"", 'Featuring stacking, regression, and cross-correlation analyses, we found that gas accreting onto a galaxy has a *direct impact* on the energy budget, with an efficiency around 10%. https://t.co/Rfamc9Ar8O', 'The title of the paper ""Gas Accretion Can Drive Turbulence in Galaxies"" is a friendly poke at @PFHopkins_Astro\'s 2013 paper ""Accretion does not drive the turbulence in galactic discs""', 'Many thanks to my coauthors, virtually none of whom are on twitter! @StephTonnesen @cchayward82']",https://arxiv.org/abs/2204.05344,"The driving of turbulence in galaxies is deeply connected with the physics of feedback, star formation, outflows, accretion, and radial transport in disks. The velocity dispersion of gas in galaxies therefore offers a promising observational window into these processes. However, the relative importance of each of these mechanisms remains controversial. In this work we revisit the possibility that turbulence on galactic scales is driven by the direct impact of accreting gaseous material on the disk. We measure this effect in a disk-like star-forming galaxy in IllustrisTNG, using the high-resolution cosmological magnetohydrodynamical simulation TNG50. We employ Lagrangian tracer particles with a high time cadence of only a few Myr to identify accretion and other events, such as star formation, outflows, and movement within the disk. The energies of particles as they arrive in the disk are measured by stacking the events in bins of time before and after the event. The average effect of each event is measured on the galaxy by fitting explicit models for the kinetic and turbulent energies as a function of time in the disk. These measurements are corroborated by measuring the cross-correlation of the turbulent energy in the different annuli of the disk with other time series, and searching for signals of causality, i.e. asymmetries in the cross-correlation across zero time lag. We find that accretion contributes to the large-scale turbulent kinetic energy even if it is not the dominant driver of turbulence in this $\sim 5 \times 10^{9} M_\odot$ stellar mass galaxy. Extrapolating this finding to a range of galaxy masses, we find that there are regimes where energy from direct accretion may dominate the turbulent energy budget, particularly in disk outskirts, galaxies less massive than the Milky Way, and at redshift $\sim 2$. ",Gas Accretion Can Drive Turbulence in Galaxies
99,1514294647354544129,154458506,Valerio Brussani,"['My paper ""ASVAAN: Semi-automatic #sidechannel analysis of #Android NDK"" is now available on\n@arxiv at <LINK>. My paper provides a new approach to discover Android NDK side-channel leaks.', 'The approach described in the paper allowed to identify new side-channel leaks in several Android NDK functions. However, the best results (and the examples detailed in the paper) are based on some custom templates invoking libc functions.', 'The findings were responsibly disclosed to the Android Security Team of Google. Four of them were rewarded with a 4 digits bounty']",https://arxiv.org/abs/2204.05911,"Android is the most popular operating systems for smartphones and is also well-known for its flexibility and security. However, although it is overall considered very secure, there are still some vulnerabilities occasionally discovered that allow getting user sensitive information bypassing security controls and boundaries: among these, side-channel vulnerabilities are a significant concern these days. Although there are several types of side-channel vulnerabilities, ones focused on APIs still represent a great area to explore, which, until now, has often been analysed manually. Only in the latest years, there have been published some automatic solutions which focus on performing automatic scanning of side-channel flaws in Android, created due to the increasing codebase of the operating system; however, they present some limitations. This paper introduces a new approach to discover Android NDK side-channel leaks, which at the best of the author knowledge have never been investigated through the usage of automatic or semi-automatic solutions. The approach described in the work, allowed to identify more than 8 new side-channel leaks in several Android NDK functions,which permitted to infer with great accuracy application and websites launches on a victim device. The findings represents the first discovered side-channel leaks in Android NDK functions, and were responsibly disclosed to the Android Security Team of Google. ",ASVAAN: Semi-automatic side-channel analysis of Android NDK
100,1514195756156084225,924551762224320512,Nils Strodthoff,"['üì¢preprint alert: in our new paper <LINK> (w AML group @fraunhoferhhi) we benchmark a broad range of SOTA image recognition models on classification tasks in histopathology wrt performance, interpretability and robustness- with some surprising results 1/4', 'The good old Inception V3 outperforms all other models including vision transformers (across five public datasets), improvements through ensemble/hybrid models are marginal 2/4 https://t.co/Nhcj0DA8IX', 'We use #XAI methods in combination with segmentation models to assess which image regions (nuclei, surr. tissue, bg) are most relevant for the decision: all models focus on nuclei; however, different strategies seem to lead to comparable performance 3/4 https://t.co/G6joDRYHVX', 'Finally, we looked into robustness to stain variations through a trained CycleGAN. Robustness still represents a major challenge for the application of image recognition models in the wild! 4/4 https://t.co/o0oVhi3td2']",https://arxiv.org/abs/2204.05044,"While machine learning is currently transforming the field of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classification accuracy. In order to fill this gap, we conducted an extensive evaluation by benchmarking a wide range of classification models, including recent vision transformers, convolutional neural networks and hybrid models comprising transformer and convolutional models. We thoroughly tested the models on five widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classification model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of the models' classification strategies that allow for plausibility checks and systematic comparisons. The study resulted in specific model recommendations for practitioners as well as putting forward a general methodology to quantify a model's quality according to complementary requirements that can be transferred to future model architectures. ","From CNNs to Vision Transformers -- A Comprehensive Evaluation of Deep
  Learning Models for Histopathology"
101,1514186163917053956,106843613,Jacob Haqq Misra,"['Alien farming could be detectable by searching for ammonia and nitrous oxide in exoplanet atmospheres as evidence of nitrogen cycle management. \n\nRead more about this #technosignature in our new paper!\n\n@ThomasFauchez, @nogreenstars, @ravi_kopparapu\n\n<LINK>', '@ThomasFauchez @nogreenstars @ravi_kopparapu We also summarize our ""ExoFarm"" study in this @sciworthy article\n\nhttps://t.co/b0ZD02gewU']",https://arxiv.org/abs/2204.05360,"Agriculture is one of the oldest forms of technology on Earth. The cultivation of plants requires a terrestrial planet with active hydrological and carbon cycles and depends on the availability of nitrogen in soil. The technological innovation of agriculture is the active management of this nitrogen cycle by applying fertilizer to soil, at first through the production of manure excesses but later by the Haber-Bosch industrial process. The use of such fertilizers has increased the atmospheric abundance of nitrogen-containing species such as NH$_3$ and N$_2$O as agricultural productivity intensifies in many parts of the world. Both NH$_3$ and N$_2$O are effective greenhouse gases, and the combined presence of these gases in the atmosphere of a habitable planet could serve as a remotely detectable spectral signature of technology. Here we use a synthetic spectral generator to assess the detectability of NH$_3$ and N$_2$O that would arise from present-day and future global-scale agriculture. We show that present-day Earth abundances of NH$_3$ and N$_2$O would be difficult to detect but hypothetical scenarios involving a planet with 30-100 billion people could show a change in transmittance of about 50-70% compared to pre-agricultural Earth. These calculations suggest the possibility of considering the simultaneous detection of NH$_3$ and N$_2$O in an atmosphere that also contains H$_2$O, O$_2$, and CO$_2$ as a technosignature for extraterrestrial agriculture. The technology of agriculture is one that could be sustainable across geologic timescales, so the spectral signature of such an ""ExoFarm"" is worth considering in the search for technosignatures. ","Disruption of a Planetary Nitrogen Cycle as Evidence of Extraterrestrial
  Agriculture"
102,1514052936376090628,1326946405378764801,Mohan Sarovar,"['New paper!\nWe show how to build cheap and effective surrogate models for variational quantum circuits, which can make variational optimization much more accurate and scalable. Joint work with @ryanmshaffer &amp; Lucas Kocia.\n\n<LINK>', ""This technique is already being used to increase signal-to-noise on the QSCOUT testbed @SandiaLabs. Please get in touch if you're interested in deploying it too.""]",https://arxiv.org/abs/2204.05451,"Variational quantum algorithms are a class of techniques intended to be used on near-term quantum computers. The goal of these algorithms is to perform large quantum computations by breaking the problem down into a large number of shallow quantum circuits, complemented by classical optimization and feedback between each circuit execution. One path for improving the performance of these algorithms is to enhance the classical optimization technique. Given the relative ease and abundance of classical computing resources, there is ample opportunity to do so. In this work, we introduce the idea of learning surrogate models for variational circuits using few experimental measurements, and then performing parameter optimization using these models as opposed to the original data. We demonstrate this idea using a surrogate model based on kernel approximations, through which we reconstruct local patches of variational cost functions using batches of noisy quantum circuit results. Through application to the quantum approximate optimization algorithm and preparation of ground states for molecules, we demonstrate the superiority of surrogate-based optimization over commonly-used optimization techniques for variational algorithms. ",Surrogate-based optimization for variational quantum algorithms
103,1513993652032327681,16714100,Cayman Unterborn,"['How old can a rocky exoplanet be and still have a temperate climate today, when we observe it? Which are young enough and which may already be too old? Check out our new interdisciplinary paper where we  blend astro and geo to estimate how old is too old! <LINK>', 'A planet being in the ""habitable zone"" does not mean it is necessarily habitable. It must also have the geologic conditions necessary to maintain a temperate climate by regulating greenhouse gasses within the atmosphere.', 'There may be many a few ways to do this, but we focused on the planetary-scale carbon cycle, where CO2 is degassed during volcanism brought about by mantle convection, becomes a carbonate, weathers and is eventually returned to the mantle to do it all again.', 'Powering this whole cycle requires heat. For a non-tidally locked exoplanet it will start with some total budget of heat that will dwindle as the planet ages and cools. This cooling means the mantle degassing rate of C can slow enough to potentially shut down the C-cycle.', ""There are a few sources of a planet's internal heat, but we focused on the heat from the radioactive decay of Th, U and K within the planet. Observational data from stars suggests these elements may vary quite a bit in exoplanets compared to the Earth (see paper for why)"", 'These isotopes are made via supernovae and because they are produced and decay at different rates, when a planet formed in the history of the Milky Way may  influence just how much of each element a planet inherits upon its formation.', ""We explored how each of these effects on a planet's  initial radiogenic heat budget affects the longevity of mantle degassing. We decided to focus on planets with stagnant-lids and maximizing the mantle cooling rate. This makes our degassing lifetime estimates a bit pessimistic."", ""But that's okay! This means those planets we pessimistically think are plausibly young enough to degas today could be even more likely under optimistic conditions. We explored some (but not all) of these more optimistic effects on the degas lifetime and didn't find much variation"", ""I won't give away everything, but some key takeaways:  a planet's abundance of K may be most important for setting the lifetime of degassing, TRAPPIST-1 may be too old (~8+/-2 Gyr!) and we really need to measure more host-star ages and their abundances of Th, U and K!"", 'This paper is packed with interesting stuff (I hope!) and was not possible without my superlative co-authors from across astro, geo and meteoritics: Brad Foley (not on twitter), @Deschscoveries, @ExoCytherean, Patrick Young, Steve Vance and Lee Chieffle (also not on twitter)!']",https://arxiv.org/abs/2204.04243,"The ideal exoplanets to search for life are those within a star's habitable zone. However, even within the habitable zone planets can still develop uninhabitable climate states. Sustaining a temperate climate over geologic ($\sim$Gyr) timescales requires a planet contain sufficient internal energy to power a planetary-scale carbon cycle. A major component of a rocky planet's energy budget is the heat produced by the decay of radioactive elements, especially $^{40}$K, $^{232}$Th, $^{235}$U and $^{238}$U. As the planet ages and these elements decay, this radiogenic energy source dwindles. Here we estimate the probability distribution of the amount of these heat producing elements (HPEs) that enter into rocky exoplanets through Galactic history, by combining the system-to-system variation seen in stellar abundance data with the results from Galactic chemical evolution models. Using these distributions, we perform Monte-Carlo thermal evolution models that maximize the mantle cooling rate. This allows us to create a pessimistic estimate of lifetime a rocky, stagnant-lid exoplanet can support a global carbon cycle and temperate climate as a function of its mass and when it in Galactic history. We apply this framework to a sample of 17 likely rocky exoplanets with measured ages, 7 of which we predict are likely to be actively degassing today despite our pessimistic assumptions. For the remaining planets, including those orbiting TRAPPIST-1, we cannot confidently assume they currently contain sufficient internal heat to support mantle degassing at a rate sufficient to sustain a global carbon cycle or temperate climate without additional tidal heating or undergoing plate tectonics. ","Mantle Degassing Lifetimes through Galactic Time and the Maximum Age
  Stagnant-lid Rocky Exoplanets can Support Temperate Climates"
104,1513884607279767554,1180579200,Ioannis Kontoyiannis,"['üì¢ More fun ü§∏üôÇ with entropy! üé≤\n\nNew paper with @LGavalakis at <LINK> <LINK>', '@atishayokti @LGavalakis Yes, exactly the same thing!']",https://arxiv.org/abs/2204.05033,"We recall some of the history of the information-theoretic approach to deriving core results in probability theory and indicate parts of the recent resurgence of interest in this area with current progress along several interesting directions. Then we give a new information-theoretic proof of a finite version of de Finetti's classical representation theorem for finite-valued random variables. We derive an upper bound on the relative entropy between the distribution of the first $k$ in a sequence of $n$ exchangeable random variables, and an appropriate mixture over product distributions. The mixing measure is characterised as the law of the empirical measure of the original sequence, and de Finetti's result is recovered as a corollary. The proof is nicely motivated by the Gibbs conditioning principle in connection with statistical mechanics, and it follows along an appealing sequence of steps. The technical estimates required for these steps are obtained via the use of a collection of combinatorial tools known within information theory as `the method of types.' ","Information in probability: Another information-theoretic proof of a
  finite de Finetti theorem"
105,1513811805906382853,1214911964356452353,Sebastian Lerch,"['New paper ""Convolutional autoencoders for spatially-informed ensemble post-processing"" accepted at the AI for Earth and Space Science Workshop at #ICLR2022 - available at <LINK>. Joint work with @AstroInformatix üßµ', ""Motivation: Station-based post-processing models require localized predictors interpolated from the NWP model's spatial forecast fields to the target locations. Predictability information contained in large-scale spatial structures is potentially lost in this interpolation step. https://t.co/yPk5wg9wVK"", 'We propose the use of convolutional autoencoders to learn compact representations of spatial input fields which can then be used to augment location-specific information as additional inputs to post-processing models. https://t.co/en6AmO8pMj', 'Convolutional autoencoders are applied to spatial forecast fields of different variables, and generally do a good job at reconstructing the mean forecast fields. https://t.co/8aHbXTeQrI', 'Using AE representations as additional inputs improves the performance of post-processing models, but the results vary substantially across variables and dimensionality of the latent representations. https://t.co/AMVxnnpdES', '@vitusbenson Thanks! This was done by others for example here: https://t.co/NP46aanoao. We mainly wanted to try an alternative approach that appeared to be computationally more affordable.']",https://arxiv.org/abs/2204.05102,"Ensemble weather predictions typically show systematic errors that have to be corrected via post-processing. Even state-of-the-art post-processing methods based on neural networks often solely rely on location-specific predictors that require an interpolation of the physical weather model's spatial forecast fields to the target locations. However, potentially useful predictability information contained in large-scale spatial structures within the input fields is potentially lost in this interpolation step. Therefore, we propose the use of convolutional autoencoders to learn compact representations of spatial input fields which can then be used to augment location-specific information as additional inputs to post-processing models. The benefits of including this spatial information is demonstrated in a case study of 2-m temperature forecasts at surface stations in Germany. ","Convolutional autoencoders for spatially-informed ensemble
  post-processing"
106,1513757232504180738,77036349,Alice Booth,"['New paper out yesterday led by PhD student in Leiden Observatory Margot Leemker. Here we use 13CO 6-5 emission (ALMA Band 9!) to investigate the ""Gas temperature structure across transition disk cavities"". \nA few important takeaways...üßµ \n<LINK>', 'The ratio of 13CO 6-5 and 2-1 (or 3-2) can be used as a probe of gas temperature so if we do this across the cavities (or gaps) in disks we will have a better constraint on the gas density and therefore a better answer to ""are planets causing the sub-structures in disks?""  ü™ê', ""Plot twist, just because your disk shows a cavity in the 13CO2-1 line doesn't mean there is a deep gas gap. We see the 13CO 6-5 line peaking inside the LkCa15 cavity indicating warm gas in the cavity. For HD169142 we see two very neat rings of gas just inside the dust cavity. üî• https://t.co/0OZf9gS0zz"", 'After a lot of work we arrived at the scenario where LkCa15 is host to lower-mass planets that can carve a gap in the dust but not yet the gas and HD169142 hosts a giant planet in the cavity. https://t.co/N4B1FNQIpK', ""I'll leave it there though and recommend you read the paper for lots of caveats on line ratio analysis in disks (emitting heights, optical depths, ...) and even thermochemical models to test the mechanisms that set the disk temperature structure (PAHs, inner disks, ...) ü§î https://t.co/WZGUNZkGuj""]",https://arxiv.org/abs/2204.03666,"[Abridged] Most disks observed at high angular resolution show substructures. Knowledge about the gas surface density and temperature is essential to understand these. The aim of this work is to constrain the gas temperature and surface density in two transition disks: LkCa15 and HD 169142. We use new ALMA observations of the $^{13}$CO $J=6-5$ transition together with archival $J=2-1$ data of $^{12}$CO, $^{13}$CO and C$^{18}$O to observationally constrain the gas temperature and surface density. Furthermore, we use the thermochemical code DALI to model the temperature and density structure of a typical transition disk. The $6-5/2-1$ line ratio in LkCa15 constrains the gas temperature in the emitting layers inside the dust cavity to be up to 65 K, warmer than in the outer disk at 20-30 K. For the HD 169142, the peak brightness temperature constrains the gas in the dust cavity of HD 169142 to be 170 K, whereas that in the outer disk is only 100 K. Models also show that a more luminous central star, a lower abundance of PAHs and the absence of a dusty inner disk increase the temperature of the emitting layers and hence the line ratio in the gas cavity. The gas column density in the LkCa15 dust cavity drops by a factor >2 compared to the outer disk, with an additional drop of an order of magnitude inside the gas cavity at 10 AU. In the case of HD 169142, the gas column density drops by a factor of 200$-$500 inside the gas cavity, which could be due to a massive companion of several M$_{\mathrm{J}}$. The broad dust-depleted gas region from 10-68 AU for LkCa15 may imply several lower mass planets. This work demonstrates that knowledge of the gas temperature is important to determine the gas surface density and thus whether planets, and if so what kind of planets, are the most likely carving the dust cavities. ",Gas temperature structure across transition disk cavities
107,1513717911038406657,300302265,Abhishek Maniyar,"[""First attempt at a paper thread üßµNew paper on the Arxiv today!\n\nWe propose to use the 'Doppler boosted dust emission' in the Universe as a new cosmological probe: <LINK>"", 'If a dusty galaxy has a non-zero lineof-sight peculiar velocity, its emission is Doppler boosted (which we call DB-CIB). \n\nThe large-scale cosmic velocity field results in galaxy bulk motions, which in turn source the DB-CIB signal of ininterest', 'Therefore this effect is an independent probe of the cosmic velocity field which is an excellent tool to study the dark energy, modified gravity, cosmic growth rate of structure, primordial non-Gaussianity of local type (fNL), etc.', ""The kinematic Sunyaev-Zel'dovich effect (kSZ) is also a similar probe of the cosmic velocity field. However, kSZ suffers from the wellknown problem of ‚ÄòkSZ‚Äìoptical depth degeneracy‚Äô where the overall normalization of the electron profile in a halo is not known very well."", ""DB-CIB effect is very analogous to the kSZ effect. However, it is completely 'calibratable' and thus does not suffer from this 'optical depth degeneracy issue'. This makes it immune from the complex astrophysics of galaxy formation!"", 'Finally, we forecast that such a promising DB-CIB effect is detectable in the crosscorrelation of CCAT-Prime and DESI-like experiments. \n\nPretty cool that we can use the dust in the local Universe to probe the physics in the early Universe!', 'Had a lot of fun working on this with Simone Ferraro and Emmanuel Schaan at @NYUPhysics and @BerkeleyLab!']",https://arxiv.org/abs/2204.05299,"We identify a new cosmological signal, the Doppler-boosted Cosmic Infrared Background (DB-CIB), arising from the peculiar motion of the galaxies whose thermal dust emission source the cosmic infrared background (CIB). This new observable is an independent probe of the cosmic velocity field, highly analogous to the well-known kinematic Sunyaev-Zel'dovich (kSZ) effect. Interestingly, DB-CIB does not suffer from the 'kSZ optical depth degeneracy', making it immune from the complex astrophysics of galaxy formation. We forecast that the DB-CIB effect is detectable in the cross-correlation of CCAT-Prime and DESI-like experiments. We show that it also acts as a new CMB foreground which can bias future kSZ cross-correlations, if not properly accounted for. ","Doppler boosted dust emission and CIB-galaxy cross-correlations: a new
  probe of cosmology and astrophysics"
108,1513684417138806784,22399655,Ryota Kanaiüí°,['Our new paper on the link between consciousness and intelligence just appeared on arxiv. Written with @awjuliani @kaixhin @shuxnys at Araya.\n \n<LINK>'],https://arxiv.org/abs/2204.05133,"In popular media, there is often a connection drawn between the advent of awareness in artificial agents and those same agents simultaneously achieving human or superhuman level intelligence. In this work, we explore the validity and potential application of this seemingly intuitive link between consciousness and intelligence. We do so by examining the cognitive abilities associated with three contemporary theories of conscious function: Global Workspace Theory (GWT), Information Generation Theory (IGT), and Attention Schema Theory (AST). We find that all three theories specifically relate conscious function to some aspect of domain-general intelligence in humans. With this insight, we turn to the field of Artificial Intelligence (AI) and find that, while still far from demonstrating general intelligence, many state-of-the-art deep learning methods have begun to incorporate key aspects of each of the three functional theories. Given this apparent trend, we use the motivating example of mental time travel in humans to propose ways in which insights from each of the three theories may be combined into a unified model. We believe that doing so can enable the development of artificial agents which are not only more generally intelligent but are also consistent with multiple current theories of conscious function. ","On the link between conscious function and general intelligence in
  humans and machines"
109,1513570484134236166,1292002106468048896,Vipul Gupta,"['Happy to announce our new CVPR paper - SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in VQA\n\nWe study the robustness of VQA models from a new perspective: visual context.\n\nPaper: <LINK>\n\n#CVPR2022 #ComputerVision \n\n1/5', 'Task : Given a model, try to measure the visual bias in the model based on its reliance on visual context i.e. irrelevant objects in the image, to make predictions. \nIdeally, changing irrelevant objects should not change model‚Äôs prediction.\n\n2/5 https://t.co/cmMjbdqxfu', 'We found that all types of VQA models uses visual context to answer questions. \nCan the model be improved? Yes, using SwapMix as a data augmentation strategy decreases the model‚Äôs reliance on visual context.\n\n3/5', 'We used the GQA dataset for our analysis. We also found that training VQA models with perfect embeddings (using scene graph embeddings) results in better or equivalent performance as compared to FasterRCNN embeddings. Moreover, these models rely less on visual context.\n\n4/5', 'Thanks to amazing collaborators  \n@zhuowanli @AdamKortylewski Chenyu @yingwei_li @YuilleAlan \nWe hope this new research direction to study reliance on visual context can serve as a new starting point to study VQA robustness and reliability.\n\n5/5']",https://arxiv.org/abs/2204.02285,"While Visual Question Answering (VQA) has progressed rapidly, previous works raise concerns about robustness of current VQA models. In this work, we study the robustness of VQA models from a novel perspective: visual context. We suggest that the models over-rely on the visual context, i.e., irrelevant objects in the image, to make predictions. To diagnose the model's reliance on visual context and measure their robustness, we propose a simple yet effective perturbation technique, SwapMix. SwapMix perturbs the visual context by swapping features of irrelevant context objects with features from other objects in the dataset. Using SwapMix we are able to change answers to more than 45 % of the questions for a representative VQA model. Additionally, we train the models with perfect sight and find that the context over-reliance highly depends on the quality of visual representations. In addition to diagnosing, SwapMix can also be applied as a data augmentation strategy during training in order to regularize the context over-reliance. By swapping the context object features, the model reliance on context can be suppressed effectively. Two representative VQA models are studied using SwapMix: a co-attention model MCAN and a large-scale pretrained model LXMERT. Our experiments on the popular GQA dataset show the effectiveness of SwapMix for both diagnosing model robustness and regularizing the over-reliance on visual context. The code for our method is available at this https URL ","SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context
  in Visual Question Answering"
110,1513541077172641793,2766925212,Andrew Childs,"['New paper on the strengths and limitations of teleportation-based quantum routing with @ddhruv97, @EddieSchoute, @aniruddha_bapat, and Alexey Gorshkov, presented today at #qctip2022. <LINK>']",http://arxiv.org/abs/2204.04185,"We study the problem of implementing arbitrary permutations of qubits under interaction constraints in quantum systems that allow for arbitrarily fast local operations and classical communication (LOCC). In particular, we show examples of speedups over swap-based and more general unitary routing methods by distributing entanglement and using LOCC to perform quantum teleportation. We further describe an example of an interaction graph for which teleportation gives a logarithmic speedup in the worst-case routing time over swap-based routing. We also study limits on the speedup afforded by quantum teleportation - showing an $O(\sqrt{N \log N})$ upper bound on the separation in routing time for any interaction graph - and give tighter bounds for some common classes of graphs. ",Quantum Routing with Teleportation
111,1513532304068546562,1388198782924255232,Tristan Thrush,"['Happy to announce our new CVPR paper - Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality.\n\nAll tested SOTA multimodal models perform very poorly on our new vision-language eval dataset. \n\nPaper: <LINK>\n\n #CVPR2022, #NLProc\n\n1/5', 'The task: Given two images and two captions, the goal is to match them correctly‚Äîbut crucially, both captions contain the same words/morphemes, only in a different order. Identical words between captions means that BOW models cannot perform above chance.\n\n2/5 https://t.co/Sanyid8CYs', 'We found that all of these models are very poor overall: FLAVA, CLIP, UNITER, ViLLA, VinVL, VisualBERT, ViLT, LXMERT, ViLBERT, UniT, VSE++, and VSRN. Can your model do better? \n\n3/5', 'The dataset was hand-curated by a group of expert annotators and validated by crowdworkers. To assist in analyzing model performance, the annotators tagged examples from a set of 70 fine-grained linguistic tags, 5 coarse linguistic tags, and 3 visual tags.\n\n4/5 https://t.co/wx2pKhAbxy', 'Thanks for the incredible teamwork! @this_is_ryanj, @max_nlp, @apsdehal, @adinamwilliams, @douwekiela, @candacerossio. We hope this dataset and task for visio-linguistic compositional understanding will contribute towards the development of truly grounded multimodal models!\n\n5/5']",https://arxiv.org/abs/2204.03162,"We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly - but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set of fine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at this https URL ","Winoground: Probing Vision and Language Models for Visio-Linguistic
  Compositionality"
112,1513485298868666371,175972825,Diego Correa,"['Hey researchers, today I have good news. Our new paper is in preprint and can be read on <LINK>. We entitle the paper as ‚ÄúIntroducing a Framework and a Decision Protocol to Calibrate Recommender Systems‚Äù. It is the last part of my master dissertation.']",https://arxiv.org/abs/2204.03706,"Recommender Systems use the user's profile to generate a recommendation list with unknown items to a target user. Although the primary goal of traditional recommendation systems is to deliver the most relevant items, such an effort unintentionally can cause collateral effects including low diversity and unbalanced genres or categories, benefiting particular groups of categories. This paper proposes an approach to create recommendation lists with a calibrated balance of genres, avoiding disproportion between the user's profile interests and the recommendation list. The calibrated recommendations consider concomitantly the relevance and the divergence between the genres distributions extracted from the user's preference and the recommendation list. The main claim is that calibration can contribute positively to generate fairer recommendations. In particular, we propose a new trade-off equation, which considers the users' bias to provide a recommendation list that seeks for the users' tendencies. Moreover, we propose a conceptual framework and a decision protocol to generate more than one thousand combinations of calibrated systems in order to find the best combination. We compare our approach against state-of-the-art approaches using multiple domain datasets, which are analyzed by rank and calibration metrics. The results indicate that the trade-off, which considers the users' bias, produces positive effects on the precision and to the fairness, thus generating recommendation lists that respect the genre distribution and, through the decision protocol, we also found the best system for each dataset. ","Introducing a Framework and a Decision Protocol to Calibrate Recommender
  Systems"
113,1513472978595549184,3131175701,Tsai Shang-Min (Shami),"['An elevator pitch for our new paper: We devised a mini-chemical network for H2-atmospheres with only 10 forward reactions. Affordable for 3D GCMs. <LINK>  (1/2)', ""a (weak) analogy: It's done by having a handful of generals (net reactions) instead of thousands of soldiers (conventional kinetics) fighting a battle. Photochemistry on the way (2/2) https://t.co/tL0PHto1Y4""]",https://arxiv.org/abs/2204.04201,"Growing evidence has indicated that the global composition distribution plays an indisputable role in interpreting observational data. 3D general circulation models (GCMs) with a reliable treatment of chemistry and clouds are particularly crucial in preparing for the upcoming observations. In the effort of achieving 3D chemistry-climate modeling, the challenge mainly lies in the expensive computing power required for treating a large number of chemical species and reactions. Motivated by the need for a robust and computationally efficient chemical scheme, we devise a mini-chemical network with a minimal number of species and reactions for H$_2$-dominated atmospheres. We apply a novel technique to simplify the chemical network from a full kinetics model -- VULCAN by replacing a large number of intermediate reactions with net reactions. The number of chemical species is cut down from 67 to 12, with the major species of thermal and observational importance retained, including H$_2$O, CH$_4$, CO, CO$_2$, C$_2$H$_2$, NH$_3$, and HCN. The size of the total reactions is greatly reduced from $\sim$ 800 to 20. The mini-chemical scheme is validated by verifying the temporal evolution and benchmarking the predicted compositions in four exoplanet atmospheres (GJ 1214b, GJ 436b, HD 189733b, HD 209458b) against the full kinetics of VULCAN. It reproduces the chemical timescales and composition distributions of the full kinetics well within an order of magnitude for the major species in the pressure range of 1 bar -- 0.1 mbar across various metallicities and carbon-to-oxygen (C/O) ratios. The small scale of the mini-chemical scheme permits simple use and fast computation, which is optimal for implementation in a 3D GCM or a retrieval framework. We focus on the thermochemical kinetics of net reactions in this paper and address photochemistry in a follow-up paper. ","A Mini-Chemical Scheme with Net Reactions for 3D GCMs I.: Thermochemical
  Kinetics"
114,1513442710228803588,2811702236,Sebastian Pucher,['We have uploaded a new paper with the latest results from our lab on @arxiv. Check out the paper with @ChrisLiedl as first author:\n\n<LINK> <LINK>'],https://arxiv.org/abs/2204.04106,"The collective absorption and emission of light by an ensemble of atoms is at the heart of many fundamental quantum optical effects and the basis for numerous applications. However, beyond weak excitation, both experiment and theory become increasingly challenging. Here, we explore the regimes from weak excitation to inversion with ensembles of up to one thousand atoms that are trapped and optically interfaced using the evanescent field surrounding an optical nanofiber. We realize strong inversion, with about 80% of the atoms being excited, and study their subsequent radiative decay into the guided modes. The data is very well described by a simple model that assumes a cascaded interaction of the guided light with the atoms. Our results contribute to the fundamental understanding of the collective interaction of light and matter and are relevant for applications ranging from quantum memories to sources of nonclassical light to optical frequency standards. ","Collective excitation and decay of waveguide-coupled atoms: from timed
  Dicke states to inverted ensembles"
115,1513441963562962946,747383862154559488,Kuniyuki Takahashi,['Our paper on sim2real bipedal walking is available.\nThere is a reality gap issue in sim2real learning. We solve this problem by proposing a new actuator model and system identification using failure behavior.\n<LINK>\n\n<LINK>'],https://arxiv.org/abs/2204.03897,"In deep reinforcement learning, sim-to-real is the mainstream method as it needs a large number of trials, however, it is challenging to transfer trained policy due to reality gap. In particular, it is known that the characteristics of actuators in leg robots have a considerable influence on the reality gap, and this is also noticeable in high reduction ratio gears. Therefore, we propose a new simulation model of high reduction ratio gears to reduce the reality gap. The instability of the bipedal locomotion causes the sim-to-real transfer to fail catastrophically, making system identification of the physical parameters of the simulation difficult. Thus, we also propose a system identification method that utilizes the failure experience. The realistic simulations obtained by these improvements allow the robot to perform compliant bipedal locomotion by reinforcement learning. The effectiveness of the method is verified using a actual biped robot, ROBOTIS-OP3, and the sim-to-real transferred policy archived to stabilize the robot under severe disturbances and walk on uneven terrain without force and torque sensors. ","Sim-to-Real Learning of Robust Compliant Bipedal Locomotion on Torque
  Sensor-Less Gear-Driven Humanoid"
116,1513336799313158146,3276698761,Albert S. Berahas,"['New paper that combines SQP and SVRG for solving equality constrained finite sum problems!\n\nAmazing coauthors: @JiahaoShi5 (@umichioe), Z. Yi (@UMichCSE) &amp; @baoyu_zhou (@lehigh_ise)\n\nGreat work team!!\n\n<LINK>']",https://arxiv.org/abs/2204.04161,"In this paper, we propose a stochastic method for solving equality constrained optimization problems that utilizes predictive variance reduction. Specifically, we develop a method based on the sequential quadratic programming paradigm that employs variance reduction in the gradient approximations. Under reasonable assumptions, we prove that a measure of first-order stationarity evaluated at the iterates generated by our proposed algorithm converges to zero in expectation from arbitrary starting points, for both constant and adaptive step size strategies. Finally, we demonstrate the practical performance of our proposed algorithm on constrained binary classification problems that arise in machine learning. ","Accelerating Stochastic Sequential Quadratic Programming for Equality
  Constrained Optimization using Predictive Variance Reduction"
117,1512746952659959810,561987213,Sarah McIntyre,"['This has been a long time coming...new paper ""Tidally driven tectonic activity as a parameter in exoplanet habitability"" is out!! <LINK>\nOne day I will write a thread, but now it is time to celebrate.ü•≥üéâüéäüôå <LINK>']",https://arxiv.org/abs/2204.03501,"Aims. On Earth, plate tectonics play an integral role in driving the long-term carbon cycle; however, on tidally locked rocky exoplanets alternative tectonic mechanisms driven by tidal stress and tidal heating could serve in an analogous way. Methods. We calculate tidal stress and tidal heating rates to model the likelihood of tectonic activity maintaining stable climates suitable for surface liquid water on tidally locked rocky exoplanets with radii ${R}_{p}$ $\le$ 1.23R$_\oplus$. Results. Applying the tidal models to our sample of 767 tidally locked rocky exoplanets reveals that $\sim$10% of exoplanets, including Proxima Cen b and GJ 1061 d from the circumstellar habitable zone (CHZ), pass the tidal stress subduction threshold for mobile lid tectonic activity and reside within the optimal tidal heating zone. This subset of exoplanets could sustain tidally induced temperate mobile lid tectonic activity comparable to plate tectonics on Earth, aiding in maintaining the presence of surface liquid water. Furthermore, $\sim$40% of exoplanets from our sample located in the CHZ would be unable to maintain the tectonic activity needed to stabilise the climate and are unlikely to retain surface liquid water. When broadening our modelling to establish the overlap between tidal stress, tidal heating, and the CHZ, to discover optimal regions to target for future observations, we determine that tidally driven tectonic activity conducive to the maintenance of surface liquid water occurs predominantly around M dwarfs, and identify intersections, where both mobile lid and optimal tidal heating could be sustained on eccentric (e>0.1) Earth-sized exoplanets (${R}_{p}$ = 1.0-1.23R$_\oplus$) orbiting in the CHZ of low-mass M dwarfs. ","Tidally driven tectonic activity as a parameter in exoplanet
  habitability"
118,1512732238898769920,1142415620270690304,Gitta Kutyniok,"['Check out our new paper (<LINK>) showing that inverse problems can be solved with checkable precision by #DeepLearning on analog, but not on digital #Hardware, joint with H. Boche @TU_Muenchen and A. Fono. @LMU_Muenchen  @researchbavaria @ResearchGermany @BayFOR']",https://arxiv.org/abs/2204.02066,"Inverse problems are used to model numerous tasks in imaging sciences, in particular, they encompass any task to reconstruct data from measurements. Thus, the algorithmic solvability of inverse problems is of significant importance. The study of this question is inherently related to the underlying computing model and hardware, since the admissible operations of any implemented algorithm are defined by the computing model and the hardware. Turing machines provide the fundamental model of today's digital computers. However, it has been shown that Turing machines are incapable of solving finite dimensional inverse problems for any given accuracy. This stimulates the question of how powerful the computing model must be to enable the general solution of finite dimensional inverse problems. This paper investigates the general computation framework of Blum-Shub-Smale (BSS) machines which allows the processing and storage of arbitrary real values. Although a corresponding real world computing device does not exist at the moment, research and development towards real number computing hardware, usually referred to by the term ""neuromorphic computing"", has increased in recent years. In this work, we show that real number computing in the framework of BSS machines does enable the algorithmic solvability of finite dimensional inverse problems. Our results emphasize the influence of the considered computing model in questions of algorithmic solvability of inverse problems. ",Inverse Problems Are Solvable on Real Number Signal Processing Hardware
119,1512510994039328774,48008938,Yann LeCun,"['New paper:\n""The Effects of Regularization and Data Augmentation are Class Dependent""\nby Randall Balestriero, Leon Bottou, Yann LeCun\n\nTL;DR: Turns out some types of data augmentation helps some categories and hurt others...\n\n<LINK> <LINK>', 'Seems intuitive: categories identifiable by color or texture (yellow bird, textured mushroom) are unaffected by aggressive cropping, while categories identifiable by shape (corkscrew) see a performance degradation with aggressive cropping that only contains part of the object...', 'Conversely, color jitter does not affect shape or texture-based categories (e.g. zebra), but affects color-based categories (like basket ball).\nStrangely, even weight decay affects different classes differently.']",https://arxiv.org/abs/2204.03632,"Regularization is a fundamental technique to prevent over-fitting and to improve generalization performances by constraining a model's complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation (DA) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as DA or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of DA or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the ""barn spider"" classification test accuracy falls from $68\%$ to $46\%$ only by introducing random crop DA during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -- averaged over all classes and samples -- has left us with models and regularizers that silently sacrifice performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on INaturalist sees its performances fall from $70\%$ to $30\%$ on class \#8889 when introducing random crop DA during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question. ",The Effects of Regularization and Data Augmentation are Class Dependent
120,1512463979427385349,1485396714454503424,Konstantin Gerbig,"['New paper drop!!! We propose a Hurricane-like mechanism to operate in protoplanetary disks. We find that even modestly under-saturated flows near the water ice line can be sufficient to drive long-lived vortices, warranting further investigation.\n<LINK>', 'Here is a cool figure from our paper. It shows the evolution of the vortex energy for several simulation setups. The vortices that incorporate the Hurricane-mechanism strengthen, grow and ultimately merge, as seen in the top panels. https://t.co/p02nDfrHmu', 'Terrestrial Hurricanes have quite complex flow patterns, and so do our space-analogues. In particular, the shear caused by Keplerian rotation naturally favors vortices to that rotate clockwise in a counter-clockwise rotating disk. Here, a pic of the flow, including isobars. https://t.co/6ovUNNmTYO', 'Thus, disk hurricanes are not cyclones like their  terrestrial counterpart, but anticyclones. However, they are still powered by the same process, that is transfer of water latent heat via mixing with an external medium (ocean and icy-dust for Earth and disk respectively).', 'In a steady-state, this latent-heat induced heating is exactly balance by friction, in form of mainly viscous dissipation and radiative losses. Key parameter is the under-saturation ""Delta q"" which is necessary to induce water sublimation and thus vortex energy increase. https://t.co/X3rf4QC5Vn', 'Our work included proof-of-concept 2d simulations. In the future, we want to model the 3d structure of the vortex which would include vertical convection and thus would allow for a transverse circulation as seen in terrestrial Hurricanes. https://t.co/EZp40pOQmw', 'Our conclusion is, that the Hurricane-mechanism may indeed spring into operation in protoplanetary disks if the flow just outside the water ice-line is sufficiently under-saturated. The existence of this mechanism would have interesting implications for planet formation.']",https://arxiv.org/abs/2204.03007,"When ice on the surface of dust grains in protoplanetary disk sublimates, it adds its latent heat of water sublimation to the surrounding flow. Drawing on the analogy provided by tropical cyclones on Earth, we investigate if this energy source is sufficient to sustain or magnify anticyclonic disk vortices that would otherwise fall victim to viscous dissipation. An analytical treatment, supported by exploratory two-dimensional simulations, suggests that even modestly under-saturated flows can extend the lifetime of vortices, potentially to a degree sufficient to aid particle trapping and planetesimal formation. We expect the best conditions for this mechanism to occur will be found near the disk's water ice line if turbulent motions displace gas parcels out of thermodynamic equilibrium with the dust mid-plane. ",The Prospects for Hurricane-like Vortices in Protoplanetary Disks
121,1512458719925264386,1362475407874867200,Max Hunter Gordon,"['New paper out today!\n<LINK>\nWe show how to prepare the covariance matrix on a quantum computer to do quantum principal component analysis: use an ensemble average density matrix! \nThanks to @MvsCerezo , @LCincio  and  @ColesQuantum for a great collaboration! 1/n', 'Principal component analysis (PCA) is a common technique in data analysis and machine learning.\nOur work provides a simple method to prepare the covariance matrix on a quantum computer, a key step in quantum PCA, using the ensemble average density matrix. 2/n https://t.co/Iufr3Nwmv5', 'We find our method is equivalent to doing ‚ÄòPCA without centering‚Äô which we interpret as doing PCA on a symmetrized dataset. We rigorously analyze the difference between these two approaches. 3/n https://t.co/KrNfnh0Ind', 'We argue that PCA is natural on quantum data (with complex amplitudes) and in this case there is no difference between PCA and PCA on symmetrized data! For classical datasets one can bound the deviation of the spectrum obtained with our method from that of standard PCA. 4/n https://t.co/ldiGfbeZA1', 'To explore the performance of our method on classical data we do PCA on the MNIST data set. We find that our method for constructing the covariance matrix to calculate the principal components gives practically equivalent results to conventional PCA. 5/n https://t.co/zfRpzaOtzs', 'We also look at how our approach performs on doing PCA of H2 and BeH2 ground states and make some pretty plots! \n\nOur work paves the way for the implementation of quantum PCA  on near-term quantum computers. 6/n https://t.co/jASuN1F0LV']",https://arxiv.org/abs/2204.03495,"Principal component analysis (PCA) is a dimensionality reduction method in data analysis that involves diagonalizing the covariance matrix of the dataset. Recently, quantum algorithms have been formulated for PCA based on diagonalizing a density matrix. These algorithms assume that the covariance matrix can be encoded in a density matrix, but a concrete protocol for this encoding has been lacking. Our work aims to address this gap. Assuming amplitude encoding of the data, with the data given by the ensemble $\{p_i,| \psi_i \rangle\}$, then one can easily prepare the ensemble average density matrix $\overline{\rho} = \sum_i p_i |\psi_i\rangle \langle \psi_i |$. We first show that $\overline{\rho}$ is precisely the covariance matrix whenever the dataset is centered. For quantum datasets, we exploit global phase symmetry to argue that there always exists a centered dataset consistent with $\overline{\rho}$, and hence $\overline{\rho}$ can always be interpreted as a covariance matrix. This provides a simple means for preparing the covariance matrix for arbitrary quantum datasets or centered classical datasets. For uncentered classical datasets, our method is so-called ""PCA without centering"", which we interpret as PCA on a symmetrized dataset. We argue that this closely corresponds to standard PCA, and we derive equations and inequalities that bound the deviation of the spectrum obtained with our method from that of standard PCA. We numerically illustrate our method for the MNIST handwritten digit dataset. We also argue that PCA on quantum datasets is natural and meaningful, and we numerically implement our method for molecular ground-state datasets. ",Covariance matrix preparation for quantum principal component analysis
122,1512450149226000384,1001311590275149825,Ram Ramrakhya,"['New #CVPR2022 paper üì¢üì¢üì¢\n\nHow can we teach agents to intelligently search for objects?\nAnswer: lots of human demonstrations\n\nLarge scale imitation of human demonstrations leads to agents learning efficient object-search strategies\n\nArxiv: <LINK>\n\nüßµ1/7 <LINK>', ""We present a large-scale study of imitating human demonstrations on tasks that require embodied agents to search for objects in new environments (1) ObjectNav (e.g. 'find &amp; go to a chair'),(2) Pick&amp;Place (e.g. 'find mug, pick mug, find counter, place mug on counter'). \n\nüßµ2/7 https://t.co/2658kUYsvr"", 'We built Habitat-Web, a scalable web-based infrastructure that connects Habitat simulator to mechanical turk users that allows us to collect human demonstrations for embodied tasks at scale.\n\nüßµ3/7 https://t.co/EDur1KmE32', 'Using Habitat-Web we collect a large dataset of ~92k human demonstrations in simulation.\n\nüßµ4/7 https://t.co/r3PZPcWogX', 'And Imitation Learning (IL) (with no bells or whistles) on 70k human demonstration works very well, outperforming Reinforcement Learning (RL) with 240k agent-gathered trajectories achieving state-of-the-art on ObjectNav.\n\nüßµ 5/7 https://t.co/4GdSWZMRGJ', 'These IL-trained agents demonstrates efficient object-search behavior -- it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view -- none of these are exhibited by as prominently by the RL agent.\n\nüßµ 6/7 https://t.co/LhVIKzzqZt', 'Read more about our paper - Habitat-Web: Learning Embodied Object-Search Strategies from Human Demonstrations at Scale (https://t.co/HZtyx5jF8g)\n\nProject Webpage: https://t.co/4paf2VltZ5\nCode: https://t.co/Fn9GlIYy6J\n\nüßµ 7/7', 'Thanks to my amazing collaborators Eric Undersander, @DhruvBatraDB, and @abhshkdz.']",https://arxiv.org/abs/2204.03514,"We present a large-scale study of imitating human demonstrations on tasks that require a virtual robot to search for objects in new environments -- (1) ObjectGoal Navigation (e.g. 'find & go to a chair') and (2) Pick&Place (e.g. 'find mug, pick mug, find counter, place mug on counter'). First, we develop a virtual teleoperation data-collection infrastructure -- connecting Habitat simulator running in a web browser to Amazon Mechanical Turk, allowing remote users to teleoperate virtual robots, safely and at scale. We collect 80k demonstrations for ObjectNav and 12k demonstrations for Pick&Place, which is an order of magnitude larger than existing human demonstration datasets in simulation or on real robots. Second, we attempt to answer the question -- how does large-scale imitation learning (IL) (which hasn't been hitherto possible) compare to reinforcement learning (RL) (which is the status quo)? On ObjectNav, we find that IL (with no bells or whistles) using 70k human demonstrations outperforms RL using 240k agent-gathered trajectories. The IL-trained agent demonstrates efficient object-search behavior -- it peeks into rooms, checks corners for small objects, turns in place to get a panoramic view -- none of these are exhibited as prominently by the RL agent, and to induce these behaviors via RL would require tedious reward engineering. Finally, accuracy vs. training data size plots show promising scaling behavior, suggesting that simply collecting more demonstrations is likely to advance the state of art further. On Pick&Place, the comparison is starker -- IL agents achieve ${\sim}$18% success on episodes with new object-receptacle locations when trained with 9.5k human demonstrations, while RL agents fail to get beyond 0%. Overall, our work provides compelling evidence for investing in large-scale imitation learning. Project page: this https URL ","Habitat-Web: Learning Embodied Object-Search Strategies from Human
  Demonstrations at Scale"
123,1512433937146167307,775759968221868032,Bo Wang,"['Big pre-trained models (e.g., Bert) not only disrupt NLP but also revolutionize computer vision now! See the new paper: Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection. abs: <LINK> This is a game-changer!']",http://arxiv.org/abs/2204.02964,"We present an approach to efficiently and effectively adapt a masked image modeling (MIM) pre-trained vanilla Vision Transformer (ViT) for object detection, which is based on our two novel observations: (i) A MIM pre-trained vanilla ViT can work surprisingly well in the challenging object-level recognition scenario even with random sampled partial observations, e.g., only 25% ~ 50% of the input sequence. (ii) In order to construct multi-scale representations for object detection, a random initialized compact convolutional stem supplants the pre-trained large kernel patchify stem, and its intermediate features can naturally serve as the higher resolution inputs of a feature pyramid without upsampling. While the pre-trained ViT is only regarded as the third-stage of our detector's backbone instead of the whole feature extractor, resulting in a ConvNet-ViT hybrid architecture. The proposed detector, named MIMDet, enables a MIM pre-trained vanilla ViT to outperform hierarchical Swin Transformer by 2.3 box AP and 2.5 mask AP on COCO, and achieve even better results compared with other adapted vanilla ViT using a more modest fine-tuning recipe while converging 2.8x faster. Code and pre-trained models are available at \url{this https URL}. ","Unleashing Vanilla Vision Transformer with Masked Image Modeling for
  Object Detection"
124,1512408416513413127,1252993183686025219,Oliver Philcox,"['New paper on the #HubbleTension!\n\nWe measure H0 from the matter-radiation equality scale using @Planck, @sdssurveys, and supernovae, finding H0 = 64.8+2.2-2.5 km/s/Mpc.\n\nThis is in 3 sigma tension with #SH0ES *without* assuming sound horizon physics! \n\n<LINK> <LINK>', 'The agreement of #H0 constraints from equality (z=3500) and recombination (z=1100) make it difficult for new physics to solve the #HubbleTension, as both scales must be altered in the same way. Maybe systematics are to blame??\n\n@ESA_Euclid and @desisurvey will tell us more soon!', 'We find similar results using only BOSS galaxies (H0=63.6+-3.3) and only Planck lensing (H0=67.1+-2.7) and our constraints are greatly aided by #pantheon supernovae!\n\nWe pass a number of checks for residual sound horizon dependence and are stable to changes in the neutrino prior. https://t.co/wRvl1pmZrY', 'Pleasure to collaborate on this with @farren_gerrit, Blake Sherwin, Eric Baxter at @Princeton, @the_IAS, @HarvardITC, @KICC_official and @UHIfA!']",https://arxiv.org/abs/2204.02984,"Many theoretical resolutions to the so-called ""Hubble tension"" rely on modifying the sound horizon at recombination, $r_s$, and thus the acoustic scale used as a standard ruler in the cosmic microwave background (CMB) and large scale structure (LSS) datasets. As shown in a number of recent works, these observables can also be used to compute $r_s$-independent constraints on $H_0$ by making use of the horizon scale at matter-radiation equality, $k_{\rm eq}$, which has different sensitivity to high redshift physics than $r_s$. In this work, we present the tightest $k_{\rm eq}$-based constraints on the expansion rate from current data, finding $H_0=64.8^{+2.2}_{-2.5}$ at 68$\%$ CL from a combination of BOSS galaxy power spectra, Planck CMB lensing, and the newly released Pantheon+ supernova constraints, as well as physical priors on the baryon density, neutrino mass, and spectral index (in $\mathrm{km}\,\mathrm{s}^{-1}\mathrm{Mpc}^{-1}$ units). The BOSS and Planck measurements have different degeneracy directions, leading to the improved combined constraints, with a bound of $H_0 = 67.1^{+2.5}_{-2.9}$ ($63.6^{+2.9}_{-3.6}$) from BOSS (Planck) alone. The results show some dependence on the neutrino mass bounds, with the constraint broadening to $H_0 = 68.0^{+2.9}_{-3.2}$ if we instead impose a weak prior on $\sum m_\nu$ from terrestrial experiments rather than assuming $\sum m_\nu<0.26\,\mathrm{eV}$, or shifting to $H_0 = 64.6\pm2.4$ if the neutrino mass is fixed to its minimal value. Even without any dependence on the sound horizon, our results are in $\approx 3\sigma$ tension with those obtained from the Cepheid-calibrated distance ladder, providing evidence against new physics models that vary $H_0$ by changing acoustic physics or the expansion history immediately prior to recombination. ","Determining the Hubble Constant without the Sound Horizon: A $3.6\%$
  Constraint on $H_0$ from Galaxy Surveys, CMB Lensing and Supernovae"
125,1512381845253640194,4866589137,Dr. Nathan Adams,"[""As well as the @LADUMA_meerKAT result yesterday, there is a new @mighteesurvey paper today! Led by Madalina Tudorache, a PhD student @OxfordPhysics, we've examined the connection between a galaxy's spin vector with nearby filaments of the cosmic web! <LINK>""]",https://arxiv.org/abs/2204.03041,"We study the 3D axis of rotation (3D spin) of 77 HI galaxies from the MIGHTEE-HI Early Science observations, and its relation to the filaments of the cosmic web. For this HI-selected sample, the alignment between the spin axis and the closest filament ($\lvert \cos \psi \rvert$) is higher for galaxies closer to the filaments, with $\langle\lvert \cos \psi \rvert\rangle= 0.66 \pm 0.04$ for galaxies $<5$ Mpc from their closest filament compared to $\langle\lvert \cos \psi \rvert\rangle= 0.37 \pm 0.08$ for galaxies at $5 < d <10$ Mpc. We find that galaxies with a low HI-to-stellar mass ratio ($\log_{10}(M_{\rm HI}/M_{\star}) < 0.11$) are more aligned with their closest filaments, with $\langle\lvert \cos \psi \rvert\rangle= 0.58 \pm 0.04$; whilst galaxies with ($\log_{10}(M_{\rm HI}/M_{\star}) > 0.11$) tend to be mis-aligned, with $\langle\lvert \cos \psi \rvert\rangle= 0.44 \pm 0.04$. We find tentative evidence that the spin axis of HI-selected galaxies tend to be aligned with associated filaments ($d<10$ Mpc), but this depends on the gas fractions. Galaxies that have accumulated more stellar mass compared to their gas mass tend towards stronger alignment. Our results suggest that those galaxies that have accrued high gas fraction with respect to their stellar mass may have had their spin axis alignment with the filament disrupted by a recent gas-rich merger, whereas the spin vector for those galaxies in which the neutral gas has not been strongly replenished through a recent merger tend to orientate towards alignment with the filament. We also investigate the spin transition between galaxies with a high HI content and a low HI content at a threshold of $M_{\mathrm{HI}}\approx 10^{9.5} M_{\odot}$ found in simulations, however we find no evidence for such a transition with the current data. ","MIGHTEE-HI: The relation between the HI gas in galaxies and the cosmic
  web"
126,1512372950221086721,888310189,Prof Mike Wood,"['#Chornobyl radiation spikes, reported in late February at the time of the Russian invasion, were not due to military vehicles disturbing soil.  Our new paper, available here in pre-print, explains why: <LINK> <LINK>', ""@ionactive @radioecology @SalfordScience @SalfordImpact @SRP_UK @RadioXchange @SalfordUni You should have just said you'd invented new notation for Cs-137/Ba-137m! üòâ""]",https://arxiv.org/abs/2204.03157,"On 25th February 2022, increased gamma radiation dose rates were reported within the Chornobyl Exclusion Zone (CEZ). This coincided with Russian military vehicles entering the Ukrainian part of the CEZ from neighbouring Belarus. It was speculated that contaminated soil resuspension by vehicle movements or a leak from the Chornobyl Nuclear Power Plant (ChNPP) complex may explain these spikes in radiation dose rates. The gamma dose rate monitoring network in the CEZ provides a crucial early warning system for releases of radioactivity to the environment and is part of the international safeguards for nuclear facilities. With the potential for further military action in the CEZ and concerns over nuclear safety, it is essential that such anomalous readings are investigated. We evaluate the hypotheses suggested to explain the apparent gamma dose rate increases, demonstrating that neither military vehicle-induced soil resuspension nor a leak from the ChNPP are plausible. However, disruption of the Chornobyl base-station's reception of wireless signals from the CRMS network may potentially explain the dose rate increases recorded. ","Chornobyl radiation spikes are not due to military vehicles disturbing
  soil"
127,1512341011367870467,1188106338116820992,Konrad Kollnig,"['Apple shook up stock markets with its privacy changes under iOS 14, but have these changes actually improved user privacy meaningfully?\n\nThis is what we investigated in a new @FAccTConference paper. @anastasia_shuba @RDBinns @emax @Nigel_Shadbolt \n\n<LINK>', '@FAccTConference @anastasia_shuba @RDBinns @emax @Nigel_Shadbolt Note that this is still a pre-print, and there might be minor changes of the paper until publication.\n\nWe analysed two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules.', '@FAccTConference @anastasia_shuba @RDBinns @emax @Nigel_Shadbolt Tracking users is now harder for apps because they cannot access the Identifier for Advertisers (IDFA) anymore. This is positive for user privacy.', '@FAccTConference @anastasia_shuba @RDBinns @emax @Nigel_Shadbolt We find evidence that Apple, itself, engages in tracking activities in forms that other companies would not be able to. For example, Apple regularly collects the UDID, which other developers have not had access to since 2013. This allows Apple to track device sales accurately.', ""@FAccTConference @anastasia_shuba @RDBinns @emax @Nigel_Shadbolt We also present evidence that cohort tracking + fingerprinting is still possible, and used to practice mitigate the impacts of Apple's changes. This means that more tracking is taking place through trackers' server-side code, which is nearly impossible for researchers to study."", ""@FAccTConference @anastasia_shuba @RDBinns @emax @Nigel_Shadbolt Overall, we conclude that Apple's changes have traded more privacy for more concentration of data collection with fewer tech companies. This underlines that privacy and competition problems can be highly intertwined in digital markets and need holistic study.""]",https://arxiv.org/abs/2204.03556,"Tracking is a highly privacy-invasive data collection practice that has been ubiquitous in mobile apps for many years due to its role in supporting advertising-based revenue models. In response, Apple introduced two significant changes with iOS 14: App Tracking Transparency (ATT), a mandatory opt-in system for enabling tracking on iOS, and Privacy Nutrition Labels, which disclose what kinds of data each app processes. So far, the impact of these changes on individual privacy and control has not been well understood. This paper addresses this gap by analysing two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules. We find that Apple's new policies, as promised, prevent the collection of the Identifier for Advertisers (IDFA), an identifier for cross-app tracking. Smaller data brokers that engage in invasive data practices will now face higher challenges in tracking users - a positive development for privacy. However, the number of tracking libraries has roughly stayed the same in the studied apps. Many apps still collect device information that can be used to track users at a group level (cohort tracking) or identify individuals probabilistically (fingerprinting). We find real-world evidence of apps computing and agreeing on a fingerprinting-derived identifier through the use of server-side code, thereby violating Apple's policies. We find that Apple itself engages in some forms of tracking and exempts invasive data practices like first-party tracking and credit scoring. We also find that the new Privacy Nutrition Labels are sometimes inaccurate and misleading. Overall, our findings suggest that, while tracking individual users is more difficult now, the changes reinforce existing market power of gatekeeper companies with access to large troves of first-party data and motivate a countermovement. ","Goodbye Tracking? Impact of iOS App Tracking Transparency and Privacy
  Labels"
128,1512280044529336320,1388198782924255232,Tristan Thrush,"['New ACL 2022 System Demo paper!\n\nIt used to take a lot of technical effort to set up custom AI tasks, evaluate models, and collect crowdworker data with models in-the-loop. We‚Äôve added a new framework to @DynabenchAI that aims to help: Dynatask.\n\n<LINK>\n\n1/3', 'To create a custom AI task on Dynabench, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated.', 'Thanks for all of the amazing teamwork! @kushgodt, @anmolgupta1707, @max_nlp, @EntilZhaPR, Tariq Kane, William Gaviria Rojas, Peter Mattson, @adinamwilliams, @douwekiela']",https://arxiv.org/abs/2204.01906,"We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at this https URL and the full library can be found at this https URL ",Dynatask: A Framework for Creating Dynamic AI Benchmark Tasks
129,1512238190219964420,838994466719449092,Junyu Liu,['<LINK> Our new blueprint paper is out!'],https://arxiv.org/abs/2204.03381,"It is for the first time that Quantum Simulation for High Energy Physics (HEP) is studied in the U.S. decadal particle-physics community planning, and in fact until recently, this was not considered a mainstream topic in the community. This fact speaks of a remarkable rate of growth of this subfield over the past few years, stimulated by the impressive advancements in Quantum Information Sciences (QIS) and associated technologies over the past decade, and the significant investment in this area by the government and private sectors in the U.S. and other countries. High-energy physicists have quickly identified problems of importance to our understanding of nature at the most fundamental level, from tiniest distances to cosmological extents, that are intractable with classical computers but may benefit from quantum advantage. They have initiated, and continue to carry out, a vigorous program in theory, algorithm, and hardware co-design for simulations of relevance to the HEP mission. This community whitepaper is an attempt to bring this exciting and yet challenging area of research to the spotlight, and to elaborate on what the promises, requirements, challenges, and potential solutions are over the next decade and beyond. ",Quantum Simulation for High Energy Physics
130,1512110847979671552,1258771704995815424,Beren Millidge,"['Excited to announce our new preprint: ‚ÄúHybrid Predictive Coding: Inferring, Fast and Slow‚Äù. This is joint work with @a_tschantz, @drclbuckley, and @anilkseth. A short thread on the paper <LINK> 1/11', 'Predictive Coding (PC) is often presented as a model of information processing in the visual cortex. However, in order to ensure that beliefs are correctly inferred, predictive coding networks (PCNs) require multiple inference iterations to converge 2/11', 'Yet, empirically, many aspects visual perception in the brain, such as core object recognition and gist perception,  are known to be accomplished in a single ‚Äòfeedforward‚Äô sweep, which currently has no counterpart in PCNs 3/11', 'Recurrent iterative processing is also slow, requiring many cycles until convergence. This is bad for biological brains that must respond rapidly to urgent stimuli, such as predators, and for modeling, where the inference phase constitutes the most costly computational step 4/11', 'We cast this feedforward sweep as amortized inference‚Äîlearning a mapping directly from observations to beliefs‚Äîand integrate it with a standard iterative PCN. Our hybrid PCN is thus bidirectional with both ascending and descending predictions and prediction errors 5/11 https://t.co/c4lX4bQTA2', 'Unlike a standard classifier, the amortized network is trained to map observations to the final beliefs of the iterative PCN and thus provide a computational ‚Äòshortcut‚Äô to accurate beliefs which only uses local information (the iterative beliefs) at each layer 6/11', 'By learning the mapping to the optimized beliefs of the iterative network, we provide a local layerwise objective which can be learnt in a biologically plausible way with only Hebbian connectivity 7/11 https://t.co/n7ivW6Qe8v', 'Our hybrid architecture possesses the benefits of both amortized and iterative inference, enabling rapid amortized initial guesses and fast responses to known stimuli, while maintaining the context sensitivity and disambiguation abilities of recurrent inference algorithms 8/11', 'Our hybrid network achieves equal or superior performance on computer vision tasks vs. iterative PCNs and is more stable to train. It can perform both classification and generation tasks simultaneously using the same network, and can adaptively increase computational effort 9/11', 'Back to neuroscience, our model may help explain the combination of feedforward sweep and recurrent processing observed in the visual system. We offer a mathematical description in terms of iterative and amortized inference to account for aspects of visual phenomenology 10/11', 'Just want to say again that it has been fantastic working with Alec, Anil, and Chris, and that the credit for the computational experiments should go to Alec Tschantz. The code for the paper can be found at https://t.co/iXk1sSVFWv 11/11']",https://arxiv.org/abs/2204.02169,"Predictive coding is an influential model of cortical neural activity. It proposes that perceptual beliefs are furnished by sequentially minimising ""prediction errors"" - the differences between predicted and observed data. Implicit in this proposal is the idea that perception requires multiple cycles of neural activity. This is at odds with evidence that several aspects of visual perception - including complex forms of object recognition - arise from an initial ""feedforward sweep"" that occurs on fast timescales which preclude substantial recurrent activity. Here, we propose that the feedforward sweep can be understood as performing amortized inference and recurrent processing can be understood as performing iterative inference. We propose a hybrid predictive coding network that combines both iterative and amortized inference in a principled manner by describing both in terms of a dual optimization of a single objective function. We show that the resulting scheme can be implemented in a biologically plausible neural architecture that approximates Bayesian inference utilising local Hebbian update rules. We demonstrate that our hybrid predictive coding model combines the benefits of both amortized and iterative inference -- obtaining rapid and computationally cheap perceptual inference for familiar data while maintaining the context-sensitivity, precision, and sample efficiency of iterative inference schemes. Moreover, we show how our model is inherently sensitive to its uncertainty and adaptively balances iterative and amortized inference to obtain accurate beliefs using minimum computational expense. Hybrid predictive coding offers a new perspective on the functional relevance of the feedforward and recurrent activity observed during visual perception and offers novel insights into distinct aspects of visual phenomenology. ","Hybrid Predictive Coding: Inferring, Fast and Slow"
131,1512078969599774730,1855871,Arkaitz Zubiaga,"['üîµ New paper led by @PeilingYi and forthcoming in #icwsm: detecting cases of cyberbullying in unseen social media platforms üëá\n\n""Cyberbullying detection across social media platforms via platform-aware adversarial encoding""\n\n<LINK> <LINK>']",https://arxiv.org/abs/2204.00334,"Despite the increasing interest in cyberbullying detection, existing efforts have largely been limited to experiments on a single platform and their generalisability across different social media platforms have received less attention. We propose XP-CB, a novel cross-platform framework based on Transformers and adversarial learning. XP-CB can enhance a Transformer leveraging unlabelled data from the source and target platforms to come up with a common representation while preventing platform-specific training. To validate our proposed framework, we experiment on cyberbullying datasets from three different platforms through six cross-platform configurations, showing its effectiveness with both BERT and RoBERTa as the underlying Transformer models. ","Cyberbullying detection across social media platforms via platform-aware
  adversarial encoding"
132,1512028313333682182,414250569,sara cazzoli,['Check out our new paper in arXiv about ‚ÄúUnexplored outflows in nearby low luminosity AGNs: the case of NGC1052‚Äù\nDo not miss it ! üëâ <LINK> <LINK>'],https://arxiv.org/abs/2204.02416,"Outflows play a central role in galaxy evolution shaping the properties of galaxies. Understanding outflows and their effects in low luminosity AGNs, such as LINERs, is essential (e.g. they are a numerous AGN population in the local Universe). We obtained VLT/MUSE and GTC/MEGARA optical IFS-data for NGC1052, the prototypical LINER. The stars are distributed in a dynamically hot disc, with a centrally peaked velocity dispersion map and large observed velocity amplitudes. The ionised gas, probed by the primary component is detected up to $\sim$30arcsec ($\sim$3.3 kpc) mostly in the polar direction with blue and red velocities ($\mid$V$\mid$$<$250 km/s). The velocity dispersion map shows a notable enhancement ($\sigma$$>$90 km/s) crossing the galaxy along the major axis of rotation in the central 10arcsec. The secondary component has a bipolar morphology, velocity dispersion larger than 150 km/s and velocities up to 660 km/s. A third component is detected but not spatially resolved. The maps of the NaD absorption indicate optically thick neutral gas with a velocity field consistent with a slow rotating disc ($\Delta$V = 77$\pm$12 km/s) but the velocity dispersion map is off-centred without any counterpart in the flux map. We found evidence of an ionised gas outflow with mass of 1.6$\pm$0.6 $\times$ 10$^{5}$ Msun, and mass rate of 0.4$\pm$0.2 Msun/yr. The outflow is propagating in a cocoon of gas with enhanced turbulence and might be triggering the onset of kpc-scale buoyant bubbles (polar emission). Taking into account the energy and kinetic power of the outflow (1.3$\pm$0.9 $\times$ 10$^{53}$ erg and 8.8$\pm$3.5 $\times$ 10$^{40}$ erg/s, respectively) as well as its alignment with both the jet and the cocoon, and that the gas is collisionally ionised, we consider that the outflow is jet-powered, although some contribution from the AGN is possible. ",Unexplored outflows in nearby low luminosity AGNs: the case of NGC 1052
133,1511974593464049665,1048613768668823553,Laila Linke,"[""It's #arXivday! Excited to present our newest paper, submitted to @aanda_journal and now on @arXiv! We are figuring out how #galaxies are distributed using a new tool: Galaxy-Galaxy-Galaxy-lensing (G3L). A üßµ 1/13 #paperday #cosmology <LINK> @AstroAngus"", 'Most ""stuff"" in the Universe is dark matter (DM). DM forms the cosmic web, the skeleton of the cosmic large-scale structure. Ordinary matter, such as gas or galaxies is distributed within this web. We are just not fully sure how. 2/13 Image: Springel+ (2005) https://t.co/d2YrBAAq8F', 'Dark matter forms halos:Giant self-bound spheres that also contain galaxies.But how many?This depends on the halos mass and the galaxy type.The average galaxy number in a halo is the Halo Occupation Distribution (HOD).In our paper we measure HODs with a new method using G3L. 3/13 https://t.co/U1Xpqcfu52', 'G3L is a gravitational lensing effect. Due to General Relativity masses distort images, much like a pair of glasses. If you compare the distortion of far-away galaxies with the position of foreground objects, you can understand how the mass relates to the foreground objects. 4/13', 'In our case, the foreground objects are galaxy pairs. So, we are measuring how much mass (most of it dark matter) exists around galaxy pairs. With this, we can infer where galaxy pairs form in the dark matter distribution. 5/13 https://t.co/K00J6GRsbU', 'We derived a model for the G3L signal and tested it with the Millennium Simulation - a recreation of our Universe in a computer. And - the model (black line) works! It can accurately describe the signal (Points) in the simulation. But what about real observations? 6/13 https://t.co/u2tEORmdam', 'We used data from three galaxy surveys and compared our model to the measured G3L. From this, we could infer the HODs of different galaxy types. We found for example that red galaxies prefer more massive halos compared to blue ones.  7/13 https://t.co/xovWKzgc7n', 'An interesting aspect of our method is that we can measure not only the HODs of each galaxy type but also how they depend on each other.Since we are looking at galaxy pairs we can find out how likely it is that pairs consisting of different types are in the same halo. Our result:', 'We find +ve correlations.This means:Red and blue galaxies are more likely to be in the same halo than expected by chance. We quantify this by the parameter r.r is positive for the observation (green) and the simulation (orange) and increases with halo mass. But why is that? 9/13 https://t.co/FuHZvrfaK7', 'One possible reason for the +ve correlation is that blue galaxies turn into red galaxies with a fixed probability. This means, that adding blue galaxies into a halo also increases the number of red galaxies over time. 10/13', 'There are several mechanisms that turn blue galaxies into red ones, for example, the stripping of gas due to tidal forces in a halo. Once the blue galaxy has no gas left, it cannot form new stars, and as blue stars die earlier, the galaxy turns red over time. 11/13', ""So what's next? Our study showed that G3L is a viable tool to learn about the distribution of galaxies. There are some kinks in the model we noticed that could be refined in a follow-up study. At our level of precision, they seem to not matter too much, though. 12/13"", ""Other things to do could be a comparison to other methods to infer HODs or exploring other galaxy types. But, most important right now: Waiting for the referee's comments, which might help improve the work even further :) 13/13""]",https://arxiv.org/abs/2204.02418,"Halo models and halo occupation distributions (HODs) are important tools to model the galaxy and matter distribution. We present and assess a new method for constraining the parameters of HODs using the gravitational lensing shear around galaxy pairs, galaxy-galaxy-galaxy-lensing (G3L). In contrast to galaxy-galaxy-lensing, G3L is sensitive to correlations between the per-halo numbers of galaxies from different populations. We use G3L to probe these correlations and test the default hypothesis that they are negligible. We derive a halo model for G3L and validate it with realistic mock data from the Millennium Simulation and a semi-analytic galaxy model. Then, we analyse public data from the Kilo-Degree Survey (KiDS), the VISTA Infrared Kilo-Degree Galaxy Survey (VIKING) and data from the Galaxy And Mass Assembly Survey (GAMA) to infer the HODs of galaxies at $z<0.5$ in five different stellar mass bins between $10^{8.5}h^{-2} M_\odot$ and $10^{11.5}h^{-2} M_\odot$ and two colours (red and blue), as well as correlations between satellite numbers. The analysis recovers the true HODs in the simulated data within the $68\%$ credibility range. The inferred HODs vary significantly with colour and stellar mass. There is also strong evidence ($>3\sigma$) for correlations, increasing with halo mass, between the numbers of red and blue satellites and galaxies with stellar masses below $10^{10} \Msun. Possible causes of these correlations are the selection of similar galaxies in different samples, the survey flux limit, or physical mechanisms like a fixed ratio between the satellite numbers of distinct populations. The decorrelation for halos with smaller masses is probably an effect of shot noise by low-occupancy halos. The inferred HODs can be used to complement galaxy-galaxy-lensing or galaxy clustering HOD studies or as input to cosmological analyses and improved mock galaxy catalogues. ","KiDS+VIKING+GAMA: Halo occupation distributions and correlations of
  satellite numbers with a new halo model of the galaxy-matter bispectrum for
  galaxy-galaxy-galaxy lensing"
134,1511902764690944000,36714410,Yang Li,"[""Don't just let the model tell how good your UI design is, but also how you can improve your design. See our new CHI paper: <LINK> and the released dataset: <LINK>. @zinosys @leebirrrd @bjo3rn <LINK>"", '@zinosys @leebirrrd @bjo3rn @iamzrchen']",https://arxiv.org/abs/2204.02448,"We use a deep learning based approach to predict whether a selected element in a mobile UI screenshot will be perceived by users as tappable, based on pixels only instead of view hierarchies required by previous work. To help designers better understand model predictions and to provide more actionable design feedback than predictions alone, we additionally use ML interpretability techniques to help explain the output of our model. We use XRAI to highlight areas in the input screenshot that most strongly influence the tappability prediction for the selected region, and use k-Nearest Neighbors to present the most similar mobile UIs from the dataset with opposing influences on tappability perception. ","Predicting and Explaining Mobile UI Tappability with Vision Modeling and
  Saliency Analysis"
135,1511796332788342791,493582529,Michele Lucente üá∫üá¶,"['New paper out! I am very happy with this one: we show that particle physics scenarios producing non-thermal dark radiation in the early Universe can be efficiently probed by long-lived particle searches at the LHC.   \n\n1/4\n\n<LINK>', 'It turns out that LHC searches can be complementary or  even superior to cosmological ones. For instance, in the model we study the CMB-S4 target value ‚àÜN_eff = 0.06 is already excluded by current searches, and even smaller values can be probed. \n\n2/4 https://t.co/KNaPgodONK', 'As an interesting by-product we realised that prompt searches at the LHC put relevant bounds to macroscopic lifetimes as well. There is actually a very strong complementarity between prompt and displaced searches in probing the lifetime parameter space \n\n3/4', 'Thank you very much to Felix and Elias for the very interesting collaboration, and special thanks to Alessandro who led the project. \n\n4/4']",https://arxiv.org/abs/2204.01759,"In this work we explore the intriguing connections between searches for long-lived particles (LLPs) at the LHC and early universe cosmology. We study the non-thermal production of ultra-relativistic particles (i.e. dark radiation) in the early universe via the decay of weak-scale LLPs and show that the cosmologically interesting range $\Delta N_\text{eff} \sim 0.01-0.1$ corresponds to LLP decay lengths in the mm to cm range. These decay lengths lie at the boundary between prompt and displaced signatures at the LHC and can be comprehensively explored by combining searches for both. To illustrate this point, we consider a scenario where the LLP decays into a charged lepton and a (nearly) massless invisible particle. By reinterpreting searches for promptly decaying sleptons and for displaced leptons at both ATLAS and CMS we can then directly compare LHC exclusions with cosmological observables. We find that the CMB-S4 target value of $\Delta N_\text{eff}=0.06$ is already excluded by current LHC searches and even smaller values can be probed for LLP masses at the electroweak scale. ",Searching for dark radiation at the LHC
136,1511781759301693450,1355984415248293895,Bruno Valeixo Bento,"['New paper finally out üòÅ\nCool work on gravity, warped throats and braneworlds, with Dibya Chakraborty, Susha Parameswaran and Ivonne Zavala! \n\n<LINK>', ""@LandgrenFilip Thanks! I think that means you're doing some interesting stuff üòâ""]",https://arxiv.org/abs/2204.02086,"We study the gravitational signatures that arise from compactifying Type IIB supergravity on a compact space containing a Klebanov-Strassler warped throat. After reviewing the dimensional reduction of the 10d graviton and explicitly obtaining the equations of motion for the 4d tensor $h_{\mu\nu}$, vector $h_{\mu n}$ and scalar $h_{mn}$ modes, we find the masses and wavefunctions of the Kaluza-Klein tower of spin-2 states. We explore how the masses and wavefunctions depend on the balance between the strength of the warping and the size of the bulk, and how these relate to the range and strength of the interactions which correct the Newtonian gravitational potential. By computing the modified Newtonian potential for sources on a brane somewhere along the throat, and applying consistency constraints on the Klebanov-Strassler parameters, we obtain predictions for the phenomenological parameter space. In the case of a fully warped throat, and depending on where the brane is along the throat, these predictions are narrow in range and consistent with current observational and experimental constraints. We also begin an exploration of gravitational wave signatures of KK gravitons in warped throats, finding that strong warping can bring the corresponding frequencies down to the windows of current and proposed experiments. ",Gravity at the Tip of the Throat
137,1511764384439316485,95734122,Cl√©ment Livache,"['Our paper on optically excited ASE in a fully-working, high current quantum dot LED is available on arXiv! We show that we can make a solution-processed LED that supports light amplification thanks to new high-gain dots and reduced optical losses. <LINK>']",https://arxiv.org/abs/2204.01929,"Laser diodes based on solution-processable materials could benefit numerous technologies including integrated electronics and photonics, telecommunication, and medical diagnostics. An attractive system for implementing these devices is colloidal semiconductor quantum dots (QDs). The primary challenge that hampered progress towards a QD laser diode (QLD) has been fast nonradiative Auger decay of optical-gain-active multicarrier states. Recently, this problem has been resolved by employing continuously graded QDs (cg-QDs) wherein Auger recombination is strongly suppressed. The use of these structures allowed for demonstrations of optical gain with electrical pumping and optically-excited lasing in multilayered LED-like devices. Here we report on achieving the next critical milestone towards a QLD, which is the demonstration of optically excited amplified spontaneous emission from a fully functional high-current density electroluminescent device. This advance has become possible due to excellent optical gain properties of novel 'compact' cg-QDs and a new LED architecture, which allows for concerted optimization of its optical and electrical properties. The results of this work strongly suggest the feasibility of the final step towards a functional QLD, which is the demonstration of lasing with electrical pumping. ","Optically Excited Two-Band Amplified Spontaneous Emission from a
  High-Current-Density Quantum-Dot LED"
138,1511681676115591176,328430286,Jad C. Halimeh,"['New paper <LINK>. Through a finite-scaling analysis of the truncated Schwinger model, we show that scarring is a salient feature of lattice quantum electrodynamics in massless quenches of extreme vacuum states.\n@JDesaules\n@theoryleeds\n@MCQST_cluster\n@ERC_Research <LINK>', 'Our work settles the question as to whether scars are an artifact of quantum link formulations of U(1) gauge theories or an inherent feature thereof, in favor of the latter. Another great collaboration with @theoryleeds, Debasish, and Arnab.']",https://arxiv.org/abs/2204.01745,"The high level of control and precision achievable in current synthetic quantum matter setups has enabled first attempts at quantum-simulating various intriguing phenomena in condensed matter physics, including those probing thermalization or its absence in closed quantum systems. In a recent work [Desaules \textit{et al.} [arXiv:2203.08830], we have shown that quantum many-body scars -- special low-entropy eigenstates that weakly break ergodicity in nonintegrable systems -- arise in spin-$S$ quantum link models that converge to $(1+1)-$D lattice quantum electrodynamics (Schwinger model) in the Kogut--Susskind limit $S\to\infty$. In this work, we further demonstrate that quantum many-body scars exist in a truncated version of the Schwinger model, and are qualitatively more prominent than their counterparts in spin-$S$ quantum link models. We illustrate this by, among other things, performing a finite-$S$ scaling analysis that strongly suggests that scarring persists in the truncated Schwinger model in the limit $S\to\infty$. Although it does not asymptotically converge to the Schwinger model, the truncated formulation is relevant to synthetic quantum matter experiments, and also provides fundamental insight into the nature of quantum many-body scars, their connection to lattice gauge theories, and the thermalization dynamics of the latter. Our conclusions can be readily tested in current cold-atom setups. ",Prominent quantum many-body scars in a truncated Schwinger model
139,1511606594034970628,1063135448493686785,Sergio Llana,"['IS IT WORTH THE EFFORT?\n\nNew paper to understand players and teams‚Äô high-intensity efforts.\nEstimation of the impact of a player‚Äôs run on his team‚Äôs possession value.\nDeveloped with 500+ matches of tracking data of the big-5 European competitions.\n\nLink: <LINK> <LINK>', 'We detect the players‚Äô runs from tracking data and we contextualize them thanks to different layers of tactical concepts to better understand high-intensity efforts. Some of the concepts taken into account are the possession phases, defensive lines, and the attack/defense types. https://t.co/hkQrq4Q5j8', 'Do teams with higher possession percentages run less? Do all players run in the same way?\n\nWe create team and player profiles that mix tactical and physical metrics derived from HI runs. As an example, we discuss the relationship between more ball possession and running less. https://t.co/tGlKpE6Pe5', 'How much does a player depend on his speed to add value?\n\nWe present examples of how different strikers tend to do on-ball actions at different speeds. But we highlight those who rely on their HI runs to add value from those who are consistent in adding value at any speed. https://t.co/cMbp4uEJGi', ""What is the impact of the HI runs on the possession value?\n\nWe aim to distinguish the players who run more from the ones who run better by estimating the impact of their high-intensity runs on the increase in their team's possession value (via EPV). https://t.co/ag0x5NRfTW"", 'We see this work as a new path of research thanks to the impact on the different profiles of experts in a soccer club. From players and coaches to scouts and even physical coaches.\n\nCo-authors: @BorjaBurriel @PauMadrero @JaviOnData\nData: @SkillCorner @StatsBomb']",https://arxiv.org/abs/2204.02313,"We present a framework that gives a deep insight into the link between physical and technical-tactical aspects of soccer and it allows associating physical performance with value generation thanks to a top-down approach. First, we estimate physical indicators from tracking data. Then, we contextualize each player's run to understand better the purpose and circumstances in which it is done, adding a new dimension to the creation of team and player profiles. Finally, we assess the value-added by off-ball high-intensity runs by linking with a possession-value model. This novel approach allows answering practical questions from very different profiles of practitioners within a soccer club, from analysts, coaches, and scouts to physical coaches and readaptation physiotherapists. ","Is it worth the effort? Understanding and contextualizing physical
  metrics in soccer"
140,1511581785557291008,1283150444,Maurizio Pierini,"['New paper in the NPLM saga: by using kernel methods with the Falkon library, the whole training time is now measured in sec, rather than hour. A step fwd to the deployment of this method in LHC analyses <LINK> @Dakkar162 @GrossoGaia @lrntzrsc @mzanetti79']",https://arxiv.org/abs/2204.02317,"We present a machine learning approach for model-independent new physics searches. The corresponding algorithm is powered by recent large-scale implementations of kernel methods, nonparametric learning algorithms that can approximate any continuous function given enough data. Based on the original proposal by D'Agnolo and Wulzer (arXiv:1806.02350), the model evaluates the compatibility between experimental data and a reference model, by implementing a hypothesis testing procedure based on the likelihood ratio. Model-independence is enforced by avoiding any prior assumption about the presence or shape of new physics components in the measurements. We show that our approach has dramatic advantages compared to neural network implementations in terms of training times and computational resources, while maintaining comparable performances. In particular, we conduct our tests on higher dimensional datasets, a step forward with respect to previous studies. ",Learning new physics efficiently with nonparametric methods
141,1511575910687858707,1110110589202956289,Alessandro Strumia,['New paper and its 10-minute webinar presentation <LINK>\n<LINK> <LINK>'],https://arxiv.org/abs/2204.01744,"We consider the classically scale invariant Higgs-dilaton model of dynamical symmetry breaking extended with an extra scalar field that plays the role of dark matter. The Higgs boson is light near a critical boundary between different symmetry breaking phases, where quantum corrections beyond the usual Gildener-Weinberg approximation become relevant. This implies a tighter connection between dark matter and Higgs phenomenology. The model has only three free parameters, yet it allows for the observed relic abundance of dark matter while respecting all constraints. The direct detection cross section mediated by the Higgs boson is determined by the dark matter mass alone and is testable at future experiments. ",Dark Matter-Induced Multi-Phase Dynamical Symmetry Breaking
142,1511480013870157828,1134563269262360577,Yang Zheng,"[""Two new papers on arXiv: \n1. <LINK>. The optimization landscape of LQG problems has saddle points. We show in this paper that many of them are not bad in the sense that they are ``equivalent'' to strict saddles. Perturbed gradient descent can escape them quickly."", '2. https://t.co/x6vNhfpohQ. Controller parameterization in the frequency domain is convex but infinitely dimensional. They have no immediately efficient computation. In this paper, we make a somewhat surprising connection with filtering and introduce the first LMI for computation']",https://arxiv.org/abs/2204.00912,"First order policy optimization has been widely used in reinforcement learning. It guarantees to find the optimal policy for the state-feedback linear quadratic regulator (LQR). However, the performance of policy optimization remains unclear for the linear quadratic Gaussian (LQG) control where the LQG cost has spurious suboptimal stationary points. In this paper, we introduce a novel perturbed policy gradient (PGD) method to escape a large class of bad stationary points (including high-order saddles). In particular, based on the specific structure of LQG, we introduce a novel reparameterization procedure which converts the iterate from a high-order saddle to a strict saddle, from which standard random perturbations in PGD can escape efficiently. We further characterize the high-order saddles that can be escaped by our algorithm. ","Escaping High-order Saddles in Policy Optimization for Linear Quadratic
  Gaussian (LQG) Control"
143,1511353812480663553,198132424,Claudio Pinhanez,['We just released a paper arguing for a new scientific foundation to address fake news and disinformation. Great work together of the IBM Research labs of Brazil and Almaden.\n<LINK>\n#ibmresearch #disinformation #fakenews'],https://arxiv.org/abs/2204.01489,"How can we best address the dangerous impact that deep learning-generated fake audios, photographs, and videos (a.k.a. deepfakes) may have in personal and societal life? We foresee that the availability of cheap deepfake technology will create a second wave of disinformation where people will receive specific, personalized disinformation through different channels, making the current approaches to fight disinformation obsolete. We argue that fake media has to be seen as an upcoming cybersecurity problem, and we have to shift from combating its spread to a prevention and cure framework where users have available ways to verify, challenge, and argue against the veracity of each piece of media they are exposed to. To create the technologies behind this framework, we propose that a new Science of Disinformation is needed, one which creates a theoretical framework both for the processes of communication and consumption of false content. Key scientific and technological challenges facing this research agenda are listed and discussed in the light of state-of-art technologies for fake media generation and detection, argument finding and construction, and how to effectively engage users in the prevention and cure processes. ",Towards a New Science of Disinformation
144,1511348060756090884,19089454,Dr. Teddy Kareta,"['Interested in the properties of low-activity comets or in back-up targets for @CometIntercept? You should read our new paper accepted @AAS_PSJ ""Near-Infrared Spectroscopy Of The Nucleus Of Low-Activity Comet P/2016 BA14 During Its 2016 Close Approach"" <LINK> <LINK> <LINK>', 'We looked at P/2016 BA14 (PanSTARRS), a kilometer-scale comet with only about 0.01% of its surface releasing gas &amp; dust. A previous study using Subaru data showed it had a super strange mid-IR spectrum, but our obs. at the NASA IRTF showed it looked pretty normal in the near-IR. https://t.co/b6WXepuu0q', ""The turn-up at longer wavelengths in the previous plot is direct thermal emission from its nucleus*, but the reflected light from the surface is likely phase-reddened. Disentangling thermal emission from phase reddening with these kinds of observations hasn't exactly been done..."", '...so we tried messing around with a few techniques and came up with a workable correction to make sure we still get reasonable albedos from our thermal models. Radar said this thing had a 3%-or-less albedo: we agree. A few comet nuclei and ~2% of NEOs have albedos this low. https://t.co/lRvdyFOZ0I', ""In addition to our newly MCMC-ified thermal model (which if you'd like, email me!), we also discuss the relevance of this kinda work to Lucy, Comet Interceptor, and to understanding dormant-and-low-activity comets generally."", ""There's a lot of other stuff buried in the paper! Take a look. Thanks to coauthors @moonyguy, Juan Sanchez, and @WaltHarris. I *think* this is the first peer-reviewed thing I'm on (or lead) that has me affiliated with @LowellObs, so huzzah to that.""]",https://arxiv.org/abs/2204.01104,"The Near-Earth Comet P/2016 BA$_{14}$ (PanSTARRS) is a slow-rotatating nearly-dormant object, a likely dynamical twin of 252P/LINEAR, and was recently shown to have a mid-infrared spectrum very dissimilar to other comets. BA$_{14}$ also recently selected one of the back-up targets for the ESA's \textit{Comet Interceptor}, so a clearer understanding of BA$_{14}$'s modern properties would not just improve our understanding of how comets go dormant, but could also aid planning for a potential spacecraft visit. We present observations of BA$_{14}$ taken on two dates during its 2016 Earth close approach with the NASA Infrared Telescope Facility, both of which are consistent with direct observations of its nucleus. The reflectance spectrum of BA$_{14}$ is similar to 67P/Churyumov-Gerasimenko, albeit highly phase-reddened. Thermal emission contaminates the reflectance spectrum at longer wavelengths, which we correct with a new Markov Chain Monte Carlo thermal modeling code. The models suggest $BA_{14}$'s visible geometric albedo is $p_V=0.01-0.03$, consistent with radar observations, its beaming parameter is typical for NEOs observed in its geometry, and its reflectrance spectrum is red and linear throughout H and K band. It appears very much like a ""normal"" comet nucleus, despite its mid-infrared oddities. A slow loss of fine grains as the object's activity diminished might help to reconcile some of the lines of evidence, and we discuss other possibilities. A spacecraft flyby past BA$_{14}$ could get closer to the nucleus than with a more active target, and we highlight some science questions that could be addressed with a visit to a (nearly-)dormant comet. ","Near-Infrared Spectroscopy Of The Nucleus Of Low-Activity Comet P/2016
  BA$_{14}$ During Its 2016 Close Approach"
145,1511304112629035011,1203290389307891713,Saugata Basu,"['New paper with Daniel Perrucci on topology of real varieties defined by multiaffine polynomials, symmetric functions in an infinite number of variables, stability and all the good stuff. Comments most welcome.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2204.01595,"Let $\mathrm{R}$ be a real closed field. We prove that the number of semi-algebraically connected components of a real hypersurface in $\mathrm{R}^n$ defined by a multi-affine polynomial of degree $d$ is bounded by $2^{d-1}$. This bound is sharp and is independent of $n$ (as opposed to the classical bound of $d(2d -1)^{n-1}$ on the Betti numbers of hypersurfaces defined by arbitrary polynomials of degree $d$ in $\mathrm{R}^n$ due to Petrovski{\u\i} and Ole{\u\i}nik, Thom and Milnor). Moreover, we show there exists $c > 1$, such that given a sequence $(B_n)_{n >0}$ where $B_n$ is a closed ball in $\mathrm{R}^n$ of positive radious, there exist hypersurfaces $(V_n)_{n_>0}$ defined by symmetric multi-affine polynomials of degree $4$, such that $\sum_{i \leq 5} b_i(V_n \cap B_n) > c^n$, where $b_i(\cdot)$ denotes the $i$-th Betti number with rational coeffcients. Finally, as an application of the main result of the paper we verify a representational stability conjecture due to Basu and Riener on the cohomology modules of symmetric real algebraic sets for a new and much larger class of symmetric real algebraic sets than known before. ","Topology of real multi-affine hypersurfaces and a homological stability
  property"
146,1511278833600798725,780754694419296256,Markus C. Braun,['New paper by @JoSQUANTUM !! Error Resilient Quantum Amplitude Estimation from Parallel Quantum Phase Estimation\n#QuantumComputing #montecarlo \n<LINK>'],https://arxiv.org/abs/2204.01337,"We show how phase and amplitude estimation algorithms can be parallelized. This can reduce the gate depth of the quantum circuits to that of a single Grover operator with a small overhead. Further, we show that for quantum amplitude estimation, the parallelization can lead to vast improvements in resilience against quantum errors. The resilience is not caused by the lower gate depth, but by the structure of the algorithm. Even in cases with errors that make it impossible to read out the exact or approximate solutions from conventional amplitude estimation, our parallel approach provided the correct solution with high probability. The results on error resilience hold for the standard version and for low depth versions of quantum amplitude estimation. Methods presented are subject of a patent application [Quantum computing device: Patent application EP 21207022.1]. ","Error Resilient Quantum Amplitude Estimation from Parallel Quantum Phase
  Estimation"
147,1511271289679396866,822867138,Bradley Kavanagh,"['New paper today! In a paper lead by \n@IoP_UvA PhD student Joran Angevaare, we looked at how different detectors looking for #DarkMatter may give us complementary information about this new particle in the event of a detection: <LINK> <LINK>', ""In particular, we thought about how searches for nuclear recoils might complement searches for the so-called 'Migdal effect': when a nuclear recoil leads to the ionisation of an electron (see e.g. https://t.co/CfQyhsvpPa), which might be easier to see! https://t.co/9IWaabDzqD"", 'To quantify this, we define a parameter œï, which measures how well we could constrain the mass M + cross section œÉ of the DM - smaller is better!\n\nFor DM &lt; mass of the proton, the constraining power œï is substantially improved by using data from standard nuclear + Migdal searches https://t.co/RcTV7eG6Ts']",https://arxiv.org/abs/2204.01580,"Dark Matter experiments searching for Weakly interacting massive particles (WIMPs) primarily use nuclear recoils (NRs) in their attempt to detect WIMPs. Migdal-induced electronic recoils (ERs) provide additional sensitivity to light Dark Matter with $\mathcal{O}(\text{GeV}/c^2)$ masses. In this work, we use Bayesian inference to find the parameter space where future detectors like XENONnT and SuperCDMS SNOLAB will be able to detect WIMP Dark Matter through NRs, Migdal-induced ERs or a combination thereof. We identify regions where each detector is best at constraining the Dark Matter mass and spin independent cross-section and infer where two or more detection configurations are complementary to constraining these Dark Matter parameters through a combined analysis. ","Complementarity of direct detection experiments in search of light Dark
  Matter"
148,1511260351307821056,1133734070293323776,Hammer Lab ML,"['New (wet) paper alert ü•≥üåä\non learning and adapting to the #dynamics in #water distribution networks.\nA #preprint of our work ""SAM-kNN Regressor for Online Learning in Water Distribution Networks"" by Jonathan Jakob and Andr√© Artelt now available @arxiv\n<LINK>']",https://arxiv.org/abs/2204.01436,"Water distribution networks are a key component of modern infrastructure for housing and industry. They transport and distribute water via widely branched networks from sources to consumers. In order to guarantee a working network at all times, the water supply company continuously monitors the network and takes actions when necessary -- e.g. reacting to leakages, sensor faults and drops in water quality. Since real world networks are too large and complex to be monitored by a human, algorithmic monitoring systems have been developed. A popular type of such systems are residual based anomaly detection systems that can detect events such as leakages and sensor faults. For a continuous high quality monitoring, it is necessary for these systems to adapt to changed demands and presence of various anomalies. In this work, we propose an adaption of the incremental SAM-kNN classifier for regression to build a residual based anomaly detection system for water distribution networks that is able to adapt to any kind of change. ",SAM-kNN Regressor for Online Learning in Water Distribution Networks
149,1511222484879785986,1283150444,Maurizio Pierini,"['New paper on particle reconstruction with #GraphNetworks: Distance Weighted message passing and object condensation to reconstruct particles from 200 simultaneous collisions. Great work by @ShahRukhQasim <LINK> #DeepLearning #AI #LHC #HEP <LINK>', '@JoosepPata @ShahRukhQasim I think we should put it on @ZENODO_ORG , indeed @ShahRukhQasim @JanKieseler']",https://arxiv.org/abs/2204.01681,"We present an end-to-end reconstruction algorithm to build particle candidates from detector hits in next-generation granular calorimeters similar to that foreseen for the high-luminosity upgrade of the CMS detector. The algorithm exploits a distance-weighted graph neural network, trained with object condensation, a graph segmentation technique. Through a single-shot approach, the reconstruction task is paired with energy regression. We describe the reconstruction performance in terms of efficiency as well as in terms of energy resolution. In addition, we show the jet reconstruction performance of our method and discuss its inference computational cost. To our knowledge, this work is the first-ever example of single-shot calorimetric reconstruction of ${\cal O}(1000)$ particles in high-luminosity conditions with 200 pileup. ","End-to-end multi-particle reconstruction in high occupancy imaging
  calorimeters with graph neural networks"
150,1511195640272855040,1361405140784345092,Shawn Geller,['Our new paper on adaptive strategies for measurement of quantum systems is now on arXiv: <LINK>'],https://arxiv.org/abs/2204.00710,"For many quantum systems intended for information processing, one detects the logical state of a qubit by integrating a continuously observed quantity over time. For example, ion and atom qubits are typically measured by driving a cycling transition and counting the number of photons observed from the resulting fluorescence. Instead of recording only the total observed count in a fixed time interval, one can observe the photon arrival times and get a state detection advantage by using the temporal structure in a model such as a Hidden Markov Model. We study what further advantage may be achieved by applying pulses to adaptively transform the state during the observation. We give a three-state example where adaptively chosen transformations yield a clear advantage, and we compare performances on an ion example, where we see improvements in some regimes. We provide a software package that can be used for exploration of temporally resolved strategies with and without adaptively chosen transformations. ",Improving quantum state detection with adaptive sequential observations
151,1511181276094648324,1508991637850107910,Nhat Ho,"['I would like to share a new paper that @khainb12 and I just finished on revising the usage of sliced Wasserstein distance for ML and DL applications with images (e.g., GAN, Domain Adaptation, etc.): <LINK>', 'The idea here is that sliced Wasserstein only defines for data in vector form. When apply to images, which are in tensor form, we usually vectorized the images and then project them to a one-dimensional subspace via Radon transform. The vectorization step can be sub-optimal.', 'To address the issue of vectorization, we incorporate convolution operators (with stride, dilation, non-linear activation,etc.) into the Radon transform of images. It not only can keep the spatial structure of images but also greatly save memory (with far fewer parameters).']",https://arxiv.org/abs/2204.01188,"The conventional sliced Wasserstein is defined between two probability measures that have realizations as vectors. When comparing two probability measures over images, practitioners first need to vectorize images and then project them to one-dimensional space by using matrix multiplication between the sample matrix and the projection matrix. After that, the sliced Wasserstein is evaluated by averaging the two corresponding one-dimensional projected probability measures. However, this approach has two limitations. The first limitation is that the spatial structure of images is not captured efficiently by the vectorization step; therefore, the later slicing process becomes harder to gather the discrepancy information. The second limitation is memory inefficiency since each slicing direction is a vector that has the same dimension as the images. To address these limitations, we propose novel slicing methods for sliced Wasserstein between probability measures over images that are based on the convolution operators. We derive convolution sliced Wasserstein (CSW) and its variants via incorporating stride, dilation, and non-linear activation function into the convolution operators. We investigate the metricity of CSW as well as its sample complexity, its computational complexity, and its connection to conventional sliced Wasserstein distances. Finally, we demonstrate the favorable performance of CSW over the conventional sliced Wasserstein in comparing probability measures over images and in training deep generative modeling on images. ","Revisiting Sliced Wasserstein on Images: From Vectorization to
  Convolution"
152,1511156712388005888,15327263,Carl-Johan Haster,"['New paper on the ArXiv led by @MITKavli grad student @sylvia_bisco.\nWe look at the population of observed binary black holes (BBHs) to see if there are any correlations between the spin-mass-redshift properties of the population, and we found something!\n<LINK>', 'There is a robust correlation between the effective binary spin (chiEff) and redshift, in that the width of the chiEff distribution increases with increasing redshift.', 'Apart from Sylvia and myself, this project includes Tom Callister (from @FlatironCCA), Ken Ng (also a grad student at @MITKavli) as well as @sasomao and @farrwill.', 'And it continues on the exciting @FlatironCCA-@MITKavli ""correlation-collaboration"" that was started last year with https://t.co/9axMQU2o5U where we found a correlation between chiEff and the mass ratio, again for the population of observed BBHs.']",https://arxiv.org/abs/2204.01578,"The population-level distributions of the masses, spins, and redshifts of binary black holes (BBHs) observed using gravitational waves can shed light on how these systems form and evolve. Because of the complex astrophysical processes shaping the inferred BBH population, models allowing for correlations among these parameters will be necessary to fully characterize these sources. We hierarchically analyze the BBH population detected by LIGO and Virgo with a model allowing for correlations between the effective aligned spin and the primary mass and redshift. We find that the width of the effective spin distribution grows with redshift at 98.6% credibility. We determine this trend to be robust under the application of several alternative models and additionally verify that such a correlation is unlikely to be spuriously introduced using a simulated population. We discuss the possibility that this correlation could be due to a change in the natal black hole spin distribution with redshift. ",The binary black hole spin distribution likely broadens with redshift
153,1510874708392259592,2563532985,Prof Anna Watts,"['New paper! ""GRMHD Simulations of Accreting Neutron Stars with Non-Dipole Fields"" led by @pushpita_das_ with @DrOliverPorth.  What might the NICER pulsars, with their non-dipolar magnetic fields, have looked like during their accretion phase?  \n\n<LINK>']",https://arxiv.org/abs/2204.00249,"NASA's NICER telescope has recently provided evidence for non-dipolar magnetic field structures in rotation-powered millisecond pulsars. These stars are assumed to have gone through a prolonged accretion spin-up phase, begging the question of what accretion flows onto stars with complex magnetic fields would look like. We present results from a suite of GRMHD simulations of accreting neutron stars for dipole, quadrupole, and quadrudipolar stellar field geometries. This is a first step towards simulating realistic hotspot shapes in a general relativistic framework to understand hotspot variability in accreting millisecond pulsars. We find that the location and size of the accretion columns resulting in hotspots changes significantly depending on initial stellar field strength and geometry. We also find that the strongest contributions to the stellar torque are from disk-connected fieldlines and the pulsar wind, leading to spin-down in almost all of the parameter regime explored here. We further analyze angular momentum transport in the accretion disk due to large scale magnetic stresses, turbulent stresses, wind- and compressible effects which we identify with convective motions. The disk collimates the initial open stellar flux forming jets. For dipoles, the disk-magnetosphere interaction can either enhance or reduce jet power compared to the isolated case. However for quadrupoles, the disk always leads to an enhanced net open flux making the jet power comparable to the dipolar case. We discuss our results in the context of observed neutron star jets and provide a viable mechanism to explain radio power both in the low- and high-magnetic field case. ",GRMHD Simulations of Accreting Neutron Stars with Non-Dipole Fields
154,1510872932448452613,847986180,Fred Jendrzejewski,"['We have a new paper that came out on the arXiv today. It is a collaboration of @SynQS, @HaukeGroup @ICFOnians @Fraunhofer_AST and the Honda research institute <LINK>. It focuses on the application of qudits on optimization problems.', 'In quantum optimization the focus is often put on problems which have only binary variables. In classical optimization the case of continuous variables is especially nice as it is rather easy to solve.', 'The case of integer variables is especially interesting as it is hard in both cases and qudits are the natural implementation. And this is exactly the mapping we focus on here...']",https://arxiv.org/abs/2204.00340,"A frequent starting point of quantum computation platforms are two-state quantum systems, i.e., qubits. However, in the context of integer optimization problems, relevant to scheduling optimization and operations research, it is often more resource-efficient to employ quantum systems with more than two basis states, so-called qudits. Here, we discuss the quantum approximate optimization algorithm (QAOA) for qudits, layout its implementation in platforms with long-range interactions between qudits such as trapped ions, cold atomic mixtures, Rydberg atoms and atoms in cavities. We illustrate how the QAOA can be used to formulate a variety of integer optimization problems such as graph coloring problems or electric vehicle (EV) charging optimization. In addition, we comment on the implementation of constraints and describe three methods to include these into a quantum circuit of a QAOA by penalty contributions to the cost Hamiltonian, conditional gates using ancilla qubits, and a dynamical decoupling strategy. Finally, as a showcase of qudit-based QAOA, we present numerical results for a charging optimization problem mapped onto a max-$k$-graph coloring problem. Our work illustrates the flexibility of qudit systems to solve integer optimization problems. ","Quantum approximate optimization algorithm for qudit systems with
  long-range interactions"
155,1510790979326029829,15127834,Robert McNees,"['Our long national nightmare of not enough papers by me is finally over. (New paper on the @arxiv.)\n<LINK> <LINK>', '@VergaraLautaro @arxiv We expect the model to be dual to something like the Sachdev‚ÄìYe‚ÄìKitaev model (describing strongly correlated fermions), and this behavior is precisely what Susskind conjectures for the SYK model at low and high temperatures.']",https://arxiv.org/abs/2204.00045,"We introduce a family of 2D dilaton gravity models with state-dependent constant curvature so that dS$_2$ emerges as an excitation of AdS$_2$. Curiously, the strong coupling region corresponds to the asymptotic region geometrically. Apart from these key differences, many features resemble the Almheiri--Polchinski model. We discuss perturbative and non-perturbative thermodynamical stability, bubble nucleation through matter shockwaves, and semiclassical backreaction effects. In some of these models, we find that low temperatures are dominated by AdS$_2$ but high temperatures are dominated by dS$_2$, concurrent with a recent proposal by Susskind. ",dS$\boldsymbol{_2}$ as excitation of AdS$\boldsymbol{_2}$
156,1522345910079959040,1039548691114221568,Shuyang Li,"['We can instill type awareness in language models via QA-based pre-training!\n\nNew work with Mukund Sridhar, Chandana Satya Prakash, Jin Cao, Wael Hamza, and Julian McAuley accepted to Findings of #NAACL2022\n\nPaper: <LINK>\nDataset: <LINK>']",https://arxiv.org/abs/2204.13796,"Understanding human language often necessitates understanding entities and their place in a taxonomy of knowledge -- their types. Previous methods to learn entity types rely on training classifiers on datasets with coarse, noisy, and incomplete labels. We introduce a method to instill fine-grained type knowledge in language models with text-to-text pre-training on type-centric questions leveraging knowledge base documents and knowledge graphs. We create the WikiWiki dataset: entities and passages from 10M Wikipedia articles linked to the Wikidata knowledge graph with 41K types. Models trained on WikiWiki achieve state-of-the-art performance in zero-shot dialog state tracking benchmarks, accurately infer entity types in Wikipedia articles, and can discover new types deemed useful by human judges. ",Instilling Type Knowledge in Language Models via Multi-Task QA
157,1519670277633941512,958693602208698369,Clara Marie L√ºders,"['üìïNew paper ‚ÄúBeyond Duplicates: Towards Understanding and Predicting Link Types in Issue Tracking Systems‚Äù \nüë• @Lueders_Clara, @ABouraffa, and @maalejw \nPublished @msrconf \n\n #Jira #issuelinks #linktypes #dependencies\n Available at <LINK>', '@ABouraffa @maalejw @msrconf üìëContext\nSoftware projects use ITS like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links that have different types (Duplicate, Depend, Relate). Issue networks can become large, complex structures.', 'üí™Motivation\nDuplicate detection has been a focal point of research. @Jira projects are often using customized link types to link similar issues. We explore how JIRA users use their link types and how duplicate detection models deal with issues that aren‚Äôt duplicates but linked.', 'üîéFindings\nWe present an analysis with graph theory metrics on how stakeholders structure their issues depending on the link type. We evaluate the applicability of duplicate detection models on the data and find that they struggle to differentiate linked from duplicate issues.', 'üîéFindings\nHowever, we also find that duplicate detection models work well as link detection models. üí™', 'ü•∞ Thanks\nmy supervisor @maalejw and my co-author @ABouraffa. Our discussions about this paper were inspiring.\nI am particularly thankful, for how made me focus on the task at hand, as I like to get sidetracked and blow up the scope of the paper when researching üòÑ.', 'ü•∞ Thanks cont.\nI also want to thank @LloydThinks for the dataset.\nhttps://t.co/rRFqVcSINb \nIt offers many more exciting possibilities, so check it out!', 'This work was partly funded by @HorizonEU grant agreement No. 732463. The second author is funded by the German Science Foundation DFG (MA 6149)']",https://arxiv.org/abs/2204.12893,"Software projects use Issue Tracking Systems (ITS) like JIRA to track issues and organize the workflows around them. Issues are often inter-connected via different links such as the default JIRA link types Duplicate, Relate, Block, or Subtask. While previous research has mostly focused on analyzing and predicting duplication links, this work aims at understanding the various other link types, their prevalence, and characteristics towards a more reliable link type prediction. For this, we studied 607,208 links connecting 698,790 issues in 15 public JIRA repositories. Besides the default types, the custom types Depend, Incorporate, Split, and Cause were also common. We manually grouped all 75 link types used in the repositories into five general categories: General Relation, Duplication, Composition, Temporal / Causal, and Workflow. Comparing the structures of the corresponding graphs, we observed several trends. For instance, Duplication links tend to represent simpler issue graphs often with two components and Composition links present the highest amount of hierarchical tree structures (97.7%). Surprisingly, General Relation links have a significantly higher transitivity score than Duplication and Temporal / Causal links. Motivated by the differences between the link types and by their popularity, we evaluated the robustness of two state-of-the-art duplicate detection approaches from the literature on the JIRA dataset. We found that current deep-learning approaches confuse between Duplication and other links in almost all repositories. On average, the classification accuracy dropped by 6% for one approach and 12% for the other. Extending the training sets with other link types seems to partly solve this issue. We discuss our findings and their implications for research and practice. ","Beyond Duplicates: Towards Understanding and Predicting Link Types in
  Issue Tracking Systems"
158,1519253294723514369,1477660122738413571,Manibrata Sen,"['In pandemic times, even neutrinos need a booster shot üòâ . \nNew paper today, <LINK>,  in collaboration with @yuberfpg  and Anirban.', 'We show that in presence of neutrino secret-self interactions, neutrinos from the diffuse supernova neutrino background could scatter off the non-relativistic relic neutrinos left over from the Big Bang, and boost them to MeV energies.', 'We use the latest results from Super-Kamiokande to measure the up-scattered cosmic neutrino flux, and derive constraints on such secret interactions. We also study prospects for detection in future lead-based coherent elastic neutrino-nucleus scattering experiments.', 'A sneak-peek into the boosted cosmic neutrino flux (solid colored lines) https://t.co/V6isanGhFN']",https://arxiv.org/abs/2204.11885,"Neutrinos might interact among themselves through forces that have so far remained hidden. Throughout the history of the Universe, such secret interactions could lead to scatterings between the neutrinos from supernova explosions and the non-relativistic relic neutrinos left over from the Big Bang. Such scatterings can boost the cosmic neutrino background (C$\nu$B) to energies of ${\cal O}$(MeV), making it, in principle, observable in experiments searching for the diffuse supernova neutrino background. Assuming a model-independent four-Fermi interaction, we determine the upscattered cosmic neutrino flux, and derive constraints on such secret interactions from the latest results from Super-Kamiokande. Furthermore, we also study prospects for detection of the boosted flux in future lead-based coherent elastic neutrino-nucleus scattering experiments. ","Neutrino secret self-interactions: a booster shot for the cosmic
  neutrino background"
159,1518904062783152133,2710269284,Graham Kerr,"[""We have a new paper accepted by the Astrophysical Journal, led by @NASAGoddard's Joel Allred, finding that the behaviour of mass flows, and cooling times in our #solarflare model is more consistent with observations when conduction is suppressed  <LINK>""]",https://arxiv.org/abs/2204.11684,"During solar flares plasma is typically heated to very high temperatures, and the resulting redistribution of energy via thermal conduction is a primary mechanism transporting energy throughout the flaring solar atmosphere. The thermal flux is usually modeled using Spitzer's theory, which is based on local Coulomb collisions between the electrons carrying the thermal flux and those in the background. However, often during flares, temperature gradients become sufficiently steep that the collisional mean free path exceeds the temperature gradient scale size, so that thermal conduction becomes inherently non-local. Further, turbulent angular scattering, which is detectable in nonthermal widths of atomic emission lines, can also act to increase the collision frequency and so suppress the heat flux. Recent work by Emslie & Bian (2018) extended Spitzer's theory of thermal conduction to account for both non-locality and turbulent suppression. We have implemented their theoretical expression for the heat flux (which is a convolution of the Spitzer flux with a kernel function) into the RADYN flare-modeling code and performed a parameter study to understand how the resulting changes in thermal conduction affect flare dynamics and hence the radiation produced. We find that models with reduced heat fluxes predict slower bulk flows, less intense line emission, and longer cooling times. By comparing features of atomic emission lines predicted by the models with Doppler velocities and nonthermal line widths deduced from a particular flare observation, we find that models with suppression factors between 0.3 to 0.5 relative to the Spitzer value best reproduce observed Doppler velocities across emission lines forming over a wide range of temperatures. Interestingly, the model that best matches observed nonthermal line widths has a kappa-type velocity distribution function. ",Solar Flare Heating with Turbulent Suppression of Thermal Conduction
160,1518595735935422465,913799254237437952,Shruti Phadke,"['New #ICWSM22 @icwsm paper with @hide_yourself &amp; @tanmit modeling various radicalization phases in online conspiracy theory discussion participants. <LINK>  \nWe aim at providing empirical insights into conspiracy radicalization as well as the recovery process.', 'We find that users with increasing conspiracy theory discussion engagement progress successively through various radicalization phases, whereas users who eventually disengage from conspiracy theory discussions show distinct behavior.', 'Specifically, users who disengage, limit their conspiracy theory discussions to specialized topics, show low language conformity with conspiracy theory discussion communities, and participate in diverse discussion groups.']",https://arxiv.org/abs/2204.10729,"The disruptive offline mobilization of participants in online conspiracy theory (CT) discussions has highlighted the importance of understanding how online users may form radicalized conspiracy beliefs. While prior work researched the factors leading up to joining online CT discussions and provided theories of how conspiracy beliefs form, we have little understanding of how conspiracy radicalization evolves after users join CT discussion communities. In this paper, we provide the empirical modeling of various radicalization phases in online CT discussion participants. To unpack how conspiracy engagement is related to radicalization, we first characterize the users' journey through CT discussions via conspiracy engagement pathways. Specifically, by studying 36K Reddit users through their 169M contributions, we uncover four distinct pathways of conspiracy engagement: steady high, increasing, decreasing, and steady low. We further model three successive stages of radicalization guided by prior theoretical works. Specific sub-populations of users, namely those on steady high and increasing conspiracy engagement pathways, progress successively through various radicalization stages. In contrast, users on the decreasing engagement pathway show distinct behavior: they limit their CT discussions to specialized topics, participate in diverse discussion groups, and show reduced conformity with conspiracy subreddits. By examining users who disengage from online CT discussions, this paper provides promising insights about conspiracy recovery process. ","Pathways through Conspiracy: The Evolution of Conspiracy Radicalization
  through Engagement in Online Conspiracy Discussions"
161,1517557495329398786,2921890701,JB Herv√©,"['Our new paper, ""Automated Isovist Computation for Minecraft"", is now available on Arxiv (<LINK>). @ChristophSalge and I were looking new automatically computed quality metrics for generated game\'s environments. #PCG #Minecraft (1/10)', 'We decided to try using a concept commonly used in architecture: Isovists. An isovist is basically your field of view at a given location. Each isovist are defined by a set of properties, like its area, its perimeter, roundness, etc. (2/10) https://t.co/OHGczUJr8e', 'What interested us in using them for testing procedurally generated video game structures, is that some of these property are being proven to be tied how spaces is being experienced by human visiting them. (3/10)', 'We were hoping that those correlation could also apply in virtual environment. In the paper we describe how we adapted the isovist theory to match this switch of context.  (4/10)', 'Our experiment is running in #Minecraft, using the maps from the #GDMC competition, which contains settlements created by submitted generators and rated by an human jury (according to 4 criterias: Aesthetic, Functionality, Adaptability and Narrative).(5/10) https://t.co/B8thrCy2NV', 'We created a software to compute the isovist at a given set of coordinates, by casting rays in every direction. Our implementation is as generic as we could, in opposition of making it Minecraft specific. (This is running on the GPU, and compute 10^3 isovists in ~2h). (6/10) https://t.co/7uVIUuty33', 'We extracted isovist values from each map, averaged them, and tested how they matched with the human scores. What we observed was that our only domain specific metric, the Diversity of block types visible, is our best performing metric.(7/10)', 'But several layout based metrics, like Perimeter and Openess, are capturing some aspects of the settlements (via the Adaptability score). We also have a range of smaller correlation, which motivates us to further investigate isovist and other related theories.(8/10) https://t.co/9OXCO3SfrK', 'Additionnally, we represented our values as heatmaps on a set of particularly interesting maps (namely the default map without any settlement, the judged best settlement, and the judged best looking settlement).', 'Those showcase how locally based are these metrics, and how they highlight features of the terrain (altitude, vision, biomes, inside of buildings, etc.).  (9/10) https://t.co/xxNW78y2F5', ""Both these results are very encouraging to us, and we are planning to continue investigating space theories, with the intent of developing metrics capable to predict the player's experience environment are creating. (10/10)""]",https://arxiv.org/abs/2204.03752,"Procedural content generation for games is a growing trend in both research and industry, even though there is no consensus of how good content looks, nor how to automatically evaluate it. A number of metrics have been developed in the past, usually focused on the artifact as a whole, and mostly lacking grounding in human experience. In this study we develop a new set of automated metrics, motivated by ideas from architecture, namely isovists and space syntax, which have a track record of capturing human experience of space. These metrics can be computed for a specific game state, from the player's perspective, and take into account their embodiment in the game world. We show how to apply those metrics to the 3d blockworld of Minecraft. We use a dataset of generated settlements from the GDMC Settlement Generation Challenge in Minecraft and establish several rank-based correlations between the isovist properties and the rating human judges gave those settelements. We also produce a range of heat maps that demonstrate the location based applicability of the approach, which allows for development of those metrics as measures for a game experience at a specific time and space. ",Automated Isovist Computation for Minecraft
162,1516767315857190919,1138171396083859457,Dennis Ulmer,"[""I am excited to announce a new version of deep-significance, version 1.2.6! ü•≥ Below I'll talk about changes that were made while writing a demo paper, which can be found here: <LINK>, the package itself is at <LINK> üßµ (1/5) <LINK>"", 'First of all, several changes where made that should make the ASO test even faster and reliable, for instance by reusing data structures and better vectorization ‚ö°Ô∏è (2/5)', ""Secondly, some changes to make the API easier: The confidence level argument is now more intuitive (see below). You're afraid of values set in legacy code? Don't worry, a warning will be shown when the value does not seem sensible. (3/5) https://t.co/0R48dSasx9"", 'Thirdly, when using the Bonferroni correction for multiple comparisons in aso(), the confidence level does not have to be adjusted manually anymore! Simply use the new num_comparisons argument instead (for multi_aso(), use use_bonferroni=True as before) (3/5) https://t.co/XG2wRHSf9c', 'Some arguments, such as num_samples and use_symmetry where deprecated. To avoid compatibility problems, the arguments still exist in the code, but are ignored and a warning is given when a non-default value is passed to the function. (4/5)', 'Feel free to approach me with any issues, improvements or ideas! Read the most recent version of the readme here (https://t.co/x9v1y2dyRj), or click through this small demo here: https://t.co/V8xc22yyaH (5/5)']",https://arxiv.org/abs/2204.06815,"A lot of Machine Learning (ML) and Deep Learning (DL) research is of an empirical nature. Nevertheless, statistical significance testing (SST) is still not widely used. This endangers true progress, as seeming improvements over a baseline might be statistical flukes, leading follow-up research astray while wasting human and computational resources. Here, we provide an easy-to-use package containing different significance tests and utility functions specifically tailored towards research needs and usability. ","deep-significance - Easy and Meaningful Statistical Significance Testing
  in the Age of Neural Networks"
163,1515776377714356224,1049729676,Shathushan Sivashangaran,"['New paper on X-CAR: An Experimental Vehicle Platform for Connected Autonomy Research developed by Virginia Tech ASIM Lab, powered by U.S. Department of Transportation Federal Highway Administration‚Äôs CARMA platform. It‚Äôll appear in IEEE ITS Magazine. \n\n <LINK> <LINK>']",https://arxiv.org/abs/2204.02559,"Autonomous vehicles promise a future with a safer, cleaner, more efficient, and more reliable transportation system. However, the current approach to autonomy has focused on building small, disparate intelligences that are closed off to the rest of the world. Vehicle connectivity has been proposed as a solution, relying on a vision of the future where a mix of connected autonomous and human-driven vehicles populate the road. Developed by the U.S. Department of Transportation Federal Highway Administration as a reusable, extensible platform for controlling connected autonomous vehicles, the CARMA Platform is one of the technologies enabling this connected future. Nevertheless, the adoption of the CARMA Platform has been slow, with a contributing factor being the limited, expensive, and somewhat old vehicle configurations that are officially supported. To alleviate this problem, we propose X-CAR (eXperimental vehicle platform for Connected Autonomy Research). By implementing the CARMA Platform on more affordable, high quality hardware, X-CAR aims to increase the versatility of the CARMA Platform and facilitate its adoption for research and development of connected driving automation. ","X-CAR: An Experimental Vehicle Platform for Connected Autonomy Research
  Powered by CARMA"
164,1514786754812780560,366380609,Evan Rosenman,"['New paper with @LMiratrix just hit arXiv:  <LINK>\n""Designing Experiments Toward Shrinkage Estimation"" considers how to design an RCT when the goal is to merge its causal estimates with those from an observational study to yield greater accuracy. (1/4)', 'We operate in a stratified setting, and propose using an Empirical Bayes shrinker to combine the estimates, such that we have strong guarantees of risk reduction. We proceed using \\kappa_2, a shrinker proposed in https://t.co/Gle8NlpGto. (2/4)', 'We show the exact risk of \\kappa_2 can be computed via a numerical integral discussed in Bao and Kan (2013). The RCT design is optimized over the value of this integral. We propose three heuristics -- Neyman, naive, and robust allocations -- for designing under uncertainty. (3/4)', 'Lastly, we show in a simulation study that the resultant designs outperform, whether or not there is unmeasured confounding in the observational study. We hope these results can help researchers to better leverage obs. data while retaining risk reduction guarantees. (4/4)', 'Addendum: I am deeply disappointed that I did not start this thread with: ""A paper for Pesach!""']",https://arxiv.org/abs/2204.06687,"We consider how increasingly available observational data can be used to improve the design of randomized controlled trials (RCTs). We seek to design a prospective RCT, with the intent of using an Empirical Bayes estimator to shrink the causal estimates from our trial toward causal estimates obtained from an observational study. We ask: how might we design the experiment to better complement the observational study in this setting? We propose using an estimator that shrinks each component of the RCT causal estimator toward its observational counterpart by a factor proportional to its variance. First, we show that the risk of this estimator can be computed efficiently via numerical integration. We then propose algorithms for determining the best allocation of units to strata (the best ""design""). We consider three options: Neyman allocation; a ""naive"" design assuming no unmeasured confounding in the observational study; and a ""defensive"" design accounting for the imperfect parameter estimates we would obtain from the observational study with unmeasured confounding. We also incorporate results from sensitivity analysis to establish guardrails on the designs, so that our experiment could be reasonably analyzed with and without shrinkage. We demonstrate the superiority of these experimental designs with a simulation study involving causal inference on a rare, binary outcome. ",Designing Experiments Toward Shrinkage Estimation
165,1514335061344235521,158052350,Giovanni Iacca,"['Is it possible to use AI to make informed, data-driven interpretable decisions for pandemic control? Check out our new paper at <LINK> With @LLCustode. #ai #learning #covid #pandemic #reinforcementlearning #machinelearning #deeplearning #decisiontrees', 'We show that, by combining decision trees (induced by grammatical evolution) with Q-learning, it is possible to find very simple control policies that are way more effective than deep reinforcement learning based strategies.']",https://arxiv.org/abs/2204.04256,"Since the first wave of the COVID-19 pandemic, governments have applied restrictions in order to slow down its spreading. However, creating such policies is hard, especially because the government needs to trade-off the spreading of the pandemic with the economic losses. For this reason, several works have applied machine learning techniques, often with the help of special-purpose simulators, to generate policies that were more effective than the ones obtained by governments. While the performance of such approaches are promising, they suffer from a fundamental issue: since such approaches are based on black-box machine learning, their real-world applicability is limited, because these policies cannot be analyzed, nor tested, and thus they are not trustable. In this work, we employ a recently developed hybrid approach, which combines reinforcement learning with evolutionary computation, for the generation of interpretable policies for containing the pandemic. These policies, trained on an existing simulator, aim to reduce the spreading of the pandemic while minimizing the economic losses. Our results show that our approach is able to find solutions that are extremely simple, yet very powerful. In fact, our approach has significantly better performance (in simulated scenarios) than both previous work and government policies. ",Interpretable AI for policy-making in pandemics
166,1514251198395785226,1488574877380616197,Han Zhou,"[""I'm thrilled to share our preprint on multi-domain and multi-lingual dialogue state tracking in machine reading comprehension. \n\nNew work with @iiacobacNLP and @PMinervini from @ucl_nlp. Check it out!\n\nPaper: <LINK>\nüßµüëá <LINK>"", 'We introduce XQA-DST, a domain-independent and transferable dialogue state tracker. The model is able to recognise slot values by reformulating the task as an answer to a specially designed domain-slot prompt by span prediction, which extracts answers from the input utterance. https://t.co/Oj5qotpCx2', 'The user may shift the domain of conversation across turns. We introduce a turn-domain filtering strategy that puts a strict constraint and only allows the model to pay attention to the current domain. Hence, it can reduce the potential noises introduced by unnecessary domains.', 'We analyse the individual slot accuracy to study the impact of shared slots over domains on the performance of zero-shot domain adaptation. It is observable that the slots that have been shared among multiple domains lead to a relatively higher domain adaptation performance. https://t.co/vcuYHEjl9U', 'Our XQA-DST model has shown a certain level of common knowledge across domains and languages! It holds a strong potential to overcome the challenging data scarcity problem for either domains or languages in the real application of task-oriented dialogue systems.']",https://arxiv.org/abs/2204.05895,"In a task-oriented dialogue system, Dialogue State Tracking (DST) keeps track of all important information by filling slots with values given through the conversation. Existing methods generally rely on a predefined set of values and struggle to generalise to previously unseen slots in new domains. In this paper, we propose a multi-domain and multi-lingual dialogue state tracker in a neural reading comprehension approach. Our approach fills the slot values using span prediction, where the values are extracted from the dialogue itself. With a novel training strategy and an independent domain classifier, empirical results demonstrate that our model is a domain-scalable and open-vocabulary model that achieves 53.2% Joint Goal Accuracy (JGA) on MultiWOZ 2.1. We show its competitive transferability by zero-shot domain-adaptation experiments on MultiWOZ 2.1 with an average JGA of 31.6% for five domains. In addition, it achieves cross-lingual transfer with state-of-the-art zero-shot results, 64.9% JGA from English to German and 68.6% JGA from English to Italian on WOZ 2.0. ",XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking
167,1514186633519710211,946678150079139840,Giuseppe Attanasio,"['New paper out üö®\nCLIP understands now #fashion (stylists, beware üòé) and recognizes very improbable products üëÄ\n\nPreprint: <LINK>\n\n#NLProc #ecommerce <LINK>']",https://arxiv.org/abs/2204.03972,"The steady rise of online shopping goes hand in hand with the development of increasingly complex ML and NLP models. While most use cases are cast as specialized supervised learning problems, we argue that practitioners would greatly benefit from more transferable representations of products. In this work, we build on recent developments in contrastive learning to train FashionCLIP, a CLIP-like model for the fashion industry. We showcase its capabilities for retrieval, classification and grounding, and release our model and code to the community. ",FashionCLIP: Connecting Language and Images for Product Representations
168,1513929428694290433,765341324279156736,Sinong Wang,"['Prompt tuning can be instance-dependent. Thrilled to share our new work! \n\n""IDPG: An Instance-Dependent Prompt Generation Method"".\n\nCheck out our paper here: <LINK> <LINK>']",https://arxiv.org/abs/2204.04497,"Prompt tuning is a new, efficient NLP transfer learning paradigm that adds a task-specific prompt in each input instance during the model training stage. It freezes the pre-trained language model and only optimizes a few task-specific prompts. In this paper, we propose a conditional prompt generation method to generate prompts for each input instance, referred to as the Instance-Dependent Prompt Generation (IDPG). Unlike traditional prompt tuning methods that use a fixed prompt, IDPG introduces a lightweight and trainable component to generate prompts based on each input sentence. Extensive experiments on ten natural language understanding (NLU) tasks show that the proposed strategy consistently outperforms various prompt tuning baselines and is on par with other efficient transfer learning methods such as Compacter while tuning far fewer model parameters. ",IDPG: An Instance-Dependent Prompt Generation Method
169,1512381906276470791,4856966603,ThunderKAT,"['#meerKAT discovers serendipitously a new #radio flaring nearby M dwarf! Check out the newly accepted paper on #arXiv <LINK> (image from Andersson et al. 2022). <LINK>', 'https://t.co/Baxt59WtgX']",https://arxiv.org/abs/2204.03481,"We report on the detection of MKT J174641.0$-$321404, a new radio transient found in untargeted searches of wide-field MeerKAT radio images centred on the black hole X-ray binary H1743$-$322. MKT J174641.0$-$321404 is highly variable at 1.3 GHz and was detected three times during 11 observations of the field in late 2018, reaching a maximum flux density of 590 $\pm$ 60 $\mu$Jy. We associate this radio transient with a high proper motion, M dwarf star SCR~1746$-$3214 12 pc away from the Sun. Multiwavelength observations of this M dwarf indicate flaring activity across the electromagnetic spectrum, consistent with emission expected from dMe stars, and providing upper limits on quiescent brightness in both the radio and X-ray regimes. \textit{TESS} photometry reveals a rotational period for SCR~1746$-$3214 of $0.2292 \pm 0.0025$ days, which at its estimated radius makes the star a rapid rotator, comparable to other low mass systems. Dedicated spectroscopic follow up confirms the star as a mid-late spectral M dwarf with clear magnetic activity indicated by strong H$\alpha$ emission. This transient's serendipitous discovery by MeerKAT, along with multiwavelength characterisation, make it a prime demonstration of both the capabilities of the current generation of radio interferometers and the value of simultaneous observations by optical facilities such as MeerLICHT. Our results build upon the literature of of M dwarfs' flaring behaviour, particularly relevant to the habitability of their planetary systems. ","Serendipitous discovery of radio flaring behaviour from a nearby M dwarf
  with MeerKAT"
170,1512334358497083396,38941661,Daniel Stilck Fran√ßa,"['New paper out with Cambyse Rouze, Giacomo de Palma and Milad Marvian! <LINK>\nThe paper is about concentration inequalities for the outputs of shallow and noisy quantum circuits. We obtain our results using quantum optimal transport techniques. 1/n', 'But the title has variational quantum algorithms in it. So what is the connection between these topics? A simple way to see the connection is to consider GHZ states on n qubits when measured in the computational basis. 2/n', 'Now consider the observable \\sum_iZ_i. Its expectation value for the GHZ is 0, but you will either observe the outcomes -n or n. That is, the outputs DO NOT concentrate around the mean. And any family of circuits that produces outputs that concentrate cannot produce GHZs. 3/n', 'So concentration inequalities can be used to prove barriers to prepare states. In the paper we prove that constant depth circuits produce outputs that concentrate around the mean, improving and simplifying previous results in the literature on limitations of shallow circuits. 4/n', 'In the noisy setting, i.e. circuits with a constant density of errors, we show Gaussian concentration inequalities. In short, these show that if the local noise rate is p, at depth ~1/p, probability of having an output that deviates significantly 5/n', 'from one would obtain when measuring a trivial product state is exponentially small in system size. This strengthens the results I previously obtained with @RaulGarciaPatr1. There we arrived at similar conclusions, but only in expectation! 6/n', 'Hopefully our relatively simple proofs can convince more people to take a closer look at quantum optimal techniques! I certainly believe they provide an elegant framework to study shallow and noisy quantum circuits! 7/n', 'And I would also like to thank my collaborators for this fun project! It was particularly nice to finally write a paper with Giacomo, with whom I overlapped for a while at @QMATH_KU! 8/8']",https://arxiv.org/abs/2204.03455,"The impressive progress in quantum hardware of the last years has raised the interest of the quantum computing community in harvesting the computational power of such devices. However, in the absence of error correction, these devices can only reliably implement very shallow circuits or comparatively deeper circuits at the expense of a nontrivial density of errors. In this work, we obtain extremely tight limitation bounds for standard NISQ proposals in both the noisy and noiseless regimes, with or without error-mitigation tools. The bounds limit the performance of both circuit model algorithms, such as QAOA, and also continuous-time algorithms, such as quantum annealing. In the noisy regime with local depolarizing noise $p$, we prove that at depths $L=\cO(p^{-1})$ it is exponentially unlikely that the outcome of a noisy quantum circuit outperforms efficient classical algorithms for combinatorial optimization problems like Max-Cut. Although previous results already showed that classical algorithms outperform noisy quantum circuits at constant depth, these results only held for the expectation value of the output. Our results are based on newly developed quantum entropic and concentration inequalities, which constitute a homogeneous toolkit of theoretical methods from the quantum theory of optimal mass transport whose potential usefulness goes beyond the study of variational quantum algorithms. ","Limitations of variational quantum algorithms: a quantum optimal
  transport approach"
171,1524731761280057346,1092465966,Francesco De Toni,"['Can we use pre-trained Large Language Models to study historical texts with no fine tuning?\nIn our paper, our working group on historical texts at  @BigscienceW investigate zero-shot information extraction on multilingual historical texts with LLM.\n<LINK>üå∏\n\nA üßµ', 'Training or fine-tuning LLMs is costly, and these models have been shown to work quite well in zero-shot for many tasks and datasets: we investigate how true this is for multilingual historical texts, using the T0 model and a prompt-based approach on the CLEF-HIPE 2020 dataset. https://t.co/Yk64b96jny', 'We first study zero-shot NER, for texts ranging from 1780 to the present day, in English, French and German, and 1) design prompts for the task 2) filter model answers 3) disambiguate. https://t.co/FKls9HZy6s', 'Our results show that zero-shot NER on multilingual historical texts is still hard to achieve, but there are promising directions for improvement. Although the predicted entities are incorrect, they are often semantically close to the correct entity.', 'We identify three main issues with prompt-based zero-shot NER. 1) preference for non-empty answers, 2) OCR noise, 3) unwanted translation into English. https://t.co/HQPKlxuvOS', '1) Prompt-based LLMs tend to output non-empty answers, regardless of the presence of entities. This generates many false positives in zero-shot NER, which need to be filtered out.', '2) Historical texts are usually OCR‚Äôed so they can be processed with NLP. However, OCR is often noisy.  For example, the scanned line below was OCR\'ed as\n""ed by the Legiflature . We fltall tramfit a com ¬¨"". üëá https://t.co/ff5vyyTwyC', 'LLMs tend to automatically correct OCR noise. As we apply a filter to the LLM answers to avoid false positives (see point 1 above), the filter excludes answers with ‚Äúcorrected‚Äù OCR.', '3) English is the predominant language in many LLMs, including T0. This produces unwanted automatic translation of the correct entities from languages other than English (German and French) into English.', 'We also observe variation in performance across languages and historical periods. Interestingly, English does not have a clear edge in performance over the other languages. https://t.co/ymiPyZs5x3', 'However, precision for LOC and PERS increases in more recent English texts, while it remains flat or declines for other languages and entities.', 'Finally, we run two experiments to test T0‚Äôs zero-shot ability in differentiating texts based on language and historical period, using prompts. Surprisingly, we find that performance on language detection is lower on English texts. https://t.co/41BkzdYJ0I', 'On the other hand, T0 does quite well in identifying the publication dates of texts, especially if we consider that measurable language change can happen even in the span of 10 years. https://t.co/ybLvvFNAbH', 'Our paper suggests that there are promising ways forward to do zero-shot NER with LLM. Our next steps will be looking at different prompting strategies and enhanced filtering methods to balance false positives with OCR noise issues.', 'This paper is the joint effort of several people working at different universities, institutes and libraries: @versae @christopher, @clefourrier, @stefan_it_, @vanstriendaniel, @emanjava. We are thankful to @BigscienceW for supporting us!üå∏', 'We will present this paper at the Workshop on Challenges &amp; Perspectives in Creating Large Language Models at #ACL2022']",http://arxiv.org/abs/2204.05211,"In this work, we explore whether the recently demonstrated zero-shot abilities of the T0 model extend to Named Entity Recognition for out-of-distribution languages and time periods. Using a historical newspaper corpus in 3 languages as test-bed, we use prompts to extract possible named entities. Our results show that a naive approach for prompt-based zero-shot multilingual Named Entity Recognition is error-prone, but highlights the potential of such an approach for historical languages lacking labeled datasets. Moreover, we also find that T0-like models can be probed to predict the publication date and language of a document, which could be very relevant for the study of historical texts. ","Entities, Dates, and Languages: Zero-Shot on Historical Texts with T0"
172,1524078294458462209,1278822973575577601,Sander Tonkens,"['üö®New exciting work: <LINK> with @SylviaLHerbert \n\nWe propose a simple yet powerful method to refine approximately valid CBFs (e.g. neural CBFs) using dynamic programming based reachability analysis, to guarantee safety. \n(1/n) üßµüëá', 'Our method iteratively updates a CBF (pink to green). \n\nWe prove that our method becomes provably less unsafe with every iteration, and converges to a valid CBF. In practice, we often recover the largest safe set (shaded green) (2/n) https://t.co/f0nYvBFSfS', 'We show that we can update conservative CBFs, rendering the safety filter less invasive, while remaining safe! \n\nWe are particularly excited about leveraging this method on hardware (currently ongoing!) to adapt to sudden changes in the environment (3/n) https://t.co/cMmRAOq3ya', 'In addition, we can refine invalid CBFs to guarantee safety. We provide an open-source implementation https://t.co/nwaVQetGzO. Give it a try!\n\nWe also provide an intuitive CBF library in python: https://t.co/omJ5MuKtI5 \n(4/n) https://t.co/7xzaf8oL5n']",http://arxiv.org/abs/2204.12507,"Safety filters based on Control Barrier Functions (CBFs) have emerged as a practical tool for the safety-critical control of autonomous systems. These approaches encode safety through a value function and enforce safety by imposing a constraint on the time derivative of this value function. However, synthesizing a valid CBF that is not overly conservative in the presence of input constraints is a notorious challenge. In this work, we propose refining candidate CBFs using formal verification methods to obtain a valid CBF. In particular, we update an expert-synthesized or backup CBF using dynamic programming (DP) based reachability analysis. Our framework guarantees that with every DP iteration the obtained CBF is provably at least as safe as the prior iteration and converges to a valid CBF. Therefore, our proposed method can be used in-the-loop for robotic systems. We demonstrate the practicality of our method to enhance safety and/or reduce conservativeness on a range of nonlinear control-affine systems using various CBF synthesis techniques in simulation. ",Refining Control Barrier Functions through Hamilton-Jacobi Reachability
173,1521259672782704640,1487935233215315974,Guy Emerson,"['What are truth conditions good for? Learning grounded semantics! New ACL paper with @YinhongLiu2: <LINK>\n\nTraining on the @VisualGenome dataset, and evaluating on four semantic datasets, we find that a truth-conditional model gives better performance. #acl2022nlp', ""On a more personal level, this is the first paper where I've supervised work that directly builds on my PhD thesis (as opposed to supervising work on a different topic).  Feels like quite a milestone!"", 'Also, shoutout to @ah__cl, @davidschlangen, and @anitaveroe, for exciting work in this space which is complementary to ours.', ""If you have questions, feel free to ask... but please be patient for my response, as I'm currently on parental leave!""]",https://arxiv.org/abs/2204.10624,"Functional Distributional Semantics is a recently proposed framework for learning distributional semantics that provides linguistic interpretability. It models the meaning of a word as a binary classifier rather than a numerical vector. In this work, we propose a method to train a Functional Distributional Semantics model with grounded visual data. We train it on the Visual Genome dataset, which is closer to the kind of data encountered in human language acquisition than a large text corpus. On four external evaluation datasets, our model outperforms previous work on learning semantics from Visual Genome. ",Learning Functional Distributional Semantics with Visual Data
174,1521228025500737536,960527787428900864,Michael Albergo,"['Very excited to share a new paper with Peter Lunts and Michael Lindsey!\nWe worked to push HMC into state-of-the-art large lattices in condensed matter to study the quantum critical phenomena of the spin-fermion model: <LINK> <LINK>', 'Understanding quantum criticality in metals is an important problem very relevant to high-temperature superconductivity. https://t.co/mB9fWgJEtG', 'This is one of the hardest problems in theoretical condensed matter physics due to (a) a Fermi surface, (b) strong coupling of the fermions and collective bosonic modes, (c) diverging correlation length, and (d) lack of any small control parameter.', 'We developed a suite of HMC tuning tricks that enables us to study the universal scaling regime by simulating unprecedentedly large system sizes at a low enough temperature. All the way to V =  80 x 80 x 200! https://t.co/JwMPZvwoPy', 'We found a form of the critical susceptibility that (a) violates the so-far numerically observed Hertz-Millis form and (b) shows strong evidence of corroborating the fixed point found in an exactly solvable limit. https://t.co/RhpILbw4uf https://t.co/6WmJpOAOyZ', 'We are pretty psyched about the method. It can be applied to other models of quantum criticality, as well as electron-phonon models.', 'Big shout outs to Peter and Michael for teaching me a bunch of physics along the way! And to @KyleCranmer for guidance along the way, as well as Phiala Shanahan for HMC insights :)']",https://arxiv.org/abs/2204.14241,"We numerically study the $\mathrm{O}(3)$ spin-fermion model, a minimal model of the onset of antiferromagnetic spin-density wave (SDW) order in a two-dimensional metal. We employ a Hybrid Monte Carlo (HMC) algorithm with a novel auto-tuning procedure, which learns the optimal HMC hyperparameters in an initial warmup phase. This allows us to study unprecedentedly large systems, even at criticality. At the quantum critical point, we find a critical scaling of the dynamical spin susceptibility $\chi(\omega, \vec q)$ that strongly violates the Hertz-Millis form, which is the first demonstrated instance of such a phenomenon in this model. The form that we do observe provides strong evidence that the universal scaling is actually governed by the fixed point near perfect hot-spot nesting of Schlief, Lunts, and Lee [Phys. Rev. X 7, 021010 (2017)], even away from perfect nesting. Our work provides a concrete link between controlled calculations of SDW metallic criticality in the long-wavelength and small nesting angle limits and a microscopic finite-size model at realistic appreciable values of the nesting angle. Additionally, the HMC method we introduce is generic and can be used to study other fermionic models of quantum criticality, where there is a strong need to simulate large systems. ","Non-Hertz-Millis scaling of the antiferromagnetic quantum critical metal
  via scalable Hybrid Monte Carlo"
175,1521137661104345088,328430286,Jad C. Halimeh,"['New paper <LINK> (2nd of 2 today). In a work led by Ana Hudomal of @theoryleeds, we study analytically and numerically the driven PXP model, and map out two distinct classes of optimal driving. \n@JDesaules\n@QManyBody\n@MCQST_cluster\n@ERC_Research\n@LeverhulmeTrust <LINK>']",https://arxiv.org/abs/2204.13718,"Periodic driving has been established as a powerful technique for engineering novel phases of matter and intrinsically out-of-equilibrium phenomena such as time crystals. Recent work by Bluvstein et al. [Science 371, 1355 (2021)] has demonstrated that periodic driving can also lead to a significant enhancement of quantum many-body scarring, whereby certain non-integrable systems can display persistent quantum revivals from special initial states. Nevertheless, the mechanisms behind driving-induced scar enhancement remain poorly understood. Here we report a detailed study of the effect of periodic driving on the PXP model describing Rydberg atoms in the presence of a strong Rydberg blockade - the canonical static model of quantum many-body scarring. We show that periodic modulation of the chemical potential gives rise to a rich phase diagram, with at least two distinct types of scarring regimes that we distinguish by examining their Floquet spectra. We formulate a toy model, based on a sequence of square pulses, that accurately captures the details of the scarred dynamics and allows for analytical treatment in the large-amplitude and high-frequency driving regimes. Finally, we point out that driving with a spatially inhomogeneous chemical potential allows to stabilize quantum revivals from arbitrary initial states in the PXP model, via a mechanism similar to prethermalization. ",Driving quantum many-body scars
176,1521043648556720130,1430890835902562313,Silvia Galli,"['New paper led by brilliant PhD student Etienne Camphuis! We studied the accuracy of analytical CMB covariance matrices in the case of observations of a small patch of the sky, like the one from the @SPTelescope.  #neucosmos @ERC_Research  <LINK> üßµ 1/', '1) For the first time (afaik) we found  tricks to calculate the matrices exactly (computationally expensive) 2) We used this exact calculation to check the accuracy of existing fast approximations. They are accurate at the ~5% level. 2/', '3) We then found a new approximation which is 4 times more accurate! 4) Finally, we worked out the details to calculate the covariance matrix for the specific case of the Polspice power spectrum estimator. 3/', 'All the original ideas of the paper come from Etienne (tricks for exact calculation, new approximation, Polspice adaptation) and great work was done by all our collaborators Karim Benabed, Eric Hivon and Marc Lilley! 4/', 'This work allows us to have a reliable reference for the covariance matrices we will build for the next release of the SPT-3G data. 5/']",https://arxiv.org/abs/2204.13721,"Accurate covariance matrices are required for a reliable estimation of cosmological parameters from pseudo-power spectrum estimators. In this work, we focus on the analytical calculation of covariance matrices. We consider the case of observations of the Cosmic Microwave Background in temperature and polarization on a small footprint such as in the SPT-3G experiment, which observes 4% of the sky. Power spectra evaluated on small footprints are expected to have large correlations between modes, and these need to be accurately modelled. We present, for the first time, an algorithm that allows an efficient (but computationally expensive) exact calculation of analytic covariance matrices. Using it as our reference, we test the accuracy of existing fast approximations of the covariance matrix. We find that, when the power spectrum is binned in wide bandpowers, current approaches are correct up to the 5% level on the SPT-3G small sky footprint. Furthermore, we propose a new approximation which improves over the previous ones reaching a precision of 1% in the wide bandpowers case and generally more than 4 times more accurate than current approaches. Finally, we derive the covariance matrices for mask-corrected power spectra estimated by the PolSpice code. In particular, we include, in the case of a small sky fraction, the effect of the apodization of the large scale modes. While we considered the specific case of the CMB, our results are applicable to any other cosmological probe which requires the calculation of pseudo-power spectrum covariance matrices. ",Accurate CMB covariance matrices: exact calculation and approximations
177,1520105865365270529,789998140782882817,Amy Zhang,"['Building generalizable goal-conditioned agents from rich observations is a key to solving real world problems. We propose a new form of state abstraction, goal-conditioned bisimulation, that captures functional equivariance. 1/n\nPaper: <LINK> <LINK>', 'This allows for skill reuse to achieve new goals. Prior work has proposed representation learning objectives that induce invariance to irrelevant factors through the use of domain knowledge about the relevant state features for a task. 2/n', 'In goal-conditioned problems, which features are relevant depends on the goal. While there may be uncontrollable aspects of an environment that can always be ignored, knowledge of the task and current state is necessary to determine what is relevant to complete the task. 3/n', 'Rather than construct representations that ignore features that are never relevant, our goal is to construct functionally equivariant representations that only capture changes between state-goal pairs. 4/n https://t.co/quTNGLBZpw', 'If we can acquire representations that have these properties, then we can similarly specify goals for goal-conditioned policies in one setting, and have them carry out those goals in many diverse settings. 5/n', 'As shown in the gif above, this frees us from needing an exact goal image for achieving the same functional task in a new setting. Joint work with Philippe Hansen-Estruch, @ashvinair, Patrick Yin, and @svlevine.']",https://arxiv.org/abs/2204.13060,"Building generalizable goal-conditioned agents from rich observations is a key to reinforcement learning (RL) solving real world problems. Traditionally in goal-conditioned RL, an agent is provided with the exact goal they intend to reach. However, it is often not realistic to know the configuration of the goal before performing a task. A more scalable framework would allow us to provide the agent with an example of an analogous task, and have the agent then infer what the goal should be for its current state. We propose a new form of state abstraction called goal-conditioned bisimulation that captures functional equivariance, allowing for the reuse of skills to achieve new goals. We learn this representation using a metric form of this abstraction, and show its ability to generalize to new goals in simulation manipulation tasks. Further, we prove that this learned representation is sufficient not only for goal conditioned tasks, but is amenable to any downstream task described by a state-only reward function. Videos can be found at this https URL ",Bisimulation Makes Analogies in Goal-Conditioned Reinforcement Learning
178,1519949000140632065,1344699023073140737,Ludovico Lami,"['The transmissivity of an optical fibre drops exponentially with length, and when it gets below 1/2 the Q capacity is zero. Here we find a practical scheme to restore a constant value of the Q capacity for all nonzero values of the transmissivity\n<LINK>', ""Thanks for the great work to @FrancescoMeleAn, who's the lead author on this, and by Vittorio Giovannetti. See also the longer version\nhttps://t.co/q1cPfM2LSs""]",https://arxiv.org/abs/2204.13128,"In the absence of quantum repeaters, quantum communication proved to be nearly impossible across optical fibres longer than $\gtrsim 20\text{ km}$ due to the drop of transmissivity below the critical threshold of $1/2$. However, if the signals fed into the fibre are separated by a sufficiently short time interval, memory effects must be taken into account. In this paper we show that by properly accounting for these effects it is possible to devise schemes that enable unassisted quantum communication across arbitrarily long optical fibres at a fixed positive qubit transmission rate. We also demonstrate how to achieve entanglement-assisted communication over arbitrarily long distances at a rate of the same order of the maximum achievable in the unassisted noiseless case. ",Restoring quantum communication efficiency over high loss optical fibres
179,1519728054762942466,756230494422007808,Terra Blevins,"['Are any large-scale pretrained models truly monolingual? In new work with @LukeZettlemoyer, we find that automatic data collection methods leak millions of non-English tokens into popular pretraining corpora. (1/3)\n\n‚ú®Paper‚ú®: <LINK> <LINK>', 'Specifically, we see that although the overall percentages of non-English text in these corpora are small, this corresponds to millions of out-of-language tokens (2/3) https://t.co/usumROHmmf', 'Prior work has found that monolingual models transfer surprisingly well across languages -- we show that cross-lingual performance is strongly correlated with the amount of data leaked in the pretraining corpus. (3/3)', '@yanaiela I was at Meta/Facebook while working on this project!']",https://arxiv.org/abs/2204.08110,"English pretrained language models, which make up the backbone of many modern NLP systems, require huge amounts of unlabeled training data. These models are generally presented as being trained only on English text but have been found to transfer surprisingly well to other languages. We investigate this phenomenon and find that common English pretraining corpora actually contain significant amounts of non-English text: even when less than 1% of data is not English (well within the error rate of strong language classifiers), this leads to hundreds of millions of foreign language tokens in large-scale datasets. We then demonstrate that even these small percentages of non-English data facilitate cross-lingual transfer for models trained on them, with target language performance strongly correlated to the amount of in-language data seen during pretraining. In light of these findings, we argue that no model is truly monolingual when pretrained at scale, which should be considered when evaluating cross-lingual transfer. ","Language Contamination Explains the Cross-lingual Capabilities of
  English Pretrained Models"
180,1519501156258844672,754692834268180481,Arif Ullah,"['Just deposited the preprint in @arxiv where we propose a one-short trajectory learning approach, propagating 2.5ps long time dynamics in less than 50 milliseconds <LINK> \n#compchem @PavloDral <LINK>']",http://arxiv.org/abs/2204.12661,"Nonadiabatic quantum dynamics are important for understanding light-harvesting processes, but their propagation with traditional methods can be rather expensive. Here we present a one-short trajectory learning approach that allows to directly make ultra-fast prediction of the entire trajectory for a new set of such simulation parameters as temperature and reorganization energy. The whole 2.5 ps long propagation takes 50 milliseconds as we demonstrate on the comparatively large quantum system, the Fenna-Matthews-Olsen (FMO) complex. Our approach also significantly reduces time and memory requirements for training. ",One-shot trajectory learning of open quantum systems dynamics
181,1519355670612439040,726053744731807744,Yuchen Lin,"['We can recall acquired skills and generalize them to perform new tasks, even w/o any supervision. Can multi-task LMs (e.g., T0) also benefit from retrieval for cross-task generalization? We show it‚Äôs promising and propose such a method, ReCross.üßµ [1/7] <LINK>', 'We study the unsupervised cross-task generalization problem (aka. zero-shot task generalization): given a massively multi-task NLP model such as T0 @huggingface, how can we generalize it to perform an unseen task, only using a few unlabeled data (i.e., query examples)? [2/7] https://t.co/gHhBsb9LQ6', 'We propose ReCross, a retrieval-augmentation framework for cross-task generalization. It has two key modules: a dense retriever that provides initial retrieved data via MIPS. Then, a reranker module selects a subset of them to update the upstream model via fine-tuning. [3/7] https://t.co/WSLuKHMGlU', 'To develop these two modules of ReCross, we reuse the encoders of the given multi-task LM for encoding both upstream data and query examples in performing MIPS. We use this dense retriever to create distant supervision for training a cross-encoder as the desired reranker.  [4/7] https://t.co/L5LWi5iFGo', 'To evaluate at scale, we train a T0-like model, named BART0, which is comparable to T0-3B yet 8x smaller (t5-3b vs bart-large). Our main experimental results with BART0 show that ReCross outperforms both non-ret. methods and other ret. baselines by a large margin. [5/7] https://t.co/0yZps8WFqz', 'We draw a heatmap to show the dist. of retrieved data by each ret-aug method. It‚Äôs clear that ReCross and SBERT have very different patterns. We believe it is an important future direction to analyze &amp; predict correlation between upstream data and target tasks at scale. [6/7] https://t.co/T8Tn56JKye', 'We hope our ReCross paper will spur further research on retrieval-aug methods for cross-task generalization. Many thanks to @xiangrenNLP, Kangmin, @chrmill &amp; Beiwen @nlp_usc @CSatUSC! #NLProc [7/7]\n\nPaper link: https://t.co/7urLFQjhKC \nProject website: https://t.co/iAiQ5P9Uj9']",https://arxiv.org/abs/2204.07937,"Humans can perform unseen tasks by recalling relevant skills that are acquired previously and then generalizing them to the target tasks, even if there is no supervision at all. In this paper, we aim to improve such cross-task generalization ability of massive multi-task language models such as T0 (Sanh et al., 2021) in an unsupervised setting. We propose a retrieval-augmentation method named ReCross that takes a few unlabelled examples as queries to retrieve a small subset of upstream data and uses them to update the multi-task model for better generalization. Our empirical results show that the proposed ReCross consistently outperforms non-retrieval baselines by a significant margin. ",Unsupervised Cross-Task Generalization via Retrieval Augmentation
182,1519352654899691520,1158317142447706112,Stephanie Brandl,"['You can find our new paper ‚ÄúHow Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns‚Äù which we will present at @naaclmeeting now on arxiv: <LINK>. \nTL,DR: In this paper, we show that gender-neutral pronouns in Danish, English', 'and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance in NLI and coreference resolution.\nWe see a clear increase in perplexity for gender-neutral pronouns in comparison to gendered pronouns such', 'as she/he and a correlation between attention flow and perplexity, particularly in later layers. This suggests that there is some development across layers\nthat is stronger for gender-neutral pronouns than for gendered pronouns https://t.co/vJ77wJ8pFA', 'Furthermore, we observe a drastic drop in performance in pronoun resolution in English https://t.co/GG9xAofanl', 'as well as a smaller decrease for Danish where we applied the full coreference resolution https://t.co/ZWCKMNc4BI', 'We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved.\nThis is joint work with @ruixiangcui and Anders S√∏gaard @coastalcph']",https://arxiv.org/abs/2204.10281,"Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved. ","How Conservative are Language Models? Adapting to the Introduction of
  Gender-Neutral Pronouns"
183,1519239388458369024,894875488094760960,Andrea Dittadi,"['Excited to share our new study on object-centric learning! <LINK>\n\nIn this work led by @oneapra (w @OleWinther1) we look for architectural inductive biases that may help scale unsupervised object-centric representation learning to more complex images.\n\n1/ <LINK>', 'We add complex textures to standard multi-object datasets, and train state-of-the-art models.\n\nThe added textures highlight opposite failure modes:\n\n(1) In some cases, segmentation is mostly color-based and practically ignores objects (see input image &amp; reconstructed slots).\n\n2/ https://t.co/kKUyp47lDz', '(2) In other cases, the objects are segmented relatively well but the texture details are largely disregarded.\n\n3/ https://t.co/kMUa8xi3TI', 'We modify the ‚Äúfailure mode 1‚Äù models to sacrifice reconstruction accuracy in favor of smoother segmentation, but do not obtain substantial improvements in object separation or representation usefulness. These models tend to have a bias towards color-based segmentation.\n\n4/ https://t.co/ilZieeKKU2', 'Similarly, we modify ‚Äúfailure mode 2‚Äù models to improve reconstructions. Interestingly, we observe that hyper-segmentation still does not occur ‚Äì these models can still separate objects correctly, even as their reconstruction quality improves.\n\n5/ https://t.co/EupAB1220t', 'Main conclusion: methods that use a single module to reconstruct both shape (alpha-blending masks) and appearance of each object tend to learn more useful representations and separate objects more easily.\n\nCheck out the paper for further interesting experiments and results!\n\n6/6']",http://arxiv.org/abs/2204.08479,"Understanding which inductive biases could be helpful for the unsupervised learning of object-centric representations of natural scenes is challenging. We use neural style transfer to generate datasets where objects have complex textures while still retaining ground-truth annotations. We find that methods that use a single module to reconstruct both the shape and visual appearance of each object learn more useful representations and achieve better object separation. In addition, we observe that adjusting the latent space size is not sufficient to improve segmentation performance. Finally, the downstream usefulness of the representations is significantly more strongly correlated with segmentation quality than with reconstruction accuracy. ","Inductive Biases for Object-Centric Representations in the Presence of
  Complex Textures"
184,1519221900655964161,420176893,Gavin,"['We‚Äôve all heard of the bias-variance decomposition. Did you know it also holds for some **margin** losses (some, but not all‚Ä¶) see our AISTATS 2022 paper (<LINK>) to find out more! @EchoStatements @csmcr @idsai_uom']",http://arxiv.org/abs/2204.12155,"We introduce a novel bias-variance decomposition for a range of strictly convex margin losses, including the logistic loss (minimized by the classic LogitBoost algorithm), as well as the squared margin loss and canonical boosting loss. Furthermore, we show that, for all strictly convex margin losses, the expected risk decomposes into the risk of a ""central"" model and a term quantifying variation in the functional margin with respect to variations in the training data. These decompositions provide a diagnostic tool for practitioners to understand model overfitting/underfitting, and have implications for additive ensemble models -- for example, when our bias-variance decomposition holds, there is a corresponding ""ambiguity"" decomposition, which can be used to quantify model diversity. ",Bias-Variance Decompositions for Margin Losses
185,1519212970953297920,1501847085037019141,H√©ctor Gil Mar√≠n,"['New paper on cosmology constraints using @sdssurveys data.\n\nWe apply ShapeFit on @eBOSSurvey data and perform a model-agnostic parameter inference on 0.2&lt;z&lt;2.2, and interpreted in the light of LCDM++ models.\n\n<LINK>\n\nSee the üßµ to find its main results üëá', 'The whole SDSS 0.2&lt;z&lt;2.2 range is compressed in 4 model-agnostic parameters per redshift bin: angular and radial BAO distances, the growth of structure times sigma8; and the novel ShapeFit parameter m.\n\nThis represents a compression of 1,723,267 galaxies in just 16 parameters! https://t.co/5gN9OOWyqp', 'We interpret this parametrization in the light of LCDM finding, \nOm = 0.2971 ¬± 0.0061;\ns8 = 0.857 ¬± 0.040;\n\nIf a BBN prior is added we find,\nŒ©m = 0.3001 ¬± 0.0057;\nH0 = 68.16 ¬± 0.67;\ns8 = 0.858 ¬± 0.036;\nThe results are a substantial improvement with respect to BAO and RSD analyses https://t.co/gWPKtQOmrf', 'Neutrinos!\nThanks to the shape information we are able to constrain the sum of the mass of neutrinos with LSS+BBN only information finding Œ£mŒΩ &lt; 0.40 eV (95%). When combined with Planck, we find Œ£mŒΩ &lt; 0.082 eV (95%) https://t.co/Le581MmhZ8', 'Curvature!\nWhen we relax the flatness condition we find the the results very consistent with a flat Universe, \nŒ©k = ‚àí0.022 (+0.032) (‚àí0.038) without using any other dataset; and, \nŒ©k = +0.0015 ¬± 0.0016 when data from Planck is added. https://t.co/NE2gXVlNXn', 'Dark Energy!\nWe constraint the dark energy equation of state parameter,\n\nw = ‚àí0.998 (+0.085) (‚àí0.073) without using any other dataset; and,\nw = ‚àí1.093(+0.048) (‚àí0.044) when data from Planck is added. https://t.co/lxoJgbMqSc', 'Tensions,We find that our results on sigma8 arein well agreement from those of Planck, \ns8_ShapeFit = 0.858 ¬± 0.036;\ns8_Planck = 0.8101 ¬± 0.0062;\n\nWe also find good internal s8 consistency,\ns8 = 0.820 ¬± 0.043 (LRGs 0.2&lt;z&lt;1.0)\ns8 = 0.993 ¬± 0.072 (QSOs, 0.8&lt;z&lt;2.2)', 'It has been a pleasure to collaborate with my PhD student @Cosmosamu (who will be defending his thesis soon) and @licia_verde. We also thank the effort of @sdssurveys team on producing such good quality data, which has made this work possible.', ""@AddisonGraeme @sdssurveys @eBOSSurvey Hi Graeme, we haven't quantify it, we simply were aware that in some of those papers that effect was not taken correctly into account. We believe it doesn't belong to us to quantify by how much sigma8 increases in their when this is properly accounted for."", ""@AddisonGraeme @sdssurveys @eBOSSurvey Having said that, we've recently seen some papers where this effect should be correctly taken into account, and that still have a low œÉ8. \nSee for e.g., this Monday arxiv,\nhttps://t.co/by1SLonWBA\nwhere they find œÉ8=0.707 ¬± 0.035.\nI don't understand the origin of this discrepancy.""]",https://arxiv.org/abs/2204.11868,"We present the first model-agnostic analysis of the complete set of Sloan Digital Sky Survey III (BOSS) and -IV (eBOSS) catalogues of luminous red galaxy and quasar clustering in the redshift range $0.2\leq z \leq 2.2$ (10 billion years of cosmic evolution), which consistently includes the baryon acoustic oscillations (BAO), redshift space distortions (RSD) and the shape of the transfer function signatures, from pre- and post-reconstructed catalogues in Fourier space. This approach complements the standard analyses techniques which only focus on the BAO and RSD signatures, and the full-modeling approaches which assume a specific underlying cosmology model to perform the analysis. These model-independent results can then easily be interpreted in the context of the cosmological model of choice. In particular, when combined with $z>2.1$ Ly-$\alpha$ BAO measurements, the clustering BAO, RSD and {\it Shape} parameters can be interpreted within a flat-$\Lambda$CDM model yielding $h=0.6816\pm0.0067$, $\Omega_{\rm m}=0.3001\pm0.0057$ and $10^{9}\times A_s= 2.43\pm0.20$ (or $\sigma_8=0.858\pm0.036$) with a Big Bang Nucleosynthesis prior on the baryon density. Without any external dataset, the BOSS and eBOSS data alone imply $\Omega_{\rm m}=0.2971\pm 0.0061$ and $10^{9}\times A_s=2.39^{+0.24}_{-0.43}$ (or $\sigma_8=0.857\pm0.040$). For models beyond $\Lambda$CDM, eBOSS data alone (in combination with Planck) constrain the sum of neutrino mass to be $\Sigma m_\nu< 0.40$ eV with a BBN prior ($\Sigma m_\nu <0.082$ eV) at 95\% CL, the curvature energy density to $\Omega_\mathrm{k} = -0.022_{-0.038}^{+0.032}$ ($\Omega_\mathrm{k} = 0.0015\pm 0.0016$) and the dark energy equation of state parameter to $w=-0.998_{-0.073}^{+0.085}$ ($w=-1.093_{-0.044}^{+0.048}$) at 68\% CL without a BBN prior. ","Model-agnostic interpretation of 10 billion years of cosmic evolution
  traced by BOSS and eBOSS data"
186,1519201215036510208,802543221943439360,Andrea Caputo,"['New paper out! <LINK>\n\nStarting from first principles, we study radiative transfer by new bosons. A particularly relevant conclusion is that in the strong trapping regime the Stefan-Boltzmann emission of bosons is a very good proxy of the true flux. <LINK>', 'This is a bit at odds with other works. The difference is that previous works adopted some approximations when dealing with the integral of the optical depth. In particular one usually finds a volume integral done with the optical depth considered only in the radial direction.', 'In the attached plot -- Primakoff process in SN -- you can see that this approximation (dashed blue curve) largely overestimates the true flux (solid blue curve).']",https://arxiv.org/abs/2204.11862,"Starting from first principles, we study radiative transfer by new feebly-interacting bosons (FIBs) such as axions, axion-like particles (ALPs), dark photons, and others. Our key simplification is to include only boson emission or absorption (including decay), but not scattering between different modes of the radiation field. Based on a given distribution of temperature and FIB absorption rate in a star, we derive explicit volume-integral expressions for the boson luminosity, reaching from the free-streaming to the strong-trapping limit. The latter is seen explicitly to correspond to quasi-thermal emission from a ""FIB sphere"" according to the Stefan-Boltzmann law. Our results supersede expressions and approximations found in the recent literature on FIB emission from a supernova core and, for radiatively unstable FIBs, provide explicit expressions for the nonlocal (""ballistic"") transfer of energy recently discussed in horizontal-branch stars. ",Radiative transfer in stars by feebly interacting bosons
187,1519012528382255104,2438508702,Carlos Gracia,"['Our last contribution w/ Fabio Dercole and @cosnet_bifi. Through a socio-economic agent-based model, we study the opinion spreading in an economic union and its effect on adhesions and withdrawals (1/3) <LINK> <LINK>', 'We show that both Union and local taxes promote the exits, whereas customs fees out of the Union boost cohesion. More: heterogeneity in both business conditions and wealth distribution promotes withdrawals. (2/3)', 'i) For low Union taxes, the wealth inequality within the country is the leading cause of anti-Union opinion spreading; ii) for high Union taxes, the country‚Äôs performance turns out to be the main driving force, resulting in a risk of wealthier countries leaving the Union. (3/3)']",http://arxiv.org/abs/2204.11553,"Economic unions are international agreements oriented to increase economic efficiency and establishing political and cultural ties between the member countries. Becoming a member of an existing union usually requires the approval of both the candidate and members, while leaving it may require only the unilateral will of the exiting country. There are many examples of accession of states to previously consolidated economic unions, and a recent example of leaving is the withdrawal of the United Kingdom from the European Union. Motivated by the Brexit process, in this paper we propose an agent-based model to study the determinant factors driving withdrawals from an economic union. We show that both Union and local taxes promote the exits, whereas customs fees out of the Union boost cohesion. Furthermore, heterogeneity in both business conditions and wealth distribution promotes withdrawals, while countries' size diversity does not have a significant effect on them. We also deep into the individual causes that lead to dissatisfaction and, ultimately, to exits. We found that, for low Union taxes, the wealth inequality within the country is the leading cause of anti-Union opinion spreading. Conversely, for high Union taxes, the country's performance turns out to be the main driving force, resulting in a risk of wealthier countries leaving the Union. These findings will be helpful for the design of economic policies and effective informative campaigns. ","Dynamics of economic unions: an agent-based model to investigate the
  economic and social drivers of withdrawals"
188,1518761665847775233,1019760963569049601,Almog Yalinewich,"['New paper on the arxiv, in which we propose a model for fast radio bursts, in which fast spinning neutron stars with low magnetic fields are the progenitors, rather than magnetars (which are the progenitors in most other models)\n<LINK>', '@di_goldene_pave oops, should be equation 19. Thanks for catching it!\nThe emission mechanism for the counterpart is the same as for the radio emission, the only difference is that the shock radius is smaller, and the Lorentz factor is higher, so the photons get boosted to a higher energy.']",https://arxiv.org/abs/2204.11663,"Recent observations of coherent radiation from the Crab pulsar (Bij et al 2021) suggest the emission is driven by an ultra - relativistic ($\gamma \sim 10^4$), cold plasma flow. A relativistically expanding plasma shell can compress the ambient magnetic field, like a moving mirror, and thus produce coherent radiation whose wavelength is shorter than that of the ambient medium by $\gamma^2$. This mechanism has been studied in the past by Colgate and Noerdelinger (1971), in the context of radio loud supernova explosions. In this work we propose that a similar mechanism drives the coherent emission in fast radio bursts. The high Lorenz factors dramatically lower the implied energy and magnetic field requirements, allowing the spin down energy of regular (or even recycled), fast spinning pulsars, rather than slow spinning magnetars, to explain FRBs. We show that this model can explain the frequency and the time evolution of observed FRBs, as well as their duration, energetics and absence of panchromatic counterparts. We also predict that the peak frequency of sub pulses decline with observation time as $\omega_{\rm obs} \propto t_{\rm obs}^{-1/2}$. Unfortunately, with current capabilities it is not possible to constrain the shape of the curve $\omega_{\rm obs} \left(t_{\rm obs} \right)$. Finally, we find that a variation of this model can explain weaker radio transients, such as the one observed from a galactic magnetar. In this variant, the shock wave produces low frequency photons which are then Compton scattered to the GHz range. ",The Moving Mirror model for Fast Radio Bursts
189,1518758768791621632,1375959506,Monica Vidaurri,"[""It's arxiv day!! So excited to share my first first-author *science* paper where we further define the Venus zone and look at the overlap between the Venus zone and the habitable zone! So when we search for Earths, we may just find Venuses instead :) <LINK>"", ""So incredibly grateful for my co-authors and mentors @shawndgoldman, @ravi_kopparapu, Sandra Bastelberger, and Eric Wolf for making this probably the smoothest paper submitting process I've ever done! I've learned so much from them!"", '@keshawnrants it takes one to know one!!', '@RocketToLulu WE LOVE VENUSES', 'I am especially fond of section 5.2, where you can learn all about why planets are *really weird* and why we need to learn more about Venus!', '@Divya_M_P thank you!!! ‚ù§Ô∏è', '@SpaceTides thank you!!! ü§†ü§†ü§†', '@Rose_D_Luna thank you Rose! ‚ù§Ô∏è', '@toomanyspectra https://t.co/9WRDuX3eVX']",https://arxiv.org/abs/2204.10919,"A key item of interest for planetary scientists and astronomers is the habitable zone, or the distance from a host star where a terrestrial planet can maintain necessary temperatures in order to retain liquid water on its surface. However, when observing a system's habitable zone, it is possible that one may instead observe a Venus-like planet. We define ""Venus-like"" as greenhouse-gas-dominated atmosphere occurring when incoming solar radiation exceeds infrared radiation emitted from the planet at the top of the atmosphere, resulting in a runaway greenhouse. Our definition of Venus-like includes both incipient and post-runaway greenhouse states. Both the possibility of observing a Venus-like world and the possibility that Venus could represent an end-state of evolution for habitable worlds, requires an improved understanding of the Venus-like planet; specifically, the distances where these planets can exist. Understanding this helps us define a ""Venus zone"", or the region in which Venus-like planets could exist, and assess the overlap with the aforementioned ""Habitable Zone"". In this study, we use a 1D radiative-convective climate model to determine the outer edge of the Venus zone for F0V, G2V, K5V, and M3V and M5V stellar spectral types. Our results show that the outer edge of the Venus zone resides at 3.01, 1.36, 0.68, 0.23, and 0.1 AU, respectively. These correspond to incident stellar fluxes of 0.8, 0.55, 0.38, 0.32, and 0.3 S, respectively, where stellar flux is relative to Earth (1.0). These results indicate that there may be considerable overlap between the habitable zone and the Venus zone. ",The Outer Edge of the Venus Zone Around Main-Sequence Stars
190,1518675417590292483,256442599,Patrick Fernandes,"[""Trying to interpret your neural networks but out-of-the-box methods aren't working?\nIn our new preprint, we propose a framework for automatically learning to explain NN decisions!\n\n<LINK>\n\nco-led @MarcosTreviso, w/ @danish037 @andre_t_martins @gneubig \n\n1/14 <LINK>"", 'While many works propose methods for extracting explanations from neural networks, the interpretability community is still trying to figure out what explanations are supposed to *achieve* and how to *evaluate* them.\n\n2/14', 'Recently, some have argued for the use of *simulability*: how much do explanations help humans/other models predict the decisions of the model being explained.\n\n3/14', 'In particular, @danish037 et al. proposed a framework   for evaluating explanations by how well the *scaffold*: how much they improve students being trained to simulate a *teacher* (the model being explained), as measured by its simulation accuracy *without* explanations\n\n4/14 https://t.co/nz81BHk12H', 'In this work, we extend this framework to allow directly optimizing explanations for improving the training of the student, in what we call Scaffold-Maximizing Training (SMaT).\n\n5/14', 'We formulate this as a bi-level optimization problem and, by making certain assumptions on the methods for extracting explanations (the explainer), we use higher-order differentiation / meta-learning to optimize the explainer.\n\n6/14 https://t.co/KN5C9BdmSe', 'We then introduce a new parametrized attention-based explainer that, when trained with SMaT, learns which attention heads in transformers are relevant for explaining a decision!\n\n7/14 https://t.co/2tu7Zo83qF', 'To evaluate SMaT, we train students to simulate pretrained transformers on three tasks: text classification (IMDb), quality estimation (MLQE-PE) and image classification (CIFAR100)\n\nWe compare our SMaT-trained explainer with static attention and gradient-based explainers\n\n8/14', 'We find that, *for all tasks considered*, the SMaT trained explainers lead to students that can simulate the teacher *much more* effectively than students trained with static attention or gradient-based explainers!\n\n9/14 https://t.co/ngJtPudsFi', ""To ensure that improvements in simulability translate to better explanations to humans, we measure their plausibility: how well they align with how humans would explain the model's decision\n\nWe find that SMaT leads to explanations that align better with human explanations!\n\n10/14 https://t.co/AkSsrcyegP"", 'We also have some cool analyses showing what attention heads are learned to be important for each model/task, and the impact of an important decision in the design of our attention-based explainer!\n\n11/14 https://t.co/krbtuoTLRE', 'Our work shows that scaffolding is a suitable criterion for both evaluating and optimizing explainability methods. We hope that the interpretability community starts to use it when proposing and evaluating new methods!\n\n12/14', 'We make all our code available in Github\nhttps://t.co/9Gc7I9MHf1\n\nIf you use Jax/Flax and @huggingface transformers, it should be fairly easy to train explainers for your models. Check out the repo for an example in training explainers for BERT on SST-2 (not in the paper!)\n\n13/14 https://t.co/MxUiybpUNP', ""I would like to thank my amazing collaborators for all their help! This project took over a year to complete and had many setbacks, but I'm super proud of the final results. Any feedback is welcome!\n\nThis was work done within the scope of @CMUPortugal's MAIA project.\n\n14/14""]",http://arxiv.org/abs/2204.10810,"Modern machine learning models are opaque, and as a result there is a burgeoning academic subfield on methods that explain these models' behavior. However, what is the precise goal of providing such explanations, and how can we demonstrate that explanations achieve this goal? Some research argues that explanations should help teach a student (either human or machine) to simulate the model being explained, and that the quality of explanations can be measured by the simulation accuracy of students on unexplained examples. In this work, leveraging meta-learning techniques, we extend this idea to improve the quality of the explanations themselves, specifically by optimizing explanations such that student models more effectively learn to simulate the original model. We train models on three natural language processing and computer vision tasks, and find that students trained with explanations extracted with our framework are able to simulate the teacher significantly more effectively than ones produced with previous methods. Through human annotations and a user study, we further find that these learned explanations more closely align with how humans would explain the required decisions in these tasks. Our code is available at this https URL ",Learning to Scaffold: Optimizing Model Explanations for Teaching
191,1518501490578661377,703555806973845504,Michiel Lambrechts,"['In this new study, we (Liu, @astroAnders, me, Bizzarro, Haugb√∏lle ) show how pebble drift is consistent with the emergence of the so-called  carbonaceous and non-carbonaceous meteoritic reservoirs. <LINK>\n<LINK> <LINK>', 'Check this thread to have a full summary:\nhttps://t.co/KaiBvSAlVT']",https://arxiv.org/abs/2204.10651,"Meteorites display an isotopic composition dichotomy between non-carbonaceous (NC) and carbonaceous (CC) groups, indicating that planetesimal formation in the solar protoplanetary disk occurred in two distinct reservoirs. The prevailing view is that a rapidly formed Jupiter acted as a barrier between these reservoirs. We show a fundamental inconsistency in this model: if Jupiter is an efficient blocker of drifting pebbles, then the interior NC reservoir is depleted by radial drift within a few hundred thousand years. If Jupiter lets material pass it, then the two reservoirs will be mixed. Instead, we demonstrate that the arrival of the CC pebbles in the inner disk is delayed for several million years by the viscous expansion of the protoplanetary disk. Our results support that Jupiter formed in the outer disk (>10 AU) and allowed a considerable amount of CC material to pass it and become accreted by the terrestrial planets. ","Natural separation of two primordial planetary reservoirs in an
  expanding solar protoplanetary disk"
192,1517451185019572226,216729597,Marcel S. Pawlowski,"[""New paper on the arXiv, and it's an exciting one! \n\nWe studied a possible joint explanation for the formation of *both* planes of satellite galaxies in the Local Group, the one of the Milky Way and Andromeda's, via a past MW-M31 encounter ‚Ä¶ in MOND.\n\n<LINK> <LINK>"", ""You might've heard of the planes of satellite galaxies problem of cosmology. The finding that e.g. the Milky Way satellite galaxies are distributed, and preferentially rotate, in a flattened structure. A similar structure was found for Andromeda (M31). https://t.co/ZZcZNDFE31"", 'Such coherent arrangements are exceedingly rare for satellites in cosmological simulations based on the standard ŒõCDM model.\n\nA possible source of coherently orbiting satellites are tidal dwarf galaxies. Those should be free of DM in ŒõCDM, but would look like they had DM in MOND.', 'Prime candidates for the interacting galaxies producing tidal debris is a past fly-by encounter of the MW and M31. Using the Phantom of RAMSES code, such a collision was simulated in MOND, to see if the resulting debris are oriented similar to the observed satellite planes. https://t.co/W0B86uZlaz', ""Wait, what? A past MW-M31 encounter?\n\nIn MOND, since all mass is in baryons, you can calculate the MW-M31 orbit backwards (similar to the Timing Argument, but no freedom to add DM). Turns out that due to the stronger acceleration, there must've been a past pericenter; a fly-by. https://t.co/n5WbAyBHWZ"", ""The orbit that works best even turns out to have a MW-M31 proper motion consistent with observations, even though this wasn't taken as a constraint in building the model. A nice consistency giving more credibility to this approach. https://t.co/amun4TqJbX"", 'So, can the tidal debris reproduce the observed satellite galaxy coherence? Yes!\nThe orbital poles of the observed, brightest MW satellites (left) cluster, indicating that they co-orbit. The tidal debris in the simulation (right) cluster similarly, and in the same direction! https://t.co/ozlIPx72AZ', ""Similarly, there are tidal debris around the M31 analog of the simulation that also cluster where the expected orbital pole of Andromeda's satellite galaxy plane points (pink cross). https://t.co/b9siki79CH"", ""There's a lot more the paper talks about, but I'll leave it at this here and recommend you read it if interested in the details of the simulation and its parameters, the approach, MOND, or the resulting distribution, mass and angular momenta of the debris: https://t.co/MWObSrZuVs"", ""There's of course a lot still left to be done, for example forming actual tidal dwarfs from the debris, forming enough of them, or checking if they could show sufficiently old stellar populations and low metallicities to be consistent."", ""So, we have a possible explanation of the orientation of both satellite planes in the Local Group (if we live in a MOND universe). There was no reason this had to work, so showing it is possible is a big deal! Doesn't prove the scenario, but definitely encourages further work."", ""@maximetrebitsch @benfamaey Thanks! Since these are early galaxies, they should have a rather high gas fraction. And especially for the tidal debris it is important to model the disks  to large radii, where there's gas but no stars."", ""@maximetrebitsch @benfamaey Those gas disks might experience some hydro-specific effects during the encounter that a pure N-body model wouldn't catch ‚Ä¶  though I believe the results for an analog, purely stellar model should be similar at this stage."", ""@maximetrebitsch @benfamaey Of course a full model with cooling, SF, feedback would be better. That's also needed to potentially form actual tidal dwarfs. But one step at a time. ;-)"", ""@mschaller_ @benfamaey I'm afraid I'll have to disappoint you: there aren't any new galaxies formed in this simulation, so nothing to show for the MDAR. Except maybe the two initial disk galaxies, which however are set up to be stable in MOND, so they'll follow the MDAR by definition.""]",https://arxiv.org/abs/2204.09687,"The existence of mutually correlated thin and rotating planes of satellite galaxies around both the Milky Way (MW) and Andromeda (M31) calls for an explanation. Previous work in Milgromian dynamics (MOND) indicated that a past MW-M31 encounter might have led to the formation of these satellite planes. We perform the first-ever hydrodynamical MOND simulation of the Local Group using Phantom of RAMSES. We show that an MW-M31 encounter at $z \approx 1$, with a perigalactic distance of about 80 kpc, can yield two disc galaxies at $z=0$ oriented similarly to the observed galactic discs and separated similarly to the observed M31 distance. Importantly, the tidal debris are distributed in phase space similarly to the observed MW and M31 satellite planes, with the correct preferred orbital pole for both. The MW-M31 orbital geometry is consistent with the presently observed M31 proper motion despite this not being considered as a constraint when exploring the parameter space. The mass of the tidal debris around the MW and M31 at $z=0$ compare well with the mass observed in their satellite systems. The remnant discs of the two galaxies have realistic radial scale lengths and velocity dispersions, and the simulation naturally produces a much hotter stellar disc in M31 than in the MW. However, reconciling this scenario with the ages of stellar populations in satellite galaxies would require that a higher fraction of stars previously formed in the outskirts of the progenitors ended up within the tidal debris, or that the MW-M31 interaction occurred at $z>1$. ","3D hydrodynamic simulations for the formation of the Local Group
  satellite planes"
193,1517409532011372544,1271852576296906755,Ashley Chrimes,"['And here it is, another new paper on arxiv today! We compare the NIR photometry of Galactic magnetar counterparts with @astroBPASS expectations for NS companions. We find one (strong) candidate for a bound companion to a magnetar! Read more here - <LINK> <LINK>']",https://arxiv.org/abs/2204.09701,"It is well established that magnetars are neutron stars with extreme magnetic fields and young ages, but the evolutionary pathways to their creation are still uncertain. Since most massive stars are in binaries, if magnetars are a frequent result of core-collapse supernovae, some fraction are expected to have a bound companion at the time of observation. In this paper, we utilise literature constraints, including deep Hubble Space Telescope imaging, to search for bound stellar companions to magnetars. The magnitude and colour measurements are interpreted in the context of binary population synthesis predictions. We find two candidates for stellar companions associated with CXOU J171405.7-381031 and SGR 0755-2933, based on their J-H colours and H-band absolute magnitudes. Overall, the proportion of the Galactic magnetar population with a plausibly stellar near-infrared counterpart candidate, based on their magnitudes and colours, is between 5 and 10 per cent. This is consistent with a population synthesis prediction of 5 per cent, for the fraction of core-collapse neutron stars arising from primaries which remain bound to their companion after the supernova. These results are therefore consistent with magnetars being drawn in an unbiased way from the natal core-collapse neutron star population, but some contribution from alternative progenitor channels cannot be ruled out. ","Where are the magnetar binary companions? Candidates from a comparison
  with binary population synthesis predictions"
194,1517314815055872002,848456472,Kevin Napier,"[""Simulations show that Earth is capable of hosting primordial Trojan asteroids. So, why can't we find any? Well... there might not be any left. It turns out that giant rocks smashing into Earth billions of years ago may have eliminated them all. \n<LINK>""]",https://arxiv.org/abs/2204.10316,"Due to their strong resonances with their host planet, Trojan asteroids can remain in stable orbits for billions of years. As a result, they are powerful probes for constraining the dynamical and chemical history of the solar system. Although we have detected thousands of Jupiter Trojans and dozens of Neptune Trojans, there are currently no known long-term stable Earth Trojans. Dynamical simulations show that the parameter space for stable Earth Trojans in substantial, so their apparent absence poses a mystery. This work uses a large ensemble of N-body simulations to explore how the Trojan population dynamically responds if Earth suffers large collisions, such as those thought to have occurred to form the Moon and/or to have given Earth its Late Veneer. We show that such collisions can be highly disruptive to the primordial Trojan population, and could have eliminated it altogether. More specifically, if Earth acquired the final 1\% of its mass through ${\cal O}(10)$ collisions, then only $\sim1\%$ of the previously bound Trojan population would remain. ",A Collision Mechanism for the Removal of Earth's Trojan Asteroids
195,1517024804116865026,422672164,Dr Michael Reidinger,"['Freeze-in and freeze-out of sterile neutrino dark matter\n\n""We propose a new production mechanism involving the decay and annihilation of a complex scalar singlet with a Higgs portal coupling which develops a vacuum expectation value.""\n<LINK>']",https://arxiv.org/abs/2204.08795,A sterile neutrino with a keV-scale mass is a compelling dark matter candidate. We propose a new production mechanism involving the decay and annihilation of a complex scalar singlet with a Higgs portal coupling which develops a vacuum expectation value. The interactions of the resulting pseudo Nambu-Goldstone boson may thermalise the dark sector. We determine the region of parameter space where dark sector thermalisation is reached and discuss the most relevant cosmological observables. The scenario can be considered as the combination of a freeze-in of the dark sector followed by relativistic freeze-out. ,Freeze-in and freeze-out of sterile neutrino dark matter
196,1517017320979415041,1379356298714742784,SummarizedML,"[""We use attention information from trajectory prediction models to find and rank surrounding agents by their impact on the ego's plan.\nüìÑ <LINK> <LINK>""]",http://arxiv.org/abs/2204.09121v1,"Trajectory prediction is an important task in autonomous driving. State-of-the-art trajectory prediction models often use attention mechanisms to model the interaction between agents. In this paper, we show that the attention information from such models can also be used to measure the importance of each agent with respect to the ego vehicle's future planned trajectory. Our experiment results on the nuPlans dataset show that our method can effectively find and rank surrounding agents by their impact on the ego's plan. ","] Importance is in your attention: agent importance prediction for
  autonomous driving"
197,1516784363299553288,859171889410953221,Melissa Morris,"['Ever find yourself awake late at night, thinking ""wow, radio AGN with bent jets are super weird, I wonder what their environments look like compared to other radio AGN and if we can use them to learn about galaxy groups?"" Well, my paper has you covered!! <LINK>']",https://arxiv.org/abs/2204.08510,"Galaxies hosting Active Galactic Nuclei (AGN) with bent radio jets are used as tracers of dense environments, such as galaxy groups and clusters. The assumption behind using these jets is that they are bent under ram pressure from a dense, gaseous medium through which the host galaxy moves. However, there are many AGN in groups and clusters with jets that are not bent, which leads us to ask: why are some AGN jets affected so much by their environment while others are seemingly not? We present the results of an environmental study on a sample of 185 AGN with bent jets and 191 AGN with unbent jets in which we characterize their environments by searching for neighboring galaxies using a Friends-of-Friends algorithm. We find that AGN with bent jets are indeed more likely to reside in groups and clusters, while unbent AGN are more likely to exist in singles or pairs. When considering only AGN in groups of 3 or more galaxies, we find that bent AGN are more likely to exist in halos with more galaxies than unbent AGN. We also find that unbent AGN are more likely than bent AGN to be the brightest group galaxy. Additionally, groups hosting AGN with bent jets have a higher density of galaxies than groups hosting unbent AGN. Curiously, there is a population of AGN with bent jets that are in seemingly less dense regions of space, indicating they may be embedded in a cosmic web filament. Overall, our results indicate that bent doubles are more likely to exist in in larger, denser, and less relaxed environments than unbent doubles, potentially linking a galaxy's radio morphology to its environment. ",How does environment affect the morphology of radio AGN?
198,1516737320254414850,945820940109336579,Jessica M√©gane,"['Our article has been accepted at @GeccoConf ü•≥\nWe propose Co-PSGE and compare the performance on 4 benchmark problems. GE was outperformed on all problems and SGE on 2 of the 4.\nThe preprint version is available at: <LINK> <LINK>', 'The code is available on Github, and if anyone has any suggestions or questions about the work, feel free to ask! üòá\nhttps://t.co/7obrP4CgdL', '@yurilavinas @GeccoConf Thank you! See you in Boston üòä']",https://arxiv.org/abs/2204.08985,"This work proposes an extension to Structured Grammatical Evolution (SGE) called Co-evolutionary Probabilistic Structured Grammatical Evolution (Co-PSGE). In Co-PSGE each individual in the population is composed by a grammar and a genotype, which is a list of dynamic lists, each corresponding to a non-terminal of the grammar containing real numbers that correspond to the probability of choosing a derivation rule. Each individual uses its own grammar to map the genotype into a program. During the evolutionary process, both the grammar and the genotype are subject to variation operators. The performance of the proposed approach is compared to 3 different methods, namely, Grammatical Evolution (GE), Probabilistic Grammatical Evolution (PGE), and SGE on four different benchmark problems. The results show the effectiveness of the approach since Co-PSGE is able to outperform all the methods with statistically significant differences in the majority of the problems. ",Co-evolutionary Probabilistic Structured Grammatical Evolution
199,1516686749170290690,1455258712961167361,Bel√©n Alastruey,['Happy to share ‚ÄúOn the Locality of Attention in Direct Speech Translation‚Äùüí¨üîç accepted at the ACL-SRW!\n\nWe use interpretability techniques to\npropose an efficient architecture that\nmatches the baseline performance while reducing computational cost.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2204.09028,"Transformers have achieved state-of-the-art results across multiple NLP tasks. However, the self-attention mechanism complexity scales quadratically with the sequence length, creating an obstacle for tasks involving long sequences, like in the speech domain. In this paper, we discuss the usefulness of self-attention for Direct Speech Translation. First, we analyze the layer-wise token contributions in the self-attention of the encoder, unveiling local diagonal patterns. To prove that some attention weights are avoidable, we propose to substitute the standard self-attention with a local efficient one, setting the amount of context used based on the results of the analysis. With this approach, our model matches the baseline performance, and improves the efficiency by skipping the computation of those weights that standard attention discards. ",On the Locality of Attention in Direct Speech Translation
200,1516271391787536386,714535792366981121,Chris,"['Hi folks, we have a new submitted paper! In short, we estimate scaleheights and ages of low-mass stars and brown dwarfs in deep fields. We find that M dwarfs are older than L dwarfs as a population, in agreement with models. <LINK> <LINK>', 'We also make predictions for a similar survey with JWST called PASSAGES. With the upcoming Nancy Grace Roman Telescope  and the Vera Rubin Observatory, we will be able to see these tiny ""stars"" throughout the Galaxy.', 'The goal is to start doing galactic archeology with brown dwarfs and low-mass stars and to learn something potentially new about how our galaxy formed and evolved.', 'There are significant systematic uncertainties in evolutionary models and age-velocity dispersion relations in addition to limitations due to small sample sizes, which will be solved, in part, by the next generation of surveys.', 'Major thanks to my co-authors for being awesome ! @ChihChunHsu @browndwarfs @philosicist @astro_daniella  @RobTejada42  + others']",https://arxiv.org/abs/2204.07621,"Ultracool dwarfs represent a significant proportion of stars in the Milky Way,and deep samples of these sources have the potential to constrain the formation history and evolution of low-mass objects in the Galaxy. Until recently, spectral samples have been limited to the local volume (d<100 pc). Here, we analyze a sample of 164 spectroscopically-characterized ultracool dwarfs identified by Aganze et al. (2022) in the Hubble Space Telescope WFC3 Infrared Spectroscopic Parallel (WISP) Survey and 3D-HST. We model the observed luminosity function using population simulations to place constraints on scaleheights, vertical velocity dispersions and population ages as a function of spectral type. Our star counts are consistent with a power-law mass function and constant star formation history for ultracool dwarfs, with vertical scaleheights 249$_{-61}^{+48}$ pc for late M dwarfs, 153$_{-30}^{+56}$ pc for L dwarfs, and 175$_{-56}^{+149}$ pc for T dwarfs. Using spatial and velocity dispersion relations, these scaleheights correspond to disk population ages of 3.6$_{-1.0}^{+0.8}$ for late M dwarfs, 2.1$_{-0.5}^{+0.9}$ Gyr for L dwarfs, and 2.4$_{-0.8}^{+2.4}$ Gyr for T dwarfs, which are consistent with prior simulations that predict that L-type dwarfs are on average a younger and less dispersed population. There is an additional 1-2 Gyr systematic uncertainty on these ages due to variances in age-velocity relations. We use our population simulations to predict the UCD yield in the JWST PASSAGES survey, a similar and deeper survey to WISPS and 3D-HST, and find that it will produce a comparably-sized UCD sample, albeit dominated by thick disk and halo sources. ","Beyond the Local Volume II: Population Scaleheights and Ages of
  Ultracool Dwarfs in Deep HST/WFC3 Parallel Fields"
201,1516129686601932800,1196838055803416576,Dimitrios Fraggedakis,"['In my recent work with Hasyim and Mandadapu, we propose that the binding-unbinding transition of elastic dipolar excitations predicts the onset temperature of glassy dynamics in two-dimensional supercooled #liquids. arxiv: <LINK> <LINK>']",https://arxiv.org/abs/2204.07528,"Below the onset temperature $T_\text{o}$, the equilibrium relaxation time of most glass-forming liquids exhibits glassy dynamics characterized by super-Arrhenius temperature dependence. In this supercooled regime, the relaxation dynamics also proceeds through localized elastic excitations corresponding to hopping events between inherent states, i.e., potential-energy minimizing configurations of the liquid. Despite its importance in distinguishing the supercooled regime from the high-temperature regime, the microscopic origin of $T_\text{o}$ is not yet known. Here, we construct a theory for the onset temperature in two dimensions and find that inherent-state melting transition, described by the binding-unbinding transition of dipolar elastic excitations, delineates the supercooled regime from the high-temperature regime. The corresponding melting transition temperature is in good agreement with the onset temperature found in various two-dimensional atomistic models of glass formers. We discuss the predictions of our theory on the displacement and density correlations of two-dimensional supercooled liquids, which are consistent with observations of the Mermin-Wagner fluctuations in recent experiments and molecular simulations. ","Inherent-State Melting and the Onset of Glassy Dynamics in
  Two-Dimensional Supercooled Liquids"
202,1514911641762156546,220480514,Cihan Okay,"['Very excited to share the first publication <LINK>, of my group @Bilkent_math, joint with my postdocs Selman Ipek and Aziz Kharoof. Combining simplicial sets and probabilities we introduce simplicial distributions and study a fundamental quantum phenomenon.']",https://arxiv.org/abs/2204.06648,"We introduce a new framework for contextuality based on simplicial sets, combinatorial models of topological spaces that play a prominent role in modern homotopy theory. Our approach extends measurement scenarios to consist of spaces (rather than sets) of measurements and outcomes, and thereby generalizes nonsignaling distributions to simplicial distributions, which are distributions on spaces modeled by simplicial sets. Using this formalism we present a topologically inspired new proof of Fine's theorem for characterizing noncontextuality in Bell scenarios. Strong contextuality is generalized suitably for simplicial distributions, allowing us to define cohomological witnesses that extend the earlier topological constructions restricted to algebraic relations among quantum observables to the level of probability distributions. Foundational theorems of quantum theory such as the Gleason's theorem and Kochen-Specker theorem can be expressed naturally within this new language. ",Simplicial quantum contextuality
203,1514868617468387335,328430286,Jad C. Halimeh,"[""New paper <LINK>. We propose a cold-atom quantum simulator to implement the topological Œ∏-angle in gauge theories. We study the effect of confinement due to this angle on Coleman's phase transition and scarring.\n@HaukeGroup\n@QManyBody\n@MCQST_cluster\n@ERC_Research <LINK>""]",https://arxiv.org/abs/2204.06570,"The topological $\theta$-angle in gauge theories engenders a series of fundamental phenomena, including violations of charge-parity (CP) symmetry, dynamical topological transitions, and confinement--deconfinement transitions. At the same time, it poses major challenges for theoretical studies, as it implies a sign problem in numerical simulations. Analog quantum simulators open the promising prospect of treating quantum many-body systems with such topological terms, but, contrary to their digital counterparts, they have not yet demonstrated the capacity to control the $\theta$-angle. Here, we demonstrate how a tunable topological $\theta$-term can be added to a prototype theory with $\mathrm{U}(1)$ gauge symmetry, a discretized version of quantum electrodynamics in one spatial dimension. As we show, the model can be realized experimentally in a single-species Bose--Hubbard model in an optical superlattice with three different spatial periods, thus requiring only standard experimental resources. Through numerical calculations obtained from the time-dependent density matrix renormalization group method and exact diagonalization, we benchmark the model system, and illustrate how salient effects due to the $\theta$-term can be observed. These include charge confinement, the weakening of quantum many-body scarring, as well as the disappearance of Coleman's phase transition due to explicit breaking of CP symmetry. This work opens the door towards studying the rich physics of topological gauge-theory terms in large-scale cold-atom quantum simulators. ","Tuning the Topological $\theta$-Angle in Cold-Atom Quantum Simulators of
  Gauge Theories"
204,1514714893252972551,1289141085588029445,Kerem Zaman,"['üö®New pre-print on evaluating explainability methods for #NLProc!\n\nWe propose a new crosslingual framework for faithfulness evaluation and provide a new multilingual highlights dataset, called e-XNLI.\n\nLink: <LINK>\n\nJoint work w/ @boknilev!', 'Prior work shows erasure-based faithfulness evaluation suffers from OOD perturbations. To overcome this problem, we treat translation pairs as naturally occurring input perturbations. \n\nWe evaluate faithfulness by comparing attribution scores obtained from a multilingual model++ https://t.co/OxLXSxdI7p', 'for different translation pairs across several languages. \n\nFor this comparison, we look at the correlation between ""aligned"" attribution scores of translation pairs. https://t.co/WHahkVuP3O', 'Our results show our method can distinguish different attribution methods better than erasure-based faithfulness evaluation.\n\nWe also provide a multilingual highlights dataset consisting of automatically++', 'extracted highlights, which may support future exNLP studies.\n\nFurthermore, we present a comprehensive evaluation of a wide range of attribution methods. Apart from previous benchmarks, we compare both different output mechanisms (top predicted class vs. loss) and aggregation++ https://t.co/jUnqZ5CV6J', 'methods. Finally, we present main takeways which may be useful for practitioners while applying these methods.\n\nAll code and data can be found on the Github: https://t.co/MF5m2jO363 \n\nPlease feel free to share any thoughts, comments and feedback!']",https://arxiv.org/abs/2204.05428,"Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of plausibility and faithfulness properties. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the potential downsides of erasure-based evaluations. We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, which may support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different. ","A Multilingual Perspective Towards the Evaluation of Attribution Methods
  in Natural Language Inference"
205,1514702332889546760,1404001821060915205,Yerong,"['Our new paper on 1H 1934-063 appear on arxiv now (<LINK>)! We find a relatively low-ionization (logxi~1.6), low-speed (0.075c) ultra-fast outflow and explain the unknown 1 keV emission with a secondary reflection on the base of outflows.', 'This discovery may reveal the link between the reprocessing of the inner accretion flow photons and ejection. https://t.co/xjuP3Z7aUb']",http://arxiv.org/abs/2204.06075,"Accretion and ejection of matter in active galactic nuclei (AGN) are tightly connected phenomena and represent fundamental mechanisms regulating the growth of the central supermassive black hole and the evolution of the host galaxy. However, the exact physical processes involved are not yet fully understood. We present a high-resolution spectral analysis of a simultaneous \xmm\ and \nustar\ observation of the narrow line Seyfert 1 (NLS1) AGN 1H 1934-063, during which the X-ray flux dropped by a factor of $\sim6$ and subsequently recovered within 140 kiloseconds. By means of the time-resolved and flux-resolved X-ray spectroscopy, we discover a potentially variable warm absorber and a relatively stable ultra-fast outflow (UFO, $v_\mathrm{UFO}\sim-0.075\,c$) with a mild ionization state ($\log(\xi/\mathrm{erg\,cm\,s^{-1})}\sim1.6$). The detected emission lines (especially a strong and broad feature around 1\,keV) are of unknown origin and cannot be explained with emission from plasmas in photo- or collisional-ionization equilibrium. Such emission lines could be well described by a strongly blueshifted ($z\sim-0.3$) secondary reflection off the base of the equatorial outflows, which may reveal the link between the reprocessing of the inner accretion flow photons and the ejection. However, this scenario although being very promising is only tentative and will be tested with future observations. ",Ejection-accretion connection in NLS1 AGN 1H 1934-063
206,1514643119119155203,1398705059551223808,Chris Harrison,"['Latest Quasar Feedback Survey paper, led by student Silpa Sasikumar: <LINK>. We reveal the complex interplay between jets, winds and gas using polarisation-sensitive radio observations. Find out more about the survey and data products: <LINK>']",https://arxiv.org/abs/2204.05613,"We present results from a combined radio polarization and emission line study of five type 2 quasars at $z<0.2$ with the Karl G. Jansky Very Large Array (VLA) B-array at 5 GHz and Hubble Space Telescope (HST) [O~{\sc{iii}}] observations. These five sources are known to exhibit close association between radio structures and ionized gas morphology and kinematics. Four sources (J0945+1737, J1000+1242, J1356+1026 and J1430+1339) show polarization in the current data. J1010+1413 is the unpolarized source in our sample. We detect $0.5-1\%$ fractional polarization in the radio cores and a high fractional polarization ($10-30\%$) in the lobes of these sources. The morphological, spectral and polarization properties suggest a jet origin for radio emission in J0945+1737, J1000+1242, J1010+1413 and J1430+1339 whereas the current data cannot fully discern the origin of radio emission (jet or wind) in J1356+1026. An anti-correlation between various polarized knots in the radio and [O~{\sc{iii}}] emission is observed in our sources, similar to that observed in some radio-loud AGN in the literature. This suggests that the radio emission is likely to be depolarized by the emission-line gas. By modeling the depolarization effects, we estimate the size of the emission-line gas clouds to be $\sim(2.8\pm1.7)\times10^{-5}$ parsec and the amount of thermal material mixed with the synchrotron plasma to be $\sim(9.2\pm0.8)\times10^{5}$~M$_\odot$ in the lobe of J0945+1737 (which exhibits the most prominent polarization signature in its lobe). The current work demonstrates that the interplay of jets/winds and emission-line gas is most likely responsible for the nature of radio outflows in radio-quiet AGN. ","The Quasar Feedback Survey: Revealing the Interplay of Jets, Winds \&
  Emission Line Gas in Type 2 Quasars with Radio Polarization"
207,1514554294200147976,2176486874,Steven Thomson,"['New paper out today, with Ancel Larzul and @MarcoSchiro_ of @cdf1530. In this work, we ask the question ""Are fast scramblers good thermal baths?"". But what does this mean, and why is it an interesting question? ü§î Read on to find out...! üßµ\n\n<LINK>', ""First off, what even is a 'fast scrambler'? ü§î\n\nWhen most physical systems are prepared in a far-from-equilibrium state, they undergo a process known as 'thermalisation' where after some (often chaotic!) dynamics, they eventually reach an equilibrium steady state. https://t.co/beb4gcw9ki"", 'Some physical systems thermalise very slowly, and others not at all. Major examples of the latter include glasses (https://t.co/I5zBkVuYuM) and many-body localisation (https://t.co/sqCU1sbjA8). https://t.co/02IRQubvH5', ""But some systems thermalise extremely quickly - as fast as possible, in fact - and these are known as 'fast scramblers'. The model we study in this paper is one of those. It's called the Sachdev-Ye-Kitaev model (SYK for short). https://t.co/mo9vYZQQ8f https://t.co/lhQkIBqLY8"", ""(The term 'scrambling' comes from the idea that any information contained in the initial state of the system - or any 'memory' of the initial state - is very quickly lost as the system evolves over time in a chaotic manner.) https://t.co/pzj0m46psg"", ""The SYK model is interesting for a lot of reasons, partly because it's one of the fastest scramblers, partly because it's exactly solvable (at least in certain limits...!), and partly because it has deep connections to black holes and quantum gravity that I won't get into here. https://t.co/7cP6guyOMA"", ""But these aspects of the SYK model are old news. \n\nWe're interested in what happens if we connect an SYK model to another quantum system - does the fast scrambling property of the SYK model mean that it efficiently transfers heat to and from anything that it's connected to? ü§î https://t.co/cZmri094Ei"", ""In a flagrant violation of Betteridge's law, the answer to the question in our title turns out to be 'yes'!\n\nWell...with caveats. üòâ\n\nIt turns out that for weak couplings between the SYK model and the target system, the SYK model is indeed a highly effective thermal bath. üëç"", ""The main reason is that the SYK4 model has a huge peak in its spectral function close to zero frequency (green line in bottom plot), which means that even at weak coupling, it has a huge 'appetite' for low frequency excitations, which are rapidly absorbed by the SYK model. https://t.co/iMZyRxqyX5"", ""This translates into the SYK model being very efficient at absorbing energy from anything that it's coupled to, more so than a generic thermal bath (blue line in the above image). https://t.co/w3N8yUHD7p"", 'But when the system-bath coupling is strong, this advantage is negated somewhat, and it turns out that the SYK model is no better than any other thermal bath. Basically, given a strong enough coupling, the detailed spectral properties of the bath no longer make a big difference. https://t.co/Bunohykib0', ""We demonstrate this effect in the paper using lead author Ancel's wonderful numerical simulations and some fantastic insights from the quantum Boltzmann equation - but for more on that, it's best to read the paper. üòâ\n\nPlease do check it out and share! üëç https://t.co/XfCBMVhlhM"", ""PS: This work has its origins in something we spent a lot of time on a few years ago involving coupling SYK models to general thermal baths, but divergent terms in the self-energy caused problems. I'm really happy that something based on that has finally seen the light of day! üòÅ https://t.co/vdkMQ9iqu0"", ""@Sayan_Quantum @MarcoSchiro_ @cdf1530 Thanks for your interest! That's a really interesting question. I'd guess they probably perform badly, but I'm not aware of any situation where this has been studied - 'generic' baths are usually arrays of harmonic oscillators rather than locally interacting systems."", ""@Sayan_Quantum @MarcoSchiro_ @cdf1530 As for the link between scrambling and bath effectiveness, I think we'd need to explore a wider class of models to say for sure, as we only looked at two extremes (the very fast scrambler SYK4, and integrable SYK2). Checking intermediate cases could help establish this link. ü§î"", ""@Sayan_Quantum @MarcoSchiro_ @cdf1530 Ultimately the SYK4 was a good bath at weak coupling due to the enhanced spectral density at low frequencies as compared with SYK2, so the deeper question is whether this sort of spectral feature is directly indicative of scrambling rates, and I don't know the answer to this. ü§î"", ""@TanmayBhore It's very likely to be one of the entries, at least! üòÖ Although I have to say, there are quite a few really interesting papers on today's mailing list, so I might have to make some hard choices...!""]",https://arxiv.org/abs/2204.06434,"The Sachdev-Ye-Kitaev (SYK$_{4}$) model has attracted attention for its fast scrambling properties and its thermalization rate that is set only by the temperature. In this work we ask the question of whether the SYK$_{4}$ model is also a good thermal bath, in the sense that it allows a system coupled to it to thermalize. We address this question by considering the dynamics of a system of $N$ random non-interacting Majorana fermions coupled to an SYK$_{4}$ bath with $M$ Majorana fermions that we solve with Keldysh techniques in the limit of $M\gg N\gg 1$. We compare this nonequilibrium setting with a conventional bath made of free non-interacting degrees of freedom with a continous spectrum. We show that the SYK$_{4}$ bath is more efficient in thermalising the system at weak coupling, due to its enhanced density of states at low frequency, while at strong system-bath couplings both type of environments give rise to a similar time scale for thermalisation. ",Are fast scramblers good thermal baths?
208,1514505370030514181,847986180,Fred Jendrzejewski,"['We have an easter egg on the #arxiv. We studied dynamics of fluctuations in atomic mixtures. This allowed us to identify regimes of (non)-thermalization in a controlled fashion. <LINK>', 'It also provided us detailled access to the different sources of fluctuations like thermal, quantum or just plain technical. So it greatly improved our understanding of the system underlying our effort on #quantumsimulation of lattice gauge theories.', 'Thanks once more for the great collaboration with @SynQS, #isoquant and @ICFOnians']",https://arxiv.org/abs/2204.06456,"We investigate an ultra-cold mixture of Bose gases interacting via spin-changing collisions by studying the dynamics of spin fluctuations. The experimental implementation employs $^{23}$Na and $^{7}$Li atoms, which are prepared out of equilibrium across a wide range of initial conditions. We identify three regimes in the dynamics of the system for different initial states: a long-lived metastable regime, an instability range with strong growth of fluctuations, and a fast relaxing regime approaching thermal equilibrium. Theoretical modelling of the data allows us to reconstruct effective potentials which characterize the different dynamical regimes of the system. ",Non-equilibrium dynamics of fluctuations in an ultra-cold atomic mixture
209,1514399905615278083,288414412,Ollie Hines,['Check out our latest preprint where we propose new CATE variable importance measures for understanding heterogeneous causal effects. Pretty excited about this one! @karlado @SVansteelandt: <LINK>'],https://arxiv.org/abs/2204.06030,"The conditional average treatment effect (CATE) of a binary exposure on an outcome is often studied on the basis that personalised treatment decisions lead to better clinical outcomes. The CATE, however, may be a complicated function of several covariates representing patient characteristics. As such, clinicians rely on researchers to summarise insights related to treatment effect heterogeneity. Clinical research usually focuses on the ATE, which averages the CATE across all patient characteristics, or considers the treatment effect in patient strata, though strata are rarely chosen systematically. Here we present new nonparametric treatment effect variable importance measures (TE-VIMs). These may guide clinicians as to which patient characteristics are important to consider when making treatment decisions, and help researchers stratify patient populations for further study. TE-VIMs extend recent regression-VIMs, viewed as nonparametric analogues to ANOVA statistics. Moreover, TE-VIMs are not tied to a particular model, thus are amenable to data-adaptive (machine learning) estimation of the CATE, itself an active area of research. Estimators for the proposed statistics are derived from their efficient influence curves and these are illustrated through a simulation study and an applied example. ",Variable importance measures for heterogeneous causal effects
210,1514167220674379782,796296603963424768,Fabien Ferrage,"['What models should we use to analyze NMR relaxation rates in proteins? Is model-free one-size-fits-all? In two preprints, we propose a series of explicit models of motions of protein side-chains. A tour de force by the fantastic @BolikCoulon  and coll <LINK> 1/n', 'For a while, we did our best with model-free types of correlation functions to analyze broad relaxation datasets recorded for methyl groups in ubiquitin, including relaxometry measurements down to 0.3 T and cross-correlated relation. https://t.co/C8rI7vdXhB 2/n', 'This approach worked great for auto-correlation rates but not cross-correlation. Two years ago, while confined at home @BolikCoulon decided to work on solving this problem. 3/n', 'We were guided by molecular dynamics simulations that define very well the nature of side-chain motions in proteins, which are quite anisotropic: jumps between rotamer states, methyl rotation, etc. 4/n', 'Inspired by the literature from the pre-model-free era, @BolikCoulon built correlation functions relevant for such combinations of motions, including correlation between motions and changes of CSA tensors between rotamers ü§Ø. https://t.co/jikgjpe5to 5/n', 'With our excellent collaborators at @IbpcF and @Chimie__ENS, we used molecular dynamics simulations to design explicit models of motions for each side-chain and analyzed 38 relaxation rates per residue with these models. https://t.co/PKoAeUq85v 6/n', 'We find better agreement than with our previous model-free models and can interpret the motions with mechanistic insight. For instance, the best model for Ile23 in ubiquitin introduces slower methyl rotation in one rotamer, when steric hindrance is likely. 7/n', 'By defining explicitly the motion, the link with configuration entropy is direct. Importantly, introducing populations of all rotamers, we show that the relationship between order parameters and conformational entropy is not bijective. 8/n', '@falk_ho, Mulder and Schaefer, have already shown that estimating side-chain entropy from relaxation is difficult. But we expect hybrid approaches combining MD and NMR will improve our understanding of the molecular origin of conformational entropy. 9/n https://t.co/saCf6XZEgM', 'I want to thank all authors, @BolikCoulon, Olivier, Milan, Diego, Damien, Fabio and Guillaume, for their many contributions to this project, allowing us to combine NMR, MD, DFT and theory. I thank also @psl_univ and the EU project @HMultidyn for funding. 10/10']",https://arxiv.org/abs/2204.05813,"Nuclear magnetic relaxation is widely used to probe protein dynamics. For decades, most analyses of relaxation in proteins have relied successfully on the model-free approach, forgoing mechanistic descriptions of motions. Model-free types of correlation functions cannot describe a large carbon-13 relaxation dataset in protein sidechains. Here, we use molecular dynamics simulations to design explicit models of motion and solve Fokker-Planck diffusion equations. These models of motion provide better agreement with relaxation data, mechanistic insight and a direct link to configuration entropy. ",Explicit models of motions to understand protein side-chain dynamics
211,1514011117080784908,156804540,Francisco Rodrigues,"['Our new work on @arxiv: Forecasting new diseases in low-data settings using transfer learning. We find that transfer learning offers the potential to improve predictions, even beyond a model based on data from the target disease.\n<LINK> <LINK>']",https://arxiv.org/abs/2204.05059,"Recent infectious disease outbreaks, such as the COVID-19 pandemic and the Zika epidemic in Brazil, have demonstrated both the importance and difficulty of accurately forecasting novel infectious diseases. When new diseases first emerge, we have little knowledge of the transmission process, the level and duration of immunity to reinfection, or other parameters required to build realistic epidemiological models. Time series forecasts and machine learning, while less reliant on assumptions about the disease, require large amounts of data that are also not available in early stages of an outbreak. In this study, we examine how knowledge of related diseases can help make predictions of new diseases in data-scarce environments using transfer learning. We implement both an empirical and a theoretical approach. Using empirical data from Brazil, we compare how well different machine learning models transfer knowledge between two different disease pairs: (i) dengue and Zika, and (ii) influenza and COVID-19. In the theoretical analysis, we generate data using different transmission and recovery rates with an SIR compartmental model, and then compare the effectiveness of different transfer learning methods. We find that transfer learning offers the potential to improve predictions, even beyond a model based on data from the target disease, though the appropriate source disease must be chosen carefully. While imperfect, these models offer an additional input for decision makers during pandemic response. ",Forecasting new diseases in low-data settings using transfer learning
212,1514007913970421763,874897164,Michael Green,"['Ever play a level and felt ""tied up""? Like it was too punishing or maybe not punishing enough? Introducing PDSM: Persona-driven Dominant/Submissive Map Generation for Tutorials. We use quality-diversity methods to find #tutorial levels that suit personas\n<LINK> <LINK>', 'Tutorial levels are scenarios in which the player can explore and discover different rules and #game #mechanics. Procedural personas can guide generators to create content which encourages or discourages certain #playstyle behaviors.', ""In this system, we use procedural personas to calculate the behavioral characteristics of levels which are evolved using the quality-diversity algorithm known as Constrained MAP-Elites. An evolved map's quality is determined by its simplicity: the simpler it is, the better it is. https://t.co/Z84zknMnz1"", 'We measure #persona health for our behavioral characteristics. The maps are therefore measured by how high or low HP is for different personas. Some personas dominate certain maps but are submissive on others. https://t.co/KZ9XaKmWF2', 'We show that our generated maps can strongly encourage or discourage different persona-like behaviors and range from simple solutions to complex puzzle-levels, making them perfect candidates for a tutorial generative system. https://t.co/pH1s5JJfFr', 'Our intention is to incorporate this system into a persona-adaptive tutorial sequencer so that players may learn to adapt to different playstyles. I hope that one day this could be present in commercial, so that players could play #personalized levels just for them! https://t.co/kWUAM8uVJq', 'Big thanks to my collaborators @Amidos2006 from the University of Malta and @MasterMilkX and @togelius from the Game Innovation Lab at NYU for their help on this research!', '@Amidos2006 @MasterMilkX @togelius Heres a link to the paper if you want to check it out: https://t.co/gP13HY25Zx']",https://arxiv.org/abs/2204.05217,"In this paper, we present a method for automated persona-driven video game tutorial level generation. Tutorial levels are scenarios in which the player can explore and discover different rules and game mechanics. Procedural personas can guide generators to create content which encourages or discourages certain playstyle behaviors. In this system, we use procedural personas to calculate the behavioral characteristics of levels which are evolved using the quality-diversity algorithm known as Constrained MAP-Elites. An evolved map's quality is determined by its simplicity: the simpler it is, the better it is. Within this work, we show that the generated maps can strongly encourage or discourage different persona-like behaviors and range from simple solutions to complex puzzle-levels, making them perfect candidates for a tutorial generative system. ",Persona-driven Dominant/Submissive Map (PDSM) Generation for Tutorials
213,1513771265471172617,1352638067761311744,Joseph Imperial,"[""Say you're writing a story for a Grade 2 learner, how do you ensure that what you'll write next will still be readable by the student? Can you maintain the text's complexity throughout?\n\nWe propose the task of Uniform Complexity for Text Generation.\n\n<LINK>"", 'We investigate if humans and neural language models (GPT-2) trained to produce coherent stories can *maintain* the linguistic complexities of generated continuations with respect to the prompts. \n\nWe explored over 160 features. Turns out, both generally struggle in this task.', 'This project is supported by my @GoogleAI and @TensorFlow grant. Also big thanks to @Alexir563 for helping me with the GPT-2 models üíØ', '@mrinmayasachan @GoogleAI @TensorFlow @Alexir563 I was about to email you the link üòÖ']",https://arxiv.org/abs/2204.05185,"Powerful language models such as GPT-2 have shown promising results in tasks such as narrative generation which can be useful in an educational setup. These models, however, should be consistent with the linguistic properties of triggers used. For example, if the reading level of an input text prompt is appropriate for low-leveled learners (ex. A2 in the CEFR), then the generated continuation should also assume this particular level. Thus, we propose the task of uniform complexity for text generation which serves as a call to make existing language generators uniformly complex with respect to prompts used. Our study surveyed over 160 linguistic properties for evaluating text complexity and found out that both humans and GPT-2 models struggle in preserving the complexity of prompts in a narrative generation setting. ",Uniform Complexity for Text Generation
214,1513717911038406657,300302265,Abhishek Maniyar,"[""First attempt at a paper thread üßµNew paper on the Arxiv today!\n\nWe propose to use the 'Doppler boosted dust emission' in the Universe as a new cosmological probe: <LINK>"", 'If a dusty galaxy has a non-zero lineof-sight peculiar velocity, its emission is Doppler boosted (which we call DB-CIB). \n\nThe large-scale cosmic velocity field results in galaxy bulk motions, which in turn source the DB-CIB signal of ininterest', 'Therefore this effect is an independent probe of the cosmic velocity field which is an excellent tool to study the dark energy, modified gravity, cosmic growth rate of structure, primordial non-Gaussianity of local type (fNL), etc.', ""The kinematic Sunyaev-Zel'dovich effect (kSZ) is also a similar probe of the cosmic velocity field. However, kSZ suffers from the wellknown problem of ‚ÄòkSZ‚Äìoptical depth degeneracy‚Äô where the overall normalization of the electron profile in a halo is not known very well."", ""DB-CIB effect is very analogous to the kSZ effect. However, it is completely 'calibratable' and thus does not suffer from this 'optical depth degeneracy issue'. This makes it immune from the complex astrophysics of galaxy formation!"", 'Finally, we forecast that such a promising DB-CIB effect is detectable in the crosscorrelation of CCAT-Prime and DESI-like experiments. \n\nPretty cool that we can use the dust in the local Universe to probe the physics in the early Universe!', 'Had a lot of fun working on this with Simone Ferraro and Emmanuel Schaan at @NYUPhysics and @BerkeleyLab!']",https://arxiv.org/abs/2204.05299,"We identify a new cosmological signal, the Doppler-boosted Cosmic Infrared Background (DB-CIB), arising from the peculiar motion of the galaxies whose thermal dust emission source the cosmic infrared background (CIB). This new observable is an independent probe of the cosmic velocity field, highly analogous to the well-known kinematic Sunyaev-Zel'dovich (kSZ) effect. Interestingly, DB-CIB does not suffer from the 'kSZ optical depth degeneracy', making it immune from the complex astrophysics of galaxy formation. We forecast that the DB-CIB effect is detectable in the cross-correlation of CCAT-Prime and DESI-like experiments. We show that it also acts as a new CMB foreground which can bias future kSZ cross-correlations, if not properly accounted for. ","Doppler boosted dust emission and CIB-galaxy cross-correlations: a new
  probe of cosmology and astrophysics"
215,1513570484134236166,1292002106468048896,Vipul Gupta,"['Happy to announce our new CVPR paper - SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in VQA\n\nWe study the robustness of VQA models from a new perspective: visual context.\n\nPaper: <LINK>\n\n#CVPR2022 #ComputerVision \n\n1/5', 'Task : Given a model, try to measure the visual bias in the model based on its reliance on visual context i.e. irrelevant objects in the image, to make predictions. \nIdeally, changing irrelevant objects should not change model‚Äôs prediction.\n\n2/5 https://t.co/cmMjbdqxfu', 'We found that all types of VQA models uses visual context to answer questions. \nCan the model be improved? Yes, using SwapMix as a data augmentation strategy decreases the model‚Äôs reliance on visual context.\n\n3/5', 'We used the GQA dataset for our analysis. We also found that training VQA models with perfect embeddings (using scene graph embeddings) results in better or equivalent performance as compared to FasterRCNN embeddings. Moreover, these models rely less on visual context.\n\n4/5', 'Thanks to amazing collaborators  \n@zhuowanli @AdamKortylewski Chenyu @yingwei_li @YuilleAlan \nWe hope this new research direction to study reliance on visual context can serve as a new starting point to study VQA robustness and reliability.\n\n5/5']",https://arxiv.org/abs/2204.02285,"While Visual Question Answering (VQA) has progressed rapidly, previous works raise concerns about robustness of current VQA models. In this work, we study the robustness of VQA models from a novel perspective: visual context. We suggest that the models over-rely on the visual context, i.e., irrelevant objects in the image, to make predictions. To diagnose the model's reliance on visual context and measure their robustness, we propose a simple yet effective perturbation technique, SwapMix. SwapMix perturbs the visual context by swapping features of irrelevant context objects with features from other objects in the dataset. Using SwapMix we are able to change answers to more than 45 % of the questions for a representative VQA model. Additionally, we train the models with perfect sight and find that the context over-reliance highly depends on the quality of visual representations. In addition to diagnosing, SwapMix can also be applied as a data augmentation strategy during training in order to regularize the context over-reliance. By swapping the context object features, the model reliance on context can be suppressed effectively. Two representative VQA models are studied using SwapMix: a co-attention model MCAN and a large-scale pretrained model LXMERT. Our experiments on the popular GQA dataset show the effectiveness of SwapMix for both diagnosing model robustness and regularizing the over-reliance on visual context. The code for our method is available at this https URL ","SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context
  in Visual Question Answering"
216,1513450976883232773,699180629246672897,Subhrajit Roy,"['Can we move away from in-clinic towards at-home measurements for multiple sclerosis predictions? Our work investigates multiple different endpoints on 2 datasets - one from a clinical study, one from mobile devices. #CHIL2022\n\n<LINK>', '@d_mincu @negar_rz @kat_heller @JessicaSchrouff @weballergy.']",https://arxiv.org/abs/2204.03969,"Literature on machine learning for multiple sclerosis has primarily focused on the use of neuroimaging data such as magnetic resonance imaging and clinical laboratory tests for disease identification. However, studies have shown that these modalities are not consistent with disease activity such as symptoms or disease progression. Furthermore, the cost of collecting data from these modalities is high, leading to scarce evaluations. In this work, we used multi-dimensional, affordable, physical and smartphone-based performance outcome measures (POM) in conjunction with demographic data to predict multiple sclerosis disease progression. We performed a rigorous benchmarking exercise on two datasets and present results across 13 clinically actionable prediction endpoints and 6 machine learning models. To the best of our knowledge, our results are the first to show that it is possible to predict disease progression using POMs and demographic data in the context of both clinical trials and smartphone-base studies by using two datasets. Moreover, we investigate our models to understand the impact of different POMs and demographics on model performance through feature ablation studies. We also show that model performance is similar across different demographic subgroups (based on age and sex). To enable this work, we developed an end-to-end reusable pre-processing and machine learning framework which allows quicker experimentation over disparate MS datasets. ","Disability prediction in multiple sclerosis using performance outcome
  measures and demographic data"
217,1512463979427385349,1485396714454503424,Konstantin Gerbig,"['New paper drop!!! We propose a Hurricane-like mechanism to operate in protoplanetary disks. We find that even modestly under-saturated flows near the water ice line can be sufficient to drive long-lived vortices, warranting further investigation.\n<LINK>', 'Here is a cool figure from our paper. It shows the evolution of the vortex energy for several simulation setups. The vortices that incorporate the Hurricane-mechanism strengthen, grow and ultimately merge, as seen in the top panels. https://t.co/p02nDfrHmu', 'Terrestrial Hurricanes have quite complex flow patterns, and so do our space-analogues. In particular, the shear caused by Keplerian rotation naturally favors vortices to that rotate clockwise in a counter-clockwise rotating disk. Here, a pic of the flow, including isobars. https://t.co/6ovUNNmTYO', 'Thus, disk hurricanes are not cyclones like their  terrestrial counterpart, but anticyclones. However, they are still powered by the same process, that is transfer of water latent heat via mixing with an external medium (ocean and icy-dust for Earth and disk respectively).', 'In a steady-state, this latent-heat induced heating is exactly balance by friction, in form of mainly viscous dissipation and radiative losses. Key parameter is the under-saturation ""Delta q"" which is necessary to induce water sublimation and thus vortex energy increase. https://t.co/X3rf4QC5Vn', 'Our work included proof-of-concept 2d simulations. In the future, we want to model the 3d structure of the vortex which would include vertical convection and thus would allow for a transverse circulation as seen in terrestrial Hurricanes. https://t.co/EZp40pOQmw', 'Our conclusion is, that the Hurricane-mechanism may indeed spring into operation in protoplanetary disks if the flow just outside the water ice-line is sufficiently under-saturated. The existence of this mechanism would have interesting implications for planet formation.']",https://arxiv.org/abs/2204.03007,"When ice on the surface of dust grains in protoplanetary disk sublimates, it adds its latent heat of water sublimation to the surrounding flow. Drawing on the analogy provided by tropical cyclones on Earth, we investigate if this energy source is sufficient to sustain or magnify anticyclonic disk vortices that would otherwise fall victim to viscous dissipation. An analytical treatment, supported by exploratory two-dimensional simulations, suggests that even modestly under-saturated flows can extend the lifetime of vortices, potentially to a degree sufficient to aid particle trapping and planetesimal formation. We expect the best conditions for this mechanism to occur will be found near the disk's water ice line if turbulent motions displace gas parcels out of thermodynamic equilibrium with the dust mid-plane. ",The Prospects for Hurricane-like Vortices in Protoplanetary Disks
218,1512367505096032263,1508578910417661957,Andrea Pizzi,"['Our new pre-print: <LINK>! We find classical analogues to distinctive features of the entanglement entropy in many-body quantum dynamics, e.g., linear growth and measurement induced transitions. The key? Dealing with exponentially large probability distributions.']",https://arxiv.org/abs/2204.03016,"The fundamental question of how information spreads in closed quantum many-body systems is often addressed through the lens of the bipartite entanglement entropy, a quantity that describes correlations in a comprehensive (nonlocal) way. Among the most striking features of the entanglement entropy are its unbounded linear growth in the thermodynamic limit, its asymptotic extensivity in finite-size systems, and the possibility of measurement-induced phase transitions, all of which have no obvious classical counterpart. Here, we show how these key qualitative features emerge naturally also in classical information spreading, as long as one treats the classical many-body problem on par with the quantum one, that is, by explicitly accounting for the exponentially large classical probability distribution. Our analysis is supported by extensive numerics on prototypical cellular automata and Hamiltonian systems, for which we focus on the classical mutual information and also introduce a `classical entanglement entropy'. Our study sheds light on the nature of information spreading in classical and quantum systems, and opens new avenues for quantum-inspired classical approaches across physics, information theory, and statistics. ","Bridging the gap between classical and quantum many-body information
  dynamics"
219,1512131361682907140,842208793463201792,Wei-Hong Li,"[""We've released a preprint of our work where we propose a unified look at jointly learning multiple vision tasks and visual domains (many-shot and few-shot learning) through universal representations, a single deep neural network. See <LINK>"", 'In the paper, we propose a Universal Representation Learning framework to learn a single universal network for multiple tasks/domains by aligning representations of a single universal network and single-task/domain networks through small capacity task/domain-specific adapters.', 'We show that our method generalizes over multi-task dense prediction tasks, multi-domain many-shot learning, cross-domain few-shot learning. The code will be available at https://t.co/xMp7YAQR2T']",https://arxiv.org/abs/2204.02744,"We propose a unified look at jointly learning multiple vision tasks and visual domains through universal representations, a single deep neural network. Learning multiple problems simultaneously involves minimizing a weighted sum of multiple loss functions with different magnitudes and characteristics and thus results in unbalanced state of one loss dominating the optimization and poor results compared to learning a separate model for each problem. To this end, we propose distilling knowledge of multiple task/domain-specific networks into a single deep neural network after aligning its representations with the task/domain-specific ones through small capacity adapters. We rigorously show that universal representations achieve state-of-the-art performances in learning of multiple dense prediction problems in NYU-v2 and Cityscapes, multiple image classification problems from diverse domains in Visual Decathlon Dataset and cross-domain few-shot learning in MetaDataset. Finally we also conduct multiple analysis through ablation and qualitative studies. ","Universal Representations: A Unified Look at Multiple Task and Domain
  Learning"
220,1512029224571445256,1669743397,Michael Alexander Riegler,['Are visual explanations of #AI algorithms helpful for #medical doctors? Check our preprint where we conducted a study involving a large number of #gastroenterologists <LINK> @Strumke @sravmd @ThomasdeLange1 @AndreaStoras @vlbthambawita @simula_research'],https://arxiv.org/abs/2204.00617,"Deep learning has in recent years achieved immense success in all areas of computer vision and has the potential of assisting medical doctors in analyzing visual content for disease and other abnormalities. However, the current state of deep learning is very much a black box, making medical professionals highly skeptical about integrating these methods into clinical practice. Several methods have been proposed in order to shine some light onto these black boxes, but there is no consensus on the opinion of the medical doctors that will consume these explanations. This paper presents a study asking medical doctors about their opinion of current state-of-the-art explainable artificial intelligence methods when applied to a gastrointestinal disease detection use case. We compare two different categories of explanation methods, intrinsic and extrinsic, and gauge their opinion of the current value of these explanations. The results indicate that intrinsic explanations are preferred and that explanation. ","Visual explanations for polyp detection: How medical doctors assess
  intrinsic versus extrinsic explanations"
221,1511991130472210432,717772944,Bernd Riederer,"['üö®Publication Dayüö® @paddyjenny @axelmaas\n\nToday our recent work on vector boson scattering in the Higgs-channel hit the arXiv.\nWe study this process in a fully gauge-invariant way, combining nonperturbative and perturbative approaches.\nWhat did we find?\n\n<LINK>', 'To start out, a quick motivation why it is interesting to study vector boson scattering (VBS).\nVBS plays a central role in the search for new physics at collider experiments.\nTherefore understanding this process in full detail is key to correctly identify SM and BSM-physics.', 'And (IMHO) to fully understand a physical process it needs to be looked at from various perspectives: in this case perturbation theory and also nonperturbative methods (i.e. lattice simulations).\n\nHowever, (sadly) it is not yet possible to simulate the full standard model (SM).', 'This restricts us to study VBS only in a reduced SM setup. However, there is one key feature about electroweak physics that makes EW-processes more intricate than it seems at first look: namely the Brout-Englert-Higgs effect.\n\nThe issue with gauge+higgs theories is quite subtle.', 'Some of you may have read this already but I will be repeating it again. When performing perturbative calculations for gauge+higgs theories it is quite common to use the elementary fields as the degrees of freedom (DOF). These are however gauge-dependent and thus unphysical.', 'The way to go is to use gauge-invariant, composite objects instead as DOFs, e.g. as in- and out-states for scattering. (see https://t.co/FN48LejEZD)\n\nWhile lattice automatically circumvents this problem, perturbation theory needs to be augmented to deal with composite objects.', 'In the paper we used exactly this augmented version of perturbation theory, which allows us to study VBS in a fully gauge-invariant way. These results have then been combined with/compared to lattice results and we observed some quite interesting things:', '1. Particle spectra stays unchanged\n\nAgreeing with previous investigations we again observed, that lattice simulations do indeed recover the same particle spectrum as usual PT. This is especially convincing since it agrees with predictions from augmented PT and experiment.', '2. Indication of bound state structure for the scalar degree of freedom\n\nAs discussed above, gauge invariance requires the physical DOF to be a composite objects. In our lattice simulations we obtain the scattering length of VBS and observe for some setups a negative value.', 'This is indicative of a bound state like structure of the scalar DOF.\nFurther we obtained the (differential) cross sections from the lattice data. Also this deviates from the augmented tree-level PT predictions in a way one would expect it for the scattering of composite objects.', '3. Possible resonance in VBS\n\nFinally, we were also able to use the perturbative prediction as a tool for finding resonances in the scattering process. For some simulations we did not find a stable particle in the spectrum below the elastic threshold.', 'However, it turned out that it is possible to use the perturbative prediction to fit our data and find a resonance mass in the elastic region. A usual Breit-Wigner fit failed to achieve this.', ""4. Improved lattice technicalities\n\nLast but not least I want to mention the several technical difficulties we had to face during our analysis (especially very poor signal-to-noise ratios). It has been two years since my master's thesis now, where we started this work."", ""Special thanks here to @paddyjenny who worked on a technique to obtain the energy spectra from the lattice data to improve our results.\n\nI hope you've read this thread and the paper until the very end and enjoyed it. üòâ\nThanks to everyone and I am looking forward to comments."", 'Also tagging @SimonPlaetzer here to thank him for his comments on the manuscript and in general valuable discussions during our work on this paper üòä']",https://arxiv.org/abs/2204.02756,"We study vector-boson scattering of the physical, gauge-invariant states in a reduced standard-model setup on the lattice for various parameter sets. To this end, the phase shift in the scalar channel is determined using a L\""uscher-type analysis. The results can be readily interpreted in terms of the Higgs properties and a reunitarized Fr\""ohlich-Morchio-Strocchi analysis at Born level. The only deviation appears for a Higgs mass below the elastic threshold, where we find a negative scattering length indicative of the bound-state nature of the physical scalar degree of freedom. We assess the possible implications for an experimental detection of the effect. ",Vector boson scattering from the lattice
222,1511930133921386498,826482000597024770,Deividas Sabonis,['New preprint out today on @arxiv: \n<LINK>\n\nWe use local and nonlocal conductance spectroscopy to study charge character of Andreev bound states realised in gate defined InAs/Al heterostructure.'],https://arxiv.org/abs/2204.02430,"The charge character of Andreev bound states (ABSs) in a three-terminal semiconductor-superconductor hybrid nanowire was measured using local and nonlocal tunneling spectroscopy. The device is fabricated using an epitaxial InAs/Al two-dimensional heterostructure with several gate-defined side probes. ABSs are found to oscillate around zero as a function of gate voltage, with modifications of their charge consistent with theoretical expectations for the total Bardeen-Cooper- Schrieffer (BCS) charge of ABSs. ","Nonlocal conductance spectroscopy of Andreev bound states in
  gate-defined InAs/Al nanowires"
223,1511833880579514369,794462632908558336,Javier Fern√°ndez,"['We are excited to share our most recent work, where we propose a rich tactical framework for assessing the real impact of physical effort in soccer.\n\n<LINK> <LINK>', 'More than a decade after the introduction of GPS/EPTS devices in professional sports, the use of physical data is still primarily limited to producing simple aggregated metrics lacking tactical context.\n\nThis has led many coaches to believe that physical metrics are useless.', ""By contextualizing runs according to the players' location and the phase of the game we are able to:\n\n- assess the impact of runs on goal probability\n- predict a player's movements relative to the opponent\n- Identify smart and irrelevant runs\n- create a player's physical profile https://t.co/jYSpyqrR6e"", 'We wanted to publish this work ages ago since we believe that better integration between Sports Analytics and Sports Science will lead to a deeper understanding of the sport.\n\nFor more detailed information check this great thread from @SergioMinuto90 \n\nhttps://t.co/gYukIEu4B1']",https://arxiv.org/abs/2204.02313,"We present a framework that gives a deep insight into the link between physical and technical-tactical aspects of soccer and it allows associating physical performance with value generation thanks to a top-down approach. First, we estimate physical indicators from tracking data. Then, we contextualize each player's run to understand better the purpose and circumstances in which it is done, adding a new dimension to the creation of team and player profiles. Finally, we assess the value-added by off-ball high-intensity runs by linking with a possession-value model. This novel approach allows answering practical questions from very different profiles of practitioners within a soccer club, from analysts, coaches, and scouts to physical coaches and readaptation physiotherapists. ","Is it worth the effort? Understanding and contextualizing physical
  metrics in soccer"
224,1511624651126067202,890517217850294273,Pepa Atanasova,"['üìú Happy to share that our work on #factchecking with insufficient evidence has been accepted to TACL! We propose a new diagnostic dataset, SufficientFacts, and a novel data augmentation strategy for contrastive self-learning of missing evidence.\n<LINK> #NLProc <LINK>', 'Work with my amazing co-authors @IAugenstein, Jakob, and Christina.']",https://arxiv.org/abs/2204.02007,"Automating the fact checking (FC) process relies on information obtained from external sources. In this work, we posit that it is crucial for FC models to make veracity predictions only when there is sufficient evidence and otherwise indicate when it is not enough. To this end, we are the first to study what information FC models consider sufficient by introducing a novel task and advancing it with three main contributions. First, we conduct an in-depth empirical analysis of the task with a new fluency-preserving method for omitting information from the evidence at the constituent and sentence level. We identify when models consider the remaining evidence (in)sufficient for FC, based on three trained models with different Transformer architectures and three FC datasets. Second, we ask annotators whether the omitted evidence was important for FC, resulting in a novel diagnostic dataset, SufficientFacts, for FC with omitted evidence. We find that models are least successful in detecting missing evidence when adverbial modifiers are omitted (21% accuracy), whereas it is easiest for omitted date modifiers (63% accuracy). Finally, we propose a novel data augmentation strategy for contrastive self-learning of missing evidence by employing the proposed omission method combined with tri-training. It improves performance for Evidence Sufficiency Prediction by up to 17.8 F1 score, which in turn improves FC performance by up to 2.6 F1 score. ",Fact Checking with Insufficient Evidence
225,1511200169848582153,1112727918,Neco Kriel,"['üéâ My first first-author study hits the arXiv today üéâ\n<LINK>\n\nIn this study we measure the scale where turbulent dynamo amplified magnetic fields become most concentrated (peak magnetic field) in direct numerical simulations of MHD turbulence.\n\nA quick thread /5', 'Big thanks to: @astro_magnetism (James Beattie), @amitseta90 (Amit Seta), and Christoph Federrath!']",https://arxiv.org/abs/2204.00828,"The turbulent dynamo is a powerful mechanism that converts turbulent kinetic energy to magnetic energy. A key question regarding the magnetic field amplification by turbulence, is, on what scale, $k_{\rm p}$, do magnetic fields become most concentrated? There has been some disagreement about whether $k_{\rm p}$ is controlled by the viscous scale, $k_\nu$ (where turbulent kinetic energy dissipates), or the resistive scale, $k_\eta$ (where magnetic fields dissipate). Here we use direct numerical simulations of magnetohydrodynamic turbulence to measure characteristic scales in the kinematic phase of the turbulent dynamo. We run $104$-simulations with hydrodynamic Reynolds numbers of $10 \leq {\rm Re} \leq 3600$, and magnetic Reynolds numbers of $270 \leq {\rm Rm} \leq 4000$, to explore the dependence of $k_{\rm p}$ on $k_\nu$ and $k_\eta$. Using physically motivated models for the kinetic and magnetic energy spectra, we measure $k_\nu$, $k_\eta$ and $k_{\rm p}$, making sure that the obtained scales are numerically converged. We determine the overall dissipation scale relations $k_\nu = (0.025^{+0.005}_{-0.006})\, k_{\rm turb}\, {\rm Re}^{3/4}$ and $k_\eta = (0.88^{+0.21}_{-0.23})\, k_\nu\, {\rm Pm}^{1/2}$, where $k_{\rm turb}$ is the turbulence driving wavenumber and ${\rm Pm}={\rm Rm}/{\rm Re}$ is the magnetic Prandtl number. We demonstrate that the principle dependence of $k_{\rm p}$ is on $k_\eta$. For plasmas where ${\rm Re} \gtrsim 100$, we find that $k_{\rm p} = (1.2_{-0.2}^{+0.2})\, k_\eta$, with the proportionality constant related to the power-law `Kazantsev' exponent of the magnetic power spectrum. Throughout this study, we find a dichotomy in the fundamental properties of the dynamo where ${\rm Re} > 100$, compared to ${\rm Re} < 100$. We report a minimum critical hydrodynamic Reynolds number, ${\rm Re}_{\rm crit} = 100$ for bonafide turbulent dynamo action. ",Fundamental scales in the kinematic phase of the turbulent dynamo
226,1511156871125557250,1549244394,Sreejan Kumar,"['<LINK> Excited to share a new preprint! In this work, we present ""task metamers"" as a way to study the difference between human and neural network behavior in the context of meta-reinforcement learning of task distributions with underlying abstract structure. <LINK>']",https://arxiv.org/abs/2204.01437,"The ability to acquire abstract knowledge is a hallmark of human intelligence and is believed by many to be one of the core differences between humans and neural network models. Agents can be endowed with an inductive bias towards abstraction through meta-learning, where they are trained on a distribution of tasks that share some abstract structure that can be learned and applied. However, because neural networks are hard to interpret, it can be difficult to tell whether agents have learned the underlying abstraction, or alternatively statistical patterns that are characteristic of that abstraction. In this work, we compare the performance of humans and agents in a meta-reinforcement learning paradigm in which tasks are generated from abstract rules. We define a novel methodology for building ""task metamers"" that closely match the statistics of the abstract tasks but use a different underlying generative process, and evaluate performance on both abstract and metamer tasks. In our first set of experiments, we found that humans perform better at abstract tasks than metamer tasks whereas a widely-used meta-reinforcement learning agent performs worse on the abstract tasks than the matched metamers. In a second set of experiments, we base the tasks on abstractions derived directly from empirically identified human priors. We utilize the same procedure to generate corresponding metamer tasks, and see the same double dissociation between humans and agents. This work provides a foundation for characterizing differences between humans and machine learning that can be used in future work towards developing machines with human-like behavior. ","Disentangling Abstraction from Statistical Pattern Matching in Human and
  Machine Learning"
227,1511133788197191680,1261022659325898752,Zoe de Beurs (she/hers),"['Excited to share our newly accepted paper on using machine learning to classify X-ray binaries (XRBs)! We train 3 ML methods and find that they are highly accurate for most sources, paving the way towards more robust XRB classification. (Image:Chandra) <LINK> 1/4 <LINK>', 'XRBs consist of a compact object, such as a black hole, neutron star, or pulsar, accreting matter from a companion star. New XRBs are constantly being discovered, but current methods to determine the type of compact object are limited to bright objects. (Image: ZL de Beurs) 2/4 https://t.co/7c3pXzPaED', 'In our paper, we compare three ML methods (Bayesian Gaussian Processes, K-Nearest Neighbors, and Support Vector Machines). We train our methods to use spatial patterns in 3D Color-Color-Intensity diagrams so that new, previously unseen XRB sources can then be classified. 3/4 https://t.co/v2OPOfXjBS', 'All three methods achieve high accuracy, and provide a probability distribution for each source to help quantify uncertainty. We hope that this work will lead to more exploration of ML methods for efficient and reliable XRB classification! 4/4 https://t.co/P7orS6Ykuj']",http://arxiv.org/abs/2204.00346,"X-ray Binaries (XRBs) consist of a compact object that accretes material from an orbiting secondary star. The most secure method we have for determining if the compact object is a black hole is to determine its mass: this is limited to bright objects, and requires substantial time-intensive spectroscopic monitoring. With new X-ray sources being discovered with different X-ray observatories, developing efficient, robust means to classify compact objects becomes increasingly important. We compare three machine learning classification methods (Bayesian Gaussian Processes (BGP), K-Nearest Neighbors (KNN), Support Vector Machines (SVM)) for determining the compact objects as neutron stars or black holes (BHs) in XRB systems. Each machine learning method uses spatial patterns which exist between systems of the same type in 3D Color-Color-Intensity diagrams. We used lightcurves extracted using six years of data with MAXI/GSC for 44 representative sources. We find that all three methods are highly accurate in distinguishing pulsing from non-pulsing neutron stars (NPNS) with 95\% of NPNS and 100\% of pulsars accurately predicted. All three methods have high accuracy distinguishing BHs from pulsars (92\%) but continue to confuse BHs with a subclass of NPNS, called the Bursters, with KNN doing the best at only 50\% accuracy for predicting BHs. The precision of all three methods is high, providing equivalent results over 5-10 independent runs. In a future work, we suggest a fourth dimension be incorporated to mitigate the confusion of BHs with Bursters. This work paves the way towards more robust methods to efficiently distinguish BHs, NPNS, and pulsars. ","A Comparative Study of Machine Learning Methods for X-ray Binary
  Classification"
228,1510929114986909696,261865146,Dr Sofia Qvarfort,['New article on the @arxiv! We show how time-dependent modulations of the trapping potential can lead to cooling and propose a measurement protocol for cooling down arbitrary quantum states to near the ground-state:\n<LINK>\nGreat collab with Sreenath Manikandan! <LINK>'],https://arxiv.org/abs/2204.00476,We propose a cooling protocol that uses phase-preserving quantum measurements and phase-dependent modulations of the trapping potential at parametric resonance to cool a quantum oscillator to near its quantum-mechanical ground-state. The sequential measurements and feedback provide a definite phase reference and stabilize the oscillator in the long-time limit. The protocol is robust against moderate amounts of dissipation and phase errors in the feedback loop. Our work has implications for the cooling of mechanical resonators and the integration of quantum refrigerators into quantum circuits. ,"Cooling through parametric modulations and phase-preserving quantum
  measurements"
229,1510812130433724418,836417100864344064,Takuma Udagawa,"['Does large-scale language model (LLM) rescoring help near state-of-the-art ASR systems? We study the effect of LLM rescoring on a competitive Conformer-Transducer baseline and conducted some informative analyses. Paper submitted to Interspeech 2022!\n\n<LINK>', 'Happy to share my first paper at IBM Research, and thank you to my coauthors @szuk_m and @gakuto_kurata!']",https://arxiv.org/abs/2204.00212,"Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been successfully applied to ASR N-best rescoring. However, whether or how they can benefit competitive, near state-of-the-art ASR systems remains unexplored. In this study, we incorporate LLM rescoring into one of the most competitive ASR baselines: the Conformer-Transducer model. We demonstrate that consistent improvement is achieved by the LLM's bidirectionality, pretraining, in-domain finetuning and context augmentation. Furthermore, our lexical analysis sheds light on how each of these components may be contributing to the ASR performance. ","Effect and Analysis of Large-scale Language Model Rescoring on
  Competitive ASR Systems"
230,1510781069423001608,949703828064129024,Mario Krenn,"['Very happy to announce:\nSELFIES and the future of molecular string representations\n<LINK>\n\nwith 31 co-authors from 10 different countries (4 continents)!\n\nWe look into the future of #MachineLearning in chemistry &amp; propose 16 exciting future research ideas.\n\nüßµ1/n <LINK>', 'We discuss potentials for new robust representations of macromolecules, crystals, inorganic chemistry or reactions for generative models\n\nWe compare three different graph representations strings, adjacency matrices and images for ML (comparing RNNs, GNNs, CNNs) for molecules\n\n2/n https://t.co/dAmR2V8oqJ', 'We ask about robust programming languages &amp; about interpretability of representations for humans and machines üß†ü§ñ\n\nWe raise 16 new research ideas that could lead to new stand-alone science projects. From very chemistry-based to very #ArtificialIntelligence based!\n\n3/n https://t.co/qtZA1o6aR0', 'Together with the amazing team: @QaiAlex @senja_barthel @SuperScienceGrl @Angelo_Frei @nc_frey @P_Friederich @TheophileGaudin Alberto Alexander Gayle, @kmjablonka, Rafael F. Lameiro, @Dom1Lemm @alston1o @SMohMoosavi, Jose Manuel Napoles-Duarte, @akshat_ai @robpollice ü§òüöÄ\n\n4/n', '... @KohulanRajan @BioInorgChem_UW @pschwllr @martoskreto @SmitBerend @felix_s_k @ChongSun20, Gary Tom, @ferchault Andrew Wang, @andrewwhite01 @AdamoYoung @yuqirose and @A_Aspuru_Guzik ü§ò‚ö°Ô∏è (here is a map of the origins of our authors)\n\n5/n https://t.co/T4snTKTYtc', 'The paper was initiated during a very focused online mini-workshop, organized by @IOPPublishing and @acceleration_c THANK YOU!\n\nBefore the WS, we prepared numerous concrete questions that participants shall discuss in breakout rooms. After a quick summary, we switched...\n\n6/n', '... to @discord - which worked perfectly. A HUUGE thanks to all participants for the very professional and entertaining collaboration. So many people from so many background made this possible. I am still amazed that this experiment worked so flawlessly üòÉ\n\n7/n', 'Of course, if you have comments or suggestions, please let us know!\n\n8/n, n=8.']",https://arxiv.org/abs/2204.00056,"Artificial intelligence (AI) and machine learning (ML) are expanding in popularity for broad applications to challenging tasks in chemistry and materials science. Examples include the prediction of properties, the discovery of new reaction pathways, or the design of new molecules. The machine needs to read and write fluently in a chemical language for each of these tasks. Strings are a common tool to represent molecular graphs, and the most popular molecular string representation, SMILES, has powered cheminformatics since the late 1980s. However, in the context of AI and ML in chemistry, SMILES has several shortcomings -- most pertinently, most combinations of symbols lead to invalid results with no valid chemical interpretation. To overcome this issue, a new language for molecules was introduced in 2020 that guarantees 100\% robustness: SELFIES (SELF-referencIng Embedded Strings). SELFIES has since simplified and enabled numerous new applications in chemistry. In this manuscript, we look to the future and discuss molecular string representations, along with their respective opportunities and challenges. We propose 16 concrete Future Projects for robust molecular representations. These involve the extension toward new chemical domains, exciting questions at the interface of AI and robust languages and interpretability for both humans and machines. We hope that these proposals will inspire several follow-up works exploiting the full potential of molecular string representations for the future of AI in chemistry and materials science. ",SELFIES and the future of molecular string representations
231,1520739744795271169,1456487114,Weisi Guo,"['V. proud of this paper (credit to Cian) even tho not found a home yet: <LINK>\nWe show geographic diversity benefits research up to a point: 1. Well-trodden routes benefit far less, 2. Too far away is problematic, 3. Some sectors (e.g., medicine) benefit more! <LINK>']",https://arxiv.org/abs/2204.11713,"Diversity in human capital is widely seen as critical to creating holistic and high quality research, especially in areas that engage with diverse cultures, environments, and challenges. Quantifying diverse academic collaborations and its effect on research quality is lacking, especially at international scale and across different domains. Here, we present the first effort to measure the impact of geographic diversity in coauthorships on the citation of their papers across different academic domains. Our results unequivocally show that geographic coauthor diversity improves paper citation, but very long distance collaborations has variable impact. We also discover ""well-trodden"" collaboration circles that yield much less impact than similar travel distances. These relationships are observed to exist across different subject areas, but with varying strengths. These findings can help academics identify new opportunities from a diversity perspective, as well as inform funders on areas that require additional mobility support. ",Impact of Geographic Diversity on Citation of Collaborative Research
