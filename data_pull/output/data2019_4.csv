,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1127870799166234624,2676457430,MAGIC telescopes 🌴🌺,"['A new MAGIC paper has been accepted on MNRAS! Check out our new study on the extragalactic background light, including data from the most distant very-high-energy sources up to redshift 1!!!\n<LINK>\n#theMAGICtelescopes #MAGICpapers']",https://arxiv.org/abs/1904.00134v1,"We present a measurement of the extragalactic background light (EBL) based on a joint likelihood analysis of 32 gamma-ray spectra for 12 blazars in the redshift range z = 0.03 to 0.944, obtained by the MAGIC telescopes and Fermi-LAT. The EBL is the part of the diffuse extragalactic radiation spanning the ultraviolet, visible and infrared bands. Major contributors to the EBL are the light emitted by stars through the history of the universe, and the fraction of it which was absorbed by dust in galaxies and re-emitted at longer wavelengths. The EBL can be studied indirectly through its effect on very-high energy photons that are emitted by cosmic sources and absorbed via photon-photon interactions during their propagation across cosmological distances. We obtain estimates of the EBL density in good agreement with state-of-the-art models of the EBL production and evolution. The 1-sigma upper bounds, including systematic uncertainties, are between 13% and 23% above the nominal EBL density in the models. No anomaly in the expected transparency of the universe to gamma rays is observed in any range of optical depth.We also perform a wavelength-resolved EBL determination, which results in a hint of an excess of EBL in the 0.18 - 0.62 $\mu$m range relative to the studied models, yet compatible with them within systematics. ","] Measurement of the Extragalactic Background Light using MAGIC and
  Fermi-LAT gamma-ray observations of blazars up to z = 1"
1,1126783544548888576,2773113120,Tom Rivlin,"['🚨🚨🚨NEW PAPER🚨🚨🚨\n\nOur newest paper has finally been published! It has the very sexy title of ""Low temperature scattering with the R-matrix method: argon-argon scattering""\n\nCheck it out here: \n<LINK>\n\nAnd get it on arXiv here:\n<LINK>']",https://arxiv.org/abs/1904.11964,"Results for elastic atom-atom scattering are obtained as a first practical application of RmatReact, a new code for generating high-accuracy scattering observables from potential energy curves. RmatReact has been created in response to new experimental methods which have paved the way for the routine production of ultracold atoms and molecules, and hence the experimental study of chemical reactions involving only a small number of partial waves. Elastic scattering between argon atoms is studied here. There is an unresolved discrepancy between different argon-argon potential energy curves which give different numbers of vibrational bound states and different scattering lengths for the argon-argon dimer. Depending on the number of bound states, the scattering length is either large and positive or large and negative. Scattering observables, specifically the scattering length, effective range, and partial and total cross-sections, are computed at low collision energies and compared to previous results. In general, good agreement is obtained, although our full scattering treatment yields resonances which are slightly lower in energy and narrower than previous determinations using the same potential energy curve. ","Low temperature scattering with the R-matrix method: argon-argon
  scattering"
2,1126557712748302336,765802241576042496,Eric Arazo,"['Our upcoming #ICML2019 paper ""Unsupervised Label Noise Modeling and Loss Correction"" is now on arXiv <LINK>. New SoTA in CIFAR10/100 under high-levels of noise using Beta mixtures to model &amp; correct losses. Code: <LINK> @oconnorn @kevinmcguinness']",https://arxiv.org/abs/1904.11238,"Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at this https URL ",Unsupervised Label Noise Modeling and Loss Correction
3,1126525949711998976,19772318,Kevin McGuinness,"['Our upcoming #ICML2019 paper ""Unsupervised Label Noise Modeling and Loss Correction"" is now on arXiv <LINK>. New SoTA in CIFAR10/100 under high-levels of label noise using Beta mixtures to model &amp; correct losses. Code: <LINK> @oconnorn @ArazoEric']",https://arxiv.org/abs/1904.11238,"Despite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-component mixture model as an unsupervised generative model of sample loss values during training to allow online estimation of the probability that a sample is mislabelled. Specifically, we propose a beta mixture to estimate this probability and correct the loss by relying on the network prediction (the so-called bootstrapping loss). We further adapt mixup augmentation to drive our approach a step further. Experiments on CIFAR-10/100 and TinyImageNet demonstrate a robustness to label noise that substantially outperforms recent state-of-the-art. Source code is available at this https URL ",Unsupervised Label Noise Modeling and Loss Correction
4,1126520006580363264,31049199,Drew Dimmery,"['New paper (accepted at KDD)! With Jas Sekhon and @eytan we show how the analysis of experiments may be improved by using a very simple shrinkage estimator. <LINK>', ""In particular, we show that James-Stein estimators are effective in the context of experimentation with multiple treatment groups, like many of the experiments we run at Facebook (students of statistics won't be surprised by this!)."", 'Most interestingly, we find that shrinkage improves decision making in sequential experimentation (we focus on the Thompson sampling algorithm).', 'It does this by concentrating exploration on the _set_ of near-optimal arms at any given point in time. In turn, this makes it more effective at finding the best arm later in the experiment.', 'This is our default way of running batch-based bandit optimization in Ax (https://t.co/K4UTsp452o).']",https://arxiv.org/abs/1904.12918,"We develop and analyze empirical Bayes Stein-type estimators for use in the estimation of causal effects in large-scale online experiments. While online experiments are generally thought to be distinguished by their large sample size, we focus on the multiplicity of treatment groups. The typical analysis practice is to use simple differences-in-means (perhaps with covariate adjustment) as if all treatment arms were independent. In this work we develop consistent, small bias, shrinkage estimators for this setting. In addition to achieving lower mean squared error these estimators retain important frequentist properties such as coverage under most reasonable scenarios. Modern sequential methods of experimentation and optimization such as multi-armed bandit optimization (where treatment allocations adapt over time to prior responses) benefit from the use of our shrinkage estimators. Exploration under empirical Bayes focuses more efficiently on near-optimal arms, improving the resulting decisions made under uncertainty. We demonstrate these properties by examining seventeen large-scale experiments conducted on Facebook from April to June 2017. ",Shrinkage Estimators in Online Experiments
5,1126350351551139846,559014573,John Collomosse,"['New @CVPR2019 paper with @AdobeResearch - LiveSketch, solving drawing ambiguity for Sketch based Visual Search over web scale image collections. Building on #SketchRNN. Preprint on arXiv <LINK>  <LINK> @cvssp_research @UniOfSurrey']",http://arxiv.org/abs/1904.06611,"LiveSketch is a novel algorithm for searching large image collections using hand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch search by creating visual suggestions that augment the query as it is drawn, making query specification an iterative rather than one-shot process that helps disambiguate users' search intent. Our technical contributions are: a triplet convnet architecture that incorporates an RNN based variational autoencoder to search for images using vector (stroke-based) queries; real-time clustering to identify likely search intents (and so, targets within the search embedding); and the use of backpropagation from those targets to perturb the input stroke sequence, so suggesting alterations to the query in order to guide the search. We show improvements in accuracy and time-to-task over contemporary baselines using a 67M image corpus. ",LiveSketch: Query Perturbations for Guided Sketch-based Visual Search
6,1125847973211049985,39107965,kfir bar,"['""Semantic Characteristics of Schizophrenic Speech"", our new paper for CLPsych @naacl. We try to automatically detect thought disorder in Hebrew speech <LINK>', '@boknilev @naacl Thanks! Yes we are thinking about this, it seems like there are some basic acoustic characteristics to start with.']",https://arxiv.org/abs/1904.07953,Natural language processing tools are used to automatically detect disturbances in transcribed speech of schizophrenia inpatients who speak Hebrew. We measure topic mutation over time and show that controls maintain more cohesive speech than inpatients. We also examine differences in how inpatients and controls use adjectives and adverbs to describe content words and show that the ones used by controls are more common than the those of inpatients. We provide experimental results and show their potential for automatically detecting schizophrenia in patients by means only of their speech patterns. ,Semantic Characteristics of Schizophrenic Speech
7,1125554431473250304,1115880604560691200,NII Yamagishi Lab,"['New preprint on arXiv: “Neural source-filter waveform models for statistical parametric speech synthesis”\nPaper: <LINK>\nSamples: <LINK>\nSubmitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing.']",https://arxiv.org/abs/1904.12088,"Neural waveform models such as WaveNet have demonstrated better performance than conventional vocoders for statistical parametric speech synthesis. As an autoregressive (AR) model, WaveNet is limited by a slow sequential waveform generation process. Some new models that use the inverse-autoregressive flow (IAF) can generate a whole waveform in a one-shot manner. However, these IAF-based models require sequential transformation during training, which severely slows down the training speed. Other models such as Parallel WaveNet and ClariNet bring together the benefits of AR and IAF-based models and train an IAF model by transferring the knowledge from a pre-trained AR teacher to an IAF student without any sequential transformation. However, both models require additional training criteria, and their implementation is prohibitively complicated. We propose a framework for neural source-filter (NSF) waveform modeling without AR nor IAF-based approaches. This framework requires only three components for waveform generation: a source module that generates a sine-based signal as excitation, a non-AR dilated-convolution-based filter module that transforms the excitation into a waveform, and a conditional module that pre-processes the acoustic features for the source and filer modules. This framework minimizes spectral-amplitude distances for model training, which can be efficiently implemented by using short-time Fourier transform routines. Under this framework, we designed three NSF models and compared them with WaveNet. It was demonstrated that the NSF models generated waveforms at least 100 times faster than WaveNet, and the quality of the synthetic speech from the best NSF model was better than or equally good as that from WaveNet. ","Neural source-filter waveform models for statistical parametric speech
  synthesis"
8,1124417974889324544,160687843,Alexey Melnikov,"['Our new paper on the use of reinforcement learning in quantum communication, where we show improved solutions to long-distance communication problems  <LINK> “Machine learning for long-distance quantum communication” <LINK>']",https://arxiv.org/abs/1904.10797,"Machine learning can help us in solving problems in the context big data analysis and classification, as well as in playing complex games such as Go. But can it also be used to find novel protocols and algorithms for applications such as large-scale quantum communication? Here we show that machine learning can be used to identify central quantum protocols, including teleportation, entanglement purification and the quantum repeater. These schemes are of importance in long-distance quantum communication, and their discovery has shaped the field of quantum information processing. However, the usefulness of learning agents goes beyond the mere re-production of known protocols; the same approach allows one to find improved solutions to long-distance communication problems, in particular when dealing with asymmetric situations where channel noise and segment distance are non-uniform. Our findings are based on the use of projective simulation, a model of a learning agent that combines reinforcement learning and decision making in a physically motivated framework. The learning agent is provided with a universal gate set, and the desired task is specified via a reward scheme. From a technical perspective, the learning agent has to deal with stochastic environments and reactions. We utilize an idea reminiscent of hierarchical skill acquisition, where solutions to sub-problems are learned and re-used in the overall scheme. This is of particular importance in the development of long-distance communication schemes, and opens the way for using machine learning in the design and implementation of quantum networks. ",Machine learning for long-distance quantum communication
9,1123657559196323841,2671619821,Tyler Hughes,['Check out our new paper on using wave physics to build analog RNNs!\n<LINK>'],https://arxiv.org/abs/1904.12831,"Analog machine learning hardware platforms promise to be faster and more energy-efficient than their digital counterparts. Wave physics, as found in acoustics and optics, is a natural candidate for building analog processors for time-varying signals. Here we identify a mapping between the dynamics of wave physics, and the computation in recurrent neural networks. This mapping indicates that physical wave systems can be trained to learn complex features in temporal data, using standard training techniques for neural networks. As a demonstration, we show that an inverse-designed inhomogeneous medium can perform vowel classification on raw audio signals as their waveforms scatter and propagate through it, achieving performance comparable to a standard digital implementation of a recurrent neural network. These findings pave the way for a new class of analog machine learning platforms, capable of fast and efficient processing of information in its native domain. ",Wave Physics as an Analog Recurrent Neural Network
10,1123626070605852672,1032007830386012160,Paul Dalba,['We acquired speckle imaging for a bunch of targets from the Anglo-Australian Planet Survey. Wanna know how many planetary/stellar companions we found? Check out this new paper by @ExoCytherean and collaborators: <LINK>'],https://arxiv.org/abs/1904.12931,"The sensitivity of radial velocity (RV) surveys for exoplanet detection are extending to increasingly long orbital periods, where companions with periods of several years are now being regularly discovered. Companions with orbital periods that exceed the duration of the survey manifest in the data as an incomplete orbit or linear trend, a feature that can either present as the sole detectable companion to the host star, or as an additional signal overlain on the signatures of previously discovered companion(s). A diagnostic that can confirm or constrain scenarios in which the trend is caused by an unseen stellar, rather than planetary, companion is the use of high-contrast imaging observations. Here, we present RV data from the Anglo-Australian Planet Search (AAPS) for twenty stars that show evidence of orbiting companions. Of these, six companions have resolved orbits, with three that lie in the planetary regime. Two of these (HD~92987b and HD~221420b) are new discoveries. Follow-up observations using the Differential Speckle Survey Instrument (DSSI) on the Gemini South telescope revealed that five of the twenty monitored companions are likely stellar in nature. We use the sensitivity of the AAPS and DSSI data to place constraints on the mass of the companions for the remaining systems. Our analysis shows that a planetary-mass companion provides the most likely self-consistent explanation of the data for many of the remaining systems. ","Detection of Planetary and Stellar Companions to Neighboring Stars via a
  Combination of Radial Velocity and Direct Imaging Techniques"
11,1123550674665574401,2603024598,Ricardo Pérez-Marco,['New paper on #SelfishMining in #Ethereum with @CGrunspan \nThe analysis is more complex than in #Bitcoin. We give closed-form formulas for the profitability of different strategies. Strategies that do not signal uncle blocks are ver effective.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1904.13330,"We study selfish mining in Ethereum. The problem is combinatorially more complex than in Bitcoin because of major differences in the reward system and a different difficulty adjustment formula. Equivalent strategies in Bitcoin do have different profitabilities in Ethereum. The attacker can either broadcast his fork one block by one, or keep them secret as long as possible and publish them all at once at the end of an attack cycle. The first strategy is damaging for substantial hashrates, and we show that the second strategy is even worse. This confirms what we already proved for Bitcoin: Selfish mining is most of all an attack on the difficulty adjustment formula. We show that the current reward for signaling uncle blocks is a weak incentive for the attacker to signal blocks. We compute the profitabilities of different strategies and find out that for a large parameter space values, strategies that do not signal blocks are the best ones. We compute closed-form formulas for the apparent hashrates for these strategies and compare them. We use a direct combinatorics analysis with Dyck words to find these closed-form formulas. ",Selfish Mining in Ethereum
12,1123539108217937920,95683474,Pablo Samuel Castro,"['want to do collaborative musical improv with an #ML model but no time to train one?\nlook no further! new paper explains how you can be #ml-splained during your musical improvs, and how bots can generate a beat to accompany all this.\n\n<LINK>\n\n🤖🎹🥁\n#CreativeAI\n1/ <LINK>', '2/\ni start by playing a bassline, over which i construct a ""deterministic drum beat"" (see image below).\nthis drum beat is then fed into an #RNN which generates a new drum beat. this may require me to come up with a new bass line to match the accents of the new beat. https://t.co/TxLFePJHJZ', '3/\ni then play a set of chords to give it more harmonic grounding (no #ML for chords... yet).\nnow i can just improvise freely.', '4/ once i engage #ML-splainer, my notes get fed into a buffer. once the buffer\'s full it gets sent in an async thread to another #RNN model which will produce a new melody.\nonce new #ML melody is ready, i discard rhythmic information and use pitches to ""intercept"" my notes. https://t.co/OZLipVttLZ', ""5/ by merging the rhythm of my improv with the notes from the #ML model i can make sure we're on the beat, and it maintains my personal style (i find rhythm carries most of an improviser's style)."", '6/ but by not being in control of pitches, i can\'t rely on the ""mechanical"" licks i often use when improvising and i\'m forced to approach improvisation in a completely new manner. \na similar idea is at play when the #ML-generated drum beat forces me to write a new bass line.', '7/ this is what excites me most about this work. as an experienced improviser one tends to stick to one\'s comfort zone because we know we can ""nail it"" there.\ni want to use #ML models to push expert musicians out of their comfort zone and force them to be creative in new ways.', '8/ this idea of using technology to push expert musicians out of their comfort zone is something i feel @tepferdan does incredibly well with his ""natural machines"" project: https://t.co/SGQqSPNAAi\n\nhis work was actually an inspiration for the work that is in my paper!', ""9/ all the code is available here: https://t.co/BmUJvipSKt\n\nthe latest code i've been using for talks is here: https://t.co/NFIYE9QxF3"", '10/ here\'s a demo video showing this setup with @herbiehancock \'s ""chameleon"" bassline:\nhttps://t.co/t0U328Xxq7\n\nand here\'s a video with a weird 7/8 groove: https://t.co/0yIx1AZ5H7', ""11/ if you've ever been to one of my talks on #ML and creativity, this paper explains what i'm doing when i perform the live jam demo at the end.\n\ni'll be giving one of these talks at the @GoogleAI expo @iclr2019 #iclr2019 on may 7th at noon:\nhttps://t.co/ozr7B2MeZJ"", ""12/ if you're interested more in my music than in my #ML, check out my trio: https://t.co/cwyuuoyPuP \ndates in #montreal and #ottawa coming up in june!\ni've used this #ML setup a few times during some shows with the trio, but i prefer to keep the trio with humans only."", '@korymath @jacrichton @hardmaru @PiotrImprov @thomas_wint @h0h0h0 @adammeggido Thanks so much, @korymath !', ""11a/ and if you're in #ottawa i will be talking about #ML and creativity at @pintofscienceCA , organized by @axielyb , which should be a fun event!\n\n🤖🎹🥁🍺\nhttps://t.co/jOu5syeugR""]",https://arxiv.org/abs/1904.13285,"The quality of outputs produced by deep generative models for music have seen a dramatic improvement in the last few years. However, most deep learning models perform in ""offline"" mode, with few restrictions on the processing time. Integrating these types of models into a live structured performance poses a challenge because of the necessity to respect the beat and harmony. Further, these deep models tend to be agnostic to the style of a performer, which often renders them impractical for live performance. In this paper we propose a system which enables the integration of out-of-the-box generative models by leveraging the musician's creativity and expertise. ","Performing Structured Improvisations with pre-trained Deep Learning
  Models"
13,1123298458071457792,2983164057,Mohsen Fayyaz,"['""Holistic Large Scale Video Understanding""\nA new paper by me,@AliDiba67,Vivek Sharma,Manohar Paluri,Juergen Gall, Rainer Stiefelhagen,@lucgool1\n<LINK>\nWe propose a new video dataset for multi-label &amp; multi-task video understanding and a new architecture ""HATNet"" <LINK>', 'Action recognition mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video.', 'We fill in this gap by presenting the large-scale ""Holistic Video Understanding Dataset""(HVU). HVU focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene.', 'HVU contains ~577k videos in total with 13M annotations for training and validation set spanning over 4378 classes. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes &amp; concepts, which naturally captures the real-world scenarios', 'Further, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues.', 'HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. The experiments show that HATNet trained on HVU outperforms current state-of-the-art methods on challenging human action datasets: HMDB51, UCF101, and Kinetics.', 'The dataset and codes will be made publicly available.\nStay tuned for more info on our workshop at #ICCV2019\nhttps://t.co/DAebuwJh4u']",https://arxiv.org/abs/1904.11451,"Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale ""Holistic Video Understanding Dataset""~(HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios. We demonstrate the generalization capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications. ",Large Scale Holistic Video Understanding
14,1123272555740033029,383494771,Ian Williamson,['Last night we posted our new paper about building analog recurrent neural networks (RNNs) using the physics of waves: <LINK> The repository for all the code is available here: <LINK>'],https://arxiv.org/abs/1904.12831,"Analog machine learning hardware platforms promise to be faster and more energy-efficient than their digital counterparts. Wave physics, as found in acoustics and optics, is a natural candidate for building analog processors for time-varying signals. Here we identify a mapping between the dynamics of wave physics, and the computation in recurrent neural networks. This mapping indicates that physical wave systems can be trained to learn complex features in temporal data, using standard training techniques for neural networks. As a demonstration, we show that an inverse-designed inhomogeneous medium can perform vowel classification on raw audio signals as their waveforms scatter and propagate through it, achieving performance comparable to a standard digital implementation of a recurrent neural network. These findings pave the way for a new class of analog machine learning platforms, capable of fast and efficient processing of information in its native domain. ",Wave Physics as an Analog Recurrent Neural Network
15,1123187426413953025,5620142,Edward Grefenstette 🇪🇺,"['One more paper from my @DeepMindAI days, from great intern Chenglong Wang et al.: <LINK>\nContinuous relaxation + some new objectives allow gradient attacks on sequence models.\nUpcoming at #CVPR2019. <LINK>']",https://arxiv.org/abs/1904.12004,"Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in real world applications. While the ability of these neural architectures to produce variable-length outputs makes them extremely effective for problems like Machine Translation and Image Captioning, it also leaves them vulnerable to failures of the form where the model produces outputs of undesirable length. This behavior can have severe consequences such as usage of increased computation and induce faults in downstream modules that expect outputs of a certain length. Motivated by the need to have a better understanding of the failures of these models, this paper proposes and studies the novel output-size modulation problem and makes two key technical contributions. First, to evaluate model robustness, we develop an easy-to-compute differentiable proxy objective that can be used with gradient-based algorithms to find output-lengthening inputs. Second and more importantly, we develop a verification approach that can formally verify whether a network always produces outputs within a certain length. Experimental results on Machine Translation and Image Captioning show that our output-lengthening approach can produce outputs that are 50 times longer than the input, while our verification approach can, given a model and input domain, prove that the output length is below a certain size. ","Knowing When to Stop: Evaluation and Verification of Conformity to
  Output-size Specifications"
16,1123109107538378752,913238472357437445,Fuminobu TAKAHASHI,['Oue new paper on the bouncing universe appeared today. The bounce takes place in 4D Einstein gravity w/o singularity nor violating NEC. A scalar field with a flat potential is the key for the bounce. The slow-roll inflation naturally follows the bounce.\n<LINK>'],https://arxiv.org/abs/1904.12312,"We find a class of solutions for a homogeneous and isotropic universe in which the initially expanding universe stops expanding, experiences contraction, and then expands again (the ""bounce""), in the framework of Einstein gravity with a real scalar field without violating the null energy condition nor encountering any singularities. Two essential ingredients for the bouncing universe are the positive spatial curvature and the scalar potential which becomes flatter at large field values. Depending on the initial condition, either the positive curvature or the negative potential stops the cosmic expansion and begins the contraction phase. The flat potential plays a crucial role in triggering the bounce. After the bounce, the flat potential naturally allows the universe to enter the slow-roll inflation regime, thereby making the bouncing universe compatible with observations. If the e-folding of the subsequent inflation is just enough, a positive spatial curvature may be found in the future observations. Our scenario nicely fits with the creation of the universe from nothing, which leads to the homogeneous and isotropic universe with positive curvature. As a variant of the mechanism, we also find solutions representing a cyclic universe. ",Bouncing Universe from Nothing
17,1123018578431598592,120404165,Filip Ligmajer,['Our new paper about plasmonics of silver amalgam - an interesting electrochemical material - is on arXiv. <LINK> #plasmonics <LINK>'],https://arxiv.org/abs/1904.11160,"Plasmonic nanoparticles from unconventional materials can improve or even bring some novel functionalities into the disciplines inherently related to plasmonics such as photochemistry or (spectro)electrochemistry. They can, for example, catalyze various chemical reactions or act as nanoelectrodes and optical transducers in various applications. Silver amalgam is the perfect example of such an unconventional plasmonic material, albeit it is well-known in the field of electrochemistry for its wide cathodic potential window and strong adsorption affinity of biomolecules to its surface. In this study, we investigate in detail the optical properties of nanoparticles and microparticles made from silver amalgam and correlate their plasmonic resonances with their morphology. We use optical spectroscopy techniques on the ensemble level and electron energy loss spectroscopy on the single-particle level to demonstrate the extremely wide spectral range covered by the silver amalgam localized plasmonic resonances, ranging from ultraviolet all the way to the mid-infrared wavelengths. Our results establish silver amalgam as a suitable material for introduction of plasmonic functionalities into photochemical and spectroelectrochemical systems, where the plasmonic enhancement of electromagnetic fields and light emission processes could synergistically meet with the superior electrochemical characteristics of mercury. ","Silver Amalgam Nanoparticles and Microparticles: A Novel Plasmonic
  Platform for Spectroelectrochemistry"
18,1122968591563988992,242066463,Iñaki Ugarte,['Our new paper about the magnetic properties of high temperature coronal loops in the solar corona: <LINK>'],https://arxiv.org/abs/1904.11976,"Understanding the relationship between the magnetic field and coronal heating is one of the central problems of solar physics. However, studies of the magnetic properties of impulsively heated loops have been rare. We present results from a study of 34 evolving coronal loops observed in the Fe XVIII line component of AIA/SDO 94 A filter images from three active regions with different magnetic conditions. We show that the peak intensity per unit cross-section of the loops depends on their individual magnetic and geometric properties. The intensity scales proportionally to the average field strength along the loop ($B_{avg}$) and inversely with the loop length ($L$) for a combined dependence of $(B_{avg}/L)^{0.52\pm0.13}$. These loop properties are inferred from magnetic extrapolations of the photospheric HMI/SDO line-of-sight and vector magnetic field in three approximations: potential and two Non Linear Force-Free (NLFF) methods. Through hydrodynamic modeling (EBTEL model) we show that this behavior is compatible with impulsively heated loops with a volumetric heating rate that scales as $\epsilon_H\sim B_{avg}^{0.3\pm0.2}/L^{0.2\pm^{0.2}_{0.1}}$. ","The Magnetic Properties of Heating Events on High-Temperature Active
  Region Loops"
19,1122884892575117318,199409067,Alex Teachey 齊孝嵐,"[""We have a new paper out today digging a bit deeper into the Kepler-1625b exomoon candidate! <LINK> \n\nWe've run lots of additional tests, and we address the recent paper from Kreidberg et al (KLB19). Since that's gotten some attention, let me start there: 1/n"", 'So far there have been three independent analyses of the HST observation. Of those, two find evidence for a moon-like transit feature, one does not. And all three find evidence for transit timing variations (TTVs) (see also: https://t.co/mHOWZDa1t9). What to make of this? 2/n', 'We had extensive discussions with L. Kreidberg to determine the source of the discrepant results. Our conclusion? Basically the same as hers: the different reduction pipelines are responsible for the different conclusions. So which one is right? 3/n', 'Kreidberg has a track record working with HST which we do not. Fair enough. However -- and this is critical -- neither have any flaws been found in our own reduction, nor has any step in our analysis been identified as being the culprit for the moon-like signal we see. 4/n', ""Obviously you'd hope a genuine signal would be robust against (evidently equally reasonable) choices made along the way of producing a light curve. But we're really pushing the data to the limit, so a weak signal could be introduced *or removed* in the process. 5/n"", ""KLB19 graciously allowed us access to their data. While we don't find any flaws in their pipeline, we do see systematics (a larger mid-transit offset, larger centroid variations, and anomalously low scatter in orbit 22) suggesting this dataset isn't inherently cleaner / better. 6 https://t.co/Q8R9ADzHB4"", 'Of course, the TTVs, which are a key piece of the moon hypothesis, have now been doubly validated, but we need more observations to determine whether these could be explained by an unseen, perturbing planet in the system, rather than the moon. 7/n', 'Radial velocity observations can tell us the mass of the planet (predicted by the moon fit) and can put strong limits on the mass and location of a hypothesized perturbing planet. These observations will be very important, because *something* has to be inducing the TTVs. 8/n https://t.co/uxozBrV5Wc', ""Where does this leave us? The moon-like signal we reported last year has now been both validated and called into question by independent teams... 2:1 for the transit, 3:0 for the TTV. I'd say that means the existence of this moon remains very much an open question. 9/n"", ""The rest of the paper deals with a variety of other potential explanations for the signals we're seeing. First, we examine additional detrending models &amp; find that more flexible models can remove the moon, but this is entirely expected given more degrees of freedom. 10/n https://t.co/S6xcOnkRXV"", 'Next we test the effect of detrending as a function of centroid position rather than / in addition to detrending as a function of time, and find that there is no strong impetus to adopt this modeling approach. 11/n', 'We explore the possibility that an additional planet in the system is responsible for the transit feature we attribute to the moon, and calculate that probability to be less than 0.75%. 12/n https://t.co/F6HUu9HwfB', ""We performed additional tests on the star's activity based on the Kepler light curves, and find that the star is very quiet and unlikely to introduce the transit-like signature that we attribute to the moon. 13/n https://t.co/ktkVuIBZhZ"", ""Finally, we run the clock forward and discuss the future observability of the moon. Given its inferred inclination, the moon's not guaranteed to transit each time the planet does, so this makes future transit observations quite difficult. RVs and TTVs will be important. 14/n https://t.co/Dk1EezSKgl"", 'Final thoughts: controversy is inevitable in a case like this, and skepticism is a good thing in science. We all want to get to the truth. But all of us have to be honest with ourselves about our (confirmation) biases, our assumptions, what we might *want* to be true. 15/n', ""For our part, we've tried to be at least as hard-nosed about this moon candidate as anyone else. Just follow the data... but clearly the data can tell you different things when you're pushing the limits! So we continue to look for other ways of testing this hypothesis. 16/n"", 'To me, the most intellectually honest interpretation of all the available evidence is ""we don\'t know yet"". But I want to know, and I\'m thrilled to see so many other people want to know, too. It may take some time, but we\'ll figure this out. Stay tuned. 17/17', '@bmac_astro Could be interesting, though since most (all?) other HST transit observations are Hot Jupiters the observing strategies are rather different (multiple epochs, fewer out of transit exposures, shorter durations, brighter targets...). Maybe apples and oranges, quite challenging.', ""@timholtastro @AscendingNode Probably not, unfortunately. The planet transits next in May, but TESS won't observe the Kepler field until July. And anyway the sensitivity probably wouldn't be enough. But continuing to monitor the transit timings would be valuable.""]",https://arxiv.org/abs/1904.11896,"The claim of an exomoon candidate in the Kepler-1625b system has generated substantial discussion regarding possible alternative explanations for the purported signal. In this work we examine in detail these possibilities. First, the effect of more flexible trend models is explored and we show that sufficiently flexible models are capable of attenuating the signal, although this is an expected byproduct of invoking such models. We also explore trend models using X and Y centroid positions and show that there is no data-driven impetus to adopt such models over temporal ones. We quantify the probability that the 500 ppm moon-like dip could be caused by a Neptune-sized transiting planet to be < 0.75%. We show that neither autocorrelation, Gaussian processes nor a Lomb-Scargle periodogram are able to recover a stellar rotation period, demonstrating that K1625 is a quiet star with periodic behavior < 200 ppm. Through injection and recovery tests, we find that the star does not exhibit a tendency to introduce false-positive dip-like features above that of pure Gaussian noise. Finally, we address a recent re-analysis by Kreidberg et al (2019) and show that the difference in conclusions is not from differing systematics models but rather the reduction itself. We show that their reduction exhibits i) slightly higher intra-orbit and post-fit residual scatter, ii) $\simeq$ 900 ppm larger flux offset at the visit change, iii) $\simeq$ 2 times larger Y-centroid variations, and iv) $\simeq$ 3.5 times stronger flux-centroid correlation coefficient than the original analysis. These points could be explained by larger systematics in their reduction, potentially impacting their conclusions. ",Loose Ends for the Exomoon Candidate Host Kepler-1625b
20,1122873535444209666,338526004,Sam Bowman,"['New paper alert: Some neat fine-grained analysis of pretrained models like BERT, led by @najoungkim and Ellie Pavlick based on work from our JSALT team. To appear at *SEM. \n<LINK> <LINK>']",https://arxiv.org/abs/1904.11544,"We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on language modeling performs the best on average across our probing tasks, supporting its widespread use for pretraining state-of-the-art NLP models, and CCG supertagging and NLI pretraining perform comparably. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation. ","Probing What Different NLP Tasks Teach Machines about Function Word
  Comprehension"
21,1122871153079533568,2800204849,Andrew Gordon Wilson,"['Stochastic Weight Averaging in Low Precision Training (SWALP)! Our new ICML paper (with PyTorch code). SWALP can match the performance of full-precision training, even with all numbers quantized down to 8 bits! <LINK> <LINK>']",https://arxiv.org/abs/1904.11943,"Low precision operations can provide scalability, memory savings, portability, and energy efficiency. This paper proposes SWALP, an approach to low precision training that averages low-precision SGD iterates with a modified learning rate schedule. SWALP is easy to implement and can match the performance of full-precision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the optimal solution for quadratic objectives, and to a noise ball asymptotically smaller than low precision SGD in strongly convex settings. ",SWALP : Stochastic Weight Averaging in Low-Precision Training
22,1122859351218245633,1524510067,Max Saller,"['New paper submitted, expanding our alternative definition of the mapping population operator to many electronic states:\n<LINK>']",https://arxiv.org/abs/1904.11847,"The mapping approach addresses the mismatch between the continuous nuclear phase space and discrete electronic states by creating an extended, fully continuous phase space using a set of harmonic oscillators to encode the populations and coherences of the electronic states. Existing quasiclassical dynamics methods based on mapping, such as the linearised semiclassical initial value representation (LSC-IVR) and Poisson bracket mapping equation (PBME) approaches, have been shown to fail in predicting the correct relaxation of electronic-state populations following an initial excitation. Here we generalise our recently published modification to the standard quasiclassical approximation for simulating quantum correlation functions. We show that the electronic-state population operator in any system can be exactly rewritten as a sum of a traceless operator and the identity operator. We show that by treating the latter at a quantum level instead of using the mapping approach, the accuracy of traditional quasiclassical dynamics methods can be drastically improved, without changes to their underlying equations of motion. We demonstrate this approach for the seven-state Frenkel-Exciton model of the Fenna-Matthews-Olson light harvesting complex, showing that our modification significantly improves the accuracy of traditional mapping approaches when compared to numerically exact quantum results. ","Improved population operators for multi-state nonadiabatic dynamics with
  the mixed quantum-classical mapping approach"
23,1121938334287323136,1068545181576773632,Kenneth Brown,"[""New paper examining circuit level error models for error correction circuits composed of qubits that leak and noisier qubits that don't leak. We find that for a  physically motivated leakage model, a mixture of qubits works best. <LINK>"", 'This paper is the third in a series.\nIn paper 1, Natalie Brown and I compared two types of ion qubits (hyperfine=leaky, Zeeman=noisy) for toric codes using a standard leakage model. Zeeman wins for low magnetic field noise. https://t.co/4pbNQzJeHN', 'In paper 2, Michael Newman, Natalie, and I found that subsystem codes are good for handling leakage. We also introduced a new leakage error model based leaked states not interacting with qubit states during two-qubit gates.\nhttps://t.co/8GtMy7jM05', 'In this paper, we see that the new error model improves the performance of hyperfine qubits in the context of error correction.  There is still a magnetic field stability where Zeeman qubits are better, but at this magnetic field stability a mixed-qubit strategy is the best.']",https://arxiv.org/abs/1904.10724,"Leakage errors take qubits out of the computational subspace and will accumulate if not addressed. A leaked qubit will reduce the effectiveness of quantum error correction protocols due to the cost of implementing leakage reduction circuits and the harm caused by interacting leaked states with qubit states. Ion trap qubits driven by Raman gates have a natural choice between qubits encoded in magnetically insensitive hyperfine states that can leak and qubits encoded in magnetically sensitive Zeeman states of the electron spin that cannot leak. In our previous work, we compared these two qubits in the context of the toric code with a depolarizing leakage error model and found that for magnetic field noise with a standard deviation less than 32 $\mu$G that the $^{174}$Yb$^+$ Zeeman qubit outperforms the $^{171}$Yb$^+$ hyperfine qubit. Here we examine a physically motivated leakage error model based on ions interacting via the Molmer-Sorenson gate. We find that this greatly improves the performance of hyperfine qubits but the Zeeman qubits are more effective for magnetic field noise with a standard deviation less than 10 $\mu$G. At these low magnetic fields, we find that the best choice is a mixed qubit scheme where the hyperfine qubits are the ancilla and the leakage is handled without the need of an additional leakage reduction circuit. ","Leakage mitigation for quantum error correction using a mixed qubit
  scheme"
24,1121816093318762496,2927266003,Prof. Erika Hamden,"['New paper from our group accepted to @NatureAstronomy! Learn about giant clouds of mostly hydrogen, the best element!  <LINK>']",https://arxiv.org/abs/1904.11465,"Theory suggests that there are two primary modes of accretion through which dark matter halos acquire the gas to form and fuel galaxies, hot and cold mode accretion. In cold mode accretion, gas streams along cosmic web filaments to the center of the halo, allowing for the efficient delivery of star-forming fuel. Recently, two QSO-illuminated HI Lyman alpha (Ly{\alpha}) emitting objects were reported to have properties of cold, rotating structures (Martin et al. 2015, Martin et al. 2016). However, the spatial and spectral resolution available was insufficient to constrain radial flows associated with connecting filaments. With the Keck Cosmic Web Imager (KCWI) we now have eight times the spatial resolution, permitting the detection of these in-spiraling flows. In order to detect these inflows, we introduce a suite of models which incorporate zonal radial flows, demonstrate their performance on a numerical simulation that exhibits coldflow accretion, and show that they are an excellent match to KCWI velocity maps of two Ly{\alpha} emitters observed around high-redshift quasars. These Multi-Filament Inflow models kinematically isolate zones of radial inflow that correspond to extended filamentary emission. The derived gas flux and inflow path is sufficient to fuel the inferred central galaxy star formation rate and angular momentum. Thus, our kinematic emission maps provide strong evidence for the inflow of gas from the cosmic web building galaxies at the peak of star formation. ",Multi-Filament Inflows Fueling Young Star Forming Galaxies
25,1121780337061896192,3422471637,Elias Kammoun,"['Here is our new paper on NGC 5347: a bona fide Compton-thick AGN just sitting in our backyard!\n\nMore exciting results from our survey of nearby obscured AGN with @NASANuSTAR are coming soon.. \n\n<LINK> <LINK>', '@NASANuSTAR In ~12 years the @AthenaXIFU on board of @AthenaXobs will allow us to look at the finest details in X-ray spectra of obscured AGN, opening a whole new window in X-ray astronomy, going from the current data-quality (left) to the well-resolved emission lines with Athena (right). https://t.co/ohySAjTJkl']",https://arxiv.org/abs/1904.11028,"Current measurements show that the observed fraction of Compton-thick (CT) AGN is smaller than the expected values needed to explain the cosmic X-ray background. Prior fits to the X-ray spectrum of the nearby Seyfert-2 galaxy NGC 5347 ($z=0.00792,\, D =35.5 \rm ~Mpc $) have alternately suggested a CT and Compton-thin source. Combining archival data from $Suzaku$, $Chandra$, and - most importantly - new data from $NuSTAR$, and using three distinct families of models, we show that NGC 5347 is an obscured CTAGN ($N_{\rm H} > 2.23\times 10^{24}~\rm cm^{-2}$). Its 2-30~keV spectrum is dominated by reprocessed emission from distant material, characterized by a strong Fe K$\alpha$ line and a Compton hump. We found a large equivalent width of the Fe K$\alpha$ line ($\rm EW = 2.3 \pm 0.3$ keV) and a high intrinsic-to-observed flux ratio ($\sim 100$). All of these observations are typical for bona fide CTAGN. We estimate a bolometric luminosity of $L_{\rm bol} \simeq 0.014 \pm 0.005~L_{\rm Edd.}$. The $Chandra$ image of NGC 5347 reveals the presence of extended emission dominating the soft X-ray spectrum ($E < 2\,\rm keV$), which coincides with the [O III] emission detected in the $Hubble ~Space~ Telescope$ images. Comparison to other CTAGN suggests that NGC 5347 is broadly consistent with the average properties of this source class. We simulated $XRISM$ and $Athena$/X-IFU spectra of the source, showing the potential of these future missions in identifying CTAGN in the soft X-rays. ",A hard look at NGC 5347: revealing a nearby Compton-thick AGN
26,1121761214453960705,2780394713,Ali Diba,"['Our new paper: ""DynamoNet: Dynamic Action and Motion Network"". Learning new and dynamic deep representation to extract motion features and recognizing action in an end-to-end deep 3D-CNN.\n\n<LINK> <LINK>']",https://arxiv.org/abs/1904.11407,"In this paper, we are interested in self-supervised learning the motion cues in videos using dynamic motion filters for a better motion representation to finally boost human action recognition in particular. Thus far, the vision community has focused on spatio-temporal approaches using standard filters, rather we here propose dynamic filters that adaptively learn the video-specific internal motion representation by predicting the short-term future frames. We name this new motion representation, as dynamic motion representation (DMR) and is embedded inside of 3D convolutional network as a new layer, which captures the visual appearance and motion dynamics throughout entire video clip via end-to-end network learning. Simultaneously, we utilize these motion representation to enrich video classification. We have designed the frame prediction task as an auxiliary task to empower the classification problem. With these overall objectives, to this end, we introduce a novel unified spatio-temporal 3D-CNN architecture (DynamoNet) that jointly optimizes the video classification and learning motion representation by predicting future frames as a multi-task learning problem. We conduct experiments on challenging human action datasets: Kinetics 400, UCF101, HMDB51. The experiments using the proposed DynamoNet show promising results on all the datasets. ",DynamoNet: Dynamic Action and Motion Network
27,1121753497924440065,1077995761487568896,Jon Miller,"[""Check out @eskammoun's new paper on NGC 5347, a Compton-thick AGN: <LINK>.  Black holes grow fastest in these shrouded environments, and a number of them appear to be fairly nearby. We are doing a complete, volume-limited survey of obscured AGN with @NASANuSTAR. <LINK>""]",https://arxiv.org/abs/1904.11028,"Current measurements show that the observed fraction of Compton-thick (CT) AGN is smaller than the expected values needed to explain the cosmic X-ray background. Prior fits to the X-ray spectrum of the nearby Seyfert-2 galaxy NGC 5347 ($z=0.00792,\, D =35.5 \rm ~Mpc $) have alternately suggested a CT and Compton-thin source. Combining archival data from $Suzaku$, $Chandra$, and - most importantly - new data from $NuSTAR$, and using three distinct families of models, we show that NGC 5347 is an obscured CTAGN ($N_{\rm H} > 2.23\times 10^{24}~\rm cm^{-2}$). Its 2-30~keV spectrum is dominated by reprocessed emission from distant material, characterized by a strong Fe K$\alpha$ line and a Compton hump. We found a large equivalent width of the Fe K$\alpha$ line ($\rm EW = 2.3 \pm 0.3$ keV) and a high intrinsic-to-observed flux ratio ($\sim 100$). All of these observations are typical for bona fide CTAGN. We estimate a bolometric luminosity of $L_{\rm bol} \simeq 0.014 \pm 0.005~L_{\rm Edd.}$. The $Chandra$ image of NGC 5347 reveals the presence of extended emission dominating the soft X-ray spectrum ($E < 2\,\rm keV$), which coincides with the [O III] emission detected in the $Hubble ~Space~ Telescope$ images. Comparison to other CTAGN suggests that NGC 5347 is broadly consistent with the average properties of this source class. We simulated $XRISM$ and $Athena$/X-IFU spectra of the source, showing the potential of these future missions in identifying CTAGN in the soft X-rays. ",A hard look at NGC 5347: revealing a nearby Compton-thick AGN
28,1121688662037544960,2705638878,Dr. Adelle Goodwin,"['New paper now on ArXiv! <LINK>\n\nIn which we discovered some interesting bursts from the known X-ray binary XTE J1812-182, and were able to classify it as an ultracompact source, distant (14 kpc) and frequent burster (every 1.4 hours) 💥💫 #astrophysics #phDing', 'This source is cool because all the data point toward it being a helium rich donor, which poses interesting questions as to how a low mass X-ray binary could evolve to be ultra compact with a significantly evolved donor star.', 'And as an added bonus we observed 2 bursts just 18 minutes apart! These very short recurrence time bursts are thought to be caused by further burning of the accreted fuel, creating 2 (or more) short, fainter bursts very close together']",https://arxiv.org/abs/1904.10970,"We report the discovery of Type I (thermonuclear) X-ray bursts from the transient source XMMU J181227.8-181234 = XTE J1812-182. We found 7 X-ray bursts in Rossi X-ray Timing Explorer observations during the 2008 outburst, confirming the source as a neutron star low mass X-ray binary. Based on the measured burst fluence and the average recurrence time of 1.4$^{+0.9}_{-0.5}$ hr, we deduce that the source is accreting almost pure helium ($X \leq 0.1$) fuel. Two bursts occurred just 18 minutes apart; the first short waiting time bursts observed in a source accreting hydrogen-poor fuel. Taking into consideration the effects on the burst and persistent flux due to the inferred system inclination of $30\pm{10}$ degrees, we estimate the distance to be $14\pm{2}$ kpc, where we report the statistical uncertainty but note that there could be up to $20\%$ variation in the distance due to systematic effects discussed in the paper. The corresponding maximum accretion rate is $0.30\pm0.05$ times the Eddington limit. Based on the low hydrogen content of the accreted fuel and the short average recurrence time, we classify the source as a transient ultracompact low-mass X-ray binary. ",XMMU J181227.8-181234: a new ultracompact X-ray binary candidate
29,1121626673240109058,1003652696723873792,Max Gaspari,"['New paper on linking SMBHs to the hot plasma halos of galaxies, groups, and clusters of galaxies! Presenting new key scaling relations of SMBHs (tighter than the classic M-sigma) and tests to probe BH feeding theories:\n<LINK>\n#astronomy #astrophysics #BlackHoles']",https://arxiv.org/abs/1904.10972,"We carry out a comprehensive Bayesian correlation analysis between hot halos and direct masses of supermassive black holes (SMBHs), by retrieving the X-ray plasma properties (temperature, luminosity, density, pressure, masses) over galactic to cluster scales for 85 diverse systems. We find new key scalings, with the tightest relation being the $M_\bullet-T_{\rm x}$, followed by $M_\bullet-L_{\rm x}$. The tighter scatter (down to 0.2 dex) and stronger correlation coefficient of all the X-ray halo scalings compared with the optical counterparts (as the $M_\bullet-\sigma_{\rm e}$) suggest that plasma halos play a more central role than stars in tracing and growing SMBHs (especially those that are ultramassive). Moreover, $M_\bullet$ correlates better with the gas mass than dark matter mass. We show the important role of the environment, morphology, and relic galaxies/coronae, as well as the main departures from virialization/self-similarity via the optical/X-ray fundamental planes. We test the three major channels for SMBH growth: hot/Bondi-like models have inconsistent anti-correlation with X-ray halos and too low feeding; cosmological simulations find SMBH mergers as sub-dominant over most of the cosmic time and too rare to induce a central-limit-theorem effect; the scalings are consistent with chaotic cold accretion (CCA), the rain of matter condensing out of the turbulent X-ray halos that sustains a long-term self-regulated feedback loop. The new correlations are major observational constraints for models of SMBH feeding/feedback in galaxies, groups, and clusters (e.g., to test cosmological hydrodynamical simulations), and enable the study of SMBHs not only through X-rays, but also via the Sunyaev-Zel'dovich effect (Compton parameter), lensing (total masses), and cosmology (gas fractions). ",The X-ray Halo Scaling Relations of Supermassive Black Holes
30,1121436102903070721,8802602,Emily Rice,"['#BDNYC has a new paper on the arXiv, titled “Radial Velocities, Space Motions, and Nearby Young Moving Group Memberships of Eleven Candidate Young Brown Dwarfs”, led by @AstroAdric: <LINK>! Let me tell you why I am so proud of this paper...', 'First, the science: we obtained high-resolution near-infrared spectra (using @keckobservatory) of 11 suspected young, very low mass objects in order to measure their radial velocity (motion along our line of sight), combine that with other motion &amp; distance measurements to...', '...calculate 3D position &amp; motion in space to see if those matched the properties of known young moving groups, which are loose associations of stars young enough to be moving together in space, but too old to be “star-forming regions”/too sparse &amp; dispersed to be “clusters”.', 'Our radial velocities combined with other measurements, including from @ESAGaia, were good enough to place 5 objects in groups, thus giving us an age estimate, usually based on the group’s more massive member stars. Yay!', 'I should say we used several different codes, including LACEwING by @AstroAdric, various versions of BANYAN by @jgagneastro, &amp; @Strakul’s Convergence code, in order to be as thorough as possible.', 'Of the 5 confirmed group members, 4 were known before. Were we scooped? Not in my opinion - solid scientific results need to be reproducible! Not many young brown dwarfs are known (less than 200) so even one brand new one is exciting, too.', 'Perhaps more interesting are the 6 objects that seem like they should be young, but that don’t seem to fit in with known young groups - why not? What are we missing?! It’s an increasingly interesting mystery as we find more &amp; more of these rogue young objects.', 'What I’m *most* proud of about this paper is the people! This paper is 100% #BDNYC, from beginning to end, with huge student contributions, and was a long time coming! Here’s that story...', 'First, the timeline: all new observations were done at @keckobservatory in 2014, which means we wrote proposals for time in 2013, submitted to highly competitive national time allocation committees. We had 2 successful proposals, 1 w/ @NASA &amp; 1 w/ @NOAONorth, led by @AstroAdric.', 'I’ll take some credit for lending expertise with the instrument &amp; proposed observations to the proposal, &amp; credit @kellecruz with providing the target list of compelling candidate young brown dwarfs &amp; @jfaherty with expertise in kinematics of young associations. #BDNYC', '#CUNYAstro students were involved almost from the very beginning, starting with @GC_CUNY @GCsciences physics student (now) Dr. Kay Hiranaka who observed with me &amp; @AstroAdric using remote facilities at @YaleAstronomy, with an important road trip stop at the PEZ factory!', 'Then @SeeTheStarsRise &amp; @victoriadi2MASS joined #CUNYAstro &amp; #BDNYC in summer 2015, reduced all the data, and began radial velocity measurements using code that was started by @vbaldassare, if memory serves, continued by @AstroAdric.', 'Both @SeeTheStarsRise &amp; @victoriadi2MASS presented award-winning posters &amp; talks about the project, and Victoria continued to work on it while Ellie moved on to another project, bringing us to 2016 or so.', '@johngizis @AstroAdric maintains a catalogue of the known young stars (i.e. w/ memberships), but I’m not sure anyone has compiled or examined just the “rogue” objects (“orphan” seems too sad, and I want to steal “rogue” from the free-floating “planets” 🙄).... @jgagneastro @jfaherty ?', 'The main analysis for the paper is fairly straightforward (RVs➕other measurements ➡️3D positions &amp; motions➡️memberships➡️age/mass), but some aspects are harder to nail down, plus @victoriadi2MASS explores what else we can test with our data...', '(Ok this thread is taking forever and doesn’t have nearly enough GIFs or even photos, but I am typing one-handed on my phone while holding a sleeping baby so...) https://t.co/H1XMVfJlYk', 'Oh, in 2015 @AstroAdric moved on from #BDNYC at @AMNH to a postdoc position at @Caltech, so he &amp; @victoriadi2MASS are meeting remotely. Plus Victoria takes on new projects in summers 2016 &amp; 2017, I have 1st kid in late 2016 - basically this project is back-burnered for everyone.', 'But! #BDNYC is going strong &amp; our wonderful interactivity gets two more group members involved in this project! Munazza Alam uses her PhEW code (written with @stephtdouglas: https://t.co/LlJDIEs3TV) to measure gravity-sensitive line strengths so we can compare age indicators', 'And, Hunter College High School student James Cook (now at @UCLA!) makes plots of infrared colors of our sample compared to averages for each spectral type from @jfaherty’s work, which was also the basis for his award-winning Regeneron science project in 2017!', 'I think I’ve covered every author’s contribution, but here’s where @victoriadi2MASS deserves all the credit: in addition to her kick-astro analysis, including deep dives into systematics &amp; uncertainties, this paper would not have been finished without her!', '.@victoriadi2MASS took on the often thankless role of taskmaster by scheduling meetings, reminding us of action items &amp; self-imposed deadlines, and maintaining our momentum. I recognize that this was probably even more challenging to do as a student!', 'And “student” is really an oversimplification of all that @victoriadi2MASS was doing. In addition to @Hunter_College physics coursework and completing &amp; presenting her two other summer projects, she applied to graduate school &amp; for fellowships while pushing this paper along!', 'Those turned out well, too, considering that @victoriadi2MASS is currently working with @sjs917 at @AIP_Potsdam on a @FulbrightPrgrm Fellowship and will start a Ph.D. at Harvard this fall!', 'All this bragging it not to say that students/projects who/that don’t produce a paper are less awesome or deserving. Science is a messy process, and often (most of the time?) things don’t work out with proposals, funding, data, analysis, timing, etc etc etc.', 'That’s why I’m celebrating this accomplishment, 6+ years in the making! Here’s the paper again: https://t.co/M4Juw8MA4Q by @AstroAdric, @victoriadi2MASS, me, Munazza Alam, @SeeTheStarsRise, James Cook, @kellecruz, &amp; @jfaherty!']",https://arxiv.org/abs/1904.10579,"We present new radial velocity (RV) measurements for 11 candidate young very-low-mass stars and brown dwarfs, with spectral types from M7 to L7. Candidate young objects were identified by features indicative of low surface gravity in their optical and/or near-infrared spectra. RV measurements are derived from high resolution (R=$\lambda$/$\Delta\lambda$=20,000) $J$ band spectra taken with NIRSPEC at the Keck Observatory. We combine RVs with proper motions and trigonometric distances to calculate three-dimensional space positions and motions and to evaluate membership probabilities for nearby young moving groups (NYMGs). We propose 2MASS J00452143+1634446 (L2$\beta$, $J$=13.06) as an RV standard given the precision and stability of measurements from three different studies. We test the precision and accuracy of our RV measurements as a function of spectral type of the comparison object, finding that RV results are essentially indistinguishable even with differences of $\pm$5 spectral subtypes. We also investigate the strengths of gravity-sensitive K~{\sc i} lines at 1.24--1.25 $\mu$m and evaluate their consistency with other age indicators. We confirm or re-confirm four brown dwarf members of NYMGs -- 2MASS J00452143+1634446, WISE J00470038+6803543, 2MASS J01174748$-$3403258, and 2MASS J19355595$-$2846343 -- and their previous age estimates. We identify one new brown dwarf member of the Carina-Near moving group, 2MASS J21543454$-$1055308. The remaining objects do not appear to be members of any known NYMGs, despite their spectral signatures of youth. These results add to the growing number of very-low-mass objects exhibiting signatures of youth that lack likely membership in a known NYMG, thereby compounding the mystery regarding local, low-density star formation. ","Radial Velocities, Space Motions, and Nearby Young Moving Group
  Memberships of Eleven Candidate Young Brown Dwarfs"
31,1121422314040590336,979419456,lluís galbany,['A new @PESSTOsurvey paper about SN2016hnk on arxiv:\n\n<LINK>'],https://arxiv.org/abs/1904.10034,"We present a comprehensive dataset of optical and near-infrared photometry and spectroscopy of type~Ia supernova (SN) 2016hnk, combined with integral field spectroscopy (IFS) of its host galaxy, MCG -01-06-070, and nearby environment. Properties of the SN local environment are characterized by means of single stellar population synthesis applied to IFS observations taken two years after the SN exploded. SN 2016hnk spectra are compared to other 1991bg-like SNe Ia, 2002es-like SNe Ia, and Ca-rich transients. In addition, abundance stratification modelling is used to identify the various spectral features in the early phase spectral sequence and the dataset is also compared to a modified non-LTE model previously produced for the sublumnious SN 1999by. SN 2016hnk is consistent with being a sub-luminous (M$_{\rm B}=-16.7$ mag, s$_{\rm BV}$=0.43$\pm$0.03), highly reddened object. IFS of its host galaxy reveals both a significant amount of dust at the SN location, as well as residual star formation and a high proportion of old stellar populations in the local environment compared to other locations in the galaxy, which favours an old progenitor for SN 2016hnk. Inspection of a nebular spectrum obtained one year after maximum contains two narrow emission lines attributed to the forbidden [Ca II] $\lambda\lambda$7291,7324 doublet with a Doppler shift of 700 km s$^{-1}$. Based on various observational diagnostics, we argue that the progenitor of SN 2016hnk was likely a near Chandrasekhar-mass ($M_{\rm Ch}$) carbon-oxygen white dwarf that produced 0.108 $M_\odot$ of $^{56}$Ni. Our modeling suggests that the narrow [Ca II] features observed in the nebular spectrum are associated with $^{48}$Ca from electron capture during the explosion, which is expected to occur only in white dwarfs that explode near or at the $M_{\rm Ch}$ limit. ","On the Ca-strong 1991bg-like type Ia supernova 2016hnk: evidence for a
  Chandrasekhar-mass explosion"
32,1121214635615694849,72074540,Justin Lanier,['Extra! Extra!\n\n@MarissaKawehi and I just uploaded a new paper on the arXiv. :D\n\n<LINK>\n\nCome for this picture! Stay for this picture! (There is only one picture! 😂) <LINK>'],http://arxiv.org/abs/1904.10060v1,"In this note we show that many subgroups of mapping class groups of infinite-type surfaces without boundary have trivial centers, including all normal subgroups. Using similar techniques, we show that every nontrivial normal subgroup of a big mapping class group contains a nonabelian free group. In contrast, we show that no big mapping class group satisfies the strong Tits alternative enjoyed by finite-type mapping class groups. We also give examples of big mapping class groups that fail to satisfy even the classical Tits alternative and give a proof that every countable group appears as a subgroup of some big mapping class group. ","] Centers of subgroups of big mapping class groups and the Tits
  alternative"
33,1120901615463075840,1710697381,Diego F. Torres,"['New paper today in arXiv (sub. to ApJ) studying the transitional pulsar  J1023+0038, critical new observations and a new theoretical idea: ""Pulsating in unison at optical and X-ray energies: simultaneous high-time resolution observations of PSR J1023+0038"" <LINK> <LINK>', 'We show observations with XMM, Swift, NuStar, NICER, TNG, NOT, TJO, and GTC, for the first report on the high time resolution optical/X-ray/IR/UV observational campaign of this transitional pulsar in the disk state. https://t.co/qwk50KfhOF', 'Optical and X-ray pulsations were detected simultaneously in the X-ray high intensity mode in which the source spends ∼ 70% of the time, and both disappeared in the low mode, indicating a common underlying physical mechanism. The correlated pulsations are clear. https://t.co/5pDy18ESra', 'We propose that optical and X-ray pulses are produced by synchrotron  emission from the intrabinary shock that forms where a striped pulsar  wind meets the accretion disk, *within a few light cylinder radii  away*, ~100 km, from the pulsar. Introducing the concept of a mini-PWN! https://t.co/D55HTuW78V']",https://arxiv.org/abs/1904.10433,"PSR J1023+0038 is the first millisecond pulsar discovered to pulsate in the visible band; such a detection took place when the pulsar was surrounded by an accretion disk and also showed X-ray pulsations. We report on the first high time resolution observational campaign of this transitional pulsar in the disk state, using simultaneous observations in the optical (TNG, NOT, TJO), X-ray (XMM-Newton, NuSTAR, NICER), infrared (GTC) and UV (Swift) bands. Optical and X-ray pulsations were detected simultaneously in the X-ray high intensity mode in which the source spends $\sim$ 70% of the time, and both disappeared in the low mode, indicating a common underlying physical mechanism. In addition, optical and X-ray pulses were emitted within a few km, had similar pulse shape and distribution of the pulsed flux density compatible with a power-law relation $F_{\nu} \propto \nu^{-0.7}$ connecting the optical and the 0.3-45 keV X-ray band. Optical pulses were detected also during flares with a pulsed flux reduced by one third with respect to the high mode; the lack of a simultaneous detection of X-ray pulses is compatible with the lower photon statistics. We show that magnetically channeled accretion of plasma onto the surface of the neutron star cannot account for the optical pulsed luminosity ($\sim 10^{31}$ erg/s). On the other hand, magnetospheric rotation-powered pulsar emission would require an extremely efficient conversion of spin-down power into pulsed optical and X-ray emission. We then propose that optical and X-ray pulses are instead produced by synchrotron emission from the intrabinary shock that forms where a striped pulsar wind meets the accretion disk, within a few light cylinder radii away, $\sim$ 100 km, from the pulsar. ","Pulsating in unison at optical and X-ray energies: simultaneous
  high-time resolution observations of the transitional millisecond pulsar PSR
  J1023+0038"
34,1120683469150011393,702241209276829697,Cecilia Garraffo 💚,['Check out our new paper on stellar coronal mass ejections led by @SofiaMoschou: <LINK> Masses follow the solar trend but with lower kinetic energies. @cosmodrake @AstroRaikoh'],https://arxiv.org/abs/1904.09598,"Solar CMEs and flares have a statistically well defined relation, with more energetic X-ray flares corresponding to faster and more massive CMEs. How this relation extends to more magnetically active stars is a subject of open research. Here, we study the most probable stellar CME candidates associated with flares captured in the literature to date, all of which were observed on magnetically active stars. We use a simple CME model to derive masses and kinetic energies from observed quantities, and transform associated flare data to the GOES 1--8~\AA\ band. Derived CME masses range from $\sim 10^{15}$ to $10^{22}$~g. Associated flare X-ray energies range from $10^{31}$ to $10^{37}$~erg. Stellar CME masses as a function of associated flare energy generally lie along or below the extrapolated mean for solar events. In contrast, CME kinetic energies lie below the analogous solar extrapolation by roughly two orders of magnitude, indicating approximate parity between flare X-ray and CME kinetic energies. These results suggest that the CMEs associated with very energetic flares on active stars are more limited in terms of the ejecta velocity than the ejecta mass, possibly because of the restraining influence of strong overlying magnetic fields and stellar wind drag. Lower CME kinetic energies and velocities present a more optimistic scenario for the effects of CME impacts on exoplanets in close proximity to active stellar hosts. ",The Stellar CME-flare relation: What do historic observations reveal?
35,1120661119822114818,935291763362877440,Jeremy Drake 🇺🇦,"['Candidate coronal mass ejection events from the literature analysed in a new paper lead by @SofiaMoschou It looks like CME masses follow extrapolated solar CME trends, but kinetic energies are lower.\n<LINK> <LINK>']",https://arxiv.org/abs/1904.09598,"Solar CMEs and flares have a statistically well defined relation, with more energetic X-ray flares corresponding to faster and more massive CMEs. How this relation extends to more magnetically active stars is a subject of open research. Here, we study the most probable stellar CME candidates associated with flares captured in the literature to date, all of which were observed on magnetically active stars. We use a simple CME model to derive masses and kinetic energies from observed quantities, and transform associated flare data to the GOES 1--8~\AA\ band. Derived CME masses range from $\sim 10^{15}$ to $10^{22}$~g. Associated flare X-ray energies range from $10^{31}$ to $10^{37}$~erg. Stellar CME masses as a function of associated flare energy generally lie along or below the extrapolated mean for solar events. In contrast, CME kinetic energies lie below the analogous solar extrapolation by roughly two orders of magnitude, indicating approximate parity between flare X-ray and CME kinetic energies. These results suggest that the CMEs associated with very energetic flares on active stars are more limited in terms of the ejecta velocity than the ejecta mass, possibly because of the restraining influence of strong overlying magnetic fields and stellar wind drag. Lower CME kinetic energies and velocities present a more optimistic scenario for the effects of CME impacts on exoplanets in close proximity to active stellar hosts. ",The Stellar CME-flare relation: What do historic observations reveal?
36,1120595746376560641,719928410814955520,Evgenii Zheltonozhskii,"['Our new paper, ""Towards Learning of Filter-Level Heterogeneous Compression of CNNs"": <LINK>\ntl;dr:  we played with differentiable NAS for network compression (quantization  and pruning). It turned out to be tricky, unstable and still requires tons of resources. <LINK>']",https://arxiv.org/abs/1904.09872,"Recently, deep learning has become a de facto standard in machine learning with convolutional neural networks (CNNs) demonstrating spectacular success on a wide variety of tasks. However, CNNs are typically very demanding computationally at inference time. One of the ways to alleviate this burden on certain hardware platforms is quantization relying on the use of low-precision arithmetic representation for the weights and the activations. Another popular method is the pruning of the number of filters in each layer. While mainstream deep learning methods train the neural networks weights while keeping the network architecture fixed, the emerging neural architecture search (NAS) techniques make the latter also amenable to training. In this paper, we formulate optimal arithmetic bit length allocation and neural network pruning as a NAS problem, searching for the configurations satisfying a computational complexity budget while maximizing the accuracy. We use a differentiable search method based on the continuous relaxation of the search space proposed by Liu et al. (arXiv:1806.09055). We show, by grid search, that heterogeneous quantized networks suffer from a high variance which renders the benefit of the search questionable. For pruning, improvement over homogeneous cases is possible, but it is still challenging to find those configurations with the proposed method. The code is publicly available at this https URL and this https URL ","Towards Learning of Filter-Level Heterogeneous Compression of
  Convolutional Neural Networks"
37,1120583308096737280,16357083,John See,"[""I'm happy to share a forthcoming work in #CVPR 2019 on a new one-stage object detector based on average precision loss done in collaboration with SJTU and Tencent YouTu Lab. \nPaper : <LINK>  #cvpr2019 #objectdetection <LINK>""]",https://arxiv.org/abs/1904.06373,"One-stage object detectors are trained by optimizing classification-loss and localization-loss simultaneously, with the former suffering much from extreme foreground-background class imbalance issue due to the large number of anchors. This paper alleviates this issue by proposing a novel framework to replace the classification task in one-stage detectors with a ranking task, and adopting the Average-Precision loss (AP-loss) for the ranking problem. Due to its non-differentiability and non-convexity, the AP-loss cannot be optimized directly. For this purpose, we develop a novel optimization algorithm, which seamlessly combines the error-driven update scheme in perceptron learning and backpropagation algorithm in deep networks. We verify good convergence property of the proposed algorithm theoretically and empirically. Experimental results demonstrate notable performance improvement in state-of-the-art one-stage detectors based on AP-loss over different kinds of classification-losses on various benchmarks, without changing the network architectures. Code is available at this https URL ",Towards Accurate One-Stage Object Detection with AP-Loss
38,1120498893832237057,989251872107085824,Quoc Le,"['Exciting new work on replacing convolutions with self-attention for vision. Our paper shows that full attention is good, but loses a few percents in accuracy. And a middle ground that combines convolutions and self-attention is better. Link: <LINK> <LINK>', '@gwern @lorenlugosch I will check caption 11, but I totally agree with @gwern']",https://arxiv.org/abs/1904.09925,"Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a $1.3\%$ top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline. ",Attention Augmented Convolutional Networks
39,1120185554111430656,1454878428,Avishai Gilkis,"[""Happy to share this new paper, with @astro_jje: How to get rid of your star's hydrogen in three easy steps! But be careful what you assume for step 3... 🌟\n<LINK>"", '@norhaslizayusof @astro_jje The Schwarzschild criterion was used for another set of models which are mentioned only briefly in the discussion. More models with the Nugis &amp; Lamers prescription then retain hydrogen, probably because of the interplay of mixing and the dependence on surface helium abundance.']",https://arxiv.org/abs/1904.09221,"We find that applying a theoretical wind mass-loss rate from Monte Carlo radiative transfer models for hydrogen-deficient stars results in significantly more leftover hydrogen following stable mass transfer through Roche-lobe overflow than when we use an extrapolation of an empirical fit for Galactic Wolf-Rayet stars, for which a negligible amount of hydrogen remains in a large set of binary stellar evolution computations. These findings have implications for modelling progenitors of Type Ib and Type IIb supernovae. Most importantly, our study stresses the sensitivity of the stellar evolution models to the assumed mass-loss rates and the need to develop a better theoretical understanding of stellar winds. ","Effects of winds on the leftover hydrogen in massive stars following
  Roche lobe overflow"
40,1120133724253900800,20703003,Peter B Denton,"['We break from your regularly scheduled neutrino based programming to bring you the following new paper with Hooman Davoudiasl wherein we use:\n\nLight from a Black Hole to Constrain Fuzzy Dark Matter\n\n(what a weird phrase to type out)\n\n<LINK>\n\na thread 1/8', ""The @ehtelescope recently made headlines around the world with this stunning image (the fuzzy orange donut picture) of M87*, the Super Massive Black Hole at the Center of M87, a relatively nearby galaxy.\n\nBut what next? What do we now know that we didn't before?\n\n2/8 https://t.co/S3sOtlJupj"", ""The EHT measurement allows for the inference of various properties: mass, spin, and accretion rate, among other things. Some of these we already had some idea about via other means, some we didn't. But now we have enough information to do particle physics in space with M87*!\n\n3/8"", 'Rotating black holes are even weirder than just plain ol black holes. When a black hole rotates, it is possible to create particles out of the vacuum. Energy is conserved as it is extracted from the rotation of a black hole, but this process causes black holes to spin down.\n\n4/8', ""*IF* this process can happen, it will no matter what. It'll cause the black hole to spin down. If a black hole is measured with large spin (*cough* M87* *cough*) then you can infer that this process can't be happening. So what are the conditions for this to happen?\n\n5/8"", ""There has to be a particle with a characteristic size (de Broglie wavelength) comparable to the size of the black hole. What is awesome about this probe, is that it doesn't require the particle to be present, just that it *could* exist. The size of M87* maps onto 10^-21 eV.\n\n6/8"", '10^-21 eV is a tiny mass, but this is exactly the regime that is interesting for fuzzy dark matter. While some other constraints exist, this is completely independent and in a slightly different region. Plus it uses black holes to probe dark matter which is pretty awesome!\n\n7/8', 'Finally, the money plot (thanks for reading way down here!). The gray band is roughly the region of interest for dark matter. The orange (vector) and blue (scalar) bands are values ruled out by this analysis. Previous constraints such as Ark 120 only cover larger masses.\n\n8/8 https://t.co/IChIKqTKMK']",https://arxiv.org/abs/1904.09242,"The initial data from the Event Horizon Telescope (EHT) on M87$^*$, the supermassive black hole at the center of the M87 galaxy, provide direct observational information on its mass, spin, and accretion disk properties. A combination of the EHT data and other constraints provide evidence that M87$^*$ has a mass $\sim 6.5 \times 10^9\,M_\odot$ and dimensionless spin parameter $|a^*|\gtrsim 0.5$. These determinations disfavor ultra light bosons of mass $\mu_b\sim 10^{-21}$ eV, within the range considered for fuzzy dark matter, invoked to explain dark matter distribution on $\sim$ kpc scales. Future observations of M87$^*$ could be expected to strengthen our conclusions. ","Ultra Light Boson Dark Matter and Event Horizon Telescope Observations
  of M87*"
41,1120121208920363009,11778512,Mason Porter,"['My new paper with Heather Brooks (@HZinnbrooks): ""A Model for the Influence of Media on the Ideology of Content in Online Social Networks"": <LINK>\n\n""[Our] model can produce distinct communities (""echo chambers"") that are polarized in both ideology and quality."" <LINK>']",https://arxiv.org/abs/1904.09238,"Many people rely on online social networks as sources of news and information, and the spread of media content with ideologies across the political spectrum influences online discussions and impacts actions offline. To examine the impact of media in online social networks, we generalize bounded-confidence models of opinion dynamics by incorporating media accounts as influencers in a network. We quantify partisanship of content with a continuous parameter on an interval, and we formulate higher-dimensional generalizations to incorporate content quality and increasingly nuanced political positions. We simulate our model with one and two ideological dimensions, and we use the results of our simulations to quantify the ""entrainment"" of content from non-media accounts to the ideologies of media accounts in a network. We maximize media impact in a social network by tuning the number of media accounts that promote the content and the number of followers of the accounts. Using numerical computations, we find that the entrainment of the ideology of content spread by non-media accounts to media ideology depends on a network's structural features, including its size, the mean number of followers of its nodes, and the receptiveness of its nodes to different opinions. We then introduce content quality --- a key novel contribution of our work --- into our model. We incorporate multiple media sources with ideological biases and quality-level estimates that we draw from real media sources and demonstrate that our model can produce distinct communities (""echo chambers"") that are polarized in both ideology and quality. Our model provides a step toward understanding content quality and ideology in spreading dynamics, with ramifications for how to mitigate the spread of undesired content and promote the spread of desired content. ","A Model for the Influence of Media on the Ideology of Content in Online
  Social Networks"
42,1119652750453874688,84871401,Marco Peressotti,"['Taking Linear Logic Apart, a new paper with @wenkokke and @famontesi is out <LINK> \nWe introduce a reduction semantics for Hypersequent Classical Processes <LINK> the first process calculus with a logical characterisation of parallel composition! <LINK>']",https://arxiv.org/abs/1904.06848v1,"Process calculi based on logic, such as $\pi$DILL and CP, provide a foundation for deadlock-free concurrent programming. However, in previous work, there is a mismatch between the rules for constructing proofs and the term constructors of the $\pi$-calculus: the fundamental operator for parallel composition does not correspond to any rule of linear logic. Kokke et al. (2019) introduced Hypersequent Classical Processes (HCP), which addresses this mismatch using hypersequents (collections of sequents) to register parallelism in the typing judgements. However, the step from CP to HCP is a big one. As of yet, HCP does not have reduction semantics, and the addition of delayed actions means that CP processes interpreted as HCP processes do not behave as they would in CP. We introduce HCP-, a variant of HCP with reduction semantics and without delayed actions. We prove progress, preservation, and termination, and show that HCP- supports the same communication protocols as CP. ",] Taking Linear Logic Apart
43,1119588717059104769,1090429279513522178,Vitor Possebom,"['💡Working Paper Update💡I just uploaded a new version of ""Sharp Bounds for the Marginal Treatment Effect (MTE) with Sample Selection"" (<LINK>). Using my partial identification strategy, I analyze the Job Corps Training Program (JCTP) and bound its MTE (figure). <LINK>', 'The lower bound is positive, but it is statistically significant only when the latent heterogeneity is between 0.35 and 0.73. Since the ATT is between $.33 and $.99 and the ATU between $.71 and $3.00, there is some unobserved constraint blocking agents who would benefit.', 'Since my paper is mostly theoretical, analyzing why agents who would benefit from attending the JCTP are not doing so is beyond its scope. But it is an important policy question!', 'Chen, Flores and @A_FloresLagunes (2017, https://t.co/dQe0Ae2FJy) argue that it may be due to lack of childcare services, incomplete information, overconfidence or personal preferences for non-enrollment', 'If you like Monte Carlos, the proposed confidence intervals are (mostly) conservative for the MTE function when the estimated parametric model is correctly specified (Designs 1-3). When the estimated model is misspecified (Designs 4-6), there is undercoverage, https://t.co/7zZ4P9Q2Ww']",https://arxiv.org/abs/1904.08522,"I analyze treatment effects in situations when agents endogenously select into the treatment group and into the observed sample. As a theoretical contribution, I propose pointwise sharp bounds for the marginal treatment effect (MTE) of interest within the always-observed subpopulation under monotonicity assumptions. Moreover, I impose an extra mean dominance assumption to tighten the previous bounds. I further discuss how to identify those bounds when the support of the propensity score is either continuous or discrete. Using these results, I estimate bounds for the MTE of the Job Corps Training Program on hourly wages for the always-employed subpopulation and find that it is decreasing in the likelihood of attending the program within the Non-Hispanic group. For example, the Average Treatment Effect on the Treated is between \$.33 and \$.99 while the Average Treatment Effect on the Untreated is between \$.71 and \$3.00. ",Sharp Bounds for the Marginal Treatment Effect with Sample Selection
44,1119347697822130177,846041727232331776,Roei Herzig,"['Excited to share our new CVPR19 paper, <LINK>, on 𝐩𝐫𝐞𝐜𝐢𝐬𝐞 𝐨𝐛𝐣𝐞𝐜𝐭 𝐝𝐞𝐭𝐞𝐜𝐭𝐢𝐨𝐧. \n\nCode &amp; dataset are on <LINK>!\n\n#CVPR19 #ObjectDetection #TraxRetail #BIU <LINK>', 'We collected a new SKU-110K dataset which takes detection challenges to unexplored territories: millions of possible facets; hundreds of heavily crowded objects per image.', 'We propose a novel mechanism to learn deep overlap rates for each detection, and use an accurate clustering algorithm to resolve duplicates.', 'Research done in collaboration with @TraxRetail, Prof. Tal Hassner, Prof. Jacob Goldberger,  Eran Goldman, and Aviv Eisenschtat. We wish to express our gratitude for Dr. Ziv Mhabary and Dr. Yair Adato from Trax Research Group for their essential support in this work.']",https://arxiv.org/abs/1904.00853,"Man-made scenes can be densely packed, containing numerous objects, often identical, positioned in close proximity. We show that precise object detection in such scenes remains a challenging frontier even for state-of-the-art object detectors. We propose a novel, deep-learning based method for precise object detection, designed for such challenging settings. Our contributions include: (1) A layer for estimating the Jaccard index as a detection quality score; (2) a novel EM merging unit, which uses our quality scores to resolve detection overlap ambiguities; finally, (3) an extensive, annotated data set, SKU-110K, representing packed retail environments, released for training and testing under such extreme settings. Detection tests on SKU-110K and counting tests on the CARPK and PUCPR+ show our method to outperform existing state-of-the-art with substantial margins. The code and data will be made available on \url{www.github.com/eg4000/SKU110K_CVPR19}. ",Precise Detection in Densely Packed Scenes
45,1119097272824782849,3245949691,Rebecca Leane,"['New paper out today!\n\nDark Matter Strikes Back at the Galactic Center\n<LINK>\n\nWe show that dark matter annihilation might explain the excess of gamma rays detected at the center of our galaxy, after all.\nMega-thread explaining our results and backstory below!', '1/ Back in 2009, Dan Hooper (@DanHooperAstro) and Lisa Goodenough used publicly available data from the Fermi Gamma-Ray Space Telescope (@NASAFermi) to examine gamma rays (very high-energy light) from the center of our Milky Way galaxy.', '2/ They found an excess of gamma rays with energies a billion times that of visible light. Since then, there has been extensive debate over the origin of these mysterious excess high-energy gamma rays.', '3/ An exciting possibility is that we are seeing the first signal of annihilating dark matter (DM) - DM interacting with visible matter. This is an appealing explanation because:', '4/  1. The center of the galaxy has a lot of gravitational pull, and DM likely accumulates there, where it could annihilate to produce visible particles we can detect with telescopes,', ""5/ 2. The excess is approximately spherically symmetric around the galactic center, unlike most signals from visible matter, which look more like the disk of our galaxy. This is close to what's expected from DM annihilation if DM density follows one common estimate (called NFW)"", '6/ 3. The DM annihilation rate required to produce the excess matches the annihilation rate DM would need to have in the early universe to produce the amount of DM we observe in the universe today (assuming it is a type of DM called a thermal WIMP).', '7/ The leading alternative hypothesis is that the excess is produced by millisecond pulsars (old stars spinning with millisecond periods). This is a popular alternative as pulsars can produce a comparable energy spectrum of gamma-rays to what we see in the excess.', ""8/ This is a good possible explanation, but there are still questions as to how you get the right number and distribution of pulsars to explain the excess. The point is, we really don't know the answer."", ""9/ So, how do we tell these hypotheses apart? If it were pulsars, can't we just measure their gamma-rays and and compare directly? Well, none of the pulsars we have detected so far explain this excess; it can't be coming from bright pulsars."", '10/ If there are many faint pulsars that are too dim to be detected individually, there could be enough of them to collectively contribute enough gamma-rays to explain the excess. But they are too faint to see, so we need a technique to characterize them.', '11/ An excellent method was introduced in 2011 by Malyshev and Hogg, and was first applied to this excess in 2015 by Lee, Lisanti, Safdi, Slatyer &amp; Xue, which is called ""Non-Poissonian template fitting"" (NPTF).', '12/ Traditional template fitting builds up a picture of the sky as a combination of ""templates"", each corresponding to a particular spatial distribution of gamma rays. We can calculate which combination gives the best description of the data.', '13/ NPTF is an extension of this which exploits the fact that the statistics of gamma-rays produced by DM vs pulsars can be vastly different.', '14/ That is, in a given pixel of the sky, the variance in the number of photons can be much higher from pulsars, as you may have several pulsars in a pixel _all_ giving you gamma-rays - or zero pulsars, giving you none. In contrast, DM can be everywhere.', '15/ This increased variance doesn\'t apply just to pulsars, but to any ""point sources"" (PSs) of gamma-rays - individual gamma-ray-emitting sources.', '16/ Ok, so, you can add a ""NFW DM template"", can see how much of the gamma rays are attributed to DM, compared to PSs distributed the same way (an ""NFW PS"" template), based on the fact that they can make different predictions for the number of gamma-rays observed.', '17/ (Recall NFW is just the profile shape which matches the emission of excess gamma rays, so we are comparing really if DM or PS is the better fit to the excess.)', '18/  In 2015 when NPTF was first applied, the result was as follows: evidence for a PS population was found, with high statistical significance, and the DM contribution was consistent with zero. https://t.co/FTIj5b9DYV', '19/ We asked, what is driving this preference? We guessed that if there are some PSs really present in the data, but not described by the templates we are using, it is possible that this could bias the result away from the DM explanation.', '20/ We set out to test this in two ways: (i) using simulated data, and (ii) using the real Fermi data.', ""21/ We first simulated data, putting PSs in a large structure in our galaxy called the Fermi Bubbles. It wasn't known before if PSs are in the Bubbles, so they are the new ingredient for testing a bias. We then simulated a DM signal, as well as all the backgrounds."", ""22/We then analyzed the simulated data with the standard templates (the ones to make the 2015 figure above,where there is no DM found).This does not include a Bubbles PS template.When we do this, even though we simulated DM, it's not found, and is instead misattributed to NFW PS! https://t.co/ZjYBI3YxEd"", '23/ We added even more DM flux in our simulation, and even more flux went into the PSs!! https://t.co/UZUbHrI5Cp', ""24/ Amazing. So, next we looked for PSs in the Fermi Bubbles for real in the Fermi data, but they aren't there. So, this exact example isn't happening in the data, but serves as a proof-of-principle example of how a DM signal can hide behind other unmodeled PS distributions."", ""25/We then wanted to see if something like this is happening in the real data. Of course in the real data you don't know the true distributions of all the faint PSs, which is a complication. But, in the simulated data, we found that adding more DM kept pushing up the flux in PSs."", '26/ So, if this sort of biasing effect is present in the real data, it is likely not saturated in its ability to absorb DM flux. So we injected an artifical DM signal to the real data, to see if it was recovered, or if it was _also_ misattributed to PSs.', '27/ In the real Fermi data... it was also misattributed to PS!! https://t.co/m6Gh3O5HeY', '28/ We add more and more, still totally misattributed until at least a factor 5 larger than the excess size itself is injected! https://t.co/wCgvQ0hMyk', '29/ What do we learn from this? We could have DM explaining this excess, and previous studies using the NPTF to disentangle what was producing the excess might have missed it. DM might fully explain the excess, after all.', '30/ To be clear, this is not positive evidence for dark matter, but evidence of a biasing effect which can drive down the DM flux, and push up PSs. This bias can hide a real DM signal! DM is back in the game, though maybe it is still PS. Lots of interesting follow-ups to be done.', 'I also want to say a massive thanks to my awesome collaborator Tracy Slatyer!!', '@QuantumMessage Thanks Djuna!!']",https://arxiv.org/abs/1904.08430,"Statistical evidence has previously suggested that the Galactic Center GeV Excess (GCE) originates largely from point sources, and not from annihilating dark matter. We examine the impact of unmodeled source populations on identifying the true origin of the GCE using non-Poissonian template fitting (NPTF) methods. In a proof-of-principle example with simulated data, we discover that unmodeled sources in the Fermi Bubbles can lead to a dark matter signal being misattributed to point sources by the NPTF. We discover striking behavior consistent with a mismodeling effect in the real Fermi data, finding that large artificial injected dark matter signals are completely misattributed to point sources. Consequently, we conclude that dark matter may provide a dominant contribution to the GCE after all. ",Dark Matter Strikes Back at the Galactic Center
46,1119070873980325889,141440459,Rod Van Meter 🌻,"['New paper to distract you from that 400-page report:\nTakaaki Matsuo, Clement Durand, and yours truly on quantum networking:\n<LINK>', '@ddp ...and four?']",https://arxiv.org/abs/1904.08605,"Establishing end-to-end quantum connections requires quantified link characteristics, and operations need to coordinate decision-making between nodes across a network. We introduce the RuleSet-based communication protocol for supporting quantum operations over distant nodes to minimize classical packet transmissions for guaranteeing synchronicity. RuleSets are distributed to nodes along a path at connection set up time, and hold lists of operations that need to be performed in real time. We simulate the RuleSet-based quantum link bootstrapping protocol, which consists of recurrent purifications and link-level tomography, to quantify the quantum link fidelity and its throughput. Our Markov-Chain Monte-Carlo simulation includes various error sources, such as the memory error, gate error and channel error, modeled on currently available hardware. We found that when two quantum nodes, each with 100 memory qubits capable of emitting photons ideally to the optical fiber, are physically connected with a 10km MeetInTheMiddle link, the Recurrent Single selection - Single error purification (RSs-Sp) protocol is capable of bringing up the fidelity from an average input $F_{r}=0.675$ to around $F_{r}=0.865$ with a generation rate of 1106 Bell pairs per second, as determined by simulated tomography. The system gets noisier with longer channels, in which case errors may develop faster than the purification gain. In such a situation, a stronger purification method, such as the double selection-based purification, shows an advantage for improving the fidelity. The knowledge acquired from bootstrapping can later be distributed to nodes within the same network, and used for other purposes such as route selection. ",Quantum link bootstrapping using a RuleSet-based communication protocol
47,1118855573469048832,121804256,Emmanuel Kahembwe,"['Research note: Dynamic evaluation also works for Transformer models. It improves Transformer-XL language models, achieving new SOTA on enwik8 (0.99 to 0.94 bpc), text8 (1.08 to 1.04 bpc) and WT103 (18.3 to 16.4 ppl). \n\nPaper: <LINK>\nCode: <LINK>', 'Transformers by themselves are not fully capable of capturing locally re-occurring patterns in language, indicating room for improvement in LM architectures.']",https://arxiv.org/abs/1904.08378,"This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation. Transformers use stacked layers of self-attention that allow them to capture long range dependencies in sequential data. Dynamic evaluation fits models to the recent sequence history, allowing them to assign higher probabilities to re-occurring sequential patterns. By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points. ",Dynamic Evaluation of Transformer Language Models
48,1118796817448222720,1045330821278838784,@GermanRodrigoCSIC,"['We have a new paper today with interesting new results on ""Causality,   unitarity thresholds, anomalous thresholds and infrared singularities   from the loop-tree duality at higher orders"" <LINK>  @IFICorpuscular <LINK>']",http://arxiv.org/abs/arXiv:1904.08389,"We present the first comprehensive analysis of the unitarity thresholds and anomalous thresholds of scattering amplitudes at two loops and beyond based on the loop-tree duality, and show how non-causal unphysical thresholds are locally cancelled in an efficient way when the forest of all the dual on-shell cuts is considered as one. We also prove that soft and collinear singularities at two loops and beyond are restricted to a compact region of the loop three-momenta, which is a necessary condition for implementing a local cancellation of loop infrared singularities with the ones appearing in real emission; without relying on a subtraction formalism. ","Causality, unitarity thresholds, anomalous thresholds and infrared
  singularities from the loop-tree duality at higher orders"
49,1118787861543882752,2541941466,Alba Cervera-Lierta,['New paper on arXiv! 📃\nQuantum circuits for maximally entangled states: a hard but necessary test for a quantum computer. In collaboration with Dardo Goyeneche and José Ignacio Latorre @j_i_latorre\n<LINK>'],https://arxiv.org/abs/1904.07955,"We design a series of quantum circuits that generate absolute maximally entangled (AME) states to benchmark a quantum computer. A relation between graph states and AME states can be exploited to optimize the structure of the circuits and minimize their depth. Furthermore, we find that most of the provided circuits obey majorization relations for every partition of the system, and at every step of the quantum computation. The rational for our approach is to benchmark quantum computers with maximal useful entanglement, which can be used to implement multipartite quantum protocols. ",Quantum circuits for maximally entangled states
50,1118672088846491648,2603024598,Ricardo Pérez-Marco,"['New paper with @CGrunspan using a killer combinatorial approach to Selfish Mining in #Bitcoin and #Ethereum networks, using Dyck words and Catalan distributions.. \n\n""Selfish Mining and Dyck Words in Bitcoin and Ethereum Networks""\n\n<LINK> <LINK>']",https://arxiv.org/abs/1904.07675,"The main goal of this article is to present a direct approach for the formula giving the long-term apparent hashrates of Selfish Mining strategies using only elementary probabilities and combinatorics, more precisely, Dyck words. We can avoid computing stationary probabilities on Markov chain, nor stopping times for Poisson processes as in previous analysis. We do apply these techniques to other block withholding strategies in Bitcoin, and then, we consider also selfish mining in Ethereum. ",Selfish Mining and Dyck Words in Bitcoin and Ethereum Networks
51,1118514682367741952,1004365363574902784,Kevin J. Kelly,"['One more new paper (but this should be it for a while, I promise!) out today, with my collaborators André de Gouvêa, @StenicoVitti, and Pedro Pasquini.\n\n<LINK>', 'Studying tau leptons and tau neutrinos is notoriously hard, but the @DUNEScience experiment should detect hundreds of them!\n\nWe looked into what kind of physics you can learn by studying tau events. The OPERA Experiment just put out (https://t.co/gV7dSU4kDv) similar results.', ""Not too surprisingly, studying tau neutrinos isn't as powerful as studying electron and muon neutrinos (which is what DUNE is designed for), but taus can provide an important cross check in seeing whether there's new physics lurking in the neutrino sector."", ""Here's a look at how many events the detector will identify as tau neutrinos in 3.5 years of data collection -- this sample alone will be larger than all existing measurements of tau neutrinos to date! https://t.co/Tvn9rUYKkj"", ""Finally, here's how well you can measure the amplitude and frequency of neutrino oscillations using each channel alone (electrons blue, taus green, muons red). Again, the tau neutrino measurement isn't powerful compared to the others, but it's a cross check that needs to be done! https://t.co/RZJp4eFdgy"", ""Again, this couldn't have been done without my collaborators. Thanks André, @StenicoVitti, and Pedro!""]",https://arxiv.org/abs/1904.07265,"We explore the capabilities of the upcoming Deep Underground Neutrino Experiment (DUNE) to measure $\nu_\tau$ charged-current interactions and the associated oscillation probability $P(\nu_\mu \to \nu_\tau)$ at its far detector, concentrating on how such results can be used to probe neutrino properties and interactions. DUNE has the potential to identify significantly more $\nu_\tau$ events than all existing experiments and can use this data sample to nontrivially test the three-massive-neutrinos paradigm by providing complementary measurements to those from the $\nu_e$ appearance and $\nu_\mu$ disappearance channels. We further discuss the sensitivity of the $\nu_\tau$ appearance channel to several hypotheses for the physics that may lurk beyond the three-massive-neutrinos paradigm: a non-unitary lepton mixing matrix, the $3+1$ light neutrinos hypothesis, and the existence of non-standard neutral-current neutrino interactions. Throughout, we also consider the relative benefits of the proposed high-energy tune of the Long-Baseline Neutrino Facility (LBNF) beam-line. ",Physics with Beam Tau-Neutrino Appearance at DUNE
52,1118506090851917824,1158874141,Ben Roberts,"['New paper: DAMA/LIBRA phase2, and implications for GeV scale WIMP dark matter\n<LINK>\n#DarkMatter']",https://arxiv.org/abs/1904.07127,"We investigate the possibility for the direct detection of low mass (GeV scale) WIMP dark matter in scintillation experiments. Such WIMPs are typically too light to leave appreciable nuclear recoils, but may be detected via their scattering off atomic electrons. In particular, the DAMA Collaboration [R. Bernabei et al., Nucl. Phys. At. Energy 19, 307 (2018)] has recently presented strong evidence of an annual modulation in the scintillation rate observed at energies as low as 1 keV. Despite a strong enhancement in the calculated event rate at low energies, we find that an interpretation in terms of electron-interacting WIMPs cannot be consistent with existing constraints. We also demonstrate the importance of correct treatment of the atomic wavefunctions, and show the resulting event rate is very sensitive to the low-energy performance of the detectors, meaning it is crucial that the detector uncertainties be taken into account. Finally, we demonstrate that the potential scintillation event rate can be much larger than may otherwise be expected, meaning that competitive searches can be performed for GeV scale WIMPs using the conventional prompt S1 scintillation signals. This is important given the recent and upcoming very large liquid xenon detectors. ","Electron-interacting dark matter: Implications from DAMA/LIBRA-phase2
  and prospects for liquid xenon detectors and NaI detectors"
53,1118458143422451714,547776192,Chris Lovell,"['New paper klaxon! 📯 \n\nWhere we (@stewilkins et al.) use the Bluetides simulation to predict nebular line emission for z &gt; 8, in anticipation of @NASAWebb\n\n<LINK>\n\nBelow: combined Hβ and [Oiii]λ4959,5007 equivalent widths and stellar mass distribution (Fig.8) <LINK>']",https://arxiv.org/abs/1904.07504v1,"Nebular emission lines associated with galactic HII regions carry information about both physical properties of the ionised gas and the source of ionising photons as well as providing the opportunity of measuring accurate redshifts and thus distances once a cosmological model is assumed. While nebular line emission has been extensively studied at lower redshift there are currently only few constraints within the epoch of reionisation (EoR, $z>6$), chiefly due to the lack of sensitive near-IR spectrographs. However, this will soon change with the arrival of the Webb Telescope providing sensitive near-IR spectroscopy covering the rest-frame UV and optical emission of galaxies in the EoR. In anticipation of Webb we combine the large cosmological hydrodynamical simulation Bluetides with photoionisation modelling to predict the nebular emission line properties of galaxies at $z=8\to 13$. We find good agreement with the, albeit limited, existing direct and indirect observational constraints on equivalent widths though poorer agreement with luminosity function constraints. We also find that the predicted H$\alpha$ - star formation rate calibration differs significantly from commonly assumed values. ",] Nebular Line Emission During the Epoch of Reionization
54,1118414531535962112,4249537197,Christian Wolf,['New paper out by @Quentin_Debard: Learning 3D Navigation Protocols on @itekube Touch Tables with Cooperative Multi-Agent Reinforcement Learning. We learn the user together with the agent. Finetuning on humans to follow. <LINK> (with Jilles Dibangoye and @scanu) <LINK>'],https://arxiv.org/abs/1904.07802,"Using touch devices to navigate in virtual 3D environments such as computer assisted design (CAD) models or geographical information systems (GIS) is inherently difficult for humans, as the 3D operations have to be performed by the user on a 2D touch surface. This ill-posed problem is classically solved with a fixed and handcrafted interaction protocol, which must be learned by the user. We propose to automatically learn a new interaction protocol allowing to map a 2D user input to 3D actions in virtual environments using reinforcement learning (RL). A fundamental problem of RL methods is the vast amount of interactions often required, which are difficult to come by when humans are involved. To overcome this limitation, we make use of two collaborative agents. The first agent models the human by learning to perform the 2D finger trajectories. The second agent acts as the interaction protocol, interpreting and translating to 3D operations the 2D finger trajectories from the first agent. We restrict the learned 2D trajectories to be similar to a training set of collected human gestures by first performing state representation learning, prior to reinforcement learning. This state representation learning is addressed by projecting the gestures into a latent space learned by a variational auto encoder (VAE). ","Learning 3D Navigation Protocols on Touch Interfaces with Cooperative
  Multi-Agent Reinforcement Learning"
55,1118343162156724231,1658162341,Narayanan Rengaswamy,['Our new paper connecting classical Kerdock codes to quantum unitary 2-designs is now online! <LINK> This also includes *logical* unitary 2-designs. See abstract for GitHub link to implementations. @kenbrownquantum @JoshKoomz'],http://arxiv.org/abs/1904.07842,"The non-linear binary Kerdock codes are known to be Gray images of certain extended cyclic codes of length $N = 2^m$ over $\mathbb{Z}_4$. We show that exponentiating these $\mathbb{Z}_4$-valued codewords by $\imath \triangleq \sqrt{-1}$ produces stabilizer states, that are quantum states obtained using only Clifford unitaries. These states are also the common eigenvectors of commuting Hermitian matrices forming maximal commutative subgroups (MCS) of the Pauli group. We use this quantum description to simplify the derivation of the classical weight distribution of Kerdock codes. Next, we organize the stabilizer states to form $N+1$ mutually unbiased bases and prove that automorphisms of the Kerdock code permute their corresponding MCS, thereby forming a subgroup of the Clifford group. When represented as symplectic matrices, this subgroup is isomorphic to the projective special linear group PSL($2,N$). We show that this automorphism group acts transitively on the Pauli matrices, which implies that the ensemble is Pauli mixing and hence forms a unitary $2$-design. The Kerdock design described here was originally discovered by Cleve et al. (arXiv:1501.04592), but the connection to classical codes is new which simplifies its description and translation to circuits significantly. Sampling from the design is straightforward, the translation to circuits uses only Clifford gates, and the process does not require ancillary qubits. Finally, we also develop algorithms for optimizing the synthesis of unitary $2$-designs on encoded qubits, i.e., to construct logical unitary $2$-designs. Software implementations are available at this https URL, which we use to provide empirical gate complexities for up to $16$ qubits. ",Kerdock Codes Determine Unitary 2-Designs
56,1117870820347367424,250846437,Nouha Dziri,"['New work ""Evaluating Coherence in Dialogue Systems using Entailment"" to appear at @NAACLHLT  2019 @ehsk0 @korymath @ozaiane. Our approach measures coherence in dialogs allowing an unbiased and a fast evaluation.\n\nCoda/data: <LINK>\nPaper: <LINK>']",https://arxiv.org/abs/1904.03371,"Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses. ",Evaluating Coherence in Dialogue Systems using Entailment
57,1117841310876979200,767345894,Xiang Ren,"['Happy to share our new work ""Recurrent Event Network for Reasoning over Temporal Knowledge Graphs""(short version accepted to @iclr2019-RLGM): modeling temporal, multi-relational, concurrent interactions in dynamic KGs. \nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/1904.05530,"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-NET), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-NET employs a recurrent event encoder to encode past facts and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RENET, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets. Code and data can be found at this https URL ","Recurrent Event Network: Autoregressive Structure Inference over
  Temporal Knowledge Graphs"
58,1117824818991099904,19740214,Kory Mathewson,"['New work @NAACLHLT 2019 w/ @nouhadziri @ehsk0 @ozaiane\nEntailment is a fast, automated technique to measure coherence in dialogue systems which allows scalable evaluation approximating human judgement\n\n💻 Code and Data: <LINK>\n📝 Paper: <LINK> <LINK>', 'Thank you for the continued support on this work from @AmiiThinks and @ualbertaScience as well as grants supporting graduate work from @NSERC_CRSNG.']",https://arxiv.org/abs/1904.03371,"Evaluating open-domain dialogue systems is difficult due to the diversity of possible correct answers. Automatic metrics such as BLEU correlate weakly with human annotations, resulting in a significant bias across different models and datasets. Some researchers resort to human judgment experimentation for assessing response quality, which is expensive, time consuming, and not scalable. Moreover, judges tend to evaluate a small number of dialogues, meaning that minor differences in evaluation configuration may lead to dissimilar results. In this paper, we present interpretable metrics for evaluating topic coherence by making use of distributed sentence representations. Furthermore, we introduce calculable approximations of human judgment based on conversational coherence by adopting state-of-the-art entailment techniques. Results show that our metrics can be used as a surrogate for human judgment, making it easy to evaluate dialogue systems on large-scale datasets and allowing an unbiased estimate for the quality of the responses. ",Evaluating Coherence in Dialogue Systems using Entailment
59,1117646268220805120,823277120944242689,Will Kinney,"['New paper put today with my excellent students Wei-Chen Lin and Michael Morse. Woot!\n\n<LINK> <LINK>', '*out* today. 😖']",https://arxiv.org/abs/1904.06289,"There has been considerable recent interest in a new class of non-slow roll inflationary solutions known as \textit{constant roll} inflation. Constant roll solutions are a generalization of the ultra-slow roll (USR) solution, where the first Hubble slow roll parameter $\epsilon$ is small, but the second Hubble slow roll parameter $\eta$ is not. While it is known that the USR solutions represent dynamical transients, there has been some disagreement in literature about whether or not large-$\eta$ constant roll solutions are attractors or are also a class of transient solutions. In this paper we show that the large-$\eta$ constant roll solutions do in fact represent transient solutions by performing stability analysis on the exact analytic (large-$\eta$) constant roll solutions. ",Dynamical Analysis of Attractor Behavior in Constant Roll Inflation
60,1116673026517536770,50901426,Rafael Alves Batista,['Our new paper on very-high-energy emission by SgrA*\n<LINK> <LINK>'],https://arxiv.org/abs/1904.05765,"The cosmic-ray (CR) accelerator at the galactic centre (GC) is not yet established by current observations. Here we investigate the radiative-inefficient accretion flow (RIAF) of Sagittarius A* (SgrA*) as a CR accelerator assuming acceleration by turbulent magnetic reconnection, and derive possible emission fluxes of CRs interacting within the RIAF (the central $\sim10^{13}$cm). The target environment of the RIAF is modelled with numerical, general relativistic magneto-hydrodynamics (GRMHD) together with leptonic radiative transfer simulations. The acceleration of the CRs is not computed here. Instead, we inject CRs constrained by the magnetic reconnection power of the accretion flow and compute the emission/absorption of $\gamma$-rays due to these CRs interacting with the RIAF, through Monte Carlo simulations employing the {\tt CRPropa 3} code. The resulting very-high-energy (VHE) fluxes are not expected to reproduce the point source HESS J1745-290 as the emission of this source is most likely produced at pc scales. The emission profiles derived here intend to trace the VHE signatures of the RIAF as a CR accelerator and provide predictions for observations of the GC with improved angular resolution and differential flux sensitivity as those of the forthcoming Cherenkov Telescope Array (CTA). Within the scenario presented here, we find that for mass accretion rates $\gtrsim 10^{-7}$M$_\odot$yr$^{-1}$, the RIAF of SgrA* produces VHE fluxes which are consistent with the H.E.S.S. upper limits for the GC and potentially observable by the future CTA. The associated neutrino fluxes are negligible compared with the diffuse neutrino emission measured by the IceCube. ",VHE Emission from Magnetic Reconnection in the RIAF of SgrA*
61,1116612314159910912,263265637,Dennis Prangle,"['New paper! With Sophie Harbisher and @csgillespie <LINK>. We do high dimensional experimental design using SGD + autodiff. As a quick tractable utility we use the trace of Fisher info, which we prove has a decision theoretic derivation from the Hyvarinen score. <LINK>']",https://arxiv.org/abs/1904.05703,"Most computational approaches to Bayesian experimental design require making posterior calculations repeatedly for a large number of potential designs and/or simulated datasets. This can be expensive and prohibit scaling up these methods to models with many parameters, or designs with many unknowns to select. We introduce an efficient alternative approach without posterior calculations, based on optimising the expected trace of the Fisher information, as discussed by Walker (2016). We illustrate drawbacks of this approach, including lack of invariance to reparameterisation and encouraging designs in which one parameter combination is inferred accurately but not any others. We show these can be avoided by using an adversarial approach: the experimenter must select their design while a critic attempts to select the least favourable parameterisation. We present theoretical properties of this approach and show it can be used with gradient based optimisation methods to find designs efficiently in practice. ","Bayesian experimental design without posterior calculations: an
  adversarial approach"
62,1116371773442883584,2902314813,Jean Kossaifi,"['Checkout our new T-Net paper @cvpr2019 <LINK>\n\nBy parametrizing fully-convolutional nets with a single high order #tensor, we are able to leverage redundancies &amp; get SOTA results on various tasks, with comparatively less parameters. @CVPR #TensorLy #DeepLearning', 'With @AdrianBulat, Georgios Tzimirpoulos and @MajaPantic70']",https://arxiv.org/abs/1904.02698,"Recent findings indicate that over-parametrization, while crucial for successfully training deep neural networks, also introduces large amounts of redundancy. Tensor methods have the potential to efficiently parametrize over-complete representations by leveraging this redundancy. In this paper, we propose to fully parametrize Convolutional Neural Networks (CNNs) with a single high-order, low-rank tensor. Previous works on network tensorization have focused on parametrizing individual layers (convolutional or fully connected) only, and perform the tensorization layer-by-layer separately. In contrast, we propose to jointly capture the full structure of a neural network by parametrizing it with a single high-order tensor, the modes of which represent each of the architectural design parameters of the network (e.g. number of convolutional blocks, depth, number of stacks, input features, etc). This parametrization allows to regularize the whole network and drastically reduce the number of parameters. Our model is end-to-end trainable and the low-rank structure imposed on the weight tensor acts as an implicit regularization. We study the case of networks with rich structure, namely Fully Convolutional Networks (FCNs), which we propose to parametrize with a single 8th-order tensor. We show that our approach can achieve superior performance with small compression rates, and attain high compression rates with negligible drop in accuracy for the challenging task of human pose estimation. ","T-Net: Parametrizing Fully Convolutional Nets with a Single High-Order
  Tensor"
63,1116268083973373953,522358859,Leonardo F de Fibonacci 🦁❤️ 🇺🇦,['New paper on Developable surface patches bounded by NURBS curves\n<LINK> Geometry and ships! I am very much proud of it.'],https://arxiv.org/abs/1904.04603,"In this paper we construct developable surface patches which are bounded by two rational or NURBS curves, though the resulting patch is not a rational or NURBS surface in general. This is accomplished by reparameterizing one of the boundary curves. The reparameterization function is the solution of an algebraic equation. For the relevant case of cubic or cubic spline curves, this equation is quartic at most, quadratic if the curves are Bezier or splines and lie on parallel planes, and hence it may be solved either by standard analytical or numerical methods. ",Developable surface patches bounded by NURBS curves
64,1116128302299672576,1105690148501667841,Shuoyang Ding,"['tl;dr for our new paper ""Parallelizable Stack Long Short-Term Memory"" appearing at NAACL Workshop on Structured Prediction for NLP\n\npreprint: <LINK>\ncode: <LINK>\n\n#NAACL2019 #SPNLP <LINK>']",https://arxiv.org/abs/1904.03409,"Stack Long Short-Term Memory (StackLSTM) is useful for various applications such as parsing and string-to-tree neural machine translation, but it is also known to be notoriously difficult to parallelize for GPU training due to the fact that the computations are dependent on discrete operations. In this paper, we tackle this problem by utilizing state access patterns of StackLSTM to homogenize computations with regard to different discrete operations. Our parsing experiments show that the method scales up almost linearly with increasing batch size, and our parallelized PyTorch implementation trains significantly faster compared to the Dynet C++ implementation. ",Parallelizable Stack Long Short-Term Memory
65,1116114983266463744,45985352,Florian Golemo,"['New paper: “Active Domain Randomization”. A better, safer, and easier-to-debug domain randomization for transferring your #robot policy from #sim2real .\n<LINK>\nWork lead by @SportsballBMan, colab with @linuxpotter @fgolemo, @chrisjpal, and @duckietown_coo <LINK>']",https://arxiv.org/abs/1904.04762,"Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies. ",Active Domain Randomization
66,1115958776820129792,15333148,Gerard de Melo,"[""What is a kiwi? Well, it really depends on the context. Our new #IWCS2019Gothenburg paper shows that sense embeddings outperform BERT's contextualized embeddings on the reverse dictionary task. <LINK> <LINK>""]",https://arxiv.org/abs/1904.01451,"Popular word embedding methods such as word2vec and GloVe assign a single vector representation to each word, even if a word has multiple distinct meanings. Multi-sense embeddings instead provide different vectors for each sense of a word. However, they typically cannot serve as a drop-in replacement for conventional single-sense embeddings, because the correct sense vector needs to be selected for each word. In this work, we study the effect of multi-sense embeddings on the task of reverse dictionaries. We propose a technique to easily integrate them into an existing neural network architecture using an attention mechanism. Our experiments demonstrate that large improvements can be obtained when employing multi-sense embeddings both in the input sequence as well as for the target representation. An analysis of the sense distributions and of the learned attention is provided as well. ",Using Multi-Sense Vector Embeddings for Reverse Dictionaries
67,1115945798225866755,1278434768,Parthasaarathy Sudarsanam,"['Our new paper on ""Recursive speech separation for unknown number of speakers"" is now available on arXiv. Check it out! <LINK>']",https://arxiv.org/abs/1904.03065,"In this paper we propose a method of single-channel speaker-independent multi-speaker speech separation for an unknown number of speakers. As opposed to previous works, in which the number of speakers is assumed to be known in advance and speech separation models are specific for the number of speakers, our proposed method can be applied to cases with different numbers of speakers using a single model by recursively separating a speaker. To make the separation model recursively applicable, we propose one-and-rest permutation invariant training (OR-PIT). Evaluation on WSJ0-2mix and WSJ0-3mix datasets show that our proposed method achieves state-of-the-art results for two- and three-speaker mixtures with a single model. Moreover, the same model can separate four-speaker mixture, which was never seen during the training. We further propose the detection of the number of speakers in a mixture during recursive separation and show that this approach can more accurately estimate the number of speakers than detection in advance by using a deep neural network based classifier. ",Recursive speech separation for unknown number of speakers
68,1115897268127830016,1707692827,Paul McMillan,"['New paper today, for which I helped out with a few distance estimates to stars. @ga_braganca et al. ""Radial abundance gradients in the outer Galactic disk as traced by main-sequence OB stars"", <LINK>', ""Also featuring @LundObservatory's Thomas Bensby (amongst many others), who is the one who got me involved in the first place.""]",https://arxiv.org/abs/1904.04340,"Using a sample of 31 main-sequence OB stars located between galactocentric distances 8.4 - 15.6 kpc, we aim to probe the present-day radial abundance gradients of the Galactic disk. The analysis is based on high-resolution spectra obtained with the MIKE spectrograph on the Magellan Clay 6.5-m telescope on Las Campanas. We used a non-NLTE analysis in a self-consistent semi-automatic routine based on TLUSTY and SYNSPEC to determine atmospheric parameters and chemical abundances. Stellar parameters (effective temperature, surface gravity, projected rotational velocity, microturbulence, and macroturbulence) and silicon and oxygen abundances are presented for 28 stars located beyond 9 kpc from the Galactic centre plus three stars in the solar neighborhood. The stars of our sample are mostly on the main-sequence, with effective temperatures between 20800 - 31300 K, and surface gravities between 3.23 - 4.45 dex. The radial oxygen and silicon abundance gradients are negative and have slopes of -0.07 dex/kpc and -0.09 dex/kpc, respectively, in the region $8.4 \leq R_G \leq 15.6$\,kpc. The obtained gradients are compatible with the present-day oxygen and silicon abundances measured in the solar neighborhood and are consistent with radial metallicity gradients predicted by chemodynamical models of Galaxy Evolution for a subsample of young stars located close to the Galactic plane. ","Radial abundance gradients in the outer Galactic disk as traced by
  main-sequence OB stars"
69,1115800129498312704,1338201043,Koichi Hamaguchi,['New paper:\n<LINK>\nFor me this is the first paper\n- with non-alphabetical-ordered author list. (The ``1st author” is Yanagi-kun! @yana_phys )\n- which does not mention any “Physics Beyond the Standard Model” at all.'],https://arxiv.org/abs/1904.04667,"Recent observations have found several candidates for old warm neutron stars whose surface temperatures are above the prediction of the standard neutron star cooling scenario, and thus require some heating mechanism. Motivated by these observations, we study the non-equilibrium beta process in the minimal cooling scenario of neutron stars, which inevitably occurs in pulsars. This out-of-equilibrium process yields the late time heating in the core of a neutron star, called the rotochemical heating, and significantly changes the time evolution of the neutron star surface temperature. To perform a realistic analysis of this heating effect, we include the singlet proton and triplet neutron pairing gaps simultaneously in the calculation of the rate and emissivity of this process, where the dependence of these pairing gaps on the nucleon density is also taken into account. We then compare the predicted surface temperature of neutron stars with the latest observational data. We show the simultaneous inclusion of both proton and neutron gaps is advantageous for the explanation of the old warm neutron stars since it enhances the heating effect. It is then found that the observed surface temperatures of the old warm millisecond pulsars, J2124-3358 and J0437-4715, are explained for various choices of nucleon gap models. The same setup is compatible with the observed temperatures of ordinary pulsars including old warm ones, J0108-1431 and B0950+08, by choosing the initial rotational period of each neutron star accordingly. In particular, the upper limit on the surface temperature of J2144-3933 can be satisfied if its initial period is $\gtrsim 10\,\mathrm{ms}$. ","Cooling Theory Faced with Old Warm Neutron Stars: Role of
  Non-Equilibrium Processes with Proton and Neutron Gaps"
70,1115750197236133888,2527310100,Dr. Ashutosh Modi,"['How to automate the authoring of interactive agents? Check out our new paper at @aamas2019 : ""Domain Authoring Assistant for Intelligent Virtual Agents"" <LINK>']",https://arxiv.org/abs/1904.03266,"Developing intelligent virtual characters has attracted a lot of attention in the recent years. The process of creating such characters often involves a team of creative authors who describe different aspects of the characters in natural language, and planning experts that translate this description into a planning domain. This can be quite challenging as the team of creative authors should diligently define every aspect of the character especially if it contains complex human-like behavior. Also a team of engineers has to manually translate the natural language description of a character's personality into the planning domain knowledge. This can be extremely time and resource demanding and can be an obstacle to author's creativity. The goal of this paper is to introduce an authoring assistant tool to automate the process of domain generation from natural language description of virtual characters, thus bridging between the creative authoring team and the planning domain experts. Moreover, the proposed tool also identifies possible missing information in the domain description and iteratively makes suggestions to the author. ",Domain Authoring Assistant for Intelligent Virtual Agents
71,1115726179636400128,94236519,Yuki Mitsufuji,['Check out our new paper about recursive speech separation for unknown number of speakers. <LINK>'],https://arxiv.org/abs/1904.03065,"In this paper we propose a method of single-channel speaker-independent multi-speaker speech separation for an unknown number of speakers. As opposed to previous works, in which the number of speakers is assumed to be known in advance and speech separation models are specific for the number of speakers, our proposed method can be applied to cases with different numbers of speakers using a single model by recursively separating a speaker. To make the separation model recursively applicable, we propose one-and-rest permutation invariant training (OR-PIT). Evaluation on WSJ0-2mix and WSJ0-3mix datasets show that our proposed method achieves state-of-the-art results for two- and three-speaker mixtures with a single model. Moreover, the same model can separate four-speaker mixture, which was never seen during the training. We further propose the detection of the number of speakers in a mixture during recursive separation and show that this approach can more accurately estimate the number of speakers than detection in advance by using a deep neural network based classifier. ",Recursive speech separation for unknown number of speakers
72,1115604309993857026,7773042,Yasser Souri,"['""Weakly Supervised Action Segmentation Using Mutual Consistency""\nA new paper by me, @MohsenFyz and our advisor, Juergen Gall.\n<LINK>\nWe propose a new approach for action segmentation using transcripts as the weak supervision. <LINK>', 'Our network produces two redundant representations for action segmentation (frame-level and segment-level representations) and during training requires them to match each other using a new loss function that we term MuCon.\nWe also have a transcript prediction loss (as in seq2seq)', ""The approach is fast during training: doesn't require Viterbi or pseudo ground truth generation like many of the previous works.\nIt is also differentiable, so no heuristic updates of parameters, only SGD.\nAt test time we predict both representations and fuse them for smoothness."", 'Unlike current state-of-the-art methods that are truly only able to perform action alignment at test time and mimic action segmentation by choosing one of the training transcripts, we are able to perform action segmentation at test time.', 'Our work is currently under review. Code is ""coming soon""!\nWe would really appreciate your feedback.\nsouri@iai.uni-bonn.de or tweet at me.']",https://arxiv.org/abs/1904.03116,"Action segmentation is the task of predicting the actions for each frame of a video. As obtaining the full annotation of videos for action segmentation is expensive, weakly supervised approaches that can learn only from transcripts are appealing. In this paper, we propose a novel end-to-end approach for weakly supervised action segmentation based on a two-branch neural network. The two branches of our network predict two redundant but different representations for action segmentation and we propose a novel mutual consistency (MuCon) loss that enforces the consistency of the two redundant representations. Using the MuCon loss together with a loss for transcript prediction, our proposed approach achieves the accuracy of state-of-the-art approaches while being $14$ times faster to train and $20$ times faster during inference. The MuCon loss proves beneficial even in the fully supervised setting. ",Fast Weakly Supervised Action Segmentation Using Mutual Consistency
73,1115579220351123458,5850692,Aaron Roth,"['Excited about our new paper about interactivity in local differential privacy: <LINK> with @mgtjoseph, Jieming Mao, and @datasciwell. We show how to compile fully interactive LDP protocols to sequentially interactive ones, with tight sample complexity blowup. 1/4', ""Here is how to do it. 1) A Bayesian thought experiment. Imagine that before you send a user their next query, you resample them from the posterior distribution on their data given the transcript. This doesn't change the output distribution on transcripts. 2/4"", '2) Rejection sampling. Every time you want to sample someone from their posterior given the transcript, sample a new person (from the prior), and use rejection sampling. With Bayes rule, users themselves have enough information to do this. Now you are sequentially interactive 3/4', ""But your sample complexity is high. So, 3) Reduce the sample complexity with a data-independent decomposition of local randomizers. You only actually need new users rarely. It turns out this combination is optimal and can't be improved in general. 4/4""]",https://arxiv.org/abs/1904.03564,"We study the power of interactivity in local differential privacy. First, we focus on the difference between fully interactive and sequentially interactive protocols. Sequentially interactive protocols may query users adaptively in sequence, but they cannot return to previously queried users. The vast majority of existing lower bounds for local differential privacy apply only to sequentially interactive protocols, and before this paper it was not known whether fully interactive protocols were more powerful. We resolve this question. First, we classify locally private protocols by their compositionality, the multiplicative factor $k \geq 1$ by which the sum of a protocol's single-round privacy parameters exceeds its overall privacy guarantee. We then show how to efficiently transform any fully interactive $k$-compositional protocol into an equivalent sequentially interactive protocol with an $O(k)$ blowup in sample complexity. Next, we show that our reduction is tight by exhibiting a family of problems such that for any $k$, there is a fully interactive $k$-compositional protocol which solves the problem, while no sequentially interactive protocol can solve the problem without at least an $\tilde \Omega(k)$ factor more examples. We then turn our attention to hypothesis testing problems. We show that for a large class of compound hypothesis testing problems --- which include all simple hypothesis testing problems as a special case --- a simple noninteractive test is optimal among the class of all (possibly fully interactive) tests. ",The Role of Interactivity in Local Differential Privacy
74,1115553086116773888,507704346,Isabelle Augenstein,"['Pre-print of our #naacl2019 paper ""Issue Framing in Online Discussion Fora"" is now available. We introduce a new issue frame annotated corpus of online discussions.\nMareike Hartmann, Tallulah Jansen @IAugenstein Anders Søgaard\n<LINK>\n#NLProc @coastalcph @CopeNLU <LINK>']",https://arxiv.org/abs/1904.03969,"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain. ",Issue Framing in Online Discussion Fora
75,1115479296821403648,20369448,Themos Stafylakis,"['Check our new paper on Voice Biometrics (collaboration with Brno University of Technology). We show how to train speaker embedding extractors with self-supervision, enabling the use of unlabelled utterances in training. <LINK>']",https://arxiv.org/abs/1904.03486,"Contrary to i-vectors, speaker embeddings such as x-vectors are incapable of leveraging unlabelled utterances, due to the classification loss over training speakers. In this paper, we explore an alternative training strategy to enable the use of unlabelled utterances in training. We propose to train speaker embedding extractors via reconstructing the frames of a target speech segment, given the inferred embedding of another speech segment of the same utterance. We do this by attaching to the standard speaker embedding extractor a decoder network, which we feed not merely with the speaker embedding, but also with the estimated phone sequence of the target frame sequence. The reconstruction loss can be used either as a single objective, or be combined with the standard speaker classification loss. In the latter case, it acts as a regularizer, encouraging generalizability to speakers unseen during training. In all cases, the proposed architectures are trained from scratch and in an end-to-end fashion. We demonstrate the benefits from the proposed approach on VoxCeleb and Speakers in the wild, and we report notable improvements over the baseline. ",Self-supervised speaker embeddings
76,1115451008350425088,1435944493,Mike Kuhn,"[""Here's a new paper about the Serpens-Aquila Rift that I was involved with. This is our first figure, which we hope will clarify the nomenclature for this region. #GaiaDR2\n<LINK> (@GregHerczeg et al. 2019) <LINK>""]",https://arxiv.org/abs/1904.04085,"The dense clusters within the Serpens Molecular Cloud are among the most active regions of nearby star formation. In this paper, we use Gaia DR2 parallaxes and proper motions to statistically measure 1167 kinematic members of Serpens, few of which were previously identified, to evaluate the star formation history of the complex. The optical members of Serpens are concentrated in three distinct groups located at 380 to 480 pc; the densest clusters are still highly obscured by optically-thick dust and have few optical members. The total population of young stars and protostars in Serpens is at least 2000 stars, including past surveys that were most sensitive to protostars and disks, and may be far higher. Distances to dark clouds measured from deficits in star counts are consistent with the distances to the optical star clusters. The Serpens Molecular Cloud is seen in the foreground of the Aquila Rift, dark clouds located at 600 to 700 pc, and behind patchy extinction, here called the Serpens Cirrus, located at ~250 pc. Based on the lack of a distributed population of older stars, the star formation rate throughout the Serpens Molecular Cloud increased by at least a factor of 20 within the past ~5 Myr. The optically bright stars in Serpens Northeast are visible because their natal molecular cloud has been eroded and not because they were flung outwards from a central factory of star formation. The separation between subclusters of 20 to 100 pc and the absence of an older population leads to speculation that an external forcing was needed to trigger the active star formation. ","An initial overview of the extent and structure of recent star formation
  within the Serpens Molecular Cloud using Gaia Data Release 2"
77,1115436908862042112,2800204849,Andrew Gordon Wilson,['White paper for the new SysML conference: <LINK>'],https://arxiv.org/abs/1904.03257,"Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two. ",MLSys: The New Frontier of Machine Learning Systems
78,1115430058959802368,42536691,sliman bensmaia,"['New paper from the lab, posted on the arXiv, in which we decode time-varying hand postures from population responses in somatosensory and motor cortex: <LINK> with @liza_okorokova  @jimmy_m_goode  and Nicho Hatsopoulos Check it out!']",https://arxiv.org/abs/1904.03531,"The hand, a complex effector comprising dozens of degrees of freedom of movement, endows us with the ability to flexibly, precisely, and effortlessly interact with objects. The neural signals associated with dexterous hand movements in primary motor cortex (M1) and somatosensory cortex (SC) have received comparatively less attention than have those that are associated with proximal limb control. To fill this gap, we trained three monkeys to grasp objects varying in size, shape and orientation while tracking their hand postures and recording single-unit activity from M1 and SC. We then decoded their hand kinematics across 30 joints from population activity in these areas. We found that we could accurately decode kinematics with a small number of neural signals and that performance was higher for decoding joint angles than joint angular velocities, in contrast to what has been found with proximal limb decoders. We conclude that cortical signals can be used for dexterous hand control in brain machine interface applications and that postural representations in SC may be exploited via intracortical stimulation to close the sensorimotor loop. ","Decoding hand kinematics from population responses in sensorimotor
  cortex during grasping"
79,1115394838638129152,30847264,Olexandr Maksymets,"[""Our whitepaper paper @ai_habitat: A Platform for Embodied AI Research went public <LINK>. That's amazing how easy was to evaluate generalization across realistic and simulated environments with new framework.""]",https://arxiv.org/abs/1904.01201,"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents. These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI. ",Habitat: A Platform for Embodied AI Research
80,1115276930524172288,953616889,Justin Read,"['Very excited to announce the launch of our new ""EDGE"" project simulating the smallest galaxies in the Universe. In this first paper, we study what happens when we change the resolution and sub-grid physics for a single low-mass dwarf:\n\n<LINK>\n\n[A thread: 1/N] <LINK>', 'Our simulations (with a resolution of ~3pc) capture the impact of each individual supernova explosion, making us insensitive to our sub-grid physics choices. All models with stellar feedback match the size, luminosity &amp; stellar kinematics of ultra-faint dwarfs.\n\n[2/N] https://t.co/SfrtypqWe3', 'However, the luminosity-metallicity relation is uniquely sensitive to our subgrid physics model. If the supernovae are over/under powered, they eject too many/too few metals, leading to too low/high metallicity. Our fiducial model, however, gets it just right.\n\n[3/N] https://t.co/98fRIekusg', 'There will be lots more to follow from this project; we are just getting started! If the success of these models scales to larger galaxy masses, then we may be well on the road towards truly ""ab-initio"" galaxy formation simulations.\n\n[End]']",https://arxiv.org/abs/1904.02723,"We introduce the ""Engineering Dwarfs at Galaxy formation's Edge"" (EDGE) project to study the cosmological formation and evolution of the smallest galaxies in the Universe. In this first paper, we explore the effects of resolution and sub-grid physics on a single low mass halo ($M_{\rm halo}=10^{9}~M_\odot$), simulated to redshift $z=0$ at a mass and spatial resolution of $\sim 20~M_\odot$ and $\sim 3$ pc. We consider different star formation prescriptions, supernova feedback strengths and on-the-fly radiative transfer (RT). We show that RT changes the mode of galactic self-regulation at this halo mass, suppressing star formation by causing the interstellar and circumgalactic gas to remain predominantly warm ($\sim 10^4$ K) even before cosmic reionisation. By contrast, without RT, star formation regulation occurs only through starbursts and their associated vigorous galactic outflows. In spite of this difference, the entire simulation suite (with the exception of models without any feedback) matches observed dwarf galaxy sizes, velocity dispersions, $V$-band magnitudes and dynamical mass-to-light-ratios. This is because such structural scaling relations are predominantly set by the host dark matter halo, with the remaining model-to-model variation being smaller than the observational scatter. We find that only the stellar mass-metallicity relation differentiates the galaxy formation models. Explosive feedback ejects more metals from the dwarf, leading to a lower metallicity at a fixed stellar mass. We conclude that the stellar mass-metallicity relation of the very smallest galaxies provides a unique constraint on galaxy formation physics. ","EDGE: the mass-metallicity relation as a critical test of galaxy
  formation physics"
81,1115145534036152322,46165258,Shane Steinert-Threlkeld,"['New paper -- first with a student I supervised :) -- to appear at #CMCL2019. ""Neural Models of the Psychosemantics of \'Most\'"": <LINK>.', ""Recurrent models of visual attention exhibit some patterns of human behavior from Pietroski et al's experimental paradigm, including sensitivity to ratio and realistic Weber curves.  Differ in crucial ways as well, e.g. both column types pattern together."", ""I think the initial results are promising.  But there's a lot more to be done!  Much of which we talk about in the last paragraph."", 'Oh, and of course code and data is available: https://t.co/hL1E7usEtn']",https://arxiv.org/abs/1904.02734,"How are the meanings of linguistic expressions related to their use in concrete cognitive tasks? Visual identification tasks show human speakers can exhibit considerable variation in their understanding, representation and verification of certain quantifiers. This paper initiates an investigation into neural models of these psycho-semantic tasks. We trained two types of network -- a convolutional neural network (CNN) model and a recurrent model of visual attention (RAM) -- on the ""most"" verification task from \citet{Pietroski2009}, manipulating the visual scene and novel notions of task duration. Our results qualitatively mirror certain features of human performance (such as sensitivity to the ratio of set sizes, indicating a reliance on approximate number) while differing in interesting ways (such as exhibiting a subtly different pattern for the effect of image type). We conclude by discussing the prospects for using neural models as cognitive models of this and other psychosemantic tasks. ",Neural Models of the Psychosemantics of `Most'
82,1115053823091859456,769188626324398080,Heiga Zen (全 炳河),"['We released a new large-scale corpus of English speech derived for TTS;\n\nLibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech\n\nDataset: <LINK>\nPaper: <LINK>', 'About this resource:\n\nLibriTTS is a multi-speaker English corpus of approximately 585 hours of read English speech at 24kHz sampling rate, prepared by @heiga_zen with the assistance of Google Speech and Google Brain team members. The LibriTTS corpus is designed for TTS research.', 'It is derived from the original materials (mp3 audio files from LibriVox and text files from Project Gutenberg) of the LibriSpeech corpus. The main differences from the LibriSpeech corpus are listed below:', '1. The audio files are at 24kHz sampling rate.\n2. The speech is split at sentence breaks.\n3. Both original and normalized texts are included.\nContextual information (e.g., neighbouring sentences) can be extracted.\n4. Utterances with significant background noise are excluded.', 'For more information, refer to the paper ""LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech"", Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu, arXiv, 2019. \n\nhttps://t.co/utGLNzSfCu', '@cyrta Do you mean text normalization?', '@cyrta Yeah, these texts are normalized by our textnorm engine (WFST-based) and matche audio.  Of course our engine sometimes make mistake.  Then they are not included in the release.  So at least you can use them as a benchmark against our engine.', '@lc0d3r @hardmaru Let me check.']",http://arxiv.org/abs/1904.02882,"This paper introduces a new speech corpus called ""LibriTTS"" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from this http URL ",LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech
83,1115051266038730752,2203468841,Dr Jade Powell,['Our new paper on the impact of pair instability mass loss on the binary black hole mass distribution is out today 😁😁💥@simon4nine <LINK>'],https://arxiv.org/abs/1904.02821,"A population of binary black hole mergers has now been observed in gravitational waves by Advanced LIGO and Virgo. The masses of these black holes appear to show evidence for a pile-up between $30$--$45$ $M_\odot$ and a cut-off above $\sim 45$ $M_\odot$. One possible explanation for such a pile-up and subsequent cut-off are pulsational pair-instability supernovae (PPISNe) and pair-instability supernovae (PISNe) in massive stars. We investigate the plausibility of this explanation in the context of isolated massive binaries. We study a population of massive binaries using the rapid population synthesis software COMPAS, incorporating models for PPISNe and PISNe. Our models predict a maximum black hole mass of $40$ $M_\odot$. We expect $\sim 10$\% of all binary black hole mergers at redshift z = 0 will include at least one component that went through a PPISN (with mass $30$--$40$ $M_\odot$), constituting $\sim 20$--$50$\% of binary black hole mergers observed during the first two observing runs of Advanced LIGO and Virgo. Empirical models based on fitting the gravitational-wave mass measurements to a combination of a power law and a Gaussian find a fraction too large to be associated with PPISNe in our models. The rates of PPISNe and PISNe track the low metallicity star formation rate, increasing out to redshift $z = 2$. These predictions may be tested both with future gravitational-wave observations and with observations of superluminous supernovae. ","The impact of pair-instability mass loss on the binary black hole mass
  distribution"
84,1115019305274548225,43543495,Jelle Zuidema,"['My postdoc, PhD student and RA have a cool new paper on arXiv, at CICLing this Tuesday at 12h10, &amp; as poster on Tuesday at the MPI Nijmegen\n\nLisa Beinborn, Samira Abnar, Rochelle Choenni (2019): Robust evaluation of language-brain encoding experiments, <LINK>\n1/5', ""It's part of a new research line, where we study how well neural language models allow us to predict brain activity -- *cool* because we learn more about the cognitive plausibility of our models!\n\nEg, in early 2018, we studied how well 8 word embeddings models predict fMRI. https://t.co/hODHLo6OmZ"", 'But it is also *hard*, because we need few best practices have been established yet. \n\nPrevious encoding models have typically only been evaluated on a single datasets (&amp; are likely overtuned), code is not published, evaluation measures have not been standardised.', 'Lisa, Samira &amp; Rochelle now use 4 datasets, a constant encoding model, constant experimental parameters, a reasonable baseline AND compare results for multiple metrics &amp; parameters. Results, in many cases, are not\nsignificantly different from chance. Disappointing, but important.', 'Their code is on github: https://t.co/fA7SOSgJkm\n\nThe 2018 paper is also on arXiv: https://t.co/l7OfFV58fI\n\nAnd an associated demo online:\nhttps://t.co/yvMx4b6YMi', ""@yoavgo It's one of those very-good-scores-but-rejected papers from NAACL! And we were too impatient to wait for another xACL deadline -- good new stuff coming up, and eager to move on."", ""@zehavoc @yoavgo Haha, well, 'calm' is not exactly the word that describes my life over the last period. 'Overwhelmed' is a better one, by other commitments and family duties -- so much so that I didn't even deserve a coauthorship on this paper!""]",https://arxiv.org/abs/1904.02547,"Language-brain encoding experiments evaluate the ability of language models to predict brain responses elicited by language stimuli. The evaluation scenarios for this task have not yet been standardized which makes it difficult to compare and interpret results. We perform a series of evaluation experiments with a consistent encoding setup and compute the results for multiple fMRI datasets. In addition, we test the sensitivity of the evaluation measures to randomized data and analyze the effect of voxel selection methods. Our experimental framework is publicly available to make modelling decisions more transparent and support reproducibility for future comparisons. ",Robust Evaluation of Language-Brain Encoding Experiments
85,1114217469281849346,2384462192,BICEP2 (OFFICIAL),['New BICEP2 / Keck Array paper is out: Beam Characterization and Temperature-to-Polarization Leakage in the BK15 Dataset <LINK>'],https://arxiv.org/abs/1904.01640,"Precision measurements of cosmic microwave background (CMB) polarization require extreme control of instrumental systematics. In a companion paper we have presented cosmological constraints from observations with the BICEP2 and Keck Array experiments up to and including the 2015 observing season (BK15), resulting in the deepest CMB polarization maps to date and a statistical sensitivity to the tensor-to-scalar ratio of $\sigma(r) = 0.020$. In this work we characterize the beams and constrain potential systematic contamination from main beam shape mismatch at the three BK15 frequencies (95, 150, and 220 GHz). Far-field maps of 7,360 distinct beam patterns taken from 2010-2015 are used to measure differential beam parameters and predict the contribution of temperature-to-polarization leakage to the BK15 B-mode maps. In the multifrequency, multicomponent likelihood analysis that uses BK15, Planck, and WMAP maps to separate sky components, we find that adding this predicted leakage to simulations induces a bias of $\Delta r = 0.0027 \pm 0.0019$. Future results using higher-quality beam maps and improved techniques to detect such leakage in CMB data will substantially reduce this uncertainty, enabling the levels of systematics control needed for BICEP Array and other experiments that plan to definitively probe large-field inflation. ","BICEP2 / Keck Array XI: Beam Characterization and
  Temperature-to-Polarization Leakage in the BK15 Dataset"
86,1114075199928971264,2317423928,Nora Hollenstein,['Our new paper in collaboration with @coastalcph and @langer_nicolas using #cognitive language processing data (🧠+👀) to improve #NLProc is now on ArXiv: <LINK> <LINK>'],https://arxiv.org/abs/1904.02682,"When we read, our brain processes language and generates cognitive processing data such as gaze patterns and brain activity. These signals can be recorded while reading. Cognitive language processing data such as eye-tracking features have shown improvements on single NLP tasks. We analyze whether using such human features can show consistent improvement across tasks and data sources. We present an extensive investigation of the benefits and limitations of using cognitive processing data for NLP. Specifically, we use gaze and EEG features to augment models of named entity recognition, relation classification, and sentiment analysis. These methods significantly outperform the baselines and show the potential and current limitations of employing human language processing data for NLP. ",Advancing NLP with Cognitive Language Processing Signals
87,1114054798976868352,244251826,Curtis Miller,"['The preprint of the paper ""A new class of change point test statistics of Rényi type"", for which I\'m a co-author, is now publicly available on #arXiv. This will appear in JBES sometime this year. <LINK> #Statistics #researchpaper']",https://arxiv.org/abs/1904.02250,"A new class of change point test statistics is proposed that utilizes a weighting and trimming scheme for the cumulative sum (CUSUM) process inspired by R\'enyi (1953). A thorough asymptotic analysis and simulations both demonstrate that this new class of statistics possess superior power compared to traditional change point statistics based on the CUSUM process when the change point is near the beginning or end of the sample. Generalizations of these ""R\'enyi"" statistics are also developed to test for changes in the parameters in linear and non-linear regression models, and in generalized method of moments estimation. In these contexts we applied the proposed statistics, as well as several others, to test for changes in the coefficients of Fama-French factor models. We observed that the R\'enyi statistic was the most effective in terms of retrospectively detecting change points that occur near the endpoints of the sample. ",A new class of change point test statistics of R\'enyi type
88,1113994344091070465,367297219,Melanie Mitchell,"['New paper from my research group: ""Revisiting Visual Grounding"".  <LINK>. We investigate whether certain visual grounding methods are actually learning what has been claimed.  To appear in Proceedings of Workshop on Shortcomings in Vision and Language, NAACL-2019']",https://arxiv.org/abs/1904.02225,"We revisit a particular visual grounding method: the ""Image Retrieval Using Scene Graphs"" (IRSG) system of Johnson et al. (2015). Our experiments indicate that the system does not effectively use its learned object-relationship models. We also look closely at the IRSG dataset, as well as the widely used Visual Relationship Dataset (VRD) that is adapted from it. We find that these datasets exhibit biases that allow methods that ignore relationships to perform relatively well. We also describe several other problems with the IRSG dataset, and report on experiments using a subset of the dataset in which the biases and other problems are removed. Our studies contribute to a more general effort: that of better understanding what machine learning methods that combine language and vision actually learn and what popular datasets actually test. ",Revisiting Visual Grounding
89,1113972053197983745,30989098,Karin Sandstrom,"['Hey interstellar dust fans!  Check out the new paper from UCSD postdoc Jérémy Chastenet on polycyclic aromatic hydrocarbons in the Magellanic Clouds! <LINK>', 'The SMC is has a much smaller fraction of its dust in the form of PAHs compared to the LMC. https://t.co/UGL8QXdAUu', 'HII regions show up as holes in the PAH fraction map.  Even ionized gas in the diffuse ISM seems to have an effect on the PAH population though.', 'And when you separate the Magellanic Clouds up into their ISM phases, you find that the PAH fraction is largest in molecular/diffuse neutral gas and lowest in HII regions.', '@aprilfollies The lower PAH fraction seems to be related to whatever sources produce ionizing radiation to create the HII regions and diffuse ionized gas.', '@dr_paul_woods I don’t think we know for the MW very well, the 4.6% is for the diffuse neutral gas, not the global average. The comparison to full galaxy qpah values from SINGS suggests the LMC global average is pretty normal, but there are galaxies with higher path fractions.']",https://arxiv.org/abs/1904.02705,"We present maps of the dust properties in the Small and Large Magellanic Clouds (SMC, LMC) from fitting Spitzer and Herschel observations with the \citet{DL07} dust model. We derive the abundance of the small carbonaceous grain (or polycyclic aromatic hydrocarbon; PAH) component. The global PAH fraction (q_pah, the fraction of the dust mass in the form of PAHs) is smaller in the SMC (1.0$^{+0.3}_{-0.3}$%) than in the LMC (3.3$^{+1.4}_{-1.3}$%). We measure the PAH fraction in different gas phases (H II regions, ionized gas outside of H II regions, molecular gas, and diffuse neutral gas). H II regions appear as distinctive holes in the spatial distribution of the PAH fraction. In both galaxies, the PAH fraction in the diffuse neutral medium is higher than in the ionized gas, but similar to the molecular gas. Even at equal radiation field intensity, the PAH fraction is lower in the ionized gas than in the diffuse neutral gas. We investigate the PAH life-cycle as a function of metallicity between the two galaxies. The PAH fraction in the diffuse neutral medium of the LMC is similar to that of the Milky Way ($\sim4.6$%), while it is significantly lower in the SMC. Plausible explanations for the higher PAH fraction in the diffuse neutral medium of the LMC compared to the SMC include: a more effective PAH production by fragmentation of large grains at higher metallicity, and/or the growth of PAHs in molecular gas. ","The Polycyclic Aromatic Hydrocarbon Mass Fraction on a 10 pc scale in
  the Magellanic Clouds"
90,1113902332217970689,847801154473996288,Ilya Razenshteyn,"['Finally, the paper ""SANNS: Scaling Up Secure Approximate k-Nearest Neighbors Search"" I\'ve been working on for almost a year is out: <LINK> We show two new fast algorithms for similarity search with a privacy constraint. More details in the thread! (1/8) <LINK>', 'We solve the following problem: a server has a dataset of points, a client has a query point. The goal is for the client to learn k closest data points in a way that the server learns nothing about the query, and the client learns nothing about the dataset except the answer (2/8)', 'Cryptography readily offers two approaches to such problems: fully homomorphic encryption (FHE) and secure two-party computation (2PC). However, if we simply apply such techniques generically, it will result in highly inefficient protocols. (3/8)', 'Instead, we design two new specialized protocols for secure similarity search that combine FHE, 2PC and oblivious RAM. They are faster than the prior work by at least two orders of magnitude! (4/8)', 'The paper has two algorithmic contributions: first, we design a randomized linear-sized circuit for top-k selection, second, we design a new sublinear-time similarity search algorithm tailored to the ""secure computation"" world. (5/8)', 'The new algorithm performs relatively few *non-adaptive* memory accesses (crucial for ORAM) and computes many distances at once (crucial for exploiting heavy SIMD capabilities of modern FHE protocols). These two features make it much more suitable for secure computation... (6/8)', '... than graph-based approaches that are currently fastest in the ""insecure world"". Finally, we provide a number of optimizations to the cryptographic ingredients, which dramatically improve the concrete efficiency of our protocols. (7/8)', 'This is joint work with @HaoChenMSR, @IChillotti, Yihe Dong, Oxana Poburinnaya and @SadeghRiazi. It is hard to express in words how excited I am to work with such a scientifically diverse team of talented people, from whom I kept learning non-stop! (8/8)', 'cc @Anshumali_ @IgorCarron @Vinod_MIT @jhasomesh @RasmusPagh1 @ERC_SSS @sergey_nog @BeidiChen @erichorvitz @jainprateek_ @StefanoMTessaro @optiML @srchvrs @MSFTResearch', '@jainprateek_ @Anshumali_ @IgorCarron @Vinod_MIT @jhasomesh @RasmusPagh1 @ERC_SSS @sergey_nog @BeidiChen @erichorvitz @StefanoMTessaro @optiML @srchvrs @MSFTResearch I would estimate it as 100x -- 1000x, which is actually good given that:\n\n1. For the secure problem, networking is involved\n2. The insecure version of the problem admits a very efficient implementation that utilizes AVX and whatnot using BLAS or similar libraries']",https://arxiv.org/abs/1904.02033,"The $k$-Nearest Neighbor Search ($k$-NNS) is the backbone of several cloud-based services such as recommender systems, face recognition, and database search on text and images. In these services, the client sends the query to the cloud server and receives the response in which case the query and response are revealed to the service provider. Such data disclosures are unacceptable in several scenarios due to the sensitivity of data and/or privacy laws. In this paper, we introduce SANNS, a system for secure $k$-NNS that keeps client's query and the search result confidential. SANNS comprises two protocols: an optimized linear scan and a protocol based on a novel sublinear time clustering-based algorithm. We prove the security of both protocols in the standard semi-honest model. The protocols are built upon several state-of-the-art cryptographic primitives such as lattice-based additively homomorphic encryption, distributed oblivious RAM, and garbled circuits. We provide several contributions to each of these primitives which are applicable to other secure computation tasks. Both of our protocols rely on a new circuit for the approximate top-$k$ selection from $n$ numbers that is built from $O(n + k^2)$ comparators. We have implemented our proposed system and performed extensive experimental results on four datasets in two different computation environments, demonstrating more than $18-31\times$ faster response time compared to optimally implemented protocols from the prior work. Moreover, SANNS is the first work that scales to the database of 10 million entries, pushing the limit by more than two orders of magnitude. ",SANNS: Scaling Up Secure Approximate k-Nearest Neighbors Search
91,1113792562236620800,236956325,Aaron Rieke,"['New research: Facebook itself can skew the delivery of job and housing ads along race and gender lines, *even when advertisers target broad audiences.*\n\nThis is not what equal opportunity looks like.\n\nOur paper: <LINK>.\n\nKey findings and reflections. 👇\n\n1/n', 'In our experiments, we didn’t target based on race or gender, but Facebook delivered ads for jobs in the lumber industry to an audience that was ~70% white and 90% men, while supermarket cashier positions went to 85% women.\n\n2/n https://t.co/kEnrZD8mwA', 'Housing sale ads were delivered to audiences of ~75% white users, while ads for rentals were shown to a more demographically balanced audience — again, with broad targeting criteria.\n\n3/n https://t.co/JDvSfTRRkx', 'This isn\'t ad targeting. It\'s the work of Facebook\'s ad delivery algorithms, which have learned who the ""right kind"" of people are for different kinds of ads.\n\nIt\'s even crystal clear for music genres:\n\n4/n https://t.co/3OC6uYu68g', ""This is digital marketing working exactly as intended. Add together business incentives, personal data, and machine learning, and this is the expected result.\n\nBut it's not a result we should accept.\n\n5/n"", 'The settlements secured by @NFHA, @ACLU and others were ahead of the curve. Facebook has already made a court-enforceable commitment to study bias in ad delivery and implement feasible reforms. I hope this research informs, and adds healthy pressure to, that process.\n\n7/n', ""It's worth emphasizing that this is almost certainly not unique to Facebook. There are deep, important questions here that the industry as a whole needs to face.\n\nLooking forward to working on solutions in the months ahead.\n\n8/n"", 'Finally, thank you to all the authors: @lukshmichowk, @sapiezynski, @mbogen, @korolova, and @amislove. It was a blast working with you on this.\n\nAnd a shout out to @masooga, whose work at @TeamUpturn in 2018 inspired this research!\n\n9/9']",http://arxiv.org/abs/1904.02095,"The enormous financial success of online advertising platforms is partially due to the precise targeting features they offer. Although researchers and journalists have found many ways that advertisers can target---or exclude---particular groups of users seeing their ads, comparatively little attention has been paid to the implications of the platform's ad delivery process, comprised of the platform's choices about which users see which ads. It has been hypothesized that this process can ""skew"" ad delivery in ways that the advertisers do not intend, making some users less likely than others to see particular ads based on their demographic characteristics. In this paper, we demonstrate that such skewed delivery occurs on Facebook, due to market and financial optimization effects as well as the platform's own predictions about the ""relevance"" of ads to different groups of users. We find that both the advertiser's budget and the content of the ad each significantly contribute to the skew of Facebook's ad delivery. Critically, we observe significant skew in delivery along gender and racial lines for ""real"" ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive. This underscores the need for policymakers and platforms to carefully consider the role of the ad delivery optimization run by ad platforms themselves---and not just the targeting choices of advertisers---in preventing discrimination in digital advertising. ","Discrimination through optimization: How Facebook's ad delivery can lead
  to skewed outcomes"
92,1113743491304239104,4249537197,Christian Wolf,"['New paper: our student @edwardbeeching has created a new RL benchmark in 3D environments, which is visually simple and fast but requires complex reasoning abilities =&gt; train agents on complex tasks with low compute (based on Vizdoom) <LINK> @chroma_inria <LINK>']",https://arxiv.org/abs/1904.01806,"An important goal of research in Deep Reinforcement Learning in mobile robotics is to train agents capable of solving complex tasks, which require a high level of scene understanding and reasoning from an egocentric perspective. When trained from simulations, optimal environments should satisfy a currently unobtainable combination of high-fidelity photographic observations, massive amounts of different environment configurations and fast simulation speeds. In this paper we argue that research on training agents capable of complex reasoning can be simplified by decoupling from the requirement of high fidelity photographic observations. We present a suite of tasks requiring complex reasoning and exploration in continuous, partially observable 3D environments. The objective is to provide challenging scenarios and a robust baseline agent architecture that can be trained on mid-range consumer hardware in under 24h. Our scenarios combine two key advantages: (i) they are based on a simple but highly efficient 3D environment (ViZDoom) which allows high speed simulation (12000fps); (ii) the scenarios provide the user with a range of difficulty settings, in order to identify the limitations of current state of the art algorithms and network architectures. We aim to increase accessibility to the field of Deep-RL by providing baselines for challenging scenarios where new ideas can be iterated on quickly. We argue that the community should be able to address challenging problems in reasoning of mobile agents without the need for a large compute infrastructure. ","Deep Reinforcement Learning on a Budget: 3D Control and Reasoning
  Without a Supercomputer"
93,1113603294453280773,184338062,Andrew Drozdov,"['Now with paper link: <LINK>\nAnd code: <LINK>\n\nNew results on unsupervised parsing: +6.5 F1 compared to ON-LSTM (2019), +6 F1 compared to PRLG (2011). <LINK>', 'Our model learns to predict tokens with a cloze-like objective, incorporating a latent chart parser that allows us to decode constituency parses after training with the CKY algorithm. https://t.co/aHkxTiuF0C', 'Our results improve on previous work. We also use a simple heuristic during inference time that further boosts performance. https://t.co/7qoEWeYI8g', 'We include a handful of examples parses in the paper / appendix, including on some longer examples. Don\'t be offended by the ""box tree"" parse representation — they\'re compact and easy to stack vertically. https://t.co/CNV7Sfbupg', 'Of course, I had a typo in the initial tweet. The full title is: Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive Autoencoders (DIORA) #NAACL2019']",https://arxiv.org/abs/1904.02142,"We introduce deep inside-outside recursive autoencoders (DIORA), a fully-unsupervised method for discovering syntax that simultaneously learns representations for constituents within the induced tree. Our approach predicts each word in an input sentence conditioned on the rest of the sentence and uses inside-outside dynamic programming to consider all possible binary trees over the sentence. At test time the CKY algorithm extracts the highest scoring parse. DIORA achieves a new state-of-the-art F1 in unsupervised binary constituency parsing (unlabeled) in two benchmark datasets, WSJ and MultiNLI. ","Unsupervised Latent Tree Induction with Deep Inside-Outside Recursive
  Autoencoders"
94,1113516983083991040,28889546,Dr. Qusai Al Shidi,"[""I should write a better sci-comm tweet about my new paper: <LINK> \n\nI wrote a code from scratch to solve conditions you'd expect from the dense sun atmosphere called the 'chromosphere'. The approach is to seperate the plasma into two fluids."", 'The charged plasma and uncharged plasma. The uncharged plasma does not feel electromagnetic forces so it provides friction with the charged plasma in presence of magnetic fields.', 'A case we studied which requires further investigation shows a jet can form due to frictional heating instead of magnetic effects only as previously thought. Further use of this code will help understand this further.', '@SofiaMoschou Still in the review process but thank you!']",https://arxiv.org/abs/1904.01572,"The sun's chromosphere is a highly dynamic, partially-ionized region where spicules (hot jets of plasma) form. Here we present a two-fluid MHD model to study the chromosphere, which includes ion-neutral interaction and frictional heating. Our simulation recovers a magnetic canopy shape that forms quickly, but is also quickly disrupted by the formation of a jet. Our simulation produces a shock self-consistently, where the jet is driven by the frictional heating, which is much greater than the ohmic heating. Thus, our simulation demonstrates that the jet could be driven purely by thermal effects due to ion-neutral collisions and not by magnetic reconnection. We plan to improve the model to include photo-chemical effects and radiation. ","Time-Dependent Two-Fluid Magnetohydrodynamic Model and Simulation of the
  Chromosphere"
95,1113482395531993088,4692890148,Dr. Ryan Alimo,['Our new paper on a safe exploration algorithm for deterministic Markov Decision Processes with unknown transition models. @DorsaSadigh @ebiyik94 is now online —&gt;\n<LINK>'],http://arxiv.org/abs/1904.01068,"We propose a safe exploration algorithm for deterministic Markov Decision Processes with unknown transition models. Our algorithm guarantees safety by leveraging Lipschitz-continuity to ensure that no unsafe states are visited during exploration. Unlike many other existing techniques, the provided safety guarantee is deterministic. Our algorithm is optimized to reduce the number of actions needed for exploring the safe space. We demonstrate the performance of our algorithm in comparison with baseline methods in simulation on navigation tasks. ","Efficient and Safe Exploration in Deterministic Markov Decision
  Processes with Unknown Transition Models"
96,1113469610051932160,28889546,Dr. Qusai Al Shidi,"[""My paper is on Arxiv. A new 2fluid collisional mhd model for the sun's chromosphere.\n<LINK>"", ""@brewstronomy I got that elevator pitch version prepared depending on understanding and you'll get to that point too for any of your papers. Unfortunately I'm unsure twitter character limit is condusive.. I should try.""]",https://arxiv.org/abs/1904.01572,"The sun's chromosphere is a highly dynamic, partially-ionized region where spicules (hot jets of plasma) form. Here we present a two-fluid MHD model to study the chromosphere, which includes ion-neutral interaction and frictional heating. Our simulation recovers a magnetic canopy shape that forms quickly, but is also quickly disrupted by the formation of a jet. Our simulation produces a shock self-consistently, where the jet is driven by the frictional heating, which is much greater than the ohmic heating. Thus, our simulation demonstrates that the jet could be driven purely by thermal effects due to ion-neutral collisions and not by magnetic reconnection. We plan to improve the model to include photo-chemical effects and radiation. ","Time-Dependent Two-Fluid Magnetohydrodynamic Model and Simulation of the
  Chromosphere"
97,1113337957145108481,281711973,Dr. Emily Rickman,['Today my first first author paper is on arXiv! 👇\n\n<LINK>\n\nWe have discovered 3 new planets &amp; 2 new low mass brown dwarfs with the CORALIE\nsurvey for extra-solar planets 🌕🌟\n\nWhat is super cool is some of these look really promising for imaging so stay tuned!🔭'],https://arxiv.org/abs/1904.01573,"Context. Since 1998, a planet-search around main sequence stars within 50~pc in the southern hemisphere has been carried out with the CORALIE spectrograph at La Silla Observatory. Aims. With an observing time span of more than 20 years, the CORALIE survey is able to detect long term trends in data with masses and separations large enough to select ideal targets for direct imaging. Detecting these giant companion candidates will allow us to start bridging the gap between radial velocity detected exoplanets and directly imaged planets and brown dwarfs. Methods. Long-term precise Doppler measurements with the CORALIE spectrograph reveal radial velocity signatures of massive planetary companions and brown dwarfs on long-period orbits. Results. In this paper we report the discovery of new companions orbiting HD~181234, HD~13724, HD~25015, HD~92987 and HD~50499. We also report updated orbital parameters for HD~50499b, HD~92788b and HD~98649b. In addition, we confirm the recent detection of HD~92788c. The newly reported companions span a period range of 15.6 to 40.4 years and a mass domain of 2.93 to 26.77 $M_{\mathrm{Jup}}$, the latter of which straddles the nominal boundary between planets and brown dwarfs. Conclusion. We have reported the detection of five new companions and updated parameters of four known extrasolar planets. We identify at least some of these companions to be promising candidates for imaging and further characterisation. ","The CORALIE survey for southern extrasolar planets XVIII. Three new
  massive planets and two low mass brown dwarfs at separation larger than 5 AU"
98,1113287201016696832,4032064210,Sesh Nadathur,"['New paper day! <LINK>\n\nIn which we show how you can take the best BAO measurements from @sdssurveys BOSS data, and improve them by a factor of 2 by adding information from voids.', ""To put that in perspective, to get that improvement without using voids, you'd expect to need a galaxy survey 4 times the size of BOSS.\n\nWith voids, you can get it for free without needing any new data. That's pretty cool!"", ""I'm going to bed now (it's midnight in Canada) but will pick this thread back up tomorrow!"", ""Here's two of the most important plots in the paper, which shows how we gain information from BOSS by adding voids to the usual BAO and RSD analyses: https://t.co/0JVaoso8Wi"", 'Basically, we are measuring distortions in the apparent positions of galaxies around voids. These distortions are caused by two effects: galaxy velocities, which affect their observed redshift, and the choice of cosmological model to convert redshifts to distances.', ""(If the model you use is not the correct one, you add additional distortions, known as the Alcock-Paczynski effect. If you can remove the AP distortions, you've got the right cosmology. Of course this is the hard part.)"", ""We spend a lot of the paper carefully modelling and measuring the distortions. Eventually we can model them extremely well! Here's the plot of data vs theory: https://t.co/e8N19K7wrO"", 'Fitting the model to the data gives the orange likelihood contours from this plot. These are perfect because they exactly complement the constraints from traditional methods in blue! (The degeneracy directions are orthogonal.) So we can combine to get the green constraints ... https://t.co/4meFpG4Iva', ""... which mean a *big* gain in constraining power of BOSS. Here's a different view of the same thing: https://t.co/yWc4VkrMmc"", 'In terms of measuring something like the Hubble parameter H0, adding voids gives the same information gain as going from the Planck 2015 to 2018 CMB results, *and* adding CMB lensing, *and* adding all other non-BOSS BAO measurements! https://t.co/vldoRwIJaQ', ""... which, it's safe to say, we were not expecting to see when we started work on this!\n\nAnyway, big shout out to excellent @UoPCosmology PhD student @PaulCarter1992 and to Will Percival, Hans Winther and Julian Bautista for this team effort."", ""@WKCosmo Anything that uses the sound horizon at the drag epoch says it is 67. Our final result does this (because we include the BAO constraints), so we're no different!\n\nIn principle voids can also give a completely independent measure that doesn't need the sound horizon though ...""]",https://arxiv.org/abs/1904.01030,"We present a measurement of the anisotropic void-galaxy cross-correlation function in the CMASS galaxy sample of the BOSS DR12 data release. We perform a joint fit to the data for redshift space distortions (RSD) due to galaxy peculiar velocities and anisotropies due to the Alcock-Paczynski (AP) effect, for the first time using a velocity field reconstruction technique to remove the complicating effects of RSD in the void centre positions themselves. Fits to the void-galaxy function give a 1% measurement of the AP parameter combination $D_A(z)H(z)/c = 0.4367\pm 0.0045$ at redshift $z=0.57$, where $D_A$ is the angular diameter distance and $H$ the Hubble parameter, exceeding the precision obtainable from baryon acoustic oscillations (BAO) by a factor of ~3.5 and free of systematic errors. From voids alone we also obtain a 10% measure of the growth rate, $f\sigma_8(z=0.57)=0.501\pm0.051$. The parameter degeneracies are orthogonal to those obtained from galaxy clustering. Combining void information with that from BAO and galaxy RSD in the same CMASS sample, we measure $D_A(0.57)/r_s=9.383\pm 0.077$ (at 0.8% precision), $H(0.57)r_s=(14.05\pm 0.14)\;10^3$ kms$^{-1}$Mpc$^{-1}$ (1%) and $f\sigma_8=0.453\pm0.022$ (4.9%), consistent with cosmic microwave background (CMB) measurements from Planck. These represent a factor \sim2 improvement in precision over previous results through the inclusion of void information. Fitting a flat cosmological constant $\Lambda$CDM model to these results in combination with Planck CMB data, we find up to an 11% reduction in uncertainties on $H_0$ and $\Omega_m$ compared to use of the corresponding BOSS consensus values. Constraints on extended models with non-flat geometry and a dark energy of state that differs from $w=-1$ show an even greater improvement. ","Beyond BAO: improving cosmological constraints from BOSS with
  measurement of the void-galaxy cross-correlation"
99,1113144459145830400,986037210309783552,Aida Behmard,"['New paper - do you like **cool** (i.e., &lt;4700 K) stars? Data-driven/machine learning methods? Then check this out! 🌟\xa0<LINK>\n\n...More science explanation below:', 'Small, cool stars like K and M dwarfs are super important! They are the most common type of stars in the galaxy, and are good targets for finding small, cool planets like the Earth. 🌍', ""However, it's really hard to characterize such stars with stellar models - they're cool enough to harbor molecular species that create dense clusters of lines from rotational transitions in their spectra. We don't have the chemical info needed to model these effects."", ""And there's a ton of other physics going on in these stars that we also aren't capable of modeling well right now."", '...So, we turned to data-driven / ML methods! Specifically, The Cannon, a data-driven framework created by @melissakness et al. (2015).', 'The Cannon frees us from physics-based stellar models by allowing us to model the spectra of small, cool stars using **just** a training set of spectra from similar, well-characterized stars.', 'And the results are good! Precisions of ~68 K in Teff, 5% in Rstar, and 0.08 dex in [Fe/H] - not bad! 🌠', ""People often think of machine learning as super complicated - a kind of statistical magic. But under the hood, The Cannon is just linear regression. Anything that generates a model from data alone can be described as data-driven / ML. It's not hard, and anyone can learn! 🌟👩\u200d🔬""]",https://arxiv.org/abs/1904.00094,"The advent of large-scale spectroscopic surveys underscores the need to develop robust techniques for determining stellar properties (""labels"", i.e., physical parameters and elemental abundances). However, traditional spectroscopic methods that utilize stellar models struggle to reproduce cool ($<$4700 K) stellar atmospheres due to an abundance of unconstrained molecular transitions, making modeling via synthetic spectral libraries difficult. Because small, cool stars such as K and M dwarfs are both common and good targets for finding small, cool planets, establishing precise spectral modeling techniques for these stars is of high priority. To address this, we apply The Cannon, a data-driven method of determining stellar labels, to Keck High Resolution Echelle Spectrometer (HIRES) spectra of 141 cool ($<$5200 K) stars from the California Planet Search. Our implementation is capable of predicting labels for small ($<$1 $R_{\odot}$) stars of spectral types K and later with accuracies of 68 K in effective temperature ($T_{eff}$), 5% in stellar radius ($R_{*}$), and 0.08 dex in bulk metallicity ([Fe/H]), and maintains this performance at low spectral resolutions ($R$ $<$ 5000). As M-dwarfs are the focus of many future planet-detection surveys, this work can aid efforts to better characterize the cool star population and uncover correlations between cool star abundances and planet occurrence for constraining planet formation theories. ",Data-Driven Spectroscopy of Cool Stars at High Spectral Resolution
100,1113095264892407816,1103855178137001985,Christopher J. Stein,['Our paper on the autoCAS program for fully automated multi-configurational calculations is finally on arXiv: <LINK>\nAn update with the new algorithm for very large valence spaces will be released upon acceptance of the paper.'],https://arxiv.org/abs/1904.00097,"We present our implementation autoCAS for fully automated multi-configurational calculations, which we also make available free of charge on our webpages. The graphical user interface of autoCAS connects a general electronic structure program with a density matrix renormalization group program to carry out our recently introduced automated active space selection protocol for multi-configurational calculations [J. Chem. Theory Comput., 2016, 12, 1760]. Next to this active space selection, autoCAS carries out several steps of multi-configurational calculations so that only a minimal input is required to start them, comparable to that of a standard Kohn-Sham density functional theory calculation, so that black-box multi-configurational calculations become feasible. Furthermore, we introduce a new extension to the selection algorithm that facilitates automated selections for molecules with large valence orbital spaces consisting of several hundred orbitals. ","autoCAS: a program for fully automated multi-configurational
  calculations"
101,1113050072474968064,525824056,Matthew Wicker,"['Finally got around to preprinting our CVPR2019 paper! We propose a new method to test 3D deep learning and we show that current methods overestimate classification under attacks by 70%. \n\nPaper: <LINK>\nCode: <LINK> <LINK>', ""Note: that first 'pedestrian' comes from a 3D model of Santa. After removal of 6% of this input, he was misclassified as a plant. Convenient, right? I am not saying that we have cracked how Santa has gone undetected all this time... but he definitely uses adversarial examples"", ""By 6% I meant 1.5% of the input, whoops! Santa is better than I gave him credit for. Hopefully I don't end up on the naughty list for that miscalculation.""]",https://arxiv.org/abs/1904.00923,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space. ",Robustness of 3D Deep Learning in an Adversarial Setting
102,1112997613140434944,846745670644109317,Hugo Caselles-Dupré,"['Our paper ""Symmetry-Based Disentangled Representation Learning requires Interaction with Environments"" is accepted to the Workshop on Structure &amp; Priors in RL at @iclr2019 ! See you in New Orleans ! Arxiv: <LINK>\n\ncc: @ENSTAParisTech @FlowersINRIA', 'We start from a recently proposed symmetry-based approach on Disentangled Representation Learning by Higgins et al. (https://t.co/5JjpV38a9s).\n\nThis approach states that the transformations that change only some properties of the world state can be used to define disentanglement.', 'We then provide evidence that such approach should not use training sets of still samples (s_t, s_t+1, ...) as in the original work but transitions ((s_t, a_t, s_t+1), (s_t+1, a_t+1, s_t+2), ...).', 'We also study, theoretically and empirically, how to learn the two types of Symmetry-Based disentangled representation: linear and non-linear ones. We propose methods and architectures for learning them.', 'In future work, we will investigate usefulness of disentangled representation over entangled representations for downstream tasks, as well as more complex environments.']",https://arxiv.org/abs/1904.00243,"Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab and the code is available on GitHub. ","Symmetry-Based Disentangled Representation Learning requires Interaction
  with Environments"
103,1112887185483628549,326843207,Yuta Notsu,"['Our new superflare paper (Notsu et al. ApJ in press) is opened in the today\'s arXiv !\n\n""Do Kepler superflare stars really include slowly-rotating Sun-like stars ? - Results using APO 3.5m telescope spectroscopic observations and Gaia-DR2 data -""\n<LINK>', '""We report the latest view of Kepler solar-type (G-type main-sequence) superflare stars, including recent updates with Apache Point Observatory (APO) 3.5m telescope spectroscopic observations and Gaia-DR2 data""', '@cosmodrake Thank you !!', '@johngizis Thanks!!']",https://arxiv.org/abs/1904.00142,"We report the latest view of Kepler solar-type (G-type main-sequence) superflare stars, including recent updates with Apache Point Observatory (APO) 3.5m telescope spectroscopic observations and Gaia-DR2 data. First, we newly conducted APO3.5m spectroscopic observations of 18 superflare stars found from Kepler 1-min time cadence data. More than half (43 stars) are confirmed to be ""single"" stars, among 64 superflare stars in total that have been spectroscopically investigated so far in this APO3.5m and our previous Subaru/HDS observations. The measurements of $v\sin i$ (projected rotational velocity) and chromospheric lines (Ca II H\&K and Ca II 8542\AA) support the brightness variation of superflare stars is caused by the rotation of a star with large starspots. We then investigated the statistical properties of Kepler solar-type superflare stars by incorporating Gaia-DR2 stellar radius estimates. As a result, the maximum superflare energy continuously decreases as the rotation period $P_{\mathrm{rot}}$ increases. Superflares with energies $\lesssim 5\times10^{34}$ erg occur on old, slowly-rotating Sun-like stars ($P_{\mathrm{rot}}\sim$25 days) approximately once every 2000--3000 years, while young rapidly-rotating stars with $P_{\mathrm{rot}}\sim$ a few days have superflares up to $10^{36}$ erg. The maximum starspot area does not depend on the rotation period when the star is young, but as the rotation slows down, it starts to steeply decrease at $P_{\mathrm{rot}}\gtrsim$12 days for Sun-like stars. These two decreasing trends are consistent since the magnetic energy stored around starspots explains the flare energy, but other factors like spot magnetic structure should also be considered. ","Do Kepler superflare stars really include slowly-rotating Sun-like stars
  ? - Results using APO 3.5m telescope spectroscopic observations and Gaia-DR2
  data -"
104,1112884635682320384,2930561996,Foivos Diakogiannis,"['From astronomy to computer vision: new paper on semantic segmentation (includes multi-tasking as well). If you are into this type of problems, you will enjoy the discussion on the Generalized Dice loss. Code and blog will follow. \n\nWith @cwperth \n<LINK> <LINK>', 'From the paper: ""not all loss functions are created equal"". Gradient flow and Laplacian operator on a 2D toy problem for a set of losses is intuitive for their performance (rightmost column is our choice). https://t.co/4omZ0tEFSS']",https://arxiv.org/abs/1904.00592,"Scene understanding of high resolution aerial images is of great importance for the task of automated monitoring in various remote sensing applications. Due to the large within-class and small between-class variance in pixel values of objects of interest, this remains a challenging task. In recent years, deep convolutional neural networks have started being used in remote sensing applications and demonstrate state of the art performance for pixel level classification of objects. \textcolor{black}{Here we propose a reliable framework for performant results for the task of semantic segmentation of monotemporal very high resolution aerial images. Our framework consists of a novel deep learning architecture, ResUNet-a, and a novel loss function based on the Dice loss. ResUNet-a uses a UNet encoder/decoder backbone, in combination with residual connections, atrous convolutions, pyramid scene parsing pooling and multi-tasking inference. ResUNet-a infers sequentially the boundary of the objects, the distance transform of the segmentation mask, the segmentation mask and a colored reconstruction of the input. Each of the tasks is conditioned on the inference of the previous ones, thus establishing a conditioned relationship between the various tasks, as this is described through the architecture's computation graph. We analyse the performance of several flavours of the Generalized Dice loss for semantic segmentation, and we introduce a novel variant loss function for semantic segmentation of objects that has excellent convergence properties and behaves well even under the presence of highly imbalanced classes.} The performance of our modeling framework is evaluated on the ISPRS 2D Potsdam dataset. Results show state-of-the-art performance with an average F1 score of 92.9\% over all classes for our best model. ","ResUNet-a: a deep learning framework for semantic segmentation of
  remotely sensed data"
105,1127200712516153345,174298756,Adel Bibi,['How to train robust networks outperforming 2-21x fold data augmentation (without any data augmentation)? New paper out with Modar and Bernard.\n<LINK>'],https://arxiv.org/abs/1904.11005,"Despite the impressive performance of deep neural networks (DNNs) on numerous vision tasks, they still exhibit yet-to-understand uncouth behaviours. One puzzling behaviour is the subtle sensitive reaction of DNNs to various noise attacks. Such a nuisance has strengthened the line of research around developing and training noise-robust networks. In this work, we propose a new training regularizer that aims to minimize the probabilistic expected training loss of a DNN subject to a generic Gaussian input. We provide an efficient and simple approach to approximate such a regularizer for arbitrary deep networks. This is done by leveraging the analytic expression of the output mean of a shallow neural network; avoiding the need for the memory and computationally expensive data augmentation. We conduct extensive experiments on LeNet and AlexNet on various datasets including MNIST, CIFAR10, and CIFAR100 demonstrating the effectiveness of our proposed regularizer. In particular, we show that networks that are trained with the proposed regularizer benefit from a boost in robustness equivalent to performing 3-21 folds of data augmentation. ",Analytical Moment Regularizer for Gaussian Robust Networks
106,1122961744928833549,790183810973458432,Jakub Tomczak,['Our new paper (+@r_lepert &amp; Auke Wiggers) from @Qualcomm AI Research in Amsterdam on optimizing tensor programs using graph neural networks (<LINK>) will be presented at RLGM @iclr2019. The dataset will be available soon! <LINK>'],https://arxiv.org/abs/1904.11876,"Optimizing the execution time of tensor program, e.g., a convolution, involves finding its optimal configuration. Searching the configuration space exhaustively is typically infeasible in practice. In line with recent research using TVM, we propose to learn a surrogate model to overcome this issue. The model is trained on an acyclic graph called an abstract syntax tree, and utilizes a graph convolutional network to exploit structure in the graph. We claim that a learnable graph-based data processing is a strong competitor to heuristic-based feature extraction. We present a new dataset of graphs corresponding to configurations and their execution time for various tensor programs. We provide baselines for a runtime prediction task. ",Simulating Execution Time of Tensor Programs using Graph Neural Networks
107,1122903120042909696,23560339,Kartik Nayak,"['We have a new paper: Flexible BFT, a family of BFT\nprotocols that support clients with heterogenous assumptions. <LINK> with @dahlia_malkhi @LingRen', 'Key technical contributions:\n(1) It introduces a synchronous BFT protocol in which only the commit step requires synchrony (and replicas run at the network speed)', '(2) It deconstructs BFT quorums to understand the role played by different quorums and considers a new fault -- ""alive-but-corrupt"" fault!']",https://arxiv.org/abs/1904.10067,"This paper introduces Flexible BFT, a new approach for BFT consensus solution design revolving around two pillars, stronger resilience and diversity. The first pillar, stronger resilience, involves a new fault model called alive-but-corrupt faults. Alive-but-corrupt replicas may arbitrarily deviate from the protocol in an attempt to break safety of the protocol. However, if they cannot break safety, they will not try to prevent liveness of the protocol. Combining alive-but-corrupt faults into the model, Flexible BFT is resilient to higher corruption levels than possible in a pure Byzantine fault model. The second pillar, diversity, designs consensus solutions whose protocol transcript is used to draw different commit decisions under diverse beliefs. With this separation, the same Flexible BFT solution supports synchronous and asynchronous beliefs, as well as varying resilience threshold combinations of Byzantine and alive-but-corrupt faults. At a technical level, Flexible BFT achieves the above results using two new ideas. First, it introduces a synchronous BFT protocol in which only the commit step requires to know the network delay bound and thus replicas execute the protocol without any synchrony assumption. Second, it introduces a notion called Flexible Byzantine Quorums by dissecting the roles of different quorums in existing consensus protocols. ",Flexible Byzantine Fault Tolerance
108,1121761428590014465,38941661,Daniel Stilck França,['New paper out! Check it out if you are interested in functional inequalities and quantum semigroups!\n<LINK>'],https://arxiv.org/abs/1904.11043,"Capacities of quantum channels and decoherence times both quantify the extent to which quantum information can withstand degradation by interactions with its environment. However, calculating capacities directly is known to be intractable in general. Much recent work has focused on upper bounding certain capacities in terms of more tractable quantities such as specific norms from operator theory. In the meantime, there has also been substantial recent progress on estimating decoherence times with techniques from analysis and geometry, even though many hard questions remain open. In this article, we introduce a class of continuous-time quantum channels that we called transferred channels, which are built through representation theory from a classical Markov kernel defined on a compact group. We study two subclasses of such kernels: H\""ormander systems on compact Lie-groups and Markov chains on finite groups. Examples of transferred channels include the depolarizing channel, the dephasing channel, and collective decoherence channels acting on $d$ qubits. Some of the estimates presented are new, such as those for channels that randomly swap subsystems. We then extend tools developed in earlier work by Gao, Junge and LaRacuente to transfer estimates of the classical Markov kernel to the transferred channels and study in this way different non-commutative functional inequalities. The main contribution of this article is the application of this transference principle to the estimation of various capacities as well as estimation of entanglement breaking times, defined as the first time for which the channel becomes entanglement breaking. Moreover, our estimates hold for non-ergodic channels such as the collective decoherence channels, an important scenario that has been overlooked so far because of a lack of techniques. ","Group transference techniques for the estimation of the decoherence
  times and capacities of quantum Markov semigroups"
109,1121370039012212736,3334758406,Alejandro Lumbreras-Calle,"['We published a new paper today! We analyze the morphology of low-mass star-forming galaxies, masking the star-forming regions, and found two different classes! A smaller, roundish, redder class and a larger, disk-like, bluer one. Check it out here: <LINK> <LINK>', 'To mask the star-forming regions and analyze only the stellar host, we had to create high-resolution Hα images using broad-band HST data, and we used a new bayesian code, PHI (https://t.co/i7BqQjJ1UU) to perform the fitting. We applied it to 7 HST bands, independently. Enjoy!']",https://arxiv.org/abs/1904.10462,"The morphological evolution of star-forming galaxies provides important clues to understand their physical properties, as well as the triggering and quenching mechanisms of star formation. We aim at connecting morphology and star-formation properties of low-mass galaxies (median stellar mass $\sim$ 10$^{8.5}$ M$_{\odot}$) at low redshift ($z<0.36$). We use a sample of medium-band selected star-forming galaxies from the GOODS-North field. H$\alpha$ images for the sample are created combining both spectral energy distribution fits and HST data. Using them, we mask the star forming regions to obtain an unbiased two-dimensional model of the light distribution of the host galaxies. For this purpose we use $\texttt{PHI}$, a new Bayesian photometric decomposition code. We apply it independently to 7 HST bands assuming a S\'ersic surface brightness model. Star-forming galaxy hosts show low S\'ersic index (with median $n$ $\sim$ 0.9), as well as small sizes (median $R_e$ $\sim$ 1.6 kpc), and negligible change of the parameters with wavelength (except for the axis ratio, which grows with wavelength). Using a clustering algorithm, we find two different classes of star-forming galaxies: A more compact, redder, and high-$n$ (class A) and a more extended, bluer and lower-$n$ one (class B). We also find evidence that the first class is more spheroidal-like. In addition, we find that 48% of the analyzed galaxies present negative color gradients (only 5% are positive). The host component of low-mass star-forming galaxies at $z<0.36$ separates into two different classes, similar to what has been found for their higher mass counterparts. The results are consistent with an evolution from class B to class A. Several mechanisms from the literature, like minor and major mergers, and violent disk instability, can explain the physical process behind the likely transition between the classes. [abridged] ","The stellar host in star-forming low-mass galaxies: Evidence for two
  classes"
110,1121292485916073984,733679466761834496,Frederic Barraquand,"['New Theor Ecol paper showing #coexistence mechanisms (classic niche differences, the storage effect) can produce even more diversity when acting together <LINK>  Pdf <LINK> By Coralie Picoche (mostly) &amp; me. #traits #seasonality #stochasticity']",https://arxiv.org/abs/1904.10845,"Explaining coexistence in species-rich communities of primary producers remains a challenge for ecologists because of their likely competition for shared resources. Following Hutchinson's seminal suggestion, many theoreticians have tried to create diversity through a fluctuating environment, which impairs or slows down competitive exclusion. However, fluctuating-environment models often only produce a dozen of coexisting species at best. Here, we investigate how to create richer communities in fluctuating environments, using an empirically parameterized model. Building on the forced Lotka-Volterra model of Scranton and Vasseur (Theor Ecol 9(3):353-363, 2016), inspired by phytoplankton communities, we have investigated the effect of two coexistence mechanisms, namely the storage effect and higher intra- than interspecific competition strengths (i.e., strong self-regulation). We tuned the intra/inter competition ratio based on empirical analyses, in which self-regulation dominates interspecific interactions. Although a strong self-regulation maintained more species (50%) than the storage effect (25%), we show that none of the two coexistence mechanisms considered could ensure the coexistence of all species alone. Realistic seasonal environments only aggravated that picture, as they decreased persistence relative to a random environment. However, strong self-regulation and the storage effect combined superadditively so that all species could persist with both mechanisms at work. Our results suggest that combining different coexistence mechanisms into community models might be more fruitful than trying to find which mechanism best explains diversity. We additionally highlight that while biomass-trait distributions provide some clues regarding coexistence mechanisms, they cannot indicate unequivocally which mechanisms are at play. ","How self-regulation, the storage effect and their interaction contribute
  to coexistence in stochastic and seasonal environments"
111,1119651920380088321,91758004,José Ignacio Latorre,['Our new paper on benchmarking of quantum computers with maximally entangled states: <LINK> .'],https://arxiv.org/abs/1904.07955,"We design a series of quantum circuits that generate absolute maximally entangled (AME) states to benchmark a quantum computer. A relation between graph states and AME states can be exploited to optimize the structure of the circuits and minimize their depth. Furthermore, we find that most of the provided circuits obey majorization relations for every partition of the system, and at every step of the quantum computation. The rational for our approach is to benchmark quantum computers with maximal useful entanglement, which can be used to implement multipartite quantum protocols. ",Quantum circuits for maximally entangled states
112,1117612630687010817,321794593,José G. Fernández-Trincado,['Here is my anothe paper based on chemical anomalies :). <LINK> -- I identified new outliers (chemically distinct of the Milky Way) based on $^{28}$Si =) in the H-band of @APOGEEsurvey and integrated precise orbits with @ESAGaia  and @GravPot16 <LINK>'],https://arxiv.org/abs/1904.05884,"We report the discovery of a unique collection of metal-poor giant-stars, that exhibit anomalously high levels of $^{28}$Si, clearly above typical Galactic levels. Our sample spans a narrow range of metallicities, peaking at $-1.07\pm 0.06$, and exhibit abundance ratios of [Si,Al/Fe] that are as extreme as those observed in Galactic globular clusters (GCs), and Mg is slightly less overabundant. In almost all the sources we used, the elemental abundances were re-determined from high-resolution spectra, which were re-analyzed assuming LTE. Thus, we compiled the main element families, namely the light elements (C, N), $\alpha-$elements (O, Mg, Si), iron-peak element (Fe), $\textit{s}-$process elements (Ce, Nd), and the light odd-Z element (Al). We also provide dynamical evidence that most of these stars lie on tight (inner)halo-like and retrograde orbits passing through the bulge. Such kinds of objects have been found in present-day halo GCs, providing the clearest chemical signature of past accretion events in the (inner) stellar halo of the Galaxy, formed possibly as the result of dissolved halo GCs. Their chemical composition is, in general, similar to that of typical GCs population, although several differences exist. ","Discovery of a new stellar sub-population residing in the (inner)
  stellar halo of the Milky Way"
113,1116618543112417280,935441422626541568,Rima Alaifari,"['A new paper that has been quite fun to work on (with my awesome collaborators X Cheng, S Steinerberger, L Pierce): what can we say about the size of the spectral norms of products of two matrices? Does ordering the product lead to larger norms? Paper link: <LINK>']",https://arxiv.org/abs/1904.05239,"Given two symmetric and positive semidefinite square matrices $A, B$, is it true that any matrix given as the product of $m$ copies of $A$ and $n$ copies of $B$ in a particular sequence must be dominated in the spectral norm by the ordered matrix product $A^m B^n$? For example, is $$ \| AABAABABB \| \leq \| AAAAABBBB \|\ ? $$ Drury has characterized precisely which disordered words have the property that an inequality of this type holds for all matrices $A,B$. However, the $1$-parameter family of counterexamples Drury constructs for these characterizations is comprised of $3 \times 3$ matrices, and thus as stated the characterization applies only for $N \times N$ matrices with $N \geq 3$. In contrast, we prove that for $2 \times 2$ matrices, the general rearrangement inequality holds for all disordered words. We also show that for larger $N \times N$ matrices, the general rearrangement inequality holds for all disordered words, for most $A,B$ (in a sense of full measure) that are sufficiently small perturbations of the identity. ",On Matrix Rearrangement Inequalities
114,1113871577811124224,2613619922,byron wallace,"['Can we infer the findings from full-text articles describing RCTs? New task, dataset, models (w/E Lehman, J DeYoung &amp; R Barzilay) #NAACL2019\n\nBlog post: <LINK>\nWeb: <LINK>\nPaper: <LINK>\nCode/data: <LINK>']",https://arxiv.org/abs/1904.01606,"How do we know if a particular medical treatment actually works? Ideally one would consult all available evidence from relevant clinical trials. Unfortunately, such results are primarily disseminated in natural language scientific articles, imposing substantial burden on those trying to make sense of them. In this paper, we present a new task and corpus for making this unstructured evidence actionable. The task entails inferring reported findings from a full-text article describing a randomized controlled trial (RCT) with respect to a given intervention, comparator, and outcome of interest, e.g., inferring if an article provides evidence supporting the use of aspirin to reduce risk of stroke, as compared to placebo. We present a new corpus for this task comprising 10,000+ prompts coupled with full-text articles describing RCTs. Results using a suite of models --- ranging from heuristic (rule-based) approaches to attentive neural architectures --- demonstrate the difficulty of the task, which we believe largely owes to the lengthy, technical input texts. To facilitate further work on this important, challenging problem we make the corpus, documentation, a website and leaderboard, and code for baselines and evaluation available at this http URL ",Inferring Which Medical Treatments Work from Reports of Clinical Trials
115,1113857219626401798,3312021076,Arman Cohan,['Our #Naacl2019 paper on citation intent prediction with @waleed_ammar. SOTA on the ACL-ARC dataset as well as our new dataset SciCite using a multitask model incorporating structure of scientific papers into citations.\nPaper:  <LINK>\nCode: <LINK>'],https://arxiv.org/abs/1904.01608,"Identifying the intent of a citation in scientific papers (e.g., background information, use of methods, comparing results) is critical for machine reading of individual publications and automated analysis of the scientific literature. We propose structural scaffolds, a multitask model to incorporate structural information of scientific papers into citations for effective classification of citation intents. Our model achieves a new state-of-the-art on an existing ACL anthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without relying on external linguistic resources or hand-engineered features as done in existing methods. In addition, we introduce a new dataset of citation intents (SciCite) which is more than five times larger and covers multiple scientific domains compared with existing datasets. Our code and data are available at: this https URL ","Structural Scaffolds for Citation Intent Classification in Scientific
  Publications"
116,1113498125606445056,349172730,Ranjay Krishna,"['Measuring progress in generative models is akin to hill climbing on noise. Automatic metrics are heuristics and human evaluations unreliable. Our latest paper presents a new human evaluation grounded in psychophysics, consistent, turnkey and cost-effective <LINK> <LINK>', 'We can consistently separate the performance of numerous GANs based on human perceptual fidelity and show that contrary to prior literature, StyleGAN with truncation trick is significantly better than without it. https://t.co/yn1AYkHfM5', 'This work was done with Sharon Zhou, @mitchellgordon, Austin Narcomey, Durim Morina and @msbernst', '@poolio @mitchellgordon @msbernst yikes... finding good acronyms is hard!!', '@gwern We plan on doing both --- Having a leaderboard for established datasets as well as providing users with the ability to create new challenges with custom datasets.']",https://arxiv.org/abs/1904.01121,"Generative models often use human evaluations to measure the perceived quality of their outputs. Automated metrics are noisy indirect proxies, because they rely on heuristics or pretrained embeddings. However, up until now, direct human evaluation strategies have been ad-hoc, neither standardized nor validated. Our work establishes a gold standard human benchmark for generative realism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) able to produce separable model performances, and (4) efficient in cost and time. We introduce two variants: one that measures visual perception under adaptive time constraints to determine the threshold at which a model's outputs appear real (e.g. 250ms), and the other a less expensive variant that measures human error rate on fake and real images sans time constraints. We test HYPE across six state-of-the-art generative adversarial networks and two sampling techniques on conditional and unconditional image generation using four datasets: CelebA, FFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements across training epochs, and we confirm via bootstrap sampling that HYPE rankings are consistent and replicable. ","HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative
  Models"
117,1123298458071457792,2983164057,Mohsen Fayyaz,"['""Holistic Large Scale Video Understanding""\nA new paper by me,@AliDiba67,Vivek Sharma,Manohar Paluri,Juergen Gall, Rainer Stiefelhagen,@lucgool1\n<LINK>\nWe propose a new video dataset for multi-label &amp; multi-task video understanding and a new architecture ""HATNet"" <LINK>', 'Action recognition mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video.', 'We fill in this gap by presenting the large-scale ""Holistic Video Understanding Dataset""(HVU). HVU focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene.', 'HVU contains ~577k videos in total with 13M annotations for training and validation set spanning over 4378 classes. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes &amp; concepts, which naturally captures the real-world scenarios', 'Further, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues.', 'HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. The experiments show that HATNet trained on HVU outperforms current state-of-the-art methods on challenging human action datasets: HMDB51, UCF101, and Kinetics.', 'The dataset and codes will be made publicly available.\nStay tuned for more info on our workshop at #ICCV2019\nhttps://t.co/DAebuwJh4u']",https://arxiv.org/abs/1904.11451,"Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale ""Holistic Video Understanding Dataset""~(HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx.~572k videos in total with 9 million annotations for training, validation, and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes, and concepts which naturally captures the real-world scenarios. We demonstrate the generalization capability of HVU on three challenging tasks: 1.) Video classification, 2.) Video captioning and 3.) Video clustering tasks. In particular for video classification, we introduce a new spatio-temporal deep neural network architecture called ""Holistic Appearance and Temporal Network""~(HATNet) that builds on fusing 2D and 3D architectures into one by combining intermediate representations of appearance and temporal cues. HATNet focuses on the multi-label and multi-task learning problem and is trained in an end-to-end manner. Via our experiments, we validate the idea that holistic representation learning is complementary, and can play a key role in enabling many real-world applications. ",Large Scale Holistic Video Understanding
118,1123091705069047811,989251872107085824,Quoc Le,"['Data augmentation is often associated with supervised learning. We find *unsupervised* data augmentation works better. It combines well with transfer learning (e.g. BERT) and improves everything when datasets have a small number of labeled examples. Link: <LINK> <LINK>', 'Key idea behind the method is consistency training: model prediction on an unlabeled example and model prediction on an *augmented* unlabeled example are consistent. This idea is also related to some recent papers and our own paper on Cross View Training: https://t.co/D2G4OPViuz', 'Results with Unsupervised Data Augmentation: It achieves SOTA on IMDb with 20 labeled examples (~1000x less labeled data than other methods), and achieves SOTA on CIFAR10 4k, SVHN 1k.', ""@msweeny I don't understand your question well, but the method is used in a semi-supervised learning where you have two losses. One is supervised loss and the other is unsupervised consistency loss. The figure below may help. https://t.co/oq7DZldYut""]",http://arxiv.org/abs/1904.12848,"Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at this https URL ",Unsupervised Data Augmentation for Consistency Training
119,1121938334287323136,1068545181576773632,Kenneth Brown,"[""New paper examining circuit level error models for error correction circuits composed of qubits that leak and noisier qubits that don't leak. We find that for a  physically motivated leakage model, a mixture of qubits works best. <LINK>"", 'This paper is the third in a series.\nIn paper 1, Natalie Brown and I compared two types of ion qubits (hyperfine=leaky, Zeeman=noisy) for toric codes using a standard leakage model. Zeeman wins for low magnetic field noise. https://t.co/4pbNQzJeHN', 'In paper 2, Michael Newman, Natalie, and I found that subsystem codes are good for handling leakage. We also introduced a new leakage error model based leaked states not interacting with qubit states during two-qubit gates.\nhttps://t.co/8GtMy7jM05', 'In this paper, we see that the new error model improves the performance of hyperfine qubits in the context of error correction.  There is still a magnetic field stability where Zeeman qubits are better, but at this magnetic field stability a mixed-qubit strategy is the best.']",https://arxiv.org/abs/1904.10724,"Leakage errors take qubits out of the computational subspace and will accumulate if not addressed. A leaked qubit will reduce the effectiveness of quantum error correction protocols due to the cost of implementing leakage reduction circuits and the harm caused by interacting leaked states with qubit states. Ion trap qubits driven by Raman gates have a natural choice between qubits encoded in magnetically insensitive hyperfine states that can leak and qubits encoded in magnetically sensitive Zeeman states of the electron spin that cannot leak. In our previous work, we compared these two qubits in the context of the toric code with a depolarizing leakage error model and found that for magnetic field noise with a standard deviation less than 32 $\mu$G that the $^{174}$Yb$^+$ Zeeman qubit outperforms the $^{171}$Yb$^+$ hyperfine qubit. Here we examine a physically motivated leakage error model based on ions interacting via the Molmer-Sorenson gate. We find that this greatly improves the performance of hyperfine qubits but the Zeeman qubits are more effective for magnetic field noise with a standard deviation less than 10 $\mu$G. At these low magnetic fields, we find that the best choice is a mixed qubit scheme where the hyperfine qubits are the ancilla and the leakage is handled without the need of an additional leakage reduction circuit. ","Leakage mitigation for quantum error correction using a mixed qubit
  scheme"
120,1121231185475096576,728042672695484416,Nimit Sohoni,"['Tired of running out of memory? We study four techniques for reducing the memory required to train neural networks, while still preserving accuracy as much as possible. Techniques:\n(1) Sparsity\n(2) Half precision\n(3) Microbatching\n(4) Checkpointing\n\n<LINK>', 'Before/after charts of the total memory usage of training: https://t.co/4PzGZjLeg1']",https://arxiv.org/abs/1904.10631,"Memory is increasingly often the bottleneck when training neural network models. Despite this, techniques to lower the overall memory requirements of training have been less widely studied compared to the extensive literature on reducing the memory requirements of inference. In this paper we study a fundamental question: How much memory is actually needed to train a neural network? To answer this question, we profile the overall memory usage of training on two representative deep learning benchmarks -- the WideResNet model for image classification and the DynamicConv Transformer model for machine translation -- and comprehensively evaluate four standard techniques for reducing the training memory requirements: (1) imposing sparsity on the model, (2) using low precision, (3) microbatching, and (4) gradient checkpointing. We explore how each of these techniques in isolation affects both the peak memory usage of training and the quality of the end model, and explore the memory, accuracy, and computation tradeoffs incurred when combining these techniques. Using appropriate combinations of these techniques, we show that it is possible to the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up to 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train a DynamicConv model on IWSLT'14 German to English translation by up to 8.7x with a BLEU score drop of 0.15. ",Low-Memory Neural Network Training: A Technical Report
121,1118879532528758785,114485232,Jimmy Lin,"[""I think @kchonyc is burying the lede here so let me try: Hey, we've got the top leaderboard position on MS MARCO. Find out how we did it! <LINK> <LINK>""]",https://arxiv.org/abs/1904.08375,"One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster. ",Document Expansion by Query Prediction
122,1118354187484401664,989251872107085824,Quoc Le,"['We used architecture search to find a better architecture for object detection. Results: Better and faster architectures than Mask-RCNN, FPN and SSD architectures. Architecture also looks unexpected and pretty funky. Link: <LINK> <LINK>']",https://arxiv.org/abs/1904.07392,"Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in [32] and achieves 48.3 AP which surpasses Mask R-CNN [10] detection accuracy with less computation time. ","NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object
  Detection"
123,1118331392977330176,928301283034914817,Lihua Lei,"['1/ Excited to announce my new work with Mike on adaptivity of stochastic gradient methods! <LINK>. We propose SCSG that is provably adaptive to strong convexity and target accuracy for convex finite-sum optimization. The main technique is called ""geometrization"".', '2/ We say an algorithm is adaptive to a feature/property/parameter if it takes advantage of it w/o knowing it. SCSG achieves the following complexity as the minimum of four terms, while only requires the knowledge of the smoothness parameter. https://t.co/WkBMdjrQ2z', '3/ Geometrization is a technique that sets the number of updates as a geometric random variable. It is a magical alternative to telescoping, which is ubiquitous in optimization literature. The magic is shown in our Lemma 3.1. We believe it can be useful in other contexts. https://t.co/UQYFWtBo0o']",https://arxiv.org/abs/1904.04480,"Stochastic-gradient-based optimization has been a core enabling methodology in applications to large-scale problems in machine learning and related areas. Despite the progress, the gap between theory and practice remains significant, with theoreticians pursuing mathematical optimality at a cost of obtaining specialized procedures in different regimes (e.g., modulus of strong convexity, magnitude of target accuracy, signal-to-noise ratio), and with practitioners not readily able to know which regime is appropriate to their problem, and seeking broadly applicable algorithms that are reasonably close to optimality. To bridge these perspectives it is necessary to study algorithms that are adaptive to different regimes. We present the stochastically controlled stochastic gradient (SCSG) method for composite convex finite-sum optimization problems and show that SCSG is adaptive to both strong convexity and target accuracy. The adaptivity is achieved by batch variance reduction with adaptive batch sizes and a novel technique, which we referred to as geometrization, which sets the length of each epoch as a geometric random variable. The algorithm achieves strictly better theoretical complexity than other existing adaptive algorithms, while the tuning parameters of the algorithm only depend on the smoothness parameter of the objective. ",On the Adaptivity of Stochastic Gradient-Based Optimization
124,1117966659723993088,92263871,Masashi Nakatani,"['Our latest study on sound frisson, which may be related to #ASMR, is now preliminary visible @arxiv. We recorded binaural sounds that can cause chilling feeling, at least to some of the participants. \n&gt; Proximal binaural sound can induce subjective frisson\n<LINK>', 'Thank you for @sfujiidr and all authors that support this study (launching recording environment and lab spaces @KeioSFC, performing experiments &amp; massive data analysis and draft writing with fruitful comments for three years. I fully appreciate it.', '@AndrewHires @arxiv You can try at here: https://t.co/4flPCyfEYh']",https://arxiv.org/abs/1904.06851,"Auditory frisson is the experience of feeling of cold or shivering related to sound in the absence of a physical cold stimulus. Multiple examples of frisson-inducing sounds have been reported, but the mechanism of auditory frisson remains elusive. Typical frisson-inducing sounds may contain a looming effect, in which a sound appears to approach the listener's peripersonal space. Previous studies on sound in peripersonal space have provided objective measurements of sound-inducing effects, but few have investigated the subjective experience of frisson-inducing sounds. Here we explored whether it is possible to produce subjective feelings of frisson by moving a noise sound (white noise, rolling beads noise, or frictional noise produced by rubbing a plastic bag) stimulus around a listener's head. Our results demonstrated that sound-induced frisson can be experienced stronger when auditory stimuli are rotated around the head (binaural moving sounds) than the one without the rotation (monaural static sounds), regardless of the source of the noise sound. Pearson's correlation analysis showed that several acoustic features of auditory stimuli, such as variance of interaural level difference (ILD), loudness, and sharpness, were correlated with the magnitude of subjective frisson. We had also observed that the subjective feelings of frisson by moving a musical sound had increased comparing with a static musical sound. ",Proximal binaural sound can induce subjective frisson
125,1116278415961751554,761633681777852416,Enrico Ronchi,"['Why do we ignore evacuation instructions? Read on a known, but little studied issue in #evacuation research, the cry-wolf effect: <LINK> with @alexrigos and @karlerikmohlin #bettersafethansorry #egress #alarms Free copy available here: <LINK> <LINK>', '@ProfJohnDrury @alexrigos @karlerikmohlin @ProfJohnDrury you will be surprised, but I had classical studies in high school, so Aesop was a must read and I had to read it in ancient greek! The image is funny though 😎']",https://arxiv.org/abs/1904.01963,"In today's terrorism-prone and security-focused world, evacuation emergencies, drills, and false alarms are becoming more and more common. Compliance to an evacuation order made by an authority in case of emergency can play a key role in the outcome of an emergency. In case an evacuee experiences repeated emergency scenarios which may be a false alarm (e.g., an evacuation drill, a false bomb threat, etc.) or an actual threat, the Aesop's cry wolf effect (repeated false alarms decrease order compliance) can severely affect his/her likelihood to evacuate. To analyse this key unsolved issue of evacuation research, a game-theoretic approach is proposed. Game theory is used to explore mutual best responses of an evacuee and an authority. In the proposed model the authority obtains a signal of whether there is a threat or not and decides whether to order an evacuation or not. The evacuee, after receiving an evacuation order, subsequently decides whether to stay or leave based on posterior beliefs that have been updated in response to the authority's action. Best-responses are derived and Sequential equilibrium and Perfect Bayesian Equilibrium are used as solution concepts (refining equilibria with the intuitive criterion). Model results highlight the benefits of announced evacuation drills and suggest that improving the accuracy of threat detection can prevent large inefficiencies associated with the cry wolf effect. ",The Cry Wolf Effect in Evacuation: a Game-Theoretic Approach
126,1115668448019996672,1045108610198638592,Anjalie Field,"['Our #icwsm2019 paper titled ""Contextual Affective Analysis: A Case Study of People Portrayals in Online #MeToo Stories"" is now available! We examine power, agency, and sentiment in media coverage of the MeToo movement <LINK>']",https://arxiv.org/abs/1904.04164,"In October 2017, numerous women accused producer Harvey Weinstein of sexual harassment. Their stories encouraged other women to voice allegations of sexual harassment against many high profile men, including politicians, actors, and producers. These events are broadly referred to as the #MeToo movement, named for the use of the hashtag ""#metoo"" on social media platforms like Twitter and Facebook. The movement has widely been referred to as ""empowering"" because it has amplified the voices of previously unheard women over those of traditionally powerful men. In this work, we investigate dynamics of sentiment, power and agency in online media coverage of these events. Using a corpus of online media articles about the #MeToo movement, we present a contextual affective analysis---an entity-centric approach that uses contextualized lexicons to examine how people are portrayed in media articles. We show that while these articles are sympathetic towards women who have experienced sexual harassment, they consistently present men as most powerful, even after sexual assault allegations. While we focus on media coverage of the #MeToo movement, our method for contextual affective analysis readily generalizes to other domains. ","Contextual Affective Analysis: A Case Study of People Portrayals in
  Online #MeToo Stories"
127,1115627677388611584,4727352557,Carlo Forestiere,"['The  hybridization theory has been a cornerstone in the modelling of the electromagnetic scattering from metal nano-structures. However, it is not able to describe dielectric resonators. Here we propose a full-wave hybridization theory for open-resonators: <LINK> <LINK>']",https://arxiv.org/abs/1904.02569,"The plasmon hybridization theory is based on a quasi-electrostatic approximation of the Maxwell's equations. It does not take into account magnetic interactions, retardation effects, and radiation losses. Magnetic interactions play a dominant role in the scattering from dielectric nanoparticles. The retardation effects play a fundamental role in the coupling of the modes with the incident radiation and in determining their radiative strength; their exclusion may lead to erroneous predictions of the excited modes and of the scattered power spectra. Radiation losses may lead to a significant broadening of the scattering resonances. We propose a hybridization theory for non-hermitian composite systems based on the full-Maxwell equations that, overcoming all the limitations of the plasmon hybridization theory, unlocks the description of dielectric dimers. As an example, we decompose the scattered field from silicon and silver dimers, under different excitation conditions and gap-sizes, in terms of dimer modes, pinpointing the hybridizing isolated-sphere modes behind them. ",Full-wave electromagnetic modes and hybridization in nanoparticle dimers
128,1115604309993857026,7773042,Yasser Souri,"['""Weakly Supervised Action Segmentation Using Mutual Consistency""\nA new paper by me, @MohsenFyz and our advisor, Juergen Gall.\n<LINK>\nWe propose a new approach for action segmentation using transcripts as the weak supervision. <LINK>', 'Our network produces two redundant representations for action segmentation (frame-level and segment-level representations) and during training requires them to match each other using a new loss function that we term MuCon.\nWe also have a transcript prediction loss (as in seq2seq)', ""The approach is fast during training: doesn't require Viterbi or pseudo ground truth generation like many of the previous works.\nIt is also differentiable, so no heuristic updates of parameters, only SGD.\nAt test time we predict both representations and fuse them for smoothness."", 'Unlike current state-of-the-art methods that are truly only able to perform action alignment at test time and mimic action segmentation by choosing one of the training transcripts, we are able to perform action segmentation at test time.', 'Our work is currently under review. Code is ""coming soon""!\nWe would really appreciate your feedback.\nsouri@iai.uni-bonn.de or tweet at me.']",https://arxiv.org/abs/1904.03116,"Action segmentation is the task of predicting the actions for each frame of a video. As obtaining the full annotation of videos for action segmentation is expensive, weakly supervised approaches that can learn only from transcripts are appealing. In this paper, we propose a novel end-to-end approach for weakly supervised action segmentation based on a two-branch neural network. The two branches of our network predict two redundant but different representations for action segmentation and we propose a novel mutual consistency (MuCon) loss that enforces the consistency of the two redundant representations. Using the MuCon loss together with a loss for transcript prediction, our proposed approach achieves the accuracy of state-of-the-art approaches while being $14$ times faster to train and $20$ times faster during inference. The MuCon loss proves beneficial even in the fully supervised setting. ",Fast Weakly Supervised Action Segmentation Using Mutual Consistency
129,1115524149840154626,636864273,Christoph Molnar,"['Quantifying Interpretability of Arbitrary Machine Learning Models Through Functional Decomposition -- my first PhD paper now available on arxiv: <LINK> 🎉🎉🎉\n\nWe propose methods to describe the complexity of arbitrary machine learning models. <LINK>', '@mgershoff You also have to consider the changes in predictive performance moving from a tree to a NN or from NN to 2x-sized NN. How much complexity are you willing to add a certain increase in predictive performance? Probably also a diminishing utility for the last 0.001% accuracy']",https://arxiv.org/abs/1904.03867,"Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main effect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity. ","Quantifying Model Complexity via Functional Decomposition for Better
  Post-Hoc Interpretability"
130,1115276930524172288,953616889,Justin Read,"['Very excited to announce the launch of our new ""EDGE"" project simulating the smallest galaxies in the Universe. In this first paper, we study what happens when we change the resolution and sub-grid physics for a single low-mass dwarf:\n\n<LINK>\n\n[A thread: 1/N] <LINK>', 'Our simulations (with a resolution of ~3pc) capture the impact of each individual supernova explosion, making us insensitive to our sub-grid physics choices. All models with stellar feedback match the size, luminosity &amp; stellar kinematics of ultra-faint dwarfs.\n\n[2/N] https://t.co/SfrtypqWe3', 'However, the luminosity-metallicity relation is uniquely sensitive to our subgrid physics model. If the supernovae are over/under powered, they eject too many/too few metals, leading to too low/high metallicity. Our fiducial model, however, gets it just right.\n\n[3/N] https://t.co/98fRIekusg', 'There will be lots more to follow from this project; we are just getting started! If the success of these models scales to larger galaxy masses, then we may be well on the road towards truly ""ab-initio"" galaxy formation simulations.\n\n[End]']",https://arxiv.org/abs/1904.02723,"We introduce the ""Engineering Dwarfs at Galaxy formation's Edge"" (EDGE) project to study the cosmological formation and evolution of the smallest galaxies in the Universe. In this first paper, we explore the effects of resolution and sub-grid physics on a single low mass halo ($M_{\rm halo}=10^{9}~M_\odot$), simulated to redshift $z=0$ at a mass and spatial resolution of $\sim 20~M_\odot$ and $\sim 3$ pc. We consider different star formation prescriptions, supernova feedback strengths and on-the-fly radiative transfer (RT). We show that RT changes the mode of galactic self-regulation at this halo mass, suppressing star formation by causing the interstellar and circumgalactic gas to remain predominantly warm ($\sim 10^4$ K) even before cosmic reionisation. By contrast, without RT, star formation regulation occurs only through starbursts and their associated vigorous galactic outflows. In spite of this difference, the entire simulation suite (with the exception of models without any feedback) matches observed dwarf galaxy sizes, velocity dispersions, $V$-band magnitudes and dynamical mass-to-light-ratios. This is because such structural scaling relations are predominantly set by the host dark matter halo, with the remaining model-to-model variation being smaller than the observational scatter. We find that only the stellar mass-metallicity relation differentiates the galaxy formation models. Explosive feedback ejects more metals from the dwarf, leading to a lower metallicity at a fixed stellar mass. We conclude that the stellar mass-metallicity relation of the very smallest galaxies provides a unique constraint on galaxy formation physics. ","EDGE: the mass-metallicity relation as a critical test of galaxy
  formation physics"
131,1114239075719032832,38226810,Abhinav Dhall,"['Our IJCNN 2019 paper ""Unsupervised Learning of Eye Gaze Representation\nfrom the Web"" is now available <LINK>. We propose a self-supervised method for learning discriminative eye gaze representation from YouTube videos with automatically generated noisy labels.']",https://arxiv.org/abs/1904.02459,"Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large `in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently fine-tuned for any eye gaze estimation dataset. ",Unsupervised Learning of Eye Gaze Representation from the Web
132,1113781597927563271,923346378343903232,Emilien Dupont,"['We show that there are functions Neural ODEs cannot represent. To overcome this we propose Augmented Neural ODEs which are more expressive, empirically reduce computational cost and improve generalization 🌟 With @ArnaudDoucet1 @yeewhye\n\nPaper: <LINK> <LINK>']",http://arxiv.org/abs/1904.01681,"We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs. ",Augmented Neural ODEs
133,1113779219207684101,941929573742235649,Venu Kalari,"['Our paper with @jorick73 @wjmdewit on the origin of very massive stars around NGC3603 is out- <LINK> We find a 100 solar mass star  (RFS7) traceable back to the NGC3603, but another 70 solar mass shows no clear evidence of the same (RFS8). <LINK>']",https://arxiv.org/abs/1904.02126,"The formation of the most massive stars in the Universe remains an unsolved problem. Are they able to form in relative isolation in a manner similar to the formation of solar-type stars, or do they necessarily require a clustered environment? In order to shed light on this important question, we study the origin of two very massive stars (VMS): the O2.5If*/WN6 star RFS7 ($\sim$100 $M_{\odot}$), and the O3.5If* star RFS8 ($\sim$70 $M_{\odot}$), found within $\approx$ 53 and 58 pc respectively from the Galactic massive young cluster NGC 3603, using Gaia data. RFS7 is found to exhibit motions resembling a runaway star from NGC 3603. This is now the most massive runaway star candidate known in the Milky Way. Although RFS8 also appears to move away from the cluster core, it has proper-motion values that appear inconsistent with being a runaway from NGC 3603 at the $3\sigma$ level (but with substantial uncertainties due to distance and age). Furthermore, no evidence for a bow-shock or a cluster was found surrounding RFS8 from available near-infrared photometry. In summary, whilst RFS7 is likely a runaway star from NGC 3603, making it the first VMS runaway in the Milky Way, RFS8 is an extremely young ($\sim$2 Myr) VMS, which might also be a runaway, but this would need to be established from future spectroscopic and astrometric observations, as well as precise distances. If RFS8 were still not meeting the criteria for being a runaway from NGC 3603 from such future data, this would have important ramifications for current theories of massive star formation, as well as the way the stellar initial mass function (IMF) is sampled. ",On the origin of very massive stars around NGC 3603
134,1113727677788229632,373332154,Vivien Parmentier,"[""Today's phase curve is brought to you by Arcangeli et al. <LINK>.  Ultra hot Jupiters are hot enough to break water molecules on their dayside, making them look like blackbodies between 1 and 2 microns. We find that it is true at all phases ! <LINK>"", 'What else ? The winds seem to do something unusual. They are clearly dragged, leading to an inefficient heat transport, but they also seem to blow the ""wrong"" way. That\'s a smoking gun for magnetic interactions ! But MHD models are clearly needed to get further insights... https://t.co/ZJDSDnSOw0', ""Also, the star matter ! We cannot reliably constrain the day/night contrast of the planet because we don't observe a nightside flux, but also because we don't know the star well enough and thus we don't know how hot the planet is supposed to be in the first place ! https://t.co/Ntl4KSdaeW"", 'Finally, we compare the phase curve of WASP-18b with WASP-103b another ultra hot Jupiter and they look different ! Sure they are different planets, but not that different. Maybe one has a stronger magnetic field that the other ?', ""As always we are faced with a population spanning a wide range of parameters and we need clearly more data ! Let's go back finish writing some @NASAHubble proposals to solve these mysteries."", ""@JDLothringer @NASAHubble It's such a high gravity that getting the transit spectrum is hopeless !""]",https://arxiv.org/abs/1904.02069,"We present the analysis of a full-orbit, spectroscopic phase curve of the ultra hot Jupiter WASP-18b, obtained with the Wide Field Camera 3 aboard the Hubble Space Telescope. We measure the planet's normalized day-night contrast as >0.96 in luminosity: the disk-integrated dayside emission from the planet is at 964+-25 ppm, corresponding to 2894+-30 K, and we place an upper limit on the nightside emission of <32ppm or 1430K at the 3-sigma level. We also find that the peak of the phase curve exhibits a small, but significant offset in brightness of 4.5+-0.5 degrees eastward. We compare the extracted phase curve and phase resolved spectra to 3D Global Circulation Models and find that broadly the data can be well reproduced by some of these models. We find from this comparison several constraints on the atmospheric properties of the planet. Firstly we find that we need efficient drag to explain the very inefficient day-night re-circulation observed. We demonstrate that this drag could be due to Lorentz-force drag by a magnetic field as weak as 10 Gauss. Secondly, we show that a high metallicity is not required to match the large day-night temperature contrast. In fact, the effect of metallicity on the phase curve is different from cooler gas-giant counterparts, due to the high-temperature chemistry in WASP-18b's atmosphere. Additionally, we compare the current UHJ spectroscopic phase curves, WASP-18b and WASP-103b, and show that these two planets provide a consistent picture with remarkable similarities in their measured and inferred properties. However, key differences in these properties, such as their brightness offsets and radius anomalies, suggest that UHJ could be used to separate between competing theories for the inflation of gas-giant planets. ","Climate of an Ultra hot Jupiter: Spectroscopic phase curve of WASP-18b
  with HST/WFC3"
135,1113088369851056131,446634627,Roberto Santana,"['#GaussianProcesses have been used for #sentiment_analysis, but the kernel choice is always fixed. Here <LINK>, using @ibaidev code, we propose structural kernel search with #genetic_programming  to estimate the degree in which a text expresses a given sentiment. <LINK>']",https://arxiv.org/abs/1904.00977,"Sentiment analysis consists of evaluating opinions or statements from the analysis of text. Among the methods used to estimate the degree in which a text expresses a given sentiment, are those based on Gaussian Processes. However, traditional Gaussian Processes methods use a predefined kernel with hyperparameters that can be tuned but whose structure can not be adapted. In this paper, we propose the application of Genetic Programming for evolving Gaussian Process kernels that are more precise for sentiment analysis. We use use a very flexible representation of kernels combined with a multi-objective approach that simultaneously considers two quality metrics and the computational time spent by the kernels. Our results show that the algorithm can outperform Gaussian Processes with traditional kernels for some of the sentiment analysis tasks considered. ",Sentiment analysis with genetically evolved Gaussian kernels
136,1113050072474968064,525824056,Matthew Wicker,"['Finally got around to preprinting our CVPR2019 paper! We propose a new method to test 3D deep learning and we show that current methods overestimate classification under attacks by 70%. \n\nPaper: <LINK>\nCode: <LINK> <LINK>', ""Note: that first 'pedestrian' comes from a 3D model of Santa. After removal of 6% of this input, he was misclassified as a plant. Convenient, right? I am not saying that we have cracked how Santa has gone undetected all this time... but he definitely uses adversarial examples"", ""By 6% I meant 1.5% of the input, whoops! Santa is better than I gave him credit for. Hopefully I don't end up on the naughty list for that miscalculation.""]",https://arxiv.org/abs/1904.00923,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space. ",Robustness of 3D Deep Learning in an Adversarial Setting
137,1121370039012212736,3334758406,Alejandro Lumbreras-Calle,"['We published a new paper today! We analyze the morphology of low-mass star-forming galaxies, masking the star-forming regions, and found two different classes! A smaller, roundish, redder class and a larger, disk-like, bluer one. Check it out here: <LINK> <LINK>', 'To mask the star-forming regions and analyze only the stellar host, we had to create high-resolution Hα images using broad-band HST data, and we used a new bayesian code, PHI (https://t.co/i7BqQjJ1UU) to perform the fitting. We applied it to 7 HST bands, independently. Enjoy!']",https://arxiv.org/abs/1904.10462,"The morphological evolution of star-forming galaxies provides important clues to understand their physical properties, as well as the triggering and quenching mechanisms of star formation. We aim at connecting morphology and star-formation properties of low-mass galaxies (median stellar mass $\sim$ 10$^{8.5}$ M$_{\odot}$) at low redshift ($z<0.36$). We use a sample of medium-band selected star-forming galaxies from the GOODS-North field. H$\alpha$ images for the sample are created combining both spectral energy distribution fits and HST data. Using them, we mask the star forming regions to obtain an unbiased two-dimensional model of the light distribution of the host galaxies. For this purpose we use $\texttt{PHI}$, a new Bayesian photometric decomposition code. We apply it independently to 7 HST bands assuming a S\'ersic surface brightness model. Star-forming galaxy hosts show low S\'ersic index (with median $n$ $\sim$ 0.9), as well as small sizes (median $R_e$ $\sim$ 1.6 kpc), and negligible change of the parameters with wavelength (except for the axis ratio, which grows with wavelength). Using a clustering algorithm, we find two different classes of star-forming galaxies: A more compact, redder, and high-$n$ (class A) and a more extended, bluer and lower-$n$ one (class B). We also find evidence that the first class is more spheroidal-like. In addition, we find that 48% of the analyzed galaxies present negative color gradients (only 5% are positive). The host component of low-mass star-forming galaxies at $z<0.36$ separates into two different classes, similar to what has been found for their higher mass counterparts. The results are consistent with an evolution from class B to class A. Several mechanisms from the literature, like minor and major mergers, and violent disk instability, can explain the physical process behind the likely transition between the classes. [abridged] ","The stellar host in star-forming low-mass galaxies: Evidence for two
  classes"
138,1118114115086688256,426509606,Yamir Moreno,"['Our latest: ""Directionality reduces the impact of epidemics in multilayer networks"" (<LINK> …). We (w/ @xiangrongwang @SrAleta &amp; D Lu ) study analytically and via simulation of real networks how diseases spread in directed multilayer networks. <LINK>']",https://arxiv.org/abs/1904.06959,"The study of how diseases spread has greatly benefited from advances in network modeling. Recently, a class of networks known as multilayer graphs has been shown to describe more accurately many real systems, making it possible to address more complex scenarios in epidemiology such as the interaction between different pathogens or multiple strains of the same disease. In this work, we study in depth a class of networks that have gone unnoticed up to now, despite of its relevance for spreading dynamics. Specifically, we focus on directed multilayer networks, characterized by the existence of directed links, either within the layers or across layers. Using the generating function approach and numerical simulations of a stochastic susceptible-infected-susceptible (SIS) model, we calculate the epidemic threshold for these networks for different degree distributions of the networks. Our results show that the main feature that determines the value of the epidemic threshold is the directionality of the links connecting different layers, regardless of the degree distribution chosen. Our findings are of utmost interest given the ubiquitous presence of directed multilayer networks and the widespread use of disease-like spreading processes in a broad range of phenomena such as diffusion processes in social and transportation systems. ",Directionality reduces the impact of epidemics in multilayer networks
139,1117727549436715008,426509606,Yamir Moreno,"['Our latest work is out: ""Explore with caution: mapping the evolution of scientific interest in Physics"" (<LINK>). Here we study an essential tension in science: whether to explore new boundaries or exploit previous works. w/ @SrAleta @Sandro_Meloni @net_science <LINK>']",https://arxiv.org/abs/1904.06306,"In the book The Essential Tension Thomas Kuhn described the conflict between tradition and innovation in scientific research --i.e., the desire to explore new promising areas, counterposed to the need to capitalize on the work done in the past. While it is true that along their careers many scientists probably felt this tension, only few works have tried to quantify it. Here, we address this question by analyzing a large-scale dataset, containing all the papers published by the American Physical Society (APS) in more than $25$ years, which allows for a better understanding of scientists' careers evolution in Physics. We employ the Physics and Astronomy Classification Scheme (PACS) present in each paper to map the scientific interests of $181,397$ authors and their evolution along the years. Our results indeed confirm the existence of the `essential tension' with scientists balancing between exploring the boundaries of their area and exploiting previous work. In particular, we found that although the majority of physicists change the topics of their research, they stay within the same broader area thus exploring with caution new scientific endeavors. Furthermore, we quantify the flows of authors moving between different subfields and pinpoint which areas are more likely to attract or donate researchers to the other ones. Overall, our results depict a very distinctive portrait of the evolution of research interests in Physics and can help in designing specific policies for the future. ","Explore with caution: mapping the evolution of scientific interest in
  Physics"
