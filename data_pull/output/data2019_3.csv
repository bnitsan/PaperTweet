,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1115345837633224707,97191218,Pantelis Sopasakis,['Safe autonomous+distributed intersection crossing with online nonlinear model predictive control - our new paper <LINK>.\n\nCode generation toolbox: <LINK> (written in @rustlang).\n\n@Ford #AutonomousVehicles #SelfDrivingCars #Driverless <LINK>'],https://arxiv.org/abs/1903.12091,"The coordination of highly automated vehicles (or agents) in road intersections is an inherently nonconvex and challenging problem. In this paper, we propose a distributed motion planning scheme under reasonable vehicle-to-vehicle communication requirements. Each agent solves a nonlinear model predictive control problem in real time and transmits its planned trajectory to other agents, which may have conflicting objectives. The problem formulation is augmented with conditional constraints that enable the agents to decide whether to wait at a stopping line, if safe crossing is not possible. The involved nonconvex problems are solved very efficiently using the proximal averaged Newton method for optimal control (PANOC). We demonstrate the efficiency of the proposed approach in a realistic intersection crossing scenario. ","Nonlinear Model Predictive Control for Distributed Motion Planning in
  Road Intersections Using PANOC"
1,1113875811105267713,713117884239753216,Xiaolong Wang,"[""We released our CVPR'19 paper on Learning Correspondence from the Cycle-Consistency of Time (<LINK>) with code (<LINK>). We hope this opens up new opportunities for unsupervised learning with videos. Check out our result for long-range tracking! <LINK>""]",https://arxiv.org/abs/1903.07593,"We introduce a self-supervised method for learning visual correspondence from unlabeled video. The main idea is to use cycle-consistency in time as free supervisory signal for learning visual representations from scratch. At training time, our model learns a feature map representation to be useful for performing cycle-consistent tracking. At test time, we use the acquired representation to find nearest neighbors across space and time. We demonstrate the generalizability of the representation -- without finetuning -- across a range of visual correspondence tasks, including video object segmentation, keypoint tracking, and optical flow. Our approach outperforms previous self-supervised methods and performs competitively with strongly supervised methods. ",Learning Correspondence from the Cycle-Consistency of Time
2,1113545075672457216,1011944195341381633,Manami Sasaki ‰Ωê„ÄÖÊú®ÊÑõÁæé,['We found a new bow-shock #PulsarWindNebula in #SupernovaRemnant DEM S5 in the #SmallMagellanicCloud! Paper is accepted. See <LINK> @MFilipovic23'],https://arxiv.org/abs/1903.03226,"We report the discovery of a new Small Magellanic Cloud Pulsar Wind Nebula (PWN) at the edge of the Supernova Remnant (SNR)-DEM S5. The pulsar powered object has a cometary morphology similar to the Galactic PWN analogs PSR B1951+32 and 'the mouse'. It is travelling supersonically through the interstellar medium. We estimate the Pulsar kick velocity to be in the range of 700-2000 km/s for an age between 28-10 kyr. The radio spectral index for this SNR PWN pulsar system is flat (-0.29 $\pm$ 0.01) consistent with other similar objects. We infer that the putative pulsar has a radio spectral index of -1.8, which is typical for Galactic pulsars. We searched for dispersion measures (DMs) up to 1000 cm/pc^3 but found no convincing candidates with a S/N greater than 8. We produce a polarisation map for this PWN at 5500 MHz and find a mean fractional polarisation of P $\sim 23$ percent. The X-ray power-law spectrum (Gamma $\sim 2$) is indicative of non-thermal synchrotron emission as is expected from PWN-pulsar system. Finally, we detect DEM S5 in Infrared (IR) bands. Our IR photometric measurements strongly indicate the presence of shocked gas which is expected for SNRs. However, it is unusual to detect such IR emission in a SNR with a supersonic bow-shock PWN. We also find a low-velocity HI cloud of $\sim 107$ km/s which is possibly interacting with DEM S5. SNR DEM S5 is the first confirmed detection of a pulsar-powered bow shock nebula found outside the Galaxy. ","Discovery of a Pulsar-powered Bow Shock Nebula in the Small Magellanic
  Cloud Supernova Remnant DEMS5"
3,1113411522066374656,1260729972,Prof. Christelle Vincent,['My incomparable co-author Chris Rasmussen (@BranchedCover) just wrote a very nice (and accessible!) article on our new paper about solving the S-unit equation: <LINK> (direct link to the paper if this whets your appetite:<LINK>)'],https://arxiv.org/abs/1903.00977,"Let $K$ be a number field, and $S$ a finite set of places in $K$ containing all infinite places. We present an implementation for solving the $S$-unit equation $x + y = 1$, $x,y \in\mathscr{O}_{K,S}^\times$ in the computer algebra package SageMath. This paper outlines the mathematical basis for the implementation. We discuss and reference the results of extensive computations, including exponent bounds for solutions in many fields of small degree for small sets $S$. As an application, we prove an asymptotic version of Fermat's Last Theorem for totally real cubic number fields with bounded discriminant where 2 is totally ramified. In addition, we use the implementation to find all solutions to some cubic Ramanujan-Nagell equations. ","A robust implementation for solving the $S$-unit equation and several
  applications"
4,1113092488720474113,24488603,Matthew Mirman,['New Paper on DiffAI:  A Provable Defense for Deep Residual Networks <LINK> #diffai @the_sri_lab #provabledefense #certifieddefense #abstractinterpretation #deeplearning'],https://arxiv.org/abs/1903.12519?fbclid=IwAR3ZAisEoqUezQk6Pxt6G8-AnO-TnVUzo67Ld-L50HrDwBRUjeGg8A2JoCg,"We present a training system, which can provably defend significantly larger neural networks than previously possible, including ResNet-34 and DenseNet-100. Our approach is based on differentiable abstract interpretation and introduces two novel concepts: (i) abstract layers for fine-tuning the precision and scalability of the abstraction, (ii) a flexible domain specific language (DSL) for describing training objectives that combine abstract and concrete losses with arbitrary specifications. Our training method is implemented in the DiffAI system. ",A Provable Defense for Deep Residual Networks
5,1112871407199764480,47588593,Omid Alemi,['Our new paper (pre-print) on ML-powered movement generation:\n\n<LINK>\n\n#machinelearning #paper #movement'],https://arxiv.org/abs/1903.08356,"The rise of non-linear and interactive media such as video games has increased the need for automatic movement animation generation. In this survey, we review and analyze different aspects of building automatic movement generation systems using machine learning techniques and motion capture data. We cover topics such as high-level movement characterization, training data, features representation, machine learning models, and evaluation methods. We conclude by presenting a discussion of the reviewed literature and outlining the research gaps and remaining challenges for future work. ","Machine Learning for Data-Driven Movement Generation: a Review of the
  State of the Art"
6,1112851733041111042,2977051573,Quan Vuong,"['New work on domain randomization!\n\nJoint work with @sharadvikram, Dr. Hao Su, Dr. Sean Gao and my dear advisor @hiskov \n\nPaper: <LINK>\nCode: <LINK>', 'Disclaimer: this is not a fully fleshed paper, but a 2-page abstract to demonstrate the potential of a research direction. I‚Äôm submitting it to a workshop mainly to get feedback from the community. ü§ì', 'Summary: Domain randomization is a technique for sim-to-real transfer. It works by training a policy on a distribution of simulated environment. If the environments are diverse enough, the policy has been demonstrated to generalize to the real world.', 'But what distribution of environment should we use?? We show that it is possible to optimize for the parameters of this distribution to maximize transfer success!', 'Major props to @sharadvikram, whose software engineering skills and research advices made my life so much easier in this project! üôèüôèüôè', 'gez, why is there a giant picture of my face üò≥']",https://arxiv.org/abs/1903.11774,"Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world ","How to pick the domain randomization parameters for sim-to-real transfer
  of reinforcement learning policies?"
7,1112832664703131648,23225413,Frederick Davies,"['New theory paper on post-reionization (z~6) quasar proximity zones is now on arxiv: <LINK>. Final result is that proximity zones are sensitive to variability on &lt;100,000 year timescales, should lead to quantitative constraints in the near future.', 'Maybe it will get some extra views since everyone checks for joke papers on April 1st ;)', 'Extra content for Twitter + talks: movie showing how the proximity zone transmission profile (bottom) evolves if the quasar luminosity varies on ~10,000 year timescales (top). Grey = assumed equilibrium, black = full non-eq. calculation. (apologies for twitter compression) https://t.co/jAcaBR0eA3']",https://arxiv.org/abs/1903.12346,"Since the discovery of $z\sim 6$ quasars two decades ago, studies of their Ly$\alpha$-transparent proximity zones have largely focused on their utility as a probe of cosmic reionization. But even when in a highly ionized intergalactic medium, these zones provide a rich laboratory for determining the timescales that govern quasar activity and the concomitant growth of their supermassive black holes. In this work, we use a suite of 1D radiative transfer simulations of quasar proximity zones to explore their time-dependent behaviour for activity timescales from $\sim10^3$ to $10^8$ years. The sizes of the simulated proximity zones, as quantified by the distance at which the smoothed Ly$\alpha$ transmission drops below 10% (denoted $R_p$), are in excellent agreement with observations, with the exception of a handful of particularly small zones that have been attributed to extremely short $\lesssim 10^4$ year lifetimes. We develop a physically motivated semi-analytic model of proximity zones which captures the bulk of their equilibrium and non-equilibrium behaviour, and use this model to investigate how quasar variability on $\lesssim10^5$ year timescales is imprinted on the distribution of observed proximity zone sizes. We show that large variations in the ionizing luminosity of quasars on timescales of $\lesssim10^4$ years are disfavored based on the good agreement between the observed distribution of $R_p$ and our model prediction based on ""lightbulb"" (i.e. steady constant emission) light curves. ",Time-dependent behaviour of quasar proximity zones at $z \sim 6$
8,1112737652602601477,70874545,Josh Lothringer,"['New paper on the arXiv today: <LINK>! We explore the wacky world of ultra-hot Jupiters further by looking at how they change when around different type stars.', 'This should be important since ultra-hot Jupiters are the most highly irradiated gaseous planets and are often around earlier-type host stars (e.g., KELT-9, the host star of the hottest Jovian exoplanet, is an A0, emitting ~50% of its flux in the UV).', 'We find that planets of the same Teq around earlier-type host stars will have stronger, steeper inversions b/c of more short-wavelength (&lt;0.5 mu) absorption. Additionally, treating the star in NLTE also affects the modeled planet b/c of metal line depths. https://t.co/TXYtKd9OXb', 'This trend should be readily detectable by future observations. Such observations will see more muted transit spectra in planets around earlier-type host stars (b/c more H-) and larger brightness temperature variations in emission spectra. https://t.co/JPQYtRc8Jd', 'Lastly, we took a closer look at what is making up the absorption at short-wavelengths, responsible for heating the atmosphere. Here we show that Fe, Fe+, Mg, and C are responsible for absorbing all the irradiation. Na, Ca, and K are important deeper. https://t.co/iBb0VveBKw', ""If you're in Tucson, you can hear me talk about this *today* at the Theoretical Astrophysics Program graduate research prize colloquium at 3:30 in Kuiper 312!""]",https://arxiv.org/abs/1903.12183,"Ultra-hot Jupiters are the most highly irradiated gas giant planets, with equilibrium temperatures from 2000 to over 4000 K. Ultra-hot Jupiters are amenable to characterization due to their high temperatures, inflated radii, and short periods, but their atmospheres are atypical for planets in that the photosphere possesses large concentrations of atoms and ions relative to molecules. Here we evaluate how the atmospheres of these planets respond to irradiation by stars of different spectral type. We find that ultra-hot Jupiters exhibit temperature inversions that are sensitive to the spectral type of the host star. The slope and temperature range across the inversion both increase as the host star effective temperature increases due to enhanced absorption at short wavelengths and low pressures. The steep temperature inversions in ultra-hot Jupiters around hot stars result in increased thermal dissociation and ionization compared to similar planets around cooler stars. The resulting increase in H$^{-}$ opacity leads to a transit spectrum that has muted absorption features. The emission spectrum, however, exhibits a large contrast in brightness temperature, a signature that will be detectable with both secondary eclipse observations and high-dispersion spectroscopy. We also find that the departures from local thermodynamic equilibrium in the stellar atmosphere can affect the degree of heating caused by atomic metals in the planet's upper atmosphere. Additionally, we further quantify the significance of heating by different opacity sources in ultra-hot Jupiter atmospheres. ","The Influence of Host Star Spectral Type on Ultra-Hot Jupiter
  Atmospheres"
9,1112714584438247424,1025860187096317953,Norman Di Palo,"['We released our new paper on sample efficient model-based RL using denoising autoencoders . New SOTA in learning speed compared to current best methods (PETS, MB-MPO). <LINK>']",https://arxiv.org/abs/1903.11981,"Trajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency. ",Regularizing Trajectory Optimization with Denoising Autoencoders
10,1112599382426173440,310626357,Yu Cheng,['Our new paper on VQA: <LINK>\nachieving the state-of-the-art.'],https://arxiv.org/abs/1903.12314,"In order to answer semantically-complicated questions about an image, a Visual Question Answering (VQA) model needs to fully understand the visual scene in the image, especially the interactive dynamics between different objects. We propose a Relation-aware Graph Attention Network (ReGAT), which encodes each image into a graph and models multi-type inter-object relations via a graph attention mechanism, to learn question-adaptive relation representations. Two types of visual object relations are explored: (i) Explicit Relations that represent geometric positions and semantic interactions between objects; and (ii) Implicit Relations that capture the hidden dynamics between image regions. Experiments demonstrate that ReGAT outperforms prior state-of-the-art approaches on both VQA 2.0 and VQA-CP v2 datasets. We further show that ReGAT is compatible to existing VQA architectures, and can be used as a generic relation encoder to boost the model performance for VQA. ",Relation-Aware Graph Attention Network for Visual Question Answering
11,1112514766642339840,1019760963569049601,Almog Yalinewich,['New paper with @MubdiRahman and @AstroAlysa on the arxiv: The energy in a fast radio burst is enough to vaporise a terrestrial planet. Coincidence? We think not! <LINK>'],https://arxiv.org/abs/1903.12186,"Fast radio bursts (FRBs) are, as the name implies, short and intense pulses of radiation at wavelengths of roughly one metre. FRBs have extremely high brightness temperatures, which points to a coherent source of radiation. The energy of a single burst ranges from $10^{36}$ to $10^{39}$ erg. At the high end of the energy range, FRBs have enough energy to unbind an earth-sized planet, and even at the low end, there is enough energy to vaporise and unbind the atmosphere and the oceans. We therefore propose that FRBs are signatures of an artificial terraformer, capable of eradicating life on another planet, or even destroy the planet entirely. The necessary energy can be harvested from Wolf-Rayet stars with a Dyson sphere ($\sim 10^{38}$ erg s$^{-1}$) , and the radiation can be readily produced by astrophysical masers. We refer to this mechanism as Volatile Amplification of a Destructive Emission of Radiation (VADER). We use the observational information to constrain the properties of the apparatus. We speculate that the non-repeating FRBs are low-energy pulses used to exterminate life on a single planet, but leaving it otherwise intact, and that the stronger repeating FRB is part of an effort to destroy multiple objects in the same solar system, perhaps as a preventative measure against panspermia. In this picture, the persistent synchrotron source associated with the first repeating FRB arises from the energy harvesting process. Finally we propose that Oumuamua might have resulted from a destruction of a planet in this manner. ",Fast Radio Bursts from Terraformation
12,1111802067968888832,14910772,kamalikac,"['Local differential privacy is highly convenient, but has low privacy-utility tradeoff. Our new arXiv paper <LINK> introduces profile based privacy ‚Äî  a form of local privacy with better utility. 1/3']",https://arxiv.org/abs/1903.09084,"Differential privacy has emerged as a gold standard in privacy-preserving data analysis. A popular variant is local differential privacy, where the data holder is the trusted curator. A major barrier, however, towards a wider adoption of this model is that it offers a poor privacy-utility tradeoff. In this work, we address this problem by introducing a new variant of local privacy called profile-based privacy. The central idea is that the problem setting comes with a graph G of data generating distributions, whose edges encode sensitive pairs of distributions that should be made indistinguishable. This provides higher utility because unlike local differential privacy, we no longer need to make every pair of private values in the domain indistinguishable, and instead only protect the identity of the underlying distribution. We establish privacy properties of the profile-based privacy definition, such as post-processing invariance and graceful composition. Finally, we provide mechanisms that are private in this framework, and show via simulations that they achieve higher utility than the corresponding local differential privacy mechanisms. ",Profile-Based Privacy for Locally Private Computations
13,1111747014319054850,2995969527,Farshad,"['Our new work at Swarm Robotics Research Team @UomRobotics, ""Human-Swarm Interaction using VR"", in Science News:\n<LINK>\n\nThe paper is available at <LINK>\n@OfficialUoM @EEE_UniOfMan']",https://arxiv.org/abs/1903.10064,"This paper proposes an intuitive human-swarm interaction framework inspired by our childhood memory in which we interacted with living ants by changing their positions and environments as if we were omnipotent relative to the ants. In virtual reality, analogously, we can be a super-powered virtual giant who can supervise a swarm of mobile robots in a vast and remote environment by flying over or resizing the world and coordinate them by picking and placing a robot or creating virtual walls. This work implements this idea by using Virtual Reality along with Leap Motion, which is then validated by proof-of-concept experiments using real and virtual mobile robots in mixed reality. We conduct a usability analysis to quantify the effectiveness of the overall system as well as the individual interfaces proposed in this work. The results revealed that the proposed method is intuitive and feasible for interaction with swarm robots, but may require appropriate training for the new end-user interface device. ",Omnipotent Virtual Giant for Remote Human-Swarm Interaction
14,1111739025696657411,2427184074,Christopher Berry,"['New on @arxiv this week, our #Astro2020 white paper on observing binary black holes with next-generation #GravitationalWave detectors\n\nDeeper, wider, sharper: Next-generation ground-based gravitational-wave observations of binary black holes\n<LINK> <LINK>', 'We ask what is required of a detector to\n üî≠ Detect binary black holes throughout the observable universe?\n ‚öñÔ∏è Precisely measure their masses, spins, etc. and how these distributions evolve?\n üå±Uncover the seeds of the massive black holes found in galactic centres?\n#Astro2020', ""Current detectors like @LIGO can do great things, but we're limited in how far away we can detect sources. More sensitive detectors = deeper survey of the Universe. We'd like to survey back before the peak in star formation (z ~ 2) to the end of the cosmological dark ages z ~ 20"", 'How far could we see a binary black hole system with a future detector? \nThe white solid line shows horizon with Cosmic Explore, the white dashed with the Einstein telescope. The colour coding is for a boost Œ≤ relative to @LIGO A+, and the blue line is for Œ≤ =10 #Astro2020 https://t.co/jfQyu52U20', 'If we increase the frequency range of our detectors we can detect more systems. Wider frequency range = wider range of sources. Lower frequency sensitivities lets us detect higher mass systems. A 100+100 solar mass binary at z = 10 is unobservable above 10 Hz #Astro2020 https://t.co/xIsVzhsXfB', 'We want to detect heavier systems as these might tell us where supermassive black holes come from. This plot shows the low frequency sensitivity we need to detect a 100+100 solar mass system at z = 10 if we parametrize the PSD as S = S10 (f/10 Hz)^Œ± down to a cut-off f = fmin https://t.co/PgmMsGTlkc', 'Improved detector sensitivity + wider frequency bandwidth = sharper parameter measurements. With next-gen #GravitationalWave detectors, we would precisely pin down the mass and spin distributions of black holes, which are clues to how they form https://t.co/4oZISD4pat #Astro2020', 'Next-gen #GravitationalWave detectors will let us measure how mass and spin distributions evolve through cosmic time. We need lots of detections to do this. This plot shows detections per year for different redshift bins‚Äîthey max out when we detect ALL of them #Astro2020 https://t.co/I1vo81yEIA']",https://arxiv.org/abs/1903.09220,"Next-generation observations will revolutionize our understanding of binary black holes and will detect new sources, such as intermediate-mass black holes. Primary science goals include: Discover binary black holes throughout the observable Universe; Reveal the fundamental properties of black holes; Uncover the seeds of supermassive black holes. ","Deeper, Wider, Sharper: Next-Generation Ground-Based Gravitational-Wave
  Observations of Binary Black Holes"
15,1111564737408651264,1097489155620245504,Christopher Morris,['Wanna catch up on the last 15 years of graph kernel research? Check out our new survey <LINK>.\n\nAlso a great addition to the related work of your new GNN paper. üòâ\n\nSupported by @sfb876.'],https://arxiv.org/abs/1903.11835,"Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification. ",A Survey on Graph Kernels
16,1111260751698821122,156427199,Raffaele D'Abrusco,"['In our never-ending quest to squeeze all we can learn about the Œ≥-ray @NASAFermi sky from @WISE_Mission data, we describe two new catalogs of candidate blazars in our freshly accepted paper. Check it out! <LINK>']",https://arxiv.org/abs/1903.11124,"We present two catalogs of radio-loud candidate blazars whose WISE mid-infrared colors are selected to be consistent with the colors of confirmed gamma-ray emitting blazars. The first catalog is the improved and expanded release of the WIBRaLS catalog presented by D'Abrusco et al. (2014): it includes sources detected in all four WISE filters, spatially cross-matched with radio source in one of three radio surveys and radio-loud based on their q22 spectral parameter. WIBRaLS2 includes 9541 sources classified as BL Lacs, FSRQs or mixed candidates based on their WISE colors. The second catalog, called KDEBLLACS, based on a new selection technique, contains 5579 candidate BL Lacs extracted from the population of WISE sources detected in the first three WISE passbands ([3.4], [4.6] and [12]) only, whose mid-infrared colors are similar to those of confirmed, gamma-ray BL Lacs. KDBLLACS members area also required to have a radio counterpart and be radio-loud based on the parameter q12, defined similarly to q22 used for the WIBRaLS2. We describe the properties of these catalogs and compare them with the largest samples of confirmed and candidate blazars in the literature. We crossmatch the two new catalogs with the most recent catalogs of gamma-ray sources detected by Fermi LAT instrument. Since spectroscopic observations of candidate blazars from the first WIBRaLS catalog within the uncertainty regions of gamma-ray unassociated sources confirmed that ~90% of these candidates are blazars, we anticipate that these new catalogs will play again an important role in the identification of the gamma-ray sky. ",Two new catalogs of blazar candidates in the WISE infrared sky
17,1111229665547612162,972586737871572997,Shota Gugushvili,"['New preprint: #Bayesian decompounding for discrete distributions. Joint with Frank van der Meulen and Ester Mariucci. The paper is a nice mixture of theory and practice. #julialang implementation is available on GitHub.\n\n<LINK>', 'Computer code and datasets are here: https://t.co/thnCi9xbG1']",https://arxiv.org/abs/1903.11142,"Suppose that a compound Poisson process is observed discretely in time and assume that its jump distribution is supported on the set of natural numbers. In this paper we propose a non-parametric Bayesian approach to estimate the intensity of the underlying Poisson process and the distribution of the jumps. We provide a MCMC scheme for obtaining samples from the posterior. We apply our method on both simulated and real data examples, and compare its performance with the frequentist plug-in estimator proposed by Buchmann and Gr\""ubel. On a theoretical side, we study the posterior from the frequentist point of view and prove that as the sample size $n\rightarrow\infty$, it contracts around the `true', data-generating parameters at rate $1/\sqrt{n}$, up to a $\log n$ factor. ",Decompounding discrete distributions: A non-parametric Bayesian approach
18,1111222142580985858,56340478,Ad√©lie Gorce,"[""As a modern scientist, I thought I should post my new paper on twitter! have a look if you're interested in how to learn about the morphology of EoR with bispectrum phases: <LINK>""]",https://arxiv.org/abs/1903.11402,"We present a new statistical tool, called the triangle correlation function (TCF), inspired by the earlier work of Obreschkow et al. It is derived from the three-point correlation function and aims to probe the characteristic scale of ionized regions during the epoch of reionization from 21cm interferometric observations. Unlike most works, which focus on power spectrum, i.e. amplitude information, our statistic is based on the information we can extract from the phases of the Fourier transform of the ionization field. In this perspective, it may benefit from the well-known interferometric concept of closure phases. We find that this statistical estimator performs very well on simple ionization fields. For example, with well-defined fully ionized discs, there is a peaking scale, which we can relate to the radius of the ionized bubbles. We explore the robustness of the TCF when observational effects such as angular resolution and noise are considered. We also get interesting results on fields generated by more elaborate simulations such as 21CMFAST. Although the variety of sources and ionized morphologies in the early stages of the process make its interpretation more challenging, the nature of the signal can tell us about the stage of reionization. Finally, and in contrast to other bubble size distribution algorithms, we show that the TCF can resolve two different characteristic scales in a given map. ","Studying the morphology of reionisation with the triangle correlation
  function of phases"
19,1111218838148915200,235735524,Jonathan Pritchard,['Congratulations to my PhD student @adeliegorce for her new paper on phase based statistics as a probe of morphology in the EoR! Now on the arXiv near you. <LINK>'],https://arxiv.org/abs/1903.11402,"We present a new statistical tool, called the triangle correlation function (TCF), inspired by the earlier work of Obreschkow et al. It is derived from the three-point correlation function and aims to probe the characteristic scale of ionized regions during the epoch of reionization from 21cm interferometric observations. Unlike most works, which focus on power spectrum, i.e. amplitude information, our statistic is based on the information we can extract from the phases of the Fourier transform of the ionization field. In this perspective, it may benefit from the well-known interferometric concept of closure phases. We find that this statistical estimator performs very well on simple ionization fields. For example, with well-defined fully ionized discs, there is a peaking scale, which we can relate to the radius of the ionized bubbles. We explore the robustness of the TCF when observational effects such as angular resolution and noise are considered. We also get interesting results on fields generated by more elaborate simulations such as 21CMFAST. Although the variety of sources and ionized morphologies in the early stages of the process make its interpretation more challenging, the nature of the signal can tell us about the stage of reionization. Finally, and in contrast to other bubble size distribution algorithms, we show that the TCF can resolve two different characteristic scales in a given map. ","Studying the morphology of reionisation with the triangle correlation
  function of phases"
20,1111127085903278080,742606297,Ben Oppenheimer üá∫üá¶,"['I had a lot of help with this Astro2020 Decadal white paper. The X-ray circumgalactic medium is a true frontier.  I will always be a UV CGM person, but I also love new challenges!  Thanks goes to my twitter co-authors @astrogrant and @rcrain_astro.  \n<LINK>']",https://arxiv.org/abs/1903.11130,"The majority of baryons reside beyond the optical extent of a galaxy in the circumgalactic and intergalactic media (CGM/IGM). Gaseous halos are inextricably linked to the appearance of their host galaxies through a complex story of accretion, feedback, and continual recycling. The energetic processes, which define the state of gas in the CGM, are the same ones that 1) regulate stellar growth so that it is not over-efficient, and 2) create the diversity of today's galaxy colors, SFRs, and morphologies spanning Hubble's Tuning Fork Diagram. They work in concert to set the speed of growth on the star-forming Main Sequence, transform a galaxy across the Green Valley, and maintain a galaxy's quenched appearance on the Red Sequence. Most baryons in halos more massive than 10^12 Msolar along with their high-energy physics and dynamics remain invisible because that gas is heated above the UV ionization states. We argue that information on many of the essential drivers of galaxy evolution is primarily contained in this ""missing"" hot gas phase. Completing the picture of galaxy formation requires uncovering the physical mechanisms behind stellar and SMBH feedback driving mass, metals, and energy into the CGM. By opening galactic hot halos to new wavebands, we not only obtain fossil imprints of >13 Gyrs of evolution, but observe on-going hot-mode accretion, the deposition of superwind outflows into the CGM, and the re-arrangement of baryons by SMBH feedback. A description of the flows of mass, metals, and energy will only be complete by observing the thermodynamic states, chemical compositions, structure, and dynamics of T>=10^6 K halos. These measurements are uniquely possible with a next-generation X-ray observatory if it provides the sensitivity to detect faint CGM emission, spectroscopic power to measure absorption lines and gas motions, and high spatial resolution to resolve structures. ",Imprint of Drivers of Galaxy Formation in the Circumgalactic Medium
21,1111097259465138176,980073199332282369,Toshihiko Yamasaki,"['New publication on ArXiv (accepted as an oral paper for MVA 2019).\n\nSourav Mishra, Toshihiko Yamasaki, and Hideaki Imaizumi, ‚ÄúImproving image classifiers for small datasets by learning rate adaptations,‚Äù arXiv:1903.10726 (26 Mar 2019)\n\n<LINK>']",https://arxiv.org/abs/1903.10726,"Our paper introduces an efficient combination of established techniques to improve classifier performance, in terms of accuracy and training time. We achieve two-fold to ten-fold speedup in nearing state of the art accuracy, over different model architectures, by dynamically tuning the learning rate. We find it especially beneficial in the case of a small dataset, where reliability of machine reasoning is lower. We validate our approach by comparing our method versus vanilla training on CIFAR-10. We also demonstrate its practical viability by implementing on an unbalanced corpus of diagnostic images. ","Improving image classifiers for small datasets by learning rate
  adaptations"
22,1111077948507996161,2271048799,Rupam Mahmood,"['Learning from scratch is elegant, inexpensive and fun. We bring it to real robots with a new class of autoregressive policies, which induce smooth, safe motion and explore efficiently in tasks with sparse rewards.\nPaper: <LINK>\n\n<LINK>']",https://arxiv.org/abs/1903.11524,"Reinforcement learning algorithms rely on exploration to discover new behaviors, which is typically achieved by following a stochastic policy. In continuous control tasks, policies with a Gaussian distribution have been widely adopted. Gaussian exploration however does not result in smooth trajectories that generally correspond to safe and rewarding behaviors in practical tasks. In addition, Gaussian policies do not result in an effective exploration of an environment and become increasingly inefficient as the action rate increases. This contributes to a low sample efficiency often observed in learning continuous control tasks. We introduce a family of stationary autoregressive (AR) stochastic processes to facilitate exploration in continuous control domains. We show that proposed processes possess two desirable features: subsequent process observations are temporally coherent with continuously adjustable degree of coherence, and the process stationary distribution is standard normal. We derive an autoregressive policy (ARP) that implements such processes maintaining the standard agent-environment interface. We show how ARPs can be easily used with the existing off-the-shelf learning algorithms. Empirically we demonstrate that using ARPs results in improved exploration and sample efficiency in both simulated and real world domains, and, furthermore, provides smooth exploration trajectories that enable safe operation of robotic hardware. ","Autoregressive Policies for Continuous Control Deep Reinforcement
  Learning"
23,1110959455204769794,599434673,Rafael - Michael Karampatsis,"[""Our new paper (with @RandomlyWalking ) 'Maybe Deep Neural Networks are the Best Choice for Modeling Source Code' is now available in arxiv: <LINK>\nThe Tensorflow implementation used for experiments is open source and available here: <LINK>"", ""We've built the first open vocabulary language model for code.\nWe model out-of-vocabulary (OOV) identifiers by segmenting them into subword units. We learn the least entropic segmentation using the BPE algorithm"", 'We utilized the model for code completion and show that the subword unit representation allows for effective and quick adaptation of the model on new code projects achieving state-of-the-art results both in cross-entropy (log perplexity) and a code completion task.', 'The small size of the model allows it to be used in any consumer GPU (300mB) and to easily be integrated by IDE vendors.\nThe subword representations can be used to provide embeddings for OOV identifiers in order to improve downstream taks.']",https://arxiv.org/abs/1903.05734,"Statistical language modeling techniques have successfully been applied to source code, yielding a variety of new software development tools, such as tools for code suggestion and improving readability. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. But traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance. But the open vocabulary version of the neural network language models for code have not been introduced in the literature. We present a new open-vocabulary neural language model for code that is not limited to a fixed vocabulary of identifier names. We employ a segmentation into subword units, subsequences of tokens chosen based on a compression criterion, following previous work in machine translation. Our network achieves best in class performance, outperforming even the state-of-the-art methods of Hellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to a new test project, resulting in increased performance. We showcase our methodology on code corpora in three different languages of over a billion tokens each, hundreds of times larger than in previous work. To our knowledge, this is the largest neural language model for code that has been reported. ",Maybe Deep Neural Networks are the Best Choice for Modeling Source Code
24,1110889397380153346,1004365363574902784,Kevin J. Kelly,"['New paper out today with Valentina De Romeri (from IFIC Valencia) and Pedro Machado! \n\n<LINK>\n\nWe combined two existing ideas and did something new with them -- 1/ <LINK>', ""A) A lot of really smart people have proposed to look for Dark Matter produced in the beam of a neutrino experiment. They suggested looking off-axis, since the DM isn't focused like a neutrino beam is. \n\nhttps://t.co/CF8oLcOpax\nhttps://t.co/XsNOhjrx7A\n\n2/"", 'B) Recently, the DUNE experiment has proposed a near detector that moves off-axis over the course of data collection, DUNE-PRISM. (https://t.co/X7NnOx6QlX)\n\nWe decided to see what you could learn by combining on- and off-axis searches for light Dark matter. \n\n3/', 'It turns out you can learn a lot! By combining these searches for DM scattering with electrons in your detector, you can ""close off"" a lot of parameter space for these theories, reaching spaces where thermal, relic, dark matter is predicted to live.\n\n4/', 'Thanks to my collaborators for a great project!\n\n5/5']",https://arxiv.org/abs/1903.10505,"We explore the sensitivity of the Deep Underground Neutrino Experiment (DUNE) near detector and the proposed DUNE-PRISM movable near detector to sub-GeV dark matter, specifically scalar dark matter coupled to the Standard Model via a sub-GeV dark photon. We consider dark matter produced in the DUNE target that travels to the detector and scatters off electrons. By combining searches for dark matter at many off-axis positions with DUNE-PRISM, sensitivity to this scenario can be much stronger than when performing a measurement at one on-axis position. ",Hunting On- and Off-Axis for Light Dark Matter with DUNE-PRISM
25,1110857295020331010,3122708111,Federico Lelli,"['New paper led by J. Fensch in coll. with @padastr, @PWei888, and many others. In a nutshell: tidal dwarf galaxies (TDG) -forming out of collisional debris- can form star clusters themselves with high efficiency! Based on stunning HST images. Read it here: <LINK> <LINK>']",https://arxiv.org/abs/1903.10789,"The formation of globular clusters remains an open debate. Dwarf starburst galaxies are efficient at forming young massive clusters with similar masses as globular clusters and may hold the key to understanding their formation. We study star cluster formation in a tidal debris - including the vicinity of three tidal dwarf galaxies - in a massive gas dominated collisional ring around NGC~5291. These dwarfs have physical parameters which differ significantly from local starbursting dwarfs. They are gas-rich, highly turbulent, have a gas metallicity already enriched up to half-solar, and are expected to be free of dark matter. The aim is to study massive star cluster formation in this as yet unexplored type of environment. We use imaging from the Hubble Space Telescope using broadband filters covering the wavelength range from the near-ultraviolet to the near-infrared. We determine the masses and ages of the cluster candidates by using the spectral energy distribution-fitting code CIGALE, carefully considering age-extinction degeneracy effects on the estimation of the physical parameters. We find that the tidal dwarf galaxies in the ring of NGC 5291 are forming star clusters with an average efficiency of $\sim40\%$, comparable to blue compact dwarf galaxies. We also find massive star clusters for which the photometry suggests that they were formed at the very birth of the tidal dwarf galaxies and have survived for several hundred million years. Therefore our study shows that extended tidal dwarf galaxies and compact clusters may be formed simultaneously. In the specific case observed here, the young star clusters are not massive enough to survive for a Hubble time. However one may speculate that similar objects at higher redshift, with higher star formation rate, might form some of the long lived globular clusters. ",Massive star cluster formation and evolution in tidal dwarf galaxies
26,1110809171816050688,216729597,Marcel S. Pawlowski,"['New paper with @jbprime and @benfamaey on the arXiv today: ""Do halos that form early, have high concentration, are part of a pair, or contain a central galaxy potential host more pronounced planes of satellite galaxies?""\n<LINK> <LINK>', 'I admit, the title is pretty long, so let me answer it in a short thread:', 'No', ""@morphp87 @jbprime @benfamaey Luckily they expanded the maximum lengths, otherwise the title wouldn't have fit in a single tweet. üòâ"", '@DanceMichi If they exist they are wrong. üòÜ', '@RobIzzard @DanceMichi https://t.co/Yb3YY1UMHy', ""@AstronomerEric No worries, I'll write a longer thread soon. üòâ"", ""Now that I've found some time to sit down and tweet, here's the more detailed discussion: https://t.co/w22mvlrSih"", '@VergaraLautaro Brevity is the soul of wit. üòú', '@astro_francesca Thanks! üòä', '@nfmartin1980 @jbprime @benfamaey Couldn‚Äôt have made the joke on Twitter then. üòâ']",https://arxiv.org/abs/1903.10513,"The Milky Way, the Andromeda galaxy, and Centaurus A host flattened distributions of satellite galaxies which exhibits coherent velocity trends indicative of rotation. Comparably extreme satellite structures are very rare in cosmological LCDM simulations, giving rise to the `satellite plane problem'. As a possible explanation it has been suggested that earlier-forming, higher concentration host halos contain more flattened and kinematically coherent satellite planes. We have tested for such a proposed correlation between the satellite plane and host halo properties in the ELVIS suite of simulations. We find evidence neither for a correlation of plane flattening with halo concentration or formation time, nor for a correlation of kinematic coherence with concentration. The height of the thinnest sub-halo planes does correlate with the host virial radius and with the radial extent of the sub-halo system. This can be understood as an effect of not accounting for differences in the radial distribution of sub-halos, and selecting them from different volumes than covered by the actual observations. Being part of a halo pair like the Local Group does not result in more narrow or more correlated satellite planes either. Additionally, using the PhatELVIS simulations we show that the presence of a central galaxy potential does not favor more narrow or more correlated satellite planes, it rather leads to slightly wider planes. Such a central potential is a good approximation of the dominant effect baryonic physics in cosmological simulations has on a sub-halo population. This suggests that, in contrast to other small-scale problems, the planes of satellite galaxies issue is made worse by accounting for baryonic effects. ","Do halos that form early, have high concentration, are part of a pair,
  or contain a central galaxy potential host more pronounced planes of
  satellite galaxies?"
27,1110796733213216769,141440459,Rod Van Meter üåª,"['New paper dance! Compile better for your quantum computer.\n<LINK>\nPossibly of interest to @meQuanics @margmartonosi @kenbrownquantum @dajmeyer', 'Proud of @shin_tsujido @_pandaman64_ , the undergrads who did all the work. I was just a light hand on the tiller.', 'Nice to have #QuantumNative students!']",https://arxiv.org/abs/1903.10963,"NISQ (Noisy, Intermediate-Scale Quantum) computing requires error mitigation to achieve meaningful computation. Our compilation tool development focuses on the fact that the error rates of individual qubits are not equal, with a goal of maximizing the success probability of real-world subroutines such as an adder circuit. We begin by establishing a metric for choosing among possible paths and circuit alternatives for executing gates between variables placed far apart within the processor, and test our approach on two IBM 20-qubit systems named Tokyo and Poughkeepsie. We find that a single-number metric describing the fidelity of individual gates is a useful but imperfect guide. Our compiler uses this subsystem and maps complete circuits onto the machine using a beam search-based heuristic that will scale as processor and program sizes grow. To evaluate the whole compilation process, we compiled and executed adder circuits, then calculated the KL-divergence (a measure of the distance between two probability distributions). For a circuit within the capabilities of the hardware, our compilation increases estimated success probability and reduces KL-divergence relative to an error-oblivious placement. ","Extracting Success from IBM's 20-Qubit Machines Using Error-Aware
  Compilation"
28,1110789192550760448,1038979323653312512,elenamanjavacas,['Our new paper is out! Variability found in the late-T planetary-mass object Ross 458C! Clouds and more clouds! @NASAHubble @danielapai @astro_benlew @TheodoraKaralid @browndwarfs @astromarkmarley @smetchev <LINK> <LINK>'],https://arxiv.org/abs/1903.10702,"Measurements of photometric variability at different wavelengths provide insights into the vertical cloud structure of brown dwarfs and planetary-mass objects. In seven Hubble Space Telescope consecutive orbits, spanning $\sim$10 h of observing time}, we obtained time-resolved spectroscopy of the planetary-mass T8-dwarf Ross 458C using the near-infrared Wide Field Camera 3. We found spectrophotometric variability with a peak-to-peak signal of 2.62$\pm$0.02 % (in the 1.10-1.60~$\mu$m white light curve). Using three different methods, we estimated a rotational period of 6.75$\pm$1.58~h for the white light curve, and similar periods for narrow $J$- and $H$- band light curves. Sine wave fits to the narrow $J$- and $H$-band light curves suggest a tentative phase shift between the light curves with wavelength when we allow different periods between both light curves. If confirmed, this phase shift may be similar to the phase shift detected earlier for the T6.5 spectral type 2MASS J22282889-310262. We find that, in contrast with 2M2228, the variability of Ross~458C shows evidence for a {color trend} within the narrow $J$-band, but gray variations in the narrow $H$-band. The spectral time-resolved variability of Ross 458C might be potentially due to heterogeneous sulfide clouds in the atmosphere of the object. Our discovery extends the study of spectral modulations of condensate clouds to the coolest T dwarfs, planetary-mass companions. ","Cloud Atlas: Rotational Spectral Modulations and potential Sulfide
  Clouds in the Planetary-mass, Late T-type Companion Ross 458C"
29,1110701276377960448,3409898008,„Äà Berger | Dillon „Äâ,"['Check out my new paper ""Dark Matter Through the Quark Vector Current Portal""\n\n<LINK>']",https://arxiv.org/abs/1903.10632,"We consider models of light dark matter coupled to quarks through a vector current interaction. For low energies, these models must be treated through the effective couplings to mesons, which are implemented here through the chiral Lagrangian. We find the rates of dark matter annihilation and decay to the light mesons, and find the expected photon spectrum from the decay of the hadrons. We compare to current and future observations, and show that there is a significant discovery reach for these models. ",Dark Matter Through the Quark Vector Current Portal
30,1110680737039044609,986037210309783552,Aida Behmard,"['New paper - wanna know where some **cool** (i.e., not just water) hydrocarbon snowlines are in your simulated protoplanetary planetary disks?? Try out our binding energies! üå†  <LINK> \n\n...more science explanation below:', 'This was laboratory astrochemistry study that basically involved releasing tiny amounts of hydrocarbon gas (C2H2, C2H4, C2H6, C3H4, C3H6, C3H8) into a vacuum sealed chamber cooled to ~10 K.', 'As soon as the gas hit a substrate within the chamber, it froze out, and then we heated the chamber up **really** slowly and determined at what temperature points the hydrocarbon ices desorbed into the gas phase.', 'We then backed out binding energies which can be used to extrapolate where within a protoplanetary disk these different hydrocarbons would exist in the solid or gas phase, which has major implications for the eventual compositions of planets that form there! üåç', ""@astroshrey bruh i can't believe it's finally over""]",https://arxiv.org/abs/1903.09720,"Small hydrocarbons are an important organic reservoir in protostellar and protoplanetary environments. Constraints on desorption temperatures and binding energies of such hydrocarbons are needed for accurate predictions of where these molecules exist in the ice vs. gas-phase during the different stages of star and planet formation. Through a series of temperature programmed desorption (TPD) experiments, we constrain the binding energies of 2 and 3-carbon hydrocarbons (C$_{2}$H$_{2}$ - acetylene, C$_{2}$H$_{4}$ - ethylene, C$_{2}$H$_{6}$ - ethane, C$_{3}$H$_{4}$ - propyne, C$_{3}$H$_{6}$ - propene, and C$_{3}$H$_{8}$ - propane) to 2200-4200 K in the case of pure amorphous ices, to 2400-4400 K on compact amorphous H$_{2}$O, and to 2800-4700 K on porous amorphous H$_{2}$O. The 3-carbon hydrocarbon binding energies are always larger than the 2-carbon hydrocarbon binding energies. Within the 2- and 3-carbon hydrocarbon families, the alkynes (i.e., least-saturated) hydrocarbons exhibit the largest binding energies, while the alkane and alkene binding energies are comparable. Binding energies are $\sim$5-20% higher on water ice substrates compared to pure ices, which is a small increase compared to what has been measured for other volatile molecules such as CO and N$_{2}$. Thus in the case of hydrocarbons, H$_{2}$O has a less pronounced effect on sublimation front locations (i.e., snowlines) in protoplanetary disks. ",Desorption Kinetics and Binding Energies of Small Hydrocarbons
31,1110523839757180928,3433220662,Anthony Bonato,"['Latest submitted paper on the cop throttling number.  Cop throttling is a new take on cop number and capture time, optimizing the sum of these parameters. My first 9-co-authored paper! #Math #NetworkScience #GraphTheory #Researchpaper <LINK>']",https://arxiv.org/abs/1903.10087,"The cop throttling number $th_c(G)$ of a graph $G$ for the game of Cops and Robbers is the minimum of $k + capt_k(G)$, where $k$ is the number of cops and $capt_k(G)$ is the minimum number of rounds needed for $k$ cops to capture the robber on $G$ over all possible games in which both players play optimally. In this paper, we construct a family of graphs having $th_c(G)= \Omega(n^{2/3})$, establish a sublinear upper bound on the cop throttling number, and show that the cop throttling number of chordal graphs is $O(\sqrt{n})$. We also introduce the product cop throttling number $th_c^{\times}(G)$ as a parameter that minimizes the person-hours used by the cops. This parameter extends the notion of speed-up that has been studied in the context of parallel processing and network decontamination. We establish bounds on the product cop throttling number in terms of the cop throttling number, characterize graphs with low product cop throttling number, and show that for a chordal graph $G$, $th_c^{\times}=1+rad(G)$. ","Optimizing the trade-off between number of cops and capture time in Cops
  and Robbers"
32,1110429659144118275,547776192,Chris Lovell,"['New paper! üéâüö®üìØ <LINK>\n\nWhere we use convolutional neural networks and cosmological simulations to learn the relationship between galaxies spectra and their star formation histories', ""Here's a ~12 minute talk I gave recently @RoyalAstroSoc for a specialist discussion on Machine Learning and AI in Astronomy \n\nhttps://t.co/zEUoOwsQXH""]",https://arxiv.org/abs/1903.10457,"We present a new method for inferring galaxy star formation histories (SFH) using machine learning methods coupled with two cosmological hydrodynamic simulations. We train Convolutional Neural Networks to learn the relationship between synthetic galaxy spectra and high resolution SFHs from the EAGLE and Illustris models. To evaluate our SFH reconstruction we use Symmetric Mean Absolute Percentage Error (SMAPE), which acts as a true percentage error in the low-error regime. On dust-attenuated spectra we achieve high test accuracy (median SMAPE $= 10.5\%$). Including the effects of simulated observational noise increases the error ($12.5\%$), however this is alleviated by including multiple realisations of the noise, which increases the training set size and reduces overfitting ($10.9\%$). We also make estimates for the observational and modelling errors. To further evaluate the generalisation properties we apply models trained on one simulation to spectra from the other, which leads to only a small increase in the error (median SMAPE $\sim 15\%$). We apply each trained model to SDSS DR7 spectra, and find smoother histories than in the VESPA catalogue. This new approach complements the results of existing SED fitting techniques, providing star formation histories directly motivated by the results of the latest cosmological simulations. ","Learning the Relationship between Galaxies Spectra and their Star
  Formation Histories using Convolutional Neural Networks and Cosmological
  Simulations"
33,1110429179915522048,2563532985,Prof Anna Watts,"['New group paper! ""A search for the 835 Hz superburst oscillation signal in the regular thermonuclear bursts of 4U 1636-536"", by Emma van der Wateren, Anna Watts &amp; Laura Ootes, ApJ accepted, <LINK>', 'One of our ongoing research themes is working out why bright spots (burst oscillations) develop in the (thermonuclear) exploding oceans of accreting neutron stars.', ""One star, 4U 1636-536, has shown bright spots both in 'normal' thermonuclear bursts triggered by hydrogen/helium exploding, and in superbursts, which are triggered by carbon exploding."", ""We don't know what causes the spots to form, but one option is the development of large scale mode patterns (standing waves) in the burning ocean."", 'So there was some excitement a few years ago when Simin Mahmoodifar and @xrayastroh found a signal at a different (higher) frequency during a superburst.  Different frequencies co-existing may be a hallmark of mode development.', 'We wondered whether that signal might be there in the normal bursts as well, and so we took a detailed look, combining data for a very sensitive search.', 'We found absolutely nothing at the higher frequency. Nada!    So if that mode is excited, it is very faint - useful constraint on theory development.', 'What we did find however was the usual burst oscillation signal (at lower frequencies, thought to be the spin frequency of the star) - but in the sample of bursts that do not show that signal when studied individually.', 'This means that the cutoff in the burst oscillation mechanism occurs below the detection threshold of existing X-ray telescopes.  So if we build bigger telescopes, like eXTP and @STROBEXastro, there are interesting things to be found!', 'So this is a null result paper - which is useful for theorists - with an encouraging twist!', ""It's also Emma's first paper, based on work that she did for her Bachelors project at @uva_api.  We try to get our Bachelors students working on cutting edge research problems, and it's great to see this work getting to press in a top journal!"", 'Stay tuned for her Masters thesis paper, and for her PhD papers with @gemske79 !!']",https://arxiv.org/abs/1903.09991,"Burst oscillations are brightness asymmetries that develop in the burning ocean during thermonuclear bursts on accreting neutron stars. They have been observed during H/He-triggered (Type I) bursts and Carbon-triggered superbursts. The mechanism responsible is not unknown, but the dominant burst oscillation frequency is typically within a few Hz of the spin frequency, where this is independently known. One of the best-studied burst oscillation sources, 4U 1636-536, has oscillations at $581\,\text{Hz}$ in both its regular Type I bursts and in one superburst. Recently however, Strohmayer \& Mahmoodifar reported the discovery of an additional signal at a higher frequency, $835\,\text{Hz}$, during the superburst. This higher frequency is consistent with the predictions for several types of global ocean mode, one of the possible burst oscillation mechanisms. If this is the case then the same physical mechanism may operate in the normal Type I bursts of this source. In this paper we report a stacked search for periodic signals in the regular Type I bursts: we found no significant signal at the higher frequency, with upper limits for the single trial root mean square (rms) fractional amplitude of 0.57(6)\%. Our analysis did however reveal that the dominant $581\,\text{Hz}$ burst oscillation signal is present at a weak level even in the sample of bursts where it cannot be detected in individual bursts. This indicates that any cutoff in the burst oscillation mechanism occurs below the detection threshold of existing X-ray telescopes. ","A search for the $835\,\text{Hz}$ superburst oscillation signal in the
  regular thermonuclear bursts of 4U 1636-536"
34,1110376836880654336,837133583558987776,Colin Raffel,"['New work w/ @yaoqinucsd, Nicholas Carlini, @goodfellow_ian, and Gary Cottrell on generating imperceptible, robust, and targeted adversarial examples for speech recognition systems! \nPaper: <LINK>\nAudio samples: <LINK>', '@YaoQinUCSD did this awesome work during her internship at Brain last summer. Super excited that she will be returning for an internship this coming summer too!']",https://arxiv.org/abs/1903.10346,"Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions. ","Imperceptible, Robust, and Targeted Adversarial Examples for Automatic
  Speech Recognition"
35,1110357973614776320,393372877,Riccardo Di Sipio üá®üá¶üáÆüáπüá™üá∫,"[""Version 2 of our paper about dijet GAN is out. What's new: extrapolation to high invariant mass, mathematical description of the network, more references to similar works. An yup, it does work with boosted top quarks!\n<LINK> #AI #LHC #Simulation <LINK>""]",https://arxiv.org/abs/1903.02433,"A Generative-Adversarial Network (GAN) based on convolutional neural networks is used to simulate the production of pairs of jets at the LHC. The GAN is trained on events generated using MadGraph5 + Pythia8, and Delphes3 fast detector simulation. We demonstrate that a number of kinematic distributions both at Monte Carlo truth level and after the detector simulation can be reproduced by the generator network with a very good level of agreement. The code can be checked out or forked from the publicly accessible online repository this https URL . ","DijetGAN: A Generative-Adversarial Network Approach for the Simulation
  of QCD Dijet Events at the LHC"
36,1109185523711688704,29178343,Alex Dimakis,"[""New paper: Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes. Our geometry result: Polytopes for ReLu nets are `perfectly glued' i.e. partition space into a polyhedral complex. \n<LINK>\nCode: <LINK>"", '@FeiziSoheil Thanks @FeiziSoheil for pointing this one out. We will look at your paper carefully. Do you explore within one polytope or do you travel from polytope to polytope growing the certificate in your paper?', '@FeiziSoheil ok cool. It is indeed related but different approach. Yours certainly seems faster. We will make sure to refer to your work, thanks for pointing it out.']",https://arxiv.org/abs/1903.08778,"We propose a novel method for computing exact pointwise robustness of deep neural networks for all convex $\ell_p$ norms. Our algorithm, GeoCert, finds the largest $\ell_p$ ball centered at an input point $x_0$, within which the output class of a given neural network with ReLU nonlinearities remains unchanged. We relate the problem of computing pointwise robustness of these networks to that of computing the maximum norm ball with a fixed center that can be contained in a non-convex polytope. This is a challenging problem in general, however we show that there exists an efficient algorithm to compute this for polyhedral complices. Further we show that piecewise linear neural networks partition the input space into a polyhedral complex. Our algorithm has the ability to almost immediately output a nontrivial lower bound to the pointwise robustness which is iteratively improved until it ultimately becomes tight. We empirically show that our approach generates distance lower bounds that are tighter compared to prior work, under moderate time constraints. ","Provable Certificates for Adversarial Examples: Fitting a Ball in the
  Union of Polytopes"
37,1109151349629992961,236480788,Gabor Vattay,['Urban scaling in word usage on @twitter See our new paper on\n@arxiv <LINK>'],https://arxiv.org/abs/1903.04329,"Scaling properties of language are a useful tool for understanding generative processes in texts. We investigate the scaling relations in citywise Twitter corpora coming from the Metropolitan and Micropolitan Statistical Areas of the United States. We observe a slightly superlinear urban scaling with the city population for the total volume of the tweets and words created in a city. We then find that a certain core vocabulary follows the scaling relationship of that of the bulk text, but most words are sensitive to city size, exhibiting a super- or a sublinear urban scaling. For both regimes we can offer a plausible explanation based on the meaning of the words. We also show that the parameters for Zipf's law and Heaps law differ on Twitter from that of other texts, and that the exponent of Zipf's law changes with city size. ",Scaling in Words on Twitter
38,1109115986802700288,253418172,Kenneth Hung,"['A new paper from @wfithian and me on replicability from a statistical perspective! We provide new metrics, new methods to estimate these metrics and applied them to the Reproducibility Project: Psychology data! <LINK>', 'The RP:P metrics are clearly affected by selection bias, but does selection bias alone explain the observed phenomenon? Our metrics and methods for assessing replicability allow us to see through the effects of selection bias.\n\nhttps://t.co/Ee7cQwkGVX']",https://arxiv.org/abs/1903.08747,"Large-scale replication studies like the Reproducibility Project: Psychology (RP:P) provide invaluable systematic data on scientific replicability, but most analyses and interpretations of the data fail to agree on the definition of ""replicability"" and disentangle the inexorable consequences of known selection bias from competing explanations. We discuss three concrete definitions of replicability based on (1) whether published findings about the signs of effects are mostly correct, (2) how effective replication studies are in reproducing whatever true effect size was present in the original experiment, and (3) whether true effect sizes tend to diminish in replication. We apply techniques from multiple testing and post-selection inference to develop new methods that answer these questions while explicitly accounting for selection bias. Our analyses suggest that the RP:P dataset is largely consistent with publication bias due to selection of significant effects. The methods in this paper make no distributional assumptions about the true effect sizes. ",Statistical Methods for Replicability Assessment
39,1108956349260419073,2819715191,Antonella Palmese,"['Can we use the stellar content of galaxy clusters for cosmological analyses with @theDESurvey? Check out my new paper with @msoares_santos, @MariaElidaiana and DES! <LINK>']",http://arxiv.org/abs/1903.08813,"We introduce a galaxy cluster mass observable, $\mu_\star$, based on the stellar masses of cluster members, and we present results for the Dark Energy Survey (DES) Year 1 observations. Stellar masses are computed using a Bayesian Model Averaging method, and are validated for DES data using simulations and COSMOS data. We show that $\mu_\star$ works as a promising mass proxy by comparing our predictions to X-ray measurements. We measure the X-ray temperature-$\mu_\star$ relation for a total of 150 clusters matched between the wide-field DES Year 1 redMaPPer catalogue, and Chandra and XMM archival observations, spanning the redshift range $0.1<z<0.7$. For a scaling relation which is linear in logarithmic space, we find a slope of $\alpha = 0.488\pm0.043$ and a scatter in the X-ray temperature at fixed $\mu_\star$ of $\sigma_{{\rm ln} T_X|\mu_\star}=0.266^{+0.019}_{-0.020}$ for the joint sample. By using the halo mass scaling relations of the X-ray temperature from the Weighing the Giants program, we further derive the $\mu_\star$-conditioned scatter in mass, finding $\sigma_{{\rm ln} M|\mu_\star}=0.26^{+ 0.15}_{- 0.10}$. These results are competitive with well-established cluster mass proxies used for cosmological analyses, showing that $\mu_\star$ can be used as a reliable and physically motivated mass proxy to derive cosmological constraints. ","Stellar mass as a galaxy cluster mass proxy: application to the Dark
  Energy Survey redMaPPer clusters"
40,1108942932466270209,77317235,Antonio Ortega,"['Check out our new tool to visualize Graph Fourier Transforms. Paper: <LINK> Code: <LINK> Makes it easier to visualize localization, sampling, filtering #graphsignalprocessing <LINK>', 'This GFT corresponds to a graph with 3 clusters, separated by thick vertical lines. The largest frequencies only appear in two of the clusters. Red circles indicate the order of sampling.']",https://arxiv.org/abs/1903.08827,"Recent progress in graph signal processing (GSP) has addressed a number of problems, including sampling and filtering. Proposed methods have focused on generic graphs and defined signals with certain characteristics, e.g., bandlimited signals, based on t he graph Fourier transform (GFT). However, the effect of GFT properties (e.g., vertex localization) on the behavior of such methods is not as well understood. In this paper, we propose novel GFT visualization tools and provide some examples to illustrate certain GFT properties and their impact on sampling or wavelet transforms. ","What's in a frequency: new tools for graph Fourier Transform
  visualization"
41,1108918845576609792,565140816,K-G Lee,"[""I'm excited to co-author a new paper by UC Berkeley student Ben Horowitz, presenting the Tomographic Absorption Reconstruction &amp; Density Inference Scheme (TARDIS), a method to study the cosmic web observed with Ly-alpha forest tomography at z~2-3 (1/6)\n\n<LINK>"", 'Given a dense grid of 1D Ly-a forest sightlines observed at z~2-3, TARDIS iterates over initial density fluctuations to solve for the one most likely to match the observed data given a forward model.\n\nThis yields the underlying 3D matter density + velocity field (2/6) https://t.co/lsCSmBqbfm', '(Strong overdensities are still a bit ratty due to saturation+blending of Ly-alpha lines, but the moderate density cosmic web looks good and reproduces the cosmic web components in the true underlying field by 70-80%) (3/6)', ""What's more fun is that with TARDIS, we can go ahead and evolve our inferred z=2.5 density field all the way to z=0! \n\nBelow is the inferred z=0 density field assuming z=2.5 observations with Keck-CLAMATO/Subaru-PFS or with 30m-class telescopes. Not too bad! (4/6) https://t.co/RzumaZqMjg"", 'If you have a sample of z=2.5 galaxies in the same volume as our IGM tomography data, they act as tracer particles as the density evolves.\n\nSo you can track them together with the TARDIS density field and see where they end up at z=0. (5/6) https://t.co/GnQDoav0f7', 'We can *predict*, to reasonable accuracy, whether individual galaxies at Cosmic Noon will end up in nodes, filaments, sheets or voids by z=0. \n\nCLAMATO and PFS will be able to make the first attempts at this, but we need enough data! (6/6)']",https://arxiv.org/abs/1903.09049,"Recent Lyman-$\alpha$ forest tomography measurements of the intergalactic medium (IGM) have revealed a wealth of cosmic structures at high redshift ($z\sim 2.5$). In this work, we present the Tomographic Absorption Reconstruction and Density Inference Scheme (TARDIS), a new chrono-cosmographic analysis tool for understanding the formation and evolution of these observed structures. We use maximum likelihood techniques with a fast non-linear gravitational model to reconstruct the initial density field of the observed regions. We find that TARDIS allows accurate reconstruction of smaller scale structures than standard Wiener filtering techniques. Applying this technique to mock Lyman-$\alpha$ forest data sets that simulate ongoing and future surveys such as CLAMATO, Subaru-PFS or the ELTs, we are able to infer the underlying matter density field at observed redshift and classify the cosmic web structures. We find good agreement with the underlying truth both in the characteristic eigenvalues and eigenvectors of the pseudo-deformation tensor, with the eigenvalues inferred from 30m-class telescopes correlated at $r=0.95$ relative to the truth. As an output of this method, we are able to further evolve the inferred structures to late time ($z=0$), and also track the trajectories of coeval $z=2.5$ galaxies to their $z=0$ cosmic web environments. ","TARDIS Paper I: A Constrained Reconstruction Approach to Modeling the
  z~2.5 Cosmic Web Probed by Lyman-alpha Forest Tomography"
42,1108694486497005570,17373048,Rodrigo Nemmen,"['New paper out with Rogemar Riffel et al. MNRAS in press  <LINK> Precessing winds from the nucleus of the prototype Red Geyser? Brief description in this thread #astronomy #astrophysics @AstroUSP @AstronomiaUSP #papers (image credit: @esa AOES) <LINK>', 'We studied the Akira galaxy‚Äînamed after the Akira manga. Its companion galaxy is called Tetsuo. Akira is an interesting galaxy because it hosts a supermassive black hole fed at quite low rates‚Äîwe call it a low-luminosity active galactic nucleus https://t.co/TBqTNcmTpg https://t.co/WZSGhKwsq7', 'The black hole seems to be ejecting gas quite vigorously. In fact so vigorously that the BH outflow is capable of quenching star formation in the galaxy. Cheung et al. called this galaxy a ""red geyser"".', 'We observed the nucleus of the galaxy (where the black hole is) with Gemini integral field spectroscopy in order to characterize the black hole outflow. This is a powerful instrument because it gives high-spatial resolution information on several emission and absorption lines', 'Here is a plot of the equivalent widths of hydrogen alpha. The outflow is marked in orange. Notice that the EWs are telling us that this outflow is consistent with being produced from a black hole (AGN), not from stars https://t.co/AhGE2Ag8x5', 'Here is the money plot of the paper. It tells us that the outflow coming from the black hole is changing its orientation as it propagates away from the galactic nucleus! How to interpret this? First of all, we do not think we are seeing a jet because this galaxy does not show https://t.co/ghwgTyZksP', 'extended radio structures. We think this is a subrelativistic, uncollimated wind like the att. cartoon. We interpreted this as a precessing wind, with the likely cause of the precession being a misalignment between the accretion disk and the BH spin aka Lense-Thirring precession https://t.co/bH4kpEVx6v', 'Read the paper for more info\n\nCredit for previous illustration: @esascience AOES Medialab https://t.co/nt3B7sTZCk']",https://arxiv.org/abs/1903.08032,"Super-massive black holes (SMBH) are present at the center of most galaxies, with the related mass accretion processes giving origin to outflows in Active Galactic Nuclei (AGN). It has been presumed that only intense winds from luminous AGN were able to suppress star formation until the discovery of a new class of galaxies with no recent star formation and with the nucleus in a quiescent state showing kpc scale outflows. We used SDSS MaNGA and Gemini Integral Field Spectroscopy of the prototype Red Geyser Akira and found that the orientation of the outflow changes by about 50$^\circ$ from its nucleus to kpc scales. A possible interpretation is that the outflow is produced by a precessing accretion disk due to a misalignment between the orientation of the disk and the spin of the SMBH. The precession of the central source is also supported by a similar change in the orientation of the ionization pattern. Although similar behavior has commonly being reported for collimated relativistic jets, the precession of an AGN wide wind is reported here for the first time, implying on a larger work surface of the wind, which in turn increases the star formation suppression efficiency of the outflow. ",Precessing winds from the nucleus of the prototype Red Geyser?
43,1108387694072287233,2800204849,Andrew Gordon Wilson,"['Exact Gaussian Processes on a Million Points! Our new paper (with GPyTorch code): <LINK> <LINK>', '@roydanroy The method does not have any such restrictions. It is generally applicable.']",https://arxiv.org/abs/1903.08114,"Gaussian processes (GPs) are flexible non-parametric models, with a capacity that grows with the available data. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points, a task previously thought to be impossible with current computing hardware, in less than 2 hours. Moreover, our approach is generally applicable, without constraints to grid data or specific kernel classes. Enabled by this scalability, we perform the first-ever comparison of exact GPs against scalable GP approximations on datasets with $10^4 \!-\! 10^6$ data points, showing dramatic performance improvements. ",Exact Gaussian Processes on a Million Data Points
44,1108368964621606919,1254221522,Leo Burtscher,"['New paper out today (first author: David Rosario @durham_uni ) about NGC 2110 showing that appearances are deceiving: A region seemingly devoid of molecular gas (left) glares brightly in the near-IR molecular hydrogen line (center). (1/2) <LINK> <LINK>', 'Molecular gas may be more resilient to AGN feedback than you have thought! (2/2) #AGN https://t.co/TUSzz84lVx', '@brandherd81 Thanks. Looking forward to seeing the CARS paper, too! :-) The radio jet in this galaxy is well studied (Nagar+ 1999) and roughly along the same direction as the ALMA 1mm continuum shown in our figure in the right panel (see their figure attached here). https://t.co/B0J8tPlcU0']",https://arxiv.org/abs/1903.07637,"The impact of Active Galactic Nuclei (AGN) on star formation has implications for our understanding of the relationships between supermassive black holes and their galaxies, as well as for the growth of galaxies over the history of the Universe. We report on a high-resolution multi-phase study of the nuclear environment in the nearby Seyfert galaxy NGC 2110 using the Atacama Large Millimeter Array (ALMA), Hubble and Spitzer Space Telescopes, and the Very Large Telescope/SINFONI. We identify a region that is markedly weak in low-excitation CO $2\rightarrow1$ emission from cold molecular gas, but appears to be filled with ionised and warm molecular gas, which indicates that the AGN is directly influencing the properties of the molecular material. Using multiple molecular gas tracers, we demonstrate that, despite the lack of CO line emission, the surface densities and kinematics of molecular gas vary smoothly across the region. Our results demonstrate that the influence of an AGN on star-forming gas can be quite localized. In contrast to widely-held theoretical expectations, we find that molecular gas remains resilient to the glare of energetic AGN feedback. ","An accreting supermassive black hole irradiating molecular gas in NGC
  2110"
45,1108365781174272006,3813580887,Paul B√ºrkner,"['In our (@avehtari, @StatModeling, @dan_p_simpson, Bob, and my) new paper, we fix the Rhat convergence diagnostic and provide additional diagnostics that we argue should become the new standard to assess convergence of MCMC algorithms. Preprint available at <LINK>']",https://arxiv.org/abs/1903.08008,"Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic $\widehat{R}$ of Gelman and Rubin (1992) has serious flaws. Traditional $\widehat{R}$ will fail to correctly diagnose convergence failures when the chain has a heavy tail or when the variance varies across the chains. In this paper we propose an alternative rank-based diagnostic that fixes these problems. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give recommendations for how these methods should be used in practice. ","Rank-normalization, folding, and localization: An improved $\widehat{R}$
  for assessing convergence of MCMC"
46,1108250619452760065,399544729,Enzo KatolaZ Nicosia,"[""Our new paper on Algorithmic Complexity of Multiplex Networks today on the arxiv <LINK> and also presented by @andreasantor0 at @CompleNet. Don't miss out :-P @QMULMaths""]",https://arxiv.org/abs/1903.08049,"Multilayer networks preserve full information about the different interactions among the constituents of a complex system, and have recently proven quite useful in modelling transportation networks, social circles, and the human brain. A fundamental and still open problem is to assess if and when the multilayer representation of a system provides a qualitatively better model than the classical single-layer aggregated network. Here we tackle this problem from an algorithmic information theory perspective. We propose an intuitive way to encode a multilayer network into a bit string, and we define the complexity of a multilayer network as the ratio of the Kolmogorov complexity of the bit strings associated to the multilayer and to the corresponding aggregated graph. We find that there exists a maximum amount of additional information that a multilayer model can encode with respect to the equivalent single-layer graph. We show how our complexity measure can be used to obtain low-dimensional representations of multidimensional systems, to cluster multilayer networks into a small set of meaningful super-families, and to detect tipping points in the evolution of different time-varying multilayer graphs. Interestingly, the low-dimensional multiplex networks obtained with the proposed method also retain most of the dynamical properties of the original systems, as demonstrated for instance by the preservation of the epidemic threshold in the multiplex SIS model. These results suggest that information-theoretic approaches can be effectively employed for a more systematic analysis of static and time-varying multidimensional complex systems. ",Algorithmic complexity of multiplex networks
47,1108234366046617600,109394186,Rodrigo Canaan,"['My new paper with @ChristophSalge @togelius and @nealen is out!  We discuss what it means for competition in games between humans and AI to be ""fair"" and which factors impact this perception. Read at <LINK> <LINK>']",https://arxiv.org/abs/1903.07008,"From the beginning if the history of AI, there has been interest in games as a platform of research. As the field developed, human-level competence in complex games became a target researchers worked to reach. Only relatively recently has this target been finally met for traditional tabletop games such as Backgammon, Chess and Go. Current research focus has shifted to electronic games, which provide unique challenges. As is often the case with AI research, these results are liable to be exaggerated or misrepresented by either authors or third parties. The extent to which these games benchmark consist of fair competition between human and AI is also a matter of debate. In this work, we review the statements made by authors and third parties in the general media and academic circle about these game benchmark results and discuss factors that can impact the perception of fairness in the contest between humans and machines ","Leveling the Playing Field -- Fairness in AI Versus Human Game
  Benchmarks"
48,1108182688521842689,699180629246672897,Subhrajit Roy,['Check out our new paper on semi-supervised deep learning for identifying abnormal brain signals.\n\n<LINK>'],https://arxiv.org/abs/1903.07822,"Systems that can automatically analyze EEG signals can aid neurologists by reducing heavy workload and delays. However, such systems need to be first trained using a labeled dataset. While large corpuses of EEG data exist, a fraction of them are labeled. Hand-labeling data increases workload for the very neurologists we try to aid. This paper proposes a semi-supervised learning workflow that can not only extract meaningful information from large unlabeled EEG datasets but also make predictions with minimal supervision, using labeled datasets as small as 5 examples. ","A semi-supervised deep learning algorithm for abnormal EEG
  identification"
49,1108091061656793090,92989497,Robert Haines,"['New paper from the @csmcr @IoCoding team, ""A Methodology for Using GitLab for Software Engineering Learning Analytics"", to be presented at #CHASE/#ICSE2019:\n\n<LINK>\n\nCC @UoM_eResearch']",https://arxiv.org/abs/1903.06772,"To bridge the digital skills gap, we need to train more people in Software Engineering techniques. This paper reports on a project exploring the way students solve tasks using collaborative development platforms and version control systems, such as GitLab, to find patterns and evaluation metrics that can be used to improve the course content and reflect on the most common issues the students are facing. In this paper, we explore Learning Analytics approaches that can be used with GitLab and similar tools, and discuss the challenges raised when applying those approaches in Software Engineering Education, with the objective of building a pipeline that supports the full Learning Analytics cycle, from data extraction to data analysis. We focus in particular on the data anonymisation step of the proposed pipeline to explore the available alternatives to satisfy the data protection requirements when handling personal information in academic environments for research purposes. ","A Methodology for Using GitLab for Software Engineering Learning
  Analytics"
50,1108040446998130689,97191218,Pantelis Sopasakis,['Our new paper on risk-averse **risk-constrained** optimal control is out: <LINK>. \nFree #MATLAB toolbox: <LINK>. <LINK>'],https://arxiv.org/abs/1903.06749,"Multistage risk-averse optimal control problems with nested conditional risk mappings are gaining popularity in various application domains. Risk-averse formulations interpolate between the classical expectation-based stochastic and minimax optimal control. This way, risk-averse problems aim at hedging against extreme low-probability events without being overly conservative. At the same time, risk-based constraints may be employed either as surrogates for chance (probabilistic) constraints or as a robustification of expectation-based constraints. Such multistage problems, however, have been identified as particularly hard to solve. We propose a decomposition method for such nested problems that allows us to solve them via efficient numerical optimization methods. Alongside, we propose a new form of risk constraints which accounts for the propagation of uncertainty in time. ",Risk-averse risk-constrained optimal control
51,1108036811207659521,1556995800,Ted Corcovilos,"[""New paper! Jenna and I show how we're building #quasicrystal potentials.\n\nApplied Optics <LINK>\nor on arXiv: <LINK>""]",https://arxiv.org/abs/1903.06610,"Quasicrystals are nonperiodic structures having no translational symmetry but nonetheless possessing long-range order. The material properties of quasicrystals, particularly their low-temperature behavior, defy easy description. We present a compact optical setup for creating quasicrystal optical potentials with 5-fold symmetry using interference of nearly co-propagating beams for use in ultracold atom quantum simulation experiments. We verify the optical design through numerical simulations and demonstrate a prototype system. We also discuss generating phason excitations and quantized transport in the quasicrystal through phase modulation of the beams. ","Two-dimensional optical quasicrystal potentials for ultracold atom
  experiments"
52,1108008936756506624,4475055297,Ming-Yu Liu,"['Check out our #CVPR19 oral paper on a new conditional normalization layer for semantic image synthesis #SPADE and its demo app #GauGAN \npaper <LINK> \nwebsite <LINK> \nvideo1 <LINK> \n@tcwang0509 @junyanz89 <LINK>', '@ankurhandos @hardmaru Many training photos contain trees near water body with reflections. The #GAN eventually learns this correlation in the distribution. Interestingly, this reflection was only learned in later epochs.', '@avsa @genekogan @tcwang0509 @junyanz89 I think it will still take sometime to reach production quality. But our engineering team will release this demo as a free online service this summer for people to try out.']",https://arxiv.org/abs/1903.07291,"We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at this https URL . ",Semantic Image Synthesis with Spatially-Adaptive Normalization
53,1107990608361279489,97191218,Pantelis Sopasakis,['Our new paper on #SuperSCS is out (<LINK>). SuperSCS is a fast and accurate solver for large-scale conic optimization; it is available at <LINK>. <LINK>'],https://arxiv.org/abs/1903.06477,We present SuperSCS: a fast and accurate method for solving large-scale convex conic problems. SuperSCS combines the SuperMann algorithmic framework with the Douglas-Rachford splitting which is applied on the homogeneous self-dual embedding of conic optimization problems: a model for conic optimization problems which simultaneously encodes the optimality conditions and infeasibility/unboundedness certificates for the original problem. SuperMann allows the use of fast quasi-Newtonian directions such as a modified restarted Broyden-type direction and Anderson's acceleration. ,SuperSCS: fast and accurate large-scale conic optimization
54,1107922578109353984,523241142,Juste Raimbault,"['New paper: ""Multi-dimensional Urban Network Percolation""\nA new percolation algorithm for urban networks takes into account population distribution and road network topology; applied to quantify sustainability of endogenous European mega-city-regions.\n<LINK>', '@OpenMOLE It is indeed a next step suggested in the paper, calibrating four stage model on transportation ghg emissions !']",https://arxiv.org/abs/1903.07141,"Network percolation has recently been proposed as a method to characterize the global structure of an urban system form the bottom-up. This paper proposes to extend urban network percolation in a multi-dimensional way, to take into account both urban form (spatial distribution of population) and urban functions (here as properties of transportation networks). The method is applied to the European urban system to reconstruct endogenous urban regions. The variable parametrization allows to consider patterns of optimization for two stylized contradictory sustainability indicators (economic performance and greenhouse gases emissions). This suggests a customizable spatial design of policies to develop sustainable territories. ",Multi-dimensional Urban Network Percolation
55,1107921960451878912,523241142,Juste Raimbault,"['New paper with J. Perret from @IGNFrance : ""Generating urban morphologies at large scales"", using @OpenMOLE\nWe introduce measures of urban form, compute them on a large set of real configurations, and calibrate multiple synthetic generators\n<LINK>']",https://arxiv.org/abs/1903.06807,"At large scales, typologies of urban form and corresponding generating processes remain an open question with important implications regarding urban planning policies and sustainability. We propose in this paper to generate urban configurations at large scales, typically of districts, with morphogenesis models, and compare these to real configurations according to morphological indicators. Real values are computed on a large sample of districts taken in European urban areas. We calibrate each model and show their complementarity to approach the variety of real urban configurations, paving the way to multi-model approaches of urban morphogenesis. ",Generating urban morphologies at large scales
56,1107894517032599553,617213120,Daniel Murfet,['My new paper on A-infinity categories and matrix factorisations is now on the arXiv: <LINK>'],https://arxiv.org/abs/1903.07211,"We study constructive $A_\infty$-models of the DG-category of matrix factorisations of a potential over a commutative $\mathbb{Q}$-algebra $k$, consisting of a Hom-finite $A_\infty$-category equipped with an $A_\infty$-idempotent functor. ",Constructing $A_\infty$-categories of matrix factorisations
57,1107812099864973312,4475055297,Ming-Yu Liu,"['Check out our #CVPR19 oral paper on a new conditional normalization layer for semantic image synthesis #SPADE and its demo app #GauGAN \npaper <LINK>\nwebsite <LINK>\nvideo1 <LINK>\n@tcwang0509 @junyanz89', 'We will run the demo live in the NVIDIA booth through #GTC19 . Please come to show us your creativity.\nI will give a talk on #SPADE in #GDC19 at 3:30pm on this Wednesday\n@tcwang0509 will talk about #SPADE and #vid2vid in #GTC19 at 10am on this Thursday.', 'also check out our second video https://t.co/KJgEnveYY9 on how to use the app.']",https://arxiv.org/abs/1903.07291,"We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at this https URL . ",Semantic Image Synthesis with Spatially-Adaptive Normalization
58,1107676415162494976,34376328,Tal Linzen,"['New NAACL paper with @ravfogel and @yoavgo: to study what makes the syntax of language difficult to learn for an RNN, we created synthetic versions of an English corpus. <LINK>', ""For example, we permuted the order of the subject, verb and object, and trained an RNN to predict at the verb whether its subject is singular or plural (and same for object, following up on @ravfogel's work on Basque https://t.co/FedbBgHC9I): https://t.co/qfkqgAcvyA"", 'RNNs were susceptible to attraction effects - subject prediction was easier in the subject-verb-object language (like English), where the subject and the verb are adjacent, than in the subject-object-verb language https://t.co/FScDUEXymf', '(This is unlike the typology of word order - subject-object-verb is a very common word order in the languages of the world.)', 'When objects were withheld in training, the RNNs learned the generalization that the subject is the argument that precedes the verb rather the generalization that it is the *first* argument (which is equally compatible with the data), supporting a recency inductive bias.', 'Overt case marking made syntactic prediction easier, and so did polypersonal agreement (with both object and subject), as in Basque.', ""We clearly just scratched the surface of the questions that can be asked with this methodology (which was borrowed from @Laplace_wdd and @adveisner's parsing work) - other learners, other typological properties, comparison to humans, etc, would all be interesting directions.""]",https://arxiv.org/abs/1903.06400,"How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs' syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. ","Studying the Inductive Biases of RNNs with Synthetic Variations of
  Natural Languages"
59,1107659318864670720,1513989272,Dr. Breanna Binder,"[""My new paper is on arXiv today!  <LINK>  TLDR; Stellar wind bow shocks *should* produce X-rays. @chandraxray can't see them, but @lynxobservatory will.\n\n(and come see my poster/chat with me about it at #head2019!)""]",https://arxiv.org/abs/1903.06279,"We present a stacking analysis of 2.61 Msec of archival Chandra observations of stellar wind bow shocks. We place an upper limit on the X-ray luminosity of IR-detected bow shocks of $<2\times10^{29}$ erg s$^{-1}$, a more stringent constraint than has been found in previous archival studies and dedicated observing campaigns of nearby bow shocks. We compare the X-ray luminosities and $L_X/L_{\rm bol}$ ratios of bow shock driving stars to those of other OB stars within the Chandra field of view. Driving stars are, on average, of later spectral type than the ""field of view"" OB stars, and we do not observe any unambiguously high $L_X/L_{\rm bol}$ ratios indicative of magnetic stars in our sample. We additionally asses the feasibility of detecting X-rays from stellar wind bow shocks with the proposed Lynx X-ray Observatory. If the X-ray flux originating from the bow shocks is just below our Chandra detection limit, the nearest bow shock in our sample (at $\sim$0.4 kpc with an absorbing column of $\sim10^{21}$ cm$^{-2}$) should be observable with Lynx in exposure times on the order of $\sim$100 kiloseconds. ",Searching for Faint X-ray Emission from Galactic Stellar Wind Bow Shocks
60,1107646084338380800,706175245976248321,Oscar Viyuela,['Our new paper on the arXiv!\nMore on long-range interactions in topological superconductors:\n<LINK>'],https://arxiv.org/abs/1903.06175,"Planar topological superconductors with power-law-decaying pairing display different kinds of topological phase transitions where quasiparticles dubbed nonlocal-massive Dirac fermions emerge. These exotic particles form through long-range interactions between distant Majorana modes at the boundary of the system. We show how these propagating-massive Dirac fermions neither mix with bulk states nor Anderson-localize up to large amounts of static disorder despite being finite energy. Analyzing the density of states (DOS) and the band spectrum of the long-range topological superconductor, we identify the formation of an edge gap and a surprising double peak structure in the DOS which can be linked to a twisting of energy bands with nontrivial topology. Our findings are amenable to experimental verification in the near future using atom arrays on conventional superconductors, planar Josephson junctions on two-dimensional electron gases, and Floquet driving of topological superconductors. ","Band twisting and resilience to disorder in long-range topological
  superconductors"
61,1107466644916338689,2685835440,Tian Zhang,"['New paper with Oscar Dahlsten and Vlatko Vedral, on defining spacetime quantum states in continuous variables, i.e. non-relativistic quantum fields, from measurement correlations. [1903.06312] <LINK>']",https://arxiv.org/abs/1903.06312,"Space-time is one of the most essential, yet most mysterious concepts in physics. In quantum mechanics it is common to understand time as a marker of instances of evolution and define states around all the space but at one time; while in general relativity space-time is taken as a combinator, curved around mass. Here we present a unified approach on both space and time in quantum theory, and build quantum states across spacetime instead of only on spatial slices. We no longer distinguish measurements on the same system at different times with measurements on different systems at one time and construct spacetime states upon these measurement statistics. As a first step towards non-relativistic quantum field theory, we consider how to approach this in the continuous-variable multi-mode regime. We propose six possible definitions for spacetime states in continuous variables, based on four different measurement processes: quadratures, displaced parity operators, position measurements and weak measurements. The basic idea is to treat different instances of time as different quantum modes. They are motivated by the pseudo-density matrix formulation among indefinite causal structures and the path integral formalism. We show that these definitions lead to desirable properties, and raise the differences and similarities between spatial and temporal correlations. An experimental proposal for tomography is presented, construing the operational meaning of the spacetime states. ","Different instances of time as different quantum modes: quantum states
  across space-time for continuous variables"
62,1106930520301150210,2785337469,Sebastian Ruder,"['New paper with @mattthemathman &amp; @nlpnoah on adapting pretrained representations: We compare feature extraction &amp; fine-tuning with ELMo and BERT and try to give several guidelines for adapting pretrained representations in practice. <LINK> <LINK>', ""@michalwols @mattthemathman @nlpnoah For BERT, we mostly modified the provided Colaboratory notebooks, while for ELMo, we modified scripts in AllenNLP, so both of those should be fairly easily reproducible using existing resources. I might upload the analysis scripts if there's interest.""]",https://arxiv.org/abs/1903.05987,"While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner. ","To Tune or Not to Tune? Adapting Pretrained Representations to Diverse
  Tasks"
63,1106573677380792321,3290170484,Jarrod McClean,"['Will quantum codes prove useful on near term quantum computers? To find out, read more in our new paper that we previewed at APS March meeting! <LINK>']",https://arxiv.org/abs/1903.05786v1,"With the rapid developments in quantum hardware comes a push towards the first practical applications on these devices. While fully fault-tolerant quantum computers may still be years away, one may ask if there exist intermediate forms of error correction or mitigation that might enable practical applications before then. In this work, we consider the idea of post-processing error decoders using existing quantum codes, which are capable of mitigating errors on encoded logical qubits using classical post-processing with no complicated syndrome measurements or additional qubits beyond those used for the logical qubits. This greatly simplifies the experimental exploration of quantum codes on near-term devices, removing the need for locality of syndromes or fast feed-forward, allowing one to study performance aspects of codes on real devices. We provide a general construction equipped with a simple stochastic sampling scheme that does not depend explicitly on a number of terms that we extend to approximate projectors within a subspace. This theory then allows one to generalize to the correction of some logical errors in the code space, correction of some physical unencoded Hamiltonians without engineered symmetries, and corrections derived from approximate symmetries. In this work, we develop the theory of the method and demonstrate it on a simple example with the perfect $[[5,1,3]]$ code, which exhibits a pseudo-threshold of $p \approx 0.50$ under a single qubit depolarizing channel applied to all qubits. We also provide a demonstration under the application of a logical operation and performance on an unencoded hydrogen molecule, which exhibits a significant improvement over the entire range of possible errors incurred under a depolarizing channel. ",] Decoding quantum errors with subspace expansions
64,1106553929243021314,191872322,Jan Claes,"['New preprint available with Sarah Pissierssens and @ProfPoels : ""The ""Physics of Diagrams"": Revealing the scientific basis of graphical representation design"" <LINK>, published at <LINK>. <LINK>']",https://arxiv.org/abs/1903.05941,"Data is omnipresent in the modern, digital world and a significant number of people need to make sense of data as part of their everyday social and professional life. Therefore, together with the rise of data, the design of graphical representations has gained importance and attention. Yet, although a large body of procedural knowledge about effective visualization exists, the quality of representations is often reported to be poor, proposedly because these guidelines are scattered, unstructured and sometimes perceived as contradictive. Therefore, this paper describes a literature research addressing these problems. The research resulted in the collection and structuring of 81 guidelines and 34 underlying propositions, as well as in the derivation of 7 foundational principles about graphical representation design, called the ""Physics of Diagrams"", which are illustrated with concrete, practical examples throughout the paper. ","The ""Physics of Diagrams"": Revealing the scientific basis of graphical
  representation design"
65,1106468015040839680,92989497,Robert Haines,"['New paper, to be presented at #CHASE/#ICSE2019 (<LINK>):\n\n""What Makes Research Software Sustainable? An Interview Study With Research Software Engineers"" by Mario Rosado de Souza, @CarolineEJay, @markelvigo and me!\n\n<LINK>', '@CarolineEJay @markelvigo And I should have also mentioned that full (anonymized) interview transcriptions are available as well: https://t.co/ltwUbQqsPL']",http://arxiv.org/abs/1903.06039,"Software is now a vital scientific instrument, providing the tools for data collection and analysis across disciplines from bioinformatics and computational physics, to the humanities. The software used in research is often home-grown and bespoke: it is constructed for a particular project, and rarely maintained beyond this, leading to rapid decay, and frequent `reinvention of the wheel'. Understanding how to develop sustainable research software, such that it is suitable for future reuse, is therefore of interest to both researchers and funders, but how to achieve this remains an open question. Here we report the results of an interview study examining how research software engineers -- the people actively developing software in an academic research environment -- subjectively define software sustainability. Thematic analysis of the data reveals two interacting dimensions: \emph{intrinsic sustainability}, which relates to internal qualities of software, such as modularity, encapsulation and testability, and \emph{extrinsic sustainability}, concerning cultural and organisational factors, including how software is resourced, supported and shared. Research software engineers believe an increased focus on quality and discoverability are key factors in increasing the sustainability of academic research software. ","What Makes Research Software Sustainable? An Interview Study With
  Research Software Engineers"
66,1106318826956820486,797888987675365377,Tom Rainforth,"['Check out our new paper: ""Variational Estimators for Bayesian Optimal Experimental Design"" lead by Adam Foster. <LINK>\n\nWe introduce efficient variational estimators for expected information gain, allowing fast and accurate Bayesian optimal experimental design. <LINK>', 'Full author list: Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, @yeewhye, myself, and Noah Goodman', '@drfeldt @yeewhye Thanks. The code is not quite ready yet but will be released soon as part of pyro']",http://arxiv.org/abs/1903.05480,"Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments. ",Variational Bayesian Optimal Experimental Design
67,1106207075892449280,348355646,Victor See,"[""A new paper out from your's truly today! We compare Zeeman broadening (small scale) and Zeeman-Doppler imaging (large scale) observations. @seanpmatt @AdamF_Astro @AlineVidotto @pascalou_petit @mengelm (and many others not on twitter!) <LINK>"", 'On a personal note, I‚Äôve had some mental health issues while I worked on this paper. While I am well over the worst of it, it meant that this paper has taken longer to finish up than anticipated. But finish it I did. #MentalHealthMatters', 'I could not have managed it without the help of a great many friends, family and mentors for which I am extremely grateful! If anyone out there is struggling, I am more than happy to pay forward the support that others gave me. Feel free to contact me! #MentalHealthMatters']",https://arxiv.org/abs/1903.05595,"Low-mass stars are known to have magnetic fields that are believed to be of dynamo origin. Two complementary techniques are principally used to characterise them. Zeeman-Doppler imaging (ZDI) can determine the geometry of the large-scale magnetic field while Zeeman broadening can assess the total unsigned flux including that associated with small-scale structures such as spots. In this work, we study a sample of stars that have been previously mapped with ZDI. We show that the average unsigned magnetic flux follows an activity-rotation relation separating into saturated and unsaturated regimes. We also compare the average photospheric magnetic flux recovered by ZDI, $\langle B_V\rangle$, with that recovered by Zeeman broadening studies, $\langle B_I\rangle$. In line with previous studies, $\langle B_V\rangle$ ranges from a few % to $\sim$20% of $\langle B_I\rangle$. We show that a power law relationship between $\langle B_V\rangle$ and $\langle B_I\rangle$ exists and that ZDI recovers a larger fraction of the magnetic flux in more active stars. Using this relation, we improve on previous attempts to estimate filling factors, i.e. the fraction of the stellar surface covered with magnetic field, for stars mapped only with ZDI. Our estimated filling factors follow the well-known activity-rotation relation which is in agreement with filling factors obtained directly from Zeeman broadening studies. We discuss the possible implications of these results for flux tube expansion above the stellar surface and stellar wind models. ",Estimating magnetic filling factors from Zeeman-Doppler magnetograms
68,1106099441692565504,3079023467,Dr. Emma Beasor,"['New paper! <LINK>', 'In short: we‚Äôve looked at 4 young clusters and compared ages found for them when using the main sequence turn off and when using the lowest luminosity red supergiants', 'From this, we‚Äôve found evidence for blue straggler-like stars (objects probably formed through mergers/binary interaction) in young clusters.', 'Who knew ü§∑\u200d‚ôÄÔ∏èüòä']",https://arxiv.org/abs/1903.05106,"There is growing evidence that star clusters can no longer be considered simple stellar populations (SSPs). Intermediate and old age clusters are often found to have extended main sequence turn-offs (eMSTOs) which are difficult to explain with single age isochrones, an effect attributed to rotation. In this paper, we provide the first characterisation of this effect in young (<20Myr) clusters. We determine ages for 4 young massive clusters (2 LMC, 2 Galactic) by three different methods: using the brightest single turn-off (TO) star; using the luminosity function (LF) of the TO; and by using the lowest $L_{\rm bol}$ red supergiant (RSG). The age found using the cluster TO is consistently younger than the age found using the lowest RSG $L_{\rm bol}$. Under the assumption that the lowest luminosity RSG age is the `true' age, we argue that the eMSTOs of these clusters cannot be explained solely by rotation or unresolved binaries. We speculate that the most luminous stars above the TO are massive blue straggler stars formed via binary interaction, either as mass gainers or merger products. Therefore, using the cluster TO method to infer ages and initial masses of post-main sequence stars such as Wolf-Rayet stars, luminous blue variables and RSGs, will result in ages inferred being too young and masses too high. ",Discrepancies in the ages of young star clusters; evidence for mergers?
69,1106016016268705792,913058608883142657,Patrick Vallely üõ∞Ô∏èüååüî≠,"[""AWESOME new paper by @bigskybooms now posted to @arxiv! If you hadn't been convinced already, this should be the last nail in the coffin ruling out the single-degenerate scenario producing a significant fraction of Type Ia supernovae.\n\nFor more: <LINK> <LINK>""]",https://arxiv.org/abs/1903.05115,"We place statistical constraints on Type Ia supernova (SN Ia) progenitors using 227 nebular phase spectra of 111 SNe Ia. We find no evidence of stripped companion emission in any of the nebular phase spectra. Upper limits are placed on the amount of mass that could go undetected in each spectrum using recent hydrodynamic simulations. With these null detections, we place an observational $3\sigma$ upper limit on the fraction of SNe Ia that are produced through the classical H-rich non-degenerate companion scenario of < 5.5%. Additionally, we set a tentative $3\sigma$ upper limit on He star progenitor scenarios of < 6.4%, although further theoretical modelling is required. These limits refer to our most representative sample including normal, 91bg-like, 91T-like, and ""Super Chandrasekhar"" \sne but excluding SNe Iax and SNe Ia-CSM. As part of our analysis, we also derive a Nebular Phase Phillips Relation, which approximates the brightness of a SN Ia from $150-500$~days after maximum using the peak magnitude and decline rate parameter $\Delta m_{15} (B)$. ","Nebular Spectra of 111 Type Ia Supernovae Disfavor Single Degenerate
  Progenitors"
70,1105996526986252288,148996025,Ye Quanzhi (Âè∂Ê≥âÂøó),['Our @ztfsurvey paper on the new active asteroid (6478) Gault is out! <LINK> we also made a time lapse movie using all the ZTF images -- fun to watch how those two tails sprout out. <LINK>'],https://arxiv.org/abs/1903.05320,"Main-belt asteroid (6478) Gault unexpectedly sprouted two tails in late 2018 and early 2019, identifying it as a new active asteroid. Here we present observations obtained by the 1.2-m Zwicky Transient Facility survey telescope that provide detailed time-series coverage of the onset and evolution of Gault's activity. Gault exhibited two brightening events, with the first one starting on 2018 Oct. 18$\pm5$ days and a second one starting on 2018 Dec. 24$\pm1$ days. The amounts of mass released are $2\times10^7$ kg and $1\times10^6$ kg, respectively. Based on photometric measurements, each event persisted for about a month. Gault's color has not changed appreciably over time, with a pre-outburst color of $g_\mathrm{PS1}-r_\mathrm{PS1}=0.50\pm0.04$ and $g_\mathrm{PS1}-r_\mathrm{PS1}=0.46\pm0.04$ during the two outbursts. Simulations of dust dynamics shows that the ejecta consists of dust grains of up to 10 $\mu$m in size that are ejected at low velocities below $1~\mathrm{m~s^{-1}}$ regardless of particle sizes. This is consistent with non-sublimation-driven ejection events. The size distribution of the dust exhibits a broken power-law, with particles at 10--20 $\mu$m following a power-law of $-2.5$ to $-3.0$, while larger particles follow a steeper slope of $-4.0$. The derived properties can be explained by either rotational excitation of the nucleus or a merger of a near-contact binary, with the latter scenario to be statistically more likely. ",Multiple Outbursts of Asteroid (6478) Gault
71,1105943735219572736,2178267337,noriaki_hirose,['We unveiled new navigation method only using an RGB camera. We trained the control policy through Visual MPC. The control policy can realize the navigation in the real environment with obstacle avoidance.\n\nwebsite\n<LINK>\n\narXiv paper\n<LINK> <LINK>'],https://arxiv.org/abs/1903.02749,"Humans can routinely follow a trajectory defined by a list of images/landmarks. However, traditional robot navigation methods require accurate mapping of the environment, localization, and planning. Moreover, these methods are sensitive to subtle changes in the environment. In this paper, we propose a Deep Visual MPC-policy learning method that can perform visual navigation while avoiding collisions with unseen objects on the navigation path. Our model PoliNet takes in as input a visual trajectory and the image of the robot's current view and outputs velocity commands for a planning horizon of $N$ steps that optimally balance between trajectory following and obstacle avoidance. PoliNet is trained using a strong image predictive model and traversability estimation model in a MPC setup, with minimal human supervision. Different from prior work, PoliNet can be applied to new scenes without retraining. We show experimentally that the robot can follow a visual trajectory when varying start position and in the presence of previously unseen obstacles. We validated our algorithm with tests both in a realistic simulation environment and in the real world. We also show that we can generate visual trajectories in simulation and execute the corresponding path in the real environment. Our approach outperforms classical approaches as well as previous learning-based baselines in success rate of goal reaching, sub-goal coverage rate, and computational load. ",Deep Visual MPC-Policy Learning for Navigation
72,1105787306236104706,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"[""new paper <LINK> on arXiv shows that continuous time quantum walks are a pretty 'good' algorithm for solving optimization problems and explains why (no one had ever tried this before, to our knowledge) @jqcDurNew @DurhamQlm""]",https://arxiv.org/abs/1903.05003,"Quantum computation using continuous-time evolution under a natural hardware Hamiltonian is a promising near- and mid-term direction toward powerful quantum computing hardware. We investigate the performance of continuous-time quantum walks as a tool for finding spin glass ground states, a problem that serves as a useful model for realistic optimization problems. By performing detailed numerics, we uncover significant ways in which solving spin glass problems differs from applying quantum walks to the search problem. Importantly, unlike for the search problem, parameters such as the hopping rate of the quantum walk do not need to be set precisely for the spin glass ground state problem. Heuristic values of the hopping rate determined from the energy scales in the problem Hamiltonian are sufficient for obtaining a better than square-root scaling. This makes it practical to use quantum walks for solving such problems, and opens the door for a range of applications on suitable quantum hardware. ",Finding spin-glass ground states using quantum walks
73,1105785748974956545,802858315172737024,Nicholas Chancellor #OneOfUsAllOfUs,"[""new arXiv paper <LINK> shows a new way of encoding integer variables in quantum annealers, in some cases this gives a similar relative advantage to embedding in the @dwavesys Pegasus versus chimera graph (of course advantages will 'stack') @jqcDurNew @DurhamQlm""]",https://arxiv.org/abs/1903.05068,"In this paper I propose a new method of encoding discrete variables into Ising model qubits for quantum optimization. The new method is based on the physics of domain walls in one dimensional Ising spin chains. I find that these encodings and the encoding of arbitrary two variable interactions is possible with only two body Ising terms. Following on from similar results for the `one hot' method of encoding discrete variables [Hadfield et. al. Algorithms 12.2 (2019): 34] I also demonstrate that it is possible to construct two body mixer terms which do not leave the logical subspace, an important consideration for optimising using the quantum alternating operator ansatz (QAOA). I additionally discuss how, since the couplings in the domain wall encoding only need to be ferromagnetic and therefore could in principle be much stronger than anti-ferromagnetic couplers, application specific quantum annealers for discrete problems based on this construction may be beneficial. Finally, I compare embedding for synthetic scheduling and colouring problems with the domain wall and one hot encodings on two graphs which are relevant for quantum annealing, the chimera graph and the Pegasus graph. For every case I examine I find a similar or better performance from the domain wall encoding as compared to one hot, but this advantage is highly dependent on the structure of the problem. For encoding some problems, I find an advantage similar to the one found by embedding in a Pegasus graph compared to embedding in a chimera graph. ","Domain wall encoding of discrete variables for quantum annealing and
  QAOA"
74,1105784884629565440,1978330974,Jacob D Biamonte,"['My new paper appeared on the arXiv today. <LINK> \n\n""Universal Variational Quantum Computation""\n\nCurrent quantum processors, being built by IBM, Google, etc. enable a new model of computation, called variational... <LINK>', '@sycramore The early work on variational quantum computing started with chemistry, see: A. Peruzzo,  @JarrodMcclean, ... @A_Aspuru_Guzik A variational eigenvalue solver on a photonic quantum processor. @NatureComms, 5:4213, 2014. The point was actually to extend outside of chem :)']",https://arxiv.org/abs/1903.04500,"Variational quantum algorithms dominate contemporary gate-based quantum enhanced optimisation, eigenvalue estimation and machine learning. Here we establish the quantum computational universality of variational quantum computation by developing two objective functions which minimise to prepare outputs of arbitrary quantum circuits. The fleeting resource of variational quantum computation is the number of expected values which must be iteratively minimised using classical-to-quantum outer loop optimisation. An efficient solution to this optimisation problem is given by the quantum circuit being simulated itself. The first construction is efficient in the number of expected values for $n$-qubit circuits containing $\mathcal{O}({poly} \ln n)$ non-Clifford gates -- the number of expected values has no dependence on Clifford gates appearing in the simulated circuit. The second approach yields $\mathcal{O}(L^2)$ expected values while introducing not more than $\mathcal{O}(\ln L)$ slack qubits, for a quantum circuit partitioned into $L$ gates. Hence, the utilitarian variational quantum programming procedure -- based on the classical evaluation of objective functions and iterated feedback -- is in principle as powerful as any other model of quantum computation. This result elevates the formal standing of the variational approach while establishing a new universal model of quantum computation. ",Universal Variational Quantum Computation
75,1105713139000139777,465023908,Ben Bartlett,['Check out our new paper on reconfigurable optical activation functions for nanophotonic neural networks! <LINK> #photonics #MachineLearning #NeuralNetworks <LINK>'],https://arxiv.org/abs/1903.04579,"We introduce an electro-optic hardware platform for nonlinear activation functions in optical neural networks. The optical-to-optical nonlinearity operates by converting a small portion of the input optical signal into an analog electric signal, which is used to intensity-modulate the original optical signal with no reduction in processing speed. Our scheme allows for complete nonlinear on-off contrast in transmission at relatively low optical power thresholds and eliminates the requirement of having additional optical sources between each layer of the network. Moreover, the activation function is reconfigurable via electrical bias, allowing it to be programmed or trained to synthesize a variety of nonlinear responses. Using numerical simulations, we demonstrate that this activation function significantly improves the expressiveness of optical neural networks, allowing them to perform well on two benchmark machine learning tasks: learning a multi-input exclusive-OR (XOR) logic function and classification of images of handwritten numbers from the MNIST dataset. The addition of the nonlinear activation function improves test accuracy on the MNIST task from 85% to 94%. ","Reprogrammable Electro-Optic Nonlinear Activation Functions for Optical
  Neural Networks"
76,1105656194557865985,3433220662,Anthony Bonato,"['Latest paper with PhD candidate Erin Meger + post-docs on a new model for social networks. From the abstract: ""We propose the Iterated Local Model (ILM) for social networks synthesizing both transitive and anti-transitive triads over time.""\n\n<LINK>']",https://arxiv.org/abs/1903.04523,"On-line social networks, such as in Facebook and Twitter, are often studied from the perspective of friendship ties between agents in the network. Adversarial ties, however, also play an important role in the structure and function of social networks, but are often hidden. Underlying generative mechanisms of social networks are predicted by structural balance theory, which postulates that triads of agents, prefer to be transitive, where friends of friends are more likely friends, or anti-transitive, where adversaries of adversaries become friends. The previously proposed Iterated Local Transitivity (ILT) and Iterated Local Anti-Transitivity (ILAT) models incorporated transitivity and anti-transitivity, respectively, as evolutionary mechanisms. These models resulted in graphs with many observable properties of social networks, such as low diameter, high clustering, and densification. We propose a new, generative model, referred to as the Iterated Local Model (ILM) for social networks synthesizing both transitive and anti-transitive triads over time. In ILM, we are given a countably infinite binary sequence as input, and that sequence determines whether we apply a transitive or an anti-transitive step. The resulting model exhibits many properties of complex networks observed in the ILT and ILAT models. In particular, for any input binary sequence, we show that asymptotically the model generates finite graphs that densify, have clustering coefficient bounded away from 0, have diameter at most 3, and exhibit bad spectral expansion. We also give a thorough analysis of the chromatic number, domination number, Hamiltonicity, and isomorphism types of induced subgraphs of ILM graphs. ",The Iterated Local Model for Social Networks
77,1105634550766542849,3344155571,Jeffrey De Fauw,"['Hierarchically stacking discrete autoencoders to allow likelihood models to capture long-range structure in images, new paper with @sedielem and Karen. We generate realistic images at 128x128 and 256x256!\n\nPaper:\xa0<LINK>\n\nSamples: <LINK> <LINK>']",https://arxiv.org/abs/1903.04933,"Autoregressive generative models of images tend to be biased towards capturing local structure, and as a result they often produce samples which are lacking in terms of large-scale coherence. To address this, we propose two methods to learn discrete representations of images which abstract away local detail. We show that autoregressive models conditioned on these representations can produce high-fidelity reconstructions of images, and that we can train autoregressive priors on these representations that produce samples with large-scale coherence. We can recursively apply the learning procedure, yielding a hierarchy of progressively more abstract image representations. We train hierarchical class-conditional autoregressive models on the ImageNet dataset and demonstrate that they are able to generate realistic images at resolutions of 128$\times$128 and 256$\times$256 pixels. We also perform a human evaluation study comparing our models with both adversarial and likelihood-based state-of-the-art generative models. ",Hierarchical Autoregressive Image Models with Auxiliary Decoders
78,1105569383085158401,2427184074,Christopher Berry,"['New on the arXiv today: an #Astro2020 white paper on the awesomeness of extreme-mass-ratio inspirals, a unique #GravitationalWave source for @LISACommunity <LINK> <LINK>', 'Extreme-mass-ratio inspirals are when one smaller stellar-mass black hole orbits a supermassive one. The resulting orbits are extremely complicated (as illustrated in this rather lovely figure). It is this structure which encodes lots of information into the #GravitationalWaves https://t.co/T39r9yPVij', ""We're not sure how many inspirals we'll see‚Äîmeasuring the number would teach us lots about galactic cores. We estimated ~1‚Äì2000 per year, so they're a bankable source across a 4 year @LISACommunity mission! I describe more in my blog https://t.co/N0XOHYqXId #Astro2020"", ""Since the orbits are so complicated, we'll be able to measure the properties of the source *really* well. Black hole spins are hard to measure with @LIGO, but here we'll get them to 1 part in 10,000‚Äì1,000,000! The spins encode lots of information about how black holes grow"", ""Because we'll get such ridiculously detailed measurements of the properties, we can really test the structure of the massive black hole. If there's a missing piece to @AlbertEinstein's theory of general relativity, here is an excellent place to look! #Astro2020"", ""Putting together a few extreme-mass-ratio inspirals, we can start to reconstruct the mass distribution of massive black holes. @LISACommunity is sensitive to black holes with masses 10^4‚Äì10^7 times the mass of our Sun. We don't know what the distribution is like at the lower end!"", 'If we can cross-correlate the location of the source we work out from the #GravitaitonalWave signal with galaxy catalogues, we can also measure the Hubble constant, perhaps to 1% after 20 detections! This is a great check of other methods #Astro2020', 'Extreme-mass-ratio inspirals are a wonder opportunity for #GravitationalWave astronomy. There is only one currently scheduled mission that can detect them: @LISACommunity https://t.co/wJEjE7ORi6  üíûüíì‚ù§Ô∏è #Astro2020']",https://arxiv.org/abs/1903.03686,"The inspiral of a stellar-mass compact object into a massive ($\sim 10^{4}$-$10^{7} M_{\odot}$) black hole produces an intricate gravitational-wave signal. Due to the extreme-mass ratios involved, these systems complete $\sim 10^{4}$-$10^{5}$ orbits, most of them in the strong-field region of the massive black hole, emitting in the frequency range $\sim10^{-4}-1~$Hz. This makes them prime sources for the space-based observatory LISA (Laser Interferometer Space Antenna). LISA observations will enable high-precision measurements of the physical characteristics of these extreme-mass-ratio inspirals (EMRIs): redshifted masses, massive black hole spin and orbital eccentricity can be determined with fractional errors $\sim 10^{-4}$-$10^{-6}$, the luminosity distance with better than $\sim 10\%$ precision, and the sky localization to within a few square degrees. EMRIs will provide valuable information about stellar dynamics in galactic nuclei, as well as precise data about massive black hole populations, including the distribution of masses and spins. They will enable percent-level measurements of the multipolar structure of massive black holes, precisely testing the strong-gravity properties of their spacetimes. EMRIs may also provide cosmographical data regarding the expansion of the Universe if inferred source locations can be correlated with galaxy catalogs. ","The unique potential of extreme mass-ratio inspirals for
  gravitational-wave astronomy"
79,1105511647148474369,99679924,Ye Wang,['The new paper by Wager and Xu is a really impressive attempt to combine experiments with market design. They show that local randomization allows us to block interference and infer the global optimum at the same time. <LINK>'],https://arxiv.org/abs/1903.02124,"Classical approaches to experimental design assume that intervening on one unit does not affect other units. There are many important settings, however, where this non-interference assumption does not hold, as when running experiments on supply-side incentives on a ride-sharing platform or subsidies in an energy marketplace. In this paper, we introduce a new approach to experimental design in large-scale stochastic systems with considerable cross-unit interference, under an assumption that the interference is structured enough that it can be captured via mean-field modeling. Our approach enables us to accurately estimate the effect of small changes to system parameters by combining unobstrusive randomization with lightweight modeling, all while remaining in equilibrium. We can then use these estimates to optimize the system by gradient descent. Concretely, we focus on the problem of a platform that seeks to optimize supply-side payments p in a centralized marketplace where different suppliers interact via their effects on the overall supply-demand equilibrium, and show that our approach enables the platform to optimize p in large systems using vanishingly small perturbations. ",Experimenting in Equilibrium
80,1105500829254070278,2427184074,Christopher Berry,"['New @GravitySpyZoo paper led by @cardiffPHYSX PhD student Scotty Coughlin on how we can enable our #CitizenScience volunteers identify new glitch classes <LINK> #ML <LINK>', '#GravitySpy asks citizen scientists to classify gravitational-wave detector glitches. These results are used to train a machine learning algorithm. But how do we add new, previously identified glitches? These can be rare, so finding a training set can be hard', ""We've implemented #GravitySpy tools which enables citizen scientists to search for similar looking glitches from @the_zooniverse https://t.co/rVkfLDMq9v Using these they can create a collection of a new glitch, and retrain the neural network to include them"", 'We hope that the new #GravitySpy tools will both hope us identify new glitches (especially important after detector upgrades, when things might have changed), and also enable citizen scientists to take ownership of their own investigations', ""@GWastrology @GravitySpyZoo @cardiffPHYSX @LIGOLA @LIGOWA @ego_virgo Paired dove = Gemini? There are currently more glitch types than zodiac signs though. Perhaps it's better to associate people with glitches than constellations?""]",https://arxiv.org/abs/1903.04058,"The observation of gravitational waves from compact binary coalescences by LIGO and Virgo has begun a new era in astronomy. A critical challenge in making detections is determining whether loud transient features in the data are caused by gravitational waves or by instrumental or environmental sources. The citizen-science project \emph{Gravity Spy} has been demonstrated as an efficient infrastructure for classifying known types of noise transients (glitches) through a combination of data analysis performed by both citizen volunteers and machine learning. We present the next iteration of this project, using similarity indices to empower citizen scientists to create large data sets of unknown transients, which can then be used to facilitate supervised machine-learning characterization. This new evolution aims to alleviate a persistent challenge that plagues both citizen-science and instrumental detector work: the ability to build large samples of relatively rare events. Using two families of transient noise that appeared unexpectedly during LIGO's second observing run (O2), we demonstrate the impact that the similarity indices could have had on finding these new glitch types in the Gravity Spy program. ","Classifying the unknown: discovering novel gravitational-wave detector
  glitches using similarity learning"
81,1105447608384917504,1068545181576773632,Kenneth Brown,['Leaked qubits can cause a lot of damage in quantum error correction. Subsystem codes can contain the damage.  Read more in our new paper:\nHandling Leakage with Subsystem Codes <LINK>'],https://arxiv.org/abs/1903.03937,"Leakage is a particularly damaging error that occurs when a qubit state falls out of its two-level computational subspace. Compared to independent depolarizing noise, leaked qubits may produce many more configurations of harmful correlated errors during error-correction. In this work, we investigate different local codes in the low-error regime of a leakage gate error model. When restricting to bare-ancilla extraction, we observe that subsystem codes are good candidates for handling leakage, as their locality can limit damaging correlated errors. As a case study, we compare subspace surface codes to the subsystem surface codes introduced by Bravyi et al. In contrast to depolarizing noise, subsystem surface codes outperform same-distance subspace surface codes below error rates as high as $\lessapprox 7.5 \times 10^{-4}$ while offering better per-qubit distance protection. Furthermore, we show that at low to intermediate distances, Bacon-Shor codes offer better per-qubit error protection against leakage in an ion-trap motivated error model below error rates as high as $\lessapprox 1.2 \times 10^{-3}$. For restricted leakage models, this advantage can be extended to higher distances by relaxing to unverified two-qubit cat state extraction in the surface code. These results highlight an intrinsic benefit of subsystem code locality to error-corrective performance. ",Handling Leakage with Subsystem Codes
82,1105110892600012801,1048984881131401217,Cora Dvorkin,"['Our new paper with my fantastic graduate student Ana Diaz Rivero and my collaborator Vivian Miranda is finally out:\n""Observable Predictions for Massive-Neutrino Cosmologies with Model-Independent Dark Energy""\n<LINK>\n@harvardphysics @Planck @theDESurvey']",https://arxiv.org/abs/1903.03125,"We investigate the bounds on the sum of neutrino masses in a cosmic-acceleration scenario where the equation of state $w(z)$ of dark energy (DE) is constructed in a model-independent way, using a basis of principal components (PCs) that are allowed to cross the phantom barrier $w(z)=-1$. We find that the additional freedom provided to $w(z)$ means the DE can undo changes in the background expansion induced by massive neutrinos at low redshifts. This has two significant consequences: (1) it leads to a substantial increase in the upper bound for the sum of the neutrino masses ($M_{\nu} < 0.33 - 0.55$ eV at the 95\% C.L. depending on the data sets and number of PCs included) compared to studies that choose a specific parametrization for $w(z)$; and (2) it causes $\sim1\sigma$ deviations from $\Lambda$CDM in the luminosity distance and the Hubble expansion rate at higher redshifts ($z \gtrsim 2$), where the contribution of DE is subdominant and there is little constraining data. The second point consequently means that there are also observable deviations in the shear power spectrum and in the matter power spectrum at low redshift, since the clustering of matter throughout cosmic time depends on the expansion rate. This provides a compelling case to pursue high-$z$ BAO and SN measurements as a way of disentangling the effects of neutrinos and dark energy. Finally, we find that the additional freedom given to the dark energy component has the effect of lowering $S_8$ with respect to $\Lambda$CDM. ","Observable Predictions for Massive-Neutrino Cosmologies with
  Model-Independent Dark Energy"
83,1105083532387540992,61434104,Scott H. Hawley,"['My \'new\' paper is now on arXiv: ""Challenges for an Ontology of Artificial Intelligence"" <LINK>. Passed peer-rev last fall but didn\'t submit to arXiv til recently. Thanks to great RA @TommyKess,  and @UniofBath/@ARTAIBath group for chance to speak &amp; get feedback.']",http://arxiv.org/abs/1903.03171,"Of primary importance in formulating a response to the increasing prevalence and power of artificial intelligence (AI) applications in society are questions of ontology. Questions such as: What ""are"" these systems? How are they to be regarded? How does an algorithm come to be regarded as an agent? We discuss three factors which hinder discussion and obscure attempts to form a clear ontology of AI: (1) the various and evolving definitions of AI, (2) the tendency for pre-existing technologies to be assimilated and regarded as ""normal,"" and (3) the tendency of human beings to anthropomorphize. This list is not intended as exhaustive, nor is it seen to preclude entirely a clear ontology, however, these challenges are a necessary set of topics for consideration. Each of these factors is seen to present a 'moving target' for discussion, which poses a challenge for both technical specialists and non-practitioners of AI systems development (e.g., philosophers and theologians) to speak meaningfully given that the corpus of AI structures and capabilities evolves at a rapid pace. Finally, we present avenues for moving forward, including opportunities for collaborative synthesis for scholars in philosophy and science. ",Challenges for an Ontology of Artificial Intelligence
84,1105077732118065153,175921010,Earl Patrick Bellinger,"['New paper out! In this paper I present new scaling relations for accurately &amp; precisely estimating stellar masses and radii, as well as a scaling relation for stellar age. \n\n<LINK> <LINK>', 'Here is an example to show how well it works. On the left is the classical seismic scaling relation for estimating stellar masses. On the right is the new relation developed in the paper. The scatter is greatly reduced, and the bias is eliminated. https://t.co/Jr1NhzLXa9', 'This figure shows same for age. How does it work? The idea is simple. I took a ""gold sample"" of the 80 best stars observed by Kepler (stars for which we have the best age, mass, and radius estimates) and simply used them to (re-)calibrate the exponents of the scaling relations. https://t.co/NDk8SOeYsN', ""Here's another cool thing: a radius scaling relation that does not need any spectroscopic data (no effective temperature) and actually works better than the classical scaling relation. So from TESS data alone you can find 2.5% determinations of stellar radii, without follow-up. https://t.co/XtVzPAKFhc"", 'The relations are so straightforward that you can now accurately calculate the age of a star using a handheld calculator---a task that would usually require access to a supercomputer. Nevertheless, even though the formulas are simple, I provide some source code at the end :-) https://t.co/lkuIVJzUuH']",https://arxiv.org/abs/1903.03110,"A simple solar scaling relation for estimating the ages of main-sequence stars from asteroseismic and spectroscopic data is developed. New seismic scaling relations for estimating mass and radius are presented as well, including a purely seismic radius scaling relation (i.e., no dependence on temperature). The relations show substantial improvement over the classical scaling relations and perform similarly well to grid-based modeling. ",A seismic scaling relation for stellar age
85,1105017934840250368,129802464,Niall Deacon,"['New paper. We‚Äôve used Skymapper, 2MASS, WISE &amp; Gaia to estimate stellar parameters for almost a million southern TESS FGK targets.\n\n<LINK>', 'Our method generates initial stellar parameters from a grid of evolutionary models but then generates colours using a data-driven model trained on stars in a spectroscopic survey.', 'Anyway, given the human brain is wired to spot patterns in things here‚Äôs the diving hawk of stellar radius differences https://t.co/LdTv0qQSmT']",https://arxiv.org/abs/1903.03115,"We present stellar parameter estimates for 939,457 southern FGK stars that are candidate targets for the TESS mission. Using a data-driven method similar to the CANNON, we build a model of stellar colours as a function of stellar parameters. We then use these in combination with stellar evolution models to estimate the effective temperature, gravity, metallicity, mass, radius and extinction for our selected targets. Our effective temperature estimates compare well with those from spectroscopic surveys and the addition of Gaia DR2 parallaxes allows us to identify subgiant interlopers into the TESS sample. We are able to estimate the radii of TESS targets with a typical uncertainty of 9.3\%. This catalogue can be used to screen exoplanet candidates from TESS and provides a homogeneous set of stellar parameters for statistical studies. ",Data-driven stellar parameters for southern TESS FGK targets
86,1104907139745607681,2337598033,Geraint F. Lewis,['Brand new paper on the arXiv with PhD student Mathew Varidel\n\n<LINK> <LINK>'],https://arxiv.org/abs/1903.03121,"We present a novel Bayesian method, referred to as Blobby3D, to infer gas kinematics that mitigates the effects of beam smearing for observations using Integral Field Spectroscopy (IFS). The method is robust for regularly rotating galaxies despite substructure in the gas distribution. Modelling the gas substructure within the disk is achieved by using a hierarchical Gaussian mixture model. To account for beam smearing effects, we construct a modelled cube that is then convolved per wavelength slice by the seeing, before calculating the likelihood function. We show that our method can model complex gas substructure including clumps and spiral arms. We also show that kinematic asymmetries can be observed after beam smearing for regularly rotating galaxies with asymmetries only introduced in the spatial distribution of the gas. We present findings for our method applied to a sample of 20 star-forming galaxies from the SAMI Galaxy Survey. We estimate the global H$\alpha$ gas velocity dispersion for our sample to be in the range $\bar{\sigma}_v \sim $[7, 30] km s$^{-1}$. The relative difference between our approach and estimates using the single Gaussian component fits per spaxel is $\Delta \bar{\sigma}_v / \bar{\sigma}_v = - 0.29 \pm 0.18$ for the H$\alpha$ flux-weighted mean velocity dispersion. ","The SAMI Galaxy Survey: Bayesian Inference for Gas Disk Kinematics using
  a Hierarchical Gaussian Mixture Model"
87,1103972921792442370,113588704,Antonio J. Nebro,['A preprint of the paper introducing our new framework for multi-objective optimization (JMetalPy) is available at: <LINK>'],https://arxiv.org/abs/1903.02915v1,"This paper describes jMetalPy, an object-oriented Python-based framework for multi-objective optimization with metaheuristic techniques. Building upon our experiences with the well-known jMetal framework, we have developed a new multi-objective optimization software platform aiming not only at replicating the former one in a different programming language, but also at taking advantage of the full feature set of Python, including its facilities for fast prototyping and the large amount of available libraries for data processing, data analysis, data visualization, and high-performance computing. As a result, jMetalPy provides an environment for solving multi-objective optimization problems focused not only on traditional metaheuristics, but also on techniques supporting preference articulation and dynamic problems, along with a rich set of features related to the automatic generation of statistical data from the results generated, as well as the real-time and interactive visualization of the Pareto front approximations produced by the algorithms. jMetalPy offers additionally support for parallel computing in multicore and cluster systems. We include some use cases to explore the main features of jMetalPy and to illustrate how to work with it. ","] jMetalPy: a Python Framework for Multi-Objective Optimization with
  Metaheuristics"
88,1103935193876901889,777776718928941056,Daniel Muthukrishna,"['New paper with @drdrparky and @btucker22 today on our new spectral classification tool, DASH. It uses deep convolutional neural networks built with @TensorFlow to classify supernovae :) <LINK>', 'Feel free to try it out! https://t.co/cqoaSHgkBK']",https://arxiv.org/abs/1903.02557,"We present DASH (Deep Automated Supernova and Host classifier), a novel software package that automates the classification of the type, age, redshift, and host galaxy of supernova spectra. DASH makes use of a new approach that does not rely on iterative template matching techniques like all previous software, but instead classifies based on the learned features of each supernova's type and age. It has achieved this by employing a deep convolutional neural network to train a matching algorithm. This approach has enabled DASH to be orders of magnitude faster than previous tools, being able to accurately classify hundreds or thousands of objects within seconds. We have tested its performance on four years of data from the Australian Dark Energy Survey (OzDES). The deep learning models were developed using TensorFlow, and were trained using over 4000 supernova spectra taken from the CfA Supernova Program and the Berkeley SN Ia Program as used in SNID (Supernova Identification software, Blondin & Tonry 2007). Unlike template matching methods, the trained models are independent of the number of spectra in the training data, which allows for DASH's unprecedented speed. We have developed both a graphical interface for easy visual classification and analysis of supernovae, and a Python library for the autonomous and quick classification of several supernova spectra. The speed, accuracy, user-friendliness, and versatility of DASH presents an advancement to existing spectral classification tools. We have made the code publicly available on GitHub and PyPI (pip install astrodash) to allow for further contributions and development. The package documentation is available at this https URL ","DASH: Deep Learning for the Automated Spectral Classification of
  Supernovae and their Hosts"
89,1103878909236436992,3302448566,Sai Krishna G.V.,"['New paper out: Deep Active Localization <LINK>   Thanks to all my coauthors Keehong, @dhaivat1729, Vincent, @krrish94 , @duckietown_coo. @MILAMontreal @UMontrealDIRO']",https://arxiv.org/abs/1903.01669,"Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator. ",Deep Active Localization
90,1103858909041868800,18850305,Zachary Lipton,"['New work by my student Yifan Wu identifies problems with traditional *deep domain adaptation* objectives &amp; holes in the theory supporting it. Our paper offers new analysis, a new algorithm that escapes one identified failure mode, &amp; experimental validation <LINK>', 'Looking forward to discussing this work on Monday at Berkeley\'s ""Trustworthy Deep Learning"" seminar https://t.co/0il5wVTHef', '@WilliamWangNLP Our key idea here is that enforcing strict alignment can often be a bad thing. For example, what if source and target distributions have different label distributions? Then alignment is actually lower-bounding your target error. Instead we optimize a relaxed objective.']",https://arxiv.org/abs/1903.01689,"Domain adaptation addresses the common problem when the target distribution generating our test data drifts from the source (training) distribution. While absent assumptions, domain adaptation is impossible, strict conditions, e.g. covariate or label shift, enable principled algorithms. Recently-proposed domain-adversarial approaches consist of aligning source and target encodings, often motivating this approach as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, e.g. they can break down under shifting label distributions. We propose asymmetrically-relaxed distribution alignment, a new approach that overcomes some limitations of standard domain-adversarial algorithms. Moreover, we characterize precise assumptions under which our algorithm is theoretically principled and demonstrate empirical benefits on both synthetic and real datasets. ",Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment
91,1103853176619499521,1019760963569049601,Almog Yalinewich,"['Our new paper is on the arxiv: the unbound debris can drive the radio emission in tidal disruption events, and we demonstrate it for @SuperASASSN 14li <LINK>', '@SuperASASSN Will do in the next revision']",https://arxiv.org/abs/1903.02575,"When a star gets too close to a supermassive black hole, it is torn apart by the tidal forces. Roughly half of the stellar mass becomes unbound and flies away at tremendous velocities - around $10^4$ km/s. In this work we explore the idea that the shock produced by the interaction of the unbound debris with the ambient medium gives rise to the synchrotron radio emission observed in several TDEs. We use a moving mesh numerical simulation to study the evolution of the unbound debris and the bow shock around it. We find that as the periapse distance of the star decreases, the outflow becomes faster and wider. A tidal disruption event whose periapse distance is a factor of 7 smaller than the tidal radius can account for the radio emission observed in ASASSN-14li. This model also allows us to obtain a more accurate estimate for the gas density around the centre of the host galaxy of ASASSN-14li. ",Radio Emission from the unbound Debris of Tidal Disruption Events
92,1103779809430249475,866221331184050176,Maximilian N. G√ºnther,"[""New paper on @arxiv today! Led by @ZhuchangZ (MIT), we found the weirdest and fastest rotating M-dwarfs ever seen. Thanks to @NASA_TESS / @TESSatMIT! The paper is in the stellar (not planetary) arxiv, so don't miss it: <LINK> :)"", 'I mean, just look at these weird phase-folded lightcurves... https://t.co/HkFEAqFrm1', '...and those crazy flares that completely change the pattern for a few rotations... https://t.co/fbo5e26mav', '...and this is what we think might cause these profiles: spots and gas/dust rings (follow-up paper in prep.) https://t.co/yuKJlkP56d']",https://arxiv.org/abs/1903.02061,"We have searched for short periodicities in the light curves of stars with $T_{\rm eff}$ cooler than 4000 K made from 2-minute cadence data obtained in TESS sectors 1 and 2. Herein we report the discovery of 10 rapidly rotating M-dwarfs with highly structured rotational modulation patterns among 10 M dwarfs found to have rotation periods less than 1 day. Star-spot models cannot explain the highly structured periodic variations which typically exhibit between 10 and 40 Fourier harmonics. A similar set of objects was previously reported following K2 observations of the Upper Scorpius association (Stauffer et al. 2017). We examine the possibility that the unusual structured light-curves could stem from absorption by charged dust particles that are trapped in or near the stellar magnetosphere. We also briefly explore the possibilities that the sharp structured features in the lightcurves are produced by extinction by coronal gas, by beaming of the radiation emitted from the stellar surface, or by occultations of spots by a dusty ring that surrounds the star. The latter is perhaps the most promising of these scenarios. Most of the structured rotators display flaring activity, and we investigate changes in the modulation pattern following the largest flares. As part of this study, we also report the discovery of 371 rapidly rotating M-dwarfs with rotational periods below 4 hr, of which the shortest period is 1.63 hr. ","Complex Rotational Modulation of Rapidly Rotating M-Stars Observed with
  TESS"
93,1103700514921832450,32230551,Adrian Roitberg,"['New paper in arxiv by the amazing Xiang Gao @gaoxiang_ai , in collab with Emilio Gallicchio @emiliaogallo \n""The Boltzmann Distribution is the Only Distribution That Gibbs-Shannon Entropy is the Thermodynamic Entropy""\n\nComments and very welcome!\n\n<LINK>']",https://arxiv.org/abs/1903.02121,"We show that the generalized Boltzmann distribution is the only distribution for which the Gibbs-Shannon entropy equals the thermodynamic entropy. This result means that the thermodynamic entropy and the Gibbs-Shannon entropy are not generally equal, but rather than the equality holds only in the special case where a system is in equilibrium with a reservoir. ","The Generalized Boltzmann Distribution is the Only Distribution in Which
  the Gibbs-Shannon Entropy Equals the Thermodynamic Entropy"
94,1103679531473342467,993479870784196608,Bradley Gram-Hansen üåå,"['Our new AISTATS paper. LF-PPL: A Low-Level First-Order Probabilistic Programming Language for Non-Differentiable Models. Yuan Zhou*, Me*, Tobias Kohn, @tom_rainforth , @hyang144, and @frankdonaldwood is now available here <LINK> with code <LINK>', 'And walkthroughs of how to use LF-PPL as a compilation target for Probabilistic Programming Systems. You make use of Pytorch, pure Python and Clojure (if you crave Lambda Calculus). See here https://t.co/BCBmWi9AQO and here https://t.co/jv6j5SzIEA']",http://arxiv.org/abs/1903.02482,"We develop a new Low-level, First-order Probabilistic Programming Language (LF-PPL) suited for models containing a mix of continuous, discrete, and/or piecewise-continuous variables. The key success of this language and its compilation scheme is in its ability to automatically distinguish parameters the density function is discontinuous with respect to, while further providing runtime checks for boundary crossings. This enables the introduction of new inference engines that are able to exploit gradient information, while remaining efficient for models which are not everywhere differentiable. We demonstrate this ability by incorporating a discontinuous Hamiltonian Monte Carlo (DHMC) inference engine that is able to deliver automated and efficient inference for non-differentiable models. Our system is backed up by a mathematical formalism that ensures that any model expressed in this language has a density with measure zero discontinuities to maintain the validity of the inference engine. ","LF-PPL: A Low-Level First Order Probabilistic Programming Language for
  Non-Differentiable Models"
95,1103592738765893632,797888987675365377,Tom Rainforth,"['Excited to announce our new AISTATS paper. LF-PPL: A Low-Level First Order Probabilistic Programming Language for Non-Differentiable Models. Yuan Zhou*, @BayesianBrad*, Tobias Kohn, myself, @hyang144, and @frankdonaldwood. Preprint available here <LINK>']",http://arxiv.org/abs/1903.02482,"We develop a new Low-level, First-order Probabilistic Programming Language (LF-PPL) suited for models containing a mix of continuous, discrete, and/or piecewise-continuous variables. The key success of this language and its compilation scheme is in its ability to automatically distinguish parameters the density function is discontinuous with respect to, while further providing runtime checks for boundary crossings. This enables the introduction of new inference engines that are able to exploit gradient information, while remaining efficient for models which are not everywhere differentiable. We demonstrate this ability by incorporating a discontinuous Hamiltonian Monte Carlo (DHMC) inference engine that is able to deliver automated and efficient inference for non-differentiable models. Our system is backed up by a mathematical formalism that ensures that any model expressed in this language has a density with measure zero discontinuities to maintain the validity of the inference engine. ","LF-PPL: A Low-Level First Order Probabilistic Programming Language for
  Non-Differentiable Models"
96,1103545069859008513,1576235694,Michael Brown,"['.@vparkash2014 has a new paper on @arxiv today, finding that gas rich galaxies with low star formation rates often have extended LINER emission. In other words, an abundance of LIERs! \n\n<LINK> <LINK>']",https://arxiv.org/abs/1903.02024,"We present a sample of 91 HI galaxies with little or no star formation and discuss the analysis of the integral field unit (IFU) spectra of 28 of these galaxies. We identified HI galaxies from the HI Parkes All-Sky Survey Catalog (HICAT) with Wide-field Infrared Survey Explorer (WISE) colours consistent with low specific star formation (< 10$^{-10.4}$ yr$^{-1}$), and obtained optical IFU spectra with the Wide-Field Spectrograph (WiFeS). Visual inspection of the PanSTARRS, Dark Energy Survey, and Carnegie-Irvine imaging of 62 galaxies reveals that at least 32 galaxies in the sample have low levels of star formation, primarily in arms/rings. New IFU spectra of 28 of these galaxies reveal 3 galaxies with central star formation, 1 galaxy with low-ionisation nuclear emission-line regions (LINERs), 20 with extended low-ionisation emission-line regions (LIERs) and 4 with high excitation Seyfert (Sy) emission. From the spectroscopic analysis of HI-selected galaxies with little star formation, we conclude that 75% of this population are LINERs/LIERs. ",HI galaxies with little star formation: an abundance of LIERs
97,1103473416152084480,393372877,Riccardo Di Sipio üá®üá¶üáÆüáπüá™üá∫,"['You have seen GANs being used to create fake pictures of Hollywood stars. Today, you can generate 1M high-energy physics collisions in one minute, including fast detector simulation and pileup. Check out our new paper <LINK> #DeepLearning #LHC #HEP #QCD <LINK>']",https://arxiv.org/abs/1903.02433,"A Generative-Adversarial Network (GAN) based on convolutional neural networks is used to simulate the production of pairs of jets at the LHC. The GAN is trained on events generated using MadGraph5 + Pythia8, and Delphes3 fast detector simulation. We demonstrate that a number of kinematic distributions both at Monte Carlo truth level and after the detector simulation can be reproduced by the generator network with a very good level of agreement. The code can be checked out or forked from the publicly accessible online repository this https URL . ","DijetGAN: A Generative-Adversarial Network Approach for the Simulation
  of QCD Dijet Events at the LHC"
98,1103350340261306370,60893773,James Bullock,"['New paper using cosmological zoom simulations with Victor Robles, Tyler Kelley, Manoj Kaplinghat\n\n- Predictions for the Milky Way in Self-Interacting Dark Matter change a lot once you account for the presence of the galaxy \n\n<LINK> <LINK>', 'Without a central galaxy, SIDM predicts a huge ~7kpc core in the Milky Way.  Once the galaxy is included, the SIDM prediction is *denser* than CDM. https://t.co/GpKbCyoyOJ', 'Substructure counts/radial distributions also become nearly identical https://t.co/Dr1H66BO1Z', 'Only difference: massive subhalos in SIDM are less dense than in CDM\n- Intriguing mismatch with data: both CDM and SIDM simulations predict that subhalos with smallest percenters should be less dense.\n- Percenters from @ESAGaia suggest this is not the case in the Milky Way https://t.co/iv1eOmTI9B']",https://arxiv.org/abs/1903.01469,"We perform high-resolution simulations of a MW-like galaxy in a self-interacting cold dark matter model with elastic cross section over mass of $1~\rm cm^2/g$ (SIDM) and compare to a model without self-interactions (CDM). We run our simulations with and without a time-dependent embedded potential to capture effects of the baryonic disk and bulge contributions. The CDM and SIDM simulations with the embedded baryonic potential exhibit remarkably similar host halo profiles, subhalo abundances and radial distributions within the virial radius. The SIDM host halo is denser in the center than the CDM host and has no discernible core, in sharp contrast to the case without the baryonic potential (core size $\sim 7 \, \rm kpc$). The most massive subhalos (with $V_{\mathrm{peak}}> 20 \, \rm km/s$) in our SIDM simulations, expected to host the classical satellite galaxies, have density profiles that are less dense than their CDM analogs at radii less than 500 pc but the deviation diminishes for less massive subhalos. With the baryonic potential included in the CDM and SIDM simulations, the most massive subhalos do not display the too-big-to-fail problem. However, the least dense among the massive subhalos in both these simulations tend to have the smallest pericenter values, a trend that is not apparent among the bright MW satellite galaxies. ",The Milky Way's Halo and Subhalos in Self-Interacting Dark Matter
99,1103293737248190470,2239670346,Jonathan Frankle,"['Just released our new paper about ""The Lottery Ticket Hypothesis at Scale,"" (with Gintare Karolina Dziugaite, @roydanroy, and @mcarbin) extending our prior work to find small trainable subnetworks within deeper, state-of-the-art neural networks. <LINK> <LINK>', 'We had already shown that, at initialization, small neural networks contain subnetworks ~10x smaller that learn just as effectively. This paper extends these results to deeper networks and a state-of-the-art network for ImageNet (Resnet-50).', 'The trick is that the subnetworks don\'t always emerge at initialization. Instead, we found that training these subnetworks from an iteration slightly after initialization (between a few iterations and a few epochs) often works much better. We term this technique ""late resetting.""', ""Here's an example on Resnet-50 for ImageNet. Orange = no late resetting. Blue = late resetting to epoch 6 (dashed = randomly reinitialized). With late resetting, we find a sparse trainable subnetwork with 90% of parameters removed that is within 1% of the original accuracy. https://t.co/MA2IlbaNFu"", 'But why does late resetting work? Stability to pruning: when the subnetwork follows (approximately) the same trajectory whether trained alone or as part of the full network. When late resetting begins to work, stability improves (blue). Random subnetworks do not improve (orange). https://t.co/zeo7ZM6oti', 'Check out the full paper at https://t.co/31fxatjJS6!']",https://arxiv.org/abs/1903.01611,"Pruning is a well-established technique for removing unnecessary structure from neural networks after training to improve the performance of inference. Several recent results have explored the possibility of pruning at initialization time to provide similar benefits during training. In particular, the ""lottery ticket hypothesis"" conjectures that typical neural networks contain small subnetworks that can train to similar accuracy in a commensurate number of steps. The evidence for this claim is that a procedure based on iterative magnitude pruning (IMP) reliably finds such subnetworks retroactively on small vision tasks. However, IMP fails on deeper networks, and proposed methods to prune before training or train pruned networks encounter similar scaling limitations. In this paper, we argue that these efforts have struggled on deeper networks because they have focused on pruning precisely at initialization. We modify IMP to search for subnetworks that could have been obtained by pruning early in training (0.1% to 7% through) rather than at iteration 0. With this change, it finds small subnetworks of deeper networks (e.g., 80% sparsity on Resnet-50) that can complete the training process to match the accuracy of the original network on more challenging tasks (e.g., ImageNet). In situations where IMP fails at iteration 0, the accuracy benefits of delaying pruning accrue rapidly over the earliest iterations of training. To explain these behaviors, we study subnetwork ""stability,"" finding that - as accuracy improves in this fashion - IMP subnetworks train to parameters closer to those of the full network and do so with improved consistency in the face of gradient noise. These results offer new insights into the opportunity to prune large-scale networks early in training and the behaviors underlying the lottery ticket hypothesis ",Stabilizing the Lottery Ticket Hypothesis
100,1103250653021904897,805477439648440321,Rob Kavanagh,"['New paper on arXiv today! We model the stellar wind of the hot Jupiter host HD189733, and calculate radio emission from the stellar wind and the planet. Our results show that propagation of the planetary radio emission may be inhibited by the stellar wind: <LINK> <LINK>']",https://arxiv.org/abs/1903.01809,"We present stellar wind modelling of the hot Jupiter host HD189733, and predict radio emission from the stellar wind and the planet, the latter arising from the interaction of the stellar wind with the planetary magnetosphere. Our stellar wind models incorporate surface stellar magnetic field maps at the epochs Jun/Jul 2013, Sep 2014, and Jul 2015 as boundary conditions. We find that the mass-loss rate, angular momentum-loss rate, and open magnetic flux of HD189733 vary by 9%, 40%, and 19% over these three epochs. Solving the equations of radiative transfer, we find that from 10 MHz-100 GHz the stellar wind emits fluxes in the range of $10^{-3}$-$5$ $\mu$Jy, and becomes optically thin above 10 GHz. Our planetary radio emission model uses the radiometric Bode's law, and neglects the presence of a planetary atmosphere. For assumed planetary magnetic fields of 1-10 G, we estimate that the planet emits at frequencies of 2-25 MHz, with peak flux densities of $\sim10^2$ mJy. We find that the planet orbits through regions of the stellar wind that are optically thick to the emitted frequency from the planet. As a result, unattenuated planetary radio emission can only propagate out of the system and reach the observer for 67% of the orbit for a 10 G planetary field, corresponding to when the planet is approaching and leaving primary transit. We also find that the plasma frequency of the stellar wind is too high to allow propagation of the planetary radio emission below 21 MHz. This means a planetary field of at least 8 G is required to produce detectable radio emission. ",MOVES II. Tuning in to the radio environment of HD189733b
101,1103215883558760448,2999702157,Anton Ilderton,"['My new paper with Tim Adamo from @ImperialPhysics is out! #QCD physics in a strong, coherent gluonic background. Studied using methods developed for #QED in intense #laser fields. @plym_math @PlymUni @PlymUniNews @SciEngPlymUni <LINK> <LINK>']",https://arxiv.org/abs/1903.01491,"We compute the leading probability for a gluon to flip helicity state upon traversing a background plane wave gauge field in pure Yang-Mills theory and QCD, with an arbitrary number of colours and flavours. This is a one-loop calculation in perturbative gauge theory around the gluonic plane wave background, which is treated without approximation (i.e., to all orders in the coupling). We introduce a background-dressed version of the spinor helicity formalism and use it to obtain simple formulae for the flip amplitude with pure external gluon polarizations. We also give in-depth examples for gauge group SU(2), and evaluate both the high- and low-energy limits. Throughout, we compare and contrast with the calculation of photon helicity flip in strong-field QED. ",Gluon helicity flip in a plane wave background
102,1103208448303325184,793639812825690112,Xiaojian Ma,"['(1/2) New preprint on audio-based perception for robotic pouring. Compared to conventional visual and haptic sensing, we found audio works surprisingly well on unknown containers and different types/status of liquid.\n\npaper:<LINK>\nvideo:<LINK>', '(2/2) We also collected a dataset with &gt;3000 human pouring sequences. Besides audio and liquid status, video, haptic (torque and force) and trajectory data are also recorded. Checkout the project page for more details(may still under constructions): https://t.co/6Cpb8S0lkk']",https://arxiv.org/abs/1903.00650,"In this paper, we focus on the challenging perception problem in robotic pouring. Most of the existing approaches either leverage visual or haptic information. However, these techniques may suffer from poor generalization performances on opaque containers or concerning measuring precision. To tackle these drawbacks, we propose to make use of audio vibration sensing and design a deep neural network PouringNet to predict the liquid height from the audio fragment during the robotic pouring task. PouringNet is trained on our collected real-world pouring dataset with multimodal sensing data, which contains more than 3000 recordings of audio, force feedback, video and trajectory data of the human hand that performs the pouring task. Each record represents a complete pouring procedure. We conduct several evaluations on PouringNet with our dataset and robotic hardware. The results demonstrate that our PouringNet generalizes well across different liquid containers, positions of the audio receiver, initial liquid heights and types of liquid, and facilitates a more robust and accurate audio-based perception for robotic pouring. ","Making Sense of Audio Vibration for Liquid Height Estimation in Robotic
  Pouring"
103,1103120500371456000,28847789,Zsolt Kira,"['Excited about our #cvpr2019  oral paper on using progress monitoring for instruction-based navigation, with a new learned rollback mechanism for backtracking. \n\narXiv: <LINK>\nWebpage: <LINK> (w/ code)\n\nWork with Salesforce Research @CaimingXiong <LINK>', ""@omar_javd @jigarkdoshi @CaimingXiong Thanks! Hope to see you all there if you're going!""]",https://arxiv.org/abs/1903.01602,"As deep learning continues to make progress for challenging perception tasks, there is increased interest in combining vision, language, and decision-making. Specifically, the Vision and Language Navigation (VLN) task involves navigating to a goal purely from language instructions and visual information without explicit knowledge of the goal. Recent successful approaches have made in-roads in achieving good success rates for this task but rely on beam search, which thoroughly explores a large number of trajectories and is unrealistic for applications such as robotics. In this paper, inspired by the intuition of viewing the problem as search on a navigation graph, we propose to use a progress monitor developed in prior work as a learnable heuristic for search. We then propose two modules incorporated into an end-to-end architecture: 1) A learned mechanism to perform backtracking, which decides whether to continue moving forward or roll back to a previous state (Regret Module) and 2) A mechanism to help the agent decide which direction to go next by showing directions that are visited and their associated progress estimate (Progress Marker). Combined, the proposed approach significantly outperforms current state-of-the-art methods using greedy action selection, with 5% absolute improvement on the test server in success rates, and more importantly 8% on success rates normalized by the path length. Our code is available at this https URL . ","The Regretful Agent: Heuristic-Aided Navigation through Progress
  Estimation"
104,1103001281227091975,861775999,Laura A. Hayes,"[""Great start to the week - started a new postdoc @NASAGoddard and my paper just got accepted for publication in ApJ üíÉ check it out here 'Persistent Quasi-Periodic Pulsations During a Large X-Class Solar Flare' #AR12673 <LINK> @petertgallagher @diana_morosan""]",https://arxiv.org/abs/1903.01328,"Solar flares often display pulsating and oscillatory signatures in the emission, known as quasi-periodic pulsations (QPP). QPP are typically identified during the impulsive phase of flares, yet in some cases, their presence is detected late into the decay phase. Here, we report extensive fine structure QPP that are detected throughout the large X8.2 flare from 2017 September 10. Following the analysis of the thermal pulsations observed in the GOES/XRS and the 131 A channel of SDO/AIA, we find a pulsation period of ~65 s during the impulsive phase followed by lower amplitude QPP with a period of ~150 s in the decay phase, up to three hours after the peak of the flare. We find that during the time of the impulsive QPP, the soft X-ray source observed with RHESSI rapidly rises at a velocity of approximately 17 km/s following the plasmoid/coronal mass ejection (CME) eruption. We interpret these QPP in terms of a manifestation of the reconnection dynamics in the eruptive event. During the long-duration decay phase lasting several hours, extended downward contractions of collapsing loops/plasmoids that reach the top of the flare arcade are observed in EUV. We note that the existence of persistent QPP into the decay phase of this flare are most likely related to these features. The QPP during this phase are discussed in terms of MHD wave modes triggered in the post-flaring loops. ",Persistent Quasi-Periodic Pulsations During a Large X-Class Solar Flare
105,1102857539631353856,253279512,Stefano Sanvito,['Our new paper of spin-relaxation in qbits from first principle is now out on arXiV. Lots of work but great understanding on the microscopic mechanisms for relaxation @cranntcd @TCD_physics \n<LINK>'],https://arxiv.org/abs/1903.01424,"The coupling between electronic spins and lattice vibrations is fundamental for driving relaxation in magnetic materials. The debate over the nature of spin-phonon coupling dates back to the 40's, but the role of spin-spin, spin-orbit and hyperfine interactions, has never been fully established. Here we present a comprehensive study of the spin dynamics of a crystal of Vanadyl-based molecular qubits by means of first-order perturbation theory and first-principles calculations. We quantitatively determine the role of the Zeeman, hyperfine and electronic spin dipolar interactions in the direct mechanism of spin relaxation. We show that, in a high magnetic field regime, the modulation of the Zeeman Hamiltonian by the intra-molecular components of the acoustic phonons dominates the relaxation mechanism. In low fields, hyperfine coupling takes over, with the role of spin-spin dipolar interaction remaining the less important for the spin relaxation. ","Spin-Phonon Relaxation in Molecular Qubits from First Principles Spin
  Dynamics"
106,1102856773373968384,19485737,Sean Benson,['Real-time analysis @LHCbExperiment comprehensively described now in a new paper <LINK> #CERN #LHCb'],https://arxiv.org/abs/1903.01360,"An evolved real-time data processing strategy is proposed for high-energy physics experiments, and its implementation at the LHCb experiment is presented. The reduced event model allows not only the signal candidate firing the trigger to be persisted, as previously available, but also an arbitrary set of other reconstructed or raw objects from the event. This allows for higher trigger rates for a given output data bandwidth, when compared to the traditional model of saving the full raw detector data for each trigger, whilst accommodating inclusive triggers and preserving data mining capabilities. The gains in physics reach and savings in computing resources already made possible by the model are discussed, along with the prospects of employing it more widely for Run 3 of the Large Hadron Collider. ",A comprehensive real-time analysis model at the LHCb experiment
107,1102837544796864512,115805003,Matteo Cantiello,"['New MESA instrument paper is out! Pulsating Variable Stars, Rotation, Convective Boundaries, and Energy Conservation. Proud of being part of this amazing project <LINK> #MESAStar @FlatironCCA He-flash while conserving energy to better than 0.001%? Piece of cake! <LINK>']",https://arxiv.org/abs/1903.01426,"We update the capabilities of the open-knowledge software instrument Modules for Experiments in Stellar Astrophysics (MESA). RSP is a new functionality in MESAstar that models the non-linear radial stellar pulsations that characterize RR Lyrae, Cepheids, and other classes of variable stars. We significantly enhance numerical energy conservation capabilities, including during mass changes. For example, this enables calculations through the He flash that conserve energy to better than 0.001 %. To improve the modeling of rotating stars in MESA, we introduce a new approach to modifying the pressure and temperature equations of stellar structure, and a formulation of the projection effects of gravity darkening. A new scheme for tracking convective boundaries yields reliable values of the convective-core mass, and allows the natural emergence of adiabatic semiconvection regions during both core hydrogen- and helium-burning phases. We quantify the parallel performance of MESA on current generation multicore architectures and demonstrate improvements in the computational efficiency of radiative levitation. We report updates to the equation of state and nuclear reaction physics modules. We briefly discuss the current treatment of fallback in core-collapse supernova models and the thermodynamic evolution of supernova explosions. We close by discussing the new MESA Testhub software infrastructure to enhance source-code development. ","Modules for Experiments in Stellar Astrophysics (MESA): Pulsating
  Variable Stars, Rotation, Convective Boundaries, and Energy Conservation"
108,1102822037347741697,628665575,Robert R. Thomson,"['Interested in how photonic lanterns open up powerful new imaging modalities? \n\nIf so, check out our new paper on the Arxiv:\n\n<LINK>', '@sgleonsaval Thanks @sgleonsaval \n\nDid you guys think those devices would have so many applications back in 2005?\n\nhttps://t.co/Qdl41qNSPG\n\nI wonder what I‚Äôd be working on now if @JossBlandHawtho hadn‚Äôt needed to get that multimode light into those pesky single mode FBGs.']",https://arxiv.org/abs/1903.01288,"The thin and flexible nature of optical fibres often makes them the ideal technology to view biological processes in-vivo, but current microendoscopic approaches are limited in spatial resolution. Here, we demonstrate a new route to high resolution microendoscopy using a multicore fibre (MCF) with an adiabatic multimode-to-singlemode photonic lantern transition formed at the distal end by tapering. We show that distinct multimode patterns of light can be projected from the output of the lantern by individually exciting the single-mode MCF cores, and that these patterns are highly stable to fibre movement. This capability is then exploited to demonstrate a form of single-pixel imaging, where a single pixel detector is used to detect the fraction of light transmitted through the object for each multimode pattern. A custom compressive imaging algorithm we call SARA-COIL is used to reconstruct the object using only the pre-measured multimode patterns themselves and the detector signals. ",Compressive optical imaging with a photonic lantern
109,1102815882684846080,617492880,Benoit Famaey,"['New paper on the arxiv today with T. Haines, @elenadonghia , @cfl2126 &amp; Lars Hernquist: we showed how the disequilibrium of a vertically perturbed disk affects our determination of the disk surface density with Jeans modelling <LINK>', 'Taking the 1st order moments of the collisionless Boltzmann eq leads to the Jeans eqs. Using the Jeans equation for vertical velocities of stars accross the Galactic disk in combination with the Poisson eq allows to determine the disk surface density as a function of height: nice', 'This method has been used countless times to determine the dynamical surface density of the Milky Way disk, and the associated density of dark matter in it. However, this is of course valid only at equilibrium', 'But back in 2012, Larry Widrow et al. discovered with SEGUE North-South asymmetries in the vertical density and velocity of stars accross the Galactic plane: ""Galactoseismology"" was born https://t.co/sPnS0PNU82', 'Soon later, various studies showed that the outer disk was actually a buckled mess, with disk stars actually ejected multiple kpc from the plane, see e.g. the recent work of @MariaBergemann et al https://t.co/TDlnMpJZmI', 'The original kinematic finding of Widrow et al., which has been confirmed by various spectroscopic surveys such as RAVE, has recently taken yet a new turn with the publication of the Gaia DR2', ""Indeed, @AntojaTeresa et al. discovered that solar neighbourhood's stars vertical motions are correlated with their motions in the plane of the Galaxy, creating a groovy phase-space spiral! https://t.co/TfdLY7NMhU"", 'This has been attributed to various potential causes, such as a relic from the buckling phase of the bar formation https://t.co/2TGsp6Uaku , or to the impact of the Sagittarius dwarf galaxy on the Milky Way disk https://t.co/TOgf9a4cjh', 'The recent pre-Gaia DR2 Sgr dwarf impact models of @cfl2126 demonstrated that they could simultaneously account for the morphology of the outer disk and amplitude of density and streaming motion fluctuations in the solar neighbourhood', 'We thus set out to analyze those simulations to check the validity of Jeans equations to evaluate the disk surface density in the presence of such non-equilibrium phenomena', 'We first showed that the Jeans method works perfectly fine at the beginning of the simulation, when the Sgr dwarf hasnt affected the Galactic disk just yet', 'But once the vertical perturbation is acting at full steam, things are different. First of all, because of the North-South asymmetries, it is best to perform the Jeans analysis separately in the North and South', 'In the most overdense regions of the disk, we noticed that the phase-space spiral is less well-defined than in the underdense regions. This is actually a nice prediction of the simulations, which could be checked with future Gaia data releases', 'Because of this reasonable phase-space distribution, we found out that the Jeans modelling *does* work reasonably well in such overdense regions', 'On this plot, the green (North) and red (South) lines, which are the result of our Jeans analysis (for the surface density in Msun/pc^2 as a function of z in kpc), fits the orange line (which is the true surface density in the simulation) reasonably well https://t.co/rgqQ3XzUkD', 'In the regions of intermediate (typical) density, we found out that the North and South Jeans analyses yield vastly different results. This is worrying, but an observer performing a Jeans analysis on the data would quickly notice the problem...', 'Things are however different in the most underdense regions. There, the Jeans modelling *does* give similar results in the North and South, but severely overestimates the disk surface density (and hence the dark matter density!) https://t.co/KED34twHw9', 'The consequences of this are profound: it means that we cannot necessarily trust dark matter density estimates based on the equilbrium assumption. This is actually already an issue locally, in our neighbourhood, but is going to be much worse in the outer disk!', ""So what's next? Clearly, exciting times ahead, as we do need to develop non-equilibrium methods to determine the gravitational potential of the outer disk. Some preliminary approaches, but neglecting the disk self-gravity, have recently been proposed, e.g. https://t.co/qyGVuwzTpT"", 'Building on such methods to include the disk self-gravity in the equations is certainly a promising way forward. But equilibrium models, especially for the messy outer Milky Way disk, are dead! #End']",https://arxiv.org/abs/1903.00607,"Recent studies have shown that the passage of a massive satellite through the disk of a spiral galaxy can induce vertical wobbles in the disk and produce features such as in-plane rings and phase-space spirals. Here we analyze a high-resolution N-body simulation of a live stellar disk perturbed by the recent passage of a massive dwarf galaxy that induces such perturbations. We study the implications of the phase-space structures for the estimate of the matter density through traditional Jeans modelling. The dwarf satellite excites rapid time-variations in the potential, leading to a significant bias of the local matter surface density determined through such a method. In particular, while the Jeans modelling gives reasonable estimates in the most overdense regions of the disk, we show that it tends to overestimate the dynamical surface density in underdense regions. In these regions, the phase-space spiral is indeed more marked in the surface of section of height vs. vertical velocity. This prediction can be verified with future Gaia data releases. Our finding is highly relevant for future attempts at determining the dynamical surface density of the outer Milky Way disk as a function of radius. The outer disk of the Milky Way is indeed heavily perturbed, and \textit{Gaia} DR2 data have clearly shown that such phase-space perturbations are even present locally. While our results show that traditional Jeans modelling should give reliable results in overdense regions of the disk, the important biases in underdense regions call for the development of non-equilibrium methods to estimate the dynamical matter density locally and in the outer disk. ","Implications of a time-varying Galactic potential for determinations of
  the dynamical surface density"
110,1102385502089441280,866469276978364416,Matt Gardner,"['Announcing DROP, a new reading comprehension benchmark that requires discrete reasoning over paragraphs of text.  New @NAACLHLT paper by @ddua17, @yizhongwyz, @pdasigi, @GabiStanovsky, @sameer_, and me. <LINK> <LINK>', ""I am super excited about this; I've been thinking about this for over a year, and we finally decided to pursue it as our first collaboration between AI2 Irvine and the UCI NLP group. This is a hard dataset that uses complex questions to test comprehensive understanding."", 'Key idea: use compositional questions inspired by the semantic parsing literature to put together many pieces of information from a single paragraph. You must get _multiple_ structures from the paragraph correct in order to answer most questions.', 'Things like ""Which players made touchdowns longer than 10 yards?"", ""How many empires attacked Guadalajara?"", ""Which two ethnicities were tied for the most common?"" and ""How long was the second longest field goal?"".', 'The data is collected with an *adversarial baseline* running in the background - when crowd workers ask questions, we send them to a server running BiDAF, and if BiDAF gets it right, we tell them to ask a harder question. We also give lots of examples of hard questions.', '@ddua17 did an amazing job designing the data collection aspect of this, and pushing through getting a 96k dataset. @yizhongwyz came up with a great extension to QANet that adds some simple numerical reasoning on top.', ""Key result: best baseline model (BERT SQuAD model, retrained on DROP) gets ~32 F1. @yizhongwyz's NAQANet gets ~47 F1. Humans get ~96 F1. Still a *long* way to go. But it's feasible, with a similar format to SQuAD - just adding numbers, dates, and multiple spans as outputs."", 'You can try out a demo here: https://t.co/Z8ea4TjTSK. I put a lot of interesting examples in there, highlighting the different kinds of questions and capabilities of the model (and some where it fails!).', 'I was pretty shocked at how well this model can do maxes and mins. You can change the numbers in the paragraph and it gets it right most of the time. Its counting ability is pretty rudimentary, though (it answers 2 most of the time), and ""second largest"" is too hard for it.', ""Play around with the demo, and let us know what you think! Self-serve leaderboard with a hidden test set will be coming soon (getting the self-serve part ready is taking a bit of time - it'll be based on docker and beaker)."", ""Oh, another thing - this model often does *worse than BiDAF* on SQuAD-like questions (and BiDAF isn't all that great these days). You can see this in the demo by switching back and forth. We still have a long way to go on general reading comprehension systems."", 'Also, the paper is still not camera ready - some of the dataset analysis needs to be updated to include the (harder) second half of the dataset that was collected after the submission deadline. But the results are correct, and on the full dataset.', ""@jackclarkSF @NAACLHLT @ddua17 @yizhongwyz @pdasigi @GabiStanovsky @sameer_ Thanks! We haven't made any internal bets, and how long it takes really depends on how many people take it up. More people working on it means it'll get solved faster. This is fundamentally much harder than SQuAD, but doesn't need *that* many innovations to be solved."", ""@jackclarkSF @NAACLHLT @ddua17 @yizhongwyz @pdasigi @GabiStanovsky @sameer_ I'd be both happy and sad to see it solved in a year or two - sad because I was hoping it would last longer, but happy because I'm pretty sure that solving this actually requires some real understanding, and it would mean we're making progress."", '@maninblack815 @yizhongwyz See table 6. On single-span questions, BERT gets 70 F1. @yizhongwyz tried doing the numerical stuff on top of BERT, but ran out of GPU memory...', 'Additional commentary thread: https://t.co/0dBZ32nWBw', 'Another additional commentary thread (just attaching these here for easy finding): https://t.co/94jXQo2jmD', '@sanjaykamath @yizhongwyz Baselines just used prior reading comprehension models. NAQNet was our attempt to add stuff on top, and we started it before BERT came out (and BERT was too memory hungry for us to use with what we had when it did come out).', ""@sanjaykamath @yizhongwyz I'm sure someone will take NAQNet (or something similar) and use BERT with it, and scores will go up, but I don't think by much. See table 6 - BERT isn't much better than NAQANet on single span questions."", ""@sanjaykamath @yizhongwyz Yeah, there are lots of options here, definitely. I'm excited to see what people come up with!""]",https://arxiv.org/abs/1903.00161,"Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1. ","DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning
  Over Paragraphs"
111,1115132928265265152,97191218,Pantelis Sopasakis,['Safe Learning-Based Control - our new paper is online at <LINK> #learning #control <LINK>'],https://arxiv.org/abs/1903.10040,"We consider the problem of designing control laws for stochastic jump linear systems where the disturbances are drawn randomly from a finite sample space according to an unknown distribution, which is estimated from a finite sample of i.i.d. observations. We adopt a distributionally robust approach to compute a mean-square stabilizing feedback gain with a given probability. The larger the sample size, the less conservative the controller, yet our methodology gives stability guarantees with high probability, for any number of samples. Using tools from statistical learning theory, we estimate confidence regions for the unknown probability distributions (ambiguity sets) which have the shape of total variation balls centered around the empirical distribution. We use these confidence regions in the design of appropriate distributionally robust controllers and show that the associated stability conditions can be cast as a tractable linear matrix inequality (LMI) by using conjugate duality. The resulting design procedure scales gracefully with the size of the probability space and the system dimensions. Through a numerical example, we illustrate the superior sample complexity of the proposed methodology over the stochastic approach. ","Safe Learning-Based Control of Stochastic Jump Linear Systems: a
  Distributionally Robust Approach"
112,1114675138937729025,1018558062444064768,Sanja Fidler,"['We released our new interactive annotation approach, which outperforms Polygon-RNN++ and is 10x faster. Great work by @HuanLing6, @JunGao33210520, @amlankar95, @ChenWenzheng!\nPaper: <LINK>\nVideo: <LINK>\nCode (@PyTorch): <LINK> <LINK>']",https://arxiv.org/abs/1903.06874,"Manually labeling objects by tracing their boundaries is a laborious process. In Polygon-RNN++ the authors proposed Polygon-RNN that produces polygonal annotations in a recurrent manner using a CNN-RNN architecture, allowing interactive correction via humans-in-the-loop. We propose a new framework that alleviates the sequential nature of Polygon-RNN, by predicting all vertices simultaneously using a Graph Convolutional Network (GCN). Our model is trained end-to-end. It supports object annotation by either polygons or splines, facilitating labeling efficiency for both line-based and curved objects. We show that Curve-GCN outperforms all existing approaches in automatic mode, including the powerful PSP-DeepLab and is significantly more efficient in interactive mode than Polygon-RNN++. Our model runs at 29.3ms in automatic, and 2.6ms in interactive mode, making it 10x and 100x faster than Polygon-RNN++. ",Fast Interactive Object Annotation with Curve-GCN
113,1113621126452740096,108617137,Alvise Raccanelli,"['Given the recent news on primordial black holes as dark matter, here is an advertisement for our new paper with constraints from Lyman-Œ±. Another great work of my former student Giulio Scelfo! <LINK> #primordialblackholes']",https://arxiv.org/abs/1903.10509,"The renewed interest in the possibility that primordial black holes (PBHs) may constitute a significant part of the dark matter has motivated revisiting old observational constraints, as well as developing new ones. We present new limits on the PBH abundance, from a comprehensive analysis of high-resolution, high-redshift Lyman-$\alpha$ forest data. Poisson fluctuations in the PBH number density induce a small-scale power enhancement which departs from the standard cold dark matter prediction. Using a grid of hydrodynamic simulations exploring different values of astrophysical parameters, {we obtain a marginalized upper limit on the PBH mass of $f_{\rm PBH}M_{\rm PBH} \sim 60~M_{\odot}$ at $2\sigma$, when a Gaussian prior on the reionization redshift is imposed, preventing its posterior distribution to peak on very high values, which are disfavoured by the most recent estimates obtained both through Cosmic Microwave Background and Inter-Galactic Medium observations. Such bound weakens to $f_{\rm PBH}M_{\rm PBH} \sim 170~M_{\odot}$, when a conservative flat prior is instead assumed. Both limits significantly improves previous constraints from the same physical observable.} We also extend our predictions to non-monochromatic PBH mass distributions, ruling out large regions of the parameter space for some of the most viable PBH extended mass functions. ","Lyman-$\alpha$ forest constraints on Primordial Black Holes as Dark
  Matter"
114,1110753735263678464,179821795,Scott McCue,['New paper with @ProfMJSimpson and @wjinmathbio on the arXiv about using nonlinear degenerate diffusion to model 2D wound healing assays <LINK> <LINK>'],https://arxiv.org/abs/1903.10800,"Continuum mathematical models for collective cell motion normally involve reaction-diffusion equations, such as the Fisher-KPP equation, with a linear diffusion term to describe cell motility and a logistic term to describe cell proliferation. While the Fisher-KPP equation and its generalisations are commonplace, a significant drawback for this family of models is that they are not able to capture the moving fronts that arise in cell invasion applications such as wound healing and tumour growth. An alternative, less common, approach is to include nonlinear degenerate diffusion in the models, such as in the Porous-Fisher equation, since solutions to the corresponding equations have compact support and therefore explicitly allow for moving fronts. We consider here a hole-closing problem for the Porous-Fisher equation whereby there is initially a simply connected region (the hole) with a nonzero population outside of the hole and a zero population inside. We outline how self-similar solutions (of the second kind) describe both circular and non-circular fronts in the hole-closing limit. Further, we present new experimental and theoretical evidence to support the use of nonlinear degenerate diffusion in models for collective cell motion. Our methodology involves setting up a 2D wound healing assay that has the geometry of a hole-closing problem, with cells initially seeded outside of a hole that closes as cells migrate and proliferate. For a particular class of fibroblast cells, the aspect ratio of an initially rectangular wound increases in time, so the wound becomes longer and thinner as it closes; our theoretical analysis shows that this behaviour is consistent with nonlinear degenerate diffusion but is not able to be captured with commonly used linear diffusion. This work is important because it provides a clear test for degenerate diffusion over linear diffusion in cell lines. ","Hole-closing model reveals exponents for nonlinear degenerate
  diffusivity functions in cell biology"
115,1108676367619682307,743028716457070592,Franco Vazza,['New #magcow article led by C.Gheller ( @EPFL ) on the analysis of thermal and magnetic properties of all filaments (~700-800) in our CHRONOS++ simulations (@cscsch ). <LINK>  A small digest of the paper in a thread later ! <LINK>'],https://arxiv.org/abs/1903.08401,"In this paper, we exploit a large suite of {\enzo} cosmological magneto-hydrodynamical simulations adopting uniform mesh resolution, to investigate the properties of cosmic filaments under different baryonic physics and magnetogenesis scenarios. We exploit a isovolume based algorithm to identify filaments and determine their attributes from the continuous distribution of gas mass density in the simulated volumes. The global (e.g. mass, size, mean temperature and magnetic field strength, enclosed baryon fraction) and internal (e.g. density, temperature, velocity and magnetic field profiles) properties of filaments in our volume are calculated across almost four orders of magnitude in mass. The inclusion of variations in non-gravitational physical processes (radiative cooling, star formation, feedback from star forming regions and active galactic nuclei) as well as in the seeding scenarios for magnetic fields (early magnetisation by primordial process vs later seeding by galaxies) allows us to study both the large-scale thermodynamics and the magnetic properties of the Warm-Hot Intergalactic Medium (WHIM) with an unprecedented detail. We show how the impact of non-gravitational physics on the global thermodynamical properties of filaments is modest, with the exception of the densest gas environment surrounding galaxies in filaments. Conversely, the magnetic properties of the WHIM in filament are found to dramatically vary as different seeding scenarios are considered. We study the correlation between the properties of galaxy-sized halos and their host filaments, as well as between the halos and the local WHIM in which they lie. Significant general statistical trends are reported. ",A survey of the thermal and non-thermal properties of cosmic filaments
116,1107668283468251136,74163970,Tim Harries,['A new paper on the TORUS radiative transfer code. This has been a bit of a labour of love! with esteemed colleagues @TomHaworthAstro Dave Acreman @plingaling and Tom Douglas <LINK> <LINK>'],https://arxiv.org/abs/1903.06672,"We present a review of the TORUS radiation transfer and hydrodynamics code. TORUS uses a 1-D, 2-D or3-D adaptive mesh refinement scheme to store and manipulate the state variables, and solves the equation of radiative transfer using Monte Carlo techniques. A framework of microphysics modules is described, including atomic and molecular line transport in moving media, dust radiative equilibrium, photoionisation equilibrium, and time-dependent radiative transfer. These modules provide a flexible scheme for producing synthetic observations, either from analytical models or as post-processing of hydrodynamical simulations (both grid-based and Lagrangian). A hydrodynamics module is also presented, which maybe used in combination with the radiation-transport modules to perform radiation-hydrodynamics simulations. Benchmarking and validation tests of each major mode of operation are detailed, along with descriptions and performance/scaling tests of the various parallelisation schemes. We give examples of the uses of the code in the literature, including applications to low- and high-mass star formation, cluster feedback, and stellar winds, along with an Appendix listing the refereed papers that have used TORUS. ",The TORUS radiation transfer code
117,1106840083217477632,1095087149932822534,Will Smith,"['New paper: <LINK> appearing as an oral @cvpr2019. We release CFHM (Combined Face and Head Model) a new publicly available 3D morphable model built by merging LSFM trained on 10k face scans and LYHM trained on 1.2k full heard scans. <LINK>', 'Favourite result in the paper: reconstructing bald Agassi from Agassi with a mullet! https://t.co/QOSgcLbNDq']",https://arxiv.org/abs/1903.03785,"Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for representing the 3D surfaces of an object class. In this context, we identify an interesting question that has previously not received research attention: is it possible to combine two or more 3DMMs that (a) are built using different templates that perhaps only partly overlap, (b) have different representation capabilities and (c) are built from different datasets that may not be publicly-available? In answering this question, we make two contributions. First, we propose two methods for solving this problem: i. use a regressor to complete missing parts of one model using the other, ii. use the Gaussian Process framework to blend covariance matrices from multiple models. Second, as an example application of our approach, we build a new face-and-head shape model that combines the variability and facial detail of the LSFM with the full head modelling of the LYHM. The resulting combined shape model achieves state-of-the-art performance and outperforms existing head models by a large margin. Finally, as an application experiment, we reconstruct full head representations from single, unconstrained images by utilizing our proposed large-scale model in conjunction with the FaceWarehouse blendshapes for handling expressions. ",Combining 3D Morphable Models: A Large scale Face-and-Head Model
118,1106142940949291009,946520034465255424,JoseMunozCastaneda,['First draft of our new paper is available on the arXiv today:\n<LINK>\nIs a very nice result about what happens to relativistic electrons interacting with the sermonic Dirac-delta potential'],https://arxiv.org/abs/1903.05568,"We study the spectrum of the 1D Dirac Hamiltonian encompassing the bound and scattering states of a fermion distorted by a static background built from $\delta$-function potentials. We distinguish between ""mass-spike"" and ""electrostatic"" $\delta$-potentials. Differences in the spectra arising depending on the type of $\delta$-potential studied are thoroughly explored. ",One-dimensional scattering of fermions on $\delta$-impurities
119,1105918633635569664,503452360,William Wang,['Standard predefined labels &amp; train/dev/test setting are suboptimal for streaming data. Our #NAACL2019 paper Sentence Embedding Alignment for Lifelong Relation Extraction introduces a new lifelong IE problem &amp; an efficient SOTA solution. Paper+Code: <LINK>\n#NLProc <LINK>'],https://arxiv.org/abs/1903.02588,"Conventional approaches to relation extraction usually require a fixed set of pre-defined relations. Such requirement is hard to meet in many real applications, especially when new data and relations are emerging incessantly and it is computationally expensive to store all data and re-train the whole model every time new data and relations come in. We formulate such a challenging problem as lifelong relation extraction and investigate memory-efficient incremental learning methods without catastrophically forgetting knowledge learned from previous tasks. We first investigate a modified version of the stochastic gradient methods with a replay memory, which surprisingly outperforms recent state-of-the-art lifelong learning methods. We further propose to improve this approach to alleviate the forgetting problem by anchoring the sentence embedding space. Specifically, we utilize an explicit alignment model to mitigate the sentence embedding distortion of the learned model when training on new data and new relations. Experiment results on multiple benchmarks show that our proposed method significantly outperforms the state-of-the-art lifelong learning approaches. ",Sentence Embedding Alignment for Lifelong Relation Extraction
120,1105471143958454272,312448486,Dr. Karan Jani,"['Astrophysics for the next decade - our new paper on listening to the Universe from ground and space. With @lirarandall, @sasomao and scientists with the @LISACommunity. #Astro2020 \n\n<LINK> <LINK>', 'This is part of white papers submitted by the entire astronomy and astrophysics community to the @theNASEM for prioritizing science of the next 10 years. \n\nThe impact of these papers raises all the way to the United States Congressional committees. #Astro2020 https://t.co/zRSYXiHB8n']",https://arxiv.org/abs/1903.04069,"The LIGO/Virgo gravitational-wave (GW) interferometers have to-date detected ten merging black hole (BH) binaries, some with masses considerably larger than had been anticipated. Stellar-mass BH binaries at the high end of the observed mass range (with ""chirp mass"" ${\cal M} \gtrsim 25 M_{\odot}$) should be detectable by a space-based GW observatory years before those binaries become visible to ground-based GW detectors. This white paper discusses some of the synergies that result when the same binaries are observed by instruments in space and on the ground. We consider intermediate-mass black hole binaries (with total mass $M \sim 10^2 -10^4 M_{\odot}$) as well as stellar-mass black hole binaries. We illustrate how combining space-based and ground-based data sets can break degeneracies and thereby improve our understanding of the binary's physical parameters. While early work focused on how space-based observatories can forecast precisely when some mergers will be observed on the ground, the reverse is also important: ground-based detections will allow us to ""dig deeper"" into archived, space-based data to confidently identify black hole inspirals whose signal-to-noise ratios were originally sub-threshold, increasing the number of binaries observed in both bands by a factor of $\sim 4 - 7$. ",What we can learn from multi-band observations of black hole binaries
121,1104025383857463301,94807833,Diego Lorenzo-Oliveira,"['Check out our new paper on the past and future of solar rotation through solar twins! <LINK> <LINK>', '@rachel_b93 Thanks, Rachel! üòÑ']",https://arxiv.org/abs/1903.02630,"The stellar Rotation $vs.$ Age relation is commonly considered as a useful tool to derive reliable ages for Sun-like stars. However, in the light of \kepler\ data, the presence of apparently old and fast rotators that do not obey the usual gyrochronology relations led to the hypothesis of weakened magnetic breaking in some stars. In this letter, we constrain the solar rotation evolutionary track using solar twins. Predicted rotational periods as a function of mass, age, [Fe/H] and given critical Rossby number ($Ro_{\rm crit}$) were estimated for the entire rotational sample. Our analysis favors the smooth rotational evolution scenario and suggests that, if the magnetic weakened breaking scenario takes place at all, it should arise after $Ro_{\rm crit}\gtrsim2.29$ or ages $\gtrsim$5.3 Gyr (at 95$\%$ confidence level). ",Constraining the evolution of stellar rotation using solar twins
122,1103293262390087683,953282872550584321,Patrick Tamburo üå≤,"['New paper with @Paul_Dalba! We caught a Spitzer transit of Kepler-167e, a Jupiter-analog exoplanet, to rule out TTVs and get an accurate transit ephemeris through the JWST era. This light curve brought to you by the magic of Pixel-Level Decorrelation. <LINK> <LINK>']",https://arxiv.org/abs/1903.01478,"We acquired observations of a partial transit of Kepler-167e, a Jupiter-analog exoplanet on a 1,071-day orbit, well beyond its water ice line, with the Spitzer Space Telescope. The timing of the Spitzer transit is consistent with the ephemeris measured from the two transits observed previously by the Kepler Space Telescope. The Spitzer observation rules out the existence of transit timing variations (TTVs) of order hours to days that are known to exist for other long-period exoplanets. Such TTVs render transit follow-up efforts intractable due to the substantial observing time required and the high risk of non-detection. For Kepler-167e, however, we are now able to predict future transit times through the anticipated era of the James Webb Space Telescope with uncertainties of less than six minutes. We interpret the lack of TTVs as an indication that Kepler-167e either does not have an exterior massive companion or that the gravitational interactions with any companions are below our detection threshold. We also measure Kepler-167e's 3.6-$\mu$m transit depth and use exoplanet and solar system models to make predictions about its transmission spectrum. The transiting nature of Kepler-167e and its similarity to Jupiter make it a unique and exceptional target for follow-up atmospheric characterization. Kepler-167e falls into a truly rare category among transiting exoplanets, and with a precisely constrained transit ephemeris, it is poised to serve as a benchmark in comparative investigations between exoplanets and the solar system. ",Spitzer Detection of the Transiting Jupiter-analog Exoplanet Kepler-167e
123,1114191978021904384,19658565,Corey Lynch,"['We want our robots to learn complete visual representations, e.g. all object variation in a scene, not just pose. In <LINK>, led by the amazing @sherjilozair, we find MI-based\nrepr suffer from a ""completeness"" problem, and propose a fix based on Wasserstein dist. <LINK>']",http://arxiv.org/abs/1903.11780,"Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge. ",Wasserstein Dependency Measure for Representation Learning
124,1113478084919853057,880873768310030336,Curtis Atkisson,"['Got a little paper submitted to Evo Anth arguing for using multiplex networks in study of human evo. We talk informally about ""multiplex structuring processes"": aspects of multiplex networks that prevent formation of optimal layers. Pre-print lives here <LINK>']",https://arxiv.org/abs/1903.11183,"Social scientists have long appreciated that relationships between individuals cannot be described from observing a single domain, and that the structure across domains of interaction can have important effects on outcomes of interest (e.g., cooperation).1 One debate explicitly about this surrounds food sharing. Some argue that failing to find reciprocal food sharing means that some process other than reciprocity must be occurring, whereas others argue for models that allow reciprocity to span domains in the form of trade.2 Multilayer networks, high-dimensional networks that allow us to consider multiple sets of relationships at the same time, are ubiquitous and have consequences, so processes giving rise to them are important social phenomena. The analysis of multi-dimensional social networks has recently garnered the attention of the network science community.3 Recent models of these processes show how ignoring layer interdependencies can lead one to miss why a layer formed the way it did, and/or draw erroneous conclusions.6 Understanding the structuring processes that underlie multiplex networks will help understand increasingly rich datasets, giving more accurate and complete pictures of social interactions. ","Why understanding multiplex social network structuring processes will
  help us better understand the evolution of human behavior"
125,1112655408609398784,947321780,Stanislas Dehaene,"['Our paper on ""The emergence of number and syntax units in LSTM language models"" is now officially out:\n<LINK>\nWe find discrete units for singular and plural, capable of holding information over intervening syntactic structures for long-distance agreement.']",https://arxiv.org/abs/1903.07435,"Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two `number units'. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs. ",The emergence of number and syntax units in LSTM language models
126,1111648110395211776,66169171,Sherjil Ozair ‚òÄÔ∏è,"['My internship work at Google Brain is out on Arxiv: <LINK> . We show that mutual information-based representation fail to capture all relevant factors of variation when the mutual information is high. We propose an alternative based on the Wasserstein distance <LINK>', ""which performs better even in large mutual information settings. Better Lipschitz regularizers (we used gradient penalty) should help more, which we're looking into.\n\nExtremely grateful to my collaborators @coreylynch, Yoshua Bengio, @avdnoord, @svlevine, @psermanet.""]",https://arxiv.org/abs/1903.11780,"Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound of mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge. ",Wasserstein Dependency Measure for Representation Learning
127,1111548904259153925,974490750103650304,Riccardo Volpi,"['Preprint of our recent work (@vmurino) on arxiv today. We propose a way to evaluate the vulnerability of computer vision models against content-preserving image transformations, and design an algorithm to address the issue. <LINK> (TLDR thread)', 'We face the problem in terms of combinatorial optimization. Given a set of image transformations, we search for the concatenations of N transformations (N-tuples) that are most harmful for a given (black-box) model via random and evolution-based search. https://t.co/MRi212Tu1A', 'We propose to train more robust models by searching over iterations for the distributional shifts the current model is vulnerable to, in terms of transformation N-tuples, and defining new data augmentation rules accordingly. https://t.co/RxKwWcodds', 'We train MNIST models more robust against transformation N-tuples from our set (&gt;97% accuracy). Models also result better performing in domain generalization settings: our MNIST models can classify SVHN samples with ~50% accuracy, without a look at SVHN samples during training.', 'Code is available at https://t.co/Z9dwRqSogO. üôÇ']",https://arxiv.org/abs/1903.11900,"We are concerned with the vulnerability of computer vision models to distributional shifts. We formulate a combinatorial optimization problem that allows evaluating the regions in the image space where a given model is more vulnerable, in terms of image transformations applied to the input, and face it with standard search algorithms. We further embed this idea in a training procedure, where we define new data augmentation rules according to the image transformations that the current model is most vulnerable to, over iterations. An empirical evaluation on classification and semantic segmentation problems suggests that the devised algorithm allows to train models that are more robust against content-preserving image manipulations and, in general, against distributional shifts. ","Addressing Model Vulnerability to Distributional Shifts over Image
  Transformation Sets"
128,1111237040195207169,810647071,Andres Olivares,"['If a circular polarisation signal is created during the evolution of the universe, would a net circular polarisation reach us today? In <LINK> we develop a formalism to study such signal and work out the necessary conditions for it to be preserved! @CelineBoehm1 <LINK>', 'This paper was written in collaboration with @MelizabethQ_ , @PhysYL  and @CelineBoehm1']",https://arxiv.org/abs/1903.11074,"The polarisation of sunlight after scattering off the atmosphere was first described by Chandrasekhar using a geometrical description of Rayleigh interactions. Kosowsky later extended Chandrasekhar's formalism by using Quantum Field Theory (QFT) to describe the polarisation of the Cosmological Microwave Background radiation. Here we focus on a case that is rarely discussed in the literature, namely the polarisation of high energy radiation after scattering off particles. After demonstrating why the geometrical and low energy QFT approaches fail in this case, we establish the transport formalism that allows to describe the change of polarisation of high energy photons when they propagate through space or the atmosphere. We primarily focus on Compton interactions but our approach is general enough to describe e.g. the scattering of high energy photons off new particles or through new interactions. Finally we determine the conditions for a circularly polarised $\gamma$--ray signal to keep the same level of circular polarisation as it propagates through its environment. ",Polarisation of high energy gamma-rays after scattering
129,1110832269961449472,1004105220505456641,Paola Dom√≠nguez,['My first article of the PhD as a first author is out!!... We (@HambObs and @franco_vazza) studied the spectral properties of the magnetic fields in the ICM of various simulated galaxy clusters \n<LINK>\n#magcow #MNRAS #ERC <LINK>'],http://arxiv.org/abs/1903.11052,"We investigate the evolution of magnetic fields in galaxy clusters starting from constant primordial fields using highly resolved ($\approx \rm 4 ~kpc$) cosmological MHD simulations. The magnetic fields in our sample exhibit amplification via a small-scale dynamo and compression during structure formation. In particular, we study how the spectral properties of magnetic fields are affected by mergers, and we relate the measured magnetic energy spectra to the dynamical evolution of the intracluster medium. The magnetic energy grows by a factor of $\sim$ 40-50 in a time-span of $\sim 9$ Gyr and equipartition between kinetic and magnetic energy occurs on a range of scales ($< 160 \rm ~kpc$ at all epochs) depending on the turbulence state of the system. We also find that, in general, the outer scale of the magnetic field and the MHD scale are not simply correlated in time. The effect of major mergers is to shift the peak magnetic spectra to it smaller scales, whereas the magnetic amplification only starts after $\lesssim$ 1 Gyr. In contrast, continuous minor mergers promote the steady growth of the magnetic field. We discuss the implications of these findings in the interpretation of future radio observations of galaxy clusters. ",Dynamical evolution of magnetic fields in the intracluster medium
130,1110527477867253760,633009378,Xavier Gir√≥üéó,['Our #icassp2019 Wav2Pix has been published today on arXIv. Find out how we use speech to imagine faces of youtubers such as @jaimealtozano or @javianmuniz. \n\nJoint work by @la_UPC @insight_centre @BSC_CNS #DLUPC\n\nPaper: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/1903.10195,"Speech is a rich biometric signal that contains information about the identity, gender and emotional state of the speaker. In this work, we explore its potential to generate face images of a speaker by conditioning a Generative Adversarial Network (GAN) with raw speech input. We propose a deep neural network that is trained from scratch in an end-to-end fashion, generating a face directly from the raw speech waveform without any additional identity information (e.g reference image or one-hot encoding). Our model is trained in a self-supervised approach by exploiting the audio and visual signals naturally aligned in videos. With the purpose of training from video data, we present a novel dataset collected for this work, with high-quality videos of youtubers with notable expressiveness in both the speech and visual signals. ","Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial
  Networks"
131,1110413700433960960,1050529973751205888,j√∂rn jacobsen,"['1/4 Norm-bounded robustness can cause invariance-based vulnerability. We are able to find adversarial examples within robust epsilon balls around data! Paper: <LINK> with: @JensBehrmann, Nicholas Carlini, Florian Tram√®r, @NicolasPapernot <LINK>', '2/4 This insight allows us to design *model-agnostic* attacks, which exploit excessive invariance around datapoints in norm-bounded adversarially robust models. https://t.co/M8VPw6QxBz', '3/4 Humans agree *less often* with perturbation-robust models than with unrobust models on invariance-based adversarial examples. Indicating that more robustness to one model failure can imply less robustness to another failure. https://t.co/15IX77Y57j', '4/4 In summary, we argue that the term adversarial example captures a series of model limitations. We call for a set of precise definitions that taxonomize and address each of the shortcomings in learning. Invariance-based adversarial examples need to be taken more seriously!']",https://arxiv.org/abs/1903.10484,"Adversarial examples are malicious inputs crafted to cause a model to misclassify them. Their most common instantiation, ""perturbation-based"" adversarial examples introduce changes to the input that leave its true label unchanged, yet result in a different model prediction. Conversely, ""invariance-based"" adversarial examples insert changes to the input that leave the model's prediction unaffected despite the underlying input's label having changed. In this paper, we demonstrate that robustness to perturbation-based adversarial examples is not only insufficient for general robustness, but worse, it can also increase vulnerability of the model to invariance-based adversarial examples. In addition to analytical constructions, we empirically study vision classifiers with state-of-the-art robustness to perturbation-based adversaries constrained by an $\ell_p$ norm. We mount attacks that exploit excessive model invariance in directions relevant to the task, which are able to find adversarial examples within the $\ell_p$ ball. In fact, we find that classifiers trained to be $\ell_p$-norm robust are more vulnerable to invariance-based adversarial examples than their undefended counterparts. Excessive invariance is not limited to models trained to be robust to perturbation-based $\ell_p$-norm adversaries. In fact, we argue that the term adversarial example is used to capture a series of model limitations, some of which may not have been discovered yet. Accordingly, we call for a set of precise definitions that taxonomize and address each of these shortcomings in learning. ","Exploiting Excessive Invariance caused by Norm-Bounded Adversarial
  Robustness"
132,1107676415162494976,34376328,Tal Linzen,"['New NAACL paper with @ravfogel and @yoavgo: to study what makes the syntax of language difficult to learn for an RNN, we created synthetic versions of an English corpus. <LINK>', ""For example, we permuted the order of the subject, verb and object, and trained an RNN to predict at the verb whether its subject is singular or plural (and same for object, following up on @ravfogel's work on Basque https://t.co/FedbBgHC9I): https://t.co/qfkqgAcvyA"", 'RNNs were susceptible to attraction effects - subject prediction was easier in the subject-verb-object language (like English), where the subject and the verb are adjacent, than in the subject-object-verb language https://t.co/FScDUEXymf', '(This is unlike the typology of word order - subject-object-verb is a very common word order in the languages of the world.)', 'When objects were withheld in training, the RNNs learned the generalization that the subject is the argument that precedes the verb rather the generalization that it is the *first* argument (which is equally compatible with the data), supporting a recency inductive bias.', 'Overt case marking made syntactic prediction easier, and so did polypersonal agreement (with both object and subject), as in Basque.', ""We clearly just scratched the surface of the questions that can be asked with this methodology (which was borrowed from @Laplace_wdd and @adveisner's parsing work) - other learners, other typological properties, comparison to humans, etc, would all be interesting directions.""]",https://arxiv.org/abs/1903.06400,"How do typological properties such as word order and morphological case marking affect the ability of neural sequence models to acquire the syntax of a language? Cross-linguistic comparisons of RNNs' syntactic performance (e.g., on subject-verb agreement prediction) are complicated by the fact that any two languages differ in multiple typological properties, as well as by differences in training corpus. We propose a paradigm that addresses these issues: we create synthetic versions of English, which differ from English in one or more typological parameters, and generate corpora for those languages based on a parsed English corpus. We report a series of experiments in which RNNs were trained to predict agreement features for verbs in each of those synthetic languages. Among other findings, (1) performance was higher in subject-verb-object order (as in English) than in subject-object-verb order (as in Japanese), suggesting that RNNs have a recency bias; (2) predicting agreement with both subject and object (polypersonal agreement) improves over predicting each separately, suggesting that underlying syntactic knowledge transfers across the two tasks; and (3) overt morphological case makes agreement prediction significantly easier, regardless of word order. ","Studying the Inductive Biases of RNNs with Synthetic Variations of
  Natural Languages"
133,1106573677380792321,3290170484,Jarrod McClean,"['Will quantum codes prove useful on near term quantum computers? To find out, read more in our new paper that we previewed at APS March meeting! <LINK>']",https://arxiv.org/abs/1903.05786v1,"With the rapid developments in quantum hardware comes a push towards the first practical applications on these devices. While fully fault-tolerant quantum computers may still be years away, one may ask if there exist intermediate forms of error correction or mitigation that might enable practical applications before then. In this work, we consider the idea of post-processing error decoders using existing quantum codes, which are capable of mitigating errors on encoded logical qubits using classical post-processing with no complicated syndrome measurements or additional qubits beyond those used for the logical qubits. This greatly simplifies the experimental exploration of quantum codes on near-term devices, removing the need for locality of syndromes or fast feed-forward, allowing one to study performance aspects of codes on real devices. We provide a general construction equipped with a simple stochastic sampling scheme that does not depend explicitly on a number of terms that we extend to approximate projectors within a subspace. This theory then allows one to generalize to the correction of some logical errors in the code space, correction of some physical unencoded Hamiltonians without engineered symmetries, and corrections derived from approximate symmetries. In this work, we develop the theory of the method and demonstrate it on a simple example with the perfect $[[5,1,3]]$ code, which exhibits a pseudo-threshold of $p \approx 0.50$ under a single qubit depolarizing channel applied to all qubits. We also provide a demonstration under the application of a logical operation and performance on an unencoded hydrogen molecule, which exhibits a significant improvement over the entire range of possible errors incurred under a depolarizing channel. ",] Decoding quantum errors with subspace expansions
134,1106163179846070272,50901426,Rafael Alves Batista,"['In this proceedings of the conference Black Holes as Cosmic Batteries  (BHCB2018), we study hadronic models for very-high-energy emission by  radio galaxies.  <LINK> <LINK>']",https://arxiv.org/abs/1903.05249,"Non-blazar radio-galaxies emitting in the very-high-energy (VHE; >100 GeV) regime offer a unique perspective for probing particle acceleration and emission processes in black hole (BH) accretion-jet systems. The misaligned nature of these sources indicates the presence of an emission component that could be of hadronic origin and located in the core region. Here we consider turbulent magnetic reconnection in the BH accretion flow of radio-galaxies as a potential mechanism for cosmic-ray (CR) acceleration and VHE emission. To investigate if this scenario is able to account for the observed VHE data, we combine three numerical techniques to self-consistently model the accretion flow environment and the propagation of CRs plus electromagnetic cascades within the accretion flow zone. Here we apply our approach to the radio-galaxy Centaurus A and find that injection of CRs consistent with magnetic reconnection power partially reproduce the VHE data, provided that the accretion flow makes no substantial contribution to the radio-GeV components. The associated neutrino emission peaks at $\sim10^{16}$ eV and is two orders of magnitude below the minimum IceCube flux. ","Numerical models of neutrino and gamma-ray emission from magnetic
  reconnection in the core of radio-galaxies"
135,1105656194557865985,3433220662,Anthony Bonato,"['Latest paper with PhD candidate Erin Meger + post-docs on a new model for social networks. From the abstract: ""We propose the Iterated Local Model (ILM) for social networks synthesizing both transitive and anti-transitive triads over time.""\n\n<LINK>']",https://arxiv.org/abs/1903.04523,"On-line social networks, such as in Facebook and Twitter, are often studied from the perspective of friendship ties between agents in the network. Adversarial ties, however, also play an important role in the structure and function of social networks, but are often hidden. Underlying generative mechanisms of social networks are predicted by structural balance theory, which postulates that triads of agents, prefer to be transitive, where friends of friends are more likely friends, or anti-transitive, where adversaries of adversaries become friends. The previously proposed Iterated Local Transitivity (ILT) and Iterated Local Anti-Transitivity (ILAT) models incorporated transitivity and anti-transitivity, respectively, as evolutionary mechanisms. These models resulted in graphs with many observable properties of social networks, such as low diameter, high clustering, and densification. We propose a new, generative model, referred to as the Iterated Local Model (ILM) for social networks synthesizing both transitive and anti-transitive triads over time. In ILM, we are given a countably infinite binary sequence as input, and that sequence determines whether we apply a transitive or an anti-transitive step. The resulting model exhibits many properties of complex networks observed in the ILT and ILAT models. In particular, for any input binary sequence, we show that asymptotically the model generates finite graphs that densify, have clustering coefficient bounded away from 0, have diameter at most 3, and exhibit bad spectral expansion. We also give a thorough analysis of the chromatic number, domination number, Hamiltonicity, and isomorphism types of induced subgraphs of ILM graphs. ",The Iterated Local Model for Social Networks
136,1105417954982797313,743028716457070592,Franco Vazza,"[""Small thread on our <LINK> (submitted) today on the archive. It's a numerical study using synthetic X-ray and radio observations to investigate whether we can study the tip of the iceberg of the cosmic web simultaneously with X-ra and radio observations. #magcow <LINK>"", 'First, the radio emission (from shock-accelerated electrons) is in general expected to probe larger distances from the centre of structures (say galaxy clusters), because the synchrotron power does not scale with n^2sqrt(T) as X-ray emission. https://t.co/WkSemF6rpf', 'However, the level of the radio emission depends on two big unknowns a) electron acceleration effiicency by M&gt;10 shocks b) magnetic fields in filaments/cluster outskirts. If the first is &gt;1e-5 and B&gt;10nG, detecting the ""tip of the iceberg"" of the cosmic web is possible in radio.', 'It should definitely be possible with @SKA_telescope , but also with @LOFAR and MWA to some extent. Now the question is: what would deep X-ray exposure detect in the same regions? So simulate several surveys of our sky model in radio and X-rays.', 'Outside of R100, the regions that can be detected with a SKA-LOW survey (~10hours) or with an hypothetic (practically impossible!) 1Ms full-sky survey in X-ray using @AthenaXobs are really a few. What do they have in common? https://t.co/Vrbcz8NqWh', '(We already preliminary explored this issue in https://t.co/RIlIGFXzGv   #AthenaNuggets 22 ) https://t.co/ASPO5fQt9t', 'Based on our mock ""surveys"", the radio will see several filaments (beside clusters) while in X-rays the vast majority of clusters will be detected (of course), but basically no filaments. In the region detectable by both,there are a few ""extreme"" cluster outskirst (&gt;R100) though. https://t.co/8Tws76r1OG', 'The ""double detectable"" patches are in overdense regions, with many clusters around. As discussed in the paper, such clusters can have any dynamical state or mass,but for the chance of a double detection to appear,they must be in an early merger stage, (the two R100 don\'t cross). https://t.co/88qFLhmDaW', 'The ""intracluster bridges"" between merging clusters are NOT usual filaments, because filaments are much larger than them. They used however to contain warm-hot-intergalactic medium, but the ongoing interaction is compressing (and thus heating) them. It\'s a sort of transient WHIM.', 'So we explored in detail which chances will eRosita, @ESA_XMM and @AthenaXobs , and @SKA_telescope , @LOFAR and MWA have to detect such bridges, with long targeted exposures. https://t.co/rG8UyPKwqx', '(let me stress we mostly tested survey configurations, but of course most instruments can increase a bit their performance with ad-hoc adjustments for single observations)', 'The statistics over ~50 of such bridges suggest that with 100ks or better 1Ms integrations, @AthenaXobs will detect 10-30% of their extent, outside of R100. Radio telescopes can do even better (~20-30%).', 'XMM and eRosita (due to smaller collecting areas) can only approach detections for &gt;100ks exposures. eRosita however should do much shallower surveys (1ks full sky and 20ks on the poles).The best energy range in all cases in 0.8-1.2keV, where many calibration lines are available.', 'Bonus: thanks to M.Roncarelli, we also simulated a 1Ms integration with @AthenaXIFU on one of such bridges, to see whether a spectroscopic analysis will be possible on a nearby (z~0.1) cluster. https://t.co/ZqkJyYWGib', ""The simulation was done using the SIXTE package, which simulates realistic photon counts for XIFU, including instrumental and sky background. We integrated for the entire 5x5' of XIFU here, and assumed a fixed 0.2 solar metallicity. https://t.co/AlmlQdQ0xK"", 'The spectral fitting works remarkably well (even outside of R100 in this particular object!) and the reconstructed parameters are quite close the input one from the 3D simulation. Interestingly, @AthenaXIFU should constrain rather well even the velocity dispersion on the bridge. https://t.co/7tNu7OZnH6', '...To the point that we can may even reconstruct the shock Mach number (very important to model the particle acceleration and the radio emission) only based on X-ray spectrosopic data. https://t.co/aR9u5jNW5H', 'Some caveats: we used ""simple"" (although very big! 2400^3) cosmological non-radiative simulations, with a fixed metallicity. It should not matter really for what we studied, though.', 'Final thought: the radio detectable sky will be able to probe much into the bulk fo ""missing baryons"" (albeit with a volume-filing fraction to be calibrated) than what X-ray will ever do! https://t.co/gbxfG3q83G', '[end of thread. If the referee is reading, at this point I think I can count on a smooth acceptance, right?]']",https://arxiv.org/abs/1903.04166,"Detecting the thermal and non-thermal emission from the shocked cosmic gas surrounding large-scale structures represents a challenge for observations, as well as a unique window into the physics of the warm-hot intergalactic medium. In this work, we present synthetic radio and X-ray surveys of large cosmological simulations in order to assess the chances of jointly detecting the cosmic web in both frequency ranges. We then propose best observing strategies tailored for existing (LOFAR, MWA and XMM) or future instruments (SKA-LOW and SKA-MID, ATHENA and eROSITA). We find that the most promising targets are the extreme peripheries of galaxy clusters in an early merging stage, where the merger causes the fast compression of warm-hot gas onto the virial region. By taking advantage of a detection in the radio band, future deep X-ray observations will probe this gas in emission, and help us to study plasma conditions in the dynamic warm-hot intergalactic medium with unprecedented detail. ",Detecting shocked intergalactic gas with X-ray and radio observations
137,1105379221126504449,796296603963424768,Fabien Ferrage,"['You may have already recorded a methyl TROSY spectrum on your high-field #NMR. We have too on the two-field spectrometer and it works at 0.33 T/14 MHz! Why? Find out in our latest preprint: <LINK>', 'Also, you can find in our study what to do on your future 2+ GHz system (or what to write in your grant proposal)']",https://arxiv.org/abs/1903.04452,"The use of relaxation interference in the methyl Transverse Relaxation-Optimized SpectroscopY (TROSY) experiment has opened new avenues for the study of large proteins and protein assemblies in nuclear magnetic resonance. So far, the theoretical description of the methyl-TROSY experiment has been limited to the slow-tumbling approximation, which is correct for large proteins on high field spectrometers. In a recent paper, favorable relaxation interference was observed in the methyl groups of a small protein at a magnetic field as low as 0.33 T, well outside the slow-tumbling regime. Here, we present a model to describe relaxation interference in methyl groups over a broad range of magnetic fields, not limited to the slow-tumbling regime. We predict that the type of multiple-quantum transitions that show favorable relaxation properties change with the magnetic field. Under the condition of fast methyl-group rotation, methyl-TROSY experiments can be recorded over the entire range of magnetic fields from a fraction of 1 T up to 100 T. ","Understanding the Methyl-TROSY effect over a wide range of magnetic
  fields"
138,1103237661899309057,736190574328467457,Aline Vidotto,"['We modelled the stellar wind of HD189733, calculated the radio emission arising from the interaction between stellar wind &amp; exoplanet HD189733b and, finally, if the planetary radio emission can propagate through the stellar wind. Can it? To find out: <LINK> <LINK>']",https://arxiv.org/abs/1903.01809,"We present stellar wind modelling of the hot Jupiter host HD189733, and predict radio emission from the stellar wind and the planet, the latter arising from the interaction of the stellar wind with the planetary magnetosphere. Our stellar wind models incorporate surface stellar magnetic field maps at the epochs Jun/Jul 2013, Sep 2014, and Jul 2015 as boundary conditions. We find that the mass-loss rate, angular momentum-loss rate, and open magnetic flux of HD189733 vary by 9%, 40%, and 19% over these three epochs. Solving the equations of radiative transfer, we find that from 10 MHz-100 GHz the stellar wind emits fluxes in the range of $10^{-3}$-$5$ $\mu$Jy, and becomes optically thin above 10 GHz. Our planetary radio emission model uses the radiometric Bode's law, and neglects the presence of a planetary atmosphere. For assumed planetary magnetic fields of 1-10 G, we estimate that the planet emits at frequencies of 2-25 MHz, with peak flux densities of $\sim10^2$ mJy. We find that the planet orbits through regions of the stellar wind that are optically thick to the emitted frequency from the planet. As a result, unattenuated planetary radio emission can only propagate out of the system and reach the observer for 67% of the orbit for a 10 G planetary field, corresponding to when the planet is approaching and leaving primary transit. We also find that the plasma frequency of the stellar wind is too high to allow propagation of the planetary radio emission below 21 MHz. This means a planetary field of at least 8 G is required to produce detectable radio emission. ",MOVES II. Tuning in to the radio environment of HD189733b
139,1102557184024555520,1094635928419749888,Yash Sharma,"['We study the recently shown effectiveness of low frequency perturbations, and our results suggest that explicitly considering perceptual priors can improve robustness. Hope to see more analyses in this direction!\n\nLink: <LINK> <LINK>']",https://arxiv.org/abs/1903.00073,"Carefully crafted, often imperceptible, adversarial perturbations have been shown to cause state-of-the-art models to yield extremely inaccurate outputs, rendering them unsuitable for safety-critical application domains. In addition, recent work has shown that constraining the attack space to a low frequency regime is particularly effective. Yet, it remains unclear whether this is due to generally constraining the attack search space or specifically removing high frequency components from consideration. By systematically controlling the frequency components of the perturbation, evaluating against the top-placing defense submissions in the NeurIPS 2017 competition, we empirically show that performance improvements in both the white-box and black-box transfer settings are yielded only when low frequency components are preserved. In fact, the defended models based on adversarial training are roughly as vulnerable to low frequency perturbations as undefended models, suggesting that the purported robustness of state-of-the-art ImageNet defenses is reliant upon adversarial perturbations being high frequency in nature. We do find that under $\ell_\infty$ $\epsilon=16/255$, the competition distortion bound, low frequency perturbations are indeed perceptible. This questions the use of the $\ell_\infty$-norm, in particular, as a distortion metric, and, in turn, suggests that explicitly considering the frequency space is promising for learning robust models which better align with human perception. ",On the Effectiveness of Low Frequency Perturbations
140,1108720422152032256,743028716457070592,Franco Vazza,"['""A survey of the thermal and non-thermal properties of cosmic filaments"" by C.Gheller (@EPFL ) and myself <LINK>  We studied the statistical properties of filaments contained  by a 85Mpc volume simulated with ENZO,comparing 11 physical resimulations. <LINK>', 'The main goal is to isolate which properties of filaments are not affected by feedback from galaxies (like temperature and morphology) and which are (like magnetic fields). Will future observations of gas or B-fields in filaments tell us something crucial about their evolution?', ""first, thermal properties: we find overall small differences. Away from galaxies, the energetics is always dominated by the filament's gravity, hence T and density structures are mostly the same always. https://t.co/gy0421ZpSk"", 'There is a non-negligible effect on the enclosed baryon fraction, however: it can be lowered below the cosmic one by feedback (if it was strong in the past). Therefore, cosmic filaments do not necessarily retain the cosmic baryon fraction, kind of interesting. https://t.co/AZDPnumPUd', 'Magnetic fields should instead hugely vary for different scenarios of magnetogenesis. According to this (and prev. runs at higher res) the dynamo should be inefficient there, hence what the determines the B-field at z=0 is mostly the seeding mechanism(""galaxies"" vs ""primordial""). https://t.co/pDAPfwhpxn', 'For a filament with a length of ~10Mpc, the average B-field in the intergalactic medium can well go from B~50nG to B~0.01nG, in our models, which huge differences in expected Faraday Rotation and synchrotron emission.', 'Therefore, (very challenging) detection of filaments in the radio band can really kill many models, if they can tell us the magnetisation of the diffuse IGM there. Go @LOFAR , @SKA_telescope @mwatelescope ASKAP and @VLArray ! https://t.co/zrZijqv7Nm', 'Last:we also analysed the properties of ""galaxies""embedded in filaments, and tried to relate them with the properties of the IGM around.That\'s admittedly more unsure, because at our resolution, galaxies are actually ""galaxy blobs"" for which we can only estimate global properties. https://t.co/nJlJefggvY', 'Interestingly, there are significant (but scattered) scalings between the properties of  halos, those of the surrounding IGM, and that of the entire filament containing both (which in most cases highlight the fact that gravity is affecting all of them - except magentic fields). https://t.co/YeBRB8nUE1', '[end of thread]. Maybe, one day in the far, far future, we will be able to see the entire ""radio"" cosmic web thanks to very powerful radio telescopes (like @SKA_telescope in Phase 2) https://t.co/UlCdastv2a']",https://arxiv.org/abs/1903.08401,"In this paper, we exploit a large suite of {\enzo} cosmological magneto-hydrodynamical simulations adopting uniform mesh resolution, to investigate the properties of cosmic filaments under different baryonic physics and magnetogenesis scenarios. We exploit a isovolume based algorithm to identify filaments and determine their attributes from the continuous distribution of gas mass density in the simulated volumes. The global (e.g. mass, size, mean temperature and magnetic field strength, enclosed baryon fraction) and internal (e.g. density, temperature, velocity and magnetic field profiles) properties of filaments in our volume are calculated across almost four orders of magnitude in mass. The inclusion of variations in non-gravitational physical processes (radiative cooling, star formation, feedback from star forming regions and active galactic nuclei) as well as in the seeding scenarios for magnetic fields (early magnetisation by primordial process vs later seeding by galaxies) allows us to study both the large-scale thermodynamics and the magnetic properties of the Warm-Hot Intergalactic Medium (WHIM) with an unprecedented detail. We show how the impact of non-gravitational physics on the global thermodynamical properties of filaments is modest, with the exception of the densest gas environment surrounding galaxies in filaments. Conversely, the magnetic properties of the WHIM in filament are found to dramatically vary as different seeding scenarios are considered. We study the correlation between the properties of galaxy-sized halos and their host filaments, as well as between the halos and the local WHIM in which they lie. Significant general statistical trends are reported. ",A survey of the thermal and non-thermal properties of cosmic filaments
141,1103233271993978880,597624973,Petar Popovski,"['Distributed Ledger Technology (#DLT) and #blockchain will change the way we understand and optimize #IoT devices. Here is our latest study on ""Communication Aspects of the Integration of Wireless IoT Devices with Distributed Ledger Technology""\n\n<LINK>']",https://arxiv.org/abs/1903.01758,"The pervasive need to safely share and store information between devices calls for the replacement of centralized trust architectures with the decentralized ones. Distributed Ledger Technologies (DLTs) are seen as the most promising enabler of decentralized trust, but they still lack technological maturity and their successful adoption depends on the understanding of the fundamental design trade-offs and their reflection in the actual technical design. This work focuses on the challenges and potential solutions for an effective integration of DLTs in the context of Internet-of-Things (IoT). We first introduce the landscape of IoT applications and discuss the limitations and opportunities offered by DLTs. Then, we review the technical challenges encountered in the integration of resource-constrained devices with distributed trust networks. We describe the common traits of lightweight synchronization protocols, and propose a novel classification, rooted in the IoT perspective. We identify the need of receiving ledger information at the endpoint devices, implying a two-way data exchange that contrasts with the conventional uplink-oriented communication technologies intended for IoT systems. ","Communication Aspects of the Integration of Wireless IoT Devices with
  Distributed Ledger Technology"
