,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1359665011484860418,33113669,Tim Baldwin,"['new paper to appear #eacl2021 with the outrageously-talented Xudong Han and @trevorcohn on adversarial training -- single discriminators unreliable to train; ensembles can help, but high variance ... fix = add orthogonality constraint <LINK>']",https://arxiv.org/abs/2101.10001,"Adversarial learning can learn fairer and less biased models of language than standard methods. However, current adversarial techniques only partially mitigate model bias, added to which their training procedures are often unstable. In this paper, we propose a novel approach to adversarial learning based on the use of multiple diverse discriminators, whereby discriminators are encouraged to learn orthogonal hidden representations from one another. Experimental results show that our method substantially improves over standard adversarial removal methods, in terms of reducing bias and the stability of training. ",Diverse Adversaries for Mitigating Bias in Training
1,1357666238131023872,326864247,Amanda Clare,"[""In @jamesravey's new paper (accepted to #EACL2021  <LINK>) we look at how to find words/phrases/entities that refer to the same thing in two different documents. For example, a news article that reports on findings from a published science paper."", 'This is a hard problem, for humans and for automation. We scratched our heads many times while hand-annotating the corpus (available from https://t.co/hXKytYqsmS). News language is different to science paper language, and terms have subtle but important semantics.', ""Vector space embeddings that allow semantically related entities to be neighbours, don't provide good enough separation to distinguish whether they're co-referent or not. But we'll need this to determine how science is reported in the news: which parts and how it's slanted."", ""This work was done with @xrysoflhs @ArieCattan and Ido Dagan and you can find more in @jamesravey's thread here:  https://t.co/OyfjmhyXIs""]",https://arxiv.org/abs/2101.12637,"Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD$^2$CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD$^2$CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources. ",CD2CR: Co-reference Resolution Across Documents and Domains
2,1357442374239023104,14754639,Kai Lukoff,"['Ever get lost on YouTube? We have a new CHI 2021 paper just for you: “How the Design of YouTube Influences User Sense of Agency” <LINK> Co-authored w/ @ulyngs @himanshuzade Vera Liao, James Choi, Kaiyue Fan, @smunson &amp; Alexis Hiniker [1/5]', 'We identify which design mechanisms users say affect their sense of control over the time they spend in the YouTube mobile app. Less control: recommendations, ads, autoplay. More control: Playlists, search, subscriptions, play controls, watch history [2/5] https://t.co/Ltps7vbcT4', 'The mechanism called out most often is recommendations. YouTube is wickedly good at the local optimization problem: Out of millions of videos, which one is the user most likely to watch? But the user lacks the ability to align these video recs w/ their long-term goals. [3/5]', 'On the flipside, a design idea to support greater control: microplaning, i.e., making a lightweight plan to guide behavior for a short time. For example, encourage the user to create a short video playlist for their current session of use. [4/5]', 'Our paper builds on fantastic earlier work by @UichinLee @EricPSB @gezakovacs @youngho_yhkim @gratydesign @AnnaCox_ @jcccf @elena_agapie and many others! [5/5]']",https://arxiv.org/abs/2101.11778,"In the attention economy, video apps employ design mechanisms like autoplay that exploit psychological vulnerabilities to maximize watch time. Consequently, many people feel a lack of agency over their app use, which is linked to negative life effects such as loss of sleep. Prior design research has innovated external mechanisms that police multiple apps, such as lockout timers. In this work, we shift the focus to how the internal mechanisms of an app can support user agency, taking the popular YouTube mobile app as a test case. From a survey of 120 U.S. users, we find that autoplay and recommendations primarily undermine sense of agency, while search and playlists support it. From 13 co-design sessions, we find that when users have a specific intention for how they want to use YouTube they prefer interfaces that support greater agency. We discuss implications for how designers can help users reclaim a sense of agency over their media use. ",How the Design of YouTube Influences User Sense of Agency
3,1357342173948043277,325448885,Michael Merrifield,"['A new @sixtysymbols video on our latest paper (<LINK>).  This neat little result reminded me why I enjoy doing astronomy research so much! <LINK> <LINK>', '@NonZeroCurl @BradyHaran @sixtysymbols It depends! More usually “ln” for base e. And more likely to be a natural log for theoretical work and base 10 for observations.', '@zkzkz @BradyHaran @sixtysymbols That’s a whole other video! I think it’s because they form their stars more slowly and steadily than higher mass galaxies, giving more time for everything to mix thoroughly as the closed box requires.']",http://arxiv.org/abs/2101.11022,"The levels of heavy elements in stars are the product of enhancement by previous stellar generations, and the distribution of this metallicity among the population contains clues to the process by which a galaxy formed. Most famously, the ""G-dwarf problem"" highlighted the small number of low-metallicity G-dwarf stars in the Milky Way, which is inconsistent with the simplest picture of a galaxy formed from a ""closed box"" of gas. It can be resolved by treating the Galaxy as an open system that accretes gas throughout its life. This observation has classically only been made in the Milky Way, but the availability of high-quality spectral data from SDSS-IV MaNGA and the development of new analysis techniques mean that we can now make equivalent measurements for a large sample of spiral galaxies. Our analysis shows that high-mass spirals generically show a similar deficit of low-metallicity stars, implying that the Milky Way's history of gas accretion is common. By contrast, low-mass spirals show little sign of a G-dwarf problem, presenting the metallicity distribution that would be expected if such systems evolved as pretty much closed boxes. This distinction can be understood from the differing timescales for star formation in galaxies of differing masses. ","SDSS-IV MaNGA: the ""G-dwarf problem"" revisited"
4,1356957415254740994,1250439142997016578,Juan P. Dominguez-Morales,"['Our new paper, titled ""Neuromorphic adaptive spiking CPG towards bio-inspired locomotion of legged robots"" is now available in arXiv!  - <LINK>']",https://arxiv.org/abs/2101.09709,"In recent years, locomotion mechanisms exhibited by vertebrate animals have been the inspiration for the improvement in the performance of robotic systems. These mechanisms include the adaptability of their locomotion to any change registered in the environment through their biological sensors. In this regard, we aim to replicate such kind of adaptability in legged robots through a Spiking Central Pattern Generator. This Spiking Central Pattern Generator generates different locomotion (rhythmic) patterns which are driven by an external stimulus, that is, the output of a Force Sensitive Resistor connected to the robot to provide feedback. The Spiking Central Pattern Generator consists of a network of five populations of Leaky Integrate-and-Fire neurons designed with a specific topology in such a way that the rhythmic patterns can be generated and driven by the aforementioned external stimulus. Therefore, the locomotion of the end robotic platform (any-legged robot) can be adapted to the terrain by using any sensor as input. The Spiking Central Pattern Generator with adaptive learning has been numerically validated at software and hardware level, using the Brian 2 simulator and the SpiNNaker neuromorphic platform for the latest. In particular, our experiments clearly show an adaptation in the oscillation frequencies between the spikes produced in the populations of the Spiking Central Pattern Generator while the input stimulus varies. To validate the robustness and adaptability of the Spiking Central Pattern Generator, we have performed several tests by variating the output of the sensor. These experiments were carried out in Brian 2 and SpiNNaker; both implementations showed a similar behavior with a Pearson correlation coefficient of 0.905. ","Neuromorphic adaptive spiking CPG towards bio-inspired locomotion of
  legged robots"
5,1356625188943781891,2577741,Saeed Hassanpour,"['MHIST is a new publicly available binary classification dataset of 3,152 fixed-size images of colorectal polyps with gold-standard labels and annotator agreement levels from 7 expert pathologists per image.\n\nPaper: <LINK>\nData: <LINK>\n\n(1/4) <LINK>', 'This minimalist dataset is &lt;400MB and can serve as a petri dish to test new ideas for histopathology image analysis and make the field more accessible to the general ML community. Standard baseline performance metrics for MHIST are provided in the accompanying paper.\n\n(2/4)', 'MHIST classification task focuses on clinically important binary distinction between typically benign Hyperplastic Polyps (HPs) and high-risk Sessile Serrated Adenomas (SSAs), a challenging problem with considerable inter-pathologist variability.\n\n(3/4)', 'Special thanks to Jerry Wei, Naofumi Tomita, @_jasonwei, Lorenzo Torresani, @samgreydanus, @mattlungrenMD, Arief Suriawinata, and all 7 pathologist annotators at @DartmouthHitch Pathology &amp; Laboratory Medicine.\n\n(4/4)']",https://arxiv.org/abs/2101.12355,"With the rise of deep learning, there has been increased interest in using neural networks for histopathology image analysis, a field that investigates the properties of biopsy or resected specimens traditionally manually examined under a microscope by pathologists. However, challenges such as limited data, costly annotation, and processing high-resolution and variable-size images make it difficult to quickly iterate over model designs. Throughout scientific history, many significant research directions have leveraged small-scale experimental setups as petri dishes to efficiently evaluate exploratory ideas. In this paper, we introduce a minimalist histopathology image analysis dataset (MHIST), an analogous petri dish for histopathology image analysis. MHIST is a binary classification dataset of 3,152 fixed-size images of colorectal polyps, each with a gold-standard label determined by the majority vote of seven board-certified gastrointestinal pathologists and annotator agreement level. MHIST occupies less than 400 MB of disk space, and a ResNet-18 baseline can be trained to convergence on MHIST in just 6 minutes using 3.5 GB of memory on a NVIDIA RTX 3090. As example use cases, we use MHIST to study natural questions such as how dataset size, network depth, transfer learning, and high-disagreement examples affect model performance. By introducing MHIST, we hope to not only help facilitate the work of current histopathology imaging researchers, but also make the field more-accessible to the general community. Our dataset is available at this https URL ",A Petri Dish for Histopathology Image Analysis
6,1356372763909496832,36262769,Laurent Lessard,"[""I'm excited to share our new preprint on robust stabilization:\n<LINK>\nThis is joint work with my Ph.D. student Saman Cyrus, who just recently defended his thesis.\n\nHere is a summary of the paper. 👇"", ""Robust stabilization dates back 70+ years to seminal works of Lur'e, Zames, and Willems. The basic idea: a known system G is in feedback with an unknown nonlinearity Phi constrained to lie in a known set. We seek conditions on G that ensure stability of the interconnection. 1/ https://t.co/vldNmRKui5"", 'In this work, we looked at the case where the nonlinearity is constrained to lie in a cone (also known as ""sector-bounded""). Results of this type include: small-gain, circle, passivity, and extended conicity theorems. 2/', 'Robust stabilization results are typically formulated in the extended space L2e and can be expressed in a variety of ways, depending how which assumptions are made (causality, stability, linearity, time-invariance, etc.). Most of these results are sufficient conditions... 3/', '...while some others are ""necessary and sufficient"". Necessity can also mean two different things; when the conditions fail, can we construct worst-case signals only? Or can we also construct worst-case nonlinearities? 4/', 'Our aim in this work was to disentangle the different formulations of these results and distill them into a single unified result that makes as few assumptions as possible. We worked with relations in an arbitrary semi-inner product space, which allowed us to... 5/', '...avoid notions of ""causality"", ""stability"", ""time-invariance"", and ""well-posedness"". In fact, we don\'t even assume a notion of ""time""! In this setting, we prove a very general necessary and sufficient condition for robust boundedness that requires essentially no assumptions. 6/ https://t.co/AjE5DAN6aS', 'When our condition fails to hold, we show how to explicitly construct worst-case signals, and an associated worst-case nonlinearity. Also, our construction produces a nonlinearity that happens to be linear, which is nice!\n\nSpecializing our result to causal operators on L2e... 7/', '... we find that:\n\n1. When our condition fails, we can still construct worst-case signals. This works even when the plant G is nonlinear. \n\n2. Constructing a worst-case nonlinearity does not work in the L2e case because it typically produces a non-causal nonlinearity. 8/', '3. By making additional assumptions, namely that G is LTI, we can construct worst-case nonlinearities that are causal. Our construction produces an LTI nonlinearity (in fact, a pure delay). Under these assumptions, we also give frequency-domain and LMI flavors of our result. 9/', 'Our work unifies existing results from literature:\n\n1. Existing necessary-and-sufficient results are special cases of our result.\n\n2. Existing sufficient-only results follow directly from our result via appropriate relaxations. We give examples in the text of how this works. 10/', ""Although our results are restricted to conicity-like constraints, it may be possible to extend our work to nonlinearities characterized by dynamic constraints (multipliers, dissipativity, IQCs), but that's future work!\n\nThanks for reading! 11/11."", ""@ian__manchester Thanks for the pointer -- we'll have a look!""]",https://arxiv.org/abs/2101.01900,"Classical conditions for ensuring the robust stability of a system in feedback with a nonlinearity include passivity, small gain, circle, and conicity theorems. We present a generalized and unified version of these results in an arbitrary semi-inner product space, which avoids many of the technicalities that arise when working in traditional extended spaces. Our general formulation clarifies when the sufficient conditions for robust stability are also necessary, and we show how to construct worst-case scenarios when the sufficient conditions fail to hold. Finally, we show how our general result can be specialized to recover a wide variety of existing results, and explain how properties such as boundedness, causality, linearity, and time-invariance emerge as a natural consequence. ","Generalized Necessary and Sufficient Robust Boundedness Results for
  Feedback Systems"
7,1356289525048373248,36197539,Federico Brivio,['New paper available as preprint on #VS4 #DFT\n<LINK>'],https://arxiv.org/abs/2101.12658,"Quasi one-dimensional (1D) vanadium tetrasulfide ($VS_4$) nanowires (NWs) are synthetic semiconductors which combine with each other through Van der Waals interactions to form bulk phases. However, the properties of these individual nanowires remain unknown. Nevertheless, our calculations of their stability indicate that $VS_4$) NWs can be separated from their bulk structures. Accordingly, we theoretically investigated the geometrical, electronic, and magnetic properties of bulk phase and isolated $VS_4$ NWs. Our results indicate that both bulk phase and isolated $VS_4$ NWs are semiconductors with band gaps of 2.24 and 2.64 eV, respectively, and that they prefer the antiferromagnetic (AFM) ground state based on DFT calculations. These calculations also suggested that isolated $VS_4$ NWs show half-metallic antiferromagnetism upon electron and hole doping because carrier doping splits the spin degeneracy to induce local spin polarisation. As a result, spin polarisation currents in isolated $VS_4$ NWs can be manipulated with locally applied gate voltage. Therefore, these 1D AFM materials have a high potential for advancing both fundamental research and spintronic applications because they are more resistant to magnetic perturbation than their 1D ferromagnetic counterparts. ","Doping isolated one-dimensional antiferro-magnetic semiconductor
  Vanadium tetrasulfide ($VS_4$) nanowires with carriers induces
  half-metallicity"
8,1356269604612567046,3433220662,Anthony Bonato,"['New paper up on arXiv. We study a new model for complex hypernetworks and explore its properties related to motifs and clustering. Along the way, we introduce a new clustering coefficient for hypergraphs...there are many competing definitions!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2101.12560,"Complex networks are pervasive in the real world, capturing dyadic interactions between pairs of vertices, and a large corpus has emerged on their mining and modeling. However, many phenomena are comprised of polyadic interactions between more than two vertices. Such complex hypergraphs range from emails among groups of individuals, scholarly collaboration, or joint interactions of proteins in living cells. A key generative principle within social and other complex networks is transitivity, where friends of friends are more likely friends. The previously proposed Iterated Local Transitivity (ILT) model incorporated transitivity as an evolutionary mechanism. The ILT model provably satisfies many observed properties of social networks, such as densification, low average distances, and high clustering coefficients. We propose a new, generative model for complex hypergraphs based on transitivity, called the Iterated Local Transitivity Hypergraph (or ILTH) model. In ILTH, we iteratively apply the principle of transitivity to form new hypergraphs. The resulting model generates hypergraphs simulating properties observed in real-world complex hypergraphs, such as densification and low average distances. We consider properties unique to hypergraphs not captured by their 2-section. We show that certain motifs, which are specified subhypergraphs of small order, have faster growth rates in ILTH hypergraphs than in random hypergraphs with the same order and expected average degree. We show that the graphs admitting a homomorphism into the 2-section of the initial hypergraph appear as induced subgraphs in the 2-section of ILTH hypergraphs. We consider new and existing hypergraph clustering coefficients, and show that these coefficients have larger values in ILTH hypergraphs than in comparable random hypergraphs. ",The iterated local transitivity model for hypergraphs
9,1356255004416487429,2302304521,Dr Andra Stroe 🏳️‍🌈🇷🇴,"['I am very happy to share my new paper with @victor_savu, introducing GLEAM (Galaxy Line Emission &amp; Absorption Modeling), a Python tool we wrote for fitting Gaussian models to emission and absorption lines in large samples of 1D extragalactic spectra. <LINK> 1/3', 'With GLEAM, you can uniformly process a variety of spectra, including galaxies and AGN, in a wide range of instrument setups and signal-to-noise regimes. With a goal to enable reproducible workflows for users, we designed GLEAM to be easy to install and use. 2/3', 'Check out the paper on ArXiv and contribute to our open-source project on Github! https://t.co/OK2XT9T6gO 3/3', ""Also, this is @victor_savu's first refereed paper! I am very thankful to @victor_savu for the expertise on all things code development. Software developer turned astronomer? 4/3"", '@franco_vazza @victor_savu I worked through a number of possible acronyms and this one just fit the best. I do love your LOFAR acronym!', '@MarculewiczM @victor_savu Really glad to hear that you found it useful! Keep in touch if you have any questions or feedback!']",https://arxiv.org/abs/2101.12231,"We present GLEAM (Galaxy Line Emission & Absorption Modeling), a Python tool for fitting Gaussian models to emission and absorption lines in large samples of 1D extragalactic spectra. GLEAM is tailored to work well in batch mode without much human interaction. With GLEAM, users can uniformly process a variety of spectra, including galaxies and active galactic nuclei, in a wide range of instrument setups and signal-to-noise regimes. GLEAM also takes advantage of multiprocessing capabilities to process spectra in parallel. With the goal of enabling reproducible workflows for its users, GLEAM employs a small number of input files, including a central, user-friendly configuration in which fitting constraints can be defined for groups of spectra and overrides can be specified for edge cases. For each spectrum, GLEAM produces a table containing measurements and error bars for the detected spectral lines and continuum, and upper limits for non-detections. For visual inspection and publishing, GLEAM can also produce plots of the data with fitted lines overlaid. In the present paper, we describe GLEAM's main features, the necessary inputs, expected outputs, and some example applications, including thorough tests on a large sample of optical/infra-red multi-object spectroscopic observations and integral field spectroscopic data. gleam is developed as an open-source project hosted at this https URL and welcomes community contributions. ",GLEAM: Galaxy Line Emission & Absorption Modeling
10,1356248135127937025,187447619,Alessandro,"['New paper out there! Parsimonious latent variables model applied to milk quality and authenticity.\nJoint work with @tbmurphy and @OTomoc.\nLink: <LINK>\n@VistaMilk', 'But I guess someone is not happy to not have made it to the authors list https://t.co/zFCp358GtW', ""@michael_fop @tbmurphy I'm feeling guilty now. I should track those cows and ask their names. At least to add them in the aknowledgments!""]",https://arxiv.org/abs/2101.12499,"In recent years animal diet has been receiving increased attention, in particular examining the impact of pasture-based feeding strategies on the quality of milk and dairy products, in line with the increased prevalence of grass-fed dairy products appearing on market shelves. To date, there are limited testing methods available for the verification of grass-fed dairy therefore these products are susceptible to food fraud and adulteration. Hence statistical tools studying potential differences among milk samples coming from animals on different feeding systems are required, thus providing increased security around the authenticity of the products. Infrared spectroscopy techniques are widely used to collect data on milk samples and to predict milk related traits. While these data are routinely used to predict the composition of the macro components of milk, each spectrum provides a reservoir of unharnessed information about the sample. The interpretation of these data presents a number of challenges due to their high-dimensionality and the relationships amongst the spectral variables. In this work we propose a modification of the standard factor analysis to induce a parsimonious summary of spectroscopic data. The procedure maps the observations into a low-dimensional latent space while simultaneously clustering observed variables. The method indicates possible redundancies in the data and it helps disentangle the complex relationships among the wavelengths. A flexible Bayesian estimation procedure is proposed for model fitting, providing reasonable values for the number of latent factors and clusters. The method is applied on milk mid-infrared spectroscopy data from dairy cows on different pasture and non-pasture based diets, providing accurate modelling of the data correlation, the clustering of variables and information on differences between milk samples from cows on different diets. ","Parsimonious Bayesian Factor Analysis for modelling latent structures in
  spectroscopy data"
11,1356194180901576704,1339289901146247170,Clémence Fontanive,"['Do you like exoplanets and/or binaries? Check out our new paper with @astro_daniella: we compiled a list of 238 planet hosts in multiple systems out of &gt;900 planetary systems, and investigate the impact of stellar binarity on exoplanets properties.\n<LINK>']",https://arxiv.org/abs/2101.12667,"We present an extensive search in the literature and Gaia DR2 for visual co-moving binary companions to stars hosting exoplanets and brown dwarfs within 200 pc. We found 218 planet hosts out of 938 to be part of multiple-star systems, with 10 newly discovered binaries and 2 new tertiary stellar components. This represents an overall raw multiplicity rate of 23.2$\pm$1.6% for hosts to exoplanets across all spectral types, with multi-planet systems found to have a lower duplicity frequency at the 2.2$\sigma$ level. We found that more massive hosts are more often in binary configurations, and that planet-bearing stars in multiple systems are predominantly the most massive component of stellar binaries. Investigations of multiplicity as a function of planet mass and separation revealed that giant planets with masses >0.1 MJup are more frequently seen in stellar binaries than small sub-Jovian planets with a 3.6$\sigma$ difference, a trend enhanced for the most massive (>7 MJup) short-period (<0.5 AU) planets and brown dwarf companions. Binarity was found to have no significant effect on the demographics of low-mass planets (<0.1 MJup) or warm and cool gas giants (>0.5 AU). While stellar companion mass appears to have no impact on planet properties, binary separation seems to be an important factor in the resulting structure of planetary systems. Stellar companions on separations <1000 AU can play a role in the formation or evolution of massive close-in planets, while planets in wider binaries show similar properties to planets orbiting single stars. Finally, numerous stellar companions on separations <1-3 arcsec likely remain undiscovered to this date. Continuous efforts to complete our knowledge of stellar multiplicity on separations of tens to hundreds of AU are essential to confirm the reported trends and further our understanding of the roles played by multiplicity on exoplanets. ","The Census of Exoplanets in Visual Binaries: population trends from a
  volume-limited Gaia DR2 and literature search"
12,1356057142709477382,3245949691,Rebecca Leane,"['New paper!\n\nCelestial-Body Focused Dark Matter Annihilation Throughout the Galaxy\n<LINK>\n\nw/ Tim Linden (@trlinden), Payel Mukhopadhyay, + Natalia Toro\n\nWe point out a new dark matter annihilation signal that can dominate over standard halo annihilation! Thread:', 'Celestial bodies (stars, planets) are peppered throughout our Galaxy. They wait, gradually capturing more and more dark matter (DM), as they sweep through the Galactic halo. This capture occurs because DM particles scatter with the objects, and become gravitationally bound.', 'As more and more DM builds up in the celestial object, the DM can begin to annihilate. If it annihilates to sufficiently long-lived particles, these particles can decay outside the object, releasing detectable radiation such as gamma rays.', 'We point out that this process, occurring in full celestial body populations, leads to a symphony of radiation emitted from billions of objects. Particularly in DM dense regions, such as the center of our Galaxy, or globular clusters, this signal can be *huge*.', ""It's huge because (i) there are lots of celestial objects in these regions, and (ii) the DM density in these regions is also large."", ""Something really interesting about this signal: it doesn't have the usual morphology or scaling with galactic position, compared to standard halo annihilation."", 'Halo annihilation simply scales with the DM density squared. This new signal, that we call ""celestial-body focused annihilation"", scales instead with the number of celestial bodies in the full population, times one power of the DM density.', 'This is because there is one factor of the DM density for capture, and the multiplicity of celestial bodies emitting this signal is of course proportional to the number in the population.', 'So, which celestial bodies do we want to use for this signal? We did some sleuthing, and we identified two ideal targets: neutron stars and brown dwarfs.', 'Neutron stars are the smallest type of star, but are ultra-dense. This means that they are sensitive to the smallest dark matter scattering cross sections. They are a ball of (predominantly) neutrons.', 'Brown dwarfs are excellent as they are also very dense, but they also have the benefit of having *very* large radii, which means they have more surface area available to capture dark matter at a higher rate than neutron stars. They are a ball of (predominantly) protons.', ""So, both excellent targets. How do they compare to the standard DM halo annihilation? Let's have a look, first considering the Galactic center. BD-Focused are the brown dwarfs, NS-focused are the neutron stars:"", 'We show the annihilation rate, for given DM masses, for both the halo and celestial bodies. We see that these signals can absolutely *dominate* the annihilation rate. They can be orders of magnitude larger! https://t.co/F2dcev2q9f', 'The halo rate falls with DM mass (as it scales linearly with DM mass), while the celestial body rates are clearly superior for larger DM masses, with signals flat in DM mass.', 'For the halo rates, we\'ve shown ""s-wave"" and ""p-wave"" annihilation lines. Which ""wave"" type is relevant will depend on the particle physics model, but you can in a simple sense think of these as the ""largest type of rate"", and ""next largest type of rate"".', 'The size of our signal is great news particularly for suppressed DM annihilation -- e.g. p-wave halo rates are hard to probe with current-day measurements. P-wave annihilation can proceed in celestial bodies, therefore making our setup a superior probe for a range of DM masses.', 'Note that the left and right plots are for values of gamma=1, 1.5, corresponding to the dark matter density profile; here we\'ve assumed something called a ""generalized NFW profile"" with either of these indices, to show a range of results.', 'This effectively corresponds to a ""standard"" sharp profile towards the center (gamma=1), or an extra sharp profile (gamma=1.5), which is about as large as you\'d want to push this profile. So, probably something in between is most reasonable.', 'So great, can we detect this signal with telescopes? What data do we already have? Well, we have a lot of data for the galactic center, and in fact we already know we have an excess of gamma rays coming from this region (more on that later).', ""Let's compare our predicted signal with this already detected flux of gamma rays, which is measured by the Fermi gamma-ray Space Telescope, and at higher energies with a telescope called H.E.S.S."", 'On the y-axis, we have the maximum signal size we expect, DM mass on the x-axis. The straight lines correspond to the fluxes for brown dwarfs (BD) or neutron stars (NS), with the different profile slope choices as labelled. https://t.co/55y3nxfq4Y', 'We see that the brown dwarfs give a very large signal (remember they have really large radii, so can collect a lot of DM), for both indices. The neutron stars can be seen for the gamma=1.5, but struggle for the galactic center with the gamma=1.', ""So given we are clearly exceeding the measured flux, it's time to set some constraints. Assuming the scattering rate and annihilation rate are in equilibrium, we can use the annihilation flux measured to constrain how large the scattering rate can be."", 'We show the scattering cross section (y-axis), as a function of DM mass (x-axis). Check out that outrageous new sensitivity with brown dwarfs (labelled BD, Fermi)! Empty parameter space? Not anymore. Up to nine orders of magnitude improvement over previous limits! https://t.co/Wk9a4Q7sXG', 'The reason we get such good bounds is (i) direct detection is weak in the sub-GeV region, due to lighter recoils falling below detection thresholds, and', '(ii) the Sun, which has been previously used to set limits on this sort of signal (shown on this plot), evaporates out DM lighter than about 4 GeV. Brown dwarfs on the other hand have cooler cores, and therefore can trap much lighter DM.', 'For neutron stars, we only show gamma=1.5, as the signals are not as powerful relative to the galactic center flux. This is not a conservative choice, however we emphasize that the limit setting method used was very conservative, so the bounds will be within the dashed region.', 'In this plot, we are showing the sensitivities for direct mediator decay to gamma rays. This resulting energy spectrum will give the best possible limits; other final states are possible (e.g. electrons), though other bounds are generally a couple of orders of magnitude weaker.', 'In considering these celestial populations, and the potential sizes of the signals, we also realized something pretty amusing: you can actually potentially explain the galactic center gamma ray excess (GCE) with this setup.', 'The origin of the GCE is an ongoing puzzle, but it is already believed that GCE could potentially be due to DM annihilating in the halo.', ""Even more amusing, our signal doesn't produce the standard morphology of annihilating halo DM; it instead scales in part with the stellar density."", 'This is relevant because some recent work claims that the GCE morphology is not like that of annihilating halo DM, which has been used to suggest that the GCE is potentially not due to DM.', ""We therefore say instead, that a non-DM-density morphology doesn't necessarily mean the GCE is not from DM. Note however that we didn't perform any detailed GCE fits with our celestial bodies, and instead only pointed out the size of this signal can be compatible with the excess."", 'All right, so Galactic center aside, we also briefly discussed the size of our new signal in globular clusters. For this, we only used the neutron star population, as it seems brown dwarfs are likely ejected from the inner region of globular clusters.', 'Using some general estimates for a nearby cluster called Tucanae 47 (Tuc 47), it looks like this signal has solid potential, and can also outperform annihilation with the cluster itself.', 'We noted however that the size of this signal is too small to see with the Fermi Telescope. We pointed out that new globular clusters will be found in the future, some closer by, and they will provide a new (and better) probe of this signal.', 'So, in summary, we pointed out a new type of annihilation signal, powered by collective simultaneous annihilations across billions of neutron stars and brown dwarfs. It assumes that Galactic DM is captured by celestial bodies, and consequently annihilates to long-lived particles.', 'This new signal can be huge, and we therefore set new powerful limits on this scenario!', 'Thanks to my awesome collaborators on this project, Tim, Payel, and Natalia!']",https://arxiv.org/abs/2101.12213,"Indirect detection experiments typically measure the flux of annihilating dark matter (DM) particles propagating freely through galactic halos. We consider a new scenario where celestial bodies ""focus"" DM annihilation events, increasing the efficiency of halo annihilation. In this setup, DM is first captured by celestial bodies, such as neutron stars or brown dwarfs, and then annihilates within them. If DM annihilates to sufficiently long-lived particles, they can escape and subsequently decay into detectable radiation. This produces a distinctive annihilation morphology, which scales as the product of the DM and celestial body densities, rather than as DM density squared. We show that this signal can dominate over the halo annihilation rate in $\gamma$-ray observations in both the Milky Way Galactic center and globular clusters. We use \textit{Fermi} and H.E.S.S. data to constrain the DM-nucleon scattering cross section, setting powerful new limits down to $\sim10^{-39}~$cm$^2$ for sub-GeV DM using brown dwarfs, which is up to nine orders of magnitude stronger than existing limits. We demonstrate that neutron stars can set limits for TeV-scale DM down to about $10^{-47}~$cm$^2$. ",Celestial-Body Focused Dark Matter Annihilation Throughout the Galaxy
13,1355898548169175041,2279164099,Bruno Lepri,"['New paper accepted @FAccTConference. In this work, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. Link: <LINK>', 'Work done with @ElenaBeretta4, @phisaz, and @demartin']",https://arxiv.org/abs/2101.11358,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination. ","Detecting discriminatory risk through data annotation based on Bayesian
  inferences"
14,1355356522293780482,1097081523218599936,Shreya Gupta,['📢 #EACL2021 pre-print Alert! 📢\nOur paper accepted at EACL 2021 is up on ArXiV! Here we\n(1) propose a generalised claim detection model\n(2) release a new Twitter dataset (containing COVID-19 tweets) for claim detection\n\nLink: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2101.11891,"The conceptualization of a claim lies at the core of argument mining. The segregation of claims is complex, owing to the divergence in textual syntax and context across different distributions. Another pressing issue is the unavailability of labeled unstructured text for experimentation. In this paper, we propose LESA, a framework which aims at advancing headfirst into expunging the former issue by assembling a source-independent generalized model that captures syntactic features through part-of-speech and dependency embeddings, as well as contextual features through a fine-tuned language model. We resolve the latter issue by annotating a Twitter dataset which aims at providing a testing ground on a large unstructured dataset. Experimental results show that LESA improves upon the state-of-the-art performance across six benchmark claim datasets by an average of 3 claim-F1 points for in-domain experiments and by 2 claim-F1 points for general-domain experiments. On our dataset too, LESA outperforms existing baselines by 1 claim-F1 point on the in-domain experiments and 2 claim-F1 points on the general-domain experiments. We also release comprehensive data annotation guidelines compiled during the annotation phase (which was missing in the current literature). ","LESA: Linguistic Encapsulation and Semantic Amalgamation Based
  Generalised Claim Detection from Online Content"
15,1355262106396983298,1158593840321662976,Emanuele Bugliarello,"['Can we use syntax to improve generalization in image captioning models? Yes!\nCheck out our (w/ @delliott) new paper:\n\n“The Role of Syntactic Planning in Compositional Image Captioning”\n\n📄 <LINK>\n🗣️ to appear at #EACL2021 <LINK>', 'Inspired by the classic NLG framework, we introduce a planning phase in image captioning models. \n\nWe explore 3 simple planning approaches and 5 granularities of syntactic tags, which consistently lead to better generalization for unseen adjective-noun and noun-verb compositions! https://t.co/f0m2PcMIkw', 'Interleaving POS tags with words is model agnostic: it leads to better generalization in RNN-based and a Transformer-based captioning models.\n\nBut... it hurts the recall of the image-text retrieval module in the SOTA model. We overcome this by learning how to weigh tags and words https://t.co/VWcCTbsPU9', 'A detailed breakdown of the evaluation categories shows that planning is most useful for predicting unseen colors of inanimate objects. \n\nAll models still struggle with size adjectives, which require real world knowledge. https://t.co/yfAeBsG1hr', 'The syntax-aware decoders usually generate more accurate captions than decoders that directly generate a sequence of words. https://t.co/P1Hp6z1Kkb']",https://arxiv.org/abs/2101.11911,"Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. (2019) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjective-noun and noun-verb compositions. In this work, we investigate different methods to improve compositional generalization by planning the syntactic structure of a caption. Our experiments show that jointly modeling tokens and syntactic tags enhances generalization in both RNN- and Transformer-based models, while also improving performance on standard metrics. ",The Role of Syntactic Planning in Compositional Image Captioning
16,1355257276739645441,1200149623316172800,D Davis,"['I am excited to talk about a new paper that I helped organize describing the ways we characterize @LIGO detectors and improve their sensitivity to gravitational waves! <LINK>', 'We talk about the types of noises that impact LIGO and how we know that the GWs that LIGO sees aren’t actually caused by anything here on Earth! There’s lots of different types of noises that show up in LIGO data, but I’ll talk about some of the highlights.', 'Every few minutes, a loud burst of noise shows up in the LIGO data. Most of the time, these burst aren’t from GWU, but rather from instrumental sources. We call these glitches, and they come in a wide variety of types. https://t.co/tuWeCINh6p', 'One of the worst sources of glitches is from scattered light. This creates arch shapes in the data that you can see below. One of the biggest improvements in the last observing run was tracking down where some of the scattered light was coming from and stopping it. https://t.co/gCdHbC4uce', 'LIGO also is impacted by thunder which shakes the entire detector and adds a rumbling sound to the data. LIGO is so big that the sound from a single lightning strike can arrive at each end of the detector multiple seconds apart! https://t.co/9bBJSDrcXa', 'Another concern is magnetic noise that is correlated between both LIGO sites. The main source of these correlations is from the Earth’s magnetic field! Thankfully the noise is very weak and doesn’t impact our GW detections, but we still keep a close eye on it. https://t.co/zeYJt0zEFN', 'When we do see glitches in the data, we do our best to flag these time periods so that we don’t mistake them for a real GW.  In this case, whenever there was a spike in the optical lever laser power, there was also a glitch! https://t.co/Dlb4GXvOmc', 'Another way we address glitches is by removing a small amount of data around the glitch, called ""gating."" In the last observing run, gating loud glitches was a very important part of increasing the sensitivity of LIGO to very weak, continuous sources of GWs. https://t.co/QzKmPEv50r', 'Whenever we think we might have seen a gravitational wave, we perform lots of tests to check if the signal could actually be a glitch. Most of the time we confirm that the event is a real signal, but ever so often we find out that we really just discovered a new glitch. https://t.co/aNXcNfYHHZ']",https://arxiv.org/abs/2101.11673,"The characterization of the Advanced LIGO detectors in the second and third observing runs has increased the sensitivity of the instruments, allowing for a higher number of detectable gravitational-wave signals, and provided confirmation of all observed gravitational-wave events. In this work, we present the methods used to characterize the LIGO detectors and curate the publicly available datasets, including the LIGO strain data and data quality products. We describe the essential role of these datasets in LIGO-Virgo Collaboration analyses of gravitational-waves from both transient and persistent sources and include details on the provenance of these datasets in order to support analyses of LIGO data by the broader community. Finally, we explain anticipated changes in the role of detector characterization and current efforts to prepare for the high rate of gravitational-wave alerts and events in future observing runs. ",LIGO Detector Characterization in the Second and Third Observing Runs
17,1355178915854127107,1865461842,Luca Soldaini 🏳️‍🌈,"['New #EACL2021 paper with our fantastic intern @HanRujun is out! With @amoschitti1, we study the problem of making Answer Sentence Selection models more contextual without having to parse passages like slow Machine Reading Comprehension models do 🧵1/5 <LINK> <LINK>', '.@HanRujun experimented with two approaches for selecting relevant snippets from documents to use as context, as well as three different architectures for exploiting said context. 2/5 #EACL2021 https://t.co/b1XgdEsTqd', 'we found that, by using sentence similarity to find relevant snippets, and combining signals using a multi-way attention similar to Tan et al. (2018), we can essentially match the performance of an ensemble model with 2x parameters and 1.3x latency 3/5 #EACL2021 https://t.co/WtEM2bpqZo', 'for full analysis and results, see our paper on ArXiv https://t.co/zO0voouGWb Code is not out yet, but watch this space 👀 #EACL2021 4/5', 'lastly, I want to stress how wonderful was to have @HanRujun with us – remote internships during a pandemic are not easy, but Rujun did an amazing job with deepening our understanding of this important document reading / efficiency for QA! Ty so much for your hard work ☺️ 5/5']",https://arxiv.org/abs/2101.12093,"Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low latency, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive models designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better accuracy. In this work, we present an approach to efficiently incorporate contextual information in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode context, improves 6% to 11% over noncontextual state of the art in AS2 with minimal impact on system latency. All experiments in this work were conducted in English. ","Modeling Context in Answer Sentence Selection Systems on a Latency
  Budget"
18,1355092054771183619,869896064802934788,Jan Rybizki,"[""Submitted my new paper (<LINK>) three minutes after the opening for new submissions (don't make last minute latex changes..). Came out 9th on the list. So only about 20% of submitters try to put their article on top. Do people use submission schedulers yet?""]",https://arxiv.org/abs/2101.11641,"The Gaia early Data Release 3 has delivered exquisite astrometric data for 1.47 billion sources, which is revolutionizing many fields in astronomy. For a small fraction of these sources, the astrometric solutions are poor, and the reported values and uncertainties may not apply. Before any analysis, it is important to recognize and excise these spurious results - this is commonly done by means of quality flags in the Gaia catalog. Here, we devise a means of separating 'good' from 'bad' astrometric solutions that is an order of magnitude cleaner than any single flag: 99.3% pure and 97.3% complete, as validated on our test data. We devise an extensive sample of manifestly bad astrometric solutions, with parallax that is negative at > 4.5 sigma; and a corresponding sample of presumably good solutions, including sources in HEALPix pixels on the sky that do not contain such negative parallaxes, and sources that fall on the main sequence in a color-absolute magnitude diagram. We then train a neural network that uses 17 pertinent Gaia catalog entries and information about nearby sources to discriminate between these two samples, captured in a single 'astrometric fidelity' parameter. A diverse set of verification tests shows that our approach works very cleanly, including for sources with positive parallaxes. The main limitations of our approach are in the very low-SNR and the crowded regime. Our astrometric fidelities for all of eDR3 can be queried via the Virtual Observatory, our code and data are public. ",A classifier for spurious astrometric solutions in Gaia EDR3
19,1354837390188109831,1515424688,Armen Aghajanyan,"[""I'm happy to present our new paper MUPPET (<LINK>), arguing for an additional stage between pre-training and fine-tuning, called pre-finetuning which uses massively multi-task learning (~50 tasks) to further refine representations."", 'Recent work has shown gains from MTL/multi-stage fine-tuning, but it can be hard to know which intermediate tasks will best transfer. We show that multi-task supervised tuning is effective if done at scale (# tasks), removing the need to pre-select the best intermediate tasks.', 'Our multi-task set up consists of 46 datasets across 4 task types with close to 5 million supervised samples, using a classification head for each classification dataset, and unified heads for MRC and CommonSense tasks. https://t.co/4Nw4oQlbRo', 'After solving practical problems (loss scaling, heterogeneous batches, etc) we pre-finetune RoBERTa variants and BART. We first look at fine-tuning MUPPET over datasets available in the pre-finetuning regime. MUPPET unanimously improves over it’s base model for GLUE/SQuAD. https://t.co/UtrMe18vnW', 'MUPPET also improves over it’s base model for sentence prediction, commonsense, and summarization tasks. https://t.co/wTNwum3BB5', 'To show that MUPPET representations are more generalizable, we also measure MUPPET performance over datasets not available in the pre-finetuning regime. MUPPET once again improves consistently over it’s only pre-trained counterparts. https://t.co/lrYvGnZtBD', 'MTL historically has given inconclusive results. So why does MUPPET work? Turns out scale is fundamental for MTL. There exists a critical # of tasks (~15) under which pre-finetuning degrades representations. But over this critical point linearly improves representations. https://t.co/AJmjtfHvxp', 'As another side-effect, MUPPET variants of pre-trained models show much better data-efficiency for downstream fine-tuning.', 'This was joint work with great authors Anchit Gupta, @AkshatS07, Xilun Chen, @LukeZettlemoyer, @sonalsgupta']",https://arxiv.org/abs/2101.11038,"We propose pre-finetuning, an additional large-scale learning stage between language model pre-training and fine-tuning. Pre-finetuning is massively multi-task learning (around 50 datasets, over 4.8 million total labeled examples), and is designed to encourage learning of representations that generalize better to many different tasks. We show that pre-finetuning consistently improves performance for pretrained discriminators (e.g.~RoBERTa) and generation models (e.g.~BART) on a wide range of tasks (sentence prediction, commonsense reasoning, MRC, etc.), while also significantly improving sample efficiency during fine-tuning. We also show that large-scale multi-tasking is crucial; pre-finetuning can hurt performance when few tasks are used up until a critical point (usually above 15) after which performance improves linearly in the number of tasks. ",Muppet: Massive Multi-task Representations with Pre-Finetuning
20,1354765061596860418,722490639359709184,Benjamin Muller,"['Thrilled to share our new paper accepted at #EACL2021:\nFirst Align Then Predict, Understanding the Cross-Lingual Ability of Multilingual BERT \nJoint work with @yanaiela , @bensagot and @zehavoc\n👉 <LINK> 1/3', 'TLDR: To understand zero-shot cross-lingual transfer,  we combine a structural analysis with a simple behavioural analysis based on randomly initialising specific layers (RANDOM-INIT). We show that after fine-tuning, mBERT can be viewed as the stacking of two sub-networks...  2/3', '1- A Multilingual Encoder, located in the lower part of the model, critical for cross-lingual transfer, is in charge of aligning representations across languages\xa0 \n2- A task-specific language-agnostic predictor, located in the upper part of the model 3/3']",https://arxiv.org/abs/2101.11109,"Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model's internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis. ","First Align, then Predict: Understanding the Cross-Lingual Ability of
  Multilingual BERT"
21,1354720801107439617,106370162,Frédéric Grosshans,"['You like linear optics and quantum error correction? You should read our new paper with Paul Hilaire, Edwin Barnes and @see_quantum from @virginia_tech arXiv:2101.11082 “Error-correcting entanglement swapping using a practical logical photon encoding” <LINK>', '@see_quantum @virginia_tech If you like these subject, you should also read the paper from @PsiQuantum  on arXiv:2101.09310 «Fusion-based quantum computation» https://t.co/bhyrmNMYKu .\n\nThere is a small common part on Bell measurement, found independently, but overall, both papers are quite different']",https://arxiv.org/abs/2101.11082,"Several emerging quantum technologies, including quantum networks, modular and fusion-based quantum computing, rely crucially on the ability to perform photonic Bell state measurements. Therefore, photon losses and the 50\% success probablity upper bound of Bell state measurements pose a critical limitation to photonic quantum technologies. Here, we develop protocols that overcome these two key challenges through logical encoding of photonic qubits. Our approach uses a tree graph state logical encoding, which can be produced deterministically with a few quantum emitters, and achieves near-deterministic logical photonic Bell state measurements while also protecting against errors including photon losses, with a record loss-tolerance threshold. ","Error-correcting entanglement swapping using a practical logical photon
  encoding"
22,1354685721676681217,1231303679589978113,Pratixan Sarmah,['First paper out on arxiv! A look into the effects of new jet substructure measurements on Pythia8 tunes <LINK>'],https://arxiv.org/abs/2101.11395,This study used the recent ATLAS jet substructure measurements to see if any improvements can be made to the commonly used Pythia8 Monash and A14 tunes. ,Effect of new jet substructure measurements on Pythia8 tunes
23,1354601394284576768,21611239,Sean Carroll,"['Energy is not conserved in quantum mechanics — at least, not as seen by observers, when a quantum measurement is performed. Jackie Lodman and I investigate this in a new paper. (Explanatory blog post coming soon.)\n<LINK>', 'One of those papers where people will either think it’s trivially true, or obviously wrong. Just hoping the former slightly outnumber the latter.', '@knobsturner Ahead of your time.']",https://arxiv.org/abs/2101.11052,"We study the conservation of energy, or lack thereof, when measurements are performed in quantum mechanics. The expectation value of the Hamiltonian of a system can clearly change when wave functions collapse in accordance with the standard textbook (Copenhagen) treatment of quantum measurement, but one might imagine that the change in energy is compensated by the measuring apparatus or environment. We show that this is not true; the change in the energy of a state after measurement can be arbitrarily large, independent of the physical measurement process. In Everettian quantum theory, while the expectation value of the Hamiltonian is conserved for the wave function of the universe (including all the branches), it is not constant within individual worlds. It should therefore be possible to experimentally measure violations of conservation of energy, and we suggest an experimental protocol for doing so. ",Energy Non-Conservation in Quantum Mechanics
24,1354376269471895553,153651820,M. Celeste Artale,"['New paper on arXiv today! Lead by Ginevra Favole, we investigate secondary galaxy and dark matter halo dependencies on the galaxy clustering by constructing mock catalogs using a standard SHAM scheme on IllustrisTNG simulation. Check it out:\n<LINK>']",https://arxiv.org/abs/2101.10733,"We use the IllustrisTNG100 hydrodynamical simulation to study the dependence of the galaxy two-point correlation function on a broad range of secondary subhalo and galactic properties. We construct galaxy mock catalogues adopting a standard sub-halo abundance matching scheme coupled with a secondary assignment between galaxy colour or specific star formation rate and the following subhalo properties: starvation redshift z$_{\rm starve}$, concentration at infall, overdensity $\delta_R^{\rm env}$, tidal anisotropy $\alpha_R$, and tidal overdensity $\delta_R$. The last two quantities allow us to fully characterise the tidal field of our subhaloes, acting as mediators between their internal and large-scale properties. The resulting mock catalogues overall return good agreement with the IllustrisTNG100 measurements. The accuracy of each model strongly depends on the correlation between the secondary galaxy and subhalo properties employed. Among all the subhalo proxies tested, we find that z$_{\rm starve}$ and $c_{\rm infall}$ are the ones that best trace the large-scale structure, producing robust clustering predictions for different samples of red/blue and quenched/star-forming galaxies. ",SHAM through the lens of a hydrodynamical simulation
25,1354368128529145856,89791271,Félix de Moya Anegón,"['New paper: Bibliometric assessment of national scientific journals. Nationally oriented scientific-scholarly journals are considered from a methodological-informetric viewpoint, analysing data extracted from @ScimagoJR  based on @Scopus.\n<LINK>']",https://arxiv.org/abs/2101.10906,"Nationally oriented scientific-scholarly journals are considered from a methodological-informetric viewpoint, analysing data extracted from Scimago Journal Rank based on Scopus. An operational definition is proposed of a journal's degree of national orientation based on the geographical distribution of its publishing or citing authors, and the role of international collaboration and a country's total publication output. A comprehensive analysis is presented of trends up until 2019 in national orientation and citation impact of national journals entering Scopus, extending outcomes in earlier studies. A method to analyse national journals of given countries is applied to the set of former USSR republics and Eastern and Central European states which were under socialism, distinguishing between domestic and foreign national journals. The possible influence is highlighted of factors related to a journal's access status, publication language and subject field, international scientific migration and collaboration, database coverage policies, the size of a national research community, historical-political factors and national research assessment and funding policies. ",Bibliometric assessment of national scientific journals
26,1354254175631273988,1316039333175066624,Ruslan Shaydulin,"['New paper out w/ @SciWild! We show how by leveraging symmetries computed using fast graph automorphism solvers the cost of training QAOA purely classically can be reduced by multiple orders of magnitude even on graphs known to be hard for such solvers <LINK>', 'This is a direct corollary of our recent results on classical symmetries in QAOA (https://t.co/mYwKQDLPKI) with @stuart_hadfield, Tad Hogg and Ilya Safro. More to come, stay tuned!']",http://arxiv.org/abs/2101.10296,"A promising approach to the practical application of the Quantum Approximate Optimization Algorithm (QAOA) is finding QAOA parameters classically in simulation and sampling the solutions from QAOA with optimized parameters on a quantum computer. Doing so requires repeated evaluations of QAOA energy in simulation. We propose a novel approach for accelerating the evaluation of QAOA energy by leveraging the symmetry of the problem. We show a connection between classical symmetries of the objective function and the symmetries of the terms of the cost Hamiltonian with respect to the QAOA energy. We show how by considering only the terms that are not connected by symmetry, we can significantly reduce the cost of evaluating the QAOA energy. Our approach is general and applies to any known subgroup of symmetries and is not limited to graph problems. Our results are directly applicable to nonlocal QAOA generalization RQAOA. We outline how available fast graph automorphism solvers can be leveraged for computing the symmetries of the problem in practice. We implement the proposed approach on the MaxCut problem using a state-of-the-art tensor network simulator and a graph automorphism solver on a benchmark of 48 graphs with up to 10,000 nodes. Our approach provides an improvement for $p=1$ on $71.7\%$ of the graphs considered, with a median speedup of $4.06$, on a benchmark where $62.5\%$ of the graphs are known to be hard for automorphism solvers. ",Exploiting Symmetry Reduces the Cost of Training QAOA
27,1354253759816343553,4902145390,Gordan Krnjaic,"['Cheers to awesome collaborators Rodolfo Capdevilla, @drc83 , and Yoni Kahn for our new paper tonight. We determine the highest mass particles that could be responsible for the g-2 anomaly and argue that a suitable muon collider could cover all of them <LINK>', 'Of course, this assumes \n\n1) The g-2 anomaly is ultimately verified and\n2) Light, weakly coupled solutions are excluded by low energy measurements', '@dangaristo @drc83 Indeed, muon colliders are much smaller than other machines https://t.co/OiwB5esJe4', ""@ducciolvp @drc83 Theorem? I hardly knew 'em!"", ""@nausheenrshah I wish I had a better answer! Some months ago I heard they'd unblind in January, but here we are...""]",https://arxiv.org/abs/2101.10334,"We perform a model-exhaustive analysis of all possible beyond Standard Model (BSM) solutions to the $(g-2)_\mu$ anomaly to study production of the associated new states at future muon colliders, and formulate a no-lose theorem for the discovery of new physics if the anomaly is confirmed and weakly coupled solutions below the GeV scale are excluded. Our goal is to find the highest possible mass scale of new physics subject only to perturbative unitarity, and optionally the requirements of minimum flavour violation (MFV) and/or naturalness. We prove that a 3 TeV muon collider is guaranteed to discover all BSM scenarios in which $\Delta a_\mu$ is generated by SM singlets with masses above $\sim $ GeV; lighter singlets will be discovered by upcoming low-energy experiments. If new states with electroweak quantum numbers contribute to $(g-2)_\mu$, the minimal requirements of perturbative unitarity guarantee new charged states below $\mathcal{O}(100 {\rm TeV})$, but this is strongly disfavoured by stringent constraints on charged lepton flavour violating (CLFV) decays. Reasonable BSM theories that satisfy CLFV bounds by obeying Minimal Flavour Violation (MFV) and avoid generating two new hierarchy problems require the existence of at least one new charged state below $\sim 10$ TeV. This strongly motivates the construction of high-energy muon colliders, which are guaranteed to discover new physics: either by producing these new charged states directly, or by setting a strong lower bound on their mass, which would empirically prove that the universe is fine-tuned and violates the assumptions of MFV while somehow not generating large CLFVs. The former case is obviously the desired outcome, but the latter scenario would perhaps teach us even more about the universe by profoundly revising our understanding of naturalness, cosmological vacuum selection, and the SM flavour puzzle. ","A No-Lose Theorem for Discovering the New Physics of $(g-2)_\mu$ at Muon
  Colliders"
28,1354212835698614273,557011202,Nicola Tomassetti,"['Ah già: new paper from the arXiv:\n""Development of a web application for monitoring solar activity and cosmic radiation"", Pelosi et al. 2021, daje! \n<LINK>']",https://arxiv.org/abs/2101.09366,"The flux of cosmic rays (CRs) in the heliosphere is subjected to remarkable time variations caused by the 11-year cycle of solar activity. To help the study of this effect, we have developed a web application (Heliophysics Virtual Observatory) that collects real-time data on solar activity, interplanetary plasma, and charged radiation from several space missions or observatories. As we will show, our application can be used to visualize, manipulate, and download updated data on sunspots, heliospheric magnetic fields, solar wind, and neutron monitors counting rates. Data and calculations are automatically updated on daily basis. A nowcasting for the energy spectrum of CR protons near-Earth is also provided using calculations and real-time neutron monitor data as input. ","Development of a web application for monitoring solar activity and
  cosmic radiation"
29,1354128199362334721,2180794657,Joshua Kroll,"['Pleased to release a new paper, “Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems”, to appear in the 2021 @FAccTConference. <LINK>.']",http://arxiv.org/abs/2101.09385,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes. ","Outlining Traceability: A Principle for Operationalizing Accountability
  in Computing Systems"
30,1354123148908777472,717162062837719040,Phil Armitage,"['New paper! In work led by Rebecca Martin @unlv we study the dynamics of circumplanetary gas disks during late-stage planet formation. We find - in a simplified model - that these disks can tilt out of the plane and oscillate in quite weird ways! Thread...\n\n<LINK> <LINK>', 'Circumplanetary disks form around massive planets growing within protoplanetary disks. Massive planets open a gap in the protoplanetary disk but continue to accrete, forming a smaller circumplanetary disk. https://t.co/1QNM6XOaOg', 'Disks are where the ""regular"" satellites of the giant planets (e.g. the Galilean moons) form. They may also be signposts for very young extrasolar planets. Disks are larger than planets, and accrete onto them, so they can be more detectable.\n\nhttps://t.co/yesOiSzOUt', ""Recently, we found that circumplanetary disks can be unstable to the growth of tilt. This built on work by Steve Lubow, Caroline Terquem, and others. It's a tidal effect, that works if the disk thickness and size are in the right range.\n\nhttps://t.co/Cny772Dg9Z https://t.co/1dWm5bLylw"", ""What happens if a disk in a binary becomes *really* tilted? Its inclination and eccentricity start to oscillate, in a fluid disk analog of the Kozai-Lidov (KL) effect that's important for planetary dynamics.\n\nhttps://t.co/igcgdobEW6 https://t.co/v9tIbQ31DN"", 'In the new work, we simulated the interaction of these two instabilities. We found that tilt growth is capped by the onset of KL oscillations, after which accretion onto the planet becomes episodic.', 'As usual there are open questions. Most notably we only looked at ""detached"" circumplanetary disks, so how ongoing accretion from the protoplanetary disk would change things is not clear. The same dynamics might also apply to other disks in binary systems.', ""Big thanks to Rebecca, and to the other co-authors Zhaohuan Zhu, Chao-Chin Yang, and Hans Baehr. For this and most of our prior simulations, we used @danprice_astro's fantastic PHANTOM SPH code:\n\nhttps://t.co/VhnZbSDUsh""]",https://arxiv.org/abs/2101.09388,"Circumplanetary discs can be linearly unstable to the growth of disc tilt in the tidal potential of the star-planet system. We use three-dimensional hydrodynamical simulations to characterize the disc conditions needed for instability, together with its long term evolution. Tilt growth occurs for disc aspect ratios, evaluated near the disc outer edge, of $H/r\gtrsim 0.05$, with a weak dependence on viscosity in the wave-like regime of warp propagation. Lower mass giant planets are more likely to have circumplanetary discs that satisfy the conditions for instability. We show that the tilt instability can excite the inclination to above the threshold where the circumplanetary disc becomes unstable to Kozai--Lidov (KL) oscillations. Dissipation in the Kozai--Lidov unstable regime caps further tilt growth, but the disc experiences large oscillations in both inclination and eccentricity. Planetary accretion occurs in episodic accretion events. We discuss implications of the joint tilt--KL instability for the detectability of circumplanetary discs, for the obliquity evolution of forming giant planets, and for the formation of satellite systems. ","Kozai-Lidov oscillations triggered by a tilt instability of detached
  circumplanetary discs"
31,1354085340244533262,171302416,Enrique Paillas,"['New paper out 😃 We propose to split the galaxy density field in quantiles with a fixed local density. The advantage is that the peculiar velocity PDF in each quantile is close to Gaussian, allowing more accurate modelling of redshift-space distortions. <LINK> <LINK>']",https://arxiv.org/abs/2101.09854,"Accurate modelling of redshift-space distortions (RSD) is challenging in the non-linear regime for two-point statistics e.g. the two-point correlation function (2PCF). We take a different perspective to split the galaxy density field according to the local density, and cross-correlate those densities with the entire galaxy field. Using mock galaxies, we demonstrate that combining a series of cross-correlation functions (CCFs) offers improvements over the 2PCF as follows: 1. The distribution of peculiar velocities in each split density is nearly Gaussian. This allows the Gaussian streaming model for RSD to perform accurately within the statistical errors of a ($1.5\,h^{-1}$Gpc)$^3$ volume for almost all scales and all split densities. 2. The PDF of the density field at small scales is non-Gaussian, but the CCFs of split densities capture the non-Gaussianity, leading to improved cosmological constraints over the 2PCF. We can obtain unbiased constraints on the growth parameter $f\sigma_{12}$ at the per-cent level, and Alcock-Paczynski (AP) parameters at the sub-per-cent level with the minimal scale of $15\,h^{-1}{\rm Mpc}$. This is a $\sim$30 per cent and $\sim$6 times improvement over the 2PCF, respectively. The diverse and steep slopes of the CCFs at small scales are likely to be responsible for the improved constraints of AP parameters. 3. Baryon acoustic oscillations (BAO) are contained in all CCFs of split densities. Including BAO scales helps to break the degeneracy between the line-of-sight and transverse AP parameters, allowing independent constraints on them. We discuss and compare models for RSD around spherical densities. ",Redshift-space distortions with split densities
32,1353988457954226176,237682040,Daniel Manzano,['New paper in ArXiv. Degenerated Liouvillians and Steady-State Reduced Density Matrices.\n\n<LINK>'],https://arxiv.org/abs/2101.10236,"Symmetries in an open quantum system lead to degenerated Liouvillian that physically implies the existence of multiple steady states. In such cases, obtaining the initial condition independent stead states is highly nontrivial since any linear combination of the \emph{true} asymptotic states, which may not necessarily be a density matrix, is also a valid asymptote for the Liouvillian. Thus, in this work we consider different approaches to obtain the \emph{true} steady states of a degenerated Liouvillian. In the ideal scenario, when the open system symmetry operators are known we show how these can be used to obtain the invariant subspaces of the Liouvillian and hence the steady states. We then discuss two other approaches that do not require any knowledge of the symmetry operators. These could be a powerful tool to deal with quantum many-body complex open systems. The first approach which is based on Gramm-Schmidt orthonormalization of density matrices allows us to obtain \emph{all} the steady states, whereas the second one based on large deviations allows us to obtain the non-degenerated maximum and minimum current-carrying states. We discuss our method with the help of an open para-Benzene ring and examine interesting scenarios such as the dynamical restoration of Hamiltonian symmetries in the long-time limit and apply the method to study the eigenspacing statistics of the nonequilibrium steady state. ",Degenerated Liouvillians and Steady-State Reduced Density Matrices
33,1353969199815643136,974241983760826368,Daisuke Inoue @VRchat: Benthos,"['Recently, T. Hiraiwa @TPBGMBI, R. Akiyama, I, A.M.R Kabir \n@arif_rashedul and A. Kakugo uploaded our new preprint paper about mono-polar clustering of collectively moving self-propelled particles in the presence of L/R asymmetry in their motion.\n<LINK> <LINK>']",https://arxiv.org/abs/2101.02130,"It is still challenging to control dynamic self-organization patterns of self-propelled particles. Although varieties of patterns associated with chirality have been observed, essential control factors determining patterns remain unclear. Here, we explore numerically how torque upon particle collision affects dynamic self-organization. Based on the particle-based model with both collision-induced torque and torque in self-propulsion, we find that introducing collision-induced torque turns homogeneous bi-polar orientation templated by bi-directional alignment into rotating mono-polar flocks. ","Collision-induced torque mediates transition of dynamic patterns formed
  by active particles"
34,1353887309968642053,922847904058011649,Romy Rodríguez,"['Excited to share this new paper! Here we present “Analytic estimates of the achievable precision on the properties of transiting exoplanet using purely empirical measurements”\n\n<LINK>', 'We derived analytic approximations of the mass, density, and surface gravities of a transiting exoplanet in terms of direct transit and radial velocity observables, like the period, transit duration, ingress/egress time, transit depth, and RV semi-amplitude.', 'We find a general hierarchy in the precision with which the planetary mass, radius, density and surface can be measured, namely, the surface gravity is the best measured property, followed by the density and the mass.', 'We tested our analytic approximations with a few confirmed exoplanets from the literature and find generally a very good agreement between the uncertainties in the properties reported by the papers and the ones that we derive using our analytic approximations. https://t.co/y8M5cZUWG4', 'We also explored the connection between the surface gravity, which we believe to be an important diagnostic of the habitability of a rocky exoplanet, and the core mass fraction.', 'We measured the core-mass fraction of the super-Mercury candidate K2-229b as predicted from its mass and radius (56%) and the core-mass fraction as expected from the refractory abundance of its host star (29%).', 'The mass-radius ellipses show the 1sigma and 2sigma uncertainties in its mass and radius when the mass and radius are assumed to be uncorrelated (red) and when they’re correlated via the additional constraint of the surface gravity (black). https://t.co/l9VXH3w8A1', 'The additional constraint of the surface gravity reduces the uncertainty in the core mass fraction of K2-229b and we therefore conclude that surface gravity is a good proxy for the core mass fraction of a terrestrial exoplanet', '@j_tharindu Thank you!! :D', '@astroxicana Gracias Lupita!! &lt;3']",https://arxiv.org/abs/2101.09289,"We present analytic estimates of the fractional uncertainties on the mass, radius, surface gravity, and density of a transiting planet, using only empirical or semi-empirical measurements. We first express these parameters in terms of transit photometry and radial velocity (RV) observables, as well as the stellar radius $R_{\star}$, if required. In agreement with previous results, we find that, assuming a circular orbit, the surface gravity of the planet ($g_p$) depends only on empirical transit and RV parameters; namely, the planet period $P$, the transit depth $\delta$, the RV semi-amplitude $K_{\star}$, the transit duration $T$, and the ingress/egress duration $\tau$. However, the planet mass and density depend on all these quantities, plus $R_{\star}$. Thus, an inference about the planet mass, radius, and density must rely upon an external constraint such as the stellar radius. For bright stars, stellar radii can now be measured nearly empirically by using measurements of the stellar bolometric flux, the effective temperature, and the distance to the star via its parallax, with the extinction $A_V$ being the only free parameter. For any given system, there is a hierarchy of achievable precisions on the planetary parameters, such that the planetary surface gravity is more accurately measured than the density, which in turn is more accurately measured than the mass. We find that surface gravity provides a strong constraint on the core mass fraction of terrestrial planets. This is useful, given that the surface gravity may be one of the best measured properties of a terrestrial planet. ","Analytic Estimates of the Achievable Precision on the Physical
  Properties of Transiting Planets Using Purely Empirical Measurements"
35,1353867082853617666,30989098,Karin Sandstrom,"['Some quick advertising: a new paper from former UCSD postdoc Jeremy Chastenet on a comparative study of far-IR dust modeling results in M101! <LINK> We compare the dust masses, PAH fractions, radiation field and more from six widely used dust SED models.', ""A key finding is that over parts of M101 all physical dust models led to too much dust compared to the maximum limit you'd expect from the metallicity gradient. This may be due to the elemental depletion constraints assumed for the MW cirrus calibration in each model.""]",https://arxiv.org/abs/2101.09236,"We present a comparative study of four physical dust models and two single-temperature modified blackbody models by fitting them to the resolved WISE, Spitzer, and Herschel photometry of M101 (NGC 5457). Using identical data and a grid-based fitting technique, we compare the resulting dust and radiation field properties derived from the models. We find that the dust mass yielded by the different models can vary by up to factor of 3 (factor of 1.4 between physical models only), although the fits have similar quality. Despite differences in their definition of the carriers of the mid-IR aromatic features, all physical models show the same spatial variations for the abundance of that grain population. Using the well determined metallicity gradient in M101 and resolved gas maps, we calculate an approximate upper limit on the dust mass as a function of radius. All physical dust models are found to exceed this maximum estimate over some range of galactocentric radii. We show that renormalizing the models to match the same Milky Way high latitude cirrus spectrum and abundance constraints can reduce the dust mass differences between models and bring the total dust mass below the maximum estimate at all radii. ",Benchmarking Dust Emission Models in M101
36,1353764811998425088,1353754791881674753,Lorena Acuña,"[""Just uploaded my first paper on arXiv <LINK> a few days ago. YAAY! Applying the interior and atmosphere model I've worked on during the first year of my PhD to TRAPPIST-1 sounds mainstream, but we definitely learnt new things: <LINK>""]",https://arxiv.org/abs/2101.08172,"Context. Planetary mass and radius data are showing a wide variety in densities of low-mass exoplanets. This includes sub-Neptunes, whose low densities can be explained with the presence of a volatile-rich layer. Water is one of the most abundant volatiles, which can be in the form of different phases depending on the planetary surface conditions. To constrain their composition and interior structure, it is required to develop models that calculate accurately the properties of water at its different phases. Aims. We present an interior structure model that includes a multiphase water layer with steam, supercritical and condensed phases. We derive the constraints for planetary compositional parameters and their uncertainties, focusing on the multiplanetary system TRAPPIST-1, which presents both warm and temperate planets. Methods. We use a 1D steam atmosphere in radiative-convective equilibrium with an interior whose water layer is in supercritical phase self-consistently. For temperate surface conditions, we implement liquid and ice Ih to ice VII phases in the hydrosphere. We adopt a MCMC inversion scheme to derive the probability distributions of core and water compositional parameters Results. We refine the composition of all planets and derive atmospheric parameters for planets b and c. The latter would be in a post-runaway greenhouse state and could be extended enough to be probed by space mission such as JWST. Planets d to h present condensed ice phases, with maximum water mass fractions below 20%. Conclusions. The derived amounts of water for TRAPPIST-1 planets show a general increase with semi-major axis, with the exception of planet d. This deviation from the trend could be due to formation mechanisms, such as migration and an enrichment of water in the region where planet d formed, or an extended CO$_{2}$-rich atmosphere. ",Characterisation of the hydrospheres of TRAPPIST-1 planets
37,1353757720235692032,1102208877980991489,Eduardo Peinado,['New paper on CEvNS in reactor experiments. SBC collaboration <LINK>'],https://arxiv.org/abs/2101.08785?fbclid=IwAR2GD21c96-NYJgD8EpOd387x8Ac9xIx68CsUajobrG6-QvXzdKCohq8HNU,"The physics reach of a low threshold (100 eV) scintillating argon bubble chamber sensitive to Coherent Elastic neutrino-Nucleus Scattering (CE$\nu$NS) from reactor neutrinos is studied. The sensitivity to the weak mixing angle, neutrino magnetic moment, and a light $Z'$ gauge boson mediator are analyzed. A Monte Carlo simulation of the backgrounds is performed to assess their contribution to the signal. The analysis shows that world-leading sensitivities are achieved with a one-year exposure for a 10 kg chamber at 3 m from a 1 MW$_{th}$ research reactor or a 100 kg chamber at 30 m from a 2000 MW$_{th}$ power reactor. Such a detector has the potential to become the leading technology to study CE$\nu$NS using nuclear reactors. ","Physics reach of a low threshold scintillating argon bubble chamber in
  coherent elastic neutrino-nucleus scattering reactor experiments"
38,1353711691519979521,986408509,Kirpal Nandra,['New paper by MPE PhD student @adam_malyali on a strange X-ray outburst seen by @eROSITA_SRG in the nucleus of a relatively nearby galaxy <LINK>'],https://arxiv.org/abs/2101.08760,"We report on SRG/eROSITA, ZTF, ASAS-SN, Las Cumbres, NEOWISE-R, and Swift XRT/UVOT observations of the unique ongoing event AT 2019avd, located in the nucleus of a previously inactive galaxy at $z=0.029$. eROSITA first observed AT 2019avd on 2020-04-28 during its first all sky survey, when it was detected as an ultra-soft X-ray source ($kT\sim 85$ eV) that was $\gtrsim 90$ times brighter in the $0.2-2$ keV band than a previous 3$\sigma$ upper flux detection limit (with no archival X-ray detection at this position). The ZTF optical light curve in the $\sim 450$ days preceding the eROSITA detection is double peaked, and the eROSITA detection coincides with the rise of the second peak. Follow-up optical spectroscopy shows the emergence of a Bowen fluorescence feature and high-ionisation coronal lines ([\ion{Fe}{X}] 6375 {\AA}, [\ion{Fe}{XIV}] 5303 {\AA}), along with persistent broad Balmer emission lines (FWHM$\sim 1400$ km s$^{-1}$). Whilst the X-ray properties make AT 2019avd a promising tidal disruption event (TDE) candidate, the optical properties are atypical for optically selected TDEs. We discuss potential alternative origins that could explain the observed properties of AT 2019avd, such as a stellar binary TDE candidate, or a TDE involving a super massive black hole binary. ","AT 2019avd: A novel addition to the diverse population of nuclear
  transients"
39,1353679657711890435,3407140725,Ed Hirst,"['New paper out today, fun one to work on using AGT on dessins d’enfants...\n<LINK>\n#hep #theory']",https://arxiv.org/abs/2101.08843,"We show how to map Grothendieck's dessins d'enfants to algebraic curves as Seiberg-Witten curves, then use the mirror map and the AGT map to obtain the corresponding 4d $\mathcal{N}=2$ supersymmetric instanton partition functions and 2d Virasoro conformal blocks. We explicitly demonstrate the 6 trivalent dessins with 4 punctures on the sphere. We find that the parametrizations obtained from a dessin should be related by certain duality for gauge theories. Then we will discuss that some dessins could correspond to conformal blocks satisfying certain rules in different minimal models. ","Dessins d'Enfants, Seiberg-Witten Curves and Conformal Blocks"
40,1353630141545664513,1083716290186043392,"Valeria Grisoni, PhD","['In collaboration with Emanuele Spitoni et al., our new paper today on the arxiv: ""APOGEE DR16: a multi-zone chemical evolution model for the Galactic disc based on MCMC methods"" <LINK>']",https://arxiv.org/abs/2101.08803,"The analysis of the APOGEE DR16 data suggests the existence of a clear distinction between two sequences of disc stars at different Galactocentric distances in the [$\alpha$/Fe] vs. [Fe/H] abundance ratio space: the so-called high-$\alpha$ sequence, classically associated to an old population of stars in the thick disc, and the low-$\alpha$ sequence, which mostly comprises relatively young stars in the thin disc. We perform a Bayesian analysis based on a Markov Chain Monte Carlo method to constrain a multi-zone two-infall chemical evolution model designed for regions at different Galactocentric distances using measured chemical abundances from the APOGEE DR16 sample. An inside-out formation of the Galaxy disc naturally emerges from the best fit of our two-infall chemical-evolution model to APOGEE-DR16: inner Galactic regions are assembled on shorter time-scales compared to the external ones. In the outer disc (with radii $R>6$ kpc), the chemical dilution due to a late accretion event of gas with primordial chemical composition is the main driver of the [Mg/Fe] vs. [Fe/H] abundance pattern in the low-$\alpha$ sequence. In the inner disc, in the framework of the two-infall model, we confirm the presence of an enriched gas infall in the low-$\alpha$ phase as suggested by chemo-dynamical models. Our Bayesian analysis of the recent APOGEE DR16 data suggests a significant delay time, ranging from $\sim$3.0 to 4.7 Gyr, between the first and second gas infall events for all the analyzed Galactocentric regions. Our results propose a clear interpretation of the [Mg/Fe] vs. [Fe/H] relations along the Galactic discs. The signatures of a delayed gas-rich merger which gives rise to a hiatus in the star formation history of the Galaxy are impressed in the [Mg/Fe] vs. [Fe/H] relation, determining how the low-$\alpha$ stars are distributed in the abundance space at different Galactocentric distances. ","APOGEE DR16: a multi-zone chemical evolution model for the Galactic disc
  based on MCMC methods"
41,1353558677073010690,2377407248,Daniel Whiteson,"['New paper: Foundations of a Fast, Data-Driven, Machine-Learned Simulator\n\n<LINK>\n\nwith a fun BIG IDEA: instead of using ML to *mimic* slow detector sim, use ML on *real data* to learn the *real* detector. Then maybe we can *replace* slow simulators!', '(Led by my amazing student Jessica Howard, with UCI ML folks.)\n\nMost fast simulation these days use GANs, which can learn to mimic the output of an expensive simulation run. But what if you want to simulate something else? You have to first run the slow simulator.', 'Why can’t we just learn how to simulate the detector… from the real detector? Because we don’t know what the *truth* is — we can’t observe the latent data about the real particles from the collision. So supervised learning is impossible.', 'But there are places where we know the theory, so we don’t observe the particles directly but we know their distributions very well.  Can we do some kind of unsupervised learning, to deduce how the detector transforms things?', 'Jessica realized that Varational Autoencoders (VAEs) are almost the right tool. They learn to map from observed X to latent Z and back.  Her idea was to make Z the *physical latent space*, the unobserved particles.', 'We know the theoretical distribution in Z, and we just want to learn how the detector goes Z-&gt;X. https://t.co/3WUI3HmMxm', 'This required using a new kind of auto-encoder that can handle this setup, a Sliced Wasserstein Autoencoder, inspired by optimal transport theory, and trained in an *unsupervised manner*.', '(As a bonus, you learn the mapping X to Z, useful for unfolding)', 'And it works!  Here’s a simulation of Z-&gt;ee events. https://t.co/SJR0xQ2yNJ', 'And the hadronic W from top quark pairs. https://t.co/4jx9T2dfz2', 'The idea is to learn the detector transformation in control regions, and extrapolate.  Just like the slow simulation is tuned in data control regions.', 'Performance isn’t yet perfect, but this paper establishes the foundation of a potentially new approach to simulation.', 'And it should have applications to *any* field that uses simulations to model how latent data get transformed through observation.', ""@xmpierinix Great points.  \n\n1. Since the latent space is physical, we can directly inspect the transformation the SWAE has learned (see example in paper). That's much harder when the latent space is noise/abstract."", '@xmpierinix 2. Would love to hear more about how to adiabatically transition from supervised GANs to an approach that skips GEANT and trains on data (unless I misunderstood your comment).']",https://arxiv.org/abs/2101.08944,"We introduce a novel strategy for machine-learning-based fast simulators, which is the first that can be trained in an unsupervised manner using observed data samples to learn a predictive model of detector response and other difficult-to-model transformations. Across the physical sciences, a barrier to interpreting observed data is the lack of knowledge of a detector's imperfect resolution, which transforms and obscures the unobserved latent data. Modeling this detector response is an essential step for statistical inference, but closed-form models are often unavailable or intractable, requiring the use of computationally expensive, ad-hoc numerical simulations. Using particle physics detectors as an illustrative example, we describe a novel strategy for a fast, predictive simulator called Optimal Transport based Unfolding and Simulation (OTUS), which uses a probabilistic autoencoder to learn this transformation directly from observed data, rather than from existing simulations. Unusually, the probabilistic autoencoder's latent space is physically meaningful, such that the decoder becomes a fast, predictive simulator for a new latent sample, and has the potential to replace Monte Carlo simulators. We provide proof-of-principle results for $Z$-boson and top-quark decays, but stress that our approach can be widely applied to other physical science fields. ","Foundations of a Fast, Data-Driven, Machine-Learned Simulator"
42,1353547987914395648,1148919970660663296,Ryan Turner,"[""🚨🚨 New Paper Klaxon 🚨🚨\n\nMy paper 'Improving estimates of the growth rate using galaxy-velocity correlations: a simulation study' is now available on the arXiv for your reading pleasure\n\nCheck it out here --&gt; <LINK>\n\nMy attempts to summarise below..."", 'Peculiar velocities (PVs) present a way of testing gravity. They are imparted onto galaxies over cosmic time under the gravitational influence of large-scale structure, dictated by a value known as the growth rate, f.', 'General Relativity (GR) predicts a scale-independent growth rate that can be approximated as f ~ Ω_m ^ 0.55. If we were to measure something that differed from this, it would be evidence against GR, and could be used to constrain non-standard cosmologies.', 'This work combines the power of four separate statistics. Two which estimate the velocity auto-correlation function, one for the two-point galaxy auto-correlation function, and a new estimator developed by our team which estimates the galaxy-velocity cross-correlation function.', 'The cross-correlation between PVs and the galaxy density field contains cosmological information otherwise missed by only considering the two velocity statistics, which we can exploit in a joint analysis of all four of the velocity and density statistics.', 'We apply these statistics to simulations with a known value of the growth rate, and we find that our joint analysis recovers this fiducial value. (This 1D posterior is one of 1000 used in the analysis.) https://t.co/rfUL5Bewcd', 'When applying the method to individual datasets, the joint analysis of velocity and density also reduces the statistical uncertainty of our measurement of f by over 50%, compared to the same analysis only considering the peculiar velocity information. https://t.co/KNdD4C5JtD', 'This is a simulation paper, with the goal of testing the framework in ideal circumstances. With our analysis tested we can move to real data. We might not see such extreme improvements (although we endeavoured to implement realistic errors) but our results are encouraging!', 'There is room for additional complexities in our models which would improve our fits, add to this the sheer amount of PV data on the horizon (DES, Taipan, WALLABY, etc...) and the future of PVs as a probe of cosmological models is really exciting!']",https://arxiv.org/abs/2101.09026,"We present an improved framework for estimating the growth rate of large-scale structure, using measurements of the galaxy-velocity cross-correlation in configuration space. We consider standard estimators of the velocity auto-correlation function, $\psi_1$ and $\psi_2$, the two-point galaxy correlation function, $\xi_{gg}$, and introduce a new estimator of the galaxy-velocity cross-correlation function, $\psi_3$. By including pair counts measured from random catalogues of velocities and positions sampled from distributions characteristic of the true data, we find that the variance in the galaxy-velocity cross-correlation function is significantly reduced. Applying a covariance analysis and $\chi^2$ minimisation procedure to these statistics, we determine estimates and errors for the normalised growth rate $f\sigma_8$ and the parameter $\beta = f/b$, where $b$ is the galaxy bias factor. We test this framework on mock hemisphere datasets for redshift $z < 0.1$ with realistic velocity noise constructed from the L-PICOLA simulation code, and find that we are able to recover the fiducial value of $f\sigma_8$ from the joint combination of $\psi_1$ + $\psi_2$ + $\psi_3$ + $\xi_{gg}$, with 15\% accuracy from individual mocks. We also recover the fiducial $f\sigma_8$ to within 1$\sigma$ regardless of the combination of correlation statistics used. When we consider all four statistics together we find that the statistical uncertainty in our measurement of the growth rate is reduced by $59\%$ compared to the same analysis only considering $\psi_2$, by $53\%$ compared to the same analysis only considering $\psi_1$, and by $52\%$ compared to the same analysis jointly considering $\psi_1$ and $\psi_2$. ","Improving estimates of the growth rate using galaxy-velocity
  correlations: a simulation study"
43,1353066623045799939,818436810,Ehsan Hosseini-Asl,"['New paper on,\n- Analyzing calibration in NLU models\n- How to improve it using noise contrastive estimation training \nAccepted to EACL 2021 \n(<LINK>)\n@SFResearch [1/6] <LINK>', 'We explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. [2/6] https://t.co/a4vzRQb3K0', 'In most tasks, all three EBM variants get substantial improvement in ECE with little or no loss in accuracy comparing to the (strong) baseline methods. [3/6] https://t.co/1nIvYVAYWZ', 'We plot how test-set ECE changes during training. It is shown as the training reaches the high-accuracy area, the calibration for baseline model becomes worse, while EBM training is able to reach a better trade-off between accuracy and calibration. [4/6] https://t.co/ZLtuWiikYq', 'How does the model get better calibration? It is shown that models trained with the hidden and sharp-hidden variants tend to assign more conservative predictions (reflected by higher entropy) for higher energy (less likely) samples. [5/6] https://t.co/VcWZFCwual', 'We suspect this is due to the strong coupling between the energy function and the classification logits. We provide concrete examples here. \nHowever, we need to mention that we do not observe this interesting trend (Figure 4) in all datasets (e.g., QNLI) [6/6] https://t.co/7MTVdpldWR']",https://arxiv.org/abs/2101.06829,"In this work, we explore joint energy-based model (EBM) training during the finetuning of pretrained text encoders (e.g., Roberta) for natural language understanding (NLU) tasks. Our experiments show that EBM training can help the model reach a better calibration that is competitive to strong baselines, with little or no loss in accuracy. We discuss three variants of energy functions (namely scalar, hidden, and sharp-hidden) that can be defined on top of a text encoder, and compare them in experiments. Due to the discreteness of text data, we adopt noise contrastive estimation (NCE) to train the energy-based model. To make NCE training more effective, we train an auto-regressive noise model with the masked language model (MLM) objective. ","Joint Energy-based Model Training for Better Calibrated Natural Language
  Understanding Models"
44,1352702989467697157,1389934411,Joe Hogan,"['Bayesian additive regression trees (BART) are extremely popular in stats, ML and AI.  Using them for multinomial outcomes is difficult.\n\nHopkins postdoc Yizhen Xu @YizhenXuuuu has addressed many of these challenges in her new paper.  Check it out here.\n\n<LINK>']",https://arxiv.org/abs/2101.06823,"The multinomial probit Bayesian additive regression trees (MPBART) framework was proposed by Kindo et al. (KD), approximating the latent utilities in the multinomial probit (MNP) model with BART (Chipman et al. 2010). Compared to multinomial logistic models, MNP does not assume independent alternatives and the correlation structure among alternatives can be specified through multivariate Gaussian distributed latent utilities. We introduce two new algorithms for fitting the MPBART and show that the theoretical mixing rates of our proposals are equal or superior to the existing algorithm in KD. Through simulations, we explore the robustness of the methods to the choice of reference level, imbalance in outcome frequencies, and the specifications of prior hyperparameters for the utility error term. The work is motivated by the application of generating posterior predictive distributions for mortality and engagement in care among HIV-positive patients based on electronic health records (EHRs) from the Academic Model Providing Access to Healthcare (AMPATH) in Kenya. In both the application and simulations, we observe better performance using our proposals as compared to KD in terms of MCMC convergence rate and posterior predictive accuracy. ",Inference for BART with Multinomial Outcomes
45,1352677909161529346,151528259,Varun,"['Here is our new paper titled ""Do we need to go Deep? Knowledge Tracing with Big Data"" accepted in @RealAAAI workshop on AI in Education 2021. #KnowledgeTracing #IntelligentTutoringSystems #DeepLearning #EducationalDataMining\n#AIED\n<LINK>']",https://arxiv.org/abs/2101.08349,"Interactive Educational Systems (IES) enabled researchers to trace student knowledge in different skills and provide recommendations for a better learning path. To estimate the student knowledge and further predict their future performance, the interest in utilizing the student interaction data captured by IES to develop learner performance models is increasing rapidly. Moreover, with the advances in computing systems, the amount of data captured by these IES systems is also increasing that enables deep learning models to compete with traditional logistic models and Markov processes. However, it is still not empirically evident if these deep models outperform traditional models on the current scale of datasets with millions of student interactions. In this work, we adopt EdNet, the largest student interaction dataset publicly available in the education domain, to understand how accurately both deep and traditional models predict future student performances. Our work observes that logistic regression models with carefully engineered features outperformed deep models from extensive experimentation. We follow this analysis with interpretation studies based on Locally Interpretable Model-agnostic Explanation (LIME) to understand the impact of various features on best performing model pre-dictions. ",Do we need to go Deep? Knowledge Tracing with Big Data
46,1352579987333902338,1232679142723903491,Tom Holden-Dye,"['My first paper is out!\n\nWe report on a new way to theoretically distinguish between definite and indefinite time, discovered during my MSci with Sandu Popescu @BristolUniPhys last year.\n\nExcited to share the results!\n\n<LINK>']",https://arxiv.org/abs/2101.08739,"By studying the set of correlations that are theoretically possible between physical systems without allowing for signalling of information backwards in time, we here identify correlations that can only be achieved if the time ordering between the systems is fundamentally indefinite. These correlations, if they exist in nature, must result from non-classical, non-deterministic time, and so may have relevance for quantum (or post-quantum) gravity, where a definite global time might not exist. ",Indefinite global time
47,1352460816981737473,562151378,Ben Nachman,"['LHC Olympics paper is now out!  It has been quite a journey &amp; I am exciting to see many new anomaly detection methods.  We also exposed gaps in coverage - new techniques are still needed!  Special thanks to co-organizers @GregorKasieczka and David Shih.  <LINK> <LINK>', ""And thank you also to our great collaborators!!  @OzAmram @jmgduarte @crissms21 @xmpierinix @EricMetodiev @pkomiske @JuliaGonski @vlimant et al. !  Sorry for missing anyone who has a twitter handle and I don't know it!""]",https://arxiv.org/abs/2101.08320,"A new paradigm for data-driven, model-agnostic new physics searches at colliders is emerging, and aims to leverage recent breakthroughs in anomaly detection and machine learning. In order to develop and benchmark new anomaly detection methods within this framework, it is essential to have standard datasets. To this end, we have created the LHC Olympics 2020, a community challenge accompanied by a set of simulated collider events. Participants in these Olympics have developed their methods using an R&D dataset and then tested them on black boxes: datasets with an unknown anomaly (or not). This paper will review the LHC Olympics 2020 challenge, including an overview of the competition, a description of methods deployed in the competition, lessons learned from the experience, and implications for data analyses with future datasets as well as future colliders. ","The LHC Olympics 2020: A Community Challenge for Anomaly Detection in
  High Energy Physics"
48,1352164275196977152,59413748,Reuben Binns,"['Thread summary of new paper (at #CHI2021): ""Exploring Design and Governance Challenges in the Development of Privacy-Preserving Computation"" w/ Nitin Agrawal, @emax, Kim Laine &amp; @Nigel_Shadbolt <LINK>', ""New techniques for 'privacy-preserving computation'(PPC) (inc. homomorphic encryption, secure multi-party computation, differential privacy) present novel questions for HCI, from usability and mental models, to the values they embed/elide, &amp; their social + political implications"", 'We interviewed experts working on PPC, from academia, industry, law &amp; policy, and design, and identified challenges in moving from theory to practice, interdisciplinarity translation, developer usability, explanation, governance and accountability', ""PPCs challenge typical approaches to abstraction and complexity; hiding implementation details behind libraries and APIs makes it difficult for developers to optimise through engineering 'tricks', and to reason sensibly about the parameters that determine security guarantees"", ""While PPCs aim to protect 'privacy', we found this unpacked in subtly different ways, from human rights to corporate secrets and regulatory compliance. Several pointed to challenges in explaining and justifying these systems to decision-makers, stakeholders, and society at large"", ""In addition to studying 'acceptability' for end-users / data subjects, it is equally important to consider the plurality of different actors and contexts through which values like privacy will be understood, traded-off, and embedded in them (or not)"", ""PPC techniques have an aura of mystique. Their construction is more craft than science, and their inner workings and risks can't be easily communicated. There's a risk that they become not just technical but technocratic solutions (we call 'privacy-enhancing technocracy')"", 'Discourse around PPCs may also function to distract us from the ways computation might reinforce existing problematic power structures, or redefine them as ‘privacy’ problems, so that PPCs can be positioned as the solution, while leaving those structures intact', ""@MidasNouwens thanks! These techniques are at a good phase for asking these kinds of questions right now - 'real' enough to study, but not yet entrenched""]",https://arxiv.org/abs/2101.08048,"Homomorphic encryption, secure multi-party computation, and differential privacy are part of an emerging class of Privacy Enhancing Technologies which share a common promise: to preserve privacy whilst also obtaining the benefits of computational analysis. Due to their relative novelty, complexity, and opacity, these technologies provoke a variety of novel questions for design and governance. We interviewed researchers, developers, industry leaders, policymakers, and designers involved in their deployment to explore motivations, expectations, perceived opportunities and barriers to adoption. This provided insight into several pertinent challenges facing the adoption of these technologies, including: how they might make a nebulous concept like privacy computationally tractable; how to make them more usable by developers; and how they could be explained and made accountable to stakeholders and wider society. We conclude with implications for the development, deployment, and responsible governance of these privacy-preserving computation techniques. ","Exploring Design and Governance Challenges in the Development of
  Privacy-Preserving Computation"
49,1352064869831467010,1047159630286213120,Lauren J Beesley,['Simple new method for handling not-at-random missingness **after multiple imputation** through weighted analysis of stacked imputations. \n\nWeights are easy to calculate and R package StackImpute on GitHub gives standard errors!\nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2101.07954,"Not-at-random missingness presents a challenge in addressing missing data in many health research applications. In this paper, we propose a new approach to account for not-at-random missingness after multiple imputation through weighted analysis of stacked multiple imputations. The weights are easily calculated as a function of the imputed data and assumptions about the not-at-random missingness. We demonstrate through simulation that the proposed method has excellent performance when the missingness model is correctly specified. In practice, the missingness mechanism will not be known. We show how we can use our approach in a sensitivity analysis framework to evaluate the robustness of model inference to different assumptions about the missingness mechanism, and we provide R package StackImpute to facilitate implementation as part of routine sensitivity analyses. We apply the proposed method to account for not-at-random missingness in human papillomavirus test results in a study of survival for patients diagnosed with oropharyngeal cancer. ",Accounting for not-at-random missingness through imputation stacking
50,1351967346508890117,196589843,Helga Dénes,['A new Apertif paper on a mega maser that we found by chance in the survey data.\n<LINK>\n#science #apertif'],https://arxiv.org/abs/2101.05818,"We present the serendipitous detection of the two main OH maser lines at 1667 and 1665 MHz associated with IRAS 10597+5926 at z = 0.19612 in the untargeted Apertif Wide-area Extragalactic Survey (AWES), and the subsequent measurement of the OH 1612 MHz satellite line in the same source. With a total OH luminosity of log(L/L_Sun) = 3.90 +/- 0.03, IRAS 10597+5926 is the fourth brightest OH megamaser (OHM) known. We measure a lower limit for the 1667/1612 ratio of R_1612 > 45.9 which is the highest limiting ratio measured for the 1612 MHz OH satellite line to date. OH satellite line measurements provide a potentially valuable constraint by which to compare detailed models of OH maser pumping mechanisms. Optical imaging shows the galaxy is likely a late-stage merger. Based on published infrared and far ultraviolet fluxes, we find that the galaxy is an ultra luminous infrared galaxy (ULIRG) with log(L_TIR/L_Sun) = 12.24, undergoing a star burst with an estimated star formation rate of 179 +/- 40 M_Sun/yr. These host galaxy properties are consistent with the physical conditions responsible for very bright OHM emission. Finally, we provide an update on the predicted number of OH masers that may be found in AWES, and estimate the total number of OH masers that will be detected in each of the individual main and satellite OH 18 cm lines. ","Apertif view of the OH Megamaser IRAS 10597+5926: OH 18 cm satellite
  lines in wide-area HI surveys"
51,1351906274074652676,991380306,James Jackman,"['Today\'s the big day! We have a new paper out! It\'s a study of white-light stellar flares from ""blended and neighbouring stars"" in the Kepler short cadence data (<LINK>). But what does it mean?', ""Every so often in astronomical studies, you spot a flare that doesn't actually come from the type of star you're interested in (e.g. a Sun-like star). Instead, it comes from a faint star just in the edge of your aperture. This can be a nuisance and is thrown out."", 'These intruders can be avoided by using a carefully curated target list (e.g. no stars with close neighbours), or by vetting your data each time you see a signal. But what happens to all those discarded flares? Can we do anything with them?', ""That's what we decided to look into, using the Kepler short cadence data. We realised that some of the discarded flares could be really energetic. After all, the faint star would have to contribute a decent bit of light relative to the bright source just for the flare to be seen."", 'So, we reanalysed the Kepler SC data, looking for flares that would have been thrown out by other studies. We found that about 7% of flares in the Kepler SC data come from these interlopers.', 'We found that some of the largest flares in our sample pushed the energies that had been seen before with Kepler, and measured the flare rates of some of these stars. The highest amplitude flare changed the brightness of the star by 4 magnitudes in the Kepler bandpass!', 'Looking for flares from stars close to other stars also meant we found flares on wide binary systems! We found that in this co-eval systems, the lower mass stars flared more often, as we expected from previous works.', ""This was a fun paper to work on and it highlights that while it's good to have a clean sample, sometimes it's worth double checking the things you throw away. Enjoy :) https://t.co/LyN3CWfehp""]",https://arxiv.org/abs/2101.07269,"We present the results of a search for stellar flares from stars neighbouring the target sources in the Kepler short cadence data. These flares have been discarded as contaminants in previous surveys and therefore provide an unexplored resource of flare events, in particular high energy events from faint stars. We have measured M dwarf flare energies up to 1.5$\times$10^35 erg, pushing the limit for flare energies measured using Kepler data. We have used our sample to study theflaring activity of wide binaries, finding that the lower mass counterpart in a wide binary flares more often at a given energy. Of the 4430 flares detected in our original search, 298 came from a neighbouring star, a rate of 6.7$\pm$0.4 per cent for the Kepler short cadence lightcurves. We have used our sample to estimate a 5.8$\pm$0.1 per cent rate of false positive flare events in studies using TESS short cadence data. ","Stellar flares from blended and neighbouring stars in Kepler short
  cadence observations"
52,1351838478879879168,50901426,Rafael Alves Batista,"['New paper on #arXiv today, with my collaborators @AstroUSP. <LINK>\nWe study the production of high-energy neutrinos in clusters of galaxies. We show that it contributes to a fraction of the flux measured by IceCube. #paperday <LINK>']",https://arxiv.org/abs/2101.07702,"Clusters of galaxies can potentially produce cosmic rays (CRs) up to very-high energies via large-scale shocks and turbulent acceleration. Due to their unique magnetic-field configuration, CRs with energy $\leq 10^{17}$ eV can be trapped within these structures over cosmological time scales, and generate secondary particles, including neutrinos and gamma rays, through interactions with the background gas and photons. In this work, we compute the contribution from clusters of galaxies to the diffuse neutrino background. We employ three-dimensional cosmological magnetohydrodynamical simulations of structure formation to model the turbulent intergalactic medium. We use the distribution of clusters within this cosmological volume to extract the properties of this population, including mass, magnetic field, temperature, and density. We propagate CRs in this environment using multi-dimensional Monte Carlo simulations across different redshifts (from $z \sim 5$ to $z =0$), considering all relevant photohadronic, photonuclear, and hadronuclear interaction processes. We find that, for CRs injected with a spectral index $\alpha = 1.5 - 2.7$ and cutoff energy $E_\text{max} = 10^{16} - 5\times10^{17} \; \text{eV}$, clusters contribute to a sizeable fraction to the diffuse flux observed by the IceCube Neutrino Observatory, but most of the contribution comes from clusters with $M \gtrsim 10^{14} \; M_{\odot}$ and redshift $ z \lesssim 0.3$. If we include the cosmological evolution of the CR sources, this flux can be even higher. ",High-Energy Neutrino Production in Clusters of Galaxies
53,1351811220614090752,970380086036791296,Dr. Melissa van Beekveld,"['New paper is out! And it is all about logarithms! Those nasty things were invented ~450 years ago, but still a topic of active research: they have the annoying property that they spoil the validity of our particle-physics predicitions... \n\n<LINK>', 'But despair not: resummation to the rescue! This technique restores the predictability of the perturbative series. However, depending how you set it up, you actually get a different numerical answer...', 'This is caused by power corrections, which we study in our work. And then the magic happens... If you add the largest contribution of these power corrections, the methods actually agree! https://t.co/DXpH2ImWEe']",https://arxiv.org/abs/2101.07270,"We study next-to-leading-power (NLP) threshold corrections in colour-singlet production processes, with particular emphasis on Drell-Yan (DY) and single-Higgs production. We assess the quality of the partonic and hadronic threshold expansions for each process up to NNLO. We determine numerically the NLP leading-logarithmic (LL) resummed contribution in addition to the leading-power next-to-next-to-leading logarithmic (LP NNLL) resummed DY and Higgs cross sections, matched to NNLO. We find that the inclusion of NLP logarithms is numerically more relevant than increasing the precision to N$^3$LL at LP for these processes. We also perform an analytical and numerical comparison of LP NNLL + NLP LL resummation in soft-collinear effective theory and direct QCD, where we achieve excellent analytical and numerical agreement once the NLP LL terms are included in both formalisms. Our results underline the phenomenological importance of understanding the NLP structure of QCD cross sections. ","Next-to-leading power threshold corrections for finite order and
  resummed colour-singlet cross sections"
54,1351751862190440448,2816527975,Hiroshi Kera,"['My new paper has appeared in arXiv:\n""Border Basis Computation with Gradient-Weighted Norm,""\n<LINK>']",https://arxiv.org/abs/2101.00401,"Normalization of polynomials plays a vital role in the approximate basis computation of vanishing ideals. Coefficient normalization, which normalizes a polynomial with its coefficient norm, is the most common method in computer algebra. This study proposes the gradient-weighted normalization method for the approximate border basis computation of vanishing ideals, inspired by recent developments in machine learning. The data-dependent nature of gradient-weighted normalization leads to better stability against perturbation and consistency in the scaling of input points, which cannot be attained by coefficient normalization. Only a subtle change is needed to introduce gradient normalization in the existing algorithms with coefficient normalization. The analysis of algorithms still works with a small modification, and the time complexity of algorithms remains unchanged. We also prove that, with coefficient normalization, which does not provide the scaling consistency property, scaling of points (e.g., as a preprocessing) can cause an approximate basis computation to fail. This study is the first to theoretically highlight the crucial effect of scaling in approximate basis computation and presents the utility of data-dependent normalization. ",Border basis computation with gradient-weighted normalization
55,1351751690580500482,2816527975,Hiroshi Kera,"['My new paper with @unlimitcycle has appeared in arXiv:\n""Monomial-agnostic computation of vanishing ideals,""\n<LINK>']",https://arxiv.org/abs/2101.00243,"The approximate basis computation of vanishing ideals has recently been studied extensively and exploited in computer algebra and data-driven applications such as machine learning. However, symbolic computation and the dependency on monomial ordering remain essential gaps between the two fields. In this paper, we present the first fully numerical basis computation that efficiently works in a monomial-agnostic (i.e., monomial-ordering free) manner without suffering from the spurious vanishing problem, which is a fundamental problem in evaluating polynomials to construct bases. This is realized by a newly proposed data-dependent normalization, gradient normalization, of polynomials that normalizes a polynomial based on the magnitude of gradients. The data-dependent nature of gradient normalization brings various significant advantages: (i) it resolves the spurious vanishing problem without accessing the coefficients of terms, resulting in a drastically small basis; (ii) it provides the basis computation with a scaling consistency that ensures that input scaling does not lead to an essential change in the output; and (iii) it is provably robust against input perturbations, where the upper bound of error is determined only by the magnitude of the perturbations. Existing studies did not achieve any of these. As further applications of gradient information, we propose a monomial-agnostic basis reduction method to remove redundant polynomials and a regularization method to manage positive-dimensional ideals. We confirmed the effectiveness of (i)--(iii) through numerical experiments. ",Monomial-agnostic computation of vanishing ideals
56,1351623115923619840,3327515352,M.P. Ross,['New paper from our group describing our 2.8 mHz torsion balance whose angle is measured with a modified Michelson interferometer with ~10 picorad sensitivity. This apparatus has a range of applications including geophysics and gravitational wave astronomy.\n<LINK>'],https://arxiv.org/abs/2101.01243,"We describe a torsion pendulum with a large mass-quadrupole moment and a resonant frequency of 2.8 mHz, whose angle is measured using a modified Michelson interferometer. The system achieved noise levels of $\sim200\ \text{prad}/\sqrt{\text{Hz}}$ between 0.2-30 Hz and $\sim10\ \text{prad}/\sqrt{\text{Hz}}$ above 100 Hz. Such a system can be applied to a broad range of fields from the study of rotational seismic motion and elastogravity signals to gravitational wave observation and tests of gravity. ",A Low-Frequency Torsion Pendulum with Interferometric Readout
57,1351605561482235904,1737674246,Evan Goldstein,"['""A multi-modal approach towards mining social media data during natural disasters - a case study of Hurricane Irma""\nour new paper, led by @chk_null \n\njournal: <LINK>\npreprint: <LINK>\ndata: <LINK>']",https://arxiv.org/abs/2101.00480,"Streaming social media provides a real-time glimpse of extreme weather impacts. However, the volume of streaming data makes mining information a challenge for emergency managers, policy makers, and disciplinary scientists. Here we explore the effectiveness of data learned approaches to mine and filter information from streaming social media data from Hurricane Irma's landfall in Florida, USA. We use 54,383 Twitter messages (out of 784K geolocated messages) from 16,598 users from Sept. 10 - 12, 2017 to develop 4 independent models to filter data for relevance: 1) a geospatial model based on forcing conditions at the place and time of each tweet, 2) an image classification model for tweets that include images, 3) a user model to predict the reliability of the tweeter, and 4) a text model to determine if the text is related to Hurricane Irma. All four models are independently tested, and can be combined to quickly filter and visualize tweets based on user-defined thresholds for each submodel. We envision that this type of filtering and visualization routine can be useful as a base model for data capture from noisy sources such as Twitter. The data can then be subsequently used by policy makers, environmental managers, emergency managers, and domain scientists interested in finding tweets with specific attributes to use during different stages of the disaster (e.g., preparedness, response, and recovery), or for detailed research. ","A multi-modal approach towards mining social media data during natural
  disasters -- a case study of Hurricane Irma"
58,1351597055026860034,970337853912821760,Teddy Koker,"['New paper with @galaxygarden23, Tom Titcombe, @GKaissis at @gridai_ @openminedorg\n\nWe train a model to dynamically apply noise to images, learning which pixels of the image are necessary for downstream performance\n\nPaper: <LINK>\nCode: <LINK>\n\n1/n <LINK>', 'The trained model can then be used for interpretability, providing saliency maps that are qualitatively better than other interpretability techniques. (Shown on pancreas segmentation task)\n\n2/n https://t.co/FSxZnJLAvm', ""Additionally, we can quantify our interpretability model's performance by measuring its ability to occlude portions of the image while maintaining downstream performance.\n\n3/3 https://t.co/730oYZdEpP""]",https://arxiv.org/abs/2101.05791,"Deep Neural Networks (DNNs) are widely used for decision making in a myriad of critical applications, ranging from medical to societal and even judicial. Given the importance of these decisions, it is crucial for us to be able to interpret these models. We introduce a new method for interpreting image segmentation models by learning regions of images in which noise can be applied without hindering downstream model performance. We apply this method to segmentation of the pancreas in CT scans, and qualitatively compare the quality of the method to existing explainability techniques, such as Grad-CAM and occlusion sensitivity. Additionally we show that, unlike other methods, our interpretability model can be quantitatively evaluated based on the downstream performance over obscured images. ",U-Noise: Learnable Noise Masks for Interpretable Image Segmentation
59,1351522632500441092,1138762581164855298,Christoph Ternes,"['New paper today, <LINK> . We perform a critical analysis of Neutrino-4 data and discuss the preference for active-sterile neutrino oscillations claimed by the experiment. Check it out!']",https://arxiv.org/abs/2101.06785,"We present a deep study of the Neutrino-4 data aimed at finding the statistical significance of the large-mixing short-baseline neutrino oscillation signal claimed by the Neutrino-4 collaboration at more than $3\sigma$. We found that the results of the Neutrino-4 collaboration can be reproduced approximately only by neglecting the effects of the energy resolution of the detector. Including these effects, we found that the best fit is obtained for a mixing that is even larger, close to maximal, but the statistical significance of the short-baseline neutrino oscillation signal is only about $2.7\sigma$ if evaluated with the usual method based on Wilks' theorem. We show that the large Neutrino-4 mixing is in strong tension with the KATRIN, PROSPECT, STEREO, and solar $\nu_{e}$ bounds. Using a more reliable Monte Carlo simulation of a large set of Neutrino-4-like data, we found that the statistical significance of the Neutrino-4 short-baseline neutrino oscillation signal decreases to about $2.2\sigma$. We also show that it is not unlikely to find a best-fit point that has a large mixing, even maximal, in the absence of oscillations. Therefore, we conclude that the claimed Neutrino-4 indication in favor of short-baseline neutrino oscillations with very large mixing is rather doubtful. ",Neutrino-4 anomaly: oscillations or fluctuations?
60,1351360048988123136,32965031,Sami Douba,"['New paper on arXiv 🙃 A corollary of the main result is that the fundamental group of the mapping torus of a Dehn twist on a closed oriented surface of positive genus does not embed in a compact Lie group. Comments welcome. <LINK>', '@littmath If the mapping class is pseudo-Anosov (and the surface has genus &gt; 1), the mapping torus is a closed hyperbolic 3-manifold. The fundamental group of such a manifold is virtually special (Agol) and hence embeds in a compact Lie group (also Agol): https://t.co/MtyOFpPoU0', '@ryleealanza Thanks 💜', ""@littmath Yes; these groups are residually finite. However, the image of an element representing that curve under any representation into a compact Lie group will have finite order (and that's how I concluded that no such representation is faithful)"", '@littmath Thanks for the interest!', '@AndyPutmanMath @MarissaKawehi Thank you!', ""@agolian Button's sequence of papers in which he proves that fact and several similar ones were my starting point for this project. It's not immediately clear to me how the two facts are related, or whether there is a unified way to approach/prove them."", ""@MachineInf @littmath I think it's pretty cool!"", ""@agolian What I show in the case of the mapping torus of a Dehn twist about a curve [c] is that the image of c under any finite-dimensional representation of pi_1(fibration) is quasi-unipotent. This won't be true for the monodromy (the cyclic subgroup generated by the latter is a retract)"", ""@agolian That detail is not contained in the abstract (it's buried somewhere in the introduction; maybe I should change that). Thanks!""]",https://arxiv.org/abs/2101.06797,"Let $M$ be a graph manifold containing a single JSJ torus $T$ and whose JSJ blocks are of the form $\Sigma \times S^1$, where $\Sigma$ is a compact orientable surface with boundary. We show that if $M$ does not admit a Riemannian metric of everywhere nonpositive sectional curvature, then there is an essential curve on $T$ such that any finite-dimensional linear representation of $\pi_1(M)$ maps an element representing that curve to a matrix all of whose eigenvalues are roots of $1$. In particular, this shows that $\pi_1(M)$ does not admit a faithful finite-dimensional unitary representation, and gives a new proof that $\pi_1(M)$ is not linear over any field of positive characteristic. ",Virtually unipotent curves in some non-NPC graph manifolds
61,1351355543634518016,1230320689250553856,Christian Malacaria,"['New paper today! X-ray pulsar XTE J1858 observed with @NuSTAR_Science &amp; #Fermi/GBM. Also a cyclotron line and a new distance value. Thanks to a great team of co-authors &amp; the amazing instruments that allowed it! Accepted in ApJ, now available on @arXiver <LINK> <LINK>']",https://arxiv.org/abs/2101.07020,"Accreting X-ray pulsars (XRPs) undergo luminous X-ray outbursts during which the spectral and timing behavior of the neutron star can be studied in detail. We analyze a $NuSTAR$ observation of the XRP XTE J1858+034 during its outburst in 2019. The spectrum is fit with a phenomenological, a semi-empirical and a physical spectral model. A candidate cyclotron line is found at $48\,$keV, implying a magnetic field of $5.4\times10^{\rm 12}\,$G at the site of emission. This is also supported by the physical best-fit model. We propose an orbital period of about $81$ days based on the visual inspection of the X-ray outbursts recurrence time. Based on $Fermi$ Gamma-ray Burst Monitor data, the standard disk accretion-torque theory allowed us to infer a distance of $10.9\pm1.0\,$kpc. Pulse profiles are single-peaked and show a pulsed fraction that is strongly energy-dependent at least up to $40$ keV. ","The X-ray pulsar XTE J1858+034 observed with NuSTAR and Fermi/GBM:
  spectral and timing characterization plus a cyclotron line"
62,1351349657771810817,767659609,Yoshihiko Hasegawa,"['My new paper ""Precision enhancement via symmetry breaking in quantum Markov process"" appeared in arXiv: <LINK>']",https://arxiv.org/abs/2101.06831,"Entropy production characterizes irreversibility. This viewpoint allows us to consider the thermodynamic uncertainty relation, which states that a higher precision can be achieved at the cost of higher entropy production, as a relation between precision and irreversibility. Considering the original and perturbed dynamics, we show that the precision of an arbitrary counting observable in continuous measurement of quantum Markov processes is bounded from below by Loschmidt echo between the two dynamics, representing the irreversibility of quantum dynamics. When considering particular perturbed dynamics, our relation leads to several thermodynamic uncertainty relations, indicating that our relation provides a unified perspective on classical and quantum thermodynamic uncertainty relations. ","Irreversibility, Loschmidt echo, and thermodynamic uncertainty relation"
63,1351130958020440068,3131128329,Dr Heidi Thiemann,"['New paper day (and my second first author paper)! \n\nWe analysed the first 1 million classifications from our Zooniverse project, SuperWASP Variable Stars, finding 301 brand stellar variables, and a bunch of other exciting stars. 🌟\n\nCheck it out: <LINK> <LINK>', ""SuperWASP 🔭 observed the entire night sky for over a decade, originally hunting for exoplanets, but it's also fantastic for stellar variability studies. @ajnorton3 reanalysed the archive of ~30 million light curves, finding ~1.6 million candidate light curves of variable stars. https://t.co/wAjFCYjgcC"", ""To classify those ~1.6 million light curves, we used @the_zooniverse and asked citizen scientists to take part in a pattern matching task designed to find broad variable types. Here's an example of a lovely contact eclipsing binary! https://t.co/XKiKRHN3lS"", 'Fancy classifying a few variable stars? You can check out the project here: https://t.co/fIMXadpKop', ""Anyway, we found out loads, including how accurate the classifications are for different variable types and the discovery of 301 brand new stellar variables, including contact binaries near the short period cut off and a new binary configuration (but that's a future paper!)."", '@chrislintott @ajnorton3 @AstroAdamMc @the_zooniverse @OU_SPS @SuperWASP_stars Thanks Chris!', '@ScienceSocks @ajnorton3 @AstroAdamMc @the_zooniverse @OU_SPS @SuperWASP_stars Thank you!', ""We're now working on some exciting additions to the @SuperWASP_stars project, including bringing in machine learning, adding new workflows, and the fantastic @AstroAdamMc is building a new user interface so the variable star classifications and data will be available to all."", 'So, watch this space for more variable star news. 🌟', 'I meant ""brand new"" but I apparently can\'t type. Oh well. 🤦\u200d♀️', '@LauraForczyk @ajnorton3 @AstroAdamMc @the_zooniverse @OU_SPS @SuperWASP_stars Thanks Laura!']",http://arxiv.org/abs/2101.06216,"We present the first analysis of results from the SuperWASP Variable Stars Zooniverse project, which is aiming to classify 1.6 million phase-folded light curves of candidate stellar variables observed by the SuperWASP all sky survey with periods detected in the SuperWASP periodicity catalogue. The resultant data set currently contains $>$1 million classifications corresponding to $>$500,000 object-period combinations, provided by citizen scientist volunteers. Volunteer-classified light curves have $\sim$89 per cent accuracy for detached and semi-detached eclipsing binaries, but only $\sim$9 per cent accuracy for rotationally modulated variables, based on known objects. We demonstrate that this Zooniverse project will be valuable for both population studies of individual variable types and the identification of stellar variables for follow up. We present preliminary findings on various unique and extreme variables in this analysis, including long period contact binaries and binaries near the short-period cutoff, and we identify 301 previously unknown binaries and pulsators. We are now in the process of developing a web portal to enable other researchers to access the outputs of the SuperWASP Variable Stars project. ",SuperWASP Variable Stars: Classifying Light Curves Using Citizen Science
64,1350984686131150849,280687108,Prof Ferah Munshi,"['New paper alert! On arXiv tonight is my latest paper with in collaboration with @elaadapplebaum, Alyson Brooks and Charlotte Christensen. In this paper we introduce, for the first time, the **full** MARVELous and DC Justice League dwarf sample! <LINK>', 'With this sample, we show the stellar to halo mass (SMHM) relationship for dwarf galaxies down to ultra-faint magnitudes.  Check it out- 211 dwarfs! https://t.co/W0g9IoYkm1', 'You’ll notice that as we approach fainter and fainter magnitudes (lower and lower masses), the scatter in the SMHM increases both at z=0 and at M_peak- that is what we quantify in this paper.  At z=0 the large scatter is due to stripping. So what is interesting about this?', 'We fit the scatter at M_peak for the halos that you see on the SMHM, but we also recognize that what’s on that plot is resolution-dependent.  A lot of halos that are “dark” in our simulations probably host a galaxy with a mass below the mass resolution.', 'In fact, recent work shows that it is likely that **all** halos in the mass range we probe should host a galaxy. It follows then that we aren’t truly quantifying scatter if we use only our resolved halos.  Uh oh! What should we do? https://t.co/yKQSqpURWV', 'From our resolved halos, we note that a log-normal growing scatter fits our scatter, and is also commonly adopted in the literature.  Using that, we probabilistically populate dark halos with galaxies that we are missing because of our limited resolution.', 'In doing so, we are able to derive an “intrinsic” slope and scatter for the SMHM- which is significantly **steeper** than what we get with just the resolved galaxies. Why do we care? https://t.co/iljvWc9FLP', 'At the faintest end of the SMHM relation probed by our simulations, a galaxy cannot be assigned a unique halo mass based solely on its luminosity. With a scatter on the order of 1 dex at the faint end, a given dark matter halo can host a wide range of stellar masses/luminosities.', 'It also turns out that the growing scatter we predict *steepens* the faint end of the stellar mass function (SMF). @elaadapplebaum created an applet with a toy model to help visualize how scatter in the SMHM, occupation fraction, and the SMF are related-https://t.co/bvBIFEU8RH https://t.co/gBD1cSEqpj', 'This paper is in a series of papers where we explore how simulations can (and cannot) constrain the faintest galaxies (e.g. Munshi+ 2019). As we begin probing masses where dwarfs cannot self-regulate, it is key that we understand the limitations of our models.', 'With the Vera Rubin Observatory &amp; Roman Space telescope coming soon, and DES discovering fainter and fainter dwarfs understanding how ultra-faints populate their dark matter halos is paramount.']",https://arxiv.org/abs/2101.05822,"We predict the stellar mass -- halo mass (SMHM) relationship for dwarf galaxies, using simulated galaxies with peak halo masses of M$_{\rm peak} = 10^{11}$ M$_{\odot}$ down into the ultra-faint dwarf range to M$_{\rm peak} =$ 10$^7$ M$_{\odot}$. Our simulated dwarfs have stellar masses of M$_{\rm star} = $ 790 M$_{\odot}$ to $8.2 \times 10^8$ M$_{\odot}$, with corresponding $V$-band magnitudes from $-2$ to $-18.5$. For M$_{\rm peak} > 10^{10}$ M$_{\odot}$, the simulated SMHM relationship agrees with literature determinations, including exhibiting a small scatter of 0.3 dex. However, the scatter in the SMHM relation increases for lower-mass halos. We first present results for well-resolved halos that contain a simulated stellar population, but recognize that whether a halo hosts a galaxy is inherently mass resolution dependent. We thus adopt a probabilistic model to populate ""dark"" halos below our resolution limit to predict an ""intrinsic"" slope and scatter for the SMHM relation. We fit linearly growing log-normal scatter in stellar mass, which grows to more than 1 dex at M$_{\rm peak}$ $=$ 10$^8$ M$_{\odot}$. At the faintest end of the SMHM relation probed by our simulations, a galaxy cannot be assigned a unique halo mass based solely on its luminosity. Instead, we provide a formula to stochastically populate low-mass halos following our results. Finally, we show that our growing log-normal scatter steepens the faint-end slope of the predicted stellar mass function. ",Quantifying scatter in galaxy formation at the lowest masses
65,1350670066774204416,486471674,Sandareka Wickramanayake,"['Excited to share our new work: ""Learning Semantically Meaningful Features for Interpretable Classifications"". In this work, we build a fully interpretable CNN via learning semantically meaningful features.    \nPaper: <LINK>\n#XAI #DeepLearning #AI']",https://arxiv.org/abs/2101.03919,"Learning concepts that are consistent with human perception is important for Deep Neural Networks to win end-user trust. Post-hoc interpretation methods lack transparency in the feature representations learned by the models. This work proposes a guided learning approach with an additional concept layer in a CNN- based architecture to learn the associations between visual features and word phrases. We design an objective function that optimizes both prediction accuracy and semantics of the learned feature representations. Experiment results demonstrate that the proposed model can learn concepts that are consistent with human perception and their corresponding contributions to the model decision without compromising accuracy. Further, these learned concepts are transferable to new classes of objects that have similar concepts. ",Comprehensible Convolutional Neural Networks via Guided Concept Learning
66,1350126549270470666,742698661818339329,Markus Löning,"['What are the key software design patterns that are used in the development of #MachineLearning #Toolkits like @scikit_learn, #Weka or #mlr3? We describe some of the common patterns in our new paper: <LINK> This is a first draft - feedback very welcome!']",https://arxiv.org/abs/2101.04938,"Machine learning (ML) and AI toolboxes such as scikit-learn or Weka are workhorses of contemporary data scientific practice -- their central role being enabled by usable yet powerful designs that allow to easily specify, train and validate complex modeling pipelines. However, despite their universal success, the key design principles in their construction have never been fully analyzed. In this paper, we attempt to provide an overview of key patterns in the design of AI modeling toolboxes, taking inspiration, in equal parts, from the field of software engineering, implementation patterns found in contemporary toolboxes, and our own experience from developing ML toolboxes. In particular, we develop a conceptual model for the AI/ML domain, with a new type system, called scientific types, at its core. Scientific types capture the scientific meaning of common elements in ML workflows based on the set of operations that we usually perform with them (i.e. their interface) and their statistical properties. From our conceptual analysis, we derive a set of design principles and patterns. We illustrate that our analysis can not only explain the design of existing toolboxes, but also guide the development of new ones. We intend our contribution to be a state-of-art reference for future toolbox engineers, a summary of best practices, a collection of ML design patterns which may become useful for future research, and, potentially, the first steps towards a higher-level programming paradigm for constructing AI. ","Designing Machine Learning Toolboxes: Concepts, Principles and Patterns"
67,1350123302816260105,1077995761487568896,Jon Miller,"['New paper day!  Work by Luigi Gallo of @smuhalifax has found an eclipse of the massive black hole in the heart of NGC 6814, using @ESA_XMM.  @BonnieTylerUK might even like this result.\n<LINK> <LINK>']",https://arxiv.org/abs/2101.05433,"We report the detection of a rapid occultation event in the nearby Seyfert galaxy NGC 6814, simultaneously captured in a transient light curve and spectral variability. The intensity and hardness ratio curves capture distinct ingress and egress periods that are symmetric in duration. Independent of the selected continuum model, the changes can be simply described by varying the fraction of the central engine that is covered by transiting obscuring gas. Together, the spectral and timing analyses self-consistently reveal the properties of the obscuring gas, its location to be in the broad line region (BLR), and the size of the X-ray source to be ~25 rg . Our results demonstrate that obscuration close to massive black holes can shape their appearance, and can be harnessed to measure the active region that surrounds the event horizon. ",Eclipsing the X-ray emitting region in the active galaxy NGC 6814
68,1350052177247612928,986408509,Kirpal Nandra,['Exciting new paper from MPE PhD student Julien Wolf using @eROSITA_SRG to constrain the abundance of the most distant X-ray emitting supermassive black holes:  <LINK>'],https://arxiv.org/abs/2101.05585,"We searched for high-z quasars within the X-ray source population detected in the contiguous $\sim 140^2$ eFEDS field observed by eROSITA during the performance verification phase. We collected the available spectroscopic information in the field, including the sample of all currently known optically selected z>5.5 quasars and cross-matched secure Legacy DR8 counterparts of eROSITA-detected X-ray point-like sources with this spectroscopic sample. We report the X-ray detection of an eROSITA source securely matched to the well-known quasar SDSS J083643.85+005453.3 (z=5.81). The soft X-ray flux of the source derived from eROSITA is consistent with previous Chandra observations. In addition, we report the detection of the quasar with LOFAR at 145 MHz and ASKAP at 888 MHz. The reported flux densities confirm a spectral flattening at lower frequencies in the emission of the radio core, indicating that the quasar could be a (sub-) gigahertz peaked spectrum source. The inferred spectral shape and the parsec-scale radio morphology of SDSS J083643.85+005453.3 suggest that it is in an early stage of its evolution into a large-scale radio source or confined in a dense environment. We find no indications for a strong jet contribution to the X-ray emission of the quasar, which is therefore likely to be linked to accretion processes. The detection of this source allows us to place the first constraints on the XLF at z>5.5 based on a secure spectroscopic redshift. Compared to extrapolations from lower-redshift observations, this favours a relatively flat slope for the XLF at $z\sim 6$ beyond $L_*$. The population of X-ray luminous AGNs at high redshift may be larger than previously thought. From our XLF constraints, we make the conservative prediction that eROSITA will detect $\sim 90$ X-ray luminous AGNs at redshifts 5.7<z<6.4 in the full-sky survey (De+RU). ","First constraints on the AGN X-ray luminosity function at $z \sim 6$
  from an eROSITA-detected quasar"
69,1350021630249066496,1482462144,Mika Vesterinen,"['Fun new paper <LINK> with M. Pili @WJBarter on controlling curvature biases from tiny mis-alignments, which could otherwise be a bottleneck in precision-EW measurements with @LHCbExperiment', 'Credit to my Ph.D supervisor, T. Wyatt, for suggesting the  ""pseudomass"" of Z decays within this D0 experiment analysis https://t.co/zVNG7AiwmL . In the new work we show that this variable provides a simple and effective way to see tiny, micron-level, mis-alignments https://t.co/D2Er2Ahahc']",https://arxiv.org/abs/2101.05675,"A new data-driven method, using $Z \rightarrow \mu^+ \mu^-$ decays, is proposed to correct for charge-dependent curvature biases in spectrometer experiments at hadron colliders. The method is studied assuming a detector with a ""forward-spectrometer"" geometry similar to that of the LHCb experiment, and is shown to reliably control several simplified detector mis-alignment configurations. The applicability of the method for use in measurements of precision electroweak observables is evaluated. ","A simple method to determine charge-dependent curvature biases in track
  reconstruction in hadron collider experiments"
70,1349904802281615360,21055092,Lucas Hunt,['New paper on Luminous Compact Blue Galaxies. (LCBGs)! A rapidly evolving class of small star forming galaxies. We see a lot of them at z=1 and not so many locally. This paper is the first step to figuring out where they go and why! <LINK>'],https://arxiv.org/abs/2101.05342,"Luminous Compact Blue Galaxies (LCBGs) are compact, star-forming galaxies that are rarely observed in the local universe but abundant at z=1. This increase in LCBG number density over cosmic lookback time roughly follows the increase in the star formation rate density of the universe over the same period. We use publicly available data in the COSMOS field to study the evolution of the largest homogeneous sample of LCBGs to date by deriving their luminosity function in four redshift bins over the range $0.1\leq~z\leq1$. We find that over this redshift range, the characteristic luminosity (M$^{*}$) increases by $\sim$0.2 mag, and the number density increases by a factor of four. While LCBGs make up only about $18\%$ of galaxies more luminous than M$_{B}=-$18.5 at $z\sim0.2$, they constitute roughly $54\%$ at z$\sim$0.9. The strong evolution in number density indicates that LCBGs are an important population of galaxies to study in order to better understand the decrease in the star formation rate density of the universe since $z\sim1$. ","The Evolution of the Luminosity Function for Luminous Compact Blue
  Galaxies to z=1"
71,1349895805360984064,952949678533849088,Kareem El-Badry,"['New paper on binaries with #GaiaMission data. All the binaries! <LINK> <LINK>', 'A neat thing about binaries is that you have a strong prior that the two stars are at the same distance. This makes binaries very useful for measuring parallax uncertainties. The result: Gaia parallaxes are pretty darn good, but uncertainties are underestimated for bright stars. https://t.co/vktI6gQgUX', '@TM_Eubanks We think it\'s related to the different ""window class"" (pixel sampling scheme around detected sources) used at 11 &lt; G &lt; 13. It also leads to a sharp feature in the parallax zeropoint (see https://t.co/TE0TUHIK4A).']",https://arxiv.org/abs/2101.05282,"We construct from Gaia eDR3 an extensive catalog of spatially resolved binary stars within $\approx$ 1 kpc of the Sun, with projected separations ranging from a few au to 1 pc. We estimate the probability that each pair is a chance alignment empirically, using the Gaia catalog itself to calculate the rate of chance alignments as a function of observables. The catalog contains 1.3 (1.1) million binaries with >90% (>99%) probability of being bound, including 16,000 white dwarf -- main sequence (WD+MS) binaries and 1,400 WD+WD binaries. We make the full catalog publicly available, as well as the queries and code to produce it. We then use this sample to calibrate the published Gaia DR3 parallax uncertainties, making use of the binary components' near-identical parallaxes. We show that these uncertainties are generally reliable for faint stars ($G\gtrsim 18$), but are underestimated significantly for brighter stars. The underestimates are generally $\le 30\%$ for isolated sources with well-behaved astrometry, but are larger (up to 80%) for apparently well-behaved sources with a companion within $\lesssim 4$ arcsec, and much larger for sources with poor astrometric fits. We provide an empirical fitting function to inflate published $\sigma_{\varpi}$ values for isolated sources. The public catalog offers wide ranging follow-up opportunities: from calibrating spectroscopic surveys, to precisely constraining ages of field stars, to the masses and the initial-final mass relation of white dwarfs, to dynamically probing the Galactic tidal field. ","A million binaries from Gaia eDR3: sample selection and validation of
  Gaia parallax uncertainties"
72,1349784433109340160,107936751,Karan Goel,"['🚀Excited to release Robustness Gym, a new Python evaluation toolkit for evaluating the robustness of NLP models, as part of a collaboration between Stanford, Salesforce Research and UNC Chapel-Hill.\n\nPaper: <LINK>\nCode: <LINK>\n\npip install!', ""Our (ongoing) work is motivated by a simple question: why isn't it easier to throughly evaluate a model and understand its limitations?\n\nRobustness Gym answers this in a single toolkit for doing broad evaluation, creating versioned test benches and generating reports. https://t.co/7jvEhgYuia"", 'The evaluations we support capture a lot of what folks do today e.g. \n- filtering to subpopulations\n- transforming examples \n..and more. \n\nOur hope is that having these all in one place can make the evaluation workflow easier, and support interesting analyses.', ""Our paper goes into the challenges of evaluation that we address with RG: it's hard to understand what to evaluate, how to evaluate, what to report. \n\nWe don't have all the answers yet, and we'd love to engage on follow-up research questions."", ""🧑\u200d💻We'd love users and contributors! \n\nOur code is in alpha, and we want to build out depth for specific tasks going forward. DM/reach out to me if you'd like to participate in this effort."", ""We'll have more tutorials/features released in the coming weeks/months. Stay tuned!\n\nThanks to my awesome coauthors and collaborators: @nazneenrajani, @jesse_vig, @mohitban47 and my advisor @HazyResearch. \n\nAlso thanks to the many who gave feedback to us, keep it coming!!""]",https://arxiv.org/abs/2101.04840,"Despite impressive performance on standard benchmarks, deep neural networks are often brittle when deployed in real-world systems. Consequently, recent research has focused on testing the robustness of such models, resulting in a diverse set of evaluation methodologies ranging from adversarial attacks to rule-based data transformations. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, Robustness Gym enables practitioners to compare results from all 4 evaluation paradigms with just a few clicks, and to easily develop and share novel evaluation methods using a built-in set of abstractions. To validate Robustness Gym's utility to practitioners, we conducted a real-world case study with a sentiment-modeling team, revealing performance degradations of 18%+. To verify that Robustness Gym can aid novel research analyses, we perform the first study of state-of-the-art commercial and academic named entity linking (NEL) systems, as well as a fine-grained analysis of state-of-the-art summarization models. For NEL, commercial systems struggle to link rare entities and lag their academic counterparts by 10%+, while state-of-the-art summarization models struggle on examples that require abstraction and distillation, degrading by 9%+. Robustness Gym can be found at this https URL ",Robustness Gym: Unifying the NLP Evaluation Landscape
73,1349469399112155139,2629895873,Eduardo Martínez,['New paper: From Poincaré to May: The Genesis of Discrete Dynamics <LINK>'],https://arxiv.org/abs/2101.02036,"In this paper, the origin of discrete dynamics is stated from a historical point of view, as well as its main ideas: fixed and periodic points, chaotic behaviour, bifurcations. This travel will begin with Poincar\'e's work and will finish with May's work, one of the most important scientific papers of 20th century. This paper is based on the M.Sc. thesis ""Simple Permutations, Pasting and Reversing"", written by the first author under the guidance of the second author. ",From Poincar\'e to May: The Genesis of Discrete Dynamics
74,1349427132645089281,66175375,Jason Wang,"['A short twitter thread on our new paper on the PDS 70 protoplanets using the GRAVITY interferometer (<LINK>)! This is one of the first papers from our ExoGRAVITY large program to study all the known directly imaged planets with interferometry <LINK>', 'GRAVITY has amazing astrometric precision because we coherently combine light from telescopes separated by 130 meters. With just two epochs, we could already see that the inner planet has to be slightly eccentric while the outer planet is essentially circular. https://t.co/AHDqQ0RznR', 'Based on dynamical stability arguments, we could also place a ~10 Jupiter mass upper limit on the inner planet. We also find that this configuration is consistent with a 2:1 orbital resonance, but not required for stability. https://t.co/Qdvx2Y2UeU', ""GRAVITY also gives us a K-band spectrum of both planets. We ran A LOT of orbit fits (24 per planet), and found that the models with the best support from the data are dust-extincted planetary atmospheres. We firmly rule out blackbodies for both planets. Here's PDS 70 c: https://t.co/glMUJ7jlOv"", 'We also did a super cool experiment to try to spatially resolve the circumplanetary environment of the protoplanets! We achieved sub-au resolution in our GRAVITY observations, but unfortunately did not resolve either planet. Any bright disk would have to be smaller than 0.3 au. https://t.co/q2stzAHtq7', 'Forgot to add: this was work done with @ArthurVigan @SylvestreLacour @PlanetaryGao, a bunch of others not on twitter, and the whole GRAVITY team for helping with making these observations happen!', '@AstroThayne Nope. We basically found in the paper that we see no evidence of circumplanetary emission in the NIR data (maybe extinction though), but that the ALMA emission is totally consistent with cooler dust that is invisible at these shorter wavelengths.']",https://arxiv.org/abs/2101.04187,"We present K-band interferometric observations of the PDS 70 protoplanets along with their host star using VLTI/GRAVITY. We obtained K-band spectra and 100 $\mu$as precision astrometry of both PDS 70 b and c in two epochs, as well as spatially resolving the hot inner disk around the star. Rejecting unstable orbits, we found a nonzero eccentricity for PDS 70 b of $0.17 \pm 0.06$, a near-circular orbit for PDS 70 c, and an orbital configuration that is consistent with the planets migrating into a 2:1 mean motion resonance. Enforcing dynamical stability, we obtained a 95% upper limit on the mass of PDS 70 b of 10 $M_\textrm{Jup}$, while the mass of PDS 70 c was unconstrained. The GRAVITY K-band spectra rules out pure blackbody models for the photospheres of both planets. Instead, the models with the most support from the data are planetary atmospheres that are dusty, but the nature of the dust is unclear. Any circumplanetary dust around these planets is not well constrained by the planets' 1-5 $\mu$m spectral energy distributions (SEDs) and requires longer wavelength data to probe with SED analysis. However with VLTI/GRAVITY, we made the first observations of a circumplanetary environment with sub-au spatial resolution, placing an upper limit of 0.3~au on the size of a bright disk around PDS 70 b. ",Constraining the Nature of the PDS 70 Protoplanets with VLTI/GRAVITY
75,1349417287804194821,382008009,Miles Cranmer,"['Very excited to present our new work: we adapt Bayesian neural networks to predict the dissolution of compact planetary systems, a variant of the three-body problem!\n\nBlogpost/code: <LINK>\nPaper: <LINK>\nAPI: <LINK>\n\nThread: 👇 <LINK>', 'Work with: @astrodantamayo (@Princeton) @hannorein (@UofTAstro) @PeterWBattaglia (@DeepMind) Sam Hadden (@CenterForAstro) @philip_armitage @cosmo_shirley @DavidSpergel (@FlatironInst @FlatironCCA)\n\n(2/n)', 'Despite over three hundred years of effort, no solutions exist for predicting when a general planetary configuration will become unstable. Such systems are chaotic: shifting a planet by a centimeter can change its lifetime by a factor of three!\n\n(3/n) https://t.co/MMVKmhRWOy', 'What systems go unstable and experience a collision or ejection, and at what time will this occur? Being able to answer these questions will help improve our understanding of planet formation, and help constrain the parameters of observed exoplanet systems.\n\n(4/n) https://t.co/DxNUMiN4PA', 'Whereas previous ML models (https://t.co/AeJ1g8rtQS) make use of hand-engineered instability metrics inspired by dynamics theory, we learn instability metrics from scratch, using only the raw orbital parameters of a planetary system (https://t.co/ya7EFhRe55):\n\n(5/n) https://t.co/xgXAFXxdqd', 'Early on, we had a lot of difficulty in tackling this problem with the standard ML toolkit: CNNs, LSTMs, GPs, etc. Part of the issue is due to the existence of ""resonances"" in planetary systems: these points straddle very sharp transitions in parameter space.\n\n(6/n)', 'To get the ML to work, we had to introduce two physics-inspired inductive biases.\n1. Chaos =&gt; use a Bayesian neural network with a physics-derived likelihood\n2. Planetary dynamics =&gt; set the learned metrics to be means and variances of a learned coordinate transform\n\n(7/n) https://t.co/pTCJ5dnCvr', 'Our architecture works as follows:\n\nFirst, we record six orbital elements for each of three planets over a short integration of 10,000 orbits, along with their mass.\n\n(8/n) https://t.co/MzbG4g8CIj', 'A neural network takes these twenty-one parameters, and passes them through a learned mapping, converting them to another vector, for each time step. One can consider this essentially a learned coordinate transform.\n\n(9/n) https://t.co/KVv2FoK1vo', 'Common metrics used in other dynamics ML models are also often in the form of means and variances of instantaneous metrics over time.\n\nThus, we do the same pooling operation! We compute both the mean and variance of these learned coordinates over time.\n\n(10/n) https://t.co/1qOkMlPE2E', 'We then use these means and variances in a second neural network to finally predict a distribution over instability time. The specific distribution is a truncated log-normal - this is motivated by empirical results from https://t.co/rVMdGODRER.\n\n(11/n) https://t.co/6RpecYySUa', 'Training both neural networks at the same time means that the instability metrics learned by the first network optimize the predictive power of the second network!\n\n(12/n)', ""Because we deal with chaotic systems, we predict a distribution of instability times. We also want extrapolation uncertainty! We use Bayesian deep learning for this, which allows us to marginalize parameters like so:\n\n(from @yaringal's nice blog on MC dropout):\n\n(13/n) https://t.co/APMSQYFcHd"", 'We use the ""MultiSWAG"" Bayesian neural network to do this (@andrewgwils @Pavel_Izmailov @tim_garipov et al.), which exploits the beautiful connection between stochastic gradient descent and MCMC sampling!\n\n(14/n) https://t.co/At0rACC5DK', 'On our test dataset, we improve by two orders of magnitude (real-time space) over existing analytic estimators, while also reducing the bias of previous ML models by a factor of three. Our estimates also come with error bars, which is important for scientific modelling!\n\n(15/n) https://t.co/uOAmPDBdbX', 'Now, how do we know if our model is naively interpolating the training data, or if it has actually learned some physics? Since we only train on 3-planet systems, a nice test for this is 5-planet data.\n\n(16/n)', 'Despite these systems being completely unlike the training set, our model performs extremely well, surpassing even models designed for that particular dataset!\n\n(17/n) https://t.co/fCBsVmeksa', 'Future work will try to obtain an explicit interpretation of what physics the model has actually learned in its metrics, using symbolic learning: https://t.co/NiO6pNYRFg\n\n(18/n)', 'Check out our interactive demo on Google colab: https://t.co/uYg4RKfiLg, as well as our API: https://t.co/wPNMmTOOiq. Our model is about 10^5 times faster than a numerical integrator, so we hope it will be useful for exploring instability in compact planetary systems!\n\n(19/19)', 'Also, shout out to @ch402 for his many amazing blogs on neural nets. This paragraph from https://t.co/zTcxfSvh3w in particular has been inspiring for this work. I think viewing deep learning as flexible functional programming helps quite a bit with crafting new inductive biases! https://t.co/WOkhi2leXs', 'Thanks to @andrewgwils and @exoplaneteer for helpful early feedback and advice on this project!\n\nAlso, thanks to @_willfalcon for the @PyTorchLightnin package, which we used throughout our project, and found sped up our development time quite a bit!', '@DaniloJRezende Thanks, Danilo!!']",https://arxiv.org/abs/2101.04117,"Despite over three hundred years of effort, no solutions exist for predicting when a general planetary configuration will become unstable. We introduce a deep learning architecture to push forward this problem for compact systems. While current machine learning algorithms in this area rely on scientist-derived instability metrics, our new technique learns its own metrics from scratch, enabled by a novel internal structure inspired from dynamics theory. Our Bayesian neural network model can accurately predict not only if, but also when a compact planetary system with three or more planets will go unstable. Our model, trained directly from short N-body time series of raw orbital elements, is more than two orders of magnitude more accurate at predicting instability times than analytical estimators, while also reducing the bias of existing machine learning algorithms by nearly a factor of three. Despite being trained on compact resonant and near-resonant three-planet configurations, the model demonstrates robust generalization to both non-resonant and higher multiplicity configurations, in the latter case outperforming models fit to that specific set of integrations. The model computes instability estimates up to five orders of magnitude faster than a numerical integrator, and unlike previous efforts provides confidence intervals on its predictions. Our inference model is publicly available in the SPOCK package, with training code open-sourced. ","A Bayesian neural network predicts the dissolution of compact planetary
  systems"
76,1349367092290719750,93411059,Bob Stienen,"['A new paper appeared! The @dark_machines sampling group has put its latest work on @arxiv:  A comparison of optimisation algorithms for high-dimensional particle and astrophysics applications. \n\nPaper can be found at <LINK>. Summary thread below 👇', 'In particle and astrophysics, or rather: science in general,  many problems boil down to optimisation problems. \n\n- which theory is most favoured by measurements?\n- which parameters of this function best fit my data set?\n- how much coffee can I drink without losing sleep? https://t.co/kokomujK5O', 'Finding answers to complicated optimisation problems like these is non trivial, especially when the function you want to optimise does not have a known (or easily accessible) derivative. This is where optimisation algorithms come in to play.', 'Optimisation algorithms attempt to find the optimum of a function by iteratively ‘guessing’ the location of this optimum and evaluating the function at each of those guesses. https://t.co/N6wn3SalRO', '*how* these algorithms do this is highly algorithm dependent. And there are *many* different algorithms: particle swarm optimisation, grey wolf optimisation, gaussian particle filter, trust region bayesian optimisation, just to name a few.', 'So… which algorithm to choose? Which algorithm is the best in finding the optimum in the least amount of function evaluations (which is important, as scientific simulations can take a really long time to run)? https://t.co/30QzNJ6yaj', 'Finding the best optimisation algorithm for a problem is, essentially, an optimisation problem in and of itself. A problem, that is addressed by this latest work of ours (https://t.co/NuuIyTHBkP).', 'We tested a large collection of optimisation algorithms on a series of test functions of varying dimensionality, including the incredibly nasty one in this picture. We also tested the algorithms on a 12-dimensional likelihood function of a model for new particle physics. https://t.co/7rytPlREtD', 'Systematically exploring the algorithms on this range of test functions allowed us to benchmark them and investigate their performance relative to each other.', 'The result? Although the overall performance is dependent on the  function and dimensionality, some algorithms seemed to systematically work better than others, like the Artificial Bee Colony and Differential Evolution.', 'This can for example be seen in plots like these (more plots in the paper!), where each dot represents the result of a single optimisation run. https://t.co/faWbTGhYrL', 'By investigating the performance of each algorithm with respect to the properties of each test function, we’re able to construct some general hints on which algorithm is likely to work well in which scenario.', 'More information and results can be found in our paper 😊\n\nA comparison of optimisation algorithms for high-dimensional particle and astrophysics applications\nhttps://t.co/NuuIyTHBkP']",https://arxiv.org/abs/2101.04525,"Optimisation problems are ubiquitous in particle and astrophysics, and involve locating the optimum of a complicated function of many parameters that may be computationally expensive to evaluate. We describe a number of global optimisation algorithms that are not yet widely used in particle astrophysics, benchmark them against random sampling and existing techniques, and perform a detailed comparison of their performance on a range of test functions. These include four analytic test functions of varying dimensionality, and a realistic example derived from a recent global fit of weak-scale supersymmetry. Although the best algorithm to use depends on the function being investigated, we are able to present general conclusions about the relative merits of random sampling, Differential Evolution, Particle Swarm Optimisation, the Covariance Matrix Adaptation Evolution Strategy, Bayesian Optimisation, Grey Wolf Optimisation, and the PyGMO Artificial Bee Colony, Gaussian Particle Filter and Adaptive Memory Programming for Global Optimisation algorithms. ","A comparison of optimisation algorithms for high-dimensional particle
  and astrophysics applications"
77,1349354924639997955,2880029134,Marco Pangallo,"['In <LINK> we showed that reaching equilibrium in 2-player games could be difficult. In our new paper, <LINK>, we do lots of maths and show that it is even more difficult in games with many players, unless players best-respond in a random order! <LINK>', ""This has been my first paper more on the coordination side, and can't thank enough @mungoluca, Sam Wiese and Yoojin Jang for all their work during summer internships! And of course also @TorstenHeinriX, Bassel and Alex for their key contributions!""]",https://arxiv.org/abs/2101.04222,"We analyze the performance of the best-response dynamic across all normal-form games using a random games approach. The playing sequence -- the order in which players update their actions -- is essentially irrelevant in determining whether the dynamic converges to a Nash equilibrium in certain classes of games (e.g. in potential games) but, when evaluated across all possible games, convergence to equilibrium depends on the playing sequence in an extreme way. Our main asymptotic result shows that the best-response dynamic converges to a pure Nash equilibrium in a vanishingly small fraction of all (large) games when players take turns according to a fixed cyclic order. By contrast, when the playing sequence is random, the dynamic converges to a pure Nash equilibrium if one exists in almost all (large) games. ","Best-response dynamics, playing sequences, and convergence to
  equilibrium in random games"
78,1349267854072442885,1224997525725335552,Dom Emery,['New research with @fu_yibin (and my first paper accepted!) We show that soft slender tubes under axial loading and surface tension can develop circumferential buckling modes which compete with the well-known beading instability @KeeleMaths  \n\n<LINK> <LINK>'],https://arxiv.org/abs/2101.04165,"We provide an extension to previous analysis of the localised beading instability of soft slender tubes under surface tension and axial stretching. The primary questions pondered here are: under what loading conditions, if any, can bifurcation into circumferential buckling modes occur, and do such solutions dominate localisation and periodic axial modes? Three distinct boundary conditions are considered; in case 1 the tube's curved surfaces are traction free and under surface tension, whilst in cases 2 and 3 the inner and outer surfaces (respectively) are fixed to prevent radial displacement and surface tension. A linear bifurcation analysis is conducted to determine numerically the existence of circumferential mode solutions. In case 1 we focus on the tensile stress regime given the preference of slender compressed tubes towards Euler buckling over axial wrinkling. We show that tubes under several loading paths are highly sensitive to circumferential modes; in contrast, localised and periodic axial modes are absent, suggesting that the circumferential buckling is dominant by default. In case 2, circumferential mode solutions are associated with negative surface tension values and thus are physically implausible. Circumferential buckling solutions are shown to exist in case 3 for tensile and compressive axial loads, and we demonstrate for multiple loading scenarios their dominance over localisation and periodic axial modes within specific parameter regimes. ","Elasto-capillary circumferential buckling of soft tubes under axial
  loading: existence and competition with localised beading and periodic axial
  modes"
79,1349179950822117376,990552499693244416,jade 🇺🇦,"[""And here's another new paper I helped write: <LINK>\nNo thread for this one 😁""]",https://arxiv.org/abs/2101.04238,"We present a unified framework for Petri nets and various variants, such as pre-nets and Kock's whole-grain Petri nets. Our framework is based on a less well-studied notion that we call $\Sigma$-nets, which allow finer control over whether tokens are treated using the collective or individual token philosophy. We describe three forms of execution semantics in which pre-nets generate strict monoidal categories, $\Sigma$-nets (including whole-grain Petri nets) generate symmetric strict monoidal categories, and Petri nets generate commutative monoidal categories, all by left adjoint functors. We also construct adjunctions relating these categories of nets to each other, in particular showing that all kinds of net can be embedded in the unifying category of $\Sigma$-nets, in a way that commutes coherently with their execution semantics. ",Categories of Nets
80,1349037155205718018,77815209,Brian Skinner,"['1/ Very excited about this new experimental paper, with work primarily by Jonah Waissman in the group of Philip Kim at Harvard. (I played a minor role in it.) It announces a new technique for identifying emergent quasiparticles!\n<LINK>', '2/ In the world of condensed matter, we have a whole zoology of ""particles"" that aren\'t like the electrons and protons of empty space. Instead, they emerge as simple excitations (like waves) from a complicated soup of interacting matter. We call them ""quasiparticles"".', '3/ What\'s amazing about the condensed matter ""quasiparticle"" is that it behaves for all the world like a real particle. It\'s just that it emerges from the ""vacuum"" of a many-body interacting system, rather from the vacuum of space.\n\nhttps://t.co/xREHE2DhSk', '4/ We have a wide array of tools for trying to excite these quasiparticles and measure their properties. But almost all of them are based on exciting some kind of electrical current. Quasiparticles that have no charge are tricky to observe, since they create no current.', ""5/ But all quasiparticles have energy, so if we had a reliable method for detecting ultra-small energy currents (the flow of heat), then we could study essentially any quasiparticle.\n\nThat's what this experiment announces: a new method for measuring tiny thermal conductances."", '6/ The idea is to use a phenomenon called Johnson Noise. The electrical current through a hot resistor is not a constant, but fluctuates randomly in time. This is because the electrons themselves act like tiny current sources, so their random motion creates random currents. https://t.co/4ckyaUpLWa', '7/ The magnitude of the randomness in the current therefore tells you the electron temperature.\n\nThe idea of the technique is to bring hot quasiparticles into contact with electrons in a graphene flake. The quasiparticles heat up the electrons, and you measure their Johnson noise', ""8/ With a little theory work (that's where I came in) you can infer the energy of the quasiparticles from the change in Johnson noise in the graphene. https://t.co/yyM0xoiHR4"", '9/ The setup looks like this: there\'s a ""hot side"", where electrons are heated up by resistive heating; a ""cold side"", where you measure the Johnson noise without external heating; and a ""bridge"" between them, through which quasiparticles can carry heat. https://t.co/qn5hDTIaVk', '10/ The best thing about this technique is that the bridge can be any material, with any kind of quasiparticles that you want. In this paper they studied plasmons (like electron sound waves) in a carbon nanotube. https://t.co/iyEi4hyk6P', '11/11 Thanks again to Jonah and to Philip Kim for bringing me in on this wonderful collaboration, and congratulations to Jonah for developing a very exciting new technique in condensed matter physics.']",https://arxiv.org/abs/2101.01737,"In low-dimensional systems, the combination of reduced dimensionality, strong interactions, and topology has led to a growing number of many-body quantum phenomena. Thermal transport, which is sensitive to all energy-carrying degrees of freedom, provides a discriminating probe of emergent excitations in quantum materials and devices. However, thermal transport measurements in low dimensions are dominated by the phonon contribution of the lattice, requiring an experimental approach to isolate the electronic thermal conductance. Here, we show how the measurement of nonlocal voltage fluctuations in a multiterminal device can reveal the electronic heat transported across a mesoscopic bridge made of low-dimensional materials. By using 2-dimensional graphene as a noise thermometer, we demonstrate quantitative electronic thermal conductance measurements of graphene and carbon nanotubes up to 70 K, achieving a precision of ~1% of the thermal conductance quantum at 5 K. Employing linear and nonlinear thermal transport, we observe signatures of long-range interaction-mediated energy transport in 1-dimensional electron systems, in agreement with a theoretical model. Our versatile nonlocal noise thermometry allows new experiments probing energy transport in emergent states of matter and devices in low dimensions. ","Electronic Thermal Transport Measurement in Low-Dimensional Materials
  with Graphene Nonlocal Noise Thermometry"
81,1349026756624265219,5850692,Aaron Roth,"['How can you quantify the uncertainty of predictions, in a model free setting, in a rapidly changing environment? In our new paper, we give some answers <LINK>. In particular, we can give stronger guarantees than conformal prediction under weaker assumptions. 1/', ""We can give prediction intervals (or variance estimates of residuals) of arbitrary point prediction methods, even if the data arrives dynamically and is adversarially chosen. It doesn't matter how the point prediction method is trained, or how the data changes with time. 2/"", 'We promise empirical coverage of 1-delta +- O(1/sqrt{T}), matching the empirical coverage rates conformal prediction gives under the assumption of data exchangability (but for us there is no distribution). Our guarantees are also stronger than marginal coverage. 3/', 'You can specify an arbitrary collection of intersecting subgroups G, and our coverage intervals are valid conditional on x \\in S, for any subgroup S \\in G. (Coverage rates depend on the size of the group, but scale only logarithmically with |G|). 4/', ""To do this we draw on game theoretic techniques from the calibration literature. Briefly: our goal is to minimize the increase in our per-round coverage error, and the adversary's goal is to increase it. This is a zero sum game. We're going to use the minmax theorem. 5/"", 'If the adversary told us their label distribution, we could read off valid prediction intervals from its CDF, and guarantee that the increase in error was small. But its a zero sum game, so there exists a strategy for us that has the same guarantees against worst case labels. 6/', ""That's a non-constructive argument. To make the algorithm concrete and efficient, we need to compute the equilibrium of the zero sum game. But we can do that by solving a linear program with an efficient separation oracle. For multicalibration, the algorithm is even simpler. 7/"", 'This is joint work with Varun Gupta, @crispy_jung, Georgy Noarov, and @malleshpai. When we started thinking about this problem, we got lots of good pointers on twitter: https://t.co/Gcgu4JR3Ic which I now enjoy using to ask for references to things I know little about. :-) 8/', 'Thanks in particular to @ShalitUri, @tdietterich, @jondr44, @yisongyue, @graduatedescent, @CsabaSzepesvari for giving us useful pointers in that thread, which is what started our thinking about problems adjacent to conformal prediction. 9/9', '@BaileyTheReal Yes! In general games are a powerful way to think about learning in adversarial/worst-case settings, and by switching the role of the players, the minmax theorem often shows that problems that are solvable in distributional settings are also solvable against adversaries.']",https://arxiv.org/abs/2101.01739,"We present a general, efficient technique for providing contextual predictions that are ""multivalid"" in various senses, against an online sequence of adversarially chosen examples $(x,y)$. This means that the resulting estimates correctly predict various statistics of the labels $y$ not just marginally -- as averaged over the sequence of examples -- but also conditionally on $x \in G$ for any $G$ belonging to an arbitrary intersecting collection of groups $\mathcal{G}$. We provide three instantiations of this framework. The first is mean prediction, which corresponds to an online algorithm satisfying the notion of multicalibration from Hebert-Johnson et al. The second is variance and higher moment prediction, which corresponds to an online algorithm satisfying the notion of mean-conditioned moment multicalibration from Jung et al. Finally, we define a new notion of prediction interval multivalidity, and give an algorithm for finding prediction intervals which satisfy it. Because our algorithms handle adversarially chosen examples, they can equally well be used to predict statistics of the residuals of arbitrary point prediction methods, giving rise to very general techniques for quantifying the uncertainty of predictions of black box algorithms, even in an online adversarial setting. When instantiated for prediction intervals, this solves a similar problem as conformal prediction, but in an adversarial environment and with multivalidity guarantees stronger than simple marginal coverage guarantees. ","Online Multivalid Learning: Means, Moments, and Prediction Intervals"
82,1348811018655305728,1263189842034167808,Peter Yiğitcan Washington,"['<LINK> New paper! We create a #headbanging #activity #MachineLearning classifier, for use of detection of #autism behaviors in young children or for #heavymetal concerts.']",http://arxiv.org/abs/2101.03478,"Activity recognition computer vision algorithms can be used to detect the presence of autism-related behaviors, including what are termed ""restricted and repetitive behaviors"", or stimming, by diagnostic instruments. The limited data that exist in this domain are usually recorded with a handheld camera which can be shaky or even moving, posing a challenge for traditional feature representation approaches for activity detection which mistakenly capture the camera's motion as a feature. To address these issues, we first document the advantages and limitations of current feature representation techniques for activity recognition when applied to head banging detection. We then propose a feature representation consisting exclusively of head pose keypoints. We create a computer vision classifier for detecting head banging in home videos using a time-distributed convolutional neural network (CNN) in which a single CNN extracts features from each frame in the input sequence, and these extracted features are fed as input to a long short-term memory (LSTM) network. On the binary task of predicting head banging and no head banging within videos from the Self Stimulatory Behaviour Dataset (SSBD), we reach a mean F1-score of 90.77% using 3-fold cross validation (with individual fold F1-scores of 83.3%, 89.0%, and 100.0%) when ensuring that no child who appeared in the train set was in the test set for all folds. This work documents a successful technique for training a computer vision classifier which can detect human motion with few training examples and even when the camera recording the source clips is unstable. The general methods described here can be applied by designers and developers of interactive systems towards other human motion and pose classification problems used in mobile and ubiquitous interactive systems. ","Activity Recognition with Moving Cameras and Few Training Examples:
  Applications for Detection of Autism-Related Headbanging"
83,1348725959965384704,1291490179132166151,Daniel Polin,['Our new paper on a search for the Dark Photon is up on ArXiv! Can you detect dark matter using an antenna in a metal box? Strong maybe! \n<LINK>'],https://arxiv.org/abs/2101.02805,"We are building an experiment to search for dark matter in the form of dark photons in the nano- to milli-eV mass range. This experiment is the electromagnetic dual of magnetic detector dark radio experiments. It is also a frequency-time dual experiment in two ways: We search for a high-Q signal in wide-band data rather than tuning a high-$Q$ resonator, and we measure electric rather than magnetic fields. In this paper we describe a pilot experiment using room temperature electronics which demonstrates feasibility and sets useful limits to the kinetic coupling $\epsilon \sim 10^{-12}$ over 50--300 MHz. With a factor of 2000 increase in real-time spectral coverage, and lower system noise temperature, it will soon be possible to search a wide range of masses at 100 times this sensitivity. We describe the planned experiment in two phases: Phase-I will implement a wide band, 5-million channel, real-time FFT processor over the 30--300 MHz range with a back-end time-domain optimal filter to search for the predicted $Q\sim 10^6$ line using low-noise amplifiers. We have completed spot frequency calibrations using a biconical dipole antenna in a shielded room that extrapolate to a $5 \sigma$ limit of $\epsilon\sim 10^{-13}$ for the coupling from the dark field, per month of integration. Phase-II will extend the search to 20 GHz using cryogenic preamplifiers and new antennas. ",Search for Dark Photon Dark Matter: Dark E-Field Radio Pilot Experiment
84,1348695574254481409,2932678322,Keaton Bell,"['New paper led by Leila Calcaferro predicts that residual nuclear fusion at the surface of extremely low-mass white dwarf stars can drive stellar pulsations via the epsilon mechanism.  Now to confirm observationally... <LINK>', ""@horizonfelix No idea about expected pulsation amplitudes. Current theory is notoriously bad at predicting white dwarf pulsation amplitudes, IMO.  If similar to other pulsating low mass white dwarfs, ~1-10% I would guess. There hasn't been much effort to find them in survey data yet.""]",https://arxiv.org/abs/2101.02777,"Before reaching their quiescent terminal white-dwarf cooling branch, some low-mass helium-core white dwarf stellar models experience a number of nuclear flashes which greatly reduce their hydrogen envelopes. Just before the occurrence of each flash, stable hydrogen burning may be able to drive global pulsations that could be relevant to shed some light on the internal structure of these stars through asteroseismology. We present a pulsational stability analysis applied to low-mass helium-core stars on their early white-dwarf cooling branches going through CNO flashes in order to study the possibility that the $\varepsilon$ mechanism is able to excite gravity-mode pulsations. We carried out a nonadiabatic pulsation analysis for low-mass helium-core white-dwarf models going through CNO flashes during their early cooling phases. We found that the $\varepsilon$ mechanism due to stable hydrogen burning can excite low-order ($\ell= 1, 2$) gravity modes with periods between $\sim 80$ and $500\ $s, for stars with $0.2025 \lesssim M_{\star}/M_{\odot} \lesssim 0.3630$ located in an extended region of the $\log g - T_{\rm eff}$ diagram with effective temperature and surface gravity in the ranges $15\,000 \lesssim T_{\rm eff} \lesssim 38\,000\ $K and $5.8 \lesssim \log g \lesssim 7.1$, respectively. Since the timescales required for these modes to reach amplitudes large enough to be observable are shorter than their corresponding evolutionary timescales, the detection of pulsations in these stars is feasible. If a low-mass white dwarf star were found to pulsate with low-order gravity modes in this region of instability, it would confirm our result that such pulsations can be driven by the $\varepsilon$ mechanism. In addition, confirming a rapid rate of period change in these pulsations would support that these stars actually experience CNO flashes, as predicted by evolutionary calculations. ","A new instability domain of CNO-flashing low-mass He-core stars on their
  early white-dwarf cooling branches"
85,1348629306055004161,1192152664412475393,Fulvio Gesmundo,"['We study the dimension of Tensor Network Varieties, with a focus on cases relevant for quantum many-body physics such as MPS and PEPS.\n\nTake a look at our new paper on ""Dimension of Tensor Network Varieties"", with A. Bernardi and C. De Lazzari, at <LINK>', 'In this work, we give a general upper bound for the dimension of these varieties and we show this bound is sharp in a particular range. We highlight few examples where the varieties are ""smaller than expected"".\n\nFeel free to comment and give feedback!']",https://arxiv.org/abs/2101.03148,"The tensor network variety is a variety of tensors associated to a graph and a set of positive integer weights on its edges, called bond dimensions. We determine an upper bound on the dimension of the tensor network variety. A refined upper bound is given in cases relevant for applications such as varieties of matrix product states and projected entangled pairs states. We provide a range (the ""supercritical range"") of the parameters where the upper bound is sharp. ",Dimension of Tensor Network varieties
86,1348493750901776384,1558538456,Rodrigo Fernández,"['New paper on mass ejection in failed supernovae: more accurate estimate of neutrino losses, connect dense-matter EOS with ejecta mass and energy &amp; EM signatures \n\nLed by #UAlberta student Mario Ivanov (@IvanovAstroSlav)\n\n<LINK> <LINK>']",https://arxiv.org/abs/2101.02712,"A failed core-collapse supernova from a non-rotating progenitor can eject mass due to a weakening of gravity associated to neutrino emission by the protoneutron star. This mechanism yields observable transients and sets an upper limit to the mass of the black hole (BH) remnant. Previous global simulations of this mechanism have included neutrino losses parametrically, however, with direct implications for the ejecta mass and energy. Here we evolve the inner supernova core with a spherically-symmetric, general-relativistic neutrino radiation-hydrodynamic code until BH formation. We then use the result in a Newtonian code that follows the response of the outer layers of the star to the change in gravity and resolves the surface pressure scale height. We find that the dense-matter equation of state (EOS) can introduce a factor $\sim 2$ variation in gravitational mass lost to neutrinos, with a stiff EOS matching previous parametric results, and a soft EOS yielding lower ejecta masses and energies by a factor of several. This difference is caused primarily by the longer time to BH formation in stiffer EOSs. With a soft EOS, our red and yellow supergiant progenitors fail to unbind mass if hydrogen recombination energy is not included. Using a linear ramp in time for mass-energy lost to neutrinos (with suitable parameters) yields a stellar response within $\sim 10\%$ of that obtained using the detailed history of neutrino losses. Our results imply quantitative but not qualitative modifications to previous predictions for shock breakout, plateau emission, and final BH masses from these events. ","Mass ejection in failed supernovae: equation of state and neutrino loss
  dependence"
87,1348126920542621696,1348060735746703365,sehoonkim,"['TinyML for NLP?\nWe have just released I-BERT, which is a new low precision *integer-only* BERT quantization framework, targeted for accurate NLP at the edge. 👇\nPaper: <LINK>\nCode: <LINK>\n[1/5]', 'Unlike many CNN models, which consist only of linear and piecewise linear operations, Transformer based NLP models use non-linear operations such as GELU and Softmax. These operations prevent the previous quantization schemes from performing integer-only inference.\n[2/5] https://t.co/6bo8ryn8mI', 'To address this, we propose a novel method to approximate non-linear operations with low-degree polynomials. Since polynomials consist only of additions and multiplications, they can be computed with integer arithmetic and yield the same result as floating point arithmetic.\n[3/5] https://t.co/Dk3B4iioZf', 'Based on our approximation methods, I-BERT performs the entire inference of Transformer architectures with INT8. \nThis includes the nonlinear activations such as GELU and Softmax, as well as the rest of the network. \n[4/5]', 'I-BERT achieves comparable and even *slightly higher* GLUE score as compared to FP32 RoBERTa-Base and Large despite being quantized and 4x smaller. \n\nIn summary, our method could be the key for bringing computationally heavy Transformer based NLP models onto edge devices.\n[5/5] https://t.co/2OxHrMDkVg', 'See the details in:\n\n""I-BERT: Integer-only BERT Quantization""\n\nBy @sehoonkim14, @a__gholami, \n@ZheweiYao, Michael W. Mahoney, Kurt Keutzer\n\nPaper: https://t.co/S3OCyLx1GH']",https://arxiv.org/abs/2101.01321,"Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced. ",I-BERT: Integer-only BERT Quantization
88,1347562964048617475,17239073,Atabey Kaygun,"['New paper on the arXiv ""Enumerating Labeled Graphs that Realize a Fixed Degree Sequence"" <LINK>']",https://arxiv.org/abs/2101.02299,"A finite non-increasing sequence of positive integers $d = (d_1\geq \cdots\geq d_n)$ is called a degree sequence if there is a graph $G = (V,E)$ with $V = \{v_1,\ldots,v_n\}$ and $deg(v_i)=d_i$ for $i=1,\ldots,n$. In that case we say that the graph $G$ realizes the degree sequence $d$. We show that the exact number of labeled graphs that realize a fixed degree sequence satisfies a simple recurrence relation. Using this relation, we then obtain a recursive algorithm for the exact count. We also show that in the case of regular graphs the complexity of our algorithm is better than the complexity of the same enumeration that uses generating functions. ",Enumerating Labeled Graphs that Realize a Fixed Degree Sequence
89,1347502752822403072,51700215,Phil Bull,"['New paper! Led by Samir Choudhuri, with simulation wizardry by Hugh Garsden.\nWe studied how differences in the primary beam patterns of the receivers in a radio telescope array (like HERA) affect the recovery of the cosmological 21cm signal.\n<LINK>', 'This has been studied before, but we wanted to see if there were identifiable patterns for different types of non-redundancy in the beams. It turns out that there are! The conclusions are a bit prosaic though, so please take a look at the paper :)']",https://arxiv.org/abs/2101.02684,"Radio interferometer arrays such as HERA consist of many close-packed dishes arranged in a regular pattern, giving rise to a large number of `redundant' baselines with the same length and orientation. Since identical baselines should see an identical sky signal, this provides a way of finding a relative gain/bandpass calibration without needing an explicit sky model. In reality, there are many reasons why baselines will not be exactly identical, giving rise to a host of effects that spoil the redundancy of the array and induce spurious structure in the calibration solutions if not accounted for. In this paper, we seek to build an understanding of how differences in the primary beam response between antennas affect redundantly-calibrated interferometric visibilities and their resulting frequency (delay-space) power spectra. We use simulations to study several generic types of primary beam variation, including differences in the width of the main lobe, the angular and frequency structure of the sidelobes, and the beam ellipticity and orientation. For all of these types, we find that additional temporal structure is induced in the gain solutions, particularly when bright point sources pass through the beam. In comparison, only a low level of additional spectral structure is induced. The temporal structure modulates the cosmological 21cm power spectrum, but only at the level of a few percent in our simulations. We also investigate the possibility of signal loss due to decoherence effects when non-redundant visibilities are averaged together, finding that the decoherence is worst when bright point sources pass through the beam, and that its magnitude varies significantly between baseline groups and types of primary beam variation. Redundant calibration absorbs some of the decoherence effect however, reducing its impact compared to if the visibilities were perfectly calibrated. ","Patterns of primary beam non-redundancy in close-packed 21 cm array
  observations"
90,1347500660183478273,1139101764786053121,Matthew Orkney,['Submitted a new paper to arXiv this morning! I use the EDGE simulation suite to make the case that small dark matter core formation in dwarf galaxies can be driven by both stellar feedback and impulsive heating from minor mergers. <LINK>'],https://arxiv.org/abs/2101.02688,"In the standard Lambda cold dark matter paradigm, pure dark matter simulations predict dwarf galaxies should inhabit dark matter haloes with a centrally diverging density `cusp'. This is in conflict with observations that typically favour a constant density `core'. We investigate this `cusp-core problem' in `ultra-faint' dwarf galaxies simulated as part of the `Engineering Dwarfs at Galaxy formation's Edge' (EDGE) project. We find, similarly to previous work, that gravitational potential fluctuations within the central region of the simulated dwarfs kinematically heat the dark matter particles, lowering the dwarfs' central dark matter density. However, these fluctuations are not exclusively caused by gas inflow/outflow, but also by impulsive heating from minor mergers. We use the genetic modification approach on one of our dwarf's initial conditions to show how a delayed assembly history leads to more late minor mergers and, correspondingly, more dark matter heating. This provides a mechanism by which even ultra-faint dwarfs ($M_* < 10^5\,\text{M}_{\odot}$), in which star formation was fully quenched at high redshift, can have their central dark matter density lowered over time. In contrast, we find that late major mergers can regenerate a central dark matter cusp, if the merging galaxy had sufficiently little star formation. The combination of these effects leads us to predict significant stochasticity in the central dark matter density slopes of the smallest dwarfs, driven by their unique star formation and mass assembly histories. ",EDGE: Two routes to dark matter core formation in ultra-faint dwarfs
91,1347217611537444865,1139943231922331652,Chandreyee Maitra,['A new paper on the comprehensive broadband spectral analysis of accreting high mass X-ray binaries using #Suzaku <LINK> led by @pragati2707 . <LINK>'],https://arxiv.org/abs/2101.01727,"We present a broadband spectral analysis of accreting neutron stars using data from XIS and PIN onboard \emph{Suzaku}. From spectral fits of these sources with a single continuum model including a powerlaw and high energy cut-off, cyclotron lines (where required), we studied the correlation between various spectral parameters. Among 39 sources we studied, 16 are those where the existence of a cyclotron line is known in literature, and 29 need a cutoff energy. Among these 29 sources, 18 have cutoff energy bunched in a range of 3-10 keV while for 11 sources, it spreads over 12-25 keV. This bi-modal behaviour is not based on the specific nature of the systems being a Be XRB or supergiant HMXB, nor on different beaming patterns characterizing their X-ray emission (as inferred from simultaneous study of their pulse profiles). The broadband coverage of \emph{Suzaku} also shows that the cutoff energies saturate for higher values of cyclotron line energies - consistent with previous works in literature - for both the groups and the width of the cyclotron line show a weak correlation with the cyclotron line energy. We also find an anticorrelation with luminosity for both spectral index and folding energy, respectively. Unlike previous works, we did not detect any anticorrelation between X-ray luminosity and EW of K$\alpha$ lines. Finally, we show that the EW and flux of the iron K$\alpha$ line are smaller in SFXTs than classical NS-HMXBs. We discuss these findings in terms of different properties of stellar winds and accretion mechanisms. ","Comprehensive broadband study of accreting neutron stars with Suzaku: Is
  there a bi-modality in the X-ray spectrum?"
92,1347193932388696069,752490550452953088,Dr. Joanna Drążkowska,"['My new paper is out on arXiv today: <LINK>\nI looked at planet growth by pebble accretion in models with pebble sizes and fluxes calculated self-consistently in state-of-the-art dust evolution code.', 'In case you want to check what would be a reasonable pebble size and flux in your favorite protoplanetary disk model, here is a simple script for that, called the pebble predictor: https://t.co/qZwb44ZHuf']",https://arxiv.org/abs/2101.01728,"Pebble accretion is an emerging paradigm for the fast growth of planetary cores. Pebble flux and pebble sizes are the key parameters used in the pebble accretion models. We aim to derive the pebble sizes and fluxes from state-of-the-art dust coagulation models, understand their dependence on disk parameters and the fragmentation threshold velocity, and the impact of those on the planetary growth by pebble accretion. We use a one-dimensional dust evolution model including dust growth and fragmentation to calculate realistic pebble sizes and mass flux. We use this information to integrate the growth of planetary embryos placed at various locations in the protoplanetary disk. Pebble flux strongly depends on disk properties, such as its size and turbulence level, as well as on the dust aggregates fragmentation threshold. We find that dust fragmentation may be beneficial to planetary growth in multiple ways. First of all, it prevents the solids from growing to very large sizes, for which the efficiency of pebble accretion drops. What is more, small pebbles are depleted at a slower rate, providing a long-lasting pebble flux. As the full coagulation models are computationally expensive, we provide a simple method of estimating pebble sizes and flux in any protoplanetary disk model without substructure and with any fragmentation threshold velocity. ","How dust fragmentation may be beneficial to planetary growth by pebble
  accretion"
93,1346987361981358081,839913287240278020,Joey Rodriguez,"['NEW PAPER ALERT!!! 5 new hot giant planets from @NASA_TESS. All five planets were found in the TESS Full Frame Images, and orbit bight host star (and some evolved host stars). (Rp = 1.01-1.77 RJ, Mp = 0.85 - 6.33 MJ, 9.5 &lt; Vmag &lt; 10.8). <LINK>', 'This paper has always had a nautical theme to it within our group and was nicknamed the “Cargo Ship” paper. Or less commonly, the H.M.S. TESS. https://t.co/z9uSbbryMb', 'TOI-628 b: A 6.33 MJ hot Jupiter on a 3.41 day period. We measure a small, but statistically significant eccentricity (0.072).', 'TOI-640 b: A highly inflated (1.77 RJ) hot Jupiter orbiting a sub-giant on a near integer-day period (5.00 days). One of only three planets known to be this inflated (&gt;1.7RJ) and have a period &gt;5 days.', 'TOI-1333 b: a 2.4MJ Hot Jupiter around a bright star (V = 9.5) with a nearby companion star. The host star is slightly evolved.', 'TOI-1478 b: The lone warm Jupiter (P = 10.18 days) in the sample that orbits a near-solar analogue.', 'TOI-1601 b: A hot Jupiter on a 5.3 day period around a sub-giant. We detect a bimodal distribution in the mass of the host star (and thus, age of the star) due to the host star’s evolutionary state and precision limits. Make sure you check your posteriors!', 'We see a wider distribution of eccentricities for hot Jupiter planets on longer period orbits (&gt;5 days). Does this suggest dynamically driven migration? Possibly, but many things need to be considered like binarity.', '@NASA_TESS has made some great discoveries in giant planets, complementing its level 1 science goals of measuring the composition of small planets.', 'Co Authors on Twitter: @samuelnquinn, @amannastro, @LouiseDyregaard, @rafaelbrahm, @bsgaudi, @exofastupdates, @lis_matthews, @AstroDressing, @PlavchanPeter, @twitspek, @ProfSaraSeager, @Jonmjenkins, @tgbeatty, @mrtommyb, @bpbowler, @aussiastronomer, @nespinozap, @sleeplessinmit', '@ExoCytherean, @TheMicon, @PepperJoshua, @Lowbacca, @amannastro, @elsisrad, and many other (sorry if I missed co-authors on twitter)!']",https://arxiv.org/abs/2101.01726,"We present the discovery and characterization of five hot and warm Jupiters -- TOI-628 b (TIC 281408474; HD 288842), TOI-640 b (TIC 147977348), TOI-1333 b (TIC 395171208, BD+47 3521A), TOI-1478 b (TIC 409794137), and TOI-1601 b (TIC 139375960) -- based on data from NASA's Transiting Exoplanet Survey Satellite (TESS). The five planets were identified from the full frame images and were confirmed through a series of photometric and spectroscopic follow-up observations by the $TESS$ Follow-up Observing Program (TFOP) Working Group. The planets are all Jovian size (R$_{\rm P}$ = 1.01-1.77 R$_{\rm J}$) and have masses that range from 0.85 to 6.33 M$_{\rm J}$. The host stars of these systems have F and G spectral types (5595 $\le$ T$_{\rm eff}$ $\le$ 6460 K) and are all relatively bright (9 $<V<$ 10.8, 8.2 $<K<$ 9.3) making them well-suited for future detailed characterization efforts. Three of the systems in our sample (TOI-640 b, TOI-1333 b, and TOI-1601 b) orbit subgiant host stars (log g$_*$ $<$4.1). TOI-640 b is one of only three known hot Jupiters to have a highly inflated radius (R$_{\rm P}$ > 1.7R$_{\rm J}$, possibly a result of its host star's evolution) and resides on an orbit with a period longer than 5 days. TOI-628 b is the most massive hot Jupiter discovered to date by $TESS$ with a measured mass of $6.31^{+0.28}_{-0.30}$ M$_{\rm J}$ and a statistically significant, non-zero orbital eccentricity of e = $0.074^{+0.021}_{-0.022}$. This planet would not have had enough time to circularize through tidal forces from our analysis, suggesting that it might be remnant eccentricity from its migration. The longest period planet in this sample, TOI-1478 b (P = 10.18 days), is a warm Jupiter in a circular orbit around a near-Solar analogue. NASA's $TESS$ mission is continuing to increase the sample of well-characterized hot and warm Jupiters, complementing its primary mission goals. ","TESS Delivers Five New Hot Giant Planets Orbiting Bright Stars from the
  Full Frame Images"
94,1346856668613771268,198239722,Christopher Chen,"['Our new paper on turbulence in the streamer belt wind near the Sun, measured by @PSP_FIELDS and @TheSWEAPLife on #ParkerSolarProbe, is now available on arXiv: <LINK>']",https://arxiv.org/abs/2101.00246,"The fourth orbit of Parker Solar Probe (PSP) reached heliocentric distances down to 27.9 Rs, allowing solar wind turbulence and acceleration mechanisms to be studied in situ closer to the Sun than previously possible. The turbulence properties were found to be significantly different in the inbound and outbound portions of PSP's fourth solar encounter, likely due to the proximity to the heliospheric current sheet (HCS) in the outbound period. Near the HCS, in the streamer belt wind, the turbulence was found to have lower amplitudes, higher magnetic compressibility, a steeper magnetic field spectrum (with spectral index close to -5/3 rather than -3/2), a lower Alfv\'enicity, and a ""1/f"" break at much lower frequencies. These are also features of slow wind at 1 au, suggesting the near-Sun streamer belt wind to be the prototypical slow solar wind. The transition in properties occurs at a predicted angular distance of ~4{\deg} from the HCS, suggesting ~8{\deg} as the full-width of the streamer belt wind at these distances. While the majority of the Alfv\'enic turbulence energy fluxes measured by PSP are consistent with those required for reflection-driven turbulence models of solar wind acceleration, the fluxes in the streamer belt are significantly lower than the model predictions, suggesting that additional mechanisms are necessary to explain the acceleration of the streamer belt solar wind. ","The Near-Sun Streamer Belt Solar Wind: Turbulence and Solar Wind
  Acceleration"
95,1346799290736381953,77592002,Ahmad,"['My new paper is up on arxiv! \n\nI investigate how metallicity and dust affect the growth of H II regions in clusters.\n\nI carry out rad-hydro simulations with detailed RT, and compare photoionization heating with radiation pressure (Pgas and Prad in figs). \n\n<LINK> <LINK>', ""This was my first sole-author paper and I'm super happy with it. And there's a lot of scope to do more analysis using these simulations as well, eg with synthetic observations. (Which is just as well, as they weren't exactly cheap...)"", ""@poke haha thanks! I admit I've spent a lot of time trying out different colour schemes, so I'm glad it paid off 😊""]",https://arxiv.org/abs/2101.01193,"Gas metallicity $Z$ and the related dust-to-gas ratio $f_\textrm{d}$ can influence the growth of HII regions via metal line cooling and UV absorption. We model these effects in star-forming regions containing massive stars. We compute stellar feedback from photoionization and radiation pressure (RP) using Monte Carlo radiative transfer coupled with hydrodynamics, including stellar and diffuse radiation fields. We follow a $10^5$ M$_\odot$ turbulent cloud with $Z/$Z$_\odot$ = 2, 1, 0.5, 0.1 and $f_\textrm{d} = 0.01 Z/$Z$_\odot$ with a cluster-sink particle method for star formation. The models evolve for at least 1.5 Myr under feedback. Lower $Z$ results in higher temperatures and therefore larger HII regions. For $Z \ge$Z$_\odot$, radiation pressure $P_\textrm{rad}$ can dominate locally over the gas pressure $P_\textrm{gas}$ in the inner half-parsec around sink particles. Globally, the ratio of $P_\textrm{rad}/P_\textrm{gas}$ is around 1 (2 Z$_\odot$), 0.3 (Z$_\odot$), 0.1 (0.5 Z$_\odot$), and 0.03 (0.1 Z$_\odot$). In the solar model, excluding RP results in an ionized volume several times smaller than the fiducial model with both mechanisms. Excluding RP and UV attenuation by dust results in a larger ionized volume than the fiducial case. That is, UV absorption hinders growth more than RP helps it. The radial expansion velocity of ionized gas reaches $+$15 km/s outwards, while neutral gas has inward velocities for most of the runtime, except for 0.1 Z$_\odot$ which exceeds $+$4 km/s. $Z$ and $f_\textrm{d}$ do not significantly alter the star formation efficiency, rate, or cluster half-mass radius, with the exception of 0.1 Z$_\odot$ due to the earlier expulsion of neutral gas. ","The growth of H II regions around massive stars: the role of metallicity
  and dust"
96,1346484102153777153,836333446872051713,Mikhail Tikhonov,"['Excited to share another new paper. We all learn from experience: evolution slowly; our brains faster. Theoretically, so could bacteria! With a simple circuit: <LINK>']",https://arxiv.org/abs/2101.00051,"Bacteria live in environments that are continuously fluctuating and changing. Exploiting any predictability of such fluctuations can lead to an increased fitness. On longer timescales bacteria can ""learn"" the structure of these fluctuations through evolution. However, on shorter timescales, inferring the statistics of the environment and acting upon this information would need to be accomplished by physiological mechanisms. Here, we use a model of metabolism to show that a simple generalization of a common regulatory motif (end-product inhibition) is sufficient both for learning continuous-valued features of the statistical structure of the environment and for translating this information into predictive behavior; moreover, it accomplishes these tasks near-optimally. We discuss plausible genetic circuits that could instantiate the mechanism we describe, including one similar to the architecture of two-component signaling, and argue that the key ingredients required for such predictive behavior are readily accessible to bacteria. ","A simple regulatory architecture allows learning the statistical
  structure of a changing environment"
97,1346365234525528064,400026483,Oem Trivedi,"['Divinely, my new paper(<LINK>) just came out on the ArXiV today and its possibly my most exciting work till now and its biggest outcome is ; CHANGING THE UNCERTAINITY PRINCIPLE CAN FACILITATE US TO HAVE A QUANTUM GRAVITY CONSISTENT MULTIVERSE ! So... 1/n', ""Eternal inflation is a paradigm of inflation in where inflation doesn't really completely stop everywhere in space. Inflation is one of the possible ways that our universe expanded very early on, and the idea that it never stops gives the name eternal and suggests.. 2/n"", ""that there might be a multiverse, the idea that there can be many universes besides ours ! It is one of the most fascinating topics i've ever come across in physics but in recent years, this idea has been thwarted by quantum gravity consistency criteria from string theory..3/n"", 'named as the ""swampland criterion"". It was earlier shown in (https://t.co/LAOZrcKiov) that eternal inflation is in direct conflict with the dS conjecture in its fundamental requirement\xa0for quantum and classical perturbations. Then it was shown in ..4/n', ""the following paper by Dr. Kinney @WKCosmo  (https://t.co/6NPKz3oTbB) (on a side note, I think I've tagged you in each of my paper threads and so I just cannot thank you enough for your work sir🙏) that eternal inflation can possibly be okay with the refined dS conjecture..5/n"", 'but then it was shown in this papere (https://t.co/S5K3hNaNPp) that actually eternal inflation is not consistent with the refined\xa0dS conjecture too. Further to make the issues worse, (https://t.co/HTye70DtIN) showed that the Gibbons-Hawking entropy bounds for ...6/n', 'inflation are also in direct conflict with the dS conjecture and perhaps, this can be the most stringent of all issues for eternal inflation. Further it was shown by Lin (https://t.co/WgYyzTn51Q) that if one considers inflation in RS-II Braneworld, then the conjectures can..7/n', 'be possibly consistent with eternal inflation requirements.  it does seem that standard eternal inflation cannot fully satisfy all the swampland criterion as it is. Also when one considers warm inflation then the condition of eternal inflation itself is grim, as..8/n', ""it's pretty much not allowed in there (https://t.co/yvGAsEUGTW).\xa0The issues of eternal inflation in usual scalar field models led me to consider tachyonic scalar fields and to eradicate the entropy bound issue, I considered the RS-II Braneworld cosmology..9/n"", 'So in the paper, I firstly showed that tachyonic inflation in the high energy RS-II scenario ( I consider the high energy regime to better illustrate the quantum gravity effects) is quite consistent with the conjectures, as it evades all the issues for single field models..10/n', 'with the swampland criterion, which were raised by @WKCosmo,@SunnyVagnozzi and @lucavisinelli in https://t.co/hj8Eh5rSUR. The only requirement for it to be consistent is a lower bound on the inflation energy scale, which is quite intrinsically satisfied in this regime ..11/n', 'After this, I show that the main issues of eternal inflation and the perturbation spectra requirements which are observed in usual scalar field models is not a problem at all for tachyonic models. More so, in our regime it is quite easily evaded. Then..12/n', 'I address the entropy bounds issue. We see that in the concerned inflationary regime, the Gibbons Hawking entropy bound- dS conjecture issue can be evaded for a lower bound on the inflation energy scale, which is consistent with the earlier lower bound on the energy scale..13/n', 'Then I discuss another way to approach the entropy issue. In recent years, there has been loads of interest in modifying the Heisenberg uncertainty principle considering quantum gravity and black hole effects ( a good paper for this is - https://t.co/QQQTBh47Mv) ..14/n', 'Interestingly, this modification gives corrections to the usual bekenstein-gibbons-hawking(BGH) entropy formula. Some papers have considered this modified entropy for cosmological cases, like https://t.co/b8iDgnCeeO). So what I did is ..15/n', ""basically to use a similar approach and use this generalized uncertainty principle for an expanding cosmology.allows one to eradicate the conflict between the dS conjecture and entropy bounds in ANY Inflationary regime, doesn't matter what is the background cosmology or.. 16/n"", 'what is the form of the potential or what kind of a scalar field the inflaton is. So this suggests that changing one of the most fundamental laws of Quantum mechanics in the uncertainty principle allows one to more easily have a Swampland consistent Multiverse..17/n', 'And hence possibly, a quantum gravitationally consistent Multiverse too ! The connection between these two seemingly far off and distinct ideas is something completely bewildering for me !\n\nThank you so much for reading this whole thread !🙏🙏🙇\u200d♂️🙇\u200d♂️😃😃', ""@jfpas I'm sorry but I didnt quite get the meaning of that statement 😅🙏🏻""]",https://arxiv.org/abs/2101.00638,"The swampland conjectures from string theory have had some really interesting implications on cosmology, in particular on inflationary models. Some models of inflation have been shown to be incompatible with these criterion while some have been shown to be severely fine tuned, with most of these problems arising in single field inflationary models in a General relativistic cosmology. Recent works have although optimistically shown that single field models in more general cosmologies can be consistent with these conjectures and hence there is an optimism that not all such models lie in the swampland. However a paradigm of inflation which has been shown to not be perfectly okay with the conjectures is eternal inflation. So in this work, we discuss Tachyonic inflation in the high energy RS-II Braneworld scenario in the context of the swampland conjectures while also considering the possibility of swampland consistent eternal inflation. We show that our concerned regime evades all the prominent swampland issues for single field inflation being virtually unscathed. After this, we show that the main conflicts of eternal inflation with the swampland can easily be resolved in the considered tachyonic scenario and in particular, we also discuss the exciting prospect of a Generalized Uncertainty Principle facilitating the notion of Swampland consistent eternal inflation. Our work as a whole reignites the possibility that there can be a swampland (and possibly, quantum gravitationally) consistent picture of a ""Multiverse"". ","Rejuvenating the hope of a swampland consistent inflated multiverse with
  tachyonic inflation in the high energy RS-II Braneworld"
98,1356710970148716546,1044052956918644737,Ana Klimovic,"['New blog post about research opportunities for #ML data storage &amp; preprocessing, including lessons from a study of real ML workloads at Google, in collaboration with @jsimsa and @mrry. More details in our paper:  <LINK> <LINK>']",https://arxiv.org/abs/2101.12127,"Training machine learning models requires feeding input data for models to ingest. Input pipelines for machine learning jobs are often challenging to implement efficiently as they require reading large volumes of data, applying complex transformations, and transferring data to hardware accelerators while overlapping computation and communication to achieve optimal performance. We present tf.data, a framework for building and executing efficient input pipelines for machine learning jobs. The tf.data API provides operators which can be parameterized with user-defined computation, composed, and reused across different machine learning domains. These abstractions allow users to focus on the application logic of data processing, while tf.data's runtime ensures that pipelines run efficiently. We demonstrate that input pipeline performance is critical to the end-to-end training time of state-of-the-art machine learning models. tf.data delivers the high performance required, while avoiding the need for manual tuning of performance knobs. We show that tf.data features, such as parallelism, caching, static optimizations, and non-deterministic execution are essential for high performance. Finally, we characterize machine learning input pipelines for millions of jobs that ran in Google's fleet, showing that input data processing is highly diverse and consumes a significant fraction of job resources. Our analysis motivates future research directions, such as sharing computation across jobs and pushing data projection to the storage layer. ",tf.data: A Machine Learning Data Processing Framework
99,1356642419211698177,373525906,Weijie Su,"['Many models can explain phenomena in deep learning. OK, but do you see one predicting a *new* surprising phenomenon? Super excited to share a paper ""Layer-Peeled Model: Toward Understanding Well-Trained\nDeep Neural Networks"" (<LINK>). w/ Fang, @HangfengH, Long.', 'The Layer-Peeled Model predicts Minority Collapse: the last-layer classifiers of the minority classes become identical! This means the fundamental difficulty in learning on imbalanced data. Remarkably, this phenomenon was confirmed by experiments on various architectures. 2/n', ""Does Minority Collapse have implications for #fairness in deep learning models? I don't know.  3/n"", 'The Layer-Peeled Model is derived by taking a top-down strategy by isolating the last layer from the remaining layers. It is yet a nonconvex problem, but analytically tractable. 4/n', 'In addition to Minority Collapse, another use case of the Layer-Peeled Model is to explain neural collapse. Neural collapse was discovered in https://t.co/K7mx6FgmLT 5/n', 'An illustration of Minority Collapse is here. As the level imbalance ratio increases, the angles between minority classifiers shrink. And it is *exactly* at a certain R! The experiments also show this phase transition. https://t.co/Rk0Zylvpoo']",https://arxiv.org/abs/2101.12699,"In this paper, we introduce the \textit{Layer-Peeled Model}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse \cite{papyan2020prevalence}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term \textit{Minority Collapse}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments. ","Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse
  in Imbalanced Training"
100,1356519030316761090,1019501564174716929,Toms Bergmanis,"['Here is a pre-print of our #eacl2021 paper ""Facilitating Terminology Translation with Target Lemma Annotations"" For those interested in similar problems: the new automotive test suite available on github! #NLProc\n<LINK> <LINK>', 'In result table CD - constrained decoding by Post and Vilar, (2018)\nETA - exact target form annotations by Dinu et al. (2019)']",https://arxiv.org/abs/2101.10035,"Most of the recent work on terminology integration in machine translation has assumed that terminology translations are given already inflected in forms that are suitable for the target language sentence. In day-to-day work of professional translators, however, it is seldom the case as translators work with bilingual glossaries where terms are given in their dictionary forms; finding the right target language form is part of the translation process. We argue that the requirement for apriori specified target language forms is unrealistic and impedes the practical applicability of previous work. In this work, we propose to train machine translation systems using a source-side data augmentation method that annotates randomly selected source language words with their target language lemmas. We show that systems trained on such augmented data are readily usable for terminology integration in real-life translation scenarios. Our experiments on terminology translation into the morphologically complex Baltic and Uralic languages show an improvement of up to 7 BLEU points over baseline systems with no means for terminology integration and an average improvement of 4 BLEU points over the previous work. Results of the human evaluation indicate a 47.7% absolute improvement over the previous work in term translation accuracy when translating into Latvian. ",Facilitating Terminology Translation with Target Lemma Annotations
101,1354714135666118658,998127492467576832,Ryota Tanaka,"['Our #AAAI2021 paper with @kyoun is out on arXiv! <LINK>\nWe introduce VisualMRC that requires a system to read and reason about text in the document image. We propose new models that allow for transferring the abilities of pre-trained seq2seq models, to this task. <LINK>']",https://arxiv.org/abs/2101.11272,"Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with large-scale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding. ",VisualMRC: Machine Reading Comprehension on Document Images
102,1354263402948947970,107577627,Alejandra Ciria,"['We are very happy to announce our new paper!\nPredictive Processing in Cognitive Robotics: a Review \n@guido_schillaci @GiovanniPezzulo @vvh @quitze \nFree access to the unformatted version of the paper here\n👇 1/5\n<LINK>', 'In the literature, terms such as “predictive processing”, “hierarchical predictive processing”, “active inference”, “predictive coding” and “free energy principle” are often used interchangeably.  2/5', 'In cognitive robotics, a number of architectures and models have claimed to follow the postulates of these frameworks. 3/5', 'The aim of this article is to set working definitions and delimit the main ideas for each of these frameworks, so as to be able to analyze the literature of cognitive robotics and the different implementations in the literature. 4/5', 'This should help to highlight what has been done and what is missing, and above all, what the real impact of these frameworks in the area of robotics and artificial intelligence is. This paper sets the issues and challenges that these new frameworks bring on the table. 5/5']",https://arxiv.org/abs/2101.06611,"Predictive processing has become an influential framework in cognitive sciences. This framework turns the traditional view of perception upside down, claiming that the main flow of information processing is realized in a top-down hierarchical manner. Furthermore, it aims at unifying perception, cognition, and action as a single inferential process. However, in the related literature, the predictive processing framework and its associated schemes such as predictive coding, active inference, perceptual inference, free-energy principle, tend to be used interchangeably. In the field of cognitive robotics there is no clear-cut distinction on which schemes have been implemented and under which assumptions. In this paper, working definitions are set with the main aim of analyzing the state of the art in cognitive robotics research working under the predictive processing framework as well as some related non-robotic models. The analysis suggests that, first, both research in cognitive robotics implementations and non-robotic models needs to be extended to the study of how multiple exteroceptive modalities can be integrated into prediction error minimization schemes. Second, a relevant distinction found here is that cognitive robotics implementations tend to emphasize the learning of a generative model, while in non-robotics models it is almost absent. Third, despite the relevance for active inference, few cognitive robotics implementations examine the issues around control and whether it should result from the substitution of inverse models with proprioceptive predictions. Finally, limited attention has been placed on precision weighting and the tracking of prediction error dynamics. These mechanisms should help to explore more complex behaviors and tasks in cognitive robotics research under the predictive processing framework. ",Predictive Processing in Cognitive Robotics: a Review
103,1354211758525853706,716990588109852672,Yingtong Dou,"['🔔New paper out! \n\nOur #TheWebConference 2021 paper ""Knowledge-Preserving Incremental Social Event Detection via Heterogeneous GNNs"" (<LINK>) adapts GNN to the social event detection problem under an incremental learning setting.\n\n#graphmining #eventdetection <LINK>']",https://arxiv.org/abs/2101.08747,"Social events provide valuable insights into group social behaviors and public concerns and therefore have many applications in fields such as product recommendation and crisis management. The complexity and streaming nature of social messages make it appealing to address social event detection in an incremental learning setting, where acquiring, preserving, and extending knowledge are major concerns. Most existing methods, including those based on incremental clustering and community detection, learn limited amounts of knowledge as they ignore the rich semantics and structural information contained in social data. Moreover, they cannot memorize previously acquired knowledge. In this paper, we propose a novel Knowledge-Preserving Incremental Heterogeneous Graph Neural Network (KPGNN) for incremental social event detection. To acquire more knowledge, KPGNN models complex social messages into unified social graphs to facilitate data utilization and explores the expressive power of GNNs for knowledge extraction. To continuously adapt to the incoming data, KPGNN adopts contrastive loss terms that cope with a changing number of event classes. It also leverages the inductive learning ability of GNNs to efficiently detect events and extends its knowledge from previously unseen data. To deal with large social streams, KPGNN adopts a mini-batch subgraph sampling strategy for scalable training, and periodically removes obsolete data to maintain a dynamic embedding space. KPGNN requires no feature engineering and has few hyperparameters to tune. Extensive experiment results demonstrate the superiority of KPGNN over various baselines. ","Knowledge-Preserving Incremental Social Event Detection via
  Heterogeneous GNNs"
104,1351967463609491458,465023908,Ben Bartlett,"['My new paper with @DuttAvik and Shanhui Fan is now on arXiv! 🎉 <LINK>\n\nWe propose a design for a teleportation-based photonic quantum computer which can perform any computation using only one (1) controllable qubit. [bonus: single-photon detectors not required!] <LINK>', '[1/6] Photonics is a promising substrate for QC, but it is very difficult to integrate many identical quantum emitters into a large photonic circuit.\n\nHere we propose a design which requires only a single quantum emitter to implement any quantum operation of any size!', '[2/6] In our scheme, optical qubits are encoded as trains of single-photon pulses counter-propagating through a fiber storage ring. Optical switches can selectively scatter photons off of an atom-cavity system before returning them to the storage ring. https://t.co/AYVgE6L6ig', '[3/6] The atom is coherently controlled by a laser. After scattering a photon against the atom, a rotation is applied to the atomic qubit and the state is projectively measured in the {|g₀⟩, |g₁⟩} basis.\n\nThis measurement teleports the rotation gate onto the photonic qubit! https://t.co/YCFUb6K361', '[4/6] The teleported rotations are a universal quantum primitive: by composing 3 rotation gates with suitable angles, any single-qubit gate can be implemented through Euler angles.\n\nTwo-photon gates such as CZ can also be implemented using a similar rotate-and-measure procedure. https://t.co/Hl4SARlC17', '[5/6] CZ and single-qubit gates are a universal gate set, so the device can implement any quantum circuit! \n\nAdditionally, state readout can be done without using single-photon detectors! Simply apply SWAP between each photon and the atom and sequentially measure the atomic state https://t.co/O6V6aWGpkW', '[6/6] We also did a detailed analysis of how our scheme performs in the presence of experimental imperfections. Assuming realistic parameters, error probability per gate is potentially fault tolerant at ~5×10⁻⁴, and very deep circuits are possible even without error correction. https://t.co/kDi2cnr0If', '@AMFraine @DuttAvik No, you can actually do this with a nondeterministic source (as long as you can resolve the time bin of the photon) or you can even double-purpose the atom-cavity system as the photon source itself!', ""@AMFraine @DuttAvik that's correct, but if you only need to resolve time bins before starting the computation (rather than reading out the final quantum state after processing), then limited detection efficiencies are not a huge issue"", ""@JacquesCarolan @DuttAvik thanks! glad it's finally online so I can talk about it 😅"", 'HD version here: https://t.co/3H6UPMfLbF\n\nCode I wrote to make the animation here: https://t.co/zXVumZBoDO', '@JnEsZ @DuttAvik thanks! making animations is a good way to p̶r̶o̶c̶r̶a̶s̶t̶i̶n̶a̶t̶e̶ ̶o̶n̶ ̶r̶e̶a̶l̶ ̶w̶o̶r̶k̶ ̶i̶ ̶n̶e̶e̶d̶ ̶t̶o̶ ̶d̶o̶ convey ideas', ""@samykamkar thanks! it's kind of a wacky idea so I had fun working on this project""]",https://arxiv.org/abs/2101.07786,"Photonics offers unique advantages as a substrate for quantum information processing, but imposes fundamental scalability challenges. Nondeterministic schemes impose massive resource overheads, while deterministic schemes require prohibitively many identical quantum emitters to realize sizeable quantum circuits. Here we propose a scalable architecture for a photonic quantum computer which needs minimal quantum resources to implement any quantum circuit: a single coherently controlled atom. Optical switches endow a photonic quantum state with a synthetic time dimension by modulating photon-atom couplings. Quantum operations applied to the atomic qubit can be teleported onto the photonic qubits via projective measurement, and arbitrary quantum circuits can be compiled into a sequence of these teleported operators. This design negates the need for many identical quantum emitters to be integrated into a photonic circuit and allows effective all-to-all connectivity between photonic qubits. The proposed device has a machine size which is independent of quantum circuit depth, does not require single-photon detectors, operates deterministically, and is robust to experimental imperfections. ",Deterministic photonic quantum computation in a synthetic time dimension
105,1351892838422867972,1024247894474481665,Furkan Gürsoy,"['📜 New paper out! Read below if you are working on/with representation learning methods.\n\n""Alignment and stability of embeddings: measurement and inference improvement""\n\n<LINK>\n\n👇 <LINK>']",https://arxiv.org/abs/2101.07251,"Representation learning (RL) methods learn objects' latent embeddings where information is preserved by distances. Since distances are invariant to certain linear transformations, one may obtain different embeddings while preserving the same information. In dynamic systems, a temporal difference in embeddings may be explained by the stability of the system or by the misalignment of embeddings due to arbitrary transformations. In the literature, embedding alignment has not been defined formally, explored theoretically, or analyzed empirically. Here, we explore the embedding alignment and its parts, provide the first formal definitions, propose novel metrics to measure alignment and stability, and show their suitability through synthetic experiments. Real-world experiments show that both static and dynamic RL methods are prone to produce misaligned embeddings and such misalignment worsens the performance of dynamic network inference tasks. By ensuring alignment, the prediction accuracy raises by up to 90% in static and by 40% in dynamic RL methods. ","Alignment and stability of embeddings: measurement and inference
  improvement"
106,1351114728907665408,1020053469787566082,Alan Karthikesalingam,"['New paper from our team @GoogleHealth/@GoogleAI (<LINK>) Pre-training at scale improves AI accuracy, generalisation + fairness in many medical imaging tasks: Chest X-Ray, Dermatology &amp; Mammography! Led by @_basilM, @JanFreyberg, Aaron Loh, @neilhoulsby, @vivnat <LINK>', 'As medical images are scarce, a common medical ML approach is “transfer learning”: pre-training on non-medical data like ImageNet before fine-tuning on medical tasks. But medical tasks are complex leaving room to improve. Scaling up massively with Big Transfer (BiT) helps! 🚀']",https://arxiv.org/abs/2101.05913,"Transfer learning is a standard technique to improve performance on tasks with limited data. However, for medical imaging, the value of transfer learning is less clear. This is likely due to the large domain mismatch between the usual natural-image pre-training (e.g. ImageNet) and medical images. However, recent advances in transfer learning have shown substantial improvements from scale. We investigate whether modern methods can change the fortune of transfer learning for medical imaging. For this, we study the class of large-scale pre-trained networks presented by Kolesnikov et al. on three diverse imaging tasks: chest radiography, mammography, and dermatology. We study both transfer performance and critical properties for the deployment in the medical domain, including: out-of-distribution generalization, data-efficiency, sub-group fairness, and uncertainty estimation. Interestingly, we find that for some of these properties transfer from natural to medical images is indeed extremely effective, but only when performed at sufficient scale. ",Supervised Transfer Learning at Scale for Medical Imaging
107,1350030324730908673,367385480,Adam Langeveld,"['📢I am pleased to share our new exoplanet atmospheres paper: <LINK>\n\nWe discuss one of the major challenges with high-resolution exoplanet transmission spectroscopy:\n➡️Telluric contamination\n\nA THREAD 👇\n\n#exoplanets #astronomy #scicomm', 'Transmission spectroscopy is a technique used to observe the atmosphere of an exoplanet when it passes in front of its host star. If the planet has an atmosphere, some of the stellar light will be absorbed as it passes through on its way towards Earth. https://t.co/pxIfBwbQCg', ""However, if we are observing with ground-based spectrographs, the stellar light will also pass through Earth's atmosphere and the spectra will be contaminated with tellurics. The strength of telluric lines changes throughout the night (e.g. with airmass)."", ""In the optical domain (wavelength range of data in this paper), the predominant sources of absorption are telluric water and oxygen. It is important to remove this contamination so that we can accurately characterise the exoplanet's atmosphere."", ""We compared two popular telluric correction methods, specifically focusing on Na detections in high-res optical transmission spectra:\n\n1⃣ Calculating the telluric absorption empirically based on the airmass\n2⃣ Generating a model of Earth's transmission spectrum with molecfit"", 'The airmass method allows us to scale the telluric lines such that they appear with the same strength in each observed spectrum, and are thereby removed when calculating transmission spectrum.', 'The molecfit method is particularly satisfying:\n\n👈Left: our molecfit model (red dashed) shows tellurics in the raw spectra (coloured)\n👉Right: tellurics are reduced to the continuum after dividing spectra by the model\n\nNote: molecfit gives a unique model for each observation 👍 https://t.co/AIaFY3IUFw', ""We applied both methods to three nights of HARPS observations of the transit of HD 189733b.\n\nLet's compare the planetary transmission spectrum using both methods: https://t.co/wVDRgX98zE"", 'By visually inspecting, it is hard to determine which method has done a better job, so we measure:\n➡️Properties of Gaussian fits to the Na lines\n➡️Mean absorption depth in different sized bands within the sodium region\n➡️Variation of results over all three nights', '📢Conclusion:\n\nCorrecting tellurics using molecfit is more robust.\n\nWhy?\n➡️Na detection with greater significance\n➡️More consistent measurements across all nights\n➡️Ability to extract a signal in poorer weather conditions\n➡️Less reliant on good/numerous out-of-transit spectra', '📊Results (using molecfit):\n\n1⃣ Confirmed detection of Na doublet with line contrasts of −0.64±0.07 % (D2) and −0.53±0.07 % (D1)\n2⃣ Effective photosphere in the Na line located around 1.13Rp\n3⃣ Net eastward atmospheric winds with a speed of 1.8±1.2 km/s', '📖Please refer to the paper for more details, including discussions about:\n➡️Centre-to-Limb Variation and the Rossiter-McLaughlin effect\n➡️Radial velocity corrections\n➡️Atmospheric winds\n\n(Many interesting references too, if you want to read further)\n\nThanks for reading!']",https://arxiv.org/abs/2101.05283,"Using high-resolution ground-based transmission spectroscopy to probe exoplanetary atmospheres is difficult due to the inherent telluric contamination from absorption in Earth's atmosphere. A variety of methods have previously been used to remove telluric features in the optical regime and calculate the planetary transmission spectrum. In this paper we present and compare two such methods, specifically focusing on Na detections using high-resolution optical transmission spectra: (1) calculating the telluric absorption empirically based on the airmass, and (2) using a model of the Earth's transmission spectrum. We test these methods on the transmission spectrum of the hot Jupiter HD 189733 b using archival data obtained with the HARPS spectrograph during three transits. Using models for Centre-to-Limb Variation and the Rossiter-McLaughlin effect, spurious signals which are imprinted within the transmission spectrum are reduced. We find that correcting tellurics with an atmospheric model of the Earth is more robust and produces consistent results when applied to data from different nights with changing atmospheric conditions. We confirm the detection of sodium in the atmosphere of HD 189733 b, with doublet line contrasts of -0.64 $\pm$ 0.07 % (D2) and -0.53 $\pm$ 0.07 % (D1). The average line contrast corresponds to an effective photosphere in the Na line located around 1.13 R$_p$. We also confirm an overall blueshift of the line centroids corresponding to net atmospheric eastward winds with a speed of 1.8 $\pm$ 1.2 km/s. Our study highlights the importance of accurate telluric removal for consistent and reliable characterisation of exoplanetary atmospheres using high-resolution transmission spectroscopy. ","Assessing telluric correction methods for Na detections with
  high-resolution exoplanet transmission spectroscopy"
108,1349789297591918598,145996116,Arunesh Mathur,"['In a new paper, @jonathanmayer, Mihir Kshirsagar, and I investigate a question that has been challenging researchers and policymakers: what makes a dark pattern, well, ""dark""? <LINK> [thread] <LINK>', ""There's a growing academic literature on dark patterns that defines and describes types of dark patterns. There have also been govt. reports and legislation on dark patterns. We compiled and compared these, and found that dark patterns reflect many related but distinct concerns! https://t.co/ri0i0zFSpW"", 'We argue that there is no single definition that can capture all of the dark patterns discourse. Instead, dark patterns are a family of related problems, much like (as @DanielSolove famously observed) privacy is a family of related problems.', ""We propose two themes that, while lacking the precision of a definition, broadly capture the field: modifying a user's decision space and manipulating information flow to a user. https://t.co/id0aRDZHy9"", 'We then connect the dark patterns literature to related scholarship in other academic disciplines, including nudges, behavioral economics, manipulation in philosophy, and market manipulation in law.', ""Based on this related work, we suggest 4 normative lenses for examining dark patterns: individual &amp; collective welfare, regulatory compliance, and autonomy. To our fellow researchers: let's go beyond listing dark patterns, and ground our work in these normative concerns."", 'Doing so will help us build a case for legislators and regulators to step in. That will also help us respond to the frequent counterargument that dark patterns are nothing more than aggressive marketing.', 'We show how future dark patterns research could directly address normative concerns by applying a range of well established HCI measurement methods. We encourage the HCI community to take the lead in using these methods to examine why, exactly, dark patterns are dark.', ""The paper will appear at #CHI2021, and it builds on fantastic work by @graydesign @liorjs @thorstenholz @christine_utz @ChSaiShruthi @floschaub @ariezrawaldman @finnmyrstad @CNIL @midasnouwens @mikarv @MillennialProf1 @SeriesofTubes and others whose Twitter handles I can't find."", 'And of course @harrybr, who started it all.🙂']",https://arxiv.org/abs/2101.04843,"There is a rapidly growing literature on dark patterns, user interface designs -- typically related to shopping or privacy -- that researchers deem problematic. Recent work has been predominantly descriptive, documenting and categorizing objectionable user interfaces. These contributions have been invaluable in highlighting specific designs for researchers and policymakers. But the current literature lacks a conceptual foundation: What makes a user interface a dark pattern? Why are certain designs problematic for users or society? We review recent work on dark patterns and demonstrate that the literature does not reflect a singular concern or consistent definition, but rather, a set of thematically related considerations. Drawing from scholarship in psychology, economics, ethics, philosophy, and law, we articulate a set of normative perspectives for analyzing dark patterns and their effects on individuals and society. We then show how future research on dark patterns can go beyond subjective criticism of user interface designs and apply empirical methods grounded in normative perspectives. ","What Makes a Dark Pattern... Dark? Design Attributes, Normative
  Considerations, and Measurement Methods"
109,1349684073128726529,47913264,Andreas Kipf,"['New work on learned cardinality estimation, led by @parimarjan. We show how to focus the learning process on estimates that matter to a query optimizer.\n\nWe also introduce a new, more realistic Cardinality Estimation Benchmark (CEB).\n\nPaper: <LINK> <LINK>', 'Joint work with @RyanMarcus, @hongzimao, @tatbul, @tim_kraska, and Mohammad Alizadeh.']",https://arxiv.org/abs/2101.04964,"Previous approaches to learned cardinality estimation have focused on improving average estimation error, but not all estimates matter equally. Since learned models inevitably make mistakes, the goal should be to improve the estimates that make the biggest difference to an optimizer. We introduce a new loss function, Flow-Loss, that explicitly optimizes for better query plans by approximating the optimizer's cost model and dynamic programming search algorithm with analytical functions. At the heart of Flow-Loss is a reduction of query optimization to a flow routing problem on a certain plan graph in which paths correspond to different query plans. To evaluate our approach, we introduce the Cardinality Estimation Benchmark, which contains the ground truth cardinalities for sub-plans of over 16K queries from 21 templates with up to 15 joins. We show that across different architectures and databases, a model trained with Flow-Loss improves the cost of plans (using the PostgreSQL cost model) and query runtimes despite having worse estimation accuracy than a model trained with Q-Error. When the test set queries closely match the training queries, both models improve performance significantly over PostgreSQL and are close to the optimal performance (using true cardinalities). However, the Q-Error trained model degrades significantly when evaluated on queries that are slightly different (e.g., similar but not identical query templates), while the Flow-Loss trained model generalizes better to such situations. For example, the Flow-Loss model achieves up to 1.5x better runtimes on unseen templates compared to the Q-Error model, despite leveraging the same model architecture and training data. ",Flow-Loss: Learning Cardinality Estimates That Matter
110,1348909420789575680,1242001549053898752,Marc Aubreville,"[""We just recently released a new dataset, covering a task that is even more challenging than mitotic figure detection, and also relevant for tumor prognostication: Bi- and multinucleated cells. Received very nice reviews at this year's @BVM_workshop. Paper: <LINK>""]",https://arxiv.org/abs/2101.01445,"Tumor cells with two nuclei (binucleated cells, BiNC) or more nuclei (multinucleated cells, MuNC) indicate an increased amount of cellular genetic material which is thought to facilitate oncogenesis, tumor progression and treatment resistance. In canine cutaneous mast cell tumors (ccMCT), binucleation and multinucleation are parameters used in cytologic and histologic grading schemes (respectively) which correlate with poor patient outcome. For this study, we created the first open source data-set with 19,983 annotations of BiNC and 1,416 annotations of MuNC in 32 histological whole slide images of ccMCT. Labels were created by a pathologist and an algorithmic-aided labeling approach with expert review of each generated candidate. A state-of-the-art deep learning-based model yielded an $F_1$ score of 0.675 for BiNC and 0.623 for MuNC on 11 test whole slide images. In regions of interest ($2.37 mm^2$) extracted from these test images, 6 pathologists had an object detection performance between 0.270 - 0.526 for BiNC and 0.316 - 0.622 for MuNC, while our model archived an $F_1$ score of 0.667 for BiNC and 0.685 for MuNC. This open dataset can facilitate development of automated image analysis for this task and may thereby help to promote standardization of this facet of histologic tumor prognostication. ","Dataset on Bi- and Multi-Nucleated Tumor Cells in Canine Cutaneous Mast
  Cell Tumors"
111,1347539745371516936,1908579919,Alexa,"[""The timing feels a little weird on this but...I have a new paper out the arxiv today looking at the stellar population gradients of the Dragonfly 44! They're pretty weird!\n\n<LINK> <LINK>"", ""Here's how DF44 looks with respect to other dwarf galaxies https://t.co/VjNr6yaVuF"", 'The upshot of all this is that the internal properties of DF44 (the stellar pops and the kinematics from @DokkumPieter\'s earlier paper using the same amazing KCWI data) suggest a very different SFH than we see in what I like to call the ""canonical"" dwarf population.', '@brant_robertson Thanks! It was a lot of fun working on it']",https://arxiv.org/abs/2101.02220,"We use the Keck Cosmic Web Imager integral-field unit spectrograph to: 1) measure the global stellar population parameters for the ultra-diffuse galaxy (UDG) Dragonfly 44 (DF44) to much higher precision than previously possible for any UDG, and 2) for the first time measure spatially-resolved stellar population parameters of a UDG. We find that DF44 falls below the mass--metallicity relation established by canonical dwarf galaxies both in and beyond the Local Group. We measure a flat radial age gradient ($m_{\rm age} \sim +0.01_{-0.08}^{+0.07}$ log Gyr kpc$^{-1}$) and a flat-to-positive metallicity gradient ($m_{\rm [Fe/H]} \sim +0.08_{-0.11}^{+0.11}$ dex kpc$^{-1}$), which are inconsistent with the gradients measured in similarly pressure-supported dwarf galaxies. We also measure a flat-to-negative [Mg/Fe] gradient ($m_{\rm [Mg/Fe]} \sim -0.18_{-0.17}^{+0.17}$ dex kpc$^{-1}$) such that the central $1.5$ kpc of DF44 has stellar population parameters comparable to metal-poor globular clusters. Overall, DF44 does not have internal properties similar to other dwarf galaxies and is inconsistent with it having been puffed up through a prolonged, bursty star-formation history, as suggested by some simulations. Rather, the evidence indicates that DF44 experienced an intense epoch of ""inside-out"" star formation and then quenched early and catastrophically, such that star-formation was cut off more quickly than in canonical dwarf galaxies. ","Spatially Resolved Stellar Spectroscopy of the Ultra-diffuse Galaxy
  Dragonfly 44. III. Evidence for an Unexpected Star-Formation History"
112,1360360721922592772,278791721,Maria De-Arteaga,"['📣New preprint! “Leveraging expert consistency to improve algorithmic decision support”, with @achould and A. Dubrawski. We propose the use of influence functions to learn from historical expert decisions and mitigate limitations of observed labels. <LINK>', 'Our approach allows the ML models to learn from experts when they are consistent, and learn from the observed labels elsewhere. Importantly, we consider settings where each case has been assessed by a single expert, and do not assume random assignment of experts to cases.', 'What type of limitations of observed labels can this mitigate? We focus on the selective labels problem and omitted payoff bias. Both of these challenges can be exacerbated in the presence of expert consistency (unless we learn from that consistency, as we propose to do).', 'In the context of child maltreatment hotline screenings, we find that (1) there are high-risk cases whose risk is considered by the experts but not wholly captured in the target labels used to train a deployed model, and (2) the proposed approach improves recall for these cases.', ""@Aaron_Horowitz @achould These are some great points &amp; questions! Please do dm, I'd love to continue the conversation in a more extended format. Short answers below."", '@Aaron_Horowitz @achould Regarding expertise of others, not just call workers--this is something I am very interested in! I have been thinking about both (1) heterogeneous expertise &amp; (2) differing objectives--we should chat!', '@Aaron_Horowitz @achould Reg. expertise of cws already embedded, its partly true--but the worry is that the algorithm deployed by the County may only flag cases likely to result in out-of-home placement. Thus, cases for which other forms of (less adverse) interventions are effective may not receive attn.']",https://arxiv.org/abs/2101.09648,"Due to their promise of superior predictive power relative to human assessment, machine learning models are increasingly being used to support high-stakes decisions. However, the nature of the labels available for training these models often hampers the usefulness of predictive models for decision support. In this paper, we explore the use of historical expert decisions as a rich--yet imperfect--source of information, and we show that it can be leveraged to mitigate some of the limitations of learning from observed labels alone. We consider the problem of estimating expert consistency indirectly when each case in the data is assessed by a single expert, and propose influence functions based methodology as a solution to this problem. We then incorporate the estimated expert consistency into the predictive model meant for decision support through an approach we term label amalgamation. This allows the machine learning models to learn from experts in instances where there is expert consistency, and learn from the observed labels elsewhere. We show how the proposed approach can help mitigate common challenges of learning from observed labels alone, reducing the gap between the construct that the algorithm optimizes for and the construct of interest to experts. After providing intuition and theoretical results, we present empirical results in the context of child maltreatment hotline screenings. Here, we find that (1) there are high-risk cases whose risk is considered by the experts but not wholly captured in the target labels used to train a deployed model, and (2) the proposed approach improves recall for these cases. ",Leveraging Expert Consistency to Improve Algorithmic Decision Support
113,1357666238131023872,326864247,Amanda Clare,"[""In @jamesravey's new paper (accepted to #EACL2021  <LINK>) we look at how to find words/phrases/entities that refer to the same thing in two different documents. For example, a news article that reports on findings from a published science paper."", 'This is a hard problem, for humans and for automation. We scratched our heads many times while hand-annotating the corpus (available from https://t.co/hXKytYqsmS). News language is different to science paper language, and terms have subtle but important semantics.', ""Vector space embeddings that allow semantically related entities to be neighbours, don't provide good enough separation to distinguish whether they're co-referent or not. But we'll need this to determine how science is reported in the news: which parts and how it's slanted."", ""This work was done with @xrysoflhs @ArieCattan and Ido Dagan and you can find more in @jamesravey's thread here:  https://t.co/OyfjmhyXIs""]",https://arxiv.org/abs/2101.12637,"Cross-document co-reference resolution (CDCR) is the task of identifying and linking mentions to entities and concepts across many text documents. Current state-of-the-art models for this task assume that all documents are of the same type (e.g. news articles) or fall under the same theme. However, it is also desirable to perform CDCR across different domains (type or theme). A particular use case we focus on in this paper is the resolution of entities mentioned across scientific work and newspaper articles that discuss them. Identifying the same entities and corresponding concepts in both scientific articles and news can help scientists understand how their work is represented in mainstream media. We propose a new task and English language dataset for cross-document cross-domain co-reference resolution (CD$^2$CR). The task aims to identify links between entities across heterogeneous document types. We show that in this cross-domain, cross-document setting, existing CDCR models do not perform well and we provide a baseline model that outperforms current state-of-the-art CDCR models on CD$^2$CR. Our data set, annotation tool and guidelines as well as our model for cross-document cross-domain co-reference are all supplied as open access open source resources. ",CD2CR: Co-reference Resolution Across Documents and Domains
114,1356269604612567046,3433220662,Anthony Bonato,"['New paper up on arXiv. We study a new model for complex hypernetworks and explore its properties related to motifs and clustering. Along the way, we introduce a new clustering coefficient for hypergraphs...there are many competing definitions!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2101.12560,"Complex networks are pervasive in the real world, capturing dyadic interactions between pairs of vertices, and a large corpus has emerged on their mining and modeling. However, many phenomena are comprised of polyadic interactions between more than two vertices. Such complex hypergraphs range from emails among groups of individuals, scholarly collaboration, or joint interactions of proteins in living cells. A key generative principle within social and other complex networks is transitivity, where friends of friends are more likely friends. The previously proposed Iterated Local Transitivity (ILT) model incorporated transitivity as an evolutionary mechanism. The ILT model provably satisfies many observed properties of social networks, such as densification, low average distances, and high clustering coefficients. We propose a new, generative model for complex hypergraphs based on transitivity, called the Iterated Local Transitivity Hypergraph (or ILTH) model. In ILTH, we iteratively apply the principle of transitivity to form new hypergraphs. The resulting model generates hypergraphs simulating properties observed in real-world complex hypergraphs, such as densification and low average distances. We consider properties unique to hypergraphs not captured by their 2-section. We show that certain motifs, which are specified subhypergraphs of small order, have faster growth rates in ILTH hypergraphs than in random hypergraphs with the same order and expected average degree. We show that the graphs admitting a homomorphism into the 2-section of the initial hypergraph appear as induced subgraphs in the 2-section of ILTH hypergraphs. We consider new and existing hypergraph clustering coefficients, and show that these coefficients have larger values in ILTH hypergraphs than in comparable random hypergraphs. ",The iterated local transitivity model for hypergraphs
115,1356247413665783811,904820264021749761,Samarth Mishra,"['New preprint out (<LINK>) for work done with @kate_saenko_ and Venkatesh Saligrama. \n\nWe find how effective self-supervised pretraining and image augmentation based consistency are on semi-supervised domain adaptation. (spoiler : very effective)\n\n1/ <LINK>', 'In visual domain adaptation, a learner is tasked with identifying classes in a target visual domain given labelled data in a different source domain (e.g. classifying real images using only labelled hand-sketches of objects). \n\n2/', 'A central element to a range of prior art is (adversarial) domain alignment --- trying to find a feature space where source and target domain data are indistinguishable.  \n\n3/', 'We find that with few target labels (available in semi-supervised domain adaptation) and our pretraining and consistency approach, the need for adversarial domain alignment is mitigated. \n\n4/', 'Pretraining and Consistency (PAC) outperforms prior art based on adversarial domain alignment on multiple benchmarks including the large and challenging DomainNet. \n\n5/5 https://t.co/ZM3EMJCXMl']",http://arxiv.org/abs/2101.12727,"Most modern unsupervised domain adaptation (UDA) approaches are rooted in domain alignment, i.e., learning to align source and target features to learn a target domain classifier using source labels. In semi-supervised domain adaptation (SSDA), when the learner can access few target domain labels, prior approaches have followed UDA theory to use domain alignment for learning. We show that the case of SSDA is different and a good target classifier can be learned without needing alignment. We use self-supervised pretraining (via rotation prediction) and consistency regularization to achieve well separated target clusters, aiding in learning a low error target classifier. With our Pretraining and Consistency (PAC) approach, we achieve state of the art target accuracy on this semi-supervised domain adaptation task, surpassing multiple adversarial domain alignment methods, across multiple datasets. PAC, while using simple techniques, performs remarkably well on large and challenging SSDA benchmarks like DomainNet and Visda-17, often outperforming recent state of the art by sizeable margins. Code for our experiments can be found at this https URL ","Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining
  and Consistency"
116,1355898548169175041,2279164099,Bruno Lepri,"['New paper accepted @FAccTConference. In this work, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. Link: <LINK>', 'Work done with @ElenaBeretta4, @phisaz, and @demartin']",https://arxiv.org/abs/2101.11358,"Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our method aims to deepen knowledge and promote awareness about the sampling practices employed to create the training set, highlighting that the probability of success or failure conditioned to a minority membership is given by the structure of the data available. We empirically test our system on three datasets commonly accessed by the machine learning community and we investigate the risk of racial discrimination. ","Detecting discriminatory risk through data annotation based on Bayesian
  inferences"
117,1355356522293780482,1097081523218599936,Shreya Gupta,['📢 #EACL2021 pre-print Alert! 📢\nOur paper accepted at EACL 2021 is up on ArXiV! Here we\n(1) propose a generalised claim detection model\n(2) release a new Twitter dataset (containing COVID-19 tweets) for claim detection\n\nLink: <LINK>\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2101.11891,"The conceptualization of a claim lies at the core of argument mining. The segregation of claims is complex, owing to the divergence in textual syntax and context across different distributions. Another pressing issue is the unavailability of labeled unstructured text for experimentation. In this paper, we propose LESA, a framework which aims at advancing headfirst into expunging the former issue by assembling a source-independent generalized model that captures syntactic features through part-of-speech and dependency embeddings, as well as contextual features through a fine-tuned language model. We resolve the latter issue by annotating a Twitter dataset which aims at providing a testing ground on a large unstructured dataset. Experimental results show that LESA improves upon the state-of-the-art performance across six benchmark claim datasets by an average of 3 claim-F1 points for in-domain experiments and by 2 claim-F1 points for general-domain experiments. On our dataset too, LESA outperforms existing baselines by 1 claim-F1 point on the in-domain experiments and 2 claim-F1 points on the general-domain experiments. We also release comprehensive data annotation guidelines compiled during the annotation phase (which was missing in the current literature). ","LESA: Linguistic Encapsulation and Semantic Amalgamation Based
  Generalised Claim Detection from Online Content"
118,1355178915854127107,1865461842,Luca Soldaini 🏳️‍🌈,"['New #EACL2021 paper with our fantastic intern @HanRujun is out! With @amoschitti1, we study the problem of making Answer Sentence Selection models more contextual without having to parse passages like slow Machine Reading Comprehension models do 🧵1/5 <LINK> <LINK>', '.@HanRujun experimented with two approaches for selecting relevant snippets from documents to use as context, as well as three different architectures for exploiting said context. 2/5 #EACL2021 https://t.co/b1XgdEsTqd', 'we found that, by using sentence similarity to find relevant snippets, and combining signals using a multi-way attention similar to Tan et al. (2018), we can essentially match the performance of an ensemble model with 2x parameters and 1.3x latency 3/5 #EACL2021 https://t.co/WtEM2bpqZo', 'for full analysis and results, see our paper on ArXiv https://t.co/zO0voouGWb Code is not out yet, but watch this space 👀 #EACL2021 4/5', 'lastly, I want to stress how wonderful was to have @HanRujun with us – remote internships during a pandemic are not easy, but Rujun did an amazing job with deepening our understanding of this important document reading / efficiency for QA! Ty so much for your hard work ☺️ 5/5']",https://arxiv.org/abs/2101.12093,"Answer Sentence Selection (AS2) is an efficient approach for the design of open-domain Question Answering (QA) systems. In order to achieve low latency, traditional AS2 models score question-answer pairs individually, ignoring any information from the document each potential answer was extracted from. In contrast, more computationally expensive models designed for machine reading comprehension tasks typically receive one or more passages as input, which often results in better accuracy. In this work, we present an approach to efficiently incorporate contextual information in AS2 models. For each answer candidate, we first use unsupervised similarity techniques to extract relevant sentences from its source document, which we then feed into an efficient transformer architecture fine-tuned for AS2. Our best approach, which leverages a multi-way attention architecture to efficiently encode context, improves 6% to 11% over noncontextual state of the art in AS2 with minimal impact on system latency. All experiments in this work were conducted in English. ","Modeling Context in Answer Sentence Selection Systems on a Latency
  Budget"
119,1354839940211867649,1168263794906324992,Ricard Alert Zenón,"[""👉 Does turbulence in active fluids exhibit universal scaling laws? Do Kolmogorov's ideas carry over? Check it out in our new preprint, where we experimentally find and theoretically explain three scaling regimes in active turbulence! 🌪️😃📝\n<LINK> <LINK>"", 'After finding universal scaling in simulations (https://t.co/XPHZcHvu4d), we proposed to test it in experiments. And our colleagues did it! But the experiments did not only verify the predicted scaling laws; they also revealed a new one that results from external dissipation. 🚀 https://t.co/umObCc285g', ""Great experiments by @berta_mp in Jordi Ignés-Mullol and Francesc Sagués's lab, and collaboration with Fanlong Meng, @RGolestanian, @JFJoanny, and Jaume Casademunt on the theory side.""]",http://arxiv.org/abs/2101.11570,"Active fluids exhibit complex turbulent-like flows at low Reynolds number. Recent work predicted that 2d active nematic turbulence follows universal scaling laws. However, experimentally testing these predictions is conditioned by the coupling to the 3d environment. Here, we measure the spectrum of the kinetic energy, $E(q)$, in an active nematic film in contact with a passive oil layer. At small and intermediate scales, we find the scaling regimes $E(q)\sim q^{-4}$ and $E(q)\sim q^{-1}$, respectively, in agreement with the theoretical prediction for 2d active nematics. At large scales, however, we find a new scaling $E(q)\sim q$, which emerges when the dissipation is dominated by the 3d oil layer. In addition, we derive an explicit expression for the spectrum that spans all length scales, thus explaining and connecting the different scaling regimes. This allows us to fit the data and extract the length scale that controls the crossover to the new large-scale regime, which we tune by varying the oil viscosity. Overall, our work experimentally demonstrates the emergence of universal scaling laws in active turbulence, and it establishes how the spectrum is affected by external dissipation. ",Scaling regimes of active turbulence with external dissipation
120,1354459400228851712,14544467,Daniel Apai,"['Excited to present our Bioverse study, fresh on arXiv! Great experiments test important hypotheses. What hypotheses can we test with biosignature-survey missions as a function of their capabilities? \n<LINK> @nexssinfo @EOSNExSS <LINK>', '@nexssinfo @EOSNExSS In our new paper led by  @azstewobs Alex Bixel we offer a comprehensive (and open-source!) modeling framework to simulate the diagnostic power of different biosignature-survey mission concepts, using @luvoirtelescope and @NautilusObs as examples. @uarizona @UArizonaLPL', '@nexssinfo @EOSNExSS @azstewobs @luvoirtelescope @NautilusObs @uarizona @UArizonaLPL With Bioverse you can do trade studies &amp; explore a broad range of population-level hypotheses, (both for transiting and direct imaging). Can we observationally test the existence of the hab. zone? Determine the oxygenation time for exoearths? What hypotheses would you test?']",https://arxiv.org/abs/2101.10393,"Next-generation space observatories will conduct the first systematic surveys of terrestrial exoplanet atmospheres and search for evidence of life beyond Earth. While in-depth observations of the nearest habitable worlds may yield enticing results, there are fundamental questions about planetary habitability and evolution which can only be answered through population-level studies of dozens to hundreds of terrestrial planets. To determine the requirements for next-generation observatories to address these questions, we have developed Bioverse. Bioverse combines existing knowledge of exoplanet statistics with a survey simulation and hypothesis testing framework to determine whether proposed space-based direct imaging and transit spectroscopy surveys will be capable of detecting various hypothetical statistical relationships between the properties of terrestrial exoplanets. Following a description of the code, we apply Bioverse to determine whether an ambitious direct imaging or transit survey would be able to determine the extent of the circumstellar habitable zone and study the evolution of Earth-like planets. Given recent evidence that Earth-sized habitable zone planets are likely much rarer than previously believed (Pascucci et al. 2019), we find that space missions with large search volumes will be necessary to study the population of terrestrial and habitable worlds. Moving forward, Bioverse provides a methodology for performing trade studies of future observatory concepts to maximize their ability to address population-level questions, including and beyond the specific examples explored here. ","Bioverse: a simulation framework to assess the statistical power of
  future biosignature surveys"
121,1354123148908777472,717162062837719040,Phil Armitage,"['New paper! In work led by Rebecca Martin @unlv we study the dynamics of circumplanetary gas disks during late-stage planet formation. We find - in a simplified model - that these disks can tilt out of the plane and oscillate in quite weird ways! Thread...\n\n<LINK> <LINK>', 'Circumplanetary disks form around massive planets growing within protoplanetary disks. Massive planets open a gap in the protoplanetary disk but continue to accrete, forming a smaller circumplanetary disk. https://t.co/1QNM6XOaOg', 'Disks are where the ""regular"" satellites of the giant planets (e.g. the Galilean moons) form. They may also be signposts for very young extrasolar planets. Disks are larger than planets, and accrete onto them, so they can be more detectable.\n\nhttps://t.co/yesOiSzOUt', ""Recently, we found that circumplanetary disks can be unstable to the growth of tilt. This built on work by Steve Lubow, Caroline Terquem, and others. It's a tidal effect, that works if the disk thickness and size are in the right range.\n\nhttps://t.co/Cny772Dg9Z https://t.co/1dWm5bLylw"", ""What happens if a disk in a binary becomes *really* tilted? Its inclination and eccentricity start to oscillate, in a fluid disk analog of the Kozai-Lidov (KL) effect that's important for planetary dynamics.\n\nhttps://t.co/igcgdobEW6 https://t.co/v9tIbQ31DN"", 'In the new work, we simulated the interaction of these two instabilities. We found that tilt growth is capped by the onset of KL oscillations, after which accretion onto the planet becomes episodic.', 'As usual there are open questions. Most notably we only looked at ""detached"" circumplanetary disks, so how ongoing accretion from the protoplanetary disk would change things is not clear. The same dynamics might also apply to other disks in binary systems.', ""Big thanks to Rebecca, and to the other co-authors Zhaohuan Zhu, Chao-Chin Yang, and Hans Baehr. For this and most of our prior simulations, we used @danprice_astro's fantastic PHANTOM SPH code:\n\nhttps://t.co/VhnZbSDUsh""]",https://arxiv.org/abs/2101.09388,"Circumplanetary discs can be linearly unstable to the growth of disc tilt in the tidal potential of the star-planet system. We use three-dimensional hydrodynamical simulations to characterize the disc conditions needed for instability, together with its long term evolution. Tilt growth occurs for disc aspect ratios, evaluated near the disc outer edge, of $H/r\gtrsim 0.05$, with a weak dependence on viscosity in the wave-like regime of warp propagation. Lower mass giant planets are more likely to have circumplanetary discs that satisfy the conditions for instability. We show that the tilt instability can excite the inclination to above the threshold where the circumplanetary disc becomes unstable to Kozai--Lidov (KL) oscillations. Dissipation in the Kozai--Lidov unstable regime caps further tilt growth, but the disc experiences large oscillations in both inclination and eccentricity. Planetary accretion occurs in episodic accretion events. We discuss implications of the joint tilt--KL instability for the detectability of circumplanetary discs, for the obliquity evolution of forming giant planets, and for the formation of satellite systems. ","Kozai-Lidov oscillations triggered by a tilt instability of detached
  circumplanetary discs"
122,1354085340244533262,171302416,Enrique Paillas,"['New paper out 😃 We propose to split the galaxy density field in quantiles with a fixed local density. The advantage is that the peculiar velocity PDF in each quantile is close to Gaussian, allowing more accurate modelling of redshift-space distortions. <LINK> <LINK>']",https://arxiv.org/abs/2101.09854,"Accurate modelling of redshift-space distortions (RSD) is challenging in the non-linear regime for two-point statistics e.g. the two-point correlation function (2PCF). We take a different perspective to split the galaxy density field according to the local density, and cross-correlate those densities with the entire galaxy field. Using mock galaxies, we demonstrate that combining a series of cross-correlation functions (CCFs) offers improvements over the 2PCF as follows: 1. The distribution of peculiar velocities in each split density is nearly Gaussian. This allows the Gaussian streaming model for RSD to perform accurately within the statistical errors of a ($1.5\,h^{-1}$Gpc)$^3$ volume for almost all scales and all split densities. 2. The PDF of the density field at small scales is non-Gaussian, but the CCFs of split densities capture the non-Gaussianity, leading to improved cosmological constraints over the 2PCF. We can obtain unbiased constraints on the growth parameter $f\sigma_{12}$ at the per-cent level, and Alcock-Paczynski (AP) parameters at the sub-per-cent level with the minimal scale of $15\,h^{-1}{\rm Mpc}$. This is a $\sim$30 per cent and $\sim$6 times improvement over the 2PCF, respectively. The diverse and steep slopes of the CCFs at small scales are likely to be responsible for the improved constraints of AP parameters. 3. Baryon acoustic oscillations (BAO) are contained in all CCFs of split densities. Including BAO scales helps to break the degeneracy between the line-of-sight and transverse AP parameters, allowing independent constraints on them. We discuss and compare models for RSD around spherical densities. ",Redshift-space distortions with split densities
123,1354078011713327106,1240116365593317379,Pablo Piaggi,['New #preprint with Roberto Car! We study the order-disorder transition of the protons in hexagonal ice. Made possible by a machine learning model and the EnvironmentSimilarity CV in @plumed_org . Feedback is appreciated! <LINK>'],https://arxiv.org/abs/2101.09308,"Ice Ih, the common form of ice in the biosphere, contains proton disorder. Its proton-ordered counterpart, ice XI, is thermodynamically stable below 72 K. However, even below this temperature the formation of ice XI is kinetically hindered and experimentally it is obtained by doping ice with KOH. Doping creates ionic defects that promote the migration of protons and the associated change in proton configuration. In this article, we mimic the effect of doping in molecular dynamics simulations using a bias potential that enhances the formation of ionic defects. The recombination of the ions thus formed proceeds through fast migration of the hydroxide and results in the jump of protons along a hydrogen bond loop. This provides a physical and expedite way to change the proton configuration, and to accelerate diffusion in proton configuration space. A key ingredient of this approach is a machine learning potential trained with density functional theory data and capable of modeling molecular dissociation. We exemplify the usefulness of this idea by studying the order-disorder transition using an appropriate order parameter to distinguish the proton environments in ice Ih and XI. We calculate the changes in free energy, enthalpy, and entropy associated with the transition. Our estimated entropy agrees with experiment within the error bars of our calculation. ","Enhancing the formation of ionic defects to study the ice Ih/XI
  transition with molecular dynamics simulations"
124,1353942431415328769,526115229,Kevin Heng,"['Our study of Jupiter’s Cassini phase curves is now on arXiv. We were fortunate enough to receive a gracious first referee’s report and are holding off on resubmission to possibly receive community criticism. <LINK> [5/n] <LINK>', ""@aymeric_spiga That's fascinating. This is exactly the type of feedback I was looking for, i.e., independent lines of evidence about the Jovian aerosol properties."", '@aymeric_spiga Could you point out specific references where properties of the Jovian aerosols are reported largely *independent* of models? I was having trouble finding these... thanks.']",http://arxiv.org/abs/2101.09984,"Due to its proximity to Earth, Jupiter of the Solar System serves as a unique case study for gas-giant exoplanets. In the current study, we perform fits of ab initio, reflective, semi-infinite, homogeneous model atmospheres to 61 phase curves from 0.40 to 1.00 $\mu$m, obtained from the Cassini spacecraft, within a Bayesian framework. We reproduce the previous finding that atmospheric models using classic reflection laws (Lambertian, Rayleigh, single Henyey-Greenstein) provide poor fits to the data. Using the double Henyey-Greenstein reflection law, we extract posterior distributions of the single-scattering albedo and scattering asymmetry factors and tabulate their median values and uncertainties. We infer that the aerosols in the Jovian atmosphere are large, irregular, polydisperse particles that produce strong forward scattering together with a narrow backscattering lobe. The near-unity values of the single-scattering albedos imply that multiple scattering of radiation is an important effect. We speculate that the observed narrow backscattering lobe is caused by coherent backscattering of radiation, which is usually associated with Solar System bodies with solid surfaces and regolith. Our findings demonstrate that precise, multi-wavelength phase curves encode valuable information on the fundamental properties of cloud/haze particles. The method described in this Letter enables single-scattering albedos and scattering asymmetry factors to be retrieved from James Webb Space Telescope phase curves of exoplanets. ",Jupiter as an Exoplanet: Insights from Cassini Phase Curves
125,1353867082853617666,30989098,Karin Sandstrom,"['Some quick advertising: a new paper from former UCSD postdoc Jeremy Chastenet on a comparative study of far-IR dust modeling results in M101! <LINK> We compare the dust masses, PAH fractions, radiation field and more from six widely used dust SED models.', ""A key finding is that over parts of M101 all physical dust models led to too much dust compared to the maximum limit you'd expect from the metallicity gradient. This may be due to the elemental depletion constraints assumed for the MW cirrus calibration in each model.""]",https://arxiv.org/abs/2101.09236,"We present a comparative study of four physical dust models and two single-temperature modified blackbody models by fitting them to the resolved WISE, Spitzer, and Herschel photometry of M101 (NGC 5457). Using identical data and a grid-based fitting technique, we compare the resulting dust and radiation field properties derived from the models. We find that the dust mass yielded by the different models can vary by up to factor of 3 (factor of 1.4 between physical models only), although the fits have similar quality. Despite differences in their definition of the carriers of the mid-IR aromatic features, all physical models show the same spatial variations for the abundance of that grain population. Using the well determined metallicity gradient in M101 and resolved gas maps, we calculate an approximate upper limit on the dust mass as a function of radius. All physical dust models are found to exceed this maximum estimate over some range of galactocentric radii. We show that renormalizing the models to match the same Milky Way high latitude cirrus spectrum and abundance constraints can reduce the dust mass differences between models and bring the total dust mass below the maximum estimate at all radii. ",Benchmarking Dust Emission Models in M101
126,1353668715586449410,1345856121660178433,Annabelle Bohrdt,"['Super excited about our new work: we find resonances in a rotational extension of ARPES, strongly suggesting that dopants in an AFM can be understood as bound states of partons, similar to quarks in high energy physics. We even find Regge-like trajectories!<LINK> <LINK>']",https://arxiv.org/abs/2101.09280,"Understanding the nature of charge carriers in doped Mott insulators holds the key to unravelling puzzling properties of strongly correlated electron systems, including cuprate superconductors. Several theoretical models suggested that dopants can be understood as bound states of partons, the analogues of quarks in high-energy physics. However, direct signatures of spinon-chargon bound states are lacking, both in experiment and theory. Here we numerically identify long-lived rotational resonances at low doping, which directly reveal the microscopic structure of spinon-chargon bound states. Similar to Regge trajectories reflecting the quark structure of mesons, we establish a linear dependence of the rotational energy on the super-exchange coupling. Rotational excitations are strongly suppressed in standard angle-resolved photo-emission (ARPES) spectra, but we propose a multi-photon rotational extension of ARPES where they have strong spectral weight. Our findings suggest that multi-photon spectroscopy experiments should provide new insights into emergent universal features of strongly correlated electron systems. ","Rotational Resonances and Regge Trajectories in Lightly Doped
  Antiferromagnets"
127,1353618083743756288,2607616422,Ioannis Arapakis,"[""In the modern Web, privacy is becoming a rare commodity. With @luileito and C. Iordanou we show how straightforward is to capture behavioural data at scale and unobtrusively, and propose an adversarial method to mitigate profiling techniques (CHIIR '21)\n<LINK> <LINK>""]",https://arxiv.org/abs/2101.09087,"This paper aims to stir debate about a disconcerting privacy issue on web browsing that could easily emerge because of unethical practices and uncontrolled use of technology. We demonstrate how straightforward is to capture behavioral data about the users at scale, by unobtrusively tracking their mouse cursor movements, and predict user's demographics information with reasonable accuracy using five lines of code. Based on our results, we propose an adversarial method to mitigate user profiling techniques that make use of mouse cursor tracking, such as the recurrent neural net we analyze in this paper. We also release our data and a web browser extension that implements our adversarial method, so that others can benefit from this work in practice. ","My Mouse, My Rules: Privacy Issues of Behavioral User Profiling via
  Mouse Tracking"
128,1353184036311580672,12448762,Andrew Lavin,"['RegNet doubts confirmed:\n\n""Contrary to Radosavovic et al. (2020) which found that inverted bottlenecks (Sandler et al., 2018) were not performant, we find that inverted bottlenecks strongly outperformed their compressive bottleneck counterparts [...]"" <LINK>']",https://arxiv.org/abs/2101.08692,"Batch Normalization is a key component in almost all state-of-the-art image classifiers, but it also introduces practical challenges: it breaks the independence between training examples within a batch, can incur compute and memory overhead, and often results in unexpected bugs. Building on recent theoretical analyses of deep ResNets at initialization, we propose a simple set of analysis tools to characterize signal propagation on the forward pass, and leverage these tools to design highly performant ResNets without activation normalization layers. Crucial to our success is an adapted version of the recently proposed Weight Standardization. Our analysis tools show how this technique preserves the signal in networks with ReLU or Swish activation functions by ensuring that the per-channel activation means do not grow with depth. Across a range of FLOP budgets, our networks attain performance competitive with the state-of-the-art EfficientNets on ImageNet. ","Characterizing signal propagation to close the performance gap in
  unnormalized ResNets"
129,1353068668016451584,831775428,Yashar Mehdad,"['Great work by our intern, Yilun Zhou, just accepted at #AISTATS2021. In this work we propose a simulated annealing algorithm to search for the optimal active learning strategy for a given base learner and analyze it for several different tasks: <LINK>']",https://arxiv.org/abs/2101.00977,"Active learning (AL) algorithms may achieve better performance with fewer data because the model guides the data selection process. While many algorithms have been proposed, there is little study on what the optimal AL algorithm looks like, which would help researchers understand where their models fall short and iterate on the design. In this paper, we present a simulated annealing algorithm to search for this optimal oracle and analyze it for several tasks. We present qualitative and quantitative insights into the behaviors of this oracle, comparing and contrasting them with those of various heuristics. Moreover, we are able to consistently improve the heuristics using one particular insight. We hope that our findings can better inform future active learning research. The code is available at this https URL ","Towards Understanding the Behaviors of Optimal Deep Active Learning
  Algorithms"
130,1351906274074652676,991380306,James Jackman,"['Today\'s the big day! We have a new paper out! It\'s a study of white-light stellar flares from ""blended and neighbouring stars"" in the Kepler short cadence data (<LINK>). But what does it mean?', ""Every so often in astronomical studies, you spot a flare that doesn't actually come from the type of star you're interested in (e.g. a Sun-like star). Instead, it comes from a faint star just in the edge of your aperture. This can be a nuisance and is thrown out."", 'These intruders can be avoided by using a carefully curated target list (e.g. no stars with close neighbours), or by vetting your data each time you see a signal. But what happens to all those discarded flares? Can we do anything with them?', ""That's what we decided to look into, using the Kepler short cadence data. We realised that some of the discarded flares could be really energetic. After all, the faint star would have to contribute a decent bit of light relative to the bright source just for the flare to be seen."", 'So, we reanalysed the Kepler SC data, looking for flares that would have been thrown out by other studies. We found that about 7% of flares in the Kepler SC data come from these interlopers.', 'We found that some of the largest flares in our sample pushed the energies that had been seen before with Kepler, and measured the flare rates of some of these stars. The highest amplitude flare changed the brightness of the star by 4 magnitudes in the Kepler bandpass!', 'Looking for flares from stars close to other stars also meant we found flares on wide binary systems! We found that in this co-eval systems, the lower mass stars flared more often, as we expected from previous works.', ""This was a fun paper to work on and it highlights that while it's good to have a clean sample, sometimes it's worth double checking the things you throw away. Enjoy :) https://t.co/LyN3CWfehp""]",https://arxiv.org/abs/2101.07269,"We present the results of a search for stellar flares from stars neighbouring the target sources in the Kepler short cadence data. These flares have been discarded as contaminants in previous surveys and therefore provide an unexplored resource of flare events, in particular high energy events from faint stars. We have measured M dwarf flare energies up to 1.5$\times$10^35 erg, pushing the limit for flare energies measured using Kepler data. We have used our sample to study theflaring activity of wide binaries, finding that the lower mass counterpart in a wide binary flares more often at a given energy. Of the 4430 flares detected in our original search, 298 came from a neighbouring star, a rate of 6.7$\pm$0.4 per cent for the Kepler short cadence lightcurves. We have used our sample to estimate a 5.8$\pm$0.1 per cent rate of false positive flare events in studies using TESS short cadence data. ","Stellar flares from blended and neighbouring stars in Kepler short
  cadence observations"
131,1351855942393069568,232638177,Thijs Smolders,"['We find that pressure can have a strongly anisotropic effect on the ionic mobility in CsPbBr3, resulting in effective 2D-mobility at higher pressures. Read all about it here: <LINK> <LINK>']",https://arxiv.org/abs/2101.06765,"We study the effects of hydrostatic pressure in the range 0.0--2.0 GPa on anion mobility in the orthorhombic $Pnma$ phase of CsPbBr$_{3}$. Using density functional theory and the climbing nudged elastic band method, we calculate the transition states and activation energies for anions to migrate both within and between neighbouring PbBr$_{3}$ octahedra. The results of those calculations are used as input to a kinetic model for anion migration, which we solve in the steady state to determine the anion mobility tensor as a function of applied pressure. We find that the response of the mobility tensor to increasing pressure is highly anisotropic, being strongly enhanced in the $(010)$ lattice plane and strongly reduced in the direction normal to it at elevated pressure. These results demonstrate the potentially significant influence of pressure and strain on the magnitude and direction of anion migration in lead--halide perovskites. ",3D-to-2D Transition of Anion Mobility in CsPbBr$_{3}$ under Pressure
132,1351838478879879168,50901426,Rafael Alves Batista,"['New paper on #arXiv today, with my collaborators @AstroUSP. <LINK>\nWe study the production of high-energy neutrinos in clusters of galaxies. We show that it contributes to a fraction of the flux measured by IceCube. #paperday <LINK>']",https://arxiv.org/abs/2101.07702,"Clusters of galaxies can potentially produce cosmic rays (CRs) up to very-high energies via large-scale shocks and turbulent acceleration. Due to their unique magnetic-field configuration, CRs with energy $\leq 10^{17}$ eV can be trapped within these structures over cosmological time scales, and generate secondary particles, including neutrinos and gamma rays, through interactions with the background gas and photons. In this work, we compute the contribution from clusters of galaxies to the diffuse neutrino background. We employ three-dimensional cosmological magnetohydrodynamical simulations of structure formation to model the turbulent intergalactic medium. We use the distribution of clusters within this cosmological volume to extract the properties of this population, including mass, magnetic field, temperature, and density. We propagate CRs in this environment using multi-dimensional Monte Carlo simulations across different redshifts (from $z \sim 5$ to $z =0$), considering all relevant photohadronic, photonuclear, and hadronuclear interaction processes. We find that, for CRs injected with a spectral index $\alpha = 1.5 - 2.7$ and cutoff energy $E_\text{max} = 10^{16} - 5\times10^{17} \; \text{eV}$, clusters contribute to a sizeable fraction to the diffuse flux observed by the IceCube Neutrino Observatory, but most of the contribution comes from clusters with $M \gtrsim 10^{14} \; M_{\odot}$ and redshift $ z \lesssim 0.3$. If we include the cosmological evolution of the CR sources, this flux can be even higher. ",High-Energy Neutrino Production in Clusters of Galaxies
133,1351792835591348224,1176867972163559424,MartinHuber,"['In this study (joint with @SchelkerMark and S Berset), we use #causalmachinelearning to assess the effect of fiscal shocks (unexpected changes in property tax revenues) on public spending. Local policymakers appear to smooth fiscal shocks, see <LINK> #EconTwitter <LINK>']",https://arxiv.org/abs/2101.07661,"We study the impact of fiscal revenue shocks on local fiscal policy. We focus on the very volatile revenues from the immovable property gains tax in the canton of Zurich, Switzerland, and analyze fiscal behavior following large and rare positive and negative revenue shocks. We apply causal machine learning strategies and implement the post-double-selection LASSO estimator to identify the causal effect of revenue shocks on public finances. We show that local policymakers overall predominantly smooth fiscal shocks. However, we also find some patterns consistent with fiscal conservatism, where positive shocks are smoothed, while negative ones are mitigated by spending cuts. ",The fiscal response to revenue shocks
134,1351708095769001985,4712972292,Anh N. Thai,"['New preprint: Does Continual Learning = Catastrophic Forgetting?\n\nIn this work, we (me, @sstj5, and @RehgJim) study the surprising behavior of reconstruction tasks in continual learning.\n\narxiv: <LINK>\ncode:  <LINK>', 'Project website: https://t.co/M2Cb9eJVTo', '@m_usmanrafique Thank you! Please feel free to post any questions you might have :)']",http://arxiv.org/abs/2101.07295,"Continual learning has been extensively studied for classification tasks with methods developed to primarily avoid catastrophic forgetting, a phenomenon where earlier learned concepts are forgotten at the expense of more recent samples. In this work, we present a set of continual 3D object shape reconstruction tasks including complete 3D shape reconstruction from different input modalities and visible surface (2.5D) reconstruction which surprisingly demonstrates positive knowledge (backward and forward) transfer when training with solely vanilla SGD and without additional heuristics. We provide evidence that continuously updated representation learning of single-view 3D shape reconstruction improves the performance on learned and novel categories over time. We provide a novel analysis of knowledge transfer ability by looking at the output distribution shift across sequential learning tasks. Finally, we show that the robustness of these tasks leads to the potential of having a proxy representation learning task for continual classification. The codebase, dataset, and pre-trained models released with this article can be found at this https URL ","The Surprising Positive Knowledge Transfer in Continual 3D Object Shape
  Reconstruction"
135,1351642932223373312,1349790738591199234,Yuchen Liang,"['In our #ICLR2021 paper we study a well-established neurobiological network motif from the fruit fly brain and investigate the possibility of reusing its architecture for solving common natural language processing tasks.\nPaper: <LINK> <LINK>', 'Our fruit fly network generates binary logical word embeddings - vectors of [0,1] as opposed to continuous vectors like GloVe and word2vec, which is useful from the perspective of memory efficiency and interpretability.', 'We show that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, additionally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint).', 'Work done by @YuchenLiangRPI\n @wrong_whp\n @Ben_Hoov\n @LeopoldGrinberg\n @navlakha_lab\n @mj_zaki\n @DimaKrotov']",https://arxiv.org/abs/2101.06887,"The mushroom body of the fruit fly brain is one of the best studied systems in neuroscience. At its core it consists of a population of Kenyon cells, which receive inputs from multiple sensory modalities. These cells are inhibited by the anterior paired lateral neuron, thus creating a sparse high dimensional representation of the inputs. In this work we study a mathematical formalization of this network motif and apply it to learning the correlational structure between words and their context in a corpus of unstructured text, a common natural language processing (NLP) task. We show that this network can learn semantic representations of words and can generate both static and context-dependent word embeddings. Unlike conventional methods (e.g., BERT, GloVe) that use dense representations for word embedding, our algorithm encodes semantic meaning of words and their context in the form of sparse binary hash codes. The quality of the learned representations is evaluated on word similarity analysis, word-sense disambiguation, and document classification. It is shown that not only can the fruit fly network motif achieve performance comparable to existing methods in NLP, but, additionally, it uses only a fraction of the computational resources (shorter training time and smaller memory footprint). ",Can a Fruit Fly Learn Word Embeddings?
136,1351467778008633344,1314104323,The Anh Han,['New pre-print in which we study evolutionary game models of social vs task cohesion for promoting cooperation in repeated public goods game  <LINK>  @QXinglong'],http://arxiv.org/abs/2101.06961,"Using methods from evolutionary game theory, this paper investigates the difference between social cohesion and task cohesion in promoting the evolution of cooperation in group interactions. Players engage in public goods games and are allowed to leave their groups if too many defections occur. Both social cohesion and task cohesion may prevent players from leaving. While a higher level of social cohesion increases a player's tolerance towards defections, task cohesion is associated with her group performance in the past. With a higher level of task cohesion, it is more likely that a dissatisfied player will refer to the history and remains in her group if she was satisfied in the past. Our results reveal that social cohesion is detrimental to the evolution of cooperation while task cohesion facilitates it. This is because social cohesion hinders the conditional dissociation mechanism but task cohesion improves the robustness of cooperative groups which are usually vulnerable to mistakes. We also discuss other potential aspects of cohesion and how they can be investigated through our modelling. Overall, our analysis provides novel insights into the relationship between group cohesion and group performance through studying the group dynamics and suggests further application of evolutionary game theory in this area. ",Social cohesion V.S. task cohesion: An evolutionary game theory study
137,1350547329028419585,257986573,Khalid Saqr,['#COVID19 spreads via #airborne transmission of expiratory aerosols. #CFD is used to simulate the #aerodynamics of #SARS_CoV_2. Read my #SystematicReview of CFD studies of COVID-19 and find out how can we evaluate the reproducibility of CFD simulations: <LINK>'],https://arxiv.org/abs/2101.04874,"There is overwhelming evidence on SARS-CoV-2 Airborne Transmission (AT) in the ongoing COVID-19 outbreak. It is extraordinarily difficult, however, to deduce a generalized framework to assess the relative airborne transmission risk with respect to other modes. This is due to the complex biophysics entailed in such phenomena. Since the SARS outbreak in 2002, Computational Fluid Dynamics (CFD) has been one of the main tools scientists used to investigate AT of respiratory viruses. Now, CFD simulations produce intuitive and physically plausible colour-coded results that help scientists understand SARS-CoV-2 airborne transmission patterns. In addition to validation requirements, for any CFD model to be of epistemic value to the scientific community; it must be reproducible. In 2020, more than 45 published studies investigated SARS-CoV-2 airborne transmission in different scenarios using CFD. Here, I systematically review the published CFD studies of COVID-19 and discuss their reproducibility criteria with respect to the CFD modeling process. Using a Weighted Scoring Model (WSM), I propose a novel reproducibility index for CFD simulations of SARS-CoV-2 AT. The proposed index $(0 \leq R^{CFD}_j \leq 1)$ relies on three reproducibility criteria comprising 10 elements that represent the possibility of a CFD study (j) to be reproduced. Frustratingly, only 3 of 23 studies (13%) achieved full reproducibility index $(R^{CFD}_j\geq 0.9)$ while the remaining 87% were found generally irreproducible $(R^{CFD}_j<0.9)$. Without reproducible models, the scientific benefit of CFD simulations will remain hindered, fragmented and limited. In conclusion, I call the scientific community to apply more rigorous measures on reporting and publishing CFD simulations in COVID-19 research. ","Amicus Plato, sed magis amica veritas: There is a reproducibility crisis
  in COVID-19 Computational Fluid Dynamics studies"
138,1350185982411288577,1198960462303551488,Parrinello Group,"['Latest arXiv Preprint on the development of collective variables based on 3D structure factor peaks that are combined non-linearly using the DeepLDA method. We apply these CVs for the study of crystallization - with \n@TarakChem @inve_michele  Valerio Rizzi\n<LINK>', '@plumed_org Input files @PlumedN plumID:21.005']",https://arxiv.org/abs/2101.03150,"The phenomenon of solidification of a substance from its liquid phase is of the greatest practical and theoretical importance, and atomistic simulations can provide precious information towards its understanding and control. Unfortunately, the time scale for crystallization is much larger than what can be explored in standard simulations. Enhanced sampling methods can overcome this time scale hurdle. Here we employ the on-the-fly probability enhanced sampling method that is a recent evolution of metadynamics. This method, like many others, relies on the definition of appropriate collective variables able to capture the slow degrees of freedom. To this effect we introduce collective coordinates of general applicability to crystallization simulations. They are based on the peaks of the three-dimensional structure factor that are combined non-linearly via the Deep Linear Discriminant Analysis machine learning method. We apply the method to the study of crystallization of a multicomponent system, Sodium Chloride and a molecular system, Carbon Dioxide. ",Collective Variables for the Study of Crystallization
139,1350064428348297216,1349798626483187713,Ivan Esteban,"['Paper out!\nIn <LINK>, Jordi Salvado &amp; myself ask ourselves: why do we always assume ideal gases in cosmology? \nWe study how long-range interactions change the energy density and equation of state (eos). And, what happens if neutrinos self-interact? <LINK>', 'This plot shows eos as a function of temperature for an interacting system:\n1) It is ultrarelativistic (w=1/3) for temperatures well below the particle mass. For neutrinos, this means they would be relativistic longer!\n2) Notice the dramatic difference wrt ideal gas [dashed]! https://t.co/THqijhjsxX', 'When applied to self-interacting neutrinos,\n1) There is *no* relevant cosmological neutrino mass bound. #KATRIN could detect something soon!\n2) Future surveys as EUCLID could fail to detect neutrino masses.\n\nWe have also made our cosmo code public! https://t.co/Oi9oVsApuj']",https://arxiv.org/abs/2101.05804,"Cosmology is well suited to study the effects of long range interactions due to the large densities in the early Universe. In this article, we explore how the energy density and equation of state of a fermion system diverge from the commonly assumed ideal gas form under the presence of scalar long range interactions with a range much smaller than cosmological scales. In this scenario, ""small""-scale physics can impact our largest-scale observations. As a benchmark, we apply the formalism to self-interacting neutrinos, performing an analysis to present and future cosmological data. Our results show that the current cosmological neutrino mass bound is fully avoided in the presence of a long range interaction, opening the possibility for a laboratory neutrino mass detection in the near future. We also demonstrate an interesting complementarity between neutrino laboratory experiments and the future EUCLID survey. ",Long Range Interactions in Cosmology: Implications for Neutrinos
140,1349331747255840769,1175368802458120193,Andreas Sander,"[""Today on @arxiv you'll find an interesting paper by @astro_noel, where I am also a co-author on. H-free WN stars are benchmarks as almost ideal He stars. With WR133, we have the first dynamical mass determination of such a WN and it is lower than expected: <LINK>""]",https://arxiv.org/abs/2101.04232,"We present the first visual orbit for the nitrogen-rich Wolf-Rayet binary, WR 133 (WN5o + O9I) based on observations made with the CHARA Array and the MIRC-X combiner. This orbit represents the first visual orbit for a WN star and only the third Wolf-Rayet star with a visual orbit. The orbit has a period of 112.8 d, a moderate eccentricity of 0.36, and a separation of $a$= 0.79 mas on the sky. We combine the visual orbit with an SB2 orbit and Gaia parallax to find that the derived masses of the component stars are $M_{\rm WR}$ = $9.3\pm1.6 M_\odot$ and $M_{\rm O}$ = $22.6\pm 3.2 M_\odot$, with the large errors owing to the nearly face-on geometry of the system combined with errors in the spectroscopic parameters. We also derive an orbital parallax that is identical to the {\it Gaia}-determined distance. We present a preliminary spectral analysis and atmosphere models of the component stars, and find the mass-loss rate in agreement with polarization variability and our orbit. However, the derived masses are low compared to the spectral types and spectral model. Given the close binary nature, we suspect that WR 133 should have formed through binary interactions, and represents an ideal target for testing evolutionary models given its membership in the cluster NGC 6871. ","The First Dynamical Mass Determination of a Nitrogen-rich Wolf-Rayet
  Star using a Combined Visual and Spectroscopic Orbit"
141,1349297718695505921,978638177048162305,Mirko Signorelli,"['New #arXiv preprint -&gt; <LINK>\nWe propose #PenalizedRegressionCalibration (PRC), a statistical method that makes it possible to predict #survival using #longitudinal AND #highdimensional predictors. Implemented in the #RStats package #pencal, available on #CRAN <LINK>']",https://arxiv.org/abs/2101.04426,"Longitudinal and high-dimensional measurements have become increasingly common in biomedical research. However, methods to predict survival outcomes using covariates that are both longitudinal and high-dimensional are currently missing. In this article, we propose penalized regression calibration (PRC), a method that can be employed to predict survival in such situations. PRC comprises three modeling steps: First, the trajectories described by the longitudinal predictors are flexibly modeled through the specification of multivariate mixed effects models. Second, subject-specific summaries of the longitudinal trajectories are derived from the fitted mixed models. Third, the time to event outcome is predicted using the subject-specific summaries as covariates in a penalized Cox model. To ensure a proper internal validation of the fitted PRC models, we furthermore develop a cluster bootstrap optimism correction procedure that allows to correct for the optimistic bias of apparent measures of predictiveness. PRC and the CBOCP are implemented in the R package pencal, available from CRAN. After studying the behavior of PRC via simulations, we conclude by illustrating an application of PRC to data from an observational study that involved patients affected by Duchenne muscular dystrophy, where the goal is predict time to loss of ambulation using longitudinal blood biomarkers. ","Penalized regression calibration: a method for the prediction of
  survival outcomes using complex longitudinal and high-dimensional data"
142,1349278991501455365,723657183846666241,Giuseppe Carleo,"['We propose an efficient quantum algorithm to simulate unitary dynamics with fixed-depth quantum circuits. Two features at once: effective linear scaling with n. of parameters + global optimization, not local! with S. Barison and @philipVinc @EPFL_en  <LINK>', 'This can result in faster optimization/access to much more complex parameterized circuits, when compared to more standard methods based on the time-dependent variational principle (that require to estimate the quantum geometry tensor and scale quadratically with number of pars)', ""We dub the method projected - Variational Quantum Dynamics, and variational trajectories for the gate parameters reduce to the same obtained by McLachlan's variational principle, while completely bypassing the costly (quadratic) estimation of the quantum geometric tensor https://t.co/eRLK3cd6oh"", '@steve_quantum @philipVinc @EPFL_en Some initialization strategies are described in the appendix. The gradient decent we use is a bit special because we start infinitesimally close to the optimal solution, so this makes it less prone to plateaus. it would be interesting to develop a more formal understanding though']",https://arxiv.org/abs/2101.04579,"We introduce a novel hybrid algorithm to simulate the real-time evolution of quantum systems using parameterized quantum circuits. The method, named ""projected - Variational Quantum Dynamics"" (p-VQD) realizes an iterative, global projection of the exact time evolution onto the parameterized manifold. In the small time-step limit, this is equivalent to the McLachlan's variational principle. Our approach is efficient in the sense that it exhibits an optimal linear scaling with the total number of variational parameters. Furthermore, it is global in the sense that it uses the variational principle to optimize all parameters at once. The global nature of our approach then significantly extends the scope of existing efficient variational methods, that instead typically rely on the iterative optimization of a restricted subset of variational parameters. Through numerical experiments, we also show that our approach is particularly advantageous over existing global optimization algorithms based on the time-dependent variational principle that, due to a demanding quadratic scaling with parameter numbers, are unsuitable for large parameterized quantum circuits. ","An efficient quantum algorithm for the time evolution of parameterized
  circuits"
143,1348814338224742400,1326939843767775232,Fong Group at Northwestern,"['While LIGO is offline, we can still look for kilonovae! PhD student Jillian Rastinejad leads a study on a catalog of 85 short GRBs to place constraints on kilonova ejecta properties! So much data and diversity!\n\n<LINK>']",https://arxiv.org/abs/2101.03175,"The discovery of GW170817 and GRB 170817A in tandem with AT 2017gfo cemented the connection between neutron star mergers, short gamma-ray bursts (GRBs), and kilonovae. To investigate short GRB observations in the context of diverse kilonova behavior, we present a comprehensive optical and near-infrared (NIR) catalog of 85 bursts discovered over 2005-2020 on timescales of $\lesssim12$ days. The sample includes previously unpublished observations of 23 bursts, and encompasses both detections and deep upper limits. We identify 11.8% and 15.3% of short GRBs in our catalog with upper limits that probe luminosities lower than those of AT 2017gfo and a fiducial NSBH kilonovae model (for pole-on orientations), respectively. We quantify the ejecta masses allowed by the deepest limits in our catalog, constraining blue and `extremely blue' kilonova components of 14.1% of bursts to $M_{\rm ej}\lesssim0.01-0.1 M_{\odot}$. The sample of short GRBs is not particularly constraining for red kilonova components. Motivated by the large catalog as well as model predictions of diverse kilonova behavior, we investigate modified search strategies for future follow-up to short GRBs. We find that ground-based optical and NIR observations on timescales of $\gtrsim 2$ days can play a significant role in constraining more diverse outcomes. We expect future short GRB follow up efforts, such as from the {\it James Webb Space Telescope}, to expand the reach of kilonova detectability to redshifts of $z\approx 1$. ","Probing Kilonova Ejecta Properties Using a Catalog of Short Gamma-Ray
  Burst Observations"
144,1348629306055004161,1192152664412475393,Fulvio Gesmundo,"['We study the dimension of Tensor Network Varieties, with a focus on cases relevant for quantum many-body physics such as MPS and PEPS.\n\nTake a look at our new paper on ""Dimension of Tensor Network Varieties"", with A. Bernardi and C. De Lazzari, at <LINK>', 'In this work, we give a general upper bound for the dimension of these varieties and we show this bound is sharp in a particular range. We highlight few examples where the varieties are ""smaller than expected"".\n\nFeel free to comment and give feedback!']",https://arxiv.org/abs/2101.03148,"The tensor network variety is a variety of tensors associated to a graph and a set of positive integer weights on its edges, called bond dimensions. We determine an upper bound on the dimension of the tensor network variety. A refined upper bound is given in cases relevant for applications such as varieties of matrix product states and projected entangled pairs states. We provide a range (the ""supercritical range"") of the parameters where the upper bound is sharp. ",Dimension of Tensor Network varieties
145,1347624066971099136,928301283034914817,Lihua Lei,"['Check out our new work <LINK>! We propose a framework, inspired by conformal inference, that is able to control risk for any #MachineLearning algorithms in finite samples for iid data w/o distributional assumptions! #statstwitter #computervision #deeplearning #ai <LINK>', 'Joint work with fantastic collaborators @stats_stephen @ml_angelopoulos, Jitendra Malik and Michael I. Jordan! If you want to have a quick glance at the cool applications, check out the amazing blog by @ml_angelopoulos https://t.co/QiSlItKAQX']",https://arxiv.org/abs/2101.02703,"While improving prediction accuracy has been the focus of machine learning in recent years, this alone does not suffice for reliable decision-making. Deploying learning systems in consequential settings also requires calibrating and communicating the uncertainty of predictions. To convey instance-wise uncertainty for prediction tasks, we show how to generate set-valued predictions from a black-box predictor that control the expected loss on future test points at a user-specified level. Our approach provides explicit finite-sample guarantees for any dataset by using a holdout set to calibrate the size of the prediction sets. This framework enables simple, distribution-free, rigorous error control for many tasks, and we demonstrate it in five large-scale machine learning problems: (1) classification problems where some mistakes are more costly than others; (2) multi-label classification, where each observation has multiple associated labels; (3) classification problems where the labels have a hierarchical structure; (4) image segmentation, where we wish to predict a set of pixels containing an object of interest; and (5) protein structure prediction. Lastly, we discuss extensions to uncertainty quantification for ranking, metric learning and distributionally robust learning. ","Distribution-Free, Risk-Controlling Prediction Sets"
146,1347502752822403072,51700215,Phil Bull,"['New paper! Led by Samir Choudhuri, with simulation wizardry by Hugh Garsden.\nWe studied how differences in the primary beam patterns of the receivers in a radio telescope array (like HERA) affect the recovery of the cosmological 21cm signal.\n<LINK>', 'This has been studied before, but we wanted to see if there were identifiable patterns for different types of non-redundancy in the beams. It turns out that there are! The conclusions are a bit prosaic though, so please take a look at the paper :)']",https://arxiv.org/abs/2101.02684,"Radio interferometer arrays such as HERA consist of many close-packed dishes arranged in a regular pattern, giving rise to a large number of `redundant' baselines with the same length and orientation. Since identical baselines should see an identical sky signal, this provides a way of finding a relative gain/bandpass calibration without needing an explicit sky model. In reality, there are many reasons why baselines will not be exactly identical, giving rise to a host of effects that spoil the redundancy of the array and induce spurious structure in the calibration solutions if not accounted for. In this paper, we seek to build an understanding of how differences in the primary beam response between antennas affect redundantly-calibrated interferometric visibilities and their resulting frequency (delay-space) power spectra. We use simulations to study several generic types of primary beam variation, including differences in the width of the main lobe, the angular and frequency structure of the sidelobes, and the beam ellipticity and orientation. For all of these types, we find that additional temporal structure is induced in the gain solutions, particularly when bright point sources pass through the beam. In comparison, only a low level of additional spectral structure is induced. The temporal structure modulates the cosmological 21cm power spectrum, but only at the level of a few percent in our simulations. We also investigate the possibility of signal loss due to decoherence effects when non-redundant visibilities are averaged together, finding that the decoherence is worst when bright point sources pass through the beam, and that its magnitude varies significantly between baseline groups and types of primary beam variation. Redundant calibration absorbs some of the decoherence effect however, reducing its impact compared to if the visibilities were perfectly calibrated. ","Patterns of primary beam non-redundancy in close-packed 21 cm array
  observations"
147,1347195122002362368,151193108,Mert R. Sabuncu 🇺🇦,"['Deep learning can amortize the classical regularized regression problem in compressed sensing reconstruction. We propose to use hypernetworks to train a model, that at test time can rapidly compute reconstructions with different amounts of regularization: <LINK>', 'This builds on our prior work presented at a recent MICCAI workshop: https://t.co/d66LjgEReO', 'Joint work with @AdrianDalca and Alan Wang']",https://arxiv.org/abs/2101.02194,"Reconstructing under-sampled k-space measurements in Compressed Sensing MRI (CS-MRI) is classically solved with regularized least-squares. Recently, deep learning has been used to amortize this optimization by training reconstruction networks on a dataset of under-sampled measurements. Here, a crucial design choice is the regularization function(s) and corresponding weight(s). In this paper, we explore a novel strategy of using a hypernetwork to generate the parameters of a separate reconstruction network as a function of the regularization weight(s), resulting in a regularization-agnostic reconstruction model. At test time, for a given under-sampled image, our model can rapidly compute reconstructions with different amounts of regularization. We analyze the variability of these reconstructions, especially in situations when the overall quality is similar. Finally, we propose and empirically demonstrate an efficient and data-driven way of maximizing reconstruction performance given limited hypernetwork capacity. Our code is publicly available at this https URL ","Regularization-Agnostic Compressed Sensing MRI Reconstruction with
  Hypernetworks"
148,1347137038743703554,127954917,Andrea Guzmán Mesa 🇨🇴🇨🇭🇪🇺,"[""#Exoplaneteers, we don't want you to reinvent the wheel every time you need to transform line lists into #opacities! Check out our last paper: <LINK>.\nIf you find some super colorful opacities plots, I'm the one to blame! 😄\nDatabase:<LINK> <LINK>""]",https://arxiv.org/abs/2101.02005,"Computing and using opacities is a key part of modeling and interpreting data of exoplanetary atmospheres. Since the underlying spectroscopic line lists are constantly expanding and currently include up to ~ 10^10 - 10^11 transition lines, the opacity calculator codes need to become more powerful. Here we present major upgrades to the HELIOS-K GPU-accelerated opacity calculator and describe the necessary steps to process large line lists within a reasonable amount of time. Besides performance improvements, we include more capabilities and present a toolbox for handling different atomic and molecular data sets: from downloading and pre-processing the data to performing the opacity calculations in a user-friendly way. HELIOS-K supports line lists from ExoMol, HITRAN, HITEMP, NIST, Kurucz and VALD3. By matching the resolution of 0.1 cm^-1 and cutting length of 25 cm^-1 used by the ExoCross code for timing performance (251 seconds excluding data read-in time), HELIOS-K can process the ExoMol BT2 water line list in 12.5 seconds. Using a resolution of 0.01 cm^-1, it takes 45 seconds - equivalent to about 10^7 lines per second. As a wavenumber resolution of 0.01 cm^-1 suffices for most exoplanetary atmosphere spectroscopic calculations, we adopt this resolution in calculating opacity functions for several hundred atomic and molecular species, and make them freely available on the open-access DACE database. For the opacity calculations of the database, we use a cutting length of 100 cm^-1 for molecules and no cutting length for atoms. Our opacities are available for downloading from this https URL and may be visualized using this https URL ","HELIOS-K 2.0 Opacity Calculator and Open-source Opacity Database for
  Exoplanetary Atmospheres"
149,1354714135666118658,998127492467576832,Ryota Tanaka,"['Our #AAAI2021 paper with @kyoun is out on arXiv! <LINK>\nWe introduce VisualMRC that requires a system to read and reason about text in the document image. We propose new models that allow for transferring the abilities of pre-trained seq2seq models, to this task. <LINK>']",https://arxiv.org/abs/2101.11272,"Recent studies on machine reading comprehension have focused on text-level understanding but have not yet reached the level of human understanding of the visual layout and content of real-world documents. In this study, we introduce a new visual machine reading comprehension dataset, named VisualMRC, wherein given a question and a document image, a machine reads and comprehends texts in the image to answer the question in natural language. Compared with existing visual question answering (VQA) datasets that contain texts in images, VisualMRC focuses more on developing natural language understanding and generation abilities. It contains 30,000+ pairs of a question and an abstractive answer for 10,000+ document images sourced from multiple domains of webpages. We also introduce a new model that extends existing sequence-to-sequence models, pre-trained with large-scale text corpora, to take into account the visual layout and content of documents. Experiments with VisualMRC show that this model outperformed the base sequence-to-sequence models and a state-of-the-art VQA model. However, its performance is still below that of humans on most automatic evaluation metrics. The dataset will facilitate research aimed at connecting vision and language understanding. ",VisualMRC: Machine Reading Comprehension on Document Images
150,1353630441601978369,331497846,Arthur Gervais,"['How does your generalized front-running bot work? We find that a simple replace algorithm would have likely been able to replay over 200k transaction over 2 years, yielding a profit of over 17.6M USD (ignoring concurrent adversaries). <LINK> #DeFi #DiveDeeper <LINK>']",https://arxiv.org/abs/2101.05511,"Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward. However, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward. Moreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data. Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security. ",Quantifying Blockchain Extractable Value: How dark is the forest?
151,1351967463609491458,465023908,Ben Bartlett,"['My new paper with @DuttAvik and Shanhui Fan is now on arXiv! 🎉 <LINK>\n\nWe propose a design for a teleportation-based photonic quantum computer which can perform any computation using only one (1) controllable qubit. [bonus: single-photon detectors not required!] <LINK>', '[1/6] Photonics is a promising substrate for QC, but it is very difficult to integrate many identical quantum emitters into a large photonic circuit.\n\nHere we propose a design which requires only a single quantum emitter to implement any quantum operation of any size!', '[2/6] In our scheme, optical qubits are encoded as trains of single-photon pulses counter-propagating through a fiber storage ring. Optical switches can selectively scatter photons off of an atom-cavity system before returning them to the storage ring. https://t.co/AYVgE6L6ig', '[3/6] The atom is coherently controlled by a laser. After scattering a photon against the atom, a rotation is applied to the atomic qubit and the state is projectively measured in the {|g₀⟩, |g₁⟩} basis.\n\nThis measurement teleports the rotation gate onto the photonic qubit! https://t.co/YCFUb6K361', '[4/6] The teleported rotations are a universal quantum primitive: by composing 3 rotation gates with suitable angles, any single-qubit gate can be implemented through Euler angles.\n\nTwo-photon gates such as CZ can also be implemented using a similar rotate-and-measure procedure. https://t.co/Hl4SARlC17', '[5/6] CZ and single-qubit gates are a universal gate set, so the device can implement any quantum circuit! \n\nAdditionally, state readout can be done without using single-photon detectors! Simply apply SWAP between each photon and the atom and sequentially measure the atomic state https://t.co/O6V6aWGpkW', '[6/6] We also did a detailed analysis of how our scheme performs in the presence of experimental imperfections. Assuming realistic parameters, error probability per gate is potentially fault tolerant at ~5×10⁻⁴, and very deep circuits are possible even without error correction. https://t.co/kDi2cnr0If', '@AMFraine @DuttAvik No, you can actually do this with a nondeterministic source (as long as you can resolve the time bin of the photon) or you can even double-purpose the atom-cavity system as the photon source itself!', ""@AMFraine @DuttAvik that's correct, but if you only need to resolve time bins before starting the computation (rather than reading out the final quantum state after processing), then limited detection efficiencies are not a huge issue"", ""@JacquesCarolan @DuttAvik thanks! glad it's finally online so I can talk about it 😅"", 'HD version here: https://t.co/3H6UPMfLbF\n\nCode I wrote to make the animation here: https://t.co/zXVumZBoDO', '@JnEsZ @DuttAvik thanks! making animations is a good way to p̶r̶o̶c̶r̶a̶s̶t̶i̶n̶a̶t̶e̶ ̶o̶n̶ ̶r̶e̶a̶l̶ ̶w̶o̶r̶k̶ ̶i̶ ̶n̶e̶e̶d̶ ̶t̶o̶ ̶d̶o̶ convey ideas', ""@samykamkar thanks! it's kind of a wacky idea so I had fun working on this project""]",https://arxiv.org/abs/2101.07786,"Photonics offers unique advantages as a substrate for quantum information processing, but imposes fundamental scalability challenges. Nondeterministic schemes impose massive resource overheads, while deterministic schemes require prohibitively many identical quantum emitters to realize sizeable quantum circuits. Here we propose a scalable architecture for a photonic quantum computer which needs minimal quantum resources to implement any quantum circuit: a single coherently controlled atom. Optical switches endow a photonic quantum state with a synthetic time dimension by modulating photon-atom couplings. Quantum operations applied to the atomic qubit can be teleported onto the photonic qubits via projective measurement, and arbitrary quantum circuits can be compiled into a sequence of these teleported operators. This design negates the need for many identical quantum emitters to be integrated into a photonic circuit and allows effective all-to-all connectivity between photonic qubits. The proposed device has a machine size which is independent of quantum circuit depth, does not require single-photon detectors, operates deterministically, and is robust to experimental imperfections. ",Deterministic photonic quantum computation in a synthetic time dimension
152,1351456111384272898,331497846,Arthur Gervais,"['We find that 1.64% of the miners do not broadcast their mined transactions. I can understand that front-running is a threat, but moving towards centralization is likely a bigger threat to the vision of decentralized ledgers <LINK> #DeFi #DiveDeeper']",https://arxiv.org/abs/2101.05511,"Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward. However, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward. Moreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data. Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security. ",Quantifying Blockchain Extractable Value: How dark is the forest?
153,1347142720326950912,57571700,Taha Yasseri,['New Preprint\nWe find a positive relationship between neuroticism &amp; obsessive work passion in enterprising environments.\n<LINK>\nOne of last papers based on myPersonality data @david_stillwell &amp; @michalkosinski generated &amp; shared securely &amp; ethically for many years <LINK>'],https://arxiv.org/abs/2101.01270,"Passionate employees are essential for organisational success as they foster higher performance and exhibit lower turnover or absenteeism. While a large body of research has investigated the consequences of passion, we know only little about its antecedents. Integrating trait interaction theory with trait activation theory, this paper examines how personality traits, i.e. conscientiousness, agreeableness, and neuroticism impact passion at work across different job situations. Passion has been conceptualized as a two-dimensional construct, consisting of harmonious work passion (HWP) and obsessive work passion (OWP). Our study is based on a sample of N = 824 participants from the myPersonality project. We find a positive relationship between neuroticism and OWP in enterprising environments. Further, we find a three-way interaction between conscientiousness, agreeableness, and enterprising environment in predicting OWP. Our findings imply that the impact of personality configurations on different forms of passion is contingent on the job environment. Moreover, in line with self-regulation theory, the results reveal agreeableness as a ""cool influencer"" and neuroticism as a ""hot influencer"" of the relationship between conscientiousness and work passion. We derive practical implications for organisations on how to foster work passion, particularly HWP, in organisations. ","What drives passion? An empirical examination on the impact of
  personality trait interactions and job environments on work passion"
