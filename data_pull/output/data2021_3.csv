,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1381696727707574272,2187767059,Colorado Reed üá∫üá¶,"['New paper! Region Similarity Representation Learning, with Tete Xiao and the @berkeley_ai crew.\n\npaper: <LINK>\ncode: <LINK>\n\n-&gt; Maintaining spatial consistency on the conv maps during pretraining benefits region-based  tasks like obj detection <LINK>', 'While most instance contrastive methods perform pre-training using only the semantic features from the final convolutional layer, ReSim aligns regions of the convolutional feature maps that correspond to the same pixel-level regions in the input image across two views, e.g. crops', 'ReSim operates by sliding a fixed-sized window across the overlapping area between two views, aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views.', 'As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps;', 'This allows downstream tasks to better leverage these representations for localization. We show some very strong results on a number of region-based tasks (e.g. object detection), and also show how ReSim outperforms existing methods as we require better localization: https://t.co/JuJzAA0ACx']",https://arxiv.org/abs/2103.12902,"We present Region Similarity Representation Learning (ReSim), a new approach to self-supervised representation learning for localization-based tasks such as object detection and segmentation. While existing work has largely focused on solely learning global representations for an entire image, ReSim learns both regional representations for localization as well as semantic image-level representations. ReSim operates by sliding a fixed-sized window across the overlapping area between two views (e.g., image crops), aligning these areas with their corresponding convolutional feature map regions, and then maximizing the feature similarity across views. As a result, ReSim learns spatially and semantically consistent feature representation throughout the convolutional feature maps of a neural network. A shift or scale of an image region, e.g., a shift or scale of an object, has a corresponding change in the feature maps; this allows downstream tasks to leverage these representations for localization. Through object detection, instance segmentation, and dense pose estimation experiments, we illustrate how ReSim learns representations which significantly improve the localization and classification performance compared to a competitive MoCo-v2 baseline: $+2.7$ AP$^{\text{bb}}_{75}$ VOC, $+1.1$ AP$^{\text{bb}}_{75}$ COCO, and $+1.9$ AP$^{\text{mk}}$ Cityscapes. Code and pre-trained models are released at: \url{this https URL} ",Region Similarity Representation Learning
1,1380378275478872066,1119400976685965313,Daniel Brown,"['New paper to appear at ICRA 2021: ""Dynamically Switching Human Prediction Models for Efficient Planning.""  w/ Arjun Sripathy*, @andreea7b*, and @ancadianadragan \n\n*lead authors\nProject: <LINK>\nPaper: <LINK>\n(1/N) <LINK>', 'We show that a robot can use a suite of human models of different fidelities for more efficient planning. For example, an autonomous car can decide online whether it would benefit from using a high compute theory of mind model or whether a simpler human model suffices. \n(2/N)', 'Critically, we show how the robot can assess the potential benefit of switching to a higher fidelity model without needing to plan with this more expensive model. This allows the robot to efficiently and dynamically switch between different predictive models online. \n(3/N)', 'This dynamic switching results in significantly reduced computation compared to always planning with the highest fidelity human model, while maintaining performance comparable to always using the highest fidelity human model. \n(4/N)']",https://arxiv.org/abs/2103.07815,"As environments involving both robots and humans become increasingly common, so does the need to account for people during planning. To plan effectively, robots must be able to respond to and sometimes influence what humans do. This requires a human model which predicts future human actions. A simple model may assume the human will continue what they did previously; a more complex one might predict that the human will act optimally, disregarding the robot; whereas an even more complex one might capture the robot's ability to influence the human. These models make different trade-offs between computational time and performance of the resulting robot plan. Using only one model of the human either wastes computational resources or is unable to handle critical situations. In this work, we give the robot access to a suite of human models and enable it to assess the performance-computation trade-off online. By estimating how an alternate model could improve human prediction and how that may translate to performance gain, the robot can dynamically switch human models whenever the additional computation is justified. Our experiments in a driving simulator showcase how the robot can achieve performance comparable to always using the best human model, but with greatly reduced computation. ",Dynamically Switching Human Prediction Models for Efficient Planning
2,1380107776005922817,2152912992,Huazhu FU,"['We collect a new ""video shadow detection dataset"", which contains 120 video sequences, with a total of 11,685 frames and 390 seconds duration.  The paper has been accepted to #CVPR2021  \n\nProject link: <LINK>\n\narXiv link: <LINK>']",https://arxiv.org/abs/2103.06533,"Shadow detection in a single image has received significant research interest in recent years. However, much fewer works have been explored in shadow detection over dynamic scenes. The bottleneck is the lack of a well-established dataset with high-quality annotations for video shadow detection. In this work, we collect a new video shadow detection dataset, which contains 120 videos with 11, 685 frames, covering 60 object categories, varying lengths, and different motion/lighting conditions. All the frames are annotated with a high-quality pixel-level shadow mask. To the best of our knowledge, this is the first learning-oriented dataset for video shadow detection. Furthermore, we develop a new baseline model, named triple-cooperative video shadow detection network (TVSD-Net). It utilizes triple parallel networks in a cooperative manner to learn discriminative representations at intra-video and inter-video levels. Within the network, a dual gated co-attention module is proposed to constrain features from neighboring frames in the same video, while an auxiliary similarity loss is introduced to mine semantic information between different videos. Finally, we conduct a comprehensive study on ViSha, evaluating 12 state-of-the-art models (including single image shadow detectors, video object segmentation, and saliency detection methods). Experiments demonstrate that our model outperforms SOTA competitors. ",Triple-cooperative Video Shadow Detection
3,1379530104636796930,92182169,Ian Manchester,"['Koopman operator and contraction analysis are two frameworks for exact analysis of nonlinear dynamical systems (hard) via analysis of linear dynamical systems (easy). In our new paper, we show that they are in fact equivalent for a wide class of problems:\n<LINK>']",https://arxiv.org/abs/2103.15033,"In this paper we prove new connections between two frameworks for analysis and control of nonlinear systems: the Koopman operator framework and contraction analysis. Each method, in different ways, provides exact and global analyses of nonlinear systems by way of linear systems theory. The main results of this paper show equivalence between contraction and Koopman approaches for a wide class of stability analysis and control design problems. In particular: stability or stablizability in the Koopman framework implies the existence of a contraction metric (resp. control contraction metric) for the nonlinear system. Further in certain cases the converse holds: contraction implies the existence of a set of observables with which stability can be verified via the Koopman framework. We provide results for the cases of autonomous and time-varying systems, as well as orbital stability of limit cycles. Furthermore, the converse claims are based on a novel relation between the Koopman method and construction of a Kazantzis-Kravaris-Luenberger observer. We also provide a byproduct of the main results, that is, a new method to learn contraction metrics from trajectory data via linear system identification. ","On the equivalence of contraction and Koopman approaches for nonlinear
  stability and control"
4,1379084866906583040,838292815,Ofir Nachum,"[""Policy eval/selection is a super impactful but woefully overlooked area of RL research. In our new paper (<LINK> accepted to ICLR'21) we build an OPE benchmark on top of D4RL &amp; RLUnplugged, which we hope will encourage deep RL researchers to look at this problem."", 'Link for data and code: https://t.co/bJtDKwpdjB']",https://arxiv.org/abs/2103.16596,"Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area. ",Benchmarks for Deep Off-Policy Evaluation
5,1378775904382427139,101810581,Animesh Garg,"['When you try to open a new door, do you try to yank it up? Likely, no. \nWhy should your robot continue to do so!\n\nCheck out our #ICRA2021 paper on learning action spaces for efficient contact-rich manipulation.\n\npaper: <LINK>\n<LINK>', 'RL in contact-rich manipulation depends on the action space used for exploration. \n\nüí°Insight: Tasks within a family often lie on a reward-yielding action space manifold.\n\nWe can learn this Action-space manifold for efficient exploration in new tasks within the task family. https://t.co/Kx9TWwdiog', ""Action-Space Learning can be done both:\n- Offline: from batched trajectories of experts, \n- Online: interleaving policy learning with action space learning. \n\nThe policy itself doesn't generalize, but the action space for exploration does, similar to learned task priors. https://t.co/moi4FNHhma"", 'joint work w/ @RobobertoMM @arthurallshire C. Lin S. Manuel  @silviocinguetta \n\n@UofTRobotics @VectorInst @StanfordSVL https://t.co/KXXX9kmmEN', 'The origins of this work were in our IROS 2019 paper on Variable Impedance Control in End-Effector Action spaces, which brought forth the need for efficient action spaces. \n\n@RobobertoMM @michellearning R. Gardner  @leto__jean \n\nhttps://t.co/hSPwQirjyq \nhttps://t.co/5eUzrR7xz0', 'Also related was an application of this using cVAE for learned control mapping in Assistive Teleoperation in ICRA 2020 paper\n\nD. Losey @krishpopdesu @AjayMandlekar @DorsaSadigh\n\nhttps://t.co/XgdMlUKLEL\n\nhttps://t.co/PcmwnkSyX4\nhttps://t.co/JGxDwvixNm', 'And btw, this idea of learning in efficient action space learning is not limited to manipulation\n\nCheck out a sneak preview of RL in centroidal action spaces for generalization in Real Legged Robots:\nhttps://t.co/EO7rvAIZMU‚Ä¶\n\n@Mvandepanne @zhaomingxie @BuckBabich X. Da\n@NVIDIAAI']",https://arxiv.org/abs/2103.15793,"The process of learning a manipulation task depends strongly on the action space used for exploration: posed in the incorrect action space, solving a task with reinforcement learning can be drastically inefficient. Additionally, similar tasks or instances of the same task family impose latent manifold constraints on the most effective action space: the task family can be best solved with actions in a manifold of the entire action space of the robot. Combining these insights we present LASER, a method to learn latent action spaces for efficient reinforcement learning. LASER factorizes the learning problem into two sub-problems, namely action space learning and policy learning in the new action space. It leverages data from similar manipulation task instances, either from an offline expert or online during policy learning, and learns from these trajectories a mapping from the original to a latent action space. LASER is trained as a variational encoder-decoder model to map raw actions into a disentangled latent action space while maintaining action reconstruction and latent space dynamic consistency. We evaluate LASER on two contact-rich robotic tasks in simulation, and analyze the benefit of policy learning in the generated latent action space. We show improved sample efficiency compared to the original action space from better alignment of the action space to the task space, as we observe with visualizations of the learned action space manifold. Additional details: this https URL ","LASER: Learning a Latent Action Space for Efficient Reinforcement
  Learning"
6,1378152770792947715,769188626324398080,Heiga Zen (ÂÖ® ÁÇ≥Ê≤≥),"['New paper from our team: \n\nIsaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang, Ye Jia, RJ. Ryan, Yonghui Wu\n""Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling""\n\nArxiv: <LINK>\nSamples: <LINK>', '""This paper introduces Parallel Tacotron 2, a new non-autoregressive neural text-to-speech model. This model has fully differentiable duration modeling, learned upsampling mechanism based on attention, and an iterative reconstruction loss based on Soft Dynamic Time Warping.', 'These features allow the model to learn alignments and token durations automatically, without requiring supervised duration signals. Experimental results show that Parallel Tacotron 2 outperforms the baselines in naturalness in several diverse multi speaker evaluations.', 'Its duration control capability is also demonstrated.""']",https://arxiv.org/abs/2103.14574,"This paper introduces Parallel Tacotron 2, a non-autoregressive neural text-to-speech model with a fully differentiable duration model which does not require supervised duration signals. The duration model is based on a novel attention mechanism and an iterative reconstruction loss based on Soft Dynamic Time Warping, this model can learn token-frame alignments as well as token durations automatically. Experimental results show that Parallel Tacotron 2 outperforms baselines in subjective naturalness in several diverse multi speaker evaluations. Its duration control capability is also demonstrated. ","Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with
  Differentiable Duration Modeling"
7,1378103690221998080,899120403649404928,Sam Cree,"['New paper! To build a good quantum computer, we need to encode information so that it can be processed without getting corrupted. We found that certain codes inspired by quantum gravity only allow very limited kinds of fault-tolerant processing.\nüíª‚öõÔ∏èü§î\n<LINK>\n1/4 <LINK>', 'Context: theorists have been using holographic codes as toy models of quantum gravity for the last few years. They do a decent job at protecting information from errors/environmental noise, but their usefulness for fault-tolerant quantum computing is unclear.\n2/4', 'Our contribution: The kinds of fault-tolerant operations you can do most easily in holographic codes are very restricted - you can\'t implement non-Clifford gates, which are often the ""magic ingredient"" in universal fault-tolerant quantum computation strategies.\n3/4', ""So holographic codes might yet find a practical application for quantum computing, but we've ruled out the most straightforward way we might have hoped for.\n4/4\n\nThanks to coauthors, @DomJWilliamson, @KfirDolev\nand Vlad Calvera\n#quantph #quantum #scicomm #sciencetwitter #arxiv""]",https://arxiv.org/abs/2103.13404,"We evaluate the usefulness of holographic stabilizer codes for practical purposes by studying their allowed sets of fault-tolerantly implementable gates. We treat them as subsystem codes and show that the set of transversally implementable logical operations is contained in the Clifford group for sufficiently localized logical subsystems. As well as proving this concretely for several specific codes, we argue that this restriction naturally arises in any stabilizer subsystem code that comes close to capturing certain properties of holography. We extend these results to approximate encodings, locality-preserving gates, certain codes whose logical algebras have non-trivial centers, and discuss cases where restrictions can be made to other levels of the Clifford hierarchy. A few auxiliary results may also be of interest, including a general definition of entanglement wedge map for any subsystem code, and a thorough classification of different correctability properties for regions in a subsystem code. ","Fault-tolerant logical gates in holographic stabilizer codes are
  severely restricted"
8,1377983221946839042,1912298966,Dr. L. C. Mayorga,"['New paper was accepted with the fabulous @exowanderer and @kevinbstevenson! <LINK> \n\nWe observed a secondary eclipse of WASP-43b with HST in the optical! 1/10 <LINK>', 'WASP-43b is particularly exciting because modeling showed that it had the potential for MgSiO3 clouds or MnS clouds, which have very different reflected light properties. 2/10 https://t.co/WpN5lmKpT2', 'We asked for a whole phase curve (24 orbits), but only got awarded an eclipse (4 orbits). The TAC strikes again! We suspected that MgSiO3 clouds would be inhomogeneous because they could be cold trapped (below the observable photosphere). 3/10 https://t.co/kXaUzLkMSJ', 'Instead of staring at a motionless star, we used a new mode for HST WFC3/UVIS that ‚Äúscans‚Äù the star across the detector, allowing us to build up lots of signal without saturating our bright target. 4/10 https://t.co/4CHRWhdlQF', 'To analyze this new HST mode, @exowanderer built Arctor! https://t.co/KDGKqf9rSX a new scanning extraction and analysis pipeline! 5/10 https://t.co/plOPCn5R5e', 'Here‚Äôs what we got!!! No, April Fools. The TAC was probably onto something‚Ä¶ 6/10 https://t.co/DnMvXc8Q7c', 'We place a 67ppm 3-sigma upper limit on the eclipse depth, a geometric albedo limit (in F350LP) at 0.06. My modeling suggests that only the most lofted cloud case scenario can escape a cold trap at the deepest layer of the atmosphere on the day side. 7/10 https://t.co/3yjYwibWDU', 'From nightside observations, we can be pretty sure that clouds there were limiting the depth that we could probe in the atmosphere. This means that the clouds on WASP-43b are inhomogeneously distributed! 8/10 https://t.co/Wd2jFMBv4a', 'Model predictions suggest that there is a correlation between planet equilibrium temperature and where this cold trap starts to occur -&gt; causing an albedo drop, because MgSiO3 is normally really bright! (from @V_Parmentier work) 9/10 https://t.co/aaT6RRWipw', 'With WASP-43b proving to be the darkest of worlds, it‚Äôs now time to study other exoplanets at different temperatures and gravities to figure out where and how MgSiO3 clouds form. See the paper for more! https://t.co/OkthjJXNoN 10/10 https://t.co/4FxUmj8uxZ']",https://arxiv.org/abs/2103.16676,"Optical, reflected light eclipse observations provide a direct probe of the exoplanet scattering properties, such as from aerosols. We present here the photometric, reflected light observations of WASP-43b using the HST WFC3/UVIS instrument with the F350LP filter (346-822nm) encompassing the entire optical band. This is the first reflected light, photometric eclipse using UVIS in scanning mode; as such we further detail our scanning extraction and analysis pipeline Arctor. Our HST WFC3/UVIS eclipse light curve for WASP-43 b derived a 3-{\sigma} upper limit of 67 ppm on the eclipse depth, which implies that WASP-43b has a very dark dayside atmosphere. With our atmospheric modeling campaign, we compared our reflected light constraints with predictions from global circulation and cloud models, benchmarked with HST and Spitzer observations of WASP-43b. We infer that we do not detect clouds on the dayside within the pressure levels probed by HST WFC3/UVIS with the F350LP filter (P > 1 bar). This is consistent with the GCM predictions based on previous WASP-43b observations. Dayside emission spectroscopy results from WASP-43b with HST and Spitzer observations are likely to not be significantly affected by contributions from cloud particles. ",The Dark World: A Tale of WASP-43b in Reflected Light with HST WFC3/UVIS
9,1377823907269337092,814021861822468099,ZhedongZheng,['The spirits of conventional machine learning methods are still powerful. You are welcomed to check out our new paper on adaboost for domain adaptation.\nAdaptive Boosting for Domain Adaptation: Towards Robust Predictions in Scene Segmentation\n<LINK>'],http://arxiv.org/abs/2103.15685,"Domain adaptation is to transfer the shared knowledge learned from the source domain to a new environment, i.e., target domain. One common practice is to train the model on both labeled source-domain data and unlabeled target-domain data. Yet the learned models are usually biased due to the strong supervision of the source domain. Most researchers adopt the early-stopping strategy to prevent over-fitting, but when to stop training remains a challenging problem since the lack of the target-domain validation set. In this paper, we propose one efficient bootstrapping method, called Adaboost Student, explicitly learning complementary models during training and liberating users from empirical early stopping. Adaboost Student combines the deep model learning with the conventional training strategy, i.e., adaptive boosting, and enables interactions between learned models and the data sampler. We adopt one adaptive data sampler to progressively facilitate learning on hard samples and aggregate ""weak"" models to prevent over-fitting. Extensive experiments show that (1) Without the need to worry about the stopping time, AdaBoost Student provides one robust solution by efficient complementary model learning during training. (2) AdaBoost Student is orthogonal to most domain adaptation methods, which can be combined with existing approaches to further improve the state-of-the-art performance. We have achieved competitive results on three widely-used scene segmentation domain adaptation benchmarks. ","Adaptive Boosting for Domain Adaptation: Towards Robust Predictions in
  Scene Segmentation"
10,1377782750384545794,14280661,Ben Goertzel,['General Theory of General Intelligence: A Pragmatic Patternist Perspective -- new review paper by moi -- <LINK> -- only semi-technical -- starts w/ high level ontological and mathematical perspectives and ends up at the level of human cog sci and OpenCog Hyperon'],https://arxiv.org/abs/2103.15100,"A multi-decade exploration into the theoretical foundations of artificial and natural general intelligence, which has been expressed in a series of books and papers and used to guide a series of practical and research-prototype software systems, is reviewed at a moderate level of detail. The review covers underlying philosophies (patternist philosophy of mind, foundational phenomenological and logical ontology), formalizations of the concept of intelligence, and a proposed high level architecture for AGI systems partly driven by these formalizations and philosophies. The implementation of specific cognitive processes such as logical reasoning, program learning, clustering and attention allocation in the context and language of this high level architecture is considered, as is the importance of a common (e.g. typed metagraph based) knowledge representation for enabling ""cognitive synergy"" between the various processes. The specifics of human-like cognitive architecture are presented as manifestations of these general principles, and key aspects of machine consciousness and machine ethics are also treated in this context. Lessons for practical implementation of advanced AGI in frameworks such as OpenCog Hyperon are briefly considered. ","The General Theory of General Intelligence: A Pragmatic Patternist
  Perspective"
11,1377713476337012738,1272931080790773763,Vedu Mallela,"['Check out our new paper on BrainPainter v2! We added mouse brain visualization to the original BrainPainter as well as lots of new features including asymmetric renders and a GUI. \n<LINK>. \n\nw/ @RazMarinescu', ""Here's an example of BrainPainter's animation capabilities for mice (this is a subcortical view) https://t.co/Hn7nuzambX"", ""Here's a top view animation for the mouse brain: https://t.co/XQIFIg7btl"", '@portokalh @RazMarinescu Glad you‚Äôre finding it useful :)']",https://arxiv.org/abs/2103.14696,"BrainPainter is a software for the 3D visualization of human brain structures; it generates colored brain images using user-defined biomarker data for each brain region. However, BrainPainter is only able to generate human brain images. In this paper, we present updates to the existing BrainPainter software which enables the generation of mouse brain images. We load meshes for each mouse brain region, based on the Allen Mouse Brain Atlas, into Blender, a powerful 3D computer graphics engine. We then use Blender to color each region and generate images of subcortical, outer-cortical, inner-cortical, top and bottom view renders. In addition to those views, we add new render angles and separate visualization settings for the left and right hemispheres. While BrainPainter traditionally ran from the browser ( this https URL ), we also created a graphical user interface that launches image-generation requests in a user-friendly way, by connecting to the Blender backend via a Docker API. We illustrate a use case of BrainPainter for modeling the progression of tau protein accumulation in a mouse study. Our contributions can help neuroscientists visualize brains in mouse studies and show disease progression. In addition, integration into Blender can subsequently enable the generation of complex animations using a moving camera, generation of complex mesh deformations that simulate tumors and other pathologies, as well as visualization of toxic proteins using Blender's particle system. ",BrainPainter v2: Mouse Brain Visualization Software
12,1377615748462415878,3407899930,Freek Roelofs,"['We have a new paper out showing how well we could measure black holes in the future! Just with more EHT observations, we could get several times better mass measurements. With space telescopes, we could get 0.5% precision and constrain black hole spin.\n\n‚û°Ô∏è<LINK> <LINK>', 'The left plot shows the precision we recover the M87 black hole mass with for different simulated observations. We especially see a large improvement when we go from a single observation to 10 observations, where we average out the variable structure.', 'This variable structure makes it difficult to fit to the data, but when we average multiple images we see the black hole shadow and lensed photon ring more clearly as illustrated here, allowing for a better mass measurement. https://t.co/4pca5tv6pW', 'We also see a large improvement when considering only the input model as compared to a full simulation library (blue vs orange). This tells us that it will be very useful to have data at e.g. other wavelengths and polarizations to narrow down the model parameter space.', 'The right image above shows the distribution over black hole spin of the models that are acceptable after fitting to the simulated data from different arrays. When we observe with submillimeter telescopes in orbit around Earth (EHI), the black hole spin starts to be constrained.', 'With many developments in other parameter extraction techniques and instrument upgrades, this tells us that the future of black hole parameter measurements is bright! With a strongly constrained mass and spin, we can test general relativity to high precision.']",http://arxiv.org/abs/2103.16736,"The Event Horizon Telescope (EHT) has imaged the shadow of the supermassive black hole in M87. A library of general relativistic magnetohydrodynamics (GMRHD) models was fit to the observational data, providing constraints on black hole parameters. We investigate how much better future experiments can realistically constrain these parameters and test theories of gravity. We generate realistic synthetic 230 GHz data from representative input models taken from a GRMHD image library for M87, using the 2017, 2021, and an expanded EHT array. The synthetic data are run through a data reduction pipeline used by the EHT. Additionally, we simulate observations at 230, 557, and 690 GHz with the Event Horizon Imager (EHI) Space VLBI concept. Using one of the EHT parameter estimation pipelines, we fit the GRMHD library images to the synthetic data and investigate how the black hole parameter estimations are affected by different arrays and repeated observations. Repeated observations play an important role in constraining black hole and accretion parameters as the varying source structure is averaged out. A modest expansion of the EHT already leads to stronger parameter constraints. High-frequency observations from space rule out all but ~15% of the GRMHD models in our library, strongly constraining the magnetic flux and black hole spin. The 1$\sigma$ constraints on the black hole mass improve by a factor of five with repeated high-frequency space array observations as compared to observations with the current ground array. If the black hole spin, magnetization, and electron temperature distribution can be independently constrained, the shadow size for a given black hole mass can be tested to ~0.5% with the EHI, which allows tests of deviations from general relativity. High-precision tests of the Kerr metric become within reach from observations of the Galactic Center black hole Sagittarius A*. ","Black hole parameter estimation with synthetic Very Long Baseline
  Interferometry data from the ground and from space"
13,1377613837839515650,249039303,Salman Khan,"['Crafting adversarial perturbations to fool an unknown (black-box) model towards a specific ""target"" class is highly challenging. We devise a new approach with exciting results! Unconstrained patterns for visualization.\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2103.14641,"While the untargeted black-box transferability of adversarial perturbations has been extensively studied before, changing an unseen model's decisions to a specific `targeted' class remains a challenging feat. In this paper, we propose a new generative approach for highly transferable targeted perturbations (\ours). We note that the existing methods are less suitable for this task due to their reliance on class-boundary information that changes from one model to another, thus reducing transferability. In contrast, our approach matches the perturbed image `distribution' with that of the target class, leading to high targeted transferability rates. To this end, we propose a new objective function that not only aligns the global distributions of source and target images, but also matches the local neighbourhood structure between the two domains. Based on the proposed objective, we train a generator function that can adaptively synthesize perturbations specific to a given input. Our generative approach is independent of the source or target domain labels, while consistently performs well against state-of-the-art methods on a wide range of attack settings. As an example, we achieve $32.63\%$ target transferability from (an adversarially weak) VGG19$_{BN}$ to (a strong) WideResNet on ImageNet val. set, which is 4$\times$ higher than the previous best generative attack and 16$\times$ better than instance-specific iterative attack. Code is available at: {\small\url{this https URL}}. ",On Generating Transferable Targeted Perturbations
14,1377612005071331332,1912298966,Dr. L. C. Mayorga,"['üö®üö® New Paper Alert!!! üö®üö®\nMy fantastic co-authors and I studied a new class of astronomical objects! We have designated them UFOs: Understudied Floofy Objects.\n\n@_astronoMay @Of_FallingStars (and Jake Lustig-Yaeger, not on twitter)\n\n<LINK> 1/8 <LINK>', 'Thanks to twitter catstronomers we collected a large sample of these understudied floofy objects at a variety of viewing angles and illumination angles. \n\nThe raw data is public and available here:\nhttps://t.co/mDPUnu5MJX 2/8', 'Through a rigorous analysis of light curves we were able to measure the rotational variations of several dozen floofy objects which we classify into 13 categories, further identified by their subobserver longitudes. 3/8 https://t.co/hNds13leAt', 'We explored the change in brightness in three bands, as well as the overall brightness of the objects, and determined that some subtypes of floofy objects exhibit rotational variability with strong brightening at longitudes of 0. 4/8 https://t.co/Rp8bs11aAD', 'The high number of CL observations allowed us to create an average CL type rotation curve and reconstruct a representative object projected onto the spherical shape we assume. 5/8 https://t.co/ZC4Ed9kiU7', 'We also explored Floofy Objects in Color-Magnitude space and used a clustering algorithm on the 0 longitude observations to determine that there are 6 unique classes of Floofy Objects, generally separated by substellar point color and limb color. 6/8 https://t.co/PbAH3jXaUO', 'Finally, we explored potential for false pawsitives, specifically misidentification of WOOF Objects (Wagging tails On Objectively Friendly Objects) as Floofy Objects and found that high spatial resolution imaging missions like the upcoming CatEx observatory is a necessity. 7/8 https://t.co/1Ra6WMApA8', 'Check out the full paper here: https://t.co/TLUFAUkNft\n\nWe‚Äôre excited to see what future research can be done on this strange new class of astronomical objects! 8/8 https://t.co/dtvM9YSjAC', 'Addendum: and we finally convinced @LustigYaeger to get on Twitter to take his own credit!', ""@vicgrinberg @_astronomay @Of_FallingStars I didn't even know! Purrrfect! @LustigYaeger""]",https://arxiv.org/abs/2103.16636,"Phase resolved observations of planetary bodies allow us to understand the longitudinal and latitudinal variations that make each one unique. Rotational variations have been detected in several types of astronomical bodies beyond those of planetary mass, including asteroids, brown dwarfs, and stars. Unexpected rotational variations, such as those presented in this work, reminds us that the universe can be complicated, with more mysteries to uncover. In this work we present evidence for a new class of astronomical objects we identify as ""floofy"" with observational distinctions between several sub-types of these poorly understood objects. Using optical observations contributed by the community, we have identified rotational variation in several of these floofy objects, which suggests that they may have strong differences between their hemispheres, likely caused by differing reflectivity off their surfaces. Additional sub-types show no rotational variability suggesting a uniform distribution of reflective elements on the floofy object. While the work here is a promising step towards the categorization of floofy objects, further observations with more strictly defined limits on background light, illumination angles, and companion objects are necessary to develop a better understanding of the many remaining mysteries of these astronomical objects. ","Detection of Rotational Variability in Floofy Objects at Optical
  Wavelengths"
15,1377593288539127808,1152338625654226944,Megan Mansfield,"['Happy April 1st everyone! I\'m pleased to announce that myself and Darryl Seligman have a new paper up on the arXiv...\n""I Knew You Were Trouble: Emotional Trends in the Repertoire of Taylor Swift"" <LINK> <LINK>', ""In this work, we analyze trends between happiness and strength of commitment to a relationship across 149 of Taylor's songs (about 10 hrs of music). We create metrics to quantify the amount of happiness and strength of relationship in each song. https://t.co/k8AKsjLt1J"", 'For example, we look at the lyrics to determine whether the object of her affection returns her feelings. (See the paper for the full grading system, including example lyrics!) https://t.co/oCg1CGBnPd', 'We find a significant trend indicating higher happiness in a more committed relationship. https://t.co/CNR5MOdejp', 'We also examine subsets of the data and conclude that boys with blue eyes and/or bad reputations may be the worst choices for long-term happiness, while boys with indigo or green eyes may provide more stability. https://t.co/In7U8rxDsZ', 'Finally, and perhaps most importantly, we present the taylorswift python code, which allows users to input information about their current feelings and relationship status and receive a list of five suitable Taylor Swift songs to match their mood. https://t.co/47Ls7cO3af', 'This code is publicly available at https://t.co/4bij2ZWAgQ and we hope you all find it helpful in making the perfect playlist! https://t.co/vjX0xnRBLP', '@huei_sears Great questions! First the songs: ""Babe"" was not included because it was written by ""Sugarland ft. Taylor Swift"" so we assumed it primarily portrayed the thoughts of Sugarland. Same thing for ""This is What You Came For"" - assumed to be primarily Calvin Harris\'s thoughts', '@huei_sears Second, I agree that evermore and folklore are much less autobiographical. One interesting further interpretation of the trends in these data, then, might be to look at the changes between the more autobiographical eras and her more recent work. Future research! üòõ', '@tpanurach Aw glad to hear it!', '@noraguidegalaxy Lol I took from the Grammy website! Their profile of her is out of date. üòù', ""@Wikisteff Haha after this I'll go back to my usual exoplanet research, but if someone wants to train such a model they should go for it!""]",https://arxiv.org/abs/2103.16737,"As a modern musician and cultural icon, Taylor Swift has earned worldwide acclaim via pieces which predominantly draw upon the complex dynamics of personal and interpersonal experiences. Here we show, for the first time, how Swift's lyrical and melodic structure have evolved in their representation of emotions over a timescale of $\tau\sim14$ yr. Previous progress on this topic has been challenging based on the sheer volume of the relevant discography, and that uniquely identifying a song that optimally describes a hypothetical emotional state represents a multi-dimensional and complex task. To quantify the emotional state of a song, we separate the criteria into the level of optimism ($H$) and the strength of commitment to a relationship ($R$), based on lyrics and chordal tones. We apply these criteria to a set of 149 pieces spanning almost the entire repertoire. We find an overall trend toward positive emotions in stronger relationships, with a best-fit linear relationship of $R=0.642^{+0.086}_{-0.053}H-1.74^{+0.39}_{-0.29}$. We find no significant trends in mean happiness ($H$) within individual albums over time. The mean relationship score ($R$) shows trends which we speculate may be due to age and the global pandemic. We provide tentative indications that partners with blue eyes and/or bad reputations may lead to overall less positive emotions, while those with green or indigo-colored eyes may produce more positive emotions and stronger relationships. However, we stress that these trends are based on small sample sizes, and more data are necessary to validate them. Finally, we present the taylorswift python package which can be used to optimize song selection according to a specific mood. ","I Knew You Were Trouble: Emotional Trends in the Repertoire of Taylor
  Swift"
16,1377532599502045186,576981259,Manuel Bibes,"['New review paper : ""Oxide spin-orbitronics: spin-charge interconversion and topological spin textures"" with @Felix_Trier @NolPaul1 @joovon L. Vila and J.P. Attan√© from @SPINTEC_Lab. The preprint is available here <LINK>. Stay tuned for the published version. <LINK>', 'Ideally this should be read while enjoying this. All bubbles guaranteed topological ! Cc @joovon https://t.co/yWoIr98MA7']",https://arxiv.org/abs/2103.16271,"Quantum oxide materials possess a vast range of properties stemming from the interplay between the lattice, charge, spin and orbital degrees of freedom, in which electron correlations often play an important role. Historically, the spin-orbit coupling was rarely a dominant energy scale in oxides. It however recently came to the forefront, unleashing various exotic phenomena connected with real and reciprocal-space topology that may be harnessed in spintronics. In this article, we review the recent advances in the new field of oxide spin-orbitronics with a special focus on spin-charge interconversion from the direct and inverse spin Hall and Edelstein effects, and on the generation and observation of topological spin textures such as skyrmions. We highlight the control of spin-orbit-driven effects by ferroelectricity and give perspectives for the field. ","Oxide spin-orbitronics: spin-charge interconversion and topological spin
  textures"
17,1377484517858975747,2577596593,Chelsea Finn,"['How can robots generalize to new environments &amp; tasks?\n\nWe find that using in-the-wild videos of people can allow learned reward functions to do so!\nPaper: <LINK>\n\nLed by @_anniechen_, @SurajNair_1\nüßµ(1/5) <LINK>', 'To get reward functions that generalize, we train domain-agnostic video discriminators (DVD) with:\n* a lot of diverse human data, and\n* a narrow &amp; small amount of robot demos\n\nThe idea is super simple: predict if two videos are performing the same task or not.\n(2/5) https://t.co/4sRhfThzkI', 'This discriminator can be used as a reward by feeding in a human video of the desired task and a video of the robot‚Äôs behavior.\n\nWe use it by planning with a learned visual dynamics model.\n(3/5) https://t.co/yQBtzwlmNi', 'Does using human videos improve reward generalization compared to using only narrow robot data?\n\nWe see:\n* 20% greater task success in new environments\n* 25% greater task success on new tasks\nboth in simulation and on a real robot.\n\n(4/5) https://t.co/S0xfHCmh3F', ""For more, check out:\nPaper: https://t.co/afz2PWw0rT\nWebsite: https://t.co/geRH3tmgTe\nSummary video: https://t.co/wg2C1lEsBG\n\nI'm quite excited about how reusing broad datasets can help robots generalize, and this project has been a great indication in that direction!\n\n(5/5) https://t.co/yVPtIjSUAp""]",https://arxiv.org/abs/2103.16817,"We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, ""in-the-wild"" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo. ","Learning Generalizable Robotic Reward Functions from ""In-The-Wild"" Human
  Videos"
18,1377457383241252875,4807828837,Vishal Upendran,"['New paper with @dktripathi accepted in ApJ, is out on Arxiv today: <LINK>\n\nWe study impulsive heating of quiet solar coronal regions, observed using #AIA onboard #SDO by employing an #ML inversion of a statistical impulsive heating model. (1/3)', 'We find the small scale events to be dominant in the corona (alpha&gt;2), and extremely high frequency of events (2 events/min). \n\nCorrelations suggest conduction losses are dominant &amp; point to existence of energy reservoir depleted by impulsive events. (2/3)', 'Finally, I will let me do the talking, and take you through the work: https://t.co/414jmlmQK1\n\nThis is the talk I gave @asipoec in February. Hopefully, it inspires the Indian solar/astro community to incorporate #ML in their scientific analyses!']",https://arxiv.org/abs/2103.16824,"The solar corona consists of a million-degree Kelvin plasma. A complete understanding of this phenomenon demands the study of Quiet Sun (QS) regions. In this work, we study QS regions in the 171 {\AA}, 193 {\AA} and 211 {\AA} passbands of the Atmospheric Imaging Assembly (AIA) on board the Solar Dynamics Observatory (SDO), by combining the empirical impulsive heating forward model of Pauluhn & Solanki (2007) with a machine-learning inversion model that allows uncertainty quantification. We find that there are {\approx} 2--3 impulsive events per min, with a lifetime of about 10--20 min. Moreover, for all the three passbands, the distribution of power law slope {\alpha} peaks above 2. Our exploration of correlations among the frequency of impulsive events and their timescales and peak energy suggests that conduction losses dominate over radiative cooling losses. All these finding suggest that impulsive heating is a viable heating mechanism in QS corona. ",On the Impulsive Heating of Quiet Solar Corona
19,1377423173260439553,70614241,Dr Fiona H. Panther,"['NOT an April Fools joke but my new paper just landed: We propose a way to observe signatures of helium detonations associated with thermonuclear supernovae via gamma-rays:\n<LINK>', ""If a thermonuclear supernova results from a progenitor with a thick helium shell you get the synthesis of a lot of Cr-48 and it's daughter nucleus V-48. The decay of these radioisotopes can actually power the light curve, but some of their gamma-rays leak out"", 'In this paper, we show what this measurement could look like with observatories like @ESA_Integral, @COSIBalloon and AMEGO. We would need a relatively nearby supernova, but it would be a method to check incontrovertibly for signatures of helium detonation', 'We also show that for SN2014J, there was probably not a hugely significant amount of helium present. Not enough to rule out some models that suggested a helium torus, but getting close.', 'Anyway - this paper sits at the intersection of nucleosynthesis, transients and gamma-ray line studies. It shows how you can collaborate with a bunch of talented people to combine SN simulations, binary pop synthesis and gamma-ray astrophysics', 'I am happy to take questions', ""@astrocrash yes - however the proposed model for 2014J is for the ring not to be face on, but edge on to us on Earth (Science paper by Diehl+2015). The spectra we use from Stuart's model is angle averaged."", '@astrocrash We just constrain the total mass of Cr-48 to be relatively low based on sensitivity. What is interesting is that if we had a more sensitive instrument - say with better background rejection - then a better constraint on what the configuration of 2014J was could be made', ""@astrocrash I would have thought so, however it is plausible you have a degenerate, He rich white dwarf companion that has merged. So I wouldn't say the absence of the companion rules out there being helium involved to some extent."", ""@astrocrash You probably saw these sims by Pakmor+, basically these things do things that are unexpected. The model we use is a bit artificial in the sense that we don't say 'how' the ignition begins. Actually getting the He to explode is a separate problem https://t.co/pxD8aVbiO1"", ""@astrocrash The point of my paper is really to say 'this is an observational signature that provides conclusive evidence of helium detonations'. We just pick a canonical model to do the work, so in practice things may look quite different, which R√ºdiger's simulations seem to point at IMO"", '@astrocrash I agree - I think that practically for one of these observations then angle is an issue. I expect it will be similar to with KNe - I do wonder that at some angles you miss the gamma-rays entirely. could be worth a more detailed look in the future']",https://arxiv.org/abs/2103.16840,"Detection of gamma-rays emitted by radioactive isotopes synthesized in stellar explosions can give important insights into the processes that power transients such as supernovae, as well as providing a detailed census of the abundance of different isotope species relevant to the chemical evolution of the Universe. Observations of nearby supernovae have yielded observational proof that $^{57}$Co powered the late-time evolution of SN1987A's lightcurve, and conclusive evidence that $^{56}$Ni and its daughter nuclei power the light curves of Type Ia supernovae. In this paper we describe the prospects for detecting nuclear decay lines associated with the decay of $^{48}$V, the daughter nucleus of $^{48}$Cr, which is expected to be synthesised in large quantities - $M_{\mathrm{Cr}}\sim1.9\times10^{-2}\,\mathrm{M_\odot}$ - in transients initiated by explosive helium burning ($\alpha$-capture) of a thick helium shell. We calculate emergent gamma-ray line fluxes for a simulated explosion model of a thermonuclear explosion of carbon-oxygen white dwarf core of mass $0.45\,M_{\odot}$ surrounded by a thick helium layer of mass $0.21\,M_{\odot}$. We present observational limits on the presence of $^{48}$V in nearby SNe Ia 2014J using the \textit{INTEGRAL} space telescope, excluding a $^{48}$Cr production on the surface of more than $0.1\,\mathrm{M_{\odot}}$. We find that the future gamma-ray mission AMEGO will have an approximately 5 per cent chance of observing $^{48}$V gamma-rays from such events during the currently-planned operational lifetime, based on our birthrate predictions of faint thermonuclear transients. We describe the conditions for a $3\sigma$ detection by the gamma-ray telescopes \textit{INTEGRAL}/SPI, COSI and AMEGO. ","Prospects of direct detection of $^{48}$V gamma-rays from thermonuclear
  supernovae"
20,1377367461146718210,2819715191,Antonella Palmese,"['New paper with @mayaKfish @JamesAnnis17 Colin Burke, Xin Liu: ""Do LIGO/Virgo black hole mergers produce AGN flares?"" our view on the candidate GW binary-black-hole counterpart and prospects for future confident association\n<LINK> <LINK>']",https://arxiv.org/abs/2103.16069,"The recent report of an association of the gravitational-wave (GW) binary black hole (BBH) merger GW190521 with a flare in the Active Galactic Nuclei (AGN) J124942.3+344929 has generated tremendous excitement. However, GW190521 has one of the largest localization volumes amongst all of the GW events detected so far. The 90\% localization volume likely contains $7,400$ unobscured AGN brighter than $g \leq 20.5$ AB mag, and it results in a $\gtrsim 70\%$ probability of chance coincidence for an AGN flare consistent with the GW event. We present a Bayesian formalism to estimate the confidence of an AGN association by analyzing a population of BBH events with dedicated follow-up observations. Depending on the fraction of BBH arising from AGNs, counterpart searches of $\mathcal{O}(1)-\mathcal{O}(100)$ GW events are needed to establish a confident association, and more than an order of magnitude more for searches without followup (i.e, using only the locations of AGNs and GW events). Follow-up campaigns of the top $\sim 5\%$ (based on volume localization and binary mass) of BBH events with total rest frame mass $\ge 50~M_\odot$ are expected to establish a confident association during the next LIGO/Virgo/KAGRA observing run (O4), as long as the true value of the fraction of BBH giving rise to AGN flares is $>0.1$. Our formalism allows us to jointly infer cosmological parameters from a sample of BBH events that include chance coincidence flares. Until the confidence of AGN associations is established, the probability of chance coincidence must be taken into account to avoid biasing astrophysical and cosmological constraints. ","Do LIGO/Virgo black hole mergers produce AGN flares? The case of
  GW190521 and prospects for reaching a confident association"
21,1377329825023717380,2807461219,Tal Schuster,"['Is your Fact Verification model robust enough? Consider adding #VitaminC üçä\n\nCheck out our new #NAACL2021 paper: ""Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"" with @adamjfisch and @BarzilayRegina\n\nüîó <LINK>\n\n#NLProc #FakeNewsüì∞\nüßµ1/N <LINK>', 'We build on #Wikipedia revisions to create a large resource for fact verification tasks. #VitaminC has over 400K claim-evidence pairs on a diverse set of topics, including #COVID19. \nIt is  #Contrastive: a claim can be supported by one Evidence and refuted by another.\n\n2/N https://t.co/38gZ3Ybu9L', 'To create #VitaminC, we had to distinguish #Wikipedia revisions that express a factual change from revisions that only paraphrase a sentence or change its style.\n\n@TransperfectAI annotators helped to flag the revisions and to extract the underlying claims\n\n3/N https://t.co/Y59UfIJiXN', 'We use #VitaminC for 4 tasks:\n1. Flagging factual revisions\n2. Fact verification (focusing on #adversarial examples)\n3. Word-level rationales\n4. Factually consistent generation for (a) automating revisions given claims and (b) extracting claims from existing revisions\n\n4/N https://t.co/byxWu5sdzx', '1. For the flagging task, we find it to be slightly different than paraphrasing datasets, suggesting the value of #VitaminC.\n\nWe experiment with several models and find context-aware #ALBERT to perform relatively well (though there is still room for future improvement).\n\n5/N https://t.co/cwGxvkcoRC', '2. For fact verification, the #contrastive nature of #VitaminC helps to train more robust #FEVER and #MNLI models that avoid claim-only biases and are more sensitive to changes in the evidence. \n\n6/N https://t.co/VBgsqJljz6', 'We also explored a bit with using #GPT3 on a few examples, using an #ANLI 6-shot format. It seems to get some verdicts right but is still not as sensitive to changes in the evidence\n\n7/N https://t.co/ZLJ2ZBLyqQ', '3. Tracking the specific words that are changed in the sentence revisions provides valuable distance supervision for word-level rationales. \nWe train models to identify the anchoring spans in the evidence when examining a specific claim\n\n8/N https://t.co/49FLDePJA5', '4. #VitaminC is a useful testbed for factually consistent generation: (a) suggesting an edit to an outdated sentence, or (b) distilling the factual change from a revision.\nVitC-trained fact verification model can provide a success measure (e.g. detecting unsupported outputs)\n\n9/N https://t.co/3cbdLQHbnr', 'Many claims in the dataset have some numerical component  (e.g. ""There are more/less than _ reported cases of...""). \n\nQualitative analysis shows that #VitaminC, with its contrastive nature, helps fact verification models learn such X&lt;Y relations and be sensitive to changes\n\n10/N https://t.co/q2Qfdwqywg', 'Check out the #VitaminC dataset and our models here:\nüîóhttps://t.co/yo38BqdNAB\n\nAnd read more about our project in the @MIT_CSAIL news:\nüîó https://t.co/ICeheOu1as\n\nN/N', '@YiyuanLi1 @adamjfisch @BarzilayRegina Yes. Many of the cases in the datasets reflect developments and changes in the world such as new discoveries or updated information. Including many Covid examples']",https://arxiv.org/abs/2103.08541,"Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness -- improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation. ",Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence
22,1377183922073706500,805477439648440321,Rob Kavanagh,"['Our new paper is now up on @arxiv! We modelled the stellar winds of the active planet-hosting stars Prox Cen and AU Mic. For AU Mic, we found that the orbiting planets could induce radio emission in its corona! Find out more:\n\n<LINK> <LINK>', 'Our predicted emission ranges from 10 MHz to 3 GHz. At 140 MHz, it bears a striking resemblance to the recently reported emission from the M dwarf GJ 1151 by Vedantham et al. (2020), which is suspected of being induced by a planet:\n\nhttps://t.co/1jJF4fcFdR https://t.co/Fy84Se93er']",https://arxiv.org/abs/2103.16318,"There have recently been detections of radio emission from low-mass stars, some of which are indicative of star-planet interactions. Motivated by these exciting new results, in this paper we present Alfv\'en wave-driven stellar wind models of the two active planet-hosting M dwarfs Prox Cen and AU Mic. Our models incorporate large-scale photospheric magnetic field maps reconstructed using the Zeeman-Doppler Imaging method. We obtain a mass-loss rate of $0.25~\dot{M}_{\odot}$ for the wind of Prox Cen. For the young dwarf AU Mic, we explore two cases: a low and high mass-loss rate. Depending on the properties of the Alfv\'en waves which heat the corona in our wind models, we obtain mass-loss rates of $27$ and $590~\dot{M}_{\odot}$ for AU Mic. We use our stellar wind models to assess the generation of electron cyclotron maser instability emission in both systems, through a mechanism analogous to the sub-Alfv\'enic Jupiter-Io interaction. For Prox Cen we do not find any feasible scenario where the planet can induce radio emission in the star's corona, as the planet orbits too far from the star in the super-Alfv\'enic regime. However, in the case that AU Mic has a stellar wind mass-loss rate of $27~\dot{M}_{\odot}$, we find that both planets b and c in the system can induce radio emission from $\sim10$ MHz to 3 GHz in the corona of the host star for the majority of their orbits, with peak flux densities of $\sim10$ mJy. Detection of such radio emission would allow us to place an upper limit on the mass-loss rate of the star. ","Planet-induced radio emission from the coronae of M dwarfs: the case of
  Prox Cen and AU Mic"
23,1377181400713940994,805038313,Erik,['Excited to have finished my first paper with my supervisor! Our paper on a new geometric modification of gravity can be found here <LINK>'],https://arxiv.org/abs/2103.15906,"Starting from the original Einstein action, sometimes called the Gamma squared action, we propose a new setup to formulate modified theories of gravity. This can yield a theory with second order field equations similar to those found in other popular modified gravity models. Using a more general setting the theory gives fourth order equations. This model is based on the metric alone and does not require more general geometries. It is possible to show that our new theory and the recently proposed $f(Q)$ gravity models are equivalent at the level of the action and at the level of the field equations, provided that appropriate boundary terms are taken into account. Our theory can also match up with $f(R)$ gravity which is an expected result. Perhaps more surprisingly, we can also show that this equivalence extends to $f(T)$ gravity at the level of the action and its field equations, provided that appropriate boundary terms are taken in account. While these three theories are conceptually different and are based on different geometrical settings, we can establish the necessary conditions under which their field equations are indistinguishable. The final part requires matter to couple minimally to gravity. Through this work we emphasise the importance played by boundary terms which are at the heart of our approach. ",Modified gravity: a unified approach
24,1377178773766819849,344955311,Marinaüîª,['Our new paper about Circular polarisation of gamma rays as a probe of dark matter interactionswith cosmic ray electrons is already on arXiv\n\n<LINK>'],https://arxiv.org/abs/2103.14658,"Conventional indirect dark matter (DM) searches look for an excess in the electromagnetic emission from the sky that cannot be attributed to known astrophysical sources. Here, we argue that the photon polarisation is an important feature to understand new physics interactions and can be exploited to improve our sensitivity to DM. In particular, circular polarisation can be generated from Beyond the Standard Model interactions if they violate parity and there is an asymmetry in the number of particles which participate in the interaction. In this work, we consider a simplified model for fermionic (Majorana) DM and study the circularly polarised gamma rays below 10 GeV from the scattering of cosmic ray electrons on DM. We calculate the differential flux of positive and negative polarised photons from the Galactic Center and show that the degree of circular polarization can reach up to 90%. Finally, once collider and DM constraints have been taken into account, we estimate the required sensitivity from future experiments to detect this signal finding that, although a distinctive peak will be present in the photon flux spectrum, a near future observation is unlikely. However, different sources or models not considered in this work could provide higher intensity fluxes, leading to a possible detection by e-ASTROGAM. In the event of a discovery, we argue that the polarisation fraction is a valuable characterisation feature of the new sector. ","Circular polarisation of gamma rays as a probe of dark matter
  interactions with cosmic ray electrons"
25,1377169942252978181,481539448,Richard Alexander,"['Our new paper on HD143006, led by @GBallabio, is out today. We found that the complex morphology of this system can be explained by a giant planet in the disc around a misaligned binary star - potentially the first known misaligned circumbinary planet.\n<LINK> <LINK>', 'This project stems from an idea @GBallabio &amp; @bec_nealon came up with about 18 months ago...\n\nHD143006 is maybe the weirdest protoplanetary disc in the @almaobs DSHARP survey. Based on sub-mm and near-IR observations, the disc appears to look like this (fig from P√©rez+ 2018). https://t.co/OnU2wbRDl5', 'Giulia used @DiRAC_HPC simulations to show that neither a binary star nor a planet can explain all of the observed features. But a (co-planar) gas-giant planet orbiting in the disc around a *misaligned* binary is consistent with both the VLT/SPHERE and ALMA observations. https://t.co/7HZeicyMPW', 'Our synthetic observations also match the disc\'s kinematics. The CO channel maps from @almaobs (white contours) show a ""kink"" that may be due to a planet in the disc, and the model (colour scale) matches *beautifully*. ü§© https://t.co/97SvWi5nXa', 'As with any misaligned system, *everything* precesses (movie below). That means the observed features in the disc are expected to move, perhaps in as little as 5-10yr. Which means we can test the model...\nhttps://t.co/YGiGV1H6AY', 'If we\'re right, HD143006 would be the first known example of a circumbinary planet (a ""Tatooine"") whose orbit is not co-planar with the binary star. And we can potentially confirm it in just a few years\' time. üî≠', ""This has been one of the most fun projects I've been involved in, and credit goes to the whole team. @GBallabio &amp; @bec_nealon did the heavy lifting, with help from me, @nomadastro, Christophe Pinte &amp; @danprice_astro.üëçüèª\n\nI'll finish with another movie.üòÄ\nhttps://t.co/SiRMF1kbay""]",https://arxiv.org/abs/2103.16213,"Misalignments within protoplanetary discs are now commonly observed, and features such as shadows in scattered light images indicate departure from a co-planar geometry. VLT/SPHERE observations of the disc around HD 143006 show a large-scale asymmetry, and two narrow dark lanes which are indicative of shadowing. ALMA observations also reveal the presence of rings and gaps in the disc, along with a bright arc at large radii. We present new hydrodynamic simulations of HD 143006, and show that a configuration with both a strongly inclined binary and an outer planetary companion is the most plausible to explain the observed morphological features. We compute synthetic observations from our simulations, and successfully reproduce both the narrow shadows and the brightness asymmetry seen in IR scattered light. Additionally, we reproduce the large dust observed in the mm continuum, due to a 10 Jupiter mass planet detected in the CO kinematics. Our simulations also show the formation of a circumplanetary disc, which is misaligned with respect to the outer disc. The narrow shadows cast by the inner disc and the planet-induced ""kink"" in the disc kinematics are both expected to move on a time-scale of $\sim$ 5-10 years, presenting a potentially observable test of our model. If confirmed, HD 143006 would be the first known example of a circumbinary planet on a strongly misaligned orbit. ",HD 143006: circumbinary planet or misaligned disc?
26,1377160040897896458,280083723,Yoh Tanimoto,"['new paper~ <LINK> we construct a family of conformal nets associated with the W_3 algebra, for all c &gt;= 2~']",https://arxiv.org/abs/2103.16475,"A family of quantum fields is said to be strongly local if it generates a local net of von Neumann algebras. There are few methods of showing directly strong locality of a quantum field. Among them, linear energy bounds are the most widely used, yet a chiral conformal field of conformal weight $d>2$ cannot admit linear energy bounds. In this paper we give a new direct method to prove strong locality in two-dimensional conformal field theory. We prove that if a chiral conformal field satisfies an energy bound of degree $d-1$, then it also satisfies a certain local version of the energy bound, and this in turn implies strong locality. A central role in our proof is played by diffeomorphism symmetry. As a concrete application, we show that the vertex operator algebra given by a unitary vacuum representation of the $\mathcal{W}_3$-algebra is strongly local. For central charge $c > 2$, this yields a new conformal net. We further prove that these nets do not satisfy strong additivity, and hence are not completely rational. ",Local energy bounds and strong locality in chiral CFT
27,1377154324657184774,69282116,Fotios Petropoulos,"['In this new paper, we consider the environmental tendencies of forecasting models when selecting and combining across models for a particular time series. This is a very simple case of cross-learning based on precision and sensitivity. \n@Vspiliotis1\n<LINK>']",https://arxiv.org/abs/2103.16157,"Standard selection criteria for forecasting models focus on information that is calculated for each series independently, disregarding the general tendencies and performances of the candidate models. In this paper, we propose a new way to statistical model selection and model combination that incorporates the base-rates of the candidate forecasting models, which are then revised so that the per-series information is taken into account. We examine two schemes that are based on the precision and sensitivity information from the contingency table of the base rates. We apply our approach on pools of exponential smoothing models and a large number of real time series and we show that our schemes work better than standard statistical benchmarks. We discuss the connection of our approach to other cross-learning approaches and offer insights regarding implications for theory and practice. ",Model combinations through revised base-rates
28,1377153368477495297,1232964338,Shaikh saad,['Here is our new paper on Leptogenesis <LINK> <LINK>'],https://arxiv.org/abs/2103.14691,"We investigate low-scale resonant leptogenesis in an $SU(5) \times \mathcal{T}_{13}$ model where a single high energy phase in the complex Tribimaximal seesaw mixing produces the yet-to-be-observed low energy Dirac and Majorana ${CP}$ phases. A fourth right-handed neutrino, required to generate viable light neutrino masses within this scenario, also turns out to be necessary for successful resonant leptogenesis where $CP$ asymmetry is produced by the same high energy phase. We derive a lower bound on the right-handed neutrino mass spectrum in the $\mathrm{GeV}$ range, where part of the parameter space, although in tension with Big Bang Nucleosynthesis constrains, can be probed in planned high intensity experiments like DUNE. We also find the existence of a curious upper bound ($\text{TeV}$-scale) on the right-handed neutrino mass spectrum in majority of the parameter space due to significant washout of the resonant asymmetry by lighter right-handed neutrinos. While in most of the parameter space of our model classical Boltzmann equations are sufficient, when right-handed neutrino masses are below the electroweak sphalerons freeze-out temperature we resort to the more general density matrix formalism to take into account decays and oscillations of the right-handed neutrinos as well as their helicities which are relevant when they are relativistic. ","Low-scale Resonant Leptogenesis in $SU(5)$ GUT with $\mathcal{T}_{13}$
  Family Symmetry"
29,1377066858394877953,1238551757908840448,Sean Pinkney,['New paper on synthetic control using @mcmc_stan <LINK>'],http://arxiv.org/abs/2103.16244,"An improved and extended Bayesian synthetic control model is presented, expanding upon the latent factor model in Tuomaala 2019. The changes we make include 1) standardization of the data prior to model fit - which improves efficiency and generalization across different data sets; 2) adding time varying covariates; 3) adding the ability to have multiple treated units; 4) fitting the latent factors within the Bayesian model; and, 5) a sparsity inducing prior to automatically tune the number of latent factors. We demonstrate the similarity of estimates to two traditional synthetic control studies in Abadie, Diamond, and Hainmueller 2010 and Abadie, Diamond, and Hainmueller 2015 and extend to multiple target series with a new example of estimating digital website visitation from changes in data collection due to digital privacy laws. ",An Improved and Extended Bayesian Synthetic Control
30,1376943981553016832,2600374380,"Alex Huffman, Ph.D.","['New pre-print: ‚ÄúRespiratory aerosols and droplets in the transmission of infectious diseases‚Äù\n\nHappy to contribute to this deeply detailed review paper by Mira P√∂hlker et al. &amp; colleagues at the Max Planck Institute for Chemistry.\n\nLink: <LINK>\n\nA üßµsummary (1/x) <LINK>', '2/ Preprint review is very detailed: 50 pgs, 20 figures, 7 tables, &gt;360 references.\n\n‚òëÔ∏èReviews literature:\n- Respiratory particle size distributions (rPSDs)\n- Physical properties\n- Emission &amp; shrinkage mechanisms\n‚òëÔ∏èNew multimodal parameterization of rPSDs\n‚òëÔ∏èPractical perspective https://t.co/bdI7tomgLD', '3/ The review ends by showing a parameterized size distribution of respiratory particles &amp; the risks in different parts of a room using #masks, #ventilation, and #filtration. Broad application to SARS-CoV-2 other respiratory viruses.\n\nLink: https://t.co/XYVrsGnAwP https://t.co/zzDM3iB6ps', '4/ Reviews literature evidence &amp; summarizes the relative concentrations (C) and emission rates (Q) of different particle sizes from breathing, speaking, coughing in both the near- and far-field of a room. https://t.co/QD6XdHnQwE', ""5/ Starts by considering the 'lifecycle' of respiratory droplets and aerosols, including emission; physics of transport and evaporation; exposure by deposition &amp; inhalation. https://t.co/NISWqF8KAb"", '6/ Presents a detailed overview of the physiological locations &amp; physical mechanisms for each size range of respiratory particles emitted. https://t.co/apzy83qHI9', '7/ Summarizes respiratory particle sedimentation and travel based on well-known physics. https://t.co/kDl6l9b8uB', '8/ Reviews the processes and properties related to respiratory droplet shrinkage as a function of relative humidity, and provides links to long-standing experience with similar questions via atmospheric physics community. https://t.co/GEaruDrubn', '9/ Takes all literature-available respiratory particle size distribution data, parameterizes to a multimodal fit algorithm, and also makes conceptual links to other particle generation mechanisms in the outdoor environment. https://t.co/HggRlwm2ZE', '10/ Summarizing experimental evidence from literature, proposes that respiratory particle emissions from all standard activities (i.e. breathing, speaking, coughing) can be simplified into only five particle size modes: B1,B2 - bronchial; LT - larynx &amp; trachea; ; O1,O2 - oral. https://t.co/oQMfWqRxCr', '11/ Ultimately, the paper compares the newly parameterized distributions of respiratory particles w/ size distributions in which different respiratory viruses have been found - to suggest a physical basis for plausible physiological sources &amp; emission mechanisms. https://t.co/zEcboByxsh', '12/ Also calculates respiratory particle emission rates (aerosols &amp; droplets) from speaking when using masks* and not.\n\nUses a combination of emission rates w/ published evidence of efficiency of different types of masks.\n\n*Assuming perfect mask fit. \nAlso note log y-scale. https://t.co/FZrX4cTRxB', '13/ Among the benefits for aerosol &amp; respiratory disease researchers is the summarized literature evidence of various properties including - \n\nA summary of experimentally determined decay rates by several respiratory viral pathogens. https://t.co/A3S31cltCU', '14/ A summary of the main chemical constituents in saliva and epithelial lining fluid (both important for respiratory aerosol/droplet emissions). https://t.co/LlkDscMQ4E', '15/ A summary of aerosol number size distributions reported in the literature from respiratory activities. https://t.co/cto0b3HjcW', '16/ *Preprint* manuscript ""provides a critical review &amp; synthesis of scientific knowledge"" on emission, transport, &amp; deposition of ""Respiratory aerosols &amp; droplets in the transmission of infectious disease"".\n\nSuggestions, omissions, edits welcome.\n\nLink: https://t.co/XYVrsGnAwP', '@MinnaVakeva Thx. There is a LOT of info there! Most people won\'t want to read the whole thing, but various helpful sections for different groups. ""Review"" portion is one half &amp; the ""size distribution parameterization"" is another half. (Mira, Christopher P√∂hlker did great work to lead this).', ""@citlanx Many months ... Yes, there is a lot there, and most won't need to read everything. The table of contents will be a useful ally!\n\nThe heaviest lifting was Mira &amp; Christopher P√∂hlker. We've done a lot of work together."", '@johnwenger9 Thanks. I spent many hours with it, but the heavy lifting was definitely Mira and Christopher P√∂hlker from the MPIC.', '@PrasadKasibhat1 Thanks ... feel free to pass on comments. Impossible to make a 50-pg paper perfect, so can always use constructive comments as it goes through review.']",https://arxiv.org/abs/2103.01188v2,"Knowing the physicochemical properties of exhaled droplets and aerosol particles is a prerequisite for a detailed mechanistic understanding and effective prevention of the airborne transmission of infectious human diseases. This article provides a critical review and synthesis of scientific knowledge on the number concentrations, size distributions, composition, mixing state, and related properties of respiratory particles emitted upon breathing, speaking, singing, coughing, and sneezing. We derive and present a parameterization of respiratory particle size distributions based on five lognormal modes related to different origins in the respiratory tract, which can be used to trace and localize the sources of infectious particles. This approach may support the medical treatment as well as the risk assessment for aerosol and droplet transmission of infectious diseases. It was applied to analyze which respiratory activities may drive the spread of specific pathogens, such as Mycobacterium tuberculosis, influenza viruses, and SARS-CoV-2 viruses. The results confirm the high relevance of vocalization for the transmission of SARS-CoV-2 as well as the usefulness of physical distancing, face masks, room ventilation, and air filtration as preventive measures against COVID-19 and other airborne infectious diseases. ","] Respiratory aerosols and droplets in the transmission of infectious
  diseases"
31,1376916044254564352,17138973,Prof. Heather Gray,['New paper: <LINK>. Simulation on GPUs!'],https://arxiv.org/abs/2103.14737,"The High Energy Physics (HEP) experiments, such as those at the Large Hadron Collider (LHC), traditionally consume large amounts of CPU cycles for detector simulations and data analysis, but rarely use compute accelerators such as GPUs. As the LHC is upgraded to allow for higher luminosity, resulting in much higher data rates, purely relying on CPUs may not provide enough computing power to support the simulation and data analysis needs. As a proof of concept, we investigate the feasibility of porting a HEP parameterized calorimeter simulation code to GPUs. We have chosen to use FastCaloSim, the ATLAS fast parametrized calorimeter simulation. While FastCaloSim is sufficiently fast such that it does not impose a bottleneck in detector simulations overall, significant speed-ups in the processing of large samples can be achieved from GPU parallelization at both the particle (intra-event) and event levels; this is especially beneficial in conditions expected at the high-luminosity LHC, where extremely high per-event particle multiplicities will result from the many simultaneous proton-proton collisions. We report our experience with porting FastCaloSim to NVIDIA GPUs using CUDA. A preliminary Kokkos implementation of FastCaloSim for portability to other parallel architectures is also described. ",Porting HEP Parameterized Calorimeter Simulation Code to GPUs
32,1376901591870017536,769188626324398080,Heiga Zen (ÂÖ® ÁÇ≥Ê≤≥),"['New paper from our team: \n\nYe Jia, Heiga Zen, Jonathan Shen, Yu Zhang, Yonghui Wu\n""PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS""\n\nArxiv: <LINK>\nSamples: <LINK>', '""This paper introduces PnG BERT, a new encoder model for neural TTS. This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them.', 'It can be pre-trained on a large text corpus in a self-supervised manner, and fine-tuned in a TTS task. Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model', 'using only phoneme input with no pre-training. Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers.""']",https://arxiv.org/abs/2103.15060,"This paper introduces PnG BERT, a new encoder model for neural TTS. This model is augmented from the original BERT model, by taking both phoneme and grapheme representations of text as input, as well as the word-level alignment between them. It can be pre-trained on a large text corpus in a self-supervised manner, and fine-tuned in a TTS task. Experimental results show that a neural TTS model using a pre-trained PnG BERT as its encoder yields more natural prosody and more accurate pronunciation than a baseline model using only phoneme input with no pre-training. Subjective side-by-side preference evaluations show that raters have no statistically significant preference between the speech synthesized using a PnG BERT and ground truth recordings from professional speakers. ",PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS
33,1376849058984263687,738769492122214400,Johannes Lischner,['In our new paper we study light-induced charge transfer from transition metal doped aluminium clusters to CO2 for applications in #photocatalysis. Read here:  <LINK> #compchem <LINK>'],https://arxiv.org/abs/2103.14405,"Charge transfer between molecules and catalysts plays a critical role in determining the efficiency and yield of photo-chemical catalytic processes. In this paper, we study light-induced electron transfer between transition metal doped aluminium clusters and CO$_2$ molecules using first-principles time-dependent density-functional theory. Specifically, we carry out calculations for a range of dopants (Zr, Mn, Fe, Ru, Co, Ni and Cu) and find that the resulting systems fall into two categories: Cu- and Fe-doped clusters exhibit no ground state charge transfer, weak CO$_2$ adsorption and light-induced electron transfer into the CO$_2$. In all other systems, we observe ground state electron transfer into the CO$_2$ resulting in strong adsorption and predominantly light-induced electron back-transfer from the CO$_2$ into the cluster. These findings pave the way towards a rational design of atomically precise aluminium photo-catalysts. ","Light Induced Charge Transfer from Transition-metal Doped Aluminium
  Clusters to Carbon Dioxide"
34,1376841955443929092,4878011,Matthias Rosenkranz ‚ú®,"['Our new paper ""Variational inference with a #quantum computer"" has been out for a few days üéâ.\n\nWe develop the methods, then demonstrate them using a few graphical models (e.g. hidden Markov).\n \n<LINK>\n#QuantumComputing #MachineLearning <LINK>', 'Variational inference aims at finding an optimal, approximate posterior distribution of latent variables. This is useful for generative models or reasoning under uncertainty.', 'Our main idea is to use a quantum-circuit Born machine as a flexible and expressive family of approximate posterior distributions. The forward model remains classical. https://t.co/vCPpMQs76h', 'The Born machine only provides sample access so we develop two methods suitable for this situation. Both are inspired by advances in classical VI. The first one is based on an adversarial objective, the second one on the kernelized Stein discrepancy. https://t.co/V7tluShbbT', 'We demonstrate the methods on a simulator and an IBM quantum device. Essentially everything works. We hope to collect a few more shots from the real device for the next update. https://t.co/8uhCEgHgOw', 'Thank you to my collaborators Marcello Benedetti, @BrianC2095, @matt_fiorentini and Michal Lubasch, all at @cambridgecqc', 'We also provide a more high-level introduction to the ideas in this blog post\nhttps://t.co/TZYFz4tfLm']",https://arxiv.org/abs/2103.06720,"Inference is the task of drawing conclusions about unobserved variables given observations of related variables. Applications range from identifying diseases from symptoms to classifying economic regimes from price movements. Unfortunately, performing exact inference is intractable in general. One alternative is variational inference, where a candidate probability distribution is optimized to approximate the posterior distribution over unobserved variables. For good approximations, a flexible and highly expressive candidate distribution is desirable. In this work, we use quantum Born machines as variational distributions over discrete variables. We apply the framework of operator variational inference to achieve this goal. In particular, we adopt two specific realizations: one with an adversarial objective and one based on the kernelized Stein discrepancy. We demonstrate the approach numerically using examples of Bayesian networks, and implement an experiment on an IBM quantum computer. Our techniques enable efficient variational inference with distributions beyond those that are efficiently representable on a classical computer. ",Variational inference with a quantum computer
35,1376818540787941376,335939030,Gavin Buckingham,"[""Just in time for @IEEEVR, I've written a new paper called: Hand tracking for immersive virtual reality: opportunities and challenges\n\nYou can find this on arXiv\n\n<LINK>\n\n[thread]"", 'With integrated hand tracking in #VR headsets like the @oculus Quest 1/2, all of a sudden hand tracking has become part of a number of cool experiences, like this from @pushmatrix. https://t.co/EeNMmeTFiz', ""Being able to see your hands move in real time can be an incredibly immersive experience - it's an instant 'wow' factor when done well (such as these ones, tracked with a Leap Motion Tracker in my lab https://t.co/8NtkJ9QVPw"", ""With this sort of technology, you can readily inhabit new body types in a very compelling way. Here we've extended the length of the index finger, such that @samueljamesvine (in the HMD) feels the need to check whether his real finger has grown https://t.co/ziYOyoF1sp"", 'So what is VR hand tracking best for? NOT object interaction in my view. Without tactile/haptic cues, this feels awkward. Indeed, a recent paper shows that people are quite a bit poorer moving blocks in VR with their tracked hands than with controllers https://t.co/hEHJ66La2A', 'VR hand tracking does, however, make the experience of communication SO much better. So much of our communication is gestural, and hand tracking is far easier from a technical perspective than nuanced face tracking of microexpressions https://t.co/9GgoHcMWOX', 'Challenges? Tons! \n\nUncanny valley: very easy to hit the feelings of disgust with CG faces, but almost nothing is known about what triggers the experiences with tracked hands. Most likely it will be a multisensory effect driven by visuo-haptic mismatches https://t.co/m62X6Vfthy', ""Field of view: Most hand movements occur in our lower visual field, but hand tracking is largely straight ahead. VR headsets also cut off far above the normal human field of view. Losing tracking when hands are in the 'natural' space will kill immersion  https://t.co/WDQhLw9Eku"", 'Inclusivity - tracking: Remember that soap dispenser which failed to detect darker skin tones? Lots of anecdotal reports across the internet of VR hand tracking not working well with non-white skin tones also.... https://t.co/V9j36Fn5YI', ""Inclusivity - visualization: You probably want the option to have hands that look like your own. If you're not a white male, you might be out of luck! But research by @PsychFarmer has shown that embodiment suffers when forced into a different body type   https://t.co/mE5vn0lbWU"", ""And that's not to mention individuals with limb differences or non-average body shapes/configurations. Hopefully hand tracking and avatar generation can be fully personalized in the future with 3d scanning technologies!"", ""Please do check out the preprint and give me feedback!\n\nSide note - I'm particularly excited about the release @HandPhysicsLab on the 1st of April! So far those have been some amazing experiences""]",https://arxiv.org/abs/2103.14853,"Hand tracking has become an integral feature of recent generations of immersive virtual reality head-mounted displays. With the widespread adoption of this feature, hardware engineers and software developers are faced with an exciting array of opportunities and a number of challenges, mostly in relation to the human user. In this article, I outline what I see as the main possibilities for hand tracking to add value to immersive virtual reality as well as some of the potential challenges in the context of the psychology and neuroscience of the human user. It is hoped that this paper serves as a roadmap for the development of best practices in the field for the development of subsequent generations of hand tracking and virtual reality technologies. ","Hand tracking for immersive virtual reality: opportunities and
  challenges"
36,1376727888079257603,2956121356,Russ Salakhutdinov,"['New #ICLR2021 paper on Self-supervised Learning with Relative Predictive Coding: A new contrastive learning objective that maintains a good balance between training stability, minibatch size sensitivity, &amp; downstream task performance.\n\n<LINK>\n\nw/ H. Tsai et al. <LINK>']",https://arxiv.org/abs/2103.11275,"This paper introduces Relative Predictive Coding (RPC), a new contrastive representation learning objective that maintains a good balance among training stability, minibatch size sensitivity, and downstream task performance. The key to the success of RPC is two-fold. First, RPC introduces the relative parameters to regularize the objective for boundedness and low variance. Second, RPC contains no logarithm and exponential score functions, which are the main cause of training instability in prior contrastive objectives. We empirically verify the effectiveness of RPC on benchmark vision and speech self-supervised learning tasks. Lastly, we relate RPC with mutual information (MI) estimation, showing RPC can be used to estimate MI with low variance. ",Self-supervised Representation Learning with Relative Predictive Coding
37,1376722480803373058,764311987308204032,Namrata Roy,"['My new paper is accepted in ApJ and is now on the arxiv:\n<LINK>...\nIf you are interested in AGN-driven winds,  ""red geysers"" are the right galaxies for you! We look at high res detailed velocity profiles and confirm these are  ""outflows"", NOT ""rotating disks"".', 'If you are interested to know more about these red geysers, check out my other paper here: https://t.co/c318vhMb1Q   or the Nature discovery paper here: https://t.co/lUo7HVp8aT']",https://arxiv.org/abs/2103.14928,"Spatially resolved spectroscopy from SDSS-IV MaNGA survey has revealed a class of quiescent, relatively common early-type galaxies, termed ""red geysers"", that possibly host large scale active galactic nuclei driven winds. Given their potential importance in maintaining low level of star formation at late times, additional evidence confirming that winds are responsible for the red geyser phenomenon is critical. In this work, we present follow-up observations with the Echellette Spectrograph and Imager (ESI) at the Keck telescope of two red geysers (z$<$0.1) using multiple long slit positions to sample different regions of each galaxy. Our ESI data with a spectral resolution (R) $\sim$ 8000 improves upon MaNGA's resolution by a factor of four, allowing us to resolve the ionized gas velocity profiles along the putative wind cone with an instrumental resolution of $\rm \sigma = 16~km~s^{-1}$. The line profiles of H$\alpha$ and [NII]$\rm \lambda 6584$ show asymmetric shapes that depend systematically on location $-$ extended blue wings on the red-shifted side of the galaxy and red wings on the opposite side. We construct a simple wind model and show that our results are consistent with geometric projections through an outflowing conical wind oriented at an angle towards the line of sight. An alternative hypothesis that assigns the asymmetric pattern to ""beam-smearing"" of a rotating, ionized gas disk does a poor job matching the line asymmetry profiles. While our study features just two sources, it lends further support to the notion that red geysers are the result of galaxy-scale winds. ",Evidence of wind signatures in the gas velocity profiles of Red Geysers
38,1376553367980285955,705950701986275328,Tzanio Kolev,"['ü§î Need to couple or transfer fields between high- and low-order simulations? Check out our new paper with Will Pazner: \n\nüìÑ ""Conservative and accurate solution transfer between high-order and low-order refined finite element spaces""  \n\nüëâ <LINK>\n\n#PoweredByMFEM <LINK>']",https://arxiv.org/abs/2103.05283,"In this paper we introduce general transfer operators between high-order and low-order refined finite element spaces that can be used to couple high-order and low-order simulations. Under natural restrictions on the low-order refined space we prove that both the high-to-low-order and low-to-high-order linear mappings are conservative, constant preserving and high-order accurate. While the proofs apply to affine geometries, numerical experiments indicate that the results hold for more general curved and mixed meshes. These operators also have applications in the context of coarsening solution fields defined on meshes with nonconforming refinement. The transfer operators for $H^1$ finite element spaces require a globally coupled solve, for which robust and efficient preconditioners are developed. We present several numerical results confirming our analysis and demonstrate the utility of the new mappings in the context of adaptive mesh refinement and conservative multi-discretization coupling. ","Conservative and accurate solution transfer between high-order and
  low-order refined finite element spaces"
39,1376456668293296128,4249537197,Christian Wolf,['New paper by @JannySteeven of our group: We forecast trajectories after observing an initial seq and show that any latent representation of size &gt;= 2m+1 with linear and stable dynamics has a solution. Joint work with V. Andrieu + M. Nadri (LAGEPP).\n<LINK> <LINK>'],https://arxiv.org/abs/2103.12443,"We address the problem of output prediction, ie. designing a model for autonomous nonlinear systems capable of forecasting their future observations. We first define a general framework bringing together the necessary properties for the development of such an output predictor. In particular, we look at this problem from two different viewpoints, control theory and data-driven techniques (machine learning), and try to formulate it in a consistent way, reducing the gap between the two fields. Building on this formulation and problem definition, we propose a predictor structure based on the Kazantzis-Kravaris/Luenberger (KKL) observer and we show that KKL fits well into our general framework. Finally, we propose a constructive solution for this predictor that solely relies on a small set of trajectories measured from the system. Our experiments show that our solution allows to obtain an efficient predictor over a subset of the observation space. ",Deep KKL: Data-driven Output Prediction for Non-Linear Systems
40,1376455233166016517,490844234,Lena,"['Looking forward to presenting our paper on Mental Health, Abortion and Risky Behaviors in the Virtual Mental health Seminar tomorrow! Brand new version on Arxiv is also available! <LINK> See you tomorrow! <LINK>']",http://arxiv.org/abs/2103.12159,"In this paper, we provide causal evidence on abortions and risky health behaviors as determinants of mental health development among young women. Using administrative in- and outpatient records from Sweden, we apply a novel grouped fixed-effects estimator proposed by Bonhomme and Manresa (2015) to allow for time-varying unobserved heterogeneity. We show that the positive association obtained from standard estimators shrinks to zero once we control for grouped time-varying unobserved heterogeneity. We estimate the group-specific profiles of unobserved heterogeneity, which reflect differences in unobserved risk to be diagnosed with a mental health condition. We then analyze mental health development and risky health behaviors other than unwanted pregnancies across groups. Our results suggest that these are determined by the same type of unobserved heterogeneity, which we attribute to the same unobserved process of decision-making. We develop and estimate a theoretical model of risky choices and mental health, in which mental health disparity across groups is generated by different degrees of self-control problems. Our findings imply that mental health concerns cannot be used to justify restrictive abortion policies. Moreover, potential self-control problems should be targeted as early as possible to combat future mental health consequences. ","Mental Health and Abortions among Young Women: Time-varying Unobserved
  Heterogeneity, Health Behaviors, and Risky Decisions"
41,1376452403390713856,1297174706622205955,Andrea Palermo,"[""A week after the theoretical paper, it's time for some simulations. What would imply the new spin-shear coupling for the local polarization sign puzzle?\n\nFind out in our new paper, today on the arXiv!\n<LINK>\n\n1/6"", 'The sign of the polarization along the beam direction predicted by the local equilibrium picture is opposite with respect to the experiments. This instance is known as ""polarization sign puzzle"" and there have been several attempts to solve it. \n2/6', 'For instance, hydro simulations considering the usual spin-vorticity coupling give the following polarization as a function of transverse momentum; in experiments what is red would be blue and vice-versa! \n3/6 https://t.co/JVA4TVPKp9', 'Can the spin-shear coupling solve the problem? Indeed if we consider the thermal shear the simulation gives a result closer to the experiments, but not yet satisfactory...\n4/6 https://t.co/GZ9wR0m3pi', 'However, at very high energy the decoupling hypersurface (where the plasma is no longer a fluid) is believed to be at constant temperature. Keeping this in mind we obtain a formula for isothermal local equilibrium, coupling spin with the gradients of the velocity. \n5/6 https://t.co/ToOzbzSSnR', 'This new formula restores the agreement between theory and experiments, and we get the correct sign with two different hydro codes! Also, polarization seems to be very sensitive to the decoupling temperature providing a wonderful tool to study hadronization. \n6/6 https://t.co/Z20xOJ5GK4']",https://arxiv.org/abs/2103.14621,"We show that the inclusion of a recently found additional term of the spin polarization vector at local equilibrium which is linear in the symmetrized gradients of the velocity field, and the assumption of hadron production at constant temperature restore the quantitative agreement between hydrodynamic model predictions and local polarization measurements in relativistic heavy ion collisions at $\sqrt{s_{\rm NN}} = 200$ GeV. The longitudinal component of the spin polarization vector turns out to be very sensitive to the temperature value, with a good fit around 155 MeV. The implications of this finding are discussed. ","Local polarization and isothermal local equilibrium in relativistic
  heavy ion collisions"
42,1376370101260320771,706048860087586817,Alexey Bochkovskiy,['Vision Transformers for Dense Prediction: achieves the new SotA accuracy on depth estimation and image segmentation by improving Vision Transformer. \nIt is real-time &gt;30 FPS.\n\npaper: <LINK>\ngithub: <LINK> \ncomparison: <LINK> <LINK>'],https://arxiv.org/abs/2103.13413,"We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at this https URL ",Vision Transformers for Dense Prediction
43,1376358460208472068,1115880604560691200,NII Yamagishi Lab,"['Preprint of our new paper submitted to Interspeech, ""A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection,"" is online:  <LINK>', 'Experiments compare common strategies to deal with varying input length, margin-based training criteria, and widely-used front ends. Experiments were also contucted with different random initial seeds, which may produce significantly different performance from the same model https://t.co/pmdiIng3vR', 'Promising techniques that were observed included average pooling and a new hyper-parameter-free loss function, which led to the best single model in the experiments with an equal error rate of 1.92%.']",https://arxiv.org/abs/2103.11326,"A great deal of recent research effort on speech spoofing countermeasures has been invested into back-end neural networks and training criteria. We contribute to this effort with a comparative perspective in this study. Our comparison of countermeasure models on the ASVspoof 2019 logical access task takes into account recently proposed margin-based training criteria, widely used front ends, and common strategies to deal with varied-length input trials. We also measured intra-model differences through multiple training-evaluation rounds with random initialization. Our statistical analysis demonstrates that the performance of the same model may be significantly different when just changing the random initial seed. Thus, we recommend similar analysis or multiple training-evaluation rounds for further research on the database. Despite the intra-model differences, we observed a few promising techniques such as the average pooling to process varied-length inputs and a new hyper-parameter-free loss function. The two techniques led to the best single model in our experiment, which achieved an equal error rate of 1.92% and was significantly different in statistical sense from most of the other experimental models. ","A Comparative Study on Recent Neural Spoofing Countermeasures for
  Synthetic Speech Detection"
44,1376260821588803595,4026177965,Dr. Amanda Bailey Hass üôÇ,"['Non Covid related but, if your have ever wondered ""What is a liquid?"", I have a new paper out with the respectable Bill Hoover, the grandfather of MD modelling. Make yourself a cuppa and enjoy <LINK>', '@Druidston Oh if you like Laplace and Fourier a space, you might like my next paper ü§£']",https://arxiv.org/abs/2103.00511,"We consider the practicalities of defining, simulating, and characterizing ""Liquids"" from a pedagogical standpoint based on atomistic computer simulations. For simplicity and clarity we study two-dimensional systems throughout. In addition to the infinite-ranged Lennard-Jones 12/6 potential we consider two shorter-ranged families of pair potentials. At zero pressure one of them includes just nearest neighbors. The other longer-ranged family includes twelve additional neighbors. We find that these further neighbors can help stabilize the liquid phase. What about liquids? To implement Wikipedia's definition of liquids as conforming to their container we begin by formulating and imposing smooth-container boundary conditions. To encourage conformation further we add a vertical gravitational field. Gravity helps stabilize the relatively vague liquid-gas interface. Gravity reduces the messiness associated with the curiously-named ""spinodal"" (tensile) portion of the phase diagram. Our simulations are mainly isothermal. We control the kinetic temperature with Nos\'e-Hoover thermostating, extracting or injecting heat so as to impose a mean kinetic temperature over time. Our simulations stabilizing density gradients and the temperature provide critical-point estimates fully consistent with previous efforts from free energy and Gibbs' ensemble simulations. This agreement validates our approach. ",What is Liquid ? [in two dimensions]
45,1375687898192769025,2878552400,Tanmoy Chakraborty,"['Check out our new survey paper on Multimodal Disinformation Detection\n\n\\w @preslav_nakov @firojalam04 @s_cresci\n@fabreetseo, Dimitar, Giovanni, Shaden, Hamed\n\n@QatarComputing @facebookai @IIITDelhi\n\n#fakenews #misinformation #disinformation #multimodality\n\n<LINK> <LINK>']",https://arxiv.org/abs/2103.12541,"Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation -- (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions. ",A Survey on Multimodal Disinformation Detection
46,1375486632728616969,719928410814955520,Evgenii Zheltonozhskii,"['Our new paper, C2D (<LINK>, <LINK>) shows how self-supervised pre-training boosts learning with noisy labels, achieves SOTA performance and provides in-depth analysis. Authors @evgeniyzhe @ChaimBaskin Avi Mendelson, Alex Bronstein, @orlitany 1/n', 'The problem of learning with noisy labels (LNL) has great practical importance: large clean dataset is often expensive or impossible to get. Existing approaches to LNL either modify the loss to account for the noise or try to detect the noisy-labelled samples. 2/n', 'Yet, we need a starting point. For that, we use ""warm-up"": regular training on full dataset, which relies on the intrinsic robustness of DNNs to the noise. Main goals of warm-up are providing feature extractor and keeping loss of the noisy samples high, avoiding memorization. 3/n', ""We believe that inability to achieve those goals is a significant obstacle to improving performance of the learning with noisy labels. While robustness of the networks allows us to achieve good results, we don't know about what are its sources or how to improve it. 4/n"", 'One (very popular) solution is using large scale pre-training, for example, on ImageNet. However, outside of natural images domain, large clean datasets may not exist. Moreover, in some of our experiments ImageNet pre-training have actually harmed the performance. 5/n', 'Instead, we propose to use self-supervised pre-training.  This way, we do not require external data; by ignoring labels, we eliminate influence of noise on the pre-training;  by operating on the training set, we avoid domain gap.  This can be combined with any LNL method. 6/n', 'Our proposed framework, which we call Contrast to Divide, or C2D, comprises two stages: self-supervised pre-training (contrast phase), followed by standard algorithm for learning with noisy labels, which can now enjoy better initialization  7/n', 'Tested with two SOTAs, DivideMix and ELR+, C2D shows huge boosts both in real-life and synthetic cases. For mini-webVision, we improve by more than 3% both on WebVision (79.42%) and ImageNet (78.57%) validation sets. For CIFAR-100 with 90% noise we improve from 34% to 64% 8/n', 'For Clothing1M dataset the default approach is to utilize ImageNet-pre-trained ResNet-50 as initialization. C2D was able to match state-of-the-art performance (74.58+-0.15% as compared to 74.81%) without any external data. 9/n', 'For analysis, we started with looking on UMAP features for CIFAR-10. We took the features at the end of warm-up for 20% and 90% noise with and without C2D, and features at the end of self-supervised pre-training.  C2D enjoys nuch better separability at high noise settings 10/n https://t.co/WXOmHfM9Sd', 'To quantify warm-up goals, we measure loss separability (as ROC-AUC of GMM trained on the loss values) and feature extraction (evaluated with linear classifier) on CIFAR-100. For both, C2D provides significant accuracy boost to ImageNet pre-training and traditional warm-up. 11/n https://t.co/uZxiBl0DY3', 'We also looked at the distribution of the loss values for clean and noisy samples at the end of warm-up. Interestingly, ImageNet pre-training appears to have even larger overlap than no pre-training, both significantly worse than C2D. 12/n https://t.co/1BLKHCB5NC', 'Finally, we compared C2D with DivideMix to MixMatch. You can think of it as providing an oracle knowing which samples are noisy to DivideMix. Impressively, MixMatch is not better than C2D, and with self-supervised pre-training for MixMatch the difference is only 2%. 13/n', 'Another interesting phenomenon is that we were unable to achieve good results on CIFAR with ImageNet pre-training. While self-supervised pre-training worked both with DivideMix and ELR+ almost out of the box, we were not able to get good results with ImageNet.  14/n', ""To conclude, C2D is a simple and efficient way to boost learning with noisy labels. Our code is available, but you don't really need it: just train SimCLR on your dataset and use it as initialization to your current method to fight noise. It probably will work better. 15/n, n=15"", '@threadreaderapp unroll']",https://arxiv.org/abs/2103.13646,"The success of learning with noisy labels (LNL) methods relies heavily on the success of a warm-up stage where standard supervised training is performed using the full (noisy) training set. In this paper, we identify a ""warm-up obstacle"": the inability of standard warm-up stages to train high quality feature extractors and avert memorization of noisy labels. We propose ""Contrast to Divide"" (C2D), a simple framework that solves this problem by pre-training the feature extractor in a self-supervised fashion. Using self-supervised pre-training boosts the performance of existing LNL approaches by drastically reducing the warm-up stage's susceptibility to noise level, shortening its duration, and improving extracted feature quality. C2D works out of the box with existing methods and demonstrates markedly improved performance, especially in the high noise regime, where we get a boost of more than 27% for CIFAR-100 with 90% noise over the previous state of the art. In real-life noise settings, C2D trained on mini-WebVision outperforms previous works both in WebVision and ImageNet validation sets by 3% top-1 accuracy. We perform an in-depth analysis of the framework, including investigating the performance of different pre-training approaches and estimating the effective upper bound of the LNL performance with semi-supervised learning. Code for reproducing our experiments is available at this https URL ","Contrast to Divide: Self-Supervised Pre-Training for Learning with Noisy
  Labels"
47,1375480932417765379,52381876,Edward Frenkel,['Last week I gave a seminar at @Columbia about a new work with Etingof &amp; Kazhdan on the analytic Langlands correspondence for complex curves:\n<LINK>\nSee <LINK>\nfor more info &amp; links to related talks.\nOur new paper is here:\n<LINK>'],https://arxiv.org/abs/2103.01509,"We construct analogues of the Hecke operators for the moduli space of G-bundles on a curve X over a local field F with parabolic structures at finitely many points. We conjecture that they define commuting compact normal operators on the Hilbert space of half-densities on this moduli space. In the case F=C, we also conjecture that their joint spectrum is in a natural bijection with the set of opers on X for the Langlands dual group with real monodromy. This may be viewed as an analytic version of the Langlands correspondence for complex curves. Furthermore, we conjecture an explicit formula relating the eigenvalues of the Hecke operators and the global differential operators studied in our previous paper arXiv:1908.09677. Assuming the compactness conjecture, this formula follows from a certain system of differential equations satisfied by the Hecke operators, which we prove in this paper for G=PGL(n). ","Hecke operators and analytic Langlands correspondence for curves over
  local fields"
48,1375439934623002625,1324428524,Rikard Enberg,"['New paper today on cosmology in the very early universe, with my postdoc and previous PhD student. The paper is about the abrupt phase transition a few picoseconds after the big bang, where the Higgs field switched on and particles stopped being massless. <LINK>', ""You can't have abrupt phase transitions in the Standard Model because the Higgs is too heavy. In theories beyond you can. We look at this in effective field theory and find that with heavy new physics at the TeV scale it can be possible given existing constraints on parameters"", 'Why do we want such an abrupt (also called strongly first-order) phase transition? Two reasons:', ""Reason 1. Because then you might be able to explain why there aren't equal amounts of matter and antimatter in the universe. This is called electroweak baryogenesis and requires something very cool called sphalerons. Which you can't have if the transition is too weak."", 'This article by @jonmbutterworth talks a bit about sphalerons. https://t.co/gKW2BHumVD', 'Reason 2. Because an abrupt transition is like boiling ‚Äì bubbles are formed that can make huge amounts of gravitational waves. Which then make up the stochastic gravitational wave background that the space-based GW observatory LISA will search for.', '@davidjamesweir has done some very cool simulations and movies of such bubbles https://t.co/h2ngUmzPWb', ""So it's experimentally testable, by an insanely cool experiment (@LISACommunity) that will have three spacecraft in a triangle with sides of 2.5 million km, orbiting the Sun in the same orbit as Earth, with a laser interferometer using of laser beams between these spacecraft."", ""It's also testable by the high luminosity LHC, by looking for pair production of Higgs bosons. If our scenario in https://t.co/QUYVoIPp1k is correct the cross section is modified in a specific way.""]",https://arxiv.org/abs/2103.14022,"A first-order Electroweak Phase Transition (EWPT) could explain the observed baryon-antibaryon asymmetry and its dynamics could yield a detectable gravitational wave signature, while the underlying physics would be within the reach of colliders. The Standard Model, however, predicts a crossover transition. We therefore study the EWPT in the Standard Model Effective Field Theory (SMEFT) including dimension-six operators. A first-order EWPT has previously been shown to be possible in the SMEFT. Phenomenology studies have focused on scenarios with a tree-level barrier between minima, which requires a negative Higgs quartic coupling and a new physics scale low enough to raise questions about the validity of the EFT approach. In this work we stress that a first-order EWPT is also possible when the barrier between minima is generated radiatively, the quartic coupling is positive, the scale of new physics is higher, and there is good agreement with experimental bounds. Our calculation is done in a consistent, gauge-invariant way, and we carefully analyze the scaling of parameters necessary to generate a barrier in the potential. We perform a global fit in the relevant parameter space and explicitly find the points with a first-order transition that agree with experimental data. We also briefly discuss the prospects for probing the allowed parameter space using di-Higgs production in colliders. ","A new perspective on the electroweak phase transition in the Standard
  Model Effective Field Theory"
49,1375351092628885504,314171681,Laura Baudis,"['New paper, in which we describe the energy calibration of GERDA in detail. An excellent resolution is essential to finding a monoenergetic signal, as expected for double-beta decay without neutrinos (with a half-life above 1.8 x 10^26 years in 76-Ge): <LINK> <LINK>', '@KokoFederico Thank you Federico!']",https://arxiv.org/abs/2103.13777,"The GERmanium Detector Array (GERDA) collaboration searched for neutrinoless double-$\beta$ decay in $^{76}$Ge with an array of about 40 high-purity isotopically-enriched germanium detectors. The experimental signature of the decay is a monoenergetic signal at Q$_{\beta\beta}$ = 2039.061(7)keV in the measured summed energy spectrum of the two emitted electrons. Both the energy reconstruction and resolution of the germanium detectors are crucial to separate a potential signal from various backgrounds, such as neutrino-accompanied double-$\beta$ decays allowed by the Standard Model. The energy resolution and stability were determined and monitored as a function of time using data from regular $^{228}$Th calibrations. In this work, we describe the calibration process and associated data analysis of the full GERDA dataset, tailored to preserve the excellent resolution of the individual germanium detectors when combining data over several years. ",Calibration of the GERDA experiment
50,1375349968307884033,1248533170774999040,Kostas Migkas,"['Our new cluster anisotropies paper is out today! We use 10 scaling relations to see if H0 looks isotropic locally or if there are strong bulk flows. We find an apparent 9% spatial change in H0, or equivalently, a 900 km/s bulk flow out to 500 Mpc... (1/n)\n\n<LINK>', ""We thoroughly check everything the community suggested (and everything we thought of) but couldn't identify any known systematic causing this. Monte Carlo simulations confirm our result. BUT WAIT! It doesn't mean there are no systematics... (2/n)"", '..It simply means that, for now, the cosmological explanation seems more likely. We will keep looking for new, unknown systematics. If we find any, who knows what else will they affect (remember, they are currently unknown). (3/4)', ""However, we can confidently say this time that it's definitely not X-ray absorption, Malmquist bias, cluster population inhomogeneities, the Zone of Avoidance gap, cluster properties correlation, selection cuts, and many more... I hope you find our work interesting!""]",https://arxiv.org/abs/2103.13904,"The hypothesis that the late Universe is isotropic and homogeneous is adopted by most cosmological studies. The expansion rate $H_0$ is thought to be spatially constant, while bulk flows are often presumed to be negligible compared to the Hubble expansion, even at local scales. Their effects on the redshift-distance conversion are hence usually ignored. Any deviation from this consensus can strongly bias the results of such studies and thus the importance of testing these assumptions cannot be understated. Scaling relations of galaxy clusters can be effectively used for that. In previous works, we observed strong anisotropies in cluster scaling relations, whose origins remain ambiguous. By measuring many different cluster properties, several scaling relations with different sensitivities can be built. Nearly independent tests of cosmic isotropy and bulk flows are then feasible. We make use of up to 570 clusters with measured properties at X-ray, microwave, and infrared wavelengths, to construct 10 different cluster scaling relations (five of them presented for the first time) and test the isotropy of the local Universe. Through rigorous tests, we ensure that our analysis is not prone to generally known systematic biases and X-ray absorption issues. By combining all available information, we detect an apparent $9\%$ spatial variation in the local $H_0$ between $(l,b)\sim ({280^{\circ}}^{+35^{\circ}}_{-35^{\circ}},{-15^{\circ}}^{+20^{\circ}}_{-20^{\circ}})$ and the rest of the sky. The observed anisotropy has a nearly dipole form. Using Monte Carlo simulations, we assess the statistical significance of the anisotropy to be $>5\sigma$. This result could also be attributed to a $\sim 900$ km/s bulk flow which seems to extend out to at least $\sim 500$ Mpc. These two effects are indistinguishable until more high$-z$ clusters are observed by future all-sky surveys, such as eROSITA. ","Cosmological implications of the anisotropy of ten galaxy cluster
  scaling relations"
51,1375127883115606016,1171392017185853440,Dr Sam Grafton-Waters,['My new paper studying the X-ray emission line regions in NGC 1068 with @ESA_XMM has been accepted! Please take a look at <LINK> üõ∞Ô∏è'],https://arxiv.org/abs/2103.13374,"We investigate the photoionised X-ray emission line regions (ELRs) within the Seyfert 2 galaxy NGC 1068, to determine if there are any characteristic changes between observations taken fourteen years apart. We compare XMM-Newton observations collected in 2000 and 2014, simultaneously fitting the reflection grating spectrometer (RGS) and EPIC-pn spectra of each epoch, for the first time, with the photoionisation model, PION, in SPEX. We find that four PION components are required to fit the majority of the emission lines in the spectra of NGC 1068, with $\log \xi=1-4$, $\log N_H>26 m^{-2}$, and $v_{out}=-100$ to $-600 kms^{-1}$ for both epochs. Comparing the ionisation state of the components shows almost no difference between the two epochs, while there is an increase in the total column density. To estimate the locations of these plasma regions from the central black hole we compare distance methods, excluding the variability arguments as there is no spectral change between observations. Although the methods are unable to constrain the distances, the locations are consistent with the narrow line region, with the possibility of the higher ionised component being part of the broad line region, but we cannot conclude this for certain. In addition, we find evidence for emission from collisionally ionised plasma, while previous analysis had suggested that collisional plasma emission was unlikely. However, although PION is unable to account for the FeXVII emission lines at 15 and 17 \AA, we do not rule out that photoexcitation is a valid processes to produce these lines too. NGC 1068 has not changed, both in terms of the observed spectra or from our modelling, within the 14 year time period between observations. This suggests that the ELRs are fairly static relative to the 14 year time frame between observations, or there is no dramatic change in the black hole variability. ","Photoionisation Modelling of the X-ray Emission Line Regions within the
  Seyfert 2 AGN NGC 1068"
52,1375126794974736394,882069031234060288,Peter Anderson,"['New arxiv paper describing PanGEA, an audio annotation toolkit for photorealistic 3D environments. \n\nPanGEA was designed to collect navigation instructions for the RxR dataset, but could be used for many related tasks.\n\nPaper: <LINK>\nCode: <LINK> <LINK> <LINK>', 'One observation from using PanGEA: collecting speech annotations can be 5-6x faster than collecting text. Worth thinking about for future data collection efforts!\n\nMore details on RxR and the two RxR challenges here: https://t.co/4W0nYn1Aje']",https://arxiv.org/abs/2103.12703,"PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks -- collecting navigation instructions and navigation instruction following -- and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support. ",PanGEA: The Panoramic Graph Environment Annotation Toolkit
53,1375114167309328384,1163911282963038208,Dr. Marion Campisi,['My collaborators and I developed a new method for identifying potential partisan gerrymanders.  In this paper we introduce our metric and use it to analyze several recent elections.  We hope it will be useful in the coming fight for fair redistricting.\n<LINK>'],https://arxiv.org/abs/2103.12856,"We introduce the Geography and Election Outcome (GEO) metric, a new method for identifying potential partisan gerrymanders. In contrast with currently popular methods, the GEO metric uses both geographic information about a districting plan as well as district-level partisan data, rather than just one or the other. We motivate and define the GEO metric, which gives a count (a non-negative integer) to each political party. The count indicates the number of previously lost districts which that party potentially could have had a 50% chance of winning, without risking any currently won districts, by making reasonable changes to the input map. We then analyze GEO metric scores for each party in several recent elections. We show that this relatively easy to understand and compute metric can encapsulate the results from more elaborate analyses. ",The Geography and Election Outcome (GEO) Metric: An Introduction
54,1375046823300173830,2510537210,Marco Le Moglie,"['Is holding elections during a pandemic safe?ü¶†üó≥Ô∏è\n\nIn a new working paper with @davidecipu we show that electoral campaigns approaching them are not\n\n<LINK>\nShort threadüëá', 'Elections are crucial for legitimating democratic institutions, especially during a pandemic when wide limitations to individual rights are enforced, and the possibility for all candidates to run a proper electoral campaign is essential for providing such legitimization', 'Yet, during a pandemic, the risk that electoral campaigns would enhance the spread of the disease exists and is substantive. For this reason, countries with elections planned in 2020 strongly debated on whether to organize or postpone elections', 'Despite the importance of such a public debate, empirical evidence on the impact of electoral campaigns on the diffusion of COVID19 is lacking. Thus, we try to assess that by exploiting a natural experiment taking place in Italy at the beginning of the second wave of the pandemic', 'On the same days as a national referendum to amend the constitution, regional elections took place in seven out of twenty regions, homogenously distributed across the country', 'This plausibly exogenous variation to the intensity of the electoral campaign allows us to compare the epidemiological outcomes in areas with and without regional elections before and after the official campaign‚Äôs start https://t.co/07MC7aOy0g', 'Google‚Äôs data on individual mobility show the absence\nof significant differences in mobility patterns between treated and control regions over the entire\nobservational window, thus the two groups of regions were experiencing a similar risk of spread https://t.co/gkGXOOAz6E', 'The electoral campaign led to a significant increase in new infections (7%), percentage of positive tests (15%), ordinary hospitalizations (24%), entries in intensive care units (5.3%), and deaths (0.6%) related to COVID-19. We also find a negative impact on testing (8.3%) https://t.co/baQ0K58qgX', 'Our results inform the debate on how managing elections at the time of COVID-19 in different critical manners. First, elections boost the spread of COVID-19, thus proving the necessity to enforce a strict safety protocol to minimize such impact', 'Second, a large portion of the epidemiological risk connected to elections is concentrated during the electoral campaign preceding the vote, when sanitary precautions are more difficult to enforce than at the polling station', 'Third, the electoral campaign also negatively impacted testing capacity, which in turn contributes to the spread of the disease because infected individuals with mild symptoms are not isolated promptly', 'Lastly, the electoral campaign contributed to the spread of the disease more in low-risk groups than among high-risk individuals. This possible because individuals belonging to risk groups are less likely to spend time outside their home and participate in large meetings', '@EmanueleBracco @davidecipu Yes, it is indeed.  Our study is agnostic on whether\nsuch reduction was due to the temporary uncertainty or intentional, but still, a very interesting (indirect) mechanism to further investigate']",https://arxiv.org/abs/2103.11753,"Elections are crucial for legitimating modern democracies, and giving all candidates the possibility to run a proper electoral campaign is necessary for elections' success in providing such legitimization. Yet, during a pandemic, the risk that electoral campaigns would enhance the spread of the disease exists and is substantive. In this work, we estimate the causal impact of electoral campaigns on the spread of COVID-19. Exploiting plausibly exogenous variation in the schedule of local elections across Italy, we show that the electoral campaign preceding this latter led to a significant worsening of the epidemiological situation related to the disease. Our results strongly highlight the importance of undertaking stringent measures along the entire electoral process to minimize its epidemiological consequences. ","To vote, or not to vote: on the epidemiological impact of electoral
  campaigns at the time of COVID-19"
55,1375002271595962368,347951646,Prashasti Singh,"['Our new paper uploaded on arxiv, exploring the influence of country journals indexed on the research output of a country. <LINK>\n@vivekks12 @ashrafsau @b_sujit1965']",http://arxiv.org/abs/2103.11100,"Scientific journals are currently the primary medium used by researchers to report their research findings. The transformation of print journals into e-journals has simplified the process of submissions to journals and also their access has become wider. Journals are usually published by commercial publishers, learned societies as well as Universities. There are different number of journals published from different countries. This paper attempts to explore whether the number of journals published from a country influences its research output. Scopus master journal list is analysed to identify journals published from 50 selected countries with significant volume of research output. The following relationship are analysed: (a) number of journals from a country and its research output, (b) growth rate of journals and research output for different countries, (c) global share of journals and research output for different countries, and (d) subject area-wise number of journals and research output in that subject area for different countries. Factors like journal packing density are also analysed. The results obtained show that for majority of the countries, the number of journals is positively correlated to their research output volume, though some other factors also play a role in growth of research output. The study at the end presents a discussion of the analytical outcomes and provides useful suggestions on policy perspectives for different countries. ","Influence of journals indexed from a country on its research output: An
  empirical investigation"
56,1374932943928369154,111063946,‰ªäÈáé Á¥ÄÈõÑÔºàNorio KonnoÔºâ,"['Our new paper ""Grover/Zeta correspondence based on the Konno-Sato theorem"" has been posted on arXiv.\n\n<LINK>\n\n#QuantumWalk\n#GroverWalk\n#ZetaFunction']",https://arxiv.org/abs/2103.12971,"Recently the Ihara zeta function for the finite graph was extended to infinite one by Clair and Chinta et al. In this paper, we obtain the same expressions by a different approach from their analytical method. Our new approach is to take a suitable limit of a sequence of finite graphs via the Konno-Sato theorem. This theorem is related to explicit formulas of characteristic polynomials for the evolution matrix of the Grover walk. The walk is one of the most well-investigated quantum walks which are quantum counterpart of classical random walks. We call the relation between the Grover walk and the zeta function based on the Konno-Sato theorem ""Grover/Zeta Correspondence"" here. ",Grover/Zeta Correspondence based on the Konno-Sato theorem
57,1374923106905698309,838066124,Jeff Filippini,"['New paper: Constraint on primordial gravitational waves from the first flight of SPIDER, a balloon-borne CMB telescope: <LINK> <LINK>', 'SPIDER is an ambitious instrument: six millimeter-wave telescopes containing &gt;2000 superconducting detectors, housed in a 1300 liter cryostat, hanging from a string at 35 km altitude.', 'Ballooning lets us observe in space-like conditions, largely free from the fluctuating glow of the atmosphere that plagues even the best Earth-bound observing sites.', 'This pristine vantage point brings challenges: strict mass and power constraints, and a horrifying inability to fiddle with your instrument after launch. It has to basically run itself, with minimal communication, its first time out.', 'Ballooning is also a key way we flight-qualify technologies for future space missions. And incredible training for new crops of space scientists and engineers.', ""This has been a long road, and I've been privileged to work with a fantastic team. Here's to further adventures with new instruments, on the ground and in the air!"", 'The view from the stratosphere over Antarctica: looking down with an optical camera on the gondola, and looking up with SPIDER itself. https://t.co/luxg73CBWM', ""While I'm here, I should advertise: we have postdoc positions open, in cosmology from balloons and ground! https://t.co/h64hof8zOv""]",https://arxiv.org/abs/2103.13334,"We present the first linear polarization measurements from the 2015 long-duration balloon flight of SPIDER, an experiment designed to map the polarization of the cosmic microwave background (CMB) on degree angular scales. Results from these measurements include maps and angular power spectra from observations of 4.8% of the sky at 95 and 150 GHz, along with the results of internal consistency tests on these data. While the polarized CMB anisotropy from primordial density perturbations is the dominant signal in this region of sky, Galactic dust emission is also detected with high significance; Galactic synchrotron emission is found to be negligible in the SPIDER bands. We employ two independent foreground-removal techniques in order to explore the sensitivity of the cosmological result to the assumptions made by each. The primary method uses a dust template derived from Planck data to subtract the Galactic dust signal. A second approach, employing a joint analysis of SPIDER and Planck data in the harmonic domain, assumes a modified-blackbody model for the spectral energy distribution of the dust with no constraint on its spatial morphology. Using a likelihood that jointly samples the template amplitude and $r$ parameter space, we derive 95% upper limits on the primordial tensor-to-scalar ratio from Feldman-Cousins and Bayesian constructions, finding $r<0.11$ and $r<0.19$, respectively. Roughly half the uncertainty in $r$ derives from noise associated with the template subtraction. New data at 280 GHz from SPIDER's second flight will complement the Planck polarization maps, providing powerful measurements of the polarized Galactic dust emission. ","A Constraint on Primordial $B$-Modes from the First Flight of the SPIDER
  Balloon-Borne Telescope"
58,1374774666565361664,1202374177,Akshay Agrawal,"['New paper: Minimum-Distortion Embedding\n\npaper: <LINK>\n\nWe introduce a framework (generalizing spectral, PCA, MDS, UMAP ...) that leads to new embeddings\n\ncode: <LINK>\n\nPyMDE lets you fit custom embeddings w/constraints on a GPU via @PyTorch <LINK>', 'Some example embeddings, all generated using PyMDE*\n\nPython notebooks: https://t.co/XAh2oblNxh\n\n*PyMDE is very young software, under active development. If you run into issues, have feedback on the API, ideas for new functionality, reach out on Github: https://t.co/Ezjjwmo0AL https://t.co/EvX4EpzUdn', 'The same framework can be used to generate high-dimensional embeddings, for use as features in downstream supervised learning tasks', 'It can also be used to draw graphs https://t.co/TVZGblEiBE', ""Here's the framework: in a minimum-distortion embedding (MDE) problem, you are given a finite set of items and are told that some are similar and possibly that others are not. The goal is to find an embedding (a vector per item) of minimum distortion, possibly under  constraints https://t.co/5B0OuGYeoM"", 'The distortion E(X) is measured by distortion functions of the Euclidean distance between embedding vectors. For example, in PCA, spectral embedding, Isomap, etc., the functions are quadratic, and the constraints are (1/n)X^TX = I', 'We develop a single algorithm, a projected L-BFGS method, for approximately solving a very wide variety of MDE problems. It works quite well. https://t.co/iJyYI6qJWF', 'This monograph was over a year in the making, co-authored w/Alnur Ali &amp; Stephen Boyd.\n\nMany people have given helpful feedback; special thanks to @hippopedoid L. Saul D. Chin @yifanlu @99decisiontrees @SaberaTalukder @mihail_eric @GuilleAngeris @ShaneBarratt @_Jonathan_Tuck_ ...', ""If you have any feedback, on the paper or software, please do reach out (over email, on Github, ...). I'd love to hear from you!"", ""@nibot Eventually, yes, but not yet; it has not yet been officially published (it's still a pre-publication manuscript). I will let you know when you can!"", 'Additionally, thanks to @leland_mcinnes for implementing and open-sourcing the amazing PyNNDescent library, which PyMDE uses to compute k-nearest neighbor graphs quickly', '@kvnlxm @jbohnslav @PyTorch Awesome! Please do keep in mind that the software is very young; if you find bugs or find something to be confusing, please do reach out', '@iandanforth Not a dumb question! You‚Äôre right, the embeddings are colored by external knowledge (not used to produce the embedding). Yes you can use this to cluster graphs (though we don‚Äôt have a nice API for that yet). For example Laplacian embedding is the first step in spectral clustering', '@hamed_mo7amed @PyTorch Thanks for the kind words! Typically you want dissimilar items to be not close, which is different from far. But the nice thing about the framework is that it lets you articulate whatever you want from the embedding (eg you could insist that dissimilar items be far)', '@iandanforth Yeah, ha, that really confused me while I was preparing the dataset. I found that many researchers (including myself I think) were connected to Einstein on the co-authorship graph', '@dribnet @PyTorch Cool embedding! A grid constraint is interesting; that one might be hard to handle, but let me think about it. There are two requirements on the allowable constraints (chapter 6 of the monograph). Creating your own constraint set requires a bit of math, but it can be done.', '@dribnet @PyTorch I‚Äôm happy to implement additional constraints and add them to the library, when it makes sense. Right now we have just a few, but many more are possible', '@dribnet @PyTorch @dribnet do you have a dataset for the twitter emojis that I can play with?', '@mflux @PyTorch I have not yet implemented adding new points (it‚Äôs on my TODO list), but we describe how we would do it in the paper. One way (the way I‚Äôm thinking I‚Äôll implement) will keep the original embedding fixed. So you won‚Äôt see any flipping/rotating.', '@matteodefelice @PyTorch Hi! Embedding has several applications. You can use it to better understand high dimensional data, via visualization. You can also use it to generate numerical feature vectors for abstract things like words, for use in supervised learning tasks. Check out part 3 of the monograph', '@giffmana @dribnet @permutans @PyTorch It‚Äôll definitely do something useful (it‚Äôs not the same as scaling). One way to do this would to put a barrier function for every pair of items. This is doable for small numbers of items']",https://arxiv.org/abs/2103.02559,"We consider the vector embedding problem. We are given a finite set of items, with the goal of assigning a representative vector to each one, possibly under some constraints (such as the collection of vectors being standardized, i.e., having zero mean and unit covariance). We are given data indicating that some pairs of items are similar, and optionally, some other pairs are dissimilar. For pairs of similar items, we want the corresponding vectors to be near each other, and for dissimilar pairs, we want the corresponding vectors to not be near each other, measured in Euclidean distance. We formalize this by introducing distortion functions, defined for some pairs of the items. Our goal is to choose an embedding that minimizes the total distortion, subject to the constraints. We call this the minimum-distortion embedding (MDE) problem. The MDE framework is simple but general. It includes a wide variety of embedding methods, such as spectral embedding, principal component analysis, multidimensional scaling, dimensionality reduction methods (like Isomap and UMAP), force-directed layout, and others. It also includes new embeddings, and provides principled ways of validating historical and new embeddings alike. We develop a projected quasi-Newton method that approximately solves MDE problems and scales to large data sets. We implement this method in PyMDE, an open-source Python package. In PyMDE, users can select from a library of distortion functions and constraints or specify custom ones, making it easy to rapidly experiment with different embeddings. Our software scales to data sets with millions of items and tens of millions of distortion functions. To demonstrate our method, we compute embeddings for several real-world data sets, including images, an academic co-author network, US county demographic data, and single-cell mRNA transcriptomes. ",Minimum-Distortion Embedding
59,1374650547844878340,2375479814,cataquark,"['Evidence for physics beyond the Standard Model at the @LHCbexperiment at CERN:\nLHCb finds a 3.1-standard deviation on Lepton Flavour Universality (LFU) violation in B-decays.\nPopular: <LINK>\nTechnical: <LINK>\nPaper: <LINK>', 'According to the Standard Model (SM), Electrons (e) and Muons (mu) are identical, except for their mass, so they should have the same interactions: Lepton Flavour Universality (LFU)', 'Several experiments (Babar, Belle, @LHCbexperiment) have measured the decay of a particle called B-meson in\ne and mu, and compare them in a magnitude called Rk.\nIf LFU is right, Rk should be 1: Rk=1.', 'Since some time, the measurements of Rk are smaller than 1, but the statistical significance was small, and it might be due to a statistical fluctuation.', 'With the new measurement:\nRk=0.8416, with uncertainties: (+0.044, -0.041)\nthe statistical significance is larger than 3-sigma, which means that there is a 0.1% probability that the data is\ncompatible with the SM.', 'In particle physics, 3-sigma is the standard to claim ""evidence"" for the effect, meaning that there is a high probability for the effect to be real, and not a statistical fluctuation of a miss-measurement.\nA ""discovery"" (total confirmation) would need a 5-sigma significance.', 'So now, we have ""evidence"" that Lepton Flavour Universality is violated, and its explanation would need new physics: Physics Beyond the Standard Model.', 'On  the other side, theoretical physicists have been busy searching for possible explanations of these deviations, among others:\nhttps://t.co/WQ0ujmRB8k\nhttps://t.co/DuVrRbH22O']",https://arxiv.org/abs/2103.11769,"The Standard Model of particle physics currently provides our best description of fundamental particles and their interactions. The theory predicts that the different charged leptons, the electron, muon and tau, have identical electroweak interaction strengths. Previous measurements have shown a wide range of particle decays are consistent with this principle of lepton universality. This article presents evidence for the breaking of lepton universality in beauty-quark decays, with a significance of 3.1 standard deviations, based on proton-proton collision data collected with the LHCb detector at CERN's Large Hadron Collider. The measurements are of processes in which a beauty meson transforms into a strange meson with the emission of either an electron and a positron, or a muon and an antimuon. If confirmed by future measurements, this violation of lepton universality would imply physics beyond the Standard Model, such as a new fundamental interaction between quarks and leptons. ",Test of lepton universality in beauty-quark decays
60,1374626201495601155,2778729792,Saquib Sarfraz,"['Our new #CVPR2021 paper on unsupervised action action segmentation. SoTA on 5 benchmarks (significant improvements).\nJoint with work with @NailaMurray (FAIR) #FaceBookAI and others @AliDiba67 @vivoutlaw  #MIT  #KIT #ETHZURICH #Daimler #MFGTSS\nPaper/code: <LINK> <LINK>', 'no training (main idea: a hierarchical temporal clustering approach). Fast- can segment a video (~2K frames) in 0.16 seconds. An Interesting find: simple clustering (kmeans) or as rudimentary as an equal split of frames is often very competitive with SoTA trainable methods.']",https://arxiv.org/abs/2103.11264,"Action segmentation refers to inferring boundaries of semantically consistent visual concepts in videos and is an important requirement for many video understanding tasks. For this and other video understanding tasks, supervised approaches have achieved encouraging performance but require a high volume of detailed frame-level annotations. We present a fully automatic and unsupervised approach for segmenting actions in a video that does not require any training. Our proposal is an effective temporally-weighted hierarchical clustering algorithm that can group semantically consistent frames of the video. Our main finding is that representing a video with a 1-nearest neighbor graph by taking into account the time progression is sufficient to form semantically and temporally consistent clusters of frames where each cluster may represent some action in the video. Additionally, we establish strong unsupervised baselines for action segmentation and show significant performance improvements over published unsupervised methods on five challenging action segmentation datasets. Our code is available at this https URL ","Temporally-Weighted Hierarchical Clustering for Unsupervised Action
  Segmentation"
61,1374622622479224837,46153507,dr. Jordy Davelaar,"['üö® new paper alert üö® \n\nWe study the physical properties of four-dimensional, string-theoretical, horizonless ""fuzzball"" geometries by imaging their shadows and comparing them to black holes.\n\narxiv: <LINK>\n\nPaper lead by: Fabio Bacchini and Daniel R. Mayerson <LINK>', 'Their microstructure traps light rays straying near the would-be horizon on long-lived, highly redshifted chaotic orbits. In fuzzballs sufficiently near the scaling limit this creates a shadow much like that of a black hole while avoiding the paradoxes associated with black holes https://t.co/0vEZ4XNbkH', 'Observations of the shadow size and residual glow can potentially discriminate between fuzzballs away from the scaling limit and alternative models of black compact objects.', 'Paper by: Fabio Bacchini, Daniel R. Mayerson, Bart Ripperda (@BartRipperda), Jordy Davelaar (@jordydavelaar), H√©ctor Olivares, Thomas Hertog, Bert Vercnocke']",https://arxiv.org/abs/2103.12075,"We study the physical properties of four-dimensional, string-theoretical, horizonless ""fuzzball"" geometries by imaging their shadows. Their microstructure traps light rays straying near the would-be horizon on long-lived, highly redshifted chaotic orbits. In fuzzballs sufficiently near the scaling limit this creates a shadow much like that of a black hole, while avoiding the paradoxes associated with an event horizon. Observations of the shadow size and residual glow can potentially discriminate between fuzzballs away from the scaling limit and alternative models of black compact objects. ",Fuzzball Shadows: Emergent Horizons from Microstructure
62,1374608617429635077,1045646230158626816,Sarthak Yadav,['Just released a new sound event dataset: <LINK>\n\nYou can find the paper at <LINK>\n\nAnd the official code release at <LINK>\n\n@MaryEllenFoster @GlasgowCS'],https://arxiv.org/abs/2103.12306,"Most of the existing isolated sound event datasets comprise a small number of sound event classes, usually 10 to 15, restricted to a small domain, such as domestic and urban sound events. In this work, we introduce GISE-51, a dataset spanning 51 isolated sound events belonging to a broad domain of event types. We also release GISE-51-Mixtures, a dataset of 5-second soundscapes with hard-labelled event boundaries synthesized from GISE-51 isolated sound events. We conduct baseline sound event recognition (SER) experiments on the GISE-51-Mixtures dataset, benchmarking prominent convolutional neural networks, and models trained with the dataset demonstrate strong transfer learning performance on existing audio recognition benchmarks. Together, GISE-51 and GISE-51-Mixtures attempt to address some of the shortcomings of recent sound event datasets, providing an open, reproducible benchmark for future research along with the freedom to adapt the included isolated sound events for domain-specific applications. ",GISE-51: A scalable isolated sound events dataset
63,1374531967274864643,247758315,Noushath,['I have just released my new paper which I was working on and off for last 3 years. Creation of comprehensive unconstrained Tamil Handwritten Character Database [uTHCD]\n\narXiv id: <LINK> [preprint] \n\n@faizalhaja \n#DeepLearning #dataset #CNN #MachineLearning <LINK>'],https://arxiv.org/abs/2103.07676,"Handwritten character recognition is a challenging research in the field of document image analysis over many decades due to numerous reasons such as large writing styles variation, inherent noise in data, expansive applications it offers, non-availability of benchmark databases etc. There has been considerable work reported in literature about creation of the database for several Indic scripts but the Tamil script is still in its infancy as it has been reported only in one database [5]. In this paper, we present the work done in the creation of an exhaustive and large unconstrained Tamil Handwritten Character Database (uTHCD). Database consists of around 91000 samples with nearly 600 samples in each of 156 classes. The database is a unified collection of both online and offline samples. Offline samples were collected by asking volunteers to write samples on a form inside a specified grid. For online samples, we made the volunteers write in a similar grid using a digital writing pad. The samples collected encompass a vast variety of writing styles, inherent distortions arising from offline scanning process viz stroke discontinuity, variable thickness of stroke, distortion etc. Algorithms which are resilient to such data can be practically deployed for real time applications. The samples were generated from around 650 native Tamil volunteers including school going kids, homemakers, university students and faculty. The isolated character database will be made publicly available as raw images and Hierarchical Data File (HDF) compressed file. With this database, we expect to set a new benchmark in Tamil handwritten character recognition and serve as a launchpad for many avenues in document image analysis domain. Paper also presents an ideal experimental set-up using the database on convolutional neural networks (CNN) with a baseline accuracy of 88% on test data. ",uTHCD: A New Benchmarking for Tamil Handwritten OCR
64,1374382752385040397,198239722,Christopher Chen,"[""A preprint of our new paper, led by Jean Perez, on the applicability of the Taylor hypothesis to #ParkerSolarProbe's first four encounters is now available: <LINK>""]",https://arxiv.org/abs/2103.12022,"We investigate the validity of Taylor's Hypothesis (TH) in the analysis of Alfv\'enic fluctuations of velocity and magnetic fields in solar wind streams measured by Parker Solar Probe (PSP)~during the first four encounters. We use PSP velocity and magnetic field measurements from 24 h intervals selected from each of the first four encounters. The applicability of TH is investigated by measuring the parameter $\epsilon=\delta u_0/\sqrt{2}V_\perp$, which quantifies the ratio between the typical speed of large-scale fluctuations, $\delta u_0$, and the local perpendicular PSP speed in the solar wind frame, $V_\perp$. TH is expected to be applicable for $\epsilon\lesssim0.5$ when PSP is moving nearly perpendicular to the local magnetic field in the plasma frame, irrespective of the Alfv\'en Mach number $M_{\rm A}=V_{\rm SW}/V_{\rm A}$, where $V_{\rm SW}$ and $V_{\rm A}$ are the local solar wind and Alfv\'en speed, respectively. For the four selected solar wind intervals we find that between 10% to 60% of the time the parameter $\epsilon$ is below 0.2 when the sampling angle (between the spacecraft velocity in the plasma frame and the local magnetic field) is greater than $30^\circ$. For angles above $30^\circ$, the sampling direction is sufficiently oblique to allow one to reconstruct the reduced energy spectrum $E(k_\perp)$ of magnetic fluctuations from its measured frequency spectra. The spectral indices determined from power-law fits of the measured frequency spectrum accurately represent the spectral indices associated with the underlying spatial spectrum of turbulent fluctuations in the plasma frame. Aside from a frequency broadening due to large-scale sweeping that requires careful consideration, the spatial spectrum can be recovered to obtain the distribution of fluctuation's energy among scales in the plasma frame. ",Applicability of Taylor's Hypothesis during Parker Solar Probe perihelia
65,1374375561124966405,731352704182669317,Fabian Jankowski,"['We have a new paper on the arXiv! In 2019 we got lucky to observe the pulsar J1452-6036 ~3 hours after a glitch in its rotation. We investigated its radiative parameters around the glitch and constrained any changes, including a slight radio flux increase. <LINK>']",https://arxiv.org/abs/2103.09869,"We present high-sensitivity, wide-band observations (704 to 4032 MHz) of the young to middle-aged radio pulsar J1452-6036, taken at multiple epochs before and, serendipitously, shortly after a glitch occurred on 2019 April 27. We obtained the data using the new ultra-wide-bandwidth low-frequency (UWL) receiver at the Parkes radio telescope, and we used Markov Chain Monte Carlo techniques to estimate the glitch parameters robustly. The data from our third observing session began 3 h after the best-fitting glitch epoch, which we constrained to within 4 min. The glitch was of intermediate size, with a fractional change in spin frequency of $270.52(3) \times 10^{-9}$. We measured no significant change in spin-down rate and found no evidence for rapidly-decaying glitch components. We systematically investigated whether the glitch affected any radiative parameters of the pulsar and found that its spectral index, spectral shape, polarisation fractions, and rotation measure stayed constant within the uncertainties across the glitch epoch. However, its pulse-averaged flux density increased significantly by about 10 per cent in the post-glitch epoch and decayed slightly before our fourth observation a day later. We show that the increase was unlikely caused by calibration issues. While we cannot exclude that it was due to refractive interstellar scintillation, it is hard to reconcile with refractive effects. The chance coincidence probability of the flux density increase and the glitch event is low. Finally, we present the evolution of the pulsar's pulse profile across the band. The morphology of its polarimetric pulse profile stayed unaffected to a precision of better than 2 per cent. ","Constraints on wide-band radiative changes after a glitch in PSR
  J1452-6036"
66,1374340027577171973,1269670536,Lukasz Olejnik,"['Exciting @LHCbExperiment results hint at potential New Physics. ""the breaking of lepton universality in beauty-quark decays"". Writeup: <LINK> Paper: <LINK>, exciting not only because I worked at LHCb! <LINK>']",https://arxiv.org/abs/2103.11769,"The Standard Model of particle physics currently provides our best description of fundamental particles and their interactions. The theory predicts that the different charged leptons, the electron, muon and tau, have identical electroweak interaction strengths. Previous measurements have shown a wide range of particle decays are consistent with this principle of lepton universality. This article presents evidence for the breaking of lepton universality in beauty-quark decays, with a significance of 3.1 standard deviations, based on proton-proton collision data collected with the LHCb detector at CERN's Large Hadron Collider. The measurements are of processes in which a beauty meson transforms into a strange meson with the emission of either an electron and a positron, or a muon and an antimuon. If confirmed by future measurements, this violation of lepton universality would imply physics beyond the Standard Model, such as a new fundamental interaction between quarks and leptons. ",Test of lepton universality in beauty-quark decays
67,1374167785337716737,1071223986632171521,Cynthia Rudin,"['New review paper: ""Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges""  <LINK>']",https://arxiv.org/abs/2103.11251,"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the ""Rashomon set"" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning. ","Interpretable Machine Learning: Fundamental Principles and 10 Grand
  Challenges"
68,1374114039115223040,1170610735753441280,Mario Marino,"['As new advance in the field of deep learning for mortality forecasting, me @Susanna65406733and and @NigriAndrea exploit a neural approach to improve the Lee-Carter mortality density forecasts.  The paper is available at:\n\n<LINK>\n\nComments/suggestions are welcome!']",https://arxiv.org/abs/2103.10535?context=stat,"Undoubtedly, several countries worldwide endure to experience a continuous increase in life expectancy, extending the challenges of life actuaries and demographers in forecasting mortality. Although several stochastic mortality models have been proposed in past literature, the mortality forecasting research remains a crucial task. Recently, various research works encourage the adequacy of deep learning models to extrapolate suitable pattern within mortality data. Such a learning models allow to achieve accurate point predictions, albeit also uncertainty measures are necessary to support both model estimates reliability and risk evaluations. To the best of our knowledge, machine and deep learning literature in mortality forecasting lack for studies about uncertainty estimation. As new advance in mortality forecasting, we formalizes the deep Neural Networks integration within the Lee-Carter framework, posing a first bridge between the deep learning and the mortality density forecasts. We test our model proposal in a numerical application considering three representative countries worldwide and both genders, scrutinizing two different fitting periods. Exploiting the meaning of both biological reasonableness and plausibility of forecasts, as well as performance metrics, our findings confirm the suitability of deep learning models to improve the predictive capacity of the Lee-Carter model, providing more reliable mortality boundaries also on the long-run. ","Deepening Lee-Carter for longevity projections with uncertainty
  estimation"
69,1374109341482168323,1214215979200172033,Leonard Wong,['New paper with my PhD student Steven Campbell on arXiv now:\n<LINK>\n\nWe study a portfolio optimization problem in the context of stochastic portfolio theory. The main idea is to exploit the stability of the capital distribution curve. <LINK>'],https://arxiv.org/abs/2103.10925,"In this paper we develop a concrete and fully implementable approach to the optimization of functionally generated portfolios in stochastic portfolio theory. The main idea is to optimize over a family of rank-based portfolios parameterized by an exponentially concave function on the unit interval. This choice can be motivated by the long term stability of the capital distribution observed in large equity markets, and allows us to circumvent the curse of dimensionality. The resulting optimization problem, which is convex, allows for various regularizations and constraints to be imposed on the generating function. We prove an existence and uniqueness result for our optimization problem and provide a stability estimate in terms of a Wasserstein metric of the input measure. Then, we formulate a discretization which can be implemented numerically using available software packages and analyze its approximation error. Finally, we present empirical examples using CRSP data from the US stock market, including the performance of the portfolios allowing for dividends, defaults, and transaction costs. ",Functional portfolio optimization in stochastic portfolio theory
70,1374066660617850882,3094610676,Pranav Rajpurkar,"['Key deployment consideration for AI + medicine \n\nCan we predict when AI models for medical image diagnosis will make incorrect predictions?\n\nNew paper (#CheXbreak): \n<LINK>\n\nw/ @emma_ychen, Andy Kim, @RayanKrishnan, @JinLong19844963 @AndrewYNg @StanfordAILab\n\n1/n <LINK>', 'As background, a major barrier to the adoption of chest x-ray models to clinical settings is a\nlack of understanding about their failures.\n\nModels may misclassify on images with certain radiographic findings and patients with certain clinical features.\n\nCan we detect this?\n\n2/n', 'In this work, we:\n\n1. Identify subgroups that are more likely to be misclassified by 10 top performing chest x-ray interpretation models.\n\n2. Build misclassification identifiers from image+clinical data, &amp;\n\n3. Improve model performances using  misclassification identifiers. \n\n3/n https://t.co/vy5lWN39vh', 'We found characteristics of patient subgroups that are significant predictors of misclassification. \n\nHere, older patients and the absence of lateral views were significant clinical features associated with misclassification.\n\n4/n https://t.co/YGpPjlItnp', 'We also identified radiological findings that influence the likelihood of misclassifiation by these models.\n\nThe presence of Support Devices was a widely significant feature for predicting misclassification across tasks.\n\n5/n https://t.co/vGmJsFKW38', 'We also investigated whether we could predict the likelihood of misclassification of a chest x-ray model at inference time.\n\nWe incorporated clinical features and all model outputs of the multi-label models to build misclassification identifiers. \n\n6/n', 'We found that we could produce misclassification identifiers with high discrimination performance.\n\nOn Pleural Effusion, Consolidation and Edema, we found a mean AUC of ~0.9.\n\n7/n https://t.co/lbBre0IVf5', 'We also showed that the top performing #CheXpert models can be further improved by selectively flipping wrong predictions identified by our misclassification identifier.\n\nWe observe F1 improvement on the prediction of Consolidation and Edema.\n\n8/n https://t.co/Ef2o6VWcbS', 'We investigated not only one model, but 10 models by different teams in attempt to derive insights across chest x-ray models.\n\nFramework useful for many tasks in medical imaging where data unavailable during training time may be used to improve model deployment.\n\n9/n', 'So much fun working with the talented team of first authors @emma_ychen, Andy Kim, @RayanKrishnan -- you are amazing!\n\nThank you to @JinLong19844963 for being a fantastic team member &amp; statistical insights!\n\nRead more here: https://t.co/AwKeF3pKD5\n\n10/10', '@_amirbar @StanfordAILab @emma_ychen @RayanKrishnan @JinLong19844963 @AndrewYNg We specify this in the paper, but the idea is that there is different information available at deployment time than at training time.']",https://arxiv.org/abs/2103.09957,"A major obstacle to the integration of deep learning models for chest x-ray interpretation into clinical settings is the lack of understanding of their failure modes. In this work, we first investigate whether there are patient subgroups that chest x-ray models are likely to misclassify. We find that patient age and the radiographic finding of lung lesion, pneumothorax or support devices are statistically relevant features for predicting misclassification for some chest x-ray models. Second, we develop misclassification predictors on chest x-ray models using their outputs and clinical features. We find that our best performing misclassification identifier achieves an AUROC close to 0.9 for most diseases. Third, employing our misclassification identifiers, we develop a corrective algorithm to selectively flip model predictions that have high likelihood of misclassification at inference time. We observe F1 improvement on the prediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003, [95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and high-performing chest x-ray models, we are able to derive insights across model architectures and offer a generalizable framework applicable to other medical imaging tasks. ","CheXbreak: Misclassification Identification for Deep Learning Models
  Interpreting Chest X-rays"
71,1374061711108538372,1347497620260999168,Cormac O'Malley,"['Renewables decrease the system frequency stability, so new containment tools are needed!\n\nCheck out our new paper to see how under-freq load shedding can reduce annual UK costs and emissions by up to ¬£500 mil and 4 MtCO2/yr!  #LowInertia\n\n<LINK> <LINK>']",https://arxiv.org/abs/2103.06616,"The reduced inertia levels in low-carbon power grids necessitate explicit constraints to limit frequency's nadir and rate of change during scheduling. This can result in significant curtailment of renewable energy due to the minimum generation of thermal plants that are needed to provide frequency response (FR) and inertia. Additional consideration of fast FR, a dynamically reduced largest loss and under frequency load shedding (UFLS) allows frequency security to be achieved more cost effectively. This paper derives a novel nadir constraint from the swing equation that, for the first time, provides a framework for the optimal comparison of all these services. We demonstrate that this constraint can be accurately and conservatively approximated for moderate UFLS levels with a second order cone, resulting in highly tractable convex problems. Case studies performed on a Great Britain 2030 system demonstrate that UFLS as an option to contain single plant outages can reduce annual operational costs by up to {\pounds}559m, 52% of frequency security costs. The sensitivity of this value to wind penetration, abundance of alternative frequency services, UFLS amount and cost is explored. ","Probabilistic Scheduling of UFLS to Secure Credible Contingencies in Low
  Inertia Systems"
72,1373984522820194304,15612654,Alan Stern,"['#PI_Daily Even w/all PI‚Äôs have to do, they find time to publish new scientific results from their missions! My just accepted @NewHorizons2015 paper on KBO Arrokoth‚Äôs bright neck (the ring-like feature at the joint between it‚Äôs 2 lobes) is now available at  <LINK> <LINK>']",https://arxiv.org/abs/2103.10780,"One of the most striking and curious features of the small Kuiper Belt Object (KB), Arrokoth, explored by New Horizons, is the bright, annular neck it exhibits at the junction between its two lobes. Here we summarize past reported findings regarding the properties of this feature and report new findings regarding its dimensions, reflectivity and color, shape profile, and its lack of identifiable craters. We conclude by enumerating possible origin scenarios for this unusual feature. New results include a new measurement of the observed neck area of 8+/-1.5 km2, a total neck surface area of 32 km2, a 12.5:1 ratio of neck circumference to height, a normal reflectance histogram of the observed neck, and the fact that no significant (i.e., >2 sigma) color units were identified, meaning the neck's color is generally spatially uniform at the 1.5 km/pixel scale of the best color images. Although several origin hypotheses for the bright material in the neck are briefly discussed, none can be conclusively demonstrated to be the actual origin mechanism at this time; some future tests are identified. ","Some New Results and Perspectives Regarding the Kuiper Belt Object
  Arrokoth's Remarkable, Bright Neck"
73,1373957370108334081,1305487125488558081,Louise Breuval,"['Check out my new paper! Combining @ESAGaia parallaxes of Milky Way Cepheids with LMC and SMC distances from eclipsing binaries, I derived the effect of metallicity on the Period-Luminosity relation ‚≠êÔ∏èüõ∞\n\n<LINK> (Accepted in ApJ) \n\n@Obs_Paris @LesiaAstro <LINK>']",https://arxiv.org/abs/2103.10894,"The Cepheid Period-Luminosity (PL) relation is the key tool for measuring astronomical distances and for establishing the extragalactic distance scale. In particular, the local value of the Hubble constant ($H_0$) strongly depends on Cepheid distance measurements. The recent Gaia Data Releases and other parallax measurements from the Hubble Space Telescope (HST) already enabled to improve the accuracy of the slope ($\alpha$) and intercept ($\beta$) of the PL relation. However, the dependence of this law on metallicity is still largely debated. In this paper, we combine three samples of Cepheids in the Milky Way (MW), the Large Magellanic Cloud (LMC) and the Small Magellanic Cloud (SMC) in order to derive the metallicity term (hereafter $\gamma$) of the PL relation. The recent publication of extremely precise LMC and SMC distances based on late-type detached eclipsing binary systems (DEBs) provides a solid anchor for the Magellanic Clouds. In the MW, we adopt Cepheid parallaxes from the early third Gaia Data Release. We derive the metallicity effect in $V$, $I$, $J$, $H$, $K_S$, $W_{VI}$ and $W_{JK}$. In the $K_S$ band we report a metallicity effect of $-0.221 \pm 0.051$ mag/dex, the negative sign meaning that more metal-rich Cepheids are intrinsically brighter than their more metal-poor counterparts of the same pulsation period. ","The influence of metallicity on the Leavitt Law from geometrical
  distances of Milky Way and Magellanic Clouds Cepheids"
74,1373942001519972353,561899047,Aki Vehtari,"['New paper with Teemu S√§ilynoja and @paulbuerkner \n""Graphical Test for Discrete Uniformity and its Applications in Goodness of Fit Evaluation and Multiple Sample Comparison""\n<LINK> <LINK>', ""We've recently had papers like simulation based calibration and new Rhat paper with rank plots, which involve looking at the uniformity of rank statistics. Thinking more, we realized that histograms are suboptimal and ECDF envelopes we used were based on continuous uniformity."", 'The motivation was not uniformity testing for itself, but this paper belongs to a series of papers on improving diagnostics of diagnostics. The graphical aspect is also important to provide useful information about the shape of discrepancy away from the assumed uniformity.', ""Histograms 1) loose information by binning, 2) the choice of bins affect the result, 3) the confidence band doesn't take into account the dependency. ECDF and ECDF difference plots don't have these problems and thus would be better. https://t.co/KbT5VQAvNG"", ""The simultaneous confidence band for ECDF (and ECDF difference) doesn't have closed form solution and naive simulation based approach can be very slow. Aldor-Noiman et al proposed more efficient simulation based approach for continuous uniform case."", 'In SBC and MCMC rank plots, the rank statistics are discrete. The rank statistics are also equivalent to probability integral transformation (PIT) using ECDF. We present how we can construct the confidence bands that are correct both in continuous and discrete case. https://t.co/TmfkE5iBd1', 'We propose a new optimization based approach for simultaneous confidence band which is much more efficient than Aldor-Noiman simulation approach. We also extend the apporach to comparing rank statistics of multiple samples such as arising from different Markov chains. https://t.co/AbdCjVczxJ', ""The code is available at https://t.co/WzP6ltmbn4\n\nThis is Teemu's first paper and I'm very happy with the outcome!""]",https://arxiv.org/abs/2103.10522,"Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows. ","Graphical Test for Discrete Uniformity and its Applications in Goodness
  of Fit Evaluation and Multiple Sample Comparison"
75,1373899529871851523,1297174706622205955,Andrea Palermo,"['Our new paper is out today on the arXiv!\n\nWe found that, for a relativistic fluid at local equilibrium, polarization is not only induced by thermal vorticity, but also by the symmetric gradient of temperature, the thermal shear. \nCheck it out!\n<LINK>\n1/5', 'It is well known that spin couples to the vorticity of a relativistic fluid, which also has a striking experimental confirmation in the measurement of the global polarization of Lambda hyperon. At local equilibrium, thermal vorticity is present , but it is not alone...\n2/5', 'The symmetric derivative of the temperature is also non vanishing! This term is called thermal shear tensor. Using linear response theory we found that thermal shear (here \\xi) contributes to the polarization of fermions!\n3/5 https://t.co/Nlc8CBUiMl', 'This new contribution was neglected so far, and might help in solving the so called ""sign puzzle"" of the local Lambda polarization. \n4/5', 'Similar studies were also carried out by another group, only few days ago. \nhttps://t.co/g9Aptc6RKf\nhttps://t.co/WHPbVUohpk\n\nAll of these recent results, hint that we might be in the right direction...\n\n5/5']",https://arxiv.org/abs/2103.10917,"We show that spin polarization of a fermion in a relativistic fluid at local thermodynamic equilibrium can be generated by the symmetric derivative of the four-temperature vector, defined as thermal shear. As a consequence, besides vorticity, acceleration and temperature gradient, also the shear tensor contributes to the polarization of particles in a fluid. This contribution to the spin polarization vector, which is entirely non-dissipative, adds to the well known term proportional to thermal vorticity and may thus have important consequences for the solution of the local polarization puzzles observed in relativistic heavy ion collisions. ",Spin-thermal shear coupling in a relativistic fluid
76,1373801006056697858,972932719503081472,Akira Sone,['Our new paper with @quthermo_comp on quantum and classical ergotropy from relative entropies is now available on arXiv.\n<LINK>\nGeometric quantum mechanics could be a powerful tool to bridge the quantum and classical approach to 2nd law as a unified framework!'],https://arxiv.org/abs/2103.10850,"The quantum ergotropy quantifies the maximal amount of work that can be extracted from a quantum state without changing its entropy. Given that the ergotropy can be expressed as the difference of quantum and classical relative entropies of the quantum state with respect to the thermal state, we define the classical ergotropy, which quantifies how much work can be extracted from distributions that are inhomogeneous on the energy surfaces. A unified approach to treat both quantum as well as classical scenarios is provided by geometric quantum mechanics, for which we define the geometric relative entropy. The analysis is concluded with an application of the conceptual insight to conditional thermal states, and the correspondingly tightened maximum work theorem. ",Quantum and classical ergotropy from relative entropies
77,1373713881223196674,1180406227,"Elizabeth A. Barnes, PhD","['New paper by postdoc Dr. Antonios Mamalakis on benchmark data sets for evaluating XAI methods for climate applications! Whoo! This stuff is so fun.\n<LINK>', 'Adding first author @AntoniosMamala2 to the thread.']",https://arxiv.org/abs/2103.10005,"Despite the increasingly successful application of neural networks to many problems in the geosciences, their complex and nonlinear structure makes the interpretation of their predictions difficult, which limits model trust and does not allow scientists to gain physical insights about the problem at hand. Many different methods have been introduced in the emerging field of eXplainable Artificial Intelligence (XAI), which aim at attributing the network's prediction to specific features in the input domain. XAI methods are usually assessed by using benchmark datasets (like MNIST or ImageNet for image classification), or through deletion/insertion techniques. In either case, however, an objective, theoretically-derived ground truth for the attribution is lacking, making the assessment of XAI in many cases subjective. Also, benchmark datasets for problems in geosciences are rare. Here, we provide a framework, based on the use of additively separable functions, to generate attribution benchmark datasets for regression problems for which the ground truth of the attribution is known a priori. We generate a long benchmark dataset and train a fully-connected network to learn the underlying function that was used for simulation. We then compare estimated attribution heatmaps from different XAI methods to the ground truth in order to identify examples where specific XAI methods perform well or poorly. We believe that attribution benchmarks as the ones introduced herein are of great importance for further application of neural networks in the geosciences, and for accurate implementation of XAI methods, which will increase model trust and assist in discovering new science. ","Neural Network Attribution Methods for Problems in Geoscience: A Novel
  Synthetic Benchmark Dataset"
78,1372985207519735808,1727424900,Cory Hargus,"['Diffusion is a statistical consequence of random motion. What happens when random motion is *chiral*, e.g. preferring left turns over right turns? In a new paper we show how a non-dissipative ""odd diffusivity"" emerges, including in chiral active matter. <LINK>', 'Another team effort with coauthors @jmlepstein and Kranthi Mandadapu!']",https://arxiv.org/abs/2103.09958,"Diffusive transport is characterized by a diffusivity tensor which may, in general, contain both a symmetric and an antisymmetric component. Although the latter is often neglected, we derive Green-Kubo relations showing it to be a general characteristic of random motion breaking time-reversal and parity symmetries, as encountered in chiral active matter. In analogy with the odd viscosity appearing in chiral active fluids, we term this component the odd diffusivity. We show how odd diffusivity emerges in a chiral random walk model, and demonstrate the applicability of the Green-Kubo relations through molecular dynamics simulations of a passive tracer particle diffusing in a chiral active bath. ",Odd Diffusivity of Chiral Random Motion
79,1372940728079368201,923362183672680448,Emily Griffith,"['Check out my new paper, ‚ÄúThe Impact of Black Hole Formation on Population Averaged Supernova Yields,‚Äù written with Tuguldur Sukhbold, David Weinberg @jajohnson51 , @giganano9 , and Fiorenzo Vincenzo!\n\n<LINK> (1/5)', 'One of my favorite parts of this paper was constructing a fully exploding yield set based on Sukhbold et al. (2016) and examining how elemental yields depended on stellar mass and core properties. Each element is unique, and the total yields vary depending on which stars explode https://t.co/t8EvacUNUR', 'We examine the IMF-averaged CCSN yields resulting from a variety of black hole landscapes, such as the Z9.6+W18 and Z9.6+N20 landscapes from Sukhbold et al. (2016), a landscape where stars more massive than 40Msun form black holes, and a landscape where all stars explode. https://t.co/KXp8DTMjRH', 'We calculate predicted [X/Mg] abundances for each model and compare the theoretical CCSN yields with empirical CCSN yields. This comparison reveals what elements the models are overproducing (&gt;0) and underproducing(&lt;0, in the plot below). https://t.co/ragUl4cVW1', 'No model achieves across-the-board agreement with observations. In our paper, we discuss these discrepancies and propose changes to massive star evolution and explosion models that could improve or resolve them.', 'But there‚Äôs so much more! Check out the paper for continuous explosion landscapes, comparisons of abundances predicted by simplistic and complex landscapes, suggestions for element ratios that could constrain the level of black hole formation, and a discussion of the O/Mg problem']",https://arxiv.org/abs/2103.09837,"The landscape of black hole (BH) formation -- which massive stars explode as core-collapse supernovae (CCSN) and which implode to BHs -- profoundly affects the IMF-averaged nucleosynthetic yields of a stellar population. Building on the work of Sukhbold et al. (2016), we compute IMF-averaged yields at solar metallicity for a wide range of assumptions, including neutrino-driven engine models with extensive BH formation, models with a simple mass threshold for BH formation, and a model in which all stars from $8-120 \text{M}_{\odot}$ explode. For plausible choices, the overall yields of $\alpha$-elements span a factor of three, but changes in relative yields are more subtle, typically $0.05-0.2$ dex. For constraining the overall level of BH formation, ratios of C and N to O or Mg are promising diagnostics. For distinguishing complex, theoretically motivated landscapes from simple mass thresholds, abundance ratios involving Mn or Ni are promising because of their sensitivity to the core structure of the CCSN progenitors. We confirm previous findings of a substantial (factor $2.5-4$) discrepancy between predicted O/Mg yield ratios and observationally inferred values, implying that models either overproduce O or underproduce Mg. No landscape choice achieves across-the-board agreement with observed abundance ratios; the discrepancies offer empirical clues to aspects of massive star evolution or explosion physics still missing from the models. We find qualitatively similar results using the massive star yields of Limongi & Chieffi (2018). We provide tables of IMF-integrated yields for several landscape scenarios, and more flexible user-designed models can be implemented through the publicly available $\texttt{Versatile Integrator for Chemical Evolution}$ ($\texttt{VICE}$; this https URL). ","The Impact of Black Hole Formation on Population Averaged Supernova
  Yields"
80,1372935316236083200,1562913787,Nathan Moynihan,['New paper out today! <LINK>'],https://arxiv.org/abs/2103.10416,"We examine the double copy structure of anyons in gauge theory and gravity. Using on-shell amplitude techniques, we construct little group covariant spinor-helicity variables describing massive particles with spin, which together with locality and unitarity enables us to derive the long-range tree-level scattering amplitudes involving anyons. We discover that classical gauge theory anyon solutions double copy to their gravitational counterparts in a non-trivial manner. Interestingly, we show that the massless double copy captures the topological structure of curved spacetime in three dimensions by introducing a non-trivial mixing of the topological graviton and the dilaton. Finally, we show that the celebrated Aharonov-Bohm phase can be derived directly from the constructed on-shell amplitude and that it too enjoys a simple double copy to its gravitational counterpart. ",Anyons and the Double Copy
81,1372934162202644485,1053370661798924294,James Johnson,"['New paper day! We tackle the infamous question of what observables can be explained by conventional assumptions in chemical evolution models when realistic stellar migration is added. \n\nThread! \n<LINK>', ""We find that young, alpha-rich stars can be explained by migration-induced variability in the Type Ia supernova rate. In that sense, they're not alpha-rich stars - they're iron-poor stars! Check out figures 8 and 13 (13a below) https://t.co/euI0Lu0L3k"", ""We find that evidence for a recent starburst in the Milky Way is in tension with the observed age-[alpha/Fe] relation, predicting an increase for young stars which simply isn't seen in the data. Check out figure 13b (below) https://t.co/yFOttNiXLn"", 'Weirdly enough, we find the *opposite* for the age-metallicity relation within the same data set. Recent starburst lowers characteristic ages at ~solar and sub-solar metallicity, and better reproduces the C-shaped nature of the trend. Check out figures 17 and 18 (17 below) https://t.co/9BnUrbfuuz', 'We find that these models do *not* reproduce the infamous [alpha/Fe] dichotomy. They over-predict the frequency of intermediate alpha stars, suggesting more dramatic evolutionary pathways are required. Check out figures 7 and 12 (12 below) https://t.co/6n4TLntpaE', ""Though agreement isn't perfect, the model can account for the changing shapes of the metallicity distribution with radius in the Milky Way disc. Off the midplane it accounts for some but not all of the changes. Check out figures 10 and 11 (10 below) https://t.co/lBoFZYWxxA"", 'Special kudos to collaborators David Weinberg, Fiorenzo Vincenzo, @galaxyhistorian, @sloebman, Alyson Brooks, Tom Quinn, Charlotte Christensen, and @EmilyJoGriffith!']",https://arxiv.org/abs/2103.09838,"We develop a hybrid model of galactic chemical evolution that combines a multi-ring computation of chemical enrichment with a prescription for stellar migration and the vertical distribution of stellar populations informed by a cosmological hydrodynamic disc galaxy simulation. Our fiducial model adopts empirically motivated forms of the star formation law and star formation history, with a gradient in outflow mass loading tuned to reproduce the observed metallicity gradient. With this approach, the model reproduces many of the striking qualitative features of the Milky Way disc's abundance structure: (i) the dependence of the [O/Fe]-[Fe/H] distribution on radius $R_\text{gal}$ and midplane distance $|z|$; (ii) the changing shapes of the [O/H] and [Fe/H] distributions with $R_\text{gal}$ and $|z|$; (iii) a broad distribution of [O/Fe] at sub-solar metallicity and changes in the [O/Fe] distribution with $R_\text{gal}$, $|z|$, and [Fe/H]; (iv) a tight correlation between [O/Fe] and stellar age for [O/Fe] $>$ 0.1; (v) a population of young and intermediate-age $\alpha$-enhanced stars caused by migration-induced variability in the Type Ia supernova rate; (vi) non-monotonic age-[O/H] and age-[Fe/H] relations, with large scatter and a median age of $\sim$4 Gyr near solar metallicity. Observationally motivated models with an enhanced star formation rate $\sim$2 Gyr ago improve agreement with the observed age-[Fe/H] and age-[O/H] relations, but worsen agreement with the observed age-[O/Fe] relation. None of our models predict an [O/Fe] distribution with the distinct bimodality seen in the observations, suggesting that more dramatic evolutionary pathways are required. All code and tables used for our models are publicly available through the Versatile Integrator for Chemical Evolution (VICE; this https URL). ","Stellar Migration and Chemical Enrichment in the Milky Way Disc: A
  Hybrid Model"
82,1372922793382322179,140691162,Dr. Wasikul Islam,"['Check out our new Phenomenology paper ""Model-independent searches for new physics in multi-body invariant masses"" : <LINK> .\n\nHappy to be part of some experimental explorations of the same on behalf of ATLAS Experiment at CERN.  :)', '@SaschaCaron Thanks ! Will check.']",https://arxiv.org/abs/2103.10217,"Model-independent searches for physics beyond the Standard Model typically focus on invariant masses of two objects (jets, leptons or photons). In this study we explore opportunities for similar model-agnostic searches in multi-body invariant masses. In particular, we focus on the situations when new physics can be observed in a model-independent way in three- and four-body invariant masses of jets and leptons. Such searches may have good prospects in finding new physics in the situations when two-body invariant masses, that have been extensively explored at collider experiments in the past, cannot provide sufficient signatures for experimental observations. ","Model-independent searches for new physics in multi-body invariant
  masses"
83,1372601871479799809,201811433,Ameet Talwalkar,['What started as a series of brainstorming sessions to keep my group connected during the pandemic has resulted in a new survey paper on interpretable ML that highlights the need to ground the field in real problems and to quantifiably measure progress: <LINK> <LINK>'],https://arxiv.org/abs/2103.06254,"Despite increasing interest in the field of Interpretable Machine Learning (IML), a significant gap persists between the technical objectives targeted by researchers' methods and the high-level goals of consumers' use cases. In this work, we synthesize foundational work on IML methods and evaluation into an actionable taxonomy. This taxonomy serves as a tool to conceptualize the gap between researchers and consumers, illustrated by the lack of connections between its methods and use cases components. It also provides the foundation from which we describe a three-step workflow to better enable researchers and consumers to work together to discover what types of methods are useful for what use cases. Eventually, by building on the results generated from this workflow, a more complete version of the taxonomy will increasingly allow consumers to find relevant methods for their target use cases and researchers to identify applicable use cases for their proposed methods. ",Interpretable Machine Learning: Moving From Mythos to Diagnostics
84,1372579261186416640,1216405642371403777,William Thompson,"['Excited to announce my first paper was accepted! <LINK>\nWe show a new way to process direct images that leads to a nice improvement close to stars and may help in the search for new planets! Thread: <LINK>', '1/ This has been an interesting journey over the past couple of years that started with a nagging feeling from Christian and I that there was a more direct way to optimize planet SNR compared to tuning LOCI.', ""2/ We ultimately found a simple equation for planet SNR given a set of images that has a single, global maximum. Optimizing this directly means we don't need to reject nearby reference images like we do in LOCI."", '3/ This also lets us optimize whole chunks of a sequence simultaneously. It turns out that optimizing the SNR of planets in a whole sequence produces a better image than processing them one at a time.', '4/ Combined, this can make a big difference close to the star! The improvement over our implementation of LOCI rises to ‚âà5√ó at close separations, and we see robust performance across instruments and datasets. https://t.co/EgI9yL2f6o', '5/ The approach is actually quite straightforward. I think much of the benefit can be realized in existing LOCI pipelines with only a few hours of work.', ""6/ For KLIP/PCA/SVD on the other hand, it's still an open question about the best way to combine them. We give a few thoughts on this in the discussion."", ""7/ We wrote our new pipeline in #julialang which was a really great experience, letting experiment on big sequences with different approaches. The pipeline isn't open source quite yet, but please get in touch if you have any data that might benefit!"", '@MoseGiordano Thanks, I‚Äôm already lurking there! Our group has been working on a number projects using Julia I‚Äôm hoping to share soon.']",https://arxiv.org/abs/2103.09252,"Direct imaging of exoplanets is usually limited by quasi-static speckles. These uncorrected aberrations in a star's point spread function (PSF) obscure faint companions and limit the sensitivity of high-contrast imaging instruments. Most current approaches to processing differential imaging sequences like angular differential imaging (ADI) and spectral differential imaging (SDI) produce a self-calibrating dataset that are combined in a linear least squares solution to minimize the noise. Due to temporal and chromatic evolution of a telescope's PSF, the best correlated reference images are usually the most contaminated by the planet, leading to self-subtraction and reducing the planet throughput. In this paper, we present an algorithm that directly optimizes the non-linear equation for planet signal to noise ratio (SNR). This new algorithm does not require us to reject adjacent reference images and optimally balances noise reduction with self-subtraction. We then show how this algorithm can be applied to multiple images simultaneously for a further reduction in correlated noise, directly maximizing the SNR of the final combined image. Finally, we demonstrate the technique on an illustrative sequence of HR8799 using the new Julia-based Signal to Noise Analysis Pipeline (SNAP). We show that SNR optimization can provide up to a $5\times$ improvement in contrast close to the star. Applicable to both new and archival data, this technique will allow for the detection of lower mass, and closer in companions, or achieve the same sensitivity with less telescope time. ",Improved Contrast in Images of Exoplanets using Direct SNR Optimization
85,1372576318374563842,21611239,Sean Carroll,"['All of reality can be modeled as just a vector looping around in a really-big Hilbert space. New semi-technical paper from me:\n<LINK>', 'Roughly speaking this is the perspective advocated by @ashmeetastro and me in our Mad-Dog Everettianism paper. The new one is more aimed at philosophers.\nhttps://t.co/R3SolxesK1', ""And yes, this is the paper that I wrote first, before putting any references in. Hopefully I didn't forget anyone. But I'm confident they will remind me if I did."", 'The paper leans into the idea that the fundamental nature of reality could be *radically* different from our familiar world of objects moving around in space and interacting with each other. All that stuff is a higher-level emergent approximation.', ""It's perfectly okay to be skeptical precisely because of that radical divergence between theory and experience. But it's also worth considering. Who says the fundamental nature of reality should be anything at all like our everyday experience?"", '@LizardOrman Actually only quantum systems can be represented that way, not classical ones. The question is whether the vector is fundamental, or is built on top of some ontology such as particles/fields propagating in space.', '@henry_maxfield I presume there are any number of equivalent ways to represent the structure of a quantum theory; in the Schr√∂dinger picture it\'s the Hamiltonian. You use a time parameter in the construction, but it\'s not necessarily ""preferred,"" as I discuss in the paper.', '@watsona4 Thanks!', '@henry_maxfield Yes, that would be how boosts work in the standard Hamiltonian formulation of QFT. But here it might only be an approximate symmetry.', '@henry_maxfield I think a physical realization should be thought of as an equivalence class of solutions, each corresponding to a choice of time variable.', '@henry_maxfield This might not be a Twitterable conversation. Maybe in person someday.']",https://arxiv.org/abs/2103.09780,"I defend the extremist position that the fundamental ontology of the world consists of a vector in Hilbert space evolving according to the Schr\""odinger equation. The laws of physics are determined solely by the energy eigenspectrum of the Hamiltonian. The structure of our observed world, including space and fields living within it, should arise as a higher-level emergent description. I sketch how this might come about, although much work remains to be done. ",Reality as a Vector in Hilbert Space
86,1372573311394275329,19163241,Sariel Har-Peled,"['A new paper with Stav Ashur @AshurStav trying to understand what tasks can be performed if the access to the input is implicit via oracle queries/input primitives, using only a sublinear number of such queries.\n<LINK>']",https://arxiv.org/abs/2103.09308,"We study colored coverage and clustering problems. Here, we are given a colored point set where the points are covered by (unknown) $k$ clusters, which are monochromatic (i.e., all the points covered by the same cluster, have the same color). The access to the colors of the points (or even the points themselves) is provided indirectly via various queries (such as nearest neighbor, or separation queries). We show that if the number of clusters is a constant, then one can correctly deduce the color of all the points (i.e., compute a monochromatic clustering of the points) using a polylogarithmic number of queries. We investigate several variants of this problem, including Undecided Linear Programming, covering of points by $k$ monochromatic balls, covering by $k$ triangles/simplices, and terrain simplification. For the later problem, we present the first near linear time approximation algorithm. While our approximation is slightly worse than previous work, this is the first algorithm to have subquadratic complexity if the terrain has ""small"" complexity. ","On Undecided LP, Clustering and Active Learning"
87,1372529688200024064,1077995761487568896,Jon Miller,['New paper day:\nHow do you keep a black hole corona and disk from talking to each other?  Throw a wind into the mix.  New insights on the extremely interesting black hole in Mrk 817 here:\n<LINK>\nThanks to collaborators and @NASANuSTAR. <LINK>'],https://arxiv.org/abs/2103.09789,"Accretion disks and coronae around massive black holes have been studied extensively, and they are known to be coupled. Over a period of 30 years, however, the X-ray (coronal) flux of Mrk 817 increased by a factor of 40 while its UV (disk) flux remained relatively steady. Recent high-cadence monitoring finds that the X-ray and UV continua in Mrk 817 are also decoupled on time scales of weeks and months. These findings could require mechanical beaming of the innermost accretion flow, and/or an absorber that shields the disk and/or broad line region (BLR) from the X-ray corona. Herein, we report on a 135 ks observation of Mrk 817 obtained with NuSTAR, complemented by simultaneous X-ray coverage via the Neil Gehrels Swift Observatory. The X-ray data strongly prefer a standard relativistic disk reflection model over plausible alternatives. Comparable fits with related models constrain the spin to lie in the range 0.5 < a < 1, and the viewing angle to lie between 10 deg. < theta < 22 deg. (including 1-sigma statistical errors and small systematic errors related to differences between the models). The spectra also reveal strong evidence of moderately ionized absorption, similar to but likely less extreme than obscuring events in NGC 5548 and NGC 3783. Archival Swift data suggest that the absorption may be variable. Particularly if the column density of this absorber is higher along the plane of the disk, it may intermittently mask or prevent coupling between the central engine, disk, and BLR in Mrk 817. ",The Inner Accretion Flow in the Resurgent Seyfert-1.2 AGN Mrk 817
88,1372512566291922949,1100129665409339393,Malte S. Kurz,"['New working paper üìö ""DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R"" with @PhilippBach_HH @VC31415 and Martin Spindler now available on #arXiv <LINK>.\n\n#rstats #DataScience #machinelearning <LINK>']",https://arxiv.org/abs/2103.09603,"The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, high-quality machine learning estimation and sample splitting. Estimation of nuisance components can be performed by various state-of-the-art machine learning methods that are available in the mlr3 ecosystem. DoubleML makes it possible to perform inference in a variety of causal models, including partially linear and interactive regression models and their extensions to instrumental variable estimation. The object-oriented implementation of DoubleML enables a high flexibility for the model specification and makes it easily extendable. This paper serves as an introduction to the double machine learning framework and the R package DoubleML. In reproducible code examples with simulated and real data sets, we demonstrate how DoubleML users can perform valid inference based on machine learning methods. ","DoubleML -- An Object-Oriented Implementation of Double Machine Learning
  in R"
89,1372508790050983938,477030336,Leon Derczynski üè°üå±,"['""Set-to-Sequence Methods in Machine Learning: a Review"" - new paper covering this group of methods, w/ Mateusz Jurewicz and a help from @RasmusPagh1 . -&gt; <LINK>']",https://arxiv.org/abs/2103.09656,"Machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modeling and meta-learning to multi-agent strategy games and power grid optimization. Combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. This paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures. ",Set-to-Sequence Methods in Machine Learning: a Review
90,1372496308775305221,35482283,Dr. Sof√≠a Gallego,['Very excited about my new paper constraining the UV bkg at z&gt;3 with MUSE Ly-alpha emission observations. We also constrain the covering fraction of Lyman-limit systems to be less than 25% within 150 kpc from galaxies. \n<LINK>\n#cosmicweb #astrophysics #research <LINK>'],https://arxiv.org/abs/2103.09250,"The intensity of the Cosmic UV background (UVB), coming from all sources of ionising photons such as star-forming galaxies and quasars, determines the thermal evolution and ionization state of the intergalactic medium (IGM) and is, therefore, a critical ingredient for models of cosmic structure formation. Most of the previous estimates are based on the comparison between observed and simulated Lyman-$\alpha$ forest. We present the results of an independent method to constrain the product of the UVB photoionisation rate and the covering fraction of Lyman limit systems (LLSs) by searching for the fluorescent Lyman-$\alpha$ emission produced by self-shielded clouds. Because the expected surface brightness is well below current sensitivity limits for direct imaging, we developed a new method based on three-dimensional stacking of the IGM around Lyman-$\alpha$ emitting galaxies (LAEs) between 2.9<z<6.6 using deep MUSE observations. Combining our results with covering fractions of LLSs obtained from mock cubes extracted from the EAGLE simulation, we obtain new and independent constraints on the UVB at z>3 that are consistent with previous measurements, with a preference for relatively low UVB intensities at z=3, and which suggest a non-monotonic decrease of $\Gamma$HI with increasing redshift between 3<z<5. This could suggest a possible tension between some UVB models and current observations which however require deeper and wider observations in Lyman-$\alpha$ emission and absorption to be confirmed. Assuming instead a value of UVB from current models, our results constrain the covering fraction of LLSs at 3<z<4.5 to be less than 25% within 150kpc from LAEs. ","Constraining the cosmic UV background at z&gt;3 with MUSE Lyman-{\alpha}
  emission observations"
91,1372472473938182144,2843653680,Anna Ferre-Mateu,"['Paper day! ü•≥ It has been so long due to my cancer+pandemic and I was so excited about it that I followed the new trend of doing a trailer üé¨üìΩÔ∏è The adventures in the realm of low-mass compact galaxies have just begun! Read more at üëÄ‚û°Ô∏è<LINK>\n@rmcdermid @aworawo_ <LINK>', '@ICCUBdivulga @BecarislaCaixa @BecarioslaCaixa', '@SabineBellstedt @rmcdermid @aworawo_ the best proof that they come from a galaxy waaay larger and more massive üòâ']",https://arxiv.org/abs/2103.09241,"We present spatially-resolved two-dimensional maps and radial trends of the stellar populations and kinematics for a sample of six compact elliptical galaxies (cE) using spectroscopy from the Keck Cosmic Web Imager (KCWI). We recover their star formation histories, finding that all except one of our cEs are old and metal rich, with both age and metallicity decreasing toward their outer radii. We also use the integrated values within one effective radius to study different scaling relations. Comparing our cEs with others from the literature and from simulations we reveal the formation channel that these galaxies might have followed. All our cEs are fast rotators, with relatively high rotation values given their low ellipticites. In general, the properties of our cEs are very similar to those seen in the cores of more massive galaxies, and in particular, to massive compact galaxies. Five out of our six cEs are the result of stripping a more massive (compact or extended) galaxy, and only one cE is compatible with having been formed intrinsically as the low-mass, compact object that we see today. These results further confirm that cEs are a mixed-bag of galaxies that can be formed following different formation channels, reporting for the first time an evolutionary link within the realm of compact galaxies (at all stellar masses). ","Low-mass compact elliptical galaxies: spatially-resolved stellar
  populations and kinematics with the Keck Cosmic Web Imager"
92,1372459313957117952,21902101,Jim Geach,"['.@LOFAR\'s view of SMGs: new paper by @joramasawmy on arXiv this morning. ""Low-frequency radio spectra of submillimetre galaxies in the Lockman Hole"" <LINK> <LINK>']",https://arxiv.org/abs/2103.09677,"We investigate the radio properties of a sample of 53 sources selected at 850 $\mu$m from the SCUBA-2 Cosmology Legacy Survey using new deep, low-frequency radio imaging of the Lockman Hole field from the Low Frequency Array. Combining these data with additional radio observations from the GMRT and the JVLA, we find a variety of radio spectral shapes and luminosities within our sample despite their similarly bright submillimetre flux densities. We characterise their spectral shapes in terms of multi-band radio spectral indices. Finding strong spectral flattening at low frequencies in ~20% of sources, we investigate the differences between sources with extremely flat low-frequency spectra and those with `normal' radio spectral indices. As there are no other statistically significant differences between the two subgroups of our sample as split by the radio spectral index, we suggest that any differences are undetectable in galaxy-averaged properties that we can observe with our unresolved images, and likely relate to galaxy properties that we cannot resolve, on scales $\lesssim$ 1 kpc. We attribute the observed spectral flattening in the radio to free-free absorption, proposing that those sources with significant low-frequency spectral flattening have a clumpy distribution of star-forming gas. We estimate an average spatial extent of absorbing material of at most several hundred parsecs to produce the levels of absorption observed in the radio spectra. This estimate is consistent with the highest-resolution observations of submillimetre galaxies in the literature, which find examples of non-uniform dust distributions on scales of ~100 pc, with evidence for clumps and knots in the interstellar medium. Additionally, we find two bright (> 6 mJy) submm sources undetected at all other wavelengths. We speculate that these objects may be very high redshift sources, likely residing at z > 4. ","Low-frequency radio spectra of submillimetre galaxies in the Lockman
  Hole"
93,1372373399545114624,916864821122830336,MinKai Lin,['New paper from @JiaqingB on dust gaps opened by planets in #ProtoplanetaryDisks. TLDR: they are 3D. #PlanetFormation @epo_asiaa <LINK> <LINK>'],https://arxiv.org/abs/2103.09254,"Dust gaps and rings appear ubiquitous in bright protoplanetary disks. Disk-planet interaction with dust-trapping at the edges of planet-induced gaps is one plausible explanation. However, the sharpness of some observed dust rings indicate that sub-mm-sized dust grains have settled to a thin layer in some systems. We test whether or not such dust around gas gaps opened by planets can remain settled by performing three-dimensional, dust-plus-gas simulations of protoplanetary disks with an embedded planet. We find planets massive enough to open gas gaps stir small, sub-mm-sized dust grains to high disk elevations at the gap edges, where the dust scale-height can reach ~70% of the gas scale-height. We attribute this dust 'puff-up' to the planet-induced meridional gas flows previously identified by Fung & Chiang and others. We thus emphasize the importance of explicit 3D simulations to obtain the vertical distribution of sub-mm-sized grains around gas gaps opened by massive planets. We caution that the gas-gap-opening planet interpretation of well-defined dust rings is only self-consistent with large grains exceeding mm in size. ","Puffed up Edges of Planet-opened Gaps in Protoplanetary Disks. I.
  hydrodynamic simulations"
94,1372367547425681409,412208481,Jun Tani,"['Our new paper ""Controlling the Sense of Agency in Dyadic Robot Interaction: An Active Inference Approach."" has been published in aRxiv. <LINK> Also the two robots video here: <LINK> Nadine did a great work!!']",https://arxiv.org/abs/2103.02137,"This study investigated how social interaction among robotic agents changes dynamically depending on the individual belief of action intention. In a set of simulation studies, we examine dyadic imitative interactions of robots using a variational recurrent neural network model. The model is based on the free energy principle such that a pair of interacting robots find themselves in a loop, attempting to predict and infer each other's actions using active inference. We examined how regulating the complexity term to minimize free energy determines the dynamic characteristics of networks and interactions. When one robot trained with tighter regulation and another trained with looser regulation interact, the latter tends to lead the interaction by exerting stronger action intention, while the former tends to follow by adapting to its observations. The study confirms that the dyadic imitative interaction becomes successful by achieving a high synchronization rate when a leader and a follower are determined by developing action intentions with strong belief and weak belief, respectively. ","Leading or Following? Dyadic Robot Imitative Interaction Using the
  Active Inference Framework"
95,1372290706346156035,2377407248,Daniel Whiteson,"['New paper!\n\n""Safety of Quark/Gluon Jet Classification""\n\n<LINK>\n\nLed by Alexis Romero.', 'This paper asks the question: what information is being used when we classify jets as originating from a quark or a gluon?  Is that information infra-red and collinear safe?', ""It turns out that some of it is, and some of it isn't.\n\nBottom-line: it's important to understand what your networks are doing!\n\nPaper with @mfentonHEP, Pierre Baldi and Julian Collado.""]",https://arxiv.org/abs/2103.09103,"The classification of jets as quark- versus gluon-initiated is an important yet challenging task in the analysis of data from high-energy particle collisions and in the search for physics beyond the Standard Model. The recent integration of deep neural networks operating on low-level detector information has resulted in significant improvements in the classification power of quark/gluon jet tagging models. However, the improved power of such models trained on simulated samples has come at the cost of reduced interpretability, raising concerns about their reliability. We elucidate the physics behind quark/gluon jet classification decisions by comparing the performance of networks with and without constraints of infrared and collinear safety, and identify the nature of the unsafe information by revealing the energy and angular dependence of the learned models. This in turn allows us to approximate the performance of the low-level networks (by 99\% or higher) using equivalent sets of interpretable high-level observables, which can be used to probe the fidelity of the simulated samples and define systematic uncertainties. ",Safety of Quark/Gluon Jet Classification
96,1372239867049050115,1250192213079986178,WolframPhysics,['New Paper üö® \nMultiway Turing Machines by @stephen_wolfram now available on arXiv: <LINK> <LINK>'],https://arxiv.org/abs/2103.04961,"Multiway Turing machines (also known as nondeterministic Turing machines or NDTMs) with explicit, simple rules are studied. Even very simple rules are found to generate complex behavior, characterized by complex multiway graphs, that can be visualized in multispace that combines ""tape"" and branchial space. The threshold for complex behavior appears to be machines with just s = 1 head states, k = 2 tape colors and p = 3 possible cases, and such machines may potentially be universal. Other characteristics of multiway Turing machines are also studied, including causal invariance, cyclic tapes and generalized busy beaver problems. Multiway Turing machines provide minimal examples of a variety of issues encountered in both concurrent computing and the theory of observers in quantum mechanics, especially in our recent models of physics. ",Multiway Turing Machines
97,1372182941217009665,1364176717477273600,Fergus Cullen,['Quick recent paper plug: <LINK>. Our main result is a robust detection of alpha-enhanced element abundance ratios in high-z star-forming galaxies (+ we add 21 new entries to the list of galaxies with O/H measurements at z&gt;3). <LINK>'],https://arxiv.org/abs/2103.06300,"We present results from the NIRVANDELS survey investigating the gas-phase metallicity ($\mathrm{Z}_{\mathrm{gas}}$, tracing O/H) and stellar metallicity ($Z_{\star}$, tracing Fe/H) of 33 star-forming galaxies at redshifts $2.95 < z < 3.80$. Based on a combined analysis of deep optical and near-IR spectra, tracing the rest-frame far ultraviolet and rest-frame optical respectively, we present the first simultaneous determination of the stellar and gas-phase mass-metallicity relationships (MZRs) at $z\simeq3.4$. In both cases, we find that metallicity increases with increasing stellar mass ($M_{\star}$), and that the power-law slope at $M_{\star} \lesssim 10^{10} \mathrm{M}_{\odot}$ of both MZRs scales as $Z \propto M_{\star}^{0.3}$. Comparing the stellar and gas-phase MZRs, we present direct evidence for super-solar O/Fe ratios (i.e., $\alpha$-enhancement) at $z>3$, finding $\mathrm{(O/Fe)}\simeq (2.54 \pm 0.38) \times \mathrm{(O/Fe)}_{\odot}$, with no clear dependence on $M_{\star}$. ","The NIRVANDELS Survey: a robust detection of $\alpha$-enhancement in
  star-forming galaxies at $z\simeq3.4$"
98,1372180651047317517,945445796098473984,Patrick Schnider,"['A new paper on #arxiv.\nIn there, i study depth measures defined by some natural axioms and show lower bounds for maximal depth and a variant of the Cascade conjecture for some of them.\n\n<LINK>\n\n#compgeom #geometry', 'And another one\n\nhttps://t.co/PPWr0CJ5Uk\n\nIn this one, together with Daniel and Jonas, we characterize possible histograms defined by Tukey depth of points in a point set.']",https://arxiv.org/abs/2103.08421,"We study families of depth measures defined by natural sets of axioms. We show that any such depth measure is a constant factor approximation of Tukey depth. We further investigate the dimensions of depth regions, showing that the Cascade conjecture, introduced by Kalai for Tverberg depth, holds for all depth measures which satisfy our most restrictive set of axioms, which includes Tukey depth. Along the way, we introduce and study a new depth measure called enclosing depth, which we believe to be of independent interest, and show its relation to a constant-fraction Radon theorem on certain two-colored point sets. ",Enclosing Depth and other Depth Measures
99,1372116012309549058,187336449,Luis Welbanks,"['Advanced modeling frameworks are essential to reliably extract atmospheric properties from the data of today and tomorrow. Check out our new paper introducing Aurora, a next-generation generalized retrieval framework. Today on the arXiv <LINK> <LINK>', ""@V_Parmentier I'm glad you like the logo! All credit goes to Amanda Smith at the IoA. She is the design expert and I had to share her amazing work :)\n\nAs for the exo-Aurora, we'll work on that ;) Hope you like the paper!""]",http://arxiv.org/abs/2103.08600,"Atmospheric retrievals of exoplanetary transmission spectra provide important constraints on various properties such as chemical abundances, cloud/haze properties, and characteristic temperatures, at the day-night atmospheric terminator. To date, most spectra have been observed for giant exoplanets due to which retrievals typically assume H-rich atmospheres. However, recent observations of mini-Neptunes/super-Earths, and the promise of upcoming facilities including JWST, call for a new generation of retrievals that can address a wide range of atmospheric compositions and related complexities. Here we report Aurora, a next-generation atmospheric retrieval framework that builds upon state-of-the-art architectures and incorporates the following key advancements: a) a generalised compositional retrieval allowing for H-rich and H-poor atmospheres, b) a generalised prescription for inhomogeneous clouds/hazes, c) multiple Bayesian inference algorithms for high-dimensional retrievals, d) modular considerations for refraction, forward scattering, and Mie-scattering, and e) noise modeling functionalities. We demonstrate Aurora on current and/or synthetic observations of hot Jupiter HD209458b, mini-Neptune K218b, and rocky exoplanet TRAPPIST1d. Using current HD209458b spectra, we demonstrate the robustness of our framework and cloud/haze prescription against assumptions of H-rich/H-poor atmospheres, improving on previous treatments. Using real and synthetic spectra of K218b, we demonstrate the agnostic approach to confidently constrain its bulk atmospheric composition and obtain precise abundance estimates. For TRAPPIST1d, 10 JWST NIRSpec transits can enable identification of the main atmospheric component for cloud-free CO$_2$-rich and N$_2$-rich atmospheres, and abundance constraints on trace gases including initial indications of O$_3$ if present at enhanced levels ($\sim$10-100x Earth levels). ","Aurora: A Generalised Retrieval Framework for Exoplanetary Transmission
  Spectra"
100,1372100857878216704,717709692517163008,Stefanie Barz,['ü•≥ New paper out on #arXiv: how to communicate secretly and anonymously in a quantum network üîëüòéCheck it out here: <LINK> with @Uni_Stuttgart @tu_berlin @FU_Berlin @IQSTpress  #quantum #quantumnetworks #quantumcommunication #quantumtechnonologies <LINK>'],https://arxiv.org/abs/2103.08722,"Secure communication is one of the key applications of quantum networks. In recent years, following the demands for identity protection in classical communication protocols, the need for anonymity has also emerged for quantum networks. Here, we demonstrate that quantum physics allows parties - besides communicating securely over a network - to also keep their identities secret. We implement such an anonymous quantum conference key agreement by sharing multipartite entangled states in a quantum network. We demonstrate the protocol with four parties and establish keys in subsets of the network - different combinations of two and three parties - whilst keeping the participating parties anonymous. We additionally show that the protocol is verifiable and run multiple key generation and verification routines. Our work thus addresses one of the key challenges of networked communication: keeping the identities of the communicating parties private. ",Anonymous and secret communication in quantum networks
101,1371860664310849538,1071515035581734912,Jay Wang,"['Want to learn more about human-in-the-loop systems in #NLProc? Check out our new survey paper accepted to the HCI + NLP workshop at #EACL2021!\n\nüëâ <LINK>\n\nWork with @JinChoi17888404, @XuShenyu, and @Diyi_Yang.', 'We throughly summarize recent human-in-the-loop NLP frameworks regarding their tasks, goals, interactions, methods, and highlight open challenges and future directions for researchers from both #HCI and #NLP disciplines.\n\n@mlatgt  @GTCSE @ICatGT https://t.co/zZPAMvz92N', '@MujumdarRohit @JinChoi17888404 @XuShenyu @Diyi_Yang Thank you, Rohit! üòä', '@scottafreitas @JinChoi17888404 @XuShenyu @Diyi_Yang Thanks, Scott!']",https://arxiv.org/abs/2103.04044,"How can we design Natural Language Processing (NLP) systems that learn from human feedback? There is a growing research body of Human-in-the-loop (HITL) NLP frameworks that continuously integrate human feedback to improve the model itself. HITL NLP research is nascent but multifarious -- solving various NLP problems, collecting diverse feedback from different people, and applying different methods to learn from collected feedback. We present a survey of HITL NLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI) communities that highlights its short yet inspiring history, and thoroughly summarize recent frameworks focusing on their tasks, goals, human interactions, and feedback learning methods. Finally, we discuss future directions for integrating human feedback in the NLP development loop. ",Putting Humans in the Natural Language Processing Loop: A Survey
102,1371857006269624323,884839252382461952,Daniela Witten,"['New paper with the incomparable @yc_yc_yc_yc and Sean Jewell (not on twitter): \n\na selective inference approach for computing p-values associated with spike times estimated from calcium imaging data (using our L0 spike estimation approach)\n<LINK> 1/3 <LINK>', 'We show that screening spike estimates based on their p-values leads to increased accuracy on spikefinder data, using every metric we consider (correlation, Victor-Purpura, eyeballs) 2/3 https://t.co/4U8Mf4BGMB', 'I am super excited about this work! It really hit the sweet spot for me --- hard statistical methods development applied to an important scientific problem!! 3/3']",https://arxiv.org/abs/2103.07818,"In recent years, a number of methods have been proposed to estimate the times at which a neuron spikes on the basis of calcium imaging data. However, quantifying the uncertainty associated with these estimated spikes remains an open problem. We consider a simple and well-studied model for calcium imaging data, which states that calcium decays exponentially in the absence of a spike, and instantaneously increases when a spike occurs. We wish to test the null hypothesis that the neuron did not spike -- i.e., that there was no increase in calcium -- at a particular timepoint at which a spike was estimated. In this setting, classical hypothesis tests lead to inflated Type I error, because the spike was estimated on the same data used for testing. To overcome this problem, we propose a selective inference approach. We describe an efficient algorithm to compute finite-sample p-values that control selective Type I error, and confidence intervals with correct selective coverage, for spikes estimated using a recent proposal from the literature. We apply our proposal in simulation and on calcium imaging data from the spikefinder challenge. ",Quantifying uncertainty in spikes estimated from calcium imaging data
103,1371810220435939329,923132721760649216,kosukekurosawa,['Our new paper has been accepted for publication in GRL. We demonstrated the significance of post-shock heating due to plastic deformation on impact devolatilization of calcite.\n<LINK>'],https://arxiv.org/abs/2103.02868,"An accurate understanding of the relationship between the impact conditions and the degree of shock-induced thermal metamorphism in meteorites allows the impact environment in the early Solar System to be understood. A recent hydrocode has revealed that impact heating is much higher than previously thought. This is because plastic deformation of the shocked rocks causes further heating during decompression, which is termed post-shock heating. Here we compare impact simulations with laboratory experiments on the impact devolatilization of calcite to investigate whether the post-shock heating is also significant in natural samples. We calculated the mass of CO$_2$ produced from the calcite, based on thermodynamics. We found that iSALE can reproduce the devolatilization behavior for rocks with the strength of calcite. In contrast, the calculated masses of CO2 at lower rock strengths are systematically smaller than the experimental values. Our results require a reassessment of the interpretation of thermal metamorphism in meteorites. ","The role of post-shock heating by plastic deformation during impact
  devolatilization of calcite"
104,1371771163504951296,2813168019,EGO & the Virgo Collaboration,"['A paper published today on @arxiv by the Virgo, @LIGO  and @KAGRA_PR Collaborations, based on data from the first three observing runs of the Virgo and LIGO detectors, sets new constraints on anisotropies of stochastic gravitational wave backgrounds. <LINK> <LINK>', 'Different GW backgrounds are generated from the combination of all the too faint merger signals, that our detectors are not able to individually resolve, or from early Universe phenomena, such as phase transitions and primordial black hole mergers. Hence observing...', ""direction-dependent features in the GW backgrounds could give us insights into history of the early universe and matter distribution of the nearby one. This research didn't find any significant evidence for a gravitational-wave background, but it sets more stringent upper limits.""]",https://arxiv.org/abs/2103.08520,"We report results from searches for anisotropic stochastic gravitational-wave backgrounds using data from the first three observing runs of the Advanced LIGO and Advanced Virgo detectors. For the first time, we include Virgo data in our analysis and run our search with a new efficient pipeline called {\tt PyStoch} on data folded over one sidereal day. We use gravitational-wave radiometry (broadband and narrow band) to produce sky maps of stochastic gravitational-wave backgrounds and to search for gravitational waves from point sources. A spherical harmonic decomposition method is employed to look for gravitational-wave emission from spatially-extended sources. Neither technique found evidence of gravitational-wave signals. Hence we derive 95\% confidence-level upper limit sky maps on the gravitational-wave energy flux from broadband point sources, ranging from $F_{\alpha, \Theta} < {\rm (0.013 - 7.6)} \times 10^{-8} {\rm erg \, cm^{-2} \, s^{-1} \, Hz^{-1}},$ and on the (normalized) gravitational-wave energy density spectrum from extended sources, ranging from $\Omega_{\alpha, \Theta} < {\rm (0.57 - 9.3)} \times 10^{-9} \, {\rm sr^{-1}}$, depending on direction ($\Theta$) and spectral index ($\alpha$). These limits improve upon previous limits by factors of $2.9 - 3.5$. We also set 95\% confidence level upper limits on the frequency-dependent strain amplitudes of quasimonochromatic gravitational waves coming from three interesting targets, Scorpius X-1, SN 1987A and the Galactic Center, with best upper limits range from $h_0 < {\rm (1.7-2.1)} \times 10^{-25},$ a factor of $\geq 2.0$ improvement compared to previous stochastic radiometer searches. ","Search for anisotropic gravitational-wave backgrounds using data from
  Advanced LIGO and Advanced Virgo's first three observing runs"
105,1371745025638416384,953655661430222848,Moritz Schauer,"['New paper <LINK> on arXiv ""Sticky PDMP samplers for sparse and local inference problems"" by @jbierkens @SebastianoGraz3 @MeulenFrank and me. <LINK>', 'The sampler moves like the e.g. the Zig-Zag but ""sticks"" to the coordinate axess... to sample statistical models with spike and slab priors for high-dimensional variable selection https://t.co/OKOmVtxgrB', 'We can combine spike and slab priors with a Gaussian smoothing prior to filter out noise and solve the combinatorial problem of selecting the right roughly 100k background pixels in this structured sparse problem https://t.co/WmJP4KHPOO', 'A sampler trajectory for this example shows the mixing and convergence https://t.co/tnuzNNXpsF']",https://arxiv.org/abs/2103.08478,"We construct a new class of efficient Monte Carlo methods based on continuous-time piecewise deterministic Markov processes (PDMPs) suitable for inference in high dimensional sparse models, i.e. models for which there is prior knowledge that many coordinates are likely to be exactly $0$. This is achieved with the fairly simple idea of endowing existing PDMP samplers with 'sticky' coordinate axes, coordinate planes etc. Upon hitting those subspaces, an event is triggered during which the process sticks to the subspace, this way spending some time in a sub-model. This results in non-reversible jumps between different (sub-)models. While we show that PDMP samplers in general can be made sticky, we mainly focus on the Zig-Zag sampler. The computational efficiency of our method (and implementation) is established through numerical experiments where both the sample size and the dimension of the parameter space are large. ",Sticky PDMP samplers for sparse and local inference problems
106,1371701321359822848,1173944192822927361,astrid.eichhorn,"['New paper out on arxiv on ""Image features of regular spinning black holes based on a locality principle"": <LINK>\nwith @aaaronHeld. We construct a class of regular black holes with characteristic image features of new physics. \n@NATsdu @VILLUMscience <LINK>', ""...and the day also closes with black holes: Am having lots of fun introducing fifty students from ATU Denmark to black holes, their shadows, gravitational waves &amp; the supermassive black hole in our own galaxy. It's such an exciting time for black-hole physics!""]",https://arxiv.org/abs/2103.07473,"To understand the true nature of black holes, fundamental theoretical developments should be linked all the way to observational features of black holes in their natural astrophysical environments. Here, we take several steps to establish such a link. We construct a family of spinning, regular black-hole spacetimes based on a locality principle for new physics and analyze their shadow images. We identify characteristic image features associated to regularity (increased compactness and relative stretching) and to the locality principle (cusps and asymmetry) that persist in the presence of a simple analytical disk model. We conjecture that these occur as universal features of distinct classes of regular black holes based on different sets of construction principles for the corresponding spacetimes. ","Image features of spinning regular black holes based on a locality
  principle"
107,1371698708325101571,345254938,Snehasish Bhattacharjee (Bil),['My new paper in arXiv: <LINK>\nEnergy Conditions in f(P) Gravity'],https://arxiv.org/abs/2103.08444,"$f(P)$ gravity is a novel extension of ECG in which the Ricci scalar in the action is replaced by a function of the curvature invariant $P$ which represents the contractions of the Riemann tensor at the cubic order \cite{p}. The present work is concentrated on bounding some $f(P)$ gravity models using the concept of energy conditions where the functional forms of $f(P)$ are represented as \textbf{a)} $f(P) = \alpha \sqrt{P}$, and \textbf{b)} $f(P) = \alpha \exp (P)$, where $\alpha$ is the sole model parameter. Energy conditions are interesting linear relationships between pressure and density and have been extensively employed to derive interesting results in Einstein's gravity, and are also an excellent tool to impose constraints on any cosmological model. To place the bounds, we ensured that the energy density must remain positive, the pressure must remain negative, and the EoS parameter must attain a value close to $-1$ to make sure that the bounds respect the accelerated expansion of the Universe and are also in harmony with the latest observational data. We report that for both the models, suitable parameter spaces exist which satisfy the aforementioned conditions and therefore posit the $f(P)$ theory of gravity to be a promising modified theory of gravitation. ",Energy Conditions in $f(P)$ Gravity
108,1371642043135500290,913238472357437445,Fuminobu TAKAHASHI,['A new paper with Shota Nakagawa and Masaki Yamada on explaining the cosmic birefringence by an ALP coupled to DM density. It explains why the ALP moves at a special timing between the recombination and the present. It also works for broader masses.\n\n<LINK>'],https://arxiv.org/abs/2103.08153,"Cosmic birefringence is predicted if an axion-like particle (ALP) moves after the recombination. We show that this naturally happens if the ALP is coupled to the dark matter density because it then acquires a large effective mass after the matter-radiation equality. Our scenario applies to a broad range of the ALP mass $m_\phi \lesssim 10^{-28}$ eV, even smaller than the present Hubble constant. We give a simple model to realize this scenario, where dark matter is made of hidden monopoles, which give the ALP such a large effective mass through the Witten effect. The mechanism works if the ALP decay constant is of order the GUT scale without a fine-tuning of the initial misalignment angle. For smaller decay constant, the hidden monopole can be a fraction of dark matter. We also study the implications for the QCD axion, and show that the domain wall problem can be solved by the effective mass. ",Cosmic Birefringence Triggered by Dark Matter Domination
109,1371637308319965185,746440524052082688,Nicolas Delfosse,['Quantum LDPC codes are the future and they need better decoders.\n\nCheck out our new paper on a Union-Find decoder for LDPC codes with @vivien_londe and Michael Beverland.\n\n<LINK>'],https://arxiv.org/abs/2103.08049,"Quantum LDPC codes are a promising direction for low overhead quantum computing. In this paper, we propose a generalization of the Union-Find decoder as adecoder for quantum LDPC codes. We prove that this decoder corrects all errors with weight up to An^{\alpha} for some A, {\alpha} > 0 for different classes of quantum LDPC codes such as toric codes and hyperbolic codes in any dimension D \geq 3 and quantum expander codes. To prove this result, we introduce a notion of covering radius which measures the spread of an error from its syndrome. We believe this notion could find application beyond the decoding problem. We also perform numerical simulations, which show that our Union-Find decoder outperforms the belief propagation decoder in the low error rate regime in the case of a quantum LDPC code with length 3600. ",Toward a Union-Find decoder for quantum LDPC codes
110,1371636592708878340,51700215,Phil Bull,"['New paper: A really nice debut paper by PhD student Fraser Kennedy, where we calculate the effect of beams and foreground cuts on multipoles of the correlation function of a MeerKAT-like 21cm survey: <LINK>', ""21cm intensity mapping experiments can cover extremely large survey volumes, but have degraded angular resolution compared to galaxy surveys due to their larger instrumental beam. For 'single-dish' surveys, this can result in the transverse BAO feature being washed out."", 'Even worse, bright foreground contamination also has to be filtered out, resulting in the loss of long-wavelength radial modes (low k_parallel) too. How much information about the BAO scale can we recover from experiments like MeerKAT and SKA1-MID in spite of this?', 'Fraser calculated analytic expressions for the correlation fn multipoles and their covariance in the presence of beam and foreground effects, implemented them using fast calculation methods, and then ran fits on 1000s of realisations to see how well the BAO could be recovered.', 'Happily, the radial BAO feature is very robust to these effects, and can be recovered accurately, particularly from the quadrupole. The transverse BAO less so, although the multipole formalism is good at preserving transverse information where it can.', 'Fraser did a really thorough job of looking at how different effects and assumptions could affect the analysis. His results will be useful for upcoming surveys with both MeeeKAT and SKA.']",https://arxiv.org/abs/2103.08568,"Despite being designed as an interferometer, the MeerKAT radio array (an SKA pathfinder) can also be used in autocorrelation (`single-dish') mode, where each dish scans the sky independently. Operating in this mode allows extremely high survey speeds to be achieved, albeit at significantly lower angular resolution. We investigate the recovery of the baryon acoustic oscillation (BAO) scale from multipoles of the redshift-space correlation function as measured by a low angular resolution 21cm intensity mapping survey of this kind. Our approach is to construct an analytic model of the multipoles of the correlation function and their covariance matrix that includes foreground contamination and beam resolution effects, which we then use to generate an ensemble of mock data vectors from which we attempt to recover the BAO scale. In line with previous studies, we find that recovery of the transverse BAO scale $\alpha_{\perp}$ is hampered by the strong smoothing effect of the instrumental beam with increasing redshift, while the radial scale $\alpha_\parallel$ is much more robust. The multipole formalism naturally incorporates transverse information when it is available however, and so there is no need to perform a radial-only analysis. In particular, the quadrupole of the correlation function preserves a distinctive BAO `bump' feature even for large smoothing scales. We also investigate the robustness of BAO scale recovery to beam model accuracy, severity of the foreground removal cuts, and accuracy of the covariance matrix model, finding in all cases that the radial BAO scale can be recovered in an accurate, unbiased manner. ","Statistical recovery of the BAO scale from multipoles of the
  beam-convolved 21cm correlation function"
111,1371625992280768515,321794593,Jos√© G. Fern√°ndez-Trincado,"['Our new accepted paper üëâüèª""Chemodynamically Characterizing the Jhelum Stellar Stream with APOGEE-2"" by Allyson et al. üëâüèª<LINK>']",https://arxiv.org/abs/2103.07488,"We present the kinematic and chemical profiles of red giant stars observed by the APOGEE-2 survey in the direction of the Jhelum stellar stream, a Milky Way substructure located in the inner halo of the Milky Way at a distance from the Sun of $\approx$ 13 kpc. From the six APOGEE-2 Jhelum pointings, we isolate stars with log($g$) $<$ 3.5, leaving a sample of 289 red giant stars. From this sample of APOGEE giants, we identified seven stars that are consistent with the astrometric signal from $Gaia$ DR2 for this stream. Of these seven, one falls onto the RGB along the same sequence as the Jhelum stars presented by \cite{ji20}. This new Jhelum member has [Fe/H]=-2.2 and is at the tip of the red giant branch. By selecting high orbital eccentricity, metal-rich stars, we identify red giants in our APOGEE sample that are likely associated with the $Gaia$-Enceladus-Sausage (GES) merger. We compare the abundance profiles of the Jhelum stars and GES stars and find similar trends in $\alpha$-elements, as expected for low-metallicity populations. However, we find that the orbits for GES and Jhelum stars are not generally consistent with a shared origin. The chemical abundances for the APOGEE Jhelum star and other confirmed members of the stream are similar to stars in known stellar streams and thus are consistent with an accreted dwarf galaxy origin for the progenitor of the stream, although we cannot rule out a globular cluster origin. ",Chemodynamically Characterizing the Jhelum Stellar Stream with APOGEE-2
112,1371333166577844226,2377407248,Daniel Whiteson,"['Who says you can‚Äôt use machine learning in theoretical physics?\n\nNew paper:\n\n<LINK>\n\nEfficient sampling of constrained high-dimensional theoretical spaces with machine learning. \n\nLed by Jake Hollingsworth, with @FlipTanedo and M. Ratz.', '@FlipTanedo Simple question: what fraction of supersymmetry theories have a Higgs that matches the one in our Universe?  \n\nWE DON‚ÄôT KNOW because the space is too large to explore. \n\nIt has 100+ parameters! We can‚Äôt generate ~100^100 points.', 'Theorists try to reduce the dimensions by making a bunch of assumptions.  \n\nBut what‚Äôs missed?  Are there islands that nobody has seen in that vast space? \n\nHow can we explore it?', 'Machine learning is GREAT at exploring high-dim spaces. We used ML to learn where the Higgs-compatible spots are, letting us sample the space orders of magnitude more quickly. https://t.co/0hDbWRSLIj', ""It's much faster AND not biased. https://t.co/jfV8WuDSaQ"", 'We hope to use this tool to search for unexplored islands in theory space. Stay tuned!']",https://arxiv.org/abs/2103.06957,"Models of physics beyond the Standard Model often contain a large number of parameters. These form a high-dimensional space that is computationally intractable to fully explore. Experimental constraints project onto a subspace of viable parameters, but mapping these constraints to the underlying parameters is also typically intractable. Instead, physicists often resort to scanning small subsets of the full parameter space and testing for experimental consistency. We propose an alternative approach that uses generative models to significantly improve the computational efficiency of sampling high-dimensional parameter spaces. To demonstrate this, we sample the constrained and phenomenological Minimal Supersymmetric Standard Models subject to the requirement that the sampled points are consistent with the measured Higgs boson mass. Our method achieves orders of magnitude improvements in sampling efficiency compared to a brute force search. ","Efficient sampling of constrained high-dimensional theoretical spaces
  with machine learning"
113,1370563872168374275,1665897810,Kevin Frans,"['New paper on how Population-based Evolution is a natural meta-learning algorithm! With @okw at @crosslabstokyo <LINK>', ""@okw @crosslabstokyo Basic idea: In evolutionary algorithms, a strong gene is a gene which survives for many generations. Thus, strong genes should grant fitness not only to an individual but also to all of the individual's offspring."", 'This means that in non-stationary environments, genes that increase the *adaptive ability* of a genome are naturally selected for. Over time, the learning ability of a population increases.', 'This perspective can help explain why biological systems are so adaptable, or why languages are so robust -- with large populations, evolution naturally selects for systems that are good at adapting to new tasks.']",https://arxiv.org/abs/2103.06435,"Meta-learning models, or models that learn to learn, have been a long-desired target for their ability to quickly solve new tasks. Traditional meta-learning methods can require expensive inner and outer loops, thus there is demand for algorithms that discover strong learners without explicitly searching for them. We draw parallels to the study of evolvable genomes in evolutionary systems -- genomes with a strong capacity to adapt -- and propose that meta-learning and adaptive evolvability optimize for the same objective: high performance after a set of learning iterations. We argue that population-based evolutionary systems with non-static fitness landscapes naturally bias towards high-evolvability genomes, and therefore optimize for populations with strong learning ability. We demonstrate this claim with a simple evolutionary algorithm, Population-Based Meta Learning (PBML), that consistently discovers genomes which display higher rates of improvement over generations, and can rapidly adapt to solve sparse fitness and robotic control tasks. ",Population-Based Evolution Optimizes a Meta-Learning Objective
114,1370559112950468609,1138838376,Baishakhi Ray,"['De-noising pretraining excels for dual modeling of programming language (e.g., source code) + natural language (e.g., code comment). See our new @NAACLHLT  paper <LINK>.  Thanks to awesome collaborations by Wasi Ahmed, Saikat Chakraborty,   \n@kaiwei_chang\n.']",https://arxiv.org/abs/2103.06333,"Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., if block inside an else block is equivalent to else if block) that are crucial to program semantics and thus excels even with limited annotations. ",Unified Pre-training for Program Understanding and Generation
115,1370443921017376768,990433714948661250,Sergey Levine,"['We often hear that MaxEnt RL is ""robust"" -- but what does that mean? In his new paper &amp; blog post, @ben_eysenbach presents the first proof that MaxEnt RL *is* robust to certain kinds of dynamics perturbations!\n\nüßµ-&gt;\n\nPaper: <LINK>\nPost: <LINK>', 'The particular stochasticity induced by MaxEnt RL results in an objective that lower-bounds a min-max robust control objective, meaning that policies learned by MaxEnt RL should be provably robust to certain perturbations -- this is not the case for regular RL.', 'Of course, relationships between maximum entropy models and robustness in general are widely known, and the robustness perspective is in fact the origin of maxent modeling in the literature. Hence people suspected for a long time that MaxEnt *RL* must be robust in some way.', 'But precisely *how* MaxEnt RL is robust (ito what set of perturbations) has been a mystery. This work expands our earlier work on how MaxEnt RL is robust to *reward* perturbations: https://t.co/xOdhcvO19J\nBut we now show dynamics robustness, which is arguably much more useful.', 'Also, here is a cool animation of a robust (stochastic) policy pushing an object around a barrierüòÄ\n\nBen did all the work here, but I can claim a small contribution: in 2014 I made the robot model, and in 2015, on suggestion of Marvin Zhang and @chelseabfinn, I added the eyes... https://t.co/A3v3UCTKMh', 'As @chelseabfinn just pointed out to me, there is even proof (complete with awkward comment from me) https://t.co/XnikaLRIzS', '@Vikashplus @chelseabfinn Yeah, the model is actually a badly butchered version of the PR2 model that I believe you converted from the WG URDF.']",https://arxiv.org/abs/2103.06257,"Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that maximum entropy (MaxEnt) RL maximizes a lower bound on a robust RL objective, and thus can be used to learn policies that are robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL is a simple robust RL method with appealing formal guarantees. ",Maximum Entropy RL (Provably) Solves Some Robust RL Problems
116,1370410801211666432,123421220,Yvette Cendes,"['Gather round, Twitter, today is NEW PAPER DAY! All about a black hole that ate a star! Bonus: first reported @almaobs detection of a TDE!! And neutrino speculation!\n\n""Radio Observations of an Ordinary Outflow from the Tidal Disruption Event AT2019dsg""\n\n<LINK>\n\nüßµ', 'To begin, a Tidal Disruption Event (TDE) happens when a star wanders too close to a black hole, and is torn apart by tidal forces. You can see what this looks like in this rendition by @astrocrash (and more updated ones by Jamie Law Smith here- https://t.co/jclLMZNGeC) https://t.co/ZIFXy9HXll', ""Most TDEs are discovered in optical, and ~100 are known. Of those, about dozen are radio detected. Radio is important bc it tells you properties about the outflow- its radius, energy, magnetic field strength, even the density of material it's plowing through!"", 'Which brings us to the TDE called AT2019dsg! Discovered by @ztfsurvey in April 2019, it was REALLY bright in optical for a TDE- like top 3%- and occurred 230Mpc (750 million light years) from Earth. A lot of ppl started studying it for obvious reasons', ""What's more, a *very* intriguing study of AT2019dsg was published (Stein et al. 2021) that associated an @uw_icecube neutrino in Oct 2019 w this event! This would be the second time a neutrino was traced to a specific source like this! üòØ https://t.co/3vQxWgaRZC"", 'The paper published 4 radio VLA observations from June-Sept bolstering this claim, based on the energies calculated. Some theories also argue that a jet might be present that created the neutrino. https://t.co/pA8NLAAB8L', ""But how to know for sure? By looking at EVEN MORE DATA! Our group also observed this TDE June 2019-Oct 2020, so w the Stein data added into our modeling we have a nice sample over time for this evolution (this is Fig 1 i paper, blue's our data, orange Stein, we fit models to all) https://t.co/8pvt4LnurI"", ""If we look at the peak frequency/flux of the SED curves, we see the peak frequency pretty steadily decreases. Peak flux tho, def interesting- peaks at ~200 days, before fading. That's fairly unusual for a TDE (Fig 2 in paper) https://t.co/3JOx4RmtVm"", ""But wait, there's more- in addition to the VLA data, we got an ALMA detection!!! First reported detection of a TDE with it! https://t.co/ZY15t4U4gX"", ""Specifically, we got one detection, one UL, at 97.5 GHz. But both are good enough to show that we *definitely* have a cooling break at around ~25 GHz. If you don't include one (black) you way over-estimate what we see at ALMA frequencies https://t.co/G48Ij0w0GQ"", ""This means we can constrain the magnetic field emissivity (a term called epsilon_b) which tells you the % emission you're seeing from electrons in radio. Default assumption is 10%, we saw 2%. Second time *ever* this has been done for a TDE!"", ""This parameter gets REALLY important in the physics of a TDE. Make it smaller, your energy gets larger. Only other TDE where it's been measured was the relativistic Swift J1644+57, measured to e_b= .001 by Eftekari et al. 2018, so a lot of ppl assume this in their own TDE work"", '(Side note, did you know this is OUT OF DATE? Thanks to a little paper called Cendes et al. 2021A, we now know eps_b=0.01 is more accurate for Swift J1644+57! TL;DR, lots of ppl maybe making a more extreme deviation than they should be)\n\nhttps://t.co/VjGC5T2TPL', ""ANYWAY, I hear you clamor, what did we find for AT2019dsg?! To begin, more and better densities around the black hole! It's very similar to our own Milky Way, but steeper drop off. Fig 6 in my paper https://t.co/3zqLB3ISvd"", 'Side note, it *boggles my mind* that we know the environment surrounding some black holes millions of light years away better than our own around Saggitarius A* in the Milky Way. This is the real unsung amazing detail about radio TDEs, IMO https://t.co/iguS4kpGXk', ""But, of course, we also found a ton of other parameters (see Fig 4 in paper for all of them!) Here's the energy, assuming spherical volume for outflow, and w our eps_b values. Yellow is eps_b=0.1 (equipartition) in Stein et al, green is ebs_b=0.001. https://t.co/ecnGimwPsq"", ""What you can see here is we definitely see in our radio data that the energy increases, but peaks at ~5E48 ergs. Side note: it's tough to get a neutrino from a TDE w these energies..."", 'Here\'s an even better plot (Fig 6- side note, Kate Alexander makes amazing plots) showing the energy/velocity space for several radio TDEs, plus GRBs and SNe. As you can see, there\'s two populations emerging- one that are relativistic, one more ""ordinary"" and SNE-like. https://t.co/SHmP8yAY4p', ""Specifically, velocity for AT2019 we measured was 0.07c (7% speed of light), which is pretty normal-ish for, say, a supernova. We *did* calculate what would happen if a jet might be present btw, but it's physically unfeasible (super small angle)."", 'So, where does this leave the neutrino? Well, this is a ""common"" TDE as the title says. We really see no evidence of very high energies, or a jet, and most neutrino theories say you need one or the other...', ""Also noteworthy, it's really similar to a SNe. Those are MUCH more common, and many occur FAR closer than AT2019dsg was. So, where are the neutrinos from all *those*?"", ""So if you ask me, our results do cast doubt on the neutrino association w AT2019dsg over being a coincidence. But I look forward to hearing theorists figure out how it's still possible. :)"", ""I think that's all! Side note if I may brag a sec, this was my fastest paper turnaround ever- just shy of 5months! üòé Possible thanks to amazing coauthors, and my tendency to withdraw into my research when the world got too insane, which was often..."", ""But hey, thinking about my black hole kept me sane in some rough times of the pandemic, and I'll forever be grateful for that escape! Let me know if you have any questions on things I didn't cover here! /end"", '@philewing Thank you! We only care about the time the signal was detected on Earth, not the +750 million years it took for us to reach us. But yeah this emission is prob no longer detectable on decades-long time scales, maybe centuries tops.', '@philewing The outflow is still prob going outward a bit when no longer detectable, mind, but even supernova remnants we see are thousands of years old, not millions.', ""@philewing Don't think so. It's pretty clear the energy increased in the outflow over the first ~200 days, and the peak was at a middling value. If energy was high then decreased we def would have captured it, radio is a fairly slow evolution."", ""@philewing Mind, neutrinos travel at the speed of light. It's not like it could have been delayed a few months enroute."", '@yalinewich @almaobs Haha you\'re gonna have to send that to me, bc in Twitter form I\'m just like ""well I recognize *some* of those words..."" üôÉ', '@yalinewich @almaobs Ah I see! Cool fair enough! :)']",https://arxiv.org/abs/2103.06299,"We present detailed radio observations of the tidal disruption event (TDE) AT2019dsg, obtained with the Very Large Array (VLA) and the Atacama Large Millimeter/submillimeter Array (ALMA), and spanning $55-560$ days post-disruption. We find that the peak brightness of the radio emission increases until ~200 days and subsequently begins to decrease steadily. Using the standard equipartition analysis, including the effects of synchrotron cooling as determined by the joint VLA-ALMA spectral energy distributions, we find that the outflow powering the radio emission is in roughly free expansion with a velocity of $\approx 0.07c$, while its kinetic energy increases by a factor of about 5 from 55 to 200 days and plateaus at $\approx 5\times 10^{48}$ erg thereafter. The ambient density traced by the outflow declines as $\approx R^{-1.6}$ on a scale of $\approx (1-4)\times 10^{16}$ cm ($\approx 6300-25000$ $R_s$), followed by a steeper decline to $\approx 6\times 10^{16}$ cm ($\approx 37500$ $R_s$). Allowing for a collimated geometry, we find that to reach even mildly relativistic velocities ($\Gamma=2$) the outflow requires an opening angle of $\theta_j\approx 2^\circ$, which is narrow even by the standards of GRB jets; a truly relativistic outflow requires an unphysically narrow jet. The outflow velocity and kinetic energy in AT2019dsg are typical of previous non-relativistic TDEs, and comparable to those from Type Ib/c supernovae, raising doubts about the claimed association with a high-energy neutrino event. ","Radio Observations of an Ordinary Outflow from the Tidal Disruption
  Event AT2019dsg"
117,1370385873745874946,1280142008070307843,Ryan-Rhys Griffiths,['Excited to announce new work with @a_bourached (and others!) where we attempt to infer the structure of a black hole accretion disk using Gaussian processes in GPflow @GPflowProject \n\nPaper: <LINK>\n\nCode: <LINK> <LINK>'],https://arxiv.org/abs/2103.06838,"The optical and UV variability of the majority of AGN may be related to the reprocessing of rapidly-changing X-ray emission from a more compact region near the central black hole. Such a reprocessing model would be characterised by lags between X-ray and optical/UV emission due to differences in light travel time. Observationally however, such lag features have been difficult to detect due to gaps in the lightcurves introduced through factors such as source visibility or limited telescope time. In this work, Gaussian process regression is employed to interpolate the gaps in the Swift X-ray and UV lightcurves of the narrow-line Seyfert 1 galaxy Mrk 335. In a simulation study of five commonly-employed analytic Gaussian process kernels, we conclude that the Matern 1/2 and rational quadratic kernels yield the most well-specified models for the X-ray and UVW2 bands of Mrk 335. In analysing the structure functions of the Gaussian process lightcurves, we obtain a broken power law with a break point at 125 days in the UVW2 band. In the X-ray band, the structure function of the Gaussian process lightcurve is consistent with a power law in the case of the rational quadratic kernel whilst a broken power law with a breakpoint at 66 days is obtained from the Matern 1/2 kernel. The subsequent cross-correlation analysis is consistent with previous studies and furthermore, shows tentative evidence for a broad X-ray-UV lag feature of up to 30 days in the lag-frequency spectrum where the significance of the lag depends on the choice of Gaussian process kernel. ","Modelling the Multiwavelength Variability of Mrk 335 using Gaussian
  Processes"
118,1370363959157563398,107357817,daisukelab,"['Our new paper is out!üöÄ First professional research paper in my 20+yrs career. üò≠\nThis is for training better audio encoder, especially for a practical small audio DNN.\n""BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation""\n<LINK>', '@yuma_koizumi „Éõ„É≥„Éà„Å´„ÉÅ„Éº„É†ÊôÇ‰ª£„ÅÆ„Ç¢„Éâ„Éê„Ç§„Çπ„Å´‰Ωï„Å®„ÅäÁ§º„ÇíË®Ä„Å£„Åü„Çâ„Çà„ÅÑ„ÇÑ„Çâ„ÄÅ„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇË™≠„Çì„Åß„ÅÑ„Åü„Å†„ÅÑ„Å¶ÊÅêÁ∏Æ„Åß„Åô„ÄÇweight/code„ÅØÂÖ¨Èñã„Åó„Åü„ÅÑ„Åß„Åô„ÄÇÁ§æÂÜÖÊâøË™ç„Å®„ÄÅ„Åù„Çå‰ª•Ââç„Å´‰ΩúÊ•≠ÊôÇÈñì„ÅåÂèñ„Çå„Åù„ÅÜ„Å´„Å™„Åè„ÄÅ„Å°„Çá„Å£„Å®„ÅÇ„Å®„Å´„Å™„Çä„Åù„ÅÜ„Åß„Åô„Åå„ÄÅ„Åù„ÅÆ„ÅÜ„Å°Âá∫„Åó„Åü„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô! (Â≠¶‰ºö„ÅÆÊôÇÊúü„Åã„Å™‚Ä¶)']",https://arxiv.org/abs/2103.06695,"Inspired by the recent progress in self-supervised learning for computer vision that generates supervision using data augmentations, we explore a new general-purpose audio representation learning approach. We propose learning general-purpose audio representation from a single audio segment without expecting relationships between different time segments of audio samples. To implement this principle, we introduce Bootstrap Your Own Latent (BYOL) for Audio (BYOL-A, pronounced ""viola""), an audio self-supervised learning method based on BYOL for learning general-purpose audio representation. Unlike most previous audio self-supervised learning methods that rely on agreement of vicinity audio segments or disagreement of remote ones, BYOL-A creates contrasts in an augmented audio segment pair derived from a single audio segment. With a combination of normalization and augmentation techniques, BYOL-A achieves state-of-the-art results in various downstream tasks. Extensive ablation studies also clarified the contribution of each component and their combinations. ","BYOL for Audio: Self-Supervised Learning for General-Purpose Audio
  Representation"
119,1370323952736817152,1077995761487568896,Jon Miller,"['New paper day, part 2:\n@NASAHubble and @ESA_XMM observations of the neutron star X-ray binary EXO 0748-676 in quiescence, by Aastha Parikh. Thermal emission from the neutron star crust may dominate the emission.  Quiescent UV spectrum from HST shown below.\n<LINK> <LINK>']",https://arxiv.org/abs/2103.06278,"The accretion behaviour in low-mass X-ray binaries (LMXBs) at low luminosities, especially at <E34 erg/s, is not well known. This is an important regime to study to obtain a complete understanding of the accretion process in LMXBs, and to determine if systems that host neutron stars with accretion-heated crusts can be used probe the physics of dense matter (which requires their quiescent thermal emission to be uncontaminated by residual accretion). Here we examine ultraviolet (UV) and X-ray data obtained when EXO 0748-676, a crust-cooling source, was in quiescence. Our Hubble Space Telescope spectroscopy observations do not detect the far-UV continuum emission, but do reveal one strong emission line, Civ. The line is relatively broad (>3500 km/s), which could indicate that it results from an outflow such as a pulsar wind. By studying several epochs of X-ray and near-UV data obtained with XMM-Newton, we find no clear indication that the emission in the two wavebands is connected. Moreover, the luminosity ratio of Lx/Luv >100 is much higher than that observed from neutron star LMXBs that exhibit low-level accretion in quiescence. Taken together, this suggests that the UV and X-ray emission of EXO 0748-676 may have different origins, and that thermal emission from crust-cooling of the neutron star, rather than ongoing low-level accretion, may be dominating the observed quiescent X-ray flux evolution of this LMXB. ","UV and X-ray observations of the neutron star LMXB EXO 0748-676 in its
  quiescent state"
120,1370322526048555009,1077995761487568896,Jon Miller,"['New paper day:\nTheoretical line profiles from thermal winds in AGN, by @ShaliniGanguly2.  This helps us to understand whether different line profiles demand different outflows or outflow components.  \n<LINK> <LINK>']",https://arxiv.org/abs/2103.06497,"The warm absorbers observed in more than half of all nearby active galactic nuclei (AGN) are tracers of ionized outflows located at parsec scale distances from the central engine. If the smallest inferred ionization parameters correspond to plasma at a few $10^4$~K, then the gas undergoes a transition from being bound to unbound provided it is further heated to $\sim 10^6$~K at larger radii. Dannen et al. recently discovered that under these circumstances, thermally driven wind solutions are unsteady and even show very dense clumps due to thermal instability. To explore the observational consequences of these new wind solutions, we compute line profiles based on the one-dimensional simulations of Dannen et al. We show how the line profiles from even a simple steady state wind solution depend on the ionization energy (IE) of absorbing ions, which is a reflection of the wind ionization stratification. To organize the diversity of the line shapes, we group them into four categories: weak Gaussians, saturated boxy profiles with and without an extended blue wing, and broad weak profiles. The lines with profiles in the last two categories are produced by ions with the highest IE that probe the fastest regions. Their maximum blueshifts agree with the highest flow velocities in thermally unstable models, both steady state and clumpy versions. In contrast, the maximum blueshifts of the most high IE lines in thermally stable models can be less than half of the actual solution velocities. Clumpy solutions can additionally imprint distinguishable absorption troughs at widely separated velocities. ","On Synthetic Absorption Line Profiles of Thermally Driven Winds from
  Active Galactic Nuclei"
121,1370043452973547523,816316877551050752,Federico Vaccari,"['New working paper: Competition in Signaling\n\nLink: <LINK>\n\nQuestion: ‚ÄúHow and how much information is revealed when two equally informed senders with conflicting interests provide advice to a decision maker?‚Äù\n\nComments are welcome! (1/12) <LINK>', 'I study a model of strategic communication between two equally informed senders and an uninformed decision maker. Senders can misreport information at a cost that is tied to the size of the misrepresentation. (2/12)', 'The two senders first observe the realization of a random variable (the state) and then simultaneously or privately deliver a report to the decision maker. After observing the reports but not the state, the decision maker must select one of two alternatives. (3/12)', 'This setting is at the core of many applications in economics and politics: for example, candidates competing for consensus during an electoral campaign may provide voters with different accounts of the same events. (4/12)', 'Misreporting occurs in every equilibrium of this game, while babbling is never possible in equilibrium. Fully revealing and pure-strategy equilibria exist but are not plausible. This result motivates the quest for ‚Äúrobust‚Äù equilibria in mixed strategies. (5/12)', 'However, finding equilibria in this setting is a daunting task: signaling games have a wealth of equilibria, and there is no widely accepted way to refine equilibria in multi-sender signaling games. Mixed strategies on a continuum do not help. (6/12)', 'I identify sufficient and mild conditions under which the equilibria of this game are essentially unique, robust, and always exist. I dub equilibria that satisfy these conditions as ‚Äúdirect equilibria.‚Äù (7/12)', 'As a brief application, I study the informative value of judicial procedures and show that, when information is not fully verifiable, then inquisitorial systems may be superior to adversarial systems. This proves a conjecture by Shin (1998) to be correct. (8/12)', 'Introducing misreporting costs allows me to say something about the ‚Äúlanguage‚Äù used by senders to communicate, i.e., the map from what they know to what they say. (9/12)', 'I conclude by noting that the introduction of misreporting costs is not just a technical twist that adds an element of realism to the model, but it has a qualitative impact on information transmission. (10/12)', 'On April 2nd, I will make a brief presentation of this paper at APSA‚Äôs Virtual Formal Theory Workshop @FormalTheory. (11/12)', 'This paper is part of a project that has received funding from the EU‚Äôs Horizon 2020 Research and Innovation Programme (Marie Sk≈Çodowska-Curie grant no. 843315-PEMB. You can read more about my project at https://t.co/FrnXKsuqw2. (12/12)']",https://arxiv.org/abs/2103.05317,"I study a multi-sender signaling game between an uninformed decision maker and two senders with common private information and conflicting interests. Senders can misreport information at a cost that is tied to the size of the misrepresentation. The main results concern the amount of information that is transmitted in equilibrium and the language used by senders to convey such information. Fully revealing and pure-strategy equilibria exist but are not plausible. I first identify sufficient conditions under which equilibria are essentially unique, robust, and always exist, and then deliver a complete characterization of these equilibria. As an application, I study the informative value of different judicial procedures. ",Competition in Signaling
122,1369967331099688965,348355646,Victor See,"['New paper day from yours truly and ably assisted by Louis Amard, Julia Roquette and Sean Matt! We looked at the effect of metallicity on magnetic activity using photometric variability as an activity proxy: <LINK>', 'We compiled a sample of ~3000 Kepler stars for which we have masses, periods, variability amplitudes, metallicities and Rossby numbers for this work to test the proposal that, at a given mass and period, more metal-rich stars are more active.', 'The idea we were testing is that more metal-rich stars have deeper convection zones, longer convective turnover times and therefore smaller Rossby numbers leading to stronger activity. Spoiler alert: the data seems to match the theory! https://t.co/Ch74Vrw46y', 'We first showed that variability is well parameterised by Rossby number, showing an inverse relationship with Rossby number. We do note that there seems to be some additional structure in this activity-rotation relation compared to other activity proxies. https://t.co/H5ZqnWSMy0', 'We split our sample into bins of approx constant mass and period. Each panel/bin in this fig shows variability vs metallicity. This is quite the monster of a figure so you‚Äôre probably better off going directly to the paper for more details/ to see it properly‚Ä¶ https://t.co/jIxZb8eszy', ""This fig shows some of those bins in more detail. In general, more metal-rich stars do indeed have smaller Rossby numbers and larger variability (top 2 rows)! In some bins, the Rper-[Fe/H] trend isn't clear (bot 2 rows) which I won't get into here but we do discuss in the paper. https://t.co/EFN1lDAh0q"", ""Lastly, we looked at the detectability of rotation periods. It is generally easier to pull a period from a light curve when it has larger variability. Helpfully, the paper we adopted the periods from, McQuillan+14, also listed the stars where they couldn't detect a period."", 'We find that periods are more easily recovered in low-mass, metal-rich stars. This makes sense since lower mass stars are generally more active, and therefore more variable. Similarly, more metal-rich stars are generally more active as we have demonstrated in this paper. https://t.co/rzMh28jVRB', 'For more metallicty related things: recent @AWESoMeStarsERC work has also shown that metallicity can affect rotation evolution of low-mass stars (https://t.co/M6fFzBMGnA) and that this effect can even be seen in period distribution of the Kepler field (https://t.co/SjLVN1rdpQ)', '@travis_metcalfe Gah, definitely should have cited that paper in mine - always manage to miss literature in the course of writing a paper... Regarding your question, Reinhold+19 suggest that the local dip in variability is caused by cancellation between bright faculae and dark spots.', ""@travis_metcalfe I'm not sure I understand. Do you mean that for a given stellar mass, the rotation period at which a star transitions from spot to faculae dominated is metallicity dependent?""]",http://arxiv.org/abs/2103.05675,"Understanding how the magnetic activity of low-mass stars depends on their fundamental parameters is an important goal of stellar astrophysics. Previous studies show that activity levels are largely determined by the stellar Rossby number which is defined as the rotation period divided by the convective turnover time. However, we currently have little information on the role that chemical composition plays. In this work, we investigate how metallicity affects magnetic activity using photometric variability as an activity proxy. Similarly to other proxies, we demonstrate that the amplitude of photometric variability is well parameterised by the Rossby number, although in a more complex way. We also show that variability amplitude and metallicity are generally positively correlated. This trend can be understood in terms of the effect that metallicity has on stellar structure and, hence, the convective turnover time (or, equivalently, the Rossby number). Lastly, we demonstrate that the metallicity dependence of photometric variability results in a rotation period detection bias whereby the periods of metal-rich stars are more easily recovered for stars of a given mass. ","Photometric variability as a proxy for magnetic activity and its
  dependence on metallicity"
123,1369688157558431749,1253758756304809984,Igor Mordatch,"['What are the limits to the generalization of large pretrained transformer models?\n\nWe find minimal fine-tuning (~0.1% of params) performs as well as training from scratch on a completely new modality!\n\nwith @_kevinlu, @adityagrover_, @pabbeel\npaper: <LINK>\n\n1/8', 'We take pretrained GPT-2 and freeze the attention &amp; FF layers to obtain core of Frozen Pretrained Transformer (FPT).\n\nTo adapt to new modality &amp; task, we init *linear* input and output layers.\n\nDespite only training .1% of params, FPT matches performance of full transformer!\n\n2/8 https://t.co/1zjfCKm9rK', 'We visualize the attention maps of the frozen transformer.\n\nDespite not finetuning the self-attention layers on the new modality, FPT is able to learn to attend to the relevant bits to compute an elementwise XOR with perfect accuracy for sequences of length up to 256.\n\n3/8 https://t.co/tC1USSV4rS', 'Compared to a randomly initialized frozen transformer, pretraining with language (FPT) yields large compute benefits, showing that, much like common practice for in-modality finetuning, we can save computation by starting from a pretrained model:\n\n4/8 https://t.co/vmJlrBsfhg', 'What enables this transfer? We find that simply using a randomly initialized frozen transformer already greatly outperforms a randomly initialized frozen LSTM:\n\n5/8 https://t.co/17uMUZ1YO5', 'Additionally, by incorporating various sources of pretraining supervision, even a little pretraining, for example learning to memorize bits, can help transfer:\n\n6/8 https://t.co/KqOxlBgTY9', 'As we move from small specialist models to large generalist models, we‚Äôre excited by the potential for pretraining regimes that could train a universal computation engine.\n\nSimply adding more parameters and using a larger model already improves performance:\n\n7/8 https://t.co/sxtgfCUrGw', 'For more details, see our paper on arXiv or play our demo/code on Github!\n\narXiv: https://t.co/DtWGJ0Afh7\n\nGithub: https://t.co/9ts1FlqHFw\n\n8/8', ""@rndmcnlly @_kevinlu @adityagrover_ @pabbeel Indeed Adam! That was our starting hypothesis. One thing we're still not quite clear on is whether this FPT interpreter largely performs generic input summarization or more extensive computation.""]",http://arxiv.org/abs/2103.05247,"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks. ",Pretrained Transformers as Universal Computation Engines
124,1369685507173670914,157973000,Michael Pfarrhofer,"['New working paper with Martin Feldkircher, @FlorianHuber8 and Gary Koop on approximate Bayesian inference and forecasting in huge-dimensional multi-country VARs #econometrics #EconTwitter #research \n<LINK> <LINK>']",https://arxiv.org/abs/2103.04944,"Panel Vector Autoregressions (PVARs) are a popular tool for analyzing multi-country datasets. However, the number of estimated parameters can be enormous, leading to computational and statistical issues. In this paper, we develop fast Bayesian methods for estimating PVARs using integrated rotated Gaussian approximations. We exploit the fact that domestic information is often more important than international information and group the coefficients accordingly. Fast approximations are used to estimate the latter while the former are estimated with precision using Markov chain Monte Carlo techniques. We illustrate, using a huge model of the world economy, that it produces competitive forecasts quickly. ","Approximate Bayesian inference and forecasting in huge-dimensional
  multi-country VARs"
125,1369672742266691586,2518423562,Michael Fralick,"['New work led by the brilliant Elsa Riachi &amp; @SPOClab. It is a pre-print, so now would be a good time to get angry peer-reviewers to give their feedback! Crux of the paper is to shed light on a branch of machine learning known as Reinforcement Learning <LINK> 1/11', ""Wait a sec, what the heck is machine learning?! It has all sorts of complex definitions. I think of it is a tool-kit, and it can be broadly divided into supervised learning and unsupervised learning. Here's my slide on it: 2/11 https://t.co/37CyEMRueJ"", 'Too confusing? Think about movies. Maybe you want to create an algorithm that will predict who will win an Oscar in 2022? Awesome, feed the algorithm data from past Oscars so it can learn how the winners differ from the losers. This is supervised learning. 3/11 https://t.co/IbUE0rCqUY', 'We call it supervised learning, because the algorithm was supervised by knowing past winners and losers using old data from Oscars of years gone by. That is how it ""learned"". 4/11 https://t.co/iKxAoNcUt6', 'Maybe instead you want to understand the different genres of movies? Well there is no ""ground truth"" to genres. It is very subjective. Alas, there are common features between comedies [Mikey like] and romance [Mikey no like]. 5/11 https://t.co/aOCIG46FFT', ""You can employ unsupervised learning methods to UNDERSTAND your data. And see if there are different clusters/subgroups/categories [genres]. of course, you don't know for certainty whether Good Will Hunting was a rom-com vs drama 6/ 11 https://t.co/9HeFIlwY1f"", ""I'm sorry to say, but there is a 3rd category to machine learning: Reinforcement learning [RL]. RL is kinda like supervised learning, but instead it attempts to make a sequence of decisions rather than one single decision [think: sorcerer looking into a crystal ball] 7/11 https://t.co/gVb9dtukop"", 'Another important pearl to realize is that techniques for supervised learning (e.g. neural network) are also used for but RL... but there is an added complexity which involves reward functions. These are kinda complex so my vote would be to read the paper to learn more! 8/11 https://t.co/itXIgkreqT', 'One example we highlight is to use reinforcement learning to optimize sepsis management. Treatment can include abx, fluids, pressors, etc. We can define the reward function as minimizing the risk of death or minimizing subsequent SOFA scores 9/11 https://t.co/yLEoTyxHyi', ""The RL algorithm can help us to identify how changes in abx/fluids/pressors/etc will affect the patient's trajectory and how we can potentially pull these different levers to different amounts to optimize outcomes. 10/11 https://t.co/LkGb5BqseM"", 'Of course, there are all sorts of challenges to applying Reinforcement Learning in healthcare. Hence our paper!! https://t.co/WCHsEphM7x 11/11']",https://arxiv.org/abs/2103.05612,"Many healthcare decisions involve navigating through a multitude of treatment options in a sequential and iterative manner to find an optimal treatment pathway with the goal of an optimal patient outcome. Such optimization problems may be amenable to reinforcement learning. A reinforcement learning agent could be trained to provide treatment recommendations for physicians, acting as a decision support tool. However, a number of difficulties arise when using RL beyond benchmark environments, such as specifying the reward function, choosing an appropriate state representation and evaluating the learned policy. ",Challenges for Reinforcement Learning in Healthcare
126,1369671049751629830,3094610676,Pranav Rajpurkar,"['How do deep learning models perform in the presence of diseases not labeled for or present during training?\n\nOur new paper investigates this key AI+Medicine deployment consideration\n<LINK>\n\n@siyumd @IshaanMalhi \n@AndrewYNg  @StanfordAILab \n\n1/9 <LINK>', ""Context: Datasets used to train models typically only provide labels for a limited number of common diseases.\n\nIt's unknown whether DL models can maintain performance in presence of diseases not seen during training or whether they can detect the presence of such diseases.\n\n2/9"", 'Question 1: Can we detect diseases not seen during training?\n\nWe first design a controlled experiment to evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases.\n\n3/9', 'Finding 1: We find that models tend to falsely classify unseen diseases as ‚Äúno disease‚Äù.\n\nWe also show that DL models may succeed in identifying ‚Äúno disease‚Äù vs ‚Äúany disease‚Äù when an unseen disease co-occurs with a seen disease, but not when an unseen disease appears alone.\n\n4/9 https://t.co/ew02Mtbmv5', 'Question 2: Is there a performance drop on labeled diseases?\n\nWe evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases).\n\n5/9', 'Finding 2: We find that models are still able to detect seen diseases even when co-occurring with unseen diseases.\n\nMoreover, a model trained with both seen and unseen diseases, but without labels for the unseen diseases, performs better on seen diseases!\n\n6/9 https://t.co/KTomws6J1i', 'Question 3: Can unseen diseases be detected without explicit training? \n\nWe evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases.\n\n7/9', 'Finding 3: We find that the penultimate layer of the deep neural network provides useful features for unseen disease detection.\n\nOverall, our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes.\n\n8/9 https://t.co/7sBa3LFWjB', 'It was really fun working on this project with a talented team of first authors @siyumd and @IshaanMalhi, and Kevin Tran.\n\nRead more details in our paper here:\nhttps://t.co/wjOsxtf1Qk\n\n9/9']",https://arxiv.org/abs/2103.04590,"We systematically evaluate the performance of deep learning models in the presence of diseases not labeled for or present during training. First, we evaluate whether deep learning models trained on a subset of diseases (seen diseases) can detect the presence of any one of a larger set of diseases. We find that models tend to falsely classify diseases outside of the subset (unseen diseases) as ""no disease"". Second, we evaluate whether models trained on seen diseases can detect seen diseases when co-occurring with diseases outside the subset (unseen diseases). We find that models are still able to detect seen diseases even when co-occurring with unseen diseases. Third, we evaluate whether feature representations learned by models may be used to detect the presence of unseen diseases given a small labeled set of unseen diseases. We find that the penultimate layer of the deep neural network provides useful features for unseen disease detection. Our results can inform the safe clinical deployment of deep learning models trained on a non-exhaustive set of disease classes. ","CheXseen: Unseen Disease Detection for Deep Learning Interpretation of
  Chest X-rays"
127,1369658874421256196,281711973,Dr. Emily Rickman,['New paper day!üöÄ\n\nIn a study led by @RAsensioTorres we derive upper mass limits of companions in 15 systems that could be responsible for the structures seen in their protoplanetary disks with @ESO @SPHERE_outreach üí´‚ú®üî≠\n\nCheck out the paper here üëá\n<LINK> <LINK>'],https://arxiv.org/abs/2103.05377,"The detection of a wide range of substructures such as rings, cavities and spirals has become a common outcome of high spatial resolution imaging of protoplanetary disks, both in the near-infrared scattered light and in the thermal millimetre continuum emission. The most frequent interpretation of their origin is the presence of planetary-mass companions perturbing the gas and dust distribution in the disk (perturbers), but so far the only bona-fide detection has been the two giant planets around PDS 70. Here, we collect a sample of 15 protoplanetary disks showing substructures in SPHERE scattered light images and present a homogeneous derivation of planet detection limits in these systems. We also estimate the mass of these perturbers through a Hill radius prescription and a comparison to ALMA data. Assuming that one single planet carves each substructure in scattered light, we find that more massive perturbers are needed to create gaps within cavities than rings, and that we might be close to a detection in the cavities of RX J1604, RX J1615, Sz Cha, HD 135344B and HD 34282. We reach typical mass limits in these cavities of 3-10 Mjup. For planets in the gaps between rings, we find that the detection limits of SPHERE are about an order of magnitude away in mass, and that the gaps of PDS 66 and HD 97048 seem to be the most promising structures for planet searches. The proposed presence of massive planets causing spiral features in HD 135344B and HD 36112 are also within SPHERE's reach assuming hot-start models.These results suggest that current detection limits are able to detect hot-start planets in cavities, under the assumption that they are formed by a single perturber located at the centre of the cavity. More realistic planet mass constraints would help to clarify whether this is actually the case, which might point to perturbers not being the only way of creating substructures. ","Perturbers: SPHERE detection limits to planetary-mass companions in
  protoplanetary disks"
128,1369648729482010625,3335544083,Maximilian Ilse,"['New paper! Efficient Causal Inference from Combined Observational and Interventional Data through Causal Reductions! <LINK>! With Patrick Forr√©, @wellingmax, @JorisMooij! <LINK>', 'We introduce a causal reduction method that replaces arbitrary confounders with a single confounder that lives in the same space as the treatment variable, without changing the observational and interventional distributions entailed by the causal model!', 'For example, in the common case of one-dimensional treatment X and K-dimensional unobserved confounder Z, we reduce the confounder to 1D!', 'We propose a flexible parameterization of the reduced model using normalizing flows, which enables us to estimate the observational and interventional distributions by jointly learning from observational and interventional data without making strong parametric assumptions.', 'Last, we derive equality constraints between interventional and observational distributions entailed by linear Gaussian causal models.']",https://arxiv.org/abs/2103.04786,"Unobserved confounding is one of the main challenges when estimating causal effects. We propose a causal reduction method that, given a causal model, replaces an arbitrary number of possibly high-dimensional latent confounders with a single latent confounder that takes values in the same space as the treatment variable, without changing the observational and interventional distributions the causal model entails. This allows us to estimate the causal effect in a principled way from combined data without relying on the common but often unrealistic assumption that all confounders have been observed. We apply our causal reduction in three different settings. In the first setting, we assume the treatment and outcome to be discrete. The causal reduction then implies bounds between the observational and interventional distributions that can be exploited for estimation purposes. In certain cases with highly unbalanced observational samples, the accuracy of the causal effect estimate can be improved by incorporating observational data. Second, for continuous variables and assuming a linear-Gaussian model, we derive equality constraints for the parameters of the observational and interventional distributions. Third, for the general continuous setting (possibly nonlinear or non-Gaussian), we parameterize the reduced causal model using normalizing flows, a flexible class of easily invertible nonlinear transformations. We perform a series of experiments on synthetic data and find that in several cases the number of interventional samples can be reduced when adding observational training samples without sacrificing accuracy. ",Combining Interventional and Observational Data Using Causal Reductions
129,1369612165523644418,1077995761487568896,Jon Miller,['New paper day!  A hard look at relativistic reflection and reverberation in two key AGN.  Quick take: the corona is more complicated than we think.  Figure: the best-fit models for the time-avg. and lag spectrum (time-argh! spectrum) in Swift J2127.\n<LINK> <LINK>'],https://arxiv.org/abs/2103.04994,"X-ray reverberation mapping has emerged as a new tool to probe accretion in AGN, providing a potentially powerful probe of accretion at the black hole scale. The lags, along with relativistic spectral signatures are often interpreted in light of the lamp-post model. Focusing specifically on testing the prediction of the relativistic reverberation model, we have targeted several of the brightest Seyfert Galaxies in X-rays with different observing programs. Here, we report the results from two large campaigns with NuSATR targeting MCG-5-23-16 and SWIFT J2127.4+5654 to test the model predictions in the 3-50 keV band. These are two of three sources that showed indications of a delayed Compton hump in early data. With triple the previously analyzed exposures, we find no evidence for relativistic reverberation in MCG-5-23-16, and the energy-dependent lags are consistent with a log-linear continuum. In SWIFT J2127.4+5654, although a continuum-only model explains the data, the relativistic reverberation model provides a significant improvement to the energy and frequency-dependent lags, but with parameters that are not consistent with the time-averaged spectrum. This adds to mounting evidence showing that the lag data is not consistent with a static lamp-post model. ","A Hard Look At Relativistic Reverberation in MCG-5-23-16 & SWIFT
  J2127.4+5654: Testing the Lamp-Post Model"
130,1369455529064624131,780890274,Nathaniel Weir,"['New preprint! \n\nWe explore structurally-guided sentence generation using Frame Semantics. \n\nWith ugrad Jiefu Ou, @AVBelyy and @ben_vandurme\n\nPaper: <LINK>\nDemo: <LINK> <LINK>', 'We take advantage of FrameNet frame/lexical unit mappings (e.g. [Bringing] -&gt; {carry, bring, convey, haul,...}) to do lexically-constrained decoding of frame-triggering sentences.\n\nWe compare that strategy to just fine-tuning a GPT-2 on control codes. https://t.co/hTcAXPFc1X', 'We find the method to be quite broadly applicable to a number of different user-in-the-loop scenarios, including human/AI collaborative story writing, lexicalizing story plans, and semantically diverse generation. https://t.co/0uJ18kXwiW', '(@JefferyOU4! )']",https://arxiv.org/abs/2103.04941,"We propose a structured extension to bidirectional-context conditional language generation, or ""infilling,"" inspired by Frame Semantic theory (Fillmore, 1976). Guidance is provided through two approaches: (1) model fine-tuning, conditioning directly on observed symbolic frames, and (2) a novel extension to disjunctive lexically constrained decoding that leverages frame semantic lexical units. Automatic and human evaluations confirm that frame-guided generation allows for explicit manipulation of intended infill semantics, with minimal loss in distinguishability from human-generated text. Our methods flexibly apply to a variety of use scenarios, and we provide a codebase and interactive demo available from this https URL ",InFillmore: Frame-Guided Language Generation with Bidirectional Context
131,1369359280009224194,114562472,Prof. Emily Levesque ü§ì‚ú®üî≠üìö,"[""New paper alert! This SUPER-cool exploration of detecting gravitational wave signatures from Thorne-Zytkow objects (ü§Ø) was led by Lindsay DeMarchi (at @NUCIERA) along with @JaxYellsAtLaser and myself! It's now in press with ApJ; go check it out at <LINK> ü§©""]",https://arxiv.org/abs/2103.03887,"Thorne-\.Zytkow objects (T\.ZOs) are a class of stellar object comprised of a neutron star core surrounded by a large and diffuse envelope. Their exterior appearance is identical to red supergiants; the distinctive electromagnetic signature of a T\.ZO is a suite of unusual chemical abundance patterns, including excesses of Li, Rb, Mo, and Ca. However, electromagnetic observations cannot unambiguously identify the presence of a neutron star core. Detection of continuous gravitational wave emission from a rotating neutron star core would provide strong supporting evidence for the existence of T\.ZOs. We present a model for gravitational wave detector confirmation of T\.ZOs and demonstrate that these objects should be detectable with Advanced LIGO. We also investigate possible targets for joint optical and gravitational searches, and comment on prospects for detectability in both current and future gravitational wave detector networks. ",Prospects for Multimessenger Observations of Thorne-\.Zytkow Objects
132,1369319328378724360,1034887017421787138,Jacob Menick,"['The only interface an AR model supports is continuing a prefix. In our new work led by @charlietcnash , we train a strong autoregressive model where the ordering controls what tasks this continuation does zero-shot: colorization or superresolution.\n\npaper: <LINK> <LINK>', ""This is my own favourite aspect of the research but for more interesting bits, see Charlie's thread.\n\nhttps://t.co/rfPpA80qo8""]",http://arxiv.org/abs/2103.03841,"The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. ",Generating Images with Sparse Representations
133,1369257517348425728,3422471637,Elias Kammoun,"['New Paper Day! In this paper (<LINK>) we use our model of thermal reverberation to fi the UV/optical time-lag spectra of 7 AGN. Our model consists of a Novikov-Thorne disc illuminated by a point-like X-ray source. (1/4) <LINK>', 'Using this model, we could derive limits on the accretion rate of the black hole, and on the height of the X-ray source. (2/4) https://t.co/RqZOqpwnAC', 'We show that modeling the continuum UV/optical time-lags can be used to estimate the black hole spin, **when combined with results from multi-wavelength analysis of accretion rates of these sources** (3/4) https://t.co/8Vf4a2TBt4', 'Standard disc models fit well the time-lag spectra, contrary to previous models requiring larger discs. This is due to the fact that we consider GR-ray-tracing to estimate the disc response, leading to larger amplitude but consistent slope when compared to previous models. (4/4) https://t.co/IsFvoQqx8c']",https://arxiv.org/abs/2103.04892,"Thermal reverberation in accretion discs of active galactic nuclei is thought to be the reason of the continuum UV/optical time lags seen in these sources. Recently, we studied thermal reverberation of a standard Novikov-Thorne accretion disc illuminated by an X-ray point-like source, and we derived an analytic prescription for the time lags as function of wavelength. In this work, we use this analytic function to fit the time-lags spectra of seven Seyferts, that have been intensively monitored, in many wave-bands, in the last few years. We find that thermal reverberation can explain the observed UV/optical time lags in all these sources. Contrary to previous claims, the magnitude of the observed UV/optical time-lags is exactly as expected in the case of a standard accretion disc in the lamp-post geometry, given the black hole mass and the accretion rate estimates for the objects we study. We derive estimates of the disc accretion rates and corona height for a non-spinning and a maximally spinning black hole scenarios. We also find that the modelling of the continuum optical/UV time-lags can be used to estimate the black hole spin, when combined with additional information. We also find that the model under-predicts the observed X-ray to UV time-lags, but this difference is probably due to the broad X-ray auto-correlation function of these sources. ",Modeling the UV/optical continuum time-lags in AGN
134,1369203344229679106,869896064802934788,Jan Rybizki,['New paper led by @bucktobias6: We implement flexible yield tables into cosmological hydrodynamical simulations. The plot shows a variety of different yield table and GCE assumptions tested. We can now produce these for virtually all elements. Check it out: <LINK> <LINK>'],https://arxiv.org/abs/2103.03884,"With the advent of large spectroscopic surveys the amount of high quality chemo-dynamical data in the Milky Way (MW) increased tremendously. Accurately and correctly capturing and explaining the detailed features in the high-quality observational data is notoriously difficult for state-of-the-art numerical models. In order to keep up with the quantity and quality of observational datasets, improved prescriptions for galactic chemical evolution need to be incorporated into the simulations. Here we present a new, flexible, time resolved chemical enrichment model for cosmological simulations. Our model allows to easily change a number of stellar physics parameters such as the shape of the initial mass function (IMF), stellar lifetimes, chemical yields or SN Ia delay times. We implement our model into the Gasoline2 code and perform a series of cosmological simulations varying a number of key parameters, foremost evaluating different stellar yield sets for massive stars from the literature. We find that total metallicity, total iron abundance and gas phase oxygen abundance are robust predictions from different yield sets and in agreement with observational relations. On the other hand, individual element abundances, especially $\alpha$-elements show significant differences across different yield sets and none of our models can simultaneously match constraints on the dwarf and MW mass scale. This offers a unique way of observationally constraining model parameters. For MW mass galaxies we find for most yield tables tested in this work a bimodality in the $[\alpha$/Fe] vs. [Fe/H] plane of rather low intrinsic scatter potentially in tension with the observed abundance scatter. ","The challenge of simultaneously matching the observed diversity of
  chemical abundance patterns in cosmological hydrodynamical simulations"
135,1369132577114578945,954465907539152897,Dr. Doctor,"['üö® New paper! üö®\n\nWhen two black holes merge, what remains is a single leftover black hole.\n\n‚ö´Ô∏è+‚ö´Ô∏è=‚ö´Ô∏è \n\n@bffarr, @decohere, and I look at the ""demographics"" and number density of leftover black holes implied by LIGO/Virgo\'s gravitational wave detections:\n<LINK>', 'Using our knowledge of the demographics of the merging black hole pairs, we calculate the population properties of the leftover black holes, i.e. their ranges masses, spins, and final velocities.', 'To figure out the properties of a leftover black hole based on its two ""parent"" black holes, you need to use general relativity...well actually numerical relativity...well actually a surrogate model of numerical relativity simulations!', ""Here's a plot of the inferred distributions of remnant black hole masses, dimensionless spins, and kick speeds (left to right).  Just focus on the solid blue curves for now: those are approximately what these distributions look like. https://t.co/gXggVOwwOZ"", ""These distributions aren't anything surprising: they show that there are fewer big remnant black holes than small, the spins are peaked at 0.7, and there is a wide range of kick speeds.  This is what people roughly would have expected. We've quantified it with the latest data."", ""The other thing about these leftover black holes is that, unless they merge again, they'll stick around indefinitely (well, for many, many times longer than the current age of the universe). So we can count how many of these leftovers have accumulated since the Big Bang!"", ""We find that there are probably a few thousand leftover black holes in each galaxy. That's not a lot, esp if you're only looking in our own galaxy.  But if these leftovers merge with other black holes again within z~1, then we might be able detect them with gravitational waves."", 'Our work is a jumping off point to help people with searches for these leftover black holes, whether through gravitational waves or electromagnetically.  These leftovers definitely exist, but may be notoriously hard to detect. If we find one it would super freaking cool!!!', '@duetosymmetry I am aware now! I‚Äôll take a look at this, thank you!!']",https://arxiv.org/abs/2103.04001,"The inspiral and merger of two black holes produces a remnant black hole with mass and spin determined by the properties of its parent black holes. Using the inferred population properties of component black holes from the first two and a half observing runs of Advanced LIGO and Virgo, we calculate the population properties of the leftover remnant black holes. By integrating their rate of formation over the age of the universe, we estimate the number density of remnant black holes today. Using simple prescriptions for the cosmic star formation rate and black hole inspiral delay times, we determine the number density of this leftover black hole population to be $660_{-240}^{+440} \mathrm{Mpc}^{-3}$, corresponding to $\sim 60,000$ black hole remnants per Milky-Way-equivalent galaxy. The mass spectrum of these remnants starts at $\sim 10 M_\odot$ and can be approximated by a decreasing exponential with characteristic length $\sim 15 M_\odot$, the final spin distribution is sharply peaked at $\chi_f\sim0.7$, and the kick velocities range from tens to thousands of km/s. These kick velocities suggest that globular clusters and nuclear star clusters may retain up to $3_{-2}^{+3}\%$ and $46_{-15}^{+17}\%$ of their remnant black holes, respectively, while young star clusters would only retain a few tenths of a percent. The estimates in this work assume that none of the remnants participate in subsequent hierarchical mergers. If hierarchical mergers occur, the overall number density would drop accordingly and the remnant mass distribution shape would evolve over time. This population of leftover black holes is an inescapable result from gravitational-wave observations of binary black-hole mergers. ","Black Hole Leftovers: The Remnant Population from Binary Black Hole
  Mergers"
136,1369110409219698691,60893773,James Bullock,['Very excited!  New paper led by @UCIPhysAstro PhD student Anna (Sijie) Yu @AstroBananna  Uses FIRE sims of Milky-Way like galaxies show tight connection between star formation mode (bursty to steady) &amp; thick-to-thin disk formation. No mergers required! <LINK> <LINK>'],https://arxiv.org/abs/2103.03888,"We investigate thin and thick stellar disc formation in Milky-Way-mass galaxies using twelve FIRE-2 cosmological zoom-in simulations. All simulated galaxies experience an early period of bursty star formation that transitions to a late-time steady phase of near-constant star formation. Stars formed during the late-time steady phase have more circular orbits and thin-disc-like morphology at $z=0$, whilst stars born during the bursty phase have more radial orbits and thick-disc structure. The median age of thick-disc stars at $z=0$ correlates strongly with this transition time. We also find that galaxies with an earlier transition from bursty to steady star formation have a higher thin-disc fractions at $z=0$. Three of our systems have minor mergers with LMC-size satellites during the thin-disc phase. These mergers trigger short starbursts but do not destroy the thin disc nor alter broad trends between the star formation transition time and thin/thick disc properties. If our simulations are representative of the Universe, then stellar archaeological studies of the Milky Way (or M31) provide a window into past star-formation modes in the Galaxy. Current age estimates of the Galactic thick disc would suggest that the Milky Way transitioned from bursty to steady phase $\sim$6.5 Gyr ago; prior to that time the Milky Way likely lacked a recognisable thin disc. ",The bursty origin of the Milky Way thick disc
137,1369108886494482432,1169073912,Anna Yu,"[""It's paper dayüôå!! Our new paper (w/ @jbprime, @astro_klein, Jonathan Stern, and collaborators) finds this interesting connection between the early time bursty star formation and the formation of the galactic thick disk: <LINK> <LINK>"", 'The time when star formation transitions from bursty to steady correlates strongly with the age of the thick disk (both median age and t90) --&gt; The observations of the Milky Way thick disk thus suggest that it probably had this transition from bursty to steady ~6.5Gyr agoüëÄ https://t.co/kiOOsAWp9d', 'Big thanks to all of our collaboratorsü§ó: @AndrewWetzel, Xiangcheng Ma, @jorgito__moreno, Zach Hafen, @alexbgurvich, @PFHopkins_Astro, Du≈°an Kere≈°, Claude-Andr√© Faucher-Gigu√®r, Robert Feldmann, and Eliot Quataert!! This is without doubt team effort!']",https://arxiv.org/abs/2103.03888,"We investigate thin and thick stellar disc formation in Milky-Way-mass galaxies using twelve FIRE-2 cosmological zoom-in simulations. All simulated galaxies experience an early period of bursty star formation that transitions to a late-time steady phase of near-constant star formation. Stars formed during the late-time steady phase have more circular orbits and thin-disc-like morphology at $z=0$, whilst stars born during the bursty phase have more radial orbits and thick-disc structure. The median age of thick-disc stars at $z=0$ correlates strongly with this transition time. We also find that galaxies with an earlier transition from bursty to steady star formation have a higher thin-disc fractions at $z=0$. Three of our systems have minor mergers with LMC-size satellites during the thin-disc phase. These mergers trigger short starbursts but do not destroy the thin disc nor alter broad trends between the star formation transition time and thin/thick disc properties. If our simulations are representative of the Universe, then stellar archaeological studies of the Milky Way (or M31) provide a window into past star-formation modes in the Galaxy. Current age estimates of the Galactic thick disc would suggest that the Milky Way transitioned from bursty to steady phase $\sim$6.5 Gyr ago; prior to that time the Milky Way likely lacked a recognisable thin disc. ",The bursty origin of the Milky Way thick disc
138,1369042014084296705,1276310243123720192,Daniel Filan research-tweets,"['New paper is up about about clusterability in neural networks, authored by myself, Shlomi Hod, @StephenLCasper, @decodyng, Andrew Critch, and Stuart Russell! Link to paper: <LINK> (1/9) <LINK>', ""An old version with a somewhat more clickbaity title has been online for a while, but this is a more final version that I feel comfortable publicly promoting. So what's in the paper? (2/9)"", ""We're interested in dividing the neurons of neural nets into groups such that there's a lot of connection within the groups, but not much between the groups. We do this with graph clustering algorithms, and call the groups 'clusters'. (3/9)"", ""(The hope is that if you can do this well, then one day you'll be able to analyze neural net structure group-wise to learn  facts about the network that don't depend on knowledge of the deployment distribution. But that's pretty far away...) (4/9)"", 'We find that in many conditions, if you train neural networks with pruning and dropout, basically every net you train will be significantly more clusterable than a random neural network with the same distribution of weights. (5/9)', 'This is true for small MLPs and VGG-scale CNNs trained on various image classification tasks (but not small CNNs that we train on MNIST and Fashion-MNIST üò≥). (6/9)', 'We also find that when we cluster big CNNs other people have trained for ImageNet classification (some ResNets, some VGGs, and Inception-v3), they also are more clusterable than if their weights were arranged randomly. (7/9)', 'Another thing we do is figure out ways of changing the training process to produce nets that are more neatly divisible. We find that regularization manages to succeed at doing this with little cost in accuracy for MLPs! (8/9)', ""The paper has a bunch of stuff to dig into. If you want to look at code, it's available here: https://t.co/SBOG603OHy\n\nThanks to my co-authors, and the support of folks at @CHAI_Berkeley, for helping make this paper real! (9/9)"", '@robpmcadam @StephenLCasper @decodyng Nope. But I think some co-authors are interested in that.']",https://arxiv.org/abs/2103.03386,"The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters. ",Clusterability in Neural Networks
139,1369024530732187650,725121185952976896,Ilenna Jones,"['In our previous paper, we show that a neuron model with dendrites can do interesting machine learning problems. But is this still the case if we add biological constraints? Check out our new paper ‚ÄúDo biological constraints impair dendritic computation?‚Äù  <LINK> <LINK>', 'We use an ANN with a binary tree structure constraint so as to model dendritic structure. This causes the weight parameters to be analogous to axial resistances between nodal dendritic compartments. What if we constrained them to be non-negative, like resistances? https://t.co/DD5BsflPgN', 'Biological dendrites have voltage dependent conductances that are the basis for dendritic nonlinearities. Following deep learning model convention, our previous model uses leaky ReLU nonlinearities. What if we used voltage-gated ion channel nonlinearities instead? https://t.co/UfQh5RFcAI', 'Synaptic response is conductance-based, gated by ligands from presynaptic vesicles, and relies on the number of receptors which correlates with the size of the postsynaptic bouton. Can we map a single pixel input to have a synaptic response output in our model? https://t.co/d6hq6Jtg7q', 'Here we simulate models of dendritic computation with and without these biological constraints. https://t.co/Ko7a5WaZ4R', 'Upon comparing a variety of nonlinearities to our dendrite voltage gated conductance derived (NaCaK) function, we found that the NaCaK function turns out to be a better nonlinearity for dendritic binary tree structures. (NaCaK in grey, interestingly SWISH in green) https://t.co/sYj4HZ5ZPy', 'We added the synapse and non-negative weight constraints and found that dendritic model performance on interesting machine learning tasks is not hurt by these constraints, and may even benefit from them. (All constraints in Orange, best seen in top rows) https://t.co/crRAC9FPJn', 'The results from our model suggest that single real dendritic trees may be able to learn a surprisingly broad range of tasks! https://t.co/qjKGWLYfhq', ""Lastly and very importantly, here's our code: https://t.co/1pNal3Ccu2""]",https://arxiv.org/abs/2103.03274,"Computations on the dendritic trees of neurons have important constraints. Voltage dependent conductances in dendrites are not similar to arbitrary direct-current generation, they are the basis for dendritic nonlinearities and they do not allow converting positive currents into negative currents. While it has been speculated that the dendritic tree of a neuron can be seen as a multi-layer neural network and it has been shown that such an architecture could be computationally strong, we do not know if that computational strength is preserved under these biological constraints. Here we simulate models of dendritic computation with and without these constraints. We find that dendritic model performance on interesting machine learning tasks is not hurt by these constraints but may benefit from them. Our results suggest that single real dendritic trees may be able to learn a surprisingly broad range of tasks. ",Do biological constraints impair dendritic computation?
140,1369000539371999235,835789494,Charlie Nash,"[""Excited to release our new paper 'Generating Images with Sparse Representations'  (<LINK>, @jacobmenick @sedielem @PeterWBattaglia)\n\nOur model picks where to place content in an image, and what content to place there (see vid).\n\nThread for more info: <LINK>"", 'Likelihood-based models cover the data-distribution, and produce highly diverse samples. But they struggle with high-dimensional data like images, audio and video. \n\nMethods like VQ-VAE produce compressed data representations, that are more manageable for generative models. https://t.co/kZl4bLMcl6', 'We use a similar approach, but instead of applying neural lossy compression, we use DCT-based representations inspired by ubiquitous lossy compression methods like JPEG.\n\n(Image credit Felis_silvestris_silvestris.jpg: Michael G√§bler) https://t.co/eHM4JOiDqf', 'Following JPEG, we apply the DCT to pixel blocks and quantize the data, yielding data blocks with a large number of zero components.\n\nImages can be represented using a sparse data structure: Lists of (DCT component, spatial position, DCT value) tuples for the non-zero elements. https://t.co/VQ94q9jhfh', 'This yields sequences that are substantially more compact than dense representations. \n\nThey are also variable size, as images with less high frequency content can be represented with fewer bits. https://t.co/l8tU8rzsPB', ""We design a Transformer-based architecture 'DCTransformer' to autoregressively model the sparse DCT sequences.\n\nThe model is trained on chunks sampled from the DCT sequences, and so it can easily applied to high-resolution images https://t.co/ycX5FNkQyY"", 'We find our model can generate high-resolution images (including plant leaves at 2048 long-side resolution), while maintaining a high level of global coherence. https://t.co/6DbZ8xvoJ9', 'On ImageNet we find that our model produces highly diverse samples, although coherence / FID is in general a bit worse than BigGAN (see paper for more details) https://t.co/fVqtUuGuYp', ""Here's a selection of highly curated samples from our ImageNet model that caught my eye! https://t.co/THXTRMa1ix"", 'https://t.co/5SoSJCGAnQ', ""Including a distant cousin of @ajmooch's dogball... introducing dogtruck https://t.co/S8tBTmMcw7"", 'If we condition on the early stages of the DCT sequence, we naturally yield an upsampling model. And if we condition on grayscale image components during training we get an effective colorization model https://t.co/U80leqaCa0', 'Overall, I think there is much to be gained from exploring the data\nrepresentations used in classical data compression methods as a basis for generative modelling. \n\nAudio / video next...?', ""@msalbergo We get a likelihood on the sparse dct sequences, but it's not equivalent to a likelihood on pixels due to the lossy compression"", ""@CarstenDitzel @jacobmenick @sedielem @PeterWBattaglia Nice idea, we haven't tried this no. I think there are definitely some good options to explore. Another one is to use larger variable quadtree coding blocks as in HEVC. For this project we were aiming for simplicity so stuck the JPEG pipeline!"", ""@Zergfriend @jacobmenick @sedielem @PeterWBattaglia Not currently, we'll see if we can get a link to one"", '@Zergfriend @ak92501 @jacobmenick @sedielem @PeterWBattaglia I agree with that intuition, and it seems to be a feature of successful likelihood based models (VQ-VAE2, SPN). It would be interesting to train using a different ordering to confirm']",https://arxiv.org/abs/2103.03841,"The high dimensionality of images presents architecture and sampling-efficiency challenges for likelihood-based generative models. Previous approaches such as VQ-VAE use deep autoencoders to obtain compact representations, which are more practical as inputs for likelihood-based models. We present an alternative approach, inspired by common image compression methods like JPEG, and convert images to quantized discrete cosine transform (DCT) blocks, which are represented sparsely as a sequence of DCT channel, spatial location, and DCT coefficient triples. We propose a Transformer-based autoregressive architecture, which is trained to sequentially predict the conditional distribution of the next element in such sequences, and which scales effectively to high resolution images. On a range of image datasets, we demonstrate that our approach can generate high quality, diverse images, with sample metric scores competitive with state of the art methods. We additionally show that simple modifications to our method yield effective image colorization and super-resolution models. ",Generating Images with Sparse Representations
141,1368990731541434371,809495909356572672,Sai Vemprala,"['Check out our new work on event cameras + VAEs! We show that representation learning is feasible with fast asynchronous data, and allows event cameras to be used in perception-action loops. w/ @stmian17, @akapoor_av8r.\n\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/2103.00806,"Event-based cameras are dynamic vision sensors that provide asynchronous measurements of changes in per-pixel brightness at a microsecond level. This makes them significantly faster than conventional frame-based cameras, and an appealing choice for high-speed navigation. While an interesting sensor modality, this asynchronously streamed event data poses a challenge for machine learning techniques that are more suited for frame-based data. In this paper, we present an event variational autoencoder and show that it is feasible to learn compact representations directly from asynchronous spatiotemporal event data. Furthermore, we show that such pretrained representations can be used for event-based reinforcement learning instead of end-to-end reward driven perception. We validate this framework of learning event-based visuomotor policies by applying it to an obstacle avoidance scenario in simulation. Compared to techniques that treat event data as images, we show that representations learnt from event streams result in faster policy training, adapt to different control capacities, and demonstrate a higher degree of robustness. ",Representation Learning for Event-based Visuomotor Policies
142,1368849730474754048,493582529,Michele Lucente üá∫üá¶,"['New paper out! <LINK>\n\nI show that an overlooked production mechanism within the minimal Type-I Seesaw model can account for the observed dark matter abundance in the form of a keV sterile neutrino.\n\n@TTK_RWTH @RWTH @AvHStiftung @UCLouvain_be \n\n1/3', 'This population can be produced by the decay of the heavier neutral leptons, with masses above the Higgs mass scale, while they are in thermal equilibrium in the early Universe (freeze-in).\n\n2/3 https://t.co/gl1GQ41SoM', 'Moreover, the implementation of the relevant phenomenological constraints (relic abundance, indirect detection and structure formation) on this model automatically selects a region of the parameter space featuring an approximate lepton number symmetry!\n\n3/3 https://t.co/4g1UHJwbMx']",https://arxiv.org/abs/2103.03253,"We show that the minimal Type-I Seesaw mechanism can successfully account for the observed dark matter abundance in the form of a keV sterile neutrino. This population can be produced by the decay of the heavier neutral leptons, with masses above the Higgs mass scale, while they are in thermal equilibrium in the early Universe (freeze-in). Moreover, the implementation of the relevant phenomenological constraints (relic abundance, indirect detection and structure formation) on this model automatically selects a region of the parameter space featuring an approximate lepton number symmetry. ",Freeze-In Dark Matter within the Seesaw mechanism
143,1368841526114156548,1139943231922331652,Chandreyee Maitra,"['Good morning New paper time today. Read our paper on \nDiscovery of a new Be X-ray binary pulsar likely associated with the supernova remnant MCSNR J0507‚àí6847 in the Large Magellanic Cloud using @ESA_XMM  with @ManamiSasakiDE @MFilipovic23 \n<LINK> <LINK>', 'Estimated age of the SNR is 43‚Äì63 kyr yr which points to a middle aged to old remnant. The large diameter combined with the lack of distinctive shell counterparts in optical and radio indicates that the SNR is expanding into the tenous environment of the superbubble N103.', 'Pulsations with a periodicity of 570s are\ndiscovered from the Be X-ray binary XMMU J050722.1‚àí684758 confirming its nature as a\nHMXB pulsar. The HMXB is located near the geometric centre of MCSNR J0507‚àí6847 (0.9 from the centre) which supports the XRB-SNR association.', 'The above shows the XMM image. In in blue are radio contours from ASKAP image of the LMC. In green overlays the Hùõº image contours from the MCELS. The cyan cross shows the best-fit centre of the SNR and the magenta cross the position of the optical counterpart of the BeXRB.', 'The estimated magnetic field strength of the neutron star is ùêµ \U0010fc01 &gt; 10^14 G assuming a spin equilibrium condition which is expected from the estimated age of the parent remnant and assuming that the measured mass-accretion rate remained constant throughout.']",https://arxiv.org/abs/2103.03657,"We report the discovery of a new high mass X-ray binary pulsar, XMMU J050722.1-684758, possibly associated with the supernova remnant MCSNR J0507-6847 in the Large Magellanic Cloud, using XMM-Newton X-ray observations. Pulsations with a periodicity of 570 s are discovered from the Be X-ray binary XMMU J050722.1-684758 confirming its nature as a HMXB pulsar. The HMXB is located near the geometric centre of the supernova remnant MCSNR J0507-6847 (0.9 arcmin from the centre) which supports the XRB-SNR association. The estimated age of the supernova remnant is 43-63 kyr which points to a middle aged to old supernova remnant. The large diameter of the supernova remnant combined with the lack of distinctive shell counterparts in optical and radio indicates that the SNR is expanding into the tenous environment of the superbubble N103. The estimated magnetic field strength of the neutron star is $B\gtrsim10^{14}$ G assuming a spin equilibrium condition which is expected from the estimated age of the parent remnant and assuming that the measured mass-accretion rate remained constant throughout. ","XMMU J050722.1-684758: Discovery of a new Be X-ray binary pulsar likely
  associated with the supernova remnant MCSNR J0507-6847"
144,1368777389619113985,812446766,Archer Gong Zhang,"['Check our new paper on a recent development of the semi-parametric density ratio model: <LINK>, an efficient model to analyze data from multiple resources that have some connections and similarities!']",https://arxiv.org/abs/2103.03445,"In many applications, we collect independent samples from interconnected populations. These population distributions share some latent structure, so it is advantageous to jointly analyze the samples. One effective way to connect the distributions is the semiparametric density ratio model (DRM). A key ingredient in the DRM is that the log density ratios are linear combinations of prespecified functions; the vector formed by these functions is called the basis function. A sensible basis function can often be chosen based on knowledge of the context, and DRM-based inference is effective even if the basis function is imperfect. However, a data-adaptive approach to the choice of basis function remains an interesting and important research problem. We propose an approach based on the classical functional principal component analysis (FPCA). Under some conditions, we show that this approach leads to consistent basis function estimation. Our simulation results show that the proposed adaptive choice leads to an efficiency gain. We use a real-data example to demonstrate the efficiency gain and the ease of our approach. ",Density ratio model with data-adaptive basis function
145,1368761254748168194,1049297982170968065,Yutaka Hori,['<LINK>\nOur new work on machine learning in feedback control is now on arXiv. The paper presents the use of a linear quasi-optimal controller to assist the learning of nonlinear optimal regulator using RL. So many thanks to the great collaborators at Fujitsu Lab.'],https://arxiv.org/abs/2103.03808,"Reinforcement learning (RL) provides a model-free approach to designing an optimal controller for nonlinear dynamical systems. However, the learning process requires a considerable number of trial-and-error experiments using the poorly controlled system, and accumulates wear and tear on the plant. Thus, it is desirable to maintain some degree of control performance during the learning process. In this paper, we propose a model-free two-step design approach to improve the transient learning performance of RL in an optimal regulator design problem for unknown nonlinear systems. Specifically, a linear control law pre-designed in a model-free manner is used in parallel with online RL to ensure a certain level of performance at the early stage of learning. Numerical simulations show that the proposed method improves the transient learning performance and efficiency in hyperparameter tuning of RL. ","Model-free two-step design for improving transient learning performance
  in nonlinear optimal regulator problems"
146,1368100329187803138,943007557098115077,Marvin Chanc√°n,"[""New research paper! Inspired by biology üß† we built a deep learning toolbox for accurate robot navigation ü§ñ even in darkness üî• while setting new state-of-the-art performance standards that classical methods (used for ~10yrs) can't attain! <LINK> w/ @maththrills <LINK> <LINK>""]",http://arxiv.org/abs/2103.02074,"Sequential matching using hand-crafted heuristics has been standard practice in route-based place recognition for enhancing pairwise similarity results for nearly a decade. However, precision-recall performance of these algorithms dramatically degrades when searching on short temporal window (TW) lengths, while demanding high compute and storage costs on large robotic datasets for autonomous navigation research. Here, influenced by biological systems that robustly navigate spacetime scales even without vision, we develop a joint visual and positional representation learning technique, via a sequential process, and design a learning-based CNN+LSTM architecture, trainable via backpropagation through time, for viewpoint- and appearance-invariant place recognition. Our approach, Sequential Place Learning (SPL), is based on a CNN function that visually encodes an environment from a single traversal, thus reducing storage capacity, while an LSTM temporally fuses each visual embedding with corresponding positional data -- obtained from any source of motion estimation -- for direct sequential inference. Contrary to classical two-stage pipelines, e.g., match-then-temporally-filter, our network directly eliminates false-positive rates while jointly learning sequence matching from a single monocular image sequence, even using short TWs. Hence, we demonstrate that our model outperforms 15 classical methods while setting new state-of-the-art performance standards on 4 challenging benchmark datasets, where one of them can be considered solved with recall rates of 100% at 100% precision, correctly matching all places under extreme sunlight-darkness changes. In addition, we show that SPL can be up to 70x faster to deploy than classical methods on a 729 km route comprising 35,768 consecutive frames. Extensive experiments demonstrate the... Baseline code available at this https URL ","Sequential Place Learning: Heuristic-Free High-Performance Long-Term
  Place Recognition"
147,1367920278605598727,429863384,Sarath Chandar,"[""Are you tired of manually creating new tasks for Lifelong RL? We introduce Lifelong Hanabi in which every task is coordinating with a partner that's an expert player of Hanabi. Work led by @HadiNekoei and @akileshbadri.\npaper: <LINK>  @Mila_Quebec  1/n <LINK>"", 'The large strategy space of Hanabi facilitates generating a diverse set of partners ideal for designing LLL tasks. The Cross-play matrix shows how these partners (tasks) are related to each other, a feature not commonly found in existing LLL benchmarks. 2/n https://t.co/xyKqY50TaU', ""We also show that a simple IQL agent trained continually in our setup can coordinate well with unseen agents in Hanabi, without having access to its partner's policies or symmetries in the game. 3/n"", 'Working on a new LLL algorithm and want to benchmark its effectiveness? Make sure to try it on Lifelong Hanabi! Code to reproduce all experiments, pre-trained partners available https://t.co/9bbuO3F4c7. Reach out to us for more questions! n/n']",https://arxiv.org/abs/2103.03216,"Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple tasks sequentially by efficiently transferring and using knowledge between tasks. Despite a surge of interest in lifelong RL in recent years, the lack of a realistic testbed makes robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the other hand, can be seen as a natural scenario for lifelong RL due to its inherent non-stationarity, since the agents' policies change over time. In this work, we introduce a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings. Our setup is based on Hanabi -- a partially-observable, fully cooperative multi-agent game that has been shown to be challenging for zero-shot coordination. Its large strategy space makes it a desirable environment for lifelong RL tasks. We evaluate several recent MARL methods, and benchmark state-of-the-art LLL algorithms in limited memory and computation regimes to shed light on their strengths and weaknesses. This continual learning paradigm also provides us with a pragmatic way of going beyond centralized training which is the most commonly used training protocol in MARL. We empirically show that the agents trained in our setup are able to coordinate well with unseen agents, without any additional assumptions made by previous works. The code and all pre-trained models are available at this https URL ",Continuous Coordination As a Realistic Scenario for Lifelong Learning
148,1367885931768807424,759894532649545732,Aravind Srinivas,"['New paper, SEER,  improving both compute and memory efficiency of pixel-based RL.\n\nUsing two simple ideas: \n(1) Freeze lower layers of CNN encoders early on in training; (2) Store latents in replay buffer instead of pixels.\n\nüéì<LINK>\nüíª<LINK> <LINK>']",http://arxiv.org/abs/2103.02886,"Recent advances in off-policy deep reinforcement learning (RL) have led to impressive success in complex tasks from visual observations. Experience replay improves sample-efficiency by reusing experiences from the past, and convolutional neural networks (CNNs) process high-dimensional inputs effectively. However, such techniques demand high memory and computational bandwidth. In this paper, we present Stored Embeddings for Efficient Reinforcement Learning (SEER), a simple modification of existing off-policy RL methods, to address these computational and memory requirements. To reduce the computational overhead of gradient updates in CNNs, we freeze the lower layers of CNN encoders early in training due to early convergence of their parameters. Additionally, we reduce memory requirements by storing the low-dimensional latent vectors for experience replay instead of high-dimensional images, enabling an adaptive increase in the replay buffer capacity, a useful technique in constrained-memory settings. In our experiments, we show that SEER does not degrade the performance of RL agents while significantly saving computation and memory across a diverse set of DeepMind Control environments and Atari games. ","Improving Computational Efficiency in Visual Reinforcement Learning via
  Stored Embeddings"
149,1367871906293112834,913810182332968961,Dr. Jeanne Clelland,['New paper up on arXiv today! This is based on work that my REU group did in summer 2019 on trying to understand compactness properties of the ReCom algorithm for sampling districting plans: <LINK>'],https://arxiv.org/abs/2103.02699,"Ensemble analysis has become an important tool for quantifying gerrymandering; the main idea is to generate a large, random sample of districting plans (an ""ensemble"") to which any proposed plan may be compared. If a proposed plan is an extreme outlier compared to the ensemble with regard to various redistricting criteria, this may indicate that the plan was deliberately engineered to produce a specific outcome. Many methods have been used to construct ensembles, and a fundamental question that arises is: Given a method for constructing plans, can we identify a probability distribution on the space of plans that describes the probability of constructing any particular plan by that method? Recently, MCMC methods have become a predominant tool for constructing ensembles. Here we focus on the MCMC method known as ""ReCom,"" which was introduced in 2018 by the MGGG Redistricting Lab. ReCom tends to produce plans with more compact districts than some other methods, and we sought to better understand this phenomenon. We adopted a discrete analog of district perimeter called ""cut edges"" as a quantitative measure for district compactness; this measure was proposed by Duchin and Tenner, and it avoids some of the difficulties associated with compactness measures based on geographic perimeter, such as the Polsby-Popper score. To model the basic ReCom step, we constructed ensembles of 2-district plans for two grid graphs and for the precinct graph of Boulder County, CO. We found that the probability of sampling any particular plan -- which is roughly proportional to the product of the numbers of spanning trees for each of the two districts -- is also approximately proportional to an exponentially decaying function of the number of cut edges in the plan. This is an important step towards understanding compactness properties for districting plans produced by the ReCom method. ",Compactness statistics for spanning tree recombination
150,1367841112786894850,992203267093549057,Sebastian Gomez,"[""It's paper day! We have a new weird and luminous supernova. Is it a Type Ic supernova or a superluminous supernova? Por qu√© no los dos? <LINK>"", '@pkgw https://t.co/ix0lmdz49T']",https://arxiv.org/abs/2103.02611,"We present optical photometry and spectroscopy of SN\,2019stc (=ZTF19acbonaa), an unusual Type Ic supernova (SN Ic) at a redshift of $z=0.117$. SN\,2019stc exhibits a broad double-peaked light curve, with the first peak having an absolute magnitude of $M_r=-20.0$ mag, and the second peak, about 80 rest-frame days later, $M_r=-19.2$ mag. The total radiated energy is large, $E_{\rm rad}\approx 2.5\times 10^{50}$ erg. Despite its large luminosity, approaching those of Type I superluminous supernovae (SLSNe), SN\,2019stc exhibits a typical SN Ic spectrum, bridging the gap between SLSNe and SNe Ic. The spectra indicate the presence of Fe-peak elements, but modeling of the first light curve peak with radioactive heating alone leads to an unusually high nickel mass fraction of $f_{\rm Ni}\approx 31\%$ ($M_{\rm Ni}\approx 3.2$ M$_\odot$). Instead, if we model the first peak with a combined magnetar spin-down and radioactive heating model we find a better match with $M_{\rm ej}\approx 4$ M$_\odot$, a magnetar spin period of $P_{\rm spin}\approx 7.2$ ms and magnetic field of $B\approx 10^{14}$ G, and $f_{\rm Ni}\lesssim 0.2$ (consistent with SNe Ic). The prominent second peak cannot be naturally accommodated with radioactive heating or magnetar spin-down, but instead can be explained as circumstellar interaction with $\approx 0.7$ $M_\odot$ of hydrogen-free material located $\approx 400$ AU from the progenitor. Including the remnant mass leads to a CO core mass prior to explosion of $\approx 6.5$ M$_\odot$. The host galaxy has a metallicity of $\approx 0.26$ Z$_\odot$, low for SNe Ic but consistent with SLSNe. Overall, we find that SN\,2019stc is a transition object between normal SNe Ic and SLSNe. ","The Luminous and Double-Peaked Type Ic Supernova 2019stc: Evidence for
  Multiple Energy Sources"
151,1367839467000758272,621980734,Piotr Tomasz Makowski,"['my new paper on the impact of #MachineLearning and #AI on #innovation management coauthored with Yuya Kajikawa, forthcoming in Technological Forecasting and Social Change <LINK> @tokyotech_en @ElsevierConnect']",https://arxiv.org/abs/2103.02395,"There is a resurging interest in automation because of rapid progress of machine learning and AI. In our perspective, innovation is not an exemption from their expansion. This situation gives us an opportunity to reflect on a direction of future innovation studies. In this conceptual paper, we propose a framework of innovation process by exploiting the concept of unit process. Deploying it in the context of automation, we indicate the important aspects of innovation process, i.e. human, organizational, and social factors. We also highlight the cognitive and interactive underpinnings at micro- and macro-levels of the process. We propose to embrace all those factors in what we call Innovation-Automation-Strategy cycle (IAS). Implications of IAS for future research are also put forward. Keywords: innovation, automation of innovation, unit process, innovation-automation-strategy cycle ","Automation-driven innovation management? Toward
  Innovation-Automation-Strategy cycle"
152,1367838286081794057,3079023467,Dr. Emma Beasor,"['New paper üéâ what started out as a quick ‚Äúthese stars aren‚Äôt as bright as I expected‚Äù type of paper turned into a much more in-depth look at this weird cluster. Summary to follow... <LINK>', 'Westerlund 1 is a nearby star cluster which contains a whole bunch of massive stars, including red/yellow/blue supergiants, Wolf-Rayets, a luminous blue variable and a magnetar....', 'If we assume single stellar evolution, there‚Äôs only a reeeeally short time frame where these objects could co-exist, and it‚Äôd make the cluster very young (5Myr). This would also make the red supergiants sit right at the empirical boundary for RSG luminosity...', 'But when we measured their luminosities directly using new data from @SOFIAtelescope , they‚Äôre much fainter than we would expect for a 5Myr cluster... ü§î same for the yellow supergiants, too faint...', 'After taking a closer look at the other age estimates in the literature, we find that it‚Äôs just not possible to explain Westerlund 1 with a single age, from either binary or single stellar evolution models...', 'The red supergiants suggest an age of ~10Myr, but there‚Äôs one eclipsing binary that suggests an age of ~5 Myr... so instead we suggest that Westerlund 1 is not a single age starburst cluster, and that the stars within it formed over a period of several Myr. Enjoy! https://t.co/3c2rxAGx9J', ""@Jos_de_Bruijne haha, it's all relative!"", '@astro_jje Thank you! And of course no problem üòä', '@jendrews Thanks mate! I‚Äôll be talking about it at bigboom this week', '@AstroKirsten Thank you! It was a really fun paper üòä']",https://arxiv.org/abs/2103.02609,"The cluster Westerlund~1 (Wd1) is host to a large variety of post main-sequence (MS) massive stars. The simultaneous presence of these stars can only be explained by stellar models if the cluster has a finely-tuned age of 4-5Myr, with several published studies independently claiming ages within this range. At this age, stellar models predict that the cool supergiants (CSGs) should have luminosities of $\log(L/L_\odot) \approx 5.5$, close to the empirical luminosity limit. Here, we test that prediction using archival data and new photometry from SOFIA to estimate bolometric luminosities for the CSGs. We find that these stars are on average 0.4dex too faint to be 5Myr old, regardless of which stellar evolution model is used, and instead are indicative of a much older age of $10.4^{+1.3}_{-1.2}$Myr. We argue that neither systematic uncertainties in the extinction law nor stellar variability can explain this discrepancy. In reviewing various independent age estimates of Wd1 in the literature, we firstly show that those based on stellar diversity are unreliable. Secondly, we re-analyse Wd1's pre-MS stars employing the Damineli extinction law, finding an age of $7.2^{+1.1}_{-2.3}$Myr; older than that of previous studies, but which is vulnerable to systematic errors that could push the age close to 10Myr. However, there remains significant tension between the CSG age and that inferred from the eclipsing binary W13. We conclude that stellar evolution models cannot explain Wd1 under the single age paradigm. Instead, we propose that the stars in the Wd1 region formed over a period of several Myr. ",The Age of Westerlund 1 Revisited
153,1367780398730780675,1225094897780297729,Denis Merigoux,"['What is the future of legal expert systems? How to make sure taxes are computed correctly? Can we efficiently translate law into #rulesascode?  \n\nNew paper with @NChataing and @_protz_:\n\n‚û°Ô∏è <LINK>\nüìñ <LINK>\nüöÄ <LINK> <LINK>', ""This paper introduces the landmark features of Catala as a programming language:\n1‚É£ literate programming statutes, using default logic\n2‚É£ formalized semantics and certified compilation\n3‚É£ ready-to-use, source code library-based interoperability\n\nLet's explain those :)"", ""1‚É£ When translating a legal statute into code using Catala, you literally write the code for each statute paragraph right next to it.  \n\nAt the end of the statute, you have an executable program. Statute change ‚û°Ô∏è code change, simple.\n\nBased on @sarahlawsky's work! https://t.co/07B1sKhTMw"", ""2‚É£ Catala is designed by programming language professionals. Unlike others, it has functions and we do make use of them.\n\nOn top of https://t.co/bl0Q1S8Pj7, now we have a partial proof that the translation to machine code is correct. That's aeronautics-level safety assurance. https://t.co/YjXdq3zgZ9"", '3‚É£ To use Catala programs, no need to download bloated dependencies. The rules as code you need, as a simple library in the language you use. Currently: OCaml and JS, more to come.\n\nExample: French family benefits, 200ko-heavy library that runs in 3ms https://t.co/CLGc5oRECd', '‚ö°Ô∏è Want to try it out ? \n\n‚û°Ô∏è Online playground: https://t.co/bf7MuqiOBa\n‚û°Ô∏è Catala-powered simulator: https://t.co/TJ8kZPfihx\n‚û°Ô∏è Online discussion space: https://t.co/i8q7FWKM9K\n\nCustomary thanks to the #rulesascode community for inspiration and support!', '@craigaatkinson @NChataing @_protz_ Thanks for the insight! I wonder what you precisely mean by ""mitigated through algorithms"". Do you mean the algorithms will always know what rules are applicable all the time? If so, who do you think is capable of implementing such algorithms?', '@craigaatkinson @NChataing @_protz_ So you mean there is no public service software where you enter your situation, and it returns the most preferential tariff rate.\n\nHow many different rules are there in total? How much lines of code in total would such a public software service count to handle all situations?', '@craigaatkinson @NChataing @_protz_ I see, and @Xalgorithms definitely sounds like a reasonable solution for that use case where there is no central authority responsible for such a global software. Plus there is no existing legacy system to replace (or partial ones), so interoperability constraints are weaker.', ""@craigaatkinson @NChataing @_protz_ That makes sense. How has been your experience so far with government officials? Do they even understand what you're trying to do? Do you have a strategy to force gov agencies to publish rules using your software? \n\nI'm really interested in this üëÄ"", '@craigaatkinson @NChataing @_protz_ Ok I understand. Do you have performance constraints? Like are you able to compute the preferential tariff fast enough in a setting where there are thousands of rules to choose from?', '@craigaatkinson @NChataing @_protz_ I browsed the XAlgo GitHub but was not able to identify the rapid transaction processing, would love a pointer on that (might also be relevant to Catala). Thanks for answering my questions!', '@craigaatkinson @NChataing @_protz_ I would love what kind of algorithms and data structure he has in mind for that, because it looks like a non-trivial computer science problem.']",http://arxiv.org/abs/2103.03198,"Law at large underpins modern society, codifying and governing many aspects of citizens' daily lives. Oftentimes, law is subject to interpretation, debate and challenges throughout various courts and jurisdictions. But in some other areas, law leaves little room for interpretation, and essentially aims to rigorously describe a computation, a decision procedure or, simply said, an algorithm. Unfortunately, prose remains a woefully inadequate tool for the job. The lack of formalism leaves room for ambiguities; the structure of legal statutes, with many paragraphs and sub-sections spread across multiple pages, makes it hard to compute the intended outcome of the algorithm underlying a given text; and, as with any other piece of poorly-specified critical software, the use of informal language leaves corner cases unaddressed. We introduce Catala, a new programming language that we specifically designed to allow a straightforward and systematic translation of statutory law into an executable implementation. Catala aims to bring together lawyers and programmers through a shared medium, which together they can understand, edit and evolve, bridging a gap that often results in dramatically incorrect implementations of the law. We have implemented a compiler for Catala, and have proven the correctness of its core compilation steps using the F* proof assistant. We evaluate Catala on several legal texts that are algorithms in disguise, notably section 121 of the US federal income tax and the byzantine French family benefits; in doing so, we uncover a bug in the official implementation. We observe as a consequence of the formalization process that using Catala enables rich interactions between lawyers and programmers, leading to a greater understanding of the original legislative intent, while producing a correct-by-construction executable specification reusable by the greater software ecosystem. ",Catala: A Programming Language for the Law
154,1367761855670857730,1047105361650622464,Alexandre Dareau,"['We have a new paper on the @arXiv ! If you like ultra-cold metastable helium, bosons in lattices and the Mott transition, you should check it out üòâ &gt;&gt; <LINK> <LINK>']",https://arxiv.org/abs/2103.03007,"We report on a combined experimental and theoretical study of the low-entropy Mott transition for interacting bosons trapped in a three-dimensional (3D) cubic lattice -- namely, the interaction-induced superfluid-to-normal phase transition in the vicinity of the zero-temperature Mott transition. Our analysis relies on the measurement of the 3D momentum distribution, which allows us to extract the momentum-space density $\rho({\bf k}={\bf 0})$ at the center of the Brillouin zone. Upon varying the ratio between the interaction $U$ and the tunnelling energy $J$ across the superfluid transition, we observe that $\rho({\bf k}={\bf 0})$ exhibits a sharp transition at a value of $U/J$ consistent with the bulk prediction from quantum Monte Carlo. In addition, the variation of $\rho({\bf k}={\bf 0})$ with $U/J$ exhibits a critical behavior consistent with the expected 3D XY universality class. Our results show that the tomographic reconstruction of the momentum distribution of ultracold bosons can reveal traits of the critical behavior of the superfluid transition even in an inhomogeneous trapped system. ","Studying the low-entropy Mott transition of bosons in a
  three-dimensional optical lattice by measuring the full momentum-space
  density"
155,1367728705301315584,1003590340534853632,Mohammad Alsalti,"[""I'm happy to share my new paper: Data-based analysis and control of flat nonlinear systems. Pre-print available on ArXiv: <LINK>""]",https://arxiv.org/abs/2103.02892,"Willems et al. showed that all input-output trajectories of a discrete-time linear time-invariant system can be obtained using linear combinations of time shifts of a single, persistently exciting, input-output trajectory of that system. In this paper, we extend this result to the class of discrete-time single-input single-output flat nonlinear systems. We propose a data-based parametrization of all trajectories using only input-output data. Further, we use this parametrization to solve the data-based simulation and output-matching control problems for the unknown system without explicitly identifying a model. Finally, we illustrate the main results with numerical examples. ",Data-Based System Analysis and Control of Flat Nonlinear Systems
156,1367629352440111104,1150961015384870912,Samit Dasgupta,"['New paper out today, joint with Mahesh Kakde, solving Hilbert‚Äôs 12th problem for totally real fields. \n\n<LINK>', '@anton_hilado Thanks! The horizontal idea is only implicit here. The actual proof is in my DMJ paper‚Äîif  you allow arbitrary ramification (enlarging the set S), then you can get a whole copy of (O/p^m)^* in the Galois group, rather than the quotient by O^*, which is what happens if you only...', '@anton_hilado ...work vertically at p. So this allows you to see the whole unit mod p^m rather than just it‚Äôs norm. This is the content of Section 5.4 in the DMJ paper.  In today‚Äôs paper we allow arbitrary S, which means that the analysis from that paper applies, but we don‚Äôt discuss it again.']",https://arxiv.org/abs/2103.02516,"Let $F$ be a totally real field of degree $n$ and $p$ an odd prime. We prove the $p$-part of the integral Gross-Stark conjecture for the Brumer-Stark $p$-units living in CM abelian extensions of $F$. In previous work, the first author showed that such a result implies an exact $p$-adic analytic formula for these Brumer-Stark units up to a bounded root of unity error, including a ""real multiplication"" analogue of Shimura's celebrated reciprocity law in the theory of Complex Multiplication. In this paper we show that the Brumer-Stark units, along with $n-1$ other easily described elements (these are simply square roots of certain elements of $F$) generate the maximal abelian extension of $F$. We therefore obtain an unconditional solution to Hilbert's 12th problem for totally real fields, albeit one that involves $p$-adic integration, for infinitely many primes $p$. Our method of proof of the integral Gross-Stark conjecture is a generalization of our previous work on the Brumer-Stark conjecture. We apply Ribet's method in the context of group ring valued Hilbert modular forms. A key new construction here is the definition of a Galois module $\nabla_{\!\mathscr{L}}$ that incorporates an integral version of the Greenberg-Stevens $\mathscr{L}$-invariant into the theory of Ritter-Weiss modules. This allows for the reinterpretation of Gross's conjecture as the vanishing of the Fitting ideal of $\nabla_{\!\mathscr{L}}$. This vanishing is obtained by constructing a quotient of $\nabla_{\!\mathscr{L}}$ whose Fitting ideal vanishes using the Galois representations associated to cuspidal Hilbert modular forms. ",Brumer-Stark Units and Hilbert's 12th Problem
157,1367614872578580480,2800204849,Andrew Gordon Wilson,"['In our new #AISTATS2021 paper, ""Kernel Interpolation for Scalable Online Gaussian Processes"", we show how to do O(1) streaming Bayesian updates, while retaining exact inference! <LINK>\nwith @samscub, W. Maddox, @DelbridgeIan. 1/6 <LINK>', 'Bayesian methods provide predictive distributions that are particularly compelling for online decision making. However, scalability in these settings is a major open question. 2/6', 'Intuitively, we ought to be able to recycle computations to efficiently update our predictive distribution after observing an additional point, rather than starting training anew on n+1 points. However, it is extremely challenging to realize this intuition in practice. 3/6', 'Indeed, if we observe a new point, we must compute its interaction with every previous point, typically leading to O(n) computations. 4/6', 'We show that it is in fact possible to do O(1) updates while retaining exact inference through a careful interplay of structured kernel interpolation and caching. 5/6', 'We apply the approach, WISKI, to a variety of online settings, including active learning for measuring malaria incidence. Code is available at: https://t.co/jDY1WJSR5z. 6/6']",https://arxiv.org/abs/2103.01454,"Gaussian processes (GPs) provide a gold standard for performance in online settings, such as sample-efficient control and black box optimization, where we need to update a posterior distribution as we acquire data in a sequential fashion. However, updating a GP posterior to accommodate even a single new observation after having observed $n$ points incurs at least $O(n)$ computations in the exact setting. We show how to use structured kernel interpolation to efficiently recycle computations for constant-time $O(1)$ online updates with respect to the number of points $n$, while retaining exact inference. We demonstrate the promise of our approach in a range of online regression and classification settings, Bayesian optimization, and active sampling to reduce error in malaria incidence forecasting. Code is available at this https URL ",Kernel Interpolation for Scalable Online Gaussian Processes
158,1367564639027003401,151196583,Stefano Stramigioli,"['New contribution my ERC AdG PortWings project under review as an invited paper. We completely clarified the details of Navier Stokes energy decomposition and open boundary interconnection using a port-Hamiltonian, geometric,  coordinate free formulation.\n\n<LINK>']",https://arxiv.org/abs/2103.02277,A port-Hamiltonian model for compressible Newtonian fluid dynamics is presented in entirely coordinate-independent geometric fashion. This is achieved by use of tensor-valued differential forms that allow to describe describe the interconnection of the power preserving structure which underlies the motion of perfect fluids to a dissipative port which encodes Newtonian constitutive relations of shear and bulk stresses. The relevant diffusion and the boundary terms characterizing the Navier-Stokes equations on a general Riemannian manifold arise naturally from the proposed construction. ,"Geometric and energy-aware decomposition of the Navier-Stokes equations:
  A port-Hamiltonian approach"
159,1367519628830900228,1901672766,Dr. Megan Tannock,"['Our new paper ‚ÄúWeather on Other Worlds. V. The Three Most Rapidly Rotating Ultra-Cool Dwarfs‚Äù has been accepted for publication in the Astronomical Journal and is up on @_arXiv_astro_ph *today*! <LINK> I‚Äôm going to summarize the paper in this thread! (1/14)', 'I am also presenting this result on a poster at this week‚Äôs #CS20half conference! If you don‚Äôt have time for a 35 page paper, please see the express version ‚Äì my poster over on Zenodo! https://t.co/nMtrlN9jYz (2/14)', 'First off, big thanks to all of my co-authors, including @smetchev @jgagneastro @browndwarfs @astromarkmarley @danielapai @PlavchanPeter and others not on Twitter. Couldn‚Äôt have done it without you! (3/14)', 'We present the discovery of the three most rapidly rotating brown dwarfs known to date, with rotation periods of just 1 hr! We identified short period variability in #NASASpitzer lightcurves and confirmed the rapid rotation with line broadening with nIR spectroscopy. (4/14)', 'Here are our #NASASpitzer lightcurves. We observed each target for 10 hr at 3.6um then 10 hr at 4.5um with IRAC. The rotation periods are 1.23, 1.14, and 1.08 hr for the L3.5, L8, and T7 dwarfs respectively. The Ls show variability at 3.6um, and the T varies at 4.5um. (5/14) https://t.co/nuZpbde5pV', 'Variability is most likely caused by large-scale atmospheric structures (eg: spots/banding) rotating in and out of view. To confirm we aren‚Äôt seeing the half period of a more slowly rotating object with repeated spot patterns, we investigated spectroscopic line profiles. (6/14)', 'Line profiles of rapid rotators will be significantly Doppler-broadened, while more slowly rotating objects will not show much line broadening. In the gif below, I show a sample model in red compared to our @GeminiObs /GNIRS data for the L3.5 in black. (7/14) https://t.co/1RqaIsjspK', 'At low velocities (vsini) it appears we have selected the wrong model entirely. But turn that velocity all the way up to 83 km/s and those deep CO lines get so smeared out that they nearly disappear, and we get an excellent match to our data! (8/14)', 'Here is another example with a model in red, and our Magellan/FIRE data for the T7 in black. This one requires a whopping 95 km/s!! These lines are mostly water. (9/14) https://t.co/8ZM2atHJj1', 'These things are going *fast*. Here‚Äôs a figure where we compare their rotation periods to all of the others in the literature. See the Appendix of the paper for a complete list of the objects and references. And yes, those lines through the red dots are the error bars! (10/14) https://t.co/MawqBlnQME', 'Our fast rotators have oblatenesses comparable to that of Saturn, the most oblate planet in the Solar System. These rapid rotators are potentially the most oblate field brown dwarfs known and would probably make good targets for searches for polarized thermal emission! (11/14)', 'Brown dwarfs are known to host very strong aurorae, and it has been shown that rapid rotation is key to powering them! This means that our rapid rotators are excellent candidates for seeking auroral radio emission. (12/14)', 'Finally, these delineate a lower boundary to the rotation periods measured to date. This limit holds over a broad range of spectral types, for objects that presumably have different ages. Hence, ‚àº1 h may be close to a physical lower limit to the period of brown dwarfs! (13/14)', 'TLDR; Spinning brown dwarf go WEEEEEE. (14/14)', ""@rachelosten They would make great targets for this! We don't have any plans currently to do this, but we would love to see it.""]",https://arxiv.org/abs/2103.01990,"We present the discovery of rapid photometric variability in three ultra-cool dwarfs from long-duration monitoring with the Spitzer Space Telescope. The T7, L3.5, and L8 dwarfs have the shortest photometric periods known to date: ${1.080}^{+0.004}_{-0.005}$ h, ${1.14}^{+0.03}_{-0.01}$ h, and ${1.23}^{+0.01}_{-0.01}$ h, respectively. We confirm the rapid rotation through moderate-resolution infrared spectroscopy, which reveals projected rotational velocities between 79 and 104 km s$^{-1}$. We compare the near-infrared spectra to photospheric models to determine the objects' fundamental parameters and radial velocities. We find that the equatorial rotational velocities for all three objects are $\gtrsim$100 km s$^{-1}$. The three L and T dwarfs reported here are the most rapidly spinning and likely the most oblate field ultra-cool dwarfs known to date. Correspondingly, all three are excellent candidates for seeking auroral radio emission and net optical/infrared polarization. As of this writing, 78 L-, T-, and Y-dwarf rotation periods have now been measured. The clustering of the shortest rotation periods near 1 h suggests that brown dwarfs are unlikely to spin much faster. ","Weather on Other Worlds. V. The Three Most Rapidly Rotating Ultra-Cool
  Dwarfs"
160,1367492022555279367,24859650,Jan-Willem van de Meent,"['New working paper: Learning Proposals for Probabilistic Programs with Inference Combinators\n\nArXiv: <LINK> \n\nJoint work, several years in the making, by Sam Stites* (@SamStites), Heiko Zimmermann* (@zmheiko), Hao Wu (@Hao_Wu_), and Eli Sennesh (@EliSennesh) [1/] <LINK>', 'Amortized inference methods train a neural proposal by minimizing a variational objective. These methods are straightforward to use in unstructured models, such as standard VAEs, but can be difficult to apply to more structured models, such as (deep) probabilistic programs. [2/]', 'There has been lots of great work that improves variational methods using techniques from importance sampling and MCMC, but these more sophisticated methods are difficult to apply to probabilistic programs, in part because they are rarely fully model agnostic. [3/]', 'In this paper, we propose a language for inference algorithms. This language defines inference combinators; functions that can be composed to define user-programmable importance samplers for probabilistic programs. [4/] https://t.co/bQSHUGlgPr', ""To reason about validity of inference, we adapt Christian Naesseth's (@chris_naesseth) work on nested importance samplers and proper weighting. This allows us to design a language in which any composition of combinators defines a valid importance sampler. [5/]"", 'We combine this language with a backend for variational inference that implements gradient estimates for the forward and reverse KL divergence, as well as for our recently-proposed nested variational objectives (https://t.co/hzLJ2IpIWm). [6/]', ""The result is a language that allows users to concisely define advanced methods, such as Hao's recent work on amortized Gibbs methods (https://t.co/bqQ07ixS6E), and define objectives to train both generative models and proposals. [7/] https://t.co/bjuDT4WywY"", 'We implement combinators and gradient estimators in Probabilistic Torch (release forthcoming). However, the language in this paper can be implemented on top of many (deep) probabilistic programming languages. [8/]', 'This project has been several years in the making. We started thinking about this language design in 2018 (https://t.co/IZXCdqjfyI), but eventually realized that we did not sufficiently understand how to precisely connect importance sampling to variational inference [9/]', ""We then went back and did work on amortized population Gibbs methods, which lead to work on nested variational inference, and this work allowed us to put the pieces together in this paper. I'm super proud of Sam, Heiko, Hao, and Eli, who each made major contributions!""]",https://arxiv.org/abs/2103.00668,"We develop operators for construction of proposals in probabilistic programs, which we refer to as inference combinators. Inference combinators define a grammar over importance samplers that compose primitive operations such as application of a transition kernel and importance resampling. Proposals in these samplers can be parameterized using neural networks, which in turn can be trained by optimizing variational objectives. The result is a framework for user-programmable variational methods that are correct by construction and can be tailored to specific models. We demonstrate the flexibility of this framework by implementing advanced variational methods based on amortized Gibbs sampling and annealing. ",Learning Proposals for Probabilistic Programs with Inference Combinators
161,1367484068183625736,572140131,Paul Martini,"[""I'm happy to say that my grad student Zhefu Yu @OSUAstro has a paper today! He used data from @theDESurvey and OzDES #AAT to measure nine new, high-z black holes that we'll use to establish a better MgII radius-luminosity relation (also with @tamarastro) <LINK>""]",https://arxiv.org/abs/2103.01973,"Reverberation mapping is a robust method to measure the masses of supermassive black holes (SMBHs) outside of the local Universe. Measurements of the radius -- luminosity ($R-L$) relation using the Mg II emission line are critical for determining these masses near the peak of quasar activity at $z \approx 1 - 2$, and for calibrating secondary mass estimators based on Mg II that can be applied to large samples with only single-epoch spectroscopy. We present the first nine Mg II lags from our five-year Australian Dark Energy Survey (OzDES) reverberation mapping program, which substantially improves the number and quality of Mg II lag measurements. As the Mg II feature is somewhat blended with iron emission, we model and subtract both the continuum and iron contamination from the multi-epoch spectra before analyzing the Mg II line. We also develop a new method of quantifying correlated spectroscopic calibration errors based on our numerous, contemporaneous observations of F-stars. The lag measurements for seven of our nine sources are consistent with both the H$\beta$ and Mg II $R-L$ relations reported by previous studies. Our simulations verify the lag reliability of our nine measurements, and we estimate that the median false positive rate of the lag measurements is $4\%$. ","OzDES Reverberation Mapping Program: The first Mg II lags from five
  years of monitoring"
162,1367421082316263425,331497846,Arthur Gervais,"[""New @IEEESSP'21 paper on how to discover automagically profitable transactions in the intertwined DeFi graph: <LINK> with @lzhou1110 @KaihuaQIN @CULLYAntoine @convoluted_code ‚ú®üî•üéâ <LINK>"", 'We present two approaches:\n1) A theorem prover based method (SMT) #z3\n2) A Bellman-Ford-Moore inspired system (ARB)\n\nInsight: ARB is simpler and yields more revenue, but misses other opportunities found by SMT.', ""We show the quantitative impact of MEV on the blockchain's security.\n\nE.g. on Ethereum, a 5% rational miner will fork, if the MEV bundle is worth at least 8.5x the block reward.\n\nImportant to consider when extracting MEV @flashb0t. https://t.co/RDTT2CFeFu"", 'We find that the 2020 @bZxHQ attack-window was open for 69 days before being exploited. See our #FC21 paper for more details on that attack (https://t.co/h8YvSzz1lH). https://t.co/5bykLzJnKH', ""Come on and jump in on the DeFi research train, there's still much to be done. A good start is here https://t.co/Lptw5Sw9NU by @chainomics""]",https://arxiv.org/abs/2103.02228,"In this paper, we investigate two methods that allow us to automatically create profitable DeFi trades, one well-suited to arbitrage and the other applicable to more complicated settings. We first adopt the Bellman-Ford-Moore algorithm with DEFIPOSER-ARB and then create logical DeFi protocol models for a theorem prover in DEFIPOSER-SMT. While DEFIPOSER-ARB focuses on DeFi transactions that form a cycle and performs very well for arbitrage, DEFIPOSER-SMT can detect more complicated profitable transactions. We estimate that DEFIPOSER-ARB and DEFIPOSER-SMT can generate an average weekly revenue of 191.48ETH (76,592USD) and 72.44ETH (28,976USD) respectively, with the highest transaction revenue being 81.31ETH(32,524USD) and22.40ETH (8,960USD) respectively. We further show that DEFIPOSER-SMT finds the known economic bZx attack from February 2020, which yields 0.48M USD. Our forensic investigations show that this opportunity existed for 69 days and could have yielded more revenue if exploited one day earlier. Our evaluation spans 150 days, given 96 DeFi protocol actions, and 25 assets. Looking beyond the financial gains mentioned above, forks deteriorate the blockchain consensus security, as they increase the risks of double-spending and selfish mining. We explore the implications of DEFIPOSER-ARB and DEFIPOSER-SMT on blockchain consensus. Specifically, we show that the trades identified by our tools exceed the Ethereum block reward by up to 874x. Given optimal adversarial strategies provided by a Markov Decision Process (MDP), we quantify the value threshold at which a profitable transaction qualifies as Miner ExtractableValue (MEV) and would incentivize MEV-aware miners to fork the blockchain. For instance, we find that on Ethereum, a miner with a hash rate of 10% would fork the blockchain if an MEV opportunity exceeds 4x the block reward. ","On the Just-In-Time Discovery of Profit-Generating Transactions in DeFi
  Protocols"
163,1367118232671387648,1101220947607146497,Alon Jacovi,"['Check out our new paper üòä (w\\ @swabhz @ravfogel @yanaiela @YejinChoinka @yoavgo )\n\nContrastive Explanations for Model Interpretability\n<LINK>\n\nThis paper is about explaining classifier decisions contrastively against alternative decisions. <LINK>', ""Explanations are inherently contrastive. This means that even if they're not explicitly contrasted against something, as humans we assume they are. This has implications on what it means for an explanation to be understandable/intuitive to humans (see pic). https://t.co/xiqsfol19v"", 'Most simply, explanations are just easier to produce and to understand for all parties if they involve an explicit contrasted, alternative decision. https://t.co/lwWN1oL8oo', 'In our paper we explain contrastively by a process where we intervene on some aspect of the instance, and then measure how the model changed specifically only in respect to the explained decision, and the contrast decision. https://t.co/3ygt51qWMZ', 'We do this by measuring contrastive changes in the model + projecting its latent representation to a contrastive space that only includes the information that is used to differentiate two decisions. We use this for many experiments on BIOS and MNLI. Details in paper üëãüëã Thanks!!']",https://arxiv.org/abs/2103.01378,"Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model's decision. ",Contrastive Explanations for Model Interpretability
164,1367075307992322051,1114431596239695874,Anne-Laure Boulesteix,"['Validation of cluster analysis: what does this mean?\n\nNew paper on arXiv: ""Validation of cluster analysis results on validation data: A systematic framework"" with Theresa Ullmann and Christian Hennig.\n<LINK>']",https://arxiv.org/abs/2103.01281,"Cluster analysis refers to a wide range of data analytic techniques for class discovery and is popular in many application fields. To judge the quality of a clustering result, different cluster validation procedures have been proposed in the literature. While there is extensive work on classical validation techniques, such as internal and external validation, less attention has been given to validating and replicating a clustering result using a validation dataset. Such a dataset may be part of the original dataset, which is separated before analysis begins, or it could be an independently collected dataset. We present a systematic structured framework for validating clustering results on validation data that includes most existing validation approaches. In particular, we review classical validation techniques such as internal and external validation, stability analysis, hypothesis testing, and visual validation, and show how they can be interpreted in terms of our framework. We precisely define and formalise different types of validation of clustering results on a validation dataset and explain how each type can be implemented in practice. Furthermore, we give examples of how clustering studies from the applied literature that used a validation dataset can be classified into the framework. ","Validation of cluster analysis results on validation data: A systematic
  framework"
165,1366881479175847942,369877186,Jeremy Cohen,"['Our new ICLR paper demonstrates that when neural networks are trained using *full-batch* gradient descent: (1) the training dynamics obey a surprisingly simple ""story"", and (2) this story contradicts a lot of conventional wisdom in optimization.  <LINK> <LINK>', '2/ The teaser video above conveys the main idea: gradient descent with step size Œ∑ typically operates in a regime (the ""Edge of Stability"") in which the maximum Hessian eigenvalue hovers just above the numerical value 2/Œ∑.  This thread provides more detail.', '3/ Background: the stability of gradient descent on quadratic functions is controlled by the relationship between the step size Œ∑, and the curvature (i.e. second derivative).  If the curvature in any direction is too large for the step size, the algorithm ""spins out of control.""', '4/ On a one-dimensional quadratic objective f(x) = ¬Ω a x¬≤ + bx + c, gradient descent with step size Œ∑ spins out of control if the curvature ""a"" exceeds the threshold 2/Œ∑ --- or equivalently, if the step size Œ∑ exceeds the threshold 2/a. https://t.co/wr8uUesf2E', '5/ More generally, on a multidimensional quadratic f(x) = ¬Ω x\' A x + b\'x + c, if the curvature matrix ""A"" has any eigenvalue greater than 2/Œ∑, then gradient descent spins out of control along the corresponding eigenvector. https://t.co/zuSxp4qtT5', ""6/ Neural network training objectives are not globally quadratic.  However, it's reasonable to assume that the behavior of gradient descent on the real objective is locally well-approximated by gradient descent on the local quadratic Taylor approximation."", ""7/ For this reason, the conventional wisdom holds that gradient descent with step size Œ∑ can only function properly in regions of the loss landscape where all of the Hessian's eigenvalues are less than 2/Œ∑.  But is this true?  Is this how gradient descent on NNs actually works? https://t.co/T3l9Bd5e1j"", '8/ In our paper, we empirically study the dynamics of the maximum Hessian eigenvalue (which we call the ""sharpness"") when training neural networks using full-batch gradient descent.', '9/ We observe that whenever gradient descent is stable (i.e. whenever the sharpness is less than 2/Œ∑), the sharpness overwhelmingly tends to rise.  In other words, gradient descent is attracted to regions of parameter space with high curvature. https://t.co/8qVnw5HF0G', '10/ The speed at which the sharpness rises is dependent on the network and the dataset.  For ultrawide networks, the sharpness barely rises, consistent with NTK theory.  On the other hand, for standard architectures on CIFAR-10, the sharpness rises astronomically: https://t.co/N46TiBwTF6', ""11/ Now, it's possible to train networks using step sizes Œ∑ so small that the sharpness never rises all the way to 2/Œ∑.  However, perhaps surprisingly, our paper shows that networks can also be trained at larger step sizes Œ∑, for which the sharpness eventually rises past 2/Œ∑."", '12/ Once the sharpness crosses 2/Œ∑, gradient descent begins to oscillate with increasing magnitude along the leading Hessian eigenvector, just as one would expect from the quadratic intuition. https://t.co/KxNfMcMgiF', '13/ However, contrary to the quadratic intuition, after becoming initially destabilized, gradient descent avoids diverging entirely, and instead continues to successfully optimize the training loss, just in a non-monotonic fashion. https://t.co/hIsJ86sav5', '14/ In particular, after the sharpness crosses 2/Œ∑, gradient descent enters a regime we call the ""Edge of Stability"", in which (1) the sharpness hovers just above the value  2/Œ∑, and (2) the train loss behaves non-monotonically, yet decreases over long timescales. https://t.co/VwMvDePDsl', '15/  At the Edge of Stability, the sharpness is still ""trying"" to increase further (and that\'s what happens if you cut the step size), but it\'s being actively restrained somehow from doing so.', '16/ For general neural networks, we observe that gradient descent trains faster at step sizes that enter the ""Edge of Stability"" than at the smaller step sizes for which gradient descent remains stable for the entirety of training.', '17/ Moreover, for standard architectures on the standard dataset CIFAR-10, step sizes that remain stable for the entirety of training are ludicrously small; gradient descent at any remotely reasonable step size eventually enters the Edge of Stability.', '18/ Overall, we hope that our paper will both (1) nudge the neural net optimization community away from widespread presumptions that appear to be false, and also (2) point the way forward by identifying precise empirical phenomena that are suitable for further study.', ""19/ Over the next few weeks, I'll be doing a few other Twitter threads highlighting various implications of this paper!"", '@avt_im @jefrankle [No worries at all re: tone!].  See my response to Steven Wright here: https://t.co/ogQhuI6qh0.  The point is that, contrary to the conventional wisdom, large step sizes (and the attendant oscillations) are not a ""culprit"" -- they\'re good!', ""@avt_im @jefrankle It's much better to train at a large step size that oscillates, than to train at a small step size that yields a stable discretization of the gradient flow."", ""@weijie444 There's a big drop in generalization when moving from SGD to GD (even GD with large step sizes).  See also this recent paper: https://t.co/vMSWtKZsdJ"", '@madsjw @avt_im I responded here (see also my discussion at the bottom with Francesco): https://t.co/ogQhuI6qh0.', ""@YesThisIsLion See the discussion here: https://t.co/CAAAezJ7u4.  It's frequently stated that knowledge of the sharpness would be useful for learning rate tuning, but this does not appear to actually be true."", '@avt_im @jefrankle I think the speedup is just because large steps are better than small steps (because they move more).  Our paper is basically just saying that the instability that comes from large steps + curvature is not fatal.']",http://arxiv.org/abs/2103.00065,"We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the numerical value $2 / \text{(step size)}$, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability. Code is available at this https URL ","Gradient Descent on Neural Networks Typically Occurs at the Edge of
  Stability"
166,1366853907792723977,92526478,Sharan Banagiri,"[""It's new paper day with @AstrophysicalAC, Tommy Kuan, Vuk Mandic, Joe Romano, and @AstroStephen ! This one is a LISA paper, and I'm kind of proud of it. And this is probably the last paper that will come out entirely out of my Ph.D. work. A thread üëáüèΩ\n\n<LINK> <LINK>"", 'LISA will see GWs from many types of sources ranging from white dwarfs binaries to supermassive black holes. This is awesome, but not all signals will be strong enough to detect. The weaker ones form a stochastic noise similar to the background noise in a party (remember those?) https://t.co/AHr7M1eacJ', 'One stochastic noise in the LISA band comes from galactic white dwarf binaries. This ""galactic foreground"" will be very loud, dominating instrumental noise at some frequencies. And because it\'s galactic, it will follow the shape of the milky way on the sky. It wouldn\'t be uniform https://t.co/J29vRmHfcd', 'Mapping this structure on the sky will be astrophysically interesting and also imp to accurately measure properties of detected signals. We develop a spherical harmonic (SH) mapping to do so. But we wanted to do this in a Bayesian way, which has a bunch of advantages over the ...', 'mostly frequentist versions used in the past. To do this, we devised a novel method using Clebsch-Gordan coefficients (you might remember them if you took quantum mechanics) to constrain the SH expansion to be non-negative. This was a hindrance for Bayesian SH algorithms before.', ""Fortunately, there are now modules like the one in the symbolic python library (SymPy) to calculate the Clebsch-Gordan coefficients, and I don't have to look up the tables to do it. Phew üòå https://t.co/BMkMkrzgTB"", 'And it seems to work pretty well. Shown here are sky maps, from a simulated toy-model galactic foreground on the top and the recovered median skymap on the bottom. https://t.co/6U6M4p3h3N', 'The algorithm is of course mathematically very generalized and it can recover any arbitrary distribution of GW power on the sky. The figure is one such ad hoc example. https://t.co/EJTXLYSIow', 'With this work done, it would be very interesting to use it simultaneously with algorithms for resolvable GW signals. This is one of the advantages of doing Bayesian mapping. LISA will be astrophysically very rich, and we should be ready for anything it can throw at us!\n\nFin.', ""@MrKhagol @AstrophysicalAC @AstroStephen No, that wouldn't be possible because you have three satellites separated by 2.5 million km. Also their orbits will not be along the ecliptic. Instead they have three separate heliocentric orbits around the Sun, trailing the Earth by ~20 deg. \n\nhttps://t.co/B0JmjtKpZi"", '@MrKhagol @AstrophysicalAC @AstroStephen However the test mission, LISA Pathfinder was indeed at L1', '@scorfano Thank you Claudia!', '@EdelmanBruce @AstrophysicalAC @AstroStephen Ty ty']",https://arxiv.org/abs/2103.00826,"The millihertz gravitational-wave frequency band is expected to contain a rich symphony of signals with sources ranging from galactic white dwarf binaries to extreme mass ratio inspirals. Many of these gravitational-wave signals will not be individually resolvable. Instead, they will incoherently add to produce stochastic gravitational-wave confusion noise whose frequency content will be governed by the dynamics of the sources. The angular structure of the power of the confusion noise will be modulated by the distribution of the sources across the sky. Measurement of this structure can yield important information about the distribution of sources on galactic and extra-galactic scales, their astrophysics and their evolution over cosmic timescales. Moreover, since the confusion noise is part of the noise budget of LISA, mapping it will also be essential for studying resolvable signals. In this paper, we present a Bayesian algorithm to probe the angular distribution of the stochastic gravitational-wave confusion noise with LISA using a spherical harmonic basis. We develop a technique based on Clebsch-Gordan coefficients to mathematically constrain the spherical harmonics to yield a non-negative distribution, making them optimal for expanding the gravitational-wave power and amenable to Bayesian inference. We demonstrate these techniques using a series of simulations and analyses, including recovery of simulated distributed and localized sources of gravitational-wave power. We also apply this method to map the gravitational-wave foreground from galactic white-dwarfs using a simplified model of the galactic white dwarf distribution. ","Mapping the Gravitational-wave Sky with LISA: A Bayesian Spherical
  Harmonic Approach"
167,1366813961027522565,369758524,Lorena Barba,"['#Barbagroup News! Manuscript submitted, preprint on #arxiv! This paper showcases the integration of our new version of Exafmm and the Bempp library, for virus-scale electrostatics calculations. And get this: you can do so from a #Jupyter notebook!\n<LINK> <LINK>', ""As always, our paper is prepared with utmost care for computational #reproducibility, and readers can find materials for reproducing the results in the manuscript's GitHub repository (also archived in Zenodo) https://t.co/Dx2lheNIvF"", ""Here's the showcase calculation in our paper: the surface electrostatic potential of a Zika virus, 1.6 million atoms, 10 million boundary elements (30M points), 1.5h runtime on 1 CPU node #compchem https://t.co/3VhHQOpTZA""]",https://arxiv.org/abs/2103.01048,"Biomolecular electrostatics is key in protein function and the chemical processes affecting it.Implicit-solvent models expressed by the Poisson-Boltzmann (PB) equation can provide insights with less computational power than full atomistic models, making large-system studies -- at the scale of viruses, for example -- accessible to more researchers. This paper presents a high-productivity and high-performance computational workflow combining Exafmm, a fast multipole method (FMM) library, and Bempp, a Galerkin boundary element method (BEM) package. It integrates an easy-to-use Python interface with well-optimized computational kernels that are written in compiled languages. Researchers can run PB simulations interactively via Jupyter notebooks, enabling faster prototyping and analyzing. We provide results that showcase the capability of the software, confirm correctness, and evaluate its performance with problem sizes between 8,000 and 2 million boundary elements. A study comparing two variants of the boundary integral formulation in regards to algebraic conditioning showcases the power of this interactive computing platform to give useful answers with just a few lines of code. As a form of solution verification, mesh refinement studies with a spherical geometry as well as with a real biological structure (5PTI) confirm convergence at the expected $1/N$ rate, for $N$ boundary elements. Performance results include timings, breakdowns, and computational complexity. Exafmm offers evaluation speeds of just a few seconds for tens of millions of points, and $\mathcal{O}(N)$ scaling. This allowed computing the solvation free energy of a Zika virus, represented by 1.6 million atoms and 10 million boundary elements, at 80-min runtime on a single compute node (dual 20-core Intel Xeon Gold 6148). All results in the paper are presented with utmost care for reproducibility. ","High-productivity, high-performance workflow for virus-scale
  electrostatic simulations with Bempp-Exafmm"
168,1366776458543312898,1215310334,Timo Schick,"['üéâNew paperüéâ In ""Self-Diagnosis and Self-Debiasing"", we investigate whether pretrained LMs can use their internal knowledge to discard undesired behaviors and reduce biases in their own outputs (w/@4digitaldignity + @HinrichSchuetze) #NLProc\n\nLink: <LINK> [1/3] <LINK>', ""üìã We use simple templates to access the model's internal knowledge. Findings:\nü©∫Self-diagnosis abilities continuously improve with model size. \nü©πSelf-debiasing is able to reduce the probability of models showing undesired behavior, but cannot completely prevent it. [2/3] https://t.co/UOwHBvN0US"", 'This is still an early draft. We plan to extend the paper along several axes and to conduct extensive further experiments. Feedback is therefore very welcome üòä 3/3']",https://arxiv.org/abs/2103.00453,"When trained on large, unfiltered crawls from the internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: they often generate racist, sexist, violent or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model's parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction. ","Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based
  Bias in NLP"
169,1366757620653166594,1004365363574902784,Kevin J. Kelly,"['New paper out today! Thanks to @yuberfpg, @PedroANMachado, and our collaborator Alberto for a wonderful team-up.\n\n<LINK>\n\nThe idea is ""LEvEL"", the Low-Energy Neutrino Experiment at the LHC.\n\n1/14', '""Low-Energy"" and ""Large Hadron Collider"" (LHC) may seem in tension, but it turns out there is a huge source of neutrinos that hasn\'t been fully explored - the LHC\'s beam dump.\n\nTwice a day, the high-energy protons of the LHC are diverted onto a graphite target.\n\n2/14 https://t.co/A3fvZRj1DO', 'That results in a huge amount of particles, including neutrinos (both low-energy and high-energy) coming out.\n\nWe can compare this as a source against other ""conventional"" low-energy neutrino sources by comparing their average beam power.\n\n3/14 https://t.co/OfY9rMGypa', 'We see that LEvEL can occupy a completely new region of these sources, in a way that is complementary to current and upcoming experiments!\n\n4/14', ""Now, back to the high- and low-energy neutrino fluxes.\n\nWe'd *love* to be able to detect those high-energy neutrinos, which go mostly in the forward direction. These figures show the neutrino fluence (basically, number density) for low/high-energy components.\n\n5/14 https://t.co/2JLIa5CIcD"", ""So, to see that high-energy flux, we'd want a detector in the forward direction. There's a problem, however -- backgrounds. High-energy muons and neutrons make neutrino detection very difficult, and there are a *lot* of them moving in the forward direction too. Take a look:\n\n6/14 https://t.co/t5NW3yC3DJ"", 'We propose putting a detector in the perpendicular direction, 15 meters away at the boxes labelled ""C"" throughout. This means focusing on the low-energy neutrino flux coming from particles that stop and decay near the beam dump. It also allows us to avoid many backgrounds.\n\n7/14', ""What's so exciting though, is that there's a *lot* we can do with that low-energy flux, because it's so large and intense for a short period of time.\n\n8/14"", 'If we put a 100 ton liquid argon detector (like SBND https://t.co/2Z2E5DhgTl @FNALNeutrinos @Fermilab), we expect to see a couple of neutrino scattering events every time the LHC beam dumps on the target!\n\n9/14 https://t.co/BgXXBu52GE', 'If we measure this process over the course of a few years, we can precisely measure the process that DUNE (@DUNEScience https://t.co/Qb4hzobBOJ) wants to use to detect a nearby supernova exploding in the next decade or two!\n\n10/14', 'Another avenue we explore is putting a detector that can measure Coherent Neutrino Scattering (CEvNS -- check out @COHERENT_NUS) at this location.\n\nCEvNS scattering on liquid argon has been measured by @COHERENT_NUS, and we think more can be learned by using LEvEL too.\n\n11/14', ""If we measure this process over a year or so, we can constrain this process pretty precisely. LEvEL's low-energy neutrino flux is a bit different from the SNS (which COHERENT uses), allowing us to measure it across a range of neutrino energies.\n\n12/14 https://t.co/GBKgK68JCO"", ""A more precise understanding of both of these processes -- electron-neutrino/argon charged current scattering and CEvNS -- can allow for a deeper understanding of neutrino properties. If we're lucky, this can unlock discoveries of physics beyond the Standard Model.\n\n13/14"", ""Again, I'm indebted to @yuberfpg, @PedroANMachado, and Alberto for seeing this project through.\n\nTake a look at the paper and let us know what you think and if you have any ideas that can be carried out with LEvEL and the LHC Beam Dump!\n\n14/14""]",https://arxiv.org/abs/2103.00009,"We propose the operation of \textbf{LEvEL}, the Low-Energy Neutrino Experiment at the LHC, a neutrino detector near the Large Hadron Collider Beam Dump. Such a detector is capable of exploring an intense, low-energy neutrino flux and can measure neutrino cross sections that have previously never been observed. These cross sections can inform other future neutrino experiments, such as those aiming to observe neutrinos from supernovae, allowing such measurements to accomplish their fundamental physics goals. We perform detailed simulations to determine neutrino production at the LHC beam dump, as well as neutron and muon backgrounds. Measurements at a few to ten percent precision of neutrino-argon charged current and neutrino-nucleus coherent scattering cross sections are attainable with 100~ton-year and 1~ton-year exposures at LEvEL, respectively, concurrent with the operation of the High Luminosity LHC. We also estimate signal and backgrounds for an experiment exploiting the forward direction of the LHC beam dump, which could measure neutrinos above 100 GeV. ",LEvEL: Low-Energy Neutrino Experiment at the LHC
170,1366746160069709825,1094241003760869378,Naoki Kanazawa,['New paper about iSWAP/SWAP gate calibration is online <LINK>\n\nSimultaneously driving two CR tones from both direction with correction to the drive-induced AC Stark shift. This experiment was fully conduced with #Qiskit Pulse!'],https://arxiv.org/abs/2103.00024,"Implementation of high-fidelity swapping operations is of vital importance to execute quantum algorithms on a quantum processor with limited connectivity. We present an efficient pulse control technique, cross-cross resonance (CCR) gate, to implement iSWAP and SWAP operations with dispersively-coupled fixed-frequency transmon qubits. The key ingredient of the CCR gate is simultaneously driving both of the coupled qubits at the frequency of another qubit, wherein the fast two-qubit interaction roughly equivalent to the XY entangling gates is realized without strongly driving the qubits. We develop the calibration technique for the CCR gate and evaluate the performance of iSWAP and SWAP gates The CCR gate shows roughly two-fold improvement in the average gate error and more than 10~\% reduction in gate times from the conventional decomposition based on the cross resonance gate. ",Cross cross resonance gate
171,1366706548752400384,1682932567,Fabian R. Lux,"['New paper on ArXiv! We study the observable algebra of multi-q magnetization textures and find it determined by a noncommutative torus. The 2D quantum topological Hall effect is thus a manifestation of effective 4D physics with two ""curled up dimensions""\n\n<LINK>']",https://arxiv.org/abs/2103.01047,"The nontrivial topology of spin systems such as skyrmions in real space can promote complex electronic states. Here, we provide a general viewpoint at the emergence of topological electronic states in spin systems based on the methods of noncommutative K-theory. By realizing that the structure of the observable algebra of spin textures is determined by the algebraic properties of the noncommutative hypertorus, we arrive at a unified understanding of topological electronic states which we predict to arise in various noncollinear setups. The power of our approach lies in an ability to categorize emergent topological states algebraically without referring to smooth real- or reciprocal-space quantities. This opens a way towards an educated design of topological phases in aperiodic, disordered, or non-smooth textures of spins and charges containing topological defects. ","Unified topological characterization of electronic states in spin
  textures from noncommutative K-theory"
172,1366699821130797059,1290208688213557249,Max Marcus,"['New Paper! And it‚Äôs my first single-author one, as well. Disorder can enhance measurements? Well yes, it can! First results of this intriguing effect here, more to follow. Preprint: <LINK>. Final article out soon, hopefully. @OxChemRF @OxfordChemistry']",https://arxiv.org/abs/2103.00696,"Quantum process tomography might be the most important paradigm shift which has yet to be translated fully into theoretical chemistry. Its fundamental strength, long established in quantum information science, offers a wealth of information about quantum dynamic processes which lie at the heart of many (if not all) chemical processes. However, due to its complexity its application to real chemical systems is currently beyond experimental reach. Furthermore, it is susceptible to errors due to experimental and theoretical inaccuracies and disorder has long been thought to be an obstacle in its applicability. Here, I present the first results of a study into the use of quantum light for quantum process tomography. By using a toy model and comparing numerical simulations to theoretical predictions the possible enhancement of using non-conventional light is studied. It is found, however, that disorder is necessary make the use of quantum light suitable for process tomography and that, in contrast to conventional wisdom, disorder can make the results more accurate than in an ordered system. ",Disorder Enhanced Quantum Process Tomography using Quantum Light
173,1366683756602531843,507704346,Isabelle Augenstein,['New short survey paper on stance detection for #factchecking with @mhardalov @rnav_arora @preslav_nakov \n<LINK>\n#onlinesafety #NLProc #misinformation ‚§µÔ∏è <LINK>'],https://arxiv.org/abs/2103.00242,"Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges. ",A Survey on Stance Detection for Mis- and Disinformation Identification
174,1366683184864374784,507704346,Isabelle Augenstein,"['New short survey paper on abusive language detection with @preslav_nakov Vibha Nayak, Kyle Dent, Ameya Bhatawdekar @zzz2aaa @mhardalov Yoan Dinkov @didizlatkova @gbouchar \n<LINK>\n#onlinesafety #NLProc ‚§µÔ∏è <LINK>']",https://arxiv.org/abs/2103.00153,"Abusive language on online platforms is a major societal problem, often leading to important societal problems such as the marginalisation of underrepresented minorities. There are many different forms of abusive language such as hate speech, profanity, and cyber-bullying, and online platforms seek to moderate it in order to limit societal harm, to comply with legislation, and to create a more inclusive environment for their users. Within the field of Natural Language Processing, researchers have developed different methods for automatically detecting abusive language, often focusing on specific subproblems or on narrow communities, as what is considered abusive language very much differs by context. We argue that there is currently a dichotomy between what types of abusive language online platforms seek to curb, and what research efforts there are to automatically detect abusive language. We thus survey existing methods as well as content moderation policies by online platforms in this light, and we suggest directions for future work. ",Detecting Abusive Language on Online Platforms: A Critical Analysis
175,1366677717689774082,3430068083,Giancarlo Gatti,"['Our new paper in #arxiv today: ""Random Access codes via quantum contextual redundancy"", with\nD. Huerga, @KikeSolanoPhys &amp; @qmisanz \n<LINK>. We propose a protocol to compress classical information in quantum systems, leveraging redundancy in quantum contexts (1/2) <LINK>', 'This protocol shows quantum advantage over state-of-the-art information storage capacity (a cloud server of 1 billion users with 100GB each) for more than 44 qubits. Also, just for fun, systems above 100 qubits would be sufficient to encode a brute force solution for chess. (2/2)']",https://arxiv.org/abs/2103.01204,"We propose a protocol to encode classical bits in the measurement statistics of a set of parity observables, leveraging quantum contextual relations for a random access code task. The intrinsic information redundancy of quantum contexts allows for a posterior decoding protocol that requires few samples when encoding the information in a set of highly entangled states, which can be generated by a discretely-parametrized quantum circuit. Applications of this protocol include algorithms involving storage of large amounts of data but requiring only partial retrieval of the information, as is the case of decision trees. This classical-to-quantum encoding is a compression protocol for more than $18$ qubits and shows quantum advantage over state-of-the-art information storage capacity for more than $44$ qubits. In particular, systems above $100$ qubits would be sufficient to encode a brute force solution for games of chess-like complexity. ",Random access codes via quantum contextual redundancy
176,1366661893776097287,3236251346,Mikel Sanz,"['New paper in #arxiv today: ‚ÄúRandom Access codes via quantum contextual redundancy‚Äù with @Gatgian D. Huerga and @KikeSolanoPhys <LINK> we propose a coding-storage-decoding protocol which exploits information redundancy of quantum contexts. (1/2) <LINK>', 'This encoding shows quantum advantage over state-of-the-art information storage capacities for more than 44 qubits. Just for fun, systems above 100 qubits would be sufficient to encode a brute-force solution for games of chess-like complexity @upvehu @Ikerbasque @meetIQM (2/2)']",https://arxiv.org/abs/2103.01204,"We propose a protocol to encode classical bits in the measurement statistics of a set of parity observables, leveraging quantum contextual relations for a random access code task. The intrinsic information redundancy of quantum contexts allows for a posterior decoding protocol that requires few samples when encoding the information in a set of highly entangled states, which can be generated by a discretely-parametrized quantum circuit. Applications of this protocol include algorithms involving storage of large amounts of data but requiring only partial retrieval of the information, as is the case of decision trees. This classical-to-quantum encoding is a compression protocol for more than $18$ qubits and shows quantum advantage over state-of-the-art information storage capacity for more than $44$ qubits. In particular, systems above $100$ qubits would be sufficient to encode a brute force solution for games of chess-like complexity. ",Random access codes via quantum contextual redundancy
177,1366621774738124801,129155606,Shahroz Tariq,"['Happy to share our new #preprint on #arxiv ‚ÄúAm I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack‚Äù \nPaper: <LINK>\n\nIt is an extensive #measurement and #evaluation of Commercial Face Recognition #WebAPIs', 'against #Deepfakes. We also propose a preliminary defense mechanism against the #DeepfakeImpersonationAttack and compare several defense strategies. Finally, we are also releasing two new #DeepfakeDataset for evaluation. \n\n#impersonationattack #facerecognition']",http://arxiv.org/abs/2103.00847,"Recently, significant advancements have been made in face recognition technologies using Deep Neural Networks. As a result, companies such as Microsoft, Amazon, and Naver offer highly accurate commercial face recognition web services for diverse applications to meet the end-user needs. Naturally, however, such technologies are threatened persistently, as virtually any individual can quickly implement impersonation attacks. In particular, these attacks can be a significant threat for authentication and identification services, which heavily rely on their underlying face recognition technologies' accuracy and robustness. Despite its gravity, the issue regarding deepfake abuse using commercial web APIs and their robustness has not yet been thoroughly investigated. This work provides a measurement study on the robustness of black-box commercial face recognition APIs against Deepfake Impersonation (DI) attacks using celebrity recognition APIs as an example case study. We use five deepfake datasets, two of which are created by us and planned to be released. More specifically, we measure attack performance based on two scenarios (targeted and non-targeted) and further analyze the differing system behaviors using fidelity, confidence, and similarity metrics. Accordingly, we demonstrate how vulnerable face recognition technologies from popular companies are to DI attack, achieving maximum success rates of 78.0% and 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with any celebrity) attacks, respectively. Moreover, we propose practical defense strategies to mitigate DI attacks, reducing the attack success rates to as low as 0% and 0.02% for targeted and non-targeted attacks, respectively. ","Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web
  APIs under Deepfake Impersonation Attack"
178,1366580212188672000,767659609,Yoshihiko Hasegawa,"['Our new paper by @frrfluid and @unlimitcycle entitled ""Accelerated Jarzynski Estimator with Deterministic Virtual Trajectories"" appeared in arXiv. <LINK>']",https://arxiv.org/abs/2103.00529,"The Jarzynski estimator is a powerful tool that uses nonequilibrium statistical physics to numerically obtain partition functions of probability distributions. The estimator reconstructs partition functions with trajectories of the simulated Langevin dynamics through the Jarzynski equality. However, the original estimator suffers from slow convergence because it depends on rare trajectories of stochastic dynamics. In this paper, we present a method to significantly accelerate the convergence by introducing deterministic virtual trajectories generated in augmented state space under the Hamiltonian dynamics. We theoretically show that our approach achieves second-order acceleration compared to a naive estimator with the Langevin dynamics and zero variance estimation on harmonic potentials. We also present numerical experiments on three multimodal distributions and a practical example where the proposed method outperforms the conventional method, and provide theoretical explanations. ",Accelerated Jarzynski Estimator with Deterministic Virtual Trajectories
179,1366574185062006787,321794593,Jos√© G. Fern√°ndez-Trincado,"['Our new paper on ArXiv today üëâüèª""Estimating dust attenuation from galactic spectra. II. Stellar and gas attenuation in star-forming and diffuse ionized gas regions in MaNGA"" üëâüèª<LINK>']",https://arxiv.org/abs/2103.00666,"We investigate the dust attenuation in both stellar populations and ionized gas in kpc-scale regions in nearby galaxies, using integral field spectroscopy data from MaNGA MPL-9. We identify star-forming (HII) and diffuse ionized gas (DIG) regions from MaNGA datacubes. From the stacked spectrum of each region, we measure the stellar attenuation, $E(B-V)_{\rm star}$, using the technique developed by Li et al.(2020), as well as the gas attenuation, $E(B-V)_{\rm gas}$, from the Balmer decrement. We then examine the correlation of $E(B-V)_{\rm star}$, $E(B-V)_{\rm gas}$, $E(B-V)_{\rm gas}-E(B-V)_{\rm star}$ and $E(B-V)_{\rm star}/E(B-V)_{\rm gas}$ with 16 regional/global properties, and for regions with different $\rm H{\alpha}$ surface brightnesses ($\Sigma_{\rm H\alpha}$). We find a stronger correlation between $E(B-V)_{\rm star}$ and $E(B-V)_{\rm gas}$ in regions of higher $\Sigma_{\rm H\alpha}$. Luminosity-weighted age ($t_L$) is found to be the property that is the most strongly correlated with $E(B-V)_{\rm star}$, and consequently with $E(B-V)_{\rm gas}-E(B-V)_{\rm star}$ and $E(B-V)_{\rm star}/E(B-V)_{\rm gas}$. At fixed $\Sigma_{\rm H\alpha}$, $\log_{10}t_L$ is linearly and negatively correlated with $E(B-V)_{\rm star}/E(B-V)_{\rm gas}$ at all ages. Gas-phase metallicity and ionization level are important for the attenuation in the gas. Our results indicate that the ionizing source for DIG regions is likely distributed in the outer-skirt of galaxies, while for HII regions our results can be well explained by the two-component dust model of Charlot & Fall (2000). ","Estimating dust attenuation from galactic spectra. II. Stellar and gas
  attenuation in star-forming and diffuse ionized gas regions in MaNGA"
180,1381715191386234880,874776735824453632,Erin Amira Kara,"['#BlackHoleWeek seems like a good time to post about a new paper published in ApJL by third year grad student @JingyiWangAstro on the #BlackHole that keeps on giving, MAXI J1820+070, observed with #NICER on the ISS @ISS_Research <LINK>', 'Jingyi had the idea to search for the signature of reverberation light echoes during the hard-to-soft state transition in order to help understand what happens during this pivotal moment in the black hole outburst. https://t.co/qywoWYqhtY', 'She found the reverberation lags became longer during the transition. Modeling w/ the General Relativsitic ray-tracing model, reltrans, by A. Ingram and G. Mastroserio, she inferred the longer lags as due an increasing height of the corona https://t.co/1oiODORSCo', 'Interestingly, the inferred coronal height increase preceded a large radio flare by 5 days, which may suggest that during state transitions the corona expands vertically and launches jet knot that propagates along the jet stream at relativistic speeds. https://t.co/qAmZFld8HD', 'It was a pleasure to work with Jingyi and awesome collaborators on this work, including @jaj_garcia, Gullo Mastroserio, Adam Ingram, Matteo Lucchini, Michiel van der Klis, and way more folks who also need to get on Twitter ;) #ProudSupervisor']",https://arxiv.org/abs/2103.05616,"We analyze 5 epochs of NICER data of the black hole X-ray binary MAXI J1820+070 during the bright hard-to-soft state transition in its 2018 outburst with both reflection spectroscopy and Fourier-resolved timing analysis. We confirm the previous discovery of reverberation lags in the hard state, and find that the frequency range where the (soft) reverberation lag dominates decreases with the reverberation lag amplitude increasing during the transition, suggesting an increasing X-ray emitting region, possibly due to an expanding corona. By jointly fitting the lag-energy spectra in a number of broad frequency ranges with the reverberation model reltrans, we find the increase in reverberation lag is best described by an increase in the X-ray coronal height. This result, along with the finding that the corona contracts in the hard state, suggests a close relationship between spatial extent of the X-ray corona and the radio jet. We find the corona expansion (as probed by reverberation) precedes a radio flare by ~5 days, which may suggest that the hard-to-soft transition is marked by the corona expanding vertically and launching a jet knot that propagates along the jet stream at relativistic velocities. ","Disk, Corona, Jet Connection in the Intermediate State of MAXI J1820+070
  Revealed by NICER Spectral-Timing Analysis"
181,1379112544741261317,1204845487263752198,Ning Yu,"['New paper involving various improvements for GANs: dual contrastive loss, self-attention in the generator, and reference-attention in the discriminator. Joint work with @GuilinL, @aysegl_dndr, @drewtao, @ctnzr, Larry Davis, and Mario Fritz. More details in <LINK> <LINK>']",https://arxiv.org/abs/2103.16748,"Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fr\'{e}chet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID). Code and models are available at this https URL . ",Dual Contrastive Loss and Attention for GANs
182,1377777742339399682,874887507174981633,Ashish Vaswani,"['(1/5) In our recent CVPR paper, we develop a new family of parameter-efficient local self-attention models, HaloNets, that outperform EfficientNet in the parameter-accuracy tradeoff on ImageNet.  <LINK>. <LINK>', '(2/5) In addition to strong results on ImageNet, we also see promising improvements (up to 4.4x inference speedups) over strong baselines when pretrained on ImageNet-21k with comparable settings. https://t.co/HMPBHVrofd', '(3 / 5) In previous work (https://t.co/wneJk9RTAc), we used pixel-centered windows, similar to convolutions. Here, we develop a block centered formulation for better efficiency on matrix accelerators. https://t.co/JyeRRXWmWn', '(4/5) When applied to detection and instance segmentation, our local self-attention improves on top of strong convolutional baselines. Interestingly, local self-attention with 14x14 receptive fields performs nearly as well as 35x35. https://t.co/y7D5nfvgdg', '(5/5) Joint work with Prajit Ramachandran, @AravSrinivas , @nikiparmar09 , @BlakeHechtman , and Jonathon Shlens.']",https://arxiv.org/abs/2103.12731,"Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models. ",Scaling Local Self-Attention for Parameter Efficient Visual Backbones
183,1376886766846808070,593911501,James Cadman,"['Could the planets in AB Aurigae‚Äôs protoplanetary disc have formed through the gravitational instability? The system‚Äôs young age and high planet masses may suggest so. In our new paper (on arXiv today!) we look into this possibility... <LINK> <LINK>', '@theresphysics @cassidentprone @RoyalObs @PhysAstroEd']",https://arxiv.org/abs/2103.14945,"Recent observations of the protoplanetary disc surrounding AB Aurigae have revealed the possible presence of two giant planets in the process of forming. The young measured age of $1-4$Myr for this system allows us to place strict time constraints on the formation histories of the observed planets. Hence we may be able to make a crucial distinction between formation through core accretion (CA) or the gravitational instability (GI), as CA formation timescales are typically Myrs whilst formation through GI will occur within the first $\approx10^4-10^5$yrs of disc evolution. We focus our analysis on the $4-13$M$_{\rm Jup}$ planet observed at $R\approx30$AU. We find CA formation timescales for such a massive planet typically exceed the system's age. The planet's high mass and wide orbit may instead be indicative of formation through GI. We use smoothed particle hydrodynamic simulations to determine the system's critical disc mass for fragmentation, finding $M_{\rm d,crit}=0.3$M$_{\odot}$. Viscous evolution models of the disc's mass history indicate that it was likely massive enough to exceed $M_{\rm d,crit}$ in the recent past, thus it is possible that a young AB Aurigae disc may have fragmented to form multiple giant gaseous protoplanets. Calculations of the Jeans mass in an AB Aurigae-like disc find that fragments may initially form with masses $1.6-13.3$M$_{\rm Jup}$, consistent with the planets which have been observed. We therefore propose that the inferred planets in the disc surrounding AB Aurigae may be evidence of planet formation through GI. ","AB Aurigae: Possible evidence of planet formation through the
  gravitational instability"
184,1376798959793008641,1194794814690086912,Andrea Skolik,"['Can we teach a quantum computer to balance a pole? Find out which architectural choices are crucial to making quantum agents succeed at deep Q-learning in our new paper: <LINK> <LINK>', 'We specifically show how the choice of observables will make or break Q-learning with PQCs, and how to make an informed choice by what we know about the optimal Q-values.']",https://arxiv.org/abs/2103.15084,"Quantum machine learning (QML) has been identified as one of the key fields that could reap advantages from near-term quantum devices, next to optimization and quantum chemistry. Research in this area has focused primarily on variational quantum algorithms (VQAs), and several proposals to enhance supervised, unsupervised and reinforcement learning (RL) algorithms with VQAs have been put forward. Out of the three, RL is the least studied and it is still an open question whether VQAs can be competitive with state-of-the-art classical algorithms based on neural networks (NNs) even on simple benchmark tasks. In this work, we introduce a training method for parametrized quantum circuits (PQCs) that can be used to solve RL tasks for discrete and continuous state spaces based on the deep Q-learning algorithm. We investigate which architectural choices for quantum Q-learning agents are most important for successfully solving certain types of environments by performing ablation studies for a number of different data encoding and readout strategies. We provide insight into why the performance of a VQA-based Q-learning algorithm crucially depends on the observables of the quantum model and show how to choose suitable observables based on the learning task at hand. To compare our model against the classical DQN algorithm, we perform an extensive hyperparameter search of PQCs and NNs with varying numbers of parameters. We confirm that similar to results in classical literature, the architectural choices and hyperparameters contribute more to the agents' success in a RL setting than the number of parameters used in the model. Finally, we show when recent separation results between classical and quantum agents for policy gradient RL can be extended to inferring optimal Q-values in restricted families of environments. ","Quantum agents in the Gym: a variational quantum algorithm for deep
  Q-learning"
185,1376449805849886722,1133734070293323776,Hammer Lab ML,['New paper alertü•≥ A preprint of our work on fairness and robustness of #ContrastiveExplanations by Andr√© Artelt and Barbara Hammer is available on arXiv: <LINK>\n#MachineLearning #ExplainableAI'],https://arxiv.org/abs/2103.02354,"Transparency is a fundamental requirement for decision making systems when these should be deployed in the real world. It is usually achieved by providing explanations of the system's behavior. A prominent and intuitive type of explanations are counterfactual explanations. Counterfactual explanations explain a behavior to the user by proposing actions -- as changes to the input -- that would cause a different (specified) behavior of the system. However, such explanation methods can be unstable with respect to small changes to the input -- i.e. even a small change in the input can lead to huge or arbitrary changes in the output and of the explanation. This could be problematic for counterfactual explanations, as two similar individuals might get very different explanations. Even worse, if the recommended actions differ considerably in their complexity, one would consider such unstable (counterfactual) explanations as individually unfair. In this work, we formally and empirically study the robustness of counterfactual explanations in general, as well as under different models and different kinds of perturbations. Furthermore, we propose that plausible counterfactual explanations can be used instead of closest counterfactual explanations to improve the robustness and consequently the individual fairness of counterfactual explanations. ",Evaluating Robustness of Counterfactual Explanations
186,1375968978250567681,3877821072,Shuai Tang,"['Check out our new paper on linearising neural networks for fast adaptation. paper: <LINK>  code: <LINK>', '1) gradients w.r.t. params of a pretrained neural network as features for data samples. \n2) degenerated GP as the prediction model with uncertainty estimation for a new task', 'the hardest part is to make the linear system efficient.  wesley made it possible through his novel implementation of scalable Fisher vector product', 'coauthors made this pic to illustrate the high-level idea :D @ wesley (well, he is not twitter) @andrewgwils @pgmoren @adamianou https://t.co/tqBx2sfeeV']",https://arxiv.org/abs/2103.01439,"The inductive biases of trained neural networks are difficult to understand and, consequently, to adapt to new settings. We study the inductive biases of linearizations of neural networks, which we show to be surprisingly good summaries of the full network functions. Inspired by this finding, we propose a technique for embedding these inductive biases into Gaussian processes through a kernel designed from the Jacobian of the network. In this setting, domain adaptation takes the form of interpretable posterior inference, with accompanying uncertainty estimation. This inference is analytic and free of local optima issues found in standard techniques such as fine-tuning neural network weights to a new task. We develop significant computational speed-ups based on matrix multiplies, including a novel implementation for scalable Fisher vector products. Our experiments on both image classification and regression demonstrate the promise and convenience of this framework for transfer learning, compared to neural network fine-tuning. Code is available at this https URL ",Fast Adaptation with Linearized Neural Networks
187,1375459092622376963,1117352757793632263,Alon Cohen,"['Check out our new paper!\nTight regret for online reinforcement learning of Stochastic Shortest path:\n<LINK>\nJoint work with Yonatan Efroni, Yishay Mansour and Aviv Rosenberg.']",https://arxiv.org/abs/2103.13056,"We study the Stochastic Shortest Path (SSP) problem in which an agent has to reach a goal state in minimum total expected cost. In the learning formulation of the problem, the agent has no prior knowledge about the costs and dynamics of the model. She repeatedly interacts with the model for $K$ episodes, and has to minimize her regret. In this work we show that the minimax regret for this setting is $\widetilde O(\sqrt{ (B_\star^2 + B_\star) |S| |A| K})$ where $B_\star$ is a bound on the expected cost of the optimal policy from any state, $S$ is the state space, and $A$ is the action space. This matches the $\Omega (\sqrt{ B_\star^2 |S| |A| K})$ lower bound of Rosenberg et al. [2020] for $B_\star \ge 1$, and improves their regret bound by a factor of $\sqrt{|S|}$. For $B_\star < 1$ we prove a matching lower bound of $\Omega (\sqrt{ B_\star |S| |A| K})$. Our algorithm is based on a novel reduction from SSP to finite-horizon MDPs. To that end, we provide an algorithm for the finite-horizon setting whose leading term in the regret depends polynomially on the expected cost of the optimal policy and only logarithmically on the horizon. ",Minimax Regret for Stochastic Shortest Path
188,1374872523221782528,1354198150072786946,alewkowycz,"['New paper out <LINK>! We explore when learning rate schedules are beneficial. Main points:\na) We present ABEL: a schedule which decays the learning rate automatically after the weight norm ""bounces"". It is as good and more robust than tuned schedules.\n1/2 <LINK>', 'b) A simple schedule where one decays the learning rate at the end of training is as good as more complex schedules in setups where the weight norm does not ""bounce"", like Transformer architectures without L2 regularization.\n\nCheck the paper for more details!\n2/2 https://t.co/whYXpGxRVC']",https://arxiv.org/abs/2103.12682,"Complex learning rate schedules have become an integral part of deep learning. We find empirically that common fine-tuned schedules decay the learning rate after the weight norm bounces. This leads to the proposal of ABEL: an automatic scheduler which decays the learning rate by keeping track of the weight norm. ABEL's performance matches that of tuned schedules and is more robust with respect to its parameters. Through extensive experiments in vision, NLP, and RL, we show that if the weight norm does not bounce, we can simplify schedules even further with no loss in performance. In such cases, a complex schedule has similar performance to a constant learning rate with a decay at the end of training. ",How to decay your learning rate
189,1374446116452769795,317422543,Ricard Sol√©,"['Cooperation  is a crucial part of complexity, and can be engineered using #synbio . What is the role of randomness in shaping it? Here\'s our new paper with @JordiPinero and @sfiscience Sidney Redner on a ""Stochastic theory of two-species cooperators"" <LINK> <LINK>']",https://arxiv.org/abs/2103.10976,"Cooperative interactions pervade in a broad range of many-body populations, such as ecological communities, social organizations, and economic webs. We investigate the dynamics of a population of two equivalent species A and B that are driven by cooperative and symmetric interactions between these species. For an isolated population, we determine the probability to reach fixation, where only one species remains, as a function of the initial concentrations of the two species, as well as the time to reach fixation. The latter scales exponentially with the population size. When members of each species migrate into the population at rate $\lambda$ and replace a randomly selected individual, surprisingly rich dynamics ensues. Ostensibly, the population reaches a steady state, but the steady-state population distribution undergoes a unimodal to trimodal transition as the migration rate decreases below a critical value $\lambda_c$. In the low-migration regime, $\lambda<\lambda_c$, the steady state is not truly steady, but instead strongly fluctuates between near-fixation states, where the population consists of mostly A's or of mostly B's. The characteristic time scale of these fluctuations diverges as $\lambda^{-1}$. Thus in spite of the cooperative interaction, a typical snapshot of the population will contain almost all A's or almost all B's. ",Fixation and Fluctuations in Two-Species Cooperation
190,1374366178966142980,1056626853652426752,Jonathan Herzig,"['1/4 Much focus is given to dense retrieval of textual passages, but how should we design retrievers for tables in the context of open domain QA?\n\nNew #NAACL2021 short paper: <LINK>\n\nWith @muelletm, Syrine Krichene and @eisenjulian', '2/4 We tackle open-domain QA over tables, and show that a BERT based retriever can be improved by a retriever designed to handle tabular context based on TAPAS. \n\nWe present an effective pre-training procedure for our retriever and improve its quality with mined hard negatives.', '3/4 We extract an 11K subset of Natural Questions where answers reside in a table into a Table QA dataset (called NQ-tables), and find that our retriever improves both retrieval results and end-to-end QA results over BERT and BM25 baselines.', '4/4 Our code for generating the NQ-tables dataset, and model checkpoints will be available soon in our repository:\n\nhttps://t.co/Nh6mWkaUiL', 'And here is a figure as well üòÄ: https://t.co/5MB8kpEiy3']",https://arxiv.org/abs/2103.12011,"Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever. ",Open Domain Question Answering over Tables via Dense Retrieval
191,1374079898512465922,1133565755637657601,Aaron M. Lattanzi,"['New paper up on @arxiv <LINK> with Vahid Tavanashad, Shankar Subramaniam &amp; @jesse_caps! We close and implement a stochastic model for drag perturbations induced by neighboring particles. The new stochastic EL framework stacks up well to PR-DNS simulations.']",https://arxiv.org/abs/2103.10581,"Standard Eulerian--Lagrangian (EL) methods generally employ drag force models that only represent the mean hydrodynamic force acting upon a particle-laden suspension. Consequently, higher-order drag force statistics, arising from neighbor-induced flow perturbations, are not accounted for; with implications on predictions for particle velocity variance and dispersion. We develop a force Langevin (FL) model that treats neighbor-induced drag fluctuations as a stochastic force within an EL framework. The stochastic drag force follows an Ornstein-Uhlenbeck process and requires closure of the integral time scale for the fluctuating hydrodynamic force and the standard deviation in drag. The former is closed using the mean-free time between successive collisions, derived from the kinetic theory of non-uniform gases. For the latter, particle-resolved direct numerical simulation (PR--DNS) of fixed particle assemblies is utilized to develop a correlation. The stochastic EL framework specifies unresolved drag force statistics, leading to the correct evolution and sustainment of particle velocity variance over a wide range of Reynolds numbers and solids volume fractions when compared to PR--DNS of freely-evolving homogeneous suspensions. By contrast, standard EL infers drag statistics from variations in the resolved flow and thus under-predicts the growth and steady particle velocity variance in homogeneous suspensions. Velocity statistics from standard EL approaches are found to depend on the bandwidth of the projection function used for two-way momentum coupling, while results obtained from the stochastic EL approach are insensitive to the projection bandwidth. ","A stochastic model for the hydrodynamic force in Euler--Lagrange
  simulations of particle-laden flows"
192,1372902714464739329,839948365622300672,Juri Smirnov üåª,"['New Paper(s) on the exciting topic of dark #baryons! \n\nWith P. Asadi, E. D. Kramer, @EKuflik, Gr. W. Ridgway and T.R. Slatyer, thank you all for a fun project! \n\nThe idea: <LINK>\n\nThe details: <LINK>\n\nQuick video Summary: <LINK>', 'In a nutshell:\n\n1) In the heavy #quark regime the #phasetransition is the same as in pure Yang-Mills theories and so it is 1. Order.\nIt proceeds by #bubble nucleation and growth. The quarks, that are frozen out at this point, are squeezed in pockets of the wrong vacuum. https://t.co/D4C3RoIloc', '2) This results in a re-coupling of interactions and a second stage of massive quark and anti-quark depletion. However, in each pocket, just accidentally there will be quarks or anti-quarks that will not have a partner antiparticle to annihilate with ;-( https://t.co/4Fjy8A0fHR', '3) At the end of the day the correct #darkmatter abundance is reproduced at very large dark matter masses. So the new dark matter forming baryons could have significant interactions with the standard model. It‚Äôs intriguing to think of new #experiments that will look for those! https://t.co/laZYN59R6e', 'I will discuss the work in our informal Lunch Seminar at 11:45 EST.\nIf you are interested, come and hang out with us @osuccapp  \ndetails: https://t.co/VBFiyku47C']",https://arxiv.org/abs/2103.09822,"We study the effect of a first-order phase transition in a confining $SU(N)$ dark sector with heavy dark quarks. The baryons of this sector are the dark matter candidate. During the confinement phase transition the heavy quarks are trapped inside isolated, contracting pockets of the deconfined phase, giving rise to a second stage of annihilation that dramatically suppresses the dark quark abundance. The surviving abundance is determined by the local accidental asymmetry in each pocket. The correct dark matter abundance is obtained for $\mathcal{O}(1-100)$ PeV dark quarks, above the usual unitarity bound. ",Accidentally Asymmetric Dark Matter
193,1372235351062892548,465662176,Fabio Poiesi,['Check out our new paper to see how we synchronise consumer augmented reality mobiles and use their video streams to perform motion capture in the wild\n\npaper: <LINK>\ndataset: <LINK>\ncode: available soonüòÖ\na result ‚¨áÔ∏è\n\n#AugmentedReality #4dvideo <LINK>'],https://arxiv.org/abs/2103.07883,"We propose a system to capture nearly-synchronous frame streams from multiple and moving handheld mobiles that is suitable for dynamic object 3D reconstruction. Each mobile executes Simultaneous Localisation and Mapping on-board to estimate its pose, and uses a wireless communication channel to send or receive synchronisation triggers. Our system can harvest frames and mobile poses in real time using a decentralised triggering strategy and a data-relay architecture that can be deployed either at the Edge or in the Cloud. We show the effectiveness of our system by employing it for 3D skeleton and volumetric reconstructions. Our triggering strategy achieves equal performance to that of an NTP-based synchronisation approach, but offers higher flexibility, as it can be adjusted online based on application needs. We created a challenging new dataset, namely 4DM, that involves six handheld augmented reality mobiles recording an actor performing sports actions outdoors. We validate our system on 4DM, analyse its strengths and limitations, and compare its modules with alternative ones. ","Multi-view data capture for dynamic object reconstruction using handheld
  augmented reality mobiles"
194,1371262657840492547,1328498047513407488,Dr. Natalia Lewandowska,"['For those of you who asked me about the former Arecibo telescope in the last couple of months and what will come next: \n\nHere is our white paper about a new Arecibo telescope:\n\n<LINK>\n\nIn short, we are working on it üòé']",https://arxiv.org/abs/2103.01367,"The Arecibo Observatory (AO) is a multidisciplinary research and education facility that is recognized worldwide as a leading facility in astronomy, planetary, and atmospheric and space sciences. AO's cornerstone research instrument was the 305-m William E. Gordon telescope. On December 1, 2020, the 305-m telescope collapsed and was irreparably damaged. In the three weeks following the collapse, AO's scientific and engineering staff and the AO users community initiated extensive discussions on the future of the observatory. The community is in overwhelming agreement that there is a need to build an enhanced, next-generation radar-radio telescope at the AO site. From these discussions, we established the set of science requirements the new facility should enable. These requirements can be summarized briefly as: 5 MW of continuous wave transmitter power at 2 - 6 GHz, 10 MW of peak transmitter power at 430 MHz (also at 220MHz under consideration), zenith angle coverage 0 to 48 deg, frequency coverage 0.2 to 30 GHz and increased Field-of-View. These requirements determine the unique specifications of the proposed new instrument. The telescope design concept we suggest consists of a compact array of fixed dishes on a tiltable, plate-like structure with a collecting area equivalent to a 300m dish. This concept, referred to as the Next Generation Arecibo Telescope (NGAT), meets all of the desired specifications and provides significant new science capabilities to all three research groups at AO. This whitepaper presents a sample of the wide variety of the science that can be achieved with the NGAT, the details of the telescope design concept and the need for the new telescope to be located at the AO site. We also discuss other AO science activities that interlock with the NGAT in the white paper. ","The Future Of The Arecibo Observatory: The Next Generation Arecibo
  Telescope"
195,1370764928466964484,1057303533622575106,Tuhin Chakrabarty,"['üòïReframing arguments to reflect different connotations but same denotations üôÇ New #NAACL2021 paper  titled ‚ÄúENTRUST: Argument Reframing with Language Models and Entailment‚Äù\n<LINK>\nJoint work with @chridey and Smaranda Muresan\n#NLProc <LINK>', 'Differences in lexical framing, the focus of our work, can have large effects on peoples‚Äô opinions and beliefs. For example ""illegal aliens"" vs ""undocumented workers"". To reframe arguments', 'We use a lexical resource for ‚Äúconnotations‚Äù (https://t.co/lZTvppp2kZ)  to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post decoding entailment component (same denotation).', 'We evaluate our approach on two different tasks (reframing partisan arguments and appeal to fear/prejudice fallacies) showing that our\nmethod is preferred over several competing baseline']",https://arxiv.org/abs/2103.06758,"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Entman 1983). Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for ""connotations"" to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear. ",ENTRUST: Argument Reframing with Language Models and Entailment
196,1369232600720670720,494870213,Thomas Haworth,"[""New paper today where I take a look at how dust in discs gets warmed near massive stars. If this isn't accounted for we end up overestimating how massive the disc is... 1/2  \n\n<LINK> <LINK>"", 'This means that if radiation from massive stars reduces the disc mass, we could be suppressing the signature of that if we assume that the dust in discs is relatively cold (or assume the same dust temperature for all discs) https://t.co/3LPC4gxt19']",https://arxiv.org/abs/2103.03950,"Dust plays a key role in the formation of planets and its emission also provides one of our most accessible views of protoplanetary discs. If set by radiative equilibrium with the central star, the temperature of dust in the disc plateaus at around $10-20$K in the outer regions. However sufficiently nearby massive stars can heat the outer disc to substantially higher temperatures. In this paper we study the radiative equilibrium temperature of discs in the presence of massive external sources and gauge the effect that it has on millimetre dust mass estimates. Since millimetre grains are not entrained in any wind we focus on geometrically simple 2D-axisymmetric disc models using radiative transfer calculations with both the host star and an external source. Recent surveys have searched for evidence of massive stars influencing disc evolution using disc properties as a function of projected separation. In assuming a disc temperature of $20$K for a disc a distance $D$ from a strong radiation source, disc masses are overestimated by a factor that scales with $D^{-1/2}$ interior to the separation that external heating becomes important. This could significantly alter dust mass estimates of discs in close proximity to $\theta^1$C in the Orion Nebular Cluster. We also make an initial assessment of the effect upon snow lines. Within a parsec of an O star like $\theta^1$C a CO snow line no longer exists, though the water snow line is virtually unaffected except for very close separations of $\leq0.01\,$pc. ",Warm millimetre dust in protoplanetary discs near massive stars
197,1369195296912211969,1258092781232427009,Paul Breiding,['New preprint: we show that sensitivity of low rank matrix approximation depends on the singular value gap: <LINK>‚Ä¶\n\n... And we cite the paper ‚ÄúLow-rank matrix approximation do not need a singular value gap.‚Äù ü§∑\u200d‚ôÇÔ∏è \n#mathtwitter #lowrankmatrices'],https://arxiv.org/abs/2103.00531,"We characterize the first-order sensitivity of approximately recovering a low-rank matrix from linear measurements, a standard problem in compressed sensing. A special case covered by our analysis is approximating an incomplete matrix by a low-rank matrix. We give an algorithm for computing the associated condition number and demonstrate experimentally how the number of linear measurements affects it. In addition, we study the condition number of the rank-r matrix approximation problem. It measures in the Frobenius norm by how much an infinitesimal perturbation to an arbitrary input matrix is amplified in the movement of its best rank-r approximation. We give an explicit formula for the condition number, which shows that it does depend on the relative singular value gap between the rth and (r+1)th singular values of the input matrix. ",Sensitivity of low-rank matrix recovery
198,1368852243747901442,1156374257523462150,Elisa Garro,"[""From today there is a new globular cluster, located in the Galactic bulge: #Patchick99. We use the near-IR #VVV and optical #GaiaDR2 datasets. To find out more, read the paper, accepted by A&amp;A and published today on #arXiv: <LINK>\nIt's a great #WomensDay! <LINK>""]",https://arxiv.org/abs/2103.03592,"Globular clusters (GCs) are important tools to understand the formation and evolution of the Milky Way (MW). The known MW sample is still incomplete, so the discovery of new GC candidates and the confirmation of their nature are crucial for the census of the MW GC system. Our goal is to confirm the physical nature of two GC candidates: Patchick99 and TBJ3, located towards the Galactic bulge. We use public data in the near-IR from the VVV, VVVX and 2MASS along the with deep optical data from the Gaia DR2, in order to estimate their main physical parameters: reddening, extinction, distance, luminosity, mean cluster proper motions (PMs), size, metallicity and age. We investigate both candidates at different wavelengths. We use near-IR and optical CMDs in order to analyse Patchick99. We decontaminate CMDs following a statistical procedure and PM-selection. Reddening and extinction are derived by adopting reddening maps. Metallicity and age are evaluated by fitting stellar isochrones. Reddening and extinction are E(J-Ks)=0.12+/-0.02 mag, AKs=0.09+/-0.01 mag from the VVV data, whereas E(BP-RP)=0.21+/-0.03 mag, AG=0.68+/-0.08 mag from Gaia DR2. We estimate a distance d=6.4+/-0.2 kpc in near-IR and D=7.0+/-0.2 kpc in optical. We derive its metallicity and age fitting PARSEC isochrones, finding [Fe/H]=-0.2+/-0.2 dex and t=10+/-2 Gyr. The mean PMs for Patchick99 are pmRA=-298+/-1.74 mas/yr and pmDEC=-5.49+/-2.02 mas/yr. We confirm that it is a low-luminosity GC, with MKs=-7.0+/-0.6 mag. The radius estimation is performed building the radial density profile, finding r~10'. We recognise 7 RR Lyrae star members within 8.2 arcmin from its centre, confirming the distance found by other methods. We found that TBJ3 shows mid-IR emissions that are not present in GCs. We discard TBJ3 as GC candidate and we focus on Patchick99. We conclude that it is an old metal-rich GC, situated in the Galactic bulge. ","Confirmation and physical characterization of the new bulge globular
  cluster Patchick 99 from the VVV and Gaia surveys"
199,1367646761494450176,1047899041311412224,Francois Grondin,"['Here\'s the preprint of our new paper ""Audio scene monitoring using redundant un-localized microphone arrays"". I had the privilege to collaborate with researchers at University of California San Diego, including Pr. Peter Gerstoft and Pr. Yoav Freund.\n\n<LINK>']",https://arxiv.org/abs/2103.01830,"We present a system for localizing sound sources in a room with several ad-hoc microphone arrays. Each circular array performs direction of arrival (DOA) estimation independently using commercial software. The DOAs are fed to a fusion center, concatenated, and used to perform the localization based on two proposed methods, which require only few labeled source locations (anchor points) for training. The first proposed method is based on principal component analysis (PCA) of the observed DOA and does not require any knowledge of anchor points. The array cluster can then perform localization on a manifold defined by the PCA of concatenated DOAs over time. The second proposed method performs localization using an affine transformation between the DOA vectors and the room manifold. The PCA has fewer requirements on the training sequence, but is less robust to missing DOAs from one of the arrays. The methods are demonstrated with five IoT 8-microphone circular arrays, placed at unspecified fixed locations in an office. Both the PCA and the affine method can easily map out a rectangle based on a few anchor points with similar accuracy. The proposed methods provide a step towards monitoring activities in a smart home and require little installation effort as the array locations are not needed. ",Audio scene monitoring using redundant ad-hoc microphone array networks
200,1366945466219388928,1012125662117851136,Edward Kennedy,"[""Cool new paper by @leqi_liu!\n\nMean optimal trt rules aren't robust: sensitive to small % w/ extreme outcomes\n\nMarginal median opt rules are unfair in different way: my treatment depends on *your* outcomes\n\nWe study fair+robust median optimal trt rules:\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2103.01802,"Optimal treatment regimes are personalized policies for making a treatment decision based on subject characteristics, with the policy chosen to maximize some value. It is common to aim to maximize the mean outcome in the population, via a regime assigning treatment only to those whose mean outcome is higher under treatment versus control. However, the mean can be an unstable measure of centrality, resulting in imprecise statistical procedures, as well as unrobust decisions that can be overly influenced by a small fraction of subjects. In this work, we propose a new median optimal treatment regime that instead treats individuals whose conditional median is higher under treatment. This ensures that optimal decisions for individuals from the same group are not overly influenced either by (i) a small fraction of the group (unlike the mean criterion), or (ii) unrelated subjects from different groups (unlike marginal median/quantile criteria). We introduce a new measure of value, the Average Conditional Median Effect (ACME), which summarizes across-group median treatment outcomes of a policy, and which the median optimal treatment regime maximizes. After developing key motivating examples that distinguish median optimal treatment regimes from mean and marginal median optimal treatment regimes, we give a nonparametric efficiency bound for estimating the ACME of a policy, and propose a new doubly robust-style estimator that achieves the efficiency bound under weak conditions. To construct the median optimal treatment regime, we introduce a new doubly robust-style estimator for the conditional median treatment effect. Finite-sample properties are explored via numerical simulations and the proposed algorithm is illustrated using data from a randomized clinical trial in patients with HIV. ",Median Optimal Treatment Regimes
201,1366591700660072454,2885797996,Prof Jesse Capecelatro,"['New paper out on @arxiv: <LINK> Realistic coughs are pulsatile, resulting in vortex interactions that accelerate particles emanating from later pulses which may carry more virus. Congrats @MichiganAero undergrad Kalvin Monroe, @umichme Yuan and  @LattanziAaron <LINK>']",https://arxiv.org/abs/2103.00581,"Expiratory events, such as coughs, are often pulsatile in nature and result in vortical flow structures that transport respiratory particles. In this work, direct numerical simulation (DNS) of turbulent pulsatile jets, coupled with Lagrangian particle tracking of micron-sized droplets, is performed to investigate the role of secondary and tertiary expulsions on particle dispersion and penetration. Fully-developed turbulence obtained from DNS of a turbulent pipe flow is provided at the jet orifice. The volumetric flow rate at the orifice is modulated in time according to a damped sine wave; thereby allowing for control of the number of pulses, duration, and peak amplitude. The resulting vortex structures are analyzed for single-, two-, and three-pulse jets. The evolution of the particle cloud is then compared to existing single-pulse models. Particle dispersion and penetration of the entire cloud is found to be hindered by increased pulsatility. However, the penetration of particles emanating from a secondary or tertiary expulsion are enhanced due to acceleration downstream by vortex structures. ",Role of pulsatility on particle dispersion in expiratory flows
202,1377612005071331332,1912298966,Dr. L. C. Mayorga,"['üö®üö® New Paper Alert!!! üö®üö®\nMy fantastic co-authors and I studied a new class of astronomical objects! We have designated them UFOs: Understudied Floofy Objects.\n\n@_astronoMay @Of_FallingStars (and Jake Lustig-Yaeger, not on twitter)\n\n<LINK> 1/8 <LINK>', 'Thanks to twitter catstronomers we collected a large sample of these understudied floofy objects at a variety of viewing angles and illumination angles. \n\nThe raw data is public and available here:\nhttps://t.co/mDPUnu5MJX 2/8', 'Through a rigorous analysis of light curves we were able to measure the rotational variations of several dozen floofy objects which we classify into 13 categories, further identified by their subobserver longitudes. 3/8 https://t.co/hNds13leAt', 'We explored the change in brightness in three bands, as well as the overall brightness of the objects, and determined that some subtypes of floofy objects exhibit rotational variability with strong brightening at longitudes of 0. 4/8 https://t.co/Rp8bs11aAD', 'The high number of CL observations allowed us to create an average CL type rotation curve and reconstruct a representative object projected onto the spherical shape we assume. 5/8 https://t.co/ZC4Ed9kiU7', 'We also explored Floofy Objects in Color-Magnitude space and used a clustering algorithm on the 0 longitude observations to determine that there are 6 unique classes of Floofy Objects, generally separated by substellar point color and limb color. 6/8 https://t.co/PbAH3jXaUO', 'Finally, we explored potential for false pawsitives, specifically misidentification of WOOF Objects (Wagging tails On Objectively Friendly Objects) as Floofy Objects and found that high spatial resolution imaging missions like the upcoming CatEx observatory is a necessity. 7/8 https://t.co/1Ra6WMApA8', 'Check out the full paper here: https://t.co/TLUFAUkNft\n\nWe‚Äôre excited to see what future research can be done on this strange new class of astronomical objects! 8/8 https://t.co/dtvM9YSjAC', 'Addendum: and we finally convinced @LustigYaeger to get on Twitter to take his own credit!', ""@vicgrinberg @_astronomay @Of_FallingStars I didn't even know! Purrrfect! @LustigYaeger""]",https://arxiv.org/abs/2103.16636,"Phase resolved observations of planetary bodies allow us to understand the longitudinal and latitudinal variations that make each one unique. Rotational variations have been detected in several types of astronomical bodies beyond those of planetary mass, including asteroids, brown dwarfs, and stars. Unexpected rotational variations, such as those presented in this work, reminds us that the universe can be complicated, with more mysteries to uncover. In this work we present evidence for a new class of astronomical objects we identify as ""floofy"" with observational distinctions between several sub-types of these poorly understood objects. Using optical observations contributed by the community, we have identified rotational variation in several of these floofy objects, which suggests that they may have strong differences between their hemispheres, likely caused by differing reflectivity off their surfaces. Additional sub-types show no rotational variability suggesting a uniform distribution of reflective elements on the floofy object. While the work here is a promising step towards the categorization of floofy objects, further observations with more strictly defined limits on background light, illumination angles, and companion objects are necessary to develop a better understanding of the many remaining mysteries of these astronomical objects. ","Detection of Rotational Variability in Floofy Objects at Optical
  Wavelengths"
203,1377484517858975747,2577596593,Chelsea Finn,"['How can robots generalize to new environments &amp; tasks?\n\nWe find that using in-the-wild videos of people can allow learned reward functions to do so!\nPaper: <LINK>\n\nLed by @_anniechen_, @SurajNair_1\nüßµ(1/5) <LINK>', 'To get reward functions that generalize, we train domain-agnostic video discriminators (DVD) with:\n* a lot of diverse human data, and\n* a narrow &amp; small amount of robot demos\n\nThe idea is super simple: predict if two videos are performing the same task or not.\n(2/5) https://t.co/4sRhfThzkI', 'This discriminator can be used as a reward by feeding in a human video of the desired task and a video of the robot‚Äôs behavior.\n\nWe use it by planning with a learned visual dynamics model.\n(3/5) https://t.co/yQBtzwlmNi', 'Does using human videos improve reward generalization compared to using only narrow robot data?\n\nWe see:\n* 20% greater task success in new environments\n* 25% greater task success on new tasks\nboth in simulation and on a real robot.\n\n(4/5) https://t.co/S0xfHCmh3F', ""For more, check out:\nPaper: https://t.co/afz2PWw0rT\nWebsite: https://t.co/geRH3tmgTe\nSummary video: https://t.co/wg2C1lEsBG\n\nI'm quite excited about how reusing broad datasets can help robots generalize, and this project has been a great indication in that direction!\n\n(5/5) https://t.co/yVPtIjSUAp""]",https://arxiv.org/abs/2103.16817,"We are motivated by the goal of generalist robots that can complete a wide range of tasks across many environments. Critical to this is the robot's ability to acquire some metric of task success or reward, which is necessary for reinforcement learning, planning, or knowing when to ask for help. For a general-purpose robot operating in the real world, this reward function must also be able to generalize broadly across environments, tasks, and objects, while depending only on on-board sensor observations (e.g. RGB images). While deep learning on large and diverse datasets has shown promise as a path towards such generalization in computer vision and natural language, collecting high quality datasets of robotic interaction at scale remains an open challenge. In contrast, ""in-the-wild"" videos of humans (e.g. YouTube) contain an extensive collection of people doing interesting tasks across a diverse range of settings. In this work, we propose a simple approach, Domain-agnostic Video Discriminator (DVD), that learns multitask reward functions by training a discriminator to classify whether two videos are performing the same task, and can generalize by virtue of learning from a small amount of robot data with a broad dataset of human videos. We find that by leveraging diverse human datasets, this reward function (a) can generalize zero shot to unseen environments, (b) generalize zero shot to unseen tasks, and (c) can be combined with visual model predictive control to solve robotic manipulation tasks on a real WidowX200 robot in an unseen environment from a single human demo. ","Learning Generalizable Robotic Reward Functions from ""In-The-Wild"" Human
  Videos"
204,1377457383241252875,4807828837,Vishal Upendran,"['New paper with @dktripathi accepted in ApJ, is out on Arxiv today: <LINK>\n\nWe study impulsive heating of quiet solar coronal regions, observed using #AIA onboard #SDO by employing an #ML inversion of a statistical impulsive heating model. (1/3)', 'We find the small scale events to be dominant in the corona (alpha&gt;2), and extremely high frequency of events (2 events/min). \n\nCorrelations suggest conduction losses are dominant &amp; point to existence of energy reservoir depleted by impulsive events. (2/3)', 'Finally, I will let me do the talking, and take you through the work: https://t.co/414jmlmQK1\n\nThis is the talk I gave @asipoec in February. Hopefully, it inspires the Indian solar/astro community to incorporate #ML in their scientific analyses!']",https://arxiv.org/abs/2103.16824,"The solar corona consists of a million-degree Kelvin plasma. A complete understanding of this phenomenon demands the study of Quiet Sun (QS) regions. In this work, we study QS regions in the 171 {\AA}, 193 {\AA} and 211 {\AA} passbands of the Atmospheric Imaging Assembly (AIA) on board the Solar Dynamics Observatory (SDO), by combining the empirical impulsive heating forward model of Pauluhn & Solanki (2007) with a machine-learning inversion model that allows uncertainty quantification. We find that there are {\approx} 2--3 impulsive events per min, with a lifetime of about 10--20 min. Moreover, for all the three passbands, the distribution of power law slope {\alpha} peaks above 2. Our exploration of correlations among the frequency of impulsive events and their timescales and peak energy suggests that conduction losses dominate over radiative cooling losses. All these finding suggest that impulsive heating is a viable heating mechanism in QS corona. ",On the Impulsive Heating of Quiet Solar Corona
205,1377423173260439553,70614241,Dr Fiona H. Panther,"['NOT an April Fools joke but my new paper just landed: We propose a way to observe signatures of helium detonations associated with thermonuclear supernovae via gamma-rays:\n<LINK>', ""If a thermonuclear supernova results from a progenitor with a thick helium shell you get the synthesis of a lot of Cr-48 and it's daughter nucleus V-48. The decay of these radioisotopes can actually power the light curve, but some of their gamma-rays leak out"", 'In this paper, we show what this measurement could look like with observatories like @ESA_Integral, @COSIBalloon and AMEGO. We would need a relatively nearby supernova, but it would be a method to check incontrovertibly for signatures of helium detonation', 'We also show that for SN2014J, there was probably not a hugely significant amount of helium present. Not enough to rule out some models that suggested a helium torus, but getting close.', 'Anyway - this paper sits at the intersection of nucleosynthesis, transients and gamma-ray line studies. It shows how you can collaborate with a bunch of talented people to combine SN simulations, binary pop synthesis and gamma-ray astrophysics', 'I am happy to take questions', ""@astrocrash yes - however the proposed model for 2014J is for the ring not to be face on, but edge on to us on Earth (Science paper by Diehl+2015). The spectra we use from Stuart's model is angle averaged."", '@astrocrash We just constrain the total mass of Cr-48 to be relatively low based on sensitivity. What is interesting is that if we had a more sensitive instrument - say with better background rejection - then a better constraint on what the configuration of 2014J was could be made', ""@astrocrash I would have thought so, however it is plausible you have a degenerate, He rich white dwarf companion that has merged. So I wouldn't say the absence of the companion rules out there being helium involved to some extent."", ""@astrocrash You probably saw these sims by Pakmor+, basically these things do things that are unexpected. The model we use is a bit artificial in the sense that we don't say 'how' the ignition begins. Actually getting the He to explode is a separate problem https://t.co/pxD8aVbiO1"", ""@astrocrash The point of my paper is really to say 'this is an observational signature that provides conclusive evidence of helium detonations'. We just pick a canonical model to do the work, so in practice things may look quite different, which R√ºdiger's simulations seem to point at IMO"", '@astrocrash I agree - I think that practically for one of these observations then angle is an issue. I expect it will be similar to with KNe - I do wonder that at some angles you miss the gamma-rays entirely. could be worth a more detailed look in the future']",https://arxiv.org/abs/2103.16840,"Detection of gamma-rays emitted by radioactive isotopes synthesized in stellar explosions can give important insights into the processes that power transients such as supernovae, as well as providing a detailed census of the abundance of different isotope species relevant to the chemical evolution of the Universe. Observations of nearby supernovae have yielded observational proof that $^{57}$Co powered the late-time evolution of SN1987A's lightcurve, and conclusive evidence that $^{56}$Ni and its daughter nuclei power the light curves of Type Ia supernovae. In this paper we describe the prospects for detecting nuclear decay lines associated with the decay of $^{48}$V, the daughter nucleus of $^{48}$Cr, which is expected to be synthesised in large quantities - $M_{\mathrm{Cr}}\sim1.9\times10^{-2}\,\mathrm{M_\odot}$ - in transients initiated by explosive helium burning ($\alpha$-capture) of a thick helium shell. We calculate emergent gamma-ray line fluxes for a simulated explosion model of a thermonuclear explosion of carbon-oxygen white dwarf core of mass $0.45\,M_{\odot}$ surrounded by a thick helium layer of mass $0.21\,M_{\odot}$. We present observational limits on the presence of $^{48}$V in nearby SNe Ia 2014J using the \textit{INTEGRAL} space telescope, excluding a $^{48}$Cr production on the surface of more than $0.1\,\mathrm{M_{\odot}}$. We find that the future gamma-ray mission AMEGO will have an approximately 5 per cent chance of observing $^{48}$V gamma-rays from such events during the currently-planned operational lifetime, based on our birthrate predictions of faint thermonuclear transients. We describe the conditions for a $3\sigma$ detection by the gamma-ray telescopes \textit{INTEGRAL}/SPI, COSI and AMEGO. ","Prospects of direct detection of $^{48}$V gamma-rays from thermonuclear
  supernovae"
206,1377183922073706500,805477439648440321,Rob Kavanagh,"['Our new paper is now up on @arxiv! We modelled the stellar winds of the active planet-hosting stars Prox Cen and AU Mic. For AU Mic, we found that the orbiting planets could induce radio emission in its corona! Find out more:\n\n<LINK> <LINK>', 'Our predicted emission ranges from 10 MHz to 3 GHz. At 140 MHz, it bears a striking resemblance to the recently reported emission from the M dwarf GJ 1151 by Vedantham et al. (2020), which is suspected of being induced by a planet:\n\nhttps://t.co/1jJF4fcFdR https://t.co/Fy84Se93er']",https://arxiv.org/abs/2103.16318,"There have recently been detections of radio emission from low-mass stars, some of which are indicative of star-planet interactions. Motivated by these exciting new results, in this paper we present Alfv\'en wave-driven stellar wind models of the two active planet-hosting M dwarfs Prox Cen and AU Mic. Our models incorporate large-scale photospheric magnetic field maps reconstructed using the Zeeman-Doppler Imaging method. We obtain a mass-loss rate of $0.25~\dot{M}_{\odot}$ for the wind of Prox Cen. For the young dwarf AU Mic, we explore two cases: a low and high mass-loss rate. Depending on the properties of the Alfv\'en waves which heat the corona in our wind models, we obtain mass-loss rates of $27$ and $590~\dot{M}_{\odot}$ for AU Mic. We use our stellar wind models to assess the generation of electron cyclotron maser instability emission in both systems, through a mechanism analogous to the sub-Alfv\'enic Jupiter-Io interaction. For Prox Cen we do not find any feasible scenario where the planet can induce radio emission in the star's corona, as the planet orbits too far from the star in the super-Alfv\'enic regime. However, in the case that AU Mic has a stellar wind mass-loss rate of $27~\dot{M}_{\odot}$, we find that both planets b and c in the system can induce radio emission from $\sim10$ MHz to 3 GHz in the corona of the host star for the majority of their orbits, with peak flux densities of $\sim10$ mJy. Detection of such radio emission would allow us to place an upper limit on the mass-loss rate of the star. ","Planet-induced radio emission from the coronae of M dwarfs: the case of
  Prox Cen and AU Mic"
207,1377017243783950337,869862586610851840,Jeannette Bohg,"['Hanging objects is a common daily task.\n\nWe propose OmniHang to learn to hang arbitrary objects onto a diverse set of supporting items using contact point correspondences and neural collision estimation.\n\nProject webpage: <LINK>\n\nPaper: <LINK> <LINK>', 'Joint work with Yifan You, @linshaonju and Toki Migimatsu @StanfordIPRL']",https://arxiv.org/abs/2103.14283,"In this paper, we explore whether a robot can learn to hang arbitrary objects onto a diverse set of supporting items such as racks or hooks. Endowing robots with such an ability has applications in many domains such as domestic services, logistics, or manufacturing. Yet, it is a challenging manipulation task due to the large diversity of geometry and topology of everyday objects. In this paper, we propose a system that takes partial point clouds of an object and a supporting item as input and learns to decide where and how to hang the object stably. Our system learns to estimate the contact point correspondences between the object and supporting item to get an estimated stable pose. We then run a deep reinforcement learning algorithm to refine the predicted stable pose. Then, the robot needs to find a collision-free path to move the object from its initial pose to stable hanging pose. To this end, we train a neural network based collision estimator that takes as input partial point clouds of the object and supporting item. We generate a new and challenging, large-scale, synthetic dataset annotated with stable poses of objects hung on various supporting items and their contact point correspondences. In this dataset, we show that our system is able to achieve a 68.3% success rate of predicting stable object poses and has a 52.1% F1 score in terms of finding feasible paths. Supplemental material and videos are available on our project webpage. ","OmniHang: Learning to Hang Arbitrary Objects using Contact Point
  Correspondences and Neural Collision Estimation"
208,1376887783206232065,67043272,Rafael Mart√≠nez Galarza,"['We submitted a paper: we use radiative transfer, hydro sims. to investigate AGN role in heating of galaxy-scale cold dust, whose FIR emission is usually used as a tracer of star formation (hint: we find that the AGN contribution can be significant). <LINK> <LINK>']",https://arxiv.org/abs/2103.12747,"It is widely assumed that long-wavelength infrared (IR) emission from cold dust (T~20-40K) is a reliable tracer of star formation even in the presence of a bright active galactic nucleus (AGN). Based on radiative transfer (RT) models of clumpy AGN tori, hot dust emission from the torus contributes negligibly to the galaxy spectral energy distribution (SED) at $\lambda\ga100$ \micron. However, these models do not include AGN heating of host-galaxy-scale diffuse dust, which may have far-IR (FIR) colors comparable to cold diffuse dust heated by stars. To quantify the contribution of AGN heating to host-galaxy-scale cold dust emission at $\lambda\ga100$ \micron, we perform dust RT calculations on a simulated galaxy merger both including and excluding the bright AGN that it hosts. By differencing the SEDs yielded by RT calculations with and without AGN that are otherwise identical, we quantify the FIR cold dust emission arising solely from re-processed AGN photons. In extreme cases, AGN-heated host-galaxy-scale dust can increase galaxy-integrated FIR flux densities by factors of 2-4; star formation rates calculated from the FIR luminosity assuming no AGN contribution can overestimate the true value by comparable factors. Because the FIR colors of such systems are similar to those of purely star-forming galaxies and redder than torus models, broadband SED decomposition may be insufficient for disentangling the contributions of stars and heavily dust-enshrouded AGN in the most IR-luminous galaxies. We demonstrate how kpc-scale resolved observations can be used to identify deeply dust-enshrouded AGN with cool FIR colors when spectroscopic and/or X-ray detection methods are unavailable. ",Dust-Enshrouded AGN can Dominate Host-Galaxy-Scale Cold-Dust Emission
209,1376849058984263687,738769492122214400,Johannes Lischner,['In our new paper we study light-induced charge transfer from transition metal doped aluminium clusters to CO2 for applications in #photocatalysis. Read here:  <LINK> #compchem <LINK>'],https://arxiv.org/abs/2103.14405,"Charge transfer between molecules and catalysts plays a critical role in determining the efficiency and yield of photo-chemical catalytic processes. In this paper, we study light-induced electron transfer between transition metal doped aluminium clusters and CO$_2$ molecules using first-principles time-dependent density-functional theory. Specifically, we carry out calculations for a range of dopants (Zr, Mn, Fe, Ru, Co, Ni and Cu) and find that the resulting systems fall into two categories: Cu- and Fe-doped clusters exhibit no ground state charge transfer, weak CO$_2$ adsorption and light-induced electron transfer into the CO$_2$. In all other systems, we observe ground state electron transfer into the CO$_2$ resulting in strong adsorption and predominantly light-induced electron back-transfer from the CO$_2$ into the cluster. These findings pave the way towards a rational design of atomically precise aluminium photo-catalysts. ","Light Induced Charge Transfer from Transition-metal Doped Aluminium
  Clusters to Carbon Dioxide"
210,1376831694930579456,1120650694644596737,Paris Avgeriou,"['Architecture Erosion is a big pain in software maintenance but it is mostly studied by mining Open Source repositories. In our latest study, accepted @icpcconf, we instead survey software developers and analyze Stack Overflow posts.\nPreprint: <LINK>']",https://arxiv.org/abs/2103.11392,"As software systems evolve, their architecture is meant to adapt accordingly by following the changes in requirements, the environment, and the implementation. However, in practice, the evolving system often deviates from the architecture, causing severe consequences to system maintenance and evolution. This phenomenon of architecture erosion has been studied extensively in research, but not yet been examined from the point of view of developers. In this exploratory study, we look into how developers perceive the notion of architecture erosion, its causes and consequences, as well as tools and practices to identify and control architecture erosion. To this end, we searched through several popular online developer communities for collecting data of discussions related to architecture erosion. Besides, we identified developers involved in these discussions and conducted a survey with 10 participants and held interviews with 4 participants. Our findings show that: (1) developers either focus on the structural manifestation of architecture erosion or on its effect on run-time qualities, maintenance and evolution; (2) alongside technical factors, architecture erosion is caused to a large extent by non-technical factors; (3) despite the lack of dedicated tools for detecting architecture erosion, developers usually identify erosion through a number of symptoms; and (4) there are effective measures that can help to alleviate the impact of architecture erosion. ",Understanding Architecture Erosion: The Practitioners' Perceptive
211,1376812760936382467,3236251346,Mikel Sanz,"['Two articles today in #arxiv. In <LINK> we simulate a family of Hamiltonians containing Fermi-Hubbard with digital-analog techniques and find the optimal low-connected architecture for a quantum processor to perform it. @Ikerbasque @upvehu @meetIQM @OpenSuperQ <LINK>', 'In https://t.co/2q1AErOwt6, we design a superconducting architecture adapted to digital-analog techniques by obtaining the emerging analog block. We illustrate it by simulating a fermion lattice with a polynomial applications of this analog Hamiltonian. @Ikerbasque @OpenSuperQ https://t.co/YE0Rmr5KzB']",https://arxiv.org/abs/2103.15689,"Simulating quantum many-body systems is a highly demanding task since the required resources grow exponentially with the dimension of the system. In the case of fermionic systems, this is even harder since nonlocal interactions emerge due to the antisymmetric character of the fermionic wave function. Here, we introduce a digital-analog quantum algorithm to simulate a wide class of fermionic Hamiltonians including the paradigmatic Fermi-Hubbard model. These digital-analog methods allow quantum algorithms to run beyond digital versions via an efficient use of coherence time. Furthermore, we exemplify our techniques with a low-connected architecture for realistic digital-analog implementations of specific fermionic models. ",Digital-analog quantum simulation of fermionic models
212,1376350634832580608,1332417795477237761,Wei Jiang,"['Excited to share our latest work, COTR, on the image matching problem. We aim to find correspondences by asking the question: ""where does the point go in the other image?""\n\nabs: <LINK> <LINK>']",https://arxiv.org/abs/2103.14167,"We propose a novel framework for finding correspondences in images based on a deep neural network that, given two images and a query point in one of them, finds its correspondence in the other. By doing so, one has the option to query only the points of interest and retrieve sparse correspondences, or to query all points in an image and obtain dense mappings. Importantly, in order to capture both local and global priors, and to let our model relate between image regions using the most relevant among said priors, we realize our network using a transformer. At inference time, we apply our correspondence network by recursively zooming in around the estimates, yielding a multiscale pipeline able to provide highly-accurate correspondences. Our method significantly outperforms the state of the art on both sparse and dense correspondence problems on multiple datasets and tasks, ranging from wide-baseline stereo to optical flow, without any retraining for a specific dataset. We commit to releasing data, code, and all the tools necessary to train from scratch and ensure reproducibility. ",COTR: Correspondence Transformer for Matching Across Images
213,1376272755998482433,1229237405703643137,Michel Albonico,"['Energy-related Practices in Robotics Software. We will present our study at the @msrconf, co-located with @ICSE 2021.\n\nüìäReplication package: <LINK>\nüìÑPre-print: <LINK> \n\n@rosorg\n #energyefficiency #robotics #SoftwareEngineering \n@s2_group']",https://arxiv.org/abs/2103.13762,"Robots are becoming more and more commonplace in many industry settings. This successful adoption can be partly attributed to (1) their increasingly affordable cost and (2) the possibility of developing intelligent, software-driven robots. Unfortunately, robotics software consumes significant amounts of energy. Moreover, robots are often battery-driven, meaning that even a small energy improvement can help reduce its energy footprint and increase its autonomy and user experience. In this paper, we study the Robot Operating System (ROS) ecosystem, the de-facto standard for developing and prototyping robotics software. We analyze 527 energy-related data points (including commits, pull-requests, and issues on ROS-related repositories, ROS-related questions on StackOverflow, ROS Discourse, ROS Answers, and the official ROS Wiki). Our results include a quantification of the interest of roboticists on software energy efficiency, 10 recurrent causes, and 14 solutions of energy-related issues, and their implied trade-offs with respect to other quality attributes. Those contributions support roboticists and researchers towards having energy-efficient software in future robotics projects. ",Mining Energy-Related Practices in Robotics Software
214,1375942252216770564,1200996469144031232,Enrico Ramirez-Ruiz,"[""In this work led by @NUCIERA's luminous Nick Kaaz we study the properties of accretion flows surrounding embedded binary @LIGO BHs in vertically stratified AGN disks and discuss prospects for assembling heavier BHs than those predicted by theory.\n<LINK> <LINK>""]",https://arxiv.org/abs/2103.12088,"Stellar-mass black holes can become embedded within the gaseous disks of active galactic nuclei (AGNs). Afterwards, their interactions are mediated by their gaseous surroundings. In this work, we study the evolution of stellar-mass binary black holes (BBHs) embedded within AGN disks using a combination of three-dimensional hydrodynamic simulations and analytic methods, focusing on environments in which the AGN disk scale height $H$ is $\gtrsim$ the BBH sphere of influence. We model the local surroundings of the embedded BBHs using a wind tunnel formalism and characterize different accretion regimes based on the local properties of the disk, which range from wind-dominated to quasi-spherical. We use our simulations to develop prescriptions for mass accretion and drag for embedded BBHs. We use these prescriptions, along with AGN disk models that can represent the Toomre-unstable outer regions of AGN disks, to study the long-term evolution of the BBHs as they migrate through the disk. We find that BBHs typically merge within $\lesssim 5-30\,{\rm Myr}$, increasing their mass significantly in the process, allowing BBHs to enter (or cross) the pair-instability supernova mass gap. The rate at which gas is supplied to these BBHs often exceeds the Eddington limit, sometimes by several orders of magnitude. We conclude that most embedded BBHs will merge before migrating significantly in the disk. Depending on the conditions of the ambient gas and the distance to the system, LISA can detect the transition between the gas-dominated and gravitational wave dominated regime for inspiraling BBHs that are formed sufficiently close to the AGN ($\lesssim$ 0.1 pc). We also discuss possible electromagnetic signatures during and following the inspiral, finding that it is generally unlikely but not inconceivable for the bolometric luminosity of the BBH to exceed that of the host AGN. ","The hydrodynamic evolution of binary black holes embedded within the
  vertically stratified disks of active galactic nuclei"
215,1375349968307884033,1248533170774999040,Kostas Migkas,"['Our new cluster anisotropies paper is out today! We use 10 scaling relations to see if H0 looks isotropic locally or if there are strong bulk flows. We find an apparent 9% spatial change in H0, or equivalently, a 900 km/s bulk flow out to 500 Mpc... (1/n)\n\n<LINK>', ""We thoroughly check everything the community suggested (and everything we thought of) but couldn't identify any known systematic causing this. Monte Carlo simulations confirm our result. BUT WAIT! It doesn't mean there are no systematics... (2/n)"", '..It simply means that, for now, the cosmological explanation seems more likely. We will keep looking for new, unknown systematics. If we find any, who knows what else will they affect (remember, they are currently unknown). (3/4)', ""However, we can confidently say this time that it's definitely not X-ray absorption, Malmquist bias, cluster population inhomogeneities, the Zone of Avoidance gap, cluster properties correlation, selection cuts, and many more... I hope you find our work interesting!""]",https://arxiv.org/abs/2103.13904,"The hypothesis that the late Universe is isotropic and homogeneous is adopted by most cosmological studies. The expansion rate $H_0$ is thought to be spatially constant, while bulk flows are often presumed to be negligible compared to the Hubble expansion, even at local scales. Their effects on the redshift-distance conversion are hence usually ignored. Any deviation from this consensus can strongly bias the results of such studies and thus the importance of testing these assumptions cannot be understated. Scaling relations of galaxy clusters can be effectively used for that. In previous works, we observed strong anisotropies in cluster scaling relations, whose origins remain ambiguous. By measuring many different cluster properties, several scaling relations with different sensitivities can be built. Nearly independent tests of cosmic isotropy and bulk flows are then feasible. We make use of up to 570 clusters with measured properties at X-ray, microwave, and infrared wavelengths, to construct 10 different cluster scaling relations (five of them presented for the first time) and test the isotropy of the local Universe. Through rigorous tests, we ensure that our analysis is not prone to generally known systematic biases and X-ray absorption issues. By combining all available information, we detect an apparent $9\%$ spatial variation in the local $H_0$ between $(l,b)\sim ({280^{\circ}}^{+35^{\circ}}_{-35^{\circ}},{-15^{\circ}}^{+20^{\circ}}_{-20^{\circ}})$ and the rest of the sky. The observed anisotropy has a nearly dipole form. Using Monte Carlo simulations, we assess the statistical significance of the anisotropy to be $>5\sigma$. This result could also be attributed to a $\sim 900$ km/s bulk flow which seems to extend out to at least $\sim 500$ Mpc. These two effects are indistinguishable until more high$-z$ clusters are observed by future all-sky surveys, such as eROSITA. ","Cosmological implications of the anisotropy of ten galaxy cluster
  scaling relations"
216,1375311361316634624,2647128003,Mogens Fosgerau,"['A perturbed utility route choice model\n\nWe  propose  a  model  in  which  a  utility  maximizing  traveler  assigns  flow  across  an  entire network under a flow conservation constraint.  1/ \n<LINK> #EconTwitter', '2/ Substitution between routes depends on how much they overlap. This model can be estimated from route choice data, where the full set of route alternatives is included and no choice set generation is required.', '3/3 Nevertheless, estimation requires only linear regression and is very fast. Predictions from the model can be computed using convex optimization and is straightforward even for large networks.', '@JRehbeck', '@ERC_Research', '@cykelnorden @ThomasKjrRasmu2']",https://arxiv.org/abs/2103.13784,"We propose a route choice model in which traveler behavior is represented as a utility maximizing assignment of flow across an entire network under a flow conservation constraint}. Substitution between routes depends on how much they overlap. {\tr The model is estimated considering the full set of route alternatives, and no choice set generation is required. Nevertheless, estimation requires only linear regression and is very fast. Predictions from the model can be computed using convex optimization, and computation is straightforward even for large networks. We estimate and validate the model using a large dataset comprising 1,337,096 GPS traces of trips in the Greater Copenhagen road network. ",A perturbed utility route choice model
217,1375247172732604422,1169068112177745922,Alexis Plascencia,"['One more paper with @fileviez and @clamurgal üòÄ <LINK>\n \nWe study the mechanism of Leptogenesis in theories where Baryon and Lepton number are promoted to local gauge symmetries 1/n <LINK>', 'We numerically solved the Boltzmann equations including the effects of the process N N &lt;-&gt; Z_L &lt;-&gt; f bar(f)  and depending on how large is the ratio g_L/M_ZL this new interaction can quickly bring the right-handed neutrino into thermal equilibrium 2/n https://t.co/uOz8TYMUto', 'If this new gauge interaction is too large it keeps the right-handed neutrinos in thermal equilibrium and suppresses the final asymmetry. Thus, we find a lower bound on the symmetry breaking scale for U(1)_L of \n\nM_ZL/g_L &gt; 10^10 GeV \n\nin order to have successful leptogenesis 3/n https://t.co/QXKeiapvRK', 'the spontaneous breaking of a U(1) at such high temperatures leads to the formation of cosmic strings that radiate gravitational waves that could be probed by future Laser Interferometers such as LISA 4/n', 'Furthermore, in this scenario the ‚Äòt Hooft operator associated with the sphaleron effects is different from the SM since it needs to preserve the U(1)_B gauge symmetry: 5/n https://t.co/VrBkXRXQSp', 'and hence, sphaleron processes can transfer a lepton asymmetry and a dark matter asymmetry into a baryon asymmetry. 6/n', 'The theory has an automatic dark matter that is predicted from the cancellation of gauge anomalies. Namely, in the theory with 6 new representation, the DM candidate is generically a Dirac fermion (chi) 7/n https://t.co/q8vpS4bcez', 'Then, the question arises: How can we generate a dark matter asymmetry?  \n \nWell, it turns out that by just adding a new complex scalar phi the theory nicely accommodates the mechanism proposed in https://t.co/ELGNx0n7bt  8/n https://t.co/Tf7a5tVyGD', 'In this mechanism, the out-of-equilibrium decays of N1 -&gt; phi DM and N1 -&gt; phi* bar(DM)  can also generate a dark matter asymmetry. Then, the lepton and dark matter asymmetries are partially converted into a baryon asymmetry via sphaleron processes 9/n https://t.co/eDn7LjCnkK', 'Thus, theories with local Baryon and Lepton number can explain the baryon asymmetry, dark matter and neutrino masses üôÇ  10/n']",https://arxiv.org/abs/2103.13397,"In order to address the baryon asymmetry in the Universe one needs to understand the origin of baryon (B) and lepton (L) number violation. In this article, we discuss the mechanism of baryogenesis via leptogenesis to explain the matter-antimatter asymmetry in theories with spontaneous breaking of baryon and lepton number. In this context, a lepton asymmetry is generated through the out-of-equilibrium decays of right-handed neutrinos at the high-scale, while local baryon number must be broken below the multi-TeV scale to satisfy the cosmological bounds on the dark matter relic density. We demonstrate how the lepton asymmetry generated via leptogenesis can be converted in two different ways: a) in the theory predicting Majorana dark matter the lepton asymmetry is converted into a baryon asymmetry, and b) in the theory with Dirac dark matter the decays of right-handed neutrinos can generate lepton and dark matter asymmetries that are then partially converted into a baryon asymmetry. Consequently, we show how to explain the matter-antimatter asymmetry, the dark matter relic density and neutrino masses in theories for local baryon and lepton number. ",Baryogenesis via Leptogenesis: Spontaneous B and L Violation
218,1375166946233221126,1280821485934870536,Deependra Jadoun,['Excited to share my first pre-print as a PhD student in @Fysikum. All thanks to @SpecTqd and Mahesh.\nWe theoretical study the non-adiabatic dynamics in pyrrole using various X-ray probe methods. <LINK> <LINK>'],https://arxiv.org/abs/2103.13269,"Many recent experimental ultrafast spectroscopy studies have hinted at non-adiabatic dynamics indicating the existence of conical intersections, but their direct observation remains a challenge. The rapid change of the energy gap between the electronic states complicated their observation by requiring bandwidths of several electron volts. In this manuscript, we propose to use the combined information of different X-ray pump-probe techniques to identify the conical intersection. We theoretically study the conical intersection in pyrrole using transient X-ray absorption, time-resolved X-ray spontaneous emission, and linear off-resonant Raman spectroscopy to gather evidence of the curve crossing. ","Capturing Fingerprints of Conical Intersection: Complementary
  Information of Non-Adiabatic Dynamics from Linear X-ray Probes"
219,1375057728733913088,1201434433,Lucas Gren,"['Is it possible to disregard obsolete requirements in effort estimation? Well, seems like we should be careful. So proud of this large study with @drrbsv accepted at @ReqEngJ preprint: <LINK>']",https://arxiv.org/abs/2103.13265,"Context: Expert judgement is a common method for software effort estimations in practice today. Estimators are often shown extra obsolete requirements together with the real ones to be implemented. Only one previous study has been conducted on if such practices bias the estimations. Objective: We conducted six experiments with both students and practitioners to study, and quantify, the effects of obsolete requirements on software estimation. Method By conducting a family of six experiments using both students and practitioners as research subjects (N = 461), and by using a Bayesian Data Analysis approach, we investigated different aspects of this effect. We also argue for, and show an example of, how we by using a Bayesian approach can be more confident in our results and enable further studies with small sample sizes. Results: We found that the presence of obsolete requirements triggered an overestimation in effort across all experiments. The effect, however, was smaller in a field setting compared to using students as subjects. Still, the over-estimations triggered by the obsolete requirements were systematically around twice the percentage of the included obsolete ones, but with a large 95% credible interval. Conclusions: The results have implications for both research and practice in that the found systematic error should be accounted for in both studies on software estimation and, maybe more importantly, in estimation practices to avoid over-estimation due to this systematic error. We partly explain this error to be stemming from the cognitive bias of anchoring-and-adjustment, i.e. the obsolete requirements anchored a much larger software. However, further studies are needed in order to accurately predict this effect. ","Is it Possible to Disregard Obsolete Requirements? A Family of
  Experiments in Software Effort Estimation"
220,1375041306112712704,301426952,Arttu Rajantie üá™üá∫ üá´üáÆ #FBPE,"[""Instanton solution for Schwinger production of 't Hooft-Polyakov monopoles, by @DavidLJHo and myself. We find the field theory instanton solution that describes GUT magnetic monopole pair production in strong magnetic fields.\n<LINK>"", '@_subodhpatil @DavidLJHo Yes and no. It includes backreaction, but because it is done purely in imaginary time, it does not tell us what happens after the monopoles are produced. That would be doable by taking a slice through this solution and using it as the initial condition for real time evolution.', '@_subodhpatil @DavidLJHo Yes, it was known that this solution must exist because Affleck and Manton had found the corresponding solution in the worldline approximation in 1982. Therefore we knew pretty well what we were looking for, and the difficulty was just how to find it.']",https://arxiv.org/abs/2103.12799,"We present the results of an explicit numerical computation of a novel instanton in Georgi-Glashow SU(2) theory. The instanton is physically relevant as a mediator of Schwinger production of 't Hooft-Polyakov magnetic monopoles from strong magnetic fields. In weak fields, the pair production rate has previously been computed using the worldline approximation, which breaks down in strong fields due to the effects of finite monopole size. Using lattice field theory we have overcome this limit, including finite monopole size effects to all orders. We demonstrate that a full consideration of the internal monopole structure results in an enhancement to the pair production rate, and confirm earlier results that monopole production becomes classical at the Ambjorn-Olesen critical field strength. ","Instanton solution for Schwinger production of 't Hooft-Polyakov
  monopoles"
221,1375036239867310083,546198840,Gianluca Bertaina,"['A new study by Sebastiano Pilati, with the collaboration of Giuliano Orso and me. ""Quantum Monte Carlo simulations of two-dimensional repulsive Fermi gases with population imbalance"". We hope it will contribute to the search for itinerant ferromagnetism <LINK>']",http://arxiv.org/abs/2103.13251,"The ground-state properties of two-component repulsive Fermi gases in two dimensions are investigated by means of fixed-node diffusion Monte Carlo simulations. The energy per particle is determined as a function of the intercomponent interaction strength and of the population imbalance. The regime of universality in terms of the s-wave scattering length is identified by comparing results for hard-disk and for soft-disk potentials. In the large imbalance regime, the equation of state turns out to be well described by a Landau-Pomeranchuk functional for two-dimensional polarons. To fully characterize this expansion, we determine the polarons' effective mass and their coupling parameter, complementing previous studies on their chemical potential. Furthermore, we extract the magnetic susceptibility from low-imbalance data, finding only small deviations from the mean-field prediction. While the mean-field theory predicts a direct transition from a paramagnetic to a fully ferromagnetic phase, our diffusion Monte Carlo results suggest that the partially ferromagnetic phase is stable in a narrow interval of the interaction parameter. This finding calls for further analyses on the effects due to the fixed-node constraint. ","Quantum Monte Carlo simulations of two-dimensional repulsive Fermi gases
  with population imbalance"
222,1374963743084277760,1280981223829958657,Jivitesh Jain,"['Find out what‚Äôs kooking on the new Indian social network #kooapp.\nRead our characterisation where we analyse the user demographics, network dynamics, linguistic communities, content, prominence &amp; more: <LINK>\nWe also make the dataset public! <LINK>']",https://arxiv.org/abs/2103.13239,"Social media has grown exponentially in a short period, coming to the forefront of communications and online interactions. Despite their rapid growth, social media platforms have been unable to scale to different languages globally and remain inaccessible to many. In this paper, we characterize Koo, a multilingual micro-blogging site that rose in popularity in 2021, as an Indian alternative to Twitter. We collected a dataset of 4.07 million users, 163.12 million follower-following relationships, and their content and activity across 12 languages. We study the user demographic along the lines of language, location, gender, and profession. The prominent presence of Indian languages in the discourse on Koo indicates the platform's success in promoting regional languages. We observe Koo's follower-following network to be much denser than Twitter's, comprising of closely-knit linguistic communities. An N-gram analysis of posts on Koo shows a #KooVsTwitter rhetoric, revealing the debate comparing the two platforms. Our characterization highlights the dynamics of the multilingual social network and its diverse Indian user base. ","What's Kooking? Characterizing India's Emerging Social Network, Koo"
223,1374960903309783043,3943468754,Shradha Sehgal,"[""What's #KOOking? Read our work on characterization of the latest #Koo platform! We study the user demographic, prominent accounts, content and activity, linguistic communities, and much more.\nWe also make our dataset public for research! \nFull report at <LINK> <LINK>""]",https://arxiv.org/abs/2103.13239,"Social media has grown exponentially in a short period, coming to the forefront of communications and online interactions. Despite their rapid growth, social media platforms have been unable to scale to different languages globally and remain inaccessible to many. In this paper, we characterize Koo, a multilingual micro-blogging site that rose in popularity in 2021, as an Indian alternative to Twitter. We collected a dataset of 4.07 million users, 163.12 million follower-following relationships, and their content and activity across 12 languages. We study the user demographic along the lines of language, location, gender, and profession. The prominent presence of Indian languages in the discourse on Koo indicates the platform's success in promoting regional languages. We observe Koo's follower-following network to be much denser than Twitter's, comprising of closely-knit linguistic communities. An N-gram analysis of posts on Koo shows a #KooVsTwitter rhetoric, revealing the debate comparing the two platforms. Our characterization highlights the dynamics of the multilingual social network and its diverse Indian user base. ","What's Kooking? Characterizing India's Emerging Social Network, Koo"
224,1374815883147763714,788067417918365696,Harshitha Machiraju,['Do human vision (bio)-inspired components actually improve adversarial robustness? ü§î\nWe review such recent papers and find their results are often inconclusive. \n\nPaper: Bio-Inspired Robustness: A Review (<LINK>)\n\n@pafrossard @5hyeon1'],https://arxiv.org/abs/2103.09265,"Deep convolutional neural networks (DCNNs) have revolutionized computer vision and are often advocated as good models of the human visual system. However, there are currently many shortcomings of DCNNs, which preclude them as a model of human vision. For example, in the case of adversarial attacks, where adding small amounts of noise to an image, including an object, can lead to strong misclassification of that object. But for humans, the noise is often invisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot be taken as serious models of human vision. Many studies have tried to add features of the human visual system to DCNNs to make them robust against adversarial attacks. However, it is not fully clear whether human vision inspired components increase robustness because performance evaluations of these novel components in DCNNs are often inconclusive. We propose a set of criteria for proper evaluation and analyze different models according to these criteria. We finally sketch future efforts to make DCCNs one step closer to the model of human vision. ",Bio-inspired Robustness: A Review
225,1374740943136509952,1062812206243422213,Jalal Kazempour,"['Despite recent mathematical &amp; computational advances, electricity markets are still using a linear model, where simplifying assumptions are necessary. Shall we go beyond LP, and use a conic model? You may find this paper interesting to read: <LINK>\n@anubhavratha <LINK>', ""@themarklstone @anubhavratha @pierrepinson Indeed Mark! This is a natural extension to this work. Let's see first whether electricity markets in practice will be convinced to use a conic model, then it would be more straightforward to move towards (scalable) semi-definite markets ;o)""]",https://arxiv.org/abs/2103.12122,"We propose a new forward electricity market framework that admits heterogeneous market participants with second-order cone strategy sets, who accurately express the nonlinearities in their costs and constraints through conic bids, and a network operator facing conic operational constraints. In contrast to the prevalent linear-programming-based electricity markets, we highlight how the inclusion of second-order cone constraints enables uncertainty-, asset- and network-awareness of the market, which is key to the successful transition towards an electricity system based on weather-dependent renewable energy sources. We analyze our general market-clearing proposal using conic duality theory to derive efficient spatially-differentiated prices for the multiple commodities, comprising of energy and flexibility services. Under the assumption of perfect competition, we prove the equivalence of the centrally-solved market-clearing optimization problem to a competitive spatial price equilibrium involving a set of rational and self-interested participants and a price setter. Finally, under common assumptions, we prove that moving towards conic markets does not incur the loss of desirable economic properties of markets, namely market efficiency, cost recovery and revenue adequacy. Our numerical studies focus on the specific use case of uncertainty-aware market design and demonstrate that the proposed conic market brings advantages over existing alternatives within the linear programming market framework. ",Moving from Linear to Conic Markets for Electricity
226,1374716954276151296,14659319,Arash Badie-Modiri,"['Check out our paper on contact tracing apps (i.e. #COVID19 exposure notification) where we study effects of degree distribution, homophily/heterophily of app users and failure (or people just not paying attention!)\n<LINK>\n\n(w/ @abbas_k_rizi @Faqeeh_ali_ @bolozna)', ""For example, while heterophily is generally bad (since app users happen to interact with mostly non-app users, hence tracing apps are not used at all) too much homophily is also detrimental. There's a sweet spot for mixing of app users and non-app users."", 'Also studied is the effect of targeting/convincing high degree nodes (socially highly active people) to use the app and to what extend this pushed the epidemic threshold back.', 'Check it out on ArXiv:\nhttps://t.co/h0X6fV7NeW']",https://arxiv.org/abs/2103.12634,"Contact tracing via digital tracking applications installed on mobile phones is an important tool for controlling epidemic spreading. Its effectivity can be quantified by modifying the standard methodology for analyzing percolation and connectivity of contact networks. We apply this framework to networks with varying degree distributions, numbers of application users, and probabilities of quarantine failures. Further, we study structured populations with homophily and heterophily and the possibility of degree-targeted application distribution. Our results are based on a combination of explicit simulations and mean-field analysis. They indicate that there can be major differences in the epidemic size and epidemic probabilities which are equivalent in the normal SIR processes. Further, degree heterogeneity is seen to be especially important for the epidemic threshold but not as much for the epidemic size. The probability that tracing leads to quarantines is not as important as the application adoption rate. Finally, both strong homophily and especially heterophily with regard to application adoption can be detrimental. Overall, epidemic dynamics are very sensitive to all of the parameter values we tested out, which makes the problem of estimating the effect of digital contact tracing an inherently multidimensional problem. ","Epidemic Spreading and Digital Contact Tracing: Effects of Heterogeneous
  Mixing and Quarantine Failures"
227,1374697063078846467,333033094,Michael Horodynski,['We just put a paper on the arXiv where we propose a method for optimally cooling multiple levitated particles through far-field wavefront-shaping. I am very happy that I got to be part of this collaboration. <LINK> <LINK>'],https://arxiv.org/abs/2103.12592,"Manipulating and cooling small particles with light are long-standing challenges in many areas of science, from the foundations of physics to applications in biology and nano-technology. Light fields can, in particular, be used to isolate mesoscopic particles from their environment by levitating them optically. These levitated particles of micron size and smaller exhibit pristine mechanical resonances and can be cooled down to their motional quantum ground state. Significant roadblocks on the way to scale up levitation from a single to multiple particles in close proximity are the requirements to constantly monitor the particles' positions as well as to engineer light fields that react fast and appropriately to their displacements. Given the complexity of light scattering between particles, each of these two challenges currently seems insurmountable already in itself. Here, we present an approach that solves both problems at once by forgoing any local information on the particles. Instead, our procedure is based on the far-field information stored in the scattering matrix and its changes with time. We demonstrate how to compose from these ingredients a linear energy-shift operator, whose maximal or minimal eigenstates are identified as the incoming wavefronts that implement the most efficient heating or cooling of a moving ensemble of arbitrarily-shaped levitated particles, respectively. We expect this optimal approach to be a game-changer for the collective manipulation of multiple particles on-the-fly, i.e., without the necessity to track them. An experimental implementation is suggested based on stroboscopic scattering matrix measurements and a time-adaptive injection of the optimal light fields. ","Optimal Cooling of Multiple Levitated Particles through Far-Field
  Wavefront-Shaping"
228,1374649749110931456,71880888,thomas vogel,"['How do we Evaluate Self-adaptive Software Systems?\n\nOur mapping study on experimental evaluations published in the last decade at @SEAMSconf was accepted at #seams2021.\n\nPre-print: <LINK>\n\nWe posed and answered 5 research research questions:\n\n(thread)', '1) What is the scope of experiments? \n\nThe main evaluation target is a feedback loop approach with effectiveness and time efficiency as main evaluation objectives. Recently, we observe a rapid increase in experiments on the ability and effectiveness of new learning approaches.', '2) What is the Experimental Design? \n\nOnly one out of three studies provides a well-defined formulation of the evaluation problem. Experiments use independent variables for all parts of self-adaptive systems, with most factors relate to the managing system.', ""2) What is the Experimental Design? (cont'd)\n\nThe dominant types of dependent variables are time behavior, functional behavior, and resource utilization, typically of managed systems. New contributions are increasingly compared with other approaches."", '3) How are Experiments Operated? \n\nArtifacts are increasingly used in experiments. The managed system is mostly simulated or emulated. Yet, one on three studies uses system implementations, but the number of real-world systems or prototypes of such systems remains relatively low.', ""3) How are Experiments Operated? (cont'd)\n\nMost data of users and the environment is synthetically generated. Studies commonly consider uncertainties in the context and system. Uncertainties related to goals and humans are not frequently considered."", '4) How is the Experiment Data Analyzed? \n\nMost studies analyze data informally or with descriptive statistics. Only a fraction uses statistical tests. A limited number provides explicit answers to problems. Explicitly discussing threats to validity is not common practice.', '5) How are the Experiment Results Packaged?\n\nOnly a small fraction of the studies make the data of their experiment publicly available. Similarly, the degree of reproducibility remains low, only one study on ten offers a full replication package to the community.', 'Yes, we have improved evaluations of self-adaptive systems over the years but some issues remain. Have a look at our paper for suggestions to improve experimental evaluations of self-adaptive systems.\n\n-&gt; https://t.co/9PTsqnpkrX']",https://arxiv.org/abs/2103.11481,"With the increase of research in self-adaptive systems, there is a need to better understand the way research contributions are evaluated. Such insights will support researchers to better compare new findings when developing new knowledge for the community. However, so far there is no clear overview of how evaluations are performed in self-adaptive systems. To address this gap, we conduct a mapping study. The study focuses on experimental evaluations published in the last decade at the prime venue of research in software engineering for self-adaptive systems -- the International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS). Results point out that specifics of self-adaptive systems require special attention in the experimental process, including the distinction of the managing system (i.e., the target of evaluation) and the managed system, the presence of uncertainties that affect the system behavior and hence need to be taken into account in data analysis, and the potential of managed systems to be reused across experiments, beyond replications. To conclude, we offer a set of suggestions derived from our study that can be used as input to enhance future experiments in self-adaptive systems. ",How do we Evaluate Self-adaptive Software Systems?
229,1374622622479224837,46153507,dr. Jordy Davelaar,"['üö® new paper alert üö® \n\nWe study the physical properties of four-dimensional, string-theoretical, horizonless ""fuzzball"" geometries by imaging their shadows and comparing them to black holes.\n\narxiv: <LINK>\n\nPaper lead by: Fabio Bacchini and Daniel R. Mayerson <LINK>', 'Their microstructure traps light rays straying near the would-be horizon on long-lived, highly redshifted chaotic orbits. In fuzzballs sufficiently near the scaling limit this creates a shadow much like that of a black hole while avoiding the paradoxes associated with black holes https://t.co/0vEZ4XNbkH', 'Observations of the shadow size and residual glow can potentially discriminate between fuzzballs away from the scaling limit and alternative models of black compact objects.', 'Paper by: Fabio Bacchini, Daniel R. Mayerson, Bart Ripperda (@BartRipperda), Jordy Davelaar (@jordydavelaar), H√©ctor Olivares, Thomas Hertog, Bert Vercnocke']",https://arxiv.org/abs/2103.12075,"We study the physical properties of four-dimensional, string-theoretical, horizonless ""fuzzball"" geometries by imaging their shadows. Their microstructure traps light rays straying near the would-be horizon on long-lived, highly redshifted chaotic orbits. In fuzzballs sufficiently near the scaling limit this creates a shadow much like that of a black hole, while avoiding the paradoxes associated with an event horizon. Observations of the shadow size and residual glow can potentially discriminate between fuzzballs away from the scaling limit and alternative models of black compact objects. ",Fuzzball Shadows: Emergent Horizons from Microstructure
230,1374373986117644301,1120650694644596737,Paris Avgeriou,['We know that software engineers use search engines in their daily practice. But what exactly do they google and does it actually help them? We studied this particularly for software architecture tasks. Attend our talk @ICSAconf or get the pre-print. <LINK>'],https://arxiv.org/abs/2103.11705,"Software engineers need relevant and up-to-date architectural knowledge (AK), in order to make well-founded design decisions. However, finding such AK is quite challenging. One pragmatic approach is to search for AK on the web using traditional search engines (e.g. Google); this is common practice among software engineers. Still, we know very little about what AK is retrieved, from where, and how useful it is. In this paper, we conduct an empirical study with 53 software engineers, who used Google to make design decisions using the Attribute-Driven-Design method. Based on how the subjects assessed the nature and relevance of the retrieved results, we determined how effective web search engines are to find relevant architectural information. Moreover, we identified the different sources of AK on the web and their associated AK concepts. ",Exploring Web Search Engines to Find Architectural Knowledge
231,1374332704662167557,996420531334369281,Andres Karjus,"['Preprint ""Conceptual similarity and communicative need shape colexification"" w/ @DrAlgernon @SimonKirby Tianyu Wang @kennysmithed: <LINK>. We carry out 4 artificial language experiments (incl a self-repl) to test 2 hypotheses from a crosslinguistic study  1/5 <LINK>', ""2/5 ....Xu et al 2020 (https://t.co/DkeS91E0Zv) show using a sample of 250 langs that if two meanings get colexified (expressed w same form), it's usually similar ones, like snow&amp;ice, demonstrating a constraint on the formation of lexicons. But they hypothesize that..."", '3/5 if some similar meanings are important for a culture to distinguish, e.g. brother&amp;sister, then this communicative need prevents colexification (cf. also https://t.co/3scUpI5OjP)\nWe test both claims using dyadic artificial language experiments and find support for both:', '4/5 in the neutral condition, participants are more likely to colexify similar meanings than dissimilar ones; but when we manipulate communicative need by making them distinguish between similar meaning pairs more often, they change behaviour to mainatain efficient communication. https://t.co/nXtU8kKBZN', ""5/5 Language change is driven by numerous interacting forces; our experimental results support the importance of speakers' communicative need, as a factor modulating the complexity-informativeness tradeoff proposed in previous literature. https://t.co/fPYBRCfE5e"", ""Oh and of course all the data from the 4 experiments and the analysis code are all available: https://t.co/Jnkx03Amqx - incl the source code for the #Shiny game app I developed for the study (turns out you totally can make multiplayer games in #rstats ...if you're mad enough) https://t.co/0pWOr6Cojy""]",https://arxiv.org/abs/2103.11024,"Colexification refers to the phenomenon of multiple meanings sharing one word in a language. Cross-linguistic lexification patterns have been shown to be largely predictable, as similar concepts are often colexified. We test a recent claim that, beyond this general tendency, communicative needs play an important role in shaping colexification patterns. We approach this question by means of a series of human experiments, using an artificial language communication game paradigm. Our results across four experiments match the previous cross-linguistic findings: all other things being equal, speakers do prefer to colexify similar concepts. However, we also find evidence supporting the communicative need hypothesis: when faced with a frequent need to distinguish similar pairs of meanings, speakers adjust their colexification preferences to maintain communicative efficiency, and avoid colexifying those similar meanings which need to be distinguished in communication. This research provides further evidence to support the argument that languages are shaped by the needs and preferences of their speakers. ","Conceptual similarity and communicative need shape colexification: an
  experimental study"
232,1374257869038698497,760022547895377920,Federico Errica,"[""Really excited for this work! We perform a preliminary study on catastrophic forgetting for DGNs (deep graph nets). See you at WWW '21 GLB Workshop!\nJoint work with @Cossu94 @acarta7 and Davide Bacciu.  <LINK>"", ""And, do not forget to check out our codebase to reproduce the results! It's an extension of PyDGN for Continual learning experiments! Apparently, reviewers appreciated it very much ;)\nhttps://t.co/XtLneaYy4N""]",https://arxiv.org/abs/2103.11750,"In this work, we study the phenomenon of catastrophic forgetting in the graph representation learning scenario. The primary objective of the analysis is to understand whether classical continual learning techniques for flat and sequential data have a tangible impact on performances when applied to graph data. To do so, we experiment with a structure-agnostic model and a deep graph network in a robust and controlled environment on three different datasets. The benchmark is complemented by an investigation on the effect of structure-preserving regularization techniques on catastrophic forgetting. We find that replay is the most effective strategy in so far, which also benefits the most from the use of regularization. Our findings suggest interesting future research at the intersection of the continual and graph representation learning fields. Finally, we provide researchers with a flexible software framework to reproduce our results and carry out further experiments. ","Catastrophic Forgetting in Deep Graph Networks: an Introductory
  Benchmark for Graph Classification"
233,1374109341482168323,1214215979200172033,Leonard Wong,['New paper with my PhD student Steven Campbell on arXiv now:\n<LINK>\n\nWe study a portfolio optimization problem in the context of stochastic portfolio theory. The main idea is to exploit the stability of the capital distribution curve. <LINK>'],https://arxiv.org/abs/2103.10925,"In this paper we develop a concrete and fully implementable approach to the optimization of functionally generated portfolios in stochastic portfolio theory. The main idea is to optimize over a family of rank-based portfolios parameterized by an exponentially concave function on the unit interval. This choice can be motivated by the long term stability of the capital distribution observed in large equity markets, and allows us to circumvent the curse of dimensionality. The resulting optimization problem, which is convex, allows for various regularizations and constraints to be imposed on the generating function. We prove an existence and uniqueness result for our optimization problem and provide a stability estimate in terms of a Wasserstein metric of the input measure. Then, we formulate a discretization which can be implemented numerically using available software packages and analyze its approximation error. Finally, we present empirical examples using CRSP data from the US stock market, including the performance of the portfolios allowing for dividends, defaults, and transaction costs. ",Functional portfolio optimization in stochastic portfolio theory
234,1372907428493283336,1359059238,Dima Damen,"['What is wrong with video retrieval benchmarks?\nOur #CVPR2021 work now on ArXiv\n<LINK>\nw M Wray @doughty_hazel \nPrior works are based on instance-based assumption.\nWe propose to rank videos by their semantic similarity, with multiple videos being equally relevant. <LINK>', 'Analysing 3 common benchmarks (MSR-VTT, YouCook2 &amp;EPIC-KITCHENS), that consider the corresponding caption (in bold) as correct, with many equally similar captions deemed irrelevant. a method that potentially randomly gets the bold captions higher in the ranking is considered SoTA https://t.co/rvXsTBDGL1', 'We propose four proxies for semantic similarity in large-scale benchmarks, without any additional annotations.\nWe demonstrate that methods do not improve over simple baselines when semantic similarity is incorporated. https://t.co/3YUd7tJ55T', 'Watch 3-min intro at: https://t.co/iVujK3KeiE']",https://arxiv.org/abs/2103.10095,"Current video retrieval efforts all found their evaluation on an instance-based assumption, that only a single caption is relevant to a query video and vice versa. We demonstrate that this assumption results in performance comparisons often not indicative of models' retrieval capabilities. We propose a move to semantic similarity video retrieval, where (i) multiple videos/captions can be deemed equally relevant, and their relative ranking does not affect a method's reported performance and (ii) retrieved videos/captions are ranked by their similarity to a query. We propose several proxies to estimate semantic similarities in large-scale retrieval datasets, without additional annotations. Our analysis is performed on three commonly used video retrieval datasets (MSR-VTT, YouCook2 and EPIC-KITCHENS). ",On Semantic Similarity in Video Retrieval
235,1372026320155111424,170582613,Yohei Kawaguchi,"['Very happy to share our new preprint ""Flow-based Self-Supervised Density Estimation for Anomalous Sound Detection"" from Hitachi!\n\n<LINK>\n\nWe propose a new approach for anomaly detection from sound, so check it out!\n\n#MachineLearning #DCASE #ICASSP2021']",https://arxiv.org/abs/2103.08801,"To develop a machine sound monitoring system, a method for detecting anomalous sound is proposed. Exact likelihood estimation using Normalizing Flows is a promising technique for unsupervised anomaly detection, but it can fail at out-of-distribution detection since the likelihood is affected by the smoothness of the data. To improve the detection performance, we train the model to assign higher likelihood to target machine sounds and lower likelihood to sounds from other machines of the same machine type. We demonstrate that this enables the model to incorporate a self-supervised classification-based approach. Experiments conducted using the DCASE 2020 Challenge Task2 dataset showed that the proposed method improves the AUC by 4.6% on average when using Masked Autoregressive Flow (MAF) and by 5.8% when using Glow, which is a significant improvement over the previous method. ","Flow-based Self-supervised Density Estimation for Anomalous Sound
  Detection"
236,1371986590105399297,969190164764372993,Xudong Sun,"['We have two student papers out on arXiv today! The first is from Anna Payne (<LINK>). We studied EUV dimming associated with flux emergence, which is termed ‚Äúemerging dimming‚Äù. We used AIA and HMI data to probe its origin. 1/5 <LINK>', 'The dimming occurs only in 171 A, and coincides with brightening in 211 A. We performed DEM analysis on 18 events. The amount of sub-MK plasma decreases, and the 1-2 MK plasma increases. The changes are correlated over 8 orders of magnitude!  2/5 https://t.co/NbqF8SHGkA', 'We also look at the magnetic fields. The quiet-Sun photospheric field in the dimming region doesn‚Äôt change much. However, a potential field model shows that they are now connected to the emerged active region. 3/5 https://t.co/3Ukl6N9OaD', 'Because all regions are quiet-Sun like with no access to open field, we think the dimming is caused by cool plasma being heated and moving out of the 171 A temperature range rather than outflows. 4/5', 'We conclude that reconnection between the quiet Sun and emerging AR heats the corona and creates the ‚Äúemerging dimming‚Äù. This seems somewhat different from the dark moats around well formed ARs in 171 A. 5/5']",https://arxiv.org/abs/2103.09087,"Emerging dimming occurs in isolated solar active regions (ARs) during the early stages of magnetic flux emergence. Observed by the Atmospheric Imaging Assembly, it features a rapid decrease in extreme-ultraviolet (EUV) emission in the 171 \r{A} channel images, and a simultaneous increase in the 211 \r{A} images. Here, we analyze the coronal thermodynamic and magnetic properties to probe its physical origin. We calculate the time-dependent differential emission measures for a sample of 18 events between 2010 and 2012. The emission measure (EM) decrease in the temperature range $5.7 \le \log_{10}T \le 5.9$ is well correlated with the EM increase in $6.2 \le \log_{10}T \le 6.4$ over eight orders of magnitude. This suggests that the coronal plasma is being heated from the quiet-Sun, sub-MK temperature to 1-2 MK, more typical for ARs. Potential field extrapolation indicates significant change in the local magnetic connectivity: the dimming region is now linked to the newly emerged flux via longer loops. We conclude that emerging dimming is likely caused by coronal heating episodes, powered by reconnection between the emerging and the ambient magnetic fields. ",Emerging Dimming as Coronal Heating Episodes
237,1371855594743767040,795089712864051200,Barret Zoph,"['Revisiting ResNets: Improved Training and Scaling Strategies\n\nOur recent work that applies modern training and scaling techniques to the 2015 ResNet\n\nWe find ResNets outperform some recent state-of-the-art architectures\n\nResNets are remarkably durable!\n\n<LINK> <LINK>', 'We highlight the importance of disentangling the training methods and architectural components when making comparisons across architectures https://t.co/ZbjjBgzGlw', 'We study scaling strategies for vision models and observe the best scaling strategies heavily depends on the training setup\n\nWhen overfitting can occur (e.g. 350 epochs on ImageNet) scaling depth is best. In settings with larger datasets/fewer epochs width scaling is preferred. https://t.co/u4gvBJikWk', 'We design a Pareto curve of 11 different ResNet models named ResNet-RS by scaling the image size along with different network depths.\n\nWe obtain 1.7-2.7x speedups over EfficientNets on ImageNet. https://t.co/fvxWr5U643', 'In a large scale semi-supervised learning setup we obtain 5.5x speedups over Noisy Student EfficientNets. https://t.co/c1GBWExMk1', 'The modern training techniques (data augmentation, label smoothing, etc‚Ä¶) lead to strong representations that rival sota self-supervised learning methods (e.g. SimCLR) on a bunch of vision tasks https://t.co/TD8Mt0w5Yd', 'Hope these revamped ResNets can serve as baselines for future architectural and training method comparisons!']",https://arxiv.org/abs/2103.07579,"Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conflated with simultaneous changes to training methodology and scaling strategies. Our work revisits the canonical ResNet (He et al., 2015) and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we find that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art models. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overfitting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended (Tan & Le, 2019). Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfficientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup, ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than EfficientNet NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classification on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research. ",Revisiting ResNets: Improved Training and Scaling Strategies
238,1371722076814381056,2922037205,Dr. Alvina On Ê∫´ËñèËìÆ,['Do #cosmicrays CRs leave observational signatures as they interact &amp; propagate through #molecularclouds? Yes! Led by Dr Ellis Owen #NTHU we find that low-energy CRs tend to dominate #ionisation. High-energy CRs engage better in heating. See <LINK> @AAS_Publishing'],https://arxiv.org/abs/2103.06542,"We investigate ionization and heating of gas in the dense, shielded clumps/cores of molecular clouds bathed by an influx of energetic, charged cosmic rays (CRs). These molecular clouds have complex structures, with substantial variation in their physical properties over a wide range of length scales. The propagation and distribution of the CRs is thus regulated accordingly, in particular, by the magnetic fields threaded through the clouds and into the dense regions within. We have found that a specific heating rate reaching $10^{-26}$ erg cm$^{-3}$ s$^{-1}$ can be sustained in the dense clumps/cores for Galactic environments, and this rate increases with CR energy density. The propagation of CRs and heating rates in some star-forming filaments identified in IC 5146 are calculated, with the CR diffusion coefficients in these structures determined from magnetic field fluctuations inferred from optical and near-infrared polarizations of starlight, which is presumably a magnetic-field tracer. Our calculations indicate that CR heating can vary by nearly three orders of magnitude between different filaments within a cloud due to different levels of CR penetration. The CR ionization rate among these filaments is similar. The equilibrium temperature that could be maintained by CR heating alone is of order $1~{\rm K}$ in a Galactic environment, but this value would be higher in strongly star-forming environments, thus causing an increase in the Jeans mass of their molecular clouds. ",Observational signatures of cosmic ray interactions in molecular clouds
239,1371654825461579778,972878356319473665,Sophia Economou,"['Did you ever wonder what peeling an orange has to do with quantum control? Find out in <LINK>, where we use geometry to design error-robust Landau-Zener sweeps.\xa0üçä', 'With my student Fei Zhuang--see her talk https://t.co/WJCazOnBMJ on Thursday--collaborator Ed Barnes, and his former student\xa0@JunkaiZeng.', 'See also the cool video by @FryRsquared\nand @numberphile https://t.co/quOnvJvNBl, which largely inspired this work!']",https://arxiv.org/abs/2103.07586,"Landau-Zener physics is often exploited to generate quantum logic gates and to perform state initialization and readout. The quality of these operations can be degraded by noise fluctuations in the energy gap at the avoided crossing. We leverage a recently discovered correspondence between qubit evolution and space curves in three dimensions to design noise-robust Landau-Zener sweeps through an avoided crossing. In the case where the avoided crossing is purely noise-induced, we prove that operations based on monotonic sweeps cannot be robust to noise. Hence, we design families of phase gates based on non-monotonic drives that are error-robust up to second order. In the general case where there is an avoided crossing even in the absence of noise, we present a general technique for designing robust driving protocols that takes advantage of a relationship between the Landau-Zener problem and space curves of constant torsion. ",Noise-resistant Landau-Zener sweeps from geometrical curves
240,1371637851226583041,1167063592941891585,Bonaventure Dossou,"['Using Fon language as a case study, we attempted WEB tokenization, a human-involved super-words tokenization strategy to create a better representative vocabulary for training. It showed improvements in the translation downstream task:\n<LINK>\n\nw/ @ChrisEmezue <LINK>']",http://arxiv.org/abs/2103.08052,"Building effective neural machine translation (NMT) models for very low-resourced and morphologically rich African indigenous languages is an open challenge. Besides the issue of finding available resources for them, a lot of work is put into preprocessing and tokenization. Recent studies have shown that standard tokenization methods do not always adequately deal with the grammatical, diacritical, and tonal properties of some African languages. That, coupled with the extremely low availability of training samples, hinders the production of reliable NMT models. In this paper, using Fon language as a case study, we revisit standard tokenization methods and introduce Word-Expressions-Based (WEB) tokenization, a human-involved super-words tokenization strategy to create a better representative vocabulary for training. Furthermore, we compare our tokenization strategy to others on the Fon-French and French-Fon translation tasks. ","Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine
  Translation: The Case of Fon Language"
241,1371468428817674243,471165766,Ion Errea,"[""Today we post a beautiful preprint, where we find a strong correlation between the chemical bonding network and Tc in hydrogen-rich syperconductors. It's important for setting up search directions in the future. Superb work by @Frances89010669! #SuperH <LINK> <LINK>""]",https://arxiv.org/abs/2103.07320,"Recent experimental discoveries show that hydrogen-rich compounds can reach room temperature superconductivity, at least at high pressures. Also that there exist metallic hydrogen-abundant systems with critical temperatures of few Kelvin, or even with no trace of superconductivity at all. By analyzing through first-principles calculations the structural and electronic properties of more than one hundred compounds predicted to be superconductors in the literature, we determine that the capacity of creating a bonding network of connected localized units is the key to enhance the critical temperature in hydrogen-based superconductors, explaining the large variety of critical temperatures of superconducting hydrogen-rich materials. We define a magnitude named as the {\it networking value}, which correlates well with the predicted critical temperature, much better than any other descriptor analyzed thus far. This magnitude can be easily calculated for any compound by analyzing isosurfaces of the electron localization function. By classifying the studied compounds according to their bonding nature, we observe that the {\it networking value} correlates with the critical temperature for all bonding types. Our analysis also highlights that systems with weakened covalent bonds are the most promising candidates for reaching high critical temperatures. The discovery of the positive correlation between superconductivity and the bonding network offers the possibility of screening easily hydrogen-based compounds and, at the same time, sets clear paths for chemically engineering better superconductors. ","Strong correlation between bonding network and critical temperature in
  hydrogen-based superconductors"
242,1370492784319344648,3102084481,Samarth Sinha,"['Excited to share our recent work:\n\nSurprisingly Simple Self-Supervision for Offline RL where we propose a Surprisingly Simple method to learn representations using data augmentations from offline data which achieves SOTA performance! \n\n<LINK>\n\nw/ @animesh_garg <LINK>', 'First, we investigate the role of 6 different data augmentation strategies from states, many inspired from latest computer vision research! \n\nTurns out, the simpler strategies work best! https://t.co/LecrR7ibjS', 'Then we experiment with recently proposed self-supervised learning strategies on a host of Mujoco, Adroit, AntMaze, and robotic tasks and see consistent improvements in performance over SOTA agents https://t.co/3wrD5FTIem', 'We also experiment in the setting where only limited offline data (~10% of the dataset) is available for training. \nThe role of data augmentation and self supervised learning is significantly larger in these settings! https://t.co/WDBIL9ae7T', 'Finally, we also experiment with 5 challenging MetaWorld robotics tasks (@TianheYu et al.) where we continue to see improvements on the base CQL agent and other self-supervised learning baselines https://t.co/lElt97oO9U', 'Video + blogpost soon to follow!\n\nAny feedback is well appreciated üôÇ', '@egrefen @animesh_garg Thanks, Ed! üôÇ \n\nWonder if anyone will be able to do offline RL on challenging exploration problems like NetHack ü§î', '@egrefen @animesh_garg Yeah it would be very interesting to see how offline RL would do on very difficult environments without interactions. I would hypothesize(?) that you would need data from a near optimal policy because the state spaces are so large']",http://arxiv.org/abs/2103.06326,"Offline reinforcement learning proposes to learn policies from large collected datasets without interacting with the physical environment. These algorithms have made it possible to learn useful skills from data that can then be deployed in the environment in real-world settings where interactions may be costly or dangerous, such as autonomous driving or factories. However, current algorithms overfit to the dataset they are trained on and exhibit poor out-of-distribution generalization to the environment when deployed. In this paper, we study the effectiveness of performing data augmentations on the state space, and study 7 different augmentation schemes and how they behave with existing offline RL algorithms. We then combine the best data performing augmentation scheme with a state-of-the-art Q-learning technique, and improve the function approximation of the Q-networks by smoothening out the learned state-action space. We experimentally show that using this Surprisingly Simple Self-Supervision technique in RL (S4RL), we significantly improve over the current state-of-the-art algorithms on offline robot learning environments such as MetaWorld [1] and RoboSuite [2,3], and benchmark datasets such as D4RL [4]. ","S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement
  Learning"
243,1369850926803124226,824418390005731329,Heng Yang,"['Dynamics and Perception? With C. Doran and J.-J. Slotine, we propose DynAMical Pose estimation (DAMP), solving five pose estimation problems by simulating rigid body dynamics from virtual springs and damping:\nPaper: <LINK>\nVideo: <LINK>']",https://arxiv.org/abs/2103.06182,"We study the problem of aligning two sets of 3D geometric primitives given known correspondences. Our first contribution is to show that this primitive alignment framework unifies five perception problems including point cloud registration, primitive (mesh) registration, category-level 3D registration, absolution pose estimation (APE), and category-level APE. Our second contribution is to propose DynAMical Pose estimation (DAMP), the first general and practical algorithm to solve primitive alignment problem by simulating rigid body dynamics arising from virtual springs and damping, where the springs span the shortest distances between corresponding primitives. We evaluate DAMP in simulated and real datasets across all five problems, and demonstrate (i) DAMP always converges to the globally optimal solution in the first three problems with 3D-3D correspondences; (ii) although DAMP sometimes converges to suboptimal solutions in the last two problems with 2D-3D correspondences, using a scheme for escaping local minima, DAMP always succeeds. Our third contribution is to demystify the surprising empirical performance of DAMP and formally prove a global convergence result in the case of point cloud registration by charactering local stability of the equilibrium points of the underlying dynamical system. ",Dynamical Pose Estimation
244,1369688157558431749,1253758756304809984,Igor Mordatch,"['What are the limits to the generalization of large pretrained transformer models?\n\nWe find minimal fine-tuning (~0.1% of params) performs as well as training from scratch on a completely new modality!\n\nwith @_kevinlu, @adityagrover_, @pabbeel\npaper: <LINK>\n\n1/8', 'We take pretrained GPT-2 and freeze the attention &amp; FF layers to obtain core of Frozen Pretrained Transformer (FPT).\n\nTo adapt to new modality &amp; task, we init *linear* input and output layers.\n\nDespite only training .1% of params, FPT matches performance of full transformer!\n\n2/8 https://t.co/1zjfCKm9rK', 'We visualize the attention maps of the frozen transformer.\n\nDespite not finetuning the self-attention layers on the new modality, FPT is able to learn to attend to the relevant bits to compute an elementwise XOR with perfect accuracy for sequences of length up to 256.\n\n3/8 https://t.co/tC1USSV4rS', 'Compared to a randomly initialized frozen transformer, pretraining with language (FPT) yields large compute benefits, showing that, much like common practice for in-modality finetuning, we can save computation by starting from a pretrained model:\n\n4/8 https://t.co/vmJlrBsfhg', 'What enables this transfer? We find that simply using a randomly initialized frozen transformer already greatly outperforms a randomly initialized frozen LSTM:\n\n5/8 https://t.co/17uMUZ1YO5', 'Additionally, by incorporating various sources of pretraining supervision, even a little pretraining, for example learning to memorize bits, can help transfer:\n\n6/8 https://t.co/KqOxlBgTY9', 'As we move from small specialist models to large generalist models, we‚Äôre excited by the potential for pretraining regimes that could train a universal computation engine.\n\nSimply adding more parameters and using a larger model already improves performance:\n\n7/8 https://t.co/sxtgfCUrGw', 'For more details, see our paper on arXiv or play our demo/code on Github!\n\narXiv: https://t.co/DtWGJ0Afh7\n\nGithub: https://t.co/9ts1FlqHFw\n\n8/8', ""@rndmcnlly @_kevinlu @adityagrover_ @pabbeel Indeed Adam! That was our starting hypothesis. One thing we're still not quite clear on is whether this FPT interpreter largely performs generic input summarization or more extensive computation.""]",http://arxiv.org/abs/2103.05247,"We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning -- in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks. ",Pretrained Transformers as Universal Computation Engines
245,1369658874421256196,281711973,Dr. Emily Rickman,['New paper day!üöÄ\n\nIn a study led by @RAsensioTorres we derive upper mass limits of companions in 15 systems that could be responsible for the structures seen in their protoplanetary disks with @ESO @SPHERE_outreach üí´‚ú®üî≠\n\nCheck out the paper here üëá\n<LINK> <LINK>'],https://arxiv.org/abs/2103.05377,"The detection of a wide range of substructures such as rings, cavities and spirals has become a common outcome of high spatial resolution imaging of protoplanetary disks, both in the near-infrared scattered light and in the thermal millimetre continuum emission. The most frequent interpretation of their origin is the presence of planetary-mass companions perturbing the gas and dust distribution in the disk (perturbers), but so far the only bona-fide detection has been the two giant planets around PDS 70. Here, we collect a sample of 15 protoplanetary disks showing substructures in SPHERE scattered light images and present a homogeneous derivation of planet detection limits in these systems. We also estimate the mass of these perturbers through a Hill radius prescription and a comparison to ALMA data. Assuming that one single planet carves each substructure in scattered light, we find that more massive perturbers are needed to create gaps within cavities than rings, and that we might be close to a detection in the cavities of RX J1604, RX J1615, Sz Cha, HD 135344B and HD 34282. We reach typical mass limits in these cavities of 3-10 Mjup. For planets in the gaps between rings, we find that the detection limits of SPHERE are about an order of magnitude away in mass, and that the gaps of PDS 66 and HD 97048 seem to be the most promising structures for planet searches. The proposed presence of massive planets causing spiral features in HD 135344B and HD 36112 are also within SPHERE's reach assuming hot-start models.These results suggest that current detection limits are able to detect hot-start planets in cavities, under the assumption that they are formed by a single perturber located at the centre of the cavity. More realistic planet mass constraints would help to clarify whether this is actually the case, which might point to perturbers not being the only way of creating substructures. ","Perturbers: SPHERE detection limits to planetary-mass companions in
  protoplanetary disks"
246,1369596736054976514,471165766,Ion Errea,"['Today it is an important day in my research career: we release the SSCHA code. You can find the info about the code here (<LINK>), read the paper related to the code in <LINK>, and follow it on twitter @SSCHA_code', 'All this started when in my PhD Bruno Rousseau proposed to use the Self-Consistent Harmonic Approximation to study the high-pressure simple cubic phase of Ca. We implemented the idea, but the implementation was too primordial for any other material https://t.co/BNDNge7JkJ', 'As a postdoc I moved to Paris to work Francesco Mauri‚Äôs group. We developed there the SSCHA and started its first applications on hydrogen-based superconductors https://t.co/QLPzgta2Ou https://t.co/WbznRxy3vy', 'Later Raffaello Bianco showed how the SSCHA theory could be expanded to predict second-order phase transitions easily and describe phonon spectral properties https://t.co/rXROrJKbqH', 'These improvements have been crucial to study CDW phase transitions and perform thermal transport calculations on thermoelectric materials https://t.co/REWg8LO0Ff https://t.co/nrouwbpX5h', 'Now @mesonepi has taken the lead in the development of the code and brought to it an improved efficiency and the possibility of making structural relaxations in the quantum energy landscape  https://t.co/5qfmpNhzMF', 'The latter development was crucial to show how quantum effects make stable the high-temperature superconducting LaH10 https://t.co/aIpRNxa95S', 'Now the code is distributed as open source, available to everybody. Your contributions will be welcome and surely will improve the capacity of the code.']",https://arxiv.org/abs/2103.03973,"The efficient and accurate calculation of how ionic quantum and thermal fluctuations impact the free energy of a crystal, its atomic structure, and phonon spectrum is one of the main challenges of solid state physics, especially when strong anharmonicy invalidates any perturbative approach. To tackle this problem, we present the implementation on a modular Python code of the stochastic self-consistent harmonic approximation method. This technique rigorously describes the full thermodyamics of crystals accounting for nuclear quantum and thermal anharmonic fluctuations. The approach requires the evaluation of the Born-Oppenheimer energy, as well as its derivatives with respect to ionic positions (forces) and cell parameters (stress tensor) in supercells, which can be provided, for instance, by first principles density-functional-theory codes. The method performs crystal geometry relaxation on the quantum free energy landscape, optimizing the free energy with respect to all degrees of freedom of the crystal structure. It can be used to determine the phase diagram of any crystal at finite temperature. It enables the calculation of phase boundaries for both first-order and second-order phase transitions from the Hessian of the free energy. Finally, the code can also compute the anharmonic phonon spectra, including the phonon linewidths, as well as phonon spectral functions. We review the theoretical framework of the stochastic self-consistent harmonic approximation and its dynamical extension, making particular emphasis on the physical interpretation of the variables present in the theory that can enlighten the comparison with any other anharmonic theory. A modular and flexible Python environment is used for the implementation, which allows for a clean interaction with other packages. We briefly present a toy-model calculation to illustrate the potential of the code. ","The Stochastic Self-Consistent Harmonic Approximation: Calculating
  Vibrational Properties of Materials with Full Quantum and Anharmonic Effects"
247,1369377672082690048,66173851,Jason Ramapuram,"['Kanerva++: extending The Kanerva Machine with differentiable, locally block allocated latent memory : our recent ICLR 2021 paper in collaboration with Yan Wu &amp; Alexandros Kalousis.\n\n<LINK>\n\nWe propose a novel memory model inspired by traditional heap allocation <LINK>', 'We extend the Kanerva Machine ( https://t.co/ZM8vnC9qow ) by simplifying the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. https://t.co/utE7eZtPIE', 'Sample generation uses stochasticity in a low dimensional key distribution (R^3) which is then used to parameterize a spatial transformer (https://t.co/L0SOw7C2rI) to read contiguous memory sub-regions. https://t.co/6J6Sg4RkLP', 'Local key perturbations of a trained DMLab maze K++ model induces resultant generations that provide a natural traversal of the maze as observed by scanning the figure below row by row, from left to right. The model was  trained with IID inputs! https://t.co/KnrkH378Zi', 'A similar behavior is observed for Omniglot generations. https://t.co/YJcivStLvd', 'We also show that iterative inference via the K++ model is robust to various kinds of noise such as salt &amp; pepper, speckle and Poisson noise. https://t.co/FklOLb2fMa', 'TLDR: stochasticity in a low dimensional distribution can be leveraged to generate samples by reading contiguous spatial latents from a differentiable memory model!', '@poolio We use the std Larochelle bin-MNIST with the std splits (code in ICLR supp material). Quick run attached. Lower value reasons:\n1. Core stochasticity is in a low dim space (R^3) --&gt; decoder can better reconstruct samples. \n2. Memory houses contextual info about other samples. https://t.co/iQdD6wt0cd']",https://arxiv.org/abs/2103.03905,"Episodic and semantic memory are critical components of the human memory model. The theory of complementary learning systems (McClelland et al., 1995) suggests that the compressed representation produced by a serial event (episodic memory) is later restructured to build a more generalized form of reusable knowledge (semantic memory). In this work we develop a new principled Bayesian memory allocation scheme that bridges the gap between episodic and semantic memory via a hierarchical latent variable model. We take inspiration from traditional heap allocation and extend the idea of locally contiguous memory to the Kanerva Machine, enabling a novel differentiable block allocated latent memory. In contrast to the Kanerva Machine, we simplify the process of memory writing by treating it as a fully feed forward deterministic process, relying on the stochasticity of the read key distribution to disperse information within the memory. We demonstrate that this allocation scheme improves performance in memory conditional image generation, resulting in new state-of-the-art conditional likelihood values on binarized MNIST (<=41.58 nats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting competitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32. ","Kanerva++: extending The Kanerva Machine with differentiable, locally
  block allocated latent memory"
248,1369295578849415168,1261860960895004672,Jorge Moreno,"[""It's paper day. We find that  bursty episodes of star formation at high redshift promote the formation of the thick disc in simulated Milky Ways (FIRE zooms). Congrats to the lead author, @AstroBananna, for this fantastic paper! <LINK> <LINK>"", 'And kudos to all the coauthors! @jbprime @astro_klein @AndrewWetzel @alexbgurvich @PFHopkins_Astro']",https://arxiv.org/abs/2103.03888,"We investigate thin and thick stellar disc formation in Milky-Way-mass galaxies using twelve FIRE-2 cosmological zoom-in simulations. All simulated galaxies experience an early period of bursty star formation that transitions to a late-time steady phase of near-constant star formation. Stars formed during the late-time steady phase have more circular orbits and thin-disc-like morphology at $z=0$, whilst stars born during the bursty phase have more radial orbits and thick-disc structure. The median age of thick-disc stars at $z=0$ correlates strongly with this transition time. We also find that galaxies with an earlier transition from bursty to steady star formation have a higher thin-disc fractions at $z=0$. Three of our systems have minor mergers with LMC-size satellites during the thin-disc phase. These mergers trigger short starbursts but do not destroy the thin disc nor alter broad trends between the star formation transition time and thin/thick disc properties. If our simulations are representative of the Universe, then stellar archaeological studies of the Milky Way (or M31) provide a window into past star-formation modes in the Galaxy. Current age estimates of the Galactic thick disc would suggest that the Milky Way transitioned from bursty to steady phase $\sim$6.5 Gyr ago; prior to that time the Milky Way likely lacked a recognisable thin disc. ",The bursty origin of the Milky Way thick disc
249,1369266144964730885,1145298986548551680,Se√°n Kavanagh,"['First solo-theory paper of my PhD with @lonepair and @scanlond81! We find non-radiative recombination at cadmium vacancies reduces PV efficiency by over 5% in untreated CdTe ‚Äì a result of metastability and anharmonicity in defect PESs üíª‚òÄÔ∏èüîåüåç\n<LINK>', 'Using the powerful CarrierCapture.jl package (https://t.co/i6P3zsqudB) and TLC metric (https://t.co/y8lqsR5mjv) developed by @RealSunghyunKim üî• üî•']",http://arxiv.org/abs/2103.00984,"CdTe is a key thin-film photovoltaic technology. Non-radiative electron-hole recombination reduces the solar conversion efficiency from an ideal value of 32% to a current champion performance of 22%. The cadmium vacancy (V_Cd) is a prominent acceptor species in p-type CdTe; however, debate continues regarding its structural and electronic behavior. Using ab initio defect techniques, we calculate a negative-U double-acceptor level for V_Cd, while reproducing the V_Cd^-1 hole-polaron, reconciling theoretical predictions with experimental observations. We find the cadmium vacancy facilitates rapid charge-carrier recombination, reducing maximum power-conversion efficiency by over 5% for untreated CdTe -- a consequence of tellurium dimerization, metastable structural arrangements, and anharmonic potential energy surfaces for carrier capture. ",Rapid Recombination by Cadmium Vacancies in CdTe
250,1368841022374088707,776765039726460929,Carlo Felice Manara,"['Paper by previous @ESO student Eleonora Fiorellino is out! <LINK>\nWe used #KMOS on the #VLT to study accretion in young stars in NGC1333 in the Perseus region. We find higher accretion rates in Class I wrt Class II, but no extremely high Macc in Class I <LINK>']",https://arxiv.org/abs/2103.03863,"The mass accretion rate is the fundamental parameter to understand the process of mass assembly that results in the formation of a low-mass star. This parameter has been largely studied in Classical TTauri stars in star-forming regions with ages of 1-10Myr. However, little is known about the accretion properties of young stellar objects (YSOs) in younger regions and early stages of star formation, such as in the Class0/I phases. We present new NIR spectra of 17 ClassI/Flat and 35 ClassII sources located in the young (<1Myr) NGC1333 cluster, acquired with the KMOS instrument at the VLT. Our goal is to study whether the mass accretion rate evolves with age, as suggested by the widely adopted viscous evolution model, by comparing the properties of the NGC1333 members with samples of older regions. We measured the stellar parameters and accretion rates of our sample, finding a correlation between accretion and stellar luminosity, and between mass accretion rate and stellar mass. Both correlations are compatible within the errors with the older Lupus star-forming region, while only the latter is consistent with results from ChamaeleonI. The ClassI sample shows larger accretion luminosities with respect to the ClassII stars of the same cloud. However, the derived accretion rates are not sufficiently high to build up the inferred stellar masses, assuming steady accretion during the ClassI lifetime. This suggests that the sources are not in their main accretion phase and that most of their mass has already been accumulated during a previous stage and/or that the accretion is an episodic phenomenon. We show that some of the targets originally classified as Class I through Spitzer photometry are in fact evolved or low accreting objects. This evidence can have implications for the estimated protostellar phase lifetimes. Further observations are needed to determine if this is a general result. ","KMOS study of the mass accretion rate from Class I to Class II in NGC
  1333"
251,1368244397364699138,88273207,Matthias Gall√©,"[""Paper ad üì¢üìú\n\nFinally we too went chasing ghosts. üëªüëª\nWe asked us the questions: is it possible to find good language models that do not rediscover linguistic knowledge?\n\nThread below. \ntl;dr: you can't avoid ghosts \n\n<LINK> <LINK>"", 'Joint work with @VNikoulina and collaborators from Nazarbayev University: Maxat Tezekbayev, Nuradil Kozhakhmet, Madina Babazhanova and of course Zhenisbek Assylbekov', 'We wanted to compare similarly performing language models and see their probing performance. To obtain those we pruned them with lottery ticket hypothesis\nAcross models, the LTH experiments show a correlation between perplexity and probing perf. https://t.co/Z2d91p5HvO', 'We tried a number of other things, but we could not show a counter-example to the rediscovery hypothesis.\n\nSo we proved (some version of) it\nAssume you have linguistic knowledge T on top of words W. The key is this relationship (I=mutual information, H=entropy) https://t.co/yDrfh1RfgZ', ""Our main theorems says that the decrease in language modelling performance between an embedding x and an embedding x' that has no information on T is supralinear in œÅ"", 'We used techniques that remove linguistic information (similar to https://t.co/XUmnFcLNrR by Elazar, Ravfogel, @alon_jacovi and @yoavgo) and show that the Theorem holds empirically.\nThis plot shows the relationship on synthetic data; numbers on real data in the paper https://t.co/XxWP3rV0TC', 'We started looking for an alternative way of training LM which would avoid good probing scores.\nOur work seems to indicate that there is no way of doing that. \n\nYou want good perplexity? On the way to it you have to check-in to the linguistic knowledge inn. https://t.co/eq6ubJ4vDm']",https://arxiv.org/abs/2103.01819,"There is an ongoing debate in the NLP community whether modern language models contain linguistic knowledge, recovered through so-called probes. In this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the \textit{rediscovery hypothesis}. In the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. This result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. This framework also provides a metric to measure the impact of linguistic information on the word prediction task. We reinforce our analytical results with various experiments, both on synthetic and on real NLP tasks in English. ",The Rediscovery Hypothesis: Language Models Need to Meet Linguistics
252,1367864654500950017,1115965906448863232,Floor Broekgaarden üí´,"[""1st paper of my PhD is out!üòç It's all about the mysterious BH-NS binaries: one of the last type of compact binaries that so far has evaded our detections! üïµüïµüèæ\u200d‚ôÄÔ∏èü§î I discuss how many we should find with GW detectors and what we might learn!‚ö´Ô∏è-‚ö™Ô∏èüí´ A üßµ\n<LINK> <LINK>"", 'After detecting several mergers of BH-BH &amp; NS-NS, astronomers are still eagerly awaiting the first confident detection of a BH-NS mergers through gravitational waves by @LIGO/@ego_virgo/@KAGRA_PR. \nBut how many do we expect? And what would they look like?', 'In this study, we investigate these questions by simulating populations of binary stars and looking at which subset of binaries becomes a BH-NS system that can merge and form gravitational waves that we can observe today!', ""However, many things in the simulations are unknown. How many stars form throughout our Universe? How do they form BH-NS? We investigate this by presenting a total of 420 model permutations üïµüèæ\u200d‚ôÄÔ∏èüïµüèæ\u200d‚ôÄÔ∏è that's a lot! üò±"", 'However, there are many unknowns in these simulations. How many stars are born throughout the Universe? How do stars evolve when they live in a binary? ü§î This is why we investigated a total of 420 permutations of possible models! üïµüèæ\u200d‚ôÄÔ∏èWüïµüèæ\u200d‚ôÄÔ∏è.  .', 'We find that GW detectors at design sensitivity could find between 1-180 BH-NS per year!  üôÉ https://t.co/Qo8flx6PMO', 'we also find that as many as 70% of those GW detections could have an Electromagnetic counterpart! üí´Although most models result in percentages closer to ~1-5%.  Such a ""multi-messenger"" observation of a BH-NS would be a golden grail in Astronomy! https://t.co/cS5iZCETRA', ""We also make predictions for what these BH-NS would look like. Surprisingly, none of our models expect to detect many BH-NS mergers with BH masses &gt; 20 Msun. If these are detected this would likely mean that these BH-NS didn't form from two stars in an isolated binary. https://t.co/e82PWm1Hkd"", ""in that case it could tell us that instead BH-NS form in other environments such as clusters  (e.g, @aCarlRodriguez) triples (@silvia_toonen ) or AGN disks (e.g., @saavikford et al.) \n(sorry I don't have you all on twitter)"", ""Of course we can look at many more things than just the BH mass. Here's one of my favorite figures showing almost 3000 predicted BH-NS PDFs!!! https://t.co/BggcW3UvdJ"", ""And my 2nd favorite: showing the distribution quantiles and comparing them with the 3 possible BH-NS candidates from @LIGO/@ego_virgo's  #GWTC2.   GW190425 and GW190426 seem a bit outliers, but GW190814 looks like a v typical BH-NS üò±ü§î https://t.co/3wm0JRVavp"", ""This was a super fun project. Couldn't have done it without my great collaborators @edobergerhvd Coen, @alejandro_vigna @c_debatri @simon4nine, Martyna, Stephen, @selmademink, and Ilya, each great collaborators that I had the honor to work with!"", 'and most importantly: all data/code to reproduce all results &amp; figures (&amp; more) is publicly available through:\nhttps://t.co/yqYfhJivxS\nhttps://t.co/XQLKEbpyxB\nbecause we love @paperswithcode \n\nperfect if you want to plot/find your own BH-NS properties! https://t.co/KbEqdVUI98', 'and thanks to everyone else who helped me along the way!! incl twitter folks @victoriadi2MASS @gusbeane @CMooreSpace @SGomez_J @astroVAV @SerenaViarago @mj_borreggine @Astro_Locke']",https://arxiv.org/abs/2103.02608,"Mergers of black hole-neutron star (BHNS) binaries have now been observed by GW detectors with the recent announcement of GW200105 and GW200115. Such observations not only provide confirmation that these systems exist, but will also give unique insights into the death of massive stars, the evolution of binary systems and their possible association with gamma-ray bursts, $r$-process enrichment and kilonovae. Here we perform binary population synthesis of isolated BHNS systems in order to present their merger rate and characteristics for ground-based GW observatories. We present the results for 420 different model permutations that explore key uncertainties in our assumptions about massive binary star evolution (e.g. mass transfer, common-envelope evolution, supernovae), and the metallicity-specific star formation rate density, and characterize their relative impacts on our predictions. We find intrinsic local BHNS merger rates spanning $\mathcal{R}_{\rm{m}}^0 \approx 4$-$830\,\rm{Gpc}^{-3}\,\rm{yr}^{-1}$ for our full range of assumptions. This encompasses the rate inferred from recent BHNS GW detections, and would yield detection rates of $\mathcal{R}_{\rm{det}} \approx 1$-$180\, \rm{yr}^{-1}$ for a GW network consisting of LIGO, Virgo and KAGRA at design sensitivity. We find that the binary evolution and metallicity-specific star formation rate density each impact the predicted merger rates by order $\mathcal{O}(10)$. We also present predictions for the GW detected BHNS merger properties and find that all 420 model variations predict that $\lesssim 5\%$ of the BHNS mergers have BH masses $\gtrsim 18\,M_{\odot}$, total masses $ \gtrsim 20\,M_{\odot}$, chirp masses $\gtrsim 5.5\,M_{\odot}$, mass ratios $ \gtrsim 12$ or $\lesssim 2$. Moreover, we find that massive NSs $\gtrsim 2\,M_{\odot}$ are expected to be commonly detected in BHNS mergers in almost all our model variations. ","Impact of Massive Binary Star and Cosmic Evolution on Gravitational Wave
  Observations I: Black Hole-Neutron Star Mergers"
253,1367839174469029899,370409954,Gael Varoquaux,"['New preprint: Accounting for Variance in Machine Learning Benchmarks\n<LINK>\nLead by @bouthilx and @Mila_Quebec friends\n\nWe show that ML benchmarks contain multiple sources of uncontrolled variation, not only inits. We propose procedure for reliable conclusion 1/8', 'Data split and hyper-parameter selection (even with fancy hyper-parameter optimization) appear as the leading source of arbitrary variations in ML benchmarks, beyond random weight init.\n\nThese must be sampled to give empirical evidence on algorithm comparison that generalize 2/8 https://t.co/kE8VemNrPY', 'Even in deep-learning benchmarks, performed on large datasets, the variance of the observed performance is limited by the set of the test set 3/8 https://t.co/PAvhOpJ86O', 'Altogether, these variances are not small compared to observed improvements in the literature. Hence, there is a risk that published findings may be due to chance, for instance finding better hyper-parameters from one algorithm than another 4/8 https://t.co/qwCdfCmPj5', 'Sampling the variance of hyper-parameter tuning is very costly, eg in deep learning. We measure it in clean (and costly) experiments, but also study imperfect estimators.\nWe show that it is best to sample all sources of variation to minimize error on estimation of performance 5/8 https://t.co/uOsyCzM8C9', 'Variance of performance results must be accounted for to conclude on whether or not there is evidence that an algorithm is an improvement.\n\nTo avoid accepting trivial differences, we use to use ""non-inferiority"", or Neyman-Pearson, tests, used in clinical trials 6/8 https://t.co/5CsC13AsRb', 'Our recommendations (based on 5 case studies, deep learning &amp; classical ML):\n‚Ä¢ Randomize  as  many  sources  of  variations  as  possible (weight inits, data order, data splitting)\n‚Ä¢ Multiple validation splits\n‚Ä¢ P(A &gt; B) &gt; 0.75 (comparing improvement to variance)\n\n7/8', 'This work relies on extensive experiments, with multiple datasets and ML pipelines, including many hyper-parameter optimization of deep learning pipelines.\n\nThanks to an amazing teams, comprising @bouthilx @AssyaTrofimov @EdwardRaffML and many more\n\nhttps://t.co/StDk21LCTI\n8/8']",https://arxiv.org/abs/2103.03098,"Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, data augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51 times reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on five different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons. ",Accounting for Variance in Machine Learning Benchmarks
254,1367819732641054729,1115299382113517568,Matthew Ware,"['Mid-circuit measurement is critical for QEC as well as near-term applications. Here we @gribeill @Luke_Govia extend GST to include quantum instruments and study our mid-circuit readout <LINK>. Always fun working with Sandia! <LINK>', ""I didn't tag @KRudinger in this because I couldn't find his Twitter handle this morning! I am an idiot as advertised""]",https://arxiv.org/abs/2103.03008,"Measurements that occur within the internal layers of a quantum circuit -- mid-circuit measurements -- are an important quantum computing primitive, most notably for quantum error correction. Mid-circuit measurements have both classical and quantum outputs, so they can be subject to error modes that do not exist for measurements that terminate quantum circuits. Here we show how to characterize mid-circuit measurements, modelled by quantum instruments, using a technique that we call quantum instrument linear gate set tomography (QILGST). We then apply this technique to characterize a dispersive measurement on a superconducting transmon qubit within a multiqubit system. By varying the delay time between the measurement pulse and subsequent gates, we explore the impact of residual cavity photon population on measurement error. QILGST can resolve different error modes and quantify the total error from a measurement; in our experiment, for delay times above 1000 ns we measured a total error rate (i.e., half diamond distance) of $\epsilon_{\diamond} = 8.1 \pm 1.4 \%$, a readout fidelity of $97.0 \pm 0.3\%$, and output quantum state fidelities of $96.7 \pm 0.6\%$ and $93.7 \pm 0.7\%$ when measuring $0$ and $1$, respectively. ","Characterizing mid-circuit measurements on a superconducting qubit using
  gate set tomography"
255,1367371492930056198,948995274961309697,Andrea Botteon,"['My latest paper is out: we studied the non-thermal phenomena in the center of Abell 1775 using @chandraxray, @LOFAR, GMRT and @TheNRAO VLA data. Check a couple of nice images from the paper below, and read this thread for more info on the study üòâ 1/11\n<LINK> <LINK>', 'First, the @chandraxray data shows a complex system rich of structures, including: a ""mushroom"" feature bounded by a prominent edge and an arc-shaped cold front towards NE. The X-ray surface brightness shows a spiral-like pattern that is mirrored in the kT and pseudo-K maps 2/11 https://t.co/r9eiTeN8Nv', 'Second, the new @LOFAR image at 144 MHz shows an extended and spectacular head-tail radio galaxy and other filamentary structures (F1 and F2). The head-tail is 800 kpc long, meaning that it is 2x longer than what previously reported 3/11 https://t.co/AArkHAUec8', ""The new 'extension' of the tail is only observed at low-frequencies, likely due to its steep and curved spectrum. This outer tail is more diffuse than the inner tail and originates after a 'break' of its structure. This occurs at the position of the arc-shaped cold front 4/11 https://t.co/n4ULCrn1Af"", 'This suggests an interaction between the tail and the surrounding medium. We speculated that particle re-acceleration of dormant tail electrons is occurring in the final region of the tail. In the paper, we also reported and studied the spatial trends along the tail 5/11 https://t.co/QH7oYLplnU', 'It is not over. In the LOFAR low-resolution image we noticed the presence of diffuse radio emission in the cluster center. This emission is bounded by the arc-shaped cold front, is detected only with LOFAR, and has a radio power of P144=3.1e24 W/Hz 6/11 https://t.co/jcPe771SmS', 'What is this emission? To understand its origin, we need to understand the dynamical state of the system, which is not very clear. We proposed two scenarios to explain the X-ray features observed: sloshing and slingshot gas tail (https://t.co/DmNbWgf6kB) 7/11', ""We argued that the preferred scenario is the latter, hence we termed the diffuse emission 'slingshot radio halo'. Still, follow-up work is required to confirm the suggested dynamical configuration of the system, which was and remained ambiguous 8/11"", 'What about F1 and F2 then? These sources have ultra-steep spectrum, and we classified them as revived fossil plasma. Their distorted shape may suggest the presence of significant turbulent motions in the ICM that could re-accelerate particles in the slingshot radio halo 9/11 https://t.co/qCfjmRzfNl', 'Conclusions: Abell 1775 is a very complex system rich of X-ray and radio features on different scales, and shows ongoing interplay between thermal and non-thermal components in its center: it is a fantastic laboratory to study ICM physics! 10/11', 'I would like to thank my collaborators from @UniLeiden, @IRA_INAF, @ASTRON_NL, @SRON_Space, @HambObs++ that were involved in this project: @fabiogasta, @mcrossetti_twit, @HirokiAkamatsu, @roxycas, @fradega, @astro_jit + all the others that are not on Twitter! 11/11', '@fabiogasta @ESA_XMM @chandraxray Thank you Fabio for proposing and getting those beautiful data...and as you know their exploitation is not over yet eheh üòâ']",https://arxiv.org/abs/2103.01989,"Thermal gas in the center of galaxy clusters can show substantial motions that generate surface-brightness and temperature discontinuities known as cold fronts. The motions may be triggered by minor or off-axis mergers that preserve the cool core of the system. The dynamics of the thermal gas can also generate radio emission from the intra-cluster medium (ICM) and impact the evolution of clusters' radio sources. We aim to study the central region of Abell 1775, a system in an ambiguous dynamical state at $z=0.072$ which is known to host an extended head-tail radio galaxy, with the goal of investigating the connection between thermal and nonthermal components in its center. We made use of a deep (100 ks) Chandra observation accompanied by LOFAR 144 MHz, GMRT 235 MHz and 610 MHz, and VLA 1.4 GHz radio data. We find a spiral-like pattern in the X-ray surface brightness that is mirrored in the temperature and pseudo-entropy maps. Additionally, we characterize an arc-shaped cold front in the ICM. We interpret these features in the context of a slingshot gas tail scenario. The structure of the head-tail radio galaxy ""breaks"" at the position of the cold front, showing an extension that is detected only at low frequencies, likely due to its steep and curved spectrum. We speculate that particle reacceleration is occurring in the outer region of this tail, which in total covers a projected size of $\sim800$ kpc. We also report the discovery of revived fossil plasma with ultra-steep spectrum radio emission in the cluster core together with a central diffuse radio source that is bounded by the arc-shaped cold front. The results reported in this work demonstrate the interplay between thermal and nonthermal components in the cluster center and the presence of ongoing particle reacceleration in the ICM on different scales. ","Nonthermal phenomena in the center of Abell 1775: An 800 kpc head-tail,
  revived fossil plasma and slingshot radio halo"
256,1367038919116787713,1100602684317536256,Alvaro M. Alhambra,"['A little bit of self-promotion today: <LINK> \n\nWe study some definitions of Renyi MI, and show some nice properties like a thermal area law. Potentially some of them can be approximated with TN or other variational methods!', 'Also apologies to all the Shannon theory folks who will realize we (still) did not define the Renyi MI in the""right"" way (with the optimization over the marginal). üòÖ', ""@markwilde Thanks, that is good to hear! Yes, there is a lot of physics literature with that definition, and you really don't have to look for long to find situations where its for instance negative."", '@markwilde By the way, the result in your book showing that the ""geometric Renyi MI"" is H_0 for pure states really makes me think that it should be related to TN approximations for mixed states, but we were not able to make this precise.', ""@lukyluke_t Thanks a lot Luca, I definitely missed these! As I understand it, in CFT the analytic continuation to n-&gt;1 means you can always calculate the vN entropy right? I'm still baffled that this works so well.""]",https://arxiv.org/abs/2103.01709,"The mutual information is a measure of classical and quantum correlations of great interest in quantum information. It is also relevant in quantum many-body physics, by virtue of satisfying an area law for thermal states and bounding all correlation functions. However, calculating it exactly or approximately is often challenging in practice. Here, we consider alternative definitions based on R\'enyi divergences. Their main advantage over their von Neumann counterpart is that they can be expressed as a variational problem whose cost function can be efficiently evaluated for families of states like matrix product operators while preserving all desirable properties of a measure of correlations. In particular, we show that they obey a thermal area law in great generality, and that they upper bound all correlation functions. We also investigate their behavior on certain tensor network states and on classical thermal distributions. ",Computable R\'enyi mutual information: Area laws and correlations
257,1366741209440518147,1071640880,Petar Veliƒçkoviƒá,"[""Introducing Persistent Message Passing (PMP)!\n\nWe endow GNNs üï∏Ô∏è with an explicit, persistent, memory üíæ of their past computations. I've left several thoughts on why I find this exciting in a thread below! üßµ\n\nw/ Heiko, @BarekatainAmin &amp; @BlundellCharles\n\n<LINK> <LINK>"", 'The current SOTA in algo reasoning (eg PGN, IterGNN, Shuffle-Exchange) can simulate iterative data-structure backed DP with O(1) aux. storage only (unrealistic!).\n\nPMP enables us to align with persistent data structures, finally enabling explicit memory usage in neural executors. https://t.co/Y78GH2rrnl', 'Generally, whenever GNNs are applied on dynamically changing data, their latents are put under pressure to reason about _any_ prior state of the data. As prior work (eg. by @emaros96) showed, explicit external memory can be crucial to making GNNs overcome this pressure...', '...and within PMP, we make a very simple assumption: our memory is always, immutably, storing a previous computation result of the GNN. \n\nThis gives GNNs an _episodic memory_ of their past computation, and it can alleviate the oversmoothing effect, as no data is ever overwritten!']",https://arxiv.org/abs/2103.01043,"Graph neural networks (GNNs) are a powerful inductive bias for modelling algorithmic reasoning procedures and data structures. Their prowess was mainly demonstrated on tasks featuring Markovian dynamics, where querying any associated data structure depends only on its latest state. For many tasks of interest, however, it may be highly beneficial to support efficient data structure queries dependent on previous states. This requires tracking the data structure's evolution through time, placing significant pressure on the GNN's latent representations. We introduce Persistent Message Passing (PMP), a mechanism which endows GNNs with capability of querying past state by explicitly persisting it: rather than overwriting node representations, it creates new nodes whenever required. PMP generalises out-of-distribution to more than 2x larger test inputs on dynamic temporal range queries, significantly outperforming GNNs which overwrite states. ",Persistent Message Passing
258,1366706548752400384,1682932567,Fabian R. Lux,"['New paper on ArXiv! We study the observable algebra of multi-q magnetization textures and find it determined by a noncommutative torus. The 2D quantum topological Hall effect is thus a manifestation of effective 4D physics with two ""curled up dimensions""\n\n<LINK>']",https://arxiv.org/abs/2103.01047,"The nontrivial topology of spin systems such as skyrmions in real space can promote complex electronic states. Here, we provide a general viewpoint at the emergence of topological electronic states in spin systems based on the methods of noncommutative K-theory. By realizing that the structure of the observable algebra of spin textures is determined by the algebraic properties of the noncommutative hypertorus, we arrive at a unified understanding of topological electronic states which we predict to arise in various noncollinear setups. The power of our approach lies in an ability to categorize emergent topological states algebraically without referring to smooth real- or reciprocal-space quantities. This opens a way towards an educated design of topological phases in aperiodic, disordered, or non-smooth textures of spins and charges containing topological defects. ","Unified topological characterization of electronic states in spin
  textures from noncommutative K-theory"
259,1366677717689774082,3430068083,Giancarlo Gatti,"['Our new paper in #arxiv today: ""Random Access codes via quantum contextual redundancy"", with\nD. Huerga, @KikeSolanoPhys &amp; @qmisanz \n<LINK>. We propose a protocol to compress classical information in quantum systems, leveraging redundancy in quantum contexts (1/2) <LINK>', 'This protocol shows quantum advantage over state-of-the-art information storage capacity (a cloud server of 1 billion users with 100GB each) for more than 44 qubits. Also, just for fun, systems above 100 qubits would be sufficient to encode a brute force solution for chess. (2/2)']",https://arxiv.org/abs/2103.01204,"We propose a protocol to encode classical bits in the measurement statistics of a set of parity observables, leveraging quantum contextual relations for a random access code task. The intrinsic information redundancy of quantum contexts allows for a posterior decoding protocol that requires few samples when encoding the information in a set of highly entangled states, which can be generated by a discretely-parametrized quantum circuit. Applications of this protocol include algorithms involving storage of large amounts of data but requiring only partial retrieval of the information, as is the case of decision trees. This classical-to-quantum encoding is a compression protocol for more than $18$ qubits and shows quantum advantage over state-of-the-art information storage capacity for more than $44$ qubits. In particular, systems above $100$ qubits would be sufficient to encode a brute force solution for games of chess-like complexity. ",Random access codes via quantum contextual redundancy
260,1366661893776097287,3236251346,Mikel Sanz,"['New paper in #arxiv today: ‚ÄúRandom Access codes via quantum contextual redundancy‚Äù with @Gatgian D. Huerga and @KikeSolanoPhys <LINK> we propose a coding-storage-decoding protocol which exploits information redundancy of quantum contexts. (1/2) <LINK>', 'This encoding shows quantum advantage over state-of-the-art information storage capacities for more than 44 qubits. Just for fun, systems above 100 qubits would be sufficient to encode a brute-force solution for games of chess-like complexity @upvehu @Ikerbasque @meetIQM (2/2)']",https://arxiv.org/abs/2103.01204,"We propose a protocol to encode classical bits in the measurement statistics of a set of parity observables, leveraging quantum contextual relations for a random access code task. The intrinsic information redundancy of quantum contexts allows for a posterior decoding protocol that requires few samples when encoding the information in a set of highly entangled states, which can be generated by a discretely-parametrized quantum circuit. Applications of this protocol include algorithms involving storage of large amounts of data but requiring only partial retrieval of the information, as is the case of decision trees. This classical-to-quantum encoding is a compression protocol for more than $18$ qubits and shows quantum advantage over state-of-the-art information storage capacity for more than $44$ qubits. In particular, systems above $100$ qubits would be sufficient to encode a brute force solution for games of chess-like complexity. ",Random access codes via quantum contextual redundancy
261,1376798959793008641,1194794814690086912,Andrea Skolik,"['Can we teach a quantum computer to balance a pole? Find out which architectural choices are crucial to making quantum agents succeed at deep Q-learning in our new paper: <LINK> <LINK>', 'We specifically show how the choice of observables will make or break Q-learning with PQCs, and how to make an informed choice by what we know about the optimal Q-values.']",https://arxiv.org/abs/2103.15084,"Quantum machine learning (QML) has been identified as one of the key fields that could reap advantages from near-term quantum devices, next to optimization and quantum chemistry. Research in this area has focused primarily on variational quantum algorithms (VQAs), and several proposals to enhance supervised, unsupervised and reinforcement learning (RL) algorithms with VQAs have been put forward. Out of the three, RL is the least studied and it is still an open question whether VQAs can be competitive with state-of-the-art classical algorithms based on neural networks (NNs) even on simple benchmark tasks. In this work, we introduce a training method for parametrized quantum circuits (PQCs) that can be used to solve RL tasks for discrete and continuous state spaces based on the deep Q-learning algorithm. We investigate which architectural choices for quantum Q-learning agents are most important for successfully solving certain types of environments by performing ablation studies for a number of different data encoding and readout strategies. We provide insight into why the performance of a VQA-based Q-learning algorithm crucially depends on the observables of the quantum model and show how to choose suitable observables based on the learning task at hand. To compare our model against the classical DQN algorithm, we perform an extensive hyperparameter search of PQCs and NNs with varying numbers of parameters. We confirm that similar to results in classical literature, the architectural choices and hyperparameters contribute more to the agents' success in a RL setting than the number of parameters used in the model. Finally, we show when recent separation results between classical and quantum agents for policy gradient RL can be extended to inferring optimal Q-values in restricted families of environments. ","Quantum agents in the Gym: a variational quantum algorithm for deep
  Q-learning"
262,1376434656330014725,268337552,Nicolas Kourtellis,"['With @yelenamejova, we study the increase in cross-platform posting activity of Twitter+YouTube during 1st #COVID19 lockdown across 100+ countries: a proxy for users following restrictions in mobility!  <LINK>\nPowered by @TEFresearch, @concordiah2020 \n#Mobility <LINK>', 'We are also releasing a bunch of data and results, so check out the paper for your own follow-up studies!\n#data #reproducibleresearch #transparency']",https://arxiv.org/abs/2103.14601,"Compliance with public health measures, such as restrictions on movement and socialization, is paramount in limiting the spread of diseases such as the severe acute respiratory syndrome coronavirus 2 (also referred to as COVID-19). Although large population datasets, such as phone-based mobility data, may provide some glimpse into such compliance, it is often proprietary, and may not be available for all locales. In this work, we examine the usefulness of video sharing on social media as a proxy of the amount of time Internet users spend at home. In particular, we focus on the number of people sharing YouTube videos on Twitter before and during COVID-19 lockdown measures were imposed by 109 countries. We find that the media sharing behavior differs widely between countries, in some having immediate response to the lockdown decrees - mostly by increasing the sharing volume dramatically - while in others having a substantial lag. We confirm that these insights correlate strongly with mobility, as measured using phone data. Finally, we illustrate that both media sharing and mobility behaviors change more drastically around mandated lockdowns, and less so around more lax recommendations. We make the media sharing volume data available to the research community for continued monitoring of behavior change around public health measures. ","YouTubing at Home: Media Sharing Behavior Change as Proxy for
  MobilityAround COVID-19 Lockdowns"
263,1374956889620967426,72248158,Ponnurangam Kumaraguru ‚ÄúPK‚Äù,"['Is #kooapp, the new social network, India‚Äôs king?\nWith 4M users &amp; 163M follower relations, we find out!\nLinguistic communities; #Hindi #Bengaluru prominent; \nVideo: <LINK> \nFull report: <LINK> \n\\c @aprameya @mayankbidawatka @rsprasad @kooindia', 'Short Blog: https://t.co/kdTvNaWKj4  \nüëå job by @asmitks @chiragj_ @jiviteshjn @rishi_raj_jain_ @shradhasgl \nWe were able to collect 4mn users of 4.7Million; we also have 163 million follower-following relationships / edges in our data. https://t.co/xVkJBjcPCG', 'Active users @AnupamPKher @smritiirani\n@Swamy39  @PiyushGoyal @DrKumarVishwas @RajatSharmaLive @ChouhanShivraj @sudhirchaudhary @RailMinIndia @ZeeNewsEnglish @ANI @republic @KanganaTeam @vikramchandra @PrakashJavdekar https://t.co/FabFEAcedL', 'We found Hindi handle having the highest number of followers, 1.8 million. Number of users using Hindi as language on Koo is the highest with 1.7 Million users accounting to 3.7 Million posts. Table gives distribution of other languages with users and posts. https://t.co/EIvPZv618g', 'Account creation and Posts spike after Feb 2021. https://t.co/wYWnYPB0QJ', 'Females reported handles have higher average number of followers compared to male and others category. We also found females reported handles have more average number of likes. More number of males report single and more number of females report married. https://t.co/pvxKHB6fQ5', 'Popular hashtags, unigrams, bi-grams. https://t.co/E7X6ZSoUqA', 'Cluster of users based on the language preference, i.e. user with Kannada as a language following other Kannada users. \nFull report again: https://t.co/XSFEQJjz2t \nWant to analyze #kooapp further? Dataset for research: https://t.co/c8294bIUZG https://t.co/I3yOvirjUI']",https://arxiv.org/abs/2103.13239,"Social media has grown exponentially in a short period, coming to the forefront of communications and online interactions. Despite their rapid growth, social media platforms have been unable to scale to different languages globally and remain inaccessible to many. In this paper, we characterize Koo, a multilingual micro-blogging site that rose in popularity in 2021, as an Indian alternative to Twitter. We collected a dataset of 4.07 million users, 163.12 million follower-following relationships, and their content and activity across 12 languages. We study the user demographic along the lines of language, location, gender, and profession. The prominent presence of Indian languages in the discourse on Koo indicates the platform's success in promoting regional languages. We observe Koo's follower-following network to be much denser than Twitter's, comprising of closely-knit linguistic communities. An N-gram analysis of posts on Koo shows a #KooVsTwitter rhetoric, revealing the debate comparing the two platforms. Our characterization highlights the dynamics of the multilingual social network and its diverse Indian user base. ","What's Kooking? Characterizing India's Emerging Social Network, Koo"
264,1374304366727954432,1111231911677083653,Stephan Fahrenkrog-Petersen,['Ensuring #Privacy for Process Performance Indicators? No problem! We studied how to achieve this goal thanks to differential privacy:\n<LINK>\njoint work w/ @_martinbauer_ @mweidlich\n accepted for @CAiSEConf 2021'],https://arxiv.org/abs/2103.11740,"Process performance indicators (PPIs) are metrics to quantify the degree with which organizational goals defined based on business processes are fulfilled. They exploit the event logs recorded by information systems during the execution of business processes, thereby providing a basis for process monitoring and subsequent optimization. However, PPIs are often evaluated on processes that involve individuals, which implies an inevitable risk of privacy intrusion. In this paper, we address the demand for privacy protection in the computation of PPIs. We first present a framework that enforces control over the data exploited for process monitoring. We then show how PPIs defined based on the established PPINOT meta-model are instantiated in this framework through a set of data release mechanisms. These mechanisms are designed to provide provable guarantees in terms of differential privacy. We evaluate our framework and the release mechanisms in a series of controlled experiments. We further use a public event log to compare our framework with approaches based on privatization of event logs. The results demonstrate feasibility and shed light on the trade-offs between data utility and privacy guarantees in the computation of PPIs. ","Privacy-aware Process Performance Indicators: Framework and Release
  Mechanisms"
265,1372882580354297859,900725658409631745,Roberto Verdecchia,"['Technical debt and antipatterns in AI-based systems, what do they look like?\nWe (@JREB1990, yt, and @IliasGerostatho) answer this question in our systematic mapping study accepted @TechDebtConf.\nPreprint: <LINK> üßµ(1/n) <LINK>', 'TL;DR: We identified 4 new debt types: Data, Model, Configuration, and Ethics debt. Infrastructure, Architectural, Code, and Testing debt most recurrent variations of established types.\nFunctional Suitability, Maintainability, and Observability  most impacted QAs. (2/n) https://t.co/GpnrEIMdTw', '72 unique antipatterns in 6 categories. Most belonging to the categories Model, Data, and Design &amp; Architecture.\n46 unique solutions, most referenced ones being ""manage model configurations"" and ""use clear component and code APIs"" (3/n)', 'Conclusion: technical debt and antipatterns manifest with substantial differences in AI-based systems as compared to ""traditional"" systems. More research needed! (4/4)']",https://arxiv.org/abs/2103.09783,"Background: With the rising popularity of Artificial Intelligence (AI), there is a growing need to build large and complex AI-based systems in a cost-effective and manageable way. Like with traditional software, Technical Debt (TD) will emerge naturally over time in these systems, therefore leading to challenges and risks if not managed appropriately. The influence of data science and the stochastic nature of AI-based systems may also lead to new types of TD or antipatterns, which are not yet fully understood by researchers and practitioners. Objective: The goal of our study is to provide a clear overview and characterization of the types of TD (both established and new ones) that appear in AI-based systems, as well as the antipatterns and related solutions that have been proposed. Method: Following the process of a systematic mapping study, 21 primary studies are identified and analyzed. Results: Our results show that (i) established TD types, variations of them, and four new TD types (data, model, configuration, and ethics debt) are present in AI-based systems, (ii) 72 antipatterns are discussed in the literature, the majority related to data and model deficiencies, and (iii) 46 solutions have been proposed, either to address specific TD types, antipatterns, or TD in general. Conclusions: Our results can support AI professionals with reasoning about and communicating aspects of TD present in their systems. Additionally, they can serve as a foundation for future research to further our understanding of TD in AI-based systems. ","Characterizing Technical Debt and Antipatterns in AI-Based Systems: A
  Systematic Mapping Study"
266,1369622087342592005,802570857474203648,Vibhor Agarwal,"['<LINK> We present first ever study on #differential #tracking across topical #webpages of Indian #news #media, with amazing team @vekariayash,@pk_plus_plus,@sangeetamptra,@sakthibalanm,@nishanthsastry,@kourtellis. <LINK>', '(1/3) Topical subpages of Indian News Websites are tracked more than the homepages.\n\n#tracking #privacy #indiannews #websites', '(2/3) Third-parties show preferential attachment towards specific topic pages to track users with specific interests.\nEg: Music websites tracking only Entertainment topic pages. Heat map shows the interesting trackers. https://t.co/GAY0oQ9niT', '(3/3) Surprisingly, privacy policy pages are in English only even for news websites in Hindi, or in other regional languages. This shows that #privacy #policy pages are mostly acting as placeholders. Moreover, 8 websites do not have privacy policy page at all.']",https://arxiv.org/abs/2103.04442,"Online user privacy and tracking have been extensively studied in recent years, especially due to privacy and personal data-related legislations in the EU and the USA, such as the General Data Protection Regulation, ePrivacy Regulation, and California Consumer Privacy Act. Research has revealed novel tracking and personal identifiable information leakage methods that first- and third-parties employ on websites around the world, as well as the intensity of tracking performed on such websites. However, for the sake of scaling to cover a large portion of the Web, most past studies focused on homepages of websites, and did not look deeper into the tracking practices on their topical subpages. The majority of studies focused on the Global North markets such as the EU and the USA. Large markets such as India, which covers 20% of the world population and has no explicit privacy laws, have not been studied in this regard. We aim to address these gaps and focus on the following research questions: Is tracking on topical subpages of Indian news websites different from their homepage? Do third-party trackers prefer to track specific topics? How does this preference compare to the similarity of content shown on these topical subpages? To answer these questions, we propose a novel method for automatic extraction and categorization of Indian news topical subpages based on the details in their URLs. We study the identified topical subpages and compare them with their homepages with respect to the intensity of cookie injection and third-party embeddedness and type. We find differential user tracking among subpages, and between subpages and homepages. We also find a preferential attachment of third-party trackers to specific topics. Also, embedded third-parties tend to track specific subpages simultaneously, revealing possible user profiling in action. ",Differential Tracking Across Topical Webpages of Indian News Media
267,1369257849268895748,1324092690569535488,Peter Benner,"['In a new preprint, we propose a new deep learning architecture for inferring dynamical process models from data.\n\n<LINK>']",https://arxiv.org/abs/2103.02249,"Mathematical modeling is an essential step, for example, to analyze the transient behavior of a dynamical process and to perform engineering studies such as optimization and control. With the help of first-principles and expert knowledge, a dynamic model can be built, but for complex dynamic processes, appearing, e.g., in biology, chemical plants, neuroscience, financial markets, this often remains an onerous task. Hence, data-driven modeling of the dynamics process becomes an attractive choice and is supported by the rapid advancement in sensor and measurement technology. A data-driven approach, namely operator inference framework, models a dynamic process, where a particular structure of the nonlinear term is assumed. In this work, we suggest combining the operator inference with certain deep neural network approaches to infer the unknown nonlinear dynamics of the system. The approach uses recent advancements in deep learning and possible prior knowledge of the process if possible. We also briefly discuss several extensions and advantages of the proposed methodology. We demonstrate that the proposed methodology accomplishes the desired tasks for dynamics processes encountered in neural dynamics and the glycolytic oscillator. ","LQResNet: A Deep Neural Network Architecture for Learning Dynamic
  Processes"
268,1368962213621415936,68538286,Dan Hendrycks,"['To find the limits of Transformers, we collected 12,500 math problems. While a three-time IMO gold medalist got 90%, GPT-3 models got ~5%, with accuracy increasing slowly.\n\nIf trends continue, ML models are far from achieving mathematical reasoning.\n\n<LINK> <LINK>', '@florian_tramer Competition mathematics problems sometimes have jargon that isn\'t in many K-12 mathematics classes, such as ""triangle orthocenter"" or ""units digit."" Fortunately the training set has 7,500 problems and solutions, which should provide enough background for strong models.']",https://arxiv.org/abs/2103.03874,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community. ",Measuring Mathematical Problem Solving With the MATH Dataset
269,1368852243747901442,1156374257523462150,Elisa Garro,"[""From today there is a new globular cluster, located in the Galactic bulge: #Patchick99. We use the near-IR #VVV and optical #GaiaDR2 datasets. To find out more, read the paper, accepted by A&amp;A and published today on #arXiv: <LINK>\nIt's a great #WomensDay! <LINK>""]",https://arxiv.org/abs/2103.03592,"Globular clusters (GCs) are important tools to understand the formation and evolution of the Milky Way (MW). The known MW sample is still incomplete, so the discovery of new GC candidates and the confirmation of their nature are crucial for the census of the MW GC system. Our goal is to confirm the physical nature of two GC candidates: Patchick99 and TBJ3, located towards the Galactic bulge. We use public data in the near-IR from the VVV, VVVX and 2MASS along the with deep optical data from the Gaia DR2, in order to estimate their main physical parameters: reddening, extinction, distance, luminosity, mean cluster proper motions (PMs), size, metallicity and age. We investigate both candidates at different wavelengths. We use near-IR and optical CMDs in order to analyse Patchick99. We decontaminate CMDs following a statistical procedure and PM-selection. Reddening and extinction are derived by adopting reddening maps. Metallicity and age are evaluated by fitting stellar isochrones. Reddening and extinction are E(J-Ks)=0.12+/-0.02 mag, AKs=0.09+/-0.01 mag from the VVV data, whereas E(BP-RP)=0.21+/-0.03 mag, AG=0.68+/-0.08 mag from Gaia DR2. We estimate a distance d=6.4+/-0.2 kpc in near-IR and D=7.0+/-0.2 kpc in optical. We derive its metallicity and age fitting PARSEC isochrones, finding [Fe/H]=-0.2+/-0.2 dex and t=10+/-2 Gyr. The mean PMs for Patchick99 are pmRA=-298+/-1.74 mas/yr and pmDEC=-5.49+/-2.02 mas/yr. We confirm that it is a low-luminosity GC, with MKs=-7.0+/-0.6 mag. The radius estimation is performed building the radial density profile, finding r~10'. We recognise 7 RR Lyrae star members within 8.2 arcmin from its centre, confirming the distance found by other methods. We found that TBJ3 shows mid-IR emissions that are not present in GCs. We discard TBJ3 as GC candidate and we focus on Patchick99. We conclude that it is an old metal-rich GC, situated in the Galactic bulge. ","Confirmation and physical characterization of the new bulge globular
  cluster Patchick 99 from the VVV and Gaia surveys"
270,1366945466219388928,1012125662117851136,Edward Kennedy,"[""Cool new paper by @leqi_liu!\n\nMean optimal trt rules aren't robust: sensitive to small % w/ extreme outcomes\n\nMarginal median opt rules are unfair in different way: my treatment depends on *your* outcomes\n\nWe study fair+robust median optimal trt rules:\n\n<LINK> <LINK>""]",https://arxiv.org/abs/2103.01802,"Optimal treatment regimes are personalized policies for making a treatment decision based on subject characteristics, with the policy chosen to maximize some value. It is common to aim to maximize the mean outcome in the population, via a regime assigning treatment only to those whose mean outcome is higher under treatment versus control. However, the mean can be an unstable measure of centrality, resulting in imprecise statistical procedures, as well as unrobust decisions that can be overly influenced by a small fraction of subjects. In this work, we propose a new median optimal treatment regime that instead treats individuals whose conditional median is higher under treatment. This ensures that optimal decisions for individuals from the same group are not overly influenced either by (i) a small fraction of the group (unlike the mean criterion), or (ii) unrelated subjects from different groups (unlike marginal median/quantile criteria). We introduce a new measure of value, the Average Conditional Median Effect (ACME), which summarizes across-group median treatment outcomes of a policy, and which the median optimal treatment regime maximizes. After developing key motivating examples that distinguish median optimal treatment regimes from mean and marginal median optimal treatment regimes, we give a nonparametric efficiency bound for estimating the ACME of a policy, and propose a new doubly robust-style estimator that achieves the efficiency bound under weak conditions. To construct the median optimal treatment regime, we introduce a new doubly robust-style estimator for the conditional median treatment effect. Finite-sample properties are explored via numerical simulations and the proposed algorithm is illustrated using data from a randomized clinical trial in patients with HIV. ",Median Optimal Treatment Regimes
