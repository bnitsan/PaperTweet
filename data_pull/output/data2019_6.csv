,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1148352637689049090,48712353,Sungjin Ahn üá∫üá¶,"['Glad to introduce our new paper on ""Sequential Neural Processes"" by Gautam Singh, Jaesik Yoon et. al! This is a meta-transfer learning framework. We demonstrate that it can model dynamic 3d scenes using temporal GQN!\n<LINK>\n<LINK> <LINK>', '3D multi-objects https://t.co/crpXDd633l', '2d moving object https://t.co/UchwSY6RAo', '1d regression https://t.co/H5jyjFsMzR']",https://arxiv.org/abs/1906.10264,"Neural Processes combine the strengths of neural networks and Gaussian processes to achieve both flexible learning and fast prediction in stochastic processes. However, a large class of problems comprises underlying temporal dependency structures in a sequence of stochastic processes that Neural Processes (NP) do not explicitly consider. In this paper, we propose Sequential Neural Processes (SNP) which incorporates a temporal state-transition model of stochastic processes and thus extends its modeling capabilities to dynamic stochastic processes. In applying SNP to dynamic 3D scene modeling, we introduce the Temporal Generative Query Networks. To our knowledge, this is the first 4D model that can deal with the temporal dynamics of 3D scenes. In experiments, we evaluate the proposed methods in dynamic (non-stationary) regression and 4D scene inference and rendering. ",Sequential Neural Processes
1,1148151057442361344,92096034,Max Welling,"['check out our new arxiv paper on combining generative message passing with graphNN message passing:  <LINK> (Victor Garcia, Zeynep Akata &amp; M.W.)']",https://arxiv.org/abs/1906.02547,"A graphical model is a structured representation of the data generating process. The traditional method to reason over random variables is to perform inference in this graphical model. However, in many cases the generating process is only a poor approximation of the much more complex true data generating process, leading to suboptimal estimation. The subtleties of the generative process are however captured in the data itself and we can `learn to infer', that is, learn a direct mapping from observations to explanatory latent variables. In this work we propose a hybrid model that combines graphical inference with a learned inverse model, which we structure as in a graph neural network, while the iterative algorithm as a whole is formulated as a recurrent neural network. By using cross-validation we can automatically balance the amount of work performed by graphical inference versus learned inference. We apply our ideas to the Kalman filter, a Gaussian hidden Markov model for time sequences, and show, among other things, that our model can estimate the trajectory of a noisy chaotic Lorenz Attractor much more accurately than either the learned or graphical inference run in isolation. ",Combining Generative and Discriminative Models for Hybrid Inference
2,1147142514165596160,880722356288802817,Sibylle Hess,"['My second paper is ""C-SALT: Mining Class-Specific ALTerations in Boolean Matrix Factorization"". We explore a new model of Boolean matrix factorizations where a clustering and a discriminative model of a labelled, binary dataset  is simultaneously optimized.<LINK> <LINK>', 'The starting point was the question which genomic characteristics unify a group of patients of a specific tumor type, and which genomic characteristics discriminate samples from a normal, a tumor and a relapse cell. We found that state-of-the-art model assumptions did not apply.', 'We explored a novel, more complex factorization structure where every bi-cluster has an additional feature cluster for every class. Therewith, we discriminate the samples from one cluster into the classes. The PAL-Tiling framework from our first paper is used for optimization.', 'As a result, we proposed the first method which enabled to identify clusters together with discriminating components, and this in the most flexible way. C-Salt does not need any specifications of the number of clusters and/or discriminating components, opposed to former methods.']",https://arxiv.org/abs/1906.09907,"Given labeled data represented by a binary matrix, we consider the task to derive a Boolean matrix factorization which identifies commonalities and specifications among the classes. While existing works focus on rank-one factorizations which are either specific or common to the classes, we derive class-specific alterations from common factorizations as well. Therewith, we broaden the applicability of our new method to datasets whose class-dependencies have a more complex structure. On the basis of synthetic and real-world datasets, we show on the one hand that our method is able to filter structure which corresponds to our model assumption, and on the other hand that our model assumption is justified in real-world application. Our method is parameter-free. ","C-SALT: Mining Class-Specific ALTerations in Boolean Matrix
  Factorization"
3,1146483066325610497,1003664537298591744,Kin Quan,"['Proud to announce a new preprint!\n\n‚ÄúModelling Airway Geometry as Stock Market Data using Bayesian Changepoint Detection‚Äù with @RyutaroT92, M. Duong, @rebeccajshipley, @ProfHurst, D. Hawkes, J. Jacobs. \n\nPaper: <LINK> \n\n@CmicUcl @UCLmedphys @uclcs @Health_Eng <LINK>']",https://arxiv.org/abs/1906.12225,"Numerous lung diseases, such as idiopathic pulmonary fibrosis (IPF), exhibit dilation of the airways. Accurate measurement of dilatation enables assessment of the progression of disease. Unfortunately the combination of image noise and airway bifurcations causes high variability in the profiles of cross-sectional areas, rendering the identification of affected regions very difficult. Here we introduce a noise-robust method for automatically detecting the location of progressive airway dilatation given two profiles of the same airway acquired at different time points. We propose a probabilistic model of abrupt relative variations between profiles and perform inference via Reversible Jump Markov Chain Monte Carlo sampling. We demonstrate the efficacy of the proposed method on two datasets; (i) images of healthy airways with simulated dilatation; (ii) pairs of real images of IPF-affected airways acquired at 1 year intervals. Our model is able to detect the starting location of airway dilatation with an accuracy of 2.5mm on simulated data. The experiments on the IPF dataset display reasonable agreement with radiologists. We can compute a relative change in airway volume that may be useful for quantifying IPF disease progression. The code is available at this https URL ","Modelling Airway Geometry as Stock Market Data using Bayesian
  Changepoint Detection"
4,1145739984659308544,767345894,Xiang Ren,"['Will users\' in-app activity patterns inform their retention on the social app? Our case study on @Snapchat user activity log shows that user\'s ""action graph"" gives new clues on their engagement. #kdd19 paper: <LINK>, Code: <LINK> @kdd_news <LINK>']",https://arxiv.org/abs/1906.00355,"While mobile social apps have become increasingly important in people's daily life, we have limited understanding on what motivates users to engage with these apps. In this paper, we answer the question whether users' in-app activity patterns help inform their future app engagement (e.g., active days in a future time window)? Previous studies on predicting user app engagement mainly focus on various macroscopic features (e.g., time-series of activity frequency), while ignoring fine-grained inter-dependencies between different in-app actions at the microscopic level. Here we propose to formalize individual user's in-app action transition patterns as a temporally evolving action graph, and analyze its characteristics in terms of informing future user engagement. Our analysis suggested that action graphs are able to characterize user behavior patterns and inform future engagement. We derive a number of high-order graph features to capture in-app usage patterns and construct interpretable models for predicting trends of engagement changes and active rates. To further enhance predictive power, we design an end-to-end, multi-channel neural model to encode temporal action graphs, activity sequences, and other macroscopic features. Experiments on predicting user engagement for 150k Snapchat new users over a 28-day period demonstrate the effectiveness of the proposed models. The prediction framework is deployed at Snapchat to deliver real world business insights. Our proposed framework is also general and can be applied to other social app platforms. ","Characterizing and Forecasting User Engagement with In-app Action Graph:
  A Case Study of Snapchat"
5,1145611389593149440,870078805381132288,Achim Zeileis,"['Lisa\'s new working paper: ""The Power of Unbiased Recursive Partitioning: A Unifying View of CTree, MOB, and GUIDE"". The testing strategies are embedded in a common framework and compared in simulation experiments.\n\nPaper: <LINK>\nBlog: <LINK> <LINK>', ""@Jacobvanetten CTree and MOB are more similar than we thought ourselves. GUIDE just needs two tweaks and is then quite similar as well.\n\nHopefully, we'll be able to make the partykit extension ready for CRAN so that all three testing strategies are available inside partykit::mob."", ""@Jacobvanetten Definitely - just too little time. The idea is to first completely unify the tree infrastructure so that we then get a wide variety of forests on top of it. Let's see how the summer goes...""]",https://arxiv.org/abs/1906.10179,"A core step of every algorithm for learning regression trees is the selection of the best splitting variable from the available covariates and the corresponding split point. Early tree algorithms (e.g., AID, CART) employed greedy search strategies, directly comparing all possible split points in all available covariates. However, subsequent research showed that this is biased towards selecting covariates with more potential split points. Therefore, unbiased recursive partitioning algorithms have been suggested (e.g., QUEST, GUIDE, CTree, MOB) that first select the covariate based on statistical inference using p-values that are adjusted for the possible split points. In a second step a split point optimizing some objective function is selected in the chosen split variable. However, different unbiased tree algorithms obtain these p-values from different inference frameworks and their relative advantages or disadvantages are not well understood, yet. Therefore, three different popular approaches are considered here: classical categorical association tests (as in GUIDE), conditional inference (as in CTree), and parameter instability tests (as in MOB). First, these are embedded into a common inference framework encompassing parametric model trees, in particular linear model trees. Second, it is assessed how different building blocks from this common framework affect the power of the algorithms to select the appropriate covariates for splitting: observation-wise goodness-of-fit measure (residuals vs. model scores), dichotomization of residuals/scores at zero, and binning of possible split variables. This shows that specifically the goodness-of-fit measure is crucial for the power of the procedures, with model scores without dichotomization performing much better in many scenarios. ","The Power of Unbiased Recursive Partitioning: A Unifying View of CTree,
  MOB, and GUIDE"
6,1145518012201615360,1047899041311412224,Francois Grondin,"['I just released a new paper that shows how SVD-PHAT can be used to perform multiple sound source localization, with more accuracy than the vanilla SRP-PHAT, but with the low-complexity of SVD-PHAT: <LINK>', ""@thaytan During the last month I also participated to the DCASE 2019 sound event detection and localization challenge :P We didn't win, but it was interesting to see how CRNN would perform for detection/localization tasks. Here's the tech report: https://t.co/hrcY0EAmLt""]",https://arxiv.org/abs/1906.11913,"This paper introduces a modification of phase transform on singular value decomposition (SVD-PHAT) to localize multiple sound sources. This work aims to improve localization accuracy and keeps the algorithm complexity low for real-time applications. This method relies on multiple scans of the search space, with projection of each low-dimensional observation onto orthogonal subspaces. We show that this method localizes multiple sound sources more accurately than discrete SRP-PHAT, with a reduction in the Root Mean Square Error up to 0.0395 radians. ",Multiple Sound Source Localization with SVD-PHAT
7,1145035627001503745,633176250,Dr. Larry R. Nittler üöÄüéπüåñ‚òÑÔ∏è,['new paper!\n<LINK>'],https://arxiv.org/abs/1906.10776,"We report Mo isotopic compositions of 37 presolar SiC grains of types Y (19) and Z (18), rare types commonly argued to have formed in lower-than-solar metallicity asymptotic giant branch (AGB) stars. Direct comparison of the Y and Z grain data with data for mainstream grains from AGB stars of close-to-solar metallicity demonstrates that the three types of grains have indistinguishable Mo isotopic compositions. We show that the Mo isotope data can be used to constrain the maximum stellar temperatures (TMAX) during thermal pulses in AGB stars. Comparison of FRUITY Torino AGB nucleosynthesis model calculations with the grain data for Mo isotopes points to an origin from low-mass (~1.5-3 Msun) rather than intermediate-mass (>3-~9 Msun) AGB stars. Because of the low efficiency of 22Ne({\alpha},n)25Mg at the low TMAX values attained in low-mass AGB stars, model calculations cannot explain the large 30Si excesses of Z grains as arising from neutron capture, so these excesses remain a puzzle at the moment. ","Presolar Silicon Carbide Grains of Types Y and Z: Their Molybdenum
  Isotopic Compositions and Stellar Origins"
8,1145026397464907776,1134563269262360577,Yang Zheng,"['We posted a new paper on distributed control: <LINK>. Our idea is very simple: what kind of sparsity patterns do we require for matrices Y and X, such that we always have the desired pattern for their product YX^{-1}? We introduce a notion of sparsity invariance.']",https://arxiv.org/abs/1906.06777,"We address the problem of designing optimal linear time-invariant (LTI) sparse controllers for LTI systems, which corresponds to minimizing a norm of the closed-loop system subject to sparsity constraints on the controller structure. This problem is NP-hard in general and motivates the development of tractable approximations. We characterize a class of convex restrictions based on a new notion of Sparsity Invariance (SI). The underlying idea of SI is to design sparsity patterns for transfer matrices Y(s) and X(s) such that any corresponding controller K(s)=Y(s)X(s)^-1 exhibits the desired sparsity pattern. For sparsity constraints, the approach of SI goes beyond the notion of Quadratic Invariance (QI): 1) the SI approach always yields a convex restriction; 2) the solution via the SI approach is guaranteed to be globally optimal when QI holds and performs at least as well as considering a nearest QI subset. Moreover, the notion of SI naturally applies to designing structured static controllers, while QI is not utilizable. Numerical examples show that even for non-QI cases, SI can recover solutions that are 1) globally optimal and 2) strictly more performing than previous methods. ",Sparsity Invariance for Convex Design of Distributed Controllers
9,1144628723506827265,28840722,Phil Long,"['New paper with Peter Bartlett, G√°bor Lugosi and Alex Tsigler called ""Benign Overfitting in Linear Regression"": <LINK>.']",https://arxiv.org/abs/1906.11300,"The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lies in an infinite dimensional space versus when the data lies in a finite dimensional space whose dimension grows faster than the sample size. ",Benign Overfitting in Linear Regression
10,1144512760719597568,557011202,Nicola Tomassetti,"['Our new paper in today\'s arXiv: ""Numerical modeling of #CosmicRays transport in the heliopshere and interpretation of the proton-to-helium ratio in #Solar Cycle 24"". <LINK>']",https://arxiv.org/abs/1906.11477,"Thanks to space-borne experiments of cosmic-ray (CR) detection, such as the AMS and PAMELA missions in low-Earth orbit, or the Voyager-1 spacecraft in the interstellar space, a large collection of multi-channel and time-resolved CR data has become available. Recently, the AMS experiment has released new precision data, on the proton and helium fluxes in CRs, measured on monthly basis during its first six years of mission. The AMS data reveal a remarkable long-term behavior in the temporal evolution of the proton-to-helium ratio at rigidity $R = p/Z <$ 3 GV. As we have argued in a recent work, such a behavior may reflect the transport properties of low-rigidity CRs in the inteplanetary space. In particular, it can be caused by mass/charge dependence of the CR diffusion coefficient. In this paper, we present our developments in the numerical modeling of CR transport in the Milky Way and in the heliosphere. Within our model, and with the help of approximated analytical solutions, we describe in details the relations between the properties of CR diffusion and the time-dependent evolution of the proton-to-helium ratio. ","Numerical modeling of cosmic-ray transport in the heliosphere and
  interpretation of the proton-to-helium ratio in Solar Cycle 24"
11,1144463671197724673,976155561522794497,Juliano C√©sar Silva Neves,['My new paper links the black hole shadow to quantum gravity! \n<LINK>'],https://arxiv.org/abs/1906.11735?fbclid=IwAR1g4nW2DcIHwAsjh1MtbjMxVbUtNw_1S8y4qDyUjFSAefpe0Upz97Jm6jw,"An upper bound on the parameter that provides a generalized uncertainty principle (GUP) is obtained from the black hole shadow. With the aid of a recent constraint between regular black holes and the GUP parameter, it is indicated a relation between this parameter and the deviation from circularity of the black hole shadow. In the case of the recent announcement of the M87* results from the Event Horizon Telescope collaboration, a deviation from circularity $\lesssim 10\%$ imposes a GUP parameter $\beta_0 <10^{90}$. ",Upper bound on the GUP parameter using the black hole shadow
12,1144432969752256513,2617459274,Moussa Reda Mansour,"['Check out our new paper ""Loss Switching Fusion with Similarity Search for Video Classification"" to appear in International Conference on Image Processing (IEEE ICIP\'19) #UWA #deep_learning #artificial_inteligence #computer_vision #ICIP <LINK>']",https://arxiv.org/abs/1906.11465,"From video streaming to security and surveillance applications, video data play an important role in our daily living today. However, managing a large amount of video data and retrieving the most useful information for the user remain a challenging task. In this paper, we propose a novel video classification system that would benefit the scene understanding task. We define our classification problem as classifying background and foreground motions using the same feature representation for outdoor scenes. This means that the feature representation needs to be robust enough and adaptable to different classification tasks. We propose a lightweight Loss Switching Fusion Network (LSFNet) for the fusion of spatiotemporal descriptors and a similarity search scheme with soft voting to boost the classification performance. The proposed system has a variety of potential applications such as content-based video clustering, video filtering, etc. Evaluation results on two private industry datasets show that our system is robust in both classifying different background motions and detecting human motions from these background motions. ",Loss Switching Fusion with Similarity Search for Video Classification
13,1144414987772993537,282700930,utku,"['Our new paper with @fpedregosa, @AidanNGomez and @erich_elsen on training sparse neural networks is on arXiv. We investigate the energy landscape of sparse ResNets in an attempt to understand the difficulty of training sparse neural networks from scratch.\n<LINK> <LINK>', 'Objective function is monotonically decreasing (NO barrier) along the straight lines from sparse initial points to the solution obtained by pruning, demonstrating that even when the optimization process fails, there was a monotonically decreasing path to the ‚Äúgood‚Äù solution. https://t.co/dhXpFchfbh', 'In contrast, the linear path between the scratch and the pruned solutions depicts a high energy barrier. Our attempts to find Bezier curves with small loss between the two sparse solutions fails; suggesting that the optimization process gets attracted into a‚Äúbad‚Äù local minima. https://t.co/Dh0SJME0Ll', 'Finally,  by  removing  the  sparsity  constraint  from  the path, we are able to find decreasing objective Bezier curves between the two sparse solutions; suggesting that allowing dense connectivity might be necessary and sufficient to escape the bad stationary points converged. https://t.co/iLchlLneN7', ""@roydanroy No, we didn't look at that. Though, we were able to reproduce Lottery at Scale (@jefrankle) results. We observed that the late-reset has a significantly lower loss than a random initial point. Not sure how we can find such good initial point without the dense training.""]",https://arxiv.org/abs/1906.10732,"We investigate the difficulties of training sparse neural networks and make new observations about optimization dynamics and the energy landscape within the sparse regime. Recent work of \citep{Gale2019, Liu2018} has shown that sparse ResNet-50 architectures trained on ImageNet-2012 dataset converge to solutions that are significantly worse than those found by pruning. We show that, despite the failure of optimizers, there is a linear path with a monotonically decreasing objective from the initialization to the ""good"" solution. Additionally, our attempts to find a decreasing objective path from ""bad"" solutions to the ""good"" ones in the sparse subspace fail. However, if we allow the path to traverse the dense subspace, then we consistently find a path between two solutions. These findings suggest traversing extra dimensions may be needed to escape stationary points found in the sparse subspace. ",The Difficulty of Training Sparse Neural Networks
14,1144412114414686208,2427184074,Christopher Berry,['New paper led by @NUCIERA grad student @spacedontwait on the mystery of the chemical composition of globular clusters\n\n<LINK> <LINK>'],https://arxiv.org/abs/1906.11299,"Star-to-star dispersion of r-process elements has been observed in a significant number of old, metal-poor globular clusters. We investigate early-time neutron-star mergers as the mechanism for this enrichment. Through both numerical modeling and analytical arguments, we show that neutron-star mergers cannot be induced through dynamical interactions early in the history of the cluster, even when the most liberal assumptions about neutron-star segregation are assumed. Therefore, if neutron-star mergers are the primary mechanism for r-process dispersion in globular clusters, they likely result from the evolution of isolated, primordial binaries in the clusters. Through population modeling, we find that moderate fractions of GCs with enrichment are only possible when a significant number of double neutron-star progenitors proceed through Case BB mass transfer --- under various assumptions for the initial properties of globular clusters, a neutron-star merger with the potential for enrichment will occur in ~15-60% (~30-90%) of globular clusters if this mass transfer proceeds stably (unstably). The strong anti-correlation between the pre-supernova orbital separation and post-supernova systemic velocity due to mass loss in the supernova leads to efficient ejection of most enrichment candidates from their host clusters. Thus, most enrichment events occur shortly after the double neutron stars are born. This requires star-forming gas that can absorb the r-process ejecta to be present in the globular cluster 30-50 Myr after the initial burst of star formation. If scenarios for redistributing gas in globular clusters cannot act on these timescales, the number of neutron-star merger enrichment candidates drops severely, and it is likely that another mechanism, such as r-process enrichment from collapsars, is at play. ","Can Neutron-Star Mergers Explain the r-process Enrichment in Globular
  Clusters?"
15,1144274561480953857,1545142616,Nir Mandelker,"[""New paper on the arXiv today, from the Yale-HITS-MPA group! <LINK>\n\nIt's my first ApJ Letter (actually, it's my first paper less than 20 pages long), and the first paper about my very own cosmological simulations, with the highest resolution in the IGM to date!"", ""We study the newly-coined intra-pancake-medium (IPM) ü•û\nThermal instabilities in Zel'dovich pancakes triggered during the growth of large scale structure can lead to Lyman Limit Systems deep in the IGM, with zero metalicity.\nIn other words, we study shattered cosmic pancakes ü•û"", 'Thanks to collaborators Frank van den Bosch, Volker Springel, and Freeke van de Voort!']",https://arxiv.org/abs/1906.10693,"We present a new cosmological zoom-in simulation, where the zoom region consists of two halos with virial mass M_v~5x10^{12}M_{sun} and a ~Mpc long cosmic filament connecting them at z~2. Using this simulation, we study the evolution of the intergalactic medium in between these two halos at unprecedented resolution. At 5>z>3, the two halos are found to lie in a large intergalactic sheet, or ""pancake"", consisting of multiple co-planar dense filaments along which nearly all halos with M_v>10^9M_{sun} are located. This sheet collapses at z~5 from the merger of two smaller sheets. The strong shock generated by this merger leads to thermal instabilities in the post-shock region, and to a shattering of the sheet resulting in <~kpc scale clouds with temperatures of T>~2x10^4K and densities of n>~10^{-3}cm^{-3}, which are pressure confined in a hot medium with T~10^6K and n>~10^{-5}cm^{-3}. When the sheet is viewed face on, these cold clouds have neutral hydrogen column densities of N_{HI}>10^{17.2}cm^{-2}, making them detectable as Lyman limit systems, though they lie well outside the virial radius of any halo and even well outside the dense filaments. Their chemical composition is pristine, having zero metalicity, similar to several recently observed systems. Since these systems form far from any galaxies, these results are robust to galaxy formation physics, resulting purely from the collapse of large scale structure and radiative cooling, provided sufficient spatial resolution is available. ","Shattering of Cosmic Sheets due to Thermal Instabilities: a Formation
  Channel for Metal-Free Lyman Limit Systems"
16,1144258571980464129,738769492122214400,Johannes Lischner,"['In our new paper, we demonstrate charge-to-spin and spin-to-charge conversion for a TaS2/graphene heterostructure via Rashba-Edelstein effect. Great collab between groups at Seoul, York and Imperial with hard-core DFT calcs by @JinZhan15060030. Read here: <LINK> <LINK>']",https://arxiv.org/abs/1906.10702,"We report the observation of current-induced spin polarization, the Rashba-Edelstein effect (REE), and its Onsager reciprocal phenomenon, the spin galvanic effect (SGE), in a few-layer graphene/2H-TaS2 heterostructure at room temperature. Spin-sensitive electrical measurements unveil full spin-polarization reversal by an applied gate voltage. The observed gate-tunable charge-to-spin conversion is explained by the ideal work function mismatch between 2H-TaS2 and graphene, which allows strong interface-induced Bychkov-Rashba interaction with a spin-gap reaching 70 meV, while keeping the Dirac nature of the spectrum intact across electron and hole sectors. The reversible electrical generation and control of the nonequilibrium spin polarization vector, not previously observed in a nonmagnetic material, are elegant manifestations of emergent 2D Dirac fermions with robust spin-helical structure. Our experimental findings, supported by first-principles relativistic electronic structure and transport calculations, demonstrate a route to design low-power spin-logic circuits from layered materials. ","Gate-Tunable Reversible Rashba-Edelstein Effect in a Few-Layer
  Graphene/2H-TaS2 Heterostructure at Room Temperature"
17,1144135163405901825,561167071,Sascha Caron,"['Our new paper (mainly by @MCvBeekveld) on:\n\nThe current status of finetuning (FT) in Supersymmetry:\n\nDepending on the high-scale model and fine-tuning definition, we find a minimal\nFT of O(10%) (low scale measure) or O(1%) (high-scale).\n\n<LINK> <LINK>']",https://arxiv.org/abs/1906.10706,"In this paper, we minimize and compare two different fine-tuning measures in four high-scale supersymmetric models that are embedded in the MSSM. In addition, we determine the impact of current and future dark matter direct detection and collider experiments on the fine-tuning. We then compare the low-scale electroweak measure with the high-scale Barbieri-Giudice measure, which generally do not agree. However, we find that they do reduce to the same value when the higgsino parameter drives the degree of fine-tuning. Depending on the high-scale model and fine-tuning definition, we find a minimal fine-tuning of $3-38$ (corresponding to $\mathcal{O}(10-1)\%$) for the low-scale measure, and $63-571$ (corresponding to $\mathcal{O}(1-0.1)\%$) for the high-scale measure. In addition, minimally fine-tuned spectra give rise to a dark matter relic density that is between $10^{-3} < \Omega h^2 < 1$, when $\mu$ determines the minimum of the fine-tuning. We stress that it is too early to conclude on the fate of supersymmetry, based only on the fine-tuning paradigm. ",The current status of fine-tuning in supersymmetry
18,1144043275587686401,75498067,Dr. Dennis Foren üé®,"[""‚öõÔ∏è WE HAVE A NEW PAPER OUT ‚öõÔ∏è\n\n‚≠êÔ∏è Read it on the arXiv: <LINK>\n\nI'm stoked! This is the first of several papers my group has planned, and begins to describe something we've been working on (in one form or another) since late 2017. <LINK>"", 'We report the results from explicit calculations of amplitudes describing 2-to-2 scattering of massive spin-2 particles. An amplitude M(s) is a fxn of s ( = total incoming energy squared ) and is related to the probability that the corresponding scattering process occurs. https://t.co/DE9pjFwkjC', ""If an amplitude grows too large, that probability becomes nonsensical. Slower growth w.r.t. energy ( = ‚àös ) means a theory is reliable over a larger energy range. Paper's Thesis: we EXPLICITLY demonstrate amplitudes in certain massive spin-2 models grow MUCH slower than others."", 'How do you get a massive spin-2 particle for scattering? Try Fierz-Pauli (FP) gravity:\nüîπStep 1: perturb flat 4D spacetime to get a massless spin-2 particle = ""graviton""\nüî∏Step 2: add FP mass term.\nCongrats, you have a massive spin-2 particle!\nHowever, this theory has problems‚Ä¶ https://t.co/KhH4iAZ8pk', 'While the graviton has two polarizations (+2, -2), a massive spin-2 has five (+2, +1, 0, -1, -2). Those new helicity states are ""longitudinal"" and mean trouble: if we 2-to-2 scatter helicity-0‚Äôs from FP gravity, M(s) grows like s^5 &amp; predictions are nonsense even at low energies.', ""However, there are other ways to get massive spin-2's. Consider a 5D gravity theory where 5D spacetime = 4D spacetime + a new compact spatial dimension.\n... &amp; enforce an orbifold symmetry\n... &amp; use specific constructions (5D orbifolded torus, RS1)\nDetails are in the paper. üòõ https://t.co/xVfQalUO3c"", 'In these 5D gravity theories, the 5D scattering amplitudes grow like s^(3/2) because of coordinate invariance (much better than s^5!) By perturbing 5D spacetime like we did 4D spacetime earlier, we get a massless 5D spin-2 field &amp; a massless 5D spin-0 field. But... we need 4D...', 'We connect 5D to 4D using a process called ""KK decomposition"", which is like a fancy field theory version of Fourier expansion. This replaces our 5D spin-2 field with a massless 4D spin-2 field (""graviton"") + a sum of infinitely-many massive 4D spin-2 fields (""KK modes""). https://t.co/VSKAiw9rzz', 'Our 5D spin-0 field is replaced by a massless 4D spin-0 field called the ""radion"" (whose average value determines the compact dimension\'s length).\n\nTo summarize: we went from a (bad) set-up with one massive spin-2 particle to a (good?) set-up with INFINITELY MANY of them. Uhhh...', 'But sure enough, by explicitly calculating amplitudes for many longitudinal 2-to-2 KK mode scattering processes, we find these amplitudes (which originate from 5D gravity theories) grow linearly in s instead of like s^5! ü§© This is the 1st time this has been directly calculated! https://t.co/dRga49ECGI', 'This result is also (via coupled-channel analysis) consistent w/ the 5D s^(3/2) prediction!\n\nOne big challenge along the way: the warped 5D gravity calculations involve cancellations across many orders of magnitude, and so require many sig figs from many oscillatory integrals üòµ', ""Of course, our paper goes into much more detail than this twitter thread can, so please check it out. It's the first in a series we have planned and relies on work of which I'm very proud. I‚Äôm happy to finally share it with all of you! ‚ò∫Ô∏è Best wishes! ‚ö°Ô∏èüå∏""]",https://arxiv.org/abs/1906.11098,"We present the results of the first complete calculation of the tree-level $2\to 2$ high-energy scattering amplitudes of the longitudinal modes of massive spin-2 Kaluza-Klein states, both in the case where the internal space is a torus and in the Randall-Sundrum model where the internal space has constant negative curvature. While individual contributions to this amplitude grow as ${\cal O}(s^5$), we demonstrate explicitly that intricate cancellations occur between different contributions, reducing the growth to ${\cal O}(s)$, a slower rate of growth than previously argued in the literature. These cancellations require subtle relationships between the masses of the Kaluza-Klein states and their interactions, and reflect the underlying higher-dimensional diffeomorphism invariance. Our results provide fresh perspective on the range of validity of (effective) field theories involving massive spin-2 KK particles, with significant implications for the theory and phenomenology of these states. ","Scattering Amplitudes of Massive Spin-2 Kaluza-Klein States Grow Only as
  ${\cal O}(s)$"
19,1143924806665949184,169937939,Xiongzhi Chen,"['my new paper on ""uniformly consistently estimating the proportion of false nulls for composite hypotheses via Lebesgue-Stieltjes integral equations"" <LINK>; comments are welcome!']",https://arxiv.org/abs/1906.10246,"We consider estimating the proportion of random variables for two types of composite null hypotheses: (i) the means or medians of the random variables belonging to a non-empty, bounded interval; (ii) the means or medians of the random variables belonging to an unbounded interval that is not the whole real line. For each type of composite null hypotheses, uniform consistent estimators of the proportion of false null hypotheses are constructed respectively for random variables whose distributions are members of a Type I location-shift family or are members of the Gamma family. Further, uniformly consistent estimators of certain functions of a bounded null on the means or medians are provided for the two types of random variables mentioned earlier. These functions are continuous and of bounded variation. The estimators are constructed via solutions to Lebesgue-Stieltjes integral equations and harmonic analysis, do not rely on a concept of p-value, can be used to construct adaptive false discovery rate procedures and adaptive false nondiscovery rate procedures for multiple hypothesis testing, can be used in Bayesian inference via mixture models, and may be used to estimate the sparsity level in high-dimensional Gaussian linear models. ","Uniformly consistent proportion estimation for composite hypotheses via
  integral equations"
20,1143916572924190721,452384386,Sebastien Bubeck,"[""New paper <LINK> on black-box complexity of *parallel convex optimization*. We uncover yet again a quadratic acceleration phenomenon: while serial gradient descent is optimal up to depth D (=dim of the problem), for parallel it's true only up to depth sqrt(D)!!! <LINK>"", 'Prescient work by Nemirovski from 1994 (rediscovered recently by Balkanski-Singer https://t.co/yq1VxQScqK) essentially showed optimality of GD up to //-depth D^{1/3}: a key contribution of our work is to improve this to the optimal D^{1/2} by a new ``Wall function"" construction.', 'From the algo side, Duchi-Bartlett-Wainwright already showed that after D^{1/2} depth one can improve the 1/eps^2 rate of GD to D^{1/4}/eps. We also improve this regime, and leverage our highly smooth acceleration paper https://t.co/lU18Ee0d6h to obtain rate D^{1/3}/eps^{2/3}.', 'Conjecture: D^{1/3}/eps^{2/3} is optimal in depth range [D^{1/2}, D]. Seems like fun &amp; difficult convex geometry question! \n\nBoth highly smooth and parallel acceleration papers are joint work with  Qijia Jiang (Stanford grad student), Yin Tat Lee, Yuanzhi Li, and Aaron  Sidford.', ""@roydanroy @aminkarbasi I disagree so strongly with that last comment I don't even know where to begin..."", ""@aminkarbasi @roydanroy Dan's.\n\nI happen to midly (much much more midly) disagree with yours too, just in the sense that I think there *is* a quite significant gain for small accuracies. To be concrete, say you are in d=10^9, and you want to reach accuracy eps=10^-6, then our // alg is 100x improvement.""]",https://arxiv.org/abs/1906.10655,"A landmark result of non-smooth convex optimization is that gradient descent is an optimal algorithm whenever the number of computed gradients is smaller than the dimension $d$. In this paper we study the extension of this result to the parallel optimization setting. Namely we consider optimization algorithms interacting with a highly parallel gradient oracle, that is one that can answer $\mathrm{poly}(d)$ gradient queries in parallel. We show that in this case gradient descent is optimal only up to $\tilde{O}(\sqrt{d})$ rounds of interactions with the oracle. The lower bound improves upon a decades old construction by Nemirovski which proves optimality only up to $d^{1/3}$ rounds (as recently observed by Balkanski and Singer), and the suboptimality of gradient descent after $\sqrt{d}$ rounds was already observed by Duchi, Bartlett and Wainwright. In the latter regime we propose a new method with improved complexity, which we conjecture to be optimal. The analysis of this new method is based upon a generalized version of the recent results on optimal acceleration for highly smooth convex optimization. ",Complexity of Highly Parallel Non-Smooth Convex Optimization
21,1143860072361185281,974769773539155968,Daniel Baumann,"['New paper with @nu_phases and Tom Hartman: <LINK>\nWe relate inflation to a special type of RG flow, and then use causality and unitarity to derive new constraints on the low-energy couplings of the theory.', '@nu_phases The main result is a sum rule relating the speed at which a certain operator spreads along the RG flow to the UV completion of the theory. In inflation this translates into a constraint on the speed of propagation of inflationary fluctuations.', '@nu_phases Relating IR observables to the UV will play an important role in connecting cosmological observables to the microscopic origin of inflation.', '@nu_phases All credit to my fantastic collaborators Dan and Tom for making this work. It has been a lot of fun to learn from them and explore the connection between fundamental questions in cosmology and advanced techniques in quantum field theory.']",https://arxiv.org/abs/1906.10226,"Sum rules connecting low-energy observables to high-energy physics are an interesting way to probe the mechanism of inflation and its ultraviolet origin. Unfortunately, such sum rules have proven difficult to study in a cosmological setting. Motivated by this problem, we investigate a precise analogue of inflation in anti-de Sitter spacetime, where it becomes dual to a slow renormalization group flow in the boundary quantum field theory. This dual description provides a firm footing for exploring the constraints of unitarity, analyticity, and causality on the bulk effective field theory. We derive a sum rule that constrains the bulk coupling constants in this theory. In the bulk, the sum rule is related to the speed of radial propagation, while on the boundary, it governs the spreading of nonlocal operators. When the spreading speed approaches the speed of light, the sum rule is saturated, suggesting that the theory becomes free in this limit. We also discuss whether similar results apply to inflation, where an analogous sum rule exists for the propagation speed of inflationary fluctuations. ",Dynamical Constraints on RG Flows and Cosmology
22,1143818657086132224,2285825876,Johanna,"['Excited for new paper by Jennifer Winters et al., describing @TESSatMIT/@NASA_TESS detection of small rocky planet around M dwarf 6.9 pc away! That makes it the 2nd closest transiting planet system discovered! Check it out here <LINK>', 'Also, how about this title to spark the imagination? :) ‚ÄùThree Red Suns in the Sky: A Transiting, Terrestrial Planet in a Triple M Dwarf System at 6.9 Parsecs‚Äù']",https://arxiv.org/abs/1906.10147,"We present the discovery from TESS data of LTT 1445Ab. At a distance of 6.9 parsecs, it is the second nearest transiting exoplanet system found to date, and the closest one known for which the primary is an M dwarf. The host stellar system consists of three mid-to-late M dwarfs in a hierarchical configuration, which are blended in one TESS pixel. We use data from MEarth and results from the SPOC DV report to determine that the planet transits the primary star in the system. The planet has a radius 1.38 R_Earth, an orbital period of 5.35882 days, and an equilibrium temperature of 433 K. With radial velocities from HARPS, we place a three-sigma upper mass limit of 8.4 M_Earth on the candidate. The planet provides one of the best opportunities to date for the spectroscopic study of the atmosphere of a terrestrial world. The presence of stellar companions of similar spectral type may facilitate such ground-based studies by providing a calibration source to remove telluric variations. In addition, we present a detailed characterization of the host stellar system. We use high-resolution spectroscopy and imaging to rule out the presence of any other close stellar or brown dwarf companions. Nineteen years of photometric monitoring of A and BC indicates a moderate amount of variability, in agreement with the observed low-level, short-term variability in the TESS light curve data. We derive a preliminary astrometric orbit for the BC pair that reveals an edge-on and eccentric configuration. The presence of a transiting planet in this system raises the possibility that the entire system is co-planar, which implies that the system may have formed from the early fragmentation of an individual protostellar core. ","Three Red Suns in the Sky: A Transiting, Terrestrial Planet in a Triple
  M Dwarf System at 6.9 Parsecs"
23,1143818556154531840,987061319378587649,Maksym Andriushchenko üá∫üá¶,"['Happy to share our new paper on provable robustness for boosting.\nFor boosted stumps, we can solve the min-max problem *exactly*.\nFor boosted trees, we minimize an upper bound on robust loss.\nEverything is nice &amp; convex!\nPaper <LINK>\nCode <LINK> <LINK>', ""@ducha_aiki That would be great indeed. Although this would potentially require rewriting everything from python to C. Seems like it's worth spending the time. And I hope practitioners would forgive us the O(n^2) complexity instead of O(n log n) :)"", ""Up to our knowledge, it's the first paper in the area of boosting that directly optimizes a robustness guarantee during training. And since neural networks is not the only classifier we have, it's pretty instructive to see what happens for another widely used class of models! https://t.co/I3W2cOrpZ1""]",https://arxiv.org/abs/1906.03526,"The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the \textit{exact} min-max robust loss and test error for an $l_\infty$-attack can be computed in $O(T\log T)$ time per input, where $T$ is the number of decision stumps and the optimal update step of the ensemble can be done in $O(n^2\,T\log T)$, where $n$ is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5% for $\epsilon_\infty=0.3$), FMNIST (23.2% for $\epsilon_\infty=0.1$), and CIFAR-10 (74.7% for $\epsilon_\infty=8/255$). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at this http URL ","Provably Robust Boosted Decision Stumps and Trees against Adversarial
  Attacks"
24,1143802522299244545,476582730,Shakir Mohamed,"[""Exited to share our new paper: 'Monte Carlo Gradient Estimation in Machine Learning', with @elaClaudia @mfigurnov @AndriyMnih. It reviews of all the things we know about computing gradients of probabilistic functions. <LINK>  üêæThreadüëáüèæ <LINK>"", ""We've spent almost a year learning and reading everything we can find over the last 50 years in this areaüìñ. One of our messages is that there is no uniformly best gradient estimator. Fig 3 was insightful to us. üêæ https://t.co/dUi1edcQc0"", 'There are three simple probabilistic tricks that are used to derive three estimators in the paper. The score function, exploits the definition of the derivative of a logarithm to derive the score function estimator. üêæ https://t.co/jm43GaJC9L', 'Continuous distributions have a sampling property that allows them to be simulated using samples from a simpler distribution and a known sampling path. Using this path gives the pathwise estimator (often referred to as the reparam trick).üêæ https://t.co/yfiIS32ygh', 'Derivatives of densities can be expressed as the difference of two densities, using its weak derivative, its measure-theoretic property. This allows us to derive the measure-valued gradient.üêæ https://t.co/wTNlv3rVc8', ""And these gradient estimators allows us to see extensions and connections to many areas, including probabilistic programming, optimal transport, Fourier analysis, quasi-monte carlo, Stein's Idenitity, Malliavin calculus, control variates, queuing theory, ... üêæ https://t.co/yIFSamBDXN"", 'We think there is something in here for the person just starting off in this area, and the seasoned expert. Help us improve the paper by letting us know if we missed any important connections, can explain anything better, or have  errors üôèüèæ. üêæ', 'Personally, I believe this might be the best paper I have been part of writing so far. We tried to be patient in our writing, and to use the principle that our duty is to uplift our readers. We hope we took a step in that direction. #writinggoals üëãüèΩ', '@MQBayes @elaClaudia @mfigurnov @AndriyMnih Will update. Thanks!', '@_rockt @elaClaudia @mfigurnov @AndriyMnih @j_foerst @alshedivat @greg_far @shimon8282 Thanksü§©! Will add.', ""@sebbodenstein @elaClaudia @mfigurnov @AndriyMnih Thanks. I'll leave it for now and it will get fixed soon.""]",https://arxiv.org/abs/1906.10652,"This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support. ",Monte Carlo Gradient Estimation in Machine Learning
25,1143685130512297984,1093387119148462081,Daniel Green,"['New paper today with @DD_Baumann and Tom Hartman: <LINK>.  We relate two interesting problems, mapping the space of inflationary models and RG flows.  I learned a ton from this (super fun) collaboration, hopefully people learn something from the paper.']",https://arxiv.org/abs/1906.10226,"Sum rules connecting low-energy observables to high-energy physics are an interesting way to probe the mechanism of inflation and its ultraviolet origin. Unfortunately, such sum rules have proven difficult to study in a cosmological setting. Motivated by this problem, we investigate a precise analogue of inflation in anti-de Sitter spacetime, where it becomes dual to a slow renormalization group flow in the boundary quantum field theory. This dual description provides a firm footing for exploring the constraints of unitarity, analyticity, and causality on the bulk effective field theory. We derive a sum rule that constrains the bulk coupling constants in this theory. In the bulk, the sum rule is related to the speed of radial propagation, while on the boundary, it governs the spreading of nonlocal operators. When the spreading speed approaches the speed of light, the sum rule is saturated, suggesting that the theory becomes free in this limit. We also discuss whether similar results apply to inflation, where an analogous sum rule exists for the propagation speed of inflationary fluctuations. ",Dynamical Constraints on RG Flows and Cosmology
26,1143635100426313728,51257255,Natasha Jaques,"['Our new paper shows that to evaluate a dialog model, you need a human to actually talk to it! We then use self-play to accurately approximate the human @ghandeharioun @judyhshen \n\nPaper: <LINK>\nCode: <LINK>\nPlatform: <LINK>']",https://arxiv.org/abs/1906.09308,"Building an open-domain conversational agent is a challenging problem. Current evaluation methods, mostly post-hoc judgments of static conversation, do not capture conversation quality in a realistic interactive context. In this paper, we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel, model-agnostic, and dataset-agnostic method to approximate it. In particular, we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date, achieving a significant Pearson correlation (r>.7, p<.05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations, we perform extended experiments with a set of models, including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally, we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efficiently deploy and evaluate dialog models. ","Approximating Interactive Human Evaluation with Self-Play for
  Open-Domain Dialog Systems"
27,1143548242963001344,610427323,Desika Narayanan,"[""hey astrotwitter we've got a new paper out led by @UFastro grad student Qi Li!  <LINK>  we put in a cool new model for dust formation/growth/destruction in cosmo sims to ask what drives variations in the dust to gas ratio/dust to metals ratios in galaxies [1/]"", 'In short - the dust to gas ratio is driven in large part by the metallicity of galaxies (due in part to the dependence of metallicity in dust growth rates in the ISM), [2/]', 'There are secondary factors the dust to gas ratio or dust to metals ratio depend on that we tease out with some machine learning techniques, but to first order is mostly set by the metallicity. [3/]', 'For observers and theorists who want a dust[gas] mass from their observed/simulated galaxy, we provide a public code to make it happen!   https://t.co/fdMWpMDFpA']",https://arxiv.org/abs/1906.09277v1,"We present predictions for the evolution of the galaxy dust-to-gas (DGR) and dust-to-metal (DTM) ratios from z=0 to 6, using a model for the production, growth, and destruction of dust grains implemented into the \simba\ cosmological hydrodynamic galaxy formation simulation. In our model, dust forms in stellar ejecta, grows by the accretion of metals, and is destroyed by thermal sputtering and supernovae. Our simulation reproduces the observed dust mass function at z=0, but modestly under-predicts the mass function by ~x3 at z ~ 1-2. The z=0 DGR vs metallicity relationship shows a tight positive correlation for star-forming galaxies, while it is uncorrelated for quenched systems. There is little evolution in the DGR-metallicity relationship between z=0-6. We use machine learning techniques to search for the galaxy physical properties that best correlate with the DGR and DTM. We find that the DGR is primarily correlated with the gas-phase metallicity, though correlations with the depletion timescale, stellar mass and gas fraction are non-negligible. We provide a crude fitting relationship for DGR and DTM vs. the gas-phase metallicity, along with a public code package that estimates the DGR and DTM given a set of galaxy physical properties. ",] The Dust-to-Gas and Dust-to-Metals Ratio in Galaxies from z=0-6
28,1143512906417373187,838292815,Ofir Nachum,"['New paper out! An advancement in properly estimating off-policy occupancy ratios. We apply it to off-policy policy evaluation with great results, but we believe it should be useful in many more off-policy settings!\n<LINK> <LINK>']",https://arxiv.org/abs/1906.04733,"In many real-world reinforcement learning applications, access to the environment is limited to a fixed dataset, instead of direct (online) interaction with the environment. When using this data for either evaluation or training of a new policy, accurate estimates of discounted stationary distribution ratios -- correction terms which quantify the likelihood that the new policy will experience a certain state-action pair normalized by the probability with which the state-action pair appears in the dataset -- can improve accuracy and performance. In this work, we propose an algorithm, DualDICE, for estimating these quantities. In contrast to previous approaches, our algorithm is agnostic to knowledge of the behavior policy (or policies) used to generate the dataset. Furthermore, it eschews any direct use of importance weights, thus avoiding potential optimization instabilities endemic of previous methods. In addition to providing theoretical guarantees, we present an empirical study of our algorithm applied to off-policy policy evaluation and find that our algorithm significantly improves accuracy compared to existing techniques. ","DualDICE: Behavior-Agnostic Estimation of Discounted Stationary
  Distribution Corrections"
29,1143490301870448640,46153507,dr. Jordy Davelaar,"['New paper online! We modeled the accreting black hole in M87! The @ehtelescope models did not include non-thermal electrons, we studied the effect of including them. We were able to fit the radio to NIR fluxes of M87*, obtain source sizes and core shifts!\n\n<LINK> <LINK>', 'Co authors: Hector Olivares, Oliver Porth, @thomasbronzwaer, Michael Janssen, @froeloefs, Yosuke Mizuno, Christian Fromm, @hfalcke, and Luciano Rezzolla\n\n@RUastro @FlatironInst @FlatironCCA @goetheuni @uva_api', 'The model are based on cartesian adaptive mesh refined general-relativistic magnetohydrodynamics simulations (code: BHAC) containing more than 70M cells, resulting in a high resolution jet simulation. For the first time we ray trace this non uniform simulation data (code: RAPTOR) https://t.co/OLEiaTFNz3', 'Our non-thermal electrons are included by using parametrisations of first-principle particle-in-cell simulations by Ball et al. 2018 where they studied trans-relativistic reconnection. Allowing us to get the right spectral slope without fine tuning. https://t.co/LLqdHML7zT', 'Comparing to thermal models we see a more compact emission region, lower mass accretion rates, and lower jet powers. \n\nPaper is submitted to A&amp;A https://t.co/rcUuGx6UjY']",http://arxiv.org/abs/1906.10065,"The galaxy M 87 harbors a kiloparsec-scale relativistic jet, whose origin coincides with a supermassive black hole. Observational mm-VLBI campaigns are capable of resolving the jet-launching region at the scale of the event horizon. In order to provide a context for interpreting these observations, realistic general-relativistic magnetohydrodynamical (GRMHD) models of the accretion flow are constructed. The characteristics of the observed spectral-energy distribution (SED) depend on the shape of the electrons' energy-distribution function (eDF). The dependency on the eDF is omitted in the modeling of the first Event Horizon Telescope results. In this work, we aim to model the M 87 SED from radio up to NIR/optical frequencies using a thermal-relativistic Maxwell- J\""uttner distribution, as well as a relativistic $\kappa$-distribution function. The electrons are injected based on sub-grid, particle-in-cell parametrizations for sub-relativistic reconnection. A GRMHD simulation in Cartesian-Kerr-Schild coordinates, using eight levels of adaptive mesh refinement (AMR), forms the basis of our model. To obtain spectra and images, the GRMHD data is post-processed with the ray-tracing code RAPTOR, which is capable of ray tracing through AMR GRMHD simulation data. We obtain radio spectra in both the thermal-jet and $\kappa$-jet models consistent with radio observations. Additionally, the $\kappa$-jet models also recover the NIR/optical emission. The models recover the observed source sizes and core shifts and obtain a jet power of $\approx 10^{43}$ ergs/s. In the $\kappa$-jet models, both the accretion rates and jet powers are approximately two times lower than the thermal-jet model. The frequency cut-off observed at $\nu \approx 10^{15}$ Hz is recovered when the accelerator size is $10^6$ - $10^8$ cm, this could potentially point to an upper limit for plasmoid sizes in the jet of M 87. ","Modeling non-thermal emission from the jet-launching region of M 87 with
  adaptive mesh refinement"
30,1143488651579600896,3813580887,Paul B√ºrkner,"['Pushing the limits of importance sampling to improve accuracy and efficiency of approximate Bayesian cross-validation. Our new paper (by @PaananenTopi, Juho Piironen, @avehtari, and me) is now on arXiv (<LINK>)!', '@bachlaw @PaananenTopi @avehtari Topi (@PaananenTopi) did most of this and I am very impressed by his work!']",https://arxiv.org/abs/1906.08850,"Adaptive importance sampling is a class of techniques for finding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive. ",Implicitly Adaptive Importance Sampling
31,1143454005277986818,45326386,Prof Theo Arvanitis,"['Our new pre-publication paper, entitled ""Multisensory cues facilitate coordination of stepping movements with a virtual reality avatar"" is now available at <LINK>']",https://arxiv.org/abs/1906.09850,"The effectiveness of simple sensory cues for retraining gait have been demonstrated, yet the feasibility of humanoid avatars for entrainment have yet to be investigated. Here, we describe the development of a novel method of visually cued training, in the form of a virtual partner, and investigate its ability to provide movement guidance in the form of stepping. Real stepping movements were mapped onto an avatar using motion capture data. The trajectory of one of the avatar step cycles was then accelerated or decelerated by 15% to create a perturbation. Healthy participants were motion captured while instructed to step in time to the avatar's movements, as viewed through a virtual reality headset. Step onset times were used to measure the timing errors (asynchronies) between them. Participants completed either a visual-only condition, or auditory-visual with footstep sounds included. Participants' asynchronies exhibited slow drift in the Visual-Only condition, but became stable in the Auditory-Visual condition. Moreover, we observed a clear corrective response to the phase perturbation in both auditory-visual conditions. We conclude that an avatar's movements can be used to influence a person's own gait, but should include relevant auditory cues congruent with the movement to ensure a suitable accuracy is achieved. ","Multisensory cues facilitate coordination of stepping movements with a
  virtual reality avatar"
32,1143435069148655616,914692885286498307,Guillaume Lambard,"['When machines autonomously characterize molecules by just reading them!\nPlease, have a look at our new paper: <LINK>\nand Github page: <LINK> <LINK>']",https://arxiv.org/abs/1906.09938,"There is more and more evidence that machine learning can be successfully applied in materials science and related fields. However, datasets in these fields are often quite small ($\ll1000$ samples). It makes the most advanced machine learning techniques remain neglected, as they are considered to be applicable to big data only. Moreover, materials informatics methods often rely on human-engineered descriptors, that should be carefully chosen, or even created, to fit the physicochemical property that one intends to predict. In this article, we propose a new method that tackles both the issue of small datasets and the difficulty of task-specific descriptors development. The SMILES-X is an autonomous pipeline for molecular compounds characterisation based on a \{Embed-Encode-Attend-Predict\} neural architecture with a data-specific Bayesian hyper-parameters optimisation. The only input to the architecture -- the SMILES strings -- are de-canonicalised in order to efficiently augment the data. One of the key features of the architecture is the attention mechanism, which enables the interpretation of output predictions without extra computational cost. The SMILES-X shows new state-of-the-art results in the inference of aqueous solubility ($\overline{RMSE}_{test} \simeq 0.57 \pm 0.07$ mols/L), hydration free energy ($\overline{RMSE}_{test} \simeq 0.81 \pm 0.22$ kcal/mol, which is $\sim 24.5\%$ better than molecular dynamics simulations), and octanol/water distribution coefficient ($\overline{RMSE}_{test} \simeq 0.59 \pm 0.02$ for LogD at pH 7.4) of molecular compounds. The SMILES-X is intended to become an important asset in the toolkit of materials scientists and chemists. The source code for the SMILES-X is available at \href{this https URL}{github.com/GLambard/SMILES-X}. ","SMILES-X: autonomous molecular compounds characterization for small
  datasets without descriptors"
33,1143417686388613121,2242019113,christian cachin,['#blockchain #consensus with subjective trust: asymmetric quorums generalize distributed protocols to flexible assumptions. New blog <LINK> &amp; paper <LINK> . Will present @CVConf_ in #cryptovalley @thecryptovalley today #CVC19 #cryptovalleyweek'],https://arxiv.org/abs/1906.09314,"Quorum systems are a key abstraction in distributed fault-tolerant computing for capturing trust assumptions. They can be found at the core of many algorithms for implementing reliable broadcasts, shared memory, consensus and other problems. This paper introduces asymmetric Byzantine quorum systems that model subjective trust. Every process is free to choose which combinations of other processes it trusts and which ones it considers faulty. Asymmetric quorum systems strictly generalize standard Byzantine quorum systems, which have only one global trust assumption for all processes. This work also presents protocols that implement abstractions of shared memory and broadcast primitives with processes prone to Byzantine faults and asymmetric trust. The model and protocols pave the way for realizing more elaborate algorithms with asymmetric trust. ",Asymmetric Distributed Trust
34,1143322978039533568,401811181,Walter Tangarife,['New paper on the arXiv with friends at @UdeA:  <LINK>\n#DarkMatter #Neutrinos <LINK>'],https://arxiv.org/abs/1906.09685,"We examine an extension of the Standard Model that addresses the dark matter puzzle and generates Dirac neutrinos masses through the radiative seesaw mechanism. The new field content includes a scalar field that plays an important role in setting the relic abundance of dark matter. We analyze the phenomenology in the light of direct, indirect, and collider searches of dark matter. In this framework, the dark matter candidate is a Dirac particle that is a mixture of new singlet-doublet fields with mass $m_{\chi_1^0}\lesssim 1.1\,\text{TeV}$. We find that the allowed parameter space of this model is broader than the well-known Majorana dark matter scenario. ",Singlet-Doublet Dirac Dark Matter and Neutrino Masses
35,1143122073897902081,955113738620809224,Sergio G√≥mez,"['New paper with Alberto Fern√°ndez, accepted in Journal of Classification. About Versatile linkage, a family of algorithms for hierarchical clustering which includes UPGMA, complete linkage, ... We also define space distortion and tree balance of dendrograms <LINK>']",https://arxiv.org/abs/1906.09222,"Agglomerative hierarchical clustering can be implemented with several strategies that differ in the way elements of a collection are grouped together to build a hierarchy of clusters. Here we introduce versatile linkage, a new infinite system of agglomerative hierarchical clustering strategies based on generalized means, which go from single linkage to complete linkage, passing through arithmetic average linkage and other clustering methods yet unexplored such as geometric linkage and harmonic linkage. We compare the different clustering strategies in terms of cophenetic correlation, mean absolute error, and also tree balance and space distortion, two new measures proposed to describe hierarchical trees. Unlike the $\beta$-flexible clustering system, we show that the versatile linkage family is space-conserving. ","Versatile linkage: a family of space-conserving strategies for
  agglomerative hierarchical clustering"
36,1142958547866705920,19491219,Maxim Raginsky,"['New paper with my graduate student Josh Hanson on universal approximation of causal, time-invariant sequence-to-sequence maps by temporal convolutional nets with ReLU activations. <LINK> 1/4', ""We build on a qualitative universal approximation result due to Irwin Sandberg (1991): any seq-to-seq map with approximately finite memory can be approximated by composing a sliding window, an affine map, and a lattice map (finite sequence of coordinatewise max's and min's). 2/4"", 'We extend cool recent work by John Miller and @mrtz on feedforward approx. of exponentially stable recurrent models. We show that this holds more broadly for a class of nonlinear state-space models that are incrementally stable (eventually forget their initial condition). 3/4', 'This does not require exponential contraction, but rather a much milder summability condition; the Miller-Hardt result is a special case that has connections to the Demidovich stability criterion for nonlinear systems. Comments welcome! https://t.co/xhIyHCuWFn 4/4']",https://arxiv.org/abs/1906.09211,"There has been a recent shift in sequence-to-sequence modeling from recurrent network architectures to convolutional network architectures due to computational advantages in training and operation while still achieving competitive performance. For systems having limited long-term temporal dependencies, the approximation capability of recurrent networks is essentially equivalent to that of temporal convolutional nets (TCNs). We prove that TCNs can approximate a large class of input-output maps having approximately finite memory to arbitrary error tolerance. Furthermore, we derive quantitative approximation rates for deep ReLU TCNs in terms of the width and depth of the network and modulus of continuity of the original input-output map, and apply these results to input-output maps of systems that admit finite-dimensional state-space realizations (i.e., recurrent models). ","Universal Approximation of Input-Output Maps by Temporal Convolutional
  Nets"
37,1142845208771649540,974303612330545152,Cicuta Group üêä,"['Paper on arXiv on non-equilibrium fluctuations, cause and effect.   Theory and experiments!  A collaboration with Nader Seyyed. This extends Meiners-Quake correlations, and 3-bead swimmers, in new directions.     <LINK>']",https://arxiv.org/abs/1906.07621,"In low Reynolds number swimming and pumping, differently to everyday experience, a net motion (or flow) can be achieved only if the constructing parts of the swimmer (or pump) follow a non-trivial pattern of motion, in order to break time reciprocity. The case of a driven fan, which spins to create a flow of air, but conversely rotates when turned off and subjected to a strong external flow, is a familiar example of reciprocal connection between physical cause and effect. We explore here in a well controlled low Reynolds number system whether such an exchange of the cause and effect also holds in the low Reynolds number regime. As a case study we investigate the motion of two microspheres which interact hydrodynamically through their surrounding fluid. Each sphere is constrained in a fixed optical trap potential, allowing local fluctuations around an equilibrium position. An external flow is shown to induce non-trivial coupled motion. We find a signature of reciprocity: the nonequilibrium sphere fluctuations mimic the symmetry of the motions that one would impose in order for them to produce a constant flow. ",Two Microspheres in an External Flow: a Dance of Cause and Effect
38,1142822231837466625,471109812,Yan,"['My new paper on predicting sets with neural networks: ""Deep Set Prediction Networks""! MLP outputs are ordered, which is not ideal for predicting unordered sets. I created a model that naturally has unordered outputs.\n\nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/1906.06565,"Current approaches for predicting sets from feature vectors ignore the unordered nature of sets and suffer from discontinuity issues as a result. We propose a general model for predicting sets that properly respects the structure of sets and avoids this problem. With a single feature vector as input, we show that our model is able to auto-encode point sets, predict the set of bounding boxes of objects in an image, and predict the set of attributes of these objects. ",Deep Set Prediction Networks
39,1142546347637334020,890132360397803520,Christoph Salge,"['New paper out:\n\nMeasuring Time with Minimal Clocks, in Artificial Life Journal (@alifeofficial)\nwith @AndreiDRobu, me, @NehanivCL and Daniel Polani, from @UniofHerts \n\nLooking at #time as it could be measured in minimal biological or artificial systems.\n\n<LINK> <LINK>']",https://arxiv.org/abs/1906.07564,"Being able to measure time, whether directly or indirectly, is a significant advantage for an organism. It allows for the timely reaction to regular or predicted events, reducing the pressure for fast processing of sensory input. Thus, clocks are ubiquitous in biology. In the present paper, we consider minimal abstract pure clocks in different configurations and investigate their characteristic dynamics. We are especially interested in optimally time-resolving clocks. Among these, we find fundamentally diametral clock characteristics, such as oscillatory behavior for purely local time measurement or decay-based clocks measuring time periods of a scale global to the problem. We include also sets of independent clocks (""clock bags""), sequential cascades of clocks and composite clocks with controlled dependency. Clock cascades show a ""condensation effect"" and the composite clock shows various regimes of markedly different dynamics. ",Measuring Time with Minimal Clocks
40,1142086205707825153,1009799569390096384,Brenden Lake,"['More robust ImageNet classifiers using elements of human visual cognition: an episodic memory and a shape bias. New paper by Emin Orhan extending his work on cache-based object recognition models <LINK>', ""Emin's NeurIPS paper from last year on cache-based recognition models, https://t.co/gtqcvs8Lwo"", '@iraphas13 Thanks for the pointer, I will look forward to going through it. It would be interesting to understand if Patch Gaussian is a way of emphasizing shape-based features.']",https://arxiv.org/abs/1906.08416,"We investigate the robustness properties of image recognition models equipped with two features inspired by human vision, an explicit episodic memory and a shape bias, at the ImageNet scale. As reported in previous work, we show that an explicit episodic memory improves the robustness of image recognition models against small-norm adversarial perturbations under some threat models. It does not, however, improve the robustness against more natural, and typically larger, perturbations. Learning more robust features during training appears to be necessary for robustness in this second sense. We show that features derived from a model that was encouraged to learn global, shape-based representations (Geirhos et al., 2019) do not only improve the robustness against natural perturbations, but when used in conjunction with an episodic memory, they also provide additional robustness against adversarial perturbations. Finally, we address three important design choices for the episodic memory: memory size, dimensionality of the memories and the retrieval method. We show that to make the episodic memory more compact, it is preferable to reduce the number of memories by clustering them, instead of reducing their dimensionality. ","Improving the robustness of ImageNet classifiers using elements of human
  visual cognition"
41,1142076911541215237,2541954109,Victoria Krakovna,['New paper on modeling AI safety approaches with causal influence diagrams <LINK>'],https://arxiv.org/abs/1906.08663,"Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks. ",Modeling AGI Safety Frameworks with Causal Influence Diagrams
42,1142065712573423616,2759165280,Jorge Melendez,"['Congrats to my PhD student Henrique Reggiani (Univ. S√£o Paulo üáßüá∑), for his new paper on Non-LTE corrections for potassium:\n\n<LINK> <LINK>']",https://arxiv.org/abs/1906.08281,"Older GCE models predict [K/Fe] ratios as much as 1 dex lower than those inferred from stellar observations. Abundances of potassium are mainly based on analyses of the 7698 $\AA$ resonance line, and the discrepancy between models and observations is in part caused by the LTE assumption. We study the statistical equilibrium of KI, focusing on the non-LTE effects on the $7698 \ \AA$ line. We aim to determine how non-LTE abundances of K can improve the analysis of its chemical evolution, and help to constrain the yields of models. We construct a model atom that employs the most up-to-date data. In particular, we calculate and present inelastic e+K collisional excitation cross-sections from the convergent close-coupling and the $B$-Spline $R$-matrix methods, and H+K collisions from the two-electron model. We constructed a fine grid of non-LTE abundance corrections that span $4000<\teff / \rm{K}<8000$, $0.50<\lgg<5.00$, $-5.00<\feh<+0.50$, and applied the corrections to abundances from the literature. In concordance with previous studies, we find severe non-LTE effects in the $7698 \ \AA$ line, which is stronger in non-LTE with abundance corrections that can reach $\sim-0.7\,\dex$. We explore the effects of atmospheric inhomogeneity by computing a full 3D non-LTE stellar spectrum of KI for a test star. We find that 3D is necessary to predict a correct shape of the resonance 7698 $\AA$ line, but the line strength is similar to that found in 1D non-LTE. Our non-LTE abundance corrections reduce the scatter and change the cosmic trends of literature K abundances. In the regime [Fe/H]$\lesssim-1.0$ the non-LTE abundances show a good agreement with the GCE model with yields from rotating massive stars. The reduced scatter of the non-LTE corrected abundances of a sample of solar twins shows that line-by-line differential analysis techniques cannot fully compensate for systematic modelling errors. ",Non-LTE analysis of K I in late-type stars
43,1142054396525785088,1604541715,"Paul Beck, PhD","['A new exciting paper by Carina Persson and the #KESPRINT consortium reports on ""Greening of the Brown Dwarf Desert"" through the detection of EPIC212036875b. This detection was based on data from the #KeplerK2 mission <LINK>']",https://arxiv.org/abs/1906.05048,"Our aim is to investigate the nature and formation of brown dwarfs by adding a new well-characterised object to the small sample of less than 20 transiting brown dwarfs. One brown dwarf candidate was found by the KESPRINT consortium when searching for exoplanets in the K2 space mission Campaign 16 field. We combined the K2 photometric data with a series of multi-colour photometric observations, imaging and radial velocity measurements to rule out false positive scenarios and to determine the fundamental properties of the system. We report the discovery and characterisation of a transiting brown dwarf in a 5.17 day eccentric orbit around the slightly evolved F7V star EPIC 212036875. We find a stellar mass of 1.15+/-0.08 M$_\odot$, a stellar radius of 1.41+/-0.05 R$_\odot$, and an age of 5.1+/-0.9 Gyr. The mass and radius of the companion brown dwarf are 51+/-2 MJ and 0.83+/-0.03 RJ, respectively, corresponding to a mean density of 108+15-13 g cm-3. EPIC 212036875 b is a rare object that resides in the brown dwarf desert. In the mass-density diagram for planets, brown dwarfs and stars, we find that all giant planets and brown dwarfs follow the same trend from ~0.3 MJ to the turn-over to hydrogen burning stars at ~73 MJ. EPIC 212036875 b falls close to the theoretical model for mature H/He dominated objects in this diagram as determined by interior structure models, as well as the empirical fit. We argue that EPIC 212036875 b formed via gravitational disc instabilities in the outer part of the disc, followed by a quick migration. Orbital tidal circularisation may have started early in its history for a brief period when the brown dwarf's radius was larger. The lack of spin-orbit synchronisation points to a weak stellar dissipation parameter which implies a circularisation timescale of >23 Gyr, or suggests an interaction between the magnetic and tidal forces of the star and the brown dwarf. ","Greening of the Brown Dwarf Desert. EPIC 212036875 b -- a 51
  M$_\mathrm{J}$ object in a 5 day orbit around an F7 V star"
44,1142052468320342017,2303004390,Brandon Amos,"['Excited to share my new tech report from my @IntelAI internship on the Limited Multi-Label projection layer! Joint work with Vladlen Koltun and @zicokolter\n\nPaper: <LINK>\n@PyTorch Code: <LINK> <LINK>', 'We start by motiving our work with other projections in machine learning and reviewing that the ReLU, sigmoid, and softmax layers are just explicit closed-form solutions to convex and constrained optimization problems that project into polytopes. https://t.co/NRxZt27wWp', ""We then propose that projecting onto another polytope, that we call LML polytope, is useful for learning in top-k settings. It doesn't have an explicit closed-form solution but we show that solving and differentiating through this projection operation is easy and tractable. https://t.co/wR4Rju9WyK"", 'Now you can maximize the top-k recall with the LML layer by just posing it as a maximum likelihood problem over the labels that you observe *without* worrying about your model collapsing https://t.co/MPREGGW9Ga', 'We add the LML layer with a few lines of code to existing code for top-k CIFAR-100 classification and scene graph generation and recover or surpass the accuracy of the state-of-the-art models. https://t.co/ykDDP26uWj', 'Notably, we also revive and revisit the truncated top-k entropy loss from Lapin et al. as another reasonable baseline for top-k classification that Berrada, Zisserman, and Kumar did not consider and show how it can be extended to multi-label settings for scene graph generation']",https://arxiv.org/abs/1906.08707,"We propose the Limited Multi-Label (LML) projection layer as a new primitive operation for end-to-end learning systems. The LML layer provides a probabilistic way of modeling multi-label predictions limited to having exactly k labels. We derive efficient forward and backward passes for this layer and show how the layer can be used to optimize the top-k recall for multi-label tasks with incomplete label information. We evaluate LML layers on top-k CIFAR-100 classification and scene graph generation. We show that LML layers add a negligible amount of computational overhead, strictly improve the model's representational capacity, and improve accuracy. We also revisit the truncated top-k entropy method as a competitive baseline for top-k classification. ",The Limited Multi-Label Projection Layer
45,1142047121249570816,1093387119148462081,Daniel Green,"['New paper today: <LINK>.  Great work by Florian, Matteo, Ben and An≈æe!  The main result is a BOSS-only constraint on oscillations from inflation that is stronger than the CMB (Planck).  Galaxy surveys are really powerful when you can use all the data! <LINK>']",https://arxiv.org/abs/1906.08758,"Sharp features in the primordial power spectrum are a powerful window into the inflationary epoch. To date, the cosmic microwave background (CMB) has offered the most sensitive avenue to search for these signatures. In this paper, we demonstrate the power of large-scale structure observations to surpass the CMB as a probe of primordial features. We show that the signatures in galaxy surveys can be separated from the broadband power spectrum and are as robust to the nonlinear evolution of matter as the standard baryon acoustic oscillations. As a result, analyses can exploit a significant range of scales beyond the linear regime available in the datasets. We develop a feature search for large-scale structure, apply it to the final data release of the Baryon Oscillation Spectroscopic Survey and find new bounds on oscillatory features that exceed the sensitivity of Planck for a significant range of frequencies. Moreover, we forecast that the next generation of galaxy surveys, such as DESI and Euclid, will be able to improve current constraints by up to an order of magnitude over an expanded frequency range. ",Primordial Features from Linear to Nonlinear Scales
46,1141701147524079617,601568012,Dr. Michelle Ntampaka,"['My new paper is on arxiv this morning -- cosmological constraints using cluster dynamics and forward modeling.\n\nthe TL;DR is Fig 8:  from dynamics of a galaxy cluster sample, S8 is low.  \n\n<LINK>']",https://arxiv.org/abs/1906.07729v1,"We apply the Velocity Distribution Function (VDF) to a sample of Sunyaev-Zel'dovich (SZ)-selected clusters, and we report preliminary cosmological constraints in the $\sigma_8$-$\Omega_m$ cosmological parameter space. The VDF is a forward-modeled test statistic that can be used to constrain cosmological models directly from galaxy cluster dynamical observations. The method was introduced in Ntampaka et al. (2017) and employs line-of-sight velocity measurements to directly constrain cosmological parameters; it is less sensitive to measurement error than a standard halo mass function approach. The method is applied to the Hectospec Survey of Sunyaev-Zeldovich-Selected Clusters (HeCS-SZ) sample, which is a spectroscopic follow up of a Planck-selected sample of 83 galaxy clusters. Credible regions are calculated by comparing the VDF of the observed cluster sample to that of mock observations, yielding $\mathcal{S}_8 \equiv \sigma_8 \left(\Omega_m/0.3\right)^{0.25} = 0.751\pm0.037$. These constraints are in tension with the Planck Cosmic Microwave Background (CMB) TT fiducial value, which lies outside of our 95% credible region, but are in agreement with some recent analyses of large scale structure that observe fewer massive clusters than are predicted by the Planck fiducial cosmological parameters. ","] Cluster Cosmology with the Velocity Distribution Function of the HeCS-SZ
  Sample"
47,1141425970093494273,36762530,Jesse Vig,"['New #BlackboxNLP paper analyzing attention in GPT-2: <LINK> Some highlights: attention targets different parts of speech at different layer depths, and a simple algorithm reveals highly specialized generation patterns (e.g. acronyms from names). With @boknilev. <LINK>']",https://arxiv.org/abs/1906.04284,"The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads. ",Analyzing the Structure of Attention in a Transformer Language Model
48,1141380479108747265,185910194,Graham Neubig,"['New #acl2019nlp paper on cross-lingual transfer for syntactic tasks (POS tagging, parsing) using not *discriminative* but *generative* models: <LINK>\nUsing generative models allows us to perform unsupervised fine-tuning on the target language for large gains! <LINK>', 'I think this result is a major selling point for generative modeling approaches, and points to the need for more research in this area! These nice results are thanks to @junxian_he, along with Zhisong Zhang and @BergKirkpatrick.']",https://arxiv.org/abs/1906.02656,"Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of source model and target model are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use English as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this dataset that are distant from English, our method yields an average of 5.2% absolute improvement on POS tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models. ","Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of
  Invertible Projections"
49,1141372418163953665,763831084924755968,Kirill Neklyudov,"['Our next paper <LINK> formulates the Metropolis-Hastings algorithm for empirical target distributions and implicit proposals. Check it out for theoretical analysis, implicit Markov proposals, and new loss functions for DRE. With @eeevgen @bayesgroup <LINK>']",http://arxiv.org/abs/1906.03644,"Recent works propose using the discriminator of a GAN to filter out unrealistic samples of the generator. We generalize these ideas by introducing the implicit Metropolis-Hastings algorithm. For any implicit probabilistic model and a target distribution represented by a set of samples, implicit Metropolis-Hastings operates by learning a discriminator to estimate the density-ratio and then generating a chain of samples. Since the approximation of density ratio introduces an error on every step of the chain, it is crucial to analyze the stationary distribution of such chain. For that purpose, we present a theoretical result stating that the discriminator loss upper bounds the total variation distance between the target distribution and the stationary distribution. Finally, we validate the proposed algorithm both for independent and Markov proposals on CIFAR-10 and CelebA datasets. ",The Implicit Metropolis-Hastings Algorithm
50,1141164088514502658,1024926454923112448,Dominik Schleicher,['Our new paper on the origin of eclipsing time variations in close binary system. First ever demonstration with 3D simulations of a possible magneto-hydrodynamical origin: <LINK>'],https://arxiv.org/abs/1906.06787,"Eclipsing time variations have been observed for a wide range of binary systems, including post-common-envelope binaries. A frequently proposed explanation, apart from the possibility of having a third body, is the effect of magnetic activity, which may alter the internal structure of the secondary star, particularly its quadrupole moment, and thereby cause quasi-periodic oscillations. Here we present two compressible non-ideal magneto-hydrodynamical (MHD) simulations of the magnetic dynamo in a solar mass star, one of them with three times the solar rotation rate (""slow rotator""), the other one with twenty times the solar rotation rate (""rapid rotator""), to account for the high rotational velocities in close binary systems. For the slow rotator, we find that both the magnetic field and the stellar quadrupole moment change in a quasi-periodic manner, leading to O-C (observed - corrected times of the eclipse) variations of ~0.025 s. For the rapid rotator, the behavior of the magnetic field as well as the quadrupole moment changes become considerably more complex, due to the less coherent dynamo solution. The resulting O-C variations are of the order 0.13 s. The observed system V471~Tau shows two modes of eclipsing time variations, with amplitudes of 151 s and 20 s, respectively. However, the current simulations may not capture all relevant effects due to the neglect of the centrifugal force and self-gravity. Considering the model limitations and that the rotation of V471 Tau is still a factor of 2.5 faster than our rapid rotator, it may be conceivable to reach the observed magnitudes. ","Magneto-hydrodynamical origin of eclipsing time variations in
  post-common-envelope binaries for solar mass secondaries"
51,1140998803379478530,1140990683303432193,Yichen Jiang,"['Our new @ACL2019_Italy paper: ""Explore, Propose, &amp; Assemble: An Interpretable Model for Multi-Hop RC"" (represents divergent evidence chains as a \'reasoning tree\')\n\n(cc @nitishjoshi23* @YenChunChen4 @mohitban47)\n\nPdf: <LINK>\nCode: <LINK>\n\n#UNCNLP <LINK>']",https://arxiv.org/abs/1906.05210,"Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. To achieve this, we propose an interpretable 3-module system called Explore-Propose-Assemble reader (EPAr). First, the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree. Finally, the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Intuitively, EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage's output. On two multi-hop reading comprehension datasets WikiHop and MedHop, our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system's ability to perform interpretable and accurate reasoning. ","Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop
  Reading Comprehension"
52,1140793766124515328,326843207,Yuta Notsu,"['Our new paper (ApJ in press), which I joined as a collaborator, are now in the arXiv !\n\n""Impact of Stellar Superflares on Planetary Habitability""\nYamashiki et al.  <LINK>', 'We used the Particle and Heavy Ion Transport code System [PHITS] code and evaluate the effects of stellar particle event caused by large superflares in the atmosphere of habitable planets.']",https://arxiv.org/abs/1906.06797,"High-energy radiation caused by exoplanetary space weather events from planet-hosting stars can play a crucial role in conditions promoting or destroying habitability in addition to the conventional factors. In this paper, we present the first quantitative impact evaluation system of stellar flares on the habitability factors with an emphasis on the impact of Stellar Proton Events. We derive the maximum flare energy from stellar starspot sizes and examine the impacts of flare associated ionizing radiation on CO$_2$, H$_2$, N$_2$+O$_2$ --rich atmospheres of a number of well-characterized terrestrial type exoplanets. Our simulations based on the Particle and Heavy Ion Transport code System [PHITS] suggest that the estimated ground level dose for each planet in the case of terrestrial-level atmospheric pressure (1 bar) for each exoplanet does not exceed the critical dose for complex (multi-cellular) life to persist, even for the planetary surface of Proxima Centauri b, Ross-128 b and TRAPPIST-1 e. However, when we take into account the effects of the possible maximum flares from those host stars, the estimated dose reaches fatal levels at the terrestrial lowest atmospheric depth on TRAPPIST-1 e and Ross-128 b. Large fluxes of coronal XUV radiation from active stars induces high atmospheric escape rates from close-in exoplanets suggesting that the atmospheric depth can be substantially smaller than that on the Earth. In a scenario with the atmospheric thickness of 1/10 of Earth's, the radiation dose from close-in planets including Proxima Centauri b and TRAPPIST-1 e reach near fatal dose levels with annual frequency of flare occurrence from their hoststars. ",Impact of Stellar Superflares on Planetary Habitability
53,1140793305569157125,1113856096119197699,Lucas Lamata,['<LINK>\n\nNew paper today! A theory proposal for computing the eigenvalues and eigenvectors of a quantum operator by means of quantum reinforcement learning!'],https://arxiv.org/abs/1906.06702,"The characterization of an operator by its eigenvectors and eigenvalues allows us to know its action over any quantum state. Here, we propose a protocol to obtain an approximation of the eigenvectors of an arbitrary Hermitian quantum operator. This protocol is based on measurement and feedback processes, which characterize a reinforcement learning protocol. Our proposal is composed of two systems, a black box named environment and a quantum state named agent. The role of the environment is to change any quantum state by a unitary matrix $\hat{U}_E=e^{-i\tau\hat{\mathcal{O}}_E}$ where $\hat{\mathcal{O}}_E$ is a Hermitian operator, and $\tau$ is a real parameter. The agent is a quantum state which adapts to some eigenvector of $\hat{\mathcal{O}}_E$ by repeated interactions with the environment, feedback process, and semi-random rotations. With this proposal, we can obtain an approximation of the eigenvectors of a random qubit operator with average fidelity over 90\% in less than 10 iterations, and surpass 98\% in less than 300 iterations. Moreover, for the two-qubit cases, the four eigenvectors are obtained with fidelities above 89\% in 8000 iterations for a random operator, and fidelities of $99\%$ for an operator with the Bell states as eigenvectors. This protocol can be useful to implement semi-autonomous quantum devices which should be capable of extracting information and deciding with minimal resources and without human intervention. ","Reinforcement learning for semi-autonomous approximate quantum
  eigensolver"
54,1140793150996275200,977614647426625536,Stephen Baek,['Check out our new paper on Body Shape vs. Income relationship. We present a more accurate quantification of the fact that our market is unfairly biased. <LINK>'],https://arxiv.org/abs/1906.06747,We study the association between physical appearance and family income using a novel data which has 3-dimensional body scans to mitigate the issue of reporting errors and measurement errors observed in most previous studies. We apply machine learning to obtain intrinsic features consisting of human body and take into account a possible issue of endogenous body shapes. The estimation results show that there is a significant relationship between physical appearance and family income and the associations are different across the gender. This supports the hypothesis on the physical attractiveness premium and its heterogeneity across the gender. ,"Shape Matters: Evidence from Machine Learning on Body Shape-Income
  Relationship"
55,1140697144397090816,216729597,Marcel S. Pawlowski,"['New paper on the arXiv today. Chen Chris Gong, a student of @satellitegalaxy, looked further into how satellite systems of paired host halos become lopsided in ŒõCDM simulations.\n<LINK> <LINK>', 'Some backstory: a few years ago I was visiting @satellitegalaxy at @AIP_Potsdam and we discussed things related to phase-space correlations that we could look at in observed satellite systems.', 'Noam then wrote a paper reporting that satellite systems of host galaxy pairs in SDSS are significantly lopsided: in projection and after stacking many such pairs there is an ~8% excess of satellite galaxies in the direction facing the partner host. \nhttps://t.co/cGanR0DozK https://t.co/nNCsK32d9a', 'I then wondered how this compares to cosmological expectations. So I checked for the effect in several ŒõCDM simulations, with @jbprime and Rodrigo Ibata. We found that the satellite systems of simulated host galaxy pairs show a comparable lopsidedness.\nhttps://t.co/Vq3mEHkIot https://t.co/C3xDiYl6Ov', ""What we didn't do is look into how this happens. That's where the new paper picks up. Chris identifies lopsided satellite distributions at z=0, and then traces the satellites back in time."", 'Turns out the satellites found at z=0 were significantly more lopsided in the past, and this initial lopsidedness gets diluted by satellites interacting with the hosts, i.e. having fly-by encounters. So the lopsided signal seen at z=0 is mostly driven by satellites on 1st infall. https://t.co/49MuvbkQjT', ""It is fun to see how we progress in understanding the effect better, and nice to be part of this, especially now that I'm working at the same institute, only a few doors down from the meeting room where we first bounced these ideas around.""]",https://arxiv.org/abs/1906.06128v1,"It is well known that satellite galaxies are not isotropically distributed among their host galaxies as suggested by most interpretations of the $\Lambda$CDM model. One type of anisotropy recently detected in the SDSS (and seen when examining the distribution of satellites in the Local Group and in the Centaurus group) is a tendency to be so-called ""lopsided"". Namely, in pairs of galaxies (like Andromeda and the Milky Way) the satellites are more likely to inhabit the region in between the pair, rather than on opposing sides. Although recent studies found a similar set up when comparing pairs of galaxies in $\Lambda$CDM simulations indicating that such a set up is not inconsistent with $\Lambda$CDM, the origin has yet to be explained. Here we examine the origin of such lopsided setups by first identifying such distributions in pairs of galaxies in numerical cosmological simulations, and then tracking back the orbital trajectories of satellites (which at $z=0$ display the effect). We report two main results: first, the lopsided distribution was stronger in the past and weakens towards $z=0$. Second, the weakening of the signal is due to the interaction of satellite galaxies with the pair. Finally, we show that the $z=0$ signal is driven primarily by satellites that are on first approach, who have yet to experience a ""flyby"". This suggests that the signal seen in the observations is also dominated by dynamically young accretion events. ",] The Origin of Lopsided Satellite Galaxy Distribution in Galaxy Pairs
56,1140649942505013249,1698006024,niki parmar,"['New Paper:\nStand-Alone Self-Attention in Vision Models  \n<LINK>\n\nCan attention work as a stand-alone primitive for vision models? \nWe develop a pure self-attention model by replacing the spatial convolutions in a ResNet by a simple, local self-attention layer. <LINK>', 'This model outperforms the baseline on ImageNet classification with fewer parameters and FLOPS. On COCO object detection, it matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters.', 'Further studies show that self-attention is the most useful in later layers while convolutions better capture lower-level features. Combining their will be an interesting research direction.', 'Work done with Prajit Ramachandran, @ashVaswani , @IrwanBello , @anselmlevskaya , Jonathon Shlens.', 'Content-based interactions prove useful again and hoping to see more of attention for vision in the future!', '@Veqtor We will release the code in the coming few weeks.']",https://arxiv.org/abs/1906.05909,"Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox. ",Stand-Alone Self-Attention in Vision Models
57,1140559642663239680,259050097,Namhoon Lee,"['1/4 A Signal Propagation Perspective for Pruning Neural Networks at Initialization: our new work on pruning neural networks at initialization is available now.\npaper: <LINK>\nwith @tha_ajanthan, Stephen Gould, Philip Torr.', '2/4 In SNIP (ICLR19), we showed that pruning can be done on a randomly initialized network without pretraining: ""pruning at initialization"". However, it\'s unclear exactly why pruning untrained networks is effective, how it should be understood, whether it can be extended further.', '3/4 This work provides a signal propagation perspective based on dynamical isometry and mean field theory to pruning at initialization. We improve performance by orthogonal initialization, present unsupervised pruning, and bring forth the notion of neural architecture sculpting.', '4/4 We are really excited about what this work could possibly bring us even more, so please stay tuned for our future endeavours!', '@_onionesque Thanks :))) Yeah I realized he had SNIPER paper a little while after I submitted SNIP; and of course I had chat with him about it :)', '@_onionesque Maybe I should name my future work SNIPER, aiming at the lottery ticket lol', '@_onionesque preempted here thanks to your help lol']",https://arxiv.org/abs/1906.06307,"Network pruning is a promising avenue for compressing deep neural networks. A typical approach to pruning starts by training a model and then removing redundant parameters while minimizing the impact on what is learned. Alternatively, a recent approach shows that pruning can be done at initialization prior to training, based on a saliency criterion called connection sensitivity. However, it remains unclear exactly why pruning an untrained, randomly initialized neural network is effective. In this work, by noting connection sensitivity as a form of gradient, we formally characterize initialization conditions to ensure reliable connection sensitivity measurements, which in turn yields effective pruning results. Moreover, we analyze the signal propagation properties of the resulting pruned networks and introduce a simple, data-free method to improve their trainability. Our modifications to the existing pruning at initialization method lead to improved results on all tested network models for image classification tasks. Furthermore, we empirically study the effect of supervision for pruning and demonstrate that our signal propagation perspective, combined with unsupervised pruning, can be useful in various scenarios where pruning is applied to non-standard arbitrarily-designed architectures. ","A Signal Propagation Perspective for Pruning Neural Networks at
  Initialization"
58,1140426802155929601,360083962,Nathan Lundblad,['<LINK>\nNew paper on the arxiv- modeling and plans for ISS-based ultracold bubble experiments with NASA CAL'],https://arxiv.org/abs/1906.05885,"Extending the understanding of Bose-Einstein condensate (BEC) physics to new geometries and topologies has a long and varied history in ultracold atomic physics. One such new geometry is that of a bubble, where a condensate would be confined to the surface of an ellipsoidal shell. Study of this geometry would give insight into new collective modes, self-interference effects, topology-dependent vortex behavior, dimensionality crossovers from thick to thin shells, and the properties of condensates pushed into the ultradilute limit. Here we discuss a proposal to implement a realistic experimental framework for generating shell-geometry BEC using radiofrequency dressing of magnetically-trapped samples. Such a tantalizing state of matter is inaccessible terrestrially due to the distorting effect of gravity on experimentally-feasible shell potentials. The debut of an orbital BEC machine (NASA Cold Atom Laboratory, aboard the International Space Station) has enabled the operation of quantum-gas experiments in a regime of perpetual freefall, and thus has permitted the planning of microgravity shell-geometry BEC experiments. We discuss specific experimental configurations, applicable inhomogeneities and other experimental challenges, and outline potential experiments. ",Shell potentials for microgravity Bose-Einstein condensates
59,1139556040666750978,843188676834164737,Yevgen Chebotar üá∫üá¶,"['Our new work on performing meta-learning using learned loss functions! \nAlso visit our talk and poster at Multi-Task and Lifelong Reinforcement Learning Workshop at ICML tomorrow!\nPaper: <LINK>\nw/ @amolchanov86, S. Bechtle, @ludo_righetti, @_kainoa_, G. Sukhatme <LINK>']",https://arxiv.org/abs/1906.05374,"Typically, loss functions, regularization mechanisms and other important aspects of training parametric models are chosen heuristically from a limited set of options. In this paper, we take the first step towards automating this process, with the view of producing models which train faster and more robustly. Concretely, we present a meta-learning method for learning parametric loss functions that can generalize across different tasks and model architectures. We develop a pipeline for meta-training such loss functions, targeted at maximizing the performance of the model trained under them. The loss landscape produced by our learned losses significantly improves upon the original task-specific losses in both supervised and reinforcement learning tasks. Furthermore, we show that our meta-learning framework is flexible enough to incorporate additional information at meta-train time. This information shapes the learned loss function such that the environment does not need to provide this information during meta-test time. We make our code available at this https URL ",Meta-Learning via Learned Loss
60,1139477376864260097,892059194240532480,Mikel Artetxe,"['1/4 New @ACL2019_Italy paper by our awesome student @aormazabalo on the limitations of cross-lingual word embedding mappings (w/ @glabaka, @Aitor57, @eagirre &amp; myself) <LINK>\n\nThread üëá <LINK>', ""2/4 It was shown that the isomorphism assumption in cross-lingual embeddings doesn't fully hold. But is this a consequence of aligning separately trained embeddings (so an inherent limitation of mapping methods)? Or a more general issue caused by divergences across languages?"", '3/4 We try to answer this question by comparing mapping to joint learning on parallel corpora. In these ideal conditions, joint learning yields more isomorphic embeddings, is less sensitive to hubness, and better at bilingual lexicon induction, especially for distant languages.', '4/4 Mapping methods still have the advantage of requiring less (or no) supervision, but this shows that they also have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.']",https://arxiv.org/abs/1906.05407,"Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal. ",Analyzing the Limitations of Cross-lingual Word Embedding Mappings
61,1139464723672551424,1138704090106646528,Aitor Ormazabal,"['Check out our new @ACL2019_Italy paper ""Analyzing the Limitations of Cross-lingual Word Embedding Mappings"" (w/ @artetxem , @glabaka @Aitor57 and @eagirre). We show that joint learning &gt;&gt; offline mapping for cross-lingual embeddings on parallel data.\n<LINK>']",http://arxiv.org/abs/1906.05407,"Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal. ",Analyzing the Limitations of Cross-lingual Word Embedding Mappings
62,1139382993414307841,141960494,Tudor David,"['New paper with colleagues at @IBMResearch, in which we introduce Mir, a BFT protocol that maintains high throughput  (23K+ tps) with up to 100 geo-distributed replicas: <LINK>. Particularly relevant for #blockchain #consensus']",https://arxiv.org/abs/1906.05552,"This paper presents Mir-BFT, a robust Byzantine fault-tolerant (BFT) total order broadcast protocol aimed at maximizing throughput on wide-area networks (WANs), targeting deployments in decentralized networks, such as permissioned and Proof-of-Stake permissionless blockchain systems. Mir-BFT is the first BFT protocol that allows multiple leaders to propose request batches independently (i.e., parallel leaders), in a way that precludes request duplication attacks by malicious (Byzantine) clients, by rotating the assignment of a partitioned request hash space to leaders. As this mechanism removes a single-leader bandwidth bottleneck and exposes a computation bottleneck related to authenticating clients even on a WAN, our protocol further boosts throughput using a client signature verification sharding optimization. Our evaluation shows that Mir-BFT outperforms state-of-the-art and orders more than 60000 signed Bitcoin-sized (500-byte) transactions per second on a widely distributed 100 nodes, 1 Gbps WAN setup, with typical latencies of few seconds. We also evaluate Mir-BFT under different crash and Byzantine faults, demonstrating its performance robustness. Mir-BFT relies on classical BFT protocol constructs, which simplifies reasoning about its correctness. Specifically, Mir-BFT is a generalization of the celebrated and scrutinized PBFT protocol. In a nutshell, Mir-BFT follows PBFT ""safety-wise"", with changes needed to accommodate novel features restricted to PBFT liveness. ",Mir-BFT: High-Throughput Robust BFT for Decentralized Networks
63,1139245840260710400,40285266,Stanislav Fort at EAGx Prague ¬¨(üî•üìéüî•üìé),"[""I'll be presenting our new paper (w/ my great coauthor @kudkudakpl) on the Large Scale Structure of Neural Network Loss Landscapes (<LINK> ) at the #ICML2019 Physics and Deep Learning workshop. Come around to see what the loss landscape looks like in high dims! <LINK>""]",http://arxiv.org/abs/1906.04724,"There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional \emph{wedges} that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and $L_2$ regularization, affect the path optimizer takes through the landscape in a similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model. ",Large Scale Structure of Neural Network Loss Landscapes
64,1139240969738452992,16003634,Santiago Castro,"['New paper accepted at @ACL2019_Italy in collaboration with @hdevamanyu (first co-author along with me), @vperez_r, Roger Zimmermann, @radamihalcea and @soujanyaporia:\n\n""Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)""\n\n<LINK> #NLProc <LINK>', ""It's a balanced corpus with 690 5-second-length video clips in English, marked as either sarcastic or non-sarcastic + ~15s of previous video context + transcriptions.\n\nThe dataset is mostly composed of @bigbangtheory and @FriendsTV video clips. https://t.co/BAZEGAEa6m"", ""We called it MUStARD üå≠üòã, and it's available here (+ code): https://t.co/4OZskIvNTd"", ""We have a lot from Sheldon and Chandler, and you know they are quite sarcastic,even though Sheldon doesn't know what it is!üòÇThey have different styles clearly, btw\n\nYou may also know Dorothy, from Golden Girls (I didn't before). She's also pretty sarcastic and it's here as well! https://t.co/AKdtPucZjC"", ""Here's an example sarcastic instance from the dataset. https://t.co/g4BJU6vE3e"", 'We ran some simple baselines and found out that audio in English plays an important role when trying to identify sarcasm for unknown speakers. If you add text is slightly better for our simple baselines. https://t.co/UbjtlThF4B', 'This work was done in a collaboration between @UMich, @NUSingapore and @sutdsg']",https://arxiv.org/abs/1906.01815,"Sarcasm is often expressed through several verbal and non-verbal cues, e.g., a change of tone, overemphasis in a word, a drawn-out syllable, or a straight looking face. Most of the recent work in sarcasm detection has been carried out on textual data. In this paper, we argue that incorporating multimodal cues can improve the automatic classification of sarcasm. As a first step towards enabling the development of multimodal approaches for sarcasm detection, we propose a new sarcasm dataset, Multimodal Sarcasm Detection Dataset (MUStARD), compiled from popular TV shows. MUStARD consists of audiovisual utterances annotated with sarcasm labels. Each utterance is accompanied by its context of historical utterances in the dialogue, which provides additional information on the scenario where the utterance occurs. Our initial results show that the use of multimodal information can reduce the relative error rate of sarcasm detection by up to 12.9% in F-score when compared to the use of individual modalities. The full dataset is publicly available for use at this https URL ",Towards Multimodal Sarcasm Detection (An _Obviously_ Perfect Paper)
65,1139232020519768064,950189417754513409,Charles üéâ Frye,"['1/9 New short paper up on arXiV: ""Critical Point Finding with Newton-MR by Analogy to Computing Square Roots""\n\n<LINK>\n\ntl;dr: my PhD research is harder than calculating the square root of pi, but only just barely.', '2/9 Critical points are points where gradients are 0. These are of interest for optimization, because they are the points towards which algorithms like gradient descent are attracted. Finding critical points of optimization problems helps us understand them better.', '3/9 Points that cause equations to be 0 are called ""roots"", and the ""square root"" of a number x is just one example: it makes sqrt(x)*sqrt(x) - x = 0. The classic algorithm for solving this is called ""Newton-Raphson"". But the version of it for square roots dates back to 60 AD!', '4/9 When you try to implement a version of Newton-Raphson for a nonlinear optimization problem, like a neural network, you run into problems: you end up needing to divide by a certain matrix, and this is _hard_.', '5/9 The trick is to turn the problem from ""y is equal x divided by this matrix"" into ""this matrix times y is equal to x"" and then _optimize over y_. When I first came across methods that did this, I was very weirded out. Shouldn\'t division just be, you know, division?', '6/n But here\'s the thing: this trick is used to compute square roots with Newton-Raphson! There, you divide numbers, and when the numbers involved get very long, the fastest way to divide them is to turn ""y equals x divided by m"" into ""m times y equals x"" and _optimize over y_!', '7/9 So this ""one weird trick"" isn\'t so weird after all. Or maybe it is weird, but it\'s not exotic. It\'s as plain vanilla as a button on a TI-89!', ""8/9 We need one more thing to make Newton-MR, the algorithm I like to use. Once our optimization over y gives us an answer, we need to scale it (in ML, this is called a learning rate). Fixed scaling doesn't work, so we have to pick a good value on each step. And how do we do it?"", '9/9 Optimization strikes again! So in summary: we want to understand an optimization problem, so we solve an optimization problem. On each step, we have to solve two optimization problems, one after the other. And this all bears a striking resemblance to computing square roots!']",https://arxiv.org/abs/1906.05273,"Understanding of the behavior of algorithms for resolving the optimization problem (hereafter shortened to OP) of optimizing a differentiable loss function (OP1), is enhanced by knowledge of the critical points of that loss function, i.e. the points where the gradient is 0. Here, we describe a solution to the problem of finding critical points by proposing and solving three optimization problems: 1) minimizing the norm of the gradient (OP2), 2) minimizing the difference between the pre-conditioned update direction and the gradient (OP3), and 3) minimizing the norm of the gradient along the update direction (OP4). The result is a recently-introduced algorithm for optimizing invex functions, Newton-MR, which turns out to be highly effective at the problem of finding the critical points of the loss surfaces of neural networks. We precede this derivation with an analogous, but simpler, derivation of the nested-optimization algorithm for computing square roots by combining Heron's Method with Newton-Raphson division. ","Critical Point Finding with Newton-MR by Analogy to Computing Square
  Roots"
66,1139182873569845248,341126513,Francesca Fragkoudi,['New paper on arXiv today! Get it hot off the press and learn about how to identify bar resonances in Gaia DR2 action space! ü§©üåå <LINK>'],https://arxiv.org/abs/1906.04786,"Action space synthesizes the orbital information of stars and is well-suited to analyse the rich kinematic substructure of the disc in the \emph{Gaia} DR2 radial velocity sample (RVS). We revisit the strong perturbation induced in the Milky Way (MW) disc by an $m=2$ bar, using test particle simulations and the actions $(J_R,L_z,J_z)$ estimated in an axisymmetric potential. These make three useful diagnostics cleanly visible. (1.) We use the well-known characteristic flip from outward to inward motion at the Outer Lindblad Resonance (OLR, $l=+1,m=2$), which occurs along the axisymmetric resonance line (ARL) in $(L_z,J_R)$, to identify in the \emph{Gaia} action data three candidates for the bar's OLR and pattern speed $\Omega_\text{bar}$: $1.85\Omega_0$, $1.20\Omega_0$, and $1.63\Omega_0$ (with $\sim0.1\Omega_0$ systematic uncertainty). The \emph{Gaia} data is therefore consistent with both slow and fast bar models in the literature, but disagrees with recent measurements of $\sim1.45\Omega_0$. (2.) For the first time, we demonstrate that bar resonances -- especially the OLR -- cause a gradient in vertical action $\langle J_z \rangle$ with $L_z$ around the ARL via ""$J_z$-sorting"" of stars. This could contribute to the observed coupling of $\langle v_R \rangle$ and $\langle | v_z | \rangle$ in the Galactic disc. (3.) We confirm prior results that the behaviour of resonant orbits is well approximated by scattering and oscillation in $(L_z,J_R)$ along a slope $\Delta J_R/\Delta L_z = l/m$ centered on the $l$:$m$ ARL. Overall, we demonstrate that axisymmetrically estimated actions are a powerful diagnostic tool even in non-axisymmetric systems. ","Identifying resonances of the Galactic bar in Gaia DR2: I. Clues from
  action space"
67,1139170768749596672,747724782842548224,George J. Pappas,['Lipschitz constants for deep neural networks are an important measure of DNN sensitivity and robustness.  Our new paper shows how to compute them accurately and efficiently: <LINK> @arXiv_Daily'],https://arxiv.org/abs/1906.04893,"Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees. ","Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural
  Networks"
68,1138912024954621959,865254426101067778,Mohit Iyyer,['our new #acl2019nlp paper (<LINK>) improves decoding speed and BLEU of non-autoregressive neural MT by first predicting a syntactic chunk sequence before producing the target tokens. first paper from my student @dojoteef! all code at <LINK>'],https://arxiv.org/abs/1906.02780,"Standard decoders for neural machine translation autoregressively generate a single target token per time step, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi- autoregressive decoding produce multiple tokens per time step independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences ~ 5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets. ","Syntactically Supervised Transformers for Faster Neural Machine
  Translation"
69,1138872528569548807,72781449,Nikos Aletras,['Also a new paper w/ Daniel Preotiuc @daniel_preotiuc and Mihaela Gaman to appear at ACL. We introduce a systematic analysis of complaints in comput. linguistics and test models on identifying tweets expressing complaints written in EN <LINK> #nlproc'],https://arxiv.org/abs/1906.03890,"Complaining is a basic speech act regularly used in human and computer mediated communication to express a negative mismatch between reality and expectations in a particular situation. Automatically identifying complaints in social media is of utmost importance for organizations or brands to improve the customer experience or in developing dialogue systems for handling and responding to complaints. In this paper, we introduce the first systematic analysis of complaints in computational linguistics. We collect a new annotated data set of written complaints expressed in English on Twitter.\footnote{Data and code is available here: \url{this https URL}} We present an extensive linguistic analysis of complaining as a speech act in social media and train strong feature-based and neural models of complaints across nine domains achieving a predictive performance of up to 79 F1 using distant supervision. ",Automatically Identifying Complaints in Social Media
70,1138871422053376000,537861776,Kevin Clark,"['Check out our new #BlackboxNLP paper ""What Does BERT Look At? An Analysis of BERT\'s Attention"" with @ukhndlwl @omerlevy @chrmanning! <LINK> Among other things, we show that BERT\'s attention corresponds surprisingly well to aspects of syntax and coreference. <LINK>']",https://arxiv.org/abs/1906.04341,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention. ",What Does BERT Look At? An Analysis of BERT's Attention
71,1138870130606256128,72781449,Nikos Aletras,['New paper w/ Ilias Chalkidis @KiddoThe2B and Ion Androutsopoulos @ionandrou to appear at ACL. We evaluate neural models on legal judgment prediction tasks (in English). We also test if models are biased towards demographic info <LINK> #nlproc #legaltech #law'],https://arxiv.org/abs/1906.02059v1,"Legal judgment prediction is the task of automatically predicting the outcome of a court case, given a text describing the case's facts. Previous work on using neural models for this task has focused on Chinese; only feature-based models (e.g., using bags of words and topics) have been considered in English. We release a new English legal judgment prediction dataset, containing cases from the European Court of Human Rights. We evaluate a broad variety of neural models on the new dataset, establishing strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT's length limitation. ",] Neural Legal Judgment Prediction in English
72,1138820976051990528,36995017,ulas bagci,"['Please read our recent paper (MICCAI2019) for generic medical image segmentation w/ adversarial network, and in particular for pancreas segmentation, new state of the art (&gt;85%) with an elegant method  #AI #DL @KhosravanNaji  @AAMortazi @mikewallacemd <LINK>']",https://arxiv.org/abs/1906.04378,"Adversarial learning has been proven to be effective for capturing long-range and high-level label consistencies in semantic segmentation. Unique to medical imaging, capturing 3D semantics in an effective yet computationally efficient way remains an open problem. In this study, we address this computational burden by proposing a novel projective adversarial network, called PAN, which incorporates high-level 3D information through 2D projections. Furthermore, we introduce an attention module into our framework that helps for a selective integration of global information directly from our segmentor to our adversarial network. For the clinical application we chose pancreas segmentation from CT scans. Our proposed framework achieved state-of-the-art performance without adding to the complexity of the segmentor. ",PAN: Projective Adversarial Network for Medical Image Segmentation
73,1138819369507598341,40285266,Stanislav Fort at EAGx Prague ¬¨(üî•üìéüî•üìé),"[""I'm excited to present our new paper (w/ my great coauthor @kudkudakpl) on the Large Scale Structure of Neural Network Loss Landscapes (<LINK>) at the #ICML2019 Understanding and Improving Generalization in Deep Learning workshop Friday 2.30 Grand Ballroom A <LINK>""]",http://arxiv.org/abs/1906.04724,"There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional \emph{wedges} that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and $L_2$ regularization, affect the path optimizer takes through the landscape in a similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model. ",Large Scale Structure of Neural Network Loss Landscapes
74,1138817919230578690,2561665507,Dr. Daniella Bardalez Gagliuffi,['Check out our new paper on the 25pc sample of M7-L5 dwarfs! <LINK> @browndwarfs @jfaherty @kellecruz @sjs917 @philosicist @jgagneastro and more!'],https://arxiv.org/abs/1906.04166,"We present a volume-limited, spectroscopically-verified sample of M7$-$L5 ultracool dwarfs within 25\,pc. The sample contains 410 sources, of which $93\%$ have trigonometric distance measurements ($80\%$ from \textit{Gaia} DR2), and $81\%$ have low-resolution ($R\sim120$), near-infrared (NIR) spectroscopy. We also present an additional list of 60 sources which may be M7$-$L5 dwarfs within 25\,pc when distance or spectral type uncertainties are taken into account. The spectra provide NIR spectral and gravity classifications, and we use these to identify young sources, red and blue $J-K_S$ color outliers, and spectral binaries. We measure very low gravity and intermediate gravity fractions of $2.1^{+0.9}_{-0.8}\%$ and $7.8^{+1.7}_{-1.5}\%$, respectively; fractions of red and blue color outliers of $1.4^{+0.6}_{-0.5}$\% and $3.6^{+1.0}_{-0.9}$\%, respectively; and a spectral binary fraction of $1.6^{+0.5}_{-0.5}\%$. We present an updated luminosity function for M7$-$L5 dwarfs continuous across the hydrogen burning limit that agrees with previous studies. We estimate our completeness to range between $69-80\%$ when compared to an isotropic model. However, we find that the literature late-M sample is severely incomplete compared to L dwarfs, with completeness of $62^{+8}_{-7}\%$ and $83^{+10}_{-9}\%$, respectively. This incompleteness can be addressed with astrometric-based searches of ultracool dwarfs with \textit{Gaia} to identify objects previously missed by color- and magnitude-limited surveys. ","The Ultracool SpeXtroscopic Survey. I. Volume-Limited Spectroscopic
  Sample and Luminosity Function of M7$-$L5 Ultracool Dwarfs"
75,1138760461153755136,923132721760649216,kosukekurosawa,['Our paper has been accepted for publication in Geophysical Research Letters. We developed a new experimental method for 2-stage H2 gas guns. Our method allows us to do in-situ  analysis of impact-generated vapor with a small risk of chemical contamination. <LINK>'],https://arxiv.org/abs/1906.03913,"Dry lakebeds might constitute large volatile reservoirs on Mars. Hypervelocity impacts onto ancient dry lakebeds would have affected the volatile distribution on Mars. We developed a new experimental method to investigate the response of evaporitic minerals (halite and gypsum) to impact shocks in an open system. This technique does not result in chemical contamination from the operation of the gas gun. The technique is termed the two-valve method and the gun system is located in the Planetary Exploration Research Center, Chiba Institute of Technology, Japan. We detected the vaporization of halite at 31 GPa and devolatilization from gypsum at 11 GPa, suggesting that impact-induced volatile release from dry lakebeds has periodically occurred throughout Martian history. The vaporization of halite deposits might have enhanced the production of perchlorates, which are found globally on Mars. The water loss from gypsum possibly explains the coexisting types of Ca-sulfates found in Gale Crater. ","Shock vaporization/devolatilization of evaporitic minerals, halite and
  gypsum, in an open system investigated by a two-stage light gas gun"
76,1138736449820475392,216729597,Marcel S. Pawlowski,"['New paper on the arXiv by Sean Fillingham, one of the amazing grad students of @UCIPhysAstro. I had the honor to be a part of it together with @cooperUCI @AstronoMouse_ @MBKplus @jbprime @SheaGKosmo and @coralrosew. <LINK> <LINK>', 'Sean used Gaia DR2 proper motions for the Milky Way satellite galaxies to constrain their infall times, by matching them with sub-halos in the Phat ELVIS simulations that have similar binding energies and distances from their host. https://t.co/2q0zh9tayi', 'He compares the infall times of the satellite galaxies with their quenching times from star formation histories, and calculates the quenching timescale, i.e. for how long after their infall do the satellites continue to form stars.', 'Turns out most classical satellites (M* ‚â• 10^5 Msun) are quenched quickly after infall, consistent with environmental quenching (e.g. ram-pressure stripping). The least massive satellites, in contrast, were quenched *before* infall, in line with quenching due to reionisation. https://t.co/tHxvKDWaXV', 'Looking at the orbits of sats with M*‚â•10^5 Msun, 2 of the 3 with the longest quenching timescales have the largest percenters and low eccentricities. Thus they have likely experienced the lowest ram-pressure stripping, letting them keep gas &amp; form stars for longer after infall. https://t.co/QT4HqZVNIk']",https://arxiv.org/abs/1906.04180v1,"Observations of low-mass satellite galaxies in the nearby Universe point towards a strong dichotomy in their star-forming properties relative to systems with similar mass in the field. Specifically, satellite galaxies are preferentially gas poor and no longer forming stars, while their field counterparts are largely gas rich and actively forming stars. Much of the recent work to understand this dichotomy has been statistical in nature, determining not just that environmental processes are most likely responsible for quenching these low-mass systems but also that they must operate very quickly after infall onto the host system, with quenching timescales $\lesssim 2~ {\rm Gyr}$ at ${M}_{\star} \lesssim 10^{8}~{\rm M}_{\odot}$. This work utilizes the newly-available $Gaia$ DR2 proper motion measurements along with the Phat ELVIS suite of high-resolution, cosmological, zoom-in simulations to study low-mass satellite quenching around the Milky Way on an object-by-object basis. We derive constraints on the infall times for $37$ of the known low-mass satellite galaxies of the Milky Way, finding that $\gtrsim~70\%$ of the `classical' satellites of the Milky Way are consistent with the very short quenching timescales inferred from the total population in previous works. The remaining classical Milky Way satellites have quenching timescales noticeably longer, with $\tau_{\rm quench} \sim 6 - 8~{\rm Gyr}$, highlighting how detailed orbital modeling is likely necessary to understand the specifics of environmental quenching for individual satellite galaxies. Additionally, we find that the $6$ ultra-faint dwarf galaxies with publicly available $HST$-based star-formation histories are all consistent with having their star formation shut down prior to infall onto the Milky Way -- which, combined with their very early quenching times, strongly favors quenching driven by reionization. ","] Characterizing the Infall Times and Quenching Timescales of Milky Way
  Satellites with $Gaia$ Proper Motions"
77,1138661826265198592,1359972247,Jay Thiagarajan,"['Our new paper studies the effect of sample design on generalization of ML models, through the lens of spectral sampling theory. an we do better than uniform random sampling? exciting work with @bkailkhu\n\nLink: <LINK>']",https://arxiv.org/abs/1906.02732,"This paper provides a general framework to study the effect of sampling properties of training data on the generalization error of the learned machine learning (ML) models. Specifically, we propose a new spectral analysis of the generalization error, expressed in terms of the power spectra of the sampling pattern and the function involved. The framework is build in the Euclidean space using Fourier analysis and establishes a connection between some high dimensional geometric objects and optimal spectral form of different state-of-the-art sampling patterns. Subsequently, we estimate the expected error bounds and convergence rate of different state-of-the-art sampling patterns, as the number of samples and dimensions increase. We make several observations about generalization error which are valid irrespective of the approximation scheme (or learning architecture) and training (or optimization) algorithms. Our result also sheds light on ways to formulate design principles for constructing optimal sampling methods for particular problems. ","A Look at the Effect of Sample Design on Generalization through the Lens
  of Spectral Analysis"
78,1138548867400708096,1112663133355737088,Penny Mealy,['Worried about the the potential impact of automation on your job? The automation risk to other jobs you can switch to also matters. New @INETOxford working paper out led by @RitaMaria_dRC on Automation and Occupational Mobility: <LINK> <LINK>'],http://arxiv.org/abs/1906.04086,"The potential impact of automation on the labor market is a topic that has generated significant interest and concern amongst scholars, policymakers, and the broader public. A number of studies have estimated occupation-specific risk profiles by examining the automatability of associated skills and tasks. However, relatively little work has sought to take a more holistic view on the process of labor reallocation and how employment prospects are impacted as displaced workers transition into new jobs. In this paper, we develop a new data-driven model to analyze how workers move through an empirically derived occupational mobility network in response to automation scenarios which increase labor demand for some occupations and decrease it for others. At the macro level, our model reproduces a key stylized fact in the labor market known as the Beveridge curve and provides new insights for explaining the curve's counter-clockwise cyclicality. At the micro level, our model provides occupation-specific estimates of changes in short and long-term unemployment corresponding to a given automation shock. We find that the network structure plays an important role in determining unemployment levels, with occupations in particular areas of the network having very few job transition opportunities. Such insights could be fruitfully applied to help design more efficient and effective policies aimed at helping workers adapt to the changing nature of the labor market. ",Automation and occupational mobility: A data-driven network model
79,1138535615186817027,750411947661811712,Dr. Sarah Pearson,"['My new paper is out today! With WFIRST (@NASAWFIRST), we should be able to detect gaps in streams in external galaxies out distances of at least 3.5 Mpc. Check out the paper here: <LINK> <LINK>']",https://arxiv.org/abs/1906.03264,"The morphology of thin stellar streams can be used to test the nature of dark matter. It is therefore crucial to extend searches for globular cluster streams to other galaxies than the Milky Way. In this paper, we investigate the current and future prospects of detecting globular cluster streams in external galaxies in resolved stars (e.g. with WFIRST) and using integrated light (e.g. with HSC, LSST and Euclid). In particular, we inject mock-streams to data from the PAndAS M31 survey, and produce simulated M31 backgrounds mimicking what WFIRST will observe in M31. Additionally, we estimate the distance limit to which globular cluster streams will be observable. Our results demonstrate that for a 1 hour (1000 sec.) exposure, using conservative estimates, WFIRST should detect globular cluster streams in resolved stars in galaxies out to distances of ~3.5 Mpc (~2 Mpc). This volume contains 199 (122) galaxies of which >90% are dwarfs. With integrated light, thin streams can be resolved out to ~100 Mpc with HSC and LSST and to ~600 Mpc with WFIRST and Euclid. The low surface brightness of the streams (typically >30 mag/arcsec$^2$), however, will make them difficult to detect, unless the streams originate from very young clusters. We emphasize that if the external galaxies do not host spiral arms or galactic bars, gaps in their stellar streams provide an ideal test case for evidence of interactions with dark matter subhalos. Furthermore, obtaining a large samples of thin stellar streams can help constrain the orbital structure and hence the potentials of external halos. ","Detecting Thin Stellar Streams in External Galaxies: Resolved Stars &
  Integrated Light"
80,1138466456998481922,976155561522794497,Juliano C√©sar Silva Neves,['My new paper says that regular black holes (black holes without a singularity) could have a different diet... #BlackHole #Singularity \n<LINK>'],https://arxiv.org/abs/1906.03718,"We consider the stationary spherical accretion process of perfect fluids onto a class of spherically symmetric regular black holes corresponding to quantum-corrected Schwarzschild spacetimes. We show that the accretion rates can differ from the Schwarzschild case, suggesting that the de Sitter core inside these regular black holes, which indeed precludes the central singularity, can act for some cases as a sort of antigravitational source, decreasing the fluid's radial infall velocity in the accretion process, and for others as a gravitational enhancer, increasing the fluid flow into the black hole horizon. Our analysis and results can be extended and also applied to the problem of black hole evaporation in cosmological scenarios with phantom fluids. In particular, we show that the mass of typical regular black holes can be used in order to constrain turnaround events in cyclic cosmologies. ",Accretion of perfect fluids onto a class of regular black holes
81,1138456963476594688,2650692774,Christian Baumgartner,"['Our new work on principled uncertainty estimation in medical image segmentation has been accepted to @miccai2019! The proposed probabilistic hierarchical model allows to sample from the distribution of segmentations. \n\nPaper: <LINK>\nCode: <LINK> <LINK>', 'In reality radiologists often tend to disagree in segmentation tasks. We show in the paper, that we are able to match very closely the distribution of segmentations produced by different radiologists. Here an example of 6 expert annotations for the same image. https://t.co/ew3fuoNvsV', 'Our method relies on constructing a hierarchical probabilistic model in which hidden latent variables generate the segmentations one resolution level at a time. Each latent variable is only responsible for producing the **residual** to the previous level. https://t.co/KEEgnDM91v', 'Inference in the model can be efficiently performed using the autoencoding variational Bayes framework. This is joint work with K. Tezcan, @krishnaethz, A. H√∂tker, U. Muehlematter, K. Schawkat, @dr_becker, @OlivioDonati and @EnderKonukoglu']",https://arxiv.org/abs/1906.04045,"Segmentation of anatomical structures and pathologies is inherently ambiguous. For instance, structure borders may not be clearly visible or different experts may have different styles of annotating. The majority of current state-of-the-art methods do not account for such ambiguities but rather learn a single mapping from image to segmentation. In this work, we propose a novel method to model the conditional probability distribution of the segmentations given an input image. We derive a hierarchical probabilistic model, in which separate latent variables are responsible for modelling the segmentation at different resolutions. Inference in this model can be efficiently performed using the variational autoencoder framework. We show that our proposed method can be used to generate significantly more realistic and diverse segmentation samples compared to recent related work, both, when trained with annotations from a single or multiple annotators. ",PHiSeg: Capturing Uncertainty in Medical Image Segmentation
82,1138455205094338563,2785337469,Sebastian Ruder,"['In our new paper (my first collaboration at DeepMind, yay!) with Cyprien, @ikekong, &amp; @DaniYogatama, we leverage episodic memory during training (sparse replay) and inference (local adaptation) for continual learning (on QA and classification tasks).\n<LINK> <LINK>']",https://arxiv.org/abs/1906.01076,We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly (~50-90%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction. ,Episodic Memory in Lifelong Language Learning
83,1138419133815644165,131264473,Norbert Schmidt,['We have tested a gazillion of smartphone cameras with your help. We collected the metadata and stored it in a public database which will be the start of cool new (citizen) science apps. Kudos to Olivier Burggraaff of Leiden University. Here is our paper: <LINK> <LINK>'],https://arxiv.org/abs/1906.04155,"Consumer cameras, particularly onboard smartphones and UAVs, are now commonly used as scientific instruments. However, their data processing pipelines are not optimized for quantitative radiometry and their calibration is more complex than that of scientific cameras. The lack of a standardized calibration methodology limits the interoperability between devices and, in the ever-changing market, ultimately the lifespan of projects using them. We present a standardized methodology and database (SPECTACLE) for spectral and radiometric calibrations of consumer cameras, including linearity, bias variations, read-out noise, dark current, ISO speed and gain, flat-field, and RGB spectral response. This includes golden standard ground-truth methods and do-it-yourself methods suitable for non-experts. Applying this methodology to seven popular cameras, we found high linearity in RAW but not JPEG data, inter-pixel gain variations >400% correlated with large-scale bias and read-out noise patterns, non-trivial ISO speed normalization functions, flat-field correction factors varying by up to 2.79 over the field of view, and both similarities and differences in spectral response. Moreover, these results differed wildly between camera models, highlighting the importance of standardization and a centralized database. ",Standardized spectral and radiometric calibration of consumer cameras
84,1138321250160390144,89467037,Mariano Beguerisse,['New paper! Automation and occupational mobility: A data-driven network model. A fantastic work led by Maria del Rio-Chanona bringing networks to the study of autiomation shocks. Read -&gt;  <LINK>'],https://arxiv.org/abs/1906.04086,"The potential impact of automation on the labor market is a topic that has generated significant interest and concern amongst scholars, policymakers, and the broader public. A number of studies have estimated occupation-specific risk profiles by examining the automatability of associated skills and tasks. However, relatively little work has sought to take a more holistic view on the process of labor reallocation and how employment prospects are impacted as displaced workers transition into new jobs. In this paper, we develop a new data-driven model to analyze how workers move through an empirically derived occupational mobility network in response to automation scenarios which increase labor demand for some occupations and decrease it for others. At the macro level, our model reproduces a key stylized fact in the labor market known as the Beveridge curve and provides new insights for explaining the curve's counter-clockwise cyclicality. At the micro level, our model provides occupation-specific estimates of changes in short and long-term unemployment corresponding to a given automation shock. We find that the network structure plays an important role in determining unemployment levels, with occupations in particular areas of the network having very few job transition opportunities. Such insights could be fruitfully applied to help design more efficient and effective policies aimed at helping workers adapt to the changing nature of the labor market. ",Automation and occupational mobility: A data-driven network model
85,1138318389804322816,826924323113807872,Mohammad Rastegari,"['Depth-wise Convolution and Point-wise Convolution showed tremendous benefit for efficient architecture design and now we are presenting  a new efficient CNN model DiCENet based on  Dimension-wise convolution #deeplearning , #ComputerVision \npaper:  <LINK> <LINK>']",https://arxiv.org/abs/1906.03516,"We introduce a novel and generic convolutional unit, DiCE unit, that is built using dimension-wise convolutions and dimension-wise fusion. The dimension-wise convolutions apply light-weight convolutional filtering across each dimension of the input tensor while dimension-wise fusion efficiently combines these dimension-wise representations; allowing the DiCE unit to efficiently encode spatial and channel-wise information contained in the input tensor. The DiCE unit is simple and can be seamlessly integrated with any architecture to improve its efficiency and performance. Compared to depth-wise separable convolutions, the DiCE unit shows significant improvements across different architectures. When DiCE units are stacked to build the DiCENet model, we observe significant improvements over state-of-the-art models across various computer vision tasks including image classification, object detection, and semantic segmentation. On the ImageNet dataset, the DiCENet delivers 2-4% higher accuracy than state-of-the-art manually designed models (e.g., MobileNetv2 and ShuffleNetv2). Also, DiCENet generalizes better to tasks (e.g., object detection) that are often used in resource-constrained devices in comparison to state-of-the-art separable convolution-based efficient networks, including neural search-based methods (e.g., MobileNetv3 and MixNet. Our source code in PyTorch is open-source and is available at this https URL ",DiCENet: Dimension-wise Convolutions for Efficient Networks
86,1138250374958190598,1420649448,Scott Weichenthal,['The paper describing our new model to predict annual average PM2.5 concentrations using Satellite Images is now on arxiv: <LINK>\n#arxiv @UNEnvironment @mcgillu @GoogleAI @NvidiaAI @LambdaAPI @HEISoGA @ISEE_global'],https://arxiv.org/abs/1906.03975,"Here we present a new method of estimating global variations in outdoor PM$_{2.5}$ concentrations using satellite images combined with ground-level measurements and deep convolutional neural networks. Specifically, new deep learning models were trained over the global PM$_{2.5}$ concentration range ($<$1-436 $\mu$g/m$^3$) using a large database of satellite images paired with ground level PM$_{2.5}$ measurements available from the World Health Organization. Final model selection was based on a systematic evaluation of well-known architectures for the convolutional base including InceptionV3, Xception, and VGG16. The Xception architecture performed best and the final global model had a root mean square error (RMSE) value of 13.01 $\mu$g/m$^3$ (R$^2$=0.75) in the disjoint test set. The predictive performance of our new global model (called IMAGE-PM$_{2.5}$) is similar to the current state-of-the-art model used in the Global Burden of Disease study but relies only on satellite images as input. As a result, the IMAGE-PM$_{2.5}$ model offers a fast, cost-effective means of estimating global variations in long-term average PM$_{2.5}$ concentrations and may be particularly useful for regions without ground monitoring data or detailed emissions inventories. The IMAGE-PM$_{2.5}$ model can be used as a stand-alone method of global exposure estimation or incorporated into more complex hierarchical model structures. ","Predicting Global Variations in Outdoor PM2.5 Concentrations using
  Satellite Images and Deep Convolutional Neural Networks"
87,1138147747431833603,112717746,Michela Paganini,"['New paper on #LotteryTickets in deep nets &amp; transfer across datasets and optimizers now out on @arxiv_org! ""One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers"". Work led by @arimorcos at @facebookai ‚û°Ô∏è\xa0<LINK>']",https://arxiv.org/abs/1906.02773,"The success of lottery ticket initializations (Frankle and Carbin, 2019) suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these ""winning ticket"" initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods. ","One ticket to win them all: generalizing lottery ticket initializations
  across datasets and optimizers"
88,1138093627954126851,907681936675676163,Maria Gorinova,"['Want to automatically transform your probabilistic programs for more efficient inference? Check out our new paper on automatic reparameterisation: <LINK>; with @davmre and Matt Hoffman.', '@davmre Code available at https://t.co/GtduLfAtpz']",https://arxiv.org/abs/1906.03028,"Probabilistic programming has emerged as a powerful paradigm in statistics, applied science, and machine learning: by decoupling modelling from inference, it promises to allow modellers to directly reason about the processes generating data. However, the performance of inference algorithms can be dramatically affected by the parameterisation used to express a model, requiring users to transform their programs in non-intuitive ways. We argue for automating these transformations, and demonstrate that mechanisms available in recent modeling frameworks can implement non-centring and related reparameterisations. This enables new inference algorithms, and we propose two: a simple approach using interleaved sampling and a novel variational formulation that searches over a continuous space of parameterisations. We show that these approaches enable robust inference across a range of models, and can yield more efficient samplers than the best fixed parameterisation. ",Automatic Reparameterisation of Probabilistic Programs
89,1138077201436487680,471109812,Yan,"['My new paper, ""FSPool: Learning Set Representations with Featurewise Sort Pooling"", is out! I look at why neural networks are bad at auto-encoding sets, and how to learn better representations for set classification.\n\nPaper: <LINK>\nCode: <LINK>']",https://arxiv.org/abs/1906.02795,"Traditional set prediction models can struggle with simple datasets due to an issue we call the responsibility problem. We introduce a pooling method for sets of feature vectors based on sorting features across elements of the set. This can be used to construct a permutation-equivariant auto-encoder that avoids this responsibility problem. On a toy dataset of polygons and a set version of MNIST, we show that such an auto-encoder produces considerably better reconstructions and representations. Replacing the pooling function in existing set encoders with FSPool improves accuracy and convergence speed on a variety of datasets. ",FSPool: Learning Set Representations with Featurewise Sort Pooling
90,1137953854845116418,243517888,Zhuohan Li,['Our new work! Paper can be found at <LINK> \nCodes and pretrained models can be found at <LINK> <LINK>'],https://arxiv.org/abs/1906.02762,"The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is ""Macaron-like"", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at this https URL ","Understanding and Improving Transformer From a Multi-Particle Dynamic
  System Point of View"
91,1137105286420946945,19920203,Fernanda Vi√©gas,"['Analyzing and visualizing syntax trees in the high-dimensional spaces of neural nets. \n\nCheck out the new PAIR paper on BERT geometry <LINK> \n\nAnd the blog post on ‚ÄúLanguage, trees, and geometry in neural networks‚Äù <LINK> <LINK>']",https://arxiv.org/abs/1906.02715,"Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations. ",Visualizing and Measuring the Geometry of BERT
92,1136883377762492416,2416760538,Peter Gao,"['New paper! This one is led by one of the grad students in my old research group, Siteng Fan, and I helped out a little bit. Link: <LINK>\n\nIt‚Äôs all about retrievals of the atmospheric composition of Titan through stellar occultations.', 'As Titan occults a background star, the star‚Äôs light will first filter through Titan‚Äôs atmosphere. During this time, spectroscopic signatures of atmospheric gases would get imprinted on the light from the star. It‚Äôs like an exoplanet transit, with slightly different geometry', 'UVIS, a @CassiniSaturn instrument, was (üò≠) able to measure the UV spectra of the stars undergoing these occultation events, and thus could be used to constrain the abundance profiles of various molecules in Titan‚Äôs atmosphere like methane, hydrogen cyanide, and acetylene', 'The issue is that this required Cassini to point very precisely at the star, which is not easy to do when in the gravity well of the massive Titan', 'Thus, most of the occultations were very difficult to use for abundance measurements due to pointing errors...until now!', 'Siteng combined a UVIS instrument model and a Bayesian retrieval framework that took into account errors in Cassini‚Äôs pointing to successfully retrieve abundances from a particularly jittery occultation', 'This opens the door to retrievals of nearly all of the occultations! As each occultation provides abundance profiles for numerous species at a specific time and location on Titan, retrieving all the occultations gives a 4D map of the chemical inventory of Titan‚Äôs atmosphere!', 'Siteng did a fantastic job and I‚Äôm honoured to have been part of the paper. His work shows that @CassiniSaturn will keep on giving us amazing science even after its demise for many years to come! https://t.co/6uS2qXAI5v', '@ExoEhsan Thanks!', '@ExoEhsan Abundance profiles = vertical dimension. Occultations at different locations on Titan = longitude and latitude. Occultations at different times = temporal evolution. 4D! It‚Äôll be a pretty sparse map but a map regardless', 'Oh man this would‚Äôve been the perfect gif for this https://t.co/et9xG3BJyx']",https://arxiv.org/abs/1906.02230,"Cassini/UVIS FUV observations of stellar occultations at Titan are well suited for probing its atmospheric composition and structure. However, due to instrument pointing motion, only five out of tens of observations have been analyzed. We present an innovative retrieval method that corrects for the effect of pointing motion by forward modeling the Cassini/UVIS instrument response function with the pointing motion value obtained from the SPICE C-kernel along the spectral dimension. To illustrate the methodology, an occultation observation made during flyby T52 is analyzed, when the Cassini spacecraft had insufficient attitude control. A high-resolution stellar model and an instrument response simulator that includes the position of the point source on the detector are used for the analysis of the pointing motion. The Markov Chain Monte-Carlo method is used to retrieve the line-of-sight abundance profiles of eleven species (CH4, C2H2, C2H4, C2H6, C4H2, C6H6, HCN, C2N2, HC3N, C6N2 and haze particles) in the spectral vector fitting process. We obtain tight constraints on all of the species aside from C2H6, C2N2 and C6N2, for which we only retrieved upper limits. This is the first time that the T52 occultation was used to derive abundances of major hydrocarbon and nitrile species in Titan's upper and middle atmosphere, as pointing motion prohibited prior analysis. With this new method, nearly all of the occultations obtained over the entire Cassini mission could yield reliable profiles of atmospheric composition, allowing exploration of Titan's upper atmosphere over seasons, latitudes, and longitudes. ","Retrieval of Chemical Abundances in Titan's Upper Atmosphere from
  Cassini UVIS Observations with Pointing Motion"
93,1136868293883514881,73928069,Shadab Khan,"['New paper accepted in @miccai2019 with @AhmedHassan9922 \n\nWe show a principled approach to incorporating priors from extreme points to generate reliable training data for segmentation tasks.\n\nPreprint: <LINK>\n\nCode+blog coming soon!', 'Extreme points, as shown by Dim Papadopoulos et al (ICCV), can be annotated much more quickly (~5x) than a bounding box. We take extreme points, and compute a full confidence map by incorporating intuitive priors. These weight maps have desirable isocontours as shown: https://t.co/0HrptE4aS9', 'We train a DeepLab inspired architecture with image and confidence map as input, and use it to predict class-agnostic segmentation of an object. At test time, user provides extreme points and the model generates pixel-level segmentation that can be used for supervised training. https://t.co/4hAyZeSfoD', 'We experimented to see how well supervised training does with generated data compared to ground truth for segmentation of a previously unseen organ (but we fine tuned on a small fraction of the data). We obtained comparable results. https://t.co/qN3ulyVmFs', 'We hope with this method, we will be able to generate reliable training data for several supervised learning scenarios.\n\nBONUS: A fast algorithm to compute distance of points on a 2D grid from a line segment. How fast? ~88ms for a 512*512 grid using non-multi-threaded python. https://t.co/OG4oC73SJp', 'Thanks to reviewers for useful feedback, though we particularly thank reviewer number 1 for useful feedback.']",https://arxiv.org/abs/1906.02421,"To automate the process of segmenting an anatomy of interest, we can learn a model from previously annotated data. The learning-based approach uses annotations to train a model that tries to emulate the expert labeling on a new data set. While tremendous progress has been made using such approaches, labeling of medical images remains a time-consuming and expensive task. In this paper, we evaluate the utility of extreme points in learning to segment. Specifically, we propose a novel approach to compute a confidence map from extreme points that quantitatively encodes the priors derived from extreme points. We use the confidence map as a cue to train a deep neural network based on ResNet-101 and PSP module to develop a class-agnostic segmentation model that outperforms state-of-the-art method that employs extreme points as a cue. Further, we evaluate a realistic use-case by using our model to generate training data for supervised learning (U-Net) and observed that U-Net performs comparably when trained with either the generated data or the ground truth data. These findings suggest that models trained using cues can be used to generate reliable training data. ","Extreme Points Derived Confidence Map as a Cue For Class-Agnostic
  Segmentation Using Deep Neural Network"
94,1136831569765605382,1010768532756402176,Arindam Ghosh,"['Latest update: Just uploaded a new paper on arxiv with some new evidence of possible superconductivity in Ag/Au nanostructures. Eager to see how the community reacts.  <LINK>', '@GyanCMehta @aalokelab Measurements are on. Our first goal was to straighten the resistivity data, and associated features. Magnetization measurement takes more sample quantity, hence much more time to synthesize.', '@sanjayu24688430 Thank you for pointing it out.', '@Sanu_M_Physics We do have oxidation and homogeneity issues. We are trying to optimize the best condition to synthesize the material.']",https://arxiv.org/abs/1906.02291,"Transitions to immeasurably small electrical resistance in thin films of Ag/Au nanostructure-based films have generated significant interest because such transitions can occur even at ambient temperature and pressure. While the zero-bias resistance and magnetic transition in these films have been reported recently, the non-equilibrium current-voltage ($I-V$) transport characteristics at the transition remains unexplored. Here we report the $I-V$ characteristics at zero magnetic field of a prototypical Ag/Au nanocluster film close to its resistivity transition at the critical temperature $T_{C}$ of $\approx160$ K. The $I-V$ characteristics become strongly hysteretic close to the transition and exhibit a temperature-dependent critical current scale beyond which the resistance increases rapidly. Intriguingly, the non-equilibrium transport regime consists of a series of nearly equispaced resistance steps when the drive current exceeds the critical current. We have discussed the similarity of these observations with resistive transitions in ultra-thin superconducting wires via phase slip centres. ","Current-voltage characteristics in Ag/Au nanostructures at resistive
  transitions"
95,1136807522432454657,2295554268,Justin Johnson,"[""I'm excited to share our new paper that jointly detects objects and predicts 3D triangle meshes in real-world images, called Mesh R-CNN.\n\nWith Georgia Gkioxari and Jitendra Malik\n<LINK> <LINK>""]",https://arxiv.org/abs/1906.02739,"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes. ",Mesh R-CNN
96,1136730353987719169,371403672,Eshwar Chandrasekharan,['Our new #ACL2019 position paper with @david__jurgens\nand @libbyh identifies three key future directions for #NLProc when trying to tackle online abuse.\n<LINK>'],https://arxiv.org/abs/1906.01738,"Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities. ",A Just and Comprehensive Strategy for Using NLP to Address Online Abuse
97,1136715334214438913,221857665,David Jurgens,['What direction should #NLProc head when trying to tackle abusive behavior?  A new position paper at #ACL2019 with my colleagues @libbyh and @eshwar_chan that identifies three key directions <LINK>'],https://arxiv.org/abs/1906.01738,"Online abusive behavior affects millions and the NLP community has attempted to mitigate this problem by developing technologies to detect abuse. However, current methods have largely focused on a narrow definition of abuse to detriment of victims who seek both validation and solutions. In this position paper, we argue that the community needs to make three substantive changes: (1) expanding our scope of problems to tackle both more subtle and more serious forms of abuse, (2) developing proactive technologies that counter or inhibit abuse before it harms, and (3) reframing our effort within a framework of justice to promote healthy communities. ",A Just and Comprehensive Strategy for Using NLP to Address Online Abuse
98,1136535682573320192,1104703454,Francesco Silvestri,"['New paper! ‚ÄúFair Near Neighbor Search: Independent Range Sampling in High Dimensions‚Äú by M. Aum√ºller @RasmusPagh1 and myself: <LINK>. Given a query q, we show how to evenly sample near neighbors of q in a high dimensional space. (1/2)', 'It is ""fair"" in the sense that all points in the neighborhood have equal opportunity. Just using LSH is not enough: the closest point is more likely to be returned! We show how to make any LSH fair and also give a nearly-linear space for inner product. (2/2)']",https://arxiv.org/abs/1906.01859,"Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. There are several variants of the similarity search problem, and one of the most relevant is the $r$-near neighbor ($r$-NN) problem: given a radius $r>0$ and a set of points $S$, construct a data structure that, for any given query point $q$, returns a point $p$ within distance at most $r$ from $q$. In this paper, we study the $r$-NN problem in the light of fairness. We consider fairness in the sense of equal opportunity: all points that are within distance $r$ from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee. To address this, we propose efficient data structures for $r$-NN where all points in $S$ that are near $q$ have the same probability to be selected and returned by the query. Specifically, we first propose a black-box approach that, given any LSH scheme, constructs a data structure for uniformly sampling points in the neighborhood of a query. Then, we develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights (un)fairness in a recommendation setting on real-world datasets and discusses the inherent unfairness introduced by solving other variants of the problem. ",Fair Near Neighbor Search: Independent Range Sampling in High Dimensions
99,1136499009432895493,35591633,Ryohei Seto,['New paper with Romain on arXiv now.  <LINK>'],https://arxiv.org/abs/1906.02103,"The origin of the abrupt shear thickening observed in some dense suspensions has been recently argued to be a transition from frictionless (lubricated) to frictional interactions between immersed particles. The Wyart-Cates rheological model, built on this scenario, introduced the concept of fraction of frictional contacts $f$ as the relevant order parameter for the shear thickening transition. Central to the model is the ""equation-of-state"" relating $f$ to the applied stress $\sigma$, which is directly linked to the distribution of the normal components of non-hydrodynamics interparticle forces. Here, we develop a model for this force distribution, based on the so-called $q$-model that we borrow from granular physics. This model explains the known $f(\sigma)$ in the simple case of sphere contacts displaying only sliding friction, but also predicts strong deviation from this ""usual"" form when stronger kinds of constraints are applied on relative motion. We verify these predictions in the case of contacts with rolling friction, in particular a broadening of the stress range over which shear thickening occurs. We finally discuss how a similar approach can be followed to predict $f(\sigma)$ in systems with other variations from the canonical system of monodisperse spheres with sliding friction, in particular the case of large bidispersity. ",Force transmission and the order parameter of shear thickening
100,1136458893444276224,118592197,Tsuyoshi Okubo,"['Our new paper: ""Anisotropic Tensor Renormalization Group"" Daiki Adachi, Tsuyoshi Okubo, and Synge Todo <LINK>']",https://arxiv.org/abs/1906.02007,"We propose a new tensor renormalization group algorithm, Anisotropic Tensor Renormalization Group (ATRG), for lattice models in arbitrary dimensions. The proposed method shares the same versatility with the Higher-Order Tensor Renormalization Group (HOTRG) algorithm, i.e., it preserves the lattice topology after the renormalization. In comparison with HOTRG, both of the computation cost and the memory footprint of our method are drastically reduced, especially in higher dimensions, by renormalizing tensors in an anisotropic way after the singular value decomposition. We demonstrate the ability of ATRG for the square lattice and the simple cubic lattice Ising models. Although the accuracy of the present method degrades when compared with HOTRG of the same bond dimension, the accuracy with fixed computation time is improved greatly due to the drastic reduction of the computation cost. ",Anisotropic Tensor Renormalization Group
101,1136440986098212865,267958924,Christian Ott,['New paper by Andr√© Schneider &amp; team. Excited to be part of it. <LINK>'],https://arxiv.org/abs/1906.02009,"Uncertainties in our knowledge of the properties of dense matter near and above nuclear saturation density are among the main sources of variations in multi-messenger signatures predicted for core-collapse supernovae (CCSNe) and the properties of neutron stars (NSs). We construct 97 new finite-temperature equations of state (EOSs) of dense matter that obey current experimental, observational, and theoretical constraints and discuss how systematic variations in the EOS parameters affect the properties of cold nonrotating NSs and the core collapse of a $20\,M_\odot$ progenitor star. The core collapse of the $20\,M_\odot$ progenitor star is simulated in spherical symmetry using the general-relativistic radiation-hydrodynamics code GR1D where neutrino interactions are computed for each EOS using the NuLib library. We conclude that the effective mass of nucleons at densities above nuclear saturation density is the largest source of uncertainty in the CCSN neutrino signal and dynamics even though it plays a subdominant role in most properties of cold NS matter. Meanwhile, changes in other observables affect the properties of cold NSs, while having little effect in CCSNe. To strengthen our conclusions, we perform six octant three-dimensional CCSN simulations varying the effective mass of nucleons at nuclear saturation density. We conclude that neutrino heating and, thus, the likelihood of explosion is significantly increased for EOSs where the effective mass of nucleons at nuclear saturation density is large. ",Equation of state effects in the core collapse of a $20$-$M_\odot$ star
102,1135848920511320064,590944541,Gavin Gray,"['Evidence in our new paper that there are better ways to do deep learning than matrix multiplication with a weight matrix: <LINK>', 'tl;dr FC layers (and therefore pointwise convolutions) have many proposed efficient alternatives, and we show that they many work in large CNNs with a weight decay fix.', 'What does this mean? Right now, to make a more efficient model use grouped pointwise convolutions, riffle shuffles and scale the weight decay in each layer by the compression factor. In future, architecture search has an entire new space to explore.']",https://arxiv.org/abs/1906.00859,"In response to the development of recent efficient dense layers, this paper shows that something as simple as replacing linear components in pointwise convolutions with structured linear decompositions also produces substantial gains in the efficiency/accuracy tradeoff. Pointwise convolutions are fully connected layers and are thus prepared for replacement by structured transforms. Networks using such layers are able to learn the same tasks as those using standard convolutions, and provide Pareto-optimal benefits in efficiency/accuracy, both in terms of computation (mult-adds) and parameter count (and hence memory). Code is available at this https URL ",Separable Layers Enable Structured Efficient Linear Substitutions
103,1135753752605089792,2352463609,Carlos Blanco,['Check out our new paper! Annihilation Signatures of Hidden Sector Dark Matter Within Early-Forming Microhalos. (arXiv:1906.00010v1) #darkmatter @DanHooperAstro  \n\n<LINK>'],https://arxiv.org/abs/1906.00010,"If the dark matter is part of a hidden sector with only very feeble couplings to the Standard Model, the lightest particle in the hidden sector will generically be long-lived and could come to dominate the energy density of the universe prior to the onset of nucleosynthesis. During this early matter-dominated era, density perturbations will grow more quickly than otherwise predicted, leading to a large abundance of sub-earth-mass dark matter microhalos. Since the dark matter does not couple directly to the Standard Model, the minimum halo mass is much smaller than expected for weakly interacting dark matter, and the smallest halos could form during the radiation-dominated era. In this paper, we calculate the evolution of density perturbations within the context of such hidden sector models and use a series of $N$-body simulations to determine the outcome of nonlinear collapse during radiation domination. The resulting microhalos are extremely dense, which leads to very high rates of dark matter annihilation and to large indirect detection signals that resemble those ordinarily predicted for decaying dark matter. We find that the Fermi Collaboration's measurement of the high-latitude gamma-ray background rules out a wide range of parameter space within this class of models. The scenarios that are most difficult to constrain are those that feature a very long early matter-dominated era; if microhalos form prior to the decay of the unstable hidden sector matter, the destruction of these microhalos effectively heats the dark matter, suppressing the later formation of microhalos. ","Annihilation Signatures of Hidden Sector Dark Matter Within
  Early-Forming Microhalos"
104,1146414486724759553,726743088979427328,Joerg Arnscheidt,['The space dimension of water science: How can we focus the search for habitable worlds beyond Planet Earth? This new paper describes the lower gravity boundary for a sufficiently long water retention to enable the evolution of life. <LINK> #astrobiology #water <LINK>'],https://arxiv.org/abs/1906.10561,"Low-gravity waterworlds ($M\lesssim 0.1 M_{\oplus}$) are of interest for their potential habitability. The weakly bound atmospheres of such worlds have proportionally larger radiative surfaces and are more susceptible to escape. We conduct a unified investigation into these phenomena, combining analytical energy balance and hydrodynamic escape with line-by-line radiative transfer calculations. Because outgoing radiation is forced to increase with surface temperature by the expansion of the radiative surface, we find that these worlds do not experience a runaway greenhouse. Furthermore, we show that a long-lived liquid water habitable zone is possible for low-gravity waterworlds of sufficient mass. Its inner edge is set by the rate of atmospheric escape, because a short-lived atmosphere limits the time available for life to evolve. In describing the physics of the parameter space transition from ""planet-like"" to ""comet-like"", our model produces a lower bound for habitability in terms of gravity. These results provide valuable insights in the context of the ongoing hunt for habitable exoplanets and exomoons. ",Atmospheric Evolution on Low-gravity Waterworlds
105,1144699704527085568,1079592050973061122,Pieter van Dokkum,"['A new Dragonfly paper! We looked at the beautiful nearby edge-on galaxy NGC5907: <LINK>\nWe were particularly curious to see what its iconic double loop stellar stream looks like - we were expecting something like this image (see <LINK> ): <LINK>', 'The Dragonfly data yield a revised view of the stream: we see only one loop, with a previously undetected long extension to the West (with a surface brightness of ~29 mag/arcsec^2!). https://t.co/vaTGtiCNR8', ""@anabonaca created a dynamical model that reproduces all the major features of the stream. The NGC5907 stream is similar to the Milky Way's Sagittarius stream, in terms of its total extent and the stellar mass. https://t.co/KBMLbXnadl"", 'The census of these rare, relatively high mass events complements the census of common, low mass ones that is provided by studies of streams in the Milky Way halo.', ""It was a lot of fun and an awesome team effort!  @anabonaca @DanieliShany @roberto_abraham @johnnypgreco Look for more Dragonfly images of edge-on galaxies in Colleen Gilhuly's upcoming papers and thesis!""]",https://arxiv.org/abs/1906.11260,"In 2008 it was reported that the stellar stream of the edge-on spiral NGC5907 loops twice around the galaxy, enveloping it in a giant corkscrew-like structure. Here we present imaging of this iconic object with the Dragonfly Telephoto Array, reaching a $1\sigma$ surface brightness level of $\mu_g\approx 30.5$ mag/arcsec$^2$ on spatial scales of 1' (the approximate width of the stream). We find a qualitatively different morphology from that reported in the 2008 study. The Dragonfly data do not show two loops but a single curved stream with a total length of 45' (220 kpc). The surface brightness of the stream ranges from $\mu_g \approx 27.6$ mag/arcsec$^2$ to $\mu_g\approx 28.8$ mag/arcsec$^2$, and it extends significantly beyond the region where tidal features had previously been detected. We find a density enhancement near the luminosity-weighted midpoint of the stream which we identify as the likely remnant of a nearly-disrupted progenitor galaxy. A restricted N-body simulation provides a qualitative match to the detected features. In terms of its spatial extent and stellar mass the stream is similar to Sagittarius, and our results demonstrate the efficacy of low surface brightness-optimized telescopes for obtaining maps of such large streams outside the Local Group. The census of these rare, relatively high mass events complements the census of common, low mass ones that is provided by studies of streams in the Milky Way halo. ","Dragonfly imaging of the galaxy NGC5907: a revised view of the iconic
  stellar stream"
106,1144539074163814402,2770143649,Jose Fernando Mendes,"['Our new paper: ""Complex distributions emerging in filtering and compression"", <LINK> <LINK>']",https://arxiv.org/abs/1906.11266,"In filtering, each output is produced by a certain number of different inputs. We explore the statistics of this degeneracy in an explicitly treatable filtering problem in which filtering performs the maximal compression of relevant information contained in inputs (arrays of zeroes and ones). This problem serves as a reference model for the statistics of filtering and related sampling problems. The filter patterns in this problem conveniently allow a microscopic, combinatorial consideration. This allows us to find the statistics of outputs, namely the exact distribution of output degeneracies, for arbitrary input sizes. We observe that the resulting degeneracy distribution of outputs decays as $e^{-c\log^\alpha \!d}$ with degeneracy $d$, where $c$ is a constant and exponent $\alpha>1$, i.e. faster than a power law. Importantly, its form essentially depends on the size of the input data set, appearing to be closer to a power-law dependence for small data set sizes than for large ones. We demonstrate that for sufficiently small input data set sizes typical for empirical studies, this distribution could be easily perceived as a power law. We extend our results to filter patterns of various sizes and demonstrate that the shortest filter pattern provides the maximum informative representations of the inputs. ",Complex distributions emerging in filtering and compression
107,1144400300335423488,46153507,dr. Jordy Davelaar,"['New paper! We improved our Black Hole Accretion Code (BHAC) that allows us to keep divergence of magnetic fields close to zero! It turned out to be the key for our latest high-resolution simulations of M87! \n\nFirst author: Hector Olivares @BlackHoleCam <LINK> <LINK>', 'Title: Constrained transport and adaptive mesh refinement in the Black Hole Accretion Code\nAuthor list: Hector Olivares, Oliver Porth, Jordy Davelaar, Elias R. Most, Christian M. Fromm, Yosuke Mizuno, Ziri Younsi, and Luciano Rezzolla', '@StefanPWinc @BlackHoleCam thanks!']",https://arxiv.org/abs/1906.10795,"Worldwide very long baseline radio interferometry arrays are expected to obtain horizon-scale images of supermassive black hole candidates as well as of relativistic jets in several nearby active galactic nuclei. This motivates the development of models for magnetohydrodynamic flows in strong gravitational fields. The Black Hole Accretion Code (BHAC) intends to aid with the modelling of such sources by means of general relativistic magnetohydrodynamical (GRMHD) simulations in arbitrary stationary spacetimes. New additions were required to guarantee an accurate evolution of the magnetic field when small and large scales are captured simultaneously. We discuss the adaptive mesh refinement (AMR) techniques employed in BHAC, essential to keep several problems computationally tractable, as well as staggered-mesh-based constrained transport (CT) algorithms to preserve the divergence-free constraint of the magnetic field, including a general class of prolongation operators for face-allocated variables compatible with them. Through several standard tests, we show that the choice of divergence-control method can produce qualitative differences in simulations of scientifically relevant accretion problems. We demonstrate the ability of AMR to reduce the computational costs of accretion simulations while sufficiently resolving turbulence from the magnetorotational instability. In particular, we describe a simulation of an accreting Kerr black hole in Cartesian coordinates using AMR to follow the propagation of a relativistic jet while self-consistently including the jet engine, a problem set up-for which the new AMR implementation is particularly advantageous. The CT methods and AMR strategies discussed here are being employed in the simulations performed with BHAC used in the generation of theoretical models for the Event Horizon Telescope Collaboration. ","Constrained transport and adaptive mesh refinement in the Black Hole
  Accretion Code"
108,1143129155988578304,24859650,Jan-Willem van de Meent,"[""I'm excited about our new working paper on analysis of fMRI data: <LINK>. Co-authors: Eli Sennesh (@EliSennesh), Zulqarnain Khan, Jennifer Dy, Ajay Satpute (@AjayBSatpute), and Ben Hutchinson. [thread ‚Üµ] <LINK>"", 'We develop a probabilistic factor analysis model that represents participants and stimuli using low-dimensional embeddings. This mechanism for parameter sharing across results in higher reconstruction accuracies and lower-dimensional parameterizations. https://t.co/0DbnuEWpfv', ""More importantly, the inferred embeddings characterize variation among participants and stimuli. We're particularly excited about the potential of these models to help us investigate subjective experience ‚Äì the range of variation in how individuals experience that same stimuli. https://t.co/MRAh6HKpMv"", 'p.s.: Zulqarnain Khan is @zq90 on Twitter']",https://arxiv.org/abs/1906.08901,"Neuroimaging studies produce gigabytes of spatio-temporal data for a small number of participants and stimuli. Rarely do researchers attempt to model and examine how individual participants vary from each other -- a question that should be addressable even in small samples given the right statistical tools. We propose Neural Topographic Factor Analysis (NTFA), a probabilistic factor analysis model that infers embeddings for participants and stimuli. These embeddings allow us to reason about differences between participants and stimuli as signal rather than noise. We evaluate NTFA on data from an in-house pilot experiment, as well as two publicly available datasets. We demonstrate that inferring representations for participants and stimuli improves predictive generalization to unseen data when compared to previous topographic methods. We also demonstrate that the inferred latent factor representations are useful for downstream tasks such as multivoxel pattern analysis and functional connectivity. ",Neural Topographic Factor Analysis for fMRI Data
109,1142089429080772608,312448486,Dr. Karan Jani,['Black holes are either sun-like or supremely massive. What about the ones in between them? Is there a mass-gap?\n\nOur new paper provides the strongest constraints EVER on the population of Intermediate-Mass Black Hole (IMBH) with @LIGO @ego_virgo \n\nMore: <LINK> <LINK>'],https://arxiv.org/abs/1906.08000,"Gravitational wave astronomy has been firmly established with the detection of gravitational waves from the merger of ten stellar mass binary black holes and a neutron star binary. This paper reports on the all-sky search for gravitational waves from intermediate mass black hole binaries in the first and second observing runs of the Advanced LIGO and Virgo network. The search uses three independent algorithms: two based on matched filtering of the data with waveform templates of gravitational wave signals from compact binaries, and a third, model-independent algorithm that employs no signal model for the incoming signal. No intermediate mass black hole binary event was detected in this search. Consequently, we place upper limits on the merger rate density for a family of intermediate mass black hole binaries. In particular, we choose sources with total masses $M=m_1+m_2\in[120,800]$M$_\odot$ and mass ratios $q = m_2/m_1 \in[0.1,1.0]$. For the first time, this calculation is done using numerical relativity waveforms (which include higher modes) as models of the real emitted signal. We place a most stringent upper limit of $0.20$~Gpc$^{-3}$yr$^{-1}$ (in co-moving units at the 90% confidence level) for equal-mass binaries with individual masses $m_{1,2}=100$M$_\odot$ and dimensionless spins $\chi_{1,2}= 0.8$ aligned with the orbital angular momentum of the binary. This improves by a factor of $\sim 5$ that reported after Advanced LIGO's first observing run. ","Search for intermediate mass black hole binaries in the first and second
  observing runs of the Advanced LIGO and Virgo network"
110,1141770573305044992,1868950795,Ruth Misener,"['New work on robust optimization for the pooling problem by @johanneswiebe (joint w/ Schlumberger\'s In√™s Cec√≠lio &amp; me). Preprint: <LINK> Journal: <LINK> The paper will be part of a special issue titled ""I&amp;ECR 2019 Class of Influential Researchers"" <LINK>']",https://arxiv.org/abs/1906.07612,"The pooling problem has applications, e.g., in petrochemical refining, water networks, and supply chains and is widely studied in global optimization. To date, it has largely been treated deterministically, neglecting the influence of parametric uncertainty. This paper applies two robust optimization approaches, reformulation and cutting planes, to the non-linear, non-convex pooling problem. Most applications of robust optimization have been either convex or mixed-integer linear problems. We explore the suitability of robust optimization in the context of global optimization problems which are concave in the uncertain parameters by considering the pooling problem with uncertain inlet concentrations. We compare the computational efficiency of reformulation and cutting plane approaches for three commonly-used uncertainty set geometries on 14 pooling problem instances and demonstrate how accounting for uncertainty changes the optimal solution. ",Robust optimization for the pooling problem
111,1141575320861999104,937127267850846208,Mohammad Javad Amiri,"['Checkout our new paper: ""SeeMoRe: A Fault-Tolerant Protocol for Hybrid Cloud Environments""\n\n<LINK>\n\nP.S. SeeMoRe is derived from Seemorq, a benevolent, mythical bird in Persian mythology which appears as a peacock with the head of a dog and the claws of a lion.']",https://arxiv.org/abs/1906.07850,"Large scale data management systems utilize State Machine Replication to provide fault tolerance and to enhance performance. Fault-tolerant protocols are extensively used in the distributed database infrastructure of large enterprises such as Google, Amazon, and Facebook, as well as permissioned blockchain systems like IBM's Hyperledger Fabric. However, and in spite of years of intensive research, existing fault-tolerant protocols do not adequately address all the characteristics of distributed system applications. In particular, hybrid cloud environments consisting of private and public clouds are widely used by enterprises. However, fault-tolerant protocols have not been adapted for such environments. In this paper, we introduce SeeMoRe, a hybrid State Machine Replication protocol to handle both crash and malicious failures in a public/private cloud environment. SeeMoRe considers a private cloud consisting of nonmalicious nodes (either correct or crash) and a public cloud with both Byzantine faulty and correct nodes. SeeMoRe has three different modes which can be used depending on the private cloud load and the communication latency between the public and the private cloud. We also introduce a dynamic mode switching technique to transition from one mode to another. Furthermore, we evaluate SeeMoRe using a series of benchmarks. The experiments reveal that SeeMoRe's performance is close to the state of the art crash fault-tolerant protocols while tolerating malicious failures. ",SeeMoRe: A Fault-Tolerant Protocol for Hybrid Cloud Environments
112,1141416826670202880,968555423467945984,Matthew Schlegel,"['We\'ve just released our newest paper ""Importance Resampling for Off-policy Prediction"" on Arxiv. We introduce a new off-policy learning algorithm based on Sample-Importance-Resampling, and discuss its various properties. #ReinforcementLearning #prediction\n\n<LINK>', 'Watch out for experiment code written in pure Julia! #julialang @JuliaLanguage']",https://arxiv.org/abs/1906.04328,"Importance sampling (IS) is a common reweighting strategy for off-policy prediction in reinforcement learning. While it is consistent and unbiased, it can result in high variance updates to the weights for the value function. In this work, we explore a resampling strategy as an alternative to reweighting. We propose Importance Resampling (IR) for off-policy prediction, which resamples experience from a replay buffer and applies standard on-policy updates. The approach avoids using importance sampling ratios in the update, instead correcting the distribution before the update. We characterize the bias and consistency of IR, particularly compared to Weighted IS (WIS). We demonstrate in several microworlds that IR has improved sample efficiency and lower variance updates, as compared to IS and several variance-reduced IS strategies, including variants of WIS and V-trace which clips IS ratios. We also provide a demonstration showing IR improves over IS for learning a value function from images in a racing car simulator. ",Importance Resampling for Off-policy Prediction
113,1139502907487903750,74163970,Tim Harries,"['A new paper from Dr Ahmad Ali (@plingaling) on massive star feedback in clusters, focussing  on the variations of the UV radiation field of cluster members <LINK>. <LINK>']",https://arxiv.org/abs/1906.05858,"We investigate radiative feedback from a 34 M$_\odot$ star in a $10^4$ M$_\odot$ turbulent cloud using three-dimensional radiation-hydrodynamics (RHD) models. We use Monte Carlo radiative transfer to accurately compute photoionization equilibrium and radiation pressure, with multiple atomic species and silicate dust grains. We include the diffuse radiation field, dust absorption/re-emission, and scattering. The cloud is efficiently dispersed, with 75 per cent of the mass leaving the (32.3 pc)$^3$ grid within 4.3 Myr (1.1 $t_{ff}$). This compares to all mass exiting within 1.6 Myr (0.74 $t_{ff}$) in our previously published $10^3$ M$_\odot$ cloud. At most 20 per cent of the mass is ionized, compared to 40 per cent in the lower mass model, despite the ionized volume fraction being 80 per cent in both, implying the higher mass cloud is more resilient to feedback. The total Jeans-unstable mass increases linearly up to 1500 M$_\odot$ before plateauing after 2 Myr, corresponding to a core formation efficiency of 15 per cent. We also measure the time-variation of the far-ultraviolet (FUV) radiation field, $G_0$, impinging on other cluster members, taking into account for the first time how this changes in a dynamic cluster environment with intervening opacity sources and stellar motions. Many objects remain shielded in the first 0.5 Myr whilst the massive star is embedded, after which $G_0$ increases by orders of magnitude. Gas motions later on cause comparable drops which happen instantaneously and last for $\sim$ 1 Myr before being restored. This highly variable UV field will influence the photoevaporation of protoplanetary discs near massive stars. ","Massive star feedback in clusters: variation of the FUV interstellar
  radiation field in time and space"
114,1139238317717774336,349172730,Ranjay Krishna,"['Structured prediction requires substantial training data. Our new paper introduces the first few-shot scene graph model with predicates as functions within a graph convolution framework, resulting in the first semantically &amp; spatially interpretable model. <LINK> <LINK>', 'in collaboration with my advisors @drfeifei and @msbernst and my amazing team: @adornadula and @AustinNarcomey', '@roeiherzig Yes we are. Updating to make sure we cite you.']",https://arxiv.org/abs/1906.04876,"Scene graph prediction --- classifying the set of objects and predicates in a visual scene --- requires substantial training data. However, most predicates only occur a handful of times making them difficult to learn. We introduce the first scene graph prediction model that supports few-shot learning of predicates. Existing scene graph generation models represent objects using pretrained object detectors or word embeddings that capture semantic object information at the cost of encoding information about which relationships they afford. So, these object representations are unable to generalize to new few-shot relationships. We introduce a framework that induces object representations that are structured according to their visual relationships. Unlike past methods, our framework embeds objects that afford similar relationships closer together. This property allows our model to perform well in the few-shot setting. For example, applying the 'riding' predicate transformation to 'person' modifies the representation towards objects like 'skateboard' and 'horse' that enable riding. We generate object representations by learning predicates trained as message passing functions within a new graph convolution framework. The object representations are used to build few-shot predicate classifiers for rare predicates with as few as 1 labeled example. We achieve a 5-shot performance of 22.70 recall@50, a 3.7 increase when compared to strong transfer learning baselines. ","Learning Predicates as Functions to Enable Few-shot Scene Graph
  Prediction"
115,1138709842703585280,247800333,Ahmad ÿ∑Ÿá,"[""üö®New paper alert: Our work on designing robust and tuning-free state estimation method for GPS receivers, under spoofing attacks,  is accepted for publication in IEEE Control System Letters: <LINK> \nThis is Junhwan's first publication--and I'm so happy for him."", '@VaSansone Thanks Vanessa!']",https://arxiv.org/abs/1906.03928,"The operation of critical infrastructures such as the electrical power grid, cellphone towers, and financial institutions relies on precise timing provided by stationary GPS receivers. These GPS devices are vulnerable to a type of spoofing called Time Synchronization Attack (TSA), whose objective is to maliciously alter the timing provided by the GPS receiver. The objective of this paper is to design a tuning-free, low memory robust estimator to mitigate such spoofing attacks. The contribution is that the proposed method dispenses with several limitations found in the existing state-of-the-art methods in the literature that require parameter tuning, availability of the statistical distributions of noise, real-time optimization, or heavy computations. Specifically, we (i) utilize an observer design for linear systems under unknown inputs, (ii) adjust it to include a state-correction algorithm, (iii) design a realistic experimental setup with real GPS data and sensible spoofing attacks, and (iv) showcase how the proposed tuning-free, low memory robust estimator can combat TSAs. Numerical tests with real GPS data demonstrate that accurate time can be provided to the user under various attack conditions. ","Tuning-Free, Low Memory Robust Estimator to Mitigate GPS Spoofing
  Attacks"
116,1138658726716399616,18262687,Rushil,"['New preprint: We find that decoupling domain alignment from the final task improves domain adaptation. A simple subspace based alignment consistently outperforms adversarial DA like CDAN etc.  Exciting work from @kowshik0808, @jjayaram7 &amp; @pturaga1\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1906.04338,"Unsupervised domain adaptation aims to transfer and adapt knowledge learned from a labeled source domain to an unlabeled target domain. Key components of unsupervised domain adaptation include: (a) maximizing performance on the target, and (b) aligning the source and target domains. Traditionally, these tasks have either been considered as separate, or assumed to be implicitly addressed together with high-capacity feature extractors. When considered separately, alignment is usually viewed as a problem of aligning data distributions, either through geometric approaches such as subspace alignment or through distributional alignment such as optimal transport. This paper represents a hybrid approach, where we assume simplified data geometry in the form of subspaces, and consider alignment as an auxiliary task to the primary task of maximizing performance on the source. The alignment is made rather simple by leveraging tractable data geometry in the form of subspaces. We synergistically allow certain parameters derived from the closed-form auxiliary solution, to be affected by gradients from the primary task. The proposed approach represents a unique fusion of geometric and model-based alignment with gradients from a data-driven primary task. Our approach termed SALT, is a simple framework that achieves comparable or sometimes outperforms state-of-the-art on multiple standard benchmarks. ","SALT: Subspace Alignment as an Auxiliary Learning Task for Domain
  Adaptation"
117,1136563600049496065,802147912612454400,Martin Schmitt,"[""Test your models' knowledge about #lexicalsemantics on SherLIiC, a new lexical inference benchmark for #NLI to be presented @ACL2019_Italy.\nPaper: <LINK>\n#acl2019nlp #nlproc #ACL2019""]",https://arxiv.org/abs/1906.01393,"We present SherLIiC, a testbed for lexical inference in context (LIiC), consisting of 3985 manually annotated inference rule candidates (InfCands), accompanied by (i) ~960k unlabeled InfCands, and (ii) ~190k typed textual relations between Freebase entities extracted from the large entity-linked corpus ClueWeb09. Each InfCand consists of one of these relations, expressed as a lemmatized dependency path, and two argument placeholders, each linked to one or more Freebase types. Due to our candidate selection process based on strong distributional evidence, SherLIiC is much harder than existing testbeds because distributional evidence is of little utility in the classification of InfCands. We also show that, due to its construction, many of SherLIiC's correct InfCands are novel and missing from existing rule bases. We evaluate a number of strong baselines on SherLIiC, ranging from semantic vector space models to state of the art neural models of natural language inference (NLI). We show that SherLIiC poses a tough challenge to existing NLI systems. ","SherLIiC: A Typed Event-Focused Lexical Inference Benchmark for
  Evaluating Natural Language Inference"
118,1136503353637257216,1002014071,Frank Wilczek,"[""New paper on how to tell whether you've got emergent QED and measure couplings, and why you should care <LINK>\nThresholds rule. <LINK>""]",https://arxiv.org/abs/1906.01628,"We calculate the effect of the emergent photon on threshold production of spinons in $U(1)$ Coulomb spin liquids such as quantum spin ice. The emergent Coulomb interaction modifies the threshold production cross-section dramatically, changing the weak turn-on expected from the density of states to an abrupt onset reflecting the basic coupling parameters. The slow photon typical in existing lattice models and materials suppresses the intensity at finite momentum and allows profuse Cerenkov radiation beyond a critical momentum. These features are broadly consistent with recent numerical and experimental results. ",Spectroscopy of spinons in Coulomb quantum spin liquids
119,1136321581851062273,933084565895286786,Dan Hooper,"['(1/3) I\'m excited about a new paper that I\'ve written with Carlos Blanco (@CBlanco2718), Sten Delos and Adrienne Erickcek:\n""Annihilation Signatures of Hidden Sector Dark Matter Within Early-FormingMicrohalos"".\n#DarkMatter \n<LINK>', '(2/3) In hidden sector dark matter models, there can be an era in the early universe that was dominated by matter (instead of radiation, as usual). During this era, density perturbations grow quickly, leading to the formation of a HUGE population of sub-Earth mass microhalos.', '(3/3) These microhalos lead to novel signatures that we can look for with gamma-ray telescopes, and provides us with a powerful way to test this interesting and exciting class of dark matter models.']",https://arxiv.org/abs/1906.00010,"If the dark matter is part of a hidden sector with only very feeble couplings to the Standard Model, the lightest particle in the hidden sector will generically be long-lived and could come to dominate the energy density of the universe prior to the onset of nucleosynthesis. During this early matter-dominated era, density perturbations will grow more quickly than otherwise predicted, leading to a large abundance of sub-earth-mass dark matter microhalos. Since the dark matter does not couple directly to the Standard Model, the minimum halo mass is much smaller than expected for weakly interacting dark matter, and the smallest halos could form during the radiation-dominated era. In this paper, we calculate the evolution of density perturbations within the context of such hidden sector models and use a series of $N$-body simulations to determine the outcome of nonlinear collapse during radiation domination. The resulting microhalos are extremely dense, which leads to very high rates of dark matter annihilation and to large indirect detection signals that resemble those ordinarily predicted for decaying dark matter. We find that the Fermi Collaboration's measurement of the high-latitude gamma-ray background rules out a wide range of parameter space within this class of models. The scenarios that are most difficult to constrain are those that feature a very long early matter-dominated era; if microhalos form prior to the decay of the unstable hidden sector matter, the destruction of these microhalos effectively heats the dark matter, suppressing the later formation of microhalos. ","Annihilation Signatures of Hidden Sector Dark Matter Within
  Early-Forming Microhalos"
120,1146543744461758469,2577596593,Chelsea Finn,"['How should robots learn from humans? While most use rewards or demos, we study how we might have agents learn tasks via interaction with humans.\n\nTry for yourself here!\n<LINK>\n\nw/ Mark Woodward, @hausman_k\nArxiv version: <LINK>']",https://arxiv.org/abs/1906.10187,"When deploying autonomous agents in the real world, we need effective ways of communicating objectives to them. Traditional skill learning has revolved around reinforcement and imitation learning, each with rigid constraints on the format of information exchanged between the human and the agent. While scalar rewards carry little information, demonstrations require significant effort to provide and may carry more information than is necessary. Furthermore, rewards and demonstrations are often defined and collected before training begins, when the human is most uncertain about what information would help the agent. In contrast, when humans communicate objectives with each other, they make use of a large vocabulary of informative behaviors, including non-verbal communication, and often communicate throughout learning, responding to observed behavior. In this way, humans communicate intent with minimal effort. In this paper, we propose such interactive learning as an alternative to reward or demonstration-driven learning. To accomplish this, we introduce a multi-agent training framework that enables an agent to learn from another agent who knows the current task. Through a series of experiments, we demonstrate the emergence of a variety of interactive learning behaviors, including information-sharing, information-seeking, and question-answering. Most importantly, we find that our approach produces an agent that is capable of learning interactively from a human user, without a set of explicit demonstrations or a reward function, and achieving significantly better performance cooperatively with a human than a human performing the task alone. ",Learning to Interactively Learn and Assist
121,1146091519172124672,606105909,Vijay Chidambaram,"['New work from our group led by @jayashree2912 on analyzing the Privacy Policies of GDPR-compliant EU companies. We analyze 10 EU cloud services, and find many privacy policies serious problems such as purpose bundling and unclear data sharing policies.\n\n<LINK> <LINK>', '@jayashree2912 As with our other GDPR-related work, this is in collaboration with the awesome Melissa Wasserman at @UTexasLaw :)']",https://arxiv.org/abs/1906.12038,"With the arrival of the European Union's General Data Protection Regulation (GDPR), several companies are making significant changes to their systems to achieve compliance. The changes range from modifying privacy policies to redesigning systems which process personal data. This work analyzes the privacy policies of large-scaled cloud services which seek to be GDPR compliant. The privacy policy is the main medium of information dissemination between the data controller and the users. We show that many services that claim compliance today do not have clear and concise privacy policies. We identify several points in the privacy policies which potentially indicate non-compliance; we term these GDPR vulnerabilities. We identify GDPR vulnerabilities in ten cloud services. Based on our analysis, we propose seven best practices for crafting GDPR privacy policies. ",Analyzing GDPR Compliance Through the Lens of Privacy Policy
122,1145744428675395584,743563342971813888,"Javad Seif, PhD","['Can we say that 80% of tweets are influenced by 20% of websites? Does the Pareto principle hold? Find out here, in a study @Imantahamtan and I did on tweets limited to #meeToo:\n<LINK> \n\n#textmining #tweeteranalytics #pareto']",https://arxiv.org/abs/1906.12321,"In this paper we examine the most influential resources shared on Twitter about the #MeToo movement. We also examine whether a small proportion of domain names and URLs (e.g. 20%) appear in a large number of tweets (e.g. 80%) that contain #MeToo (known as the 80/20 rule or Pareto principle). R and Python were used to analyze the data. Results demonstrated that the most frequently shared domains were twitter.com (47.20%), nytimes.com (4.42%) and youtube.com (3.69%). The most frequently shared content was a recent poll which indicated ""men are afraid to mentor women after the #MeToo movement"". In accordance with the Pareto principle, 8% of domain names accounted for 80% of the shared content on Twitter that contained #MeToo. This study provides a base for researchers who are interested in understanding what online resources people rely on when sharing information about online social movements (e.g. #MeToo). ","The Online Resources Shared on Twitter About the #MeToo Movement: The
  Pareto Principle"
123,1144772662713049088,500364862,Sergey Dorogovtsev,"['Our new work:\nG.J. Baxter, R.A. da Costa, S.N. Dorogovtsev, J.F.F. Mendes,\nComplex distributions emerging in filtering and compression\n<LINK>\nEach output is produced by a certain number of inputs. We find the distribution of this degeneracy:\nP(d) ‚àº e^{-(ln d)^a}']",https://arxiv.org/abs/1906.11266,"In filtering, each output is produced by a certain number of different inputs. We explore the statistics of this degeneracy in an explicitly treatable filtering problem in which filtering performs the maximal compression of relevant information contained in inputs (arrays of zeroes and ones). This problem serves as a reference model for the statistics of filtering and related sampling problems. The filter patterns in this problem conveniently allow a microscopic, combinatorial consideration. This allows us to find the statistics of outputs, namely the exact distribution of output degeneracies, for arbitrary input sizes. We observe that the resulting degeneracy distribution of outputs decays as $e^{-c\log^\alpha \!d}$ with degeneracy $d$, where $c$ is a constant and exponent $\alpha>1$, i.e. faster than a power law. Importantly, its form essentially depends on the size of the input data set, appearing to be closer to a power-law dependence for small data set sizes than for large ones. We demonstrate that for sufficiently small input data set sizes typical for empirical studies, this distribution could be easily perceived as a power law. We extend our results to filter patterns of various sizes and demonstrate that the shortest filter pattern provides the maximum informative representations of the inputs. ",Complex distributions emerging in filtering and compression
124,1144251719792177155,2267596599,Prof. Gregory Rudnick,['Outstanding work by Jeffrey Chan at on the rest-frame NIR luminosity function of distant passive cluster galaxies.  We find that low-mass galaxies were quenched by their environment in clusters even with the Universe was only 4 Billion yrs old (<LINK>).'],https://arxiv.org/abs/1906.10707,"We present results on the rest-frame $H$-band luminosity functions (LF) of red sequence galaxies in seven clusters at 1.0 < z < 1.3 from the Gemini Observations of Galaxies in Rich Early Environments Survey (GOGREEN). Using deep GMOS-z' and IRAC $3.6 \mu$m imaging, we identify red sequence galaxies and measure their LFs down to $M_{H} \sim M_{H}^{*} + (2.0 - 3.0)$. By stacking the entire sample, we derive a shallow faint end slope of $ \alpha \sim -0.35^{+0.15}_{-0.15} $ and $ M_{H}^{*} \sim -23.52^{+0.15}_{-0.17} $, suggesting that there is a deficit of faint red sequence galaxies in clusters at high redshift. By comparing the stacked red sequence LF of our sample with a sample of clusters at z~0.6, we find an evolution in the faint end of the red sequence over the ~2.6 Gyr between the two samples, with the mean faint end red sequence luminosity growing by more than a factor of two. The faint-to-luminous ratio of our sample ($0.78^{+0.19}_{-0.15}$) is consistent with the trend of decreasing ratio with increasing redshift as proposed in previous studies. A comparison with the field shows that the faint-to-luminous ratios in clusters are consistent with the field at z~1.15 and exhibit a stronger redshift dependence. Our results support the picture that the build up of the faint red sequence galaxies occurs gradually over time and suggest that faint cluster galaxies, similar to bright cluster galaxies, experience the quenching effect induced by environment already at z~1.15. ","The Rest-frame $H$-band Luminosity Function of Red Sequence Galaxies in
  Clusters at $1.0 &lt; z &lt; 1.3$"
125,1144135163405901825,561167071,Sascha Caron,"['Our new paper (mainly by @MCvBeekveld) on:\n\nThe current status of finetuning (FT) in Supersymmetry:\n\nDepending on the high-scale model and fine-tuning definition, we find a minimal\nFT of O(10%) (low scale measure) or O(1%) (high-scale).\n\n<LINK> <LINK>']",https://arxiv.org/abs/1906.10706,"In this paper, we minimize and compare two different fine-tuning measures in four high-scale supersymmetric models that are embedded in the MSSM. In addition, we determine the impact of current and future dark matter direct detection and collider experiments on the fine-tuning. We then compare the low-scale electroweak measure with the high-scale Barbieri-Giudice measure, which generally do not agree. However, we find that they do reduce to the same value when the higgsino parameter drives the degree of fine-tuning. Depending on the high-scale model and fine-tuning definition, we find a minimal fine-tuning of $3-38$ (corresponding to $\mathcal{O}(10-1)\%$) for the low-scale measure, and $63-571$ (corresponding to $\mathcal{O}(1-0.1)\%$) for the high-scale measure. In addition, minimally fine-tuned spectra give rise to a dark matter relic density that is between $10^{-3} < \Omega h^2 < 1$, when $\mu$ determines the minimum of the fine-tuning. We stress that it is too early to conclude on the fate of supersymmetry, based only on the fine-tuning paradigm. ",The current status of fine-tuning in supersymmetry
126,1143490301870448640,46153507,dr. Jordy Davelaar,"['New paper online! We modeled the accreting black hole in M87! The @ehtelescope models did not include non-thermal electrons, we studied the effect of including them. We were able to fit the radio to NIR fluxes of M87*, obtain source sizes and core shifts!\n\n<LINK> <LINK>', 'Co authors: Hector Olivares, Oliver Porth, @thomasbronzwaer, Michael Janssen, @froeloefs, Yosuke Mizuno, Christian Fromm, @hfalcke, and Luciano Rezzolla\n\n@RUastro @FlatironInst @FlatironCCA @goetheuni @uva_api', 'The model are based on cartesian adaptive mesh refined general-relativistic magnetohydrodynamics simulations (code: BHAC) containing more than 70M cells, resulting in a high resolution jet simulation. For the first time we ray trace this non uniform simulation data (code: RAPTOR) https://t.co/OLEiaTFNz3', 'Our non-thermal electrons are included by using parametrisations of first-principle particle-in-cell simulations by Ball et al. 2018 where they studied trans-relativistic reconnection. Allowing us to get the right spectral slope without fine tuning. https://t.co/LLqdHML7zT', 'Comparing to thermal models we see a more compact emission region, lower mass accretion rates, and lower jet powers. \n\nPaper is submitted to A&amp;A https://t.co/rcUuGx6UjY']",http://arxiv.org/abs/1906.10065,"The galaxy M 87 harbors a kiloparsec-scale relativistic jet, whose origin coincides with a supermassive black hole. Observational mm-VLBI campaigns are capable of resolving the jet-launching region at the scale of the event horizon. In order to provide a context for interpreting these observations, realistic general-relativistic magnetohydrodynamical (GRMHD) models of the accretion flow are constructed. The characteristics of the observed spectral-energy distribution (SED) depend on the shape of the electrons' energy-distribution function (eDF). The dependency on the eDF is omitted in the modeling of the first Event Horizon Telescope results. In this work, we aim to model the M 87 SED from radio up to NIR/optical frequencies using a thermal-relativistic Maxwell- J\""uttner distribution, as well as a relativistic $\kappa$-distribution function. The electrons are injected based on sub-grid, particle-in-cell parametrizations for sub-relativistic reconnection. A GRMHD simulation in Cartesian-Kerr-Schild coordinates, using eight levels of adaptive mesh refinement (AMR), forms the basis of our model. To obtain spectra and images, the GRMHD data is post-processed with the ray-tracing code RAPTOR, which is capable of ray tracing through AMR GRMHD simulation data. We obtain radio spectra in both the thermal-jet and $\kappa$-jet models consistent with radio observations. Additionally, the $\kappa$-jet models also recover the NIR/optical emission. The models recover the observed source sizes and core shifts and obtain a jet power of $\approx 10^{43}$ ergs/s. In the $\kappa$-jet models, both the accretion rates and jet powers are approximately two times lower than the thermal-jet model. The frequency cut-off observed at $\nu \approx 10^{15}$ Hz is recovered when the accelerator size is $10^6$ - $10^8$ cm, this could potentially point to an upper limit for plasmoid sizes in the jet of M 87. ","Modeling non-thermal emission from the jet-launching region of M 87 with
  adaptive mesh refinement"
127,1141503748323176454,887992016,Luke Metz,"['Our work exploring the use of learned optimizers to make more robust image models is on arXiv! We find that in some cases learned optimizers are capable of learning more robustness image classifiers!\n\n<LINK> <LINK>', 'This was a fun ICML workshop paper using some of the technology we developed in (https://t.co/gIiqPsbmQx) for new applications!\nThanks to my awesome collaborators: @niru_m, Jonathon Shlens, @jaschasd, @ekindogus']",https://arxiv.org/abs/1906.03367,"State-of-the art vision models can achieve superhuman performance on image classification tasks when testing and training data come from the same distribution. However, when models are tested on corrupted images (e.g. due to scale changes, translations, or shifts in brightness or contrast), performance degrades significantly. Here, we explore the possibility of meta-training a learned optimizer that can train image classification models such that they are robust to common image corruptions. Specifically, we are interested training models that are more robust to noise distributions not present in the training data. We find that a learned optimizer meta-trained to produce models which are robust to Gaussian noise trains models that are more robust to Gaussian noise at other scales compared to traditional optimizers like Adam. The effect of meta-training is more complicated when targeting a more general set of noise distributions, but led to improved performance on half of held-out corruption tasks. Our results suggest that meta-learning provides a novel approach for studying and improving the robustness of deep learning models. ",Using learned optimizers to make models robust to input noise
128,1141364596566925312,1082272780542988289,Gavindya,"['We (@Gavindya2, @OpenMaze, @slpmichalek) propose an approach to use cognitive memory to improve the accessibility of Digital Collections. Check out our blog at <LINK> ‚Ä¶ and the extended paper on arXiv <LINK> . @WebSciDL @oducs @ODUSCI']",https://arxiv.org/abs/1906.07183,"ADHD is being recognized as a diagnosis which persists into adulthood impacting economic, occupational, and educational outcomes. There is an increased need to accurately diagnose and recommend interventions for this population. One consideration is the development and implementation of reliable and valid outcome measures which reflect core diagnostic criteria. For example, adults with ADHD have reduced working memory capacity when compared to their peers (Michalek et al., 2014). A reduction in working memory capacity indicates attentional control deficits which align with many symptoms outlined on behavioral checklists used to diagnose ADHD. Using computational methods, such as eye tracking technology, to generate a relationship between ADHD and measures of working memory capacity would be useful to advancing our understanding and treatment of the diagnosis in adults. This chapter will outline a feasibility study in which eye tracking was used to measure eye gaze metrics during a working memory capacity task for adults with and without ADHD and machine learning algorithms were applied to generate a feature set unique to the ADHD diagnosis. The chapter will summarize the purpose, methods, results, and impact of this study. ","Eye Gaze Metrics and Analysis of AOI for Indexing Working Memory towards
  Predicting ADHD"
129,1140965195998777345,954335153723183104,Luciano da F. Costa,"['Consonance/dissonance are important properties of sound, and music is characterized by creative application of respective patterns.  How would combinations of sounds be affected by amplification?  We study this problem in this recent work:\n\n<LINK> <LINK>']",https://arxiv.org/abs/1906.06559,"After briefly revising the concepts of consonance/dissonance, a respective mathematic-computational model is described, based on Helmholtz's consonance theory and also considering the partials intensity. It is then applied to characterize five scale temperaments, as well as some minor and major triads and electronic amplification. In spite of the simplicity of the described model, a surprising agreement is often observed between the obtained consonances/dissonances and the typically observed properties of scales and chords. The representation of temperaments as graphs where links correspond to consonance (or dissonance) is presented and used to compare distinct temperaments, allowing the identification of two main groups of scales. The interesting issue of nonlinearities in electronic music amplification is also addressed while considering quadratic distortions, and it is shown that such nonlinearities can have drastic effect in changing the original patterns of consonance and dissonance. ","Modeling Consonance and its Relationships with Temperament, Harmony, and
  Electronic Amplification"
130,1139559167079960576,3053460216,Matthias Niessner,['Our new end-to-end CAD model retrieval and 9DoF alignment approach in 3D scans: <LINK>\n\nWe propose a differentiable Procrust for 9DOF alignment which makes our method roughly 250x faster and also over 19% more accurate than previous scan-to-CAD alignment methods. <LINK>'],https://arxiv.org/abs/1906.04201,"We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by $19.04\%$ for CAD model alignment to scans, with $\approx 250\times$ faster runtime than previous data-driven approaches. ",End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans
131,1138442087949643776,228792418,Tim Rockt√§schel,"['How can RL agents exploit the compositional, relational and hierarchical structure of the world? A growing number of authors propose learning from natural language. We are excited to share our @IJCAIconf survey of this emerging field! <LINK> \nTL;DR:ü§ñ+üìñ=üìàüéØüèÜü•≥ <LINK>']",https://arxiv.org/abs/1906.03926,"To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks. ",A Survey of Reinforcement Learning Informed by Natural Language
132,1145635599182942208,1360175358,Michele Starnini,"['In our last preprint w/ Fabian Baumann, @philipplenz6 and Igor Sokolov, we propose a model of radicalization dynamics in temporal networks able to reproduce some empirical features of echo chambers in social networks: check it out! <LINK>']",https://arxiv.org/abs/1906.12325,"Echo chambers and opinion polarization recently quantified in several sociopolitical contexts and across different social media, raise concerns on their potential impact on the spread of misinformation and on openness of debates. Despite increasing efforts, the dynamics leading to the emergence of these phenomena stay unclear. We propose a model that introduces the dynamics of radicalization, as a reinforcing mechanism driving the evolution to extreme opinions from moderate initial conditions. Inspired by empirical findings on social interaction dynamics, we consider agents characterized by heterogeneous activities and homophily. We show that the transition between a global consensus and emerging radicalized states is mostly governed by social influence and by the controversialness of the topic discussed. Compared with empirical data of polarized debates on Twitter, the model qualitatively reproduces the observed relation between users' engagement and opinions, as well as opinion segregation in the interaction network. Our findings shed light on the mechanisms that may lie at the core of the emergence of echo chambers and polarization in social media. ",Modeling echo chambers and polarization dynamics in social networks
133,1138658726716399616,18262687,Rushil,"['New preprint: We find that decoupling domain alignment from the final task improves domain adaptation. A simple subspace based alignment consistently outperforms adversarial DA like CDAN etc.  Exciting work from @kowshik0808, @jjayaram7 &amp; @pturaga1\n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1906.04338,"Unsupervised domain adaptation aims to transfer and adapt knowledge learned from a labeled source domain to an unlabeled target domain. Key components of unsupervised domain adaptation include: (a) maximizing performance on the target, and (b) aligning the source and target domains. Traditionally, these tasks have either been considered as separate, or assumed to be implicitly addressed together with high-capacity feature extractors. When considered separately, alignment is usually viewed as a problem of aligning data distributions, either through geometric approaches such as subspace alignment or through distributional alignment such as optimal transport. This paper represents a hybrid approach, where we assume simplified data geometry in the form of subspaces, and consider alignment as an auxiliary task to the primary task of maximizing performance on the source. The alignment is made rather simple by leveraging tractable data geometry in the form of subspaces. We synergistically allow certain parameters derived from the closed-form auxiliary solution, to be affected by gradients from the primary task. The proposed approach represents a unique fusion of geometric and model-based alignment with gradients from a data-driven primary task. Our approach termed SALT, is a simple framework that achieves comparable or sometimes outperforms state-of-the-art on multiple standard benchmarks. ","SALT: Subspace Alignment as an Auxiliary Learning Task for Domain
  Adaptation"
