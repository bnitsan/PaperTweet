,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1060876482212495360,842935264813096960,Marco Fiore,['Our new TMC paper pre-print is out at <LINK>. We use #mobilephonedata to infer the evolution of urban population dynamics at order-of-minute timescales. Data available at <LINK>. @vincentgauthier @dawn4ioe <LINK>'],https://arxiv.org/abs/1810.12909,"Communication-enabled devices routinely carried by individuals have become pervasive, opening unprecedented opportunities for collecting digital metadata about the mobility of large populations. In this paper, we propose a novel methodology for the estimation of people density at metropolitan scales, using subscriber presence metadata collected by a mobile operator. Our approach suits the estimation of static population densities, i.e., of the distribution of dwelling units per urban area contained in traditional censuses. More importantly, it enables the estimation of dynamic population densities, i.e., the time-varying distributions of people in a conurbation. By leveraging substantial real-world mobile network metadata and ground-truth information, we demonstrate that the accuracy of our solution is superior to that granted by state-of-the-art methods in practical heterogeneous urban scenarios. ","Estimation of Static and Dynamic Urban Populations with Mobile Network
  Metadata"
1,1060687394842861568,2800204849,Andrew Gordon Wilson,['Our new #NIPS2018 paper about scaling Gaussian processes with derivatives: <LINK> <LINK>'],https://arxiv.org/abs/1810.12283,"Gaussian processes (GPs) with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at $n$ points in $d$ dimensions requires linear solves and log determinants with an ${n(d+1) \times n(d+1)}$ positive definite matrix -- leading to prohibitive $\mathcal{O}(n^3d^3)$ computations for standard direct methods. We propose iterative solvers using fast $\mathcal{O}(nd)$ matrix-vector multiplications (MVMs), together with pivoted Cholesky preconditioning that cuts the iterations to convergence by several orders of magnitude, allowing for fast kernel learning and prediction. Our approaches, together with dimensionality reduction, enables Bayesian optimization with derivatives to scale to high-dimensional problems and large evaluation budgets. ",Scaling Gaussian Process Regression with Derivatives
2,1060527050770055168,882257115863187457,Sanjeev Arora,"['New blog post by Nadav Cohen. If we want to understand deep learning, we have to start analysing the trajectory of gradient descent rather than the landscape. <LINK>. The paper is here <LINK>']",https://arxiv.org/abs/1810.02281,"We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as $x \mapsto W_N W_{N-1} \cdots W_1 x$) by minimizing the $\ell_2$ loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018). ","A Convergence Analysis of Gradient Descent for Deep Linear Neural
  Networks"
3,1060383067783618561,797812753348132864,Martin Houde,"['Check out our new paper ‚ÄúTriggered superradiance and fast radio bursts,‚Äù with @FereshtehRajabi, @SciBry, et al., now accepted for MNRAS <LINK>', '@horsepharmer @FereshtehRajabi @SciBry Glad we could help! üôÇ']",https://arxiv.org/abs/1810.04364,"In this paper we develop a model for fast radio bursts (FRBs) based on triggered superradiance (SR) and apply it to previously published data of FRB 110220 and FRB 121102. We show how a young pulsar located at ~100 pc or more from an SR/FRB system could initiate the onset of a powerful burst of radiation detectable over cosmological distances. Our models using the OH$^2\Pi_{3/2}$ $\left(J=3/2\right)$ 1612 MHz and $^2\Pi_{3/2}$ $\left(J=5/2\right)$ 6030 MHz spectral lines match the light curves well and suggest the entanglement of more than $10^{30}$ initially inverted molecules over lengths of approximately 300 au for a single SR sample. SR also accounts for the observed temporal narrowing of FRB pulses with increasing frequency for FRB 121102, and predicts a scaling of the FRB spectral bandwidth with the frequency of observation, which we found to be consistent with the existing data. ",Triggered superradiance and fast radio bursts
4,1060285853937909765,91865755,Els de Wolf,['New @km3net paper on the Astro-Ph arXiv describes the potential of the future #ARCA detector of #KM3NeT to observe #neutrinos from known gamma-ray sources in our galaxy. Also the expected potential of #ARCA to observe extra-Galactic neutrinos is shown.\n<LINK>'],https://arxiv.org/abs/1810.08499v1,"KM3NeT will be a network of deep-sea neutrino telescopes in the Mediterranean Sea. The KM3NeT/ARCA detector, to be installed at the Capo Passero site (Italy), is optimised for the detection of high-energy neutrinos of cosmic origin. Thanks to its geographical location on the Northern hemisphere, KM3NeT/ARCA can observe upgoing neutrinos from most of the Galactic Plane, including the Galactic Centre. Given its effective area and excellent pointing resolution, KM3NeT/ARCA will measure or significantly constrain the neutrino flux from potential astrophysical neutrino sources. At the same time, it will test flux predictions based on gamma-ray measurements and the assumption that the gamma-ray flux is of hadronic origin. Assuming this scenario, discovery potential and sensitivity to a selected list of Galactic sources and to generic point sources with an $E^{-2}$ spectrum are presented. These spectra are assumed to be time independent. The results indicate that an observation with $3\sigma$ significance is possible in about six years of operation for the most intense sources, such as Supernovae Remnants RX J1713.7-3946 and Vela Jr. If no signal will be found during this time, the fraction of the gamma-ray flux coming from hadronic processes can be constrained to be below 50% for these two objects. ","] Sensitivity of the KM3NeT/ARCA neutrino telescope to point-like neutrino
  sources"
5,1059712639385919489,841031248839618560,Relja Arandjeloviƒá,"['Our new paper is out ""On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models"" <LINK> <LINK>']",https://arxiv.org/abs/1810.12715,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet. ","On the Effectiveness of Interval Bound Propagation for Training
  Verifiably Robust Models"
6,1059471458747981824,50036150,Jacob Gildenblat,['New paper with Roche on using CycleGan for virtual staining.\n<LINK>\n#digitalpathology #deeplearning'],https://arxiv.org/abs/1810.06415,"Histopathological evaluation of tissue samples is a key practice in patient diagnosis and drug development, especially in oncology. Historically, Hematoxylin and Eosin (H&E) has been used by pathologists as a gold standard staining. However, in many cases, various target specific stains, including immunohistochemistry (IHC), are needed in order to highlight specific structures in the tissue. As tissue is scarce and staining procedures are tedious, it would be beneficial to generate images of stained tissue virtually. Virtual staining could also generate in-silico multiplexing of different stains on the same tissue segment. In this paper, we present a sample application that generates FAP-CK virtual IHC images from Ki67-CD8 real IHC images using an unsupervised deep learning approach based on CycleGAN. We also propose a method to deal with tiling artifacts caused by normalization layers and we validate our approach by comparing the results of tissue analysis algorithms for virtual and real images. ","Virtualization of tissue staining in digital pathology using an
  unsupervised deep learning approach"
7,1058706270931304448,318652204,Sven Gowal,"['Our new paper ""On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models"" is on ArXiv <LINK>']",http://arxiv.org/abs/1810.12715,"Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet. ","On the Effectiveness of Interval Bound Propagation for Training
  Verifiably Robust Models"
8,1058697711195697152,408718170,Amanda Prorok,"['Awarded Best Paper at DARS 2018: our paper ""Redundant Robot Assignment on Graphs with Uncertain Edge Costs"" develops new foundational theory that shows how redundancy hedges against uncertainty. <LINK>']",https://arxiv.org/abs/1810.04016,"We provide a framework for the assignment of multiple robots to goal locations, when robot travel times are uncertain. Our premise is that time is the most valuable asset in the system. Hence, we make use of redundant robots to counter the effect of uncertainty and minimize the average waiting time at destinations. We apply our framework to transport networks represented as graphs, and consider uncertainty in the edge costs (i.e., travel time). Since solving the redundant assignment problem is strongly NP-hard, we exploit structural properties of our problem to propose a polynomial-time solution with provable sub-optimality bounds. Our method uses distributive aggregate functions, which allow us to efficiently (i.e., incrementally) compute the effective cost of assigning redundant robots. Experimental results on random graphs show that the deployment of redundant robots through our method reduces waiting times at goal locations, when edge traversals are uncertain. ",Redundant Robot Assignment on Graphs with Uncertain Edge Costs
9,1058449921030922247,14551614,Jason Weston,"['Engaging Image commenting &amp; dialogue via personality - two new papers.\n\nPaper 1: image commenting,  best model has near human performance in terms of engagingness üí£üí•üí´üî•üî•:\n<LINK> \n\nPaper 2: hot on its heelsüèÉ\u200d‚ôÄÔ∏èüë¢üë†üëû with full image chat:\n<LINK> <LINK>']",https://arxiv.org/abs/1810.10665,"Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., ""a man playing a guitar""). While such tasks are useful to verify that a machine understands the content of an image, they are not engaging to humans as captions. With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits. We build models that combine existing work from (i) sentence representations (Mazare et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images. We obtain state-of-the-art performance on Flickr30k and COCO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance. ",Engaging Image Captioning Via Personality
10,1058102446969577473,1610691422,Patrick Schwab,['The code for our Perfect Match paper (<LINK>) on learning counterfactual representations is now available on Github (MIT license). The code was designed to be easily extensible with new benchmarks and methods. Comments welcome!\nLink: <LINK>'],https://arxiv.org/abs/1810.00656,"Learning representations for counterfactual inference from observational data is of high practical relevance for many domains, such as healthcare, public policy and economics. Counterfactual inference enables one to answer ""What if...?"" questions, such as ""What would be the outcome if we gave this patient treatment $t_1$?"". However, current methods for training neural networks for counterfactual inference on observational data are either overly complex, limited to settings with only two available treatments, or both. Here, we present Perfect Match (PM), a method for training neural networks for counterfactual inference that is easy to implement, compatible with any architecture, does not add computational complexity or hyperparameters, and extends to any number of treatments. PM is based on the idea of augmenting samples within a minibatch with their propensity-matched nearest neighbours. Our experiments demonstrate that PM outperforms a number of more complex state-of-the-art methods in inferring counterfactual outcomes across several benchmarks, particularly in settings with many treatments. ","Perfect Match: A Simple Method for Learning Representations For
  Counterfactual Inference With Neural Networks"
11,1058007590569984001,24443979,Dan Stowell,"[""New paper from us - how to do time-domain audio source separation, efficiently, using Bayesian signal processing. <LINK>  Pablo's method outperforms NMF as well as a previous time-domain method - and the source-code is online too. #machinelistening @c4dm""]",https://arxiv.org/abs/1810.12679,"Gaussian process (GP) audio source separation is a time-domain approach that circumvents the inherent phase approximation issue of spectrogram based methods. Furthermore, through its kernel, GPs elegantly incorporate prior knowledge about the sources into the separation model. Despite these compelling advantages, the computational complexity of GP inference scales cubically with the number of audio samples. As a result, source separation GP models have been restricted to the analysis of short audio frames. We introduce an efficient application of GPs to time-domain audio source separation, without compromising performance. For this purpose, we used GP regression, together with spectral mixture kernels, and variational sparse GPs. We compared our method with LD-PSDTF (positive semi-definite tensor factorization), KL-NMF (Kullback-Leibler non-negative matrix factorization), and IS-NMF (Itakura-Saito NMF). Results show that the proposed method outperforms these techniques. ","Sparse Gaussian Process Audio Source Separation Using Spectrum Priors in
  the Time-Domain"
12,1057757196438700038,27089985,Andrew Hundt,"['I\'ve written a new paper on automatically designing and improving multi-input neural network architectures for robotics: ""Training Frankenstein\'s Creature to Stack: HyperTree Architecture Search"". ü§ñüëª A preprint is now on arXiv! Happy Halloween! <LINK> <LINK>']",https://arxiv.org/abs/1810.11714,"A robot can now grasp an object more effectively than ever before, but once it has the object what happens next? We show that a mild relaxation of the task and workspace constraints implicit in existing object grasping datasets can cause neural network based grasping algorithms to fail on even a simple block stacking task when executed under more realistic circumstances. To address this, we introduce the JHU CoSTAR Block Stacking Dataset (BSD), where a robot interacts with 5.1 cm colored blocks to complete an order-fulfillment style block stacking task. It contains dynamic scenes and real time-series data in a less constrained environment than comparable datasets. There are nearly 12,000 stacking attempts and over 2 million frames of real data. We discuss the ways in which this dataset provides a valuable resource for a broad range of other topics of investigation. We find that hand-designed neural networks that work on prior datasets do not generalize to this task. Thus, to establish a baseline for this dataset, we demonstrate an automated search of neural network based models using a novel multiple-input HyperTree MetaModel, and find a final model which makes reasonable 3D pose predictions for grasping and stacking on our dataset. The CoSTAR BSD, code, and instructions are available at this https URL ",The CoSTAR Block Stacking Dataset: Learning with Workspace Constraints
13,1057755163824414720,15751681,Irene Celino,['#DBLP added a new #paper <LINK> to my page <LINK>'],https://arxiv.org/abs/1810.10771,"We introduce our approach for incremental truth inference over the contributions provided by players of Games with a Purpose: we motivate the need for such a method with the specificity of GWAP vs. traditional crowdsourcing; we explain and formalize the proposed process and we explain its positive consequences; finally, we illustrate the results of an experimental comparison with state-of-the-art approaches, performed on data collected through two different GWAPs, thus showing the properties of our proposed framework. ","An Incremental Truth Inference Approach to Aggregate Crowdsourcing
  Contributions in Games with a Purpose"
14,1057692045635186688,2770265776,Peter Cox,"['New paper on the theoretical basis for an emergent constraint on climate sensitivity from global temperature variability, by Mark Williamson, @FemkeNijsse and me: <LINK>']",https://arxiv.org/abs/1810.12765,"There is as yet no theoretical framework to guide the search for emergent constraints. As a result, there are significant risks that indiscriminate data-mining of the multidimensional outputs from GCMs could lead to spurious correlations and less than robust constraints on future changes. To mitigate against this risk, Cox et al (hereafter CHW18) proposed a theory-motivated emergent constraint, using the one-box Hasselmann model to identify a linear relationship between ECS and a metric of global temperature variability involving both temperature standard deviation and autocorrelation ($\Psi$). A number of doubts have been raised about this approach, some concerning the theory and the application of the one-box model to understand relationships in complex GCMs which are known to have more than the single characteristic timescale. We illustrate theory driven testing of emergent constraints using this as an example, namely we demonstrate that the linear $\Psi$-ECS proportionality is not an artifact of the one-box model and rigorously features to a good approximation in more realistic, yet still analytically soluble conceptual models, namely the two-box and diffusion models. Each of the conceptual models predict different power spectra with only the diffusion model's pink spectrum being compatible with observations and the complex CMIP5 GCMs. We also show that the theoretically predicted $\Psi$-ECS relationship exists in the \texttt{piControl} as well as \texttt{historical} CMIP5 experiments and that the differing gradients of the proportionality are inversely related to the effective forcing in that experiment. ","Theoretical foundations of emergent constraints: relationships between
  climate sensitivity and global temperature variability in conceptual models"
15,1057675599601315841,989251872107085824,Quoc Le,"['Our latest paper on DropBlock, a new regularization method for convolutional networks. DropBlock drops continuous region of a feature map. It improves ImageNet top1 of ResNet from 76.5% to 78.1%, and COCO mAP of RetinaNet from 36.8% to 38.4%.\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/1810.12890,"Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves $78.13\%$ accuracy, which is more than $1.6\%$ improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from $36.8\%$ to $38.4\%$. ",DropBlock: A regularization method for convolutional networks
16,1057652787578896384,978500233368760320,Biao Zhang,"['Our new EMNLP2018 paper, entitled ""Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks"" is available online: <LINK>. A novel ATR recurrent cell with only 2 weights is proposed. Source code: <LINK>.']",https://arxiv.org/abs/1810.12546,"In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English- German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks. ","Simplifying Neural Machine Translation with Addition-Subtraction
  Twin-Gated Recurrent Networks"
17,1057650248535834630,245956666,Lina Necib,"['New paper out <LINK>! Using the #FIRE simulation to establish the correlation between the velocity of stars and that of dark matter, and extrapolating the amount of dark matter brought in by the #Gaia sausage, and its effect on direct detection!']",https://arxiv.org/abs/1810.12301,"The Gaia era opens new possibilities for discovering the remnants of disrupted satellite galaxies in the Solar neighborhood. If the population of local accreted stars is correlated with the dark matter sourced by the same mergers, one can then map the dark matter distribution directly. Using two cosmological zoom-in hydrodynamic simulations of Milky Way-mass galaxies from the Latte suite of Fire-2 simulations, we find a strong correlation between the velocity distribution of stars and dark matter at the solar circle that were accreted from luminous satellites. This correspondence holds for dark matter that is either relaxed or in kinematic substructure called debris flow, and is consistent between two simulated hosts with different merger histories. The correspondence is more problematic for streams because of possible spatial offsets between the dark matter and stars. We demonstrate how to reconstruct the dark matter velocity distribution from the observed properties of the accreted stellar population by properly accounting for the ratio of stars to dark matter contributed by individual mergers. After demonstrating this method using the Fire-2 simulations, we apply it to the Milky Way and use it to recover the dark matter velocity distribution associated with the recently discovered stellar debris field in the Solar neighborhood. Based on results from Gaia, we estimate that $42 ^{+26}_{-22}\%$ of the local dark matter that is accreted from luminous mergers is in debris flow. ","Under the Firelight: Stellar Tracers of the Local Dark Matter Velocity
  Distribution in the Milky Way"
18,1057458429684412416,72316187,Eric Nunes,"['New paper ""DARKMENTION: A Deployed System to Predict\nEnterprise-Targeted External Cyberattacks"" available on arXiv.\n<LINK>']",https://arxiv.org/abs/1810.12492,"Recent incidents of data breaches call for organizations to proactively identify cyber attacks on their systems. Darkweb/Deepweb (D2web) forums and marketplaces provide environments where hackers anonymously discuss existing vulnerabilities and commercialize malicious software to exploit those vulnerabilities. These platforms offer security practitioners a threat intelligence environment that allows to mine for patterns related to organization-targeted cyber attacks. In this paper, we describe a system (called DARKMENTION) that learns association rules correlating indicators of attacks from D2web to real-world cyber incidents. Using the learned rules, DARKMENTION generates and submits warnings to a Security Operations Center (SOC) prior to attacks. Our goal was to design a system that automatically generates enterprise-targeted warnings that are timely, actionable, accurate, and transparent. We show that DARKMENTION meets our goal. In particular, we show that it outperforms baseline systems that attempt to generate warnings of cyber attacks related to two enterprises with an average increase in F1 score of about 45% and 57%. Additionally, DARKMENTION was deployed as part of a larger system that is built under a contract with the IARPA Cyber-attack Automated Unconventional Sensor Environment (CAUSE) program. It is actively producing warnings that precede attacks by an average of 3 days. ","DARKMENTION: A Deployed System to Predict Enterprise-Targeted External
  Cyberattacks"
19,1057240333967339520,882883898685915137,C Schroeder de Witt,"['Our new paper ""Multi-Agent Common Knowledge Reinforcement Learning"" is  now on ArXiv:  <LINK> ! Thanks to my colleagues Jakob N. Foerster, Gregory Farquhar, Philip H. S. Torr,  Wendelin Boehmer, and Shimon Whiteson @j_foerst @greg_far @shimon8282 <LINK>']",https://arxiv.org/abs/1810.11702,"Cooperative multi-agent reinforcement learning often requires decentralised policies, which severely limit the agents' ability to coordinate their behaviour. In this paper, we show that common knowledge between agents allows for complex decentralised coordination. Common knowledge arises naturally in a large number of decentralised cooperative multi-agent tasks, for example, when agents can reconstruct parts of each others' observations. Since agents an independently agree on their common knowledge, they can execute complex coordinated policies that condition on this knowledge in a fully decentralised fashion. We propose multi-agent common knowledge reinforcement learning (MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical policy tree. Higher levels in the hierarchy coordinate groups of agents by conditioning on their common knowledge, or delegate to lower levels with smaller subgroups but potentially richer common knowledge. The entire policy tree can be executed in a fully decentralised fashion. As the lowest policy tree level consists of independent policies for each agent, MACKRL reduces to independently learnt decentralised policies as a special case. We demonstrate that our method can exploit common knowledge for superior performance on complex decentralised coordination tasks, including a stochastic matrix game and challenging problems in StarCraft II unit micromanagement. ",Multi-Agent Common Knowledge Reinforcement Learning
20,1057211869965025280,762420558,Ciaran O'Hare,"['New paper out today looking at the various changes and refinements we can make to the standard halo model post-#Gaia. <LINK>', 'In particular we add a new component to the model - the #GaiaSausage the remnants of a large merger event in the MWs history that provides a population of highly radially anisotropic dark matter https://t.co/HDm6QB72D3', 'This leads to shifts in DM limits, but the largest impact is for directionally sensitive experiments https://t.co/FcLd310Fjy']",https://arxiv.org/abs/1810.11468,"Predicting signals in experiments to directly detect dark matter (DM) requires a form for the local DM velocity distribution. Hitherto, the standard halo model (SHM), in which velocities are isotropic and follow a truncated Gaussian law, has performed this job. New data, however, suggest that a substantial fraction of our stellar halo lies in a strongly radially anisotropic population, the 'Gaia Sausage'. Inspired by this recent discovery, we introduce an updated DM halo model, the SHM$^{++}$, which includes a `Sausage' component, thus better describing the known features of our galaxy. The SHM$^{++}$ is a simple analytic model with five parameters: the circular speed, local escape speed and local DM density, which we update to be consistent with the latest data, and two new parameters: the anisotropy and the density of DM in the Sausage. The impact of the SHM$^{++}$ on signal models for WIMPs and axions is rather modest since the multiple changes and updates have competing effects. The largest change occurs for directional detectors which have sensitivity to the full three-dimensional velocity distribution. ","SHM$^{++}$: A Refinement of the Standard Halo Model for Dark Matter
  Searches in Light of the Gaia Sausage"
21,1057193546258751488,1940856026,Pranav Shyam,"['Check out our new paper:\n\nModel-Based Active Exploration\n<LINK>\n\nSimple Idea: \n- learn multiple models of the env\n- areas where the models agree are explored. area unexplored if they disagree\n- learn policies to get to unexplored areas using the models themselves', 'Common intrinsic motivation methods require that the agent to first experience some novelty. Then agent is incentivised to do more exploration around that area using bonus rewards. e Therefore we see this as ""reactive"" exploration. Encountering novelty by chance is rare', 'In contrast, we present ""active"" exploration: the agent plans to go to novel areas of the env by guessing where novelty could lie. We can predict the information that a policy can obtain about the env by measuring the disagreement in predicted future among an ensemble of models', 'We derive this algorithm from first principles in Bayesian setting with little assumptions. This naturally avoids issues such as the agent being distracted by noise, with prediction error based methods: noise appears as common confusion among all models, hence no disagreement.', 'We test the algorithm on a canonical hard-to-explore MDP. We find that it is at least an order of magnitude faster at exploring the environment compared to exploration bonus or posterior sampling based methods. It is also robust and not distracted by noise.']",https://arxiv.org/abs/1810.12162,"Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task. ",Model-Based Active Exploration
22,1057189643966238720,911769564500725761,Gautam Goel,"['I have a new paper up on arXiv! I show that Online Balanced Descent (OBD), an online learning algorithm I designed, gives state-of-the-art performance guarantees for important problems like online logistic regression and LQR control: <LINK>']",https://arxiv.org/abs/1810.10132,"We consider Online Convex Optimization (OCO) in the setting where the costs are $m$-strongly convex and the online learner pays a switching cost for changing decisions between rounds. We show that the recently proposed Online Balanced Descent (OBD) algorithm is constant competitive in this setting, with competitive ratio $3 + O(1/m)$, irrespective of the ambient dimension. Additionally, we show that when the sequence of cost functions is $\epsilon$-smooth, OBD has near-optimal dynamic regret and maintains strong per-round accuracy. We demonstrate the generality of our approach by showing that the OBD framework can be used to construct competitive algorithms for a variety of online problems across learning and control, including online variants of ridge regression, logistic regression, maximum likelihood estimation, and LQR control. ",Smoothed Online Optimization for Regression and Control
23,1057077610549821440,3301643341,Roger Grosse,"['Think you understand weight decay?  Think again. We found three distinct mechanisms by which it achieves a regularization effect, depending on the architecture and optimization algorithm. New paper w/ @Guodzh, Chaoqi Wang, and Bowen Xu.\n\n<LINK>', ""It's tempting in deep learning to just do what works, but it's important to sometimes dig deeper to track down what's really happening."", 'In this case, the answer often had to do with the nuts and bolts of the scales of parameters and how this interacts with optimization hyperparameters. Not the stuff we usually write papers about, but it seems to matter a lot.', '@ogrisel @Guodzh Each K-FAC update is about 50% more expensive than SGD or Adam. So you can mentally adjust the plots based on that. But note that our hyperparameters were chosen based on final validation accuracy, so they might not be optimal for earlier in training.', '@AmirRosenfeld @Guodzh Yeah, I guess one way to think about it is that learning rate schedules have a huge impact on final performance, and with BN, the WD parameter gives you a knob with which to tune the effective learning rate decay schedule.']",https://arxiv.org/abs/1810.12281,"Weight decay is one of the standard tricks in the neural network toolbox, but the reasons for its regularization effect are poorly understood, and recent results have cast doubt on the traditional interpretation in terms of $L_2$ regularization. Literal weight decay has been shown to outperform $L_2$ regularization for optimizers for which they differ. We empirically investigate weight decay for three optimization algorithms (SGD, Adam, and K-FAC) and a variety of network architectures. We identify three distinct mechanisms by which weight decay exerts a regularization effect, depending on the particular optimization algorithm and architecture: (1) increasing the effective learning rate, (2) approximately regularizing the input-output Jacobian norm, and (3) reducing the effective damping coefficient for second-order optimization. Our results provide insight into how to improve the regularization of neural networks. ",Three Mechanisms of Weight Decay Regularization
24,1057067939554242560,101980926,Masahito Yamazaki,"['My new paper in arXiv, this time with Andrew in <LINK> !\n""Lens Generalisation of œÑ-functions for the Elliptic Discrete Painlev√© Equation""\n<LINK>']",https://arxiv.org/abs/1810.12103,"We propose a new bilinear Hirota equation for $\tau$-functions associated with the $E_8$ root lattice, that provides a ""lens"" generalisation of the $\tau$-functions for the elliptic discrete Painlev\'e equation. Our equations are characterized by a positive integer $r$ in addition to the usual elliptic parameters, and involve a mixture of continuous variables with additional discrete variables, the latter taking values on the $E_8$ root lattice. We construct explicit $W(E_7)$-invariant hypergeometric solutions of this bilinear Hirota equation, which are given in terms of elliptic hypergeometric sum/integrals. ","Lens Generalisation of $\tau$-functions for the Elliptic Discrete
  Painlev\'e Equation"
25,1056885931553234950,109255123,Danny Caballero üá≤üáΩ,['Y‚Äôall should check out @johnmaiken‚Äôs new paper on analyzing intra-STEM switchers. <LINK>'],http://arxiv.org/abs/1810.11272,"Physics education research has used quantitative modeling techniques to explore learning, affect, and other aspects of physics education. However, these studies have rarely examined the predictive output of the models, instead focusing on the inferences or causal relationships observed in various data sets. This research introduces a modern predictive modeling approach to the PER community using transcript data for students declaring physics majors at Michigan State University (MSU). Using a machine learning model, this analysis demonstrates that students who switch from a physics degree program to an engineering degree program do not take the third semester course in thermodynamics and modern physics, and may take engineering courses while registered as a physics major. Performance in introductory physics and calculus courses, measured by grade as well as a students' declared gender and ethnicity play a much smaller role relative to the other features included the model. These results are used to compare traditional statistical analysis to a more modern modeling approach. ",Modeling student pathways in a physics bachelor's degree program
26,1056849403074568194,2932678322,Keaton Bell,"['The preprint of our new paper is out today titled ""Transition from spot to faculae domination---An alternate explanation for the dearth of intermediate Kepler rotation periods."" <LINK> 1/9', 'We use decades of measurements of photometric brightness (top) and a spectroscopic indicator of magnetic activity (bottom) for 30 stars to understand what features dominate the activity cycles: dark starspots or bright faculae. 2/9 https://t.co/Rnd17dpCrU', 'If stars are overall brighter when they are at the most active part of their cycle, they are facula dominated; if they are darker at peak activity, they are spot dominated. We fit simultaneous sinusoids to both data sets and compare phases for each star. 3/9 https://t.co/cWyyYXpG5K', 'We find that more active stars have spot-dominated activity cycles (top right) and less active stars have facula-dominated cycles (bottom left). The transition coincides with the Vaughan-Preston gap, where there is a noted absence of stars with intermediate activity levels. 4/9 https://t.co/Za86v6XLT8', 'This transition also coincides with a Rossby number of around 1, where the rotation period approximately equals the convective turnover time. 5/9 https://t.co/BxCpixWcko', 'We also see that younger, more active stars with spot-dominated activity cycles exhibit larger amplitudes of photometric variability. 6/9 https://t.co/5AMQjogEKJ', 'This transition happens at a stellar age of around 2550 Myr---much older than the location of a noted dearth of stars with intermediate rotation periods at an age of around 800 Myr from gyrochronology relations. 7/9 https://t.co/DUrRkvtkyN', 'Many explanations have been advanced to explain this rotation period gap, but none have been fully satisfying. Our results provide useful observational context for considering the problem. The overtake of faculae as the dominant feature does not happen in this gap. 8/9', 'We speculate that the spot-to-faculae transition may begin ~800 Myr, with the earliest faculae being localized near spots, causing photometric cancellation and undetectable rotation periods. Later, faculae networks go global and rotation from spots can again be measured. 9/9']",https://arxiv.org/abs/1810.11250,"The study of stellar activity cycles is crucial to understand the underlying dynamo and how it causes activity signatures such as dark spots and bright faculae. We study the appearance of activity signatures in contemporaneous photometric and chromospheric time series. Lomb-Scargle periodograms are used to search for cycle periods present in both time series. To emphasize the signature of the activity cycle we account for rotation-induced scatter in both data sets by fitting a quasi-periodic Gaussian process model to each observing season. After subtracting the rotational variability, cycle amplitudes and the phase difference between the two time series are obtained by fitting both time series simultaneously using the same cycle period. We find cycle periods in 27 of the 30 stars in our sample. The phase difference between the two time series reveals that the variability in fast rotating active stars is usually in anti-phase, while the variability of slowly rotating inactive stars is in phase. The photometric cycle amplitudes are on average six times larger for the active stars. The phase and amplitude information demonstrates that active stars are dominated by dark spots, whereas less active stars are dominated by bright faculae. We find the transition from spot to faculae domination at the Vaughan-Preston gap, and around a Rossby number equal to one. We conclude that faculae are the dominant ingredient of stellar activity cycles at ages >2.55 Gyr. The data further suggest that the Vaughan-Preston gap can not explain the previously detected dearth of Kepler rotation periods between 15-25 days. Nevertheless, our results led us to propose an explanation for the rotation period dearth to be due to the non-detection of periodicity caused by the cancellation of dark spots and bright faculae at 800 Myr. ","Transition from spot to faculae domination -- An alternate explanation
  for the dearth of intermediate \textit{Kepler} rotation periods"
27,1056722709617340416,296161364,Chris Power,['Congratulations @Foivos_Diak! New paper on arXiv on his work from time at @ICRAR - with @Cosmic_Horizons et al. - on deriving dark matter mass profiles for dwarf satellites and overcoming the challenge of the velocity anisotropy! See <LINK>. <LINK>'],https://arxiv.org/abs/1810.11375,"We present an innovative approach to the methodology of dynamical modelling, allowing practical reconstruction of the underlying dark matter mass without assuming both the density and anisotropy functions. With this, the mass-anisotropy degeneracy is reduced to simple model inference, incorporating the uncertainties inherent with observational data, statistically circumventing the mass-anisotropy degeneracy in spherical collisionless systems. We also tackle the inadequacy that the Jeans method of moments has on small datasets, with the aid of Generative Adversarial Networks: we leverage the power of artificial intelligence to reconstruct non-parametrically the projected line-of-sight velocity distribution. We show with realistic numerical simulations of dwarf spheroidal galaxies that we can distinguish between competing dark matter distributions and recover the anisotropy and mass profile of the system. ",Reliable mass calculation in spherical gravitating systems
28,1056711764543328257,2337598033,Geraint F. Lewis,['New paper on the arXiv with @Foivos_Diak @astroconfusion @doctorcbpower  &amp; Rodrigo Ibata and Mark Wilkinson - the journey of this paper has been the definition of agony and ecstasy! \n\n<LINK> <LINK>'],https://arxiv.org/abs/1810.11375,"We present an innovative approach to the methodology of dynamical modelling, allowing practical reconstruction of the underlying dark matter mass without assuming both the density and anisotropy functions. With this, the mass-anisotropy degeneracy is reduced to simple model inference, incorporating the uncertainties inherent with observational data, statistically circumventing the mass-anisotropy degeneracy in spherical collisionless systems. We also tackle the inadequacy that the Jeans method of moments has on small datasets, with the aid of Generative Adversarial Networks: we leverage the power of artificial intelligence to reconstruct non-parametrically the projected line-of-sight velocity distribution. We show with realistic numerical simulations of dwarf spheroidal galaxies that we can distinguish between competing dark matter distributions and recover the anisotropy and mass profile of the system. ",Reliable mass calculation in spherical gravitating systems
29,1056606478285455360,2235411914,Surya Ganguli,"['1/ New #deeplearning paper at the intersection of #AI  #mathematics #psychology and #neuroscience: A mathematical theory of semantic development in deep neural networks: <LINK> Thanks to awesome collaborators Andrew Saxe and Jay McClelland! <LINK>', '2/ We study how many phenomena in human semantic cognition arise in deep neural networks, and how these phenomena can be understood analytically in a simple deep linear network.  Such phenomena include‚Ä¶', '3/ The hierarchical differentiation of concepts over infant semantic development: children acquire broad categorical distinctions before they acquire fine categorical distinctions', '4/ Rapid stage-like transitions in developmental learning in which children seem to learn nothing for a while and then suddenly acquire a new concept', '5/ The existence of a firm belief in incorrect facts over child development which cannot be explained by classical association models since data supporting such facts are never present in the world: i.e. ‚Äúworms have bones‚Äù', '6/ The notion of typical versus atypical members of a category and why neural networks can more quickly semantically process typical members (i.e. a canary is a typical bird but an ostrich is not)', '7/ Some categories are more ‚Äúcoherent‚Äù (i.e. set of dogs), and others are not (i.e. set of ‚Äúall things blue.‚Äù ).  We define a quantitative notion of category coherence and prove mathematically that more coherent categories are learned faster', '8/ Basic level effects: explaining why neural networks may preferentially learn and name structure at a basic level of hierarchy (i.e. bird), rather than superordinate (i.e. animal) or subordinate (i.e. robin) levels', '9/ Changing patterns of inductive generalization over development: why children and neural networks over-generalize early in learning and then progressively restrict their patterns of generalization', '10/ Representational similarity analysis (RSA):  two deep linear nets trained on the same data from diff initial weights will learn hidden reps with the same similarity structure *if and only if* learning is optimal (smallest norm weights): RSA = invariant of optimal learning!', '11/ Surprising how much can be explained using the nonlinear dynamics of learning in deep linear nets; but of course much left unexplained:  context dependence, causal inference, binding; many adventures left. Thanks again to awesome collaborators Andrew Saxe and Jay McClelland!']",https://arxiv.org/abs/1810.10531,"An extensive body of empirical research has revealed remarkable regularities in the acquisition, organization, deployment, and neural representation of human semantic knowledge, thereby raising a fundamental conceptual question: what are the theoretical principles governing the ability of neural networks to acquire, organize, and deploy abstract knowledge by integrating across many individual experiences? We address this question by mathematically analyzing the nonlinear dynamics of learning in deep linear networks. We find exact solutions to this learning dynamics that yield a conceptual explanation for the prevalence of many disparate phenomena in semantic cognition, including the hierarchical differentiation of concepts through rapid developmental transitions, the ubiquity of semantic illusions between such transitions, the emergence of item typicality and category coherence as factors controlling the speed of semantic processing, changing patterns of inductive projection over development, and the conservation of semantic similarity in neural representations across species. Thus, surprisingly, our simple neural model qualitatively recapitulates many diverse regularities underlying semantic development, while providing analytic insight into how the statistical structure of an environment can interact with nonlinear deep learning dynamics to give rise to these regularities. ",A mathematical theory of semantic development in deep neural networks
30,1055978577617801216,374233623,Shane Barratt,"['New paper on Learning Probabilistic Trajectory Models of Aircraft in Terminal Airspace from Position Data. Includes a ""pilot turing test"" <LINK>']",https://arxiv.org/abs/1810.09568v1,"Models for predicting aircraft motion are an important component of modern aeronautical systems. These models help aircraft plan collision avoidance maneuvers and help conduct offline performance and safety analyses. In this article, we develop a method for learning a probabilistic generative model of aircraft motion in terminal airspace, the controlled airspace surrounding a given airport. The method fits the model based on a historical dataset of radar-based position measurements of aircraft landings and takeoffs at that airport. We find that the model generates realistic trajectories, provides accurate predictions, and captures the statistical properties of aircraft trajectories. Furthermore, the model trains quickly, is compact, and allows for efficient real-time inference. ","] Learning Probabilistic Trajectory Models of Aircraft in Terminal
  Airspace from Position Data"
31,1055901568799854592,2441942726,Aleksas Mazeliauskas,"['""Prescaling and far-from-equilibrium hydrodynamics in the quark-gluon plasma"" - my new paper with J√ºrgen Berges. #isoquant \n<LINK>']",https://arxiv.org/abs/1810.10554,"Prescaling is a far-from-equilibrium phenomenon which describes the rapid establishment of a universal scaling form of distributions much before the universal values of their scaling exponents are realized. We consider the example of the spatio-temporal evolution of the quark-gluon plasma explored in heavy-ion collisions at sufficiently high energies. Solving QCD kinetic theory with elastic and inelastic processes, we demonstrate that the gluon and quark distributions very quickly adapt a self-similar scaling form, which is independent of initial condition details and system parameters. The dynamics in the prescaling regime is then fully encoded in a few time-dependent scaling exponents, whose slow evolution gives rise to far-from-equilibrium hydrodynamic behavior. ","Prescaling and far-from-equilibrium hydrodynamics in the quark-gluon
  plasma"
32,1055858024642961409,888921414555901953,Alex Riley,"['Paper day! <LINK> Using new Gaia DR2 proper motions for 38 MW satellites, we modeled the velocity anisotropy (beta) of the system. Satellites near the center of the MW have tangentially-biased motions while satellites further out have radially-biased motions <LINK>', 'We also looked at simulations in the APOSTLE and Auriga suites.  We found that the anisotropy profiles for the DMO and APOSTLE w/ baryon halos are isotropic at all radii, while the Auriga halos actually had more tangential motions and a dip in beta near the center https://t.co/Df7Ngi76nI', ""This is because the Auriga simulations form nice stellar disks, comparable to the Milky Way's. The disks preferentially destroy satellites on radial orbits which pass near the center, resulting in a tangential beta value (see @SheaGKosmo et al. 2017 https://t.co/kBZTxzeiw3)"", 'However, our inferred beta profile depends on the radial distribution of sats. When matching to that of the MW, we get beta dips even in DMO sims. The MW sats are more centrally concentrated than in both sims and M31, but our surveys are biased to close things (LSST will help) https://t.co/quJH7l6UtG', 'On a final note, we compare our anisotropy profile with those from other tracers. Our different answer may suggest that halo stars and GCs might have both originated in dark matter halos which were destroyed by the stellar disk, maintaining the radial orbits of their progenitor https://t.co/lU7a0oFUin', 'My thanks to all my co-authors, including @LStrigari @Facundoag79 and the APOSTLE and Auriga collaborations']",https://arxiv.org/abs/1810.10645,"We analyse the orbital kinematics of the Milky Way (MW) satellite system utilizing the latest systemic proper motions for 38 satellites based on data from Gaia Data Release 2. Combining these data with distance and line-of-sight velocity measurements from the literature, we use a likelihood method to model the velocity anisotropy, $\beta$, as a function of Galactocentric distance and compare the MW satellite system with those of simulated MW-mass haloes from the APOSTLE and Auriga simulation suites. The anisotropy profile for the MW satellite system increases from $\beta\sim -2$ at $r\sim20$ kpc to $\beta\sim 0.5$ at $r\sim200$ kpc, indicating that satellites closer to the Galactic centre have tangentially-biased motions while those farther out have radially-biased motions. The motions of satellites around APOSTLE host galaxies are nearly isotropic at all radii, while the $\beta(r)$ profiles for satellite systems in the Auriga suite, whose host galaxies are substantially more massive in baryons than those in APOSTLE, are more consistent with that of the MW satellite system. This shape of the $\beta(r)$ profile may be attributed to the central stellar disc preferentially destroying satellites on radial orbits, or intrinsic processes from the formation of the Milky Way system. ",The velocity anisotropy of the Milky Way satellite system
33,1055822590508445696,729735371475554304,Markus D Schirmer,"[""New preprint is out (<LINK>) and paper submitted. I'm excited to see what the community thinks of our (easy to apply) subnetwork identification in the connectome using network structural dependency. @nsanar @aiwernc <LINK>""]",https://arxiv.org/abs/1810.09935,"Principles of network topology have been widely studied in the human connectome. Of particular interest is the modularity of the human brain, where the connectome is divided into subnetworks and subsequently changes with development, aging or disease are investigated. We present a weighted network measure, the Network Dependency Index (NDI), to identify an individual region's importance to the global functioning of the network. Importantly, we utilize NDI to differentiate four subnetworks (Tiers) in the human connectome following Gaussian Mixture Model fitting. We analyze the topological aspects of each subnetwork with respect to age and compare it to rich-club based subnetworks (rich-club, feeder and seeder). Our results first demonstrate the efficacy of NDI to identify more consistent, central nodes of the connectome across age-groups, when compared to the rich-club framework. Stratifying the connectome by NDI led to consistent subnetworks across the life-span revealing distinct patterns associated with age where, e.g., the key relay nuclei and cortical regions are contained in a subnetwork with highest NDI. The divisions of the human connectome derived from our data-driven NDI framework have the potential to reveal topological alterations described by network measures through the life-span. ","Network Structural Dependency in the Human Connectome Across the
  Life-Span"
34,1055642553880588289,28535459,Dougal Mackey,"['I‚Äôm about to jump on a very long flight home from Chile, but before I do here‚Äôs a new paper on today‚Äôs arXiv from myself and a group of PAndAS colleagues incl. @Cosmic_Horizons @nfmartin1980 @michelle_lmc and @ickbat \n<LINK>', '... where we trace the accretion history of M31 using its globular cluster system!']",https://arxiv.org/abs/1810.10719,"We utilise the final catalogue from the Pan-Andromeda Archaeological Survey to investigate the links between the globular cluster system and field halo in M31 at projected radii $R_p=25-150$ kpc. In this region the cluster radial density profile exhibits a power-law decline with index $\Gamma=-2.37\pm0.17$, matching that for the stellar halo component with [Fe/H] $<-1.1$. Spatial density maps reveal a striking correspondence between the most luminous substructures in the metal-poor field halo and the positions of many globular clusters. By comparing the density of metal-poor halo stars local to each cluster with the azimuthal distribution at commensurate radius, we reject the possibility of no correlation between clusters and field overdensities with high confidence. We use our stellar density measurements and previous kinematic data to demonstrate that $\approx35-60\%$ of clusters exhibit properties consistent with having been accreted into the outskirts of M31 at late times with their parent dwarfs. Conversely, at least $\sim40\%$ of remote clusters show no evidence for a link with halo substructure. The radial density profile for this subgroup is featureless and closely mirrors that observed for the apparently smooth component of the metal-poor stellar halo. We speculate that these clusters are associated with the smooth halo; if so, their properties appear consistent with a scenario where the smooth halo was built up at early times via the destruction of primitive satellites. In this picture the entire M31 globular cluster system outside $R_p=25$ kpc comprises objects accumulated from external galaxies over a Hubble time of growth. ","The outer halo globular cluster system of M31 - III. Relationship to the
  stellar halo"
35,1055628035947401216,3301643341,Roger Grosse,"['Reversible RNNs: reduce memory costs of GRU and LSTM networks by 10-15x without loss in performance. Also 5-10x for attention-based architectures. New paper with Matt MacKay, Paul Vicol, and Jimmy Ba, to appear at NIPS. <LINK>']",https://arxiv.org/abs/1810.10999,"Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs---RNNs for which the hidden-to-hidden transition can be reversed---offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. We first show that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. We then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting. Our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10--15. We extend our technique to attention-based sequence-to-sequence models, where it maintains performance while reducing activation memory cost by a factor of 5--10 in the encoder, and a factor of 10--15 in the decoder. ",Reversible Recurrent Neural Networks
36,1055514818940039168,93358739,Mandar Joshi,"['New paper! pair2vec: We train word pair embeddings from predictive contexts, and use them to inject background knowledge into QA &amp; NLI models. We get significant gains on top of ELMo, including SotA on the adversarial Glockner NLI dataset (93.4). <LINK> (1/n) <LINK>', 'Why embed word *pairs*? Because QA &amp; NLI often involve reasoning over implied relations (e.g. lexical or common sense) between pairs, and relying solely on end-task data for this background knowledge often results in poor generalization. (2/n) https://t.co/MsmiMiYba2', 'We train pair representations on Wikipedia by maximizing the PMI between pairs and contexts. The intuition is that contexts (e.g.,  ""X is a famous Y"") for a pair (X, Y) often encode their relation (e.g. profession). (3/n)', 'Embedding word pairs close to their predictive contexts helps us capture such relations. During end-task training, we add them to the cross-sentence attention layer of existing ELMo-equipped models. (4/n) https://t.co/llgml67CDm', 'Our experiments show a gain of 2.7 F1 on SQuAD 2.0 and 1.3% on MultiNLI. The models do particularly well on adversarial datasets with 6-7% gains on adversarial SQuAD and 8.8% on the Glockner NLI test set. (5/n) https://t.co/UQVjIn5U1W', 'Our analysis shows that pair2vec is complementary to single-word embeddings. Interpolating pair2vec with fastText significantly improves encyclopedic and lexical-semantic word analogies. (6/n) https://t.co/tVOpomKP8x', ""Joint work with @eunsolc, @omerlevy_, @dsweld, and @LukeZettlemoyer. Here's the link to the paper again -- https://t.co/bxvaNccaff (7/7)"", '@seb_ruder @eunsolc @omerlevy_ @dsweld @LukeZettlemoyer Thanks Sebastian :)']",https://arxiv.org/abs/1810.08854,"Reasoning about implied relationships (e.g., paraphrastic, common sense, encyclopedic) between pairs of words is crucial for many cross-sentence inference problems. This paper proposes new methods for learning and using embeddings of word pairs that implicitly represent background knowledge about such relationships. Our pairwise embeddings are computed as a compositional function on word representations, which is learned by maximizing the pointwise mutual information (PMI) with the contexts in which the two words co-occur. We add these representations to the cross-sentence attention layer of existing inference models (e.g. BiDAF for QA, ESIM for NLI), instead of extending or replacing existing word embeddings. Experiments show a gain of 2.7% on the recently released SQuAD2.0 and 1.3% on MultiNLI. Our representations also aid in better generalization with gains of around 6-7% on adversarial SQuAD datasets, and 8.8% on the adversarial entailment test set by Glockner et al. (2018). ","pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence
  Inference"
37,1055495768134598658,2235411914,Surya Ganguli,['Our new #machinelearning paper to appear at #NIPS2018: Statistical Mechanics of Low Rank Tensor Decomposition <LINK> Congrats to @kadmonj for expertly leading this project! <LINK>'],https://arxiv.org/abs/1810.10065,"Often, large, high dimensional datasets collected across multiple modalities can be organized as a higher order tensor. Low-rank tensor decomposition then arises as a powerful and widely used tool to discover simple low dimensional structures underlying such data. However, we currently lack a theoretical understanding of the algorithmic behavior of low-rank tensor decompositions. We derive Bayesian approximate message passing (AMP) algorithms for recovering arbitrarily shaped low-rank tensors buried within noise, and we employ dynamic mean field theory to precisely characterize their performance. Our theory reveals the existence of phase transitions between easy, hard and impossible inference regimes, and displays an excellent match with simulations. Moreover, it reveals several qualitative surprises compared to the behavior of symmetric, cubic tensor decomposition. Finally, we compare our AMP algorithm to the most commonly used algorithm, alternating least squares (ALS), and demonstrate that AMP significantly outperforms ALS in the presence of noise. ",Statistical mechanics of low-rank tensor decomposition
38,1055472143062974465,9628042,Mark Brophy,"['Check out our new paper, Structured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data.\n\nPaper: <LINK>\nVideo: <LINK>', ""@xave_ruth We'd definitely like to focus on pedestrians in the future, especially pedestrians with dyscalculia ;)""]",https://arxiv.org/abs/1810.10093,"We present structured domain randomization (SDR), a variant of domain randomization (DR) that takes into account the structure and context of the scene. In contrast to DR, which places objects and distractors randomly according to a uniform probability distribution, SDR places objects and distractors randomly according to probability distributions that arise from the specific problem at hand. In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection. We demonstrate the power of SDR for the problem of 2D bounding box car detection, achieving competitive results on real data after training only on synthetic data. On the KITTI easy, moderate, and hard tasks, we show that SDR outperforms other approaches to generating synthetic data (VKITTI, Sim 200k, or DR), as well as real data collected in a different domain (BDD100K). Moreover, synthetic SDR data combined with real KITTI data outperforms real KITTI data alone. ","Structured Domain Randomization: Bridging the Reality Gap by
  Context-Aware Synthetic Data"
39,1055378031894818816,926551285788233728,Vincent Fortuin,['Our new paper on sparse discrete Gaussian Process approximations is online now: <LINK>'],https://arxiv.org/abs/1810.10368,"Kernel methods on discrete domains have shown great promise for many challenging data types, for instance, biological sequence data and molecular structure data. Scalable kernel methods like Support Vector Machines may offer good predictive performances but do not intrinsically provide uncertainty estimates. In contrast, probabilistic kernel methods like Gaussian Processes offer uncertainty estimates in addition to good predictive performance but fall short in terms of scalability. While the scalability of Gaussian processes can be improved using sparse inducing point approximations, the selection of these inducing points remains challenging. We explore different techniques for selecting inducing points on discrete domains, including greedy selection, determinantal point processes, and simulated annealing. We find that simulated annealing, which can select inducing points that are not in the training set, can perform competitively with support vector machines and full Gaussian processes on synthetic data, as well as on challenging real-world DNA sequence data. ",Scalable Gaussian Processes on Discrete Domains
40,1055309605645897729,869862586610851840,Jeannette Bohg,"['Happy to share our new paper on Making Sense of Vision and Touch: \nSelf-Supervised Learning of Multimodal Representations for Contact-Rich Tasks. Featuring RL on a real robot.  <LINK>  <LINK> <LINK>', 'Kudos to Michelle Lee, @yukez, Krishnan Srinivasan, Parth Shah, @silviocinguetta, @drfeifei  and @animesh_garg']",https://arxiv.org/abs/1810.10191,"Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented. ","Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal
  Representations for Contact-Rich Tasks"
41,1055273722054041602,101810581,Animesh Garg,"['Very excited to share our new paper on Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations. Featuring RL on real-robots from scratch in a matter of hours without any simulation! \nVideo: <LINK>\narXiv: <LINK>', 'Joint work with Michelle Lee, @yukez, Krishnan Srinivasan, Parth Shah, @drfeifei, @silviocinguetta, @leto__jean']",https://arxiv.org/abs/1810.10191,"Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented. ","Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal
  Representations for Contact-Rich Tasks"
42,1055080172284768256,51257255,Natasha Jaques,"['Excited to release our new multi-agent RL paper showing that when agents receive a social reward for having causal influence over other agents, it leads to enhanced cooperation and emergent communication <LINK>', '@daniel_bilar @hardmaru Hey, could you point me to the paper you mean? From what I remember of reading Social Physics, it mainly looked at idea flow in human social networks, not building this into agents. Very curious about this!', ""@daniel_bilar @hardmaru Oh awesome, yes, I think Sandy's work has made a lot of interesting contributions to understanding human social influence.""]",https://arxiv.org/abs/1810.08647,"We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timestep, an agent simulates alternate actions that it could have taken, and computes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. Empirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the deep RL agents, and leading to more meaningful learned communication protocols. The influence rewards for all agents can be computed in a decentralized way by enabling agents to learn a model of other agents using deep neural networks. In contrast, key previous works on emergent communication in the MARL setting were unable to learn diverse policies in a decentralized manner and had to resort to centralized training. Consequently, the influence reward opens up a window of new opportunities for research in this area. ","Social Influence as Intrinsic Motivation for Multi-Agent Deep
  Reinforcement Learning"
43,1055046291154198528,53888261,Dominic Horsman,"['New paper! A new way to coherently-control quantum channels (or: how to put two channels into superposition using this one cool trick)\n<LINK> <LINK>', ""@qubyte They are good! I can't claim credit for them, they were Alastair :)""]",https://arxiv.org/abs/1810.09826,"A completely depolarising quantum channel always outputs a fully mixed state and thus cannot transmit any information. In a recent Letter [D. Ebler et al., Phys. Rev. Lett. 120, 120502 (2018)], it was however shown that if a quantum state passes through two such channels in a quantum superposition of different orders - a setup known as the ""quantum switch"" - then information can nevertheless be transmitted through the channels. Here, we show that a similar effect can be obtained when one coherently controls between sending a target system through one of two identical depolarising channels. Whereas it is tempting to attribute this effect in the quantum switch to the indefinite causal order between the channels, causal indefiniteness plays no role in this new scenario. This raises questions about its role in the corresponding effect in the quantum switch. We study this new scenario in detail and we see that, when quantum channels are controlled coherently, information about their specific implementation is accessible in the output state of the joint control-target system. This allows two different implementations of what is usually considered to be the same channel to therefore be differentiated. More generally, we find that to completely describe the action of a coherently controlled quantum channel, one needs to specify not only a description of the channel (e.g., in terms of Kraus operators), but an additional ""transformation matrix"" depending on its implementation. ",Communication through coherent control of quantum channels
44,1055033456315510785,134603954,Alejandra Avalos,"['Very excited about this new paper with @21stCenturySci and #DavidRossell:\n""Heterogeneous large datasets integration using Bayesian factor regression""\n<LINK>']",https://arxiv.org/abs/1810.09894,"Two key challenges in modern statistical applications are the large amount of information recorded per individual, and that such data are often not collected all at once but in batches. These batch effects can be complex, causing distortions in both mean and variance. We propose a novel sparse latent factor regression model to integrate such heterogeneous data. The model provides a tool for data exploration via dimensionality reduction while correcting for a range of batch effects. We study the use of several sparse priors (local and non-local) to learn the dimension of the latent factors. Our model is fitted in a deterministic fashion by means of an EM algorithm for which we derive closed-form updates, contributing a novel scalable algorithm for non-local priors of interest beyond the immediate scope of this paper. We present several examples, with a focus on bioinformatics applications. Our results show an increase in the accuracy of the dimensionality reduction, with non-local priors substantially improving the reconstruction of factor cardinality, as well as the need to account for batch effects to obtain reliable results. Our model provides a novel approach to latent factor regression that balances sparsity with sensitivity and is highly computationally efficient. ","Heterogeneous large datasets integration using Bayesian factor
  regression"
45,1054902731352100864,759118366468481024,Decker French,"['New paper out today on identifying TDE candidates using their host galaxies! <LINK>', 'Because the TDE rate is high in post-starburst and ""quiescent Balmer-strong"" galaxies, and because their star formation rates are low, a large fraction of transients in these galaxies will be TDEs with low contamination from supernovae.', 'This makes it efficient to follow up transients in such galaxies in order to (1) get rapid follow-up observations and (2) perhaps identify unusual TDEs. This strategy will help identify more TDEs from the deluge of LSST events.', 'But, LSST is in the Southern hemisphere, where we lack large spectroscopic surveys. Without spectra, we need a new way to identify post-starburst and similar galaxies.', 'In this paper, we developed a method using machine learning to identify post-starburst and quiescent Balmer-strong galaxies using photometry alone.', 'This works best at z~0 where we can also use archival UV and IR data from GALEX and WISE, but also out to z~0.5, where the rest-frame NUV becomes accessible in SDSS or LSST u.', ""All in all, using this strategy, we expect to be able to find and identify 100-250 TDEs per year with LSST. This will make all sorts of TDE science possible that we can't do with the small numbers currently known."", 'Because there is a tradeoff in this strategy in host galaxy bias vs. detection, this strategy will be complementary to other methods of identifying TDEs with LSST.', 'In advance of LSST, we constructed a catalog of 70,000 new likely TDE host galaxies from Pan-STARRS and DES.', ""These catalogs of photometrically identified galaxies as well as the catalog of spectroscopically identified galaxies will be downloadable as MRTs from ApJ, but feel free to contact me if you'd like them sooner.""]",https://arxiv.org/abs/1810.09507,"A nuclear transient detected in a post-starburst galaxy or other quiescent galaxy with strong Balmer absorption is likely to be a Tidal Disruption Event (TDE). Identifying such galaxies within the planned survey footprint of the Large Synoptic Survey Telescope (LSST)---before a transient is detected---will make TDE classification immediate and follow-up more efficient. Unfortunately, spectra for identifying most such galaxies are unavailable, and simple photometric selection is ineffective; cutting on ""green valley"" UV/optical/IR colors produces samples that are highly contaminated and incomplete. Here we propose a new strategy using only photometric optical/UV/IR data from large surveys. Applying a machine learning Random Forest classifier to a sample of ~400k SDSS galaxies with GALEX and WISE photometry, including 13,592 quiescent Balmer-strong galaxies, we achieve 53-61% purity and 8-21% completeness, given the range in redshift. For the subset of 1299 post-starburst galaxies, we achieve 63-73% purity and 5-12% completeness. Given these results, the range of likely TDE and supernova rates, and that 36-75% of TDEs occur in quiescent Balmer-strong hosts, we estimate that 13-99% of transients observed in photometrically-selected host galaxies will be TDEs and that we will discover 119-248 TDEs per year with LSST. Using our technique, we present a new catalog of 67,484 candidate galaxies expected to have a high TDE rate, drawn from the SDSS, Pan-STARRS, DES, and WISE photometric surveys. This sample is 3.5x larger than the current SDSS sample of similar galaxies, thereby providing a new path forward for transient science and galaxy evolution studies. ","Identifying Tidal Disruption Events via Prior Photometric Selection of
  Their Preferred Hosts"
46,1054893946978086912,4922348584,Keno Fischer,"[""Our new paper today: <LINK>. Compile your #julialang code straight to @Google's #CloudTPU. Must go faster! We'll have an (alpha quality) repo up soon for people to start playing with this."", ""@ChristianPeel @Google We'll have more benchmarks, etc. out in due course. For now the focus has been on getting this working end to end and then getting the code out."", ""@dzj_evalparse @Google @staticfloat is working on getting those benchmarks. Also note that the TPU results in the paper are on one core of a TPUv2. For full fairness, we'd probably need to benchmark all eight cores or even go to TPUv3.""]",https://arxiv.org/abs/1810.09868,"Google's Cloud TPUs are a promising new hardware architecture for machine learning workloads. They have powered many of Google's milestone machine learning achievements in recent years. Google has now made TPUs available for general use on their cloud platform and as of very recently has opened them up further to allow use by non-TensorFlow frontends. We describe a method and implementation for offloading suitable sections of Julia programs to TPUs via this new API and the Google XLA compiler. Our method is able to completely fuse the forward pass of a VGG19 model expressed as a Julia program into a single TPU executable to be offloaded to the device. Our method composes well with existing compiler-based automatic differentiation techniques on Julia code, and we are thus able to also automatically obtain the VGG19 backwards pass and similarly offload it to the TPU. Targeting TPUs using our compiler, we are able to evaluate the VGG19 forward pass on a batch of 100 images in 0.23s which compares favorably to the 52.4s required for the original model on the CPU. Our implementation is less than 1000 lines of Julia, with no TPU specific changes made to the core Julia compiler or any other Julia packages. ",Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs
47,1054660659693215744,742684897307963393,C. G√≥mez-Rodr√≠guez,"['A new paradigm for constituent parsing! Or a new application for a well-known machine learning task: now we can do Constituent Parsing as Sequence Labeling, using the same models as for chunking or NER. Preprint of @emnlp2018 paper with @eusondavid now up: <LINK>', '@gchrupala @emnlp2018 @eusondavid Career experience has shown me that with promotion of (good) work, better too much than too little. Especially when not from a top school.', ""@kadarakos @gchrupala @emnlp2018 @eusondavid Any at a large enough scale and when you don't have access to unbounded amount of compute power. BTW, I find this kind of question a bit weird: IMO efficiency is always important - good engineering involves not wasting resources, even big corps should want to save energy."", ""@gchrupala @emnlp2018 @eusondavid Not ironic but I wasn't saying paradigm in the Kuhnian sense! Just a new approach that doesn't fit the existing paradigms (graph-based, transition-based, seq2seq...), so a new paradigm."", ""@kadarakos @gchrupala @emnlp2018 @eusondavid That's true. Of course you can train once and run an unbounded number of times so inference time is somewhat more relevant, but training time should also be given some attention.""]",https://arxiv.org/abs/1810.08994,"We introduce a method to reduce constituent parsing to sequence labeling. For each word w_t, it generates a label that encodes: (1) the number of ancestors in the tree that the words w_t and w_{t+1} have in common, and (2) the nonterminal symbol at the lowest common ancestor. We first prove that the proposed encoding function is injective for any tree without unary branches. In practice, the approach is made extensible to all constituency trees by collapsing unary branches. We then use the PTB and CTB treebanks as testbeds and propose a set of fast baselines. We achieve 90.7% F-score on the PTB test set, outperforming the Vinyals et al. (2015) sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach achieves the fastest constituent parsing speeds reported to date on PTB by a wide margin. ",Constituent Parsing as Sequence Labeling
48,1054558753897369600,158847776,Marc Lanctot,"[""Happy to share our paper relating actor-critic methods in partially-observable multiagent env's to regret minimization, inspiring new update rules and convergence guarantees. With @fzvinicius @karl_tuyls @MichaelHBowling and other great collaborators! <LINK>""]",http://arxiv.org/abs/1810.09026,"Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero sum games, without any domain-specific state space reductions. ","Actor-Critic Policy Optimization in Partially Observable Multiagent
  Environments"
49,1054311634791993346,120199223,Nuno Garcia,['New paper on arxiv with code available on github - Learning with privileged information via adversarial discriminative modality distillation. <LINK> - with @pmorerio and @vmurino'],http://arxiv.org/abs/1810.08437,"Heterogeneous data modalities can provide complementary cues for several tasks, usually leading to more robust algorithms and better performance. However, while training data can be accurately collected to include a variety of sensory modalities, it is often the case that not all of them are available in real life (testing) scenarios, where a model has to be deployed. This raises the challenge of how to extract information from multimodal data in the training stage, in a form that can be exploited at test time, considering limitations such as noisy or missing modalities. This paper presents a new approach in this direction for RGB-D vision tasks, developed within the adversarial learning and privileged information frameworks. We consider the practical case of learning representations from depth and RGB videos, while relying only on RGB data at test time. We propose a new approach to train a hallucination network that learns to distill depth information via adversarial learning, resulting in a clean approach without several losses to balance or hyperparameters. We report state-of-the-art results on object classification on the NYUD dataset and video action recognition on the largest multimodal dataset available for this task, the NTU RGB+D, as well as on the Northwestern-UCLA. ","Learning with privileged information via adversarial discriminative
  modality distillation"
50,1054300795146027008,480344476,Rosie,"['Good news! New paper authored by @dunnhumby data scientists @adamnhornsby and @ProfData from @ucl, great example of collaboration between industry and academia driving new approaches in shopper insight.\n<LINK>']",https://arxiv.org/abs/1810.08577,"Meaning may arise from an element's role or interactions within a larger system. For example, hitting nails is more central to people's concept of a hammer than its particular material composition or other intrinsic features. Likewise, the importance of a web page may result from its links with other pages rather than solely from its content. One example of meaning arising from extrinsic relationships are approaches that extract the meaning of word concepts from co-occurrence patterns in large, text corpora. The success of these methods suggest that human activity patterns may reveal conceptual organization. However, texts do not directly reflect human activity, but instead serve a communicative function and are usually highly curated or edited to suit an audience. Here, we apply methods devised for text to a data source that directly reflects thousands of individuals' activity patterns, namely supermarket purchases. Using product co-occurrence data from nearly 1.3m shopping baskets, we trained a topic model to learn 25 high-level concepts (or ""topics""). These topics were found to be comprehensible and coherent by both retail experts and consumers. Topics ranged from specific (e.g., ingredients for a stir-fry) to general (e.g., cooking from scratch). Topics tended to be goal-directed and situational, consistent with the notion that human conceptual knowledge is tailored to support action. Individual differences in the topics sampled predicted basic demographic characteristics. These results suggest that human activity patterns reveal conceptual organization and may give rise to it. ",Conceptual Organization is Revealed by Consumer Activity Patterns
51,1054169817677127685,2337598033,Geraint F. Lewis,"['Big new paper on the arXiv with @nfmartin1980 and @michelle_lmc ! <LINK> <LINK>', '@dougalmackey @nfmartin1980 @michelle_lmc OOps! - Sorry, typing on the run and sorry for missing you out :)']",https://arxiv.org/abs/1810.08234,"The Pan-Andromeda Archaeological Survey is a survey of $>400$ square degrees centered on the Andromeda (M31) and Triangulum (M33) galaxies that has provided the most extensive panorama of a $L_\star$ galaxy group to large projected galactocentric radii. Here, we collate and summarise the current status of our knowledge of the substructures in the stellar halo of M31, and discuss connections between these features. We estimate that the 13 most distinctive substructures were produced by at least 5 different accretion events, all in the last 3 or 4 Gyrs. We suggest that a few of the substructures furthest from M31 may be shells from a single accretion event. We calculate the luminosities of some prominent substructures for which previous estimates were not available, and we estimate the stellar mass budget of the outer halo of M31. We revisit the problem of quantifying the properties of a highly structured dataset; specifically, we use the OPTICS clustering algorithm to quantify the hierarchical structure of M31's stellar halo, and identify three new faint structures. M31's halo, in projection, appears to be dominated by two `mega-structures', that can be considered as the two most significant branches of a merger tree produced by breaking M31's stellar halo into smaller and smaller structures based on the stellar spatial clustering. We conclude that OPTICS is a powerful algorithm that could be used in any astronomical application involving the hierarchical clustering of points. The publication of this article coincides with the public release of all PAndAS data products. ","The large-scale structure of the halo of the Andromeda galaxy II.
  Hierarchical structure in the Pan-Andromeda Archaeological Survey"
52,1053187475793944578,75249390,Axel Maas,"['We have published a new paper on the structure and properties of bound states, especially mesons, see <LINK>']",https://arxiv.org/abs/1810.07955,"We discuss an approach for accessing bound state properties, like mass and decay width, of a theory within the functional renormalisation group approach. An important cornerstone is the dynamical hadronization technique for resonant interaction channels. The general framework is exemplified and put to work within the two-flavour quark-meson model. This model provides a low-energy description of the dynamics of two-flavour QCD with quark and hadronic degrees of freedom. We compare explicitly the respective results for correlation functions and observables with first principle QCD results in a quantitative manner. This allows us to estimate the validity range of low energy effective models. We also present first results for pole masses and decay widths. Next steps involving real-time formulations of the functional renormalisation group are discussed. ",Bound state properties from the Functional Renormalisation Group
53,1052781864388194304,258996336,Sandeep K. Goyal,['Our new paper on #photonic #quantummemory is out on @arxiv_org. Please have a look. Comments are welcome. <LINK>'],https://arxiv.org/abs/1810.07617,"Photonic quantum memory, such as an atomic frequency comb (AFC), is essential to make photonic quantum computation and long distance quantum communication scalable and feasible. In standard AFC the frequency of different atoms must be stable relative to each other which presents difficulties in realizing the quantum memory. Here we propose a quantum memory using an intra-atomic frequency comb which does not require frequency stabilization. We show that the transitions between two degenerate energy levels of a single atom can be used to construct the frequency comb. The spacing between the teeth of the comb is controlled by applying an external magnetic field. Since the frequency comb is constructed from individual atoms, these atoms can be used alone or in ensembles to realize the quantum memory. Furthermore, the ensemble based quantum memory with intra-AFC is robust against Doppler broadening which makes it useful for high-temperature quantum memory. As an example, we numerically show the intra-AFC in cesium atoms and demonstrate a photon echo which is essential for quantum memory. ",Photonic quantum memory using an intra-atomic frequency comb
54,1052745144095649792,1444366008,Josh Cooper,"['New paper with Hays Whitlatch and Peter Gartland, where we show that the posets for which enumerating linear extensions is the same as enumerating graph pressing sequences are just the V-posets of Misanantenaina &amp; Wagner: <LINK>']",https://arxiv.org/abs/1810.07276,"In 2016, Hasebe and Tsujie gave a recursive characterization of the set of induced $N$-free and bowtie-free posets; Misanantenaina and Wagner studied these orders further, naming them ""$\mathcal{V}$-posets"". Here we offer a new characterization of $\mathcal{V}$-posets by introducing a property we refer to as autonomy. A poset $\cP$ is said to be autonomous if there exists a directed acyclic graph $D$ (with adjacency matrix $U$) whose transitive closure is $\cP$, with the property that any total ordering of the vertices of $D$ so that Gaussian elimination of $U^TU$ proceeds without row swaps is a linear extension of $\cP$. Autonomous posets arise from the theory of pressing sequences in graphs, a problem with origins in phylogenetics. The pressing sequences of a graph can be partitioned into families corresponding to posets; because of the interest in enumerating pressing sequences, we investigate when this partition has only one block, that is, when the pressing sequences are all linear extensions of a single autonomous poset. We also provide an efficient algorithm for recognition of autonomy using structural information and the forbidden subposet characterization, and we discuss a few open questions that arise in connection with these posets. ",A New Characterization of $\mathcal{V}$-Posets
55,1052640491219681280,1384425704,Michiel Bakker,"['New paper on algorithmic fairness with Alejandro Noriega-Campero, Bernardo Garcia-Bulle and @alex_pentland ""Active Fairness in Algorithmic Decision Making"" available now on ArXiv. <LINK>']",https://arxiv.org/abs/1810.00031,"Society increasingly relies on machine learning models for automated decision making. Yet, efficiency gains from automation have come paired with concern for algorithmic discrimination that can systematize inequality. Recent work has proposed optimal post-processing methods that randomize classification decisions for a fraction of individuals, in order to achieve fairness measures related to parity in errors and calibration. These methods, however, have raised concern due to the information inefficiency, intra-group unfairness, and Pareto sub-optimality they entail. The present work proposes an alternative active framework for fair classification, where, in deployment, a decision-maker adaptively acquires information according to the needs of different groups or individuals, towards balancing disparities in classification performance. We propose two such methods, where information collection is adapted to group- and individual-level needs respectively. We show on real-world datasets that these can achieve: 1) calibration and single error parity (e.g., equal opportunity); and 2) parity in both false positive and false negative rates (i.e., equal odds). Moreover, we show that by leveraging their additional degree of freedom, active approaches can substantially outperform randomization-based classifiers previously considered optimal, while avoiding limitations such as intra-group unfairness. ",Active Fairness in Algorithmic Decision Making
56,1052602263812730880,377485322,Ralph Bird,"['A couple of days after the arXiv release but my new paper is out.  Detection of the @NASAFermi pulsar as a VHE binary by @VeritasGammaRay and @MAGICtelescopes <LINK>.  Also featuring a lovely @NASASwift lightcurve of the periastron passage.', 'I should say the source name - PSR J2032+4127 / MT91 213']",https://arxiv.org/abs/1810.05271,"We report on observations of the pulsar / Be star binary system PSR J2032+4127 / MT91 213 in the energy range between 100 GeV and 20 TeV with the VERITAS and MAGIC imaging atmospheric Cherenkov telescope arrays. The binary orbit has a period of approximately 50 years, with the most recent periastron occurring on 2017 November 13. Our observations span from 18 months prior to periastron to one month after. A new, point-like, gamma-ray source is detected, coincident with the location of PSR J2032+4127 / MT91 213. The gamma-ray light curve and spectrum are well-characterized over the periastron passage. The flux is variable over at least an order of magnitude, peaking at periastron, thus providing a firm association of the TeV source with the pulsar / Be star system. Observations prior to periastron show a cutoff in the spectrum at an energy around 0.5 TeV. This result adds a new member to the small population of known TeV binaries, and it identifies only the second source of this class in which the nature and properties of the compact object are firmly established. We compare the gamma-ray results with the light curve measured with the X-ray Telescope (XRT) on board the Neil Gehrels \textit{Swift} Observatory and with the predictions of recent theoretical models of the system. We conclude that significant revision of the models is required to explain the details of the emission we have observed, and we discuss the relationship between the binary system and the overlapping steady extended source, TeV J2032+4130. ","Periastron Observations of TeV Gamma-Ray Emission from a Binary System
  with a 50-year Period"
57,1052588770875400192,841499391508779008,Zico Kolter,"['Got the ""do I use an RNN or CNN for sequence modeling"" blues?  Use a TrellisNet, an architecture that connects these two worlds, and works better than either!  New paper with @shaojieb and Vladlen Koltun.\n\nPaper: <LINK>\nCode: <LINK> <LINK>']",https://arxiv.org/abs/1810.06682,"We present trellis networks, a new architecture for sequence modeling. On the one hand, a trellis network is a temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. On the other hand, we show that truncated recurrent networks are equivalent to trellis networks with special sparsity structure in their weight matrices. Thus trellis networks with general weight matrices generalize truncated recurrent networks. We leverage these connections to design high-performing trellis networks that absorb structural and algorithmic elements from both recurrent and convolutional models. Experiments demonstrate that trellis networks outperform the current state of the art methods on a variety of challenging benchmarks, including word-level language modeling and character-level language modeling tasks, and stress tests designed to evaluate long-term memory retention. The code is available at this https URL . ",Trellis Networks for Sequence Modeling
58,1052578088016437249,306314687,Gao Yuan (Alex),['Our new paper regarding how to consider social robotics problems under deep reinforcement learning framework. We proposed a Staged Social Behavior Learning (SSBL) scheme that decomposes social behavior learning into different learning stages. üòÑ <LINK>'],https://arxiv.org/abs/1810.06979,"Deep reinforcement learning has recently been widely applied in robotics to study tasks such as locomotion and grasping, but its application to social human-robot interaction (HRI) remains a challenge. In this paper, we present a deep learning scheme that acquires a prior model of robot approaching behavior in simulation and applies it to real-world interaction with a physical robot approaching groups of humans. The scheme, which we refer to as Staged Social Behavior Learning (SSBL), considers different stages of learning in social scenarios. We learn robot approaching behaviors towards small groups in simulation and evaluate the performance of the model using objective and subjective measures in a perceptual study and a HRI user study with human participants. Results show that our model generates more socially appropriate behavior compared to a state-of-the-art model. ","Learning Socially Appropriate Robot Approaching Behavior Toward Groups
  using Deep Reinforcement Learning"
59,1052383772052574208,460069521,Andrew Francis,"['Very pleased with our new paper, led by PhD student Michael Hendriksen @mahendriksen - on the arXiv today!\n\nThe paper introduces what we believe to be a *new* phylogenetic consensus method that has some very nice properties.  \n\n<LINK>']",https://arxiv.org/abs/1810.06831,"There is a long tradition of the axiomatic study of consensus methods in phylogenetics that satisfy certain desirable properties. One recently-introduced property is associative stability, which is desirable because it confers a computational advantage, in that the consensus method only needs to be computed ""pairwise"". In this paper, we introduce a phylogenetic consensus method that satisfies this property, in addition to being ""regular"". The method is based on the introduction of a partial order on the set of rooted phylogenetic trees, itself based on the notion of a hierarchy-preserving map between trees. This partial order may be of independent interest. We call the method ""lattice consensus"", because it takes the unique maximal element in a lattice of trees defined by the partial order. Aside from being associatively stable, lattice consensus also satisfies the property of being Pareto on rooted triples, answering in the affirmative a question of Bryant et al (2017). We conclude the paper with an answer to another question of Bryant et al, showing that there is no regular extension stable consensus method for binary trees. ","Lattice consensus: A partial order on phylogenetic trees that induces an
  associatively stable consensus method"
60,1052369697906266112,15955171,Colin Gordon,['New early results paper with my student Sergey Matskevich <LINK> on a new approach to generating code comments.  Sergey will give a presentation at @Nl4Se before @FSEconf'],https://arxiv.org/abs/1810.06599,"Good comments help developers understand software faster and provide better maintenance. However, comments are often missing, generally inaccurate, or out of date. Many of these problems can be avoided by automatic comment generation. This paper presents a method to generate informative comments directly from the source code using general-purpose techniques from natural language processing. We generate comments using an existing natural language model that couples words with their individual logical meaning and grammar rules, allowing comment generation to proceed by search from declarative descriptions of program text. We evaluate our algorithm on several classic algorithms implemented in Python. ",Generating Comments From Source Code with CCGs
61,1052281466896363520,1498729908,Nick Rhinehart,"['New work with @svlevine and @rowantmc: we learn a distribution of human driving behavior to guide planning and control of an autonomous car, without any trial-and-error data collection\nPaper: <LINK> \nWebsite: <LINK> <LINK>']",https://arxiv.org/abs/1810.06544,"Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly specified goals, such as goals on the wrong side of the road. ","Deep Imitative Models for Flexible Inference, Planning, and Control"
62,1052160556742955008,2676457430,MAGIC telescopes üå¥üå∫,"['A new #MAGICpaper has been accepted for publication in the Astrophysical Journal Letters! The MAGIC and VERITAS collaborations have discovered a new gamma-ray binary: PSRJ2032+4127/MT91 213. For more details about this system, check the paper at: <LINK> <LINK>']",http://arxiv.org/abs/1810.05271,"We report on observations of the pulsar / Be star binary system PSR J2032+4127 / MT91 213 in the energy range between 100 GeV and 20 TeV with the VERITAS and MAGIC imaging atmospheric Cherenkov telescope arrays. The binary orbit has a period of approximately 50 years, with the most recent periastron occurring on 2017 November 13. Our observations span from 18 months prior to periastron to one month after. A new, point-like, gamma-ray source is detected, coincident with the location of PSR J2032+4127 / MT91 213. The gamma-ray light curve and spectrum are well-characterized over the periastron passage. The flux is variable over at least an order of magnitude, peaking at periastron, thus providing a firm association of the TeV source with the pulsar / Be star system. Observations prior to periastron show a cutoff in the spectrum at an energy around 0.5 TeV. This result adds a new member to the small population of known TeV binaries, and it identifies only the second source of this class in which the nature and properties of the compact object are firmly established. We compare the gamma-ray results with the light curve measured with the X-ray Telescope (XRT) on board the Neil Gehrels \textit{Swift} Observatory and with the predictions of recent theoretical models of the system. We conclude that significant revision of the models is required to explain the details of the emission we have observed, and we discuss the relationship between the binary system and the overlapping steady extended source, TeV J2032+4130. ","Periastron Observations of TeV Gamma-Ray Emission from a Binary System
  with a 50-year Period"
63,1052107284984328192,1024641747438186496,Pierre Sens,"['The new paper of Sami Al-Izzii, a PhD student I""m happy (and lucky) to share with my buddy Matthew Turner from Warwick Univ. Is out on arXiv and submitted! \nShear-driven instabilities of membrane tubes and Dynamin-induced scission.\n<LINK>']",http://arxiv.org/abs/1810.05862,"Motivated by the mechanics of dynamin-mediated membrane tube fission we analyse the stability of fluid membrane tubes subjected to shear flow in azimuthal direction. We find a novel helical instability driven by the membrane shear flow which results in a non-equilibrium steady state for the tube fluctuations. This instability has its onset at shear rates that may be physiologically accessible under the action of dynamin and could also be probed using in-vitro experiments on membrane nanotubes, e.g. using magnetic tweezers. We discuss how such an instability may play a role in the mechanism for dynamin-mediated membrane tube fission. ","Shear-driven instabilities of membrane tubes and dynamin-induced
  scission"
64,1051886980588539905,804069495253962752,David Mart√≠nez Delgado,"['Our new paper, accepted in Astronomy &amp;  Astrophysics, reports the  discovery of the dwarf galaxy Donatiello I (Mirach¬¥s goblin) by the amateur astronomer Giuseppe Donatiello using a small telescope. An isolated old system behind the Andromeda galaxy?: <LINK> <LINK>']",https://arxiv.org/abs/1810.04741,"It is of broad interest for galaxy formation theory to carry out a full inventory of the numbers and properties of dwarf galaxies in the Local Volume, both satellites and isolated ones. Ultra-deep imaging in wide areas of the sky with small amateur telescopes can help to complete the census of these hitherto unknown low surface brightness galaxies, which cannot be detected by the current resolved stellar population and HI surveys. We report the discovery of Donatiello I, a dwarf spheroidal galaxy located one degree from the star Mirach (Beta And) in a deep image taken with an amateur telescope. The color--magnitude diagram obtained from follow-up observations obtained with the Gran Telescopio Canarias (La Palma, Spain) reveals that this system is beyond the Local Group and is mainly composed of old stars. The absence of young stars and HI emission in the ALFALFA survey are typical of quenched dwarf galaxies. Our photometry suggests a distance modulus for this galaxy of (m-M)=27.6 +/- 0.2 (3.3 Mpc), although this distance cannot yet be established securely owing to the crowding effects in our color--magnitude diagram. At this distance, Donatiello I's absolute magnitude (M_v =-8.3), surface brightness (mu_v=26.5 mag arcsec^-2) and stellar content are similar to the ""classical"" Milky Way companions Draco or Ursa Minor. The projected position and distance of Donatiello I are consistent with being a dwarf satellite of the closest S0-type galaxy NGC 404 (""Mirach's Ghost""). Alternatively, it could be one of the most isolated quenched dwarf galaxies reported so far behind the Andromeda galaxy. ","Mirach's Goblin: Discovery of a dwarf spheroidal galaxy behind the
  Andromeda galaxy"
65,1051885897858265088,139627907,rob tibshirani,"['Excited by the new paper from our lab ""Principal Components guided sparse regression"": <LINK>']",https://arxiv.org/abs/1810.04651,"We propose a new method for supervised learning, especially suited to wide data where the number of features is much greater than the number of observations. The method combines the lasso ($\ell_1$) sparsity penalty with a quadratic penalty that shrinks the coefficient vector toward the leading principal components of the feature matrix. We call the proposed method the ""principal components lasso"" (""pcLasso""). The method can be especially powerful if the features are pre-assigned to groups (such as cell-pathways, assays or protein interaction networks). In that case, pcLasso shrinks each group-wise component of the solution toward the leading principal components of that group. In the process, it also carries out selection of the feature groups. We provide some theory for this method and illustrate it on a number of simulated and real data examples. ",Principal component-guided sparse regression
66,1051817213601169410,3315576304,Syksy R√§s√§nen,"['Our new little paper on higher order gravity theories in inflation in the Palatini formulation.\n\nA neat little package, with more pages of references than body.\n\n<LINK>']",https://arxiv.org/abs/1810.05536,"We study scalar field inflation in $F(R)$ gravity in the Palatini formulation of general relativity. Unlike in the metric formulation, in the Palatini formulation $F(R)$ gravity does not introduce new degrees of freedom. However, it changes the relations between existing degrees of freedom, including the inflaton and spacetime curvature. Considering the case $F(R)=R+\alpha R^2$, we find that the $R^2$ term decreases the height of the effective inflaton potential. By adjusting the value of $\alpha$, this mechanism can be used to suppress the tensor-to-scalar ratio $r$ without limit in any scalar field model of inflation without affecting the spectrum of scalar perturbations. ",Inflation with $R^2$ term in the Palatini formalism
67,1051798741819244545,389891491,Bj√∂rn 'Dr. Bear' Penning,"['New paper with Nassim Bozorgnia, Andrew Cheek and @DGCerdeno: Sensitivities and how to measure #DarkMatter using higher nuclear recoil energies in  experiments such as @lzdarkmatter and @Xenon1T: <LINK> <LINK>']",http://arxiv.org/abs/1810.05576,"In this article we investigate the benefits of increasing the maximum nuclear recoil energy analysed in dark matter (DM) direct detection experiments. We focus on elastic DM-nucleus interactions, and work within the framework of effective field theory (EFT) to describe the scattering cross section. In agreement with previous literature, we show that an increased maximum energy leads to more stringent upper bounds on the DM-nucleus cross section for the EFT operators, especially those with an explicit momentum dependence. In this article we extend the energy region of interest (ROI) to show that the optimal values of the maximum energy for xenon and argon are of the order of 500 keV and 300 keV, respectively. We then show how, if a signal compatible with DM is observed, an enlarged energy ROI leads to a better measurement of the DM mass and couplings. In particular, for a xenon detector, DM masses of the order of 200 GeV (2 TeV) or lower can be reconstructed for momentum-independent (-dependent) operators. We also investigate three-dimensional parameter reconstruction and apply it to the specific case of scalar DM and anapole DM. We find that opening the energy ROI is an excellent way to identify the linear combination of momentum-dependent and momentum-independent operators, and it is crucial to correctly distinguish these models. Finally, we show how an enlarged energy ROI also allows us to test astrophysical parameters of the DM halo, such as the DM escape speed. ",Opening the energy window on direct dark matter detection
68,1051159101827416064,1446792746,Andrew Davison,"['Our new work on imitation learning from the Dyson Robotics Lab, led by Stephen James and @MichaelBloesch. We train in simulation, then adapt to a real robotic task after one demonstration. To be presented at CORL 2018: paper here <LINK>\n<LINK>']",https://arxiv.org/abs/1810.03237,"Much like humans, robots should have the ability to leverage knowledge from previously learned tasks in order to learn new tasks quickly in new and unfamiliar environments. Despite this, most robot learning approaches have focused on learning a single task, from scratch, with a limited notion of generalisation, and no way of leveraging the knowledge to learn other tasks more efficiently. One possible solution is meta-learning, but many of the related approaches are limited in their ability to scale to a large number of tasks and to learn further tasks without forgetting previously learned ones. With this in mind, we introduce Task-Embedded Control Networks, which employ ideas from metric learning in order to create a task embedding that can be used by a robot to learn new tasks from one or more demonstrations. In the area of visually-guided manipulation, we present simulation results in which we surpass the performance of a state-of-the-art method when using only visual information from each demonstration. Additionally, we demonstrate that our approach can also be used in conjunction with domain randomisation to train our few-shot learning ability in simulation and then deploy in the real world without any additional training. Once deployed, the robot can learn new tasks from a single real-world demonstration. ",Task-Embedded Control Networks for Few-Shot Imitation Learning
69,1050948622160936960,246621369,Giovanni Beltrame,"['Decentralized collaborative transport of fabrics using micro-UAVs, new work by Ryan Cotsakis and @stongedav!\n\npaper: <LINK>\nvideo:\n<LINK>\ncode:\n<LINK>\n\n@polymtl @MIST_lab @Bitcraze_se #Robotics', '@sg175 @stongedav @polymtl @MIST_lab @Bitcraze_se Thanks!']",https://arxiv.org/abs/1810.00522,"Small unmanned aerial vehicles (UAVs) have generally little capacity to carry payloads. Through collaboration, the UAVs can increase their joint payload capacity and carry more significant loads. For maximum flexibility to dynamic and unstructured environments and task demands, we propose a fully decentralized control infrastructure based on a swarm-specific scripting language, Buzz. In this paper, we describe the control infrastructure and use it to compare two algorithms for collaborative transport: field potentials and spring-damper. We test the performance of our approach with a fleet of micro-UAVs, demonstrating the potential of decentralized control for collaborative transport. ",Decentralized collaborative transport of fabrics using micro-UAVs
70,1050789685809709056,2200242594,Andreea Bobu,['Check out our new work on robot learning under misspecified objective spaces accepted at #CORL2018 !\nPaper: <LINK>\nVideo: <LINK>\n\nBig thanks to my collaborators who made this work possible: @andreaBajcsy Jaime Fisac @ancadianadragan. @berkeley_ai'],https://arxiv.org/abs/1810.05157,"Learning robot objective functions from human input has become increasingly important, but state-of-the-art techniques assume that the human's desired objective lies within the robot's hypothesis space. When this is not true, even methods that keep track of uncertainty over the objective fail because they reason about which hypothesis might be correct, and not whether any of the hypotheses are correct. We focus specifically on learning from physical human corrections during the robot's task execution, where not having a rich enough hypothesis space leads to the robot updating its objective in ways that the person did not actually intend. We observe that such corrections appear irrelevant to the robot, because they are not the best way of achieving any of the candidate objectives. Instead of naively trusting and learning from every human interaction, we propose robots learn conservatively by reasoning in real time about how relevant the human's correction is for the robot's hypothesis space. We test our inference method in an experiment with human interaction data, and demonstrate that this alleviates unintended learning in an in-person user study with a 7DoF robot manipulator. ",Learning under Misspecified Objective Spaces
71,1050786528841076736,2290974747,Chi Yan,"[""Our new paper @PlanetSabine explores the effect of a stably stratified layer in the Earth's core on the large scale geomagnetic morphology. (<LINK> also <LINK>)""]",https://arxiv.org/abs/1810.04223,"Current ""Earth-like"" numerical dynamo simulations are able to reproduce many characteristics of the observed geomagnetic field. One notable exception is the geomagnetic octupolar component. Here we investigate whether a stably stratified layer at the top of the core, a missing ingredient in standard dynamo simulations, can explain the observed geomagnetic octupole. Through numerical simulations, we find that the existence of a stable layer has significant influence on the octupolar-to-dipolar ratio of the magnetic field. Particularly, we find that a 60 km stable layer with relatively strong stability or a 130 km layer with relatively weak stability are compatible with the observations, but a 350 km stable layer, as suggested by recent seismological evidence, is not compatible with Earth's octupole field over the past 10,000 years. ","Sensitivity of the Geomagnetic Octupole to a Stably Stratified Layer in
  the Earth's Core"
72,1050455672213639168,759249,Dean Eckles,"['How are biological and social contagion affected by changes to network structure? Recent work has claimed a ""weakness of long ties"" for social contagions, unlike biological contagions. Our new paper substantially revises this conclusion. <LINK> <LINK>', 'While biological contagions and information diffusion are often modeled as ""simple"" contagion, other contagions might be better approximated by myopic best-response behaviors in a coordination game, thus yielding a threshold for adoption. https://t.co/m8LhUvRvdh', ""With a hard threshold for adoption (k), the contagion can't spread across bridges that are too narrow ‚Äî and even if it jumps across the bridge to one node, it can't spread from there. But what about a slightly soft threshold? ie what about a little bit of noise?"", 'With a little bit of below-threshold adoption ‚Äî even only via some local, short ties ‚Äî once the infected region gets large enough, a wide-enough bridge exists and then the contagion can spread locally (via below-threshold adoption) on the other side of the bridge https://t.co/R1QHyklf6Y', 'Our theoretical results show that random rewiring speeds up complex (threshold-based) contagions, as long as there is (a) a little noise (below-threshold adoption) and (b) you do enough rewiring. Real contagion and real networks seem to have both.', 'Long ties accelerate complex contagions: Random rewiring (orange) or adding random edges (red) results in faster spread than the original network (black) or adding ""short"" edges that close triangles (blue) ‚Äî here 175 networks of Chinese villagers. https://t.co/u5oXIEHMOR', 'This project has been a great first journey for me within @mitidss ‚Äî with co-authors spanning engineering, computer science, math, and social science\nhttps://t.co/elEqBote9y\nhttps://t.co/mTmQAfWHPC\nhttps://t.co/AVmCciKlgz', '@psmaldino Key, perhaps surprising, bit: all these results work *without* below-threshold adoption on ""long"" ties, but only on a subset of ""short"" ties.', '@psmaldino It does if you are using the union of a random graph with C_k where k &lt; threshold.', '@psmaldino Among other things, can that fail to even be connected... Theory is more difficult. Much prior theory work avoids W-S for related reasons. Also, our setup leads to natural notion of *adding* edges (a more natural intervention than rewiring).']",https://arxiv.org/abs/1810.03579,"Network structure can affect when and how widely new ideas, products, and behaviors are adopted. In widely-used models of biological contagion, interventions that randomly rewire edges (generally making them ""longer"") accelerate spread. However, there are other models relevant to social contagion, such as those motivated by myopic best-response in games with strategic complements, in which an individual's behavior is described by a threshold number of adopting neighbors above which adoption occurs (i.e., complex contagions). Recent work has argued that highly clustered, rather than random, networks facilitate spread of these complex contagions. Here we show that minor modifications to this model, which make it more realistic, reverse this result, thereby harmonizing qualitative facts about how network structure affects contagion. To model the trade-off between long and short edges we analyze the rate of spread over networks that are the union of circular lattices and random graphs on $n$ nodes. Allowing for noise in adoption decisions (i.e., adoptions below threshold) to occur with order $1/\sqrt{n}$ probability along some ""short"" cycle edges) is enough to ensure that random rewiring accelerates spread. This conclusion continues to hold true under partial but frequent enough rewiring and when adoption decisions are reversible but infrequently so. Simulations illustrate the robustness of these results to several variations on this noisy best-response behavior. Hypothetical interventions that randomly rewire existing edges or add random edges (versus adding ""short"", triad-closing edges) in hundreds of empirical social networks reduce time to spread. This revised conclusion suggests that those wanting to increase spread should induce formation of long ties, rather than triad-closing ties. More generally, this highlights the importance of noise in game-theoretic analyses of behavior. ",Long ties accelerate noisy threshold-based contagions
73,1050292823935909888,776743573,Mauricio A. √Ålvarez,['Our paper on Volterra series for building cool new kernels for multi-output GPs is now at  <LINK> Joint work with @wilocw and @cdguarnizo'],https://arxiv.org/abs/1810.04632,"The paper introduces a non-linear version of the process convolution formalism for building covariance functions for multi-output Gaussian processes. The non-linearity is introduced via Volterra series, one series per each output. We provide closed-form expressions for the mean function and the covariance function of the approximated Gaussian process at the output of the Volterra series. The mean function and covariance function for the joint Gaussian process are derived using formulae for the product moments of Gaussian variables. We compare the performance of the non-linear model against the classical process convolution approach in one synthetic dataset and two real datasets. ",Non-linear process convolutions for multi-output Gaussian processes
74,1049955514107514880,840225177321512961,Marion Roullet,"[""Have you ever wondered why #cream is thicker (more viscous) than #milk?\nCream contains more fat droplets, so it's difficult to move them all to pour it. \nBut what about the role of milk proteins in these #emulsions?\n\nA summary of our new paper (<LINK>): 1/n <LINK>"", 'Industrial food manufacturing often involves making ""fake milk"": mixing milk proteins and fat to form an #emulsion that can then be added to the recipe. The result is a mixture of proteins and droplets.\nWe wanted to know the viscosity of these mixtures. 2/n https://t.co/c1h7uP7kcF', 'For that we first considered each ingredient separately: a) proteins, b) droplets.\na) Milk #Proteins are very complicated beasts, and physicists HATE complexity. We needed a simple way to see them: tiny bead (had sphere) or cooked spaghetti (polymer coil)? 3/n https://t.co/HmCfsuVxsg', 'In the end, we took the middle road: proteins are like squishy beads, they can swell and deform, like tapioca pearls in your #bubbletea (soft #colloids, like microgels and star polymers). We had to find a new model to describe their viscosity. 4/n https://t.co/JVlsxmJdEj', 'b) Droplets were easier, we could just see them as tiny undeformable beads (#hardsphere). Here again, we chose a model to calculate the viscosity from the concentration.\nThis was the calibration step: we now knew the viscosity of the pure components at any concentration. 5/n', 'It was time to mix proteins with droplets: there are many ways you can do that so we used a diagram to see what samples we could prepare. It is 2D: we can choose more proteins vs more droplets, and little stuff vs lots of stuff. We tried to explore all the options. 6/n https://t.co/q1YSmZwPLN', 'And by measuring the viscosity of these mixtures, we found that we could predict it by using the measurements of the pure components. We can multiply the viscosity of the droplets by the viscosity of proteins *in the interstices between droplets* and get the measured value! 7/n https://t.co/MGncVOFLCx', ""But what if we don't know how much protein we have in the mixture (controlled by the emulsification process)?\nWe can reverse the model and calculate the amount of protein from the viscosity of the emulsion. The way the model is used depends on what we know of our system. 8/n https://t.co/mlSHTK7qL8"", 'It is a first step in helping #formulation with #physics: you can have an idea of the properties of your mix without having to prepare it, all you need is a good knowledge of the components. We think it may be used for other systems than protein-stabilised emulsions. 9/n', 'Finally, I would like to thank my #PhD supervisors Bill Frith and Paul Clegg (@PhysAstroEd) for their support. This work was funded by @Colldense_ITN , a @MSCActions network. It is under review at the Journal of #Rheology, so stay tuned for the final version. #phdchat 10/10']",https://arxiv.org/abs/1810.03905,"Protein-stabilised emulsions can be seen as mixtures of unadsorbed proteins and of protein-stabilised droplets. To identify the contributions of these two components to the overall viscosity of sodium caseinate o/w emulsions, the rheological behaviour of pure suspensions of proteins and droplets were characterised, and their properties used to model the behaviour of their mixtures. These materials are conveniently studied in the framework developed for soft colloids. Here, the use of viscosity models for the two types of pure suspensions facilitates the development of a semi-empirical model that relates the viscosity of protein-stabilised emulsions to their composition. ","Viscosity of protein-stabilised emulsions: contributions of components
  and development of a semi-predictive model"
75,1049720354674012161,946018388,Elias Khalil,"['Check out our new paper: Combinatorial Attacks on Binarized Neural Networks <LINK>! Joint with my lab mate @itsamritagupta and our adviser @BDilkina', ""Here's a TL;DR poster summarizing the paper! https://t.co/CiRT5sow05""]",https://arxiv.org/abs/1810.03538,"Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency. Concurrently, it has been shown that neural networks may be overly sensitive to ""attacks"" - tiny adversarial changes in the input - which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks. The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks. In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization. We propose a Mixed Integer Linear Programming (MILP) formulation of the problem. While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow. To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems. Experimentally, we evaluate both proposed methods against the standard gradient-based attack (FGSM) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to FGSM, while scaling beyond the limits of the MILP. ",Combinatorial Attacks on Binarized Neural Networks
76,1049693858240585734,174052756,Thiago Serra,['New paper on approximating the linear regions of neural nets: <LINK>\n\nWe exploit neat tricks:\n- MIP formulations to find which units are locally stable\n- Approximate counting methods from SAT to get probabilistic LBs on the number of solutions of a MIP\n#orms #ml <LINK>'],https://arxiv.org/abs/1810.03370,"We can compare the expressiveness of neural networks that use rectified linear units (ReLUs) by the number of linear regions, which reflect the number of pieces of the piecewise linear functions modeled by such networks. However, enumerating these regions is prohibitive and the known analytical bounds are identical for networks with same dimensions. In this work, we approximate the number of linear regions through empirical bounds based on features of the trained network and probabilistic inference. Our first contribution is a method to sample the activation patterns defined by ReLUs using universal hash functions. This method is based on a Mixed-Integer Linear Programming (MILP) formulation of the network and an algorithm for probabilistic lower bounds of MILP solution sets that we call MIPBound, which is considerably faster than exact counting and reaches values in similar orders of magnitude. Our second contribution is a tighter activation-based bound for the maximum number of linear regions, which is particularly stronger in networks with narrow layers. Combined, these bounds yield a fast proxy for the number of linear regions of a deep neural network. ",Empirical Bounds on Linear Regions of Deep Rectifier Networks
77,1049619732192796672,2773113120,Tom Rivlin,"[""üö®üö®New paper alert!!üö®üö® \n\nWe've just put a paper we've been working on up on arXiv. There's some nice results in there (from 10 months ago...) about analytic and numerical scattering off a Morse potential. Check it out :) <LINK>""]",https://arxiv.org/abs/1810.03475,Experiments are starting to probe collisions and chemical reactions between atoms and molecules at ultra-low temperatures. We have developed a new theoretical procedure for studying these collisions using the R-matrix method. Here this method is tested for the atom -- atom collisions described by a Morse potential. Analytic solutions for continuum states of the Morse potential are derived and compared with numerical results computed using an R-matrix method where the inner region wavefunctions are obtained using a standard nuclear motion algorithm. Results are given for eigenphases and scattering lengths. Excellent agreement is obtained in all cases. Progress in developing a general procedure for treating ultra-low energy reactive and non-reactive collisions is discussed. ,Low temperature scattering with the R-matrix method: the Morse potential
78,1049579426889879554,781309796741947396,EEMS Group @ MIT (PI: Vivienne Sze),"['New paper on ""Low Power Depth Estimation of Rigid Objects for Time-of-Flight Imaging."" We present an algorithm that lowers the power for depth sensing by reducing the usage of the TOF camera and estimating depth maps using concurrently collected images. <LINK> <LINK>']",https://arxiv.org/abs/1810.01930,"Depth sensing is useful in a variety of applications that range from augmented reality to robotics. Time-of-flight (TOF) cameras are appealing because they obtain dense depth measurements with minimal latency. However, for many battery-powered devices, the illumination source of a TOF camera is power hungry and can limit the battery life of the device. To address this issue, we present an algorithm that lowers the power for depth sensing by reducing the usage of the TOF camera and estimating depth maps using concurrently collected images. Our technique also adaptively controls the TOF camera and enables it when an accurate depth map cannot be estimated. To ensure that the overall system power for depth sensing is reduced, we design our algorithm to run on a low power embedded platform, where it outputs 640x480 depth maps at 30 frames per second. We evaluate our approach on several RGB-D datasets, where it produces depth maps with an overall mean relative error of 0.96% and reduces the usage of the TOF camera by 85%. When used with commercial TOF cameras, we estimate that our algorithm can lower the total power for depth sensing by up to 73%. ",Low Power Depth Estimation of Rigid Objects for Time-of-Flight Imaging
79,1049249910984527872,561899047,Aki Vehtari,"['New paper by @JuhoPiironen, Markus Paasiniemi, me: Projective Inference in High-dimensional Problems: Prediction and Feature Selection <LINK>  describes all the new methods in projpred package <LINK> The figure below shows projpred beating lasso. <LINK>']",https://arxiv.org/abs/1810.02406,"This paper discusses predictive inference and feature selection for generalized linear models with scarce but high-dimensional data. We argue that in many cases one can benefit from a decision theoretically justified two-stage approach: first, construct a possibly non-sparse model that predicts well, and then find a minimal subset of features that characterize the predictions. The model built in the first step is referred to as the \emph{reference model} and the operation during the latter step as predictive \emph{projection}. The key characteristic of this approach is that it finds an excellent tradeoff between sparsity and predictive accuracy, and the gain comes from utilizing all available information including prior and that coming from the left out features. We review several methods that follow this principle and provide novel methodological contributions. We present a new projection technique that unifies two existing techniques and is both accurate and fast to compute. We also propose a way of evaluating the feature selection process using fast leave-one-out cross-validation that allows for easy and intuitive model size selection. Furthermore, we prove a theorem that helps to understand the conditions under which the projective approach could be beneficial. The benefits are illustrated via several simulated and real world examples. ","Projective Inference in High-dimensional Problems: Prediction and
  Feature Selection"
80,1048247698053390337,966760075074461697,Brian Thomas,"[""Interested in what I'm working on these days?  Here's a new pre-print paper, thinking about how climate might be affected by a nearby supernova... <LINK>""]",https://arxiv.org/abs/1810.01995,"Motivated by the occurrence of a moderately nearby supernova near the beginning of the Pleistocene, possibly as part of a long-term series beginning in the Miocene, we investigate whether nitrate rainout resulting from the atmospheric ionization of enhanced cosmic ray flux could have, through its fertilizer effect, initiated carbon dioxide drawdown. Such a drawdown could possibly reduce the greenhouse effect and induce the climate change that led to the Pleistocene glaciations. We estimate that the nitrogen flux enhancement onto the surface from an event at 50 pc would be of order 10%, probably too small for dramatic changes. We estimate deposition of iron (another potential fertilizer) and find it is also too small to be significant. There are also competing effects of opposite sign, including muon irradiation and reduction in photosynthetic yield caused by UV increase from stratospheric ozone layer depletion, leading to an ambiguous result. However, if the atmospheric ionization induces a large increase in the frequency of lightning, as argued elsewhere, the amount of nitrate synthesis should be much larger, dominate over the other effects, and induce the climate change. More work needs to be done to clarify effects on lightning frequency. ","Climate change via co2 drawdown from astrophysically initiated
  atmospheric ionization?"
81,1048200598330511360,436029997,Noah Haber,"['New on arXiv: This paper identifies a policy-relevant causal effect, BUT\n1) Controls for almost nothing in the main spec\n2) Uses only basic regression methods\n3) Has no meaningful effect size estimate\n4) Is not a natural experiment\nCuriosity stoked yet?\n<LINK>', 'No? How about these:\n5) Uses a threshold for effect estimation, BUT\n6) Does not measure the effect across the threshold (i.e. is not regression discontinuity)\n7) Uses p-values because in this setting they are more sensible than CIs']",https://arxiv.org/abs/1810.01971,"South Africa's disability grants program is tied to its HIV/AIDS recovery program, such that individuals who are ill enough may qualify. Qualification is historically tied to a CD4 count of 200 cells/mm3, which improve when a person adheres to antiretroviral therapy. This creates a potential unintended consequence where poor individuals, faced with potential loss of their income, may choose to limit their recovery through non-adherence. To test for manipulation caused by grant rules, we identify differences in disability grant recipients and non-recipients' rate of CD4 recovery around the qualification threshold, implemented as a fixed-effects difference-in-difference around the threshold. We use data from the Africa Health Research Institute Demographic and Health Surveillance System (AHRI DSS) in rural KwaZulu-Natal, South Africa, utilizing DG status and laboratory CD4 count records for 8,497 individuals to test whether there are any systematic differences in CD4 recover rates among eligible patients. We find that disability grant threshold rules caused recipients to have a relatively slower CD4 recovery rate of about 20-30 cells/mm3/year, or a 20% reduction in the speed of recovery around the threshold. ","Disability for HIV and Disincentives for Health: The Impact of South
  Africa's Disability Grant on HIV/AIDS Recovery"
82,1048130240130822144,4814961623,Johannes Schmidt,['New preprint paper out: District heating systems under high CO2 emission prices: the role of the pass-through from emission cost to electricity prices\n\n<LINK>'],https://arxiv.org/abs/1810.02109,"Low CO2 prices have prompted discussion about political measures aimed at increasing the cost of carbon dioxide emissions. These costs affect, inter alia, integrated district heating system operators (DHSO), often owned by municipalities with some political influence, that use a variety of (CO2 emis- sion intense) heat generation technologies. We examine whether DHSOs have an incentive to support measures that increase CO2 emission prices in the short term. Therefore, we (i) develop a simplified analytical framework to analyse optimal decisions of a district heating operator, and (ii) investigate the market-wide effects of increasing emission prices, in particular the pass- through from emission costs to electricity prices. Using a numerical model of the common Austrian and German power system, we estimate a pass-through from CO2 emission prices to power prices between 0.69 and 0.53 as of 2017, depending on the absolute emission price level. We find the CO2 emission cost pass-through to be sufficiently high so that low-emission district heating systems operating at least moderately efficient generation units benefit from rising CO2 emission prices in the short term. ","District heating systems under high CO2 emission prices: the role of the
  pass-through from emission cost to electricity prices"
83,1047907118383124480,3008464880,Carnegie Astronomy,"['Read this paper today! New star with no detectible iron: ""Chemical Abundance Signature of J0023+0307 -- A Second-Generation Main-Sequence Star with [Fe/H]&lt;-6"" <LINK> \nAuthors: @annafrebel @alexanderpji @Teresehansen @AstroRana @AnirudhChiti I.Thompson &amp; T.Merle']",https://arxiv.org/abs/1810.01228,"We present a chemical abundance analysis of the faint halo metal-poor main-sequence star J0023+0307, with [Fe/H]<-6.3, based on a high-resolution (R~35,000) Magellan/MIKE spectrum. The star was originally found to have [Fe/H]< -6.6 based on a Ca II K measurement in an R~2,500 spectrum. No iron lines could be detected in our MIKE spectrum. Spectral lines of Li, C, Na, Mg, Al, Si, and Ca were detected. The Li abundance is close to the Spite Plateau, A(Li) = 1.7, not unlike that of other metal-poor stars although in stark contrast to the extremely low value found e.g., in HE~1327-2326 at a similar [Fe/H] value. The carbon G-band is detected and indicates strong C-enhancement, as is typical for stars with low Fe abundances. Elements from Na through Si show a strong odd-even effect, and J0023+0307 displays the second-lowest known [Ca/H] abundance. Overall, the abundance pattern of J0023+0307 suggests that it is a second-generation star that formed from gas enriched by a massive Population III first star exploding as a fall-back supernova The inferred dilution mass of the ejecta is 10^(5+-0.5) Msun of hydrogen, strongly suggesting J0023+0307 formed in a recollapsed minihalo. J0023+0307 is likely very old because it has a very eccentric orbit with a pericenter in the Galactic bulge. ","Chemical Abundance Signature of J0023+0307 -- A Second-Generation
  Main-Sequence Star with [Fe/H]&lt;-6"
84,1047646779221655552,16548937,Shubhendu Trivedi,['Our new paper led by @joaomgcaldeira <LINK> on lensing reconstruction of the Cosmic Microwave Background by using Residual U Nets. Project was guided by the inimitable @iamstarnord interesting problem where you predict a continuous field from input images.'],https://arxiv.org/abs/1810.01483,"Next-generation cosmic microwave background (CMB) experiments will have lower noise and therefore increased sensitivity, enabling improved constraints on fundamental physics parameters such as the sum of neutrino masses and the tensor-to-scalar ratio r. Achieving competitive constraints on these parameters requires high signal-to-noise extraction of the projected gravitational potential from the CMB maps. Standard methods for reconstructing the lensing potential employ the quadratic estimator (QE). However, the QE performs suboptimally at the low noise levels expected in upcoming experiments. Other methods, like maximum likelihood estimators (MLE), are under active development. In this work, we demonstrate reconstruction of the CMB lensing potential with deep convolutional neural networks (CNN) - ie, a ResUNet. The network is trained and tested on simulated data, and otherwise has no physical parametrization related to the physical processes of the CMB and gravitational lensing. We show that, over a wide range of angular scales, ResUNets recover the input gravitational potential with a higher signal-to-noise ratio than the QE method, reaching levels comparable to analytic approximations of MLE methods. We demonstrate that the network outputs quantifiably different lensing maps when given input CMB maps generated with different cosmologies. We also show we can use the reconstructed lensing map for cosmological parameter estimation. This application of CNN provides a few innovations at the intersection of cosmology and machine learning. First, while training and regressing on images, we predict a continuous-variable field rather than discrete classes. Second, we are able to establish uncertainty measures for the network output that are analogous to standard methods. We expect this approach to excel in capturing hard-to-model non-Gaussian astrophysical foreground and noise contributions. ","DeepCMB: Lensing Reconstruction of the Cosmic Microwave Background with
  Deep Neural Networks"
85,1047527491324313600,252867237,Juan Miguel Arrazola,"['New @XanaduAI paper out! ""Classical benchmarking of Gaussian Boson Sampling on the Titan supercomputer."" <LINK>\n\nWe simulate a GBS for a system of 800 modes and 20 detector clicks and discover that threshold detectors are awesome. <LINK>']",https://arxiv.org/abs/1810.00900,"Gaussian Boson Sampling is a model of photonic quantum computing where single-mode squeezed states are sent through linear-optical interferometers and measured using single-photon detectors. In this work, we employ a recent exact sampling algorithm for GBS with threshold detectors to perform classical simulations on the Titan supercomputer. We determine the time and memory resources as well as the amount of computational nodes required to produce samples for different numbers of modes and detector clicks. It is possible to simulate a system with 800 optical modes postselected on outputs with 20 detector clicks, producing a single sample in roughly two hours using $40\%$ of the available nodes of Titan. Additionally, we benchmark the performance of GBS when applied to dense subgraph identification, even in the presence of photon loss. We perform sampling for several graphs containing as many as 200 vertices. Our findings indicate that large losses can be tolerated and that the use of threshold detectors is preferable over using photon-number-resolving detectors postselected on collision-free outputs. ","Classical benchmarking of Gaussian Boson Sampling on the Titan
  supercomputer"
86,1047517448948371456,54849207,Ian Harrison,"[""New paper from PhD student Tom Hillier, in which we make a nice detection of a shear power spectrum in the COSMOS field, but the radio data still isn't good enough, sadly.\n<LINK>"", 'We do find some correlation in radio-optical galaxy position angles though, which is nice circumstantial evidence! https://t.co/QfIKUfaYka', '...on a personal note, this is my first publication in (almost exactly) 5 years with a new affiliation, which is nice.']",https://arxiv.org/abs/1810.01220,"We present a weak lensing analysis of the 3 GHz VLA radio survey of the COSMOS field, which we correlate with overlapping HST-ACS optical observations using both intrinsic galaxy shape and cosmic shear correlation statistics. After cross-matching sources between the two catalogues, we measure the correlations of galaxy position angles and find a Pearson correlation coefficient of $0.14 \pm 0.03$. This is a marked improvement from previous studies which found very weak, or non-existent correlations, and gives insight into the emission processes of radio and optical galaxies. We also extract power spectra of averaged galaxy ellipticities (the primary observable for cosmic shear) from the two catalogues, and produce optical-optical, radio-optical and radio-radio spectra. The optical-optical auto-power spectrum was measured to a detection significance of 9.80$\sigma$ and is consistent with previous observations of the same field. For radio spectra (which we do not calibrate, given the unknown nature of their systematics), although we do not detect significant radio-optical (1.50$\sigma$) or radio-radio (1.45$\sigma$) $E$-mode power spectra, we do find the $E$-mode spectra to be more consistent with the shear signal expected from previous studies than with a null signal, and vice versa for $B$-mode and $EB$ cross-correlation spectra. Our results give promise that future radio weak lensing surveys with larger source number densities over larger areas will have the capability to measure significant weak lensing signals. ","Radio-Optical Galaxy Shape and Shear Correlations in the COSMOS Field
  using 3 GHz VLA Observations"
87,1047392281177849856,235735524,Jonathan Pritchard,"['Our new paper led by @ClaudejpSchmit with me and @AlanHeavens on the gravitational and lensing-ISW 21cm bispectrum at low redshift. Come for the ‚Äúsail plots‚Äù, stay for the science. <LINK>', '@ClaudejpSchmit @AlanHeavens https://t.co/uMpVrDCmJ6', '@ClaudejpSchmit @AlanHeavens (Is that how this works? Or is that far too silly?)', '@ClaudejpSchmit @AlanHeavens It‚Äôs one of jubilant celebration. From Monty Python and the Holy Grail. https://t.co/fzclesTcaY']",https://arxiv.org/abs/1810.00973,"Cosmic Microwave Background experiments from COBE to Planck, have launched cosmology into an era of precision science, where many cosmological parameters are now determined to the percent level. Next generation telescopes, focussing on the cosmological 21cm signal from neutral hydrogen, will probe enormous volumes in the low-redshift Universe, and have the potential to determine dark energy properties and test modifications of Einstein's gravity. We study the 21cm bispectrum due to gravitational collapse as well as the contribution by line of sight perturbations in the form of the lensing-ISW bispectrum at low-redshifts ($z \sim 0.35-3$), targeted by upcoming neutral hydrogen intensity mapping experiments. We compute the expected bispectrum amplitudes and use a Fisher forecast model to compare power spectrum and bispectrum observations of intensity mapping surveys by CHIME, MeerKAT and SKA-mid. We find that combined power spectrum and bispectrum observations have the potential to decrease errors on the cosmological parameters by an order of magnitude compared to Planck. Finally, we compute the contribution of the lensing-ISW bispectrum, and find that, unlike for the cosmic microwave background analyses, it can safely be ignored for 21cm bispectrum observations. ",The gravitational and lensing-ISW bispectrum of 21cm radiation
88,1047311716634304513,69202541,Jonathan Le Roux,"['Our new paper is out on arXiv: ""Phasebook and Friends: Leveraging Discrete Representations for Source Separation"" <LINK>']",https://arxiv.org/abs/1810.01395,"Deep learning based speech enhancement and source separation systems have recently reached unprecedented levels of quality, to the point that performance is reaching a new ceiling. Most systems rely on estimating the magnitude of a target source by estimating a real-valued mask to be applied to a time-frequency representation of the mixture signal. A limiting factor in such approaches is a lack of phase estimation: the phase of the mixture is most often used when reconstructing the estimated time-domain signal. Here, we propose ""magbook"", ""phasebook"", and ""combook"", three new types of layers based on discrete representations that can be used to estimate complex time-frequency masks. Magbook layers extend classical sigmoidal units and a recently introduced convex softmax activation for mask-based magnitude estimation. Phasebook layers use a similar structure to give an estimate of the phase mask without suffering from phase wrapping issues. Combook layers are an alternative to the magbook-phasebook combination that directly estimate complex masks. We present various training and inference schemes involving these representations, and explain in particular how to include them in an end-to-end learning framework. We also present an oracle study to assess upper bounds on performance for various types of masks using discrete phase representations. We evaluate the proposed methods on the wsj0-2mix dataset, a well-studied corpus for single-channel speaker-independent speaker separation, matching the performance of state-of-the-art mask-based approaches without requiring additional phase reconstruction steps. ","Phasebook and Friends: Leveraging Discrete Representations for Source
  Separation"
89,1047301155355688960,101980926,Masahito Yamazaki,"['my new paper ""Polology of Superconformal Blocks""\n<LINK>']",https://arxiv.org/abs/1810.01264,"We systematically classify all possible poles of superconformal blocks as a function of the scaling dimension of intermediate operators, for all superconformal algebras in dimensions three and higher. This is done by working out the recently-proven irreducibility criterion for parabolic Verma modules for classical basic Lie superalgebras. The result applies to correlators for external operators of arbitrary spin, and indicates presence of infinitely many short multiplets of superconformal algebras, most of which are non-unitary. We find a set of poles whose positions are shifted by linear in $\mathcal{N}$ for $\mathcal{N}$-extended supersymmetry. We find an interesting subtlety for 3d $\mathcal{N}$-extended superconformal algebra with $\mathcal{N}$ odd associated with odd non-isotropic roots. We also comment on further applications to superconformal blocks. ",Polology of Superconformal Blocks
90,1047142041862516738,959567365414510597,Ryan Martin,"['New paper on high-dim Bayesian-like regression with an empirical correlation-adaptive prior, by two of my PhD students: Chang Liu and Yue Yang.  <LINK>']",https://arxiv.org/abs/1810.00739,"In the context of a high-dimensional linear regression model, we propose the use of an empirical correlation-adaptive prior that makes use of information in the observed predictor variable matrix to adaptively address high collinearity, determining if parameters associated with correlated predictors should be shrunk together or kept apart. Under suitable conditions, we prove that this empirical Bayes posterior concentrates around the true sparse parameter at the optimal rate asymptotically. A simplified version of a shotgun stochastic search algorithm is employed to implement the variable selection procedure, and we show, via simulation experiments across different settings and a real-data application, the favorable performance of the proposed method compared to existing methods. ","Bayesian inference in high-dimensional linear models using an empirical
  correlation-adaptive prior"
91,1047130083167744000,786857723660996608,Luigi Celona,"['New paper named ""Benchmark Analysis of Representative Deep Neural Network Architectures"". Check it out. <LINK>\n#imagingandvisionlaboratory']",https://arxiv.org/abs/1810.00736,"This work presents an in-depth analysis of the majority of the deep neural networks (DNNs) proposed in the state of the art for image recognition. For each DNN multiple performance indices are observed, such as recognition accuracy, model complexity, computational complexity, memory usage, and inference time. The behavior of such performance indices and some combinations of them are analyzed and discussed. To measure the indices we experiment the use of DNNs on two different computer architectures, a workstation equipped with a NVIDIA Titan X Pascal and an embedded system based on a NVIDIA Jetson TX1 board. This experimentation allows a direct comparison between DNNs running on machines with very different computational capacity. This study is useful for researchers to have a complete view of what solutions have been explored so far and in which research directions are worth exploring in the future; and for practitioners to select the DNN architecture(s) that better fit the resource constraints of practical deployments and applications. To complete this work, all the DNNs, as well as the software used for the analysis, are available online. ",Benchmark Analysis of Representative Deep Neural Network Architectures
92,1047108038849638400,822867138,Bradley Kavanagh,"['""Faint Light from #DarkMatter"": <LINK>\n\nIn a new paper out today, we try to classify &amp; update constraints on different ways that DM could interact with the Standard Model photon\n\nIn an EFT or in specific UV models, DM interacting with light is a bit of a mess <LINK>', 'The whole project started w/ the question ""What do DM-photon interactions look like in experiments?""\n\nThen: ""What are all the ways DM can interact with light?""\n\nTurns out that there are some other interactions you need for everything to be consistent. Pretty soon you end up here: https://t.co/qu60h0AKCM', ""DM-photon couplings (&amp; interactions they bring along for the ride) can be constrained by direct/indirect-detection+colliders\n\nIn some cases, constraints aren't very strong. For Majorana DM, New Physics mediating these interactions (charged scalars?) could be as light as 100 GeV https://t.co/WM41Kuc1UI""]",https://arxiv.org/abs/1810.00033,"Even if Dark Matter (DM) is neutral under electromagnetism, it can still interact with the Standard Model (SM) via photon exchange from higher-dimensional operators. Here we classify the general effective operators coupling DM to photons, distinguishing between Dirac/Majorana fermion and complex/real scalar DM. We provide model-independent constraints on these operators from direct and indirect detection. We also constrain various DM-lepton operators, which induce DM-photon interactions via RG running or which typically arise in sensible UV-completions. This provides a simple way to quickly assess constraints on any DM model that interacts mainly via photon exchange or couples to SM leptons. ","Faint Light from Dark Matter: Classifying and Constraining Dark
  Matter-Photon Effective Operators"
93,1047097990886952960,795877354266456064,KoheiKamadaPhys,['<LINK>\nOur new paper appeared. It‚Äôs on cosmological phase transition in Twin Higgs. It‚Äôs a Kohei collaboration ;)'],https://arxiv.org/abs/1810.00574,"We study twin Higgs models at non-zero temperature and discuss cosmological phase transitions as well as their implications on electroweak baryogenesis and gravitational waves. It is shown that the expectation value of the Higgs field at the critical temperature of the electroweak phase transition is much smaller than the critical temperature, which indicates two important facts: (i) the electroweak phase transition cannot be analyzed perturbatively (ii) the electroweak baryogenesis is hardly realized in the typical realizations of twin Higgs models. We also analyze the phase transition associated with the global symmetry breaking, through which the Standard Model Higgs is identified with one of the pseudo-Nambu-Goldstone bosons in terms of its linear realization, with and without supersymmetry. For this phase transition, we show that, only in the supersymmetric case, there are still some parameter spaces, in which the perturbative approach is validated and the phase transition is the first order. We find that the stochastic gravitational wave background is generated through this first order phase transition, but it is impossible to be detected by DECIGO or BBO in the linear realization and the decoupling limit. The detection of stochastic gravitational wave background with the feature of first order phase transition, therefore, will give strong constraints on twin Higgs models. ",Phase Transitions in Twin Higgs Models
94,1047013460654075905,2419577598,Hazrat Ali,"['New paper on \nGenerative Adversarial Network for Medical Images (MI-GAN), accepted in Journal of Medical Systems.\n<LINK>\n#machinelearning #deeplearning #gan #adversarial #artificialintelligence #medicalimaging']",https://arxiv.org/abs/1810.00551,"Deep learning algorithms produces state-of-the-art results for different machine learning and computer vision tasks. To perform well on a given task, these algorithms require large dataset for training. However, deep learning algorithms lack generalization and suffer from over-fitting whenever trained on small dataset, especially when one is dealing with medical images. For supervised image analysis in medical imaging, having image data along with their corresponding annotated ground-truths is costly as well as time consuming since annotations of the data is done by medical experts manually. In this paper, we propose a new Generative Adversarial Network for Medical Imaging (MI-GAN). The MI-GAN generates synthetic medical images and their segmented masks, which can then be used for the application of supervised analysis of medical images. Particularly, we present MI-GAN for synthesis of retinal images. The proposed method generates precise segmented images better than the existing techniques. The proposed model achieves a dice coefficient of 0.837 on STARE dataset and 0.832 on DRIVE dataset which is state-of-the-art performance on both the datasets. ",Generative Adversarial Network for Medical Images (MI-GAN)
95,1046923069942521856,296161364,Chris Power,"['Our new paper on the gaseous outskirts of galaxy clusters, using the nIFTy dataset, is on arXiv today - <LINK>. We show how motions of infalling groups &amp; accretion of gas from cosmic web shape spatial, kinematic, and thermodynamic structure of cluster outskirts. <LINK>']",https://arxiv.org/abs/1810.00534,"Galaxy cluster outskirts mark the transition region from the mildly non-linear cosmic web to the highly non-linear, virialised, cluster interior. It is in this transition region that the intra-cluster medium (ICM) begins to influence the properties of accreting galaxies and groups, as ram pressure impacts a galaxy's cold gas content and subsequent star formation rate. Conversely, the thermodynamical properties of the ICM in this transition region should also feel the influence of accreting substructure (i.e. galaxies and groups), whose passage can drive shocks. In this paper, we use a suite of cosmological hydrodynamical zoom simulations of a single galaxy cluster, drawn from the nIFTy comparison project, to study how the dynamics of substructure accreted from the cosmic web influences the thermodynamical properties of the ICM in the cluster's outskirts. We demonstrate how features evident in radial profiles of the ICM (e.g. gas density and temperature) can be linked to strong shocks, transient and short-lived in nature, driven by the passage of substructure. The range of astrophysical codes and galaxy formation models in our comparison are broadly consistent in their predictions (e.g. agreeing when and where shocks occur, but differing in how strong shocks will be); this is as we would expect of a process driven by large-scale gravitational dynamics and strong, inefficiently radiating, shocks. This suggests that mapping such shock structures in the ICM in a cluster's outskirts (via e.g. radio synchrotron emission) could provide a complementary measure of its recent merger and accretion history. ","nIFTy Galaxy Cluster simulations VI: The dynamical imprint of
  substructure on gaseous cluster outskirts"
96,1057558437159206912,869154646694264832,Hugh Salimbeni,"['Delighted to share our new paper, Gaussian Process Conditional Density Estimation <LINK>, appearing at NIPS. With @vdutor, @mpd37 and @jameshensman.', 'If you‚Äôre a VAE person, you can see this as a conditional VAE with a Gaussian process for the decoder.', 'If you‚Äôre a GP person, you can see this as a hybrid between (mutioutput) regression and the Bayesian-GPLVM. We combine observed inputs with latent variables via concatenation.', 'On the GP side, we develop the GPLVM model in a number of ways: natural gradients (which are essential in the minibatch setting), correlated outputs (e.g. for a priori pixel correlations) and probabilistic linear projections of the inputs', 'On the VAE side, we show being Bayesian about the mapping mitigates overfitting and improves test performance, especially in the low data regimes.', 'It was great fun working on this project with the excellent people at @PROWLER_IO ! Particular shout out to @vdutor üëèüëèüëè']",https://arxiv.org/abs/1810.12750,"Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GP) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images. ",Gaussian Process Conditional Density Estimation
97,1055736715795668992,1576276220,Dr. Emily Petroff,"[""Hey everybody! My latest paper is out today! I found a new fast radio burst in some old Parkes data and it's quite a cool one. Let me tell you about it! First off, you can find the paper here: <LINK> (1/11)"", 'So let me introduce you to FRB 110214. This FRB happened on February 14, 2011 (Valentines Day! ‚ù§Ô∏è) and was picked up by the Parkes radio telescope in Australia. (2/11) https://t.co/aMANHVVkzQ', 'It was found in the same survey that found 9 other FRBs including the first 4 after the Lorimer burst that got people excited about the field again. The FRB detected most clearly in that survey was FRB 110220, which actually happened only 6 days after this one. (3/11) https://t.co/rVntvKAsPD', ""But FRB 110214 was missed in our 1st searches. Why? A few reasons: 1. It was quite weak (e.g. hard to tell if it's real), 2. The processing failed the 1st time so the data were only looked at later, and (most importantly) 3. It has a really low dispersion measure (4/11)"", ""Dispersion measure (or DM) tells us how much material the FRB traveled through, or roughly how far away it came from. A low DM means a very nearby FRB! And FRB 110214 has one of the lowest DMs we've ever seen for an FRB. (5/11)"", ""But if it's so nearby and FRBs are so strong shouldn't it be really bright? FRB 110214 was actually found in 2 pixels of the telescope separated by quite a big distance on the sky. This means it wasn't actually in either but still so bright that it showed up in both. (6/11) https://t.co/dQ2PccI9vX"", 'We did some models and traced back 3 regions where the FRB may have come from, one close to both beams (2 &amp; 8), and two further away. Even in the nearest region it would have been BRIGHT, brighter than all but two of all the FRBs found with Parkes. (7/11) https://t.co/fCGZhfqE55', ""If it came from one of the outer regions, that would make it the brightest FRB found by Parkes! But we can't distinguish between these regions, so we can only set limits on how bright it truly was. (8/11)"", ""We also tried to look for a host galaxy in the error regions. There are a few known galaxies in there, but without more information we can't distinguish between them. (9/11)"", ""What we really hope is that FRB 110214 repeats eventually. We spent almost 100 hours looking for repeats with Parkes and didn't see anything... But if it ever does repeat, the low DM would make FRB 110214 one of our best opportunities for studying where FRBs come from! (10/11)"", ""That's it for now. Read the paper here: https://t.co/k18QCkIc1d, but stay tuned in the next week or so, where I'll write up a bit more about this fun discovery on my new blog: https://t.co/zdbwKXkrO1 (11/11)""]",https://arxiv.org/abs/1810.10773,"Fast radio bursts (FRBs) are millisecond pulses of radio emission of seemingly extragalactic origin. More than 50 FRBs have now been detected, with only one seen to repeat. Here we present a new FRB discovery, FRB 110214, which was detected in the high latitude portion of the High Time Resolution Universe South survey at the Parkes telescope. FRB 110214 has one of the lowest dispersion measures of any known FRB (DM = 168.9$\pm$0.5 pc cm$^{-3}$), and was detected in two beams of the Parkes multi-beam receiver. A triangulation of the burst origin on the sky identified three possible regions in the beam pattern where it may have originated, all in sidelobes of the primary detection beam. Depending on the true location of the burst the intrinsic fluence is estimated to fall in the range of 50 -- 2000 Jy ms, making FRB 110214 one of the highest-fluence FRBs detected with the Parkes telescope. No repeating pulses were seen in almost 100 hours of follow-up observations with the Parkes telescope down to a limiting fluence of 0.3 Jy ms for a 2-ms pulse. Similar low-DM, ultra-bright FRBs may be detected in telescope sidelobes in the future, making careful modeling of multi-beam instrument beam patterns of utmost importance for upcoming FRB surveys. ",A fast radio burst with a low dispersion measure
98,1053361963567403008,1128765451,Tiark Rompf,"['New paper, in collaboration with Google Brain: \n""AutoGraph: Imperative-Style Coding with Graph-Based Performance""\n<LINK>']",https://arxiv.org/abs/1810.08061,"There is a perceived trade-off between machine learning code that is easy to write, and machine learning code that is scalable or fast to execute. In machine learning, imperative style libraries like Autograd and PyTorch are easy to write, but suffer from high interpretive overhead and are not easily deployable in production or mobile settings. Graph-based libraries like TensorFlow and Theano benefit from whole-program optimization and can be deployed broadly, but make expressing complex models more cumbersome. We describe how the use of staged programming in Python, via source code transformation, offers a midpoint between these two library design patterns, capturing the benefits of both. A key insight is to delay all type-dependent decisions until runtime, via dynamic dispatch. We instantiate these principles in AutoGraph, a software system that improves the programming experience of the TensorFlow library, and demonstrate usability improvements with no loss in performance compared to native TensorFlow graphs. We also show that our system is backend agnostic, and demonstrate targeting an alternate IR with characteristics not found in TensorFlow graphs. ",AutoGraph: Imperative-style Coding with Graph-based Performance
99,1049745741445500928,1614231872,Aaron Fisher,"['Influence functions are central to many stat methods, but they can be hard to visualize. Our new educational paper tries to tackle this.\n\nWe organized the paper around 2 sets of visuals, which are colorful and also not scary.\n\n<LINK>\n\nwith @edwardhkennedy <LINK>', '@Lluis_Rev @edwardhkennedy Yes, still equations also.\n\nRe target audience: the people who would get the most out of this would be statisticians, stats students, and quantitative researchers in related fields, who have some familiarity with multivariate calculus.', 'Also, to clarify, we focus on how influence functions can be used for (first order) bias correction. For example, some doubly robust estimators for average treatment effects fall into this category of approaches.']",https://arxiv.org/abs/1810.03260,"Estimators based on influence functions (IFs) have been shown to be effective in many settings, especially when combined with machine learning techniques. By focusing on estimating a specific target of interest (e.g., the average effect of a treatment), rather than on estimating the full underlying data generating distribution, IF-based estimators are often able to achieve asymptotically optimal mean-squared error. Still, many researchers find IF-based estimators to be opaque or overly technical, which makes their use less prevalent and their benefits less available. To help foster understanding and trust in IF-based estimators, we present tangible, visual illustrations of when and how IF-based estimators can outperform standard ``plug-in'' estimators. The figures we show are based on connections between IFs, gradients, linear approximations, and Newton-Raphson. ",Visually Communicating and Teaching Intuition for Influence Functions
100,1049709335624867840,875429792832659456,Louie Hoang,"['Our latest paper is up, setting a new state of the art on the LAMBADA machine reading comprehension dataset. Check us out at EMNLP this year. <LINK> #NLP #MachineLearning #DeepLearning']",https://arxiv.org/abs/1810.02891,"Reading comprehension tasks test the ability of models to process long-term context and remember salient information. Recent work has shown that relatively simple neural methods such as the Attention Sum-Reader can perform well on these tasks; however, these systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset, particularly on difficult entity examples. ",Entity Tracking Improves Cloze-style Reading Comprehension
101,1048537143654670336,335473253,Jason Kalirai,"['Our group‚Äôs new paper, led by postdoc Roger Cohen, on a #Hubble investigation of substructure in the halo of the Andromeda Spiral Galaxy, M31.  Roger measures the distance and metallicity gradient of the stream to better constrain galaxy formation models. <LINK> <LINK>']",https://arxiv.org/abs/1810.01525,"The Giant Southern Stream (GSS) of M31, a keystone signature of a major accretion event, yields crucial constraints on M31 formation and evolution models. Currently, our understanding of the GSS, in terms of both its geometry and its chemistry, results from either wide-field imaging probing only a few magnitudes below the red giant branch tip, or deep imaging or spectroscopy of isolated regions. Here, we take an alternative approach, using Hubble Space Telescope (HST) imaging to characterize the horizontal branch red clump (RC) using unbinned maximum likelihood fits to luminosity functions (LFs) from observed color-magnitude diagrams (CMDs). Comparing the RC mean magnitude across three fields at projected distances of 21, 52 and 80 kpc from M31, we find a line of sight distance gradient identical to recent literature measurements in fields along the core. We also find tentative evidence that the line of sight distance dispersion increases with projected distance from M31. Meanwhile, the metallicity in the 52 kpc field westward of the GSS core is at least as high as that in the 21 kpc GSS core field, and the peak colors of the RC in these two fields imply identical metallicities to within 0.2 dex. We discuss implications for distance and metallicity gradients both along and perpendicular to the GSS in the context of recent ground-based photometric and spectroscopic results, including evidence for a dropoff in metallicity moving westward from the GSS, as well as prospects for further constraining stellar populations in the vicinity of the GSS. ","Project AMIGA: Distance and Metallicity Gradients Along Andromeda's
  Giant Southern Stream from the Red Clump"
102,1047412286330220544,803244372,Marcel Broersma,"['Our new paper on algorithmic transparency &amp; assessment of machine learning models: \n\n""Utilizing a Transparency-driven Environment toward Trusted Automatic Genre Classification"" <LINK>\n\nMore info: <LINK> @univgroningen @CWInl @eScienceCenter']",https://arxiv.org/abs/1810.00968,"With the growing abundance of unlabeled data in real-world tasks, researchers have to rely on the predictions given by black-boxed computational models. However, it is an often neglected fact that these models may be scoring high on accuracy for the wrong reasons. In this paper, we present a practical impact analysis of enabling model transparency by various presentation forms. For this purpose, we developed an environment that empowers non-computer scientists to become practicing data scientists in their own research field. We demonstrate the gradually increasing understanding of journalism historians through a real-world use case study on automatic genre classification of newspaper articles. This study is a first step towards trusted usage of machine learning pipelines in a responsible way. ","Utilizing a Transparency-driven Environment toward Trusted Automatic
  Genre Classification: A Case Study in Journalism History"
103,1060563978605207554,14073989,Gustav Markkula,"['Preprint! We extend an existing model of steering with mechanisms for visual-vestibular integration and behavioural adaptation, and find that this new model explains driver behaviour observed in slalom tasks with scaled simulator motion cues\n\n<LINK> <LINK>', 'We sort of expected/hoped that the model would reproduce the general empirical patterns of more inaccurate and effortful steering performance upon removal of motion cues, and then improvement with adaptation ... and it did üòä https://t.co/xOCPvJnRbv', 'However, there\'s another intriguing phenomenon here: Human drivers track a normative slalom path more accurately with slightly down-scaled simulator motion, i.e., they drive ""better"" with less realistic simulator motion. And unexpectedly the model did too! üòµü¶Ñ https://t.co/8U5Cwe7SGz', ""(I know simulator slalom is not a cure for cancer or anything, but as someone doing modelling, having your model explain something you didn't set out to explain is pretty üëçü•≥ü§üüòç) (of course disclaimer: model might still be completely wrong etc etc... but ü§©üåànonetheless )"", 'The model essentially explains all of these findings in the same way: With down-scaled simulator motion, drivers behave as if the simulated vehicle is rotating less than it is - and it turns out that in limited doses this can be beneficial for slalom path-tracking', 'Co-authors: @realsimcreator Rachel Waldram @Oscar_Giles @CallumDMole @WilkieRichard ... (but the emojis are on me)']",https://arxiv.org/abs/1810.12441,"It is well established that not only vision but also other sensory modalities affect drivers' control of their vehicles, and that drivers adapt over time to persistent changes in sensory cues (for example in driving simulators), but the mechanisms underlying these behavioural phenomena are poorly understood. Here, we consider the existing literature on how driver steering in slalom tasks is affected by the down-scaling of vestibular cues, and propose a driver model that can explain the empirically observed effects, namely: decreased task performance and increased steering effort during initial exposure, followed by a partial reversal of these effects as task exposure is prolonged. Unexpectedly, the model also reproduced another empirical finding: a local optimum for motion down-scaling, where path-tracking is better than when one-to-one motion cues are available. Overall, the results imply that: (1) drivers make direct use of vestibular information as part of determining appropriate steering, and (2) motion down-scaling causes a yaw rate underestimation phenomenon, where drivers behave as if the simulated vehicle is rotating more slowly than it is. However, (3) in the slalom task, a certain degree of such yaw rate underestimation is beneficial to path tracking performance. Furthermore, (4) behavioural adaptation, as empirically observed in slalom tasks, may occur due to (a) down-weighting of vestibular cues, and/or (b) increased sensitivity to control errors, in determining when to adjust steering and by how much, but (c) seemingly not in the form of a full compensatory rescaling of the received vestibular input. The analyses presented here provide new insights and hypotheses about simulator driving, and the developed models can be used to support research on multisensory integration and behavioural adaptation in both driving and other task domains. ","Modelling visual-vestibular integration and behavioural adaptation in
  the driving simulator"
104,1059752149935112192,3792459557,Joan Serr√†,['With @jordiponsdotme we took the brave path and studied training deep audio classifiers with less than 10 instances and no validation data <LINK> <LINK>'],https://arxiv.org/abs/1810.10274,"We investigate supervised learning strategies that improve the training of neural network audio classifiers on small annotated collections. In particular, we study whether (i) a naive regularization of the solution space, (ii) prototypical networks, (iii) transfer learning, or (iv) their combination, can foster deep learning models to better leverage a small amount of training examples. To this end, we evaluate (i-iv) for the tasks of acoustic event recognition and acoustic scene classification, considering from 1 to 100 labeled examples per class. Results indicate that transfer learning is a powerful strategy in such scenarios, but prototypical networks show promising results when one does not count with external or validation data. ",Training neural audio classifiers with few data
105,1057179920403718144,1015437560,Anton Osokin,"['A new episode of our study on structured prediction surrogates through the lens of consistency is finally online (and accepted to NIPS 2018).\nWe now can give some guarantees for inconsistent settings!\n<LINK>\nJoint work with Kirill Struminsky and @SimonLacosteJ', ""@MLpager @SimonLacosteJ Sure, it's Canada this time, a welcoming country :-)""]",https://arxiv.org/abs/1810.11544,"We study consistency properties of machine learning methods based on minimizing convex surrogates. We extend the recent framework of Osokin et al. (2017) for the quantitative analysis of consistency properties to the case of inconsistent surrogates. Our key technical contribution consists in a new lower bound on the calibration function for the quadratic surrogate, which is non-trivial (not always zero) for inconsistent cases. The new bound allows to quantify the level of inconsistency of the setting and shows how learning with inconsistent surrogates can have guarantees on sample complexity and optimization difficulty. We apply our theory to two concrete cases: multi-class classification with the tree-structured loss and ranking with the mean average precision loss. The results show the approximation-computation trade-offs caused by inconsistent surrogates and their potential benefits. ",Quantifying Learning Guarantees for Convex but Inconsistent Surrogates
106,1055306303076102144,887992016,Luke Metz,"['Check out our new work on meta-learning optimizers! We explore problems when meta-training with gradients, propose a solution using variational optimization, and train task specific learned optimizers that are 5x faster than Adam! <LINK> <LINK>', 'We also explore meta-training against validation loss, creating learned optimizers that generalize better than existing hand designed methods! Thanks to my amazing collaborators @niru_m @JvNixon Daniel Freeman, @jaschasd!', ""@Knix01133550 @hardmaru We do just that!\nWe train the optimizer on a collection of meta-train tasks (each with a train and valid data). Once trained, we apply the optimizer to new tasks, never seen in training. At test time, we don't have access to this new task's validation set, so no overfitting!""]",https://arxiv.org/abs/1810.10180,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process resulting in gradients that are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance, allowing us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss. ","Understanding and correcting pathologies in the training of learned
  optimizers"
107,1055295872500527104,65876824,Jascha Sohl-Dickstein,"['Learned optimizers with less mind numbing pain! We analyze, and propose a solution to, pathologies in meta-training via unrolled optimization. Then we meta-train an optimizer targeting CNN training that outperforms SGD/Adam by 5x (!!!) wall clock time. <LINK> <LINK>', 'Interestingly, we can also meta-train an optimizer to target *validation* rather than train loss, in which case we achieve better validation loss than first order methods. Work with @Luke_Metz @niru_m @JvNixon Daniel Freeman.']",https://arxiv.org/abs/1810.10180,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process resulting in gradients that are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance, allowing us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss. ","Understanding and correcting pathologies in the training of learned
  optimizers"
108,1055278437441560576,22821179,Niru Maheswaranathan,"['New work: Learning optimizers with less mind numbing pain! With improvements to stabilize meta-training, we find that we are able to train optimizers that beat well tuned baselines on wall-clock time. (1/2) <LINK> <LINK>', 'One surprising fact is that we can train optimizers against either training or validation loss, and they have a much wider train-valid. gap compared to standard optimizers. Joint work with fantastic collaborators: @Luke_Metz, @JvNixon, Daniel Freeman, and @jaschasd (2/2)']",https://arxiv.org/abs/1810.10180,"Deep learning has shown that learned functions can dramatically outperform hand-designed functions on perceptual tasks. Analogously, this suggests that learned optimizers may similarly outperform current hand-designed optimizers, especially for specific problems. However, learned optimizers are notoriously difficult to train and have yet to demonstrate wall-clock speedups over hand-designed optimizers, and thus are rarely used in practice. Typically, learned optimizers are trained by truncated backpropagation through an unrolled optimization process resulting in gradients that are either strongly biased (for short truncations) or have exploding norm (for long truncations). In this work we propose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on optimizer performance, allowing us to train neural networks to perform optimization of a specific task faster than tuned first-order methods. We demonstrate these results on problems where our learned optimizer trains convolutional networks faster in wall-clock time compared to tuned first-order methods and with an improvement in test loss. ","Understanding and correcting pathologies in the training of learned
  optimizers"
109,1054410238051332096,990433714948661250,Sergey Levine,"['How can mobile robots learn from a variety of sensory readings -- heading, segmentation, speed, collision -- at the same time, using off-policy data? This is what we try to study in ""Composable Action-Conditioned Predictions""\n\n<LINK>\n<LINK>']",https://arxiv.org/abs/1810.07167,"A general-purpose intelligent robot must be able to learn autonomously and be able to accomplish multiple tasks in order to be deployed in the real world. However, standard reinforcement learning approaches learn separate task-specific policies and assume the reward function for each task is known a priori. We propose a framework that learns event cues from off-policy data, and can flexibly combine these event cues at test time to accomplish different tasks. These event cue labels are not assumed to be known a priori, but are instead labeled using learned models, such as computer vision detectors, and then `backed up' in time using an action-conditioned predictive model. We show that a simulated robotic car and a real-world RC car can gather data and train fully autonomously without any human-provided labels beyond those needed to train the detectors, and then at test-time be able to accomplish a variety of different tasks. Videos of the experiments and code can be found at this https URL ","Composable Action-Conditioned Predictors: Flexible Off-Policy Learning
  for Robot Navigation"
110,1053347627385974785,20810416,Dr. Roman Yampolskiy,"['I could not find a good answer on the internet, so I did some research. ""Why We Do Not Evolve Software?""\n<LINK> <LINK>']",https://arxiv.org/abs/1810.07074,"In this paper, we review the state-of-the-art results in evolutionary computation and observe that we do not evolve non trivial software from scratch and with no human intervention. A number of possible explanations are considered, but we conclude that computational complexity of the problem prevents it from being solved as currently attempted. A detailed analysis of necessary and available computational resources is provided to support our findings. ",Why We Do Not Evolve Software? Analysis of Evolutionary Algorithms
111,1052984056231559168,838292815,Ofir Nachum,"['The Laplacian approach is extremely useful in discrete graphs (e.g. mixing times, min-cuts, etc.).  How can we apply similar ideas to RL, where eigendecomposition is not easy and the graph is only known via sampling?  Find out how in our recent paper: <LINK>']",https://arxiv.org/abs/1810.04586,"The smallest eigenvectors of the graph Laplacian are well-known to provide a succinct representation of the geometry of a weighted graph. In reinforcement learning (RL), where the weighted graph may be interpreted as the state transition process induced by a behavior policy acting on the environment, approximating the eigenvectors of the Laplacian provides a promising approach to state representation learning. However, existing methods for performing this approximation are ill-suited in general RL settings for two main reasons: First, they are computationally expensive, often requiring operations on large matrices. Second, these methods lack adequate justification beyond simple, tabular, finite-state settings. In this paper, we present a fully general and scalable method for approximating the eigenvectors of the Laplacian in a model-free RL context. We systematically evaluate our approach and empirically show that it generalizes beyond the tabular, finite-state setting. Even in tabular, finite-state settings, its ability to approximate the eigenvectors outperforms previous proposals. Finally, we show the potential benefits of using a Laplacian representation learned using our method in goal-achieving RL tasks, providing evidence that our technique can be used to significantly improve the performance of an RL agent. ","The Laplacian in RL: Learning Representations with Efficient
  Approximations"
112,1052979707979935744,2872512304,Dr. Elizabeth Hobson,"['How do animals choose their battles? My preprint with @dmonsterman &amp; @SimonDeDeo shows how we can back-infer social knowledge from historic data on aggression in 85 spp to find the rank-informed rules that underlie fight decisions <LINK> <LINK>', ""@EmmanuelDetrini @aaronclauset @dmonsterman @SimonDeDeo Thanks! No humans in this dataset, but stay tuned for future results where we'll have people fighting within a networked computer game....""]",https://arxiv.org/abs/1810.07215,"Members of a social species need to make appropriate decisions about who, how, and when to interact with others in their group. However, it has been difficult for researchers to detect the inputs to these decisions and, in particular, how much information individuals actually have about their social context. We present a new method that can serve as a social assay to quantify how patterns of aggression depend upon information about the ranks of individuals within social dominance hierarchies. Applied to existing data on aggression in 172 social groups across 85 species in 23 orders, it reveals three main patterns of rank-dependent social dominance: the downward heuristic (aggress uniformly against lower-ranked opponents), close competitors (aggress against opponents ranked slightly below self), and bullying (aggress against opponents ranked much lower than self). The majority of the groups (133 groups, 77%) follow a downward heuristic, but a significant minority (38 groups, 22%) show more complex social dominance patterns (close competitors or bullying) consistent with higher levels of social information use. These patterns are not phylogenetically constrained and different groups within the same species can use different patterns, suggesting that heuristics use may depend on context and the structuring of aggression by social information should not be considered a fixed characteristic of a species. Our approach provides new opportunities to study the use of social information within and across species and the evolution of social complexity and cognition. ","Aggression heuristics underlie animal dominance hierarchies and provide
  evidence of group-level social information"
113,1052329766534008832,41280228,Peter J. Liu,"['Most abstractive summarization models based on neural networks require many (expensive to obtain) document-summary pairs to train. In our recent paper, we propose a neural architecture to do abstractive multi-document summarization with no examples: <LINK>']",https://arxiv.org/abs/1810.05739,"Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder where the mean of the representations of the input reviews decodes to a reasonable summary-review while not relying on any review-specific features. We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components. We show through automated metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews. Finally, we collect a reference evaluation dataset and show that our model outperforms a strong extractive baseline. ","MeanSum: A Neural Model for Unsupervised Multi-document Abstractive
  Summarization"
114,1050678541489074176,1023681782363901952,Michal P. Heller,['We are excited to share our collaboration in the study of #complexity and #entanglement for #thermofield double states motivated by #holography [<LINK>]! @hep_th @jenseisert @LFHackl #physics #quantum #gravity <LINK>'],https://arxiv.org/abs/1810.05151,"Motivated by holographic complexity proposals as novel probes of black hole spacetimes, we explore circuit complexity for thermofield double (TFD) states in free scalar quantum field theories using the Nielsen approach. For TFD states at t = 0, we show that the complexity of formation is proportional to the thermodynamic entropy, in qualitative agreement with holographic complexity proposals. For TFD states at t > 0, we demonstrate that the complexity evolves in time and saturates after a time of the order of the inverse temperature. The latter feature, which is in contrast with the results of holographic proposals, is due to the Gaussian nature of the TFD state of the free bosonic QFT. A novel technical aspect of our work is framing complexity calculations in the language of covariance matrices and the associated symplectic transformations, which provide a natural language for dealing with Gaussian states. Furthermore, for free QFTs in 1+1 dimension, we compare the dynamics of circuit complexity with the time dependence of the entanglement entropy for simple bipartitions of TFDs. We relate our results for the entanglement entropy to previous studies on non-equilibrium entanglement evolution following quenches. We also present a new analytic derivation of a logarithmic contribution due to the zero momentum mode in the limit of vanishing mass for a subsystem containing a single degree of freedom on each side of the TFD and argue why a similar logarithmic growth should be present for larger subsystems. ",Complexity and entanglement for thermofield double states
115,1050262006178205696,1260596892,Markus Heinonen,"['Our third #AISTATS submission is ""Harmonizable mixture kernels with variational Fourier features"" with @samikaski, where we propose theoretical foundation of non-stationary kernels as harmonizable covariances with Wigner distributions, <LINK>']",https://arxiv.org/abs/1810.04416,"The expressive power of Gaussian processes depends heavily on the choice of kernel. In this work we propose the novel harmonizable mixture kernel (HMK), a family of expressive, interpretable, non-stationary kernels derived from mixture models on the generalized spectral representation. As a theoretically sound treatment of non-stationary kernels, HMK supports harmonizable covariances, a wide subset of kernels including all stationary and many non-stationary covariances. We also propose variational Fourier features, an inter-domain sparse GP inference framework that offers a representative set of 'inducing frequencies'. We show that harmonizable mixture kernels interpolate between local patterns, and that variational Fourier features offers a robust kernel learning framework for the new kernel family. ",Harmonizable mixture kernels with variational Fourier features
116,1049693858240585734,174052756,Thiago Serra,['New paper on approximating the linear regions of neural nets: <LINK>\n\nWe exploit neat tricks:\n- MIP formulations to find which units are locally stable\n- Approximate counting methods from SAT to get probabilistic LBs on the number of solutions of a MIP\n#orms #ml <LINK>'],https://arxiv.org/abs/1810.03370,"We can compare the expressiveness of neural networks that use rectified linear units (ReLUs) by the number of linear regions, which reflect the number of pieces of the piecewise linear functions modeled by such networks. However, enumerating these regions is prohibitive and the known analytical bounds are identical for networks with same dimensions. In this work, we approximate the number of linear regions through empirical bounds based on features of the trained network and probabilistic inference. Our first contribution is a method to sample the activation patterns defined by ReLUs using universal hash functions. This method is based on a Mixed-Integer Linear Programming (MILP) formulation of the network and an algorithm for probabilistic lower bounds of MILP solution sets that we call MIPBound, which is considerably faster than exact counting and reaches values in similar orders of magnitude. Our second contribution is a tighter activation-based bound for the maximum number of linear regions, which is particularly stronger in networks with narrow layers. Combined, these bounds yield a fast proxy for the number of linear regions of a deep neural network. ",Empirical Bounds on Linear Regions of Deep Rectifier Networks
117,1049411245764435973,241532071,Nataniel Ruiz,['My internship work at NEC Labs Cupertino üî•\nWe propose an algorithm that automatically learns parameters of a simulation engine to generate better training data for a machine learning model in order to maximize performance.\n\n<LINK> <LINK>'],https://arxiv.org/abs/1810.02513,"Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications. ",Learning To Simulate
118,1048128566406209537,778659791753220098,Chakresh,['<LINK> this is our work on collaboration between Indian Institutes in their publications within APS journals. Though the study does not cover the full spectrum of publications we feel it still gives useful insights.'],http://arxiv.org/abs/1810.02129,"In this work, we have studied the collaboration and citation network between Indian Institutes from publications in American Physical Society(APS) journals between 1970-2013. We investigate the role of geographic proximity on the network structure and find that it is the characteristics of the Institution, rather than the geographic distance, that plays a dominant role in collaboration networks. We find that Institutions with better federal funding dominate the network topology and play a crucial role in overall research output. We find that the citation flow across different categories of institutions is strongly linked to the collaborations between them. We have estimated the knowledge flow in and out of Institutions and identified the top knowledge source and sinks. ","Exploring the role and nature of interactions between Institutes in a
  Local Affiliation Network"
119,1047670439714201600,4639078397,John Wise,"['Grad student Yu Qiu led a study on radiative and kinetic (jet) feedback from AGN in clusters. We found that jet feedback must be present at all times (high &amp; low Mdot) to prevent the cooling catastrophe, and jets provide &gt;10% of the total AGN power. <LINK> <LINK>']",https://arxiv.org/abs/1810.01857,"Recent observations provide evidence that some cool-core clusters (CCCs) host quasars in their brightest cluster galaxies (BCGs). Motivated by these findings we use 3D radiation-hydrodynamic simulations with the code Enzo to explore the joint role of the kinetic and radiative feedback from supermassive black holes (SMBHs) in BCGs. We implement kinetic feedback as sub-relativistic plasma outflows and model radiative feedback using the ray-tracing radiative transfer or thermal energy injection. In our simulations the central SMBH transitions between the radiatively efficient and radiatively inefficient states on timescales of a few Gyr, as a function of its accretion rate. The timescale for this transition depends primarily on the fraction of power allocated to each feedback mode, and to a lesser degree on the overall feedback luminosity of the active galactic nucleus (AGN). Specifically, we find that (a) kinetic feedback must be present at both low and high accretion rates in order to prevent the cooling catastrophe, and (b) its contribution likely accounts for > 10% of the total AGN feedback power, since below this threshold simulated BCGs tend to host radio-loud quasars most of the time, in apparent contrast with observations. We also find a positive correlation between the AGN feedback power and the mass of the cold gas filaments in the cluster core, indicating that observations of H$\alpha$ filaments can be used as a measure of AGN feedback. ",The Interplay of Kinetic and Radiative Feedback in Galaxy Clusters
