,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1293491058941276160,950844713657094145,Maxwell Ramstead,"['Our new paper ""Is the free-energy principle a formal theory of semantics? From variational density dynamics to neural and phenotypic representations"" (<LINK>) was the subject of a great discussion on team_comm podcast\'s second episode.\nHighly recommend following! <LINK>']",https://arxiv.org/abs/2007.09291,"The aim of this paper is twofold: (1) to assess whether the construct of neural representations plays an explanatory role under the variational free-energy principle and its corollary process theory, active inference; and (2) if so, to assess which philosophical stance - in relation to the ontological and epistemological status of representations - is most appropriate. We focus on non-realist (deflationary and fictionalist-instrumentalist) approaches. We consider a deflationary account of mental representation, according to which the explanatorily relevant contents of neural representations are mathematical, rather than cognitive, and a fictionalist or instrumentalist account, according to which representations are scientifically useful fictions that serve explanatory (and other) aims. After reviewing the free-energy principle and active inference, we argue that the model of adaptive phenotypes under the free-energy principle can be used to furnish a formal semantics, enabling us to assign semantic content to specific phenotypic states (the internal states of a Markovian system that exists far from equilibrium). We propose a modified fictionalist account: an organism-centered fictionalism or instrumentalism. We argue that, under the free-energy principle, pursuing even a deflationary account of the content of neural representations licenses the appeal to the kind of semantic content involved in the aboutness or intentionality of cognitive systems; our position is thus coherent with, but rests on distinct assumptions from, the realist position. We argue that the free-energy principle thereby explains the aboutness or intentionality in living systems and hence their capacity to parse their sensory stream using an ontology or set of semantic factors. ","Is the free-energy principle a formal theory of semantics? From
  variational density dynamics to neural and phenotypic representations"
1,1293129396618956800,202211003,Alan Winfield 💙,"['New paper with @KatieJWinkle now on @arxiv RoboTed: a case study in Ethical Risk Assessment, accepted for #ICRES2020 <LINK>', '@djokeller_LP @KatieJWinkle @arxiv Thank you:)', '@jgarforth @KatieJWinkle @arxiv Thanks James:)', '@qeios @KatieJWinkle @arxiv Thank you!', ""@djokeller_LP @KatieJWinkle @arxiv The starting point was British Standard BS8611, but that doesn't have a complete taxonomy of ethical risks for social robots so there was additional brainstorming.""]",https://arxiv.org/abs/2007.15864,"Risk Assessment is a well known and powerful method for discovering and mitigating risks, and hence improving safety. Ethical Risk Assessment uses the same approach but extends the envelope of risk to cover ethical risks in addition to safety risks. In this paper we outline Ethical Risk Assessment (ERA) and set ERA within the broader framework of Responsible Robotics. We then illustrate ERA with a case study of a hypothetical smart robot toy teddy bear: RoboTed. The case study shows the value of ERA and how consideration of ethical risks can prompt design changes, resulting in a more ethical and sustainable robot. ",RoboTed: a case study in Ethical Risk Assessment
2,1292567361292324864,1003652696723873792,Max Gaspari,"['New paper on the spectacular capabilities of next-generation @AthenaXIFU to constrain models of chemical enrichments in #galaxy clusters, done in collaboration with Francois Mernier:\n<LINK>\n#IntraClusterMedium #astrophysics <LINK>']",https://arxiv.org/abs/2007.15910,"The chemical enrichment of the Universe at all scales is related to stellar winds and explosive supernovae phenomena. Metals produced by stars and later spread at the mega-parsec scale through the intra-cluster medium (ICM) become a fossil record of the chemical enrichment of the Universe and of the dynamical and feedback mechanisms determining their circulation. As demonstrated by the results of the soft X-ray spectrometer onboard Hitomi, high resolution X-ray spectroscopy is the path to to differentiate among the models that consider different metal production mechanisms, predict the outcoming yields, and are function of the nature, mass, and/or initial metallicity of their stellar progenitor. Transformational results shall be achieved through improvements in the energy resolution and effective area of X-ray observatories to detect rare metals (e.g. Na, Al) and constrain yet uncertain abundances (e.g. C, Ne, Ca, Ni). The X-ray Integral Field Unit (X-IFU) instrument onboard the next-generation European X-ray observatory Athena is expected to deliver such breakthroughs. Starting from 100 ks of synthetic observations of 12 abundance ratios in the ICM of four simulated clusters, we demonstrate that the X-IFU will be capable of recovering the input chemical enrichment models at both low ($z = 0.1$) and high ($z = 1$) redshifts, while statistically excluding more than 99.5% of all the other tested combinations of models. By fixing the enrichment models which provide the best fit to the simulated data, we also show that the X-IFU will constrain the slope of the stellar initial mass function within $\sim$12%. These constraints will be key ingredients in our understanding of the chemical enrichment of the Universe and its evolution. ","Constraining the origin and models of chemical enrichment in galaxy
  clusters using the Athena X-IFU"
3,1291813471256944641,23225413,Frederick Davies,"['New paper showed up on arxiv this week, <LINK>. High-z quasar proximity zones probe the intrinsic luminosity, and thus can constrain (strong) gravitational lensing!', '(nowhere near enough precision for cosmology, though. probably.)']",https://arxiv.org/abs/2007.15657,"Since their discovery twenty years ago, the observed luminosity function of $z\gtrsim6$ quasars has been suspected to be biased by gravitational lensing. Apart from the recent discovery of UHS J0439+1634 at $z\approx6.52$, no other strongly lensed $z\gtrsim6$ quasar has been conclusively identified. The hyperluminous $z\approx6.33$ quasar SDSS J0100+2802, believed to host a supermassive black hole of $\sim10^{10} M_\odot$, has recently been claimed to be lensed by a factor of $\sim450$, which would negate both its extreme luminosity and black hole mass. However, its Ly$\alpha$-transparent proximity zone is the largest known at $z>6$, suggesting an intrinsically extreme ionizing luminosity. Here we show that the lensing hypothesis of $z\gtrsim6$ quasars can be quantitatively constrained by their proximity zones. We first show that our proximity zone analysis can recover the strongly lensed nature of UHS J0439+1634, with an estimated magnification $\mu=28.0^{+18.4}_{-11.7}(^{+44.9}_{-18.3})$ at 68% (95%) credibility that is consistent with previously published lensing models. We then show that the large proximity zone of SDSS J0100+2802 rules out lensing magnifications of $\mu>4.9$ at 95% probability, and conclusively rule out the proposed $\mu>100$ scenario. Future proximity zone analyses of existing $z\gtrsim6$ quasar samples have the potential to identify promising strongly lensed candidates, constrain the distribution of $z\gtrsim6$ quasar lensing, and improve our knowledge of the shape of the intrinsic quasar luminosity function. ","Constraining the Gravitational Lensing of $z\gtrsim6$ Quasars from their
  Proximity Zones"
4,1291375557515632646,1220134326685306881,Zachary Fisher,['Interested in time-varying parameter models (including time-varying treatment effects)? Check out our new paper on estimating continuously time-varying parameters in nonlinear dynamic factor models. In press at MBR. <LINK>'],https://arxiv.org/abs/2007.09672,"Researchers collecting intensive longitudinal data (ILD) are increasingly looking to model psychological processes, such as emotional dynamics, that organize and adapt across time in complex and meaningful ways. This is also the case for researchers looking to characterize the impact of an intervention on individual behavior. To be useful, statistical models must be capable of characterizing these processes as complex, time-dependent phenomenon, otherwise only a fraction of the system dynamics will be recovered. In this paper we introduce a Square-Root Second-Order Extended Kalman Filtering approach for estimating smoothly time-varying parameters. This approach is capable of handling dynamic factor models where the relations between variables underlying the processes of interest change in a manner that may be difficult to specify in advance. We examine the performance of our approach in a Monte Carlo simulation and show the proposed algorithm accurately recovers the unobserved states in the case of a bivariate dynamic factor model with time-varying dynamics and treatment effects. Furthermore, we illustrate the utility of our approach in characterizing the time-varying effect of a meditation intervention on day-to-day emotional experiences. ","A Square-Root Second-Order Extended Kalman Filtering Approach for
  Estimating Smoothly Time-Varying Parameters"
5,1290681164320186368,1275184138664976384,Vittorio Ferrari,['📢 New paper 📢\n\nWe auto-complete user strokes in interactive full-image segmentation. We reduce annotation time by more than a factor of 2x compared to polygon drawing.\n\n<LINK> <LINK>'],https://arxiv.org/abs/2007.08173,"We propose a new approach to interactive full-image semantic segmentation which enables quickly collecting training data for new datasets with previously unseen semantic classes (A demo is available at this https URL). We leverage a key observation: propagation from labeled to unlabeled pixels does not necessarily require class-specific knowledge, but can be done purely based on appearance similarity within an image. We build on this observation and propose an approach capable of jointly propagating pixel labels from multiple classes without having explicit class-specific appearance models. To enable long-range propagation, our approach first globally measures appearance similarity between labeled and unlabeled pixels across the entire image. Then it locally integrates per-pixel measurements which improves the accuracy at boundaries and removes noisy label switches in homogeneous regions. We also design an efficient manual annotation interface that extends the traditional polygon drawing tools with a suite of additional convenient features (and add automatic propagation to it). Experiments with human annotators on the COCO Panoptic Challenge dataset show that the combination of our better manual interface and our novel automatic propagation mechanism leads to reducing annotation time by more than factor of 2x compared to polygon drawing. We also test our method on the ADE-20k and Fashionista datasets without making any dataset-specific adaptation nor retraining our model, demonstrating that it can generalize to new datasets and visual classes. ","Efficient Full Image Interactive Segmentation by Leveraging Within-image
  Appearance Similarity"
6,1290381191988879361,1022486891772502016,Issam Laradji,['We present a new weakly-supervised method that significantly reduces the cost of labeling of #COVID19 CT-Scans. It trains with a novel consistency loss on point annotations to identify infected regions accurately. \nCode: <LINK> \nPaper: <LINK> <LINK>'],https://arxiv.org/abs/2007.02180,"Coronavirus Disease 2019 (COVID-19) has spread aggressively across the world causing an existential health crisis. Thus, having a system that automatically detects COVID-19 in tomography (CT) images can assist in quantifying the severity of the illness. Unfortunately, labelling chest CT scans requires significant domain expertise, time, and effort. We address these labelling challenges by only requiring point annotations, a single pixel for each infected region on a CT image. This labeling scheme allows annotators to label a pixel in a likely infected region, only taking 1-3 seconds, as opposed to 10-15 seconds to segment a region. Conventionally, segmentation models train on point-level annotations using the cross-entropy loss function on these labels. However, these models often suffer from low precision. Thus, we propose a consistency-based (CB) loss function that encourages the output predictions to be consistent with spatial transformations of the input images. The experiments on 3 open-source COVID-19 datasets show that this loss function yields significant improvement over conventional point-level loss functions and almost matches the performance of models trained with full supervision with much less human effort. Code is available at: \url{this https URL}. ","A Weakly Supervised Consistency-based Learning Method for COVID-19
  Segmentation in CT Images"
7,1290321257272049666,966760075074461697,Brian Thomas,"['Could gamma-rays from dark matter cause a mass extinction on Earth??!!  Find out in this cool new paper I co-authored, on the arXiv!\n\n<LINK>\n#astrobiology #astrophysics']",https://arxiv.org/abs/2007.15975,"Recent studies of the effects on the Earth's atmosphere by astrophysical sources, such as nearby gamma-ray bursts or supernovae, have shown that these events could lead to severe changes in atmospheric composition. Depletion of ozone, the most notable of these changes, is extremely dangerous to living organisms as any decrease in ozone levels leads to an increase in the irradiance of harmful solar radiation at the Earth's surface. In this work we consider dark matter as an astrophysical source of gamma rays, by the annihilation and decay of WIMPs found within dark compact halo objects known as UltraCompact Minihaloes (UCMHs). We calculate the fluence of gamma rays produced in this way and simulate the resulting changes to terrestrial ozone levels using the Goddard Space Flight Center 2D Atmospheric Model. We also calculate the rate at which such events would occur, using estimates for the mass distribution of these haloes within the Milky Way. We find that the ozone depletion from UCMHs can be significant, and even of similar magnitude to the levels which have been linked to the cause of the Late-Ordovician mass extinction event. However, the probability of such encounters over the Earth's entire history is relatively low. This suggests that, while dark compact objects like UCMHs could have had an impact on the Earth's biosphere, other astrophysical phenomena like gamma-ray bursts or supernovae seem a more likely source of these effects. ","Gamma Rays from UltraCompact Minihaloes: Effects on the Earth's
  Atmosphere and Links to Mass Extinction Events"
8,1290286645992779776,1327377308,Milagros Miceli,"['📢📢Our new paper has been accepted for #CSCW2020!!! 📢📢\nThrough fieldwork at two companies, @martnsch, Tianling Yang, and I explored power imbalances in data annotation for computer vision products. \nA preprint is available here: <LINK> <LINK>', 'In this paper, we discuss the deeply normative nature of data classification and its effects on datasets:\n* What structures shape the sensemaking of data?\n* Who decides what labels best define an image? \n\ntl;dr\nHere is a short video featuring this research\nhttps://t.co/xxtRMqakom', 'Annotators interpret data according to imposed classifications that follow the demands of clients, their product and their revenue plan. \nPower imbalances related to the possession of capital not only translate into asymmetrical labor conditions but also concretely shape datasets', 'Finally, we layout implications for practitioners and researchers. We argue for power-reflexive documentation practices, as a method to restore context and improve transparency and accountability in dataset production.']",https://arxiv.org/abs/2007.14886,"The interpretation of data is fundamental to machine learning. This paper investigates practices of image data annotation as performed in industrial contexts. We define data annotation as a sense-making practice, where annotators assign meaning to data through the use of labels. Previous human-centered investigations have largely focused on annotators subjectivity as a major cause for biased labels. We propose a wider view on this issue: guided by constructivist grounded theory, we conducted several weeks of fieldwork at two annotation companies. We analyzed which structures, power relations, and naturalized impositions shape the interpretation of data. Our results show that the work of annotators is profoundly informed by the interests, values, and priorities of other actors above their station. Arbitrary classifications are vertically imposed on annotators, and through them, on data. This imposition is largely naturalized. Assigning meaning to data is often presented as a technical matter. This paper shows it is, in fact, an exercise of power with multiple implications for individuals and society. ","Between Subjectivity and Imposition: Power Dynamics in Data Annotation
  for Computer Vision"
9,1290285416529059841,1232964338,Shaikh saad,"['Here is our new paper: <LINK>', '@PedroANMachado Thanks!']",https://arxiv.org/abs/2007.16085,"The clockwork mechanism, which can naturally explain the origin of small numbers, is implemented in $SO(10)$ grand unified theories to address the origin of hierarchies in fermion masses and mixings. We show that a minimal Yukawa sector involving a $10_H$ and $\overline{126}_H$ of Higgs bosons, extended with two clockwork chains consisting of $16+\overline{16}$ vector-like fermions, can explain the hierarchical patterns with all the Yukawa couplings being of order one. Emergence of a realistic mass spectrum does not require any symmetry that distinguishes the three generations. We develop clockwork-extended $SO(10)$ GUTs both in the context of SUSY and non-SUSY frameworks. Implementation of the mechanism in non-SUSY scenario assumes a Peccei-Quinn symmetry realized at an intermediate scale, with the clockwork sector carrying non-trivial charges, which solves the strong CP problem and provides axion as a dark matter candidate. ",Flavor Hierarchies from Clockwork in SO(10) GUT
10,1290276576597176320,58326841,Daniel J. Bernstein,"['New paper ""BasicBlocker: Redesigning ISAs to Eliminate Speculative-Execution Attacks"" online from @sec_janthoma, Jakob Feldtkeller, Markus Krausz, Tim Güneysu, and me: <LINK> Speculative execution is much less important for performance than commonly believed.']",https://arxiv.org/abs/2007.15919,"Recent research has revealed an ever-growing class of microarchitectural attacks that exploit speculative execution, a standard feature in modern processors. Proposed and deployed countermeasures involve a variety of compiler updates, firmware updates, and hardware updates. None of the deployed countermeasures have convincing security arguments, and many of them have already been broken. The obvious way to simplify the analysis of speculative-execution attacks is to eliminate speculative execution. This is normally dismissed as being unacceptably expensive, but the underlying cost analyses consider only software written for current instruction-set architectures, so they do not rule out the possibility of a new instruction-set architecture providing acceptable performance without speculative execution. A new ISA requires compiler and hardware updates, but these are happening in any case. This paper introduces BasicBlocker, a generic ISA modification that works for all common ISAs and that allows non-speculative CPUs to obtain most of the performance benefit that would have been provided by speculative execution. To demonstrate the feasibility of BasicBlocker, this paper defines a variant of the RISC-V ISA called BBRISC-V and provides a thorough evaluation on both a 5-stage in-order soft core and a superscalar out-of-order processor using an associated compiler and a variety of benchmark programs. ",BasicBlocker: ISA Redesign to Make Spectre-Immune CPUs Faster
11,1290269277140836360,1009799569390096384,Brenden Lake,"['We train large-scale neural nets ""through the eyes"" of one baby across 2 years of development. New paper from Emin Orhan shows how high-level visual representations emerge from a subset of one baby\'s experience, through only self-supervised learning. <LINK> (1/2)', 'This work was possible because of the *amazing* SAYCam dataset of baby headcam videos. Please check out the SAYCam paper here from Jess Sullivan, Michelle Mei, @AmyPerfors @ewojcik @mcxfrank https://t.co/I574XmUfF7 (2/2)', ""@Werdnamai Great ideas. Yes for 1) and 2). For 3), that's a lot harder, because the videos are what they are! Action is a major factor missing from this kind of work.""]",http://arxiv.org/abs/2007.16189,"Within months of birth, children develop meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly defined domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of three young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic self-supervised learning objectives. ",Self-supervised learning through the eyes of a child
12,1290222035029962752,106370162,Frédéric Grosshans,"['1/n New paper, with @oliversium, Ulysse Chabaud and André Chailloux on “Breaking simple quantum position verification protocols with little entanglement”\narXiv:2007.15808 <LINK>\n\nWe attack BB84-like quantum position verification protocols with entangled qudits', '@oliversium 2/n This problem is impossible in the LO model and easy the LOCC cas, but this relativistic cryptographic protocol is in the intermdiate LOBC (LO and broadcast communication) casew where attacks need Instantaneous Nonlocal Quantum Computing (INQC)', '@oliversium 3/n The question is how to a photon polarization in a basis either 0 or θ given that Alice has the photon, Bob the angle, and they’re allowed a single round of communication', '@oliversium 4/n For θ=π/4 (the Clifford case / BB84 angle), the solution has been known since 2010 and a work by @Adrian_Kent, Munro, Spiller who describe a teleportation based attack using 1 ebit/qubit in arXiv:1008.2147 https://t.co/mIGaxGpCrA / PRA 84 012326 https://t.co/R8G5H46cH7', '@oliversium @Adrian_Kent 5/n For angles in the nth level of the Clifford hierarchy (θ=π/2ⁿ), exact attacks have been found by Chakraborty and @letonyo arXiv:1507.00626 https://t.co/JidJEz1mzL , and Gonzales and Chitambar arXiv:1810.00994 https://t.co/qt6uFRPKTk', '@oliversium @Adrian_Kent @letonyo 5/n Here we find new exact attacks using e-dits of dimension d≤12, for angles multiple of π/k with k ∈{8, 6, 12, 16, 20, 24}. It includes angles outside the Clifford hierarchy, and for the one which are in the Clifford hierarchy, cost is smaller than previously known attacks', '@oliversium @Adrian_Kent @letonyo 7/n, n=7 \nFor example, we have an attack for π/8 using 2 ebits instead of 27 ebits.\n\nIn term of approximate attacks, we show that 2 ebits/qubit are enough to induce an error rate lower than ≃5⋅10⁻³']",https://arxiv.org/abs/2007.15808,"Instantaneous nonlocal quantum computation (INQC) evades apparent quantum and relativistic constraints and allows to attack generic quantum position verification (QPV) protocols (aiming at securely certifying the location of a distant prover) at an exponential entanglement cost. We consider adversaries sharing maximally entangled pairs of qudits and find low-dimensional INQC attacks against the simple practical family of QPV protocols based on single photons polarized at an angle $\theta$. We find exact attacks against some rational angles, including some sitting outside of the Clifford hierarchy (e.g. $\pi/6$), and show no $\theta$ allows to tolerate errors higher than $\simeq 5\cdot 10^{-3}$ against adversaries holding two ebits per protocol's qubit. ","Breaking simple quantum position verification protocols with little
  entanglement"
13,1290192259305144321,481539448,Richard Alexander,"[""New paper, led by @PhysicsUoL's @cassidentprone (with Ruobing Dong, Richard Teague, @terepaneque, @giulod &amp; others). Cass shows that gravitational instability in planet-forming discs has a unique signature that we should be able to detect with @almaobs. \n\n<LINK>"", '@cassidentprone\'s new simulations show that the spiral waves induced by GI are different to those caused by planets or binary companions, and can be identified through a characteristic ""wiggle"" in the velocity field which is readily detectable in ALMA channel maps.', 'The key result can be seen in this figure (channel maps for the 13CO J=3-2 line).  The GI disc (top) shows ""wiggles"" in the channel maps (circled). If we see that feature in real ALMA observations, it will confirm that the the discs are gravitationally unstable. https://t.co/DjZ96MKqtv']",https://arxiv.org/abs/2007.15686,"Observations with the Atacama Large Millimeter/Submillimeter array (ALMA) have dramatically improved our understanding of the site of exoplanet formation: protoplanetary discs. However, many basic properties of these discs are not well-understood. The most fundamental of these is the total disc mass, which sets the mass budget for planet formation. Discs with sufficiently high masses can excite gravitational instability and drive spiral arms that are detectable with ALMA . Although spirals have been detected in ALMA observations of the dust , their association with gravitational instability, and high disc masses, is far from clear. Here we report a prediction for kinematic evidence of gravitational instability. Using hydrodynamics simulations coupled with radiative transfer calculations, we show that a disc undergoing such instability has clear kinematic signatures in molecular line observations across the entire disc azimuth and radius which are independent of viewing angle. If these signatures are detected, it will provide the clearest evidence for the occurrence of gravitational instability in planet-forming discs, and provide a crucial way to measure disc masses. ",Predicting the kinematic evidence of gravitational instability
14,1290191788045557762,915574287989342209,Roberto Oliveri,"['Here is my last preprint. Check it out!\n\nWe propose a new approach to find magnetically-dominated force-free #magnetospheres around highly spinning #blackholes, relevant for models of #astrophysical #jets.\n\n<LINK>\n(companion paper: <LINK>) <LINK>']",https://arxiv.org/abs/2007.15662,"We propose a new approach to find magnetically-dominated force-free magnetospheres around highly spinning black holes, relevant for models of astrophysical jets. Employing the near-horizon extreme Kerr (NHEK) limit of the Kerr black hole, any stationary, axisymmetric and regular force-free magnetosphere reduces to the same attractor solution in the NHEK limit with null electromagnetic field strength. We use this attractor solution as the universal starting point for perturbing away from the NHEK region in the extreme Kerr spacetime. We demonstrate that by going to second order in perturbation theory, it is possible to find magnetically dominated magnetospheres around the extreme Kerr black hole. Furthermore, we consider the near-horizon near-extreme Kerr (near-NHEK) limit that provides access to a different regime of highly spinning black holes. Also in this case we find a novel force-free attractor, which can be used as the universal starting point for a perturbative construction of force-free magnetospheres. Finally, we discuss the relation between the NHEK and near-NHEK attractors. ","Force-free magnetosphere attractors for near-horizon extreme and
  near-extreme limits of Kerr black hole"
15,1290089700716314626,77815209,Brian Skinner,"['1/ ""Superspreading"" is when a small minority of infectious people are responsible for the great majority of new infections. And let me tell you, there is more than a little evidence for it in the COVID statistics in the USA.\n\nNew paper from our group:\n<LINK>', '2/ The key idea of our analysis is that, when the total number of infections is still not-too-large, the growth rate of an epidemic is a random variable. If the epidemic is being driven by a relatively small minority of individuals, then it becomes especially random.', '3/ This means that information about the distribution of individual infectiousnes -- whether everyone is contributing more-or-less equally, or whether most infections come from a highly infectious few -- is encoded in the distribution of early time exponential growth rates.', '4/ This conceptual link enabled us to infer information about the statistical distribution of individual infectiousness by looking at the variance between different geographic locations in early-time epidemic growth rate. https://t.co/pONizg6Ode', '5/ The result: infectiousness follows a very fat-tailed distribution. \nIt looks like 88% of new cases are caused by just ~10% of infectious individuals (!) https://t.co/iOPSdc1f3Q', '6/ Now, you have probably already thought of an objection to this approach, or a potential confounder. So let me just say that we were careful to consider and rule out any of the following factors as the driver of our result:\n- intrinsic geographic variation\n- limited testing\n...', '7/\n- geographic variation in testing\n- interactions between different counties\n- variations in individual disease latency\n\nNone of them seems to be driving our results.\n(Note the copious appendices in the paper where these are all considered).', '8/8 But most of all, congratulations to my student Calvin Pozderac on his first academic paper!  He was really the intellectual driver of this project, and I mostly played the role of his muse.', '(9/8)\nThis is certainly not what Calvin envisioned his first paper would be like, but strange times lead to strange research I guess.', '@MattRGoldman What you can do is check that every location settles into more or less the same growth rate when there is a larger number of cases -- which is true (more or less).\n\nAlso you can check that the county-to-county variance goes as 1/N, which it does.', ""@MattRGoldman For example, look at the inset of Fig. 1b, which shows the county-to-county variance as a function of N (error bars). If different counties had different intrinsic growth rates, then the variance would remain large at large N. But it doesn't."", '@MattRGoldman I have to say, if you ever happen to read this paper with a skeptical eye, I would be very glad to hear whether it stands up to your scrutiny!', '@golwengaud We actually did worry about this, and checked for it.  If the variance is not well-defined, then you should get a different if you take a different random subset of the data. We got a consistent answer for sigma regardless of the subset, which makes me think sigma is well defined', '@golwengaud I guess we should have added a comment about this', '@MattRGoldman Thanks! Yeah, we were worried about that time-dependency, so we intentionally restricted ourselves to only the first two weeks', '@AIsakovic1 thanks!', '@SturnioloSimone @MattRGoldman yeah, it was comforting to see that our result was pretty much in line with much more difficult test+trace studies', '@alexabelson9 good question!']",https://arxiv.org/abs/2007.15673,"A number of epidemics, including the SARS-CoV-1 epidemic of 2002-2004, have been known to exhibit superspreading, in which a small fraction of infected individuals is responsible for the majority of new infections. The existence of superspreading implies a fat-tailed distribution of infectiousness (new secondary infections caused per day) among different individuals. Here, we present a simple method to estimate the variation in infectiousness by examining the variation in early-time growth rates of new cases among different subpopulations. We use this method to estimate the mean and variance in the infectiousness, $\beta$, for SARS-CoV-2 transmission during the early stages of the pandemic within the United States. We find that $\sigma_\beta/\mu_\beta \gtrsim 3.2$, where $\mu_\beta$ is the mean infectiousness and $\sigma_\beta$ its standard deviation, which implies pervasive superspreading. This result allows us to estimate that in the early stages of the pandemic in the USA, over 81% of new cases were a result of the top 10% of most infectious individuals. ",Superspreading of SARS-CoV-2 in the USA
16,1290085946101178368,1196266674954985472,Nirmal Raj,"['New paper with @GrahamKribs  and @davemckeen on hunting new forces in Nature by breaking up protons! <LINK> Thread follows. <LINK>', 'Everyone knows the Russian doll-like assembly of matter: atom &gt; nucleus &gt; protons &amp; neutrons &gt; quarks &amp; gluons. This knowledge was gained over the last century using special microscopy: smashing these objects hard with a projectile particle. https://t.co/7f6Hs96SYI', ""Per Heisenberg's uncertainty principle, transfer more momentum to the target and you resolve it better. The world within protons is still probed this way by shattering them with electrons in particle colliders. https://t.co/gHDXJphkqb"", 'The projectile electron strikes the target quark via the electromagnetic force carried by the photon;  similar strikes occur at smaller distance scales (i.e. ""deep inside"" the proton) via the weak force carried by the Z boson. https://t.co/Ye24ZP0ebQ', ""Four fundamental forces seem to govern all natural phenomena: gravity, electromagnetism, the weak force, and the strong. Is there a fifth force?  There are compelling reasons for one to exist -- in any case it'd be good to look in settings where it might show up."", 'In a minimal theoretical setup, the electrons and quarks in the proton-breaking experiment should feel this new force in addition to electromagnetism and the weak force. https://t.co/mFpLI1ANwm', 'That\'d alter the results of these experiments. We exploit that to place generic constraints on a new force-carrier often called the ""dark photon"". We show that the latest proton-breaking collider HERA and the future LHeC could glimpse dark photons of up to 5-100 TeV masses! https://t.co/HMQvxYZFcA']",https://arxiv.org/abs/2007.15655,"Deep inelastic scattering of $e^{\pm}$ off protons is sensitive to contributions from ""dark photon"" exchange. Using HERA data fit to HERA's parton distribution functions, we obtain the model-independent bound $\epsilon \lesssim 0.02$ on the kinetic mixing between hypercharge and the dark photon for dark photon masses $\lesssim 10$ GeV. This slightly improves on the bound obtained from electroweak precision observables. For higher masses the limit weakens monotonically; $\epsilon \lesssim 1$ for a dark photon mass of $5$ TeV. Utilizing PDF sum rules, we demonstrate that the effects of the dark photon cannot be (trivially) absorbed into re-fit PDFs, and in fact lead to non-DGLAP (Bjorken $x_{\rm B}$-independent) scaling violations that could provide a smoking gun in data. The proposed $e^\pm p$ collider operating at $\sqrt{s} = 1.3$ TeV, LHeC, is anticipated to accumulate $10^3$ times the luminosity of HERA, providing substantial improvements in probing the effects of a dark photon: sensitivity to $\epsilon$ well below that probed by electroweak precision data is possible throughout virtually the entire dark photon mass range, as well as being able to probe to much higher dark photon masses, up to $100$ TeV. ",Breaking up the Proton: An Affair with Dark Forces
17,1290081013876420608,2958183660,Dr. Kathryn Neugent,"[""New Paper Alert! Let's talk about the binary fraction of red supergiants ... a thread!\n\n<LINK> <LINK>"", ""For my PhD research at @uwastronomy with @emsque, I found that red supergiants (RSGs) primarily have B-type companions. I went searching for these systems in the Andromeda Galaxy, the Triangulum Galaxy and the Large and Small Magellanic Clouds. So far, we've found over 250!"", 'Next I focused on determining the binary *fraction* of RSGs in the Large Magellanic Cloud. Un-evolved OB stars  *love* to be in binary systems, but do RSGs? https://t.co/MIfdZhRIPB', 'Using a sample set of spectroscopically confirmed binary and single RSGs and their photometric colors, I used a K-Nearest Neighbor algorithm to classify RSGs in the Large Magellanic Cloud (thanks @trevorzaylen for the suggestion!). https://t.co/3Ym8O5ERDs', ""Here's the resulting color-color plot! The redder the point, the more likely the star is to be a single RSG. The bluer the point, the more likely it is to be a binary RSG! https://t.co/knQDW5Vo2y"", 'Accounting for our observational biases (such as RSGs *not* in systems with B-type stars), we find that the binary fraction of RSGs in the Large Magellanic Cloud is only around 20%! https://t.co/OhWI0j6Cvn', 'This percentage is much lower than for the un-evolved OB stars ... but we think this is because some of the RSGs will actually *merge* with their companion and appear single! (see paper for more discussion ... and more research to come!)', ""Next up I'm going to look at the binary fraction in M31 and M33 and see if there's any change with metallicity! Stay tuned! And many thanks to @emsque, @MassiveStarGuy, @mrdrout and Nidia Morrell for being great collaborators! https://t.co/uNqaVhasns"", 'Remember, as @dalcantonJD always says, #StarsAreStillInteresting!!! \n\nhttps://t.co/eOIyqs1W7G\n\n&lt;/thread&gt;']",https://arxiv.org/abs/2007.15852,"The binary fraction of unevolved massive stars is thought to be 70-100% but there are few observational constraints on the binary fraction of the evolved version of a subset of these stars, the red supergiants (RSGs). Here we identify a complete sample of RSGs in the Large Magellanic Cloud (LMC) using new spectroscopic observations and archival UV, IR and broadband optical photometry. We find 4090 RSGs with log L/Lo > 3.5 with 1820 of them having log L/Lo > 4, which we believe is our completeness limit. We additionally spectroscopically confirmed 38 new RSG+B star binaries in the LMC, bringing the total known up to 55. We then estimated the binary fraction using a k-nearest neighbors algorithm that classifies stars as single or binary based on photometry with a spectroscopic sample as a training set. We take into account observational biases such as line-of-sight stars and binaries in eclipse while also calculating model-dependent corrections for RSGs with companions that our observations were not designed to detect. Based on our data, we find an initial result of 13.5 +7.56/-6.67% for RSGs with O or B-type companions. Using the Binary Population and Spectral Synthesis (BPASS) models to correct for unobserved systems, this corresponds to a total RSG binary fraction of 19.5 +7.6/-6.7%. This number is in broad agreement with what we would expect given an initial OB binary distribution of 70%, a predicted merger fraction of 20-30% and a binary interaction fraction of 40-50%. ",The Red Supergiant Binary Fraction of the Large Magellanic Cloud
18,1290012137906110466,2176486874,Steven Thomson,"[""Really nice discovery today - while reading this (excellent) new paper from the @ITensorLib team, it was a fantastic surprise to stumble on our group's PhD student Jan Schneider in the acknowledgements. Lovely gesture from the team to all the contributors! <LINK> <LINK>""]",https://arxiv.org/abs/2007.14822,"ITensor is a system for programming tensor network calculations with an interface modeled on tensor diagram notation, which allows users to focus on the connectivity of a tensor network without manually bookkeeping tensor indices. The ITensor interface rules out common programming errors and enables rapid prototyping of tensor network algorithms. After discussing the philosophy behind the ITensor approach, we show examples of each part of the interface including Index objects, the ITensor product operator, tensor factorizations, tensor storage types, algorithms for matrix product state (MPS) and matrix product operator (MPO) tensor networks, quantum number conserving block-sparse tensors, and the NDTensors library. We also review publications that have used ITensor for quantum many-body physics and for other areas where tensor networks are increasingly applied. To conclude we discuss promising features and optimizations to be added in the future. ",The ITensor Software Library for Tensor Network Calculations
19,1289260137249554433,979379437069271043,Pedro Machado,"['New paper today on intranuclear cascades with awesome collabs: Isaacson, Jay, Lovato and Rocco.\nThe goal is to simulate the propagation of hadrons in nuclear media, which is relevant to current and future oscillation experiments. Results are promising! :-D\n<LINK>']",https://arxiv.org/abs/2007.15570,"We propose a novel approach to intranuclear cascades which takes as input quantum MonteCarlo nuclear configurations and uses a semi-classical, impact-parameter based algorithm to modelthe propagation of protons and neutrons in the nuclear medium. We successfully compare oursimulations to available proton-carbon scattering data and nuclear-transparency measurements. Byanalyzing the dependence of the simulated observables upon the ingredients entering our intranuclearcascade algorithm, we provide a quantitative understanding of their impact. Particular emphasisis devoted to the role played by nuclear correlations, the Pauli exclusion principle, and interactionprobability distributions. ",A quantum Monte Carlo based approach to intranuclear cascades
20,1289184107260334080,839104540985151490,IPPP Durham,"['New IPPP paper: ""Higgs decay to fermion pairs at NLO in SMEFT"" by Jonathan M. Cullen et al. <LINK>']",http://arxiv.org/abs/2007.15238,"The calculation of next-to-leading order (NLO) perturbative corrections at fixed operator dimension in Standard Model Effective Field Theory (SMEFT) has been a topic of much recent interest. In this paper we obtain the NLO corrections from dimension-6 operators to the Higgs boson decays $h\to f\bar{f}$, where the fermions $f \in \{\mu,\tau,c\}$. This extends previous results for $h\to b\bar{b}$ to all phenomenologically relevant Higgs boson decays into fermions, and provides the basis for future precision analyses of these decays within effective field theory. We point out the benefits of studying ratios of decay rates into different fermions in SMEFT, the most surprising of which is enhanced sensitivity to anomalous $h\gamma\gamma$ and $hgg$ couplings induced by flavor-universal SMEFT operators, especially in scenarios where flavor-dependent Wilson coefficients are constrained by Minimal Flavor Violation. ",Higgs decay to fermion pairs at NLO in SMEFT
21,1289180109136568320,1135282358528012288,Vera von Burg,['“Catalysis 2.0” leveraging quantum computing? Check out our new paper in collaboration with @MSFTResearch to find out how! <LINK>'],https://arxiv.org/abs/2007.14460,"The quantum computation of electronic energies can break the curse of dimensionality that plagues many-particle quantum mechanics. It is for this reason that a universal quantum computer has the potential to fundamentally change computational chemistry and materials science, areas in which strong electron correlations present severe hurdles for traditional electronic structure methods. Here, we present a state-of-the-art analysis of accurate energy measurements on a quantum computer for computational catalysis, using improved quantum algorithms with more than an order of magnitude improvement over the best previous algorithms. As a prototypical example of local catalytic chemical reactivity we consider the case of a ruthenium catalyst that can bind, activate, and transform carbon dioxide to the high-value chemical methanol. We aim at accurate resource estimates for the quantum computing steps required for assessing the electronic energy of key intermediates and transition states of its catalytic cycle. In particular, we present new quantum algorithms for double-factorized representations of the four-index integrals that can significantly reduce the computational cost over previous algorithms, and we discuss the challenges of increasing active space sizes to accurately deal with dynamical correlations. We address the requirements for future quantum hardware in order to make a universal quantum computer a successful and reliable tool for quantum computing enhanced computational materials science and chemistry, and identify open questions for further research. ",Quantum computing enhanced computational catalysis
22,1289115848268034048,3389488325,Paul Tardy,['*New paper* \nIn order to take advantage of the massive amount of unaligned target side data we use (i) self-supervised pre-training on target side and (ii) back-summarization to generate synthetic source for each unaligned target.\n\n<LINK>'],https://arxiv.org/abs/2007.15296,"Supervised approaches for Neural Abstractive Summarization require large annotated corpora that are costly to build. We present a French meeting summarization task where reports are predicted based on the automatic transcription of the meeting audio recordings. In order to build a corpus for this task, it is necessary to obtain the (automatic or manual) transcription of each meeting, and then to segment and align it with the corresponding manual report to produce training examples suitable for training. On the other hand, we have access to a very large amount of unaligned data, in particular reports without corresponding transcription. Reports are professionally written and well formatted making pre-processing straightforward. In this context, we study how to take advantage of this massive amount of unaligned data using two approaches (i) self-supervised pre-training using a target-side denoising encoder-decoder model; (ii) back-summarization i.e. reversing the summarization process by learning to predict the transcription given the report, in order to align single reports with generated transcription, and use this synthetic dataset for further training. We report large improvements compared to the previous baseline (trained on aligned data only) for both approaches on two evaluation sets. Moreover, combining the two gives even better results, outperforming the baseline by a large margin of +6 ROUGE-1 and ROUGE-L and +5 ROUGE-2 on two evaluation sets ","Leverage Unlabeled Data for Abstractive Speech Summarization with
  Self-Supervised Learning and Back-Summarization"
23,1289000531726225408,79272029,Inês Hipólito,"['Happy to share our new paper already available as a preprint. Soon out in Physics of Life Reviews - with @mjdramstead @ConstantAxel. <LINK>', 'Wright and Bourkes (2020) compelling article rightly points out that existing models of embryogenesis fail to explain the mechanisms and functional significance of the dynamic connections among neurons. We pursue their account of Dynamic Logic by appealing to the Markov blanket', 'formalism that underwrites the Free Energy Principle. We submit that this allows one to model embryogenesis as self-organisation in a dynamical system that minimises free-energy. The ensuing formalism may be extended to also explain the autonomous emergence of cognition,', 'specifically in the brain, as a dynamic self-assembling process.']",https://arxiv.org/abs/2007.15205?fbclid=IwAR3mMJfiPPy786IEmFlSA-EkIoqpK1BxFsADLcC6_LlIbE9PJinRcn4b5QU,"Wright and Bourkes compelling article rightly points out that existing models of embryogenesis fail to explain the mechanisms and functional significance of the dynamic connections among neurons. We pursue their account of Dynamic Logic by appealing to the Markov blanket formalism that underwrites the Free Energy Principle. We submit that this allows one to model embryogenesis as self-organisation in a dynamical system that minimises free-energy. The ensuing formalism may be extended to also explain the autonomous emergence of cognition, specifically in the brain, as a dynamic self-assembling process. ",Cognition coming about: self-organisation and free-energy
24,1288998001369681921,12427292,Anita Ponsaing,['New paper. <LINK>'],https://arxiv.org/abs/2007.14597,"We study the distribution of the largest eigenvalue in the ""Pfaffian"" classical ensembles of random matrix theory, namely in the Gaussian orthogonal (GOE) and Gaussian symplectic (GSE) ensembles, using semi-classical skew-orthogonal polynomials, in analogue to the approach of Nadal and Majumdar (NM) for the Gaussian unitary ensemble (GUE). Generalizing the techniques of Adler, Forrester, Nagao and van Moerbeke, and using ""overlapping Pfaffian"" identities due to Knuth, we explicitly construct these semi-classical skew-orthogonal polynomials in terms of the semi-classical orthogonal polynomials studied by NM in the case of the GUE. With these polynomials we obtain expressions for the cumulative distribution functions of the largest eigenvalue in the GOE and the GSE. Further, by performing asymptotic analysis of these skew-orthogonal polynomials in the limit of large matrix size, we obtain an alternative derivation of the Tracy-Widom distributions for GOE and GSE. This asymptotic analysis relies on a certain Pfaffian identity, the proof of which employs the characterization of Pfaffians in terms of perfect matchings and link diagrams. ","Tracy-Widom distributions for the Gaussian orthogonal and symplectic
  ensembles revisited: a skew-orthogonal polynomials approach"
25,1288833900991729666,1284439222187892736,Enrico Fontana,"['First paper out!\n\nWith the wonderful people at @LosAlamosNatLab Quantum Computing Summer School we just released a paper on a new phenomenon that we called Noise Induced Barren Plateaus.\n\nThis has implications for Quantum Neural Networks on NISQ devices.\n\n<LINK>', 'Technical summary: for a model of Pauli noise, with sufficient noise both the cost function and its gradient in any direction decay exponentially in depth. This suggests that deep noisy QNNs are untrainable.\n\nCompared to noiseless BPs, this is not a statistical phenomenon!', 'Understandable summary: under certain conditions, the noise of current quantum computer builds up extremely quickly as computations become more complicated. \n\nThis means that we must make better quantum computers or risk ending up with garbage results.', 'Shoutout to the first author @samson_wang for the truly impressive work, and to the magic duo @kunal_phy and @MvsCerezo for the invaluable effort and guidance.\n\nThanks to @SoneAkira @LCincio for the contributions and to the big boss @ColesQuantum for having made QCSS possible.', ""Check out @MvsCerezo 's thread for an excellent memesplanation of the results! 👇\n\nhttps://t.co/zOzhTZy7Zw""]",https://arxiv.org/abs/2007.14384,"Variational Quantum Algorithms (VQAs) may be a path to quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) computers. A natural question is whether noise on NISQ devices places fundamental limitations on VQA performance. We rigorously prove a serious limitation for noisy VQAs, in that the noise causes the training landscape to have a barren plateau (i.e., vanishing gradient). Specifically, for the local Pauli noise considered, we prove that the gradient vanishes exponentially in the number of qubits $n$ if the depth of the ansatz grows linearly with $n$. These noise-induced barren plateaus (NIBPs) are conceptually different from noise-free barren plateaus, which are linked to random parameter initialization. Our result is formulated for a generic ansatz that includes as special cases the Quantum Alternating Operator Ansatz and the Unitary Coupled Cluster Ansatz, among others. For the former, our numerical heuristics demonstrate the NIBP phenomenon for a realistic hardware noise model. ",Noise-Induced Barren Plateaus in Variational Quantum Algorithms
26,1288833841478750208,752184524121993216,Menelaos Kanakis,"['Our paper ""Reparameterizing Convolutions for Incremental Multi-Task Learning without Task Interference"" has been accepted to #ECCV2020. We reparameterize the convs to eliminate task interference and allow for the incremental learning of new tasks.\nPaper: <LINK>']",https://arxiv.org/abs/2007.12540,"Multi-task networks are commonly utilized to alleviate the need for a large number of highly specialized single-task networks. However, two common challenges in developing multi-task models are often overlooked in literature. First, enabling the model to be inherently incremental, continuously incorporating information from new tasks without forgetting the previously learned ones (incremental learning). Second, eliminating adverse interactions amongst tasks, which has been shown to significantly degrade the single-task performance in a multi-task setup (task interference). In this paper, we show that both can be achieved simply by reparameterizing the convolutions of standard neural network architectures into a non-trainable shared part (filter bank) and task-specific parts (modulators), where each modulator has a fraction of the filter bank parameters. Thus, our reparameterization enables the model to learn new tasks without adversely affecting the performance of existing ones. The results of our ablation study attest the efficacy of the proposed reparameterization. Moreover, our method achieves state-of-the-art on two challenging multi-task learning benchmarks, PASCAL-Context and NYUD, and also demonstrates superior incremental learning capability as compared to its close competitors. ","Reparameterizing Convolutions for Incremental Multi-Task Learning
  without Task Interference"
27,1288802521335685120,1285216628884602881,Santi Ávila,"['New awesome paper by @MSWangCosmos, congratulations!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2007.14962,"Future precision cosmology from large-scale structure experiments including the Dark Energy Spectroscopic Instrument (DESI) and Euclid will probe wider and deeper cosmic volumes than those covered by previous surveys. The Cartesian power spectrum analysis of anisotropic galaxy clustering based on the Fourier plane wave basis makes a number of assumptions, including the local plane-parallel approximation, that will no longer be valid on very large scales and may degrade cosmological constraints. We propose an approach that utilises a hybrid basis: on the largest scales, clustering statistics are decomposed into spherical Fourier modes which respect the natural geometry of both survey observations and physical effects along the line of sight, such as redshift-space distortions, the Alcock--Paczy\'nsky and light-cone effects; on smaller scales with far more clustering modes, we retain the computational benefit of the power spectrum analysis aided by fast Fourier transforms. This approach is particularly suited to the likelihood analysis of local primordial non-Gaussianity $f_\textrm{NL}$ through the scale-dependent halo bias, and we demonstrate its applicability with $N$-body simulations. We also release our public code Harmonia (this https URL) for galaxy clustering likelihood inference in spherical Fourier or hybrid-basis analyses. ","Hybrid-basis inference for large-scale galaxy clustering: combining
  spherical and Cartesian Fourier analyses"
28,1288795332999086080,1439446945,Lav Varshney,"['New manuscript on active sampling for generating high-quality text from language models which may work better than top-k and nucleus sampling @SFResearch @ECEILLINOIS @CSL_Illinois \n\nblog: <LINK>\npaper: <LINK>\ncode: <LINK> <LINK>', 'Nice work from @sourya_basu, Sachin, and @StrongDuality']",https://arxiv.org/abs/2007.14966,"Neural text decoding is important for generating high-quality texts using language models. To generate high-quality text, popular decoding algorithms like top-k, top-p (nucleus), and temperature-based sampling truncate or distort the unreliable low probability tail of the language model. Though these methods generate high-quality text after parameter tuning, they are ad hoc. Not much is known about the control they provide over the statistics of the output, which is important since recent reports show text quality is highest for a specific range of likelihoods. Here, first we provide a theoretical analysis of perplexity in top-k, top-p, and temperature sampling, finding that cross-entropy behaves approximately linearly as a function of p in top-p sampling whereas it is a nonlinear function of k in top-k sampling, under Zipfian statistics. We use this analysis to design a feedback-based adaptive top-k text decoding algorithm called mirostat that generates text (of any length) with a predetermined value of perplexity, and thereby high-quality text without any tuning. Experiments show that for low values of k and p in top-k and top-p sampling, perplexity drops significantly with generated text length, which is also correlated with excessive repetitions in the text (the boredom trap). On the other hand, for large values of k and p, we find that perplexity increases with generated text length, which is correlated with incoherence in the text (confusion trap). Mirostat avoids both traps: experiments show that cross-entropy has a near-linear relation with repetition in generated text. This relation is almost independent of the sampling method but slightly dependent on the model used. Hence, for a given language model, control over perplexity also gives control over repetitions. Experiments with human raters for fluency, coherence, and quality further verify our findings. ","Mirostat: A Neural Text Decoding Algorithm that Directly Controls
  Perplexity"
29,1288762452998590465,27699571,Alessandro Gasparini,"['New preprint out on #arXiv!\nThe {merlin} (🧙\u200d♂️) #rstats package now has an accompanying paper: <LINK>\nLots of hard work lead by @Emma_C_Martin and @Crowther_MJ, and I am glad I could contribute *a little* to its development. 1/2\n\n#statstwitter #epitwitter <LINK>', ""If you didn't try {merlin} before, now's the time: the paper includes several examples of increasing complexity, showcasing some of the magic of {merlin} for simple and complex statistical analyses. As always, feedback is welcome — ping us back with your comments! 2/2""]",https://arxiv.org/abs/2007.14109,"The R package merlin performs flexible joint modelling of hierarchical multi-outcome data. Increasingly, multiple longitudinal biomarker measurements, possibly censored time-to-event outcomes and baseline characteristics are available. However, there is limited software that allows all of this information to be incorporated into one model. In this paper, we present merlin which allows for the estimation of models with unlimited numbers of continuous, binary, count and time-to-event outcomes, with unlimited levels of nested random effects. A wide variety of link functions, including the expected value, the gradient and shared random effects, are available in order to link the different outcomes in a biologically plausible way. The accompanying predict.merlin function allows for individual and population level predictions to be made from even the most complex models. There is the option to specify user-defined families, making merlin ideal for methodological research. The flexibility of merlin is illustrated using an example in patients followed up after heart valve replacement, beginning with a linear model, and finishing with a joint multiple longitudinal and competing risks survival model. ","merlin: An R package for Mixed Effects Regression for Linear, Nonlinear
  and User-defined models"
30,1288644633971634183,4831156173,Ricardo Romero,['New paper:\n<LINK>'],https://arxiv.org/abs/2007.14708,"Assuming neutrinos to be of the Dirac type, the little group generators for the one-particle states, created off the vacuum by the field operator, are obtained, both in terms of the one-particle states themselves and in terms of creation/annihilation operators. It is shown that these generators act also as rotation operators in the Hilbert space of the states, providing three types of transformations: a helicity flip, the standard charge conjugation, and a combination of the two, up to phases. The transformations' properties are provided in detail and their physical implications discussed. It is also shown that one of the transformations continues to hold for chiral fields without mixing them. ",Little group generators for Dirac neutrino one-particle states
31,1288563089542086657,75351541,Kira Goldner,"['Very excited about this new paper with @Yang_Cai, his brilliant student Mingfei Zhao (could be your postdoc in a year!), and *undergrad* Steven Ma!\n\nThe first worst-case approximation to gains from trade in a multi-dimensional two-sided market: <LINK>', ""Would love to hear any/all feedback!\n\nThe gains from trade objective measures how much value the mechanism (or trade platform) adds to the market -- the value added to the market *when* trade occurs. It's harder to approximate than both two-sided welfare *and* one-sided revenue."", 'Highlights include: \n1. An approximation in the minimum trade probability by defining variables such that the prophet inequality/an OCRS will translate to a two-sided market mechanism, as well extending an ""entry fee"" notion to a number of items rather than a price.', '2. A new ""seller-adjusted posted price"" mechanism that, as a function of seller profiles, sets buyer posted prices, and uses allocation coupling to ensure budget balance.', '3. Breaking the items into ""unlikely to trade"" + ""likely to trade,"" then stitching the two mechanisms together gets an unconditional O(log^2 n)-approximation for n single-dim sellers and a constrained-additive buyer (or O(log n) when the constraint is (delta, eta)-selectable).', ""Addendum on selectability: such constraints include matroids, matchings, knapsack constraints, and the intersection of each. (Notion due to Feldman @AlgoSvensson Zenklusen in OCRS '16.)""]",https://arxiv.org/abs/2007.13934,"We study gains from trade in multi-dimensional two-sided markets. Specifically, we focus on a setting with $n$ heterogeneous items, where each item is owned by a different seller $i$, and there is a constrained-additive buyer with feasibility constraint $\mathcal{F}$. Multi-dimensional settings in one-sided markets, e.g. where a seller owns multiple heterogeneous items but also is the mechanism designer, are well-understood. In addition, single-dimensional settings in two-sided markets, e.g. where a buyer and seller each seek or own a single item, are also well-understood. Multi-dimensional two-sided markets, however, encapsulate the major challenges of both lines of work: optimizing the sale of heterogeneous items, ensuring incentive-compatibility among both sides of the market, and enforcing budget balance. We present, to the best of our knowledge, the first worst-case approximation guarantee for gains from trade in a multi-dimensional two-sided market. Our first result provides an $O(\log (1/r))$-approximation to the first-best gains from trade for a broad class of downward-closed feasibility constraints (such as matroid, matching, knapsack, or the intersection of these). Here $r$ is the minimum probability over all items that a buyer's value for the item exceeds the seller's cost. Our second result removes the dependence on $r$ and provides an unconditional $O(\log n)$-approximation to the second-best gains from trade. We extend both results for a general constrained-additive buyer, losing another $O(\log n)$-factor en-route. ",On Multi-Dimensional Gains from Trade Maximization
32,1288526720107261953,118727711,Matías Díaz,"['New paper led by @johannateske👏🏽👏🏽. Using data from @TESSatMIT 🛰and RVs from MIKE, Coralie and PFS we detected a short-period sub-Neptune orbiting the solar-type star HD 86226. Check it out here: <LINK> 🤟🏽\ncc: @ExoplanetJJ @phillippro @astrojennb']",https://arxiv.org/abs/2007.13927,"The Transiting Exoplanet Survey Satellite mission was designed to find transiting planets around bright, nearby stars. Here we present the detection and mass measurement of a small, short-period ($\approx\,4$\,days) transiting planet around the bright ($V=7.9$), solar-type star HD 86226 (TOI-652, TIC 22221375), previously known to host a long-period ($\sim$1600 days) giant planet. HD 86226c (TOI-652.01) has a radius of $2.16\pm0.08$ $R_{\oplus}$ and a mass of 7.25$^{+1.19}_{-1.12}$ $M_{\oplus}$ based on archival and new radial velocity data. We also update the parameters of the longer-period, not-known-to-transit planet, and find it to be less eccentric and less massive than previously reported. The density of the transiting planet is $3.97$ g cm$^{-3}$, which is low enough to suggest that the planet has at least a small volatile envelope, but the mass fractions of rock, iron, and water are not well-constrained. Given the host star brightness, planet period, and location of the planet near both the ``radius gap'' and the ``hot Neptune desert'', HD 86226c is an interesting candidate for transmission spectroscopy to further refine its composition. ","TESS Reveals a Short-period Sub-Neptune Sibling (HD 86226c) to a Known
  Long-period Giant Planet"
33,1288520054003261440,3874714693,augustus odena,"[""What's the fuss(le) about BUSTLE?\nIt's our new paper on program synthesis! (<LINK>)\nWe perform bottom-up search over programs, with machine learning in the inner loop. \nA thread: (1/8) <LINK>"", 'You might think ""oh, this is a natural thing to try, but it will be too slow"".\nBut you would be wrong:\nwe get substantial wall-clock time speedups over the baseline synthesizer,\nusing several fun ideas: (2/8) https://t.co/pviJePvEUj', 'First, our flavor of bottom-up-search ensures that all intermediate states are\nin normal form (as in https://t.co/oGNgCtzpu5), so we can do things with their values. (3/8)', 'The thing we do is compute Property Signatures (https://t.co/fwv6XUilde) of all these intermediate\nvalues and feed them into a neural network, which is trained to predict if an\nintermediate value will appear in the answer.  (4/8)', 'Since all of the signatures are the same size, we can make predictions in large\nbatches, which makes everything faster. (5/8)', 'Since the signatures contain a lot of information about the semantics of the intermediate\nvalues, we can use a very small neural network to do the predictions! (6/8)', 'We expect that there is substantial headroom to improve these results, and that future\nenumerative synthesizers will benefit from embedding machine learning models in a similar way (7/8)', 'This was joint work with Kensen Shi (Equal Contribution), David Bieber, @rishabhs , and @RandomlyWalking  (8/8)']",https://arxiv.org/abs/2007.14381,"Program synthesis is challenging largely because of the difficulty of search in a large space of programs. Human programmers routinely tackle the task of writing complex programs by writing sub-programs and then analyzing their intermediate results to compose them in appropriate ways. Motivated by this intuition, we present a new synthesis approach that leverages learning to guide a bottom-up search over programs. In particular, we train a model to prioritize compositions of intermediate values during search conditioned on a given set of input-output examples. This is a powerful combination because of several emergent properties. First, in bottom-up search, intermediate programs can be executed, providing semantic information to the neural network. Second, given the concrete values from those executions, we can exploit rich features based on recent work on property signatures. Finally, bottom-up search allows the system substantial flexibility in what order to generate the solution, allowing the synthesizer to build up a program from multiple smaller sub-programs. Overall, our empirical evaluation finds that the combination of learning and bottom-up search is remarkably effective, even with simple supervised learning approaches. We demonstrate the effectiveness of our technique on two datasets, one from the SyGuS competition and one of our own creation. ",BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration
34,1288479986958114816,1184198482455945218,Yuber F Perez-G,"['New paper today with André, @ivanj_ms and Manibrata. We explore different physics that you could learn by measuring the diffuse supernova neutrino background (DSNB). Take a look: <LINK> <LINK>']",https://arxiv.org/abs/2007.13748,"The Universe is awash with tens-of-MeV neutrinos of all species coming from all past core-collapse supernovae. These have never been observed, but this state of affairs will change in the near future. In the less than ten years, the Super-Kamiokande experiment, loaded with gadolinium, is expected to collect dozens of events induced by the scattering of neutrinos from the diffuse supernova neutrino background (DSNB). Next-generation projects, including Hyper-Kamiokande and Theia, are expected to collect data samples with hundreds of DSNB events after a decade of running. Here, we study quantitatively how well the DSNB, including its energy spectrum, will be measured by different current or upcoming large neutrino detectors. We analyze the simulated data in order to estimate how well measurements of the DSNB can be used to inform research topics in cosmology -- including measurements of the Hubble parameter -- astrophysics -- including the star-formation-rate -- and particle physics -- including the neutrino lifetime and the possibility that neutrinos are pseudo-Dirac fermions. ",Fundamental physics with the diffuse supernova background neutrinos
35,1288458034835062786,2804580631,Andrew Houck,"['Pranav and András have done some really nice work in our new paper, demonstrating coherence gains for fluxonium away from the sweet spot by generating dynamical sweet spots with a parametric drive.  In collaboration with the Koch group at Northwestern.\n\n<LINK>']",https://arxiv.org/abs/2007.13756,"We use the quasienergy structure that emerges when a fluxonium superconducting circuit is driven periodically to encode quantum information with dynamically induced flux-insensitive sweet spots. The framework of Floquet theory provides an intuitive description of these high-coherence working points located away from the half-flux symmetry point of the undriven qubit. This approach offers flexibility in choosing the flux bias point and the energy of the logical qubit states as shown in [\textit{Huang et al., 2020}]. We characterize the response of the system to noise in the modulation amplitude and DC flux bias, and experimentally demonstrate an optimal working point which is simultaneously insensitive against fluctuations in both. We observe a 40-fold enhancement of the qubit coherence times measured with Ramsey-type interferometry at the dynamical sweet spot compared with static operation at the same bias point. ","Floquet-engineered enhancement of coherence times in a driven fluxonium
  qubit"
36,1288403901390172160,1392935011,Ole-Chr. Granmo,"['Congratulations to @Xuanzjiao, with co-authors, for her brilliant in-depth theoretical analysis of Tsetlin Machine optimality! Using a quasi-stationary Markov chain-based analysis, she has proven several new properties. Get the paper here: <LINK> <LINK>']",https://arxiv.org/abs/2007.14268,"The Tsetlin Machine (TM) is a recent machine learning algorithm with several distinct properties, such as interpretability, simplicity, and hardware-friendliness. Although numerous empirical evaluations report on its performance, the mathematical analysis of its convergence is still open. In this article, we analyze the convergence of the TM with only one clause involved for classification. More specifically, we examine two basic logical operators, namely, the ""IDENTITY""- and ""NOT"" operators. Our analysis reveals that the TM, with just one clause, can converge correctly to the intended logical operator, learning from training data over an infinite time horizon. Besides, it can capture arbitrarily rare patterns and select the most accurate one when two candidate patterns are incompatible, by configuring a granularity parameter. The analysis of the convergence of the two basic operators lays the foundation for analyzing other logical operators. These analyses altogether, from a mathematical perspective, provide new insights on why TMs have obtained state-of-the-art performance on several pattern recognition problems. ","On the Convergence of Tsetlin Machines for the IDENTITY- and NOT
  Operators"
37,1288395874192830464,196749454,Natalie Hogg,"[""Paper day! <LINK>\n\nEver wondered if you've discovered some beyond LCDM physics by accident? Could there be some hidden modified gravity effects lurking in gravitational wave detections? Have we found a new smoking gun for modified gravity? Read on to find out!"", 'Matteo (@matmartinelli1), Savvas and I created and used mock standard siren datasets to forecast the ability of the Einstein Telescope (ET), LSST and DESI to constrain the distance duality relation (DDR), which relates luminosity distances to angular diameter distances.', 'We used a toy model in which photons decay into axions to break the DDR, and found that the combination of SNIa + GW events is competitive with the more commonly used SNIa + BAO when constraining deviations from DDR.\n\n(Paging the chair of the axion fan club @duetosymmetry 😉)', 'But it pays to be careful when using a probe of the gravitational sector as modified gravity (MG) effects could be at play! By including a generic MG model in our mock datasets, we found that the DDR analysis became extremely biased, leading to a false detection of DDR violation! https://t.co/etHLlyjjT7', 'However, the problem can be resolved by explicitly including the modified gravity in your analysis. Here, the full combination of mock datasets broke the degeneracies in parameter space and correctly recovered the fiducial cosmology. No more false detection of DDR violation! https://t.co/xLLRczzwIb', 'Of course, if you have modified gravity, you will likely have a screening mechanism to go with it. GW events and SNIa are both events in which MG could be screened -- how does this affect our results? If GW are screened, we find another false detection of DDR violation...', ""but if the SNIa are screened, we find that the cosmological parameters are also biased away from the fiducial cosmology, with only the combination LSST + DESI correctly recovering the fiducial. If this is seen in real data, it's a smoking gun for MG with this screening behaviour! https://t.co/wai5D8oxgK"", 'Savvas also applied his Genetic Algorithm machine learning code to reconstruct the DDR as a function of redshift, finding that it can correctly distinguish between the LCDM and MG mock datasets and finds the same biases as in the parameterised case, nicely confirming our results.', ""Final tweet! This is the first time I've shared a paper draft with people other than fellow authors before posting it on arXiv and we're really grateful for all the comments and feedback we received -- shout out to Ian, Kazuya, Carlos, Isaac and Bill (@BillWrightCosmo)!""]",https://arxiv.org/abs/2007.14335,"We use gravitational wave (GW) standard sirens, in addition to Type Ia supernovae (SNIa) and baryon acoustic oscillation (BAO) mock data, to forecast constraints on the electromagnetic and gravitational distance duality relations (DDR). We make use of a parameterised approach based on a specific DDR violation model, along with a machine learning reconstruction method based on the Genetic Algorithms. We find that GW provide an alternative to the use of BAO data to constrain violations of the DDR, reaching $3\%$ constraints on the violation parameter we consider when combined with SNIa, which is only improved by a factor of $\approx1.4$ if one instead considers the combination of BAO and SNIa. We also investigate the possibility that a neglected modification of gravity might lead to a false detection of DDR violations, even when screening mechanisms are active. We find that such a false detection can be extremely significant, up to $\approx10\sigma$ for very extreme modified gravity scenarios, while this reduces to $\approx4\sigma$ in a more realistic case. False detections can also provide a smoking gun for the modified gravity mechanism at play, as a result of the tension introduced between the SNIa+GW and SNIa+BAO combinations. ",Constraints on the distance duality relation with standard sirens
38,1288395405831675904,54849207,Ian Harrison,"['New paper with @Tessa_M_Baker on arXiv:\n<LINK>\nMain point is in this figure... When it comes to constraining modified gravity, LIGO standard sirens help out LSS a bit, but LISA standard sirens help a lot! <LINK>', '@Tessa_M_Baker (personal note/mea culpa: this paper has had a couple of false starts over the past year or two, so it is great for it to be public finally!)', '@SeshNadathur @Tessa_M_Baker No, we made the decision not to go through with the machinery for fsigma_8 forecasts. Some insight can be had in the propto O_de case by comparing to https://t.co/eCyC8sy3Dt Table 1 (although those are Fishers).']",https://arxiv.org/abs/2007.13791,"The first multi-messenger gravitational wave event has had a transformative effect on the space of modified gravity models. In this paper we study the enhanced tests of gravity that are possible with a future set of gravitational wave standard siren events. We perform MCMC constraint forecasts for parameters in Horndeski scalar-tensor theories. In particular, we focus on the complementarity of gravitational waves with electromagnetic large-scale structure data from galaxy surveys. We find that the addition of fifty low redshift ($z \lesssim 0.2$) standard sirens from the advanced LIGO network offers only a modest improvement (a factor 1.1 -- 1.3, where 1.0 is no improvement) over existing constraints from electromagnetic observations of large-scale structures. In contrast, high redshift (up to $z \sim 10$) standard sirens from the future LISA satellite will improve constraints on the time evolution of the Planck mass in Horndeski theories by a factor $\sim 5$. By simulating different scenarios, we find this improvement to be robust to marginalisation over unknown merger inclination angles and to variation between three plausible models for the merger source population. ","Constraining Scalar-Tensor Modified Gravity with Gravitational Waves and
  Large Scale Structure Surveys"
39,1288394561677668352,4559585553,Stephane Chretien,['Our new paper with Emmanuel Caron about the double descent phenomenon is out ! <LINK>'],https://arxiv.org/abs/2007.12882,"Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error (a.k.a. testing error) increases, but in many cases, it starts decreasing again past the threshold $p=n$. This surprising phenomenon, brought to the theoretical community attention in \cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-square problem, mostly in the asymptotic regime when $p$ and $n$ tend to $+\infty$; see e.g. \cite{hastie2019surprises}. In the present paper, we propose a finite sample analysis of non-linear models of \textit{ridge} type, where we investigate the \textit{overparametrised regime} of the double descent phenomenon for both the \textit{estimation problem} and the \textit{prediction} problem. Our results provide a precise analysis of the distance of the best estimator from the true parameter as well as a generalisation bound which complements recent works of \cite{bartlett2020benign} and \cite{chinot2020benign}. Our analysis is based on efficient but elementary tools closely related to the continuous Newton method \cite{neuberger2007continuous}. ","A finite sample analysis of the benign overfitting phenomenon for ridge
  function estimation"
40,1288374616843526145,1257348184910823425,Augustine Kshetrimayum,"['New paper! We present Stark time crystals, a non-equilibrium phase of matter that breaks translation symmetry in both space &amp; time. Our work does not rely on disorder &amp; has several features that are not seen in conventional time crystals.\n<LINK> <LINK>', 'Amazing collaboration with @jenseisert and Dante Kennes']",https://arxiv.org/abs/2007.13820,"The compelling original idea of a time crystal has referred to a structure that repeats in time as well as in space, an idea that has attracted significant interest recently. While obstructions to realize such structures became apparent early on, focus has shifted to seeing a symmetry breaking in time in periodically driven systems, a property of systems referred to as discrete time crystals. In this work, we introduce Stark time crystals based on a type of localization that is created in the absence of any spatial disorder. We argue that Stark time crystals constitute a phase of matter coming very close to the original idea and exhibit a symmetry breaking in space and time. Complementing a comprehensive discussion of the physics of the problem, we move on to elaborating on possible practical applications and argue that the physical demands of witnessing genuine signatures of many-body localization in large systems may be lessened in such physical systems. ",Stark time crystals: Symmetry breaking in space and time
41,1288326614376300544,1200996469144031232,Enrico Ramirez-Ruiz,"['In a new paper <LINK>, the brilliant Brenna Mockler @UCSCscience presents a comprehensive cosmic energy inventory to provide us with a picture of the amount and organization of the material contents of a tidally disrupted star by a supermassive  black hole.']",https://arxiv.org/abs/2007.12198,"Tidal disruption events (TDEs) offer a unique opportunity to study a single super-massive black hole (SMBH) under feeding conditions that change over timescales of days or months. However, the primary mechanism for generating luminosity during the flares remains debated. Despite the increasing number of observed TDEs, it is unclear whether most of the energy in the initial flare comes from accretion near the gravitational radius or from circularizing debris at larger distances from the SMBH. The energy dissipation efficiency increases with decreasing radii, therefore by measuring the total energy emitted and estimating the efficiency we can derive clues about the nature of the emission mechanism. Here we calculate the integrated energy, emission timescales, and average efficiencies for the TDEs using the Modular Open Source Fitter for Transients ({\tt MOSFiT}). Our calculations of the total energy generally yield higher values than previous estimates. This is predominantly because, if the luminosity follows the mass fallback rate, TDEs release a significant fraction of their energy long after their light curve peaks. We use {\tt MOSFiT} to calculate the conversion efficiency from mass to radiated energy, and find that for many of the events it is similar to efficiencies inferred for active galactic nuclei. There are, however, large systematic uncertainties in the measured efficiency due to model degeneracies between the efficiency and the mass of the disrupted star, and these must be reduced before we can definitively resolve the emission mechanism of individual TDEs. ",An Energy Inventory of Tidal Disruption Events
42,1288295734542864385,1015190157845258240,Yuji Matsumoto,"['New paper: \nMatsumoto, Gu, Kokubo, Osino, and Omiya, \nEjection of close-in super-Earths around low-mass stars in the giant impact stage\n<LINK>', 'We found ejection occurs and planetary growth in the giant impact stage is suppressed. For 0.1 M_sun at ~0.1 au, 3M_E is the limit by ejection. This limit is one possible reason why there are no ~10M_E planets around ~0.1 M_sun stars. https://t.co/GMy2Sf3LaI']",https://arxiv.org/abs/2007.14039,"Earth-sized planets were observed in close-in orbits around M dwarfs. While more and more planets are expected to be uncovered around M dwarfs, theories of their formation and dynamical evolution are still in their infancy. We investigate the giant impact growth of protoplanets, which includes strong scattering around low-mass stars. The aim is to clarify whether strong scattering around low-mass stars affects the orbital and mass distributions of the planets. We perform $N$-body simulation of protoplanets by systematically surveying the parameter space of the stellar mass and surface density of protoplanets. We find that protoplanets are often ejected after twice or three times close-scattering around late M dwarfs. The ejection sets the upper limit of the largest planet mass. Adopting the surface density scaling linearly with the stellar mass, we find that as the stellar mass decreases less massive planets are formed in orbits with higher eccentricities and inclinations. Under this scaling, we also find that a few close-in protoplanets are generally ejected. The ejection of protoplanets plays an important role in the mass distribution of super-Earths around late M dwarfs. The mass relation of observed close-in super-Earths and their central star mass is well reproduced by ejection. ","Ejection of close-in super-Earths around low-mass stars in the giant
  impact stage"
43,1288129406242664450,39697728,Flora Salim,['How to perform self supervised learning efficiently on the edge? Check out our new paper in IoT journal. Also the preprint <LINK>\n\n#IoT #DataScience #MachineLearning #SSL #DeepLearning #federated #sensing\n\n#AcademicChatter #AcademicTwitter <LINK>'],https://arxiv.org/abs/2007.13018,"Smartphones, wearables, and Internet of Things (IoT) devices produce a wealth of data that cannot be accumulated in a centralized repository for learning supervised models due to privacy, bandwidth limitations, and the prohibitive cost of annotations. Federated learning provides a compelling framework for learning models from decentralized data, but conventionally, it assumes the availability of labeled samples, whereas on-device data are generally either unlabeled or cannot be annotated readily through user interaction. To address these issues, we propose a self-supervised approach termed \textit{scalogram-signal correspondence learning} based on wavelet transform to learn useful representations from unlabeled sensor inputs, such as electroencephalography, blood volume pulse, accelerometer, and WiFi channel state information. Our auxiliary task requires a deep temporal neural network to determine if a given pair of a signal and its complementary viewpoint (i.e., a scalogram generated with a wavelet transform) align with each other or not through optimizing a contrastive objective. We extensively assess the quality of learned features with our multi-view strategy on diverse public datasets, achieving strong performance in all domains. We demonstrate the effectiveness of representations learned from an unlabeled input collection on downstream tasks with training a linear classifier over pretrained network, usefulness in low-data regime, transfer learning, and cross-validation. Our methodology achieves competitive performance with fully-supervised networks, and it outperforms pre-training with autoencoders in both central and federated contexts. Notably, it improves the generalization in a semi-supervised setting as it reduces the volume of labeled data required through leveraging self-supervised learning. ","Federated Self-Supervised Learning of Multi-Sensor Representations for
  Embedded Intelligence"
44,1288123905861758976,94412971,Wojciech Kryściński,"['New work in which we discuss summarization model evaluation and share a large set of resources for eval. metric research! 📊🧮\n\nw/ @alexfabbri4 @BMarcusMcCann @CaimingXiong @RichardSocher Dragomir Radev \n\nPaper: <LINK>\nResources: <LINK>\n\nThread:', 'We re-evaluated and studied 12 commonly used evaluation metrics in a consistent fashion and offer a side-by-side comparison of their performance in correlation to human expert annotators. (2/n) https://t.co/l6WI8rNW1K', 'We re-evaluated 23 modern summarization model outputs in a consistent and comprehensive fashion using both human annotations and automatic evaluation metrics and offer a side-by-side comparison of their performance. (3/n) https://t.co/cszvrp5Hxu', 'We implemented and shared an evaluation toolkit that offers a unified and easy way of conducting comprehensive performance evaluations of new summarization model outputs. (4/n)', 'We collected, unified, and shared the largest collection of summarization model outputs trained on the CNN/DailyMail news corpus. The collection includes 44 model outputs associated with 23 recent summarization papers. (5/n)', 'We collected and shared the largest and most diverse, in terms of model types, collection of human judgements of model-generated summaries that include expert and crowd-sourced annotations. (6/n)', 'We hope our contributions will promote a more complete evaluation protocol of summarization methods and will encourage further research into reliable evaluation metrics. (7/n)', 'We thank all authors who shared their model outputs and thus contributed to our work and we encourage the research community to join our efforts and contribute their model outputs and extend the evaluation toolkit. (8/n)', '#NLProc #TextSummarization (9/n)']",https://arxiv.org/abs/2007.12626,"The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continue to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 14 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations, 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics, 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format, 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics, 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd-source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgments. ",SummEval: Re-evaluating Summarization Evaluation
45,1288021353107525632,1541589984,Ivan P Supic,"['A new paper with J.D. Bancal and N. Brunner on network nonlocality. One cannot work in Geneva without ""attacking"" the famous triangle problem :)\n<LINK>']",https://arxiv.org/abs/2007.12950,"Quantum nonlocality can be observed in networks even in the case where every party can only perform a single measurement, i.e. does not receive any input. So far, this effect has been demonstrated under the assumption that all sources in the network are fully independent from each other. Here we investigate to what extent this independence assumption can be relaxed. After formalizing the question, we show that, in the triangle network without inputs, quantum nonlocality can be observed, even when assuming only an arbitrarily small level of independence between the sources. This means that quantum predictions cannot be reproduced by a local model unless the three sources can be perfectly correlated. ","Quantum nonlocality in networks can be demonstrated with an arbitrarily
  small level of independence between the sources"
46,1287927441852207105,1196266674954985472,Nirmal Raj,"['New paper with @DjunaCroon, \n@davemckeen, &amp; Zihui Wang! ""Subaru through a different lens"": <LINK> . \nThread follows. For a flash review of gravitational microlensing see my previous thread on it: <LINK>. <LINK>', ""You'd think a star in the Andromeda Galaxy (10^19 km away) would look like a point from here, but so sharp-eyed is the Subaru Telescope that when it looks for the brief dazzle of  starlight bent by a passing object, the star's spatial extent (~10^7 km) actually matters to it! https://t.co/Arn4clsnMc"", 'If the passing object is a dark matter structure such as a subhalo or a boson star, *its* spatial extent -- and unique mass distribution -- must figure in the lensing signal as well. That gives us a brand new constraint to the hunt for #darkmatter. https://t.co/djtQwqiXcP']",https://arxiv.org/abs/2007.12697,"We investigate gravitational microlensing signals produced by a spatially extended object transiting in front of a finite-sized source star. The most interesting features arise for lens and source sizes comparable to the Einstein radius of the setup. Using this information, we obtain constraints from the Subaru-HSC survey of M31 on the dark matter populations of NFW subhalos and boson stars of asteroid to Earth masses. These lens profiles capture the qualitative behavior of a wide range of dark matter substructures. We find that deviations from constraints on point-like lenses (e.g. primordial black holes and MACHOs) become visible for lenses of radius 0.1 $R_\odot$ and larger, with the upper bound on lens masses weakening with increasing lens size. ","Subaru through a different lens: microlensing by extended dark matter
  structures"
47,1287924170790076418,826167939107786752,Derek Buzasi,"[""New paper, showing again how diverse data sets and patience combine to show us what's happening in stars, in this case the G8 subgiant and companion in the visual triple system 94 Aqr! (1/4) \n\n<LINK>""]",https://arxiv.org/abs/2007.12755,"Most previous efforts to calibrate how rotation and magnetic activity depend on stellar age and mass have relied on observations of clusters, where isochrones from stellar evolution models are used to determine the properties of the ensemble. Asteroseismology employs similar models to measure the properties of an individual star by matching its normal modes of oscillation, yielding the stellar age and mass with high precision. We use 27 days of photometry from the Transiting Exoplanet Survey Satellite to characterize solar-like oscillations in the G8 subgiant of the 94 Aqr triple system. The resulting stellar properties, when combined with a reanalysis of 35 yr of activity measurements from the Mount Wilson HK project, allow us to probe the evolution of rotation and magnetic activity in the system. The asteroseismic age of the subgiant agrees with a stellar isochrone fit, but the rotation period is much shorter than expected from standard models of angular momentum evolution. We conclude that weakened magnetic braking may be needed to reproduce the stellar properties, and that evolved subgiants in the hydrogen shell-burning phase can reinvigorate large-scale dynamo action and briefly sustain magnetic activity cycles before ascending the red giant branch. ","The Evolution of Rotation and Magnetic Activity in 94 Aqr Aa from
  Asteroseismology with TESS"
48,1287862527578103808,1067301932610478081,Ayaka Usui,['Our new arXiv paper: Quantum Simulators in Other Frames of Reference - <LINK>\n\nIt relaxes rules on the architecture of analog and digital quantum simulators.'],https://arxiv.org/abs/2007.06740,"We develop a framework and give an example for situations where two distinct Hamiltonians living in the same Hilbert space can be used to simulate the same physics. As an example of an analog simulation, we first discuss how one can simulate an infinite-range-interaction one-axis twisting Hamiltonian using a short-range nearest-neighbor-interaction Heisenberg XXX model with a staggered field. Based on this, we show how one can build an alternative version of a digital quantum simulator. As a by-product, we present a method for creating many-body maximally entangled states using only short-range nearest-neighbor interactions. ",Simulating the same physics with two distinct Hamiltonians
49,1287836269074984960,1133496847966752769,Lukas Zalesky,"['My first, first-author paper has been accepted to MNRAS! Check it out or continue reading if you would like to hear more about a new code to model gravitational lensing by galaxy clusters, and the first characterization of new powerful lenses. <LINK>', 'My code relies on the assumption that red galaxies in clusters trace the underlying distribution of mass in these massive systems, i.e., that light-traces-mass. This is approximately true for both the luminous and dark components of matter in clusters (aside from cluster gas).', 'Since the dark matter in clusters is by far the most dominant component (by about an order of magnitude), this assumption can be used to quickly model galaxy clusters, with a few caveats.', 'We use physically motivated scaling relations to describe the mass profiles of cluster galaxies, which are tied to their luminosities. We also use the distribution of galaxies, along with a smoothing kernel, to determine the approximate scale and shape of the dark matter halo.', 'Here is how it looks: The top left is the galaxy-scale component and the top right is the cluster-scale halo. We add these two components together, and from their combined deflection we can calculate the magnification profile shown below. https://t.co/yJPkPrWbqO', 'This method uses 3 parameters that are calibrated on known lensing clusters through an MCMC. After the calibration step, we can apply the modeling code blindly to any other galaxy cluster in our sample, assuming the data are homogeneous.', 'In our sample of 96 clusters, we find many powerful new lenses! In this histogram, we show the distribution of Einstein radii, which is a measure of the total strong-lensing area within these systems. https://t.co/1AqhaRwrLC', 'We compare our results with those from the CLASH (cluster lensing and supernova survey with Hubble). This program specifically targeted massive lenses, so it serves as a good comparison with our sample. The comparison demonstrates the strength of lenses in our sample.', 'We also share some of the details of the most powerful lenses we have characterized (most for the first time). In this figure, we show the clusters and their critical lines, which are lines that trace the maximum peaks in their magnification profiles. https://t.co/6rmbDu5JzJ', 'Be on the lookout for more works using this code! :)']",https://arxiv.org/abs/2007.12182,"We use AStroLens, a newly developed gravitational lens-modeling code that relies only on geometric and photometric information of cluster galaxies as input, to map the strong-lensing regions and estimate the lensing strength of 96 galaxy clusters at $z=0.5$-$0.9$. All clusters were identified during the extended Massive Cluster Survey (eMACS) based on their X-ray flux and optical appearance. Building on the well tested assumption that the distribution of both luminous and dark matter in galaxy clusters is approximately traced by the distribution of light, i.e., that light traces mass, AStroLens uses three global parameters to automatically model the deflection from strong-gravitational lensing for all galaxy clusters in this diverse sample. We test the robustness of our code by comparing AStroLens estimates derived solely from shallow optical images in two passbands with the results of in-depth lens-modeling efforts for two well studied eMACS clusters and find good agreement, both with respect to the size and the shape of the strong-lensing regime delineated by the respective critical lines. Our study finds 31 eMACS clusters with effective Einstein radii ($\theta_{E}$) in excess of 20"" and eight with $\theta_{E} >$ 30"", thereby underlining the value of X-ray selection for the discovery of powerful cluster lenses that complement giants like MACSJ0717 at ever-increasing redshift. As a first installment toward the public release of the eMACS sample, we list physical properties of the ten calibration clusters as well as of the ten most powerful eMACS cluster lenses, according to AStroLens. ","AStroLens: Automatic Strong-Lens Modeling of X-ray Selected Galaxy
  Clusters"
50,1287797251922960384,560668519,Neil Houlsby,"['If you are interested in the important questions in vision, like ""can my big SOTA model recognize a koala, flying upside-down, in the mountains?"", and many others, you might be interested in our new paper: <LINK> <LINK>']",https://arxiv.org/abs/2007.08558,"Modern deep convolutional networks (CNNs) are often criticized for not generalizing under distributional shifts. However, several recent breakthroughs in transfer learning suggest that these networks can cope with severe distribution shifts and successfully adapt to new tasks from a few training examples. In this work we study the interplay between out-of-distribution and transfer performance of modern image classification CNNs for the first time and investigate the impact of the pre-training data size, the model scale, and the data preprocessing pipeline. We find that increasing both the training set and model sizes significantly improve the distributional shift robustness. Furthermore, we show that, perhaps surprisingly, simple changes in the preprocessing such as modifying the image resolution can significantly mitigate robustness issues in some cases. Finally, we outline the shortcomings of existing robustness evaluation datasets and introduce a synthetic dataset SI-Score we use for a systematic analysis across factors of variation common in visual data such as object size and position. ",On Robustness and Transferability of Convolutional Neural Networks
51,1287654046417985537,885067890755600384,Eric Savin,"['New paper: Uncertainty quantification applied to the propagation of wind tunnel inflow inhomogeneities on a cylinder drag, with N. Detomaso, V. Brion, J. Dandois, and M. Couliou @MarieCouliou.\n<LINK>']",https://arxiv.org/abs/2007.12517,"The uncertainty associated with the experimental inflow in a wind tunnel affects the prediction of the flow of interest by numerical simulations. We evaluate this impact using uncertainty quantification. A method is developed and applied to the simulation of the drag generated by the flow past a cylinder installed in the transonic S3Ch ONERA mid-scale facility. The inflow uncertainty results from the imperfect knowledge and variability of the flow in the settling chamber. It is taken into account via the inlet boundary condition in the numerical companion setup and evaluated experimentally by measuring the inflow using a hot-wire rake. The propagation of the input uncertainties is carried \alert{out} through a two-dimensional RANS model of the experiment. A polynomial surrogate model is developed to infer the uncertainty associated with the drag of the cylinder. Following observations of Gaussian inputs, the parameters of the stochastic model are constructed in two ways, first through a projection approach, based on the Gauss-Hermite quadrature rule, and then using a sparsity based regression approach, based on compressed sensing. The latter drastically reduces the number of deterministic numerical simulations. The drag is most influenced by the central part of the inflow but the overall uncertainty remains low. ","Uncertainty Quantification Applied to the Propagation of a Transonic
  Wind Tunnel Inflow Inhomogeneities"
52,1286906796750364673,2815546537,Viktor Cikojević,['New paper about ultradilute quantum droplets out:\n\n<LINK>\n\n on the influence of the finite-range effects on quadrupole and monopole frequency <LINK>'],https://arxiv.org/abs/2007.06977,"Some discrepancies between experimental results on quantum droplets made of a mixture of $^{39}$K atoms in different hyperfine states and their analysis within extended Gross-Pitaevskii theory (which incorporates beyond mean-field corrections) have been recently solved by introducing finite-range effects into the theory. Here, we study the influence of these effects on the monopole and quadrupole excitation spectrum of extremely dilute quantum droplets using a density functional built from first-principles quantum Monte Carlo calculations, which can be easily introduced in the existing Gross-Pitaevskii numerical solvers. Our results show differences of up to $20\%$ with those obtained within the extended Gross-Pitaevskii theory, likely providing another way to observe finite-range effects in mixed quantum droplets by measuring their lowest excitation frequencies. ","Towards a QMC-based density functional including finite-range effects:
  excitation modes of a $^{39}$K quantum droplet"
53,1286777571468808194,3031558614,Zivvy Ξpstein,"['Another day another generator.\n\nBut are the artifacts produced rote statistical averages or ""alien play"" that transcends expectations? Finding those rare gems is the topic of our new paper &amp; the below primer on how this generativity maps to intelligence\n\n<LINK> <LINK>']",https://arxiv.org/abs/2007.11119,"The latent space modeled by generative adversarial networks (GANs) represents a large possibility space. By interpolating categories generated by GANs, it is possible to create novel hybrid images. We present ""Meet the Ganimals,"" a casual creator built on interpolations of BigGAN that can generate novel, hybrid animals called ganimals by efficiently searching this possibility space. Like traditional casual creators, the system supports a simple creative flow that encourages rapid exploration of the possibility space. Users can discover new ganimals, create their own, and share their reactions to aesthetic, emotional, and morphological characteristics of the ganimals. As users provide input to the system, the system adapts and changes the distribution of categories upon which ganimals are generated. As one of the first GAN-based casual creators, Meet the Ganimals is an example how casual creators can leverage human curation and citizen science to discover novel artifacts within a large possibility space. ",Interpolating GANs to Scaffold Autotelic Creativity
54,1286692429576327170,567610832,Davis Rempe,"['Our new #ECCV2020 paper uses learned foot contact estimation and trajectory optimization to reconstruct physically-plausible 3D human motion from video. Check it out!\n\nPaper: <LINK>\nVideo: <LINK>\nProject: <LINK> <LINK>', 'This is joint work with great collaborators at Adobe @jimei_yang @AaronHertzmann @RubenEVillegas and Bryan, along with Leo Guibas.']",http://arxiv.org/abs/2007.11678,"Existing deep models predict 2D and 3D kinematic poses from video that are approximately accurate, but contain visible errors that violate physical constraints, such as feet penetrating the ground and bodies leaning at extreme angles. In this paper, we present a physics-based method for inferring 3D human motion from video sequences that takes initial 2D and 3D pose estimates as input. We first estimate ground contact timings with a novel prediction network which is trained without hand-labeled data. A physics-based trajectory optimization then solves for a physically-plausible motion, based on the inputs. We show this process produces motions that are significantly more realistic than those from purely kinematic methods, substantially improving quantitative measures of both kinematic and dynamic plausibility. We demonstrate our method on character animation and pose estimation tasks on dynamic motions of dancing and sports with complex contact patterns. ",Contact and Human Dynamics from Monocular Video
55,1286652601925341191,19333650,Vedant Chandra,"['Our new (and my first) paper was accepted to @RAS_Journals and just went up on @arxiv: ""Computational Tools for the Spectroscopic Analysis of White Dwarfs"", written with Hsiang-Chih Hwang, Nadia L. Zakamska, and Tamás Budavári. \n\n<LINK> <LINK>', 'A year ago, our research group wanted to analyze white dwarf spectra to characterize their temperature and surface gravity - but it turns out, most of the relevant code and theoretical models are proprietary and kept under restricted access.', ""So, we built the tool we needed. We developed two methods to infer stellar parameters ('labels') from white dwarf spectra. The first uses high-speed neural networks to interpolate a grid of synthetic model spectra, enabling the rapid fitting of models to observations. https://t.co/8SSGDzfcxN"", 'The second method maps line summaries of the prominent Balmer absorption lines to stellar labels with a random forest regression model. We found that these simple line summaries (width and amplitude) carry a lot of information about the stellar parameters. https://t.co/9cUac88Dlx', 'We tested our methods on over five thousand white dwarf spectra from @sdssurveys, and recovered stellar labels derived by a previous group (Tremblay et. al 2019). The random forest method is particularly fast and could be useful with upcoming large-scale spectroscopic surveys. https://t.co/IeZ7CfwaOr', 'Finally, we discuss an exciting application of our tool beyond label inference - finding interesting, exotic systems like magnetic white dwarfs (shown here) and double-degenerate binaries. Combining spectroscopic and photometric information is especially useful. https://t.co/pPDgWEFccq', 'This work is built on the shoulders of giants in the white dwarf community who have developed theoretical models that form the basis of our techniques. We hope to continuously upgrade and improve our methods as new models and codes are made publicly available.', ""Personally, I'm especially grateful to my advisors, Professor Nadia Zakamska and graduate student Hsiang-Chih Hwang, as well as @SihaoCheng and @jotajotahermes for illuminating conversations. This was fun."", 'Our tool is under active development, and the latest version is available on GitHub: https://t.co/2haXP1pp6z and has documentation hosted here: https://t.co/Mcu1XAVQ7Z.']",https://arxiv.org/abs/2007.11598,"The spectroscopic features of white dwarfs are formed in the thin upper layer of their stellar photosphere. These features carry information about the white dwarf's surface temperature, surface gravity, and chemical composition (hereafter 'labels'). Existing methods to determine these labels rely on complex ab-initio theoretical models which are not always publicly available. Here we present two techniques to determine atmospheric labels from white dwarf spectra: a generative fitting pipeline that interpolates theoretical spectra with artificial neural networks, and a random forest regression model using parameters derived from absorption line features. We test and compare our methods using a large catalog of white dwarfs from the Sloan Digital Sky Survey (SDSS), achieving the same accuracy and negligible bias compared to previous studies. We package our techniques into an open-source Python module 'wdtools' that provides a computationally inexpensive way to determine stellar labels from white dwarf spectra observed from any facility. We will actively develop and update our tool as more theoretical models become publicly available. We discuss applications of our tool in its present form to identify interesting outlier white dwarf systems including those with magnetic fields, helium-rich atmospheres, and double-degenerate binaries. ",Computational Tools for the Spectroscopic Analysis of White Dwarfs
56,1286493258311499776,4902145390,Gordan Krnjaic,"['Honored to be part of a cool hep-ex paper that reports new limits on composite dark matter models. Cheers to my awesome collaborators Fernando Monteiro, Gadi Afek, Dan Carney, Jiaxiang Wang, and Dave Moore\n\n<LINK>']",https://arxiv.org/abs/2007.12067,"Results are reported from a search for a class of composite dark matter models with feeble, long-range interactions with normal matter. We search for impulses arising from passing dark matter particles by monitoring the mechanical motion of an optically levitated nanogram mass over the course of several days. Assuming such particles constitute the dominant component of dark matter, this search places upper limits on their interaction with neutrons of $\alpha_n \leq 1.2 \times 10^{-7}$ at 95\% confidence for dark matter masses between 1--10 TeV and mediator masses $m_\phi \leq 0.1$ eV. Due to the large enhancement of the cross-section for dark matter to coherently scatter from a nanogram mass ($\sim 10^{29}$ times that for a single neutron) and the ability to detect momentum transfers as small as $\sim$200 MeV/c, these results provide sensitivity to certain classes of composite dark matter models that substantially exceeds existing searches, including those employing kg-scale or ton-scale targets. Extensions of these techniques can enable directionally-sensitive searches for a broad class of previously inaccessible heavy dark matter candidates. ",Search for composite dark matter with optically levitated sensors
57,1286475658273492993,1128508767404838912,Thayne Currie,"['Our new SCExAO paper/SCExAO Picture of the day: the HD 34700 ring-like protoplanetary disk + spirals\n\nUyama, Currie, &amp; Christiaens et al. 2020, ApJ accepted\n\n<LINK> <LINK>']",https://arxiv.org/abs/2007.11655,"We present Subaru/SCExAO+CHARIS broadband ($JHK$-band) integral field spectroscopy of HD 34700 A. CHARIS data recover HD 34700 A's disk ring and confirm multiple spirals discovered in Monnier et al. (2019). We set limits on substellar companions of $\sim12\ M_{\rm Jup}$ at $0\farcs3$ (in the ring gap) and $\sim5\ M_{\rm Jup}$ at $0\farcs75$ (outside the ring). The data reveal darkening effects on the ring and spiral, although we do not identify the origin of each feature such as shadows or physical features related to the outer spirals. Geometric albedoes converted from the surface brightness suggests a higher scale height and/or prominently abundant sub-micron dust at position angle between $\sim45^\circ$ and $90^\circ$. Spiral fitting resulted in very large pitch angles ($\sim30-50^\circ$) and a stellar flyby of HD 34700 B or infall from a possible envelope is perhaps a reasonable scenario to explain the large pitch angles. ","SCExAO/CHARIS High-Contrast Imaging of Spirals and Darkening Features in
  the HD 34700 A Protoplanetary Disk"
58,1286325263354265600,750520429777944576,Vincent Bindschaedler,"['Interested in adversarial ML attacks and defenses for speech/speaker recognition systems? Check out our new SoK paper, which will appear at IEEE S&amp;P 2021! Paper: <LINK> ; Website: <LINK> ; Thread by @patrickgtraynor describes contributions. <LINK>']",https://arxiv.org/abs/2007.06622,"Speech and speaker recognition systems are employed in a variety of applications, from personal assistants to telephony surveillance and biometric authentication. The wide deployment of these systems has been made possible by the improved accuracy in neural networks. Like other systems based on neural networks, recent research has demonstrated that speech and speaker recognition systems are vulnerable to attacks using manipulated inputs. However, as we demonstrate in this paper, the end-to-end architecture of speech and speaker systems and the nature of their inputs make attacks and defenses against them substantially different than those in the image space. We demonstrate this first by systematizing existing research in this space and providing a taxonomy through which the community can evaluate future work. We then demonstrate experimentally that attacks against these models almost universally fail to transfer. In so doing, we argue that substantial additional work is required to provide adequate mitigations in this space. ","SoK: The Faults in our ASRs: An Overview of Attacks against Automatic
  Speech Recognition and Speaker Identification Systems"
59,1286323451242917889,2344176362,Dr. Caitlin Casey,"['Hey team! I have a paper out today presenting a new approach to the dreaded ☠️ far-infrared photometric redshift (""FIR-z\'s"") game for distant, dusty galaxies. Here\'s a link and a little explanation of why we need to be more intelligent about FIR-z\'s... 🧵 <LINK>', ""Why use FIR-z's at all?  Well, often the most obscured galaxies in the Universe have a handful of photometric measurements at long wavelengths (~1mm), but no redshift, no optical/near-IR counterpart, nothing.  Learning anything about that source requires some redshift estimate."", ""FIR-z's should ALWAYS be viewed as a method of LAST RESORT for constraining a redshift.  Why?  There is a major degeneracy between a galaxies' bulk (luminosity-weighted) dust temperature and redshift.  And that temperature is mostly determined by complex physics/geometry."", ""The history of FIR-z's goes something like this.  You have some long-wavelength photometric points, you choose your fav galaxy template (like Arp220 here), redshift it, and find a chi^2 minimization.  Out pops a prob. density distribution in z. https://t.co/cnRG9eeE55"", ""That's a decent approach especially when you lack a lot of data (why introduce complex solutions to crappy data?).  But there's a problem if your friend has a different fav galaxy template, like the Pope+08 composite here.  It has a different dust temperature, so different P(z). https://t.co/1tDNLTAyuk"", ""Suddenly you have two different P(z) functions for the two templates.  We all know FIR-z's aren't super good, so maybe average them together? Pick whichever has the best fit to the data?  Play with a few more templates? All seem ~reasonable. BUT WE CAN DO BETTER. https://t.co/pHpDxf3AEr"", ""Fun fact: galaxies' dust SEDs follow a pretty simple trend.  Brighter galaxies tend to have hotter SEDs.  There's a lot of scatter (see gray region), due to variable geometries, sizes, etc., but overall the trend seems to hold across a huge range in redshift. https://t.co/CK5upaXz3n"", ""That background plot is from: https://t.co/zsFrZVHIbm\n\nNow, I realize there's a LONG discussion in the literature about the possible redshift evolution of dust temperature in galaxies.  This plot shows no redshift evolution, at least out to z~5."", 'Redshift evolution of dust temperature is really a topic for another 🧵/paper, but TLDR of my thoughts are:\n* observational evidence for it is slim to none\n* evolution in T of main sequence galaxies is caused by the underlying evolution in SFR with z. No evolution seen with LIR.', ""What I do in this paper is ASSUME galaxies will, in bulk, fall somewhere on the LIR-T_dust relation (within scatter).  Each galaxy's photometry traces out a unique track in this plane as a function of redshift.  Example: MAMBO-9 at z=5.85 (blue track). https://t.co/btlkutULqX"", 'So, instead of using a single galaxy template, you can use the aggregate knowledge of all galaxy SEDs to constrain the redshift using long-wavelength photometry.  *AND* the resulting P(z) distribution should reflect the uncertainty of what dust temperature the source might have.', 'The method, which I call ""MMpz"" for millimeter photo-z, uses the absolute photometry, not just relative colors, to constrain P(z).', 'In other words, if galaxy A has higher flux densities across the board than galaxy B, it is more likely higher LIR, so more likely hotter.  If hotter, P(z) for galaxy A peaks at higher redshift than for galaxy B.', ""And that's MMpz.  It's simple, and like I said above, should be used as a LAST RESORT to constrain a galaxy's redshift.\n\nThe code to run it is available here: https://t.co/WdH4fKi3xW"", ""It's in IDL (#sorry #notsorry).  You don't need to know IDL to run it, because it creates ascii output.  But you do need access to a machine that can run IDL.\n\nIf you want a python version, ask me after I get tenure, or ... DIY (it's pretty simple code)."", ""At some point I'll rant about how being a dinosaur clung to IDL is actually the right strategic choice for me right now (and for the past N years), but that'll have to wait until after my 5281 zooms I gotta go to...""]",https://arxiv.org/abs/2007.11012,"I present a new approach at deriving far-infrared photometric redshifts for galaxies based on their reprocessed emission from dust at rest-frame far-infrared through millimeter wavelengths. Far-infrared photometric redshifts (""FIR-$z$"") have been used over the past decade to derive redshift constraints for highly obscured galaxies that lack photometry at other wavelengths like the optical/near-infrared. Most literature FIR-z fits are performed through $\chi^2$minimization to a single galaxy's far-infrared template spectral energy distribution (SED). The use of a single galaxy template, or modest set of templates, can lead to an artificially low uncertainty estimate on FIR-$z$'s because real galaxies display a wide range in intrinsic dust SEDs. I use the observed distribution of galaxy SEDs (for well-constrained samples across $0<z<5$) to motivate a new far-infrared through millimeter photometric redshift technique called MMpz. The MMpz algorithm asserts that galaxies are most likely drawn from the empirically observed relationship between rest-frame peak wavelength, $\lambda_{\rm peak}$, and total IR luminosity, L$_{\rm IR}$; the derived photometric redshift accounts for the measurement uncertainties and intrinsic variation in SEDs at the inferred L$_{\rm IR}$, as well as heating from the CMB at $z>5$. The MMpz algorithm has a precision of $\sigma_{\Delta z/(1+z)}\approx0.3-0.4$, similar to single-template fits, while providing a more accurate estimate of the FIR-$z$ uncertainty with reduced chi-squared of order $\mathcal{O}(\chi^2_{\nu})=1$, compared to alternative far-infrared photometric redshift techniques (with $\mathcal{O}(\chi^2_{\nu})\approx10-10^{3}$). ","Far-Infrared Photometric Redshifts: A New Approach to a Highly Uncertain
  Enterprise"
60,1286309270242426881,1184838562677710848,Stefano Martiniani,"['New paper on arXiv *Vicsek Model by Time-Interlaced Compression: a Dynamical Computable Information Density*, a collaboration led by Massimiliano Viale, Andrea Puglisi &amp; Andrea Cavagna at @SapienzaRoma \n\n<LINK> <LINK>']",https://arxiv.org/abs/2007.11322,"Collective behavior, both in real biological systems as well as in theoretical models, often displays a rich combination of different kinds of order. A clear-cut and unique definition of ""phase"" based on the standard concept of order parameter may therefore be complicated, and made even trickier by the lack of thermodynamic equilibrium. Compression-based entropies have been proved useful in recent years in describing the different phases of out-of-equilibrium systems. Here, we investigate the performance of a compression-based entropy, namely the Computable Information Density (CID), within the Vicsek model of collective motion. Our entropy is defined through a crude coarse-graining of the particle positions, in which the key role of velocities in the model only enters indirectly through the velocity-density coupling. We discover that such entropy is a valid tool in distinguishing the various noise regimes, including the crossover between an aligned and misaligned phase of the velocities, despite the fact that velocities are not used by this entropy. Furthermore, we unveil the subtle role of the time coordinate, unexplored in previous studies on the CID: a new encoding recipe, where space and time locality are both preserved on the same ground, is demonstrated to reduce the CID. Such an improvement is particularly significant when working with partial and/or corrupted data, as it is often the case in real biological experiments. ","Vicsek Model by Time-Interlaced Compression: a Dynamical Computable
  Information Density"
61,1286223272674308096,91420905,Alex Smith,"['I have another new paper on the arXiv today! “Reducing the Variance of Redshift Space Distortion Measurements from Mock Galaxy Catalogues with Different Lines of Sight""  <LINK> <LINK>', 'In the eBOSS mock challenges, we noticed that different choices of observer position had a surprisingly large effect on the clustering measurements (particularly the quadrupole), leading to large variations in the growth rate measurements', 'The aim of this paper was to understand this. We found that the quadrupole measurements for two different lines of sight are anti-correlated, which leads to the large variations seen. This can be mitigated by averaging over multiple lines of sight', 'In fact, averaging over 3 orthogonal lines of sight reduces the error in the quadrupole measurements by a factor much larger than you would expect if you simply increase the volume by a factor of 3', 'This also propagates through to the growth rate measurements. So averaging clustering measurements over multiple lines of sight will help to constrain models without needing to run more simulations']",https://arxiv.org/abs/2007.11417,"Accurate mock catalogues are essential for assessing systematics in the cosmological analysis of large galaxy surveys. Anisotropic two-point clustering measurements from the same simulation show some scatter for different lines of sight (LOS), but are on average equal, due to cosmic variance. This results in scatter in the measured cosmological parameters. We use the OuterRim N-body simulation halo catalogue to investigate this, considering the 3 simulation axes as LOS. The quadrupole of the 2-point statistics is particularly sensitive to changes in the LOS, with sub-percent level differences in the velocity distributions resulting in ~1.5$\sigma$ shifts on large scales. Averaging over multiple LOS can reduce the impact of cosmic variance. We derive an expression for the Gaussian cross-covariance between the power spectrum multipole measurements, for any two LOS, including shot noise, and the corresponding reduction in variance in the average measurement. Quadrupole measurements are anti-correlated, and for three orthogonal LOS, the variance on the average measurement is reduced by more than 1/3. We perform a Fisher analysis to predict the corresponding gain in precision on the cosmological parameter measurements, which we compare against a set of 300 extended Baryon Oscillation Spectroscopic Survey (eBOSS) emission line galaxy (ELG) EZmocks. The gain in $f\sigma_8$, which measures the growth of structure, is also better than 1/3. Averaging over multiple LOS in future mock challenges will allow the RSD models to be constrained with the same systematic error, with less than 3 times the CPU time. ","Reducing the Variance of Redshift Space Distortion Measurements from
  Mock Galaxy Catalogues with Different Lines of Sight"
62,1286210933187137536,204501916,Francesco Sannino,['Our new paper on the relation between the epidemic renormalisaton group approach and time honored SIR models <LINK>'],https://arxiv.org/abs/2007.11296,"We generalise the epidemic Renormalisation Group framework while connecting it to a SIR model with time-dependent coefficients. We then confront the model with COVID-19 in Denmark, Germany, Italy and France and show that the approach works rather well in reproducing the data. We also show that a better understanding of the time dependence of the recovery rate would require extending the model to take into account the number of deaths whenever these are over 15% of the total number of infected cases. ","Renormalisation Group approach to pandemics as a time-dependent SIR
  model"
63,1286195018890387456,1163883824,Shivangee Rathi,"['New paper on dwarf galaxies! (and a first for me) in collaboration with Michele Mastropietro (@mic_mas), Sven De Rijcke, Carmen Gallart, Edouard Bernard and Robbert Verbeke (@Rbhfd): ""Observations"" of simulated dwarf galaxies (<LINK>).', 'In this paper, we observationally analyze a set of realistically simulated MoRIA dwarf galaxies (from the work of @Rbhfd), to look for any systematic bias in comparison of simulations with observations.', 'In particular, we use the synthetic color-magnidue diagram (CMD) technique to reconstruct the star formation history (SFH) of dwarf galaixes. We construct CMDs of simulated dwarfs from simulation star particle data and add observational errors to mimic real observations.', 'On comparing the reconstructed SFH from the synthetic CMD method with the ground truth from the simulation star particle data, we overall find a good agreement.', 'Our paper also explores:\n1) the effect of dust extinction on the CMD, and hence on the reconstructed SFH, and\n2) the dependence on the SFH on the aperture used.\n\nWe also analyze infrared CMDs, in view of the next generation astronomical facilities.']",https://arxiv.org/abs/2007.11413,"Apparent deviations between properties of dwarf galaxies from observations and simulations are known to exist, such as the ""Missing Dwarfs"" problem, the too-big-to-fail problem, and the cusp-core problem, to name a few. Recent studies have shown that these issues can at least be partially resolved by taking into account the systematic differences between simulations and observations. This work aims to investigate and address any systematic differences affecting the comparison of simulations with observations. To this aim, we analyzed a set of 24 realistically simulated MoRIA (Models of Realistic dwarfs In Action) dwarf galaxies in an observationally motivated way. We first constructed ""observed"" color-magnitude diagrams (CMDs) of the simulated dwarf galaxies in the typically used V- and I-bands. Then we used the CMD-fitting method to recover their star-formation histories (SFHs) from their observed CMDs. These solved SFHs were then directly compared to the true SFHs from the simulation star-particle data, mainly in terms of the star-formation rate(SFR) and the age-metallicity relation (AMR). We applied a dust extinction prescription to the simulation data to produce observed CMDs affected by dust in star-formation regions. Since future facilities, such as the JWST and E-ELT will focus on the near IR rather than the optical, we also constructed and analyzed CMDs using the I- and H-bands. We find a very good agreement between the recovered and the true SFHs of all the simulated dwarf galaxies in our sample, from the synthetic CMD analysis of their V-I versus I as well as the I-H versus H CMDs. Dust leads to an underestimation of the SFR during the last few hundred million years. Overall, our analysis indicates that quantities like SFR and AMR derived from the photometric observations of galaxies are directly comparable to their simulated counterparts. ","""Observations"" of simulated dwarf galaxies: Star-formation histories
  from color-magnitude diagrams"
64,1286173398238785539,962876421268914177,Hanlin Ren,"[""<LINK>\nNew paper on Distance Sensitivity Oracles! Compared to the previous state-of-the-art [Chechik &amp; Cohen, STOC'20], our DSO has better preprocessing time and small space, but we think the main advantage of our result is its **simplicity**."", ""In case you're not familiar with the problem: You are given a directed graph, and you want to preprocess it and answer the following queries: Given vertices u, v, x, what is the length of the shortest path from u to v **not going through x**?"", ""Let me emphasize a **drawback** of this result: I don't know how to extend it to handle negative edge weights. In contrast, all previous works ([Weimann &amp; Yuster, Grandoni &amp; Williams, Chechik &amp; Cohen]) can handle negative edge weights. See the paper for more details."", '@ccanonne_ Emmm, I actually don\'t know... Two of the three ESA reviewers suggested me to also think about negative edge weights (which I was totally ignorant when I submit the paper to ESA). I thought it ""should be easy"", but I failed to solve it...']",https://arxiv.org/abs/2007.11495,"We consider the problem of building Distance Sensitivity Oracles (DSOs). Given a directed graph $G=(V, E)$ with edge weights in $\{1, 2, \dots, M\}$, we need to preprocess it into a data structure, and answer the following queries: given vertices $u,v\in V$ and a failed vertex or edge $f\in (V\cup E)$, output the length of the shortest path from $u$ to $v$ that does not go through $f$. Our main result is a simple DSO with $\tilde{O}(n^{2.7233}M)$ preprocessing time and $O(1)$ query time. Moreover, if the input graph is undirected, the preprocessing time can be improved to $\tilde{O}(n^{2.6865}M)$. The preprocessing algorithm is randomized with correct probability $\ge 1-1/n^C$, for a constant $C$ that can be made arbitrarily large. Previously, there is a DSO with $\tilde{O}(n^{2.8729}M)$ preprocessing time and $\operatorname{polylog}(n)$ query time [Chechik and Cohen, STOC'20]. At the core of our DSO is the following observation from [Bernstein and Karger, STOC'09]: if there is a DSO with preprocessing time $P$ and query time $Q$, then we can construct a DSO with preprocessing time $P+\tilde{O}(n^2)\cdot Q$ and query time $O(1)$. (Here $\tilde{O}(\cdot)$ hides $\operatorname{polylog}(n)$ factors.) ",Improved Distance Sensitivity Oracles with Subcubic Preprocessing Time
65,1286012927816818688,289494499,"Adi Foord, PhD","['Check out this interesting new paper, led by @KateNapier7 👀 \n\n<LINK>\n\nIt’s a great example of BAYMAX analyzing a non-dual AGN source ... there’s a lot science the tool could be used for!']",https://arxiv.org/abs/2007.10368,"We report on deep Chandra X-ray Telescope imaging observations of 4C 63.20, one of the few known radio galaxies at z>3.5. The X-ray counterpart is resolved into a core plus two off-nuclear sources that (combined) account for close to 30% of the total X-ray flux. Their morphology and orientation are consistent with a diffuse, lobe-like nature, albeit compact hotspots cannot be ruled out. The broadband spectral energy distribution of 4C 63.20 can be reproduced with a jet model where the majority of the radio flux can be ascribed to synchrotron emission from the hotspots, whereas the (non-nuclear) X-ray emission is produced via Inverse Compton (IC) off of Cosmic Microwave Background (CMB) photons within the extended lobes. This scenario is broadly consistent with the expectation from highly magnetized lobes in a hotter CMB, and supports the view that IC/CMB may quench less extreme radio lobes at high redshifts. ",Extended X-ray emission from the z=4.26 radio galaxy 4C 63.20
66,1285982174517309440,908158289162194945,Aaron Tohuvavohu,['On our new paper led most excellently by Fe\n<LINK> <LINK>'],https://arxiv.org/abs/2007.10193,"High-energy neutrinos are a promising tool for identifying astrophysical sources of high and ultra-high energy cosmic rays (UHECR). Prospects of detecting neutrinos at high energies ($\gtrsim$TeV) from blazars have been boosted after the recent association of IceCube-170922A and TXS 0506+056. We investigate the high-energy neutrino, IceCube-190331A, a high-energy starting event (HESE) with a high likelihood of being astrophysical in origin. We initiated a Swift/XRT and UVOT tiling mosaic of the neutrino localisation, and followed up with ATCA radio observations, compiling a multiwavelength SED for the most likely source of origin. NuSTAR observations of the neutrino location and a nearby X-ray source were also performed. We find two promising counterpart in the 90% confidence localisation region and identify the brightest as the most likely counterpart. However, no Fermi/LAT $\gamma$-ray source and no prompt Swift/BAT source is consistent with the neutrino event. At this point it is unclear whether any of the counterparts produced IceCube-190331A. We note that the Helix Nebula is also consistent with the position of the neutrino event, and we calculate that associated particle acceleration processes cannot produce the required energies to generate a high-energy HESE neutrino. ",Multimessenger observations of counterparts to IceCube-190331A
67,1285935591230767105,1064262393981820928,Benedikt Diemer,"['Paper day #2: <LINK>\n\nBased on the new catalogs from yesterday, we look at the mass functions of splashback and SO masses. The splashback MF turns out to be remarkably universal! The image compares the MF for totally different cosmologies and redshifts. <LINK>', 'A little background: the first theoretical model of the mass function, Press &amp; Schechter 74, assumed that halos collapse when their progenitor peaks exceed a critical overdensity, delta_c. If delta_c is universal (not dependent on z and cosmology), then the MF is universal too.', 'There are good reasons to believe this should not be the case! Nevertheless, ever since, people have argued about universality. \n\nOne important aspect is that, for a universal MF, the mass definition would need to capture the ""total mass"" of halos in some physical manner.', ""There have been many claims of (non-)universality. In this paper, we show conclusively that SO mass functions are NOT universal for any definition (R500c, R200c, Rvir, R200m etc). The splashback MFs aren't perfectly universal either, but much more so!"", 'In particular, they more or less agree even between crazy self-similar universes and LCDM, plus across redshifts in the presence of dark energy.', 'In my mind (but feel free to challenge me on this!), that indicates that splashback masses are a more physical definition; or at least that they come closer to including the entire mass of the halo in the sense of spherical collapse envisioned by theoretical models.', ""Here's the figure with a legend: https://t.co/Ucvwj4k0Sw"", ""@profbradgibson Haha yes Sir! I believe you have everything that's gonna come your way, for now ;)""]",https://arxiv.org/abs/2007.10346,"The mass function of dark matter halos is one of the most fundamental statistics in structure formation. Many theoretical models (such as Press-Schechter theory) are based on the notion that it could be universal, meaning independent of redshift and cosmology, when expressed in the appropriate variables. However, simulations exhibit persistent non-universalities in the mass functions of the virial mass and other commonly used spherical overdensity definitions. We systematically study the universality of mass functions over a wide range of mass definitions, for the first time including the recently proposed splashback mass, Msp. We confirm that, in LambdaCDM cosmologies, all mass definitions exhibit varying levels of non-universality that increase with peak height and reach between 20% and 500% at the highest masses we can test. Mvir, M200m, and Msp exhibit similar levels of non-universality. There are, however, two regimes where the splashback mass functions are significantly more universal. First, they are universal to 10% at z<2, whereas spherical overdensity definitions experience an evolution due to dark energy. Second, when additionally considering self-similar cosmologies with extreme power spectra, splashback mass functions are remarkably universal (to between 40% and 60%) whereas their spherical overdensity counterparts reach non-universalities between 180% and 450%. These results strongly support the notion that the splashback radius is a physically motivated definition of the halo boundary. We present a simple, universal fitting formula for splashback mass functions that accurately reproduces our simulation data. ",Universal at last? The splashback mass function of dark matter halos
68,1285868019361689600,485445438,Dario Bercioux PhD,['We posted on the arXiv a new paper where we look in-depth at the properties of corner stated in photonic systems with high-order topology. Our results have implications for some electronic system as well. <LINK>'],https://arxiv.org/abs/2007.10624,"We analyze the robustness of corner modes in topological photonic crystals, taking a $C_6$-symmetric breathing honeycomb photonic crystal as an example. First, we employ topological quantum chemistry and Wilson loop calculations to demonstrate that the topological properties of the bulk crystal stem from an obstructed atomic limit phase. We then characterize the topological corner modes emerging within the gapped edge modes employing a semi-analytical model, determining the appropriate real space topological invariants. For the first time, we provide a detailed account of the effect of long-range interactions on the topological modes in photonic crystals, and we quantify their robustness to perturbations. We conclude that, while photonic long-range interactions inevitably break chiral symmetry, the corner modes are protected by lattice symmetries. ",On the robustness of topological corner modes in photonic crystals
69,1285857096211353601,1285850193615884289,Simone Balloccu,"['New paper!\n""How are you? Introducing stress-based text tailoring\n"" at IntelLanG 2020.\n<LINK>']",https://arxiv.org/abs/2007.09970,"Can stress affect not only your life but also how you read and interpret a text? Healthcare has shown evidence of such dynamics and in this short paper we discuss customising texts based on user stress level, as it could represent a critical factor when it comes to user engagement and behavioural change. We first show a real-world example in which user behaviour is influenced by stress, then, after discussing which tools can be employed to assess and measure it, we propose an initial method for tailoring the document by exploiting complexity reduction and affect enforcement. The result is a short and encouraging text which requires less commitment to be read and understood. We believe this work in progress can raise some interesting questions on a topic that is often overlooked in NLG. ",How are you? Introducing stress-based text tailoring
70,1285826616510353409,822867138,Bradley Kavanagh,"['New paper with @ultra_wimp: Primordial Black Holes as a dark matter candidate (<LINK>)\n\nCan black holes formed in the early Universe constitute the Dark Matter? Possibly.\n\nDid the world need another PBH review? Absolutely. <LINK>', 'We give an overview of how such PBHs can form, focusing on the collapse of large density fluctuations in the early Universe. \n\nThen summarise current straights on the PBH abundance (as a fraction of the Dark Matter abundance in the Universe): https://t.co/7TOOv2tu7U', 'We digitised *a lot* of PBH bounds. You can find them all on line, along with code for generating the PBH abundance plots: https://t.co/HsCap8Rvup\n\nSpecial thanks to @adamcoogan1 for constantly letting me know whenever a new PBH paper came out...', ""I'm very grateful to @ultra_wimp for bringing me on board with this. \n\nShe did the lion's share of the work, but I was happy to help out with a fresh pair of eyes and my superhuman ability to click on lines in plots to digitise them."", '@ultra_wimp Oh I forgot to tell you, in the last read through, I corrected some ""f_{\\rm co}"" to ""f_{\\rm CO}"", so I consider my contribution invaluable.', '@cajohare @ultra_wimp https://t.co/fQI4tB1h0W']",https://arxiv.org/abs/2007.10722,"The detection of gravitational waves from mergers of tens of Solar mass black hole binaries has led to a surge in interest in Primordial Black Holes (PBHs) as a dark matter candidate. We aim to provide a (relatively) concise overview of the status of PBHs as a dark matter candidate, circa Summer 2020. First we review the formation of PBHs in the early Universe, focusing mainly on PBHs formed via the collapse of large density perturbations generated by inflation. Then we review the various current and future constraints on the present day abundance of PBHs. We conclude with a discussion of the key open questions in this field. ",Primordial Black Holes as a dark matter candidate
71,1285772807553482752,1150920172657500161,Kunal Gupta,"['Excited to announce our new work!\n“Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows”\n\nGenerate physically realizable meshes that can be 3D printed, or used in physics simulation\n\nPaper: <LINK>\nProject Page: <LINK> <LINK>', 'Video: https://t.co/m6bIi2BxUz']",https://arxiv.org/abs/2007.10973,"Meshes are important representations of physical 3D entities in the virtual world. Applications like rendering, simulations and 3D printing require meshes to be manifold so that they can interact with the world like the real objects they represent. Prior methods generate meshes with great geometric accuracy but poor manifoldness. In this work, we propose Neural Mesh Flow (NMF) to generate two-manifold meshes for genus-0 shapes. Specifically, NMF is a shape auto-encoder consisting of several Neural Ordinary Differential Equation (NODE)[1] blocks that learn accurate mesh geometry by progressively deforming a spherical mesh. Training NMF is simpler compared to state-of-the-art methods since it does not require any explicit mesh-based regularization. Our experiments demonstrate that NMF facilitates several applications such as single-view mesh reconstruction, global shape parameterization, texture mapping, shape deformation and correspondence. Importantly, we demonstrate that manifold meshes generated using NMF are better-suited for physically-based rendering and simulation. Code and data are released. ",Neural Mesh Flow: 3D Manifold Mesh Generation via Diffeomorphic Flows
72,1285757121053614080,1018734414245720065,"Lee Altenberg, Ph.D.","[""My new paper with Joel E. Cohen on nonconcavity in the remarkably obscure Levinger's theorem has just been accepted by Linear Algebra and Its Applications: \n<LINK> <LINK>""]",http://arxiv.org/abs/2007.02618,"Let ${\bf A} \in R^{n \times n}$ be a nonnegative irreducible square matrix and let $r({\bf A})$ be its spectral radius and Perron-Frobenius eigenvalue. Levinger asserted and several have proven that $r(t):=r((1{-}t) {\bf A} + t {\bf A}^\top)$ increases over $t \in [0,1/2]$ and decreases over $t \in [1/2,1]$. It has further been stated that $r(t)$ is concave over $t \in (0,1)$. Here we show that the latter claim is false in general through a number of counterexamples, but prove it is true for ${\bf A} \in R^{2\times 2}$, weighted shift matrices (but not cyclic weighted shift matrices), tridiagonal Toeplitz matrices, and the 3-parameter Toeplitz matrices from Fiedler, but not Toeplitz matrices in general. A general characterization of the range of $t$, or the class of matrices, for which the spectral radius is concave in Levinger's homotopy remains an open problem. ",Nonconcavity of the Spectral Radius in Levinger's Theorem
73,1285736873470627841,79272029,Inês Hipólito,"['Grateful to take part and be able to share this work - a project with so many brilliant researchers, such as @adeelrazi, led by Karl Friston. This paper is a brand new technical note on Markov blankets and brain organisation\n<LINK>', '- Dare I say a new wave in brain research away from brain mapping and towards the emergence of intrinsic brain dynamics.']",https://arxiv.org/abs/2007.09704?fbclid=IwAR2G2-lD5QqeUtecPO9iWiJ3wxEsSpjCx0Bn-ts3X-r7IRn8-QWkLdsS7w0,"At the inception of human brain mapping, two principles of functional anatomy underwrote most conceptions - and analyses - of distributed brain responses: namely functional segregation and integration. There are currently two main approaches to characterising functional integration. The first is a mechanistic modelling of connectomics in terms of directed effective connectivity that mediates neuronal message passing and dynamics on neuronal circuits. The second phenomenological approach usually characterises undirected functional connectivity (i.e., measurable correlations), in terms of intrinsic brain networks, self-organised criticality, dynamical instability, etc. This paper describes a treatment of effective connectivity that speaks to the emergence of intrinsic brain networks and critical dynamics. It is predicated on the notion of Markov blankets that play a fundamental role in the self-organisation of far from equilibrium systems. Using the apparatus of the renormalisation group, we show that much of the phenomenology found in network neuroscience is an emergent property of a particular partition of neuronal states, over progressively larger scales. As such, it offers a way of linking dynamics on directed graphs to the phenomenology of intrinsic brain networks. ",Parcels and particles: Markov blankets in the brain
74,1285662107942428673,147411178,spencer woody,['🚨📝 new paper out: Estimating heterogeneous effects of continuous exposures using Bayesian tree ensembles <LINK>'],https://arxiv.org/abs/2007.09845,"In estimating the causal effect of a continuous exposure or treatment, it is important to control for all confounding factors. However, most existing methods require parametric specification for how control variables influence the outcome or generalized propensity score, and inference on treatment effects is usually sensitive to this choice. Additionally, it is often the goal to estimate how the treatment effect varies across observed units. To address this gap, we propose a semiparametric model using Bayesian tree ensembles for estimating the causal effect of a continuous treatment of exposure which (i) does not require a priori parametric specification of the influence of control variables, and (ii) allows for identification of effect modification by pre-specified moderators. The main parametric assumption we make is that the effect of the exposure on the outcome is linear, with the steepness of this relationship determined by a nonparametric function of the moderators, and we provide heuristics to diagnose the validity of this assumption. We apply our methods to revisit a 2001 study of how abortion rates affect incidence of crime. ","Estimating heterogeneous effects of continuous exposures using Bayesian
  tree ensembles: revisiting the impact of abortion rates on crime"
75,1285645483931729921,977906884886827008,Marcos Mariño,"['Quantum invariants of hyperbolic knots are similar in many ways to classical functions, but they have a mysterious additional structure which has puzzled me for years. In my new paper with Stavros Garoufalidis and Jie Gu we unveil part of this structure <LINK>']",https://arxiv.org/abs/2007.10190,"The asymptotic expansion of quantum knot invariants in complex Chern-Simons theory gives rise to factorially divergent formal power series. We conjecture that these series are resurgent functions whose Stokes automorphism is given by a pair of matrices of $q$-series with integer coefficients, which are determined explicitly by the fundamental solutions of a pair of linear $q$-difference equations. We further conjecture that for a hyperbolic knot, a distinguished entry of those matrices equals to the Dimofte-Gaiotto-Gukov 3D-index, and thus is given by a counting of BPS states. We illustrate our conjectures explicitly by matching theoretically and numerically computed integers for the cases of the $4_1$ and the $5_2$ knots. ",The resurgent structure of quantum knot invariants
76,1285593308627193858,22216766,Noah Stephens-Davidowitz,"[""New paper with new co-author Zeyong Li (and old co-author Divesh Aggarwal). D and I have been trying to prove for years that this algorithm solves SVP. In this paper, we fail to prove that, but prove something that's sort of good enough for applications!\n<LINK>""]",https://arxiv.org/abs/2007.09556,"We show a $2^{n/2+o(n)}$-time algorithm that finds a (non-zero) vector in a lattice $\mathcal{L} \subset \mathbb{R}^n$ with norm at most $\tilde{O}(\sqrt{n})\cdot \min\{\lambda_1(\mathcal{L}), \det(\mathcal{L})^{1/n}\}$, where $\lambda_1(\mathcal{L})$ is the length of a shortest non-zero lattice vector and $\det(\mathcal{L})$ is the lattice determinant. Minkowski showed that $\lambda_1(\mathcal{L}) \leq \sqrt{n} \det(\mathcal{L})^{1/n}$ and that there exist lattices with $\lambda_1(\mathcal{L}) \geq \Omega(\sqrt{n}) \cdot \det(\mathcal{L})^{1/n}$, so that our algorithm finds vectors that are as short as possible relative to the determinant (up to a polylogarithmic factor). The main technical contribution behind this result is new analysis of (a simpler variant of) an algorithm from arXiv:1412.7994, which was only previously known to solve less useful problems. To achieve this, we rely crucially on the ``reverse Minkowski theorem'' (conjectured by Dadush arXiv:1606.06913 and proven by arXiv:1611.05979), which can be thought of as a partial converse to the fact that $\lambda_1(\mathcal{L}) \leq \sqrt{n} \det(\mathcal{L})^{1/n}$. Previously, the fastest known algorithm for finding such a vector was the $2^{.802n + o(n)}$-time algorithm due to [Liu, Wang, Xu, and Zheng, 2011], which actually found a non-zero lattice vector with length $O(1) \cdot \lambda_1(\mathcal{L})$. Though we do not show how to find lattice vectors with this length in time $2^{n/2+o(n)}$, we do show that our algorithm suffices for the most important application of such algorithms: basis reduction. In particular, we show a modified version of Gama and Nguyen's slide-reduction algorithm [Gama and Nguyen, STOC 2008], which can be combined with the algorithm above to improve the time-length tradeoff for shortest-vector algorithms in nearly all regimes, including the regimes relevant to cryptography. ","A $2^{n/2}$-Time Algorithm for $\sqrt{n}$-SVP and $\sqrt{n}$-Hermite
  SVP, and an Improved Time-Approximation Tradeoff for (H)SVP"
77,1285591881108389888,857310777413443585,Sheila McBreen,['New @AandA_journal paper by Sarah Walsh @astroucd @ucdscience in collaboration with @DidierBarret and colleagues @DrRemeis and Leiden Observatory @unileidennews on the capabilities of @esascience @AthenaXIFU. Funded by @esascience PRODEX @EI_BRodgers. \n<LINK> <LINK>'],https://arxiv.org/abs/2007.10158,"At low redshifts, the observed baryonic density falls far short of the total number of baryons predicted. Cosmological simulations suggest that these baryons reside in filamentary gas structures, known as the warm-hot intergalactic medium (WHIM). As a result of the high temperatures of these filaments, the matter is highly ionised such that it absorbs and emits far-UV and soft X-ray photons. Athena, the proposed European Space Agency X-ray observatory, aims to detect the `missing' baryons in the WHIM up to redshifts of $z=1$ through absorption in active galactic nuclei and gamma-ray burst afterglow spectra, allowing for the study of the evolution of these large-scale structures of the Universe. This work simulates WHIM filaments in the spectra of GRB X-ray afterglows with Athena using the SImulation of X-ray TElescopes (SIXTE) framework. We investigate the feasibility of their detection with the X-IFU instrument, through O VII ($E=573$ eV) and O VIII ($E=674$ eV) absorption features, for a range of equivalent widths imprinted onto GRB afterglow spectra of observed starting fluxes ranging between $10^{-12}$ and $10^{-10}$ erg cm$^{-2}$ s$^{-1}$, in the 0.3-10 keV energy band. The analyses of X-IFU spectra by blind line search show that Athena will be able to detect O VII-O VIII absorption pairs with EW$_\mathrm{O VII} > 0.13$ eV and EW$_\mathrm{O VIII} > 0.09$ eV for afterglows with $F>2 \times 10^{-11}$ erg cm$^{-2}$ s$^{-1}$. This allows for the detection of $\approx$ 45-137 O VII-O VIII absorbers during the four-year mission lifetime. The work shows that to obtain an O VII-O VIII detection of high statistical significance, the local hydrogen column density should be limited at $N_\mathrm{H}<8 \times 10^{20}$ cm$^{-2}$. ","Detection capabilities of the Athena X-IFU for the warm-hot
  intergalactic medium using gamma-ray burst X-ray afterglows"
78,1285536177580277760,28734416,Sebastian Risi,"['Happy to present a new game we developed ""iNNk: A Multi-Player Game to Deceive a Neural Network"" \n\n<LINK>\nPaper: <LINK>\n\nPlayers need to communicate a secret code word to each other through drawings, without being deciphered by the neural network', 'A great collaboration between @DrexelUniv and @ITUkbh w/ @jmvillareale, Ana Acosta-Ruiz, Samuel Arcaro, Thomas Fox, Evan Freed, Robert Gray, @mathiasloewe, Panote Nuchprayoon, Aleksanteri Sladek, Rush Weigelt, Yifu Li, and @jichenz', 'With this game, we aim to foster a playful environment where players can, in a small way, go from passive consumers of neural network applications to creative thinkers and critical challengers. We show that players develop interesting strategies to fool the network. https://t.co/n9chRNkbMm', 'Our game was inspired by Google\'s ""Quick, Draw!"", adding  strong game mechanics to encourage rivalry between human players and the neural network.', ""@Jallafsen That is the plan 😈, but we're still tweaking the retraining part"", ""@guillefix Yes, that's the plan!"", '@anandsriraman @guillefix Thanks! We will have a look.', 'And of course @hardmaru and @douglas_eck work on sketch-rnn, which I now notice we missed to cite but will add.']",https://arxiv.org/abs/2007.09177,"This paper presents iNNK, a multiplayer drawing game where human players team up against an NN. The players need to successfully communicate a secret code word to each other through drawings, without being deciphered by the NN. With this game, we aim to foster a playful environment where players can, in a small way, go from passive consumers of NN applications to creative thinkers and critical challengers. ",iNNk: A Multi-Player Game to Deceive a Neural Network
79,1285459944096104455,837147913,Alvaro Valcarce,"['Who is best at building communication protocols? Humans or machines?\nFind out in our new paper, ""Towards Joint Learning of Optimal Signaling and Wireless Channel Access""\n<LINK> <LINK>']",https://arxiv.org/abs/2007.09948,"Communication protocols are the languages used by network nodes. Before a user equipment (UE) can exchange data with a base station (BS), it must first negotiate the conditions and parameters for that transmission. This negotiation is supported by signaling messages at all layers of the protocol stack. Each year, the mobile communications industry defines and standardizes these messages, which are designed by humans during lengthy technical (and often political) debates. Following this standardization effort, the development phase begins, wherein the industry interprets and implements the resulting standards. But is this massive development undertaking the only way to implement a given protocol? We address the question of whether radios can learn a pre-given target protocol as an intermediate step towards evolving their own. Furthermore, we train cellular radios to emerge a channel access policy that performs optimally under the constraints of the target protocol. We show that multi-agent reinforcement learning (MARL) and learning-to-communicate (L2C) techniques achieve this goal with gains over expert systems. Finally, we provide insight into the transferability of these results to scenarios never seen during training. ","Towards Joint Learning of Optimal MAC Signaling and Wireless Channel
  Access"
80,1285436921150410753,963909703,🚀 Anders Christensen,"['Fresh new paper on arXiv by me and @ProfvLilienfeld: \n\n""On the role of gradients for machine learning of molecular energies and forces"" <LINK> \n\nThe paper also includes an updated (and noise-free) dataset for forces: <LINK>', '@MicheleCeriotti @ProfvLilienfeld We did not test different choices of basis functions explicitly, but Fig. 3(D) does not seem to show any difference between different choices of basis functions.']",https://arxiv.org/abs/2007.09593,"The accuracy of any machine learning potential can only be as good as the data used in the fitting process. The most efficient model therefore selects the training data that will yield the highest accuracy compared to the cost of obtaining the training data. We investigate the convergence of prediction errors of quantum machine learning models for organic molecules trained on energy and force labels, two common data types in molecular simulations. When training and predicting on different geometries corresponding to the same single molecule, we find that the inclusion of atomic forces in the training data increases the accuracy of the predicted energies and forces 7-fold, compared to models trained on energy only. Surprisingly, for models trained on sets of organic molecules of varying size and composition in non-equilibrium conformations, inclusion of forces in the training does not improve the predicted energies of unseen molecules in new conformations. Predicted forces, however, also improve about 7-fold. For the systems studied, we find that force labels and energy labels contribute equally per label to the convergence of the prediction errors. Choosing to include derivatives such as atomic forces in the training set or not should thus depend on, not only on the computational cost of acquiring the force labels for training, but also on the application domain, the property of interest, and the desirable size of the machine learning model. Based on our observations we describe key considerations for the creation of datasets for potential energy surfaces of molecules which maximize the efficiency of the resulting machine learning models. ","On the role of gradients for machine learning of molecular energies and
  forces"
81,1285400841810145280,913238472357437445,Fuminobu TAKAHASHI,['Our new paper is out today. The ALP mass of keV and the decay constant of 10^9 GeV suggested by the XENON1T excess satisfy the consistency relation predicted by the ALP inflation model.  We studied the implication for thermal history after inflation.\n\n<LINK>'],https://arxiv.org/abs/2007.10311,"The recent XENON1T excess in the electron recoil data can be explained by anomaly-free axion-like particle (ALP) dark matter with mass $m_\phi = 2.3 \pm 0.2\,$keV and the decay constant $f_\phi/q_e \simeq 2 \times 10^{10} \sqrt{\Omega_\phi/\Omega_{\rm DM}}\,{\rm GeV}$. Intriguingly, the suggested mass and decay constant are consistent with the relation, $f_\phi \sim 10^3 \sqrt{m_\phi M_p}$, predicted in a scenario where the ALP plays the role of the inflaton. This raises a possibility that the ALP dark matter responsible for the XENON1T excess also drove inflation in the very early universe. We study implications of the XENON1T excess for the ALP inflation and thermal history of the universe after inflation. ",What if ALP dark matter for the XENON1T excess is the inflaton
82,1285399922741637120,1246812470968188929,Isabel Scherl,['New paper out on the @arxiv on cross-flow turbine (i.e. vertical axis wind turbine or VAWT) array optimization with @eigensteve and others! \n\nGeometric and Control Optimization of a Two Cross-Flow Turbine Array: <LINK> <LINK>'],https://arxiv.org/abs/2007.09233,"Cross-flow turbines, also known as vertical-axis turbines, convert the kinetic energy in moving fluid to mechanical energy using blades that rotate about an axis perpendicular to the incoming flow. In this work, the performance of a two-turbine array in a recirculating water channel was experimentally optimized across sixty-four unique array configurations. For each configuration, turbine performance was optimized using tip-speed ratio control, where the rotation rate for each turbine is optimized individually, and using coordinated control, where the turbines are optimized to operate at synchronous rotation rates, but with a phase difference. For each configuration and control strategy, the consequences of co- and counter-rotation were also evaluated. We hypothesize how array configurations and control cases influence interactions between turbines and impact array performance. ",Geometric and Control Optimization of a Two Cross-Flow Turbine Array
83,1285397934805319681,60724221,Maximiliano Isi,['Sometimes the simplest ideas are the most useful (and overlooked!) That’s the case here. Great work by @ScienceMIT grad student @sylvia_bisco on our most recent paper: A New Spin on @LIGO-Virgo Binary Black Holes <LINK> <LINK>'],https://arxiv.org/abs/2007.09156,"Gravitational waves from binary black holes have the potential to yield information on both of the intrinsic parameters that characterize the compact objects: their masses and spins. While the component masses are usually resolvable, the component spins have proven difficult to measure. This limitation stems in great part from our choice to inquire about the spins of the most and least massive objects in each binary, a question that becomes ill-defined when the masses are equal. In this paper we show that one can ask a different question of the data: what are the spins of the objects with the highest and lowest dimensionless spins in the binary? We show that this can significantly improve estimates of the individual spins, especially for binary systems with comparable masses. When applying this parameterization to the first 13 gravitational-wave events detected by the LIGO-Virgo collaboration (LVC), we find that the highest-spinning object is constrained to have nonzero spin for most sources and to have significant support at the Kerr limit for GW151226 and GW170729. A joint analysis of all the confident binary black hole detections by the LVC finds that, unlike with the traditional parametrization, the distribution of spin magnitude for the highest-spinning object has negligible support at zero spin. Regardless of the parameterization used, the configuration where all of the spins in the population are aligned with the orbital angular momentum is excluded from the 90% credible interval for the first ten events and from the 99% credible interval for all current confident detections. ",A new spin on LIGO-Virgo binary black holes
84,1285380994166595587,939589825602228224,Leah Jenks,"['New paper on the arXiv this evening! Looking at gravitational wave and binary pulsar constraints on noncommutative gravity                   <LINK>', 'We find that GW constraints are an order of magnitude more stringent than those from the pulsar system and that the time scale of the normalized NC tensor is constrained to be of order unity', 'With my wonderful advisor Stephon Alexander and awesome collaborator Kent Yagi!']",https://arxiv.org/abs/2007.09714,"Noncommutative gravity is a natural method of quantizing spacetime by promoting the spacetime coordinates themselves to operators which do not commute. This approach is motivated, for example, from a quantum gravity perspective, among others. Noncommutative gravity has been tested against the binary black hole merger event GW150914. Here, we extend and improve upon such a previous analysis by (i) relaxing an assumption made on the preferred direction due to noncommutativity, (ii) using posterior samples produced by the LIGO/Virgo Collaborations, (iii) consider other gravitational wave events, namely GW151226, GW170608, GW170814 and GW170817, and (iv) consider binary pulsar observations. Using Kepler's law that contains the noncommutative effect at second post-Newtonian order, we derive corrections to the gravitational waveform phase and the pericenter precession. Using the gravitational wave and double pulsar binary observations, we find bounds on a space-time noncommutative tensor $\theta^{0i}$ in terms of the preferred frame direction with respect to the orientation of each binary. We find that the gravitational wave bounds are stronger than the binary pulsar one by an order of magnitude and the noncommutative tensor normalized by the Planck length and time is constrained to be of order unity. ","Probing Noncommutative Gravity with Gravitational Wave and Binary Pulsar
  Observations"
85,1285379427250167811,3434576657,Tongyang Li,['New paper with Chenyi Zhang and @jiaqi_leng on quantum algorithms for escaping from saddle points: <LINK> Congrats Chenyi and Jiaqi for having the first paper on arXiv!'],https://arxiv.org/abs/2007.10253,"We initiate the study of quantum algorithms for escaping from saddle points with provable guarantee. Given a function $f\colon\mathbb{R}^{n}\to\mathbb{R}$, our quantum algorithm outputs an $\epsilon$-approximate second-order stationary point using $\tilde{O}(\log^{2} (n)/\epsilon^{1.75})$ queries to the quantum evaluation oracle (i.e., the zeroth-order oracle). Compared to the classical state-of-the-art algorithm by Jin et al. with $\tilde{O}(\log^{6} (n)/\epsilon^{1.75})$ queries to the gradient oracle (i.e., the first-order oracle), our quantum algorithm is polynomially better in terms of $\log n$ and matches its complexity in terms of $1/\epsilon$. Technically, our main contribution is the idea of replacing the classical perturbations in gradient descent methods by simulating quantum wave equations, which constitutes the improvement in the quantum query complexity with $\log n$ factors for escaping from saddle points. We also show how to use a quantum gradient computation algorithm due to Jordan to replace the classical gradient queries by quantum evaluation queries with the same complexity. Finally, we also perform numerical experiments that support our theoretical findings. ",Quantum algorithms for escaping from saddle points
86,1285379380047470593,91634245,Brad Marston,"['Our new paper on transport in open quantum systems is now posted on the @arxiv.  Zekun Zhuang, a PhD student in my group, is the first author.  It was pleasure working with him and Jaime Merino.  <LINK>']",https://arxiv.org/abs/2007.09810,"We derive a closed equation of motion for the one particle density matrix of a quantum system coupled to multiple baths using the Redfield master equation combined with a mean-field approximation. The steady-state solution may be found analytically with perturbation theory. Application of the method to a one-dimensional non-interacting quantum wire yields an expression for the current that reproduces the celebrated Landauer's formula. Nonlinear rectification is found for the case of a mesoscopic three-dimensional semiconductor p-n junction. The results are in good agreement with numerical simulations obtained using non-equilibrium Green's functions, supporting the validity of the Redfield equations for the description of transport. ","Transport in Conductors and Rectifiers: Mean-Field Redfield Equations
  and Non-Equilibrium Green's Functions"
87,1285297480385650689,193887081,Marc van Zee,"['New paper! ""Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures"". We assess techniques/architectures\'s effectiveness in improving compositional generalization based on the SCAN and CFQ datasets. (<LINK>) Highlights [1/5]:', '(1) We provide the most comprehensive summary so far of architectures and techniques that have been applied to SCAN or CFQ [2/5] https://t.co/VfURdjki4C', '(2) Pre-training (using T5) helps for compositional generalization, but does not solve it. By combining pre-training with an intermediate representation, we obtain a new SOTA score for CFQ of 42.1% on the MCD-mean split, beating the previous best results of 18.9%. [3/5]', '(3) For the specialized architectures we evaluated (e.g., Neural Shuffle Exchange Network and CGPS), improvements obtained on one compositional generalization benchmark do not transfer to others. [4/5]', '(4) Improvements to general-purpose architectures (e.g., LSTM -&gt; Transformer) generally lead to corresponding incremental improvements in compositional settings. [5/5]', 'Cells with a grey background are results we add in our paper; those with a white background are existing results.']",https://arxiv.org/abs/2007.08970,"While mainstream machine learning methods are known to have limited ability to compositionally generalize, new architectures and techniques continue to be proposed to address this limitation. We investigate state-of-the-art techniques and architectures in order to assess their effectiveness in improving compositional generalization in semantic parsing tasks based on the SCAN and CFQ datasets. We show that masked language model (MLM) pre-training rivals SCAN-inspired architectures on primitive holdout splits. On a more complex compositional task, we show that pre-training leads to significant improvements in performance vs. comparable non-pre-trained models, whereas architectures proposed to encourage compositional generalization on SCAN or in the area of algorithm learning fail to lead to significant improvements. We establish a new state of the art on the CFQ compositional generalization benchmark using MLM pre-training together with an intermediate representation. ","Compositional Generalization in Semantic Parsing: Pre-training vs.
  Specialized Architectures"
88,1285281321808347136,732342097458679808,Mallory Molina,"['New data announcement! The Swift+MaNGA (SwiM) VAC is a compilation of Swift &amp; SDSS images w/MaNGA maps, transformed to have the same spatial &amp; angular resolution/sampling. Find data here: <LINK>, and the submitted paper here: <LINK>']",https://arxiv.org/abs/2007.08541,"We introduce the Swift/UVOT+MaNGA (SwiM) value added catalog, which comprises 150 galaxies that have both SDSS/MaNGA integral field spectroscopy and archival Swift/UVOT near-UV (NUV) images. The similar angular resolution between the three Swift/UVOT NUV images and the MaNGA maps allows for a high-resolution comparison of optical and NUV indicators of star formation, crucial for constraining quenching and attenuation in the local universe. The UVOT NUV images, SDSS images, and MaNGA emission line and spectral index maps have all been spatially matched and re-projected to match the point spread function and pixel sampling of the Swift/UVOT uvw2 images, and are presented in the same coordinate system for each galaxy. The spectral index maps use the definition first adopted by Burstein et al. (1984), which makes it more convenient for users to compute spectral indices when binning the maps. Spatial covariance is properly taken into account in propagating the uncertainties. We also provide a catalog that includes PSF-matched aperture photometry in the SDSS optical and Swift NUV bands. In an earlier, companion paper (Molina et al. 2020) we used a subset of these galaxies to explore the attenuation laws of kiloparsec-sized star forming regions. The catalog, maps for each galaxy, and the associated data models, are publicly released on the SDSS website (this https URL). ",Swift/UVOT+MaNGA (SwiM) Value-added Catalog
89,1285270207402061827,53464710,Eric Wong,"['1/ New paper on learning perturbation sets for robust machine learning! We study how to characterize real world perturbations in a well-defined set. \n\nPaper: <LINK>\nBlog post: <LINK>\nCode: <LINK>\n\nJoint work with @zicokolter <LINK>', '2/ We define a learned perturbation set over an Lp ball in the latent space of a generator, which uses a latent vector to perturb an example, and is trained on pairs of perturbed examples.\n\nThe generator captures complex perturbations, and is well-defined over the latent Lp ball.', '3/ You may be (rightfully) suspicious of a perturbation set defined by a generative model learned from data. \n\nWe define concrete, measurable properties of a ""good"" perturbation set, in order to properly evaluate the quality of perturbation sets learned from data.', '4/ To learn the generator, we use the conditional variational autoencoder framework. \n\nWe theoretically prove that training the CVAE objective results in a perturbation set that satisfies these good properties, resulting in a principled approach for learning perturbation sets.', '5/ We can now easily leverage methods from Lp robustness to learn robustness to real-world effects captured by a learned perturbation set: simply run Lp approaches in the latent space of the generator! \n\nThis also gives another reason to care about methods for Lp robustness.', '6/ We learn a perturbation set that captures common image corruptions, and another perturbation set that captures lighting changes for scenes in the wild. \n\nCommon corruptions: https://t.co/pbSFBbLYdB\nMulti-Illumination dataset: https://t.co/xbwcA8SFSP\n\n@DanHendrycks @murmurmann', '7/ We can then train models which are adversarially robust to common corruptions and lighting changes, using PGD adversarial training and randomized smoothing. \n\nThis results in empirically and certifiably robust models to real-world perturbations.', '8/ Finally, models trained with a meaningful learned perturbation set can have non-adversarial benefits as well. \n\nFor example, for CIFAR10 common corruptions, we can get improved average-case corrupted performance over directly training on the corrupted examples.']",https://arxiv.org/abs/2007.08450,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL ",Learning perturbation sets for robust machine learning
90,1285219932368703489,1004365363574902784,Kevin J. Kelly,"['New paper out this morning with @yuberfpg, @PedroANMachado, Stephen Parke, and Renata Zukanovich Funchal.\n\nWe took a look at the neutrino mass ordering in light of updated #neutrino oscillation experiment data.\n\n<LINK>', ""What is the neutrino mass ordering? Well, we know of three light neutrinos, that we label 1, 2, and 3. We also know that at least two of them have mass, so let's call their masses m_1, m_2, and m_3, respectively.\n\nWe're confident, thanks to some great experiments, that m_2 &gt; m_1."", 'We\'re also very confident that the ""splitting"" of masses |m_3^2 - m_1^2| is much, much bigger than the splitting |m_2^2 - m_1^2|.\n\n*oscillation experiments are only really sensitive to the masses squared, because all of the neutrinos present are highly relativistic.', 'What isn\'t known is whether that bigger splitting, m_3^2 - m_1^2, is positive or negative. Put another way, we don\'t know whether\n\nm_1 &lt; m_2 &lt; m_3 (""normal ordering"")\n\nor\n\nm_3 &lt; m_1 &lt; m_2 (""inverted ordering"").', '@Fermilab\'s ""All Things Neutrino"" website has a great explainer on this: https://t.co/DixOXOZlih\n\nBe sure to check out the ""More Info"" tab if you\'re curious.', 'So, back to recent news. As of mid-2020, the experiments that are sensitive to the determining the neutrino mass ordering, all together, provided relatively strong evidence that the ordering is normal.\n\nAgain, normal -- m_1 &lt; m_2 &lt; m_3.', 'This ""strong evidence"" is compiled by several groups of physicists who perform global fits of neutrino data -- robust statistical analyses that combine results from many experiments.\n\nSome ref\'s:\n1) https://t.co/F0BCsPnNKT\n2) https://t.co/KoHHOj74sU\n3) https://t.co/HbcdRkrBdi', 'As a #neutrino community, we were fortunate enough to have the biannual @nu2020_chicago conference held completely online a few weeks ago. Many thanks to the conference organizers for providing a great experience and a venue for many wonderful talks.', 'Three experiments provided updated results relevant for determining the neutrino mass ordering -- @Tokai2Kamioka, @novaexperiment, and Super-Kamiokande.', '@Tokai2Kamioka and @novaexperiment are long-baseline accelerator-neutrino experiments. Super-K can determine the neutrino mass ordering using atmospheric neutrinos.', 'We took a look at the difference between previous T2K/NOvA results and their @nu2020_chicago results and showed that both of them are moving in the direction of preferring the *inverted* ordering, m_3 &lt; m_1 &lt; m_2 over the *normal* ordering.', ""Because it is measuring atmospheric neutrinos, Super-K is much harder to simulate at this level, but their results show the preference for normal ordering waning as well! A thorough combination of all three experiments' most updated results is necessary!"", ""The point we're making in https://t.co/gq3Cvp9TpV is that the story on determining the neutrino mass ordering is, unfortunately, far from over! It looked as if the current generation of experiments could determine it before the next generation had a shot, but maybe not anymore."", 'Big thanks to my collaborators @yuberfpg, @PedroANMachado, Stephen, and Renata for such a fun, quick work.']",https://arxiv.org/abs/2007.08526,"We inspect recently updated neutrino oscillation data -- specifically coming from the Tokai to Kamioka and NuMI Off-axis $\nu_e$ Appearance experiments -- and how they are analyzed to determine whether the neutrino mass ordering is normal ($m_1 < m_2 < m_3$) or inverted ($m_3 < m_1 < m_2$). We show that, despite previous results giving a strong preference for the normal ordering, with the newest data from T2K and NOvA, this preference has all but vanished. Additionally, we highlight the importance of this result for non-oscillation probes of neutrinos, including neutrinoless double beta decay and cosmology. Future experiments, including JUNO, DUNE, and T2HK will provide valuable information and determine the mass ordering at a high confidence level. ","Back to (Mass-)Square(d) One: The Neutrino Mass Ordering in Light of
  Recent Data"
91,1285199982769905665,354202184,Carmelo Evoli,['Happy to be co-author of a new paper that appeared today on @arxiv. We present the results of massive numerical simulations of charged particles in magnetic turbulence and we show that our findings are unexplained by present-day theories #cosmicrays <LINK> <LINK>'],https://arxiv.org/abs/2007.09142,"The diffusive motion of charged particles in synthetic magnetic turbulence with different properties is investigated by using numerical simulations with unprecedented dynamical range, which allow us to ensure that both the inertial range and the long wavelength part of the turbulent spectrum are properly described. This is of particular importance in evaluating previous suggestions that parallel and perpendicular diffusion coefficients differ in their energy dependence, an assertion at odds with the many claims of universality of the $D_{\perp}$ and $D_{\parallel}$ as functions of particle energy. Cases with and without an ordered magnetic field are discussed. Results of the numerical simulations are compared with available theoretical models, for slab, slab/2D and isotropic turbulence. We find widespread evidence that universality is broken, and that the ratio $D_{\perp}/D_{\parallel}$ is not independent of energy. The implications of this finding for the physics of cosmic ray transport are discussed in depth. ",Novel aspects of cosmic ray diffusion in synthetic magnetic turbulence
92,1285176923920965632,91420905,Alex Smith,"['My new paper ""The Completed SDSS-IV Extended Baryon Oscillation Spectroscopic Survey: N-body Mock Challenge for the Quasar Sample"" is on the arXiv today, which is part of the release of the final @eBOSSurvey cosmology results <LINK> <LINK>', 'The aim of the mock challenge was to test the models used in the eBOSS quasar clustering analysis on a wide range of mock catalogues. We include observational effects, use different models to add quasars to the mocks, and cover a range of different cosmologies', 'By looking at the scatter in the results between the different mocks, we can estimate a systematic uncertainty in our measurements', 'These uncertainties are included in the errors for the measurements in configuration space (https://t.co/aTG2zk9Nes) and in Fourier space (https://t.co/qfpxUDkn3a)', ""@BillWrightCosmo No, we didn't look at the effect of cosmologies with non-zero neutrino mass. But we made mocks with a wide range of cosmological parameters to get a conservative estimate of the systematic uncertainty"", ""@BillWrightCosmo I think it's the same for the other tracers. But even taking the conservative errors due to cosmology that we use, this is only a small part of the total error in the measurements, so shouldn't affect the neutrino mass constraints. This will be more important to test for DESI""]",https://arxiv.org/abs/2007.09003,"The growth rate and expansion history of the Universe can be measured from large galaxy redshift surveys using the Alcock-Paczynski effect. We validate the Redshift Space Distortion models used in the final analysis of the Sloan Digital Sky Survey (SDSS) extended Baryon Oscillation Spectroscopic Survey (eBOSS) Data Release 16 quasar clustering sample, in configuration and Fourier space, using a series of HOD mock catalogues generated using the OuterRim N-body simulation. We test three models on a series of non-blind mocks, in the OuterRim cosmology, and blind mocks, which have been rescaled to new cosmologies, and investigate the effects of redshift smearing and catastrophic redshifts. We find that for the non-blind mocks, the models are able to recover $f\sigma_8$ to within 3% and $\alpha_\parallel$ and $\alpha_\bot$ to within 1%. The scatter in the measurements is larger for the blind mocks, due to the assumption of an incorrect fiducial cosmology. From this mock challenge, we find that all three models perform well, with similar systematic errors on $f\sigma_8$, $\alpha_\parallel$ and $\alpha_\bot$ at the level of $\sigma_{f\sigma_8}=0.013$, $\sigma_{\alpha_\parallel}=0.012$ and $\sigma_{\alpha_\bot}=0.008$. The systematic error on the combined consensus is $\sigma_{f\sigma_8}=0.011$, $\sigma_{\alpha_\parallel}=0.008$ and $\sigma_{\alpha_\bot}=0.005$, which is used in the final DR16 analysis. For BAO fits in configuration and Fourier space, we take conservative systematic errors of $\sigma_{\alpha_\parallel}=0.010$ and $\sigma_{\alpha_\bot}=0.007$. ","The Completed SDSS-IV Extended Baryon Oscillation Spectroscopic Survey:
  N-body Mock Challenge for the Quasar Sample"
93,1284390937746001920,1928095068,Yaoqing Yang,"['Welcome to check out our new paper ""boundary thickness and robustness in learning models."" <LINK>. We show that a ""thick"" decision boundary helps improve robustness against both adversarial examples and out-of-distribution transforms.']",https://arxiv.org/abs/2007.05086,"Robustness of machine learning models to various adversarial and non-adversarial corruptions continues to be of interest. In this paper, we introduce the notion of the boundary thickness of a classifier, and we describe its connection with and usefulness for model robustness. Thick decision boundaries lead to improved performance, while thin decision boundaries lead to overfitting (e.g., measured by the robust generalization gap between training and testing) and lower robustness. We show that a thicker boundary helps improve robustness against adversarial examples (e.g., improving the robust test accuracy of adversarial training) as well as so-called out-of-distribution (OOD) transforms, and we show that many commonly-used regularization and data augmentation procedures can increase boundary thickness. On the theoretical side, we establish that maximizing boundary thickness during training is akin to the so-called mixup training. Using these observations, we show that noise-augmentation on mixup training further increases boundary thickness, thereby combating vulnerability to various forms of adversarial attacks and OOD transforms. We can also show that the performance improvement in several lines of recent work happens in conjunction with a thicker boundary. ",Boundary thickness and robustness in learning models
94,1284326599165239298,795877354266456064,KoheiKamadaPhys,"[""<LINK>\nSubmitted a new paper with Jun'ya Kume, a Ph.D student, and Yusuke Yamada, a postdoc."", 'In the gravitational leptogenesis, one of an interesting mechanisms to generate the matter-antimatter asymmetry of the Universe, there has been an issue in calculating the resultant asymmetry, originating from the UV divergent structure in the model.', 'We clarified the structure of the UV divergence and the way to regularize and renormalize. In the pessimistic case the mechanism cannot be responsible for the present Universe, but the uncertainty in the finite term that remains after renormalization allows us not to give up it.']",https://arxiv.org/abs/2007.08029,"We consider the renormalization in the pseudo-scalar inflation models with the gravitational Chern-Simons term. In this model, lepton asymmetry is generated from the chiral gravitational waves produced due to the Chern-Simons term through the gravitational chiral anomaly. However, it is known that the naive estimate of the expectation value of the gravitational Chern-Pontryagin density as well as the resultant lepton number density depend on the UV-cutoff scale, which raises a question on their validity. In this paper, we propose a way to renormalize the expectation value of the Chern-Pontryagin density to remove the UV-cutoff dependence. We also discuss the renormalized lepton number density when we adopt the minimal subtraction scheme and the viability of the gravitational leptogenesis scenario. ","Renormalization in gravitational leptogenesis with pseudo-scalar-tensor
  coupling"
95,1284185270674501632,1355052822,Benjamin Alldritt,['New paper on arXiv with great postdoc Linghao Yan. <LINK> Monolayer metal-organic framework (Cu-DCA MOF) growing across terraces on epitaxial graphene. In collaboration with SIN group at Aalto.'],https://arxiv.org/abs/2007.06899,"Achieving large-area uniform two-dimensional (2D) metal-organic frameworks (MOFs) and controlling their electronic properties on inert surfaces is a big step towards future applications in electronic devices. Here we successfully fabricated a 2D monolayer Cu-dicyanoanthracene (DCA) MOF with long-range order on an epitaxial graphene surface. Its structural and electronic properties are studied by low-temperature scanning tunneling microscopy (STM) and spectroscopy (STS) complemented by density-functional theory (DFT) calculations. We demonstrate access to multiple molecular charge states in the 2D MOF using tip-induced local electric fields. We expect that a similar strategy could be applied to fabricate and characterize 2D MOFs with exotic, engineered electronic states. ",Synthesis and Local Probe Gating of a Monolayer Metal-Organic Framework
96,1284151359491788800,1069184356533583872,Ekaterina Lobacheva,"['Our new paper On Power Laws in Deep Ensembles is on arXiv: <LINK>\nCredits to @nadiinchi, Maxim Kodryan, Dmitry Vetrov @bayesgroup\n\nWe investigate asymptotic properties of CNLL as a function of ensemble size n, network size s, and the number of parameters B. 1/4 <LINK>', 'CNLL and NLL of deep ensemble follow power law w.r.t. ensemble size n: CNLL_n = c + b n^a\nMoreover, CNNL follows power law w.r.t. network size s and the total number of parameters B.\n2/4 https://t.co/HO8aeVCgHj', 'Memory Split Advantage effect: our practically important funding is that one large network may perform worse than an ensemble of several medium-size networks with the same total number of parameters. 3/4 https://t.co/g3fD9ZWk7c', 'Given relatively small number of trained networks, we can use the discovered power laws to predict:\n- NLL and CNLL of large ensembles \n- optimal memory split given a memory budget. 4/4 https://t.co/pFK2gumGPf']",http://arxiv.org/abs/2007.08483,"Ensembles of deep neural networks are known to achieve state-of-the-art performance in uncertainty estimation and lead to accuracy improvement. In this work, we focus on a classification problem and investigate the behavior of both non-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble as a function of the ensemble size and the member network size. We indicate the conditions under which CNLL follows a power law w.r.t. ensemble size or member network size, and analyze the dynamics of the parameters of the discovered power laws. Our important practical finding is that one large network may perform worse than an ensemble of several medium-size networks with the same total number of parameters (we call this ensemble a memory split). Using the detected power law-like dependencies, we can predict (1) the possible gain from the ensembling of networks with given structure, (2) the optimal memory split given a memory budget, based on a relatively small number of trained networks. We describe the memory split advantage effect in more details in arXiv:2005.07292 ",On Power Laws in Deep Ensembles
97,1284098611098288133,3351977373,Alex Clark,['New paper up on the #arXiv today on pump-probe spectroscopy of rubidium at the single-photon level. Check it out! <LINK> @QSUMproject @ImperialPhysics @ICFOnians'],http://arxiv.org/abs/2007.08452,"We propose and demonstrate pump-probe spectroscopy of rubidium absorption which reveals the sub-Doppler hyperfine structure of the $^{5}$S$_{1/2} \leftrightarrow$ $^{5}$P$_{3/2}$ (D2) transitions. The counter propagating pump and probe lasers are independently tunable in frequency, with the probe operating at the single-photon-level. The two-dimensional spectrum measured as the laser frequencies are scanned shows fluorescence, Doppler-broadened absorption dips and sub-Doppler features. The detuning between the pump and probe lasers allows compensation of the Doppler shift for all atomic velocities in the room temperature vapor, meaning we observe sub-Doppler features for all atoms in the beam. We detail a theoretical model of the system which incorporates fluorescence, saturation effects and optical pumping and compare this with the measured spectrum, finding a mean absolute percentage error of 4.17\%. In the future this technique could assist in frequency stabilization of lasers, and the single-photon-level probe could be replaced by a single photon source. ",Single-photon-level sub-Doppler pump-probe spectroscopy of rubidium
98,1284058870227369984,1106491092915105793,Julian Bitterwolf,"['New paper <LINK> together with Alexander Meinke and Matthias Hein. We present the GOOD training scheme, leading to a classifier that has low confidence not only on out-of-distribution inputs, but provably in a whole ball around them. <LINK>']",https://arxiv.org/abs/2007.08473,"Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing the system to trigger human intervention or to transfer into a safe state. In this paper, we aim for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an $l_\infty$-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the $l_\infty$-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy. ",Certifiably Adversarially Robust Detection of Out-of-Distribution Data
99,1283932867400097793,3377160202,Djuna Croon,"['Another new paper with the same heroes! \n\n<LINK>\nMissing in Action: New Physics and the Black Hole Mass Gap\n\nIn this paper: more detail, more models, more physics! \n\nSee the quoted thread for an explanation of the black hole mass gap, and its consequences: <LINK>', 'In this new work, we give a detailed explanation of the pair instability supernovae in the presence of new physics. We show that far less mass is lost in pulsations (and a little less in wind) if new particles are emitted, as you can see from this plot: https://t.co/Vbxc9RXlZV', 'The reason is that these new losses speed up the helium burning stage (below, the orange curve gives the time until helium depletion), and as a result, the C/O ratio (cyan) is higher. Less explosive oxygen + more carbon -&gt; less extreme pulsations -&gt; bigger resulting black holes. https://t.co/gnxnFo4IYo', 'And again, a big shoutout to my fantastic collaborators Sam McDermott and @JeremySakstein!']",https://arxiv.org/abs/2007.07889,"We demonstrate the power of the black hole mass gap as a novel probe of fundamental physics. New light particles that couple to the Standard Model can act as an additional source of energy loss in the cores of population-III stars, dramatically altering their evolution. We investigate the effects of two paradigmatic weakly coupled, low-mass particles, axions and hidden photons, and find that the pulsational pair instability, which causes a substantial amount of mass loss, is suppressed. As a result, it is possible to form black holes of $72\msun$ or heavier, deep inside the black hole mass gap predicted by the Standard Model. The upper edge of the mass gap is raised to $>130{\rm M}_\odot$, implying that heavier black holes, anticipated to be observed after LIGO's sensitivity is upgraded, would also be impacted. In contrast, thermally produced heavy particles would remain in the core, leading to the tantalizing possibility that they drive a new instability akin to the electron-positron pair instability. We investigate this effect analytically and find that stars that avoid the electron-positron pair instability could experience this new instability. We discuss our results in light of current and upcoming gravitational wave interferometer detections of binary black hole mergers. ",Missing in Action: New Physics and the Black Hole Mass Gap
100,1283820072017035265,382961853,Jo Dunkley,['Love how @Planck and @ACT_Pol complement each other! And the new ACT paper today led by Sigurd Naess (<LINK>) shows how we combine the two to make super-awesome maps of the microwave background. Better together! #NSFfunded <LINK>'],https://arxiv.org/abs/2007.07290,"This paper presents a maximum-likelihood algorithm for combining sky maps with disparate sky coverage, angular resolution and spatially varying anisotropic noise into a single map of the sky. We use this to merge hundreds of individual maps covering the 2008-2018 ACT observing seasons, resulting in by far the deepest ACT maps released so far. We also combine the maps with the full Planck maps, resulting in maps that have the best features of both Planck and ACT: Planck's nearly white noise on intermediate and large angular scales and ACT's high-resolution and sensitivity on small angular scales. The maps cover over 18,000 square degrees, nearly half the full sky, at 100, 150 and 220 GHz. They reveal 4,000 optically-confirmed clusters through the Sunyaev Zel'dovich effect (SZ) and 18,500 point source candidates at $> 5\sigma$, the largest single collection of SZ clusters and millimeter wave sources to date. The multi-frequency maps provide millimeter images of nearby galaxies and individual Milky Way nebulae, and even clear detections of several nearby stars. Other anticipated uses of these maps include, for example, thermal SZ and kinematic SZ cluster stacking, CMB cluster lensing and galactic dust science. The method itself has negligible bias. However, due to the preliminary nature of some of the component data sets, we caution that these maps should not be used for precision cosmological analysis. The maps are part of ACT DR5, and are available on LAMBDA at this https URL There is also a web atlas at this https URL ","The Atacama Cosmology Telescope: DR5 maps of 18,000 square degrees of
  the microwave sky from ACT 2008-2018 data"
101,1283805206510276611,203639204,Dan Elton,['New paper on the arXiv which I am a co-author on. This is an extension of earlier work on using image-translation to remove IV contrast from CT #radiology images and improve performance of #DeepLearning segmentation models. \n<LINK>'],https://arxiv.org/abs/2007.07230,"Current deep learning based segmentation models often generalize poorly between domains due to insufficient training data. In real-world clinical applications, cross-domain image analysis tools are in high demand since medical images from different domains are often needed to achieve a precise diagnosis. An important example in radiology is generalizing from non-contrast CT to contrast enhanced CTs. Contrast enhanced CT scans at different phases are used to enhance certain pathologies or organs. Many existing cross-domain image-to-image translation models have been shown to improve cross-domain segmentation of large organs. However, such models lack the ability to preserve fine structures during the translation process, which is significant for many clinical applications, such as segmenting small calcified plaques in the aorta and pelvic arteries. In order to preserve fine structures during medical image translation, we propose a patch-based model using shared latent variables from a Gaussian mixture model. We compare our image translation framework to several state-of-the-art methods on cross-domain image translation and show our model does a better job preserving fine structures. The superior performance of our model is verified by performing two tasks with the translated images - detection and segmentation of aortic plaques and pancreas segmentation. We expect the utility of our framework will extend to other problems beyond segmentation due to the improved quality of the generated images and enhanced ability to preserve small structures. ","Cross-Domain Medical Image Translation by Shared Latent Gaussian Mixture
  Model"
102,1283796985078636550,888216099757490176,Maithra Raghu,"['Delighted our new paper ""Anatomy of Catastrophic Forgetting: Hidden Representations and Task Semantics"" just won Best Paper at the Continual Learning Workshop at #ICML2020 !!\n\nPaper: <LINK>\n\nOral *tomorrow*, details at: <LINK>\n\n⬇️ Paper thread', 'Led by @vinayramasesh and @ethansdyer\n\nWe study the phenomenon of Catastrophic Forgetting, where neural networks trained on tasks in sequence will suffer performance drops on earlier tasks --- a challenge for not only continual learning but multitask learning to data augmentation https://t.co/0qVvEnwQmP', 'We investigate how catastrophic forgetting affects hidden representations, the inner workings of techniques to mitigate forgetting, and the effects of task semantic similarity on forgetting.\n\nWe use split CIFAR10, as well as a CIFAR100 based task modeling input distribution shift', 'Does forgetting happen equally across all neural network parameters?\n\nBy using representational similarity techniques, layer freezing/resets, we find that deeper layers are the main source of forgetting. https://t.co/r06A7djJzG', 'Supporting this we find that common forgetting mitigation methods like Replay Buffers and EWC work by stabilize higher layer representations. \n\n(Note alternate approaches such as weight orthogonalization can also help mitigate forgetting) https://t.co/zI4QapQcMy', 'We next turn to understanding the effect on forgetting of (semantic) similarity between sequential tasks.\n\nDoes more similarity between tasks lead to less forgetting?\n\nWe uncover a puzzle, where this holds for some task sequences, but not others. https://t.co/YiIlvz8ytN', 'Analyzing this reveals a tension between orthogonality and overlap: to reduce forgetting, the net must be made to learn different representations across classes, and then these unique representations can also be reused for similar tasks. We formalize this with an analytic model. https://t.co/ziNTMmFk5e', 'Through this model and in experiments, we find that *intermediate* similarity in sequential results in the most forgetting. https://t.co/EqKrmprGLR']",https://arxiv.org/abs/2007.07400,"A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift. ","Anatomy of Catastrophic Forgetting: Hidden Representations and Task
  Semantics"
103,1283741875506118658,2444384845,Dr Viviane Pons,"['New paper on the arXiv! Some generalization of stack sorting involving automatons. Have fun! <LINK>', ""@aSymplecticDuck C'est parce que pour l'instant je TRAVAILLE en Martinique. Les vacances seront un peu plus tard."", ""@aSymplecticDuck C'est une association assez naturelle. La grande majorité des blancs venant en Martinique sans y vivre y vont pour des vacances. Et d'ailleurs, je vais bien en profiter moi aussi. C'est juste que je reste 3 semaines et que je ne prends pas 3 semaines de vacances""]",https://arxiv.org/abs/2007.07802,"Generalizing stack sorting and $c$-sorting for permutations, we define the permutree sorting algorithm. Given two disjoint subsets $U$ and $D$ of $\{2, \dots, n-1\}$, the $(U,D)$-permutree sorting tries to sort the permutation $\pi \in \mathfrak{S}_n$ and fails if and only if there are $1 \le i < j < k \le n$ such that $\pi$ contains the subword $jki$ if $j \in U$ and $kij$ if $j \in D$. This algorithm is seen as a way to explore an automaton which either rejects all reduced expressions of $\pi$, or accepts those reduced expressions for $\pi$ whose prefixes are all $(U,D)$-permutree sortable. ",Permutree sorting
104,1283676028901220353,1900087399,Warwick Astro Group,"['New #LockdownScience from Dr Elizabeth Stanway, @astroash42 and @astro_jje using the @astroBPASS models to look for binary fraction diagnostics in resolved stellar populations and supernova type ratios. See <LINK> for the full paper! <LINK>']",https://arxiv.org/abs/2007.07263,"The binary fraction of a stellar population can have pronounced effects on its properties, and in particular the number counts of different massive star types, and the relative subtype rates of the supernovae which end their lives. Here we use binary population synthesis models with a binary fraction that varies with initial mass to test the effects on resolved stellar populations and supernovae, and ask whether these can constrain the poorly-known binary fraction in different mass and metallicity regimes. We show that Wolf-Rayet star subtype ratios are valuable binary diagnostics, but require large samples to distinguish by models. Uncertainties in which stellar models would be spectroscopically classified as Wolf-Rayet stars are explored. The ratio of thermonuclear, stripped envelope and other core-collapse supernovae may prove a more accessible test and upcoming surveys will be sufficient to constrain both the high mass and low mass binary fraction in the z < 1 galaxy population. ","Binary Fraction Indicators in Resolved Stellar Populations and Supernova
  Type Ratios"
105,1283673803319648256,243241062,Dr Lachlan D. Urquhart,"[""New paper with Peter Craigon about the design process &amp; empirical evaluation of the Moral-IT cards as a tool for 'ethics by design' <LINK> @UoELawResearch @SCRIPTCentre @EdCDCS @HorizonDER @DesignInf cards here <LINK>""]",https://arxiv.org/abs/2007.07514,"This paper presents the design process and empirical evaluation of a new tool for enabling ethics by design: The Moral-IT Cards. Better tools are needed to support the role of technologists in addressing ethical issues during system design. These physical cards support reflection by technologists on normative aspects of technology development, specifically on emerging risks, appropriate safeguards and challenges of implementing these in the system. We discuss how the cards were developed and tested within 5 workshops with 20 participants from both research and commercial settings. We consider the role of technologists in ethics from different EU/UK policymaking initiatives and disciplinary perspectives (i.e. Science and Technology Studies (STS), IT Law, Human Computer Interaction (HCI), Computer/Engineering Ethics). We then examine existing ethics by design tools, and other cards based tools before arguing why cards can be a useful medium for addressing complex ethical issues. We present the development process for the Moral-IT cards, document key features of our card design, background on the content, the impact assessment board process for using them and how this was formulated. We discuss our study design and methodology before examining key findings which are clustered around three overarching themes. These are: the value of our cards as a tool, their impact on the technology design process and how they structure ethical reflection practices. We conclude with key lessons and concepts such as how they level the playing field for debate; enable ethical clustering, sorting and comparison; provide appropriate anchors for discussion and highlighted the intertwined nature of ethics. ",The Moral-IT Deck: A Tool for Ethics by Design
106,1283658709693063169,1601296094,David Bowler,"['A new paper with my colleague Lionel Truflandier in @bordeaux_ism on density matrix perturbation theory, examining his HPCP method compared to TC2 #CompChem \n\n<LINK>']",https://arxiv.org/abs/2007.04739,"Density matrix perturbation theory (DMPT) is known as a promising alternative to the Rayleigh-Schr\""odinger perturbation theory, in which the sum-over-state (SOS) is replaced by algorithms with perturbed density matrices as the input variables. In this article, we formulate and discuss three types of DMPT, with two of them based only on density matrices: the approach of Kussmann and Ochsenfeld [J. Chem. Phys.127, 054103 (2007)] is reformulated via the Sylvester equation, and the recursive DMPT of A.M.N. Niklasson and M. Challacombe [Phys. Rev. Lett. 92, 193001 (2004)] is extended to the hole-particle canonical purification (HPCP) from [J. Chem. Phys. 144, 091102 (2016)]. Comparison of the computational performances shows that the aformentioned methods outperform the standard SOS. The HPCP-DMPT demonstrates stable convergence profiles but at a higher computational cost when compared to the original recursive polynomial method ",Notes on density matrix perturbation theory
107,1283586543685267456,1172321171670360064,Suraj Nair,"['Can we learn dynamics models that are conditioned on goals, and only model goal-relevant quantities? We explore this question in our new work Goal-Aware Prediction, to appear at #ICML2020 at 7 AM/6 PM PDT tomorrow\nw/ @chelseabfinn @silviocinguetta \n\nPaper: <LINK> <LINK>']",https://arxiv.org/abs/2007.07170,"Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is the mismatch between the objective of the learned model (future state reconstruction), and that of the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of the scene conditioned on the goal, and as a result outperforms standard task-agnostic dynamics models and model-free reinforcement learning. ",Goal-Aware Prediction: Learning to Model What Matters
108,1283556415664087040,11778512,Mason Porter,"['New paper by Alice Schwarze (@aliceschwarze) and me: ""Motifs for Processes on Networks"": <LINK>\n\nA key point: For network motifs, don\'t just think about structure; you should also think about dynamics.']",https://arxiv.org/abs/2007.07447,"The study of motifs in networks can help researchers uncover links between the structure and function of networks in biology, sociology, economics, and many other areas. Empirical studies of networks have identified feedback loops, feedforward loops, and several other small structures as ""motifs"" that occur frequently in real-world networks and may contribute by various mechanisms to important functions in these systems. However, these mechanisms are unknown for many of these motifs. We propose to distinguish between ""structure motifs"" (i.e., graphlets) in networks and ""process motifs"" (which we define as structured sets of walks) on networks and consider process motifs as building blocks of processes on networks. Using the steady-state covariances and steady-state correlations in a multivariate Ornstein--Uhlenbeck process on a network as examples, we demonstrate that the distinction between structure motifs and process motifs makes it possible to gain quantitative insights into mechanisms that contribute to important functions of dynamical systems on networks. ",Motifs for processes on networks
109,1283430058074484736,76792433,Daniele Panozzo,['Deshana and Etai just finished developing a new easy-to-use and efficient autodiff tool: ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians. \n\nPaper: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/2007.05094,"The computation of first and second-order derivatives is a staple in many computing applications, ranging from machine learning to scientific computing. We propose an algorithm to automatically differentiate algorithms written in a subset of C99 code and its efficient implementation as a Python script. We demonstrate that our algorithm enables automatic, reliable, and efficient differentiation of common algorithms used in physical simulation and geometry processing. ",ACORNS: An Easy-To-Use Code Generator for Gradients and Hessians
110,1283426426394497024,946056863048699905,Asa Cooper Stickland,"[""New short paper with @driainmurray 'Diverse Ensembles Improve Calibration' (<LINK>). The idea is to improve calibration on i.i.d. and out of distribution data by exposing each member of an ensemble to a different input distribution (via different augmentations)!"", ""I'll be presenting this at the Uncertainty &amp; Robustness in Deep Learning workshop at ICML 2020, so you can come chat to us on Friday 5-6pm BST. P.S. there's no NLP in this paper, because similar ideas didn't work for the tasks I tried, hopefully we'll be able to fix that soon!""]",https://arxiv.org/abs/2007.04206,"Modern deep neural networks can produce badly calibrated predictions, especially when train and test distributions are mismatched. Training an ensemble of models and averaging their predictions can help alleviate these issues. We propose a simple technique to improve calibration, using a different data augmentation for each ensemble member. We additionally use the idea of `mixing' un-augmented and augmented inputs to improve calibration when test and training distributions are the same. These simple techniques improve calibration and accuracy over strong baselines on the CIFAR10 and CIFAR100 benchmarks, and out-of-domain data from their corrupted versions. ",Diverse Ensembles Improve Calibration
111,1283405499380236290,6794312,Anoop Cherian,['Our new work in ICML 2020 that combines Contrastive Learning + Optimal Transport + Adversarial Learning + Riemannian Optimization for Sequence Representation Learning. Here is the associated paper <LINK>. Check out the video to learn more: <LINK>'],https://arxiv.org/abs/2007.05840,"In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose a novel framework connecting Wasserstein GANs with a classifier, allowing a principled mechanism for producing good negative distributions for contrastive learning, which is currently a challenging problem. Our full objective is cast as a subspace learning problem on the Grassmann manifold and solved via Riemannian optimization. To empirically study our formulation, we provide experiments on the task of human action recognition in video sequences. Our results demonstrate competitive performance against challenging baselines. ",Representation Learning via Adversarially-Contrastive Optimal Transport
112,1283372671498178566,4639078397,John Wise,"['New paper day! Led by JSPS Fellow G. Chiaki. We studied the formation of 2nd gen stars, enriched by a faint Pop III supernova (3 cases with 13, 50, and 80 Msun). We find that the C dust grains produced in the 13 Msun SN induce fragmentation at [Fe/H] = -9. <LINK> <LINK>', 'Stars with such low iron abundances may be detected in larger surveys. The record holders are [Fe/H] &lt; -7.1 (Keller+) and a detected [Fe/H] = -6.2 (Nordlander+) by @annafrebel, @AstroRana, and collab.\n\nThese ancient stars are imprinted from the first stars in the Universe. https://t.co/FcFueT4pFZ']",https://arxiv.org/abs/2007.06657,"Carbon-enhanced metal-poor (CEMP) stars are the living fossils holding records of chemical enrichment from early generations of stars. In this work, we perform a set of numerical simulations of the enrichment from a supernova (SN) of a first generation of metal-free (Pop III) star and the gravitational collapse of the enriched cloud, considering all relevant cooling/heating processes and chemical reactions as well as the growth of dust grains. We adopt faint SN models for the first time with progenitor masses $M_{\rm PopIII} = 13$--$80 \ {\rm M}_{\bigodot}$, which yield C-enhanced abundance patterns (${\rm [C/Fe]} = 4.57$--$4.75$) through mixing and fallback of innermost layers of the ejecta. This model also considers the formation and destruction of dust grains. We find that the metals ejected by the SN can be partly re-accreted by the same dark matter minihalo, and carbon abundance of the enriched cloud $A({\rm C}) = 3.80$--$5.06$ is lower than the abundance range of observed CEMP stars ($A({\rm C}) \gtrsim 6$) because the mass of the metals ejected by faint SNe is smaller than normal core-collapse SNe due to extensive fallback. We also find that cloud fragmentation is induced by gas cooling from carbonaceous grains for $M_{\rm PopIII} = 13 \ {\rm M}_{\bigodot}$ even with the lowest iron abundance ${\rm [Fe/H]} \sim -9$. This leads to the formation of low-mass stars, and these ``giga metal-poor'' stars can survive until the present-day Universe and may be found by future observations. ","Seeding the second star -- II. CEMP star formation enriched from faint
  supernovae"
113,1283366904216739841,1034017323257028608,Teiji Kunihiro,['A new paper on shear viscosity in CYM theory.\n古典ヤンミルズ理論のずり粘性。CYMは時空カオスを示す。 <LINK>'],https://arxiv.org/abs/2007.06886,"We investigate the shear viscosity $\eta$ of the classical Yang-Mills (CYM) field on a lattice by using the Green-Kubo formula, where the shear viscosity is calculated from the time-correlation function of the energy-momentum tensor in equilibrium. Dependence of the shear viscosity $\eta(g,T)$ on the coupling $g$ and temperature $T$ is represented by a scaling function $f_\eta(g^2T)$ as $\eta(g,T)=Tf_\eta(g^2T)$ due to the scaling-invariant property of the CYM. The explicit functional form of $f_\eta(g^2T)$ is successfully determined from the calculated shear viscosity: It turns out that $\eta(g,T)$ of the CYM field is proportional to $1/g^{1.10-1.88}$ at weak coupling, which is a weaker dependence on $g$ than that in the leading-order perturbation theory but consistent with that of the ""anomalous viscosity"" $\eta\propto 1/g^{1.5}$ under the strong disordered field. The obtained shear viscosity is also found to be roughly consistent with that estimated through the analysis of the anisotropy of the pressure of the CYM dynamics in the expanding geometry with recourse to a hydrodynamic equation. ",Shear viscosity of classical Yang-Mills field
114,1283331204889092106,1710807372,Stephen Bullivant,"[""New working paper now live, likely of interest to some (many?): <LINK>\n\n*Power, Preferment, and Patronage: Catholic Bishops, Social Networks, and the Affair(s) of Ex-Cardinal McCarrick*\n\n(Don't let the - not very, as SNA goes - 'technical bits' put you off!) <LINK>"", 'This is something Giovanni Sadewo and I have been working on a while. TONS more to do in this area - we\'re really just making the case here. Trying to ""quantify"", and think sociologically about, the structure and dynamics of episcopal culture.', ""Releasing it now as a working paper - comments, criticisms, suggestions for future work, all welcome - for several reasons. Q awkward to place journal wise (plus v slow process). Wanted to have it 'out there' *BEFORE* 'McCarrick Report' appears, as background/context. 'Enjoy'!"", '@BirgitteUna Pour a strong drink first!']",https://arxiv.org/abs/2007.06606,"Social Network Analysis (SNA) has shed powerful light on cultures where the influence of patronage, preferment, and reciprocal obligations are traditionally important. Accordingly, we argue here that episcopal appointments, culture, and governance within the Catholic Church are ideal topics for SNA interrogation. We analyse original network data for the Catholic Bishops' Conference of England and Wales, and the United States Conference of Catholic Bishops. Significantly, we show how a network-informed approach may help with the urgent task of understanding the ecclesiastical cultures in which sexual abuse occurs, and/or is enabled, ignored, and covered up. Particular reference is made to Theodore McCarrick, the former DC Archbishop ""dismissed from the clerical state"" for sexual offences. Commentators naturally use terms like ""protege"", ""clique"", ""network"", and ""kingmaker"" when discussing both the McCarrick affair and church politics more generally: precisely such folk-descriptions of social and political life that SNA is designed to quantify and explain. ","Power, Preferment, and Patronage: Catholic Bishops, Social Networks, and
  the Affair(s) of Ex-Cardinal McCarrick"
115,1283201955804991489,1077995761487568896,Jon Miller,"['New paper day!\n<LINK>\nA failed wind in the stellar-mass black hole GRS 1915+105 led to obscuration of the central engine, and dimming by 2-3 orders of magnitude.  Key implications for Seyfert-2 &amp; Compton-thick AGN and their evolution.  @chandraxray @SPEX_Xray <LINK>']",https://arxiv.org/abs/2007.07005,"We report on Chandra gratings spectra of the stellar-mass black hole GRS 1915+105 obtained during a novel, highly obscured state. As the source entered this state, a dense, massive accretion disk wind was detected through strong absorption lines. Photionization modeling indicates that it must originate close to the central engine, orders of magnitude from the outer accretion disk. Strong, nearly sinusoidal flux variability in this phase yielded a key insight: the wind is blue-shifted when its column density is relatively low, but red-shifted as it approaches the Compton-thick threshold. At no point does the wind appear to achieve the local escape velocity; in this sense, it is a ""failed wind."" Later observations suggest that the disk ultimately fails to keep even the central engine clear of gas, leading to heavily obscured and Compton-thick states characterized by very strong Fe K emission lines. Indeed, these later spectra are successfully described using models developed for obscured AGN. We discuss our results in terms the remarkable similarity of GRS 1915+105 deep in its ""obscured state"" to Seyfert-2 and Compton-thick AGN, and explore how our understanding of accretion and obscuration in massive black holes is impacted by our observations. ","An Obscured, Seyfert-2-like State of the Stellar-mass Black Hole GRS
  1915+105 Caused by Failed Disk Winds"
116,1283198107287564289,1191056593476915200,Ray Bai,"['Excited to share my new paper, ""A Unified Computational and Theoretical Framework for High-Dimensional Bayesian Additive Models""! Our framework applies to non-Gaussian data and encompasses both canonical and non-canonical link functions. <LINK>']",https://arxiv.org/abs/2007.07021,"We study estimation and variable selection in non-Gaussian Bayesian generalized additive models (GAMs) under a spike-and-slab prior for grouped variables. Our framework subsumes GAMs for logistic regression, Poisson regression, negative binomial regression, and gamma regression, and encompasses both canonical and non-canonical link functions. Under mild conditions, we establish posterior contraction rates and model selection consistency when $p \gg n$. For computation, we propose an EM algorithm for obtaining MAP estimates in our model, which is available in the R package sparseGAM. We illustrate our method on both synthetic and real data sets. ","Spike-and-Slab Group Lasso for Consistent Estimation and Variable
  Selection in Non-Gaussian Generalized Additive Models"
117,1283110274979917825,2911287964,Thomas Kupfer,"['And its paper time.. We (incl. @evbauer_astro, @Janvanroestel, @jotajotahermes, @ebellm, friends from @DrRemeis) and many others found the 2nd member of Roche lobe-filling hot subdwarf binaries. As you know, one is an exception, two makes a new class \n<LINK> 1/10', 'We found the system in our @ztfsurvey high-cadence Galactic Plane data. The light curve shape is remarkably similar to the first member with two very different minima, brightness changes up to 30%. The orbital period is also very short at only 56min. 2/10 https://t.co/UtiBEGEuPl', 'We measured the velocities over the binary period and detected a large velocity amplitude of more than 400 km/s and we found a strong Rossiter-McLaughlin effect, indicating that the hot subdwarf is rotating rapidly and being eclipsed by something large. 3/10 https://t.co/fyOKeZdyHa', 'We were able to secure a beautiful follow-up light curve with HiPERCAM which is a high-speed photometer installed at @GTCtelescope. We only can explain the light curve with a Roche lobe-filling hot subdwarf which is being eclipsed by an accretion disc, but that is not all 4/10 https://t.co/3l0wZyEjr2', ""we detect a weak eclipse of the white dwarf companion, which allows us the measure its temperature of about 60,000K. Putting everything together we find a hot subdwarf with 40% the sun's mass and a white dwarf companion with 68% the sun's mass surrounded by an accretion disc 5/10 https://t.co/PwDrqEogqG"", 'To understand the history of the system @evbauer_astro  at @KITP_UCSB ran MESA models. The hot subdwarf was formed when a star with about 3 times the mass of the sun lost all of its hydrogen rich envelope in a common envelope event with the white dwarf companion. 6/10 https://t.co/s9xJOtjE7W', 'About 350 million years ago the hot subdwarf was born at an orbital period of about 2.5 hours. Gravitational waves tightened the orbit until the hot subdwarf filled its Roche Lobe and started accretion pretty recent and will continue to do that for another million years. 7/10 https://t.co/NyD7RysRq6', 'Interestingly, we predict a high accretion rate of 10^(-9) x mass of the sun per year or about 10 Gigatons per second. That should heat up the accreting white dwarf substantially which is exactly what we see. We should also be able to detect the orbital decay in a few years 8/10 https://t.co/WqMKWAVT7j', 'But whats next? Once the hot subdwarf runs out of fuel, accretion will stop. Gravitational waves will keep shrinking the orbit, and in ~30 million years accretion starts again at a period of a few minutes. Now, it could merge and explode as a supernova or form a single star. 9/10', ""But there is one final question. Both known systems are seen during a short lived phase of only a million years. Why haven't we seen accreting systems during the first few hundred million years where they spend most of their life as hot subdwarfs.. That is to be continued.. 10/10""]",https://arxiv.org/abs/2007.05349,"We present the discovery of the second binary with a Roche lobe-filling hot subdwarf transferring mass to a white dwarf (WD) companion. This 56 minute binary was discovered using data from the Zwicky Transient Facility. Spectroscopic observations reveal an He-sdOB star with an effective temperature of $T_{\rm eff}=33,700\pm1000$ K and a surface gravity of $log(g)=5.54\pm0.11$. The GTC+HiPERCAM light curve is dominated by the ellipsoidal deformation of the He-sdOB star and shows an eclipse of the He-sdOB by an accretion disk as well as a weak eclipse of the WD. We infer a He-sdOB mass of $M_{\rm sdOB}=0.41\pm0.04$ M$_\odot$ and a WD mass of $M_{\rm WD}=0.68\pm0.05$ M$_\odot$. The weak eclipses imply a WD black-body temperature of $63,000\pm10,000$ K and a radius $R_{\rm WD}=0.0148\pm0.0020$ M$_\odot$ as expected for a WD of such high temperature. The He-sdOB star is likely undergoing hydrogen shell burning and will continue transferring mass for $\approx1$ Myrs at a rate of $10^{-9} M_\odot {\rm yr}^{-1}$ which is consistent with the high WD temperature. The hot subdwarf will then turn into a WD and the system will merge in $\approx30$ Myrs. We suggest that Galactic reddening could bias discoveries towards preferentially finding Roche lobe-filling systems during the short-lived shell burning phase. Studies using reddening corrected samples should reveal a large population of helium core-burning hot subdwarfs with $T_{\rm eff}\approx25,000$ K in binaries of 60-90 minutes with WDs. Though not yet in contact, these binaries would eventually come into contact through gravitational wave emission and explode as a sub-luminous thermonuclear supernova or evolve into a massive single WD. ",A new class of Roche lobe-filling hot subdwarf binaries
118,1283089262594818053,908854281616412672,Neil Thompson,"[""A.I.'s most exciting successes have come from Deep Learning, which has a voracious appetite for computing power.  That's quickly becoming unsustainable, as I show in my new paper with @KGreenewald (@MITIBMLab), Keeheon Lee (@yonsei_u) and Gabriel Manso: <LINK>""]",https://arxiv.org/abs/2007.05558,"Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image recognition, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article reports on the computational demands of Deep Learning applications in five prominent application areas and shows that progress in all five is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods. ",The Computational Limits of Deep Learning
119,1283072624701181955,915661066255962113,Jim Winkens,"['New paper! Joint contrastive and supervised training improves OOD detection performance on the challenging near OOD setting by obtaining a rich and task-agnostic feature space.\n\n<LINK>\n\nThread. <LINK>', 'Supervised training for multiclass classification does not produce representations beyond the minimum necessary to classify. Contrastive training instead incentivizes the model to learn features that discriminate between all dataset images, essential for reliable OOD detection.', 'We pretrain with a SimCLR objective, followed by fine-tuning with a joint SimCLR and supervised loss. We use a standard Mahalanobis OOD detector acting on the penultimate layer and note that label smoothing establishes tighter class clusters -&gt; crucial for this detector. https://t.co/YcxlDvZTpd', 'We propose a measure of how far OOD a test sample is, called CLP. We show that joint training improves performance especially in the near OOD regime where we report a new SOTA. This regime is critical in the medical domain where fine-grained outliers are commonplace. https://t.co/y4BDgp9wlH', 'Our method is competitive with SOTA methods across the CLP spectrum in various settings, and unlike leading methods does not require additional OOD data for training or tuning. https://t.co/zZmy4S3Nq2', 'Super fun collaboration with @BunelR, @abzz4ssj,  Robert Stanforth, @vivnat, @joe_ledsam, @patmacwilliams, @pushmeet, @alan_karthi, @saakohl, @TaylanCemgilML and @ORonneberger.']",http://arxiv.org/abs/2007.05566,"Reliable detection of out-of-distribution (OOD) inputs is increasingly understood to be a precondition for deployment of machine learning systems. This paper proposes and investigates the use of contrastive training to boost OOD detection performance. Unlike leading methods for OOD detection, our approach does not require access to examples labeled explicitly as OOD, which can be difficult to collect in practice. We show in extensive experiments that contrastive training significantly helps OOD detection performance on a number of common benchmarks. By introducing and employing the Confusion Log Probability (CLP) score, which quantifies the difficulty of the OOD detection task by capturing the similarity of inlier and outlier datasets, we show that our method especially improves performance in the `near OOD' classes -- a particularly challenging setting for previous methods. ",Contrastive Training for Improved Out-of-Distribution Detection
120,1283067030833434625,885067890755600384,Eric Savin,"['New paper: Adaptive reconstruction of imperfectly-observed monotone functions, with applications to uncertainty quantification, with L. Bonnet, J.-L. Akian, and T. J. Sullivan\n<LINK>']",https://arxiv.org/abs/2007.05236,"Motivated by the desire to numerically calculate rigorous upper and lower bounds on deviation probabilities over large classes of probability distributions, we present an adaptive algorithm for the reconstruction of increasing real-valued functions. While this problem is similar to the classical statistical problem of isotonic regression, the optimisation setting alters several characteristics of the problem and opens natural algorithmic possibilities. We present our algorithm, establish sufficient conditions for convergence of the reconstruction to the ground truth, and apply the method to synthetic test cases and a real-world example of uncertainty quantification for aerodynamic design. ","Adaptive reconstruction of imperfectly-observed monotone functions, with
  applications to uncertainty quantification"
121,1283060417237934081,2166703328,Dr. Cℏarles D. Brown II,"[""Changing fields from PhD to PD has been a wild ride but I love it. In my subgroup's new paper, we load a Bose-Einstein condensate into a special lattice made of light, and study how atom-atom interactions affect the lattice-trapped atoms' motional energy <LINK>"", '@PhysMossman Thanks!', '@cosmoloony Very cool. We should definitely talk about BECs in different environments some time']",https://arxiv.org/abs/2007.05928,"Geometric frustration of particle motion in a kagome lattice causes the single-particle band structure to have a flat s-orbital band. We probe this band structure by exciting a Bose-Einstein condensate into excited Bloch states of an optical kagome lattice, and then measuring the group velocity through the atomic momentum distribution. We find that interactions renormalize the band structure of the kagome lattice, greatly increasing the dispersion of the third band that, according to non-interacting band theory, should be nearly non-dispersing. Measurements at various lattice depths and gas densities agree quantitatively with predictions of the lattice Gross-Pitaevskii equation, indicating that the observed distortion of band structure is caused by the disortion of the overall lattice potential away from the kagome geometry by interactions. ","Interaction-Enhanced Group Velocity of Bosons in the Flat Band of an
  Optical Kagome Lattice"
122,1283047741019561985,1059813876454354955,Olaf Ronneberger,"['(1/2) Our new paper  ""Contrastive Training for Improved\nOut-of-Distribution Detection"" <LINK> with @jimwinkens, @BunelR, @abzz4ssj,  Robert Stanforth, @vivnat, @joe_ledsam, @patmacwilliams, @pushmeet, @alan_karthi, @saakohl, @TaylanCemgilML, @arkitus <LINK>', '(2/2) We set a new state-of-the-art for the challenging near OOD setting without any outlier data during training. Contrastive training on in-distribution data only is sufficient to boost the performance!', '@AVMiceliBarone @jimwinkens @BunelR @abzz4ssj @vivnat @joe_ledsam @patmacwilliams @pushmeet @alan_karthi @saakohl @TaylanCemgilML @arkitus Yes, we use the standard Mahalanobis approach.']",https://arxiv.org/abs/2007.05566,"Reliable detection of out-of-distribution (OOD) inputs is increasingly understood to be a precondition for deployment of machine learning systems. This paper proposes and investigates the use of contrastive training to boost OOD detection performance. Unlike leading methods for OOD detection, our approach does not require access to examples labeled explicitly as OOD, which can be difficult to collect in practice. We show in extensive experiments that contrastive training significantly helps OOD detection performance on a number of common benchmarks. By introducing and employing the Confusion Log Probability (CLP) score, which quantifies the difficulty of the OOD detection task by capturing the similarity of inlier and outlier datasets, we show that our method especially improves performance in the `near OOD' classes -- a particularly challenging setting for previous methods. ",Contrastive Training for Improved Out-of-Distribution Detection
123,1283037353582436355,1048984881131401217,Cora Dvorkin,"['We are excited about our new paper with my PhD student Ana Díaz Rivero: ""Flow-Based Likelihoods for Non-Gaussian Inference""\n<LINK>', '@slashML @MIT_CSAIL']",https://arxiv.org/abs/2007.05535,"We investigate the use of data-driven likelihoods to bypass a key assumption made in many scientific analyses, which is that the true likelihood of the data is Gaussian. In particular, we suggest using the optimization targets of flow-based generative models, a class of models that can capture complex distributions by transforming a simple base distribution through layers of nonlinearities. We call these flow-based likelihoods (FBL). We analyze the accuracy and precision of the reconstructed likelihoods on mock Gaussian data, and show that simply gauging the quality of samples drawn from the trained model is not a sufficient indicator that the true likelihood has been learned. We nevertheless demonstrate that the likelihood can be reconstructed to a precision equal to that of sampling error due to a finite sample size. We then apply FBLs to mock weak lensing convergence power spectra, a cosmological observable that is significantly non-Gaussian (NG). We find that the FBL captures the NG signatures in the data extremely well, while other commonly used data-driven likelihoods, such as Gaussian mixture models and independent component analysis, fail to do so. This suggests that works that have found small posterior shifts in NG data with data-driven likelihoods such as these could be underestimating the impact of non-Gaussianity in parameter constraints. By introducing a suite of tests that can capture different levels of NG in the data, we show that the success or failure of traditional data-driven likelihoods can be tied back to the structure of the NG in the data. Unlike other methods, the flexibility of the FBL makes it successful at tackling different types of NG simultaneously. Because of this, and consequently their likely applicability across datasets and domains, we encourage their use for inference when sufficient mock data are available for training. ",Flow-Based Likelihoods for Non-Gaussian Inference
124,1282935969583435776,766065198721564672,Tim Langen,"['New review paper featuring results from @Uni_Stuttgart, @iqoqi, @LENSQGases, @ICFOnians, @UniHannover, @LPTMS, @otago and many others. \n\nNew states of matter with fine-tuned interactions:quantum droplets and dipolar supersolids:\n<LINK>\n\n#droplets #supersolid <LINK>']",https://arxiv.org/abs/2007.06391,Quantum fluctuations can stabilize Bose-Einstein condensates (BEC) against the mean-field collapse. Stabilization of the condensate has been observed in quantum degenerate Bose-Bose mixtures and dipolar BECs. The fine-tuning of the interatomic interactions can lead to the emergence of two new states of matter: liquid-like selfbound quantum droplets and supersolid crystals formed from these droplets. We review the properties of these exotic states of matter and summarize the experimental progress made using dipolar quantum gases and Bose-Bose mixtures. We conclude with an outline of important open questions that could be addressed in the future. ,"New states of matter with fine-tuned interactions: quantum droplets and
  dipolar supersolids"
125,1282924326484955136,206369044,Sverre Holm,"['«Simple Circuit Equivalents for the Constant Phase Element» New paper on arXiv, combining electrochemistry, bioimpedance, circuit theory. Two generations of Holm working together, <LINK> @FractalCOST #CPE']",https://arxiv.org/abs/2007.05821,The constant phase element (CPE) with a frequency-independent negative phase between current and voltage is used extensively in e.g. the bioimpedance and electrochemistry fields. Its physical meaning is only partially understood. Here we show that the responses of both the common capacitive CPE as well as the inductive CPE are exactly the same as those of simple RL and RC circuits where the inductor's or capacitor's value increases linearly with time. The resulting step and impulse responses are found and verified by simulation with the Micro-Cap simulation program. The realization with time-varying components correlates with known time-varying properties in applications. ,Simple Circuit Equivalents for the Constant Phase Element
126,1282680056548974592,890006203316674560,Dibya Ghosh,"['Super happy to release this new paper with @marcgbellemare! We formally study how representation learning can be used to stabilize off-policy RL.\n \nArXiv: <LINK>\nAt ICML 2020: \n<LINK>\n \n1/6 <LINK>', 'We study the learning dynamics of the classic off-policy value function learning algorithm, TD(0), through the lens of the state representation. Our work reveals several insights into how certain choices of state representation affect stability and divergence of RL.\n\n2/6', 'Unlike in supervised learning, stability depends on how the state representation is parametrized. Two representations can represent exactly the same set of value functions, but have widely different stability profiles.\n\n3/6 https://t.co/Tmu1166p75', 'Certain representation learning schemes can always ensure stability. One such “stable” class we study are invariant representations. These are representations whose features can be used to predict the expected feature values at the *next* timestep. \n\n4/6 https://t.co/BAYb9CMFX8', 'These stable representations aren’t limited to theory: they can be learned with neural networks using auxiliary tasks. E.g. invariant representations are learned with an auxiliary objective to predict the next timestep feature values of a target network.\n\n5/6 https://t.co/3fiSfZKuH3', 'Our work sheds light on why previously proposed auxiliary objectives work and makes me very optimistic about principled representation learning methods as a path towards stable and performant RL. \n\n6/6']",https://arxiv.org/abs/2007.05520,"Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks. ",Representations for Stable Off-Policy Reinforcement Learning
127,1282610875979845632,20174338,Daniel Cotton,"['Announcing our new paper: ""Phase-locked polarization by photospheric reflection in the semidetached eclipsing binary mu1 Sco"" <LINK> with @JeremyBailey5, @quasibody and @ain_uws #polarimetry #astronomy – in which we measure light reflected from stars!', ""The reflected light is a tiny fraction of the total light we see, but because it is highly polarised (&amp; the stars otherwise aren't) we can detect it with our very sensitive polarimeter. The polarisation changes as the stars orbit each other because the angle we see it at changes."", 'For 50 years the polarisation in binary systems has been thought to be from gas flowing between the stars. There were only a couple of exceptions where people suspected the stars themselves might be the thing(s) reflecting.', 'https://t.co/AjVS8Q9xcm', 'Last year we published a similar paper showing reflection in the Spica system: https://t.co/N5ig04DNDB \n\nWhat made Spica special was that it is a detached binary: one with no entrained material! We did some detailed atmospheric modelling and showed that the stars were reflecting.', ""The result there was striking because we didn't have to fit the polarisation amplitude. The parameters of the system were really well known, and our model matched the data almost perfectly – that was the Eureka moment. https://t.co/YsSxJfG8dg"", ""However, the polarisation in the Spica system is small. Most polarised binary systems are very close binaries with entrained gas and higher polarisations. The question was: is reflection from the star's photosphere still important in those other systems?"", 'Oh btw: The polarisation is shown as the blue bars. Polarised light waves are those with a favoured orientation (here we are dealing with linear polarisation from e- scattering). Good polarimetry intros: https://t.co/w6uXI5JoFc by @Sydonahi &amp; https://t.co/pHZ3WEzncd by @HumanBott', ""That brings us back to mu1 Sco. It's a semidetached system – the type of system that should have a flow of gas between the stars. And yet, when we modelled the polarisation from reflection, again it almost perfectly matched our observations! No amplitude fitting needed."", 'Here is the fit to one set of our data in terms of q and u. These are the vector components of polarisation. Kind of like the equivalent of x and y in projectile motion diagrams. https://t.co/4uefl0BzLg', 'We also found that the polarisation was bigger in green than red – something that is hard to explain with gas models.', 'Before I get to the implications, I should say that we actually discovered mu1 Sco was a polarimetic variable with a 35-cm telescope! This is, quite frankly, AMAZING. Mu1 Sco is a famous, large amplitude photometric binary: https://t.co/gmOOJkgyPb', ""It is so bright, and the variability so large, you could theoretically notice the variability with the naked eye! It was the 3rd star to be recognised as a spectroscopic binary (1896), &amp; Antonia Maury's 1920 determination of its orbital period as 1.446... days is still accurate!"", 'Sidebar: Antonia Maury (https://t.co/FXahDZLAUu) is someone worth learning about, she was one of the pioneers of spectral classification. Along with Annie Cannon, Willamina Fleming and their contemporaries: https://t.co/jdLOQFIYCb', ""Anyway, back to mu1 Sco: It turns out that the only time it has even had it's polarisation measured was by Kris Serkowski in the late 60s – he measured it twice and didn't notice anything out of the ordinary! (I happen to live 800m from where he took his measurements at SSO now.)"", 'But when I used this telescope fitted with a miniature version of our polarimeter, the polarisation variability was obvious after half a dozen measurements. https://t.co/jhrVKypIDn', ""So, you can make amazing scientific discoveries with significant implications using and amateur sized telescope (in the middle of Sydney*) if you have a modern polarimeter. Our instrument is about 10 to 50 times better than Serkowski's.\n\n*5 km from the Airport, 3 km from the SCG. https://t.co/ztmzxHh1EX"", ""So, our modelling suggests that hotter, more luminous stars that are close together should have a larger polarisations. This means the systems with the most detectable reflection are some of the most extreme. We don't have a lot of good measurements of masses of extreme stars."", ""To make a measurement of a star's mass you need a binary. The problem is you can only separate the component masses if you know the angle the system is tilted toward you (inclination), and we can usually only get that with an eclipsing system, but polarimetry has a special power."", 'Because orientation information is encoded into your polarimetric measurement, it is really good at giving you inclination. This means we have a shot at getting accurate inclination determinations in non-eclipsing massive stars. The kinds that become supernovas and black holes.', 'Those are the stars that produce the most energy in the galaxy, and return the most material to it (that then becomes more stars and planets, and us). In other words they are few but they have the biggest impact on the evolution of the Galaxy.', 'We only have good masses for about 6 of the most massive star types. We need more to test and refine our models, like the very excellent BPASS produced by @astro_jje. [Our knowledge of] the fate of the Galaxy depends on it!\nhttps://t.co/5AsUFtoZOd', 'Oh this is this paper by the way. Chandrasekhar predicted a different polarimetric mechanism from binaries in 1946. A lot of people built photoelectric polarimeters to go looking for this effect. Instead this type of effect was identified in the early 60s. https://t.co/E5W3xVCAH0', '[Obligatory tweet at the end of the thread in acknowledgement of all of the spelling and grammatical errors I have just noticed.]', ""I should clarify here that if you have a double line spectroscopic binary then you know the ratio of masses, but not the absolute values – you only know M x Sin(i). So that's where polarimetry can help."", ""In principle with multi-band polarimetry you might also be determine the spectral types of the components in a single line spectroscopic binary (i.e. where we don't know the ratio because the lines aren' separated enough in radial velocity measurements)."", ""@astrofullard It's about 0.07% in q and 0.065% in u. (0.1% = 1000 ppm, parts-per-million). The stars are B2.5V and B6.5V, our models predict that hotter stars will produce more polarisation, and in particular so will giants (b/c they have lower surface gravity); see Fig 2 of the Spica paper."", ""@astrofullard We haven't done any explicit modelling. We have taken data on other systems, but haven't yet done the analysis to release those yet. For a system where log(g) was 3 instead of 4, I'd expect p to be ~3 x larger, all other things being equal, and that's probably not the limit."", 'The MNRAS version of the preprint is now here: https://t.co/FFoA6xbQUN']",https://arxiv.org/abs/2007.05249,"We report the detection of phase-locked polarization in the bright ($m_V$=2.98-3.24) semidetached eclipsing binary $\mu^1$ Sco (HD 151890). The phenomenon was observed in multiple photometric bands using two different HIPPI-class (HIgh Precision Polarimetric Instrument)polarimeters with telescopes ranging in size from 35-cm to 3.9-m. The peak-to-trough amplitude of the polarization is wavelength dependent and large, $\sim$700 parts-per-million in green light, and is easily seen with even the smallest telescope. We fit the polarization phase curve with a SYNSPEC/VLIDORT polarized radiative transfer model and a Wilson-Devinney geometric formalism, which we describe in detail. Light from each star reflected by the photosphere of the other, together with a much smaller contribution from tidal distortion and eclipse effects, wholly accounts for the polarization amplitude. In the past polarization in semidetached binaries has been attributed mostly to scattering from extra-stellar gas. Our new interpretation facilitates determining masses of such stars in non-eclipsing systems. ","Phase-locked polarization by photospheric reflection in the semidetached
  eclipsing binary $\mu^1$ Sco"
128,1282593700338638849,1032796073427906561,Kenzie Nimmo,"['Our new paper is on the arXiv today: <LINK> \n\nUsing Westerbork RT1, we detected two bright bursts from the Galactic magnetar SGR 1935+2154 which recently produced a bright FRB-like radio burst. 💥📡', '@LaskyPaul Thanks Paul!']",https://arxiv.org/abs/2007.05101,"Fast radio bursts (FRBs) are millisecond-duration, bright radio signals (fluence $\mathrm{0.1 - 100\,Jy\,ms}$) emitted from extragalactic sources of unknown physical origin. The recent CHIME/FRB and STARE2 detection of an extremely bright (fluence $\sim$MJy$\,$ms) radio burst from the Galactic magnetar SGR~1935$+$2154 supports the hypothesis that (at least some) FRBs are emitted by magnetars at cosmological distances. In follow-up observations totalling 522.7$\,$hrs on source, we detect two bright radio bursts with fluences of $112\pm22\mathrm{\,Jy\,ms}$ and $24\pm5\mathrm{\,Jy\,ms}$, respectively. Both bursts appear affected by interstellar scattering and we measure significant linear and circular polarisation for the fainter burst. The bursts are separated in time by $\sim$1.4$\,$s, suggesting a non-Poissonian, clustered emission process -- similar to what has been seen in some repeating FRBs. Together with the burst reported by CHIME/FRB and STARE2, as well as a much fainter burst seen by FAST (fluence 60$\mathrm{\,mJy\,ms}$), our observations demonstrate that SGR 1935+2154 can produce bursts with apparent energies spanning roughly seven orders of magnitude, and that the burst rate is comparable across this range. This raises the question of whether these four bursts arise from similar physical processes, and whether the FRB population distribution extends to very low energies ($\sim10^{30}\,$erg, isotropic equivalent). ",Detection of two bright radio bursts from magnetar SGR 1935+2154
129,1282592569243971584,1192152664412475393,Fulvio Gesmundo,"['A lot of examples of strict subadditivity of tensor border rank under direct sum! Check out our new paper with M. Christandl, M. Michałek and J. Zuiddam (@jzuiddam):\n\n""Border rank non-additivity for higher order tensors""\n\n<LINK>', 'In 1981, Schönhage provided examples of strict subadditivity of border rank under direct sum for tensors of order three. His example was a stepping stone to all subsequent progress on upper bounds on the matrix multiplication exponent until today.', 'We provide examples of strict subadditivity for higher order tensors with connections to tensor network geometry and the complexity theory of generalizations of the matrix multiplication tensor.']",https://arxiv.org/abs/2007.05458,"Whereas matrix rank is additive under direct sum, in 1981 Sch\""onhage showed that one of its generalizations to the tensor setting, tensor border rank, can be strictly subadditive for tensors of order three. Whether border rank is additive for higher order tensors has remained open. In this work, we settle this problem by providing analogs of Sch\""onhage's construction for tensors of order four and higher. Sch\""onhage's work was motivated by the study of the computational complexity of matrix multiplication; we discuss implications of our results for the asymptotic rank of higher order generalizations of the matrix multiplication tensor. ",Border rank non-additivity for higher order tensors
130,1282569778377437184,820031736914513925,Fabrizio Leisen,"['New paper: ""Completely Random Measures and Lévy Bases in Free probability"" joint work with Francesca Collet and Steen Thorbjørnsen. <LINK>']",https://arxiv.org/abs/2007.05336,"This paper develops a theory for completely random measures in the framework of free probability. A general existence result for free completely random measures is established, and in analogy to the classical work of Kingman it is proved that such random measures can be decomposed into the sum of a purely atomic part and a (freely) infinitely divisible part. The latter part (termed a free L\'evy basis) is studied in detail in terms of the free L\'evy-Khintchine representation and a theory parallel to the classical work of Rajput and Rosinski is developed. Finally a L\'evy-It\^o type decomposition for general free L\'evy bases is established. ",Completely Random Measures and L\'evy Bases in Free probability
131,1282106231646085122,83532623,George Toderici,['Our group just released an arXiv paper that reviews Nonlinear Transform Coding approaches. We hope this will make the development of new end-to-end neural/learned compression methods easier to understand for those new to the field. <LINK> <LINK>'],https://arxiv.org/abs/2007.03034,"We review a class of methods that can be collected under the name nonlinear transform coding (NTC), which over the past few years have become competitive with the best linear transform codecs for images, and have superseded them in terms of rate--distortion performance under established perceptual quality metrics such as MS-SSIM. We assess the empirical rate--distortion performance of NTC with the help of simple example sources, for which the optimal performance of a vector quantizer is easier to estimate than with natural data sources. To this end, we introduce a novel variant of entropy-constrained vector quantization. We provide an analysis of various forms of stochastic optimization techniques for NTC models; review architectures of transforms based on artificial neural networks, as well as learned entropy models; and provide a direct comparison of a number of methods to parameterize the rate--distortion trade-off of nonlinear transforms, introducing a simplified one. ",Nonlinear Transform Coding
132,1281695930580758528,124522660,Chintan Shah,['A new paper on FeL is out: EBIT constraints on Capella \n<LINK>\nbased on <LINK> <LINK>'],https://arxiv.org/abs/2007.03843,"The Hitomi results for the Perseus cluster have shown that accurate atomic models are essential to the success of X-ray spectroscopic missions, and just as important as knowledge on instrumental calibration and astrophysical modeling. Preparing the models requires a multifaceted approach, including theoretical calculations, laboratory measurements, and calibration using real observations. In a previous paper, we presented a calculation of the electron impact cross sections on the transitions forming the Fe-L complex. In the present work, we systematically test the calculation against cross sections of ions measured in an electron beam ion trap experiment. A two-dimensional analysis in the electron beam energies and X-ray photon energies is utilized to disentangle radiative channels following dielectronic recombination, direct electron-impact excitation, and resonant excitation processes in the experimental data. The data calibrated through laboratory measurements are further fed into global modeling of the Chandra grating spectrum of Capella. We investigate and compare the fit quality, as well as sensitivity of the derived physical parameters to the underlying atomic data and the astrophysical plasma modeling. We further list the potential areas of disagreement between the observation and the present calculations, which in turn calls for renewed efforts in theoretical calculations and targeted laboratory measurements. ","X-ray spectra of the Fe-L complex II: atomic data constraints from EBIT
  experiment and X-ray grating observations of Capella"
133,1281694967086428167,426785306,Joshua Yao-Yu Lin,"['Extremely happy to share our new paper "" Feature Extraction on Synthetic Black Hole Images"" that was accepted by the ML Interpretability for Scientific Discovery Workshop at ICML 2020.\xa0\nlink to\xa0the\xa0paper:\xa0<LINK>\nVideo:\xa0<LINK>']",https://arxiv.org/abs/2007.00794,"The Event Horizon Telescope (EHT) recently released the first horizon-scale images of the black hole in M87. Combined with other astronomical data, these images constrain the mass and spin of the hole as well as the accretion rate and magnetic flux trapped on the hole. An important question for EHT is how well key parameters such as spin and trapped magnetic flux can be extracted from present and future EHT data alone. Here we explore parameter extraction using a neural network trained on high resolution synthetic images drawn from state-of-the-art simulations. We find that the neural network is able to recover spin and flux with high accuracy. We are particularly interested in interpreting the neural network output and understanding which features are used to identify, e.g., black hole spin. Using feature maps, we find that the network keys on low surface brightness features in particular. ",Feature Extraction on Synthetic Black Hole Images
134,1281687874195390465,1897268354,Ion Petre,['A new network analysis software focusing on the controllability problem just released:  <LINK>. The paper documenting it available as a draft at <LINK>. Excellent results on our benchmarks! #networkscience #systemsbiology #precisionmedicine'],https://arxiv.org/abs/2007.04853,"Control theory has seen recently impactful applications in network science, especially in connections with applications in network medicine. A key topic of research is that of finding minimal external interventions that offer control over the dynamics of a given network, a problem known as network controllability. We propose in this article a new solution for this problem based on genetic algorithms. We tailor our solution for applications in computational drug repurposing, seeking to maximise its use of FDA-approved drug targets in a given disease-specific protein-protein interaction network. We show how our algorithm identifies a number of potentially efficient drugs for breast, ovarian, and pancreatic cancer. We demonstrate our algorithm on several benchmark networks from cancer medicine, social networks, electronic circuits, and several random networks with their edges distributed according to the Erd\H{o}s-R\'{e}nyi, the small-world, and the scale-free properties. Overall, we show that our new algorithm is more efficient in identifying relevant drug targets in a disease network, advancing the computational solutions needed for new therapeutic and drug repurposing approaches. ","Identifying efficient controls of complex interaction networks using
  genetic algorithms"
135,1281676478753542144,2492016278,Adrian Raftery,['Our new paper on statistical contour models with @HannahDirector : <LINK> <LINK> <LINK>'],http://arxiv.org/abs/2007.04386,"Boundaries on spatial fields divide regions with particular features from surrounding background areas. These boundaries are often described with contour lines. To measure and record these boundaries, contours are often represented as ordered sequences of spatial points that connect to form a line. Methods to identify boundary lines from interpolated spatial fields are well-established. Less attention has been paid to how to model sequences of connected spatial points. For data of the latter form, we introduce the Gaussian Star-shaped Contour Model (GSCM). GSMCs generate sequences of spatial points via generating sets of distances in various directions from a fixed starting point. The GSCM is designed for modeling contours that enclose regions that are star-shaped polygons or approximately star-shaped polygons. Metrics are introduced to assess the extent to which a polygon deviates from star-shaped. Simulation studies illustrate the performance of the GSCM in various scenarios and an analysis of Arctic sea ice edge contour data highlights how GSCMs can be applied to observational data. ","Contour Models for Boundaries Enclosing Star-Shaped and Approximately
  Star-Shaped Polygons"
136,1281486695515529216,994520594489204737,Jack Hare,"['New arXiv paper, ""An Imaging Refractometer for Density Fluctuation Measurements in High Energy Density Plasmas"" <LINK>. This new diagnostic analyses a laser beam passing through #turbulent #Plasma, recording ray locations and ray deflections in orthogonal axes. <LINK>', 'The idea is that the spectrum of ray deflections relates to the spectrum of density fluctuations within the turbulent plasma. These fluctuations were studied with digital Fourier transforms of shadowgraphy and schlieren images, but our new technique resolves much smaller scales. https://t.co/CoHkahN2Cb', 'We compare our diagnostic to existing methods using ray tracing techniques, and present data from an experiment which shows the exquisite detail we can capture. Next up: going beyond the power spectrum and looking for signatures of intermittency in magnetohydrodynamic turbulence. https://t.co/uvxT9JaJW1', ""Thanks to everyone who has helped with this paper, it's been a long journey. The data is from 2016, so it's taken a pandemic to finally analyse it. Along the way we've developed a really nice ray-tracing/ray-transfer-matrix code which we can use for all our optical diagnostics.""]",https://arxiv.org/abs/2007.04682,"We report on a recently developed laser-probing diagnostic which allows direct measurements of ray-deflection angles in one axis, whilst retaining imaging capabilities in the other axis. This allows us to measure the spectrum of angular deflections from a laser beam which passes though a turbulent high-energy-density plasma. This spectrum contains information about the density fluctuations within the plasma, which deflect the probing laser over a range of angles. %The principle of this diagnostic is described, along with our specific experimental realisation. We create synthetic diagnostics using ray-tracing to compare this new diagnostic with standard shadowgraphy and schlieren imaging approaches, which demonstrates the enhanced sensitivity of this new diagnostic over standard techniques. We present experimental data from turbulence behind a reverse shock in a plasma and demonstrate that this technique can measure angular deflections between 0.06 and 34 mrad, corresponding to a dynamic range of over 500. ","An Imaging Refractometer for Density Fluctuation Measurements in High
  Energy Density Plasmas"
137,1281421836526518272,972555245179064320,Jordy de Vries,['New paper with UMass grad student Sachin Shain: <LINK> We study the effect of the QCD theta term on nuclear interactions finding severe sensitivity to the short-distance nature of the nuclear force. This affects EDMs and how axion DM interacts with nuclei.'],https://arxiv.org/abs/2007.04927,"Electric dipole moments of nuclei, diamagnetic atoms, and certain molecules are induced by CP-violating nuclear forces. Naive dimensional analysis predicts these forces to be dominated by long-range one-pion-exchange processes, with short-range forces entering only at next-to-next-to-leading order in the chiral expansion. Based on renormalization arguments we argue that a consistent picture of CP-violating nuclear forces requires a leading-order short-distance operator contributing to ${}^1S_0$-${}^3P_0$ transitions, due to the attractive and singular nature of the strong tensor force in the ${}^3P_0$ channel. The short-distance operator leads to $\mathcal O(1)$ corrections to static and oscillating, relevant for axion searches, electric dipole moments. We discuss strategies how the finite part of the associated low-energy constant can be determined in the case of CP violation from the QCD theta term by the connection to charge-symmetry violation in nuclear systems. ",Strong CP violation in nuclear physics
138,1281418430063796224,1077995761487568896,Jon Miller,"[""New paper day!\n<LINK>\nUsing @NASANuSTAR, graduate student Paul Draghis has measured the highest black hole spin that I've yet seen (highest ever?) in his very first paper.  The orientation of the black hole is also measured very precisely. <LINK>"", '@agniva_rc @NASANuSTAR Not in this case.  You must know the mass and distance very well.  Radio observations could, in principle, determine a parallax distance.  But the line-of-sight obscuration is too high to determine a mass via radial velocity curves.']",https://arxiv.org/abs/2007.04324,"The black hole candidate EXO 1846-031 underwent an outburst in 2019, after at least 25 years in quiescence. We observed the system using \textit{NuSTAR} on August 3rd, 2019. The 3--79 keV spectrum shows strong relativistic reflection features. Our baseline model gives a nearly maximal black hole spin value of $a=0.997_{-0.002}^{+0.001}$ ($1\sigma$ statistical errors). This high value nominally excludes the possibility of the central engine harboring a neutron star. Using several models, we test the robustness of our measurement to assumptions about the density of the accretion disk, the nature of the corona, the choice of disk continuum model, and addition of reflection from the outer regions of the accretion disk. All tested models agree on a very high black hole spin value and a high value for the inclination of the inner accretion disk of $\theta\approx73^\circ$. We discuss the implications of this spin measurement in the population of stellar mass black holes with known spins, including LIGO events. ",A New Spin on an Old Black Hole: NuSTAR Spectroscopy of EXO 1846-031
139,1281397084898222080,1074633382452051969,Kimin,"['Can ensemble improve off-policy RL by handling various issues?\n\nYes! We present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. New work with @MishaLaskin @AravSrinivas and @pabbeel \n\nPaper: <LINK>\n\n1/N', 'High-level idea:\nSUNRISE integrates three key ingredients: (a) bootstrap with random initialization for the stability of the learning process, (b) weighted Bellman backups for mitigating error propagation in Q-learning, and (c) an inference method for efficient exploration. \n\n2/N https://t.co/8vtP1lj3xT', 'Can SUNRISE be useful for continuous control tasks on low-dimensional environments?\n\nYes, SUNRISE consistently improves the performance of SAC across all environments and outperforms the state-of-the-art MBRL baselines like PETS and POPLIN.\n\n3/N https://t.co/RfHyNmIq4o', 'Can SUNRISE be useful for continuous control tasks on high-dimensional environments?\n\nYes, SUNRISE improves the performance of RAD on all environments from DM Control Suite and achieves SOTA in almost all environments against existing pixel-based RL methods.\n\n4/N https://t.co/Pna340c03V', 'Can SUNRISE be useful for discrete control tasks on high-dimensional environments?\n\nYes, SUNRISE improves the performance of Rainbow in Atari environments, and achieves state-of-the-art performance on 12 out of 26 environments\n\n5/N https://t.co/krMDV8bjEn', 'Ablation study:\n\nRemark that the performance gain from SUNRISE only with bootstrap, which corresponds to a naive extension of Bootstrap DQN (https://t.co/dCoQvEoWPu), is marginal compared to other techniques, such as weighted Bellman backup and UCB exploration.\n\n6/N https://t.co/PopXyyzsOq', 'Special thanks to collaborators: @MishaLaskin @AravSrinivas and @pabbeel !', 'code is available at https://t.co/fypvke0Iqr', ""@tesslerc Thank you for the suggestion! I'm gonna update the draft accordingly 😀"", '@agarwl_ @AravSrinivas Thank you for the pointer!!! I think finding in REM is very interesting and related to SUNRISE 😀']",https://arxiv.org/abs/2007.04938,"Off-policy deep reinforcement learning (RL) has been successful in a range of challenging domains. However, standard off-policy RL algorithms can suffer from several issues, such as instability in Q-learning and balancing exploration and exploitation. To mitigate these issues, we present SUNRISE, a simple unified ensemble method, which is compatible with various off-policy RL algorithms. SUNRISE integrates two key ingredients: (a) ensemble-based weighted Bellman backups, which re-weight target Q-values based on uncertainty estimates from a Q-ensemble, and (b) an inference method that selects actions using the highest upper-confidence bounds for efficient exploration. By enforcing the diversity between agents using Bootstrap with random initialization, we show that these different ideas are largely orthogonal and can be fruitfully integrated, together further improving the performance of existing off-policy RL algorithms, such as Soft Actor-Critic and Rainbow DQN, for both continuous and discrete control tasks on both low-dimensional and high-dimensional environments. Our training code is available at this https URL ","SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep
  Reinforcement Learning"
140,1281275899204136960,476582730,Shakir Mohamed,"['Excited to share a new paper on Decolonisation and AI. With the amazing @png_marie  @wsisaac 🤩 we explore why Decolonial theory matters for our field, and tactics to decolonise and reshape our field into a Decolonial AI. Feedback please 🙏🏾 Thread 👇🏾\n <LINK> <LINK>', 'We see AI and ML having 2 sides: when seen as an Object our field is about technical methods and applications; as a Subject it is about systems, incentives and institutions. In both views, AI has been brought into question, especially on harms to vulnerable people. 🐾 https://t.co/aEnvyeeCWz', 'These concerns centre around values and power in AI. Computer system embody values. And to build a deeper understanding of values and power is why we turn to the critical thoery and especially Decolonial theories. 🐾 https://t.co/0cxht1yCmQ', 'Decolonial theories name the continuation of divisive hierarchies of power from the past and that live in the present. We talk about coloniality: coloniality is what survives colonialism. This coloniality shapes our understanding of the world, which we now work to #decolonize. 🐾 https://t.co/DfDfJTc1xc', 'We explore the coloniality of modern AI, as object and subject in 5 sites: decision systems, ghost workers, beta-testing, national policies and AI governance, social development. Decolonial theories provide insights into the coloniality of these settings.🐾 https://t.co/Mzvbbutpko', 'Ultimately we are interested in the decolonisation of power. We submit three tactics for this: creating a critical technical practice of AI; enabling reciprocal engagements and reverse tutelage; supporting a renewed affective and political communities. 🐾', 'Any commitment to building the responsible and beneficial AI of the future ties us to the hierarchies, philosophy and technology inherited from the past, and\na renewed responsibility to the technology of the present; aligning  us to the goal of beneficence and justice for all. 🐾 https://t.co/vYPdjdUgUu', 'We are incredibly grateful to the multitude of scholars whose work has influenced us in so many ways - thank you to them all. We ave much still to learn, but reflexive practice and engagement is what we now work towards. https://t.co/piDvC7NPWQ']",https://arxiv.org/abs/2007.04068,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all. ","Decolonial AI: Decolonial Theory as Sociotechnical Foresight in
  Artificial Intelligence"
141,1281258091804340234,809477971518095361,Hans,"[""new paper out with @robertghrist: ever wondered if lattice theory and sheaf theory had a baby (and no i'm not talking about topoi...) that was raised by GSP??? look no further! applications *in progress* including consensus, distributed opt., GNNs &amp; more\n<LINK> <LINK>""]",https://arxiv.org/abs/2007.04099,"This paper initiates a discrete Hodge theory for cellular sheaves taking values in a category of lattices and Galois connections. The key development is the Tarski Laplacian, an endomorphism on the cochain complex whose fixed points yield a cohomology that agrees with the global section functor in degree zero. This has immediate applications in consensus and distributed optimization problems over networks and broader potential applications. ",Cellular Sheaves of Lattices and the Tarski Laplacian
142,1281242846201360391,2750994354,Tai-Danae Bradley,"['Today on the arXiv, I’ve a new paper with Yiannis Vlassopoulos: <LINK>. We describe an assignment of linear operators to expressions in a natural language that captures something of the statistics therein. <LINK>', 'Background: I once made a 10min trailer video of my PhD thesis, which investigated mathematical structure that’s both algebraic and statistical. The video ended in a cliff-hanger, and this new paper picks up where the video left off!\n\nhttps://t.co/R4IjwJ2lQ1', 'The paper uses a blend of tools from linear algebra, basic probability, and category theory. To help make the details more accessible, I’ve spelled out some of the main ideas on Math3ma. https://t.co/B6n5g8PTOO Abstract below! https://t.co/DKjckvrmH4']",http://arxiv.org/abs/2007.03834,"This work originates from the observation that today's state-of-the-art statistical language models are impressive not only for their performance, but also - and quite crucially - because they are built entirely from correlations in unstructured text data. The latter observation prompts a fundamental question that lies at the heart of this paper: What mathematical structure exists in unstructured text data? We put forth enriched category theory as a natural answer. We show that sequences of symbols from a finite alphabet, such as those found in a corpus of text, form a category enriched over probabilities. We then address a second fundamental question: How can this information be stored and modeled in a way that preserves the categorical structure? We answer this by constructing a functor from our enriched category of text to a particular enriched category of reduced density operators. The latter leverages the Loewner order on positive semidefinite operators, which can further be interpreted as a toy example of entailment. ",Language Modeling with Reduced Densities
143,1281117822547120129,636864273,Christoph Molnar,"['Our new paper ""Pitfalls to Avoid when Interpreting Machine Learning Models"" was accepted at the XXAI ICML workshop 🥳\n\n<LINK>\n<LINK>', 'This was a team effort of @gcskoenig , Julia Herbinger, Timo Freiesleben, @SusanneDndl , Christian Scholbeck, @GiuCasalicchio , Moritz Grosse-Wentrup , @BBischl and me', '@random_forest75 Thanks, Claudio!']",https://arxiv.org/abs/2007.04131,"An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research. ","General Pitfalls of Model-Agnostic Interpretation Methods for Machine
  Learning Models"
144,1281116198202195968,1021360423,Mubrak A Alqahtani,"['We have a new paper out where we calculated the elliptic flow of bottomonia produced in Pb-Pb collisions at 5.02 TeV.\n                <LINK> <LINK>', 'This work is done in collaboration with Partha Bhaduri,  Nicolas Borghini, Amaresh Jaiswal, and Michael Strickland.']",https://arxiv.org/abs/2007.03939,"We calculate the elliptic flow of bottomonia produced in Pb$\,+\,$Pb collisions at $\sqrt{s_{\rm NN}}=5.02$ TeV. We consider temperature-dependent decay widths for the anisotropic escape of various bottomonium states and observe that the transverse momentum dependence of bottomonia elliptic flow provides a tomographic information about the QGP fireball at different stages of its evolution. For the space-time evolution of the fireball, we employ simulation results from the 3+1D quasiparticle anisotropic hydrodynamic model. We find that our results for transverse momentum dependence of bottomonia elliptic flow are in reasonable agreement with experimental results from the ALICE and CMS collaborations. ","Fireball tomography from bottomonia elliptic flow in relativistic
  heavy-ion collisions"
145,1281027594809249792,1093387119148462081,Daniel Green,"['Quantum effects in de Sitter space / inflation have confused me for a long time.  If you are in the same boat, I hope you’ll check out my new paper with Tim Cohen\n\n<LINK>\n\nWe turned the problem of quantum effects in dS into dimensional analysis using EFT', 'In QFT, the question of whether perturbation theory works or not is obvious just from the units.  With very little work, one knows if a problem is easy or hard.  \n\nIn de Sitter, this was not the case and caused much confusion.  Our new paper now reduces the problem to units.', 'This was also a great chance to (finally) write a paper with my old officemate (from my postdoc days) and friend, Tim Cohen.  We don’t normally work on the same things which made this project especially fun and educational.', '@davidsd Good question!   Both are nice papers.  They are more or less solely interested in the IR divergence associated with loops of massless scalars.  Both arrive at the conclusion that this IR divergence is resolved by stochastic inflation, essentially by direct calculation.', '@davidsd Our approach applies to any mass and also dynamical gravity (neither cover gravity).  Most importantly for us, everything follows from power counting and symmetries in our EFT.  Stochastic inflation comes out of RG in this EFT at low mass (marginal coupling).']",http://arxiv.org/abs/2007.03693,"Calculating the quantum evolution of a de Sitter universe on superhorizon scales is notoriously difficult. To address this challenge, we introduce the Soft de Sitter Effective Theory (SdSET). This framework holds for superhorizon modes whose comoving momentum is far below the UV scale, which is set by the inverse comoving horizon. The SdSET is formulated using the same approach that yields the Heavy Quark Effective Theory. The degrees of freedom that capture the long wavelength dynamics are identified with the growing and decaying solutions to the equations of motion. The operator expansion is organized using a power counting scheme, and loops can be regulated while respecting the low energy symmetries. For massive quantum fields in a fixed de Sitter background, power counting implies that all interactions beyond the horizon are irrelevant. Alternatively, if the fields are very light, the leading interactions are at most marginal, and resumming the associated logarithms using (dynamical) renormalization group techniques yields the evolution equation for canonical stochastic inflation. The SdSET is also applicable to models where gravity is dynamical, including inflation. In this case, diffeomorphism invariance ensures that all interactions are irrelevant, trivially implying the all-orders conservation of adiabatic density fluctuations and gravitational waves. We briefly touch on the application to slow-roll eternal inflation by identifying novel relevant operators. This work serves to demystify many aspects of perturbation theory outside the horizon, and has a variety of applications to problems of cosmological interest. ",Soft de Sitter Effective Theory
146,1280949432612278272,892997634813710336,Adam Fisch,"['New paper on efficient set-valued predictions for tasks with many candidates (where multiple can be correct)! We extend Conformal Prediction, a principled method for making predictions with perf. guarantees.\n\nWith @TalSchuster, Tommi and @BarzilayRegina.\n\n<LINK>']",https://arxiv.org/abs/2007.03114,"In this paper, we present a novel approach for conformal prediction (CP), in which we aim to identify a set of promising prediction candidates -- in place of a single prediction. This set is guaranteed to contain a correct answer with high probability, and is well-suited for many open-ended classification tasks. In the standard CP paradigm, the predicted set can often be unusably large and also costly to obtain. This is particularly pervasive in settings where the correct answer is not unique, and the number of total possible answers is high. We first expand the CP correctness criterion to allow for additional, inferred ""admissible"" answers, which can substantially reduce the size of the predicted set while still providing valid performance guarantees. Second, we amortize costs by conformalizing prediction cascades, in which we aggressively prune implausible labels early on by using progressively stronger classifiers -- again, while still providing valid performance guarantees. We demonstrate the empirical effectiveness of our approach for multiple applications in natural language processing and computational chemistry for drug discovery. ","Efficient Conformal Prediction via Cascaded Inference with Expanded
  Admission"
147,1280865893384433664,1559281832,Vishvas Pandey,"['Check out our new paper on ""Nuclear Structure Physics in Coherent Elastic Neutrino-Nucleus Scattering""\xa0<LINK>\xa0#cevns #nuxsec']",https://arxiv.org/abs/2007.03658,"The prospects of extracting new physics signals in a coherent elastic neutrino-nucleus scattering (CE$\nu$NS) process are limited by the precision with which the underlying nuclear structure physics, embedded in the weak nuclear form factor, is known. We present microscopic nuclear structure physics calculations of charge and weak nuclear form factors and CE$\nu$NS cross sections on $^{12}$C, $^{16}$O, $^{40}$Ar, $^{56}$Fe and $^{208}$Pb nuclei. We obtain the proton and neutron densities, and charge and weak form factors by solving Hartree-Fock equations with a Skyrme (SkE2) nuclear potential. We validate our approach by comparing $^{208}$Pb and $^{40}$Ar charge form factor predictions with elastic electron scattering data. In view of the worldwide interest in liquid-argon based neutrino and dark matter experiments, we pay special attention to the $^{40}$Ar nucleus and make predictions for the $^{40}$Ar weak form factor and the CE$\nu$NS cross sections. Furthermore, we attempt to gauge the level of theoretical uncertainty pertaining to the description of the $^{40}$Ar form factor and CE$\nu$NS cross sections by comparing relative differences between recent microscopic nuclear theory and widely-used phenomenological form factor predictions. Future precision measurements of CE$\nu$NS will potentially help in constraining these nuclear structure details that will in turn improve prospects of extracting new physics. ","Nuclear Structure Physics in Coherent Elastic Neutrino-Nucleus
  Scattering"
148,1280810913659072512,804483812,Matteo Fasiolo,"['New paper on quantile GAM modelling with the qgam R package, featuring functional QGAMs #rstats @YannigGoude @RaphaelNedellec  <LINK> <LINK>']",https://arxiv.org/abs/2007.03303,"Generalized additive models (GAMs) are flexible non-linear regression models, which can be fitted efficiently using the approximate Bayesian methods provided by the mgcv R package. While the GAM methods provided by mgcv are based on the assumption that the response distribution is modelled parametrically, here we discuss more flexible methods that do not entail any parametric assumption. In particular, this article introduces the qgam package, which is an extension of mgcv providing fast calibrated Bayesian methods for fitting quantile GAMs (QGAMs) in R. QGAMs are based on a smooth version of the pinball loss of Koenker (2005), rather than on a likelihood function, hence jointly achieving satisfactory accuracy of the quantile point estimates and coverage of the corresponding credible intervals requires adopting the specialized Bayesian fitting framework of Fasiolo, Wood, Zaffran, Nedellec, and Goude (2020b). Here we detail how this framework is implemented in qgam and we provide examples illustrating how the package should be used in practice. ",qgam: Bayesian non-parametric quantile regression modelling in R
149,1280777848744300545,46153507,dr. Jordy Davelaar,"['Proud co-author of a new paper that appeared on arXiv today that was lead by @thomasbronzwaer. We present an brand-new version of GR ray-tracing code RAPTOR that now also includes full polarization information of black hole accretion flows.\n\n<LINK> <LINK>', 'Work done within @BlackHoleCam funded by @ERC_Research']",https://arxiv.org/abs/2007.03045,"Accreting supermassive black holes are sources of polarized radiation that propagates through highly curved spacetime before reaching the observer. In order to help interpret observations of such polarized emission, accurate and efficient numerical schemes for polarized radiative transfer in curved spacetime are needed. In this manuscript we extend our publicly available radiative transfer code RAPTOR to include polarization. We provide a brief review of different codes and methods for covariant polarized radiative transfer available in the literature and existing codes, and present an efficient new scheme. For the spacetime-propagation aspect of the computation, we develop a compact, Lorentz-invariant representation of a polarized ray. For the plasma-propagation aspect of the computation, we perform a formal analysis of the stiffness of the polarized radiative-transfer equation with respect to our explicit integrator, and develop a hybrid integration scheme that switches to an implicit integrator in case of stiffness, in order to solve the equation with optimal speed and accuracy for all possible values of the local optical/Faraday thickness of the plasma. We perform a comprehensive code verification by solving a number of well-known test problems using RAPTOR and comparing its output to exact solutions. We also demonstrate convergence with existing polarized radiative-transfer codes in the context of complex astrophysical problems. RAPTOR is capable of performing polarized radiative transfer in arbitrary, highly curved spacetimes. This capability is crucial for interpreting polarized observations of accreting black holes, which can yield information about the magnetic-field configuration in such accretion flows. The efficient formalism implemented in RAPTOR is computationally light and conceptually simple. The code is publicly available. ",RAPTOR II: Polarized radiative transfer in curved spacetime
150,1280694124203708416,285394452,Alex Ruch,"['New paper w/ Liz McQuillan,\xa0Erin McAweeney,\xa0Alicia Bargar, &amp; me on ArXiv <LINK>. We use network analysis, topic modeling, &amp; bridging centrality to find narratives that draw together disparate groups (many related to conspiracy) over time amid the COVID pandemic.', 'Super work from @Graphika_NYC, what a great team 😄']",https://arxiv.org/abs/2007.03443,"How can the birth and evolution of ideas and communities in a network be studied over time? We use a multimodal pipeline, consisting of network mapping, topic modeling, bridging centrality, and divergence to analyze Twitter data surrounding the COVID-19 pandemic. We use network mapping to detect accounts creating content surrounding COVID-19, then Latent Dirichlet Allocation to extract topics, and bridging centrality to identify topical and non-topical bridges, before examining the distribution of each topic and bridge over time and applying Jensen-Shannon divergence of topic distributions to show communities that are converging in their topical narratives. ","Cultural Convergence: Insights into the behavior of misinformation
  networks on Twitter"
151,1280619714016940034,3357857123,Jay Gambetta,['We now have seven quantum systems with a quantum volume of 32 deployed for our users to run experiments. It required a better understanding of the errors and the development of new calibration techniques. <LINK> \n\n paper can be found here <LINK> <LINK>'],https://arxiv.org/abs/2007.02925,"We present an improvement to the cross resonance gate realized with the addition of resonant, target rotary pulses. These pulses, applied directly to the target qubit, are simultaneous to and in phase with the echoed cross resonance pulses. Using specialized Hamiltonian error amplifying tomography, we confirm a reduction of error terms with target rotary -- directly translating to improved two-qubit gate fidelity. Beyond improvement in the control-target subspace, the target rotary reduces entanglement between target and target spectators caused by residual quantum interactions. We further characterize multi-qubit performance improvement enabled by target rotary pulsing using unitarity benchmarking and quantum volume measurements, achieving a new record quantum volume for a superconducting qubit system. ","Reducing unitary and spectator errors in cross resonance with optimized
  rotary echoes"
152,1280563365740531713,1614231872,Aaron Fisher,"['A popular guidance is to blind outcomes when fitting propensity scores, in order to maintain ""objectivity."" \n\nBut, blinding outcomes is actually not guaranteed to protect against bias.\n\nFor more, please check out my new, short paper!\n<LINK>\n\n(1/3) <LINK>', 'Without outcome blinding, the worst-case bias that a malicious, overfitting analyst can attain can diverge towards infinity as the sample size grows. \n\nBut, even with outcome blinding, the worst-case bias can grow at a comparable rate, so long as outcomes can be predicted.\n\n(2/3)', 'This is my first time writing a ""solo author"" paper (not really, I\'m lucky to have generous input from friends and colleagues), so please feel especially welcome to reach out if you have comments, thoughts or criticisms. Thanks in advance!\n\n(3/3)']",https://arxiv.org/abs/2007.02514,"Popular guidance on observational data analysis states that outcomes should be blinded when determining matching criteria or propensity scores. Such a blinding is informally said to maintain the ""objectivity"" of the analysis, and to prevent analysts from artificially amplifying the treatment effect by exploiting chance imbalances. Contrary to this notion, we show that outcome blinding is not a sufficient safeguard against fishing. Blinded and unblinded analysts can produce bias of the same order of magnitude in cases where the outcomes can be approximately predicted from baseline covariates. We illustrate this vulnerability with a combination of analytical results and simulations. Finally, to show that outcome blinding is not necessary to prevent bias, we outline an alternative sample partitioning procedure for estimating the average treatment effect on the controls, or the average treatment effect on the treated. This procedure uses all of the the outcome data from all partitions in the final analysis step, but does not require the analysis to not be fully prespecified. ","Treatment Effect Bias from Sample Snooping: Blinding Outcomes is Neither
  Necessary nor Sufficient"
153,1280558715930845185,2596589880,Nikolaus Kriegeskorte,"['New paper: @diedrichsenlab, @EvaBerlot, @marieke_mur, and Heiko Schuett introduce the *unbiased distance correlation* (UDC), an exciting new criterion for RSA model comparison that extends distance correlation &amp; linear CKA to unbiased distance estimates. <LINK> <LINK>']",https://arxiv.org/abs/2007.02789,"Representational similarity analysis (RSA) tests models of brain computation by investigating how neural activity patterns reflect experimental conditions. Instead of predicting activity patterns directly, the models predict the geometry of the representation, as defined by the representational dissimilarity matrix (RDM), which captures to what extent experimental conditions are associated with similar or dissimilar activity patterns. RSA therefore first quantifies the representational geometry by calculating a dissimilarity measure for each pair of conditions, and then compares the estimated representational dissimilarities to those predicted by each model. Here we address two central challenges of RSA: First, dissimilarity measures such as the Euclidean, Mahalanobis, and correlation distance, are biased by measurement noise, which can lead to incorrect inferences. Unbiased dissimilarity estimates can be obtained by crossvalidation, at the price of increased variance. Second, the pairwise dissimilarity estimates are not statistically independent, and ignoring this dependency makes model comparison statistically suboptimal. We present an analytical expression for the mean and (co)variance of both biased and unbiased estimators of the squared Euclidean and Mahalanobis distance, allowing us to quantify the bias-variance trade-off. We also use the analytical expression of the covariance of the dissimilarity estimates to whiten the RDM estimation errors. This results in a new criterion for RDM similarity, the whitened unbiased RDM cosine similarity (WUC), which allows for near-optimal model selection combined with robustness to correlated measurement noise. ","Comparing representational geometries using whitened
  unbiased-distance-matrix similarity"
154,1280488213111623680,1139943231922331652,Chandreyee Maitra,"['Paper time: <LINK>\n""Time Evolution Of Cyclotron Line of Her X-1; A Detailed StatisticalAnalysis Including New ASTROSAT Data"" a very rigorous and thorough work by Suman Bala from @IUCAApune to understand the secular evolution of the cyclotron line energy in HerX-1']",https://arxiv.org/abs/2007.02015,"The cyclotron line feature in the X-ray spectrum of the accretion powered pulsar Her X-1 has been observed and monitored for over three decades. The line energy exhibited a slow secular decline over the period 1995-2014, with a possible (not confirmed) indication of a reversal thereafter. Recent works have shown that the temporal evolution of the line energy may be modelled as a flattening after an earlier decrease until MJD 55400 ($\pm200)$. In this work, we present the results of ASTROSAT observations in the context of earlier data and offer a common interpretation through a detailed study of temporal and flux dependence. We find that the variation of the line energy does not support an upward trend but is consistent with the reported flattening after an earlier decrease until MJD $54487^{+515}_{-469}$. ","Time Evolution Of Cyclotron Line of Her X-1; A Detailed
  StatisticalAnalysis Including New ASTROSAT Data"
155,1280487975227469829,156804540,Francisco Rodrigues,"['In our new paper on @arxiv_org, we study two SIS-like models for interacting diseases when they are in an asymmetrically interacting regime over homogeneously mixed populations. With my friend @cosnet_bifi and my PhD student @paulocv92 . <LINK>\n#Epidemics <LINK>']",https://arxiv.org/abs/2007.02774,"Diseases and other contagion phenomena in nature and society can interact asymmetrically, such that one can benefit from the other, which in turn impairs the first, in analogy with predator-prey systems. Here, we consider two models for interacting disease-like dynamics with asymmetric interactions and different associated time scales. Using rate equations for homogeneously mixed populations, we show that the stationary prevalences and phase diagrams of each model behave differently with respect to variations of the relative time scales. We also characterize in detail the regime where transient oscillations are observed, a pattern that is inherent to asymmetrical interactions but often ignored in the literature. Our results contribute to a better understanding of disease dynamics in particular, and interacting processes in general, and could provide interesting insights for real-world applications, most notably, the interplay between the dynamics of fact-checked and fake news. ","The role of time scale in the spreading of asymmetrically interacting
  diseases"
156,1280482460510433281,1279570369184247810,Péter Mernyei,"['Just published our work on Wiki-CS, a new node classification benchmark! Many existing datasets are structurally similar. Our benchmark provides more variety and raises new challenges.\nPaper: <LINK>\nWill present as a spotlight at the GRL+ workshop at #icml2020! <LINK>', 'Compared to standard citation network datasets, this graph is much denser, with an average node degree of ~37 as opposed to ~4-5. The Deep Graph Mapper visualisation above also seems to indicate a more centralised, hierarchical structure. https://t.co/boSESmdxjI', ""There is a lot more inter-class connectivity: we calculated the share of each node's same-class neighbours, and plotted the distribution of this property for different datasets. There is a significant spread in Wiki-CS, most nodes are not in homogenous neighbourhoods. https://t.co/Qusn4PD0JS"", 'This suggests that more complex methods for aggregating large neighbourhoods might be able to improve prediction accuracy.', 'The work was the result of my final year undergraduate project at @Cambridge_CL, supervised by @catalinacangea. Thanks also to @PetarV_93 for inspiring my interest in this area!']",https://arxiv.org/abs/2007.02901,"We present Wiki-CS, a novel dataset derived from Wikipedia for benchmarking Graph Neural Networks. The dataset consists of nodes corresponding to Computer Science articles, with edges based on hyperlinks and 10 classes representing different branches of the field. We use the dataset to evaluate semi-supervised node classification and single-relation link prediction models. Our experiments show that these methods perform well on a new domain, with structural properties different from earlier benchmarks. The dataset is publicly available, along with the implementation of the data pipeline and the benchmark experiments, at this https URL . ",Wiki-CS: A Wikipedia-Based Benchmark for Graph Neural Networks
157,1280458604928094208,3422471637,Elias Kammoun,"['Paper Day!!\n\nYou can find on arxiv (<LINK>) our new paper, with @jonastrox, presenting the results of the @NuSTAR_Science Obscured Seyferts Legacy Survey. We analyzed the spectra of 19 sources and investigated 4 models (leading to consistent results). 1/5 <LINK>', 'Using MCMC analysis, and the spectral curvature metric we found that 79-89% of our sources are heavily obscured and 33-37% are Compton-thick. Our results reconcile the numbers found from hard X-ray surveys and multi-wavelength analysis . 2/5 https://t.co/Q0jLJc2D5e', 'We found that the fraction of reprocessed emission in those AGN is negatively correlated with the X-ray luminosity and positively correlated with the LOS column density. 3/5 https://t.co/rg7EUhdtqi', 'omparing the LOS column and obscured fraction to the Eddington ratios, we found strong hints that radiation pressure regulates the circumnuclear material. We also found a negative correlation between the photon index and Eddington ratios. 4/5 https://t.co/ROMZrCz7SR', 'Finally we could get hints of strong covering fractions in some AGN, where the equatorial and LOS column densities are consistent. 5/5 https://t.co/QuVY6ZbwOc']",https://arxiv.org/abs/2007.02616,"We study the X-ray spectra of a sample of 19 obscured, optically-selected Seyfert galaxies (Sy 1.8, 1.9 and 2) in the local universe ($d \leq 175$~Mpc), drawn from the CfA Seyfert sample. Our analysis is driven by the high sensitivity of NuSTAR in the hard X-rays, coupled with soft X-ray spectra using XMM-Newton, Chandra, Suzaku, and Swift/XRT. We also analyze the optical spectra of these sources in order to obtain accurate mass estimates and Eddington fractions. We employ four different models to analyze the X-ray spectra of these sources, which all result in consistent results. We find that 79-90 % of the sources are heavily obscured with line-of-sight column density $N_{\rm H} > 10^{23}~\rm cm^{-2}$. We also find a Compton-thick ($N_{\rm H} > 10^{24}~\rm cm^{-2}$) fraction of $37-53$ %. These results are consistent with previous estimates based on multi-wavelength analyses. We find that the fraction of reprocessed to intrinsic emission is positively correlated with $N_{\rm H}$ and negatively correlated with the intrinsic, unabsorbed, X-ray luminosity (in agreement with the Iwasawa-Taniguchi effect). Our results support the hypothesis that radiation pressure regulates the distribution of the circumnuclear material. ","A hard look at local, optically-selected, obscured Seyfert galaxies"
158,1280445061860392960,51700215,Phil Bull,"['New paper! After DESI, a high redshift (z &gt; 2) 21cm or spectroscopic galaxy survey looks like the best bet to improve constraints on dynamical dark energy: <LINK>', '@SeshNadathur But the figures are boring!', '@SeshNadathur See! https://t.co/f95CFoPo7Z']",https://arxiv.org/abs/2007.02865,"Most efforts to detect signatures of dynamical dark energy are focused on late times, $z \lesssim 2$, where the dark energy component begins to dominate the cosmic energy density. Many theoretical models involving dynamical dark energy exhibit a 'freezing' equation of state however, where $w \to -1$ at late times, with a transition to a 'tracking' behaviour at earlier times (with $w \gg -1$ at sufficiently high redshift). In this paper, we study whether large-scale structure surveys in the post-reionisation matter-dominated regime, $2 \lesssim z \lesssim 6$, are sensitive to this behaviour, on the basis that the dark energy component should remain detectable (despite being strongly subdominant) in this redshift range given sufficiently precise observations. Using phenomenological models inspired by parameter space studies of Horndeski (generalised scalar-tensor) theories, we show how existing CMB and large-scale structure measurements constrain the dark energy equation of state in the matter-dominated era, and examine how forthcoming galaxy surveys and 21cm intensity mapping instruments can improve constraints in this regime. We also find that the combination of existing CMB and LSS constraints with DESI will already come close to offering the best possible constraints on $H_0$ using BAO/galaxy power spectrum measurements, and that either a spectroscopic follow-up of the LSST galaxy sample (e.g. along the lines of MegaMapper or SpecTel) or a Stage 2/PUMA-like intensity mapping survey, both at $z \gtrsim 2$, would offer better constraints on the class of dark energy models considered here than a comparable cosmic variance-limited galaxy survey at $z \lesssim 1.5$. ",Searching for dark energy in the matter-dominated era
159,1280432294113804288,1169196060130123782,Gergely Neu,"['New EPIC paper with @CiaraPikeBurke finally online!\n\nWe provide a unifying framework for optimistic RL algorithms that formally shows how optimism in the model space is *equivalent* to optimism in the value space. Also works for linear FA.\n\n<LINK>\n\nTHREAD👇\n\n1/10 <LINK>', 'We show the equivalence through Lagrangian duality between \n* the usual ""model-optimistic"" formulation specified in terms of a local divergence D between transition probabilities and\n* the optimistic Bellman optimality equations with a specific exploration bonus CB.\n\n2/10 https://t.co/XaMZzxk4zm', 'The shape of CB is given by the ""conjugate"" of the divergence D.\n\nThe divergence D needs to be\n* jointly convex in its arguments and\n* positive homogeneous.\nAny f-divergence satisfies this, and also distances specified in terms of any norm.\n\n3/10 https://t.co/bfx3nkuhgA', 'We recover the usual regret bound for value-optimistic methods, but without the usual nasty recursive arguments: the probabilistic analysis only needs to verify that the true model is in the confidence set w.h.p.\n\n4/10 https://t.co/uTD8sLaOtA', 'The most interesting practical implication is that you are allowed to use a tractable upper bound on the conjugate for your exploration bonus and the probabilistic analysis still remains simple!\n\n5/10 https://t.co/PEH4WmX59f', 'BOTTOM LINE: YOU GET THE BEST OF BOTH WORLDS!!!  SIMPLE DYNAMIC PROGRAMMING IMPLEMENTATION AND SIMPLE PROBABILISTIC ANALYSIS.\n\nWe illustrate the technique by providing a simple analysis for a bunch of classic algorithms and some new ones.\n\n6/10 https://t.co/7NCb6ll3WK', 'The extension to realizable linear function approximation is based on a new LP formulation. This is *equivalent* to the classic LPs when the MDP has a factored linear structure. \n\nIMO this is very exciting as it finally feels like the ""right"" relaxation of the classic LPs.\n\n7/10 https://t.co/RreuzRMyK2', 'There are two ways of introducing optimistic versions of the LP:\n* using local constraints that allows a DP implementation &amp; recovers the alg. of Jin et al. (COLT 2020)\n* using global constraints that allows tighter bounds &amp; recovers the alg. of Zanette et al. (ICML 2020)\n\n8/10 https://t.co/frz6IqRdQf', ""Once again, this shows the power of the model-based perspective for designing and analyzing algorithms! \n\n(Fight me: Model-free RL algorithms don't exist 🤪)\n\n9/10"", ""That's it! Sorry about the bombastic campaign, but I'm REALLY excited about this work and about where it will lead!\n\n@CiaraPikeBurke was a wonderful and inspiring collaborator, and I hope to be able to continue working with her even after her move to @imperialcollege!!\n\nFIN/10"", '@ashipra @CiaraPikeBurke @imperialcollege thanks Shipra! i will actually talk about it at a certain upcoming ICML workshop ;)', ""@nanjiang_cs @CiaraPikeBurke that's exactly the most exciting question for me too!\n\nif you suppose that q' = Phi omega is a distribution, then the constraint says that q and q' should generate the same feature expectations, which is a reasonable way to check if two distributions are the same under FA .\n\n1/N"", '@nanjiang_cs @CiaraPikeBurke it is closely related to the REPS formulation of @Jan_R_Peters that replaces the stationarity constraint in the classic LP by stationary feature expectations.\n\nblowing this up into two separate constraints (stationarity and matched expectations) leads to our formulation.\n\n2/N', ""@nanjiang_cs @CiaraPikeBurke @Jan_R_Peters the cool thing is that we don't even need Phi omega to be a distribution and things still make sense! \n\nlinear parametrization of distributions over the state-action space seems extremely impractical, and this relaxed LP gives you a tractable alternative!\n\n3/N=3"", '@aryehazan @CiaraPikeBurke oh wow, really looking forward to it!!']",https://arxiv.org/abs/2007.01891,"The principle of optimism in the face of uncertainty underpins many theoretically successful reinforcement learning algorithms. In this paper we provide a general framework for designing, analyzing and implementing such algorithms in the episodic reinforcement learning problem. This framework is built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algorithm. Typically, it was thought that these two classes of algorithms were distinct, with model-optimistic algorithms benefiting from a cleaner probabilistic analysis while value-optimistic algorithms are easier to implement and thus more practical. With the framework developed in this paper, we show that it is possible to get the best of both worlds by providing a class of algorithms which have a computationally efficient dynamic-programming implementation and also a simple probabilistic analysis. Besides being able to capture many existing algorithms in the tabular setting, our framework can also address largescale problems under realizable function approximation, where it enables a simple model-based analysis of some recently proposed methods. ",A Unifying View of Optimism in Episodic Reinforcement Learning
160,1280331344233598977,1658162341,Narayanan Rengaswamy,"['New paper out that connects robust quantum metrology to classical coding theory! Wonderful collaboration with @YingkaiOuyang (<LINK>). We discuss quantum Fisher information for binary-codes-inspired probe states under erasure errors in robust quantum metrology.', ""One can tackle errors in metrology with quantum error correction (QEC) too, but that's very resource intensive for the near term. So, our proposed architecture of CSS-type probe states with no QEC in robust metrology is adaptable to fault-tolerant quantum metrology in the future."", 'Key result from our work is that the scaling of quantum Fisher information in our setup directly relates to variance of weight distributions (WD) of classical codes! But, with a twist: WD of 2^t shortened codes from given (possibly non-linear) code, dictated by set of t erasures.', 'Moral of the story: classical coding theorists have more opportunities to contribute to quantum information science beyond mainstream quantum error correction! Coming from that background, I love the sound of that!']",https://arxiv.org/abs/2007.02859,"Quantum metrology is expected to be a prominent use-case of quantum technologies. However, noise easily degrades these quantum probe states, and negates the quantum advantage they would have offered in a noiseless setting. Although quantum error correction can help tackle noise, fault-tolerant methods are too resource intensive for near-term use. Hence, a strategy for (near-term) robust metrology that is easily adaptable to future quantum error correction-based quantum metrology is desirable. Here, we propose such an architecture to sense a classical field, by studying the performance of quantum probe states that are constructed from $[n,k,d]$ binary block codes of minimum distance $d \geq t+1$. When at most $t$ qubits of the quantum probe state are erased, using the quantum Fisher information we show that the resultant noisy probe can estimate classical fields with a precision that scales inversely with the variances of the weight distributions of the corresponding $2^t$ shortened codes. If the shortened codes of a fixed code with $d \geq t+1$ have a non-trivial weight distribution, then the probe states obtained by concatenating this code with repetition codes of increasing length enable asymptotically optimal field-sensing that passively tolerates up to $t$ erasure errors. We emphasize that, despite the use of coding-theoretic methods, our results do not involve syndrome measurements or error correction. We complement our results with examples of probe states constructed from Reed-Muller codes. ","Weight Distribution of Classical Codes Influences Robust Quantum
  Metrology"
161,1280320315332182016,1077995761487568896,Jon Miller,"['New paper day!  \n<LINK>\nFor a long time, surveys failed to find the number of Compton-thick AGN required by the cosmic X-ray background.  @eskammoun has found them, using a clever sample and @NASANuSTAR. <LINK>']",https://arxiv.org/abs/2007.02616,"We study the X-ray spectra of a sample of 19 obscured, optically-selected Seyfert galaxies (Sy 1.8, 1.9 and 2) in the local universe ($d \leq 175$~Mpc), drawn from the CfA Seyfert sample. Our analysis is driven by the high sensitivity of NuSTAR in the hard X-rays, coupled with soft X-ray spectra using XMM-Newton, Chandra, Suzaku, and Swift/XRT. We also analyze the optical spectra of these sources in order to obtain accurate mass estimates and Eddington fractions. We employ four different models to analyze the X-ray spectra of these sources, which all result in consistent results. We find that 79-90 % of the sources are heavily obscured with line-of-sight column density $N_{\rm H} > 10^{23}~\rm cm^{-2}$. We also find a Compton-thick ($N_{\rm H} > 10^{24}~\rm cm^{-2}$) fraction of $37-53$ %. These results are consistent with previous estimates based on multi-wavelength analyses. We find that the fraction of reprocessed to intrinsic emission is positively correlated with $N_{\rm H}$ and negatively correlated with the intrinsic, unabsorbed, X-ray luminosity (in agreement with the Iwasawa-Taniguchi effect). Our results support the hypothesis that radiation pressure regulates the distribution of the circumnuclear material. ","A hard look at local, optically-selected, obscured Seyfert galaxies"
162,1280083343929982976,1169196060130123782,Gergely Neu,"['new paper on adversarial MDPs now online! \n\nmain result: sublinear regret with realizable function approximation &amp; bandit feedback.\n\nw/ my student Julia Olkhovskaya\n\n<LINK> <LINK>', 'the key technical tool is a generalization of the ""matrix geometric resampling"" technique we introduced in our COLT 2020 paper (https://t.co/ENdfc63p6H), this time used to estimate the Q-functions.\n\nall comments welcome!', '@stuz5000 we have features that map states to d-dimensional vectors and a small number of actions. i think the whole methodology should also work for state-action features, but then the computational efficiency would suffer. (unless optimizing over actions can be done efficiently..)']",https://arxiv.org/abs/2007.01612,"We consider an online learning problem where the learner interacts with a Markov decision process in a sequence of episodes, where the reward function is allowed to change between episodes in an adversarial manner and the learner only gets to observe the rewards associated with its actions. We allow the state space to be arbitrarily large, but we assume that all action-value functions can be represented as linear functions in terms of a known low-dimensional feature map, and that the learner has access to a simulator of the environment that allows generating trajectories from the true MDP dynamics. Our main contribution is developing a computationally efficient algorithm that we call MDP-LinExp3, and prove that its regret is bounded by $\widetilde{\mathcal{O}}\big(H^2 T^{2/3} (dK)^{1/3}\big)$, where $T$ is the number of episodes, $H$ is the number of steps in each episode, $K$ is the number of actions, and $d$ is the dimension of the feature map. We also show that the regret can be improved to $\widetilde{\mathcal{O}}\big(H^2 \sqrt{TdK}\big)$ under much stronger assumptions on the MDP dynamics. To our knowledge, MDP-LinExp3 is the first provably efficient algorithm for this problem setting. ","Online learning in MDPs with linear function approximation and bandit
  feedback"
163,1280078716706721793,33994542,Frank Schlosser,"[""Our new paper is online! 🎉\n\nWe looked at mobility in Germany during the Covid-19 lockdown and found 1) structural changes in mobility patterns, which 2) strongly affect how infectious diseases spread!\n\nHere's what we found: (1/8)\n\n<LINK>"", 'Mobility was reduced substantially due to the lockdown, up to around -40% below normal.\n\nThis is not really suprising. However, if we look closer at *how* mobility changed ... (2/8) https://t.co/hjMBbztrka', '... we see that it is mostly long-distance trips that are reduced.\n\nThis is important, because it changes the *structure* of the mobility network. (3/8) https://t.co/yixEttAMHI', 'During lockdown (right frame), the mobility network is more local, and more clustered.\n\nThis leads to a reduction of the so-called ""small world"" effect: During lockdown, it is harder to get from one place to a distant location - the world is not ""small"" anymore. (4/8) https://t.co/4W3l0EI4t8', 'We can see this for example in the ""shortest path length"" between locations: \n\nGenerally, if two places are farther away, the shortest path connecting them is longer. But during lockdown, the paths are much longer and they keep growing with distance! (5/8) https://t.co/Nw38MHPcyW', 'In technical terms: The shortest path length L and the clustering coefficient C peak during the lockdown, indicating the strong reduction of the small world effect. (6/8) https://t.co/aDV5gEgwsQ', 'What this all means for infectious diseases is:\n\nEpidemics take longer to spread, and take longer to reach far away places. Effectively, the changes in mobility ""flatten the curve"". (7/8) https://t.co/aSve8YwXE2', ""That's the gist, more details in the paper. (8/8)\n\nThanks to all authors involved for the months of hard work! 🙏 @BenFMaier @davhin11 @AdrianZachariae @DirkBrockmann \n\n(And phew, in personal news, my first paper as a first author. What a journey! 🎉)""]",https://arxiv.org/abs/2007.01583,"In the wake of the COVID-19 pandemic many countries implemented containment measures to reduce disease transmission. Studies using digital data sources show that the mobility of individuals was effectively reduced in multiple countries. However, it remains unclear whether these reductions caused deeper structural changes in mobility networks, and how such changes may affect dynamic processes on the network. Here we use movement data of mobile phone users to show that mobility in Germany has not only been reduced considerably: Lockdown measures caused substantial and long-lasting structural changes in the mobility network. We find that long-distance travel was reduced disproportionately strongly. The trimming of long-range network connectivity leads to a more local, clustered network and a moderation of the ""small-world"" effect. We demonstrate that these structural changes have a considerable effect on epidemic spreading processes by ""flattening"" the epidemic curve and delaying the spread to geographically distant regions. ","COVID-19 lockdown induces disease-mitigating structural changes in
  mobility networks"
164,1279948703432798208,1215176167830867969,Daniel Gilman,"['Dear 7 twitter followers, a new paper I wrote about dark matter substructure and the Hubble constant is out! Check it out: <LINK>']",https://arxiv.org/abs/2007.01308v1,"Time delay cosmography uses the arrival time delays between images in strong gravitational lenses to measure cosmological parameters, in particular the Hubble constant $H_0$. The lens models used in time delay cosmography omit dark matter subhalos and line-of-sight halos because their effects are assumed to be negligible. We explicitly quantify this assumption by analyzing realistic mock lens systems that include full populations of dark matter subhalos and line-of-sight halos, applying the same modeling assumptions used in the literature to infer $H_0$. We base the mock lenses on six quadruply-imaged quasars that have delivered measurements of the Hubble constant, and quantify the additional uncertainties and/or bias on a lens-by-lens basis. We show that omitting dark substructure does not bias inferences of $H_0$. However, perturbations from substructure contribute an additional source of random uncertainty in the inferred value of $H_0$ that scales as the square root of the lensing volume divided by the longest time delay. This additional source of uncertainty, for which we provide a fitting function, ranges from $0.6 - 2.4\%$. It may need to be incorporated in the error budget as the precision of cosmographic inferences from single lenses improves, and sets a precision limit on inferences from single lenses. ","] TDCOSMO III: Dark matter substructure meets dark energy -- the effects
  of (sub)halos on strong-lensing measurements of $H_0$"
165,1279838679394062337,4662190697,André Nicolet,"['Our new paper:\nA continuous family of exact Dispersive Quasi-Normal Modal (DQNM) Expansions for dispersive photonic structures\nMinh Duy Truong, André Nicolet, Guillaume Demésy, Frédéric Zolla\n<LINK>\n\n1/4 <LINK>', 'When you want to do Maxwell equations based model for some photonics device, permittivity is usually time dispersive... To get the resonances (associated to complex frequencies), you have to solve some non linear eigenvalue problems... \n\n2/4', 'We represent the dispersive behaviour by general rational functions of the frequency that fit well the experimental dispersive curves (for real frequencies) and provide analytic continuation to the complex plane...  \n\n3/4', 'Amazingly, there is still some nice superposition principles that allow you to perform modal expansion... and these modal expansions are not unique... there are whole continuous families of them!\n\n4/4']",https://arxiv.org/abs/2007.00470,"In photonics, Dispersive Quasi-Normal Modes (DQNMs) refer to optical resonant modes, solutions of spectral problems associated with Maxwell's equations for open photonic structures involving dispersive media. Since these DQNMs are the constituents determining optical responses, studying DQNM expansion formalisms is the key to model the physical properties of a considered system. In this paper, we emphasize the non-uniqueness of the expansions related to the over-completeness of the set of modes and discuss a family of DQNM expansions depending on continuous parameters that can be freely chosen. These expansions can be applied to dispersive, anisotropic, and even non-reciprocal materials. As an example, we particularly demonstrate the modal analysis on a 2-D scattering model where the permittivity of a silicon object is drawn directly from actual measurement data. ","A continuous family of Exact Dispersive Quasi-Normal Modal (DQNM)
  Expansions for dispersive photonic structures"
166,1279060276189581313,1661813766,Mehdi Kamani,"['Our new paper is out! We investigate the impacts of Compression on #FederatedLearning and present FedCOMGATE, which improves SOTA. We provide sharp guarantees under different settings in FL with Compression. <LINK>\n@Farzinhaddadpou @AryanMokhtari @mehrdadmahdavi <LINK>', 'We show that even without compression, our algorithm matches the SOTA results with no extra control variable. We provide guarantees for general nonconvex, PL/strongly convex, and general convex objective functions, as well as, homogeneous and heterogenous data distributions.', 'We accompanied our theoretical results with extensive experimental results. We investigate both Quantization and Sparsification as the compression method in our algorithm.\n#Quantization #Sparsification #Compression https://t.co/OYVzNyFprN', 'We will have a major code release in the coming weeks for our Distributed Learning and Federated Learning setups, including FedCOMGATE and many more. Stay tuned. For getting updates you can follow my GitHub handle: https://t.co/9Tei2W1QXt\n#MachineLearning #DistributedOptimization', '@aminkarbasi @AryanMokhtari @Farzinhaddadpou @mehrdadmahdavi Thanks 🙏']",https://arxiv.org/abs/2007.01154,"In federated learning, communication cost is often a critical bottleneck to scale up distributed optimization algorithms to collaboratively learn a model from millions of devices with potentially unreliable or limited communication and heterogeneous data distributions. Two notable trends to deal with the communication overhead of federated algorithms are gradient compression and local computation with periodic communication. Despite many attempts, characterizing the relationship between these two approaches has proven elusive. We address this by proposing a set of algorithms with periodical compressed (quantized or sparsified) communication and analyze their convergence properties in both homogeneous and heterogeneous local data distribution settings. For the homogeneous setting, our analysis improves existing bounds by providing tighter convergence rates for both strongly convex and non-convex objective functions. To mitigate data heterogeneity, we introduce a local gradient tracking scheme and obtain sharp convergence rates that match the best-known communication complexities without compression for convex, strongly convex, and nonconvex settings. We complement our theoretical results and demonstrate the effectiveness of our proposed methods by several experiments on real-world datasets. ","Federated Learning with Compression: Unified Analysis and Sharp
  Guarantees"
167,1279017657178763270,893109085,Edouard Grave,"['New work w/ @gizacard (Gautier Izacard): how much do generative models for open domain QA benefit from retrieval? A lot! Retrieving 100 passages, we get 51.4 EM on NaturalQuestions, 67.6 EM on TriviaQA. 1/3\nPaper: <LINK> <LINK>', 'Our main finding: generative models are great at combining information from multiple passages, as their performance keeps improving as the number of support documents increases. 2/3 https://t.co/f1iQ6glPez', 'By processing passages independently in the encoder, but jointly in the decoder, our models scale to large numbers of passages, and can combine information from these multiple passages. 3/3']",https://arxiv.org/abs/2007.01282,"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages. ","Leveraging Passage Retrieval with Generative Models for Open Domain
  Question Answering"
168,1278812181971206147,1194686304493150208,Sheng Liu,"['Excited to share our new paper on Robust Learning with 𝐋𝐚𝐛𝐞𝐥 𝐍𝐨𝐢𝐬𝐞; label noise existed in almost every dataset! \nEarly-Learning Regularization Prevents Memorization of Noisy Labels\nW/ J. N. Weed, @narges_razavian, C. F. Granda\nPaper: <LINK> <LINK>', '2/4 DNNs are observed to first fit clean labels before eventually Memorizing false labels. How does this happen? Unlike prior works focus on non-linear models, we show it is fundamental in high-dimensional classification tasks and can occur even in simple Linear models! https://t.co/neZ7QsMHfL', '3/4 We develop an effective regularization for noisy classification, which exploits the progress of the early learning phase. The regularization automatically ensures that the contribution to the gradient from clean labels large &amp; neutralize the influence from the wrong labels. https://t.co/jW1MmSe7dQ', '4/4 The proposed method is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the SOTA. https://t.co/MtNrYvDAXu']",https://arxiv.org/abs/2007.00151,"We propose a novel framework to perform classification via deep learning in the presence of noisy annotations. When trained on noisy labels, deep neural networks have been observed to first fit the training data with clean labels during an ""early learning"" phase, before eventually memorizing the examples with false labels. We prove that early learning and memorization are fundamental phenomena in high-dimensional classification tasks, even in simple linear models, and give a theoretical explanation in this setting. Motivated by these findings, we develop a new technique for noisy classification tasks, which exploits the progress of the early learning phase. In contrast with existing approaches, which use the model output during early learning to detect the examples with clean labels, and either ignore or attempt to correct the false labels, we take a different route and instead capitalize on early learning via regularization. There are two key elements to our approach. First, we leverage semi-supervised learning techniques to produce target probabilities based on the model outputs. Second, we design a regularization term that steers the model towards these targets, implicitly preventing memorization of the false labels. The resulting framework is shown to provide robustness to noisy annotations on several standard benchmarks and real-world datasets, where it achieves results comparable to the state of the art. ",Early-Learning Regularization Prevents Memorization of Noisy Labels
169,1278680492364173314,133148364,Devendra Chaplot,"['Posted new paper on Object Goal Navigation describing our winning entry to the #CVPR2020 Habitat ObjectNav Challenge.\n\nArxiv: <LINK>\n\nWebpage: <LINK>\n\nOur model also works in the real-world on a Locobot! 👇\n\nw. @_dgandhi, A. Gupta, @rsalakhu <LINK>', '@kesarito @_dgandhi @rsalakhu Yes! We are working on that.']",https://arxiv.org/abs/2007.00643,"This work studies the problem of object goal navigation which involves navigating to an instance of the given object category in unseen environments. End-to-end learning-based navigation methods struggle at this task as they are ineffective at exploration and long-term planning. We propose a modular system called, `Goal-Oriented Semantic Exploration' which builds an episodic semantic map and uses it to explore the environment efficiently based on the goal object category. Empirical results in visually realistic simulation environments show that the proposed model outperforms a wide range of baselines including end-to-end learning-based methods as well as modular map-based methods and led to the winning entry of the CVPR-2020 Habitat ObjectNav Challenge. Ablation analysis indicates that the proposed model learns semantic priors of the relative arrangement of objects in a scene, and uses them to explore efficiently. Domain-agnostic module design allow us to transfer our model to a mobile robot platform and achieve similar performance for object goal navigation in the real-world. ",Object Goal Navigation using Goal-Oriented Semantic Exploration
170,1278614383539695617,1140356013901721600,Stefanie Speidel@NCTDresden,"['Want to anticipate instrument usage in surgery? Check out our new @MICCAI2020 paper, we define the novel anticipation task based on video and show that we can identify instrument correlations through uncertainty quantification. <LINK>\xa0@TactileInternet #SurgicalAI <LINK>']",https://arxiv.org/abs/2007.00548,"Intra-operative anticipation of instrument usage is a necessary component for context-aware assistance in surgery, e.g. for instrument preparation or semi-automation of robotic tasks. However, the sparsity of instrument occurrences in long videos poses a challenge. Current approaches are limited as they assume knowledge on the timing of future actions or require dense temporal segmentations during training and inference. We propose a novel learning task for anticipation of instrument usage in laparoscopic videos that overcomes these limitations. During training, only sparse instrument annotations are required and inference is done solely on image data. We train a probabilistic model to address the uncertainty associated with future events. Our approach outperforms several baselines and is competitive to a variant using richer annotations. We demonstrate the model's ability to quantify task-relevant uncertainties. To the best of our knowledge, we are the first to propose a method for anticipating instruments in surgery. ","Rethinking Anticipation Tasks: Uncertainty-aware Anticipation of Sparse
  Surgical Instrument Usage for Context-aware Assistance"
171,1278512349549379585,139709337,David Dumas,"['New paper with Andrew Neitzke, ""Opers and nonabelian Hodge: numerical studies"".  Our computer experiments provide evidence for a conjecture of Gaiotto-Moore-Neitzke and Gaiotto.  <LINK>', ""And we've release our software as open source!  It can compute Stokes data of polynomial opers, flat connections associated to polynomial Higgs bundles, and more. https://t.co/SFqvuwWPly"", ""Here's an example of a visualization made with this software (building on Andy's spectral network plotter): The critical trajectories of a polynomial quadratic differential (left) and the ideal polygon that is image of the harmonic map from  ℂ to ℍ² that it determines (right). https://t.co/YIDdi2mZZa"", ""We've been working on this for a while, and we're pretty stoked 😉 to finally be making it public!  Comments are welcome.""]",https://arxiv.org/abs/2007.00503,"We present numerical experiments that test the predictions of a conjecture of Gaiotto-Moore-Neitzke and Gaiotto concerning the monodromy map for opers, the nonabelian Hodge correspondence, and the restriction of Hitchin's hyperk\""ahler metric to the Hitchin section. These experiments are conducted in the setting of polynomial holomorphic differentials on the complex plane, where the predictions take the form of conjectural formulas for the Stokes data and the Hitchin metric tensor. Overall, the results of our experiments support the conjecture. ",Opers and nonabelian Hodge: numerical studies
172,1278487869032726528,3377160202,Djuna Croon,"[""New paper!\n\n<LINK> \nMissing in Axion: where are Xenon1T's big black holes?\n\nwith two absolute legends: Sam McDermott and @JeremySakstein\n\nWe pioneer the black hole mass gap as a powerful new tool for constraining new particles.\n\nThread:"", '1/ The detection of gravitational waves by @LIGO and @ego_virgo affords us the unprecedented opportunity to examine celestial objects under a new microscope, and to test our theories of nature using the most extreme objects in the universe: black holes.', '2/ As gravitational wave astronomy enters the realm of precision science, the large volume of detected mergers will become a sensitive probe of the astrophysical populations of black holes: the remnants of supernovae explosions at the end of the lives of heavy stars.', '3/ We propose, for the first time, to use black hole population studies to probe new light particles. In particular, we show that they may probe the currently very popular electrophilic axion, which may explain the excess measured in the @XENONexperiment. Huh, but how?', '4/ Well, it turns out that stellar structure theory predicts the existence of a black hole MASS GAP. That is, there is a large range of masses for which black holes just cannot be formed. To see why, you need to consider the final chemical burning stages of a very heavy star.', '5/ This image shows the evolution (starting at the bottom, moving to the top) of the temperature and density of three different stars: of 40 solar masses (green), 70 solar masses (pink), and 120 solar masses (blue). The figure also shows a dashed ""danger zone"". https://t.co/yXHHxl1kH1', '6/ What is this danger zone (labeled unstable)? If a star comes near it, the production of electron-positron pairs from the hot plasma is so fast, that the star cannot be supported by radiation pressure anymore and will start to IMPLODE prematurely.', '7/ So, what happens to our example stars? As you can see, the 40 solar mass star (the green line) does not come near the danger zone, and proceeds as normal. This star will eventually become a black hole.', '8/ The 70 solar mass star grazes the zone, and starts to fluctuate. What happens is a sequence of implosions and thermonuclear explosions, shedding mass at every step. We call it a pulsation pair-instability supernova (PPISN) - it results in a lighter black hole.', '9/ The 120 solar mass star (blue line) enters the danger zone, and the subsequent implosion triggers such an energetic thermonuclear explosion, that all of the material is blown away. This is called a pair-instability supernova (PISN), and it results in NO REMNANT.', '10/ This image shows the masses of heavy stars vs. the masses of their black hole remnants. From the y-axis, you can read off that there is a maximum black hole mass of about 40 solar masses. Above that, nothing. This is therefore the lower edge of the mass gap. https://t.co/DjnoHAOo5G', '11/ If we would have continued this figure to the right, eventually there would be larger black holes again, of about 120 solar masses and upwards: these define the upper edge of the mass gap.', '12/ The mergers detected by @LIGO and @ego_virgo allow us to study masses of black holes in binaries. With that data, we may test our understanding of the mass gap, and therefore of stellar structure theory and nuclear physics. We cited some pioneering papers, check them out!', '13/ But, as we show here for the first time, we may also use it to test our understanding of particle physics. In particular, if new light particles exist and couple to the material in the star, they will change its final stages.', '14/ Such particles will act as an additional way to lose energy, accelerating the processes in the supernova in such a way that the pair-instability pulsations become less violent. Therefore, you would expect bigger black holes, within the original mass gap.', '15/ And indeed, performing simulations with the MESA code, that is what we find. Look in particular at the red line and dots, which correspond to the characteristic new particle suggested by the XENON1T experiment. https://t.co/PWIHUJJtAo', '16/ On the right of this plot, we also show the black hole masses inferred from the first two observing runs of the LIGO/Virgo experiments, with their error bars. It it seen that both the standard and the axion case are currently consistent, but we will know more soon.', ""17/ Here is a clearer image of the mass gap, with the strength of the coupling between axions and electrons on the x-axis. It also shows the region with which the XENON1T excess can be explained. The difference is ten times the mass of the sun - that's huge! https://t.co/2GBdbFmizi"", ""18/ So, we have shown that new particles can be tested through the masses of black holes. Isn't that cool? We demonstrated the effect with XENON1T's electrophilic axion that is on everyone's mind right now. But that is not all..."", '19/ WE CAN LEARN SO MUCH MORE FROM THIS. Stay tuned for another work, in which we will explore several other effects of new physics in detail, which will appear VERY SOON. And we have so many other things in mind!', '20/ To note, the LIGO/Virgo third observation run has observed many more black hole mergers. When that data gets released, we can learn even more. With the next generation of gravitational wave detectors, the situation will get yet better. THE SKY IS THE LIMIT!']",https://arxiv.org/abs/2007.00650,"We pioneer the black hole mass gap as a powerful new tool for constraining new particles. A new particle that couples to the Standard Model---such as an axion---acts as an additional source of loss in the cores of population-III stars, suppressing mass lost due to winds and quenching the pair-instability. This results in heavier astrophysical black holes. As an example, using stellar simulations we show that the solar axion explanation of the recent XENON1T excess implies astrophysical black holes of ~ 56 MS, squarely within the black hole mass gap predicted by the Standard Model. ",Missing in Axion: where are XENON1T's big black holes?
173,1291018278018715648,281711973,Dr. Emily Rickman,"[""I'm happy to announce our paper on a new technique to directly find disks or companions AROUND directly imaged companions of stars (woo, exomoons! 🌜) with a possible detection using @SPHERE_outreach led by C. Lazzoni\n\n👇👇👇\n\n<LINK>""]",https://arxiv.org/abs/2007.10097,"In recent decades, thousands of substellar companions have been discovered with both indirect and direct methods of detection. In this paper, we focus our attention on substellar companions detected with the direct imaging technique, with the primary goal of investigating their close surroundings and looking for additional companions and satellites, as well as disks and rings. Any such discovery would shed light on many unresolved questions, particularly with regard to their possible formation mechanisms. To reveal bound features of directly imaged companions we need to suppress the contribution from the source itself. Therefore, we developed a method based on the negative fake companion (NEGFC) technique that first estimates the position in the field of view (FoV) and the flux of the imaged companion, then subtracts a rescaled model point spread function (PSF) from the imaged companion. Next it performs techniques, such as angular differential imaging (ADI), to further remove quasi-static patterns of the star. We applied the method to the sample of substellar objects observed with SPHERE during the SHINE GTO survey. Among the 27 planets and brown dwarfs we analyzed, we detected a possible point source close to DH Tau B. This candidate companion was detected in four different SPHERE observations, with an estimated mass of $\sim 1$ M\textsubscript{Jup}, and a mass ratio with respect to the brown dwarf of $1/10$. This binary system, if confirmed, would be the first of its kind, opening up interesting questions for the formation mechanism, evolution, and frequency of such pairs. In order to address the latter, the residuals and contrasts reached for 25 companions in the sample of substellar objects observed with SPHERE were derived. If the DH Tau Bb companion is real, the binary fraction obtained is $\sim 7\%$, which is in good agreement with the results obtained for field brown dwarfs. ","The search for disks or planetary objects around directly imaged
  companions: A candidate around DH Tau B"
174,1288637695787327488,1128508767404838912,Thayne Currie,"['Pleased to announce our new paper demonstrating spatial ""linear field dark control"", a new wavefront control method for better imaging planets in reflected light with NASA missions and ELTs.  \n<LINK> <LINK>']",https://arxiv.org/abs/2007.14413,"Imaging planets in reflected light, a key focus of future NASA missions and ELTs, requires advanced wavefront control to maintain a deep, temporally correlated null of stellar halo -- i.e. a dark hole -- at just several diffraction beam widths. Using the Ames Coronagraph Experiment testbed, we present the first laboratory tests of Spatial Linear Dark Field Control (LDFC) approaching raw contrasts ($\sim$ 5$\times$10$^{-7}$) and separations (1.5--5.2 $\lambda$/D) needed to image jovian planets around Sun-like stars with space-borne coronagraphs like WFIRST-CGI and image exo-Earths around low-mass stars with future ground-based 30m class telescopes. In four separate experiments and for a range of different perturbations, LDFC largely restores (to within a factor of 1.2--1.7) and maintains a dark hole whose contrast is degraded by phase errors by an order of magnitude. Our implementation of classical speckle nulling requires a factor of 2--5 more iterations and 20--50 DM commands to reach contrasts obtained by spatial LDFC. Our results provide a promising path forward to maintaining dark holes without relying on DM probing and in the low-flux regime, which may improve the duty cycle of high-contrast imaging instruments, increase the temporal correlation of speckles, and thus enhance our ability to image true solar system analogues in the next two decades. ","Laboratory Demonstration of Spatial Linear Dark Field Control For
  Imaging Extrasolar Planets in Reflected Light"
175,1286487690783780869,4666231375,Konstantin Batygin,"['Even ""semi-active"" particles whose direct gravitational coupling is turned off, can still interact with one-another within standard N-body simulations by perturbing the central body. For details, check out our new paper, led by Shirui Peng: <LINK> @Caltech <LINK>']",https://arxiv.org/abs/2007.11758,"Over the course of the recent decades, $N$-body simulations have become a standard tool for quantifying the gravitational perturbations that ensue in planet-forming disks. Within the context of such simulations, massive non-central bodies are routinely classified into ""big"" and ""small"" particles, where big objects interact with all other objects self-consistently, while small bodies interact with big bodies but not with each other. Importantly, this grouping translates to an approximation scheme where the orbital evolution of small bodies is dictated entirely by the dynamics of the big bodies, yielding considerable computational advantages with little added cost in terms of astrophysical accuracy. Here we point out, however, that this scheme can also yield spurious dynamical behaviour, where even in absence of big bodies within a simulation, indirect coupling among small bodies can lead to excitation of the constituent ""non-interacting"" orbits. We demonstrate this self-stirring by carrying out a sequence of numerical experiments, and confirm that this effect is largely independent of the time-step or the employed integration algorithm. Furthermore, adopting the growth of angular momentum deficit as a proxy for dynamical excitation, we explore its dependence on time, the cumulative mass of the system, as well as the total number of particles present in the simulation. Finally, we examine the degree of such indirect excitation within the context of conventional terrestrial planet formation calculations, and conclude that although some level of caution may be warranted, this effect plays a negligible role in driving the simulated dynamical evolution. ","Interactions Among Non-Interacting Particles in Planet Formation
  Simulations"
176,1285913484400762880,1403815458,Amir Siraj,['New paper on the arXiv! We show that an early solar binary companion would have increased the chances of capturing both the Oort Cloud and Planet Nine <LINK>'],https://arxiv.org/abs/2007.10339,"We show that an equal-mass, temporary binary companion to the Sun in the solar birth cluster at a separation of $\sim 10^3 \; \mathrm{\; AU}$ would have increased the likelihood of forming the observed population of outer Oort cloud objects and of capturing Planet Nine. In particular, the discovery of a captured origin for Planet Nine would favor our binary model by an order of magnitude relative to a lone stellar history. Our model predicts an overabundance of dwarf planets, discoverable by LSST, with similar orbits to Planet Nine, which would result from capture by the stellar binary. ",The Case for an Early Solar Binary Companion
177,1285818689246257152,489909633,Prof. Melanie J-H,"['New paper on diffuse source &amp; cosmic web detection with @mwatelescope &amp; ASKAP by Hodgson et al. <LINK>. Turns out if you want to detect the cosmic web, ASKAP, is better than MWA Phase II, even corrected for frequency. We should fix that! <LINK>']",https://arxiv.org/abs/2007.10578,"We follow up on a report by Vacca et al. (2018) of 28 candidate large-scale diffuse synchrotron sources in an 8{\deg}$\times$8{\deg} area of the sky (centred at RA 5h0m0s Dec 5{\deg}48'0''). These sources were originally observed at 1.4 GHz using a combination of the single-dish Sardinia Radio Telescope (SRT) and archival NRAO VLA Sky Survey (NVSS) data. They are in an area with nine massive galaxy clusters at z $\approx$ 0.1, and are candidates for the first detection of filaments of the synchrotron cosmic web. We attempt to verify these candidate sources with lower frequency observations at 154 MHz with the Murchison Widefield Array (MWA) and at 887 MHz with the Australian Square Kilometre Array Pathfinder (ASKAP). We use a novel technique to calculate the surface brightness sensitivity of these instruments to show that our lower frequency observations, and in particular those by ASKAP, are ideally suited to detect large-scale, extended synchrotron emission. Nonetheless, we are forced to conclude that none of these sources are likely to be synchrotron in origin or associated with the cosmic web. ","Low(er) frequency follow-up of 28 candidate, large-scale synchrotron
  sources"
178,1285531971830112262,822078864507731969,Sergi Abadal,"['New @VisorSurf paper! Take a look to the details below: ""Radiation pattern prediction for Metasurfaces: A Neural Network based approach"". Arxiv link <LINK> <LINK>']",https://arxiv.org/abs/2007.08035,"As the current standardization for the 5G networks nears completion, work towards understanding the potential technologies for the 6G wireless networks is already underway. One of these potential technologies for the 6G networks are Reconfigurable Intelligent Surfaces (RISs). They offer unprecedented degrees of freedom towards engineering the wireless channel, i.e., the ability to modify the characteristics of the channel whenever and however required. Nevertheless, such properties demand that the response of the associated metasurface (MSF) is well understood under all possible operational conditions. While an understanding of the radiation pattern characteristics can be obtained through either analytical models or full wave simulations, they suffer from inaccuracy under certain conditions and extremely high computational complexity, respectively. Hence, in this paper we propose a novel neural networks based approach that enables a fast and accurate characterization of the MSF response. We analyze multiple scenarios and demonstrate the capabilities and utility of the proposed methodology. Concretely, we show that this method is able to learn and predict the parameters governing the reflected wave radiation pattern with an accuracy of a full wave simulation (98.8%-99.8%) and the time and computational complexity of an analytical model. The aforementioned result and methodology will be of specific importance for the design, fault tolerance and maintenance of the thousands of RISs that will be deployed in the 6G network environment. ","Radiation pattern prediction for Metasurfaces: A Neural Network based
  approach"
179,1285492962672222208,28378010,Paul A. Strøm,"['Our new paper is out on arXiv today: ""Exocomets from a Solar System Perspective"" <LINK>\n\nIn this topical review paper we provide an overview of the observational properties of Solar System #comets and #exocomets. (1/10) <LINK>', 'The paper aims to highlight commonalities and to discuss differences which may aid the communication between the involved research communities and perhaps also avoid misconceptions. (2/10)', 'A major difference between the observations of Solar System #comets and #exocomets is that the former are studied individually, whereas the latter generally cannot be resolved. Compared to Solar System comets, the information we have about exocomets is very limited. (3/10)', 'Yet there are hints that they may not be too different in composition... 😱 (4/10)', 'Observations of gas around main sequence stars, spectroscopic observations of ""polluted"" white dwarf atmospheres and spectroscopic observations of transiting exocomets suggest that exocomets may show compositional similarities with Solar System comets. (5/10)', 'For instance, the CaII lines commonly seen in the spectra of beta Pic and polluted WDs have been detected in the extreme case of the large sungrazing comet C/1965 S1 Ikeya-Seki. (6/10)', 'Solar system comets emit in high energy EUV and X-ray emission through the gradual neutralisation of highly charged solar wind ions. Similar processes are also thought to occur at exocomets encountering stellar winds (as seen by the variations of highly ionised species). (7/10)', 'Observations of interstellar visitors such as 1I/`Oumuamua and 2I/Borisov allow us to learn about the physical and chemical properties of protoplanetary disks of distant stars, although their true systems of origin are unknown to us. (8/10)', 'This raises the tantalising prospect that observations of interstellar comets may help bridge the fields of exocomet and Solar System comets. (9/10)', 'If you have a an interest in debris disks, white dwarf atmospheres and/or (exo)comets, this paper will likely be of interest to you. It is also the first time I publish as a first author under my new surname: Strøm. Enjoy! (10/10)']",https://arxiv.org/abs/2007.09155,"Exocomets are small bodies releasing gas and dust which orbit stars other than the Sun. Their existence was first inferred from the detection of variable absorption features in stellar spectra in the late 1980s using spectroscopy. More recently, they have been detected through photometric transits from space, and through far-IR/mm gas emission within debris disks. As (exo)comets are considered to contain the most pristine material accessible in stellar systems, they hold the potential to give us information about early stage formation and evolution conditions of extra Solar Systems. In the Solar System, comets carry the physical and chemical memory of the protoplanetary disk environment where they formed, providing relevant information on processes in the primordial solar nebula. The aim of this paper is to compare essential compositional properties between Solar System comets and exocomets. The paper aims to highlight commonalities and to discuss differences which may aid the communication between the involved research communities and perhaps also avoid misconceptions. Exocomets likely vary in their composition depending on their formation environment like Solar System comets do, and since exocomets are not resolved spatially, they pose a challenge when comparing them to high fidelity observations of Solar System comets. Observations of gas around main sequence stars, spectroscopic observations of ""polluted"" white dwarf atmospheres and spectroscopic observations of transiting exocomets suggest that exocomets may show compositional similarities with Solar System comets. The recent interstellar visitor 2I/Borisov showed gas, dust and nuclear properties similar to that of Solar System comets. This raises the tantalising prospect that observations of interstellar comets may help bridge the fields of exocomet and Solar System comets. ",Exocomets from a Solar System Perspective
180,1285220814665723904,1004365363574902784,Kevin J. Kelly,"[""Digging into the weeds of our new paper <LINK> a little bit. Here's the key reason that NOvA and T2K have begun to (in combination) prefer the inverted mass ordering over the normal mass ordering."", 'In the middle panel here, we have fixed our analysis to be *only* in the Normal mass ordering. As you can see, the blue (NOvA) and red (T2K) regions are pretty much perfectly complementary. @Tokai2Kamioka wants to have delta_CP = -\\pi/2 and sin^2 q_{23} of about 0.55. https://t.co/fXLi8DIP9a', '@novaexperiment wants nothing to do with that combination of parameters -- it prefers a value of delta_CP closer to 0 and smaller sin^2 q_{23}.', 'However, if you take a look at the bottom panel (where we fix to be in the inverted ordering), both experiments are relatively happy at delta_CP = -\\pi/2!', 'Despite each experiment on its own preferring the Normal Ordering (at low significance), their combination actually ends up preferring the Inverted Ordering (again, low significance).']",https://arxiv.org/abs/2007.08526,"We inspect recently updated neutrino oscillation data -- specifically coming from the Tokai to Kamioka and NuMI Off-axis $\nu_e$ Appearance experiments -- and how they are analyzed to determine whether the neutrino mass ordering is normal ($m_1 < m_2 < m_3$) or inverted ($m_3 < m_1 < m_2$). We show that, despite previous results giving a strong preference for the normal ordering, with the newest data from T2K and NOvA, this preference has all but vanished. Additionally, we highlight the importance of this result for non-oscillation probes of neutrinos, including neutrinoless double beta decay and cosmology. Future experiments, including JUNO, DUNE, and T2HK will provide valuable information and determine the mass ordering at a high confidence level. ","Back to (Mass-)Square(d) One: The Neutrino Mass Ordering in Light of
  Recent Data"
181,1284663004164833281,971652127079546880,Qi Dou,['Our @MICCAI2020 paper presents a new shape-aware meta-learning (SAML) method for generalization of image segmentation models to completely unseen domains <LINK>\n\nCode: <LINK>\nData: (6 prostate MRI sets): <LINK> <LINK>'],https://arxiv.org/abs/2007.02035,"Model generalization capacity at domain shift (e.g., various imaging protocols and scanners) is crucial for deep learning methods in real-world clinical deployment. This paper tackles the challenging problem of domain generalization, i.e., learning a model from multi-domain source data such that it can directly generalize to an unseen target domain. We present a novel shape-aware meta-learning scheme to improve the model generalization in prostate MRI segmentation. Our learning scheme roots in the gradient-based meta-learning, by explicitly simulating domain shift with virtual meta-train and meta-test during training. Importantly, considering the deficiencies encountered when applying a segmentation model to unseen domains (i.e., incomplete shape and ambiguous boundary of the prediction masks), we further introduce two complementary loss objectives to enhance the meta-optimization, by particularly encouraging the shape compactness and shape smoothness of the segmentations under simulated domain shift. We evaluate our method on prostate MRI data from six different institutions with distribution shifts acquired from public datasets. Experimental results show that our approach outperforms many state-of-the-art generalization methods consistently across all six settings of unseen domains. ","Shape-aware Meta-learning for Generalizing Prostate MRI Segmentation to
  Unseen Domains"
182,1284035229200658432,2491211646,Matthieu Bethermin,"['Is there already a lot of dust-obscured (orange and red) star formation 1 Gyr after the Big Bang? (uncorrected UV in blue) The new paper of Yana Khusanova (former PhD student at @LAM_Marseille) and the ALPINE collaboration suggest that it is probably true! <LINK> <LINK>', 'We also used the same dataset to constrain the position of the ""main-sequence"" of star-forming galaxies using both UV and FIR data in the 4&lt;z&lt;6 range. No big difference with dust-corrected UV. Phew! https://t.co/phe62urECd']",https://arxiv.org/abs/2007.08384,"Star formation rate (SFR) measurements at z>4 have relied mostly on rest-frame far-ultraviolet (FUV) observations. The corrections for dust attenuation based on IRX-$\beta$ relation are highly uncertain and are still debated in the literature. Hence, rest-frame far-infrared (FIR) observations are necessary to constrain the dust-obscured component of the SFR. In this paper, we exploit the rest-frame FIR continuum observations collected by the ALMA Large Program to INvestigate [CII] at Early times (ALPINE) to directly constrain the obscured SFR in galaxies at 4.4<z<5.9. We use stacks of continuum images to measure average infrared (IR) luminosities taking into account both detected and undetected sources. Based on these measurements, we measure the position of the main sequence of star-forming galaxies and the specific SFR (sSFR) at $z\sim4.5$ and $z\sim5.5$. We find that the main sequence and sSFR do not evolve significantly between $z\sim4.5$ and $z\sim5.5$, as opposed to lower redshifts. We develop a method to derive the obscured SFR density (SFRD) using the stellar masses or FUV-magnitudes as a proxy of FIR fluxes measured on the stacks and combining them with the galaxy stellar mass functions and FUV luminosity functions from the literature. We obtain consistent results independent of the chosen proxy. We find that the obscured fraction of SFRD is decreasing with increasing redshift but even at $z\sim5.5$ it constitutes around 61\% of the total SFRD. ","The ALPINE-ALMA [CII] Survey: Obscured Star Formation Rate Density and
  Main Sequence of star-forming galaxies at z&gt;4"
183,1283949727990644737,625753157,Muntazir Abidi,"['Our new paper on ""Density field reconstruction and its application to primordial non-Gaussianity"". <LINK>\n\n#cosmology #nonGaussianity #LSS #reconstruction #initialconditions', 'Our reconstructed initial field (from non-linear dark matter halo field) is  &gt;90% correlated with the True initial field in N-body simulations. https://t.co/AQUDAYzPwc']",https://arxiv.org/abs/2007.08472,"Large-scale Fourier modes of the cosmic density field are of great value for learning about cosmology because of their well-understood relationship to fluctuations in the early universe. However, cosmic variance generally limits the statistical precision that can be achieved when constraining model parameters using these modes as measured in galaxy surveys, and moreover, these modes are sometimes inaccessible due to observational systematics or foregrounds. For some applications, both limitations can be circumvented by reconstructing large-scale modes using the correlations they induce between smaller-scale modes of an observed tracer (such as galaxy positions). In this paper, we further develop a formalism for this reconstruction, using a quadratic estimator similar to the one used for lensing of the cosmic microwave background. We incorporate nonlinearities from gravity, nonlinear biasing, and local-type primordial non-Gaussianity, and verify that the estimator gives the expected results when applied to N-body simulations. We then carry out forecasts for several upcoming surveys, demonstrating that, when reconstructed modes are included alongside directly-observed tracer density modes, constraints on local primordial non-Gaussianity are generically tightened by tens of percents compared to standard single-tracer analyses. In certain cases, these improvements arise from cosmic variance cancellation, with reconstructed modes taking the place of modes of a separate tracer, thus enabling an effective ""multitracer"" approach with single-tracer observations. ","Density reconstruction from biased tracers and its application to
  primordial non-Gaussianity"
184,1283386196278808576,281711973,Dr. Emily Rickman,['New paper alert!\n\nIn the latest SPHERE infrared survey for exoplanets (SHINE) paper led by @ArthurVigan we present the demographics of young giant exoplanets below 300 AU with SPHERE\n\n👇👇👇\n\n<LINK> <LINK>'],https://arxiv.org/abs/2007.06573,"The SHINE project is a 500-star survey performed with SPHERE on the VLT for the purpose of directly detecting new substellar companions and understanding their formation and early evolution. Here we present an initial statistical analysis for a subsample of 150 stars that are representative of the full SHINE sample. Our goal is to constrain the frequency of substellar companions with masses between 1 and 75 MJup and semimajor axes between 5 and 300 au. We adopt detection limits as a function of angular separation from the survey data for all stars converted into mass and projected orbital separation using the BEX-COND-hot evolutionary tracks and known distance to each system. Based on the results obtained for each star and on the 13 detections in the sample, we use a MCMC tool to compare our observations to two different types of models. The first is a parametric model based on observational constraints, and the second type are numerical models that combine advanced core accretion and gravitational instability planet population synthesis. Using the parametric model, we show that the frequencies of systems with at least one substellar companion are $23.0_{-9.7}^{+13.5}\%$, $5.8_{-2.8}^{+4.7}\%$, and $12.6_{-7.1}^{+12.9}\%$ for BA, FGK, and M stars, respectively. We also demonstrate that a planet-like formation pathway probably dominates the mass range from 1-75 MJup for companions around BA stars, while for M dwarfs, brown dwarf binaries dominate detections. In contrast, a combination of binary star-like and planet-like formation is required to best fit the observations for FGK stars. Using our population model and restricting our sample to FGK stars, we derive a frequency of $5.7_{-2.8}^{+3.8}\%$, consistent with predictions from the parametric model. More generally, the frequency values that we derive are in excellent agreement with values obtained in previous studies. ","The SPHERE infrared survey for exoplanets (SHINE). III. The demographics
  of young giant exoplanets below 300 au with SPHERE"
185,1282473344428908546,1001049754787368960,Dr. Yu-Dai Tsai,"['Another new paper: <LINK>\nA new neutrino explanation to Xenon 1T, that is not constrained by astrophysical bounds!\nGreat thanks to @TheoristIan and Jason Wyenberg for this exciting collaboration.', '@QuantumMessage @TheoristIan Hi Djuna, it is indeed very interesting to think about other effects of these exotic sterile neutrinos. I am happy to chat more about this!']",https://arxiv.org/abs/2007.05513,"In this short letter, we find that a magnetic transition dipole moment between tau and sterile neutrinos can account for the XENON1T excess events. Unlike the ordinary neutrino dipole moment, the introduction of the new sterile mass scale allows for astrophysical bounds to be suppressed. Interestingly, the best-fit regions that are compatible with the SN1987A imply either boron-8 or CNO neutrinos as the source flux. We find that sterile neutrinos of either $\sim$ 260 keV or in the $\sim$(500 - 800) keV mass range are capable of evading astrophysical constraints while being able to successfully explain the XENON1T event rate. The sterile neutrino in the best fit parameter space may have significant effects on big bang nucleosynthesis (BBN). We show the region in which a low reheating temperature of the Universe may allow the BBN constraints to be alleviated. ","An Active-to-Sterile Neutrino Transition Dipole Moment and the XENON1T
  Excess"
186,1280694676824248320,156804540,Francisco Rodrigues,"['Our new paper on @arxiv_org : We study random geographic graphs by using concepts of random matrix theory. With @eestradalab, Antonio Bermúdez and collaborators. #networks \n<LINK> <LINK>']",https://arxiv.org/abs/2007.02453,"In this work we perform a detailed statistical analysis of topological and spectral properties of random geometric graphs (RGGs); a graph model used to study the structure and dynamics of complex systems embedded in a two dimensional space. RGGs, $G(n,\ell)$, consist of $n$ vertices uniformly and independently distributed on the unit square, where two vertices are connected by an edge if their Euclidian distance is less or equal than the connection radius $\ell \in [0,\sqrt{2}]$. To evaluate the topological properties of RGGs we chose two well-known topological indices, the Randi\'c index $R(G)$ and the harmonic index $H(G)$. While we characterize the spectral and eigenvector properties of the corresponding randomly-weighted adjacency matrices by the use of random matrix theory measures: the ratio between consecutive eigenvalue spacings, the inverse participation ratios and the information or Shannon entropies $S(G)$. First, we review the scaling properties of the averaged measures, topological and spectral, on RGGs. Then we show that: (i) the averaged--scaled indices, $\left\langle R(G) \right\rangle$ and $\left\langle H(G) \right\rangle$, are highly correlated with the average number of non-isolated vertices $\left\langle V_\times(G) \right\rangle$; and (ii) surprisingly, the averaged--scaled Shannon entropy $\left\langle S(G) \right\rangle$ is also highly correlated with $\left\langle V_\times(G) \right\rangle$. Therefore, we suggest that very reliable predictions of eigenvector properties of RGGs could be made by computing topological indices. ",Topological versus spectral properties of random geometric graphs
187,1292715732061626368,786855300322172928,Alkistis Pourtsidou,"['Paper alert (with a delay due to holiday season!) -- in <LINK> led by @CunningtonSD  we studied the degeneracy between primordial non-gaussianity (PNG) and foreground removal systematics for intensity mapping experiments [thread].', 'Foreground removal methods remove some signal (unless one is very conservative, but then the error budget becomes much larger due to the residuals), especially on the large scales, where PNG signatures are expected to lie!', 'With simulated data and MCMC we studied the effects of this on the precision accuracy with which we can probe the f_NL local parameter with a large SKA1-MID @SKA_telescope survey, using FG removal methods that are currently used in real data analyses.', 'If we ignore the possibility of FG removal effects on the signal, we get *extremely biased* (wrong) estimates of f_NL, as shown in the purple contour where our fiducial f_NL = 0. The other contours correspond to the unrealistic cases where FG removal is 0 or perfectly known. https://t.co/Dugeg7WHzB', 'To add some realism, we devised a model with 1 free, nuisance parameter, which is marginalised over to account for FG removal properly. We found that it works well, and that we can recover unbiased estimates (but the f_NL uncertainties increase a lot!). https://t.co/QIxyRc9uyP', 'This result has implications also for cross-correlations and multi-tracer methods, and it means that further work is required to get the precision we need to be competitive. We are thinking about solutions -- more, hopefully, soon!', ""As an aside, note that we have made suites of simulated data and power spectrum and MCMC codes available at our group's repository https://t.co/FCvccjqwAy -- @CunningtonSD @psahds also see https://t.co/aoXY9jEffy""]",https://arxiv.org/abs/2007.12126,"Potential evidence for primordial non-Gaussianity (PNG) is expected to lie in the largest scales mapped by cosmological surveys. Forthcoming 21cm intensity mapping experiments will aim to probe these scales by surveying neutral hydrogen (HI) within galaxies. However, foreground signals dominate the faint 21cm emission, meaning foreground cleaning is required to recover the cosmological signal. The effect this has is to damp the HI power spectrum on the largest scales, especially along the line-of-sight. Whilst there is agreement that this contamination is potentially problematic for probing PNG, it is yet to be fully explored and quantified. In this work we carry out the first forecasts on $f_\text{NL}$ that incorporate simulated foreground maps that are removed using techniques employed in real data. Using an MCMC analysis, we demonstrate that foreground cleaned data recovers hugely biased values ($f_\text{NL} = -102.1_{-7.96}^{+8.39}$ [68% CL]) on our $f_\text{NL}=0$ fiducial input. Introducing a model with fixed parameters for the foreground contamination allows us to recover unbiased results ($f_\text{NL} = -2.94_{-11.9}^{+11.4}$). However, it is not clear that we will have sufficient understanding of foreground contamination to allow for such rigid models. Treating the main parameter $k_\parallel^\text{FG}$ in our foreground model as a nuisance parameter and marginalizing over it, still recovers unbiased results but at the expense of much larger errors ($f_\text{NL} = 0.75^{+40.2}_{-44.5}$), that can only be reduced by imposing the Planck 2018 prior. Our results show that significant progress on understanding and controlling foreground removal effects is necessary in order to study PNG with HI intensity mapping. ","The degeneracy between primordial non-Gaussianity and foregrounds in
  21cm intensity mapping experiments"
188,1292098113902202880,1137068512286007297,Monica Agrawal,"['Clinical notes are hard to understand: for people and computers. In our paper (<LINK>) presented at #MLHC2020,  we dive into the current performance of clinical entity extraction algorithms (hint: they could be better) and propose a path forward for clinical NLP. <LINK>', ""Joint work with @david_sontag and several wonderful Twitter-less coauthors at @MIT_CSAIL and @MGHMedicine. Learn more here (https://t.co/hhHiZQDCMP), and reach out if you're interested in helping create an open-sourced dataset!""]",http://arxiv.org/abs/2007.16127,"Clinical studies often require understanding elements of a patient's narrative that exist only in free text clinical notes. To transform notes into structured data for downstream use, these elements are commonly extracted and normalized to medical vocabularies. In this work, we audit the performance of and indicate areas of improvement for state-of-the-art systems. We find that high task accuracies for clinical entity normalization systems on the 2019 n2c2 Shared Task are misleading, and underlying performance is still brittle. Normalization accuracy is high for common concepts (95.3%), but much lower for concepts unseen in training data (69.3%). We demonstrate that current approaches are hindered in part by inconsistencies in medical vocabularies, limitations of existing labeling schemas, and narrow evaluation techniques. We reformulate the annotation framework for clinical entity extraction to factor in these issues to allow for robust end-to-end system benchmarking. We evaluate concordance of annotations from our new framework between two annotators and achieve a Jaccard similarity of 0.73 for entity recognition and an agreement of 0.83 for entity normalization. We propose a path forward to address the demonstrated need for the creation of a reference standard to spur method development in entity recognition and normalization. ",Robust Benchmarking for Machine Learning of Clinical Entity Extraction
189,1290466948355100673,1231998355972096000,Yoshitomo Matsubara,"['""Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks"" was accepted @icpr2020milan\nWe propose generalized head network distillation &amp; neural filter for split computing\n\nCode: <LINK>\nPreprint: <LINK>', '@icpr2020milan Split Computingに向けたモデルへのボトルネック導入のためのGeneralized Head Network Distillation、物体の有無をR-CNNの頭で検知するNeural Filterに関する論文がICPR 2020で採択されました!\n前回紹介したEMDL論文の続編です\n\nCode: https://t.co/faDcuLq3Yf\nプレプリント: https://t.co/CIOzx8WHwm']",https://arxiv.org/abs/2007.15818,"The edge computing paradigm places compute-capable devices - edge servers - at the network edge to assist mobile devices in executing data analysis tasks. Intuitively, offloading compute-intense tasks to edge servers can reduce their execution time. However, poor conditions of the wireless channel connecting the mobile devices to the edge servers may degrade the overall capture-to-output delay achieved by edge offloading. Herein, we focus on edge computing supporting remote object detection by means of Deep Neural Networks (DNNs), and develop a framework to reduce the amount of data transmitted over the wireless link. The core idea we propose builds on recent approaches splitting DNNs into sections - namely head and tail models - executed by the mobile device and edge server, respectively. The wireless link, then, is used to transport the output of the last layer of the head model to the edge server, instead of the DNN input. Most prior work focuses on classification tasks and leaves the DNN structure unaltered. Herein, our focus is on DNNs for three different object detection tasks, which present a much more convoluted structure, and modify the architecture of the network to: (i) achieve in-network compression by introducing a bottleneck layer in the early layers on the head model, and (ii) prefilter pictures that do not contain objects of interest using a convolutional neural network. Results show that the proposed technique represents an effective intermediate option between local and edge computing in a parameter region where these extreme point solutions fail to provide satisfactory performance. The code and trained models are available at this https URL . ","Neural Compression and Filtering for Edge-assisted Real-time Object
  Detection in Challenged Networks"
190,1290191788045557762,915574287989342209,Roberto Oliveri,"['Here is my last preprint. Check it out!\n\nWe propose a new approach to find magnetically-dominated force-free #magnetospheres around highly spinning #blackholes, relevant for models of #astrophysical #jets.\n\n<LINK>\n(companion paper: <LINK>) <LINK>']",https://arxiv.org/abs/2007.15662,"We propose a new approach to find magnetically-dominated force-free magnetospheres around highly spinning black holes, relevant for models of astrophysical jets. Employing the near-horizon extreme Kerr (NHEK) limit of the Kerr black hole, any stationary, axisymmetric and regular force-free magnetosphere reduces to the same attractor solution in the NHEK limit with null electromagnetic field strength. We use this attractor solution as the universal starting point for perturbing away from the NHEK region in the extreme Kerr spacetime. We demonstrate that by going to second order in perturbation theory, it is possible to find magnetically dominated magnetospheres around the extreme Kerr black hole. Furthermore, we consider the near-horizon near-extreme Kerr (near-NHEK) limit that provides access to a different regime of highly spinning black holes. Also in this case we find a novel force-free attractor, which can be used as the universal starting point for a perturbative construction of force-free magnetospheres. Finally, we discuss the relation between the NHEK and near-NHEK attractors. ","Force-free magnetosphere attractors for near-horizon extreme and
  near-extreme limits of Kerr black hole"
191,1289452587209486336,753525918186872832,Miaomiao Liu,['Happy to share our #ECCV2020 paper:\n <LINK> \n\nwith Wei Mao and Mathieu Salzmann.\n\nHuman tends to repeat motions over long time horizon. We propose Motion Attention to discover motion patterns for better Human Motion Prediction! 👇\n<LINK>'],http://arxiv.org/abs/2007.11755,"Human motion prediction aims to forecast future human poses given a past motion. Whether based on recurrent or feed-forward neural networks, existing methods fail to model the observation that human motion tends to repeat itself, even for complex sports actions and cooking activities. Here, we introduce an attention-based feed-forward network that explicitly leverages this observation. In particular, instead of modeling frame-wise attention via pose similarity, we propose to extract motion attention to capture the similarity between the current motion context and the historical motion sub-sequences. Aggregating the relevant past motions and processing the result with a graph convolutional network allows us to effectively exploit motion patterns from the long-term history to predict the future poses. Our experiments on Human3.6M, AMASS and 3DPW evidence the benefits of our approach for both periodical and non-periodical actions. Thanks to our attention model, it yields state-of-the-art results on all three datasets. Our code is available at this https URL ",History Repeats Itself: Human Motion Prediction via Motion Attention
192,1288899710351216640,151193108,Mert R. Sabuncu 🇺🇦,"['Neural nets are increasingly used to solve the ill-posed reconstruction problem in compressed sensing (e.g., in MRI). These methods typically need fully sampled data to train on. We propose a way to do this without fully sampled data: <LINK>']",https://arxiv.org/abs/2007.14979,"Compressed Sensing MRI (CS-MRI) has shown promise in reconstructing under-sampled MR images, offering the potential to reduce scan times. Classical techniques minimize a regularized least-squares cost function using an expensive iterative optimization procedure. Recently, deep learning models have been developed that model the iterative nature of classical techniques by unrolling iterations in a neural network. While exhibiting superior performance, these methods require large quantities of ground-truth images and have shown to be non-robust to unseen data. In this paper, we explore a novel strategy to train an unrolled reconstruction network in an unsupervised fashion by adopting a loss function widely-used in classical optimization schemes. We demonstrate that this strategy achieves lower loss and is computationally cheap compared to classical optimization solvers while also exhibiting superior robustness compared to supervised models. Code is available at this https URL ","Neural Network-based Reconstruction in Compressed Sensing MRI Without
  Fully-sampled Training Data"
193,1288809133416812545,102661351,Joost Visser,['Here is an overview of 29 recommended engineering practices for #MachineLearning that we extracted from blogs and articles.\n➽ Full practice descriptions: <LINK>\n➽ Study on practice adoption: <LINK>\n➽ Sources: <LINK> <LINK>'],https://arxiv.org/abs/2007.14130,"The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams. ","Adoption and Effects of Software Engineering Best Practices in Machine
  Learning"
194,1288534528030576640,1282643620059897856,Lukasz Cincio,"['Our first 2020 Quantum Summer School paper is out!\nCongrats \n@samson_wang, @EnricoFontana19, @MvsCerezo, @kunal_phy , @SoneAkira, @ColesQuantum\nWe study how noise affects variational algorithms. Noise will make your deep ansatz untrainable. Details here:\n<LINK>']",https://arxiv.org/abs/2007.14384,"Variational Quantum Algorithms (VQAs) may be a path to quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) computers. A natural question is whether noise on NISQ devices places fundamental limitations on VQA performance. We rigorously prove a serious limitation for noisy VQAs, in that the noise causes the training landscape to have a barren plateau (i.e., vanishing gradient). Specifically, for the local Pauli noise considered, we prove that the gradient vanishes exponentially in the number of qubits $n$ if the depth of the ansatz grows linearly with $n$. These noise-induced barren plateaus (NIBPs) are conceptually different from noise-free barren plateaus, which are linked to random parameter initialization. Our result is formulated for a generic ansatz that includes as special cases the Quantum Alternating Operator Ansatz and the Unitary Coupled Cluster Ansatz, among others. For the former, our numerical heuristics demonstrate the NIBP phenomenon for a realistic hardware noise model. ",Noise-Induced Barren Plateaus in Variational Quantum Algorithms
195,1288395874192830464,196749454,Natalie Hogg,"[""Paper day! <LINK>\n\nEver wondered if you've discovered some beyond LCDM physics by accident? Could there be some hidden modified gravity effects lurking in gravitational wave detections? Have we found a new smoking gun for modified gravity? Read on to find out!"", 'Matteo (@matmartinelli1), Savvas and I created and used mock standard siren datasets to forecast the ability of the Einstein Telescope (ET), LSST and DESI to constrain the distance duality relation (DDR), which relates luminosity distances to angular diameter distances.', 'We used a toy model in which photons decay into axions to break the DDR, and found that the combination of SNIa + GW events is competitive with the more commonly used SNIa + BAO when constraining deviations from DDR.\n\n(Paging the chair of the axion fan club @duetosymmetry 😉)', 'But it pays to be careful when using a probe of the gravitational sector as modified gravity (MG) effects could be at play! By including a generic MG model in our mock datasets, we found that the DDR analysis became extremely biased, leading to a false detection of DDR violation! https://t.co/etHLlyjjT7', 'However, the problem can be resolved by explicitly including the modified gravity in your analysis. Here, the full combination of mock datasets broke the degeneracies in parameter space and correctly recovered the fiducial cosmology. No more false detection of DDR violation! https://t.co/xLLRczzwIb', 'Of course, if you have modified gravity, you will likely have a screening mechanism to go with it. GW events and SNIa are both events in which MG could be screened -- how does this affect our results? If GW are screened, we find another false detection of DDR violation...', ""but if the SNIa are screened, we find that the cosmological parameters are also biased away from the fiducial cosmology, with only the combination LSST + DESI correctly recovering the fiducial. If this is seen in real data, it's a smoking gun for MG with this screening behaviour! https://t.co/wai5D8oxgK"", 'Savvas also applied his Genetic Algorithm machine learning code to reconstruct the DDR as a function of redshift, finding that it can correctly distinguish between the LCDM and MG mock datasets and finds the same biases as in the parameterised case, nicely confirming our results.', ""Final tweet! This is the first time I've shared a paper draft with people other than fellow authors before posting it on arXiv and we're really grateful for all the comments and feedback we received -- shout out to Ian, Kazuya, Carlos, Isaac and Bill (@BillWrightCosmo)!""]",https://arxiv.org/abs/2007.14335,"We use gravitational wave (GW) standard sirens, in addition to Type Ia supernovae (SNIa) and baryon acoustic oscillation (BAO) mock data, to forecast constraints on the electromagnetic and gravitational distance duality relations (DDR). We make use of a parameterised approach based on a specific DDR violation model, along with a machine learning reconstruction method based on the Genetic Algorithms. We find that GW provide an alternative to the use of BAO data to constrain violations of the DDR, reaching $3\%$ constraints on the violation parameter we consider when combined with SNIa, which is only improved by a factor of $\approx1.4$ if one instead considers the combination of BAO and SNIa. We also investigate the possibility that a neglected modification of gravity might lead to a false detection of DDR violations, even when screening mechanisms are active. We find that such a false detection can be extremely significant, up to $\approx10\sigma$ for very extreme modified gravity scenarios, while this reduces to $\approx4\sigma$ in a more realistic case. False detections can also provide a smoking gun for the modified gravity mechanism at play, as a result of the tension introduced between the SNIa+GW and SNIa+BAO combinations. ",Constraints on the distance duality relation with standard sirens
196,1288276743254503425,1162541483431301120,James Beattie,"['Latest study on magnetic field fluctuations in supersonic, anisotropic turbulence now on the arXiv, after being accepted for publication on Monday!! \n\nSuper stoked on this study!! I think we got some very nice results by playing with the MHD equations.\n\n<LINK> <LINK>', 'Squeezing anything novel out of these equations is exciting, to say the least, and then being able to relate it to astrophysical phenomena that we observe in the Universe is just so cool!!!', '@MatthewColless 😂😂😂', 'The first figure is really just a showcase of my numerical data (I do this in all my studies). Seeing the data this way also allows one to see how the magnetic field transitions between mean-field to fluctuating dominated, which is very nice. https://t.co/OcPSWPxKUi', 'Next is another visualisation of my numerical data, this time showcasing the difference between unordered and ordered magnetic fields (blue), which significantly change the velocity structure (red). When the Ma0 &lt;&lt; 1 we get beautiful eddies orientated around the mean-field. https://t.co/jk3ATWGsDq', 'Next is a plot that supports a model from §2 of the study, which suggests that velocity gradient along the mean-field give rise to compressible modes (shocks) in the turbulence. Essentially the convergence (left, top) and velocity gradient (right,top) should be the same. https://t.co/NkFcsu3W7y', 'Certainly, the velocity gradient picks out the shocks along the field, which is awesome! But there are some differences caused by fast magnetosonic waves that form perpendicular compressions. These should disappear as the B_0 increases, which is one of the assump. from our model.', 'In the bottom panels you can see the density field (left-bottom) and some velocity streamlines (right-bottom). Showing how high-density filaments in the density correspond converging velocity flows from along and across B_0. In the online version there is a movie for this plot!', 'The next part of the study is really focussed on the magnetic field fluctuations, shown in the next plot. Regardless of the strength of B_0 these go linearly with the turbulent Mach number (which made me VERY happy). In the strong-field regime we have an analytical model. https://t.co/crTEf7cqOi', 'We derive this model from (1) assuming all fluctuations come from the turbulence and doing some algebra and (2) doing some order of mag estimates from the induction equation. You can see it in the next figure: https://t.co/tM8qFGObpS', 'When the mean-field is super strong, the fluctuations of the field are quite anisotropic. The parallel fluctuations can get about twice as large as the perpendicular fluctuations (all w.r.t B_0). The fluctuations become isotropic as the B_0 becomes weak (compared to \\delta B) https://t.co/YXDePvK6OC', 'The ratio between B_0 / \\delta B is very nice. Again, we have an analytical model for the strong-field regime, which predicts the relation should be universal for different Mach numbers. Seems to be the case but still a wee bit of scatter. https://t.co/qNTVnbyGqm', 'Almost last plot... the full magnetic field PDFs are rich with information (too much for twitter). Here I plot the standardised B variables, and discuss the intermittency, symmetry, and that large exponential tail in the top-right plot! The asymmetry is the most interesting part! https://t.co/0NsTPSfZzV', 'That extended tail comes from eddies that form low-density, low-magnetic pressure systems locally in the turbulence. Since I hold B_0 fixed, and the magnetic pressure decreases, this means parallel fluctuations in the magnetic field MUST oppose the field. Pretty neat! https://t.co/aUnWKVqIDd', 'Oops, I messed up the tweet order for my Figures!! Damn it... oh well.. you will have to read the study if you are interested 😂😆', 'Oh yes, acknowledgments!! We thanked the reviewer, who became not-so anonymous (maybe because they liked our model so much 😆), and since this was written in COVID iso, we gave a big thanks to all of the support provided from @ourANU and @StromloANU during this weird, weird time! https://t.co/o58xObLGdy']",https://arxiv.org/abs/2007.13937,"The rich structure that we observe in molecular clouds is due to the interplay between strong magnetic fields and supersonic (turbulent) velocity fluctuations. The velocity fluctuations interact with the magnetic field, causing it too to fluctuate. Using numerical simulations, we explore the nature of such magnetic field fluctuations, $\vec{\delta B}$, over a wide range of turbulent Mach numbers, $\mathcal{M} = 2 - 20$ (i.e., from weak to strong compressibility), and Alfv\'en Mach numbers, $\mathcal{M}_{\text{A}0} = 0.1 - 100$ (i.e., from strong to weak magnetic mean fields, $B_0$). We derive a compressible quasi-static fluctuation model from the magnetohydrodynamical (MHD) equations and show that velocity gradients parallel to the mean magnetic field give rise to compressible modes in sub-Alfv\'enic flows, which prevents the flow from becoming two-dimensional, as is the case in incompressible MHD turbulence. We then generalise an analytical model for the magnitude of the magnetic fluctuations to include $\mathcal{M}$, and find $|\vec{\delta B}| = \delta B = c_s\sqrt{\pi\rho_0}\mathcal{M}\mathcal{M}_{\text{A}0}$, where $c_s$ is the sound speed and $\rho_0$ is the mean density of gas. This new relation fits well in the strong $B$-field regime. We go on to study the anisotropy between the perpendicular ($ B_{\perp}$) and parallel ($ B_{\parallel}$) fluctuations and the mean-normalised fluctuations, which we find follow universal scaling relations, invariant of $\mathcal{M}$. We provide a detailed analysis of the morphology for the $\delta B_{\perp}$ and $\delta B_{\parallel}$ probability density functions and find that eddies aligned with $B_0$ cause parallel fluctuations that reduce $B_{\parallel}$ in the most anisotropic simulations. We discuss broadly the implications of our fluctuation models for magnetised gases in the interstellar medium. ","Magnetic field fluctuations in anisotropic, supersonic turbulence"
197,1287791917644865542,392413519,Matt Hall,"['New @ncats_nih_gov pre-print - we mined almost 1,000 HTS drug repurposing datasets to find biological response correlations with #COVID19 screens. Found correlations with AP-1, and autophagy assays. \n\n<LINK> <LINK>', 'The pre-print can be found here:\n\nMining of high throughput screening database reveals AP-1 and autophagy pathways as potential targets for COVID-19 therapeutics\n\nhttps://t.co/9dgHsWgUBJ', 'Perhaps not surprising to learn that Ebola and MERS screening datasets also correlated with SARS-CoV-2 - that was reassuring!! https://t.co/qb32Qi5Z6P']",https://arxiv.org/abs/2007.12242,"The recent global pandemic of Coronavirus Disease 2019 (COVID-19) caused by the new coronavirus SARS-CoV-2 presents an urgent need for new therapeutic candidates. Many efforts have been devoted to screening existing drug libraries with the hope to repurpose approved drugs as potential treatments for COVID-19. However, the antiviral mechanisms of action for the drugs found active in these phenotypic screens are largely unknown. To deconvolute the viral targets for more effective anti-COVID-19 drug development, we mined our in-house database of approved drug screens against 994 assays and compared their activity profiles with the drug activity profile in a cytopathic effect (CPE) assay of SARS-CoV-2. We found that the autophagy and AP-1 signaling pathway activity profiles are significantly correlated with the anti-SARS-CoV-2 activity profile. In addition, a class of neurology/psychiatry drugs was found significantly enriched with anti-SARS-CoV-2 activity. Taken together, these results have provided new insights into SARS-CoV-2 infection and potential targets for COVID-19 therapeutics. ","Mining of high throughput screening database reveals AP-1 and autophagy
  pathways as potential targets for COVID-19 therapeutics"
198,1286053177759019009,3221244502,Gregory Nicola,"['“We propose CovidDeep, an accurate, COVID-19 detection frame-work...It captures all the required physiological signals non-invasively through comfortably-worn WMS“ #Neutigers @RajantCorp @ignaziomarino @LaurenGoldingMD <LINK>']",https://arxiv.org/abs/2007.10497,"The novel coronavirus (SARS-CoV-2) has led to a pandemic. The current testing regime based on Reverse Transcription-Polymerase Chain Reaction for SARS-CoV-2 has been unable to keep up with testing demands, and also suffers from a relatively low positive detection rate in the early stages of the resultant COVID-19 disease. Hence, there is a need for an alternative approach for repeated large-scale testing of SARS-CoV-2/COVID-19. We propose a framework called CovidDeep that combines efficient DNNs with commercially available WMSs for pervasive testing of the virus. We collected data from 87 individuals, spanning three cohorts including healthy, asymptomatic, and symptomatic patients. We trained DNNs on various subsets of the features automatically extracted from six WMS and questionnaire categories to perform ablation studies to determine which subsets are most efficacious in terms of test accuracy for a three-way classification. The highest test accuracy obtained was 98.1%. We also augmented the real training dataset with a synthetic training dataset drawn from the same probability distribution to impose a prior on DNN weights and leveraged a grow-and-prune synthesis paradigm to learn both DNN architecture and weights. This boosted the accuracy of the various DNNs further and simultaneously reduced their size and floating-point operations. ","CovidDeep: SARS-CoV-2/COVID-19 Test Based on Wearable Medical Sensors
  and Efficient Neural Networks"
199,1286035276427599886,114636884,Sagar,"['Glad to share the final part of my Ph.D. as a preprint. In this work we propose a new method for conducting triadic motif census, which can be centred around a particular node in a conversation thread. This results in a much richer variety of triads. \n<LINK>', 'We call these variants as Anchored Triads. We find that some of these triads are significantly more prevalent in supportive conversations about suicide (r/SuicideWatch). We hypothesize that this may point to a signature of social support in online forums.', 'We investigate the significance of this finding by comparing against generic Reddit threads. \n\nJoint work with the amazing @rina_dutta , @__sumithra__ , and @nishanthsastry']",https://arxiv.org/abs/2007.10159,"Platforms like Reddit and Twitter offer internet users an opportunity to talk about diverse issues, including those pertaining to physical and mental health. Some of these forums also function as a safe space for severely distressed mental health patients to get social support from peers. The online community platform Reddit's SuicideWatch is one example of an online forum dedicated specifically to people who suffer from suicidal thoughts, or who are concerned about people who might be at risk. It remains to be seen if these forums can be used to understand and model the nature of online social support, not least because of the noisy and informal nature of conversations. Moreover, understanding how a community of volunteering peers react to calls for help in cases of suicidal posts, would help to devise better tools for online mitigation of such episodes. In this paper, we propose an approach to characterise conversations in online forums. Using data from the SuicideWatch subreddit as a case study, we propose metrics at a macroscopic level -- measuring the structure of the entire conversation as a whole. We also develop a framework to measure structures in supportive conversations at a mesoscopic level -- measuring interactions with the immediate neighbours of the person in distress. We statistically show through comparison with baseline conversations from random Reddit threads that certain macro and meso-scale structures in an online conversation exhibit signatures of social support, and are particularly over-expressed in SuicideWatch conversations. ","Analysing Meso and Macro conversation structures in an online suicide
  support forum"
200,1285786883209879552,1283081795890626560,Saining Xie,"['Almost every deep learning model for 3D recognition has been *trained from scratch*. In our #ECCV2020 spotlight paper, we propose 👉PointContrast👈, an unsupervised pre-training framework that boosts performance on 6 different 3D point cloud benchmarks.  <LINK> <LINK>', ""joint work with @thoma_gu,@demi_guo_,@charlesqi,Leo Guibas,@orlitany. \n\nOur project was inspired by @ChrisChoy208's great work on MinkowskiEngine (https://t.co/HXC40jyTrm) and FCGF (https://t.co/B79ecuxyMS) --- sparse convnet FTW!""]",https://arxiv.org/abs/2007.10985,"Arguably one of the top success stories of deep learning is transfer learning. The finding that pre-training a network on a rich source set (eg., ImageNet) can help boost performance once fine-tuned on a usually much smaller target set, has been instrumental to many applications in language and vision. Yet, very little is known about its usefulness in 3D point cloud understanding. We see this as an opportunity considering the effort required for annotating data in 3D. In this work, we aim at facilitating research on 3D representation learning. Different from previous works, we focus on high-level scene understanding tasks. To this end, we select a suite of diverse datasets and tasks to measure the effect of unsupervised pre-training on a large source set of 3D scenes. Our findings are extremely encouraging: using a unified triplet of architecture, source dataset, and contrastive loss for pre-training, we achieve improvement over recent best results in segmentation and detection across 6 different benchmarks for indoor and outdoor, real and synthetic datasets -- demonstrating that the learned representation can generalize across domains. Furthermore, the improvement was similar to supervised pre-training, suggesting that future efforts should favor scaling data collection over more detailed annotation. We hope these findings will encourage more research on unsupervised pretext task design for 3D deep learning. ","PointContrast: Unsupervised Pre-training for 3D Point Cloud
  Understanding"
201,1285400841810145280,913238472357437445,Fuminobu TAKAHASHI,['Our new paper is out today. The ALP mass of keV and the decay constant of 10^9 GeV suggested by the XENON1T excess satisfy the consistency relation predicted by the ALP inflation model.  We studied the implication for thermal history after inflation.\n\n<LINK>'],https://arxiv.org/abs/2007.10311,"The recent XENON1T excess in the electron recoil data can be explained by anomaly-free axion-like particle (ALP) dark matter with mass $m_\phi = 2.3 \pm 0.2\,$keV and the decay constant $f_\phi/q_e \simeq 2 \times 10^{10} \sqrt{\Omega_\phi/\Omega_{\rm DM}}\,{\rm GeV}$. Intriguingly, the suggested mass and decay constant are consistent with the relation, $f_\phi \sim 10^3 \sqrt{m_\phi M_p}$, predicted in a scenario where the ALP plays the role of the inflaton. This raises a possibility that the ALP dark matter responsible for the XENON1T excess also drove inflation in the very early universe. We study implications of the XENON1T excess for the ALP inflation and thermal history of the universe after inflation. ",What if ALP dark matter for the XENON1T excess is the inflaton
202,1285270207402061827,53464710,Eric Wong,"['1/ New paper on learning perturbation sets for robust machine learning! We study how to characterize real world perturbations in a well-defined set. \n\nPaper: <LINK>\nBlog post: <LINK>\nCode: <LINK>\n\nJoint work with @zicokolter <LINK>', '2/ We define a learned perturbation set over an Lp ball in the latent space of a generator, which uses a latent vector to perturb an example, and is trained on pairs of perturbed examples.\n\nThe generator captures complex perturbations, and is well-defined over the latent Lp ball.', '3/ You may be (rightfully) suspicious of a perturbation set defined by a generative model learned from data. \n\nWe define concrete, measurable properties of a ""good"" perturbation set, in order to properly evaluate the quality of perturbation sets learned from data.', '4/ To learn the generator, we use the conditional variational autoencoder framework. \n\nWe theoretically prove that training the CVAE objective results in a perturbation set that satisfies these good properties, resulting in a principled approach for learning perturbation sets.', '5/ We can now easily leverage methods from Lp robustness to learn robustness to real-world effects captured by a learned perturbation set: simply run Lp approaches in the latent space of the generator! \n\nThis also gives another reason to care about methods for Lp robustness.', '6/ We learn a perturbation set that captures common image corruptions, and another perturbation set that captures lighting changes for scenes in the wild. \n\nCommon corruptions: https://t.co/pbSFBbLYdB\nMulti-Illumination dataset: https://t.co/xbwcA8SFSP\n\n@DanHendrycks @murmurmann', '7/ We can then train models which are adversarially robust to common corruptions and lighting changes, using PGD adversarial training and randomized smoothing. \n\nThis results in empirically and certifiably robust models to real-world perturbations.', '8/ Finally, models trained with a meaningful learned perturbation set can have non-adversarial benefits as well. \n\nFor example, for CIFAR10 common corruptions, we can get improved average-case corrupted performance over directly training on the corrupted examples.']",https://arxiv.org/abs/2007.08450,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL ",Learning perturbation sets for robust machine learning
203,1285100301834485761,583141846,Nicola Lo Gullo,['A new preprint is out! We study the spectral properties of the solution of the GKBA master equation for correlated open quantum systems. <LINK>'],https://arxiv.org/abs/2007.08901,"We investigate the spectral properties of an open interacting system by solving the Generalized Kadanoff-Baym Ansatz (GKBA) master equation for the single-particle density matrix, namely the time-diagonal lesser Green's function. To benchmark its validity, we compare the solution obtained within the GKBA with the solution of the Dyson equation (equivalently the full Kadanoff-Baym equations). In both approaches, we treat the interaction within the self-consistent second-order Born approximation, whereas the GKBA still retains the retarded propagator calculated at the Hartree-Fock level. We consider the case of two leads connected through a central correlated region where particles can interact and exploit the stationary particle current at the boundary of the junction as a probe of the spectral features of the system. In this work, as an example, we take the central region to be a one-dimensional quantum wire and a two-dimensional carbon nanotube and show that the solution of the GKBA master equation well captures their spectral features. Our result demonstrates that, even when the propagator used is at the Hartree-Fock level, the GBKA solution retains the main spectral features of the self-energy used. ","Spectral properties of correlated quantum wires and carbon nanotubes
  within the Generalized Kadanoff-Baym Ansatz"
204,1284209839430754305,911675917063385089,Shangtong Zhang,"['Happy to present Reverse General Value Function <LINK> w/ @vivek_veeriah @shimon8282 @whi_rl.  GVF represents predictive knowledge, we propose Reverse GVF to represent retrospective knowledge. (1/n)', 'An example of predictive knowledge: how much fuel will be consumed in expectation if we drive from A to B? An example of retrospective knowledge: how much fuel do we expect a car to have given it is at B at time t? (2/n)', 'Key to ReverseGVF is backwards Bellman operator, which has been used in density ratio learning, from ealier dual dynamic programming to recent (discounted) COP-TD. We show that it’s actually more than a technical tool. (3/n)', 'We show such retrospective knowledge is useful in both anomaly detection and representation learning. (4/n)', ""@jdmartin86 @vivek_veeriah @shimon8282 @whi_rl Almost! We let t go to infinity, so it's kind of infinite history.""]",https://arxiv.org/abs/2007.06703,"We present a Reverse Reinforcement Learning (Reverse RL) approach for representing retrospective knowledge. General Value Functions (GVFs) have enjoyed great success in representing predictive knowledge, i.e., answering questions about possible future outcomes such as ""how much fuel will be consumed in expectation if we drive from A to B?"". GVFs, however, cannot answer questions like ""how much fuel do we expect a car to have given it is at B at time $t$?"". To answer this question, we need to know when that car had a full tank and how that car came to B. Since such questions emphasize the influence of possible past events on the present, we refer to their answers as retrospective knowledge. In this paper, we show how to represent retrospective knowledge with Reverse GVFs, which are trained via Reverse RL. We demonstrate empirically the utility of Reverse GVFs in both representation learning and anomaly detection. ",Learning Retrospective Knowledge with Reverse Reinforcement Learning
205,1284121163380203520,1151850663711784960,Simone Felicetti,['New manuscript on the arxiv: <LINK>\nWe propose a method to open an efficient photoprotection channel in uracil with the aid of lossy nanophotonic devices. @jacopo_fregoni @MMUSCLES_UAM @ifimacuam @ftmcatuam @StampaCnr <LINK>'],https://arxiv.org/abs/2007.07551,"We analyze how the photorelaxation dynamics of a molecule can be controlled by modifying its electromagnetic environment using a nanocavity mode. In particular, we consider the photorelaxation of the RNA nucleobase uracil, which is the natural mechanism to prevent photodamage. In our theoretical work, we identify the operative conditions in which strong coupling with the cavity mode can open an efficient photoprotective channel, resulting in a relaxation dynamics twice as fast than the natural one. We rely on a state-of-the-art chemically-detailed molecular model and a non-Hermitian Hamiltonian propagation approach to perform full-quantum simulations of the system dissipative dynamics. By focusing on the photon decay, our analysis unveils the active role played by cavity-induced dissipative processes in modifying chemical reaction rates, in the context of molecular polaritonics. Remarkably, we find that the photorelaxation efficiency is maximized when an optimal trade-off between light-matter coupling strength and photon decay rate is satisfied. This result is in contrast with the common intuition that increasing the quality factor of nanocavities and plasmonic devices improves their performance. Finally, we use a detailed model of a metal nanoparticle to show that the speedup of the uracil relaxation could be observed via coupling with a nanosphere pseudomode, without requiring the implementation of complex nanophotonic structures. ",Photoprotecting uracil by coupling with lossy nanocavities
206,1284113565998342146,1097855896212946945,Francesco Di Lauro,"['Very interesting! We study heterogeneities in contact structures rather than susceptibility in our most recent preprint <LINK>.  We observe similar effects on herd immunity threshold, but we also show a dtawback: control can potentially hinder this mechanism. <LINK>', 'How? This is very nicely explained in this brief thread https://t.co/sXkWjYANZq.\nThe idea is simple: if during lockdown high interacting individuals are shielded and interact only with close relatives, then the epidemic cannot exploit the contact structure heterogeneity!']",https://arxiv.org/abs/2007.06975,"The contact structure of a population plays an important role in transmission of infection. Many ``structured models'' capture aspects of the contact structure through an underlying network or a mixing matrix. An important observation in such models, is that once a fraction $1-1/\mathcal{R}_0$ has been infected, the residual susceptible population can no longer sustain an epidemic. A recent observation of some structured models is that this threshold can be crossed with a smaller fraction of infected individuals, because the disease acts like a targeted vaccine, preferentially immunizing higher-risk individuals who play a greater role in transmission. Therefore, a limited ``first wave'' may leave behind a residual population that cannot support a second wave once interventions are lifted. In this paper, we systematically analyse a number of mean-field models for networks and other structured populations to address issues relevant to the Covid-19 pandemic. In particular, we consider herd-immunity under several scenarios. We confirm that, in networks with high degree heterogeneity, the first wave confers herd-immunity with significantly fewer infections than equivalent models with lower degree heterogeneity. However, if modelling the intervention as a change in the contact network, then this effect might become more subtle. Indeed, modifying the structure can shield highly connected nodes from becoming infected during the first wave and make the second wave more substantial. We confirm this finding by using an age-structured compartmental model parameterised with real data and comparing lockdown periods implemented either as a global scaling of the mixing matrix or age-specific structural changes. We find that results regarding herd immunity levels are strongly dependent on the model, the duration of lockdown and how lockdown is implemented. ","The impact of network properties and mixing on control measures and
  disease-induced herd immunity in epidemic models: a mean-field model
  perspective"
207,1284075526777270272,887975383680798720,Jihun Yun,"['We propose a unified framework for stochastic proximal gradient methods. Our framework can allow the most general setting: (i) non-convex smooth loss, (ii) (non-convex) lower semi-continuous regularizer, and (iii) arbitrary positive preconditioners.\n\n<LINK>']",https://arxiv.org/abs/2007.07484,"We study the training of regularized neural networks where the regularizer can be non-smooth and non-convex. We propose a unified framework for stochastic proximal gradient descent, which we term ProxGen, that allows for arbitrary positive preconditioners and lower semi-continuous regularizers. Our framework encompasses standard stochastic proximal gradient methods without preconditioners as special cases, which have been extensively studied in various settings. Not only that, we present two important update rules beyond the well-known standard methods as a byproduct of our approach: (i) the first closed-form proximal mappings of $\ell_q$ regularization ($0 \leq q \leq 1$) for adaptive stochastic gradient methods, and (ii) a revised version of ProxQuant that fixes a caveat of the original approach for quantization-specific regularizers. We analyze the convergence of ProxGen and show that the whole family of ProxGen enjoys the same convergence rate as stochastic proximal gradient descent without preconditioners. We also empirically show the superiority of proximal methods compared to subgradient-based approaches via extensive experiments. Interestingly, our results indicate that proximal methods with non-convex regularizers are more effective than those with convex regularizers. ","A General Family of Stochastic Proximal Gradient Methods for Deep
  Learning"
208,1283796771769135104,1151850663711784960,Simone Felicetti,['Published today on the arxiv the results of a nice collaboration with Nicolò Piccione and Bruno Bellomo\n<LINK>\nWe study collective quantum phenomena induced by two-photon interactions'],https://arxiv.org/abs/2007.07844,"Various experimental platforms have proven to be valid testbeds for the implementation of non-dipolar light-matter interactions, where atomic systems and confined modes interact via two-photon couplings. Here, we study a damped quantum harmonic oscillator interacting with $N$ qubits via a two-photon coupling in the so-called bad-cavity limit, in the presence of finite-temperature baths and coherent and incoherent drivings. We have succeeded in applying a recently developed adiabatic elimination technique to derive an effective master equation for the qubits, presenting two fundamental differences compared to the case of a dipolar interaction: an enhancement of the qubits spontaneous-like emission rate, including a thermal contribution and a quadratic term in the coherent driving, and an increment of the effective temperature perceived by the qubits. These differences give rise to striking effects in the qubits dynamics, including a faster generation of steady-state coherence and a richer dependence on temperature of the collective effects, which can be made stronger at higher temperature. ",Two-photon interaction effects in the bad-cavity limit
209,1283444494050816002,885528008,William Fedus,"[""The interplay of RL algorithms with experience replay is poorly understood. We study this and uncover a relationship between n-step returns and replay capacity.\n\nICML '20 paper: <LINK>\n\nPrajit R.*, @agarwl_ , Yoshua, @hugo_larochelle , Mark R., @wwdabney <LINK>"", 'We study two properties in experience replay:\n1. Size of replay capacity\n2. Oldest policy in the buffer\n\nTogether, these jointly define a replay ratio: an experience learning to new data acquisition ratio. Since DQN, these have often not varied (1M, learn each 4-steps). https://t.co/0MOKpLQEHn', 'The performance of a Rainbow agent (Hessel et al., 2017) varies substantially with these factors: improving with more ""on-policy"" data and capacity.\n\nIn the easiest Deep RL boost, we find a 29% median improvement in Atari games simply by increasing replay capacity from 1M -&gt; 10M https://t.co/wNVfFttK0h', 'But the story is completely different with a DQN algorithm -- regardless of whether we control for replay ratio or the oldest policy -- there is no change.\n\nWhy? https://t.co/cdJgyz240o', 'Through ablative and additive studies, we isolate n-step returns as the crucial factor.\n\nImprovements with larger replay capacity are found only when using n-step returns. No clear signal found with prioritized experience replay, optimizer, or distributional learning. https://t.co/SlKT7Mnwqx', 'The importance of n-step returns even holds in the logical extreme:  batch reinforcement learning. \n\nThis is non-intuitive. Uncorrected n-step returns -- mathematically incorrect -- yield gains in a regime where they are the most incorrect. https://t.co/HubfHztWuW', ""A bias-variance trade-off partially explains the importance of n-step returns (refer to paper for experimental details), but it's still not complete.\n\nThe entanglement between data generation and RL algorithms is an important issue in the design of better agents! https://t.co/9lPfzcadKi"", 'This was jointly led by Prajit Ramachandran with a great collaboration across Mila, Brain, DeepMind including: @agarwl_ , Yoshua Bengio, @hugo_larochelle , Mark Rowland, @wwdabney \n\nPaper:  https://t.co/KnGLGapNGU\nCode:  https://t.co/xcg5ZI7Tt2\nICML 2020:  https://t.co/QSGtPz4Iwk']",https://arxiv.org/abs/2007.06700,"Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay -- greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counterintuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately, by directly controlling the replay ratio we contextualize previous observations in the literature and empirically measure its importance across a variety of deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits. ",Revisiting Fundamentals of Experience Replay
210,1283376473240481793,228436420,Karlson Pfannschmidt,['Is there a more elegant way to learn from choices than simply reducing every object to a single utility?\n\nWe propose to embed each object into a special higher dimensional space in which the chosen objects naturally correspond to the Pareto-Front #KI2020:\n<LINK> <LINK>'],https://arxiv.org/abs/2007.06927,"We consider the problem of learning to choose from a given set of objects, where each object is represented by a feature vector. Traditional approaches in choice modelling are mainly based on learning a latent, real-valued utility function, thereby inducing a linear order on choice alternatives. While this approach is suitable for discrete (top-1) choices, it is not straightforward how to use it for subset choices. Instead of mapping choice alternatives to the real number line, we propose to embed them into a higher-dimensional utility space, in which we identify choice sets with Pareto-optimal points. To this end, we propose a learning algorithm that minimizes a differentiable loss function suitable for this task. We demonstrate the feasibility of learning a Pareto-embedding on a suite of benchmark datasets. ",Learning Choice Functions via Pareto-Embeddings
211,1283372671498178566,4639078397,John Wise,"['New paper day! Led by JSPS Fellow G. Chiaki. We studied the formation of 2nd gen stars, enriched by a faint Pop III supernova (3 cases with 13, 50, and 80 Msun). We find that the C dust grains produced in the 13 Msun SN induce fragmentation at [Fe/H] = -9. <LINK> <LINK>', 'Stars with such low iron abundances may be detected in larger surveys. The record holders are [Fe/H] &lt; -7.1 (Keller+) and a detected [Fe/H] = -6.2 (Nordlander+) by @annafrebel, @AstroRana, and collab.\n\nThese ancient stars are imprinted from the first stars in the Universe. https://t.co/FcFueT4pFZ']",https://arxiv.org/abs/2007.06657,"Carbon-enhanced metal-poor (CEMP) stars are the living fossils holding records of chemical enrichment from early generations of stars. In this work, we perform a set of numerical simulations of the enrichment from a supernova (SN) of a first generation of metal-free (Pop III) star and the gravitational collapse of the enriched cloud, considering all relevant cooling/heating processes and chemical reactions as well as the growth of dust grains. We adopt faint SN models for the first time with progenitor masses $M_{\rm PopIII} = 13$--$80 \ {\rm M}_{\bigodot}$, which yield C-enhanced abundance patterns (${\rm [C/Fe]} = 4.57$--$4.75$) through mixing and fallback of innermost layers of the ejecta. This model also considers the formation and destruction of dust grains. We find that the metals ejected by the SN can be partly re-accreted by the same dark matter minihalo, and carbon abundance of the enriched cloud $A({\rm C}) = 3.80$--$5.06$ is lower than the abundance range of observed CEMP stars ($A({\rm C}) \gtrsim 6$) because the mass of the metals ejected by faint SNe is smaller than normal core-collapse SNe due to extensive fallback. We also find that cloud fragmentation is induced by gas cooling from carbonaceous grains for $M_{\rm PopIII} = 13 \ {\rm M}_{\bigodot}$ even with the lowest iron abundance ${\rm [Fe/H]} \sim -9$. This leads to the formation of low-mass stars, and these ``giga metal-poor'' stars can survive until the present-day Universe and may be found by future observations. ","Seeding the second star -- II. CEMP star formation enriched from faint
  supernovae"
212,1283208684047265792,777187089154736128,Guilin Liu,"['📢 Transposer: Universal Texture Synthesis Using Feature Maps as Transposed Convolution Filter📢\n\nVideo: <LINK>\nPaper: <LINK>\n\nWe propose a generalizable framework that can perform texture synthesis for unseen texture images in nearly real-time.', 'Conventional CNN designs for texture synthesis either simply enlarge the input textures or need to train one model per input texture. Our method is generalizable and hundreds of times faster, achieving state-of-the-art texture synthesis quality.', 'This is the joint work with @ctnzr @drewtao @edliu1105  @rtaori13 @tcwang0509 Zhiding Yu @fitsumreda @currenator @NVIDIAAI', 'Our work is based on the observation that the behavior of transposed convolution operation is analogous to the behavior of the assembling based method. \n\nLonger version video with technical details can be found at: https://t.co/mQK4q5PEdw', '@leexiaoju @jankautz Thanks Yijun', 'Most people use transposed convolution in their decoder for the upsampling purpose without caring about how it works. We find that the behavior of transposed convolution operation is analogous to the behavior of the assembling based method. https://t.co/OEk9jhC1k9']",https://arxiv.org/abs/2007.07243,"Conventional CNNs for texture synthesis consist of a sequence of (de)-convolution and up/down-sampling layers, where each layer operates locally and lacks the ability to capture the long-term structural dependency required by texture synthesis. Thus, they often simply enlarge the input texture, rather than perform reasonable synthesis. As a compromise, many recent methods sacrifice generalizability by training and testing on the same single (or fixed set of) texture image(s), resulting in huge re-training time costs for unseen images. In this work, based on the discovery that the assembling/stitching operation in traditional texture synthesis is analogous to a transposed convolution operation, we propose a novel way of using transposed convolution operation. Specifically, we directly treat the whole encoded feature map of the input texture as transposed convolution filters and the features' self-similarity map, which captures the auto-correlation information, as input to the transposed convolution. Such a design allows our framework, once trained, to be generalizable to perform synthesis of unseen textures with a single forward pass in nearly real-time. Our method achieves state-of-the-art texture synthesis quality based on various metrics. While self-similarity helps preserve the input textures' regular structural patterns, our framework can also take random noise maps for irregular input textures instead of self-similarity maps as transposed convolution inputs. It allows to get more diverse results as well as generate arbitrarily large texture outputs by directly sampling large noise maps in a single pass as well. ","Transposer: Universal Texture Synthesis Using Feature Maps as Transposed
  Convolution Filter"
213,1283060417237934081,2166703328,Dr. Cℏarles D. Brown II,"[""Changing fields from PhD to PD has been a wild ride but I love it. In my subgroup's new paper, we load a Bose-Einstein condensate into a special lattice made of light, and study how atom-atom interactions affect the lattice-trapped atoms' motional energy <LINK>"", '@PhysMossman Thanks!', '@cosmoloony Very cool. We should definitely talk about BECs in different environments some time']",https://arxiv.org/abs/2007.05928,"Geometric frustration of particle motion in a kagome lattice causes the single-particle band structure to have a flat s-orbital band. We probe this band structure by exciting a Bose-Einstein condensate into excited Bloch states of an optical kagome lattice, and then measuring the group velocity through the atomic momentum distribution. We find that interactions renormalize the band structure of the kagome lattice, greatly increasing the dispersion of the third band that, according to non-interacting band theory, should be nearly non-dispersing. Measurements at various lattice depths and gas densities agree quantitatively with predictions of the lattice Gross-Pitaevskii equation, indicating that the observed distortion of band structure is caused by the disortion of the overall lattice potential away from the kagome geometry by interactions. ","Interaction-Enhanced Group Velocity of Bosons in the Flat Band of an
  Optical Kagome Lattice"
214,1283054089719472135,3139883618,Daniel Nevo,"['Very excited about our new preprint:\n\nModeling semi-competing risks data as a longitudinal bivariate process\n\n<LINK>\n\nWe propose a new framework for time-to-event data analysis with dual, terminal and non-terminal, events. \n#statstwitter']",https://arxiv.org/abs/2007.04037,"The Adult Changes in Thought (ACT) study is a long-running prospective study of incident all-cause dementia and Alzheimer's disease (AD). As the cohort ages, death (a terminal event) is a prominent competing risk for AD (a non-terminal event), although the reverse is not the case. As such, analyses of data from ACT can be placed within the semi-competing risks framework. Central to semi-competing risks, and in contrast to standard competing risks, is that one can learn about the dependence structure between the two events. To-date, however, most methods for semi-competing risks treat dependence as a nuisance and not a potential source of new clinical knowledge. We propose a novel regression-based framework that views the two time-to-event outcomes through the lens of a longitudinal bivariate process on a partition of the time scale. A key innovation of the framework is that dependence is represented in two distinct forms, $\textit{local}$ and $\textit{global}$ dependence, both of which have intuitive clinical interpretations. Estimation and inference are performed via penalized maximum likelihood, and can accommodate right censoring, left truncation and time-varying covariates. The framework is used to investigate the role of gender and having $\ge$1 APOE-$\epsilon4$ allele on the joint risk of AD and death. ",Modeling semi-competing risks data as a longitudinal bivariate process
215,1282680056548974592,890006203316674560,Dibya Ghosh,"['Super happy to release this new paper with @marcgbellemare! We formally study how representation learning can be used to stabilize off-policy RL.\n \nArXiv: <LINK>\nAt ICML 2020: \n<LINK>\n \n1/6 <LINK>', 'We study the learning dynamics of the classic off-policy value function learning algorithm, TD(0), through the lens of the state representation. Our work reveals several insights into how certain choices of state representation affect stability and divergence of RL.\n\n2/6', 'Unlike in supervised learning, stability depends on how the state representation is parametrized. Two representations can represent exactly the same set of value functions, but have widely different stability profiles.\n\n3/6 https://t.co/Tmu1166p75', 'Certain representation learning schemes can always ensure stability. One such “stable” class we study are invariant representations. These are representations whose features can be used to predict the expected feature values at the *next* timestep. \n\n4/6 https://t.co/BAYb9CMFX8', 'These stable representations aren’t limited to theory: they can be learned with neural networks using auxiliary tasks. E.g. invariant representations are learned with an auxiliary objective to predict the next timestep feature values of a target network.\n\n5/6 https://t.co/3fiSfZKuH3', 'Our work sheds light on why previously proposed auxiliary objectives work and makes me very optimistic about principled representation learning methods as a path towards stable and performant RL. \n\n6/6']",https://arxiv.org/abs/2007.05520,"Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical TD algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks. ",Representations for Stable Off-Policy Reinforcement Learning
216,1282566937919270912,1246739202164895744,Thomas Pasini,"['The first paper of my PhD is out today! We studied the relationship between the X-ray emission of galaxy groups and their central radio sources. Check it out on <LINK> <LINK>', '@franco_vazza Happy to please you! We cross-matched a very deep X-ray catalog of 247 groups in COSMOS with VLA-COSMOS at 1.4 GHz and new MeerKAT observations (MIGHTEE project). After a few workarounds, the sample consists of 174 groups with detected central radio sources, and 73 upper limits.', '@franco_vazza We did a similar cross-match in order to build a control sample of clusters, using BCS (X-ray) and NVSS (radio). The resolution is obviously not the best, but still suitable for the aims we had. The two samples were then inspected using optical catalogs.', '@franco_vazza Interestingly, we found that only 30% of the groups central radio sources are hosted in BGGs, while for clusters almost 85% are BCG-hosted. This was not totally unexpected (see paper!), but follow-ups will come soon (stay tuned!)', '@franco_vazza The correlation between X-ray luminosity and radio power of the central source seems to show a trend for both clusters and groups; clusters are found at higher X-ray luminosity, but the radio power range is the same of groups: https://t.co/pXaiSCBZ3v', ""@franco_vazza We then performed a number of tests to check whether the Malmquist bias was the origin of the correlation. Among these tests, Monte-Carlo simulation shows that the bias is not the dominant effect, and that one needs a 'physical' correlation to reproduce the real data. https://t.co/nMlOH11SLN"", '@franco_vazza Now back to 2 tweets ago: the points of the plot are sized by the linear size of the central radio source. The group sample show that 4% of central radio sources are &gt; 200 kpc, reaching up to 600 kpc; the same fraction for clusters is only 0.7%! (Triangles are upper limits) https://t.co/GVzriGSall', '@franco_vazza Do groups usually host larger radio sources compared to clusters? This evidence, together with other works (especially on GRG), seems to suggest it. This could happen thanks to the lower density of groups, and the trigger of a more efficient accretion mode.', ""@franco_vazza This could have consequences on AGN feedback, since in this objects the energy injection happens at very large radii (due to the radio galaxy dimension: bonus pic), and cooling could be less suppressed than in 'normal' groups or clusters. https://t.co/vLIOKXJtqp"", '@franco_vazza As for the X-ray/radio correlation: groups and clusters could follow the same correlation, with groups on the low-luminosity region. The distribution could be broadened by accretions and mergers (clusters) and by the presence of large radio sources (groups, plot explained below) https://t.co/OiMZ4Ddn16', '@franco_vazza Grey dots are groups hosting small (&lt;200 kpc) radio sources. Most cool-cores (CC) follow the same trend , but we need more detailed information on the dynamical state of both groups and clusters to address this properly (again, stay tuned!). Check the paper for details 😉']",http://arxiv.org/abs/2007.04999,"Our understanding of how AGN feedback operates in galaxy clusters has improved in recent years owing to large efforts in multi-wavelength observations and hydrodynamical simulations. However, it is much less clear how feedback operates in galaxy groups, which have shallower gravitational potentials. In this work, using very deep VLA and new MeerKAT observations from the MIGHTEE survey, we compiled a sample of 247 X-ray selected galaxy groups detected in the COSMOS field. We have studied the relation between the X-ray emission of the intra-group medium and the 1.4 GHz radio emission of the central radio galaxy. For comparison, we have also built a control sample of 142 galaxy clusters using ROSAT and NVSS data. We find that clusters and groups follow the same correlation between X-ray and radio emission. Large radio galaxies hosted in the centres of groups and merging clusters increase the scatter of the distribution. Using statistical tests and Monte-Carlo simulations, we show that the correlation is not dominated by biases or selection effects. We also find that galaxy groups are more likely than clusters to host large radio galaxies, perhaps owing to the lower ambient gas density or a more efficient accretion mode. In these groups, radiative cooling of the ICM could be less suppressed by AGN heating. We conclude that the feedback processes that operate in galaxy clusters are also effective in groups. ","The relation between the diffuse X-ray luminosity and the radio power of
  the central AGN in galaxy groups"
217,1282504800165335047,717162062837719040,Phil Armitage,"['Work led by Rebecca Martin and Zhaohuan Zhu @unlv on the evolution of ""detached"" circumplanetary disks (i.e. neglecting accretion from the protoplanetary disk). We find the possibly surprising result that such disks are unstable to the growth of tilt.\n\n<LINK> <LINK>', 'The result follows from physics worked out in the 90s by Steve Lubow, Caroline Terquem, and John Papaloizou. The torque on a disk in a binary can be decomposed into steady and oscillatory components. Dissipation of the oscillatory torque can lead to tilt instability.', ""Analytic estimates show that circumplanetary disks often meet the conditions needed for instability. We verified that using both SPH (with @danprice_astro's PHANTOM code) and grid-based (Athena++) simulations. The disks tilt pretty quickly!"", 'Still to do: include accretion from the protoplanetary disk and the effects of planetary spin... which will affect the disk through oblateness precession. Accretion ought to damp tilt, but how these processes interact is not clear. Yet.']",https://arxiv.org/abs/2007.05022,"Accretion disks in binary systems can exhibit a tilt instability, arising from the interaction between components of the tidal potential and dissipation. Using a linear analysis, we show that the aspect ratios and outer radii of circumplanetary disks provide favorable conditions for tilt growth. We quantify the growth rate of the instability using particle-based ({\sc phantom}) and grid-based ({\sc athena++}) hydrodynamic simulations. For a disk with outer aspect ratio $H/r \simeq 0.1$, initially moderate tilts double on a time scale of about 15-30 binary orbits. Our results imply that detached circumplanetary disks, whose evolution is not entirely controlled by accretion from the circumstellar disk, may commonly be misaligned to the planetary orbital plane. We discuss implications for planetary spin evolution, and possible interactions between the tilt instability and Kozai-Lidov dynamics. ",A fast-growing tilt instability of detached circumplanetary disks
218,1281421836526518272,972555245179064320,Jordy de Vries,['New paper with UMass grad student Sachin Shain: <LINK> We study the effect of the QCD theta term on nuclear interactions finding severe sensitivity to the short-distance nature of the nuclear force. This affects EDMs and how axion DM interacts with nuclei.'],https://arxiv.org/abs/2007.04927,"Electric dipole moments of nuclei, diamagnetic atoms, and certain molecules are induced by CP-violating nuclear forces. Naive dimensional analysis predicts these forces to be dominated by long-range one-pion-exchange processes, with short-range forces entering only at next-to-next-to-leading order in the chiral expansion. Based on renormalization arguments we argue that a consistent picture of CP-violating nuclear forces requires a leading-order short-distance operator contributing to ${}^1S_0$-${}^3P_0$ transitions, due to the attractive and singular nature of the strong tensor force in the ${}^3P_0$ channel. The short-distance operator leads to $\mathcal O(1)$ corrections to static and oscillating, relevant for axion searches, electric dipole moments. We discuss strategies how the finite part of the associated low-energy constant can be determined in the case of CP violation from the QCD theta term by the connection to charge-symmetry violation in nuclear systems. ",Strong CP violation in nuclear physics
219,1281306072930750465,881959726958862337,Yuhuai (Tony) Wu,"['Can Neural Networks solve IQ tests? We propose Scattering Compositional Learner (SCL) for RPM Task. SCL improves SOTA from 63.9% to 95.0%. It is even capable of zero-shot generalization and learns disentangled representations!\n\npaper: <LINK>\n\n(1/n) <LINK>', 'SCL is designed to discover the compositional structures of the data. In RAVEN, It learns to discover the compositions of objects, attributes, and relationships. The figure shows an example where SCL learns the concept of “size”.\n\n(2/n) https://t.co/DlQk0j2WSE', 'By learning compositional structures, it can even generalize to unseen analogies. E.g.,  After learning (“color”, “constant”), and (“size”, “progression”), the model can generalize to (“color”, “progression”).\n\n(3/n)', 'Fun fact: Hu et. al. (https://t.co/5uqodKNCAf) found that most of the previous successful neural methods exploited a short-cut solution. After removing the dataset bias, those methods suffered a lot (e.g., CoPINet went from 91.4% -&gt; 46.3%). SCL was not affected at all.\n\n(4/n)', 'Last but not the least, this is a joint work with Honghua Dong, @RogerGrosse, and Jimmy Ba.', ""@marbin2050 @cjmaddison Thanks for encouraging words. We're exploring all potential of this work."", '@FelixHill84 Hi Felix, many thanks for encouraging words! PGM is a dataset of much larger scale, so we were not able to run the task and compare it with baselines by the deadline. But we are intending to try for sure!', '@iandanforth Hi Ian, thanks for pointing out the Abstraction and Reasoning Challenge. We will take a closer look to see if our model fits!']",https://arxiv.org/abs/2007.04212,"In this work, we focus on an analogical reasoning task that contains rich compositional structures, Raven's Progressive Matrices (RPM). To discover compositional structures of the data, we propose the Scattering Compositional Learner (SCL), an architecture that composes neural networks in a sequence. Our SCL achieves state-of-the-art performance on two RPM datasets, with a 48.7% relative improvement on Balanced-RAVEN and 26.4% on PGM over the previous state-of-the-art. We additionally show that our model discovers compositional representations of objects' attributes (e.g., shape color, size), and their relationships (e.g., progression, union). We also find that the compositional representation makes the SCL significantly more robust to test-time domain shifts and greatly improves zero-shot generalization to previously unseen analogies. ","The Scattering Compositional Learner: Discovering Objects, Attributes,
  Relationships in Analogical Reasoning"
220,1281153172464193536,426433638,Elías Cueto,"['In our last work we develop an Artificial Intelligence physicist. Provide it with data, and it will find the governing equations and reproduce this and similar phenomena.\n<LINK>\nA work by @kercus92, @a_badias, @_DGlez and F. Chinesta, supported by @ESIgroup. <LINK>', '@Zevna @kercus92 @a_badias @_DGlez @ESIgroup Encantados!']",https://arxiv.org/abs/2007.03758,"We present an algorithm to learn the relevant latent variables of a large-scale discretized physical system and predict its time evolution using thermodynamically-consistent deep neural networks. Our method relies on sparse autoencoders, which reduce the dimensionality of the full order model to a set of sparse latent variables with no prior knowledge of the coded space dimensionality. Then, a second neural network is trained to learn the metriplectic structure of those reduced physical variables and predict its time evolution with a so-called structure-preserving neural network. This data-based integrator is guaranteed to conserve the total energy of the system and the entropy inequality, and can be applied to both conservative and dissipative systems. The integrated paths can then be decoded to the original full-dimensional manifold and be compared to the ground truth solution. This method is tested with two examples applied to fluid and solid mechanics. ",Deep learning of thermodynamics-aware reduced-order models from data
221,1280694124203708416,285394452,Alex Ruch,"['New paper w/ Liz McQuillan,\xa0Erin McAweeney,\xa0Alicia Bargar, &amp; me on ArXiv <LINK>. We use network analysis, topic modeling, &amp; bridging centrality to find narratives that draw together disparate groups (many related to conspiracy) over time amid the COVID pandemic.', 'Super work from @Graphika_NYC, what a great team 😄']",https://arxiv.org/abs/2007.03443,"How can the birth and evolution of ideas and communities in a network be studied over time? We use a multimodal pipeline, consisting of network mapping, topic modeling, bridging centrality, and divergence to analyze Twitter data surrounding the COVID-19 pandemic. We use network mapping to detect accounts creating content surrounding COVID-19, then Latent Dirichlet Allocation to extract topics, and bridging centrality to identify topical and non-topical bridges, before examining the distribution of each topic and bridge over time and applying Jensen-Shannon divergence of topic distributions to show communities that are converging in their topical narratives. ","Cultural Convergence: Insights into the behavior of misinformation
  networks on Twitter"
222,1280597324272742400,801458384,Nalini Singh,"['In <LINK>, we propose a new neural network layer structure that learns joint frequency- and image-space features for Fourier imaging. (1/3) <LINK>', 'Networks comprised of this layer consistently outperform pure frequency- or image-space learning across three tasks: undersampled image reconstruction, motion correction, and denoising. (2/3)', 'Code available at https://t.co/Cg9UyteqE8; joint work @MIT_CSAIL with @JuanEugenioIgl1, Elfar Adalsteinsson, @AdrianDalca, and Polina Golland. (3/3)']",https://arxiv.org/abs/2007.01441,"We propose neural network layers that explicitly combine frequency and image feature representations and show that they can be used as a versatile building block for reconstruction from frequency space data. Our work is motivated by the challenges arising in MRI acquisition where the signal is a corrupted Fourier transform of the desired image. The proposed joint learning schemes enable both correction of artifacts native to the frequency space and manipulation of image space representations to reconstruct coherent image structures at every layer of the network. This is in contrast to most current deep learning approaches for image reconstruction that treat frequency and image space features separately and often operate exclusively in one of the two spaces. We demonstrate the advantages of joint convolutional learning for a variety of tasks, including motion correction, denoising, reconstruction from undersampled acquisitions, and combined undersampling and motion correction on simulated and real world multicoil MRI data. The joint models produce consistently high quality output images across all tasks and datasets. When integrated into a state of the art unrolled optimization network with physics-inspired data consistency constraints for undersampled reconstruction, the proposed architectures significantly improve the optimization landscape, which yields an order of magnitude reduction of training time. This result suggests that joint representations are particularly well suited for MRI signals in deep learning networks. Our code and pretrained models are publicly available at this https URL ","Joint Frequency and Image Space Learning for MRI Reconstruction and
  Analysis"
223,1280543336076582912,767094649181704192,Seth Neel 🇱🇰🇺🇸,"['New Preprint out today with @aaroth, and Saeed Sharifi-Malvajerdi! “Descent-to-Delete: Gradient-Based Methods for Machine Unlearning” (<LINK>) Motivated by GDPR\'s ""Right to be Forgotten"" we study the problem of efficiently deleting user data from AI models (1/n)', 'We give the first efficient data deletion algorithms that are able to handle long sequences of updates while promising both per-deletion run-time and error that do not grow with the length of the sequence...allowing deployed models to maintain a steady state (2/n)', 'Privacy laws like GDPR guarantee users a “right to be forgotten.” This is why large tech companies support user requests to delete their data, and a host of startups have sprung up to help enterprise companies do this (@transcend_io , @mineapp_company )… (3/n)', 'But what about the privacy risk of sensitive data that has already been used to train AI models? Research shows that AI models themselves can encode sensitive user data (https://t.co/Y5OMbFZz0i), and so we must support user requests to delete their data from trained models! (4/n)', 'A naïve soln would be to remove the users data from the training set and retrain from scratch – however this could be infeasible due to time, financial, and computational constraints. We design training algorithms in the convex setting that support efficient data deletion: (5/n)', 'Both a simple gradient descent based method, and a distributed optimization algorithm that leverages techniques from reservoir sampling to satisfy rigorous deletion guarantees, and can perform better in high dimensions (6/n)', 'We thank the paper of ginart, guan, valiant, and zou (https://t.co/vJzmAWv4kb) for giving key motivations and definitions for this problem, where they study the problem of k-means clustering. (n/n)', 'Cc @WarrenCntrPenn @Wharton @HarvardHBS @LISHarvard']",https://arxiv.org/abs/2007.02923,"We study the data deletion problem for convex models. By leveraging techniques from convex optimization and reservoir sampling, we give the first data deletion algorithms that are able to handle an arbitrarily long sequence of adversarial updates while promising both per-deletion run-time and steady-state error that do not grow with the length of the update sequence. We also introduce several new conceptual distinctions: for example, we can ask that after a deletion, the entire state maintained by the optimization algorithm is statistically indistinguishable from the state that would have resulted had we retrained, or we can ask for the weaker condition that only the observable output is statistically indistinguishable from the observable output that would have resulted from retraining. We are able to give more efficient deletion algorithms under this weaker deletion criterion. ",Descent-to-Delete: Gradient-Based Methods for Machine Unlearning
224,1280487975227469829,156804540,Francisco Rodrigues,"['In our new paper on @arxiv_org, we study two SIS-like models for interacting diseases when they are in an asymmetrically interacting regime over homogeneously mixed populations. With my friend @cosnet_bifi and my PhD student @paulocv92 . <LINK>\n#Epidemics <LINK>']",https://arxiv.org/abs/2007.02774,"Diseases and other contagion phenomena in nature and society can interact asymmetrically, such that one can benefit from the other, which in turn impairs the first, in analogy with predator-prey systems. Here, we consider two models for interacting disease-like dynamics with asymmetric interactions and different associated time scales. Using rate equations for homogeneously mixed populations, we show that the stationary prevalences and phase diagrams of each model behave differently with respect to variations of the relative time scales. We also characterize in detail the regime where transient oscillations are observed, a pattern that is inherent to asymmetrical interactions but often ignored in the literature. Our results contribute to a better understanding of disease dynamics in particular, and interacting processes in general, and could provide interesting insights for real-world applications, most notably, the interplay between the dynamics of fact-checked and fake news. ","The role of time scale in the spreading of asymmetrically interacting
  diseases"
225,1280458604928094208,3422471637,Elias Kammoun,"['Paper Day!!\n\nYou can find on arxiv (<LINK>) our new paper, with @jonastrox, presenting the results of the @NuSTAR_Science Obscured Seyferts Legacy Survey. We analyzed the spectra of 19 sources and investigated 4 models (leading to consistent results). 1/5 <LINK>', 'Using MCMC analysis, and the spectral curvature metric we found that 79-89% of our sources are heavily obscured and 33-37% are Compton-thick. Our results reconcile the numbers found from hard X-ray surveys and multi-wavelength analysis . 2/5 https://t.co/Q0jLJc2D5e', 'We found that the fraction of reprocessed emission in those AGN is negatively correlated with the X-ray luminosity and positively correlated with the LOS column density. 3/5 https://t.co/rg7EUhdtqi', 'omparing the LOS column and obscured fraction to the Eddington ratios, we found strong hints that radiation pressure regulates the circumnuclear material. We also found a negative correlation between the photon index and Eddington ratios. 4/5 https://t.co/ROMZrCz7SR', 'Finally we could get hints of strong covering fractions in some AGN, where the equatorial and LOS column densities are consistent. 5/5 https://t.co/QuVY6ZbwOc']",https://arxiv.org/abs/2007.02616,"We study the X-ray spectra of a sample of 19 obscured, optically-selected Seyfert galaxies (Sy 1.8, 1.9 and 2) in the local universe ($d \leq 175$~Mpc), drawn from the CfA Seyfert sample. Our analysis is driven by the high sensitivity of NuSTAR in the hard X-rays, coupled with soft X-ray spectra using XMM-Newton, Chandra, Suzaku, and Swift/XRT. We also analyze the optical spectra of these sources in order to obtain accurate mass estimates and Eddington fractions. We employ four different models to analyze the X-ray spectra of these sources, which all result in consistent results. We find that 79-90 % of the sources are heavily obscured with line-of-sight column density $N_{\rm H} > 10^{23}~\rm cm^{-2}$. We also find a Compton-thick ($N_{\rm H} > 10^{24}~\rm cm^{-2}$) fraction of $37-53$ %. These results are consistent with previous estimates based on multi-wavelength analyses. We find that the fraction of reprocessed to intrinsic emission is positively correlated with $N_{\rm H}$ and negatively correlated with the intrinsic, unabsorbed, X-ray luminosity (in agreement with the Iwasawa-Taniguchi effect). Our results support the hypothesis that radiation pressure regulates the distribution of the circumnuclear material. ","A hard look at local, optically-selected, obscured Seyfert galaxies"
226,1280419401876021248,1242032174305550340,Paulo Cesar Ventura,"['Glad to have this work online (<LINK>). We employ a simple analytical approach to study asymmetrically interacting spreading phenomena, describing some features that are often overlooked. Thank you @cosnet_bifi and @FranciscoICMC. <LINK>']",http://arxiv.org/abs/2007.02774,"Diseases and other contagion phenomena in nature and society can interact asymmetrically, such that one can benefit from the other, which in turn impairs the first, in analogy with predator-prey systems. Here, we consider two models for interacting disease-like dynamics with asymmetric interactions and different associated time scales. Using rate equations for homogeneously mixed populations, we show that the stationary prevalences and phase diagrams of each model behave differently with respect to variations of the relative time scales. We also characterize in detail the regime where transient oscillations are observed, a pattern that is inherent to asymmetrical interactions but often ignored in the literature. Our results contribute to a better understanding of disease dynamics in particular, and interacting processes in general, and could provide interesting insights for real-world applications, most notably, the interplay between the dynamics of fact-checked and fake news. ","The role of time scale in the spreading of asymmetrically interacting
  diseases"
227,1280092761333444609,1279642497355001857,Alexander L. Mitchell,['We solve for robot locomotion via gradient descent in a structured latent space and show the corresponding locomotion on the real-world ANYmal robot. Find more at <LINK> and <LINK>. <LINK>'],https://arxiv.org/abs/2007.01520,"Traditional approaches to quadruped control frequently employ simplified, hand-derived models. This significantly reduces the capability of the robot since its effective kinematic range is curtailed. In addition, kinodynamic constraints are often non-differentiable and difficult to implement in an optimisation approach. In this work, these challenges are addressed by framing quadruped control as optimisation in a structured latent space. A deep generative model captures a statistical representation of feasible joint configurations, whilst complex dynamic and terminal constraints are expressed via high-level, semantic indicators and represented by learned classifiers operating upon the latent space. As a consequence, complex constraints are rendered differentiable and evaluated an order of magnitude faster than analytical approaches. We validate the feasibility of locomotion trajectories optimised using our approach both in simulation and on a real-world ANYmal quadruped. Our results demonstrate that this approach is capable of generating smooth and realisable trajectories. To the best of our knowledge, this is the first time latent space control has been successfully applied to a complex, real robot platform. ","First Steps: Latent-Space Control with Semantic Constraints for
  Quadruped Locomotion"
228,1278996752784375813,1601296094,David Bowler,"['The latest Conquest paper on ferroelectrics from Jack Baker, who works with me, modelling polar morphologies in PTO on STO substrates in systems with &gt;2000 atoms.  We find polar waves and flux closure domains depending on the film thickness.\n\n<LINK>']",https://arxiv.org/abs/2007.00787,"Low dimensional structures comprised of ferroelectric (FE) PbTiO$_3$ (PTO) and quantum paraelectric SrTiO$_3$ (STO) are hosts to complex polarization textures such as polar waves, flux-closure domains and polar skyrmion phases. Density functional theory (DFT) simulations can provide insight into this order, but, are limited by the computational effort needed to simulate the thousands of required atoms. To relieve this issue, we use the novel multi-site support function (MSSF) method within DFT to reduce the solution time for the electronic groundstate whilst preserving high accuracy. Using MSSFs, we simulate thin PTO films on STO substrates with system sizes $>2000$ atoms. In the ultrathin limit, the polar wave texture with cylindrical chiral bubbles emerges as an intermediate phase between full flux closure domains and in-plane polarization. This is driven by an internal bias field born of the compositionally broken inversion symmetry in the [001] direction. Since the exact nature of this bias field depends sensitively on the film boundary conditions, this informs a new principle of design for manipulating chiral order on the nanoscale through the careful choice of substrate, surface termination or use of overlayers. Antiferrodistortive (AFD) order locally interacts with these polar textures giving rise to strong FE/AFD coupling at the PbO terminated surface driving a $p(2 \times \Lambda)$ surface reconstruction. This offers another pathway for the local control of ferroelectricity. ","Polar morphologies from first principles: PbTiO$_3$ films on SrTiO$_3$
  substrates and the $p(2 \times \Lambda)$ surface reconstruction"
229,1278980933329260545,1720813753,yappie,"['Context Graphs for Legal Reasoning and Argumentation. (arXiv:2007.00732v1 [cs.LO]) <LINK>\n\nWe propose a new, structured, logic-based framework for legal reasoning and argumentation: Instead of using a single, unstructured meaning space, theory graphs organize k…']",http://arxiv.org/abs/2007.00732,"We propose a new, structured, logic-based framework for legal reasoning and argumentation: Instead of using a single, unstructured meaning space, theory graphs organize knowledge and inference into collections of modular meaning spaces organized by inheritance and interpretation. Context graphs extend theory graphs by attack relations and interpret theories as knowledge contexts of agents in argumentation. We introduce the context graph paradigm by modeling the well-studied case Popov v. Hayashi, concentrating on the role of analogical reasoning in context graphs. ",Context Graphs for Legal Reasoning and Argumentation
230,1278638693205659649,1125655352660377600,Marlene Kretschmer,"['We just submitted our new work on ""Reconstructing regime-dependent causal relationships from observational time series"".🥳 Find the preprint here: <LINK>\n\nLed by @elena_sagg (@UniRdg_Met)  together with @JanadeWiljes (@unipotsdam) and @runge_jakob (@DLR_de)', 'In (climate) science there are many examples of changing causal relationships depending on some persistent backgorund state. For example, ENSO affects Indian rainfall predominantly in summer, downward propagation of SSWs likely depend on the tropospheric backgorund state, ect..', 'So far causal discovery algorithms cant handle such non-stationarities. In this new method paper, we now present an algorithm which can dectect regime-dependencies in the data. Thus, for given time-series, it detectes both the regimes and the causal links during those regimes https://t.co/f0JlhWAojV', 'For synthetic data it works extremely well! We also apply our method to real-world observations of ENSO and all-Indian rainfall (AIR) for which it sucessfully detects a seasonal dependence (no link in winter, strong ENSO -&gt; AIR link during summer months) https://t.co/Hg2fFRduST']",http://arxiv.org/abs/2007.00267,"Inferring causal relations from observational time series data is a key problem across science and engineering whenever experimental interventions are infeasible or unethical. Increasing data availability over the past decades has spurred the development of a plethora of causal discovery methods, each addressing particular challenges of this difficult task. In this paper we focus on an important challenge that is at the core of time series causal discovery: regime-dependent causal relations. Often dynamical systems feature transitions depending on some, often persistent, unobserved background regime, and different regimes may exhibit different causal relations. Here, we assume a persistent and discrete regime variable leading to a finite number of regimes within which we may assume stationary causal relations. To detect regime-dependent causal relations, we combine the conditional independence-based PCMCI method with a regime learning optimisation approach. PCMCI allows for linear and nonlinear, high-dimensional time series causal discovery. Our method, Regime-PCMCI, is evaluated on a number of numerical experiments demonstrating that it can distinguish regimes with different causal directions, time lags, effects and sign of causal links, as well as changes in the variables' autocorrelation. Further, Regime-PCMCI is employed to observations of El Ni\~no Southern Oscillation and Indian rainfall, demonstrating skill also in real-world datasets. ","Reconstructing regime-dependent causal relationships from observational
  time series"
231,1278546271595200513,1000782914,Cody Coleman,"['Can active learning scale to millions (potentially billions) of examples? Yes! We propose Similarity search for Efficient Active Learning and Search (SEALS) to restrict the candidates considered in each round and vastly reduce the computational complexity: <LINK>', '@ziamurai Thanks, @ziamurai! Very cool work. It seems like active learning and active search techniques, including SEALS, could be very relevant to error detection!']",https://arxiv.org/abs/2007.00077,"Many active learning and search approaches are intractable for large-scale industrial settings with billions of unlabeled examples. Existing approaches search globally for the optimal examples to label, scaling linearly or even quadratically with the unlabeled data. In this paper, we improve the computational efficiency of active learning and search methods by restricting the candidate pool for labeling to the nearest neighbors of the currently labeled set instead of scanning over all of the unlabeled data. We evaluate several selection strategies in this setting on three large-scale computer vision datasets: ImageNet, OpenImages, and a de-identified and aggregated dataset of 10 billion images provided by a large internet company. Our approach achieved similar mean average precision and recall as the traditional global approach while reducing the computational cost of selection by up to three orders of magnitude, thus enabling web-scale active learning. ","Similarity Search for Efficient Active Learning and Search of Rare
  Concepts"
232,1278525441632989185,1135286288204959744,Tao Li,"['1/3 Excited to share our work @sunipa17 @probablyjeff @viveksrikumar. <LINK> \nwhere we propose a balanced bias mitigation method.', '2/3 Projective debiasing can be too aggressive as it removes extra information that should actually be preserved. To address this, we propose a graded rotation operation to correct and rectify biases in embedding space (GloVe/RoBERTa).', '3/3 It’s a balanced approach of bias mitigation and information retention. Our intrinsic and extrinsic evaluations on gender-occupation bias show it performs on par with different projective methods while retaining more coherent information.']",https://arxiv.org/abs/2007.00049,"Language representations are known to carry stereotypical biases and, as a result, lead to biased predictions in downstream tasks. While existing methods are effective at mitigating biases by linear projection, such methods are too aggressive: they not only remove bias, but also erase valuable information from word embeddings. We develop new measures for evaluating specific information retention that demonstrate the tradeoff between bias removal and information retention. To address this challenge, we propose OSCaR (Orthogonal Subspace Correction and Rectification), a bias-mitigating method that focuses on disentangling biased associations between concepts instead of removing concepts wholesale. Our experiments on gender biases show that OSCaR is a well-balanced approach that ensures that semantic information is retained in the embeddings and bias is also effectively mitigated. ","OSCaR: Orthogonal Subspace Correction and Rectification of Biases in
  Word Embeddings"
233,1278503611232923649,101810581,Animesh Garg,"['How does arxiv releases affect double blind review process. Turns out it is notoriously hard to quantify. \n\nWe find correlations that suggest a need for a fix to review process.\n\n<LINK>\nPaper: <LINK>\nDylan Turpin, @animesh_garg @ashton1anderson <LINK>']",https://arxiv.org/abs/2007.00177,"In this paper, we investigate the effects of releasing arXiv preprints of papers that are undergoing a double-blind review process. In particular, we ask the following research question: What is the relation between de-anonymization of authors through arXiv preprints and acceptance of a research paper at a (nominally) double-blind venue? Under two conditions: papers that are released on arXiv before the review phase and papers that are not, we examine the correlation between the reputation of their authors with the review scores and acceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019 submissions (n=5050), we find statistically significant evidence of positive correlation between percentage acceptance and papers with high reputation released on arXiv. In order to understand this observed association better, we perform additional analyses based on self-specified confidence scores of reviewers and observe that less confident reviewers are more likely to assign high review scores to papers with well known authors and low review scores to papers with less known authors, where reputation is quantified in terms of number of Google Scholar citations. We emphasize upfront that our results are purely correlational and we neither can nor intend to make any causal claims. A blog post accompanying the paper and our scraping code will be linked in the project website this https URL ","De-anonymization of authors through arXiv submissions during
  double-blind review"
234,1288830012104245255,1165376585584787458,Bruno O. Goes,"['Fresh on Arxiv! In this work, we study the dynamical behavior of entropy production of a critical driven-dissipative system when it is subjected to a quantum quench: <LINK>']",https://arxiv.org/abs/2007.14445,"Driven-dissipative phase transitions are currently a topic of intense research due to the prospect of experimental realizations in quantum optical setups. The most paradigmatic model presenting such a transition is the Kerr model, which predicts the phenomenon of optical bistability, where the system may relax to two different steady-states for the same driving condition. These states, however, are inherently out-of-equilibrium and are thus characterized by the continuous production of irreversible entropy, a key quantifier in thermodynamics. In this paper we study the dynamics of the entropy production rate in a quench scenario of the Kerr model, where the external pump is abruptly changed. This is accomplished using a recently developed formalism, based on the Husimi $Q$-function, which is particularly tailored for driven-dissipative and non-Gaussian bosonic systems [Phys. Rev. Res. 2, 013136 (2020)]. Within this framework the entropy production can be split into two contributions, one being extensive with the drive and describing classical irreversibility, and the other being intensive and directly related to quantum fluctuations. The latter, in particular, is found to reveal the high degree of non-adiabaticity, for quenches between different metastable states. ","Entropy production dynamics in quench protocols of a driven-dissipative
  critical system"
235,1286065074889584640,1275456324399034376,Kate Napier,"[""My first-ever first-author paper is on the arXiv today!  We studied a z = 4.23 radio galaxy that has extended X-ray emission using Chandra.  If you're interested in learning more, see below.  \n<LINK>"", 'Many galaxies have central regions that are so bright they outshine the remaining galaxy light.  These nuclei are called active galactic nuclei or AGN.  Some of these systems are accompanied by collimated, relativistic jets and extended lobes, which shine in the radio. https://t.co/FmdI4IYWpW', 'Radio galaxies are AGN that are very luminous at radio wavelengths.  We have the instrument sensitivity appropriate to see radio galaxies at high redshift, and yet we do not see as many as we expect.  The problem becomes worse the farther back in time we look.  Why is this?', 'One hypothesis to explain the lack of radio galaxies at high redshift has to do with the Cosmic Microwave Background (CMB), the relic radiation from the Big Bang.  The CMB permeates the Universe, but is stronger the further back in time we look since the Universe was smaller.', ""The high-energy electrons in the radio galaxy's lobes will cool by one of two competing processes.  If the magnetic process dominates, the lobes will shine primarily in the radio.  However, if the CMB-related process dominates, the lobes will shine more in the X-ray."", ""Therefore, we wanted to see if the radio galaxy 4C 63.20 had any extended X-ray emission that was spatially coincident with the known radio lobes.  We used the premiere X-ray telescope Chandra and @AdiFoord's tool BAYMAX and determined that yes, there is extended X-ray emission!"", 'The X-ray component of 4C 63.20 is comprised of three parts, a core and two off-nuclear sources that contribute about 1/3 of the entire X-ray flux. https://t.co/l1kGqIEv3o', ""The results of our analysis showed that 4C 63.20 supports the idea that the CMB may quench less extreme radio lobes at high redshifts, thus leading to why we see so few radio galaxies at early stages of the Universe's history."", 'Thanks to all my collaborators for an enriching learning experience.', '@vbaldassare Thanks, Vivienne! I’ve heard lots of great things about you. I’ve actually switched research topics and will now be studying gravitational lensing.']",https://arxiv.org/abs/2007.10368,"We report on deep Chandra X-ray Telescope imaging observations of 4C 63.20, one of the few known radio galaxies at z>3.5. The X-ray counterpart is resolved into a core plus two off-nuclear sources that (combined) account for close to 30% of the total X-ray flux. Their morphology and orientation are consistent with a diffuse, lobe-like nature, albeit compact hotspots cannot be ruled out. The broadband spectral energy distribution of 4C 63.20 can be reproduced with a jet model where the majority of the radio flux can be ascribed to synchrotron emission from the hotspots, whereas the (non-nuclear) X-ray emission is produced via Inverse Compton (IC) off of Cosmic Microwave Background (CMB) photons within the extended lobes. This scenario is broadly consistent with the expectation from highly magnetized lobes in a hotter CMB, and supports the view that IC/CMB may quench less extreme radio lobes at high redshifts. ",Extended X-ray emission from the z=4.26 radio galaxy 4C 63.20
236,1285875001439203328,494870213,Thomas Haworth,"['In this paper we take another look at a tadpole-shaped globule in Carina. We find that it has probably been accelerated and compressed by radiation-driven rocket effect which has triggered the formation of the star it hosts. 1/2\n\n<LINK> <LINK>', ""If the star hadn't formed after the cloud was accelerated it would have been left behind. Instead we see that the jet coming from the star is bent, meaning the star is moving with the cloud. This is cool evidence that the stars birth was triggered! https://t.co/1DudhHaUaE""]",https://arxiv.org/abs/2007.10341,"We combine MUSE and ALMA observations with theoretical models to evaluate how a tadpole-shaped globule located in the Carina Nebula has been influenced by its environment. This globule is now relatively small (radius ~2500 au), hosts a protostellar jet+outflow (HH 900) and, with a blue-shifted velocity of ~10 km/s, is travelling faster than it should be if its kinematics were set by the turbulent velocity dispersion of the precursor cloud. Its outer layers are currently still subject to heating, but comparing the internal and external pressures implies that the globule is in a post-collapse phase. Intriguingly the outflow is bent, implying that the YSO responsible for launching it is comoving with the globule, which requires that the star formed after the globule was up to speed since otherwise it would have been left behind. We conclude that the most likely scenario is one in which the cloud was much larger before being subject to radiatively-driven implosion, which accelerated the globule to the high observed speeds under the photoevaporative rocket effect and triggered the formation of the star responsible for the outflow. The globule may now be in a quasi-steady state following collapse. Finally, the HH 900 YSO is likely $\gtrsim$1 M$_{\odot}$ and may be the only star forming in the globule. It may be that this process of triggered star formation has prevented the globule from fragmenting to form multiple stars (e.g., due to heating) and has produced a single higher mass star. ","Illuminating a tadpole's metamorphosis III: quantifying past and present
  environmental impact"
237,1283319615511306241,828301184368783360,Ingmar Posner,['We think a lot about inductive biases for unsupervised learning. But do we think enough about how they interact to form effective learning systems? We can start by characterising. Join @martinengelcke at #OOL2020 to find out more...\n<LINK>'],https://arxiv.org/abs/2007.06245,"A range of methods with suitable inductive biases exist to learn interpretable object-centric representations of images without supervision. However, these are largely restricted to visually simple images; robust object discovery in real-world sensory datasets remains elusive. To increase the understanding of such inductive biases, we empirically investigate the role of ""reconstruction bottlenecks"" for scene decomposition in GENESIS, a recent VAE-based model. We show such bottlenecks determine reconstruction and segmentation quality and critically influence model behaviour. ",Reconstruction Bottlenecks in Object-Centric Generative Models
238,1282609495361953792,1702174146,Janis Keuper,"['We have a few new papers out: 1: ""Learning Embeddings for Image Clustering: An Empirical Study of Triplet Loss Approaches"" accepted at ICPR. <LINK> #Deeplearning #ICPR21 <LINK>']",https://arxiv.org/abs/2007.03123,"In this work, we evaluate two different image clustering objectives, k-means clustering and correlation clustering, in the context of Triplet Loss induced feature space embeddings. Specifically, we train a convolutional neural network to learn discriminative features by optimizing two popular versions of the Triplet Loss in order to study their clustering properties under the assumption of noisy labels. Additionally, we propose a new, simple Triplet Loss formulation, which shows desirable properties with respect to formal clustering objectives and outperforms the existing methods. We evaluate all three Triplet loss formulations for K-means and correlation clustering on the CIFAR-10 image classification dataset. ","Learning Embeddings for Image Clustering: An Empirical Study of Triplet
  Loss Approaches"
239,1281681951032840200,3413263721,Eduardo Mojica-Nava,"['NEW draft ∲: ""Systemic Performance Measures from Distributional Zeta-Function"" with @saudadedaluz36 \n✓ We propose the use of the Distributional Zeta-Function  for constructing a new set of Systemic Performance Measures\n<LINK>\nComments are appreciated!!', '@Cauribe3 @saudadedaluz36 https://t.co/OVO5R2uuUF']",https://arxiv.org/abs/2007.02288,"We propose the use of the Distributional Zeta-Function (DZF) for constructing a new set of Systemic Performance Measures (SPM). SPM have been proposed to investigate network synthesis problems such as the growing of linear consensus networks. The adoption of the DZF has shown interesting physical consequences that in the usual replica method are still unclarified, i.e., the connection between the spontaneous symmetry breaking mechanism and the structure of the replica space in the disordered model. We relate topology of the network and the partition function present in the DZF by using the spectral and the Hamiltonian structure of the system. The studied objects are the generalized partition funcion, the DZF, the Expected value of the replica partition function, and the quenched free energy of a field network. We show that with these objects we need few operations to increase the percentage of performance enhancement of a network. Furthermore, we evalue the location of the optimal added links for each new SPM and calculate the performance improvement of the new network for each new SPM via the spectral zeta function, $\mathcal{H}_{2}$-norm, and the communicability between nodes. We present the advantages of this new set of SPM in the network synthesis and we propose other methods for using the DZF to explore some issues such as disorder, critical phenomena, finite-temperature, and finite-size effects on networks. Relevance of the results are discussed. ",Systemic Performance Measures from Distributional Zeta-Function
240,1281406090262061056,4455137243,Joel Ye,"['(1/n) Happy to present ""Auxiliary Tasks Speed Up Learning PointGoal Navigation."" \nDDPPO shows PointNav is learned in 2.5B frames. We propose to use multiple aux tasks to minimize this req \n\nPaper: <LINK>\nJoint work with @DhruvBatraDB , @erikwijmans ,  @abhshkdz <LINK>', '(2/n) We use self-supervised tasks, enabling use in real robots. We find: \n1. Single aux tasks help\n2. Adding multiple task losses eke out more gains\n3. Fusion of modules using different tasks does best!', '(3/n) Using modules assigned to different tasks also allows visualization of ""task usage."" Specialized representation attention also happens when fusing Taskonomy representations, but our work shows it happens even when we learn from scratch. https://t.co/VvKUUG5vNW', '(4/4) Check out our code at https://t.co/QtqU99FvD1']",https://arxiv.org/abs/2007.04561,"PointGoal Navigation is an embodied task that requires agents to navigate to a specified point in an unseen environment. Wijmans et al. showed that this task is solvable but their method is computationally prohibitive, requiring 2.5 billion frames and 180 GPU-days. In this work, we develop a method to significantly increase sample and time efficiency in learning PointNav using self-supervised auxiliary tasks (e.g. predicting the action taken between two egocentric observations, predicting the distance between two observations from a trajectory,etc.).We find that naively combining multiple auxiliary tasks improves sample efficiency,but only provides marginal gains beyond a point. To overcome this, we use attention to combine representations learnt from individual auxiliary tasks. Our best agent is 5.5x faster to reach the performance of the previous state-of-the-art, DD-PPO, at 40M frames, and improves on DD-PPO's performance at 40M frames by 0.16 SPL. Our code is publicly available at this https URL ",Auxiliary Tasks Speed Up Learning PointGoal Navigation
241,1280702492314087424,2369610869,David Setton,"['My first publication, a paper I worked on with Jenny Greene, just went up tonight. We looked at the incidence of AGN activity in post-starburst galaxies and found that the youngest galaxies are the most likely to host [OIII] AGN <LINK> <LINK>', 'It looks like this could be tied to the gas content; another paper coming out soon finds that younger post-starburst also have lots of gas that could feed their supermassive black holes. It also could be that we’re catching the tail end of whatever activity quenched these PSBs', 'Whatever it ends up being, really glad to have worked on this with Jenny and I hope y’all enjoy reading it. More to come from SQuIGGLE very soon 😄', '@dkeidar The cliff’s notes are that we’re looking at galaxies which recently shut off their star formation and trying to understand why, and we found a link where the galaxies that shut off their star formation most recently also have active black holes at their centers', '@dkeidar That’s close; the black holes are there in all these massive galaxies, but it’s the event that caused them to start being fed at high rates that may have shut off the star formation, and the gas feeding them now could not be forming new stars because of the energy from that', '@dkeidar It’s really tough to link causality though because the timescales are so short (relative to the age of galaxies as a whole) and this data doesn’t tell the whole story. That’s what we’re trying to get at in the future: what do these look like spatially resolved, in the radio, etc.', '@dkeidar Gladly will provide them :)', '@dkeidar The data for this work is publicly available as part of the Sloan Digital Sky Survey, which has taken on the order of millions of spectra of bright objects in the sky. Our sub sample has existed for ~4 years and we’re on the cusp of a few other pubs using it and ancillary data']",https://arxiv.org/abs/2007.02967,"We study the incidence of nuclear activity in a large sample of massive post-starburst galaxies at z~0.7 selected from the Sloan Digital Sky Survey, and identify active galactic nuclei based on radio continuum and optical emission lines. Over our mass range of 10^10.6-10^11.5 Msun, the incidence of radio activity is weakly dependent on stellar mass and independent of stellar age, while radio luminosity depends strongly on stellar mass. Optical nuclear activity incidence depends most strongly on the Dn4000 line index, a proxy for stellar age, with an active fraction that is ~ten times higher in the youngest versus oldest post-starburst galaxies. Since a similar trend is seen between age and molecular gas fractions, we argue that, like in local galaxies, the age trend reflects a peak in available fueling rather than feedback from the central black hole on the surrounding galaxy. ","The Role of Active Galactic Nuclei in the Quenching of Massive Galaxies
  in the SQuiGGLE Survey"
242,1280694676824248320,156804540,Francisco Rodrigues,"['Our new paper on @arxiv_org : We study random geographic graphs by using concepts of random matrix theory. With @eestradalab, Antonio Bermúdez and collaborators. #networks \n<LINK> <LINK>']",https://arxiv.org/abs/2007.02453,"In this work we perform a detailed statistical analysis of topological and spectral properties of random geometric graphs (RGGs); a graph model used to study the structure and dynamics of complex systems embedded in a two dimensional space. RGGs, $G(n,\ell)$, consist of $n$ vertices uniformly and independently distributed on the unit square, where two vertices are connected by an edge if their Euclidian distance is less or equal than the connection radius $\ell \in [0,\sqrt{2}]$. To evaluate the topological properties of RGGs we chose two well-known topological indices, the Randi\'c index $R(G)$ and the harmonic index $H(G)$. While we characterize the spectral and eigenvector properties of the corresponding randomly-weighted adjacency matrices by the use of random matrix theory measures: the ratio between consecutive eigenvalue spacings, the inverse participation ratios and the information or Shannon entropies $S(G)$. First, we review the scaling properties of the averaged measures, topological and spectral, on RGGs. Then we show that: (i) the averaged--scaled indices, $\left\langle R(G) \right\rangle$ and $\left\langle H(G) \right\rangle$, are highly correlated with the average number of non-isolated vertices $\left\langle V_\times(G) \right\rangle$; and (ii) surprisingly, the averaged--scaled Shannon entropy $\left\langle S(G) \right\rangle$ is also highly correlated with $\left\langle V_\times(G) \right\rangle$. Therefore, we suggest that very reliable predictions of eigenvector properties of RGGs could be made by computing topological indices. ",Topological versus spectral properties of random geometric graphs
243,1278623624874930176,1360175358,Michele Starnini,"['Last out! @electionstudies survey data shows that extreme opinions wrt different topics can be correlated. We propose a model where these polarized ideological opinions emerge, without assuming apriori such correlations or preexisting social structures 1/3\n<LINK> <LINK>', 'In our model, opinions evolve in a multidimensional space, driven by dynamical, homophilic social interactions. Topics form a non-orthogonal basis of the space, ie they can overlap. Ideological states emerge even between rather unrelated but sufficiently controversial topics. 2/3 https://t.co/jmG6fz7jaD', 'The phase transition between consensus, opinion polarization, and ideological states can be analytically characterized as a function of the controversialness and overlap of the topics. Thanks to F. Baumann, @philipplenz6 and I. Sokolov. Comments welcome! 3/3 https://t.co/NMR4k50jpU', '@dgarcia_eu @electionstudies Sure, we read and cited your very interesting paper! Here instead we assume that opinion formation is driven by homophilic social interactions (based on activity-driven dynamics). This also leads to a social network segregated according to different opinions.']",https://arxiv.org/abs/2007.00601,"Opinion polarization is on the rise, causing concerns for the openness of public debates. Additionally, extreme opinions on different topics often show significant correlations. The dynamics leading to these polarized ideological opinions pose a challenge: How can such correlations emerge, without assuming them a priori in the individual preferences or in a preexisting social structure? Here we propose a simple model that qualitatively reproduces ideological opinion states found in survey data, even between rather unrelated, but sufficiently controversial, topics. Inspired by skew coordinate systems recently proposed in natural language processing models, we solidify these intuitions in a formalism of opinions unfolding in a multidimensional space where topics form a non-orthogonal basis. Opinions evolve according to the social interactions among the agents, which are ruled by homophily: two agents sharing similar opinions are more likely to interact. The model features phase transitions between a global consensus, opinion polarization, and ideological states. Interestingly, the ideological phase emerges by relaxing the assumption of an orthogonal basis of the topic space, i.e. if topics thematically overlap. Furthermore, we analytically and numerically show that these transitions are driven by the controversialness of the topics discussed, the more controversial the topics, the more likely are opinion to be correlated. Our findings shed light upon the mechanisms driving the emergence of ideology in the formation of opinions. ","Emergence of polarized ideological opinions in multidimensional topic
  spaces"
