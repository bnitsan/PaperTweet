,TweetID,AuthorID,AuthorName,Tweets,arxiv_link,Abstract,Title
0,1513908739706925067,16197301,Raphael Schumann,"[""New paper to appear at #ACL2022nlp. We analyze Vision and Language Navigation in unseen outdoor areas. Spoiler: Vision isn't that important. Paper also includes strong baseline that achieves SOTA on Touchdown and introduces additional data for outdoor VLN. <LINK>""]",https://arxiv.org/abs/2203.13838,"Vision and language navigation (VLN) is a challenging visually-grounded language understanding task. Given a natural language navigation instruction, a visual agent interacts with a graph-based environment equipped with panorama images and tries to follow the described route. Most prior work has been conducted in indoor scenarios where best results were obtained for navigation on routes that are similar to the training routes, with sharp drops in performance when testing on unseen environments. We focus on VLN in outdoor scenarios and find that in contrast to indoor VLN, most of the gain in outdoor VLN on unseen data is due to features like junction type embedding or heading delta that are specific to the respective environment graph, while image information plays a very minor role in generalizing VLN to unseen outdoor areas. These findings show a bias to specifics of graph representations of urban environments, demanding that VLN tasks grow in scale and diversity of geographical environments. ","Analyzing Generalization of Vision and Language Navigation to Unseen
  Outdoor Areas"
1,1513618963330224138,1212484521678929920,Leonie Weissweiler,"['Excited to announce our new paper ""CaMEL: Case Marker Extraction without Labels üê´"", with @vjhofmann, @_masoudjalili, and @HinrichSchuetze  was accepted to #ACL2022!\n<LINK> (1/5)', 'First, we introduce the new task of extracting case markers from a parallel corpus without morphological segmentation or annotation. Then, we automatically compile a silver standard to evaluate this new task by extracting case markers from the UniMorph dataset. (2/5)', 'We also present a model featuring annotation projection using alignments and statistical tests that achieves 45% F1 on the silver standard. (3/5)', 'After extracting case markers for 83 languages from the Parallel Bible Corpus, we demonstrate how they can be used to cluster noun phrases in it by their deep case or semantic role, opening up exciting avenues for further research. (4/5)', 'We provide our silver standard and our code at https://t.co/p5j7u5fe5f and look forward to seeing you in Dublin! üáÆüá™ (5/5)']",https://arxiv.org/abs/2203.10010,"We introduce CaMEL (Case Marker Extraction without Labels), a novel and challenging task in computational morphology that is especially relevant for low-resource languages. We propose a first model for CaMEL that uses a massively multilingual corpus to extract case markers in 83 languages based only on a noun phrase chunker and an alignment system. To evaluate CaMEL, we automatically construct a silver standard from UniMorph. The case markers extracted by our model can be used to detect and visualise similarities and differences between the case systems of different languages as well as to annotate fine-grained deep cases in languages in which they are not overtly marked. ",CaMEL: Case Marker Extraction without Labels
2,1513175376700350474,807052015285207040,Michal Zajaƒçek,['A new paper on the identification of mid-infrared sources in the Galactic center led by Harshitha Bhat- we identified stellar sources and their proper motions as well as a coherent motion of compact dust clumps along the minispiral #astronomy #sgrastar\n<LINK> <LINK>'],https://arxiv.org/abs/2203.16727,"Mid-Infrared (MIR) images of the Galactic center show extended gas and dust features along with bright IRS sources. Some of these dust features are a part of ionized clumpy streamers orbiting Sgr~A*, known as the mini-spiral. We present their proper motions over 12 year time period and report their flux densities in $N$-band filters {and derive their spectral indices}. The observations were carried out by VISIR at ESO VLT. High-pass filtering led to the detection of several resolved filaments and clumps along the mini-spiral. Each source was fit by a 2-D Gaussian profile to determine the offsets and aperture sizes. We perform aperture photometry to extract fluxes in two different bands. We present the proper motions of the largest consistent set of resolved and reliably determined sources. In addition to stellar orbital motions, we identify a stream-like motion of extended clumps along the mini-spiral. We also detect MIR counterparts of the radio tail components of the IRS7 source. They show a clear kinematical deviation with respect to the star. They likely represent Kelvin-Helmholtz instabilities formed downstream in the shocked stellar wind. We also analyze the shape and the orientation of the extended late-type IRS3 star that is consistent with the ALMA sub-mm detection of the source. Its puffed-up envelope with the radius of $\sim 2\times 10^6\,R_{\odot}$ could be the result of the red-giant collision with a nuclear jet, which was followed by the tidal prolongation along the orbit. ",Mid-Infrared studies of dusty sources in the Galactic Center
3,1513139805630402561,723860928253435905,Yoni Nazarathy,['A new short queueing theory paper on arxiv: <LINK>. Thanks @ZPalmowski for pushing it all.'],https://arxiv.org/abs/2203.15456,"We study critical GI/G/1 queues under finite second moment assumptions. We show that the busy period distribution is regularly varying with index half. We also review previously known M/G/1/ and M/M/1 derivations, yielding exact asymptotics as well as a similar derivation for GI/M/1. The busy period asymptotics determine the growth rate of moments of the renewal process counting busy cycles. We further use this to demonstrate a BRAVO phenomenon (Balancing Reduces Asymptotic Variance of Outputs) for the work-output process (namely the busy-time). This yields new insight on the BRAVO effect. A second contribution of the paper is in settling previous conjectured results about GI/G/1 and GI/G/s BRAVO. Previously, infinite buffer BRAVO was generally only settled under fourth-moment assumptions together with an assumption about the tail of the busy-period. In the current paper we strengthen the previous results by reducing to assumptions to existence of $2+\epsilon$ moments. ",On Busy Periods of the Critical GI/G/1 Queue and BRAVO
4,1512554336928206848,417085651,Erik Nijkamp,['Excited to have @TechCrunch cover our open-source model CodeGen - generating executable code from English ü•≥\n\nPaper: <LINK>\nCode: <LINK>\n\n<LINK>'],https://arxiv.org/abs/2203.13474,"Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: this https URL ",A Conversational Paradigm for Program Synthesis
5,1512169438090461190,2835683058,Silvio Savarese,['Happy to share this article by TechCrunch that features our most recent work on AI for Code Generation @Salesforce AI Research \n\n<LINK>\n\nMore info about CodeGen are here:\nPaper:\xa0<LINK>\nBlog:\xa0<LINK>\nCode:\xa0<LINK>'],https://arxiv.org/abs/2203.13474,"Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: this https URL ",A Conversational Paradigm for Program Synthesis
6,1511712581064474629,1111208974064459776,Chaopeng Shen,"[""New paper on arxiv. Been talking about this work in seminars since Oct 2021. We dev'ed a differentiable, regionalized process-based model with physical fluxes that reaches median NSE of 0.715 on CAMELS, compared to 0.72 for LSTM for the same setup (NLDAS). <LINK>"", ""1. Our method's regionalized -- it's applicable to ungauged basins or regions (though not included here, we already saw it works well for PUB or PUR)\n2. We used no post-processors. Each step is interpretable\n3. We used NLDAS. Not the most ideal forcing. Will try other forcings."", ""We compared baseflow fractions and time series of ET to alternative estimates, which matched reasonably well. It's possible to diagnose other internal variables and fluxes. We demonstrate the possibility to get near state-of-the-art performance with physical interpretability."", 'Note -- please do not get us wrong. This work does not weaken the value of DL models like LSTM. In fact, it remains highly, highly valuable. We are discussing it in more detail in upcoming papers.', 'Yet I also cannot overstate the importance of this work. Hopefully we can explain it soon as it gets formally published.', 'If you have not worked with the CAMELS dataset using traditional models, you cannot appreciate how striking this number is, esp. for regionalized models, w/o post-processors, for NLDAS. Everything needs to be optimal for a model to achieve this number. LSTM represents the ceiling', 'While the performance is similar or marginally lower than LSTM, we can now get detailed internal fluxes and states like baseflow, ET, water storage, soil moisture out of the model. The model is interpretable and you can tell a story to stakeholders.']",https://arxiv.org/abs/2203.14827,"Predictions of hydrologic variables across the entire water cycle have significant value for water resource management as well as downstream applications such as ecosystem and water quality modeling. Recently, purely data-driven deep learning models like long short-term memory (LSTM) showed seemingly-insurmountable performance in modeling rainfall-runoff and other geoscientific variables, yet they cannot predict unobserved physical variables and remain challenging to interpret. Here we show that differentiable, learnable, process-based models (called {\delta} models here) can approach the performance level of LSTM for the intensively-observed variable (streamflow) with regionalized parameterization. We use a simple hydrologic model HBV as the backbone and use embedded neural networks, which can only be trained in a differentiable programming framework, to parameterize, replace, or enhance the process-based model modules. Without using an ensemble or post-processor, {\delta} models can obtain a median Nash Sutcliffe efficiency of 0.715 for 671 basins across the USA for a particular forcing data, compared to 0.72 from a state-of-the-art LSTM model with the same setup. Meanwhile, the resulting learnable process-based models can be evaluated (and later, to be trained) by multiple sources of observations, e.g., groundwater storage, evapotranspiration, surface runoff, and baseflow. Both simulated evapotranspiration and fraction of discharge from baseflow agreed decently with alternative estimates. The general framework can work with models with various process complexity and opens up the path for learning physics from big data. ","Differentiable, learnable, regionalized process-based models with
  physical outputs can approach state-of-the-art hydrologic prediction accuracy"
7,1511274289852567554,561899047,Aki Vehtari,"['New paper ""Robust, Automated, and Accurate Black-box Variational Inference"" <LINK> with great co-authors @manushivid, @Michael_riis, and @jhhhuggins <LINK>', 'RAABBVI has a new learning rate adaptation using convergence diagnostics, user-adjustable accuracy parameter, and it predicts decrease in accuracy given additional computation time. Code PR in Viabel Python package https://t.co/hbQbHChBSk https://t.co/bCf6Botors']",https://arxiv.org/abs/2203.15945,"Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust, Automated, and Accurate BBVI (RAABBVI), a framework for reliable BBVI optimization. RAABBVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RAABBVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leiber (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RAABBVI through carefully designed simulation studies and on a diverse set of real-world model and data examples. ","Robust, Automated, and Accurate Black-box Variational Inference"
8,1511065551061217284,1367548995799760898,Muhammad Haroon,"['Do YouTube recommendations lead users to ideologically extreme rabbit holes? Can ideological bias in recommendation algorithms be minimized? Read our new paper <LINK>. Short answers: Yes, but there‚Äôs nuance (especially among right-wing users) üëá [1/7] <LINK>', 'Past audits either gathered data 1Ô∏è‚É£ from real users or 2Ô∏è‚É£ through sock puppets/APIs. 1Ô∏è‚É£ does not account for recommendation algorithms. 2Ô∏è‚É£ does not account for the role of user watch history. How do we reconcile these? ü§î [2/7]', 'We do a large-scale audit of YouTube‚Äôs recommendations by training over üíØ,000 sock puppets to watch videos of different political ideologies (from v. left to v. right). We then analyze the recommendations for ideological bias, its amount, and over-time radicalization. [3/7] https://t.co/Xph4DfnPOn', 'And? Both the homepage and up-next video recommendations for a trained sock puppet *are* ideologically biased: users are more likely to encounter content that is ideologically similar to their watch history. This is especially the case for the right sock puppets. üò≥ [4/7] https://t.co/JXKlEUUvRy', 'Not only that: Exposure to ideologically biased videos increases: As the user follows the trail, they encounter a larger number of ideologically biased videos AND the videos deeper in the recommendation trail are more extreme. [5/7] https://t.co/dIA6qGEOKc', 'To mitigate these biases, we advance a principled learning-based intervention that monitors a user‚Äôs homepage for ideological bias and mitigates it by injecting intervention videos in the user‚Äôs watch history. Hint: it works but is much harder for right-leaning users. ü•µ [6/7] https://t.co/ZI6r2dXehV', 'This work is part of a multi-disciplinary collaboration at UC Davis (@nshuman_chhabra, Xin Liu, @prasantm, @zubair_shafiq, and @mwojcieszak). Check out our paper and code/data at https://t.co/FD8pFVy7eE [7/7]']",https://arxiv.org/abs/2203.10666,"Recommendations algorithms of social media platforms are often criticized for placing users in ""rabbit holes"" of (increasingly) ideologically biased content. Despite these concerns, prior evidence on this algorithmic radicalization is inconsistent. Furthermore, prior work lacks systematic interventions that reduce the potential ideological bias in recommendation algorithms. We conduct a systematic audit of YouTube's recommendation system using a hundred thousand sock puppets to determine the presence of ideological bias (i.e., are recommendations aligned with users' ideology), its magnitude (i.e., are users recommended an increasing number of videos aligned with their ideology), and radicalization (i.e., are the recommendations progressively more extreme). Furthermore, we design and evaluate a bottom-up intervention to minimize ideological bias in recommendations without relying on cooperation from YouTube. We find that YouTube's recommendations do direct users -- especially right-leaning users -- to ideologically biased and increasingly radical content on both homepages and in up-next recommendations. Our intervention effectively mitigates the observed bias, leading to more recommendations to ideologically neutral, diverse, and dissimilar content, yet debiasing is especially challenging for right-leaning users. Our systematic assessment shows that while YouTube recommendations lead to ideological bias, such bias can be mitigated through our intervention. ","YouTube, The Great Radicalizer? Auditing and Mitigating Ideological
  Biases in YouTube Recommendations"
9,1511001832344395784,1176867972163559424,MartinHuber,"['Are identifying assumptions for #CausalAnalysis and #PolicyEvaluation always untestable? My new working paper shows that under certain conditions, identifying assumptions (like unconfoundedness) can actually be tested in data: <LINK>\n#Epitwitter #Econtwitter', '@yudapearl @nickchk @instrumenthull @wuthrich_k @jondr44 @pedrohcgs @jmwooldridge @VC31415 @carolcaetanoUGA maybe you find this interesting...', '@causalinf']",https://arxiv.org/abs/2203.15890,"This study demonstrates the existence of a testable condition for the identification of the causal effect of a treatment on an outcome in observational data, which relies on two sets of variables, namely observed covariates to be controlled for and a suspected instrument. Under a causal structure commonly found in empirical applications, the testable conditional independence of the suspected instrument and the outcome given the treatment and the covariates has two implications. First, the instrument is valid, i.e.\ it does not directly affect the outcome (other than through the treatment) and is unconfounded conditional on the covariates. Second, the treatment is unconfounded conditional on the covariates such that the treatment effect is identified. We suggest tests of this conditional independence based on doubly robust estimators and investigate their finite sample performance in a simulation study. We also apply our testing approach to the evaluation of the impact of fertility on female labor supply when using the sibling sex ratio of the first two children as supposed instrument, which by and large points to a violation of our testable implication, at least for the moderate set of socio-economic covariates considered. ",Testing the identification of causal effects in data
10,1510918086509137921,1144657431382958082,Jannis Kurtz,"['If you are interested in #robustoptimization with discrete uncertainty sets maybe you want to check our new short paper: <LINK>\n\nWith Marc Goerigk we study the problem of selecting start scenarios for iterative scenario generation methods for RO problems    1/n', 'Our heuristic learns the relevance of a scenario by extracting information from training data. The main observation is that even choosing a single start scenario can lead to a significant benefit for the subsequent iterative process which was quite surprising for us.', 'Hence using available training data can be useful. Unfortunately there is still no data basis for robust optimization instances. In our case each scenario has to be labeled to be relevant or not, which implies that the instance need to be solved to optimality to be labeled.', 'We circumvent this problem by constructing instances where relevant scenarios are known by construction. However testing the idea for more realistic instances is desirable. Marc Goerigk recently started to collect instances https://t.co/3QzYWa5abW Feel free to submit instances!']",https://arxiv.org/abs/2203.16642,"In this work we study robust one- and two-stage problems with discrete uncertainty sets which are known to be hard to solve even if the underlying deterministic problem is easy. Popular solution methods iteratively generate scenario constraints and possibly second-stage variables. This way, by solving a sequence of smaller problems, it is often possible to avoid the complexity of considering all scenarios simultaneously. A key ingredient for the performance of the iterative methods is a good selection of start scenarios. In this paper we propose a data-driven heuristic to seed the iterative solution method with a set of starting scenarios that provide a strong lower bound early in the process, and result in considerably smaller overall solution times compared to other benchmark methods. Our heuristic learns the relevance of a scenario by extracting information from training data based on a combined similarity measure between robust problem instances and single scenarios. Our experiments show that predicting even a small number of good start scenarios by our method can considerably reduce the computation time of the iterative methods. ",Data-driven Prediction of Relevant Scenarios for Robust Optimization
11,1509969111044001796,815300326127534080,Jaan Aru,"['In our new paper, we argue that deep learning is not (yet) ready to conquer Theory of Mind. \n\nThe agents discover shortcuts instead of learning anything about other minds\n\n<LINK>\n\nW @aqeel_labash, @ocorcoll, &amp; Raul Vicente\n\nOur recommendations are in the figureüëá <LINK>']",https://arxiv.org/abs/2203.16540,"Theory of Mind is an essential ability of humans to infer the mental states of others. Here we provide a coherent summary of the potential, current progress, and problems of deep learning approaches to Theory of Mind. We highlight that many current findings can be explained through shortcuts. These shortcuts arise because the tasks used to investigate Theory of Mind in deep learning systems have been too narrow. Thus, we encourage researchers to investigate Theory of Mind in complex open-ended environments. Furthermore, to inspire future deep learning systems we provide a concise overview of prior work done in humans. We further argue that when studying Theory of Mind with deep learning, the research's main focus and contribution ought to be opening up the network's representations. We recommend researchers use tools from the field of interpretability of AI to study the relationship between different network components and aspects of Theory of Mind. ",Mind the gap: Challenges of deep learning approaches to Theory of Mind
12,1509954332615221254,1116002690604130305,Juliette Becker,"['New on the arXiv: <LINK>, a paper led by Michelle Belkovski, a University of Michigan senior who started this project 2 years ago as a @UROPumich student, which has now resulted in her first first-author paper!', 'In her paper, Michelle studied the cold super-puff planet HIP 41378 f, which is only 12 Earth masses but a Jupiter radii. Such a low density is hard to explain, and Michelle modeled the interior structure of the planet to find that it must have a low core mass fraction (&lt;25%). https://t.co/4J7N8Jgbbl', 'This is weird because that fact, combined with its small core mass (less than 3 Earth masses), together provide tension with the core accretion paradigm.', 'Michelle studied how the atmospheres of all planets in the system changed as the star evolved using @VPLanetCode, and was able to confirm that while the inner two planets have lost some atmospheric mass due to photoevaporation, it was not enough that they started as super-puffs.', ""This leaves one main hypothesis: that rings could explain HIP 41378 f's low density (https://t.co/Pwe1rvR2my by @astroshrey and https://t.co/J7qjiBjPHS). HIP 41378 f's interior structure's tension with the core accretion paradigm leave rings as a very plausible explanation!"", ""(we actually set on this project out to disprove the rings hypothesis, but we found that we couldn't, and it remains an extremely viable way o explain HIP 41378 f!)"", 'The other cool thing is that very recent results (https://t.co/aK8xkjEifB and https://t.co/pBYjRdOzdF) found that the HST transmission spectrum of this planet still is consistent with rings... nothing yet seems to be able to rule out the rings hypothesis for HIP 41378 f!', ""@a_santerne @NASAWebb @ESA_Webb Yes!! I can't wait to see those results. Rings would be SO COOL!!""]",https://arxiv.org/abs/2203.17180,"The census of known exoplanets exhibits a variety of physical parameters, including densities that are measured to span the range from less dense than styrofoam to more dense than iron. These densities represent a large diversity of interior structures. Despite this staggering diversity, recent analyses have shown that the densities of planets that orbit a common star exhibit remarkable uniformity. A fascinating exception to this is the system HIP 41378 (also known as K2-93), which contains a super-puff planet, HIP 41378 f, as well as several planets with more typical bulk densities. The range of densities in this system begs the question of what physical processes are responsible for the disparate planetary structures in this system. In this paper, we consider how the densities of the planets in the HIP 41378 system would have changed over time as the host star evolved and the planets' atmospheres were subsequently affected by the evolving insolation level. We also present a range of allowable core masses for HIP 41378 f based on the measured planet parameters, and comment on the feasibility of the proposed existence of planetary rings around HIP 41378 f as an explanation for its current low density. ","A Multi-Planet System's Sole Super-Puff: Exploring Allowable Physical
  Parameters for the Cold Super-Puff HIP 41378 f"
13,1509878142034493444,901266828655284225,Brian Metzger,"['New paper from my student Dhruv on nu-driven winds from rapidly spinning (~ms) just-born neutron stars.  Nucleosynthesis will differ from typical slow-spinning NS birth, but the rotation-boosted mass-loss rate allows these beasts to out-punch their weight. <LINK> <LINK>']",https://arxiv.org/abs/2203.16560,"We explore the effects of rapid rotation on the properties of neutrino-heated winds from proto-neutron stars (PNS) formed in core-collapse supernovae or neutron-star mergers by means of three-dimensional general-relativistic hydrodynamical simulations with M0 neutrino transport. We focus on conditions characteristic of a few seconds into the PNS cooling evolution when the neutrino luminosities obey $L_{\nu_e} + L_{\bar{\nu}_e} \approx 7\times 10^{51}$ erg s$^{-1}$, and over which most of the wind mass-loss will occur. After an initial transient phase, all of our models reach approximately steady-state outflow solutions with positive energies and sonic surfaces captured on the computational grid. Our non-rotating and slower-rotating models (angular velocity relative to Keplerian $\Omega/\Omega_{\rm K} \lesssim 0.4$; spin period $P \gtrsim 2$ ms) generate approximately spherically symmetric outflows with properties in good agreement with previous PNS wind studies. By contrast, our most rapidly spinning PNS solutions ($\Omega/\Omega_{\rm K} \gtrsim 0.75$; $P \approx 1$ ms) generate outflows focused in the rotational equatorial plane with much higher mass-loss rates (by over an order of magnitude), lower velocities, lower entropy, and lower asymptotic electron fractions, than otherwise similar non-rotating wind solutions. Although such rapidly spinning PNS are likely rare in nature, their atypical nucleosynthetic composition and outsized mass yields could render them important contributors of light neutron-rich nuclei compared to more common slowly rotating PNS birth. Our calculations pave the way to including the combined effects of rotation and a dynamically-important large-scale magnetic field on the wind properties within a 3D GRMHD framework. ","Three-Dimensional General-Relativistic Simulations of Neutrino-Driven
  Winds from Rotating Proto-Neutron Stars"
14,1509865606291640324,1355520746605449220,Yifan Wang,"['A new paper <LINK> done by @alexandernitz  and me! (a serious paper, not for April fool) We search for coincident gravitational waves from LIGO/Virgo and fast radio bursts from CHIME with nearly the same time and sky localization. (1/3)', 'This figure perfectly illustrates our methodology, the blue contours are the sky localization of a gravitational wave trigger, and the marker represents a fast radio burst that occurred 22 seconds later than the gravitational wave. (2/3) https://t.co/WKYAxhQl87', 'However none of the pairs are statistically significant and thus no detections. Nevertheless this demonstrates the exciting opportunity of multimessenger astronomy for gravitational wave and electromagnetic counterparts! (3/3)']",https://arxiv.org/abs/2203.17222,"Advanced LIGO and Virgo have reported ninety confident gravitational-wave (GW) observations from compact-binary coalescences from their three observation runs. In addition, numerous subthreshold gravitational-wave candidates have been identified. Binary neutron star (BNS) mergers can produce gravitational waves and short-gamma ray bursts, as confirmed by GW170817/GRB 170817A. There may be electromagnetic counterparts recorded in archival observations associated with subthreshold gravitational-wave candidates. The CHIME/FRB collaboration has reported the first large sample of fast radio bursts (FRBs), millisecond radio transients detected up to cosmological distances; a fraction of these may be associated with BNS mergers. This work searches for coincident gravitational waves and FRBs from BNS mergers using candidates from the 4th-Open Gravitational-wave Catalog (4-OGC) and the first CHIME/FRB catalog. We use a ranking statistic for GW/FRB association which combines the gravitational-wave detection statistic with the odds of temporal and spatial association. We analyze gravitational-wave candidates and non-repeating FRBs from 2019 April 1 to 2019 July 1, when both the Advanced LIGO/Virgo gravitational-wave detectors and the CHIME radio telescope were observing. The most significant coincident candidate has a false alarm rate of 0.29 per observation time, which is consistent with a null observation. The null results imply at most $\mathcal{O}(0.01)\%$ - $\mathcal{O}(1)\%$ of FRBs are produced from the BNS mergers. ","Search for Coincident Gravitational Wave and Fast Radio Burst Events
  from 4-OGC and the First CHIME/FRB Catalog"
15,1509805072242253833,2860132610,Ryo Fukuda,"['Our new paper, ""Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation"" is posted on arXiv.\n<LINK> <LINK>']",https://arxiv.org/abs/2203.15479,"Speech segmentation, which splits long speech into short segments, is essential for speech translation (ST). Popular VAD tools like WebRTC VAD have generally relied on pause-based segmentation. Unfortunately, pauses in speech do not necessarily match sentence boundaries, and sentences can be connected by a very short pause that is difficult to detect by VAD. In this study, we propose a speech segmentation method using a binary classification model trained using a segmented bilingual speech corpus. We also propose a hybrid method that combines VAD and the above speech segmentation method. Experimental results revealed that the proposed method is more suitable for cascade and end-to-end ST systems than conventional segmentation methods. The hybrid approach further improved the translation performance. ","Speech Segmentation Optimization using Segmented Bilingual Speech Corpus
  for End-to-end Speech Translation"
16,1509704651662565380,1352638067761311744,Joseph Imperial,"['New preprint. We develop a baseline ML-model for readability assessment in the Cebuano language using traditional features, syllable patterns, and neural embeddings.\n\nüìúPaper: <LINK>\nüíæCode and Data: <LINK>']",https://arxiv.org/abs/2203.17225,"In this study, we developed the first baseline readability model for the Cebuano language. Cebuano is the second most-used native language in the Philippines with about 27.5 million speakers. As the baseline, we extracted traditional or surface-based features, syllable patterns based from Cebuano's documented orthography, and neural embeddings from the multilingual BERT model. Results show that the use of the first two handcrafted linguistic features obtained the best performance trained on an optimized Random Forest model with approximately 87% across all metrics. The feature sets and algorithm used also is similar to previous results in readability assessment for the Filipino language showing potential of crosslingual application. To encourage more work for readability assessment in Philippine languages such as Cebuano, we open-sourced both code and data. ",A Baseline Readability Model for Cebuano
17,1509697364591742987,581871304,Takami Sato,"['Our #CVPR2022 paper is on arXiv! Through a large-scale study with our own dataset, we show the limitations of existing metrics for LD in autonomous driving and propose new driving-oriented metrics.\n\nüåê <LINK>\nüìù <LINK>\nüìÑ <LINK>', 'Our dataset is published in @kaggle dataset. \nhttps://t.co/8pMd3mZHkY']",https://arxiv.org/abs/2203.16851,"After the 2017 TuSimple Lane Detection Challenge, its dataset and evaluation based on accuracy and F1 score have become the de facto standard to measure the performance of lane detection methods. While they have played a major role in improving the performance of lane detection methods, the validity of this evaluation method in downstream tasks has not been adequately researched. In this study, we design 2 new driving-oriented metrics for lane detection: End-to-End Lateral Deviation metric (E2E-LD) is directly formulated based on the requirements of autonomous driving, a core downstream task of lane detection; Per-frame Simulated Lateral Deviation metric (PSLD) is a lightweight surrogate metric of E2E-LD. To evaluate the validity of the metrics, we conduct a large-scale empirical study with 4 major types of lane detection approaches on the TuSimple dataset and our newly constructed dataset Comma2k19-LD. Our results show that the conventional metrics have strongly negative correlations ($\leq$-0.55) with E2E-LD, meaning that some recent improvements purely targeting the conventional metrics may not have led to meaningful improvements in autonomous driving, but rather may actually have made it worse by overfitting to the conventional metrics. As autonomous driving is a security/safety-critical system, the underestimation of robustness hinders the sound development of practical lane detection models. We hope that our study will help the community achieve more downstream task-aware evaluations for lane detection. ",Towards Driving-Oriented Metric for Lane Detection Models
18,1509694077285146630,863828060600139776,Dr. Deep Anand,"[""Presenting a paper that's been in the works for about 3 years. \n\nhere, we provide some important insights into the #HubbleTension in #Cosmology, using a brand new local Universe measurement technique.\n\n<LINK> <LINK>"", 'so very happy to finally get to publish a paper with my friends @AstroClayt and @onetweettwomany.', 'also so very happy to be able to say that we think the tension is (close to) dead. game over folks. find a new gig.']",https://arxiv.org/abs/2203.16551,"Using sedimentary and eclipse-based measurements of the lunar recession velocity, we derive a new local-Universe measurement of the Hubble constant ($H_0$) from the recession rate of Earth's Moon. Taking into account the effects of tides, we find a value of $H_{0}$ = 63.01 $\pm$ 1.79 km s$^{-1}$ Mpc$^{-1}$, which is in approximate agreement with the Planck space mission's measurement using the cosmic microwave background (CMB) and base $\Lambda$CDM. Our new measurement represents the first ever model-independent, single-step measurement of the Universe's current expansion rate. This is also the first major local Universe measurement of $H_0$ which is below the measurement from Planck. Importantly, it is robust to the systematic errors that may be present in other $H_0$ measurements using other cosmological probes such as type Ia supernovae, baryon acoustic oscillations, or lensed quasars. Our work provides key evidence towards the notion that the existing Hubble tension may indeed be a result of systematic uncertainties in the local distance ladder. ","Worry No More, The Hubble Tension is Relieved: A Truly Direct
  Measurement of the Hubble Constant from Mooniversal Expansion"
19,1509690363539501057,141440459,Rod Van Meter üåª,"['New paper dance: handling cosmic ray events in superconducting quantum systems, by using inter-node error correction. Thanks to first author Qian Xu and professor Liang Jiang for bringing us along on this paper!\n<LINK>', ""@ChrisMihos If I knew how to type the heart-eyed emoji I would, but I'm old and unhip so I won't.""]",https://arxiv.org/abs/2203.16488,"Quantum error correction holds the key to scaling up quantum computers. Cosmic ray events severely impact the operation of a quantum computer by causing chip-level catastrophic errors, essentially erasing the information encoded in a chip. Here, we present a distributed error correction scheme to combat the devastating effect of such events by introducing an additional layer of quantum erasure error correcting code across separate chips. We show that our scheme is fault tolerant against chip-level catastrophic errors and discuss its experimental implementation using superconducting qubits with microwave links. Our analysis shows that in state-of-the-art experiments, it is possible to suppress the rate of these errors from 1 per 10 seconds to less than 1 per month. ",Distributed quantum error correction for chip-level catastrophic errors
20,1509597434703319062,1061697043389911040,Adam Wiemerslage,"['If you are interested in computational morphology and/or the needs of low-resource languages but unsure where to get started, check out our new Findings of ACL paper where we survey the state of low-resource computational morphology! <LINK>', 'We additionally present preliminary experiments on a challenging truly unsupervised paradigm completion task, incorporating some existing systems from the literature, and experimenting with a new corpus of children‚Äôs books.', '8 pages is not a lot of space to survey such a large topic ‚Äì if you are interested in discussing an extended version in a longer format please reach out!', 'This is joint work with @mpsilfve Changbing Yang @aryamccarthy @GarrettNicolai Eliana Colunga @kelina1124']",https://arxiv.org/abs/2203.08909,"Automatic morphological processing can aid downstream natural language processing applications, especially for low-resource languages, and assist language documentation efforts for endangered languages. Having long been multilingual, the field of computational morphology is increasingly moving towards approaches suitable for languages with minimal or no annotated resources. First, we survey recent developments in computational morphology with a focus on low-resource languages. Second, we argue that the field is ready to tackle the logical next challenge: understanding a language's morphology from raw text alone. We perform an empirical study on a truly unsupervised version of the paradigm completion task and show that, while existing state-of-the-art models bridged by two newly proposed models we devise perform reasonably, there is still much room for improvement. The stakes are high: solving this task will increase the language coverage of morphological resources by a number of magnitudes. ","Morphological Processing of Low-Resource Languages: Where We Are and
  What's Next"
21,1509590666979930116,774170436057731073,Alexis Conneau,"['üö®[üó£Ô∏èüîäüíªüí¨]üö® Excited to share our new benchmark ""XTREME-S"" to accelerate speech technologies for all\n\nXTREME-S evaluates recognition, translation, classification and retrieval in 100+ languages.\n\nPaper: <LINK>\nCode: <LINK>', 'We have worked hard to try &amp; make things simple for practitioners (easy download of public data, single fine-tuning for multilingual datasets)\n\nWe hope this will catalyze research in data-efficient approaches like self-supervised speech representation learning for all languages https://t.co/pBYvpjHQhU', 'This is the result of work done by many amazing folks across @GoogleAI, @huggingface, and @MetaAI. \n\nPlease consider using XTREME-S so that as a community, we can accelerate progress on speech technology for the benefit of all.\n\nhttps://t.co/Pq7IIyO6HG']",https://arxiv.org/abs/2203.10752,"We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in ""universal"" speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible at this https URL ",XTREME-S: Evaluating Cross-lingual Speech Representations
22,1509563292737585156,47680950,Mehdi Fatemi,"['We published our new paper on ‚ÄúOrchestrated Value Mapping for Reinforcement Learning‚Äù \\w @arshtvk. (1/7)\n\n<LINK>\n\n#ICLR2022 @MSFTResearch @MPI_IS @iclr_conf', 'The core idea is to map the value function to a different space and perform the training in the mapped space; thus benefit from the properties of the mapped space. (2/7) https://t.co/5XKWPLr8Rn', 'Examples of mappings include ‚Äúlog‚Äù that magnifies the return when it is near zero or ‚Äúexp‚Äù that do the same when the return is large. Such functions can help with mitigating the action-gap issue in sparse or dense reward scenarios, enabling to use a smaller discount factor (3/7) https://t.co/6n06tJlsIn', 'More generally we can have an ensemble of such mappings in a broad manner, leaving us with a blue-print for making arbitrary *convergent* agents that suit various problems of interest. The orchestration is made possible via *reward decomposition*. (4/7) https://t.co/lcMyO4wdqH', 'The coolest part is to combine various mappings either piecewise or as an ensemble to benefit from their best properties when the return falls in their particularly beneficial range. (5/7) https://t.co/8j4UNpKQaI', 'Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the Atari suite. (6/7) https://t.co/Ca89DYfzTD', 'Several prior algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition can be derived by appropriate construction from our algorithm. We are very excited about this work and would be happy to chat more at the conference. (7/7)']",https://arxiv.org/abs/2203.07171,"We present a general convergent class of reinforcement learning algorithms that is founded on two distinct principles: (1) mapping value estimates to a different space using arbitrary functions from a broad class, and (2) linearly decomposing the reward signal into multiple channels. The first principle enables incorporating specific properties into the value estimator that can enhance learning. The second principle, on the other hand, allows for the value function to be represented as a composition of multiple utility functions. This can be leveraged for various purposes, e.g. dealing with highly varying reward scales, incorporating a priori knowledge about the sources of reward, and ensemble learning. Combining the two principles yields a general blueprint for instantiating convergent algorithms by orchestrating diverse mapping functions over multiple reward channels. This blueprint generalizes and subsumes algorithms such as Q-Learning, Log Q-Learning, and Q-Decomposition. In addition, our convergence proof for this general class relaxes certain required assumptions in some of these algorithms. Based on our theory, we discuss several interesting configurations as special cases. Finally, to illustrate the potential of the design space that our theory opens up, we instantiate a particular algorithm and evaluate its performance on the Atari suite. ",Orchestrated Value Mapping for Reinforcement Learning
23,1509512569454837766,10099292,Alex Bowyer üá∫üá¶,"['Very pleased to hear that our #CHI2022 paper ""Human-GDPR Interaction: Practical Experiences of Accessing Personal Data"" has been awarded an Honourable Mention :-) \n\nRead it at <LINK> \n\nI will present the research in New Orleans on 4th May - here\'s a taster! <LINK>']",https://arxiv.org/abs/2203.05037,"In our data-centric world, most services rely on collecting and using personal data. The EU's General Data Protection Regulation (GDPR) aims to enhance individuals' control over their data, but its practical impact is not well understood. We present a 10-participant study, where each participant filed 4-5 data access requests. Through interviews accompanying these requests and discussions scrutinising returned data, it appears that GDPR falls short of its goals due to non-compliance and low-quality responses. Participants found their hopes to understand providers' data practices or harness their own data unmet. This causes increased distrust without any subjective improvement in power, although more transparent providers do earn greater trust. We propose designing more effective, data-inclusive and open policies and data access systems to improve both customer relations and individual agency, and also that wider public use of GDPR rights could help with delivering accountability and motivating providers to improve data practices. ",Human-GDPR Interaction: Practical Experiences of Accessing Personal Data
24,1509429258154029062,1144194850746540032,Jung-Woo Ha,"['The paper and official pytorch code of our #PuriDiver (#CVPR2022), a new class incremental learning setup with noisy labels, was released! Please check it out.\n\narxiv: <LINK>\ngithub: <LINK> \n\n@ClovaAiLab @ppolon']",https://arxiv.org/abs/2203.15355,"Learning under a continuously changing data distribution with incorrect labels is a desirable real-world problem yet challenging. A large body of continual learning (CL) methods, however, assumes data streams with clean labels, and online learning scenarios under noisy data streams are yet underexplored. We consider a more practical CL task setup of an online learning from blurry data stream with corrupted labels, where existing CL methods struggle. To address the task, we first argue the importance of both diversity and purity of examples in the episodic memory of continual learning models. To balance diversity and purity in the episodic memory, we propose a novel strategy to manage and use the memory by a unified approach of label noise aware diverse sampling and robust learning with semi-supervised learning. Our empirical validations on four real-world or synthetic noise datasets (CIFAR10 and 100, mini-WebVision, and Food-101N) exhibit that our method significantly outperforms prior arts in this realistic and challenging continual learning scenario. Code and data splits are available in this https URL ","Online Continual Learning on a Contaminated Data Stream with Blurry Task
  Boundaries"
25,1509428800781701122,3236251346,Mikel Sanz,"['New paper today ‚ÄúQuantum-Enhanced Doppler Radar/Lidar‚Äù (<LINK>) with M. Reichert, @di_candi2 and M. Win proposing a Heisenberg-scaling protocol resilient against losses to estimate the velocity of a target, overcoming low-signal drawback of quantum illumination. <LINK>', 'The use of a frequency-entangled squeezed state composed of a signal and an idler beam as a probe state is key. The photon-number (local) entanglement helps with the losses, while the global frequency-entanglement allows the Heisenberg scaling in one of the parameter regimes. https://t.co/G8C7ZDZSkt', '@NquireC #QMiCS project of the @QuantumFlagship @QUANTEK2122 @ehuscientia @Ikerbasque @BCAMBilbao @ryc_upvehu @ztf_fct']",https://arxiv.org/abs/2203.16424,"We propose a quantum-enhanced protocol to estimate the radial velocity $v$ of a moving target, using a frequency-entangled squeezed state composed of a signal and an idler beam as a probe state. The signal beam illuminates the moving object and it is reflected with its frequency shifted due to the Doppler effect. Then, a joint measurement between signal and idler beams is performed to estimate the velocity of the object. We aim at benchmarking this protocol against the classical one, which comprises a coherent state with the same energy illuminating the object. Indeed, employing squeezing and frequency entanglement as quantum resources provides to a precision enhancement in the estimation of the velocity of the object. We identify three distinct parameter regimes. First, the frequency entanglement-dominant regime, where the advantage is proportional to the degree of frequency entanglement and mostly insensitive to the photon number. Second, the squeezing-dominant regime, with a quantum advantage that is higher than the standard quantum limit. Third, the mixed regime, where both squeezing and frequency entanglement are comparable and the proposed quantum protocol attains the Heisenberg limit. We show that an optimal measurement to achieve these results is frequency-resolved photon-number counting. Losses in the signal beam are considered for the high frequency entanglement regime. The protocol shows resilience, outperforming the classical protocol for all channel transmissivities given a large enough frequency entanglement. ",Quantum-Enhanced Doppler Radar/Lidar
26,1509412029756547073,592862195,Jarvist Moore Frost,"[""<LINK>\nNew paper! @Neutrino155 's main project so far on extending the Feynman variational polaron method to multiple phonon modes.\n\nCodes are open source, but on a development branch for the moment.\n\nWe can now simulate freq dependent mobility / opt absorption!""]",http://arxiv.org/abs/2203.16472,"The Feynman path-integral variational solution to the polaron problem \cite{Feynman1955}, along with the associated FHIP linear-response mobility theory \cite{Feynman1962}, provides a computationally amenable method to predict the frequency-resolved temperature-dependent charge-carrier mobility, and other experimental observables in polar semi-conductors. We show that the FHIP mobility theory is capable of demonstrating non-Drude transport behaviour, and provides good agreement with the recent diagrammatic Monte-Carlo mobility simulations of Mishchenko et al. \cite{Mishchenko2019} for the abstract Fr\""ohlich Hamiltonian. We extend this method to multiple variational parameters in the model action, and to multiple phonon modes in the true action. This enables a slightly better variational solution, as inferred from the resulting energy. We carry forward this complexity into the mobility theory, where it enables richer structure in the frequency and temperature dependent mobility, due to the different phonon modes activating at different energies. ","Multiple phonon modes in Feynman path-integral variational polaron
  mobility"
27,1509357148622438403,303553181,"Ben Horne, Ph.D.","['New paper accepted @WebSciConf lead by @UTKSIS Masters student @MattChilds01, along with myself, @codybuntain, and @illegaldaydream, on YouTube and BitChute suppliers and mobilizers during U.S. Election Fraud discussions on Twitter. \n\nHere is the preprint: <LINK> <LINK>', 'There are two key takeaways from this study: 1. Despite the growing concern about BitChute‚Äôs\nrole online, YouTube is still more prevalent in the spread of disinformation (at least in the discussion of 2020 U.S. election fraud claims). https://t.co/Hr6Q55o0SV', 'Specifically, during the election fraud discussions on Twitter, YouTube videos promoting election fraud claims were linked to approximately 28 times more than BitChute videos promoting election fraud claims, and those tweets were more engaged with (on avg. 14.10 rts vs. 7.56 rts)', '2. Mobilizers of video content on Twitter, no matter the platform the video content was from, were rarely political elites or bot accounts, but instead appeared to be average Twitter users. (Importantly, this is approximated through verified accounts). https://t.co/6mpDgtUSmu', 'As others have suggested, this result suggests that research should move beyond focusing on only bots and elites to consider the role of online crowds and more complex social structures in the spread\nand production of disinformation.', 'As elegantly put by @codybuntain in our paper: Ultimately, these results suggest the roles of elites ‚Äì both political elites and elite platforms, like YouTube ‚Äì remain as core sources of mis- and disinformation in online spaces.', 'That said, less mainstream and influential entities, such as alt-tech platforms and common information consumers, remain a key participatory element in the\nspread of these disinformation campaigns.', 'Efforts to counter such campaigns must improve, both in mainstream platforms‚Äô willingness to collaborate and in how they respond to and moderate the\nfringe elements (both alt-tech and extreme consumers) in this space.', 'Of course, these results should be taken with the limitations of the study, including that the study is focused on only one event/case. However, using this single case allows us to draw comparisons between the roles of both platforms that we otherwise could not.']",https://arxiv.org/abs/2203.16274,"In this study, we characterize the cross-platform mobilization of YouTube and BitChute videos on Twitter during the 2020 U.S. Election fraud discussions. Specifically, we extend the VoterFraud2020 dataset to describe the prevalence of content supplied by both platforms, the mobilizers of that content, the suppliers of that content, and the content itself. We find that while BitChute videos promoting election fraud claims were linked to and engaged with in the Twitter discussion, they played a relatively small role compared to YouTube videos promoting fraud claims. This core finding points to the continued need for proactive, consistent, and collaborative content moderation solutions rather than the reactive and inconsistent solutions currently being used. Additionally, we find that cross-platform disinformation spread from video platforms was not prominently from bot accounts or political elites, but rather average Twitter users. This finding supports past work arguing that research on disinformation should move beyond a focus on bots and trolls to a focus on participatory disinformation spread. ","Characterizing YouTube and BitChute Content and Mobilizers During U.S.
  Election Fraud Discussions on Twitter"
28,1509336844785238019,288275969,Marlos C. Machado,"['1/7: We have a new, exciting paper on arXiv:\n\nInvestigating the Properties of Neural Network Representations in Reinforcement Learning\n\nLed by @hwang_cs_rl &amp; @erfan_mhi\nw/ @white_martha, @zaheersm_1, R. Kumaraswamy, @vl_dimension &amp; A. White.\n\n<LINK>\n\nüßµüëáüèª', '2/7: In a transfer setting, we tried to understand the properties of representations learned by DQN. We generated more than 25k agent-task settings by considering nine auxiliary tasks, different activation functions, and network capacities, across 173 transfer tasks. https://t.co/TL9bzjL9sH', '3/7: An interesting result is that fuzzy tiling activation (FTA) seems to be a darn good activation function for deep RL algorithms. It leads to representations that can be reused much more effectively than those learned with ReLUs. https://t.co/Vdo9c4fbsb', '4/7: A really cool result is that, if you want to see all layers up to the last layer of the NN as the representation, such that one is effectively doing linear function approx. at the end, you cannot really do transfer that well. You need non-linear function approximation! https://t.co/aowdzOPkMG', '5/7: Still on the plot above, notice that we show in a quantitative way that auxiliary tasks do lead to better representations, at least for transfer. Also, they drastically vary in their effectiveness, interacting with the activation function and even w/ the capacity of the NN. https://t.co/0VsD5T38Ll', '6/7: One of the coolest things is that we go beyond justifying representations only by their performance; we correlate them with properties of such representations. Representations that transferred best had high values for capacity metrics, low orthogonality &amp; medium sparsity. https://t.co/e64aTeXiIj', ""7/7: We were quite careful on all of our analyses, tracking how the properties of the representations change, and so on. Obviously, check the paper if you are interested, there's a mountain of data there! https://t.co/3tHJNF4oAB"", '@pcastr We evaluated the shared part, so yes, there‚Äôre some parts of the network that are not used for control; but, the auxiliary tasks are definitely backpropagating to a shared layer and one of the things we show is exactly how much that matters. I might have missed your point though.', ""@pcastr I can't believe you couldn't figure out exactly what I did by reading a Twitter thread! Of course, we would never do anything slightly wrong, haha.\n\nI'm looking forward to your feedback if you get a chance to read the paper."", '@TalkRLPodcast We controlled for that. That‚Äôs why we have ReLU(L) in several plots, to make sure they had the same capacity.']",https://arxiv.org/abs/2203.15955,"In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the earlier work in representation learning for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation -- good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. This analysis allows us to provide novel hypotheses regarding impact of auxiliary tasks in end-to-end training of non-linear reinforcement learning methods. We introduce and measure six representational properties over more than 25 thousand agent-task settings. We consider DQN agents with convolutional networks in a pixel-based navigation environment. We develop a method to better understand \emph{why} some representations work better for transfer, through a systematic approach varying task similarity and measuring and correlating representation properties with transfer performance. ","Investigating the Properties of Neural Network Representations in
  Reinforcement Learning"
29,1509283563451006978,910456088,Tyler Richey-Yowell,"['Our new paper is finally up on arXiv! If you (like me) are interested in K stars as host stars, then definitely check this out. More evidence that K stars are unique compared other types of stars, not only in rotational evolution, but also in the UV! \n\n<LINK>']",https://arxiv.org/abs/2203.15237,"Efforts to discover and characterize habitable zone planets have primarily focused on Sun-like stars and M dwarfs. K stars, however, provide an appealing compromise between these two alternatives that has been relatively unexplored. Understanding the ultraviolet (UV) environment around such stars is critical to our understanding of their planets, as the UV can drastically alter the photochemistry of a planet's atmosphere. Here we present near-UV and far-UV \textit{Hubble Space Telescope}'s Cosmic Origins Spectrograph observations of 39 K stars at three distinct ages: 40 Myr, 650 Myr, and $\approx$5 Gyr. We find that the K star (0.6 -- 0.8 M$_{\odot}$) UV flux remains constant beyond 650 Myr before falling off by an order of magnitude by field age. This is distinct from early M stars (0.3 -- 0.6 M$_{\odot}$), which begin to decline after only a few hundred Myr. However, the rotation-UV activity relation for K stars is nearly identical to that of early M stars. These results may be a consequence of the spin-down stalling effect recently reported for K dwarfs, in which the spin-down of K stars halts for over a Gyr when their rotation periods reach $\approx$10 d, rather than the continuous spin down that G stars experience. These results imply that exoplanets orbiting K dwarfs may experience a stronger UV environment than thought, weakening the case for K stars as hosts of potential ""super-habitable"" planets. ","HAZMAT. VIII. A Spectroscopic Analysis of the Ultraviolet Evolution of K
  Stars: Additional Evidence for K Dwarf Rotational Stalling in the First
  Gigayear"
30,1509235465538478080,1465098219847831562,Pablo Villanueva Domingo,"['Glad to announce our new paper with @vmmunoza, @carambolos and Sergio Palomares-Ruiz, on constraining the abundance of primordial black holes using neutrino fluxes at Super Kamiokande and future neutrino detectors.\n\n<LINK>', 'If primordial black holes exist, they would emit neutrinos via Hawking evaporation, which could be detected at Earth. Here we use current SK data to bound their abundance, and study how these limits could be improved in future neutrino telescopes such as Hyper Kamiokande.']",https://arxiv.org/abs/2203.14979,"Primordial black holes (PBHs) formed in the early Universe are sources of neutrinos emitted via Hawking radiation. Such astrophysical neutrinos could be detected at Earth and constraints on the abundance of comet-mass PBHs could be derived from the null observation of this neutrino flux. Here, we consider non-rotating PBHs and improve constraints using Super-Kamiokande neutrino data, as well as we perform forecasts for next-generation neutrino (Hyper-Kamiokande, JUNO, DUNE) and dark matter (DARWIN, ARGO) detectors, which we compare. For PBHs less massive than $\sim \textrm{few} \times 10^{14}$ g, PBHs would have already evaporated by now, whereas more massive PBHs would still be present and would constitute a fraction of the dark matter of the Universe. We consider monochromatic and extended (log-normal) mass distributions, and a PBH mass range spanning from $10^{12}$ g to $\sim 10^{16}$ g. Finally, we also compare our results with previous ones in the literature. ","Current and future neutrino limits on the abundance of primordial black
  holes"
31,1509212974698872851,349243149,Maxime Trebitsch,"[""Today on the #arXiv, another paper I'm super happy to be a part of! New results from the Low-z Lyman Continuum Survey led by the incredible Sophia Flury, exploring various properties of LyC-emitting galaxies and testing diagnostics of LyC escape: <LINK>. [1/7] <LINK>"", 'The LzLCS is a sample of 66 galaxies observed with HST where we detect LyC emission in 35 of them. Earlier threads: the survey paper led by Sophia Flury (https://t.co/nGNCuJUFac) and another paper by Alberto Saldana-Lopez looking at ISM properties (https://t.co/LLHgkj2RWX). [2/7]', 'In this new paper, Sophia reviews a series of indirect indicators of LyC emission, some of which correlate fairly well with LyC escape, but even these exhibit significant scatter when looking at the escape fraction. [3/7] https://t.co/spEIlLiElS', 'Not surprisingly, Lyman-alpha is a very good indicator of LyC escape. Notably, the ""peak to peak separation"" seems to work well: about 70% of the objects where the Lya peaks are separated by less than 300 km/s are LyC emitters. [4/7] https://t.co/A23HMrO9jW', 'The (now) usual O32 indicator seems to show some promising trends, with a very large fraction of the objects with O32 &gt; 10 being LyC emitters, but the relation between O32 and the LyC escape fraction has *a lot* of scatter, suggesting an indirect correlation. [5/7] https://t.co/TxHQSJSxxz', 'The paper goes in much more details, and looks a correlation between multiple diagnostics, and my take is ""you really should read it"". Hidden somewhere in this scatter is a lot of ISM and galaxy physics that we will certainly keep investigating, so stay tuned! [6/7] https://t.co/HDsFffOuVX', ""Overall, in general LCEs tend to have compact star-forming regions, suggesting that stellar feedback facilitates LyC escape, in broad agreement with simulations. However, there seems to be a diversity of mechanisms enabling LyC escape, so the story doesn't is far from over! [7/7]""]",https://arxiv.org/abs/2203.15649,"The Lyman continuum (LyC) cannot be observed at the epoch of reionization (z {\gtrsim} 6) due to intergalactic H I absorption. To identify Lyman continuum emitters (LCEs) and infer the fraction of escaping LyC, astronomers have developed various indirect diagnostics of LyC escape. Using measurements of the LyC from the Low-redshift Lyman Continuum Survey (LzLCS), we present the first statistical test of these diagnostics. While optical depth indicators based on Ly{\alpha}, such as peak velocity separation and equivalent width, perform well, we also find that other diagnostics, such as the [O III]/[O II] flux ratio and star formation rate surface density, predict whether a galaxy is a LCE. The relationship between these galaxy properties and the fraction of escaping LyC flux suggests that LyC escape depends strongly on H I column density, ionization parameter, and stellar feedback. We find LCEs occupy a range of stellar masses, metallicities, star formation histories, and ionization parameters, which may indicate episodic and/or different physical causes of LyC escape. ","The Low-Redshift Lyman Continuum Survey II: New Insights into LyC
  Diagnostics"
32,1509084870999486464,3236251346,Mikel Sanz,"['New paper ‚ÄúQuantum Genetic Algorithm with Individuals in Multiple Registers‚Äù <LINK> with @raist272 and @Gatgian we propose a subroutine-based fully quantum genetic algorithm distributable among quantum processors @OpenSuperQ @QUANTEK2122 @Ikerbasque @BCAMBilbao <LINK>', 'The use multiple registers allows us to introduce all the natural elements characterizing genetic algorithms: population-based search with selection of many individuals, crossover and mutation. Surprisingly, the mutation subroutine, has small impact on the average performance 2/3 https://t.co/l02rWINXWi', 'Finally, and I like this a lot üôÉ, we introduce a quantum channel analysis to prove the exponential convergence of our algorithm and even predict its convergence-ratio. @NquireC @ryc_upvehu @upvehu https://t.co/11SXwuDgIt']",http://arxiv.org/abs/2203.15039,"Genetic algorithms are heuristic optimization techniques inspired by Darwinian evolution, which are characterized by successfully finding robust solutions for optimization problems. Here, we propose a subroutine-based quantum genetic algorithm with individuals codified in independent registers. This distinctive codification allows our proposal to depict all the fundamental elements characterizing genetic algorithms, i.e. population-based search with selection of many individuals, crossover, and mutation. Our subroutine-based construction permits us to consider several variants of the algorithm. For instance, we firstly analyze the performance of two different quantum cloning machines, a key component of the crossover subroutine. Indeed, we study two paradigmatic examples, namely, the biomimetic cloning of quantum observables and the Bu\v zek-Hillery universal quantum cloning machine, observing a faster average convergence of the former, but better final populations of the latter. Additionally, we analyzed the effect of introducing a mutation subroutine, concluding a minor impact on the average performance. Furthermore, we introduce a quantum channel analysis to prove the exponential convergence of our algorithm and even predict its convergence-ratio. This tool could be extended to formally prove results on the convergence of general non-unitary iteration-based algorithms. ",Quantum Genetic Algorithm with Individuals in Multiple Registers
33,1509078328451469312,1419202757316186112,Harald Schmid,"['New paper out today with @loewe1212 , Katharina Franke and Felix von Oppen\n\n""Quantum Yu-Shiba-Rusinov dimers"", <LINK> <LINK>']",https://arxiv.org/abs/2203.15011,"Magnetic adatoms on a superconducting substrate undergo a quantum phase transition as their exchange coupling to the conduction electrons increases. For quantum spins, this transition is accompanied by screening of the adatom spin. Here, we explore the consequences of this screening for the phase diagrams and subgap excitation spectra of dimers of magnetic adatoms coupled by hybridization of their Yu-Shiba-Rusinov states and spin-spin interactions. We specifically account for higher spins, single-ion anisotropy, Ruderman-Kittel-Kasuya-Yosida coupling, and Dzyaloshinsky-Moriya interactions relevant in transition-metal and rare-earth systems. Our flexible approach based on a zero-bandwidth approximation provides detailed physical insight and is in excellent qualitative agreement with available numerical-renormalization group calculations on monomers and dimers. Remarkably, we find that even in the limit of large impurity spins or strong single-ion anisotropy, the phase diagrams for dimers of quantum spins remain qualitatively distinct from phase diagrams based on classical spins, highlighting the need for a theory of quantum Yu-Shiba-Rusinov dimers. ",Quantum Yu-Shiba-Rusinov dimers
34,1509069578269302785,248142831,Yuhuang Hu,['New paper accepted at #ICPR2022. Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks (<LINK>).\n\nWe propose a parameter-efficient kernel modulation (KM) method that adapts the parameters of a base network.'],https://arxiv.org/abs/2203.15297,"Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets), have achieved incredible success in many vision tasks, but they usually require millions of parameters for good accuracy performance. With increasing applications that use ConvNets, updating hundreds of networks for multiple tasks on an embedded device can be costly in terms of memory, bandwidth, and energy. Approaches to reduce this cost include model compression and parameter-efficient models that adapt a subset of network layers for each new task. This work proposes a novel parameter-efficient kernel modulation (KM) method that adapts all parameters of a base network instead of a subset of layers. KM uses lightweight task-specialized kernel modulators that require only an additional 1.4% of the base network parameters. With multiple tasks, only the task-specialized KM weights are communicated and stored on the end-user device. We applied this method in training ConvNets for Transfer Learning and Meta-Learning scenarios. Our results show that KM delivers up to 9% higher accuracy than other parameter-efficient methods on the Transfer Learning benchmark. ","Kernel Modulation: A Parameter-Efficient Method for Training
  Convolutional Neural Networks"
35,1509049733276385280,1232964338,Shaikh saad,['another new paper: lepton g-2 and its connection to dark matter and neutrino mass \n<LINK>'],https://arxiv.org/abs/2203.14983,"The origin of neutrino mass is a mystery, so is its nature, namely, whether neutrinos are Dirac or Majorana particles. On top of that, hints of large deviations of the muon and the electron anomalous magnetic moments (AMMs) are strong evidence for physics beyond the Standard Model. In this work, piecing these puzzles together, we propose a class of radiative Dirac neutrino mass models to reconcile $(g-2)_{\mu,e}$ anomalies with neutrino oscillation data. In this framework, a common set of new physics (NP) states run through the loops that generate non-zero neutrino mass and, due to chiral enhancement, provide substantial NP contributions to lepton AMMs. In addition, one of the three models studied in this work offers a Dark Matter candidate automatically stabilized by the residual symmetry, whose phenomenology is non-trivially connected to the other two puzzles mentioned above. Finally, our detailed numerical analysis reveals a successful resolution to these mysteries while being consistent with all colliders and cosmological constraints. ","Dark Matter and $(g-2)_{\mu,e}$ in radiative Dirac neutrino mass models"
36,1509049236339535883,1232964338,Shaikh saad,['our new paper on the kind of marriage I believe in üòÖ\n<LINK>'],https://arxiv.org/abs/2203.15499,"Experimental hints for lepton flavor universality violation in beauty-quark decay both in neutral- and charged-current transitions require an extension of the Standard Model for which scalar leptoquarks (LQs) are the prime candidates. Besides, these same LQs can resolve the long-standing tension in the muon and the recently reported deviation in the electron $g-2$ anomalies. These tantalizing flavor anomalies have discrepancies in the range of $2.5\sigma-4.2\sigma$, indicating that the Standard Model of particle physics may finally be cracking. In this Letter, we propose a resolution to all these anomalies within a unified framework that sheds light on the origin of neutrino mass. In this model, the LQs that address flavor anomalies run through the loops and generate neutrino mass at the two-loop order while satisfying all constraints from collider searches, including those from flavor physics. ",Marriage between neutrino mass and flavor anomalies
37,1509031228984287232,2835683058,Silvio Savarese,['Very excited to share this new research @SFResearch on using language to generate executable code. Learn more about this breakthrough in conversational AI programming! \nPaper: <LINK>\nBlog: <LINK>\nCode: <LINK>'],https://arxiv.org/abs/2203.13474,"Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: this https URL ",A Conversational Paradigm for Program Synthesis
38,1508936053478076416,48008938,Yann LeCun,"['New paper: ""projUNN: efficient method for training deep networks with unitary matrices""\nby Bobak Kiani (MIT), Randall Balestriero (FAIR), Yann LeCun (FAIR/NYU), Seth Lloyd (MIT).\n\n<LINK>', 'TL;DR: neural nets with unitary matrices are interesting beasts: invertible, no vanishing/exploding gradient, computation akin to quantum computing.  \nTraining them is hard.\nWe propose a low-rank (low-cost) update method to update unitary weight matrices with gradient descent.']",https://arxiv.org/abs/2203.05483,"In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank-$k$ updates -- or their rank-$k$ approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full $N$-dimensional unitary or orthogonal matrices with a training runtime scaling as $O(kN^2)$. Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting ($k=1$), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. By integrating our projUNN algorithm into both recurrent and convolutional neural networks, our models can closely match or exceed benchmarked results from state-of-the-art algorithms. ","projUNN: efficient method for training deep networks with unitary
  matrices"
39,1508890083792986126,377049708,Eddie Lee,"['New paper with co-authors @ChrisKempes and Geoffrey West on providing a birds-eye-view formalization of the dynamics of #innovation and #obsolescence.  Now on the arXiv. @CSHVienna @sfiscience \n\n<LINK>', 'Understanding the dynamics and structure of innovation and obsolescence has been a subject of considerable interest across many domains.', 'Innovation and obsolescence describes dynamics of ever-churning and adapting social and biological systems from the development of economic markets and scientific progress to biological evolution.', 'The shared aspect of this picture is that agents destroy and extend the ‚Äúidea lattice‚Äù in which they live, finding new possibilities and rendering old solutions irrelevant.', 'We focus on this aspect with a simple model to study the central relationship between the rates at which replicating agents discover new ideas and at which old ideas are rendered obsolete.', 'When these rates are equal, the space of the possible (e.g. markets, technologies, mutations) remains finite. A positive or negative difference distinguishes flourishing, ever-expanding spaces from Schumpeterian dystopias in which obsolescence causes the system to collapse.', 'We map the phase space in terms of the rates at which new agents enter, replicate, and die. When we extend our model to higher dimensional graphs, cooperative agents, or inverted, obsolescence-driven innovation, we find that the essential features of the model are preserved.', 'We predict variation in the density profile of agents along the spectrum of new to old such as a drop in density close to both frontiers.', 'When comparing our model to data, we discover that the density reveals a follow-the-leader dynamic in firm cost efficiency and biological evolution, whereas scientific progress reflects consensus that waits on old ideas to go obsolete.', 'We show how the fundamental forces of innovation and obsolescence provide a unifying perspective on complex systems that may help us understand, harness, and shape their collective outcomes.']",http://arxiv.org/abs/2203.14611,"Innovation and obsolescence describes dynamics of ever-churning and adapting social and biological systems from the development of economic markets and scientific progress to biological evolution. The shared aspect of this picture is that agents destroy and extend the ""idea lattice"" in which they live, finding new possibilities and rendering old solutions irrelevant. We focus on this aspect with a simple model to study the central relationship between the rates at which replicating agents discover new ideas and at which old ideas are rendered obsolete. When these rates are equal, the space of the possible (e.g. ideas, markets, technologies, mutations) remains finite. A positive or negative difference distinguishes flourishing, ever-expanding idea lattices from Schumpeterian dystopias in which obsolescence causes the system to collapse. We map the phase space in terms of the rates at which new agents enter, replicate, and die. When we extend our model to higher dimensional graphs, cooperative agents, or inverted, obsolescence-driven innovation, we find that the essential features of the model are preserved. In all cases, we predict variation in the density profile of agents along the spectrum of new to old such as a drop in density close to both frontiers. When comparing our model to data, we discover that the density reveals a follow-the-leader dynamic in firm cost efficiency and biological evolution, whereas scientific progress reflects consensus that waits on old ideas to go obsolete. We show how the fundamental forces of innovation and obsolescence provide a unifying perspective on complex systems that may help us understand, harness, and shape their collective outcomes. ","Idea engines: A unified theory of innovation and obsolescence from
  markets and genetic evolution to science"
40,1508851227941027845,947018998037843968,Dominic Sicilian,"['My latest paper is up on arXiv!\n\nWe produced a new redshift catalog for 100+ obscured AGN using the XZ method.\n\nIn the process, we used a deep neural network to improve XZ‚Äôs performance on a general data set (with lots of low redshifts).\n\nEnjoy!\n\n<LINK>']",https://arxiv.org/abs/2203.13825,"We have computed obscured AGN redshifts using the XZ method, adopting a broad treatment in which we employed a wide-ranging data set and worked primarily at the XZ counts sensitivity threshold, culminating with a redshift catalog containing 121 sources that lack documented redshifts. We considered 363 obscured AGN from the Chandra Source Catalog Release 2.0, 59 of which were selected using multiwavelength criteria while 304 were X-ray selected. One-third of the data set had cross-matched spectroscopic or photometric redshifts. These sources, dominated by low-$z$ and low-$N_H$ AGN, were supplemented by 1000 simulations to form a data set for testing the XZ method. We used a multi-layer perceptron neural network to examine and predict cases in which XZ fails to reproduce the known redshift, yielding a classifier that can identify and discard poor redshift estimates. This classifier demonstrated a statistically significant $\sim$3$\sigma$ improvement over the existing XZ redshift information gain filter. We applied the machine learning model to sources with no documented redshifts, resulting in the 121-source new redshift catalog, all of which were X-ray selected. Our neural network's performance suggests that nearly 90% of these redshift estimates are consistent with hypothetical spectroscopic or photometric measurements, strengthening the notion that redshifts can be reliably estimated using only X-rays, which is valuable to current and future missions such as Athena. We have also identified a possible Compton-thick candidate that warrants further investigation. ",X-Ray Redshifts of Obscured Chandra Source Catalog AGN
41,1508825437275049985,2883271903,Yuxiang Wu,"['üö®New ACL2022 paper!üö®‚ÄúGenerating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets‚Äù. Read the paper here: <LINK>, and check out the thread below\nw/ @nlpmattg, Pontus Stenetorp, @pdasigi @ai2_allennlp üßµ1/N <LINK>', 'Spurious correlations between task-independent features and labels are often found in NLP datasets. Models can exploit these features to perform well in-distribution, but generalise poorly to out-of-distribution datasets. 2/N', 'Manually labelling a dataset without spurious correlations will be costly. We ask the question: ""Can we automatically generate samples and transform the dataset distribution to mitigate these spurious correlations?"" 3/N', 'In this work, we propose to 1) exploit unlikelihood training in two ways to train a LM that generates high-quality and less biased NLI data samples, and 2) filter out samples that will contribute to spurious correlations with our z-filtering algorithm. 4/N', 'We produce Generated Debiased NLI (GD-NLI) datasets for SNLI and MNLI (available for download at https://t.co/ks2pQqKSoq). The experimental results show that, models trained on our debiased datasets generalise significantly better to OOD sets than the baseline. 5/N', 'When combined with an orthogonal technique product-of-experts, our models further improve and achieve SOTA results on SNLI-hard and MNLI-hard. 6/N https://t.co/isnso8qIyZ', 'Visualisation of statistics of the features also demonstrates that, the spurious correlations between task-independent features and labels are massively reduced in our debiased version of the datasets. 7/N https://t.co/W2B5qes3mD', 'This is a joint work with the incredibly colleagues @nlpmattg, Pontus Stenetorp, @pdasigi  during my internship at AI2 @ai2_allennlp @allen_ai \nüëâPaper: https://t.co/De0I6qEMx6\nüëâCode: https://t.co/ks2pQqLqdY']",https://arxiv.org/abs/2203.12942,"Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard. ","Generating Data to Mitigate Spurious Correlations in Natural Language
  Inference Datasets"
42,1508775245791969284,1378995751682859008,Yuki Saito,"['Our new paper on STUDIES, an empathetic dialogue  speech corpus for developing a voice agent that can speak in a friendly manner, is out!\n<LINK>']",https://arxiv.org/abs/2203.14757,"We present STUDIES, a new speech corpus for developing a voice agent that can speak in a friendly manner. Humans naturally control their speech prosody to empathize with each other. By incorporating this ""empathetic dialogue"" behavior into a spoken dialogue system, we can develop a voice agent that can respond to a user more naturally. We designed the STUDIES corpus to include a speaker who speaks with empathy for the interlocutor's emotion explicitly. We describe our methodology to construct an empathetic dialogue speech corpus and report the analysis results of the STUDIES corpus. We conducted a text-to-speech experiment to initially investigate how we can develop more natural voice agent that can tune its speaking style corresponding to the interlocutor's emotion. The results show that the use of interlocutor's emotion label and conversational context embedding can produce speech with the same degree of naturalness as that synthesized by using the agent's emotion label. Our project page of the STUDIES corpus is this http URL ","STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly
  Voice Agent"
43,1508728065597263872,3263851731,PPhDechant,['Our new paper is just out on the arXiv:\n\n#ClusterAlgebras: #Network Science and\n#MachineLearning\n\n<LINK> <LINK>'],https://arxiv.org/abs/2203.13847,"Cluster algebras have recently become an important player in mathematics and physics. In this work, we investigate them through the lens of modern data science, specifically with techniques from network science and machine-learning. Network analysis methods are applied to the exchange graphs for cluster algebras of varying mutation types. The analysis indicates that when the graphs are represented without identifying by permutation equivalence between clusters an elegant symmetry emerges in the quiver exchange graph embedding. The ratio between number of seeds and number of quivers associated to this symmetry is computed for finite Dynkin type algebras up to rank 5, and conjectured for higher ranks. Simple machine learning techniques successfully learn to differentiate cluster algebras from their seeds. The learning performance exceeds 0.9 accuracies between algebras of the same mutation type and between types, as well as relative to artificially generated data. ",Cluster Algebras: Network Science and Machine Learning
44,1508718414055059458,1271852576296906755,Ashley Chrimes,"['New paper on arxiv today! We find NIR emission at the location of 6 Galactic magnetars for the first time, and show that some known NIR counterparts are highly variable. We also discuss the nature of the NIR emission - more to come on that, so stay tuned! <LINK>']",https://arxiv.org/abs/2203.14947,"We report the discovery of six new magnetar counterpart candidates from deep near-infrared Hubble Space Telescope imaging. The new candidates are among a sample of nineteen magnetars for which we present HST data obtained between 2018-2020. We confirm the variability of previously established near-infrared counterparts, and newly identify candidates for PSRJ1622-4950, SwiftJ1822.3-1606, CXOUJ171405.7-381031, SwiftJ1833-0832, SwiftJ1834.9-0846 and AXJ1818.8-1559 based on their proximity to X-ray localisations. The new candidates are compared with the existing counterpart population in terms of their colours, magnitudes, and near-infrared to X-ray spectral indices. We find two candidates for AXJ1818.8-1559 which are both consistent with previously established counterparts. The other new candidates are likely to be chance alignments, or otherwise have a different origin for their near-infrared emission not previously seen in magnetar counterparts. Further observations and studies of these candidates are needed to firmly establish their nature. ","New candidates for magnetar counterparts from a deep search with the
  Hubble Space Telescope"
45,1508717472328867840,761585780149985280,Sonal Sannigrahi,"['üö®new paper!üö® \nExcited to share my upcoming paper in #RepL4NLP at #ACL2022nlp! We look at developing cross-lingual word embeddings by exploiting related language data to make better quality spaces with a higher degree of isomorphism\n\nüìú: <LINK>\n\nüßµ‚û°Ô∏è <LINK>', 'Recent study in cross-lingual WE have shown that offline mapping methods fail to create isomorphic spaces for distant language pairs, but jointly learnt spaces avoid this issue (at the cost of requiring parallel data)', 'We propose that for certain low-resource languages which have a higher-resource related language, we can exploit related language data to generate better embeddings! https://t.co/aYpOfr1PKb', 'A simple, yet effective approach: we pre-align the Source and Target to a Related Language using Mapping and Joint methods (using Related-Target parallel data)  respectively which internalises structural elements making the final Source-Target mapping isomorphic', ""This work was done for my Bachelor's thesis between January and April 2021 and since then has gone through many revisions. I would like to thank my advisor Jesse Read and all the members of DaSciM, LIX who shared their ideas with me!""]",https://arxiv.org/abs/2203.14632,"Cross-Lingual Word Embeddings (CLWEs) are a key component to transfer linguistic information learnt from higher-resource settings into lower-resource ones. Recent research in cross-lingual representation learning has focused on offline mapping approaches due to their simplicity, computational efficacy, and ability to work with minimal parallel resources. However, they crucially depend on the assumption of embedding spaces being approximately isomorphic i.e. sharing similar geometric structure, which does not hold in practice, leading to poorer performance on low-resource and distant language pairs. In this paper, we introduce a framework to learn CLWEs, without assuming isometry, for low-resource pairs via joint exploitation of a related higher-resource language. In our work, we first pre-align the low-resource and related language embedding spaces using offline methods to mitigate the assumption of isometry. Following this, we use joint training methods to develops CLWEs for the related language and the target embed-ding space. Finally, we remap the pre-aligned low-resource space and the target space to generate the final CLWEs. We show consistent gains over current methods in both quality and degree of isomorphism, as measured by bilingual lexicon induction (BLI) and eigenvalue similarity respectively, across several language pairs: {Nepali, Finnish, Romanian, Gujarati, Hungarian}-English. Lastly, our analysis also points to the relatedness as well as the amount of related language data available as being key factors in determining the quality of embeddings achieved. ",Isomorphic Cross-lingual Embeddings for Low-Resource Languages
46,1508615469279985669,1047899041311412224,Francois Grondin,"[""Here's a new method for direction of arrival estimation as accurate as SRP-PHAT, but uses up to 40% less computations. We also provide the source code in plain C to run the algorithm.\n\nPaper preprint: <LINK>\nSource code: <LINK>\n\n#Interspeech"", '@seeedstudio @miniDSP @MATRIX_Creator']",https://arxiv.org/abs/2203.14409,"This paper introduces SMP-PHAT, which performs direction of arrival (DoA) of sound estimation with a microphone array by merging pairs of microphones that are parallel in space. This approach reduces the number of pairwise cross-correlation computations, and brings down the number of flops and memory lookups when searching for DoA. Experiments on low-cost hardware with commonly used microphone arrays show that the proposed method provides the same accuracy as the former SRP-PHAT approach, while reducing the computational load by 39% in some cases. ",SMP-PHAT: Lightweight DoA Estimation by Merging Microphone Pairs
47,1508548557011243013,84902368,Abhishek Das,"['New #ICLR2022 paper on Graph Parallelism (<LINK>) ‚Äî a distributed training method for GNNs with higher-order interactions (triplets / quadruplets) common in modeling atomic systems.\n\nWork led by @anuroopsriram as part of @OpenCatalyst. <LINK>', 'Graph Parallelism enables us to scale up DimeNet++ / GemNet models by an order of magnitude in parameters, leading to state-of-the-art results across all @OpenCatalyst benchmarks https://t.co/4jF9i74b8V.\n\nCode + models coming soon on https://t.co/sMN1KlTHRQ https://t.co/dPvUf4G0II']",https://arxiv.org/abs/2203.09697,"Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy breakthroughs needed to combat climate change. However, the GNNs that have proven most effective for this task are memory intensive as they model higher-order interactions in the graphs such as those between triplets or quadruplets of atoms, making it challenging to scale these models. In this paper, we introduce Graph Parallelism, a method to distribute input graphs across multiple GPUs, enabling us to train very large GNNs with hundreds of millions or billions of parameters. We empirically evaluate our method by scaling up the number of parameters of the recently proposed DimeNet++ and GemNet models by over an order of magnitude. On the large-scale Open Catalyst 2020 (OC20) dataset, these graph-parallelized models lead to relative improvements of 1) 15% on the force MAE metric for the S2EF task and 2) 21% on the AFbT metric for the IS2RS task, establishing new state-of-the-art results. ","Towards Training Billion Parameter Graph Neural Networks for Atomic
  Simulations"
48,1508541442913886215,2377341628,Patrick Diehl,['New paper: Coupling approaches for classical linear elasticity and bond-based peridynamic models\n\nPaper: <LINK>\n\nPreprint: <LINK>\n\n#peridynamic #engineering <LINK>'],https://arxiv.org/abs/2203.09934,"Local-nonlocal coupling approaches provide a means to combine the computational efficiency of local models and the accuracy of nonlocal models. This paper studies the continuous and discrete formulations of three existing approaches for the coupling of classical linear elasticity and bond-based peridynamic models, namely 1) a method that enforces matching displacements in an overlap region, 2) a variant that enforces a constraint on the stresses instead, and 3) a method that considers a variable horizon in the vicinity of the interfaces. The performance of the three coupling approaches is compared on a series of one-dimensional numerical examples that involve cubic and quartic manufactured solutions. Accuracy of the proposed methods is measured in terms of the difference between the solution to the coupling approach and the solution to the classical linear elasticity model, which can be viewed as a modeling error. The objective of the paper is to assess the quality and performance of the discrete formulation for this class of force-based coupling methods. ","Coupling approaches for classical linear elasticity and bond-based
  peridynamic models"
49,1508407909155278848,13800042,Lukas Heinrich,"['Short new paper on lhood-free inference:We use the profile lhood ratio b/c of its asymptotically optimal properties but often need to approximate p(x|Œ∏) to compute it. But if we take its properties seriously, we can find it in a fully likelihood-free way: <LINK> <LINK>', ""As in the likelihood ratio trick, the idea is simple: if we use a test stat because it's optimal, that just means we can find it through optimization. The LRT is (asymptotically) optimal is various ways e.g. has best average power in a certain sense"", 'The idea is to optimizing a neural network-based statistic for best average power - this will generally give you a good test statistic. If the underlying model behaves asymptotically, this recovers the profile likelihood ratio without ever having to evaluate or fit p(x|Œ∏)']",http://arxiv.org/abs/2203.13079,"The design of optimal test statistics is a key task in frequentist statistics and for a number of scenarios optimal test statistics such as the profile-likelihood ratio are known. By turning this argument around we can find the profile likelihood ratio even in likelihood-free cases, where only samples from a simulator are available, by optimizing a test statistic within those scenarios. We propose a likelihood-free training algorithm that produces test statistics that are equivalent to the profile likelihood ratios in cases where the latter is known to be optimal. ",Learning Optimal Test Statistics in the Presence of Nuisance Parameters
50,1508333638735302657,1138762581164855298,Christoph Ternes,"['New paper today, <LINK> We show that the Gallium anomaly can be solved with non standard neutrino interactions. Unlike in the case of light sterile neutrinos, our preferred region in parameter space is not in tension with existing bounds from other measurements.']",https://arxiv.org/abs/2203.13659,We show that the Gallium anomaly can not be explained by CC-NSI. ,No CC-NSI explanation of the Gallium anomaly
51,1507772541414244352,891231270,Jonathan Balloch,"['My new paper ""NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty""\n<LINK>\n + code \n<LINK>\n\nwith colleagues @xxbidiao @beckypeng6  @mark_riedl @Mlovescheese21 @AarunSrinivas  and others. Check out the thread for more info! <LINK>']",https://arxiv.org/abs/2203.12117,"A robust body of reinforcement learning techniques have been developed to solve complex sequential decision making problems. However, these methods assume that train and evaluation tasks come from similarly or identically distributed environments. This assumption does not hold in real life where small novel changes to the environment can make a previously learned policy fail or introduce simpler solutions that might never be found. To that end we explore the concept of {\em novelty}, defined in this work as the sudden change to the mechanics or properties of environment. We provide an ontology of for novelties most relevant to sequential decision making, which distinguishes between novelties that affect objects versus actions, unary properties versus non-unary relations, and the distribution of solutions to a task. We introduce NovGrid, a novelty generation framework built on MiniGrid, acting as a toolkit for rapidly developing and evaluating novelty-adaptation-enabled reinforcement learning techniques. Along with the core NovGrid we provide exemplar novelties aligned with our ontology and instantiate them as novelty templates that can be applied to many MiniGrid-compliant environments. Finally, we present a set of metrics built into our framework for the evaluation of novelty-adaptation-enabled machine-learning techniques, and show characteristics of a baseline RL model using these metrics. ",NovGrid: A Flexible Grid World for Evaluating Agent Response to Novelty
52,1507455198872145930,355010768,J Craig Wheeler,"[""New paper on arXiv today: <LINK>. This may be the most complex paper I've ever worked on. 7 years of multiwavelength data, 2 years of writing on interacting SN 2014C. We are sure departures from spherical symmetry are critical.""]",https://arxiv.org/abs/2203.12747,"SN 2014C was originally classified as a Type Ib supernova, but at phase {\phi} = 127 d post-explosion strong H{\alpha} emission was observed. SN 2014C has since been observed in radio, infrared, optical and X-ray bands. Here we present new optical spectroscopic and photometric data spanning {\phi} = 947 - 2494 d post-explosion. We address the evolution of the broadened H{\alpha} emission line, as well as broad [O III] emission and other lines. We also conduct a parallel analysis of all publicly available multi-wavelength data. From our spectra, we find a nearly constant H{\alpha} FWHM velocity width of {\sim}2000 km/s that is significantly lower than that of other broadened atomic transitions ({\sim}3000 - 7000 km/s) present in our spectra ([O I] {\lambda}6300; [O III] {\lambda}{\lambda}4959,5007; He I {\lambda}7065; [Ca II] {\lambda}{\lambda}7291,7324). The late radio data demand a fast forward shock ({\sim}10,000 km/s at {\phi} = 1700 d) in rarified matter that contrasts with the modest velocity of the H{\alpha}. We propose that the infrared flux originates from a toroidal-like structure of hydrogen surrounding the progenitor system, while later emission at other wavelengths (radio, X-ray) likely originates predominantly from the reverse shock in the ejecta and the forward shock in the quasi-spherical progenitor He wind. We propose that the H{\alpha} emission arises in the boundary layer between the ejecta and torus. We also consider the possible roles of a pulsar and a binary companion. ","Seven Years of SN 2014C: a Multi-Wavelength Synthesis of an
  Extraordinary Supernova"
53,1507430141479292933,3229475660,Kanishka Misra,"['*NEW PREPRINT* \n\nI built a simple python package called minicons, to facilitate behavioral and representational analyses of transformer LMs.\n\npaper: <LINK>\ncode: <LINK>\npaper-experiments: <LINK>\n\n1/n <LINK>', ""A number of analyses focus on evaluating LMs in a zero-shot setup -- by using prompts/minimal pairs that target specific capacities (e.g., number-agreement).\n\nminicons facilitates such analyses via it's scorer module:\n\n2/n https://t.co/QXes15Mmti"", ""Using the scorer module, I analyzed the learning dynamics of the BERT-base architecture on the phenomena from BLiMP using the MultiBERTs checkpoints graciously made available by @Thibolbo et al.\n\nTLDR: Many BLiMP phenomena are reflected early on during BERT's training!\n\n3/n https://t.co/Ne4m5cGRbD"", 'I then used it to benchmark 23 different LMs on unsupervised abductive natural language inference - the task of choosing the hypothesis (out of two) that best explains the given partial observations (Bhagavatula et al., 2020; https://t.co/JgXlBeHs83).\n\n4/n https://t.co/hNoCyKDZ8y', 'TLDR: Unsurprisingly, all models struggle on this task in a zero-shot setting. ALBERT-xxlarge performs the best, despite being super small compared to the largest model (GPT-J; 6B params!). Performance is generally correlated with log-number of parameters.\n\n5/n https://t.co/BHkBFBfb3V', 'The scorer module also allows for batched computation of word and sentence-based probability measures. I also provide a CLI for this functionality.\n\nThis functionality could potentially be of interest to comp. psycholinguists! \n\n6/n https://t.co/iOZGtwRvia', 'minicons also allows for batched-extraction of word/phrase embeddings from one or more layers or their combination. Useful especially when one pre-stores all representations before probing/other representational analyses (RSA, etc.)\n\n7/n https://t.co/AmmrepCGcl', 'üíïThanks to everyone who helped in the development/improvement of minicons in various ways:\n@daemon92, @Forrest_L_Davis, @Sanghee__Kim, @bruno_nicenboim, @adelegoldberg1\n\nI am especially looking forward to receiving feedback/help in adding new functionality to the package. \n\n8/8', 'Not sure why I cannot tag the first author of the MultiBERTs paper :( tagging a few others: @iftenney @_jasonwei @nsaphra', ""@BlancheMinerva Thanks for your questions! \n1. I should have definitely mentioned what the SotA was for the AlphaNLI task -- while it is not a paper (yet), there's a brief description here: https://t.co/VPw4TizWW2 I will update the manuscript with the link."", ""@BlancheMinerva 2. The scorer module provides functions to elicit word/sentence level probability estimates from a masked/autoregressive model (as long as it's trained using huggingface). Documentation: https://t.co/ei4L10KQi7"", '@BlancheMinerva 3. Unfortunately minicons can only directly be used to query models trained using huggingface transformers, but it will surely be interesting to use the GPT3 api on these tasks.\n\n4. I was not aware of any other GPT-J models apart from the 6B one, are these different from GPT-Neo?', ""@BlancheMinerva Oh I realized what you meant -- I'll try and run analyses on those, but it may be difficult to report in the manuscript (page limit and all). Maybe it's worth creating a special website alongside the documentation to incorporate such results?""]",https://arxiv.org/abs/2203.13112,"We present minicons, an open source library that provides a standard API for researchers interested in conducting behavioral and representational analyses of transformer-based language models (LMs). Specifically, minicons enables researchers to apply analysis methods at two levels: (1) at the prediction level -- by providing functions to efficiently extract word/sentence level probabilities; and (2) at the representational level -- by also facilitating efficient extraction of word/phrase level vectors from one or more layers. In this paper, we describe the library and apply it to two motivating case studies: One focusing on the learning dynamics of the BERT architecture on relative grammatical judgments, and the other on benchmarking 23 different LMs on zero-shot abductive reasoning. minicons is available at this https URL ","minicons: Enabling Flexible Behavioral and Representational Analyses of
  Transformer Language Models"
54,1507400626979831808,972878356319473665,Sophia Economou,"['New paper on arxiv. We propose a new, easier to measure objective function combined with the ADAPT-VQE strategy to create Gibbs states with high fidelity. <LINK> With Ada Warren, @linghua_zhu, Ed Barnes and @nick_mayhall']",https://arxiv.org/abs/2203.12757,"The preparation of Gibbs thermal states is an important task in quantum computation with applications in quantum simulation, quantum optimization, and quantum machine learning. However, many algorithms for preparing Gibbs states rely on quantum subroutines which are difficult to implement on near-term hardware. Here, we address this by (i) introducing an objective function that, unlike the free energy, is easily measured, and (ii) using dynamically generated, problem-tailored ans\""atze. This allows for arbitrarily accurate Gibbs state preparation using low-depth circuits. To verify the effectiveness of our approach, we numerically demonstrate that our algorithm can prepare high-fidelity Gibbs states across a broad range of temperatures and for a variety of Hamiltonians. ",Adaptive variational algorithms for quantum Gibbs state preparation
55,1507308358717562890,864056888564084736,Pablo Lanillos (ü§ñüß†),"['üéØCheck this new mind-blowing paper: ""Reclaiming salience: rhythmic precision-modulated action and perception""\nüëá\n<LINK>\nwith AA Mera, F Novicky, T Parr, K Friston and @nsajidt \n\n#Neuroscience #Robotics #Attention #Saliency #ActivePerception <LINK>', 'Have we properly modelled attention and saliency?\n‚úîÔ∏èWe revisit neuroscience findings to propose a new model of attention and salience.\n‚úîÔ∏èWe reclaim salience as an active inference process that relies on 2 basic principles: uncertainty minimisation &amp; rhythmic scheduling', 'Did we properly use and implement saliency in ML and robotics?\n‚úîÔ∏èWe implement a precision-based model going beyond human fixation maps\n‚úîÔ∏èWe showcase numerical experiments for state and noise estimation, system identification and action selection for informative path planning. https://t.co/otE6xkWTcU', 'This work changes our view of attention and saliency going back to its original definition but considering the circular causality between perception and action. We place attention and saliency as integral processes for efficient gathering and processing of sensory information.', 'üî¥Saliency as fixation pixel-wise maps is the past. üü¢Rhythmic precision-modulation is the future: Precision control and uncertainty minimisation that influences the selection of future sensory data and that are synchronised in an oscillatory fashion. https://t.co/XnzclBAK4S']",https://arxiv.org/abs/2203.12652,"Computational models of visual attention in artificial intelligence and robotics have been inspired by the concept of a saliency map. These models account for the mutual information between the (current) visual information and its estimated causes. However, they fail to consider the circular causality between perception and action. In other words, they do not consider where to sample next, given current beliefs. Here, we reclaim salience as an active inference process that relies on two basic principles: uncertainty minimisation and rhythmic scheduling. For this, we make a distinction between attention and salience. Briefly, we associate attention with precision control, i.e., the confidence with which beliefs can be updated given sampled sensory data, and salience with uncertainty minimisation that underwrites the selection of future sensory data. Using this, we propose a new account of attention based on rhythmic precision-modulation and discuss its potential in robotics, providing numerical experiments that showcase advantages of precision-modulation for state and noise estimation, system identification and action selection for informative path planning. ",Reclaiming saliency: rhythmic precision-modulated action and perception
56,1507263152421015554,2778729792,Saquib Sarfraz,"['Checkout our new #CVPR2022 paper, h-NNE: a general purpose dimensionality reduction algorithm such as t-SNE/UMAP. It stands out for its speed scalability and simplicity. #DaimlerTSS #MercedesBenz #KITKarlsruhe\nPaper: <LINK>\nCode: <LINK> <LINK>', 'Efficient because of optimization free projection built on directly projecting a 1-NN based clustering hierarchy. Makes it operate with an order of magnitude faster run times https://t.co/P38dFRgIgH', 'Faster run-times and the ability to expose the clustering structure of the data could be particularly useful for visualizing large-scale unlabeled data. \nOn PyPI  ( pip install hnne ) \nco-authors: @marioskoul @cmseibold https://t.co/2Z5PKaT8Sq']",https://arxiv.org/abs/2203.12997,"Dimensionality reduction is crucial both for visualization and preprocessing high dimensional data for machine learning. We introduce a novel method based on a hierarchy built on 1-nearest neighbor graphs in the original space which is used to preserve the grouping properties of the data distribution on multiple levels. The core of the proposal is an optimization-free projection that is competitive with the latest versions of t-SNE and UMAP in performance and visualization quality while being an order of magnitude faster in run-time. Furthermore, its interpretable mechanics, the ability to project new data, and the natural separation of data clusters in visualizations make it a general purpose unsupervised dimension reduction technique. In the paper, we argue about the soundness of the proposed method and evaluate it on a diverse collection of datasets with sizes varying from 1K to 11M samples and dimensions from 28 to 16K. We perform comparisons with other state-of-the-art methods on multiple metrics and target dimensions highlighting its efficiency and performance. Code is available at this https URL ","Hierarchical Nearest Neighbor Graph Embedding for Efficient
  Dimensionality Reduction"
57,1507036493226815491,456021567,Richard Lange üåéü§ñüß†,"['New paper on arxiv! <LINK>\n\nWith @david_rolnick and @KordingLab\n\nWe began with the big and ambitious question: how can we design more ""modular"" neural networks?', 'Modularity is an interesting topic in part because nobody can agree on what it means, but most agree that it is useful. It also gets at questions about the nature of neural representations and how they should be structured.', 'This paper essentially grew out of a failed sanity-check... To design modular networks, we started by trying to quantify modularity by clustering. The sanity-check was supposed to show that a few different sensible clustering schemes would give similar results.', 'The main result is that we get very different clusters of hidden units when we group them by how similarly *driven* they are by upstream activity versus when we group units by how similarly they *drive* downstream activity.', 'Along the way, we also made a surprising discovery about dropout. Our clustering analysis revealed that dropout leads to in groups of hidden units that look like copies of each other. We think this is because when one unit is dropped, other copies of it fill in.', 'We\'re very interested to hear what people think! Do ""modules"" have more to do with groups of related input features, or groups of related outputs/behaviors? Or perhaps none of the above? And is this redundancy-inducing effect of dropout known?\n\nüßµ/üßµ', '@tiagopeixoto Thanks! We tried to control for this by comparing with modularity in random untrained networks. Your blog post looks great though and I will take a closer look']",https://arxiv.org/abs/2203.11815,"It has been hypothesized that some form of ""modular"" structure in artificial neural networks should be useful for learning, compositionality, and generalization. However, defining and quantifying modularity remains an open problem. We cast the problem of detecting functional modules into the problem of detecting clusters of similar-functioning units. This begs the question of what makes two units functionally similar. For this, we consider two broad families of methods: those that define similarity based on how units respond to structured variations in inputs (""upstream""), and those based on how variations in hidden unit activations affect outputs (""downstream""). We conduct an empirical study quantifying modularity of hidden layer representations of simple feedforward, fully connected networks, across a range of hyperparameters. For each model, we quantify pairwise associations between hidden units in each layer using a variety of both upstream and downstream measures, then cluster them by maximizing their ""modularity score"" using established tools from network science. We find two surprising results: first, dropout dramatically increased modularity, while other forms of weight regularization had more modest effects. Second, although we observe that there is usually good agreement about clusters within both upstream methods and downstream methods, there is little agreement about the cluster assignments across these two families of methods. This has important implications for representation-learning, as it suggests that finding modular representations that reflect structure in inputs (e.g. disentanglement) may be a distinct goal from learning modular representations that reflect structure in outputs (e.g. compositionality). ",Clustering units in neural networks: upstream vs downstream information
58,1506998476005126152,484490200,Ilan Price,"['Can ML help us produce cheap, reliable, high-resolution stochastic rain forecasts? In our new #AISTATS2022 paper we develop a promising approach using deep generative models. (with @raspstephan, done while at @climateai) <LINK> üßµ 1/7 <LINK>', 'Weather forecasts are typically produced by numerical weather models (NWMs). These are very expensive to run, and so global NWMs are run at relatively low spatial resolution. Only richer countries can run their own regional, high-res NWMs.  2/7', 'We propose and train a GAN model - CorrectorGAN - to map from low resolution NWM ensembles to distributions of high-resolution forecasts. 3/7 https://t.co/4KsLNimtbu', 'This goal is to combine (1) bias-correction, (2) super-resolution, and (3) uncertainty generation and calibration, producing reliable high resolution stochastic forecasts without running high-resolution NWMs. 4/7', 'We compare against an operational regional NWM (HREF), as well as DL based methods. CorrectorGAN approaches HREF‚Äôs performance on CRPS, Brier scores, and reliability, but its forecasts are generated at a tiny fraction of the cost (just generator forward passes). 5/7 https://t.co/uG7TnYRRF4', 'The architecture and training were informed by the specifics of the task - for details on these, and on the experimental setup and evaluation, check out the paper. 6/7', 'Big thanks @raspstephan and @climateai for the great collaboration. 7/7']",https://arxiv.org/abs/2203.12297,"Accurately forecasting extreme rainfall is notoriously difficult, but is also ever more crucial for society as climate change increases the frequency of such extremes. Global numerical weather prediction models often fail to capture extremes, and are produced at too low a resolution to be actionable, while regional, high-resolution models are hugely expensive both in computation and labour. In this paper we explore the use of deep generative models to simultaneously correct and downscale (super-resolve) global ensemble forecasts over the Continental US. Specifically, using fine-grained radar observations as our ground truth, we train a conditional Generative Adversarial Network -- coined CorrectorGAN -- via a custom training procedure and augmented loss function, to produce ensembles of high-resolution, bias-corrected forecasts based on coarse, global precipitation forecasts in addition to other relevant meteorological fields. Our model outperforms an interpolation baseline, as well as super-resolution-only and CNN-based univariate methods, and approaches the performance of an operational regional high-resolution model across an array of established probabilistic metrics. Crucially, CorrectorGAN, once trained, produces predictions in seconds on a single machine. These results raise exciting questions about the necessity of regional models, and whether data-driven downscaling and correction methods can be transferred to data-poor regions that so far have had no access to high-resolution forecasts. ","Increasing the accuracy and resolution of precipitation forecasts using
  deep generative models"
59,1506940203721711622,1502311633514909700,Daniel Dugas,"['New work! \n""NavDreams: Towards Camera-Only RL Navigation Among Humans""\n\npaper: <LINK>\ncode: <LINK>\n\nIs predicting the future useful for robots which move among people? <LINK>', ""We train world-models to predict the future (from pixels) in our simulator. These models can 'dream' examples of future trajectories, and also lead to useful abstractions (latent features) which planners can use.\n\nWe show this concept working on a real robot. https://t.co/gasVsqKHq2"", 'In the future, we want world-model evaluation to be better, so that roboticists can focus on improving world-model scores - while being confident that this leads to better planning. https://t.co/7rZdeLmIg1', ""We've open sourced all our code, including our robot-navigation simulator -&gt; https://t.co/PotXNBuNip https://t.co/CFQdu7zTEB""]",https://arxiv.org/abs/2203.12299,"Autonomously navigating a robot in everyday crowded spaces requires solving complex perception and planning challenges. When using only monocular image sensor data as input, classical two-dimensional planning approaches cannot be used. While images present a significant challenge when it comes to perception and planning, they also allow capturing potentially important details, such as complex geometry, body movement, and other visual cues. In order to successfully solve the navigation task from only images, algorithms must be able to model the scene and its dynamics using only this channel of information. We investigate whether the world model concept, which has shown state-of-the-art results for modeling and learning policies in Atari games as well as promising results in 2D LiDAR-based crowd navigation, can also be applied to the camera-based navigation problem. To this end, we create simulated environments where a robot must navigate past static and moving humans without colliding in order to reach its goal. We find that state-of-the-art methods are able to achieve success in solving the navigation problem, and can generate dream-like predictions of future image-sequences which show consistent geometry and moving persons. We are also able to show that policy performance in our high-fidelity sim2real simulation scenario transfers to the real world by testing the policy on a real robot. We make our simulator, models and experiments available at this https URL ",NavDreams: Towards Camera-Only RL Navigation Among Humans
60,1506922206399508483,717709692517163008,Stefanie Barz,"['New paper out on #arXiv ü•≥üéâ: The power of #qutrits for non-adaptive measurement-based #quantum #computing \nCheck it out here: <LINK> @Uni_Stuttgart @IQSTpress \nTogether with Jelena Mackeprang, Daniel Bhatti, Matty Hoban üòÄ']",https://arxiv.org/abs/2203.12411,"Non-locality is not only one of the most prominent quantum features but can also serve as a resource for various information-theoretical tasks. Analysing it from an information-theoretical perspective has linked it to applications such as non-adaptive measurement-based quantum computing (NMQC). In this type of quantum computing the goal is to output a multivariate function. The success of such a computation can be related to the violation of a generalised Bell inequality. So far, the investigation of binary NMQC with qubits has shown that quantum correlations can compute all Boolean functions using at most $2^n-1$ qubits, whereas local hidden variables (LHVs) are restricted to linear functions. Here, we extend these results to NMQC with qutrits and prove that quantum correlations enable the computation of all ternary functions using the generalised qutrit Greenberger-Horne-Zeilinger (GHZ) state as a resource and at most $3^n-1$ qutrits. This yields a corresponding generalised GHZ type paradox for any ternary function that LHVs cannot compute. We give an example for an $n$-variate function that can be computed with only $n+1$ qutrits, which leads to convenient generalised qutrit Bell inequalities whose quantum bound is maximal. Finally, we prove that not all functions can be computed efficiently with qutrit NMQC by presenting a counterexample. ","The power of qutrits for non-adaptive measurement-based quantum
  computing"
61,1506756429285191682,265421900,Eric Heiden,"['We introduce a new method to learn simulators from depth and RGB videos. The ""URDF"" of an articulated rigid-body mechanism is reconstructed, and the parameters of the simulator inferred through Bayesian inference.\n\nWebsite: <LINK>\nPaper: <LINK> <LINK>', 'Our pipeline leverages inverse rendering (nvdiffrast) and differentiable physics (Tiny Differentiable Simulator) to track objects in the scene, find articulations via a RANSAC approach, and infer the distribution over simulation parameters. https://t.co/ccgVoJ9wWZ', 'Our approach finds a digital twin for articulated mechanisms from real depth or RGB video.\nCheck out our paper for more details!\n\nJoint work w/ Ziang Liu, @VibhavVineet, @erwincoumans, @gauravsukhatme https://t.co/8kehFh80AF']",https://arxiv.org/abs/2203.10488,"Being able to reproduce physical phenomena ranging from light interaction to contact mechanics, simulators are becoming increasingly useful in more and more application domains where real-world interaction or labeled data are difficult to obtain. Despite recent progress, significant human effort is needed to configure simulators to accurately reproduce real-world behavior. We introduce a pipeline that combines inverse rendering with differentiable simulation to create digital twins of real-world articulated mechanisms from depth or RGB videos. Our approach automatically discovers joint types and estimates their kinematic parameters, while the dynamic properties of the overall mechanism are tuned to attain physically accurate simulations. Control policies optimized in our derived simulation transfer successfully back to the original system, as we demonstrate on a simulated system. Further, our approach accurately reconstructs the kinematic tree of an articulated mechanism being manipulated by a robot, and highly nonlinear dynamics of a real-world coupled pendulum mechanism. Website: this https URL ",Inferring Articulated Rigid Body Dynamics from RGBD Video
62,1506719093151260675,1174003704724217856,Peter West,"['Factuality is a key concern in NLG, yet most attention goes to summarization. I‚Äôm excited to announce Factual Ablation, a new eval for the grounded content transfer task to study, measure, and improve factuality\n\npaper: <LINK>\ncode: <LINK>\n\n1/2 <LINK>', 'Factual Ablation evaluates factuality by constructing datasets to probe generative model attributes. Intuitively, outputs should become less likely when relevant information is ablated from inputs\n\nThank you to terrific coauthors Michel Galley, Chris Quirk, and @YejinChoinka\n\n2/2']",https://arxiv.org/abs/2203.10133,"Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality--it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality. Measuring factuality is also simplified--to factual consistency, testing whether the generation agrees with the grounding, rather than all facts. Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem. We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding. Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document. In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one. We contribute two evaluation sets to measure this. Applying our new evaluation, we propose multiple novel methods improving over strong baselines. ",Probing Factually Grounded Content Transfer with Factual Ablation
63,1506688502603431940,1246070462679040000,Randall Balestriero,"['Happy to share our #CVPR2022 paper w/ @imtiazprio, @rbaraniuk providing a simple solution to provably sample from the (anti-)modes of pre-trained generative networks... also leading to new StyleGAN2/3/BigGAN FID SOTAs\nüßµ(1/4)\n<LINK>\ncolab: <LINK> <LINK>', 'Because the modes of the learned distribution correspond to high-confidence samples, the corresponding samples tend to be high-quality (precision). Vice-versa, focusing on the anti-modes provides more diversity (recall)\n\nüßµ(2/4) https://t.co/xJwQad9ah6', 'As a by-product of controlling the quality and diversity of the samples, it is possible to find a sweet spot that produces better FID than the original pre-trained DGN, improving current SOTA on many popular pre-trained models e.g. StyleGAN2/3, BigGAN\n\nüßµ(3/4) https://t.co/Fq7BSWic8t', 'This work extends our previous #iclr2022 paper (https://t.co/0JZpYShgNW) that was also based on spline theory to provide uniform sampling on the learned distribution... showing that splines are a powerful theoretical tool to study and improve deep networks!\n\nüßµ(4/4)']",https://arxiv.org/abs/2203.01993,"We present Polarity Sampling, a theoretically justified plug-and-play method for controlling the generation quality and diversity of pre-trained deep generative networks DGNs). Leveraging the fact that DGNs are, or can be approximated by, continuous piecewise affine splines, we derive the analytical DGN output space distribution as a function of the product of the DGN's Jacobian singular values raised to a power $\rho$. We dub $\rho$ the $\textbf{polarity}$ parameter and prove that $\rho$ focuses the DGN sampling on the modes ($\rho < 0$) or anti-modes ($\rho > 0$) of the DGN output-space distribution. We demonstrate that nonzero polarity values achieve a better precision-recall (quality-diversity) Pareto frontier than standard methods, such as truncation, for a number of state-of-the-art DGNs. We also present quantitative and qualitative results on the improvement of overall generation quality (e.g., in terms of the Frechet Inception Distance) for a number of state-of-the-art DGNs, including StyleGAN3, BigGAN-deep, NVAE, for different conditional and unconditional image generation tasks. In particular, Polarity Sampling redefines the state-of-the-art for StyleGAN2 on the FFHQ Dataset to FID 2.57, StyleGAN2 on the LSUN Car Dataset to FID 2.27 and StyleGAN3 on the AFHQv2 Dataset to FID 3.95. Demo: bit.ly/polarity-samp ","Polarity Sampling: Quality and Diversity Control of Pre-Trained
  Generative Networks via Singular Values"
64,1506659756701749251,2235411914,Surya Ganguli,['Our new #iclr2022 paper - Towards a foundation model for robotics: one transformer to control many new robot morphologies through large-scale pre-training on another set of morphologies. Expertly lead by @agrimgupta92 &amp; collab w/@drfeifei paper: <LINK> thread -&gt; <LINK>'],https://arxiv.org/abs/2203.11931,"Multiple domains like vision, natural language, and audio are witnessing tremendous progress by leveraging Transformers for large scale pre-training followed by task specific fine tuning. In contrast, in robotics we primarily train a single robot for a single task. However, modular robot systems now allow for the flexible combination of general-purpose building blocks into task optimized morphologies. However, given the exponentially large number of possible robot morphologies, training a controller for each new design is impractical. In this work, we propose MetaMorph, a Transformer based approach to learn a universal controller over a modular robot design space. MetaMorph is based on the insight that robot morphology is just another modality on which we can condition the output of a Transformer. Through extensive experiments we demonstrate that large scale pre-training on a variety of robot morphologies results in policies with combinatorial generalization capabilities, including zero shot generalization to unseen robot morphologies. We further demonstrate that our pre-trained policy can be used for sample-efficient transfer to completely new robot morphologies and tasks. ",MetaMorph: Learning Universal Controllers with Transformers
65,1506647859407564809,2210295542,Stevie Bergman,"['New paper on #NLP annotation practices for polyglossic, dialectic languages with a focus on Arabic. ""Towards Responsible Natural Language Annotation for the Varieties of Arabic,"" authored by myself and Mona Diab, accepted to #ACL2022. <LINK>', 'When building datasets for NLP training and/or evaluation, there is a tendency to aim for as broad of coverage as possible - often overlooking cultural and (socio)linguistic nuance.', 'In this paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the participation and inclusion of cultural and linguistic expertise in the process.', 'We present a playbook for responsible dataset creation for polyglossic, multidialectal languages - here focusing on Arabic varieties as a crucial case study to expand on.', 'This work is informed by a study on Arabic annotation of Facebook platform social media content, detailed in the paper.']",https://arxiv.org/abs/2203.09597,"When building NLP models, there is a tendency to aim for broader coverage, often overlooking cultural and (socio)linguistic nuance. In this position paper, we make the case for care and attention to such nuances, particularly in dataset annotation, as well as the inclusion of cultural and linguistic expertise in the process. We present a playbook for responsible dataset creation for polyglossic, multidialectal languages. This work is informed by a study on Arabic annotation of social media content. ","Towards Responsible Natural Language Annotation for the Varieties of
  Arabic"
66,1506634734411173891,1384539576266526722,Ronen Basri,"['Our exciting new paper applies spectral analysis to the neural tangent kernel to unravel the implicit bias of deep, over-parametrized *CNNs*, revealing advantages over fully-connected networks and underscoring the role of hierarchy. <LINK>']",https://arxiv.org/abs/2203.09255,"We study the properties of various over-parametrized convolutional neural architectures through their respective Gaussian process and neural tangent kernels. We prove that, with normalized multi-channel input and ReLU activation, the eigenfunctions of these kernels with the uniform measure are formed by products of spherical harmonics, defined over the channels of the different pixels. We next use hierarchical factorizable kernels to bound their respective eigenvalues. We show that the eigenvalues decay polynomially, quantify the rate of decay, and derive measures that reflect the composition of hierarchical features in these networks. Our results provide concrete quantitative characterization of over-parameterized convolutional network architectures. ","On the Spectral Bias of Convolutional Neural Tangent and Gaussian
  Process Kernels"
67,1506589624021491714,1231755108234547202,Ariel Werle,['We have a new paper on arxiv! <LINK>'],https://arxiv.org/abs/2203.08862,"We present results from MUSE spatially-resolved spectroscopy of 21 post-starburst galaxies in the centers of 8 clusters from $z\sim0.3$ to $z\sim0.4$. We measure spatially resolved star-formation histories (SFHs), the time since quenching ($t_Q$) and the fraction of stellar mass assembled in the past 1.5 Gyr ($\mu_{1.5}$). The SFHs display a clear enhancement of star-formation prior to quenching for 16 out of 21 objects, with at least 10% (and up to $>50$%) of the stellar mass being assembled in the past 1.5 Gyr and $t_Q$ ranging from less than 100 Myrs to $\sim800$ Myrs. By mapping $t_Q$ and $\mu_{1.5}$, we analyze the quenching patterns of the galaxies. Most galaxies in our sample have quenched their star-formation from the outside-in or show a side-to-side/irregular pattern, both consistent with quenching by ram-pressure stripping. Only three objects show an inside-out quenching pattern, all of which are at the high-mass end of our sample. At least two of them currently host an active galactic nucleus. In two post-starbursts, we identify tails of ionized gas indicating that these objects had their gas stripped by ram pressure very recently. Post-starburst features are also found in the stripped regions of galaxies undergoing ram-pressure stripping in the same clusters, confirming the link between these classes of objects. Our results point to ram-pressure stripping as the main driver of fast quenching in these environments, with active galactic nuclei playing a role at high stellar masses. ",Post-starburst galaxies in the centers of intermediate redshift clusters
68,1506572089385488391,563821235,Verena Rieser,"['üì¢ New survey paper on the Automatic Evaluation of Dialog: Research Directions and Challenges <LINK> with @shikibmehri,  @lfdharo, @dilekhakkanitur, @samirashaikhUNC, and many more who are wise enough to stay off Twitter :)']",https://arxiv.org/abs/2203.10012,This is a report on the NSF Future Directions Workshop on Automatic Evaluation of Dialog. The workshop explored the current state of the art along with its limitations and suggested promising directions for future work in this important and very rapidly changing area of research. ,"Report from the NSF Future Directions Workshop on Automatic Evaluation
  of Dialog: Research Directions and Challenges"
69,1506566326617358337,986408509,Kirpal Nandra,"['Some supermassive black holes give off huge, regular bursts of X-rays. But why? New paper by @MPE_Garching‚Äôs @ArcodiaRiccardo detailing @ESA_XMM and NICER observations of ‚ÄúQPEs‚Äù found by @eROSITA_SRG with @JohannesBuchner @andmerloni @ErinAstro and more! <LINK>']",https://arxiv.org/abs/2203.11939,"Quasi-periodic eruptions (QPEs) are recurrent X-ray bursts found so far in the nuclei of low-mass galaxies. Their trigger mechanism is still unknown, but recent models involving one or two stellar-mass companions around the central massive ($\approx10^5-10^6$ solar masses) black hole have gathered significant attention. While these have been compared only qualitatively with observations, the phenomenology of QPEs is developing at a fast pace, with the potential to reveal new insights. Here we report two new observational results found in eRO-QPE1, the brightest QPE source discovered so far: i) the eruptions in eRO-QPE1 occur sometimes as single isolated bursts, and at others as chaotic mixtures of multiple overlapping bursts with very different amplitudes; ii) we confirm that QPEs peak at later times and are broader at lower energies, with respect to higher energies while, for the first time, we find that QPEs also start earlier at lower energies. Furthermore, eruptions appear to undergo an anti-clockwise hysteresis cycle in a plane of hardness ratio versus total count rate. Behavior i) was not found before in any other QPE source and implies that if a common trigger mechanism is in place for all QPEs, it must be able to produce both types of timing properties, regular and complex. Result ii) implies that the X-ray emitting component does not have an achromatic evolution even during the start of QPEs, and that the rise is harder than the decay at a given total count rate. This specific energy dependence could be qualitatively compatible with inward radial propagation during the rise within a compact accretion flow, the presence of which is suggested by the stable quiescence spectrum observed in general for QPE sources. ","The complex time and energy evolution of quasi-periodic eruptions in
  eRO-QPE1"
70,1506524673110163458,3241924438,Akshansh Mishra,['Check out the new paper on arXiv. \n#MachineLearning #DataScience #fsw \n<LINK>'],https://arxiv.org/abs/2203.11649,"Nowadays, industry 4.0 plays a tremendous role in the manufacturing industries for increasing the amount of data and accuracy in modern manufacturing systems. Thanks to artificial intelligence, particularly machine learning, big data analytics have dramatically amended, and manufacturers easily exploit organized and unorganized data. This study utilized hybrid optimization algorithms to find friction stir welding and optimal hardness value at the nugget zone. A similar AA 6262 material was used and welded in a butt joint configuration. Tool rotational speed (RPM), tool traverse speed (mm/min), and the plane depth (mm) are used as controllable parameters and optimized using Taguchi L9, Random Forest, and XG Boost machine learning tools. Analysis of variance was also conducted at a 95% confidence interval for identifying the significant parameters. The result indicated that the coefficient of determination from Taguchi L9 orthogonal array is 0.91 obtained while Random Forest and XG Boost algorithm imparted 0.62 and 0.65, respectively. ","Performance Evaluation of Machine Learning-based Algorithm and Taguchi
  Algorithm for the Determination of the Hardness Value of the Friction Stir
  Welded AA 6262 Joints at a Nugget Zone"
71,1506449241635528708,14981648,The Disordered Cosmos by Chanda Prescod-Weinstein,"['TONIGHT ON THE ARXIV! \n\nFour new papers as part of #Snowmass2021 about social issues in physics.\n\nPaper 1: How to Read the Snowmass White Papers on Power Dynamics in Physics, Informal Socialization in Physics Training, and Policing and Gatekeeping in STEM\n\n<LINK>', 'Paper 2: power dynamics!\n\n‚ÄúThe purpose of this white paper is to describe how unfair power dynamics related to various aspects of identity -- race, gender identity, gender expression, sexual orientation, and ability status -- operate in physics settings‚Äù\n\nhttps://t.co/BjN3bo7zxp', ""Paper 3: how does informal socialization shape the doing of physics?\n\n‚ÄúMany physicists' careers are built on the relationships they have and develop during these critical years.‚Äù\n\nhttps://t.co/dlvJzW81pW"", 'Paper 4: POLICING/GATEKEEPING IN PHYSICS \n\n‚ÄúThe purpose of this white paper is to lay out the impacts of policing and gatekeeping in STEM, illustrated w/lived experiences of scientists of color who are achieving despite the daunting challenges they face.‚Äù\n\nhttps://t.co/9VqQtuyyvl', 'These four papers are the product of brilliant and heroic leadership contributions from @stemScholar and Shayna Krammes in particular, with contributions from myself, @iamstarnord, @DrEsquivelPhD, &amp; K√©t√©vi Assamagan.\n\n#BlackandSTEM üí™üèΩüí™üèΩüí™üèΩüí™üèΩüí™üèΩ https://t.co/oV5363lTVs']",https://arxiv.org/abs/2203.11523,"The Community Engagement Frontier presents this set of three white papers, as part of Snowmass 2021. These papers address critical issues -- Power Dynamics in Physics, Informal Socialization in Physics Training, and Policing and Gatekeeping in STEM -- that make significant impacts on the experiences of the people who work in and learn particle physics. In this introductory document, we present crosscutting concepts that appear in each paper, and some advice on how to manage readers' responses to the contents. We expect that you will learn something new here. We hope that whatever you encounter, you will be energized to increase justice in this discipline we all love. ","How to Read the Snowmass White Papers on Power Dynamics in Physics,
  Informal Socialization in Physics Training, and Policing and Gatekeeping in
  STEM"
72,1506365762058940417,1161312102486667264,Keith Burghardt,"['A new paper on understanding material states is out!\n<LINK>\nFor a range of simulations we find a simple way to determine if there are any number of changes in the material without having to know the ""order parameter"" (what the changes actually are). <LINK>', 'Research is in collaboration with @DingrevilleRemi, Marcin Abrams, @aram_galstyan, and @gesteller']",http://arxiv.org/abs/2203.10204,"The identification and classification of transitions in topological and microstructural regimes in pattern-forming processes is critical for understanding and fabricating microstructurally precise novel materials in many application domains. Unfortunately, relevant microstructure transitions may depend on process parameters in subtle and complex ways that are not captured by the classic theory of phase transition. While supervised machine learning methods may be useful for identifying transition regimes, they need labels which require prior knowledge of order parameters or relevant structures. Motivated by the universality principle for dynamical systems, we instead use a self-supervised approach to solve the inverse problem of predicting process parameters from observed microstructures using neural networks. This approach does not require labeled data about the target task of predicting microstructure transitions. We show that the difficulty of performing this prediction task is related to the goal of discovering microstructure regimes, because qualitative changes in microstructural patterns correspond to changes in uncertainty for our self-supervised prediction problem. We demonstrate the value of our approach by automatically discovering transitions in microstructural regimes in two distinct pattern-forming processes: the spinodal decomposition of a two-phase mixture and the formation of concentration modulations of binary alloys during physical vapor deposition of thin films. This approach opens a promising path forward for discovering and understanding unseen or hard-to-detect transition regimes, and ultimately for controlling complex pattern-forming processes. ","Inferring topological transitions in pattern-forming processes with
  self-supervised learning"
73,1506361914074542081,156804540,Francisco Rodrigues,['Our new paper on @arxiv. We show that causal emergence information is necessarily contained on conditional mutual information and connect this result with the intrinsic violation of faithfulness. \n<LINK> <LINK>'],https://arxiv.org/abs/2203.10665,"Built upon the concept of causal faithfulness, the so-called causal discovery algorithms have been proposing the breakdown of the contributions of the mutual information (MI) and conditional mutual information (CMI) to the sets of variables that reveal causal influences. These algorithms suffer from the lack of accounting emergent causes when connecting links resulting in a spuriously embellished view of the organization of complex systems. Here, we first show that causal emergence information is necessarily contained on CMIs. We also connect this result with the intrinsic violation of faithfulness. Finally, by using classical results from Ramsey's theory, we prove that CMIs with large conditioned datasets contain, necessarily, arbitrary correlations. These correlations appear only because of the size, and not the nature, of data. They can be found in ""randomly"" generated, large enough databases. We also connect this result with the false illusion of faithfulness. The net result proposes an update of the well-known causal discovery algorithms, which can, in principle, detect and isolate emergent causal influences in the network reconstruction problems undetected so far. ","Shaking the causal tree: On the faithfulness and minimality assumptions
  beyond pairwise interactions"
74,1506299958747705353,1262817620518211584,Arttu Sainio,"['<LINK>\nAdam Schneider, Phd, introducing new substellar candidates of Hyades family. Our stellar backyard has a lot cold worlds that are visible only in infrared light. Many of them still to be found. This paper consist multiple objects found by citizen scientists']",https://arxiv.org/abs/2203.11090,"We have used data from the UKIRT Hemisphere Survey (UHS) to search for substellar members of the Hyades cluster. Our search recovered several known substellar Hyades members, and two known brown dwarfs that we suggest may be members based on a new kinematic analysis. We uncovered thirteen new substellar Hyades candidates, and obtained near-infrared follow-up spectroscopy of each with IRTF/SpeX. Six candidates with spectral types between M7 and L0 are ruled out as potential members based on their photometric distances ($\gtrsim$100 pc). The remaining seven candidates, with spectral types between L5 and T4, are all potential Hyades members, with five showing strong membership probabilities based on BANYAN $\Sigma$ and a convergent point analysis. Distances and radial velocities are still needed to confirm Hyades membership. If confirmed, these would be some of the lowest mass free-floating members of the Hyades yet known, with masses as low as $\sim$30 $M_{\rm Jup}$. An analysis of all known substellar Hyades candidates shows evidence that the full extent of the Hyades has yet to be probed for low-mass members, and more would likely be recovered with deeper photometric and astrometric investigations. ",Substellar Hyades Candidates from the UKIRT Hemisphere Survey
75,1506233940751024133,988837119408967680,Thuerey Group at TUM,"[""Our ICLR‚Äô22 spotlight paper on half-inverse gradients is online now: <LINK> , enjoy! It's a new method bridging ‚Äúclassical‚Äù optimizers and machine learning methods. <LINK>"", 'HIGs are motivated by a continuous interpolation between GD and Gauss-Newton, and outperforms other methods nicely: https://t.co/wt0Cuxc9Mt', ""And here's the performance for a quantum control problem with differentiable physics training over 384 time steps: https://t.co/GV83pDQwcP""]",https://arxiv.org/abs/2203.10131,"Recent works in deep learning have shown that integrating differentiable physics simulators into the training process can greatly improve the quality of results. Although this combination represents a more complex optimization task than supervised neural network training, the same gradient-based optimizers are typically employed to minimize the loss function. However, the integrated physics solvers have a profound effect on the gradient flow as manipulating scales in magnitude and direction is an inherent property of many physical processes. Consequently, the gradient flow is often highly unbalanced and creates an environment in which existing gradient-based optimizers perform poorly. In this work, we analyze the characteristics of both physical and neural network optimizations to derive a new method that does not suffer from this phenomenon. Our method is based on a half-inversion of the Jacobian and combines principles of both classical network and physics optimizers to solve the combined optimization task. Compared to state-of-the-art neural network optimizers, our method converges more quickly and yields better solutions, which we demonstrate on three complex learning problems involving nonlinear oscillators, the Schroedinger equation and the Poisson problem. ",Half-Inverse Gradients for Physical Deep Learning
76,1506224076075020292,280083723,Yoh Tanimoto,"['new paper~ we show that a unitary vertex algebra, without any additional condition, give a Wightman field on the circle with some regularity conditions, and a Wightman field on the circle with the same conditions give a unitary vertex algebra~ <LINK>']",https://arxiv.org/abs/2203.10795,"We prove an equivalence between the following notions: (i) unitary M\""obius vertex algebras, and (ii) Wightman conformal field theories on the circle (with finite-dimensional conformal weight spaces) satisfying an additional condition that we call uniformly bounded order. Reading this equivalence in one direction, we obtain new analytic and operator-theoretic information about vertex operators. In the other direction we characterize OPEs of Wightman fields and show they satisfy the axioms of a vertex algebra. As an application we establish new results linking unitary vertex operator algebras with conformal nets. ",Unitary vertex algebras and Wightman conformal field theories
77,1506077378199506944,1565020099,Dr. Ryan Lagerquist üá®üá¶ #BLM #QLM #TLM,"['My new paper with @Iebertu -- ""Can we integrate spatial verification methods into neural-network loss functions for atmospheric science?"" -- just dropped on arXiv (and has been submitted for review)!\n\n<LINK>\n\nColab notebook here: <LINK>']",https://arxiv.org/abs/2203.11141,"In the last decade, much work in atmospheric science has focused on spatial verification (SV) methods for gridded prediction, which overcome serious disadvantages of pixelwise verification. However, neural networks (NN) in atmospheric science are almost always trained to optimize pixelwise loss functions, even when ultimately assessed with SV methods. This establishes a disconnect between model verification during vs. after training. To address this issue, we develop spatially enhanced loss functions (SELF) and demonstrate their use for a real-world problem: predicting the occurrence of thunderstorms (henceforth, ""convection"") with NNs. In each SELF we use either a neighbourhood filter, which highlights convection at scales larger than a threshold, or a spectral filter (employing Fourier or wavelet decomposition), which is more flexible and highlights convection at scales between two thresholds. We use these filters to spatially enhance common verification scores, such as the Brier score. We train each NN with a different SELF and compare their performance at many scales of convection, from discrete storm cells to tropical cyclones. Among our many findings are that (a) for a low (high) risk threshold, the ideal SELF focuses on small (large) scales; (b) models trained with a pixelwise loss function perform surprisingly well; (c) however, models trained with a spectral filter produce better-calibrated probabilities than a pixelwise model. We provide a general guide to using SELFs, including technical challenges and the final Python code, as well as demonstrating their use for the convection problem. To our knowledge this is the most in-depth guide to SELFs in the geosciences. ","Can we integrate spatial verification methods into neural-network loss
  functions for atmospheric science?"
78,1506020820769914880,45724845,Swarat Chaudhuri,"['How do you learn neural networks that respect end-to-end safety requirements of larger systems of which they are a part? Our new ICLR paper, led by @ChenxiYang001 (<LINK>), explores this question. <LINK> (1/n) <LINK>', ""The SOTA answer to the question is to train the NNs first, then show that the resulting system is safe. But why would you expect it to safe if the NNs haven't seen the property during training? And what do you do if safety checking fails? (2/n)"", 'Our answer: give the learner access to a signal from a formal safety analyzer during training. This idea was previously used to prove properties of isolated NNs: https://t.co/31vipcjNZl. Does it also apply to systems where NNs coexist with nondifferentiable symbolic code? (3/n)', 'Our paper takes a first step on this problem. The main ideas are to compute a safety loss using a probabilistic symbolic executor and to backprop gradients of this loss through nondifferentiable operations using a stochastic gradient estimator.  (4/n)', 'I am really excited about this direction. The distinction between ""ML/AI"" and ""software"" is spurious in 2022 -- soon, all systems will have ML components. Since you can\'t debug NNs manually, connecting learning and verification is key. (5/n)', 'And there are so many open technical challenges! We need better sound approximation techniques for NNs. We need to figure out how to handle soft requirements and unknown unknowns. We need to balance precision and scalability. (6/n)', 'Fortunately, more and more folks are working on FM + ML, so I am optimistic about the field. Feel free to ping us if you want to chat about the area or our work. (7/7)']",https://arxiv.org/abs/2203.07671,"We study the problem of learning worst-case-safe parameters for programs that use neural networks as well as symbolic, human-written code. Such neurosymbolic programs arise in many safety-critical domains. However, because they can use nondifferentiable operations, it is hard to learn their parameters using existing gradient-based approaches to safe learning. Our approach to this problem, Differentiable Symbolic Execution (DSE), samples control flow paths in a program, symbolically constructs worst-case ""safety losses"" along these paths, and backpropagates the gradients of these losses through program operations using a generalization of the REINFORCE estimator. We evaluate the method on a mix of synthetic tasks and real-world benchmarks. Our experiments show that DSE significantly outperforms the state-of-the-art DiffAI method on these tasks. ",Safe Neurosymbolic Learning with Differentiable Symbolic Execution
79,1505815422872002565,292832009,Tomasz Kacprzak,"['Check out our new paper on DeepLSS: combined probes of large structure with deep learning!\n\nDeep nets deliver quite significant improvements, especially for Intrinsic Alignments.\n\n<LINK> <LINK>']",https://arxiv.org/abs/2203.09616v1,"In classical cosmological analysis of large scale structure surveys with 2-pt functions, the parameter measurement precision is limited by several key degeneracies within the cosmology and astrophysics sectors. For cosmic shear, clustering amplitude $\sigma_8$ and matter density $\Omega_m$ roughly follow the $S_8=\sigma_8(\Omega_m/0.3)^{0.5}$ relation. In turn, $S_8$ is highly correlated with the intrinsic galaxy alignment amplitude $A_{\rm{IA}}$. For galaxy clustering, the bias $b_g$ is degenerate with both $\sigma_8$ and $\Omega_m$, as well as the stochasticity $r_g$. Moreover, the redshift evolution of IA and bias can cause further parameter confusion. A tomographic 2-pt probe combination can partially lift these degeneracies. In this work we demonstrate that a deep learning analysis of combined probes of weak gravitational lensing and galaxy clustering, which we call DeepLSS, can effectively break these degeneracies and yield significantly more precise constraints on $\sigma_8$, $\Omega_m$, $A_{\rm{IA}}$, $b_g$, $r_g$, and IA redshift evolution parameter $\eta_{\rm{IA}}$. The most significant gains are in the IA sector: the precision of $A_{\rm{IA}}$ is increased by approximately 8x and is almost perfectly decorrelated from $S_8$. Galaxy bias $b_g$ is improved by 1.5x, stochasticity $r_g$ by 3x, and the redshift evolution $\eta_{\rm{IA}}$ and $\eta_b$ by 1.6x. Breaking these degeneracies leads to a significant gain in constraining power for $\sigma_8$ and $\Omega_m$, with the figure of merit improved by 15x. We give an intuitive explanation for the origin of this information gain using sensitivity maps. These results indicate that the fully numerical, map-based forward modeling approach to cosmological inference with machine learning may play an important role in upcoming LSS surveys. We discuss perspectives and challenges in its practical deployment for a full survey analysis. ","] DeepLSS: breaking parameter degeneracies in large scale structure with
  deep learning analysis of combined probes"
80,1505719772536872966,704320172,Margarite LaBorde,"['Check out our new paper on arXiv, where we discuss testing for Hamiltonian symmetries with quantum computers--exactly as it says on the tin. Feedback welcome and appreciated! <LINK>', '@markwilde As I forgot to tag you. Oops üòÖ']",https://arxiv.org/abs/2203.10017,"Symmetries in a Hamiltonian play an important role in quantum physics because they correspond directly with conserved quantities of the related system. In this paper, we propose quantum algorithms capable of testing whether a Hamiltonian exhibits symmetry with respect to a group. We demonstrate that familiar expressions of Hamiltonian symmetry in quantum mechanics correspond directly with the acceptance probabilities of our algorithms. We execute one of our symmetry-testing algorithms on existing quantum computers for simple examples of both symmetric and asymmetric cases. ",Quantum Algorithms for Testing Hamiltonian Symmetry
81,1504852485193682945,574816439,Charli Sakari,['New paper on astro-ph about metallicities of outer halo M31 globular clusters!  <LINK>'],https://arxiv.org/abs/2203.08840,"This paper presents [Fe/H] ratios for GCs in the outer halo of the Andromeda Galaxy, M31, based on moderate-resolution, integrated light (IL) spectroscopy of the calcium-II triplet (CaT) lines. The CaT strengths are measured by fitting Voigt profiles to the lines and integrating those profiles; integrations of defined bandpasses are also considered. The [Fe/H] ratios are determined using an empirical calibration with CaT line strength, as derived from another sample of M31 GCs that were previously studied at high-resolution. The [Fe/H] ratios for the new GCs reveal that the outer halo GCs are indeed generally more metal-poor than typical inner halo GCs, though there are several more metal-rich GCs that look to have been accreted from dwarf satellites. The metallicities of these GCs also place important constraints on the nature of the substructure in the outer halo and the dwarf satellites that created this substructure. ","Metallicities of Outer Halo M31 Globular Clusters from Integrated Light
  Calcium-II Triplet Spectroscopy"
82,1504792901657714689,1212714725571612672,David Holzm√ºller,"['Are you interested in improving the sample-efficiency of neural network (NN) regression through active learning? Then, our new paper might be all you need! üî•\nPaper: <LINK>\nCode: <LINK>\nüßµüëá 1/ <LINK>', 'We study pool-based batch mode deep active learning (BMDAL) for regression: We start with a small labeled training data set and repeatedly select a batch of unlabeled data from a large pool set for labeling. We want to select large batches since NN training can be slow. 2/ https://t.co/qcAeysQ5C4', 'We propose a benchmark with 15 large tabular data sets, available in our code, on which we achieve a new state-of-the-art performance. The plot below shows the average of the log RMSE over all data sets for (regression adaptations of) existing methods and our new method. 3/ https://t.co/cOtWIdCgsF', 'All compared methods are convenient to use: They work with almost any trained NN, do not require ensembling, and scale to large pool set and batch sizes. There are also other methods based on Dropout, ensembling or auxiliary loss functions, but they are less convenient to use. 4/', 'We also propose a framework for building BMDAL methods for regression: First, choose a base kernel representing your NN. Second, optionally apply kernel transformations, e.g. for efficiency or to represent uncertainties. Third, use the resulting kernel with a selection method. 5/ https://t.co/2feefTlHLo', 'For base kernels, last-layer features of trained networks are often used. We compare these to simple baselines: Input features and the infinite-width NNGP kernel. Alternatively, we propose to use a full gradient kernel (also known as finite-width Neural Tangent Kernel). 6/ https://t.co/IDMTloAzRk', 'We consider kernel transformations corresponding to various Bayesian methods, for example computing the posterior covariance kernel of a Gaussian Process. Moreover, we propose to use random projections (sketching) for the gradient kernel to make computations more efficient. 7/ https://t.co/aHFCBVYMfm', 'From a Bayesian perspective, some combinations of base kernels and kernel transformations are related to different Laplace approximations for Bayesian NNs, for example with last-layer or (sketched) generalized Gauss-Newton Hessian approximations. 8/', 'We consider iterative selection methods with two modes: In P-mode, model uncertainty has to be represented through kernel transformations. In TP-mode, the training set is considered a part of the selected batch, and diversifying the batch favors inputs with large uncertainty. 9/ https://t.co/3EdPtZ5kyd', 'We implement random selection, naive active learning (MaxDiag), two uncertainty-based methods (MaxDet and MaxDist), a distribution-based method (FrankWolfe), and two clustering-based methods (KMeansPP and LCMD). Here, LCMD is newly proposed by us. 10/ https://t.co/d2kJpKwrSb', 'LCMD (largest cluster maximum distance) selects the next point as the maximum-distance point in the cluster with the largest sum of squared distances. Unlike in the plot below, the distance measure in our experiments is derived from the given (transformed) kernel. 11/ https://t.co/cl5sjxCyaO', 'We can represent (regression adaptations of) many existing methods in our framework. The results here and in Figure 1 above come from a three-layer fully-connected NN. We usually report results for the ReLU activation function, but here we also report results for SiLU. 12/ https://t.co/Q8Xy9ts1NT', 'Picking different kernels and modes can make a considerable difference. For this table and the plots below, we selected the best kernels and modes (in terms of average log RMSE across data sets) for each selection method, excluding slow but similar-performing configurations. 13/ https://t.co/7lcybcBEPI', 'Averaged over our benchmark data sets, the sketched full-gradient kernel achieves better results than the last-layer kernel for all selection methods. The linear and NNGP kernels, which do not depend on the trained network, perform much worse. 14/', 'Note that the runtimes for selecting a batch of 256 samples from a pool set with ~100k samples with a training set size of ~2k samples are less than a second on a RTX 3090 GPU. 15/', 'For the logarithms of MAE, RMSE, 95% quantile, 99% quantile and maximum error (MAXE), we take the average over benchmark data sets and plot the learning curves during BMDAL. LCMD-TP wins on all metrics except MAXE, where the uncertainty-based MaxDist and MaxDet excel. 16/ https://t.co/dgm2jmOWMV', 'From learning curves on individual data sets, we see that the utility of BMDAL over random selection varies a lot between data sets, although the best methods (LCMD-TP and KMeansPP-P) are never much worse than random selection. Can we predict the utility of BMDAL in advance? 17/ https://t.co/EqlWsoG9mH', 'Across our benchmark, we find that on data sets with larger quotient RMSE/MAE on the initial training set, LCMD-TP typically yields a larger benefit over random selection. Intuitively, the more the errors vary between samples, the more can be gained by careful selection. 18/ https://t.co/vphBMgbREg', 'By how much does the final accuracy of BMDAL methods deteriorate if we use fewer steps with larger batch sizes, such that we end up with the same number of points? The plot below shows that all methods except naive active learning are fairly robust to larger batch sizes. 19/ https://t.co/rZ0iOpJYgg', 'Our BMDAL framework is implemented in PyTorch and is easy to apply to custom NNs, as explained here: https://t.co/j9H8uv8jKw\n20/ https://t.co/XKIQgmgeNU']",https://arxiv.org/abs/2203.09410,"We study the performance of different pool-based Batch Mode Deep Active Learning (BMDAL) methods for regression on tabular data, focusing on methods that do not require to modify the network architecture and training. Our contributions are three-fold: First, we present a framework for constructing BMDAL methods out of kernels, kernel transformations and selection methods, showing that many of the most popular BMDAL methods fit into our framework. Second, we propose new components, leading to a new BMDAL method. Third, we introduce an open-source benchmark with 15 large tabular data sets, which we use to compare different BMDAL methods. Our benchmark results show that a combination of our novel components yields new state-of-the-art results in terms of RMSE and is computationally efficient. We provide open-source code that includes efficient implementations of all kernels, kernel transformations, and selection methods, and can be used for reproducing our results. ",A Framework and Benchmark for Deep Batch Active Learning for Regression
83,1504733441413242890,328430286,Jad C. Halimeh,['New paper <LINK> (3rd of 3 today). In a collaboration with @GoogleQuantumAI we simulate confinement dynamics and gauge symmetry stabilization in a Z2 gauge theory on a superconducting quantum chip. @ERC_Research @HaukeGroup @MCQST_cluster @QManyBody <LINK>'],https://arxiv.org/abs/2203.08905,"Digital quantum simulators provide a table-top platform for addressing salient questions in particle and condensed-matter physics. A particularly rewarding target is given by lattice gauge theories (LGTs). Their constituents, e.g., charged matter and the electric gauge field, are governed by local gauge constraints, which are highly challenging to engineer and which lead to intriguing yet not fully understood features such as confinement of particles. Here, we simulate confinement dynamics in a $\mathbb{Z}_2$ LGT on a superconducting quantum chip. We synthesize the charge--gauge-field interaction using only 6 native two-qubit gates, enabling us to reach simulation times of up to 25 Trotter steps. We observe how tuning a term that couples only to the electric field confines the charges, a manifestation of the tight bond that the local gauge constraint generates between both. Moreover, we study a different mechanism, where a modification of the gauge constraint from a $\mathbb{Z}_2$ to $U(1)$ symmetry freezes the system dynamics. Our work showcases the dramatic restriction that the underlying gauge constraint imposes on the dynamics of an LGT, it illustrates how gauge constraints can be modified and protected, and it paves the way for studying other models governed by many-body interactions. ","Probing confinement in a $\mathbb{Z}_2$ lattice gauge theory on a
  quantum computer"
84,1504733228585869336,328430286,Jad C. Halimeh,"['New paper <LINK> (2nd of 3 today). We show that the highest-excited ""extreme"" vacua of the massless Schwinger model are scarred states. We also uncover other rich scarring regimes at finite mass. @ERC_Research @MCQST_cluster @theoryleeds @QManyBody <LINK>']",https://arxiv.org/abs/2203.08830,"As a paradigm of weak ergodicity breaking in disorder-free nonintegrable models, quantum many-body scars (QMBS) can offer deep insights into the thermalization dynamics of gauge theories. Having been first discovered in a spin-$1/2$ quantum link formulation of the Schwinger model, it is a fundamental question as to whether QMBS persist for $S>1/2$ since such theories converge to the lattice Schwinger model in the large-$S$ limit, which is the appropriate version of lattice QED in one spatial dimension. In this work, we address this question by exploring QMBS in spin-$S$ $\mathrm{U}(1)$ quantum link models (QLMs) with staggered fermions. We find that QMBS persist at $S>1/2$, with the resonant scarring regime, which occurs for a zero-mass quench, arising from simple high-energy gauge-invariant initial states. We furthermore find evidence of detuned scarring regimes, which occur for finite-mass quenches starting in the physical vacua and the charge-proliferated state. Our results conclusively show that QMBS exist in a wide class of lattice gauge theories in one spatial dimension represented by spin-$S$ QLMs coupled to dynamical fermions. ",Weak Ergodicity Breaking in the Schwinger Model
85,1504732631904182285,328430286,Jad C. Halimeh,['New paper <LINK> (1st of 3 today). We show that experimentally feasible linear gauge protection schemes stabilize quantum many-body scars for all relevant timescales in U(1) and Z2 gauge theories. @ERC_Research @MCQST_cluster @QManyBody @HaukeGroup @aBohrdt <LINK>'],https://arxiv.org/abs/2203.08828,"Quantum many-body scarring is a paradigm of weak ergodicity breaking arising due to the presence of special nonthermal many-body eigenstates that possess low entanglement entropy, are equally spaced in energy, and concentrate in certain parts of the Hilbert space. Though scars have been shown to be intimately connected to gauge theories, their stability in such experimentally relevant models is still an open question, and it is generally considered that they exist only under fine-tuned conditions. In this work, we show through Krylov-based time-evolution methods how quantum many-body scars can be made robust in the presence of experimental errors through utilizing terms linear in the gauge-symmetry generator or a simplified pseudogenerator in $\mathrm{U}(1)$ and $\mathbb{Z}_2$ lattice gauge theories. Our findings are explained by the concept of quantum Zeno dynamics. Our experimentally feasible methods can be readily implemented in existing large-scale ultracold-atom quantum simulators and setups of Rydberg atoms with optical tweezers. ",Robust quantum many-body scars in lattice gauge theories
86,1504689555282313235,1120577870403911680,Torben Ferber,"['""Forecasting dark showers at Belle II"": New paper (<LINK>) with colleagues from @UBC , @RWTH , @Fermilab , @desynews  , and @KITKarlsruhe about strongly interacting dark sectors with dark quarks and cool experimental signatures both at the LHC and at Belle II. <LINK>']",https://arxiv.org/abs/2203.08824,"Dark showers from strongly interacting dark sectors that confine at the GeV scale can give rise to novel signatures at $e^+e^-$ colliders. In this work, we study the sensitivity of $B$ factory experiments to dark showers produced through an effective interaction arising from a heavy off-shell mediator. We show that a prospective search for displaced vertices from GeV-scale long-lived particles at Belle II can improve the sensitivity to dark showers substantially compared to an existing search at BaBar. We compare the sensitivity of searches for displaced signals to searches for promptly produced resonances at BaBar and KLOE and calculate sensitivity projections for a single-photon search at Belle II to invisible dark showers produced through an effective interaction. The underlying structure of the effective interaction can be resolved at higher-energy experiments, where the mediator can be produced on-shell. To study the resulting constraints, we update electroweak precision bounds on kinetically mixed $Z'$ bosons and reinterpret a search for low-mass di-muon resonances at LHCb in terms of dark showers. We find that LHCb and Belle II are most sensitive to different particle decay lengths, underscoring the complementarity of LHC and intensity frontier experiments. ",Forecasting dark showers at Belle II
87,1504649554603429888,1440653053321891843,Luis Alfredo Anchordoqui,['New paper on the arXiv today constraining  instanton-induced decay of SHDM with Auger data <LINK>'],https://arxiv.org/abs/2203.08854,"We investigate instanton-induced decay processes of super-heavy dark matter particles $X$ produced during the inflationary epoch. Using data collected at the Pierre Auger Observatory we derive a bound on the reduced coupling constant of gauge interactions in the dark sector: $\alpha_X^{\rm eff} \lesssim 0.09$, for $10^{10} < M_X/{\rm GeV} < 10^{16}$. We show that this upper limit on $\alpha_X^{\rm eff}$ is complementary to that obtained from the non-observation of tensor modes in the cosmic microwave background. ","Limits to gauge coupling in the dark sector set by the non-observation
  of instanton-induced decay of Super-Heavy Dark Matter in the Pierre Auger
  Observatory data"
88,1504639823184809984,22148802,Leo C. Stein ü¶Å,"[""üéâ New paper day! üéâ \n\nTidally-induced nonlinear resonances in EMRIs with an analogue model (<LINK>)\n\nThis is David's first paper! So, what did we study?\n1/6 <LINK>"", 'Orbits around spinning black holes have 3 frequencies, so there can be resonances. Adding a perturbation‚Äîe.g. a distant 3rd body‚Äîcan ""break"" resonant tori, creating nonlinear resonances. Here\'s what it looks like on a Poincar√© section. Our phase space is 6d ‚Üí 4d Poinc. sect.\n2/6 https://t.co/HQl3lOLS09', 'To visualize a 4-dimensional Poincar√© section, use 3 spatial dimensions, and color as a 4th dimension. Here is one for our system, a resonant torus that broke into a nonlinear resonance because of an external perturbation (the gravitational field of some distant stuff).\n3/6 https://t.co/AGnHAyU4oJ', 'Inside one of these resonances, we get libration of the ""resonance angle"" on a new time scale, seen below. So, what\'s the big idea in our paper? If we don\'t model this, will it screw up the ability of the LISA mission to detect systems that pass through resonance?\n4/6 https://t.co/26AnYhT0Hy', ""We computed the mismatch between signals where we do or don't attempt to model the nonlinear resonance, over a range of parameter space, to find out: how strong must the external perturbation be so that it *must be* modeled to get things right?\n\n5/6"", 'In the end, we found a simple approximate region of parameter space where the resonance must be modeled: Œµ ‚â≥ 300q¬≤, where q is the small mass ratio, and Œµ is a dimensionless measure of the strength of the perturbation.\n\nRead all about it here ‚û°Ô∏è https://t.co/GrWbb8uJ0F\n\n6/6ish', 'If you want to learn more about Poincar√© sections, check out this interactive web toy: https://t.co/AIGfyCcJoT', 'You can see our progress in this project by when I was tweeting about it long ago: https://t.co/CVzoT7n0zO']",https://arxiv.org/abs/2203.08841,"One of the important classes of targets for the future space-based gravitational wave observatory LISA is extreme mass ratio inspirals (EMRIs), where long and accurate waveform modeling is necessary for detection and characterization. When modeling the dynamics of an EMRI, several effects need to be included, such as the modifications caused by an external tidal field. The effects of such perturbations will generally break integrability at resonance, and can produce significant dephasing from an unperturbed system. In this paper, we use a Newtonian analogue of a Kerr black hole to study the effect of an external tidal field on the dynamics and the gravitational waveform. We have developed a numerical framework that takes advantage of the integrability of the background system to evolve it with a symplectic splitting integrator, and compute approximate gravitational waveforms to estimate the time scale over which the perturbation affects the dynamics. We find that different entry points into the resonance in phase-space can produce substantially different dynamics. Finally, by comparing this time scale with the inspiral time, we find tidal effects will need to be included when modeling EMRI gravitational waves when $\varepsilon \gtrsim 300\, q^2$, where $q$ is the small mass ratio, and $\varepsilon$ measures the strength of the external tidal field. ",Tidally-induced nonlinear resonances in EMRIs with an analogue model
89,1504459140231581703,822867138,Bradley Kavanagh,"['New paper out today, on the observation of Godzilla, an extremely magnified, very bright object in the strongly lensed Sunburst galaxy: <LINK> <LINK>', 'Godzilla is the blob labelled ""Tr"" in red in the top right of this image. It could be an intermediate-mass black hole (with accretion disk), or a luminous blue variable star. But it\'s probably not a supernova (it lasts too long). https://t.co/Vhei6JT3yi', 'The huge magnification of the image of Godzilla (Œº &gt; 1000) requires us to add a massive object (1e8 solar masses) into our lens model. We discuss the implications for different dark matter models (which might prevent the formation of DM structure around/below that mass-scale) https://t.co/Hmtc0lp5fn', '@ultra_wimp Haha tangentially. I was mostly involved in thinking about the consequences of lens substructure for DM models. But along the way I had to learn a little about critical curves and magnitudes...']",https://arxiv.org/abs/2203.08158,"We model the strong lensing effect in the galaxy cluster PSZ1 G311.65-18.48 (z=0.443) with an improved version of the hybrid method WSLAP+. We extend the number of constraints by including the position of critical points, which are combined with the classic positional constraints of the lensed galaxies. We pay special attention to a transient candidate source (Tr) previously discovered in the giant Sunburst arc (z=2.37). Our lens model predicts Tr to be within a fraction of an arcsecond from the critical curve, having a larger magnification factor than previously found, but still not large enough to explain the observed flux and lack of counterimages. Possible candidate counterimages are discussed that would lower the magnification required to explain Tr, but extreme magnification factors ($\mu>1000$) are still required, even in that case. The presence of a small mass perturber with a mass comparable to a dwarf galaxy ($M\sim 10^8 \,{\rm M}_{\odot}$) near the position of Tr is needed in order to explain the required magnification and morphology of the lensed galaxy. We discuss how the existence of this perturber could potentially be used to constrain models of dark matter. The large apparent brightness and unresolved nature of the magnified object implies a combination of extreme magnification and a very luminous and compact source ($r<0.3$ pc). Possible candidates are discussed, including an hyperluminous star or an accretion disc around an intermediate-mass black hole (IMBH). Based on spectral information, we argue that a luminous blue variable (LBV) star caught during an outburst is the most likely candidate. Owing to the extreme magnification and luminosity of this source we dub it Godzilla. ","Godzilla, a monster lurks in the Sunburst galaxy"
90,1504401145653039108,608020439,Dr. Gareth Cabourn Davies,"[""I'm happy to announce a new paper from myself and Ian Harry (also @UoPCosmology) where we look at calculating false alarm rates for gravitational-wave signals which can only be seen in one detector in PyCBC\nA (fairly long) üßµüëá\n\n<LINK>"", 'When we search for gravitational wave signals, we match filter the data from each detector\nSignals would show up at around the same time in all detectors, and so we give these coincidences a ranking statistic based on how much they look like signals vs noise https://t.co/qJjto4c3Km', 'Working out the false alarm rates usually involves creating a data set which we can be confident is noise\n\nSo we shift the triggers from each detector in time so that we are confident that they cannot be from a signal https://t.co/wiNGQ5XWxF', 'We then count how many of these fake signals are ranked higher than the possible real ones üßÆ https://t.co/nAPVTAKDEr', ""We can't do that with single-detector signals, as there is nothing to shift the triggers compared to!\n\nSo we take adapt our ranking statistic to remove the terms which depend on coincidence and extrapolate the distribution of these rankings using an exponential decay. https://t.co/df5HlYOMYz"", ""This works surprisingly well, and is almost perfect in Gaussian noise...\n\nBut the data isn't Gaussian, and we have random non-Gaussian transients in the data (glitches), which match well to gravitational-wave templates - handily explained here by @actualdrdoctor https://t.co/VdrwWhtled"", ""When requiring coincidence, because glitches aren't shared between detectors, these don't show up too much in our final results\n\nWithout this requirement they show up a lot\n\nSo we do some harsh cuts for single-detector events https://t.co/3kncTYRAtG"", 'So we use signal consistency tests, and where we would usually down-rank events, we throw them away\n\nAnd anything in a template which looks particularly badly behaved, we throw them out as well https://t.co/ilO4MNa2eL', ""So we now have a relatively clean set of triggers for use in our extrapolation, and we're back to being close to Gaussian noise again https://t.co/4j8esLuOJ8"", 'And this works! We see real events standing out from the background and being ranked really significantly!\n\nThe dotted lines in this graph show our extrapolation, and the scatter points show the ranking statistic and count of louder events for triggers https://t.co/RESXdeaXRR', 'And so we tried it on all of O3, and compared our results to other searches which search for single-detector signals\n\nWe find exactly the same events as the GWTC-2.1 and GWTC-3 papers! (https://t.co/o5jktDIPcM and https://t.co/DFQj8OaWve) https://t.co/DAVzlpt8PW', 'Now we do have methods to search for single-detector events in PyCBC already, as shown by @alexandernitz https://t.co/gnj7lsQPQb but these compute a probability of astrophysical origin (pastro) rather than the false alarm rate', 'This pastro calculation was used in 4-OGC (https://t.co/2WqmZA9MYF) and our results differ by by one event https://t.co/WRhrKMVOLs', 'We get these results while only identifying one glitch as an event with false alarm rate lower than two per year (1.7 per year). For an analysis of nearly a year of data, this is really good!']",https://arxiv.org/abs/2203.08545,"Gravitational-wave observations of compact binary coalescences are allowing us to see black holes and neutron stars further into the universe and recent results represent the most sensitive searches for compact objects ever undertaken. Most searches for gravitational waves from compact binary coalescence currently rely on detecting coincident triggers from multiple detectors. In this paper, we describe a new method for extrapolating significance of single-detector signals beyond the live-time of the analysis. Using this method, we can recover loud signals which only triggered in a single detector. We demonstrate this method in a search of O3 data, and recover seven single-detector events with a false alarm rate less than two per year. These were the same events as discovered in the GWTC-2.1 and GWTC-3 searches in a single detector, and all but one event from 3-OGC and 4-OGC. Through a campaign of injected signals, we estimate that the total time--volume sensitivity increases by a factor of up to $1.20 \pm 0.02$ at a false alarm rate of one per two years compared to completely ignoring single-detector events. ","Establishing significance of gravitational-wave signals from a single
  observatory in the PyCBC offline search"
91,1504400450413637635,914764593930625024,Stefania Ebli,['Discrete Morse theory meets Hodge theory üîÆ to compress and reconstruct signals on chain complexes. Wonder how? Check this out in our new paper! An amazing collaboration with the DMT team @kellymaggs7  @CeliaHacker_\n\n<LINK>'],https://arxiv.org/abs/2203.08571,"At the intersection of Topological Data Analysis (TDA) and machine learning, the field of cellular signal processing has advanced rapidly in recent years. In this context, each signal on the cells of a complex is processed using the combinatorial Laplacian, and the resultant Hodge decomposition. Meanwhile, discrete Morse theory has been widely used to speed up computations by reducing the size of complexes while preserving their global topological properties. In this paper, we provide an approach to signal compression and reconstruction on chain complexes that leverages the tools of algebraic discrete Morse theory. The main goal is to reduce and reconstruct a based chain complex together with a set of signals on its cells via deformation retracts, preserving as much as possible the global topological structure of both the complex and the signals. We first prove that any deformation retract of real degree-wise finite-dimensional based chain complexes is equivalent to a Morse matching. We will then study how the signal changes under particular types of Morse matching, showing its reconstruction error is trivial on specific components of the Hodge decomposition. Furthermore, we provide an algorithm to compute Morse matchings with minimal reconstruction error. ",Morse Theoretic Signal Compression and Reconstruction on Chain Complexes
92,1504266340072407043,1352219823539957762,Liam Dugan,"['Ever wanted to automatically generate flashcards from a textbook PDF? Our new paper ‚ÄúA Feasibility Study of Answer-Unaware Question Generation for Education‚Äù investigates how feasible this is given recent advancements!\n\n<LINK>\n\nThreadüëá <LINK>', ""Fully automatic generation of quizzes or flashcards necessitates ‚Äúanswer-unaware‚Äù QG models (i.e. ones that don't require manual selection of answer spans). These models have to decide what is and isn‚Äôt relevant to ask. This is non-trivial! (1/5) https://t.co/IG0kaTHPTw"", 'Unsurprisingly, these QG models (typically trained on datasets like SQuAD) tend to ask about topics that ""annotators are most likely to pick"". These topics are not necessarily the most educationally salient. Summarization helps solve this problem! (2/5)', 'Running QG on auto-summarized text allows us to restrict the model to only see sentences that are highly salient to the larger passage. This improves relevance of generated questions (61% -&gt; 78%). The effect is even more pronounced with human-summarized text (61% -&gt; 95%). (3/5) https://t.co/H7VIwZ0bzu', 'Also, since summaries tend to consist of simpler, more self-contained sentences, QG on summaries produces questions that tend to be more interpretable out of context (56% -&gt; 74%). Again, this effect is even larger when using human-written summaries as input (56% -&gt; 94%). (4/5)', 'These two factors lead to large increases in acceptability of generated Qs on summaries (83%) vs original text (33%) with no corresponding drop in bolded key-term coverage. \n\ntl;dr Automatic flashcard/quiz generation *is* currently feasible with min. supervisionüéâ (5/5) https://t.co/pLcf4i2Aq4', 'Don‚Äôt believe the results? Run our repo and try it out for yourself! \n\nhttps://t.co/OimFWV9D4U \n\nWe include a joint summarization and QG pipeline interface as well as .txt files of textbook chapters. We also provide our data and the code to reproduce our analysis.', '@abhisuri97 Oh you bet it can! Currently only takes .txt files as input but I you can pretty easily hook up some PDF extraction to run it on PDFs üòÅ']",https://arxiv.org/abs/2203.08685,"We conduct a feasibility study into the applicability of answer-agnostic question generation models to textbook passages. We show that a significant portion of errors in such systems arise from asking irrelevant or uninterpretable questions and that such errors can be ameliorated by providing summarized input. We find that giving these models human-written summaries instead of the original text results in a significant increase in acceptability of generated questions (33% $\rightarrow$ 83%) as determined by expert annotators. We also find that, in the absence of human-written summaries, automatic summarization can serve as a good middle ground. ",A Feasibility Study of Answer-Agnostic Question Generation for Education
93,1504239635220013058,2816527975,Hiroshi Kera,['Our new paper have appeared in arXiv. \n<LINK>'],https://arxiv.org/abs/2203.07138,"The vulnerability of convolutional neural networks (CNNs) to image perturbations such as common corruptions and adversarial perturbations has recently been investigated from the perspective of frequency. In this study, we investigate the effect of the amplitude and phase spectra of adversarial images on the robustness of CNN classifiers. Extensive experiments revealed that the images generated by combining the amplitude spectrum of adversarial images and the phase spectrum of clean images accommodates moderate and general perturbations, and training with these images equips a CNN classifier with more general robustness, performing well under both common corruptions and adversarial perturbations. We also found that two types of overfitting (catastrophic overfitting and robust overfitting) can be circumvented by the aforementioned spectrum recombination. We believe that these results contribute to the understanding and the training of truly robust classifiers. ",Adversarial amplitude swap towards robust image classifiers
94,1504080988300795905,1243709419,Patrick Shafto,"['New work led by @FelixCKLu1: ""On Connecting Deep Trigonometric Networks with Deep Gaussian Processes: Covariance, Expressivity, and Neural Tangent Kernel"". The paper presents a collection of new analytic results pointing in cool directions!\n\n<LINK> <LINK>']",https://arxiv.org/abs/2203.07411,"Deep Gaussian Process as a Bayesian learning model is promising because it is expressive and capable of uncertainty estimation. With Bochner's theorem, we can view the deep Gaussian process with squared exponential kernels as a deep trigonometric network consisting of the random feature layers, sine and cosine activation units, and random weight layers. Focusing on this particular class of models allows us to obtain analytical results. We shall show that the weight space view yields the same effective covariance functions which were obtained previously in function space. The heavy statistical tails can be studied with multivariate characteristic function. In addition, the trig networks are flexible and expressive as one can freely adopt different prior distributions over the parameters in weight and feature layers. Lastly, the deep trigonometric network representation of deep Gaussian process allows the derivation of its neural tangent kernel, which can reveal the mean of predictive distribution from the intractable inference. ","On Connecting Deep Trigonometric Networks with Deep Gaussian Processes:
  Covariance, Expressivity, and Neural Tangent Kernel"
95,1504064889899425802,1058011325316636672,Antoine Vendeville,"['New paper out on arxiv <LINK>! With @bguedj and @SZ_UCL we present 2 methods to ""depolarise"" social networks subject to a backfire effect. We apply them on the evolving network of members of the US House of Representatives since 1947 @voteview. 1/5', 'How does it work? Opinions evolve under the Voter Model with stubborn agents (zealots). We search for optimal amounts of zealots to (i) maximise the variance of opinions at equilibrium, or (ii) maximise the density of active links, ie edges connecting opposite-minded users. 2/5', 'In particular, we assume that the network is subject to a ""backfire effect"". Not all people are receptive to uncongenial information, and some will radicalise even more when presented with it. Formally, they may turn into zealots if they disagree with too many others. 3/5', 'What are the contributions? The density of active links at equilibrium for any directed weighted graph is given as the solution of a linear system. Problems (i) and (ii) are solved exactly in the complete graph. Methods for more general graphs are also discussed for (i). 4/5', 'What do we observe with the data? The variance of opinions (Democrat or Republican) is almost maximal. The density of active links is rather low. Depending on the intensity of the backfire effect, it is possible to efficiently control both at the same time! 5/5']",https://arxiv.org/abs/2203.02002,"As social networks are ubiquitous in everyday life, problems such as misinformation, bots and polarisation of opinion gain more and more importance. This paper focuses on the last one as we propose novel methods to reduce the amount of polarisation in a social group. We leverage the voter model in which users hold binary opinions and repeatedly update their beliefs based on others they connect with. Stubborn agents who never change their minds (""zealots"") are also disseminated through the network. We are interested in two equilibrium metrics, the diversity of opinions $\sigma$ and the density of active links $\rho$. The latter is a measure of the exposure to adverse opinions. We derive formulas to compute them that are valid in any directed, weighted network. Then we study the problem of finding optimal numbers of zealots in order to maximise these quantities. We account for the presence of a backfire effect, which may lead the group to react negatively and reinforce its level of polarisation in response. We provide exact solutions in the specific context of a complete unweighted graph, and propose a method for $\sigma$ in the general case. Finally we apply these problems to the network of the US House of Representatives. The network exhibits high levels of opinion diversity but lower levels of active links density. We find optimal numbers of zealots that maximise these quantities and show that both can be efficiently increased in some cases. ","Depolarising Social Networks: Optimisation of Exposure to Adverse
  Opinions in the Presence of a Backfire Effect"
96,1504055087026614272,386546724,AHEP Group,"['New #AHEP paper on arXiv contributing Pablo Mart√≠nez-Mirav√©, Mariam T√≥rtola and Jos√© W.F. Valle.\n\n""Synergy between cosmological and laboratory searches in neutrino physics: a white paper"" \n\nTake a look -&gt; <LINK>\n\n@pablommirave @MariamTortola @jwvalle <LINK>']",http://arxiv.org/abs/2203.07377,"The intersection of the cosmic and neutrino frontiers is a rich field where much discovery space still remains. Neutrinos play a pivotal role in the hot big bang cosmology, influencing the dynamics of the universe over numerous decades in cosmological history. Recent studies have made tremendous progress in understanding some properties of cosmological neutrinos, primarily their energy density. Upcoming cosmological probes will give higher precision on the energy density, but could also start probing other properties of the neutrino spectra. When convolved with results from terrestrial experiments, cosmology can become even more acute at probing new physics related to neutrinos or even Beyond the Standard Model (BSM). Any discordance between laboratory and cosmological data sets may reveal new BSM physics or suggest alternative models of cosmology. We give examples of the intersection between terrestrial and cosmological probes in the neutrino sector, and briefly discuss the possibilities of what different experiments may see in conjunction with cosmological observatories. ","Synergy between cosmological and laboratory searches in neutrino
  physics: a white paper"
97,1503757043630546953,1003652696723873792,Max Gaspari,['Check out the new extensive white paper done with the #LISAmission collaboration highlighting all the beautiful #astrophyiscs topics that we can tackle with a multi-messenger approach! (contributed to BH feeding &amp; feedback sections)\n<LINK>\n#gravitationalwaves <LINK>'],https://arxiv.org/abs/2203.06016,"Laser Interferometer Space Antenna (LISA) will be a transformative experiment for gravitational wave astronomy as it will offer unique opportunities to address many key astrophysical questions in a completely novel way. The synergy with ground-based and other space-based instruments in the electromagnetic domain, by enabling multi-messenger observations, will add further to the discovery potential of LISA. The next decade is crucial to prepare the astrophysical community for LISA's first observations. This review outlines the extensive landscape of astrophysical theory, numerical simulations, and astronomical observations that are instrumental for modeling and interpreting the upcoming LISA datastream. To this aim, the current knowledge in three main source classes for LISA is reviewed: ultra-compact stellar-mass binaries, massive black hole binaries, and extreme or intermediate mass ratio inspirals. The relevant astrophysical processes and the established modeling techniques are summarized. Likewise, open issues and gaps in our understanding of these sources are highlighted, along with an indication of how LISA could help make progress in the different areas. New research avenues that LISA itself, or its joint exploitation with studies in the electromagnetic domain, will enable, are also illustrated. Improvements in modeling and analysis approaches, such as the combination of numerical simulations and modern data science techniques, are discussed. This review is intended to be a starting point for using LISA as a new discovery tool for understanding our Universe. ",Astrophysics with the Laser Interferometer Space Antenna
98,1503752143735513096,216729597,Marcel S. Pawlowski,"['A bit late, but we had a new paper out on the arXiv last week, lead by Yanbin Yang: ""An extended stellar halo discovered in the Fornax dwarf spheroidal using Gaia EDR3""\n\n<LINK>', 'As the title says, we find evidence for an extended halo surrounding Fornax. By cutting in Gaia proper motions, parallaxes, and CMD, most of contamination by foreground stars and background QSOs could be avoided and thus very low (equivalent) surface brightnesses were reached. https://t.co/9ZiOabVMwx', 'The observed density profile shows a break at around 1.3¬∫ from the center of Fornax, and extends to 2.1¬∫ (5.4 kpc). The profile is well fit with a double-Sersic (blue line in the plot below), with a secondary component containing about 10% of the stars. https://t.co/hZSk6lWNUs', 'A possible interpretation is that the 2nd component is caused by stars spherically expanding from Fornax after it lost a major part of its gravity, e.g. due to gas stripping (which however assumes it was gas-rich and mostly dark matter free).', 'Will be interesting to see if internal proper motions might show such a preferential expansion motion in the future.']",https://arxiv.org/abs/2203.01953,"We have studied the extent of the Red Giant Branch stellar population in the Fornax dwarf spheroidal galaxy using the spatially extended and homogeneous data set from Gaia EDR3. Our preselection of stars belonging to Fornax is based on their proper motions, parallaxes and color-magnitude diagram. The latter criteria provide a Fornax star sample, which we further restrict by color and magnitude to eliminate contaminations due to either Milky Way stars or QSOs. The precision of the data has been sufficient to reach extremely small contaminations (0.02 to 0.3%), allowing us to reach to a background level 12 magnitudes deeper than the central surface brightness of Fornax. We discover a break in the density profile, which reveals the presence of an additional component that extents 2.1 degree in radius, i.e. 5.4 kpc, and almost seven times the half-light radius of Fornax. The extended new component represents 10% of the stellar mass of Fornax, and behaves like an extended halo. The absence of tidally elongated features at such an unprecedented depth (equivalent to $37.94\pm0.16$ mag ${\rm arcsec}^{-2}$ in V-band) rules out a possible role of tidal stripping. We suggest instead that Fornax is likely at first infall, and has lost its gas very recently, which consequently leads to a lack of gravity implying that residual stars have spherically expanded to form the newly discovered stellar halo of Fornax. ","An extended stellar halo discovered in the Fornax dwarf spheroidal using
  Gaia EDR3"
99,1503728410333392896,411037433,Alessandro Perelli,"['New work ""Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy CT Reconstruction"" with colleagues at @LatimU1101. MCAOL is able to exploit joint information in DECT reconstruction.\n\nCheck out the paper <LINK>\nand the code <LINK> <LINK>']",http://arxiv.org/abs/2203.05968,"Objective. Dual-energy computed tomography (DECT) has the potential to improve contrast, reduce artifacts and the ability to perform material decomposition in advanced imaging applications. The increased number or measurements results with a higher radiation dose and it is therefore essential to reduce either number of projections per energy or the source X-ray intensity, but this makes tomographic reconstruction more ill-posed. Approach. We developed the multi-channel convolutional analysis operator learning (MCAOL) method to exploit common spatial features within attenuation images at different energies and we propose an optimization method which jointly reconstructs the attenuation images at low and high energies with a mixed norm regularization on the sparse features obtained by pre-trained convolutional filters through the convolutional analysis operator learning (CAOL) algorithm. Main results. Extensive experiments with simulated and real computed tomography (CT) data were performed to validate the effectiveness of the proposed methods and we reported increased reconstruction accuracy compared to CAOL and iterative methods with single and joint total-variation (TV) regularization. Significance. Qualitative and quantitative results on sparse-views and low-dose DECT demonstrate that the proposed MCAOL method outperforms both CAOL applied on each energy independently and several existing state-of-the-art model-based iterative reconstruction (MBIR) techniques, thus paving the way for dose reduction. ","Multi-Channel Convolutional Analysis Operator Learning for Dual-Energy
  CT Reconstruction"
100,1503728070880071680,1291064871510069249,Calvin McPhail-Snyder,"[""I have a new paper up on the arXiv! <LINK>\n\nIt's not strictly expository (there are some new results) but it's mostly about explaining how two approaches to a problem in knot theory are equivalent. Here's a longer thread:"", 'Say you have a link L (disjoint union of circles up to isotopy) in S¬≥. Many links are *hyperbolic*: S¬≥ ‚àñ L has a complete finite-volume metric of curvature -1. This tells you very powerful information about L. I want to be able to compute with these hyperbolic structures.', ""One way to describe them is to give a representation œÄ‚ÇÅ(S¬≥ ‚àñ L) ‚Üí PSL‚ÇÇ(‚ÑÇ) that's discrete and faithful; in general I'm also interested in non-discrete/faithful representations, which we can think of as generalized hyperbolic structures."", ""This is direct and algebraic, but pretty hard to do in practice. Also, knowing the matrix coefficients doesn't tell you much about the geometry of the hyperbolic structure."", 'On the other hand, knowing the matrix coefficients is exactly what you need to compute the quantum invariants studied in my PhD thesis https://t.co/kUgn0mNpq7', 'There is a better way, due to Thurston. If you triangulate your link complement, you can describe the geometry of each tetrahedron, and there are equations to ""glue"" these together to a coherent geometry on S¬≥ ‚àñ L.', 'If you want to work with link diagrams (not triangulations) then you can use the ""octahedral decomposition"" to get a standard triangulation from a link diagram. The gluing equations for this decomposition have a nice form.\n https://t.co/PIrb0h5HZU', 'However, it turns out that these are equivalent! If you use the coordinates on SL‚ÇÇ(‚ÑÇ)-reps from quantum ùî∞ùî©‚ÇÇ they turn out to be equivalent to the shape coordinates from the octahedral decomposition.', ""Explaining this equivalence is why I wrote the paper, so if you want to know more take a look! It's designed to be pretty accessible if you know basic knot theory."", '@arun_bassoon Thanks! One goal of this is to give a better theory of string diagrams for hyperbolic links (and manifolds, via surgery) which is something you might be interested in']",https://arxiv.org/abs/2203.06042,"Hyperbolic structures (equivalently, principal $\operatorname{PSL}_2(\mathbb C)$-bundles with connection) on link complements can be described algebraically by using the octahedral decomposition, which assigns an ideal triangulation to any diagram of the link. The decomposition (like any ideal triangulation) gives a set of gluing equations in shape parameters whose solutions are hyperbolic structures. We show that these equations are closely related to a certain presentation of the Kac-de Concini quantum group $\mathcal{U}_q(\mathfrak{sl}_2)$ in terms of cluster algebras at $q = \xi$ a root of unity. Specifically, we identify ratios of the shape parameters of the octahedral decomposition with central characters of $\mathcal{U}_\xi(\mathfrak{sl}_2)$. The quantum braiding on these characters is known to be closely related to $\operatorname{SL}_2(\mathbb C)$-bundles on link complements, and our work provides a geometric perspective on this construction. ","Hyperbolic structures on link complements, octahedral decompositions,
  and quantum $\mathfrak{sl}_2$"
101,1503714803143094279,1406514069536919552,Niseem Magdy,['Our new experimental paper is finally out :)\n\nCentrality and transverse momentum dependence of higher-order flow harmonics of identified hadrons in Au+Au collisions at  200 GeV\n\n<LINK>'],https://arxiv.org/abs/2203.07204,"We present high-precision measurements of elliptic, triangular, and quadrangular flow $v_{2}$, $v_{3}$, and $v_{4}$, respectively, at midrapidity ($|\eta|<1.0$) for identified hadrons $\pi$, $p$, $K$, $\varphi$, $K_s$, $\Lambda$ as a function of centrality and transverse momentum in Au+Au collisions at the center-of-mass energy $\sqrt{s_{\rm NN}}=$ 200 GeV. We observe similar $v_{n}$ trends between light and strange mesons which indicates that the heavier strange quarks flow as strongly as the lighter up and down quarks. The number-of-constituent-quark scaling for $v_{2}$, $v_{3}$, and $v_{4}$ is found to hold within statistical uncertainty for 0-10$\%$, 10-40$\%$ and 40-80$\%$ collision centrality intervals. The results are compared to several viscous hydrodynamic calculations with varying initial conditions, and could serve as an additional constraint to the development of hydrodynamic models. ","Centrality and transverse momentum dependence of higher-order flow
  harmonics of identified hadrons in Au+Au collisions at $\sqrt{s_{\rm NN}}$ =
  200 GeV"
102,1503700997566308352,952949678533849088,Kareem El-Badry,"['New paper! We study two ‚Äúmass gap‚Äù black hole candidates in binaries with red giant stars, ‚Äúthe Unicorn‚Äù and ‚Äúthe Giraffe‚Äù. 1/n <LINK> <LINK>', 'We used spectral disentangling to search for possible luminous companions (as opposed to BHs) to the giants. We found them! 2/n https://t.co/j5WPzU3ikL', 'What that means, roughly, is that two luminous stars do (much) a better job fitting the spectra than one. https://t.co/bogv9ie8nu', 'In both systems, the disentangled spectra of the companions look like subgiant stars (i.e., cooler and larger than main-sequence stars; warmer and smaller than giants). 4/n https://t.co/2cdMaFwDn2', 'Because these subgiants are cooler than main-sequence stars of similar mass, they are faint in the UV and consistent with the observed spectral energy distributions and UV limits. 5/n https://t.co/S20JlBiznj', 'We can measure the masses of the giants from the observed ellipsoidal variation. In both systems, they are 0.3-0.5 Msun. This is very low for a giant, and implies most of their initial mass was stripped off by a companion. 6/n https://t.co/jpx3Pk4kkh', ""We can also measure the masses of the subgiants dynamically. The dynamically-inferred values (1.8 and 2.8 Msun) are in reasonably good agreement with what we'd estimate from their temperature and luminosity. 7/n https://t.co/KSLzPBlDdm"", ""We used binary evolution models to investigate how these systems formed and how they'll evolve in the future. We think the giant are almost completely stripped of their envelopes and will soon contract to become low-mass helium white dwarfs. 8/n https://t.co/NKLLw40ob0"", ""This scenario (and the component masses) is almost identical to how we think Regulus, the ~20th brightest star in the sky, formed. It's a main-sequence star with a helium white dwarf companion in a wide orbit. https://t.co/nk0iFv0E9K 9/n"", 'The fact that the companions are subgiants (i.e, off the main sequence) implies that either the initial mass ratio was very close to 1 (like, q&gt;0.99), or that the companions are temporarily inflated due to rapid accretion. 9/', 'The second possibility is particularly exciting, but it will take more work (ideally a population model of interacting giant binaries) to test it.', 'The Unicorn and Giraffe join a growing population of mass-transfer binaries recently observed at various stages of the stripping process. Several of these other objects have also been previously interpreted as BHs. https://t.co/sxKQRKlkSb', 'Summary: stellar-mass BHs are small needles in a very large haystack. But they haystack contains a lot of other interesting stuff! n/n']",https://arxiv.org/abs/2203.06348,"We analyze two binary systems containing giant stars, V723 Mon (""the Unicorn"") and 2M04123153+6738486 (""the Giraffe""). Both giants orbit more massive but less luminous companions, previously proposed to be mass-gap black holes. Spectral disentangling reveals luminous companions with star-like spectra in both systems. Joint modeling of the spectra, light curves, and spectral energy distributions robustly constrains the masses, temperatures, and radii of both components: the primaries are luminous, cool giants ($T_{\rm eff,\,giant} = 3,800\,\rm K$ and $4,000\,\rm K$, $R_{\rm giant}= 22.5\,R_{\odot}$ and $25\,R_{\odot}$) with exceptionally low masses ($M_{\rm giant} \approx 0.4\,M_{\odot}$) that likely fill their Roche lobes. The secondaries are only slightly warmer subgiants ($T_{\rm eff,\,2} = 5,800\,\rm K$ and $5,150\,\rm K$, $R_2= 8.3\,R_{\odot}$ and $9\,R_{\odot}$) and thus are consistent with observed UV limits that would rule out main-sequence stars with similar masses ($M_2 \approx 2.8\,M_{\odot}$ and $\approx 1.8\,M_{\odot}$). In the Unicorn, rapid rotation blurs the spectral lines of the subgiant, making it challenging to detect even at wavelengths where it dominates the total light. Both giants have surface abundances indicative of CNO processing and subsequent envelope stripping. The properties of both systems can be reproduced by binary evolution models in which a $1-2\,M_{\odot}$ primary is stripped by a companion as it ascends the giant branch. The fact that the companions are also evolved implies either that the initial mass ratio was very near unity, or that the companions are temporarily inflated due to rapid accretion. The Unicorn and Giraffe offer a window into into a rarely-observed phase of binary evolution preceding the formation of wide-orbit helium white dwarfs, and eventually, compact binaries containing two helium white dwarfs. ","Unicorns and Giraffes in the binary zoo: stripped giants with subgiant
  companions"
103,1503655059602694150,40754053,stefano maria iacus,['Compliance with restrictive measures during the COVID-19 pandemic in Europe: Does political partisanship influence behavioural responses? New paper with @spiros2 M. Scipioni @gtinto <LINK> <LINK>'],https://arxiv.org/abs/2203.05288,"The success of public health policies aimed at curtailing the COVID-19 pandemic have relied on large-scale and protracted compliance by the public. A series of studies have recently argued that previous voting patterns are important predictors of such compliance. Our research further investigates such connection by tracking the relationships between parties' vote shares and mobility in six European countries over an extended period of time. We observe that while vote shares are occasionally related to variations in mobility within each country, there is no systematic pattern of decrease or increase in mobility across all six selected countries depending on party family or government membership. Over time, the relationships between mobility and vote shares tend to grow stronger in some but not all countries, again suggesting that there is no clear connection between vote shares for several party families and compliance with social distancing measures. ","Compliance with restrictive measures during the COVID-19 pandemic in
  Europe: Does political partisanship influence behavioural responses?"
104,1503641142855938049,3236251346,Mikel Sanz,"['New paper today <LINK> in which we study the limits of cv microwave entanglement distribution in open air. Great #qmics collaboration with WMI, @YasserOmar_ @mpmotton @SalariVahid congrats Tasio for this work! @NquireC @QuantumFlagship @QUANTEK2122 @ehuscientia <LINK>']",https://arxiv.org/abs/2203.07295,"Microwave technology plays a central role in current wireless communications, standing among them mobile communication and local area networks (LANs). The microwave range shows relevant advantages with respect to other frequencies in open-air transmission, such as low absorption losses and low energy consumption, and it is additionally the natural working frequency in superconducting quantum technologies. Entanglement distribution between separate parties is at the core of secure quantum communications. Therefore, understanding its limitations in realistic open-air settings, specially in the rather unexplored microwave regime, is crucial for transforming microwave quantum communications into a mainstream technology. Here, we investigate the feasibility of an open-air entanglement distribution scheme with microwave two-mode squeezed states. First, we study the reach of direct entanglement transmission in open-air, obtaining a maximum distance of approximately 500 meters in a realistic setting with state-of-the-art experimental parameters. Afterwards, we adapt entanglement distillation and entanglement swapping protocols to microwave technology in order to reduce environmental entanglement degradation. While entanglement distillation helps to increase quantum correlations in the short-distance low-squeezing regime by up to $46\%$, entanglement swapping increases the reach by $14\%$. Then, we compute the fidelity of a continuous-variable quantum teleportation protocol using open-air-distributed entanglement as a resource. Finally, we adapt the machinery to explore the limitations of quantum communication between satellites, where the thermal noise impact is substantially reduced and diffraction losses are dominant. ",Open-Air Microwave Entanglement Distribution for Quantum Teleportation
105,1503633586284597248,1064646989869187072,Ivan Di Liberti,"['New paper, j/w with @AxelOsmond. Today on the arXiv.\n<LINK> <LINK>', 'https://t.co/IsuD8cH2fl', 'https://t.co/ew4YA6g5RH', 'https://t.co/8A6ZZsp5Iu', 'https://t.co/eL86sU8AAv', 'https://t.co/K1G2KMbwxJ']",https://arxiv.org/abs/2203.07046,"We develop a 2-dimensional version of accessibility and presentability compatible with the formalism of flat pseudofunctors. First we give prerequisites on the different notions of 2-dimensional colimits, filteredness and cofinality; in particular we show that \emph{$\sigma$-filteredness} and \emph{bifilteredness} are actually equivalent in practice for our purposes. Then, we define bi-accessible and bipresentable 2-categories in terms of \emph{bicompact} objects and \emph{bifiltered} bicolimits. We then characterize them as categories of \emph{flat pseudofunctors}. We also prove a bi-accessible right bi-adjoint functor theorem and deduce a 2-dimensional Gabriel-Ulmer duality relating small \emph{bilex} 2-categories and finitely bipresentable 2-categories. Finally, we show that 2-categories of pseudo-algebras of finitary 2-monads on $\Cat$ are finitely bipresentable, which in particular captures the case of $\Lex$, the 2-category of small lex categories. Invoking the technology of \emph{lex-colimits}, we prove further that several 2-categories arising in categorical logic (\textbf{Reg, Ex, Coh, Ext, Adh, Pretop}) are also finitely bipresentable. ",Bi-accessible and bipresentable 2-categories
106,1503549479072550914,929495187507859458,Sunita Chandrasekaran,"['VOILA ü•≥üôÉüòéü§ìüòá\n\nExcited to share our paper ""First Experiences in Performance Benchmarking with the New SPEChpc 2021 Suites"" \nAccepted to #CCGrid 2022üòç\n@verolero86 @cflorina @GuidoJuckeland @HenschelRobert @nicejunjie et. al\n\n<LINK> \n\nKUDOS you FABULOUS team ü•≥ <LINK>']",https://arxiv.org/abs/2203.06751,"Modern HPC systems are built with innovative system architectures and novel programming models to further push the speed limit of computing. The increased complexity poses challenges for performance portability and performance evaluation. The Standard Performance Evaluation Corporation -SPEC has a long history of producing industry standard benchmarks for modern computer systems. SPEC is a newly released SPEChpc 2021 benchmark suites, developed by the High Performance Group, are a bold attempt to provide a fair and objective benchmarking tool designed for state of the art HPC systems. With the support of multiple host and accelerator programming models, the suites are portable across both homogeneous and heterogeneous architectures. Different workloads are developed to fit system sizes ranging from a few compute nodes to a few hundred compute nodes. In this manuscript, we take a first glance at these benchmark suites and evaluate their portability and basic performance characteristics on various popular and emerging HPC architectures, including x86 CPU, NVIDIA GPU, and AMD GPU. This study provides a first-hand experience of executing the SPEChpc 2021 suites at scale on production HPC systems, discusses real-world use cases, and serves as an initial guideline for using the benchmark suites. ","First Experiences in Performance Benchmarking with the New SPEChpc 2021
  Suites"
107,1503547971815813121,994778954241380352,Nick Mayhall,"['2nd paper on pulse-level VQE on the arxiv! <LINK>  Ayush and Chenxu explore the minimal state preparation time using quantum optimal control - discover a new way to speed up state preparation! @AyushAsthana92 @see_quantum @oinamslair @VT_Science @Chenxu_liu_Phd', '(just in time for Ayush to talk about it tomorrow at #apsmarch )']",https://arxiv.org/abs/2203.06818,"Quantum simulation on NISQ devices is severely limited by short coherence times. A variational pulse-shaping algorithm known as ctrl-VQE was recently proposed to address this issue by eliminating the need for parameterized quantum circuits, which lead to long state preparation times. Here, we find the shortest possible pulses for ctrl-VQE to prepare target molecular wavefunctions for a given device Hamiltonian describing coupled transmon qubits. We find that the time-optimal pulses that do this have a bang-bang form consistent with Pontryagin's maximum principle. We further investigate how the minimal state preparation time is impacted by truncating the transmons to two versus more levels. We find that leakage outside the computational subspace (something that is usually considered problematic) speeds up the state preparation, further reducing device coherence-time demands. This speedup is due to an enlarged solution space of target wavefunctions and to the appearance of additional channels connecting initial and target states. ","Minimizing state preparation times in pulse-level variational molecular
  simulations"
108,1503544548307320832,1015053310603284480,Stephen Kane,"['I\'d like to highlight a new paper led by my student Emilie Simpson entitled ""Revisiting BD-06 1339b: A Likely False Positive Caused by Stellar Activity"". You can access the paper here: <LINK>', 'Stellar activity can pose a challenge for radial velocity (RV) surveys, often creating an effective noise floor for detection thresholds. Worse yet, periodic activity can masquerade as planetary signatures, creating potential sources of false positives.', ""Photometry can help to detect such activity signatures, but ground-based photometry can't always achieve the necessary precision to tease out the subtle variations. TESS provides an opportunity to revisit many of the known RV planets to determine if activity may be at play."", 'Our TESS survey of the known hosts revealed one such case: BD-06 1339b, reported as a 3.9 day giant planet orbiting an active star. Analysis of the TESS photometry and updated RV data showed that the photometric and RV variability are almost perfectly in phase. https://t.co/uxHNDqli7O', 'We conclude that unfortunately this particular case appears to be a product of activity from the host star. Having worked on both transit and RV surveys for many years, I can attest to how difficult a problem this is, and I greatly admire my colleagues trying to solve it.', 'Stay tuned for other results from our survey, particularly from Emilie Simpson and Tara Fetherolf who are doing an amazing job leading this work! Also, hats off to @PepperJoshua and @zhexingli for their important contributions!']",https://arxiv.org/abs/2203.06191,"As long as astronomers have searched for exoplanets, the intrinsic variability of host stars has interfered with the ability to reliably detect and confirm exoplanets. One particular source of false positives is the presence of stellar magnetic or chromospheric activity that can mimic the radial-velocity reflex motion of a planet. Here we present the results of a photometric data analysis for the known planet hosting star, BD-06 1339, observed by the Transiting Exoplanet Survey Satellite (TESS) during Sector 6 at 2 minute cadence. We discuss evidence that suggests the observed 3.9 day periodic radial velocity signature may be caused by stellar activity rather than a planetary companion, since variability detected in the photometric data are consistent with the periodic signal. We conclude that the previously reported planetary signature is likely the result of a false positive signal resulting from stellar activity, and discuss the need for more data to confirm this conclusion. ","Revisiting BD-06 1339b: A Likely False Positive Caused by Stellar
  Activity"
109,1503392824494071815,795318493675712512,Maura Pintor,"['üìåNew preprint available!\n\n""ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches.""\n\npaper: <LINK>\ncode: <LINK>\n\nw/ @DAngioni97  @biggiobattista @zangobot @ambrademontis @sotgiu_angelo <LINK>', '#mlsec #machinelearning #adversarial #ml #advml #cybersecurity #benchmark\n@adversarial_ML @mlsec_lab']",https://arxiv.org/abs/2203.04412,"Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at this https URL ","ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness
  against Adversarial Patches"
110,1503380960259178497,990478024188485633,Yukei Murakami,['New paper on arxiv!\nWe present lightcurves and analyses of 70 stripped-envelope supernovae. This is one of the first large analyses of the rise-time of such SNe. We discuss the implied ejecta mass and velocity through multi-band light curve modeling. \n<LINK>'],https://arxiv.org/abs/2203.05596,"We present BVRI and unfiltered Clear light curves of 70 stripped-envelope supernovae (SESNe), observed between 2003 and 2020, from the Lick Observatory Supernova Search (LOSS) follow-up program. Our SESN sample consists of 19 spectroscopically normal SNe~Ib, two peculiar SNe Ib, six SN Ibn, 14 normal SNe Ic, one peculiar SN Ic, ten SNe Ic-BL, 15 SNe IIb, one ambiguous SN IIb/Ib/c, and two superluminous SNe. Our follow-up photometry has (on a per-SN basis) a mean coverage of 81 photometric points (median of 58 points) and a mean cadence of 3.6d (median of 1.2d). From our full sample, a subset of 38 SNe have pre-maximum coverage in at least one passband, allowing for the peak brightness of each SN in this subset to be quantitatively determined. We describe our data collection and processing techniques, with emphasis toward our automated photometry pipeline, from which we derive publicly available data products to enable and encourage further study by the community. Using these data products, we derive host-galaxy extinction values through the empirical colour evolution relationship and, for the first time, produce accurate rise-time measurements for a large sample of SESNe in both optical and infrared passbands. By modeling multiband light curves, we find that SNe Ic tend to have lower ejecta masses and lower ejecta velocities than SNe~Ib and IIb, but higher $^{56}$Ni masses. ","The Lick Observatory Supernova Search follow-up program: photometry data
  release of 70 stripped-envelope supernovae"
111,1503340771784675330,527806509,Victoria Hodge,"['üìÑ New paper on assuring the safety of geolocation for robotics\nby @VJHodge, @RDHawkins, @jameshilderyork &amp; @IHabli\nfrom @AAIP_York and @UoY_CS \n\n<LINK>']",https://arxiv.org/abs/2203.05830,"There is a desire to move towards more flexible and automated factories. To enable this, we need to assure the safety of these dynamic factories. This safety assurance must be achieved in a manner that does not unnecessarily constrain the systems and thus negate the benefits of flexibility and automation. We previously developed a modular safety assurance approach, using safety contracts, as a way to achieve this. In this case study we show how this approach can be applied to Autonomous Guided Vehicles (AGV) operating as part of a dynamic factory and why it is necessary. We empirically evaluate commercial, indoor fog/edge localisation technology to provide geofencing for hazardous areas in a laboratory. The experiments determine how factors such as AGV speeds, tag transmission timings, control software and AGV capabilities affect the ability of the AGV to stop outside the hazardous areas. We describe how this approach could be used to create a safety case for the AGV operation. ","Analysing Ultra-Wide Band Positioning for Geofencing in a Safety
  Assurance Context"
112,1503316842458361865,1019009435283357696,Cian Eastwood,"['New short paper: ""Align-Deform-Subtract: An Interventional Framework for Explaining Object Differences""\n\nwith @li_nanbo and CKI Williams\n\nüîó <LINK>\nüßµüëá', '@li_nanbo By viewing image-space semantic alignments (see GIF above!) as counterfactual interventions on the underlying object properties, we explain object-image differences in terms of the underlying object properties (e.g. pose, shape, appearance)', '@li_nanbo In particular, we use the magnitude of the aligning transforms to quantify object-property differences, with the rigid alignment (""align"") quantifying pose differences, and the subsequent non-rigid alignment/deformation (""deform"") quantifying shape differences', '@li_nanbo The result is a set of ""disentangled"" error measures which provide a fine-grained explanation of object differences: scaling factor s; translation Œît; rotation ŒîŒ∏; shape/deformation Œîd; and appearance Œîa https://t.co/rjgPsRsJvi', 'https://t.co/2fmdLKEWo6']",https://arxiv.org/abs/2203.04694,"Given two object images, how can we explain their differences in terms of the underlying object properties? To address this question, we propose Align-Deform-Subtract (ADS) -- an interventional framework for explaining object differences. By leveraging semantic alignments in image-space as counterfactual interventions on the underlying object properties, ADS iteratively quantifies and removes differences in object properties. The result is a set of ""disentangled"" error measures which explain object differences in terms of their underlying properties. Experiments on real and synthetic data illustrate the efficacy of the framework. ","Align-Deform-Subtract: An Interventional Framework for Explaining Object
  Differences"
113,1503304036275138568,1443590603757867010,Callum Murphy-Barltrop (he/him),"['New paper up on arXiv, presenting a new model for capturing non-stationarity in extremal dependence structures! Have a read here: <LINK>']",https://arxiv.org/abs/2203.05860,"In many practical applications, evaluating the joint impact of combinations of environmental variables is important for risk management and structural design analysis. When such variables are considered simultaneously, non-stationarity can exist within both the marginal distributions and dependence structure, resulting in complex data structures. In the context of extremes, few methods have been proposed for modelling trends in extremal dependence, even though capturing this feature is important for quantifying joint impact. Motivated by the increasing dependence of data from the UK Climate Projections, we propose a novel semi-parametric modelling framework for bivariate extremal dependence structures. This framework allows us to capture a wide variety of dependence trends for data exhibiting asymptotic independence. When applied to the climate projection dataset, our model is able to capture observed dependence trends and, in combination with models for marginal non-stationarity, can be used to produce estimates of bivariate risk measures at future time points. ",Modelling non-stationarity in asymptotically independent extremes
114,1503284629524008960,1263870728870469632,Enrico Ronca,['Our new paper about Ionization Processes in QED Environments is now out on ArXiv.\n\n<LINK>'],https://arxiv.org/abs/2203.06050,"The ionization of molecular systems is important in many chemical processes such as electron transfer and hot electron injection. Strong coupling between molecules and quantized fields (e.g. inside optical cavities) represents a new promising way to modify molecular properties in a non-invasive way. Recently, strong light-matter coupling has shown the potential to significantly improve the rates of hot electron driven processes, for instance in water splitting. In this paper, we demonstrate that inside an optical cavity the residual interaction between an outgoing free electron and the vacuum field is significant. We further show that, since the quantized field is also interacting with the ionized molecule, the free electron and the molecular system are correlated. We develop a theoretical framework to account for the field induced correlation and show that the interaction between the free electron and the field free electron-field interaction has sizeable effects on the ionization potential of typical organic molecules. ",On the characteristic features of ionization in QED environments
115,1503243036628701185,1447413830611468291,Harshitra Mahalingam,"['New paper with @hbhillol and @RodinAleksandr! We introduce GrapheneQFT.jl, a #julialang numerical package for QFT calculations in graphene systems with defects. Paper: <LINK>.  . Repository: <LINK>.  .üßµ:', ""GrapheneQFT.jl is designed to be fast, easy to use and handles multiple defects simultaneously. Defect examples include hopping and onsite energy modifications, magnetic impurities and external states coupled to graphene. Let's calculate some quantities!"", 'Here is the spatial spectral function for a system with an ImpurityState at the origin. It is coupled to the atom below it and its neighbours. This is achieved with 13 lines of code (plotting included!) and takes ~50s to run. https://t.co/EJgybiIcGr', ""What if we wanted to calculate the change in the system's free energy in the presence of a defect with variable coupling strength? We can tune the system's chemical potential as well to understand the bigger picture. https://t.co/SIKu023fu2"", 'For more examples, take a look at the documentation, available at https://t.co/knFqzmu3cR. We look forward to seeing exciting defect systems in graphene being explored, as well as comments and suggestions!']",https://arxiv.org/abs/2203.05741,"We introduce a computationally efficient method based on the path integral formalism to describe defect-modified graphene. By taking into account the entire Brillouin zone, our approach respects the lattice symmetry and can be used to investigate both short-range and long-range effects. The proposed method's key advantage is that the computational complexity does not increase with the system size, scaling, instead, with the number of defects. As a demonstration of our method, we explore the graphene-mediated RKKY interaction between multiple magnetic impurities. Our results concur with earlier findings by showing that the interaction strength and sign depend on various factors like impurity separation, sublattice arrangement, and system doping. We demonstrate that frustration can be introduced between the impurity spins by controlling their relative positions and that this frustration can be switched on and off by tuning the chemical potential of the system. ","Numerical package for QFT calculations of defect-induced phenomena in
  graphene"
116,1502218034160807940,1134375290581524480,Kai Schmitz,"['New paper on the @arxiv: <LINK>, a #Snowmass White Paper on baryogenesis, in which I review our work on wash-in leptogenesis 2011.09347 and leptoflavorgenesis 2111.03082 (see Sec. 2.3). Also, this is my 20th preprint since joining @CERN in September 2019. Yay! ü•≥']",https://arxiv.org/abs/2203.05010,"The Standard Model of Particle Physics cannot explain the observed baryon asymmetry of the Universe. This observation is a clear sign of new physics beyond the Standard Model. There have been many recent theoretical developments to address this question. Critically, many new physics models that generate the baryon asymmetry have a wide range of repercussions for many areas of theoretical and experimental particle physics. This white paper provides an overview of such recent theoretical developments with an emphasis on experimental testability. ",New Ideas in Baryogenesis: A Snowmass White Paper
117,1502182067827843073,1324428524,Rikard Enberg,"[""New paper ‚Äì a contribution to Snowmass 2021: The Forward Physics Facility at the High-Luminosity LHC. It's 429 pages, 236 authors. My small contribution: I cowrote about 3 pages on high energy atmospheric neutrinos. <LINK>""]",https://arxiv.org/abs/2203.05090,"High energy collisions at the High-Luminosity Large Hadron Collider (LHC) produce a large number of particles along the beam collision axis, outside of the acceptance of existing LHC experiments. The proposed Forward Physics Facility (FPF), to be located several hundred meters from the ATLAS interaction point and shielded by concrete and rock, will host a suite of experiments to probe Standard Model (SM) processes and search for physics beyond the Standard Model (BSM). In this report, we review the status of the civil engineering plans and the experiments to explore the diverse physics signals that can be uniquely probed in the forward region. FPF experiments will be sensitive to a broad range of BSM physics through searches for new particle scattering or decay signatures and deviations from SM expectations in high statistics analyses with TeV neutrinos in this low-background environment. High statistics neutrino detection will also provide valuable data for fundamental topics in perturbative and non-perturbative QCD and in weak interactions. Experiments at the FPF will enable synergies between forward particle production at the LHC and astroparticle physics to be exploited. We report here on these physics topics, on infrastructure, detector, and simulation studies, and on future directions to realize the FPF's physics potential. ",The Forward Physics Facility at the High-Luminosity LHC
118,1502100535620550659,549460404,ÂêâÁî∞ Á¥Ö (Beni Yoshida),['A new (short) paper:\n<LINK>\n\nThis is just an expansion of section 11 of arXiv:2109.08691'],https://arxiv.org/abs/2203.04968,"We study the effect of projective measurements on the entanglement structure of quantum black holes. It is shown that the entanglement verification in monitored quantum circuits, recently discussed in condensed matter physics, is equivalent to the information recovery from a black hole with projective measurements. This correspondence provides useful predictions about non-perturbative effects on quantum gravity and some insights on the black hole interior as well as the final state proposal. ",Projective measurement of black holes
119,1502050684040105990,429494244,M Charity,"[""Hi y'all! Our paper for Baba is Y'all v2 is out on arXiv! You can read about the new site and the user study results here: <LINK>"", 'We talk about the new user-friendly updates to the site as well as the more developed back-end AI assistant to help users make better levels more efficiently. We also conducted a user study and received over 75 survey responses that described their experience with the site. https://t.co/jv2wnYkvma', 'You can also make your own levels with the help of an AI assistant! Contribute to the ever-growing database here: https://t.co/5K3GHaPnbi', ""We've also released the Keke AI Competition which uses levels directly from the site! You can make your AI solver for the Baba is Y'all levels: https://t.co/OLcVJsXPO5""]",https://arxiv.org/abs/2203.02035,"This paper describes a new version of the mixed-initiative collaborative level designing system: Baba is Y'all, as well as the results of a user study on the system. Baba is Y'all is a prototype for AI-assisted game design in collaboration with others. The updated version includes a more user-friendly interface, a better level-evolver and recommendation system, and extended site features. The system was evaluated via a user study where participants were required to play a previously submitted level from the site and then create their own levels using the editor. They reported on their individual process creating the level and their overall experience interacting with the site. The results have shown both the benefits and limitations of a mixed-initiative system and how it can help with creating a diversity of `Baba is You' levels that are both human and AI designed while maintaining their quality. ","Baba is Y'all 2.0: Design and Investigation of a Collaborative
  Mixed-Initiative System"
120,1502034648582426647,184420792,Oscar Higgott,"['Which surface code variant should we use for handling biased noise in planar architectures? We address this question in our new paper on arXiv today: <LINK>\n\nThis is joint work with Thom Bohdanowicz, Alex Kubica, @S_Flammia and @earltcampbell at @awscloud 1/ <LINK>', 'We compare two leading approaches to handling biased noise in planar architectures: the XY surface code and the CSS surface code. 2/ https://t.co/MxIC9pRLaJ', 'We introduce a new decoder, belief-matching, which we show has good performance for handling biased circuit-level noise in the XY surface code, and also outperforms the MWPM decoder for depolarising noise in the CSS surface code. 3/ https://t.co/hSnaAp2lC1', 'Using belief-matching, we find that the XY surface code has an improved threshold, but has a larger qubit overhead than the rectangular CSS surface code below threshold. 4/ https://t.co/bY113PVHoK', 'We identify failure mechanisms that inhibit the performance of the XY surface code, which we call fragile boundary errors. These errors can occur along spatial or temporal boundaries in planar architectures or during logical state preparation and measurement. 5/ https://t.co/TuZpexgHOV', 'While we make partial progress towards mitigating these errors by deforming the boundaries of the XY surface code, our work suggests that this fragility could remain a significant obstacle. 6/6']",https://arxiv.org/abs/2203.04948,"Biased noise is common in physical qubits, and tailoring a quantum code to the bias by locally modifying stabilizers or changing boundary conditions has been shown to greatly increase error correction thresholds. In this work, we explore the challenges of using a specific tailored code, the XY surface code, for fault-tolerant quantum computation. We introduce efficient and fault-tolerant decoders, belief-matching and belief-find, which exploit correlated hyperedge fault mechanisms present in circuit-level noise. Using belief-matching, we find that the XY surface code has a higher threshold and lower overhead than the square CSS surface code for moderately biased noise. However, the rectangular CSS surface code has a lower qubit overhead than the XY surface code when below threshold. We identify a contributor to the reduced performance that we call fragile boundary errors. These are string-like errors that can occur along spatial or temporal boundaries in planar architectures or during logical state preparation and measurement. While we make partial progress towards mitigating these errors by deforming the boundaries of the XY surface code, our work suggests that fragility could remain a significant obstacle, even for other tailored codes. We expect that our decoders will have other uses; belief-find has an almost-linear running time, and we show that it increases the threshold of the surface code to 0.937(2)% in the presence of circuit-level depolarising noise, compared to 0.817(5)% for the more computationally expensive minimum-weight perfect matching decoder. ",Fragile boundaries of tailored surface codes
121,1501975511953657875,1351775162266337281,Matt Bellardini,"['My new paper went up on the arxiv yesterday.  This paper, a follow up to my first paper, explores the spatial distribution of abundances in newly formed stars in the disks of MW-mass galaxies across cosmic time.  Check it out here: <LINK>', 'Reflecting gas-phase abundance gradients, disk-wide abundance gradients of newly formed stars steepened from initially flat to their present-day values. https://t.co/vu8if1aKp8', 'Additionally, azimuthal scatter in abundances of newly formed stars increases with increasing lookback time.  However, it is consistently smaller than the scatter seen in gas, and we find it to be nearly independent of radius. https://t.co/qphfYfEWjQ', 'We combine these results to determine the regime in which the distribution of abundances of newly formed stars in galaxies is dominated by radial abundance gradients and when it is dominated by azimuthal scatter.  A crucial time to characterize for chemical tagging. https://t.co/U9SvDyjzNw', ""Additionally, we provide fits to the evolution of stellar abundances, radial abundance gradients, and azimuthal scatter in abundances.  Check out the paper if you'd like to know more.  Many thanks to my co-authors @AndrewWetzel, @sloebman, and @AstroBailin.""]",https://arxiv.org/abs/2203.03653,"We characterize the 3-D spatial variations of [Fe/H], [Mg/H], and [Mg/Fe] in stars at the time of their formation, across 11 simulated Milky Way (MW)- and M31-mass galaxies in the FIRE-2 simulations, to inform initial conditions for chemical tagging. The overall scatter in [Fe/H] within a galaxy decreased with time until $\approx 7$ Gyr ago, after which it increased to today: this arises from a competition between a reduction of azimuthal scatter and a steepening of the radial gradient in abundance over time. The radial gradient is generally negative, and it steepened over time from an initially flat gradient $\gtrsim 12$ Gyr ago. The strength of the present-day abundance gradient does not correlate with when the disk `settled'; instead, it best correlates with the radial velocity dispersion within the galaxy. The strength of azimuthal variation is nearly independent of radius, and the 360 degree scatter decreased over time, from $\lesssim 0.17$ dex at $t_{\rm lb} = 11.6$ Gyr to $\sim 0.04$ dex at present day. Consequently, stars at $t_{\rm lb} \gtrsim 8$ Gyr formed in a disk with primarily azimuthal scatter in abundances. All stars formed in a vertically homogeneous disk, $\Delta$[Fe/H] $\leq 0.02$ dex within $1$ kpc of the galactic midplane, with the exception of the young stars in the inner $\approx 4$ kpc at $z \sim 0$. These results generally agree with our previous analysis of gas-phase elemental abundances, which reinforces the importance of cosmological disk evolution and azimuthal scatter in the context of stellar chemical tagging. We provide analytic fits to our results for use in chemical-tagging analyses. ","3D elemental abundances of stars at formation across the histories of
  Milky Way-mass galaxies in the FIRE simulations"
122,1501934846796390400,40639812,Colin Cotter,"[""A new paper on the ArXiV, on new numerical algorithms to solve the optimal transport formulation of the semigeostrophic equations applied to Eady's frontogenesis problem.\n\n<LINK>"", 'This came about because I had spent several years trying to understand Mike Cullen\'s ""geometric algorithm""  for solving this formulation. Mike has also inspired great new work from my co-authors in Heriot-Watt, and I helped them to apply it to the frontogenesis problem.', 'The optimal transport formulation is written in terms of a transport map from the physical domain to a ""geostrophic"" one. In the discretisation, this is replaced by a many-to-one map characterised by Laguerre cells (or power cells) in this physical domain.', 'Each cell is mapped to a point in geostrophic space, which evolve according to an equation defined by the map. The calculation of these cells is accelerated by a recent damped Newton algorithm of Merigot et al.', 'My collaborators David, Charlie, Beatrice and Mark have also used this algorithm to provide a new proof of global-in-time weak solutions for the 3D semigeostrophic equations.\n\nhttps://t.co/QWvM9UwdLC']",https://arxiv.org/abs/2203.04903,"We present a new implementation of the geometric method of Cullen & Purser (1984) for solving the semi-geostrophic Eady slice equations which model large scale atmospheric flows and frontogenesis. The geometric method is a Lagrangian discretisation, where the PDE is approximated by a particle system. An important property of the discretisation is that it is energy conserving. We restate the geometric method in the language of semi-discrete optimal transport theory and exploit this to develop a fast implementation that combines the latest results from numerical optimal transport theory with a novel adaptive time-stepping scheme. Our results enable a controlled comparison between the Eady-Boussinesq vertical slice equations and their semi-geostrophic approximation. We provide further evidence that weak solutions of the Eady-Boussinesq vertical slice equations converge to weak solutions of the semi-geostrophic Eady slice equations as the Rossby number tends to zero. ","A new implementation of the geometric method for solving the Eady slice
  equations"
123,1501866964540608516,1440653053321891843,Luis Alfredo Anchordoqui,['New paper on arXiv today in which we set constraints on Dynamical Dark Matter ensembles from cosmological observations <LINK>'],https://arxiv.org/abs/2203.04818,"Decaying cold dark matter (CDM) has been considered as a mechanism to tackle the tensions in the Hubble expansion rate and the clustering of matter. However, polarization measurements of the cosmic microwave background (CMB) severely constrain the fraction of dark matter decaying before recombination, and lensing of the CMB anisotropies by large-scale structure set strong constraints on dark matter decaying after recombination. Together, these constraints make an explanation of the Hubble tension in terms of decaying dark matter unlikely. In response to this situation, we investigate whether a dark matter ensemble with CDM particles decaying into free streaming dark radiation in different epochs can alleviate the problem. We find that it does not. ","Decay of multiple dark matter particles to dark radiation in different
  epochs does not alleviate the Hubble tension"
124,1501860954891563013,1005395495949406208,Francesco Locatello,"['New paper from the @awscloud Causal Representation Learning team and @EPFL_en: ""Score matching enables causal discovery of nonlinear additive noise models"": the gradient of the log likelihood gives the topological order.\n\n<LINK> <LINK>', ""This work introduces a new approach for scalable causal discovery with machine learning: it (1) doesn't require enforcing the dag constraint so it's efficient and deep learning friendly (2) can leverage advances in scalable score based methods."", 'Work led by P. Rolland, w/ @CevherLIONS, M. Kleindessner, @c_russl, @bschoelkopf, D. Janzing and myself.']",https://arxiv.org/abs/2203.04413,"This paper demonstrates how to recover causal graphs from the score of the data distribution in non-linear additive (Gaussian) noise models. Using score matching algorithms as a building block, we show how to design a new generation of scalable causal discovery methods. To showcase our approach, we also propose a new efficient method for approximating the score's Jacobian, enabling to recover the causal graph. Empirically, we find that the new algorithm, called SCORE, is competitive with state-of-the-art causal discovery methods while being significantly faster. ","Score matching enables causal discovery of nonlinear additive noise
  models"
125,1501853905059303425,1134375290581524480,Kai Schmitz,"['New paper on the @arxiv: <LINK>, in which I paint a broad-brush picture of modern #cosmology for a more general audience. E.g. if you are an undergraduate wondering whether pursuing a #PhD in cosmology would be something for you, I recommend having a closer look.']",https://arxiv.org/abs/2203.04757,"This essay is a nontechnical primer for a broader audience, in which I paint a broad-brush picture of modern cosmology. I begin by reviewing the evidence for the big bang, including the expansion of our Universe, the cosmic microwave background, and the primordial abundances of the light elements. Next, I discuss how these and other cosmological observations can be well explained by means of the concordance model of cosmology, putting a particular emphasis on the composition of the cosmic energy budget in terms of visible matter, dark matter, and dark energy. This sets the stage for a short overview of the history of the Universe from the earliest moments of its existence all the way to the accelerated expansion at late times and beyond. Finally, I summarize the current status of the field, including the challenges it is currently facing such as the Hubble tension, and conclude with an outlook onto the bright future that awaits us in the coming years and decades. The text is complemented by an extensive bibliography serving as a guide for readers who wish to delve deeper. ","Modern Cosmology, an Amuse-Gueule"
126,1501751386542952452,549460404,ÂêâÁî∞ Á¥Ö (Beni Yoshida),"['A new article (short review). \n\nSnowmass White Paper: New ideas for many-body quantum systems from string theory and black holes\n<LINK>', 'Á¥†Á≤íÂ≠êÁâ©ÁêÜ„Åß„ÅØ„ÄÅ„Åì„ÅÜ„ÅÑ„ÅÜ„É¨„Éì„É•„ÉºË´ñÊñá„Åø„Åü„ÅÑ„Å™„ÅÆ„ÇíÂÆöÊúüÁöÑ„Å´Êõ∏„Åè„ÅÆ„ÅåÁøíÊÖ£„Çâ„Åó„ÅÑ„Åß„Åô„ÄÇÂÉï„ÅØ„ÄÅÂº¶ÁêÜË´ñ„ÅÆÁô∫Â±ï„Åã„ÇâÂà∫ÊøÄ„Åï„Çå„Å¶Áîü„Åæ„Çå„ÅüÁâ©ÊÄßÁêÜË´ñ„ÅÆÁô∫Â±ï„Çí„ÄÅÈáèÂ≠êÊÉÖÂ†±ÁöÑ„Å™Ë¶ñÁÇπ„Åã„ÇâËß£Ë™¨„Åó„Åæ„Åó„Åü„ÄÇ‰ªñ„ÅÆÊñπ„ÅÆÁÆáÊâÄ„ÇÇÈù¢ÁôΩ„ÅÑ„ÅÆ„Åß„ÄÅÊòØÈùû„ÄÇ']",https://arxiv.org/abs/2203.04718,"During the last two decades many new insights into the dynamics of strongly coupled quantum many-body systems have been obtained using gauge/gravity duality, with black holes often playing a universal role. In this white paper we summarize the results obtained and offer some outlook for future developments, including the ongoing mutually beneficial feedback loop with the study of more general, not necessarily holographic, quantum many-body systems. ","Snowmass White Paper: New ideas for many-body quantum systems from
  string theory and black holes"
127,1501673107756568579,69202541,Jonathan Le Roux,"['New paper out w/ O. Slizovskaia @veleslavia, G. Wichern, @ZhongqiuWang, ""Locate This, Not That: Class-Conditioned Sound Event DOA Estimation,"" to be presented at #ICASSP2022.\n<LINK>', 'We propose a class-conditioned sound event localization &amp; detection (SELD) model, which alternately focuses on each target class specified as input. The model thus learns to be more robust to directional interference, while leveraging data from all classes. https://t.co/5QtClo0Za0']",https://arxiv.org/abs/2203.04197,"Existing systems for sound event localization and detection (SELD) typically operate by estimating a source location for all classes at every time instant. In this paper, we propose an alternative class-conditioned SELD model for situations where we may not be interested in localizing all classes all of the time. This class-conditioned SELD model takes as input the spatial and spectral features from the sound file, and also a one-hot vector indicating the class we are currently interested in localizing. We inject the conditioning information at several points in our model using feature-wise linear modulation (FiLM) layers. Through experiments on the DCASE 2020 Task 3 dataset, we show that the proposed class-conditioned SELD model performs better in terms of common SELD metrics than the baseline model that locates all classes simultaneously, and also outperforms specialist models that are trained to locate only a single class of interest. We also evaluate performance on the DCASE 2021 Task 3 dataset, which includes directional interference (sound events from classes we are not interested in localizing) and notice especially strong improvement from the class-conditioned model. ","Locate This, Not That: Class-Conditioned Sound Event DOA Estimation"
128,1501623912920199173,3209362451,Burton Lab,"['New paper from our group that uses supervised machine learning to extract forces from dynamics. We use real, noisy experimental data: the 3D motion of micron-sized particles in a dusty plasma. The effort was led by graduate student Wentao Yu!\n<LINK> <LINK>']",https://arxiv.org/abs/2203.03740,"Extracting environmental forces from noisy data is a common yet challenging task in complex physical systems. Machine learning represents a robust approach to this problem, yet is mostly tested on simulated data with known parameters. Here we use supervised machine learning to extract the electrostatic, hydrodynamic, and stochastic forces acting on micron-sized charged particles levitated in an argon plasma. Trained on simulated particle trajectories using more than 100 dynamical and statistical features, the model predicts system parameters with 50\% better accuracy than conventional methods, and provides non-contact measurements of the particle charge and Debye length. ",Extracting Forces from Noisy Dynamics in Dusty Plasmas
129,1501614028380119046,573729628,"Steve Taylor, PhD","['New paper today with Matthew Mould and Davide Gerosa on using Deep Neural Networks to measure the population demographics of hierarchical Black Hole Mergers! An amazing piece of work led by Matthew (1/2). <LINK>', 'We found that in the current LIGO-Virgo catalog, the distribution of cluster escape speeds favors &lt;100 km/s, but is relatively flat. Also there is evidence of multimodal structure in the mass and spin distributions from repeated black-hole mergers. @UoBIGWaves @VanderbiltU']",https://arxiv.org/abs/2203.03651,"The catalog of gravitational-wave events is growing, and so are our hopes of constraining the underlying astrophysics of stellar-mass black-hole mergers by inferring the distributions of, e.g., masses and spins. While conventional analyses parametrize this population with simple phenomenological models, we propose an innovative physics-first approach that compares gravitational-wave data against astrophysical simulations. We combine state-of-the-art deep-learning techniques with hierarchical Bayesian inference and exploit our approach to constrain the properties of repeated black-hole mergers from the gravitational-wave events in the most recent LIGO/Virgo catalog. Deep neural networks allow us to (i) construct a flexible population model that accurately emulates simulations of hierarchical mergers, (ii) estimate selection effects, and (iii) recover the branching ratios of repeated-merger generations. Among our results we find that: the distribution of host-environment escape speeds favors values <100 km s$^{-1}$ but is relatively flat; first-generation black holes are born with a maximum mass that is compatible with current estimates from pair-instability supernovae; there is multimodal substructure in both the mass and spin distributions due to repeated mergers; and binaries with a higher-generation component make up at least 15% of the underlying population. The deep-learning pipeline we present is ready to be used in conjunction with realistic astrophysical population-synthesis predictions. ","Deep learning and Bayesian inference of gravitational-wave populations:
  hierarchical black-hole mergers"
130,1501482717761916929,521154902,Massimiliano Luca,['New preprint outüéâ \n\nCan next-location predictors generalize? Not really\n\nüëâMany test trajectories are  seen during training\n\nüëâMobility laws can be used to support predictors and improve generalization \n\npaper: <LINK>\nwt @brulepri @GianniBarlacchi @lucpappalard <LINK>'],https://arxiv.org/abs/2203.03208,"Next-location prediction, consisting of forecasting a user's location given their historical trajectories, has important implications in several fields, such as urban planning, geo-marketing, and disease spreading. Several predictors have been proposed in the last few years to address it, including last-generation ones based on deep learning. This paper tests the generalization capability of these predictors on public mobility datasets, stratifying the datasets by whether the trajectories in the test set also appear fully or partially in the training set. We consistently discover a severe problem of trajectory overlapping in all analyzed datasets, highlighting that predictors memorize trajectories while having limited generalization capacities. We thus propose a methodology to rerank the outputs of the next-location predictors based on spatial mobility patterns. With these techniques, we significantly improve the predictors' generalization capability, with a relative improvement on the accuracy up to 96.15% on the trajectories that cannot be memorized (i.e., low overlap with the training set). ",Trajectory Test-Train Overlap in Next-Location Prediction Datasets
131,1501294412126560257,1092693586263457792,Greg Yang,"[""1/ You can't train GPT-3 on a single GPU, much less tune its hyperparameters (HPs).\n\nBut what if I tell you‚Ä¶\n\n‚Ä¶you *can* tune its HPs on a single GPU thanks to new theoretical advances?\n\npaper <LINK>\ncode <LINK>\nblog <LINK> <LINK>"", '2/ The idea is actually really simple: in a special parametrization introduced in https://t.co/vhvvXylq58 called ¬µP, narrow and wide neural networks share the same set of optimal hyperparameters. This works even as width -&gt; ‚àû. https://t.co/lFiPMyIs1t', '3/ The hyperparameters can include learning rate, learning rate schedule, initialization, parameter multipliers, and more, even individually for each parameter tensor. We empirically verified this on Transformers up to width 4096. https://t.co/O2CbmjMgde', '4/ Using this insight, we can just tune a tiny version of GPT-3 on a single GPU --- if the hyperparameters we get on the small model is near optimal, then they should also be near optimal on the large model! We call this way of tuning *¬µTransfer*. https://t.co/qbUQmjJZZW', '5/ We ¬µTransferred hyperparameters from a small 40 million parameter version of GPT-3 ‚Äî small enough to fit on a single GPU ‚Äî to the 6.7 billion version. \nWith some asterisks, we get a performance comparable to the original GPT-3 model with twice the parameter count! https://t.co/g8lQ2ZgffX', '6/ The total tuning cost is only 7% of the whole pretrain compute cost! Since the direct tuning of the small model costs roughly the same even as the large model increases in size, tuning the 175B GPT-3 this way would prolly cost at most 0.3% of the total pretrain compute.', '7/ You: ""wait can I shrink the model only in width?""\n\nBad news: there\'s not much theoretical guarantee for non-width stuff\n\ngood news: we empirically tested transfer across depth, batch size, sequence length, &amp; timestep work within reasonable ranges on preLN transformers. https://t.co/xP95DryEyP', '8/ We applied this to tune BERT-base and BERT-large simultaneously by shrinking them to the same small model in both width and depth, where we did the direct tuning. We got a really nice improvement over the already well-tuned megatron BERT baseline, especially for BERT-large! https://t.co/qne6thlU9Q', ""9/ In general, it seems that the larger a model is, the less well tuned it is --- which totally makes sense --- and thus the more to gain from ¬µTransfer. We didn't have compute to retrain the GPT-3 175B model, but I'll leave your mouth watering with that thought."", '10/ OK, so what actually is ¬µP and how do you implement it?', ""11/ It's encapsulated by the following table for how to scale your initialization and learning rate with fanin or fanout. The purple text is ¬µP and the gray text in parenthesis is pytorch default, for reference, and the black text is shared by both. https://t.co/uyq6s9gjYn"", ""12/ But just like you don't typically want to implement autograd by hand even though autograd is just chain rule, we recommend using our package https://t.co/5S0YAg026Z to implement ¬µP in your models."", '13/ The really curious ones of you: ""OK what is the theoretical motivation behind all this?""\n\nUnfortunately, this is already getting long, so people let me know if this is something you want to hear for another time!', ""14/ It's been an amazing collaboration with @edwardjhu @ibab_ml @sidorszymon David Farhi, Nick Ryder, @merettm @AllenLao @WeizhuChen @JianfengGao0217!"", '15/ Also thanks to my wonderful friends and colleagues who gave feedback - pinging those on twitter @ArthurJacot3 @colinraffel @ilyasut @jaschasd @jxbz @LenaicChizat @Luke_Metz @markchen90 @sschoenholz @prfsanjeevarora @TacoCohen @2prime_PKU @yisongyue', '@jacobmbuckman Thanks Jacob! ŒºP is architecturally universal (due to Tensor Programs applicable essentially to any computation graph), so if you just follow the paper/our repo https://t.co/5S0YAghCYx it should work automatically.', '@jacobmbuckman In particular, if you just following the table and scale the ""attention logit""-like calculations, where you are summing a width-dimensional vector (here, the vector is the pointwise product btw key and query), like 1/d, then you are good to go. https://t.co/x9jGkxGaSI', '@jacobmbuckman for conv layer, fan-in/fan-out are the number of input/output channels. We assume the input to the conv layer is either data or some hidden activation from previous layers, trained from random initialization. Layernorm/batchnorm are compatible with this.']",https://arxiv.org/abs/2203.03466,"Hyperparameter (HP) tuning in deep learning is an expensive process, prohibitively so for neural networks (NNs) with billions of parameters. We show that, in the recently discovered Maximal Update Parametrization (muP), many optimal HPs remain stable even as model size changes. This leads to a new HP tuning paradigm we call muTransfer: parametrize the target model in muP, tune the HP indirectly on a smaller model, and zero-shot transfer them to the full-sized model, i.e., without directly tuning the latter at all. We verify muTransfer on Transformer and ResNet. For example, 1) by transferring pretraining HPs from a model of 13M parameters, we outperform published numbers of BERT-large (350M parameters), with a total tuning cost equivalent to pretraining BERT-large once; 2) by transferring from 40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with tuning cost only 7% of total pretraining cost. A Pytorch implementation of our technique can be found at github.com/microsoft/mup and installable via `pip install mup`. ","Tensor Programs V: Tuning Large Neural Networks via Zero-Shot
  Hyperparameter Transfer"
132,1501214185555959816,959316810448318464,Theo O'Neill,"['New paper out today! üéâ Led by Jon Swift, we detail first results from the recently renovated Thacher Observatory in Ojai, CA.  \n\nTake a look if you‚Äôre interested in observatory management, astronomy education, or want to see some pretty light curves!\n\n<LINK>']",https://arxiv.org/abs/2203.02529,"Located on the campus of the Thacher School in Southern California, the Thacher Observatory has a legacy of astronomy research and education that dates back to the late 1950's. In 2016, the observatory was fully renovated with upgrades including a new 0.7-m telescope, a research grade camera, and a slit dome with full automation capabilities. The low-elevation site is bordered by the Los Padres National Forest and therefore affords dark to very dark skies allowing for accurate and precise photometric observations. We present a characterization of the site including sky brightness, weather, and seeing, and we demonstrate the on-sky performance of the facility. Our primary research programs are based around our multi-band photometric capabilities and include photometric monitoring of variable sources, a nearby supernova search and followup program, a quick response transient followup effort, and exoplanet and eclipsing binary light curves. Select results from these programs are included in this work which highlight the broad range of science available to an automated observatory with a moderately sized telescope. ",The Renovated Thacher Observatory and First Science Results
133,1501135939724263426,1411414442974355464,Sokratis Trifinopoulos,['A pure #pheno paper today constraining light new physics in anticipation of the @belle2collab results!\n<LINK>'],https://arxiv.org/abs/2203.03280,"With a design luminosity of 50 ab$^{-1}$ and detectors with tracking capabilities extending beyond 1 m, the Belle II experiment is the perfect laboratory for the search of particles that couple weakly to the Standard Model and have a characteristic decay length of a few centimetres and more. We show that for models of dark photons and other light vector bosons, Belle II will be successful in probing regions of parameter space which are as of now unexplored by any experiment. In addition, for models where the vector boson couples sub-dominantly to the electron and quarks as compared to muons, e.g. in the $L_\mu-L_\tau$ model, Belle II will probe regions of mass and couplings compatible with the anomalous magnetic moment of muon. We discuss these results and derive the projected sensitivity of Belle II for a handful of other models. Finally, even with the currently accumulated data, $\sim 200$ fb$^{-1}$, Belle II should be able to cover regions of parameter space pertaining to the X(17) boson postulated to solve the ATOMKI anomaly. ",Displaced Searches for Vector Bosons at Belle II
134,1501014250256576518,175590531,Alejandro Flores V.,"[""The preprint is out! <LINK> üì∞\nThe problem of reducing training sets while maintaining the exact class boundaries of the nearest-neighbor classifier, has been a long-standing one. I'm proposing a new algorithm for this problem, improving a fairly recent paper! üßµ"", ""For almost three decades, the best know result was Clarkson's [1], but just a few months ago Prof. David Eppstein [2] proposed a new, much improved, algorithm for this problem...\n[1] https://t.co/gbuHR43uXt\n[2] https://t.co/SQuRWKaOyN"", ""My take on this is to improve Eppstein's algorithm by simplifying it... Basically, chopping half of the algorithm while proving it is still correct. And reducing it's time complexity along the way...""]",https://arxiv.org/abs/2203.03567,"Given a training set $P \subset \mathbb{R}^d$, the nearest-neighbor classifier assigns any query point $q \in \mathbb{R}^d$ to the class of its closest point in $P$. To answer these classification queries, some training points are more relevant than others. We say a training point is relevant if its omission from the training set could induce the misclassification of some query point in $\mathbb{R}^d$. These relevant points are commonly known as border points, as they define the boundaries of the Voronoi diagram of $P$ that separate points of different classes. Being able to compute this set of points efficiently is crucial to reduce the size of the training set without affecting the accuracy of the nearest-neighbor classifier. Improving over a decades-long result by Clarkson, in a recent paper by Eppstein an output-sensitive algorithm was proposed to find the set of border points of $P$ in $O( n^2 + nk^2 )$ time, where $k$ is the size of such set. In this paper, we improve this algorithm to have time complexity equal to $O( nk^2 )$ by proving that the first steps of their algorithm, which require $O( n^2 )$ time, are unnecessary. ",Improved Search of Relevant Points for Nearest-Neighbor Classification
135,1501010017939427328,769369833259479040,Yuto Minami,['Our new paper is available.\n<LINK>'],https://arxiv.org/abs/2203.02495,"We report an improved measurement of the degree-scale CMB $B$-mode angular power spectrum over 670 square-degree sky area with POLARBEAR. In the original analysis of the data, errors in the angle measurement of the continuously rotating half-wave plate, a polarization modulator, caused significant data loss. By introducing an angle-correction algorithm, the data volume is increased by a factor of 1.8. We report a new analysis using the larger data set. We find the measured $B$-mode spectrum is consistent with the $\Lambda$CDM model with Galactic foregrounds. We place an upper limit on the tensor-to-scalar ratio $r$ < 0.33 at 95% confidence level. ","Improved upper limit on degree-scale CMB B-mode polarization power from
  the 670 square-degree POLARBEAR survey"
136,1500934607935520768,1171264866113470470,Yago Ph,"['New paper with awesome guys @manibrata_sen and @sjtwitt_1! :)\n\n<LINK>', 'The neutronization burst from a future galactic Supernova has the potential to provide a wealth of information on neutrino magnetic moments. The neutrino transition magnetic moments which can be explored are an order to several orders of magnitude better than the current limits!']",https://arxiv.org/abs/2203.01950,"A core-collapse supernova (SN) offers an excellent astrophysical laboratory to test non-zero neutrino magnetic moments. In particular, the neutronization burst phase, which lasts for few tens of milliseconds post-bounce, is dominated by electron neutrinos and can offer exceptional discovery potential for transition magnetic moments. We simulate the neutrino spectra from the burst phase in forthcoming neutrino experiments like the Deep Underground Neutrino Experiment (DUNE), and the Hyper-Kamiokande (HK), by taking into account spin-flavour conversions of SN neutrinos, caused by interactions with ambient magnetic fields. We find that the neutrino transition magnetic moments which can be explored by these experiments for a galactic SN are an order to several orders of magnitude better than the current terrestrial and astrophysical limits. Additionally, we also discuss how this realization might shed light on three important neutrino properties: (a) the Dirac/Majorana nature, (b) the neutrino mass ordering, and (c) the neutrino mass-generation mechanism. ","Exploiting a future galactic supernova to probe neutrino magnetic
  moments"
137,1500706184591908865,768092862,Thomas G. Dietterich,"['My student Alex Guyer and I just released a paper on arXiv on what we call the Familiarity Hypothesis that offers a new explanation for why using the max logit score works so well as a novelty score in open category detection in computer vision 1/\n<LINK>', 'We build upon the very nice paper by @Sagar_Vaze et al https://t.co/NvHS5744uN which shows experimentally that careful training of a standard object classifier (ResNet with label smoothing, cosine schedule, warmup, and randaugment) gives SOTA open category detection 2/', 'Their experiments match our experience that taking the max logit score is a better open category score than the max softmax probability, the softmax entropy, or the energy (denominator of the softmax). Why does this work so well? 3/', 'From a theoretical point of view, discriminatively trained classifiers learn an approximation of P(y|x), for class y and input image x, whereas novelty detection ""should"" involve modeling P(x) and detecting images for which P(x) is small. 4/', 'But many attempts to directly model P(x) (e.g., using flow models or density estimators over the latent space) often perform worse than max logit and never beat it. 5/', 'Our Familiarity Hypothesis (FH) is that these discriminative networks are not detecting novelty in the image. Instead, they are detecting a lack of familiarity, as measured by low values of the max logit score. 6/', 'As a toy example, consider a network that had never seen an elephant. The FH says it will assign a high ""novelty"" score to an elephant image not because it sees the trunk and tusks, but because it only detects a few familiar parts (legs, feet, eyes) learned on other classes 7/ https://t.co/ySI03LKukj', 'In the paper, we use the PASCAL segmented image dataset to show that the main contributor to lower max logit scores on a novel image is the drop in activations of features involving the object. 8/', 'Analysis of activations on CIFAR-10, CIFAR-100, ImageNette, and ImageNet-1K confirm this explanation. 9/', 'In retrospect, the familiarity hypothesis seems obvious. Why does it matter? First, it raises the fundamental question of whether we can learn representations that can ""see"" interesting features that were not present in the training data. 10/', ""Traditional anomaly detection methods can find outliers because the features have been hand engineered to represent instances that are larger, smaller, taller, shorter, etc. than the instances in the training data. But this doesn't seem to happen much in deep computer vision 11."", 'A second lesson is that attempts to improve deep anomaly detection by fitting density models to the latent activations are unlikely to beat max logit. Each class in a softmax classifier can be viewed as ""looking for"" a set of features.  12.', 'The logit weights indicate how important those features are, and hence, the logit score combines feature selection and feature weighting to give a specialized familiarity detector. It is hard for density estimation methods to be so specialized. 13/', 'The Familiarity Hypothesis (and Vaze et al) suggest that our efforts are better devoted to improving the classifier. 14/', 'But the FH also predicts that adversarial examples can easily fool classifier-based open category detectors. Can we find some way to detect novelty rather than merely the lack of familiarity? 15/end', '@hermannsblum Interesting! I would love to know the details. How many classes are you training on?']",https://arxiv.org/abs/2203.02486,"In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such ""novel category"" objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning. Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity detection is an inevitable consequence of representation learning. ","The Familiarity Hypothesis: Explaining the Behavior of Deep Open Set
  Methods"
138,1499815776647274502,69202541,Jonathan Le Roux,"['New paper out w/ X. Chang @kaikai0019, N. Moritz, T. Hori, S. Watanabe @shinjiw_at_cmu, ""Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR"", to be presented at #ICASSP2022.\n<LINK>', 'GTC-e extends GTC to model posteriors of both labels &amp; label transitions of a graph by a neural network. We use GTC-e to reformulate multi-speaker ASR such that tokens by multiple speakers are recognized as a single merged sequence in chronological order.\nhttps://t.co/YnCXIZIlSJ https://t.co/syXTDcpf2y']",https://arxiv.org/abs/2203.00232,"Graph-based temporal classification (GTC), a generalized form of the connectionist temporal classification loss, was recently proposed to improve automatic speech recognition (ASR) systems using graph-based supervision. For example, GTC was first used to encode an N-best list of pseudo-label sequences into a graph for semi-supervised learning. In this paper, we propose an extension of GTC to model the posteriors of both labels and label transitions by a neural network, which can be applied to a wider range of tasks. As an example application, we use the extended GTC (GTC-e) for the multi-speaker speech recognition task. The transcriptions and speaker information of multi-speaker speech are represented by a graph, where the speaker information is associated with the transitions and ASR outputs with the nodes. Using GTC-e, multi-speaker ASR modelling becomes very similar to single-speaker ASR modeling, in that tokens by multiple speakers are recognized as a single merged sequence in chronological order. For evaluation, we perform experiments on a simulated multi-speaker speech dataset derived from LibriSpeech, obtaining promising results with performance close to classical benchmarks for the task. ",Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR
139,1499791903432261634,972440922070888449,Jason Parisi,"['We just put out a new paper on electron temperature gradient turbulence in the edge of tokamak fusion reactors <LINK>', ""The highly 'shaped' magnetic geometry in the tokamak edge causes turbulence that has a different character to the tokamak core. We argue that 'topographies' of important physics effects generated by the magnetic geometry offer routes to build reactors that can reduce turbulence."", 'The tokamak edge is also a really interesting place for multiscale turbulence physics, because the scale separation between electron and ion physics is broken. We need to understand better what the consequences are for turbulence in reactors.']",https://arxiv.org/abs/2203.00831,"Nonlinear multiscale gyrokinetic simulations of a Joint European Torus edge pedestal are used to show that electron-temperature-gradient (ETG) turbulence has a rich three-dimensional structure, varying strongly according to the local magnetic-field configuration. In the plane normal to the magnetic field, the steep pedestal electron temperature gradient gives rise to anisotropic turbulence with a radial (normal) wavelength much shorter than in the binormal direction. In the parallel direction, the location and parallel extent of the turbulence are determined by the variation in the magnetic drifts and finite-Larmor-radius (FLR) effects. The magnetic drift and FLR topographies have a perpendicular-wavelength dependence, which permits turbulence intensity maxima near the flux-surface top and bottom at longer binormal scales, but constrains turbulence to the outboard midplane at shorter electron-gyroradius binormal scales. Our simulations show that long-wavelength ETG turbulence does not transport heat efficiently, and significantly decreases overall ETG transport -- in our case by $\sim$40 \% -- through multiscale interactions. ","Three-Dimensional Inhomogeneity of Electron-Temperature-Gradient
  Turbulence in the Edge of Tokamak Plasmas"
140,1499779996239421445,20865039,Tristan Deleu,"['New paper! üìÑ ""Continuous-Time Meta-Learning with Forward Mode Differentiation"", with @davidkanaa, @lylbfeng, @GcKerg, Yoshua Bengio, @g_lajoie_ &amp; @pierrelux @Mila_Quebec \n\nAccepted as a Spotlight at #ICLR2022\npaper: <LINK>\ncode: <LINK> <LINK>', 'We introduce COMLN, a new meta-learning algorithm where adaptation follows the dynamics of a gradient vector field, instead of being based on typically a few steps of gradient-descent. Computing the adapted parameters for a new task therefore requires solving an ODE.', 'Neural ODEs provided tools (adjoint method) to backpropagate through an ODE solver in constant memory. But in practice, this is numerically unstable when applied to gradient flows. Intuitively, it would require gradient *ascent* on the loss function during the backward pass. https://t.co/qLtpUyh6kR', 'We developed a memory-efficient algorithm to compute the gradients wrt. the initialization, based on forward-mode differentiation and a decomposition of the Jacobian matrices. With COMLN, we can do the equivalent of millions of gradient steps of adaptation in constant memory.', 'Finally, another advantage of treating adaptation as a continuous-time process is that the amount of adaptation can now be viewed as a meta-parameter, on par with the initialization, that can be meta-learned with SGD instead of being a hyperparameter fixed ahead of time.']",https://arxiv.org/abs/2203.01443,"Drawing inspiration from gradient-based meta-learning methods with infinitely small gradient steps, we introduce Continuous-Time Meta-Learning (COMLN), a meta-learning algorithm where adaptation follows the dynamics of a gradient vector field. Specifically, representations of the inputs are meta-learned such that a task-specific linear classifier is obtained as a solution of an ordinary differential equation (ODE). Treating the learning process as an ODE offers the notable advantage that the length of the trajectory is now continuous, as opposed to a fixed and discrete number of gradient steps. As a consequence, we can optimize the amount of adaptation necessary to solve a new task using stochastic gradient descent, in addition to learning the initial conditions as is standard practice in gradient-based meta-learning. Importantly, in order to compute the exact meta-gradients required for the outer-loop updates, we devise an efficient algorithm based on forward mode differentiation, whose memory requirements do not scale with the length of the learning trajectory, thus allowing longer adaptation in constant memory. We provide analytical guarantees for the stability of COMLN, we show empirically its efficiency in terms of runtime and memory usage, and we illustrate its effectiveness on a range of few-shot image classification problems. ",Continuous-Time Meta-Learning with Forward Mode Differentiation
141,1499743935517708288,1152338625654226944,Megan Mansfield,"['Today on the ArXiv: a new paper looking at the secondary eclipse spectrum of WASP-77Ab with the Hubble Space Telescope! <LINK> <LINK>', 'First, some fun facts about WASP-77Ab: it‚Äôs a ‚Äúmoderate‚Äù hot Jupiter, with an equilibrium T of ~1700K. It was previously observed at high resolution which showed a non-inverted T-P profile. H2O+CO were also detected and indicated a substellar metallicity. (Fig. from Line+21) https://t.co/M6i2Xmgufq', 'Here, we observed 2 eclipses of the planet with HST. We combined these data with eclipses from a set of Spitzer phase curves. We then modeled the data with both free retrievals and self-consistent equilibrium retrievals.', 'We detected a H2O absorption feature in the HST data and confirmed that the atmosphere has a decreasing T-P profile. The strength of the feature is also about what we expect when we compare WASP-77Ab to other HST observations of similar-temperature hot Jupiters! https://t.co/vBeD9Yukej', 'Our observations also match well with the best-fit model to those earlier high-res observations. This is really great news, because high-res and low-res data are reduced completely differently, so it‚Äôs a great sign that we‚Äôre on the right track when they show the same results. https://t.co/ILtPUqPOlu', 'But our equilibrium models suggest a higher metallicity than what was found from the high-res observations. We think this might be due to some sort of disequilibrium chemistry, as the equilibrium models do a poor job approximating the shape of the blue end of the HST spectrum. https://t.co/nkQJ0wb5KX', 'Ultimately, we‚Äôre hoping to combine the previous high-res data with our new HST+Spitzer data set and do a joint retrieval. Combining high+low-res data in this way gives more detailed information on the atmosphere. Look for a paper in the future by @ExoplanetPete doing just this! https://t.co/DVCdpdJhvB', 'Finally, I‚Äôd like to thank all my co-authors for making this paper possible, especially @kevinbstevenson for the Spitzer data reduction and @LindsLikesSpace, @ExoplanetPete, and twitterless Mike Line for their modeling work....', 'And everyone else who contributed to this paper! @jjfplanet, @V_Parmentier, @jmdesert, @lkreidberg, and twitterless Jacob Bean, Eliza Kempton, Jacob Arcangeli, Brian Kilpatrick, and Matej Malik. https://t.co/tQcusArbA6']",https://arxiv.org/abs/2203.01463,"Secondary eclipse observations of hot Jupiters can reveal both their compositions and thermal structures. Previous observations have shown a diversity of hot Jupiter eclipse spectra, including absorption features, emission features, and featureless blackbody-like spectra. We present a secondary eclipse spectrum of the hot Jupiter WASP-77Ab observed between $1-5$ $\mu$m with the Hubble Space Telescope (HST) and the Spitzer Space Telescope. The HST observations show signs of water absorption indicative of a non-inverted thermal structure. We fit the data with both a one-dimensional free retrieval and a grid of one-dimensional self-consistent forward models to confirm this non-inverted structure. The free retrieval places a $3\sigma$ lower limit on the atmospheric water abundance of $\log(n_\mathrm{H_2O})>-4.78$ and can not constrain the CO abundance. The grid fit produces a slightly super-stellar metallicity and constrains the carbon-to-oxygen ratio to less than or equal to the solar value. We also compare our data to recent high-resolution observations of WASP-77Ab taken with the Gemini-South/IGRINS spectrograph and find that our observations are consistent with the best-fit model to the high-resolution data. However, the metallicity derived from the IGRINS data is significantly lower than that derived from our self-consistent model fit. We find that this difference may be due to disequilibrium chemistry, and the varying results between the models applied here demonstrate the difficulty of constraining disequilibrium chemistry with low-resolution, low wavelength coverage data alone. Future work to combine observations from IGRINS, HST, and JWST will improve our estimate of the atmospheric composition of WASP-77Ab. ","Confirmation of Water Absorption in the Thermal Emission Spectrum of the
  Hot Jupiter WASP-77Ab with HST/WFC3"
142,1499692328793038851,45191927,Dr Jake Taylor,"['It\'s a new paper day for me! It is titled ""Impact of Variable Photospheric Radius on Exoplanet Atmospheric Retrievals"" and you can read it here: <LINK>. This is my first ever solo paper so I welcome comments and feedback. Here\'s a thread to summarise the results.', 'Firstly, this study was inspired by the work done by @jjfplanet (https://t.co/TQCul5ST3N). They show that the difference between a model with a fixed radius and radius that accounts for the photosphere can be large and will impact JWST observations.', 'With that in mind, I wanted to see how much this would impact the spectral retrievals we would perform on upcoming JWST observations. So I set up a little study to explore this for hot Jupiters.', 'We make a lot of assumptions in our models. The first is that the radius we assume for the planet is fixed at each wavelength. Well, what @jjfplanet shows is that this can cause some drama. So Mike Line and I implemented a variable radius into our code (https://t.co/qiTyh2ZtED).', 'Fortney et al find that the gravity of a planet is a major factor when it comes to the impact of the photospheric radius. So I consider 3 different scenarios, a WASP-43b like planet, a low gravity version of WASP-43b and a high gravity version.', 'Here I plotted the spectra of each scenario with a fixed radius (solid line) and variable (dashed line) and then the difference between the two on the right. Consistent with Fortney at el we see that low gravity planets have the most difference. https://t.co/TmKZsW4nyN', 'It is worth noting that even for a typical hot Jupiter such as WASP-43b, over the MIRI LRS wavelength coverage there is &gt;100ppm difference between the models, that is significant!', 'So using the variable radius spectra as mock datasets I explore different observing scenarios. I consider the wavelength coverage of NIRSpec PRISM, MIRI LRS and then combining the two. And then error envelopes of 100ppm, 60ppm and 30ppm.', 'I wanted to find out: if I retrieve using a fixed radius model, how much would my results be impacted? Well.. it turns out a lot! For the low gravity case, all results were biased (i.e the retrieved results were outside 2-sigma of the input value).', 'For a typical hot Jupiter, I did not see a bias over the NIRSpec PRISM wavelength region. However I did over MIRI LRS, that 100ppm difference really matters! When combining the two observing modes I saw more biases, so the influence of MIRI LRS on the NIRSpec data is important.', 'Finally, the high gravity case does not suffer the biases seen in the other two scenarios. So, what lesson did we learn? That when studying emission spectra obtained with JWST we need to consider the photospheric radius impact!', 'I want to thank @astrophpeter for proofreading the paper. Mike Line (not on twitter) for discussing the problem with me and @DrJoVian for a really detailed review which helped the clarity of the letter. Comments from the community are welcome! Sorry if I missed a citation!', '@astronomerslc25 Haha yeah! They seem to cause the least drama :P']",https://arxiv.org/abs/2203.01839,"Inverse techniques are used to extract information about an exoplanet's atmosphere. These techniques are prone to biased results if the appropriate forward model is not used. One assumption used in a forward model is to assume that the radius of the planet is constant with wavelength, however a more realistic assumption is that the photospheric radius varies with each wavelength. We explore the bias induced when attempting to extract the molecular abundance from an emission spectrum which was generated with a variable radius. We find that for low gravity planets, the retrieval model is not able to fit the data if a constant radius model is used. We find that biased results are obtained when studying a typical hot Jupiter in the MIRI LRS wavelength range. Finally, we show that high gravity planets do not suffer a bias. We recommend that future spectral retrievals that interpret exoplanet emission spectra should take into account a variable radius. ","Impact of Variable Photospheric Radius on Exoplanet Atmospheric
  Retrievals"
143,1499673277769388034,328430286,Jad C. Halimeh,['New paper <LINK> (2nd of 2 from today)! We show how to stabilize disorder-free localization in synthetic quantum matter devices for all accessible times using our new disorder-free scheme of Stark gauge protection (SGP).\n@ERC_Research\n@MCQST_cluster\n@HaukeGroup <LINK>'],https://arxiv.org/abs/2203.01338,"Disorder-free localization in translation-invariant gauge theories presents a counterintuitive yet powerful framework of ergodicity breaking in quantum many-body physics. The fragility of this phenomenon in the presence of gauge-breaking errors has recently been addressed, but no scheme has been able to reliably stabilize disorder-free localization through all accessible evolution times while preserving the disorder-free property. Here, we introduce the concept of \textit{Stark gauge protection}, which entails a linear sum in gauge-symmetry local (pseudo)generators weighted by a Stark potential. Using exact diagonalization and Krylov-based methods, we show how this scheme can stabilize or even enhance disorder-free localization against gauge-breaking errors in $\mathrm{U}(1)$ and $\mathbb{Z}_2$ gauge theories up to all accessible evolution times, without inducing \textit{bona fide} Stark many-body localization. We show through a Magnus expansion that the dynamics under Stark gauge protection is described by an effective Hamiltonian where gauge-breaking terms are suppressed locally by the protection strength and additionally by the matter site index, which we argue is the main reason behind stabilizing the localization up to all accessible times. Our scheme is readily feasible in modern ultracold-atom experiments and Rydberg-atom setups with optical tweezers. ",Disorder-free localization with Stark gauge protection
144,1499673181694709761,328430286,Jad C. Halimeh,"['New paper <LINK> (1st of 2 from today)! We study dynamical quantum phase transitions (DQPTs) in spin-S U(1) quantum link models, showing rich critical behavior in the lattice-QED limit with various distinct types of DQPTs\n@ERC_Research\n@MCQST_cluster\n@HaukeGroup <LINK>']",https://arxiv.org/abs/2203.01337,"Dynamical quantum phase transitions (DQPTs) are a powerful concept of probing far-from-equilibrium criticality in quantum many-body systems. With the strong ongoing experimental drive to quantum-simulate lattice gauge theories, it becomes important to investigate DQPTs in these models in order to better understand their far-from-equilibrium properties. In this work, we use infinite matrix product state techniques to study DQPTs in spin-$S$ $\mathrm{U}(1)$ quantum link models. Although we are able to reproduce literature results directly connecting DQPTs to a sign change in the dynamical order parameter in the case of $S=1/2$ for quenches starting in a vacuum initial state, we find that for different quench protocols or different values of the link spin length $S>1/2$ this direct connection is no longer present. In particular, we find that there is an abundance of different types of DQPTs not directly associated with any sign change of the order parameter. Our findings indicate that DQPTs are fundamentally different between the Wilson--Kogut--Susskind limit and its representation through the quantum link formalism. ","Dynamical quantum phase transitions in spin-$S$ $\mathrm{U}(1)$ quantum
  link models"
145,1499595133645627392,2842416797,Nichole Barry,"[""The sea levels are rising, we are at the brink of another world war, and the pandemic is now normal.\n\nAnywho here's a new paper: <LINK>""]",https://arxiv.org/abs/2203.01130,"Reconstruction of the sky brightness measured by radio interferometers is typically achieved through gridding techniques, or histograms in spatial Fourier space. For Epoch of Reionisation (EoR) 21 cm power spectrum measurements, extreme levels of gridding resolution are required to reduce spectral contamination, as explored in other works. However, the role of the shape of the Fourier space spreading function, or kernel, also has consequences in reconstructed power spectra. We decompose the instrumental Murchison Widefield Array (MWA) beam into a series of Gaussians and simulate the effects of finite kernel extents and differing shapes in gridding/degridding for optimal map making analyses. For the MWA, we find that the kernel must extend out to 0.001--0.0001% of the maximum value in order to measure the EoR using foreground avoidance. This requirement changes depending on beam shape, with compact kernels requiring far smaller extents for similar contamination levels at the cost of less-optimal errors. However, simple calibration using pixelated degridding results, regardless of shape of the kernel, cannot recover the EoR due to catastrophic errors caused by the pixel resolution. Including an opaque horizon with widefield beams also causes significant spectral contamination via a beam--horizon interaction that creates an infinitely extended kernel in Fourier space, which cannot be represented well. Thus, our results indicate that simple calibration via degridded models and optimal map making for extreme widefield instrumentation are not feasible. ","The Role of the Instrumental Response in 21 cm EoR Power Spectrum
  Gridding Analyses"
146,1499432071902674949,969035645938294785,Erik Thiede (he /him),"['All you free energy aficionados looking for your morning dose of math, here it is!  New paper w. Sherry Li on understanding the error in MBAR dropped on arXiv at <LINK>.  Some takeaways in the comments...', ""- You really can't ignore the effects of the sampling dynamics in the error.  Dynamical effects  can contribute just as much -- if not more -- to the error than static properties."", '- Your intuition that you should be discarding those high-free energy umbrella sampling windows is probably right: you can actually *decrease* the variance in other parts of your PMF by throwing out hard-to-sample windows.', '- In fact, sometimes surprisingly few states often contribute to most of the MBAR error.  We might be able to tune our free energy calculations much better than we have been, now that we have comprehensive tools for understanding the error.']",https://arxiv.org/abs/2203.01227,"Multiple sampling strategies commonly used in molecular dynamics, such as umbrella sampling and alchemical free energy methods, involve sampling from multiple thermodynamic states. Commonly, the data are then recombined to construct estimates of free energies and ensemble averages using the Multistate Bennett Acceptance Ratio (MBAR) formalism. However, the error of the MBAR estimator is not well-understood: previous error analysis of MBAR assumed independent samples and did not permit attributing contributions to the total error to individual thermodynamic states. In this work, we derive a novel central limit theorem for MBAR estimates. This central limit theorem yields an error estimator which can be decomposed into contributions from the individual Markov chains used to sample the states. We demonstrate the error estimator for an umbrella sampling calculation of the alanine dipeptide in two dimensions and an alchemical calculation of the hydration free energy of methane. In both cases, the states' individual contributions to the error provide insight into the sources of error of the simulations. Our numerical results demonstrate that the time required for the Markov chain to decorrelate in individual thermodynamic states contributes considerably to the total MBAR error. Moreover, they indicate that it may be possible to use the contributions to tune the sampling and improve the accuracy of MBAR calculations. ",Understanding the Sources of Error in MBAR through Asymptotic Analysis
147,1499403993918943233,19740214,Kory Mathewson,"['New @DeepMind paper on Learning Robust Real-Time Cultural Transmission without Human Data explores the emergence of collective knowledge in reinforcement learning agents. \n\nMore details: \nüåê: <LINK>\nüìù: <LINK>\nüìπ: <LINK> <LINK>', 'This work will be presented and discussed at the @royalsociety meeting on ‚ÄúThe emergence of collective knowledge and cumulative culture in animals, humans and machines‚Äù https://t.co/HNyrF5YyQI', 'Collaboration w/ @avishkar58, Bethanie Brownfield, @adriancollister, @agudallago, Ashley Edwards, @reverettai, Alexandre Frechette, @edwardfhughes,  @kikujiroK, Yanko Gitahy Oliveira, Julia Pawar, @Miruna_Pislar, Alex Platonov, @evansenter, Sukhdeep Singh, @dbltnk, @l32zhang']",https://arxiv.org/abs/2203.00715,"Cultural transmission is the domain-general social skill that allows agents to acquire and use information from each other in real-time with high fidelity and recall. In humans, it is the inheritance process that powers cumulative cultural evolution, expanding our skills, tools and knowledge across generations. We provide a method for generating zero-shot, high recall cultural transmission in artificially intelligent agents. Our agents succeed at real-time cultural transmission from humans in novel contexts without using any pre-collected human data. We identify a surprisingly simple set of ingredients sufficient for generating cultural transmission and develop an evaluation methodology for rigorously assessing it. This paves the way for cultural evolution as an algorithm for developing artificial general intelligence. ",Learning Robust Real-Time Cultural Transmission without Human Data
148,1499390426515578880,1056980495555272704,Jack Parker-Holder,"['Evolving Curricula with Regret-Based Environment Design\n\nWebsite: <LINK>\nPaper: <LINK>\n\nTL;DR: We introduce a new open-ended RL algorithm that produces complex levels and a robust agent that can solve them (e.g. below). \n\nHighlights ‚¨áÔ∏è! [1/N] <LINK>', 'We introduce ACCEL, a new algorithm that extends replay-based Unsupervised Environment Design (UED) (e.g. https://t.co/4T5CrHJfzh) by including an *editor*. The editor makes small changes to previously useful levels, which compound over time to produce complex structures. [2/N] https://t.co/UDkhxGE2Bi', 'Despite starting simple, levels in the replay buffer quickly become complex. Not only that, but ACCEL agents are capable of transfer to challenging human designed out-of-distribution environments, outperforming several strong baselines! [3/N] https://t.co/JTLKK2LNPe', 'Given the empirical gains, we wanted to see how far we could push the ACCEL agent. It turns out it gets over 50% success rate on mazes over an order of magnitude larger than the training curriculum! The next best baseline was PLR (25% success), while other methods failed. [4/N] https://t.co/zJ4MySFbJ7', 'We also tested ACCEL in the BipedalWalker environment. ACCEL produces agents that are robust to a wide range of individual challenges, while the baselines often struggle to solve even the simple test tasks. [5/N] https://t.co/Nkj3oDqu3P', 'Note that in all cases the complexity is emergent: There is no bonus for adding blocks or stumps, but this naturally occurs in the pursuit of high regret. Using the criteria from POET, we see that the ACCEL agent actually produces ‚ÄúExtremely Challenging‚Äù levels. [6/N] https://t.co/zFGKMHiQcQ', 'Given the strength and simplicity of ACCEL, we think there is huge potential for future work. In particular, scaling to larger problems may require additional mechanisms to directly encourage diversity or adapt agent configurations. Plenty to do here! [7/N]', 'This project was co-led with @MinqiJiang alongside @MichaelD1729 @samveIyan @j_foerst @egrefen &amp; @_rockt. \n\nThe code will be released soon, please get in touch if interested! [N/N, N=8]', '@utheprodigyn @MinqiJiang @MichaelD1729 @samveIyan @j_foerst @egrefen @_rockt Absolutely, feel free to get in touch üòÄ']",http://arxiv.org/abs/2203.01302,"It remains a significant challenge to train generally capable agents with reinforcement learning (RL). A promising avenue for improving the robustness of RL agents is through the use of curricula. One such class of methods frames environment design as a game between a student and a teacher, using regret-based objectives to produce environment instantiations (or levels) at the frontier of the student agent's capabilities. These methods benefit from their generality, with theoretical guarantees at equilibrium, yet they often struggle to find effective levels in challenging design spaces. By contrast, evolutionary approaches seek to incrementally alter environment complexity, resulting in potentially open-ended learning, but often rely on domain-specific heuristics and vast amounts of computational resources. In this paper we propose to harness the power of evolution in a principled, regret-based curriculum. Our approach, which we call Adversarially Compounding Complexity by Editing Levels (ACCEL), seeks to constantly produce levels at the frontier of an agent's capabilities, resulting in curricula that start simple but become increasingly complex. ACCEL maintains the theoretical benefits of prior regret-based methods, while providing significant empirical gains in a diverse set of environments. An interactive version of the paper is available at accelagent.github.io. ",Evolving Curricula with Regret-Based Environment Design
149,1499200104707678208,1100938005253181440,Jess Speedie,"['New #ApJ paper in which we test the capabilities of ALMA to indirectly discover hidden planets by detecting their spiral wake in continuum emission! üßµ1/10\n\n<LINK>\n\nw/  Dr. Ruobing Dong @PHASTatUVIC @uvicscience &amp; Dr. Richard Booth @ImperialAstro <LINK>', 'Young embedded planets create waves in their disk as they orbit around their star. The waves superimpose, and are wound into a spiral shape by the disk‚Äôs differential rotation.\n\nThe theory is well understood and predicted by hydrodynamic simulations, eg:\n\n(üí≥ Masset 2002) https://t.co/Xr9rNrOdfV', 'As astronomers interested in discovering these hidden planets, we want to use this fact to our advantage -- detecting a spiral is evidence for a forming planet!\n\nThe ALMA radio telescope, with its high angular resolution, should be perfect for the job.\n\n... right? https://t.co/ytUrISATbw', ""ALMA has already provided us with a treasure trove of observations of disks, but we're not exactly seeing planet-driven spirals all over the place.\n\nAre the spirals too faint? What are the best observing specifications to detect the signal? Which disks should we focus on? https://t.co/EyHxbTdhzO"", 'To find out, we ran a simple experiment.\n\nWe performed ~750 hydrodynamic simulations and fed the output through the ALMA simulator to see what ALMA would observe if the planets in those disks were real.\n\nIn our simulated disks, we *know* the spirals are there. Can ALMA see them? https://t.co/t60U0qRiAd', 'We determined that you have the best chance of the answer being yes, if...\n\n...the disk is adiabatic and cools slowly. Then, there is a *temperature* spiral that makes the intensity spiral nice and bright. Spirals are very difficult to observe in isothermal disks. https://t.co/2wrnMfRHvj', '...the planet is massive enough. A ~Neptune mass planet was as low as we could go, when all other conditions were optimal. https://t.co/6mfMZEEl2l', ""...you're only interested in *detecting* the spiral, not necessarily resolving it. \n\nA lower angular resolution observation has a lower spiral-signal-to-background ratio, yes, but a higher spiral-signal-to-*noise* ratio. That can lead to a more ROBUST (high S/N) discovery! https://t.co/Uf6x5rwOec"", ""...the disk is not too inclined.\n\nIf you know the disk geometry well, deprojection doesn't hurt too much -- unless the inclination is extreme. https://t.co/U1uMy4LxOo"", ""...the planet isn't opening too wide a gap or creating too narrow an outer ring.\n\nIn other words, a painter [planet] needs a canvas [background disk] to paint on if you want to see her art [spiral]. https://t.co/51Eldg68IC"", 'If all these things are the case, then planet-driven dust spirals could be detected with a few hours of integration time on ALMA. https://t.co/LigiTvG1kP', 'Read the paper here: https://t.co/rawY47iPN1\n\nDownload the data or an image gallery (compiled into movies) of all our model permutations: https://t.co/T8eEriDQGH']",https://arxiv.org/abs/2203.00692,"ALMA continuum observations of thermal emission from the dust component of protoplanetary disks have revealed an abundance of substructures that may be interpreted as evidence for embedded planets, but planet-driven spiral arms -- perhaps one of the most compelling lines of evidence -- have proven comparatively elusive. In this work, we test the capabilities of ALMA to detect the planet-driven spiral signal in continuum emission. Carrying out hydrodynamic simulations and radiative transfer calculations, we present synthetic Band 7 continuum images for a wide range of disk and observing conditions. We show that thermal mass planets at tens of au typically drive spirals detectable within a few hours of integration time, and the detectable planet mass may be as low as $\sim$Neptune mass ($0.3 \, M_{\rm th}$). The grains probed by ALMA form spirals morphologically identical to the underlying gas spiral. The temperature of the dust spiral is crucial in determining its contrast, and spirals are easier to detect in disks with an adiabatic equation of state and longer cooling times. Resolving the spiral is not necessary for its detection; with the help of residual maps, the optimal beam size is a few times the spiral width at a constant noise level. Finally, we show how the presence of gaps and rings can impair our ability to recognize co-located spirals. Our work demonstrates the planet-finding potential of the current design specification of ALMA, and suggests that observing capability is not the bottleneck in searching for spirals induced by thermal mass planets. ",Observing planet-driven dust spirals with ALMA
150,1499114065892823040,610427323,Desika Narayanan,"[""hey galaxy astronomers - have you ever wondered how well you're deriving the physical properties of your galaxies from SED fitting?  in a new paper led by super awesome grad student sidney lower, we dive into this!  \n\n<LINK>\n\n[1/"", ""now we've talked about this before (as I'm sure you remember)\n\nhttps://t.co/Ufd6WvJmVL\n\nwhere the tl;dr is that yes of course the model you implement for star formation histories matters (a lot), and non parametric SFHs get you much better derived physical properties [2/"", ""Today we're asking the question 'how well do you derive the dust attenuation curve from SED fitting'.   to do this, we're generating mock SEDs from cosmological simulations, and then pretending like we're observers and fitting those SEDs.  [3/"", ""(but, since we're not observers, we know the actual like real life true physical properties of our not actual not real life fake galaxies) [4/"", ""now we've talked about dust attenuation before.  but in case you don't remember, lemme remind you.  dust attenuation is that real difficult property to characterize that folds in both dust extinction, and star-dust geometry issues in galaxies (see my keynote art below) [5/: https://t.co/jKjM7qHJ3r"", ""(in case you want to do like a real real deep dive into dust attenuation, within the page and reference limits of an ARA&amp;A anyways, here's some light reading)\n\nhttps://t.co/lbU8kYK3SE\n\n[6/"", 'anyways, so yeah.  dust attenuation folds in the complexities of stars over here, and dust over there and all mixed up in galaxies.    so maybe unsurprisingly, the best thing to do in SED fitting is to try to encapsulate this effect.  this is exactly what we (sidney) did  [7/', 'we introduced a new model that includes a fraction of unobscured stellar light, which gives us a super flexible attenuation curve that we, like with flexible star formation histories, vary in the fitting process [8/', 'this does super well!   here, we look at how well the recovered attenuation curves from SED fitting compare to the true attenuation curves from the galaxy models.  L--&gt;R increases the flexibility of the attenuation curve we include in the SED fitting process [9/ https://t.co/bja9MdQnEW', 'quite naturally, this also results in improved derived physical properties like SFRs compared with more traditional models like uniform screens!    the cool part is - all of this is already implemented in the awesome SED fitting code prospector.   [10/', ""so anyways, now that you've made it this far, be flexible in your SFHs...be flexible in your dust attenuation modeling, and go out and observe awesome things. [11/11]""]",https://arxiv.org/abs/2203.00074,"One of the most common methods for inferring galaxy attenuation curves is via spectral energy distribution (SED) modeling, where the dust attenuation properties are modeled simultaneously with other galaxy physical properties. In this paper, we assess the ability of SED modeling to infer these dust attenuation curves from broadband photometry, and suggest a new flexible model that greatly improves the accuracy of attenuation curve derivations. To do this, we fit mock SEDs generated from the Simba cosmological simulation with the Prospector SED fitting code. We consider the impact of the commonly-assumed uniform screen model and introduce a new non-uniform screen model parameterized by the fraction of unobscured stellar light. This non-uniform screen model allows for a non-zero fraction of stellar light to remain unattenuated, resulting in a more flexible attenuation curve shape by decoupling the shape of the UV attenuation curve from the optical attenuation curve. The ability to constrain the dust attenuation curve is significantly improved with the use of a non-uniform screen model, with the median offset in UV attenuation decreasing from $-0.30$ dex with a uniform screen model to $-0.17$ dex with the non-uniform screen model. With this increase in dust attenuation modeling accuracy, we also improve the star formation rates (SFRs) inferred with the non-uniform screen model, decreasing the SFR offset on average by $0.12$ dex. We discuss the efficacy of this new model, focusing on caveats with modeling star-dust geometries and the constraining power of available SED observations. ","How Well Can We Measure Galaxy Dust Attenuation Curves? The Impact of
  the Assumed Star-Dust Geometry Model in SED Fitting"
151,1499087382666223617,1093725312368762886,Caleb Miles,"['New arXiv paper! üì£\n\nOn the Causal Interpretation of Randomized Interventional Indirect Effects <LINK>', ""Causal mediation analysis is known to be hard in that it relies on heavier ass'ns that usual. A dominant trend in recent years has been to circumvent these ass'ns by redefining the target causal parameter, leading to what are known as *randomized interventional indirect effects*."", 'In this article, I show that without stronger assumptions, these effects lack a true mediational interpretation in the conventional sense of the word. What‚Äôs different about these effects? Let‚Äôs focus on the analog to the natural indirect effect (NIE), which I denote (NIE^R). https://t.co/U3Eedx9XWQ', 'The NIE involves an intervention setting each subject‚Äôs mediator M to M(a*): the counterfactual value it naturally would have taken had we intervened to set the exposure to a*. https://t.co/CkhdMTpBse', 'On the other hand, the NIE^R involves an intervention that instead sets each subject‚Äôs M to a *random draw* from the conditional distribution of M(a‚Äô) given covariates C. Pretty subtle difference, no? https://t.co/rNRhv4oVGy', 'There are 2 key settings in which the NIE^R is identified, but the NIE is not. The first is in the presence of an exposure-induced confounder L as in the DAG below. We can‚Äôt rule these out, even in a well-designed randomized trial, without sinking our ability to study mediation. https://t.co/NRxNOsq0xv', 'The second is in the absence of cross-world counterfactual independence assumptions. For example, Y(a,m) \\perp M(a*) | C for a != a*. Such independencies are not falsifiable, since the two counterfactuals arise under conflicting interventions, or in ‚Äúseparate worlds‚Äù.', 'So, what‚Äôs wrong with the NIE^R? For an indirect effect through M to exist at the individual level, (a) a change in A must cause a change in M, and (b) the induced change in M must cause a change in Y. Formalizing this leads to the *sharp mediational null*: https://t.co/3LRcfu0PJI', 'I then define the *sharp null criterion* (SNC). Intuitively, an indirect effect measure should be null whenever there is no individual-level effect for anyone in the population. https://t.co/Ft5ZDtdX6i', ""I show that the NIE^R fails to satisfy the SNC without stronger ass'ns than have been given for its identification. I give a number of such ass'ns; however, under all but one of these, the NIE is also identified, so the NIE^R lacks an advantage as an indirect effect measure."", 'This is not to say that randomized interventional indirect effects are not useful! I provide alternative, non-mediational, but causally meaningful interpretations of such effects. In fact, variations of these have proven very relevant in disparities research!', 'To illustrate these findings, I discuss the causal interpretation of the NIE^R under various assumptions in an HIV example studying the role adherence plays in mediating the effect of ART regimen on virologic failure.', ""üîë takeaway: Resist the temptation to imbue interventional indirect effects with a mediational interpretation when the SNC is not satisfied, as will be the case without stronger ass'ns. It's often not an appropriate substitute for the NIE when the scientific Q concerns mediation."", '@edwardhkennedy Thanks, Edward! Hopefully my reviewers will agree ü§û', '@biosbenk Thanks, David! Good eye. Indeed, I discuss such an interpretation in the article. ""[T]he NIE^R can be interpreted as the difference in means of two subject-specific averages of the m-specific individual counterfactual Y (a, m), comparing one that is averaged with respect to...', '@biosbenk the conditional distribution of M(a) given C and another that is averaged with respect to the conditional distribution of M(a‚àó) given C.""', '@biosbenk You mean you smashed the like button without reading the entire paper first?']",https://arxiv.org/abs/2203.00245,"Identification of standard mediated effects such as the natural indirect effect relies on heavy causal assumptions. By circumventing such assumptions, so-called randomized interventional indirect effects have gained popularity in the causal mediation literature. In this article, I introduce an essential criterion (the sharp null criterion) that any indirect effect measure ought to satisfy in order to have a true mediational interpretation. Namely, an indirect effect measure must be null whenever there is no individual-level indirect effect. I show that without stronger assumptions, randomized interventional indirect effects do not satisfy this criterion. I additionally discuss alternative causal interpretations of such effects. ","On the Causal Interpretation of Randomized Interventional Indirect
  Effects"
152,1499076741129805829,111661096,Oscar Clivio,"['Matching is a popular method for treatment effect estimation but does not scale well with the data dimension. In our new #AISTATS2022 paper, we show how neural networks can give low-dimensional balancing scores readily usable for matching. Check it out : <LINK>', 'Work done with @fabianfalck @BrieucLehmann @GeorgeDeligian9 @cholmesuk']",https://arxiv.org/abs/2203.00554,"Traditional methods for matching in causal inference are impractical for high-dimensional datasets. They suffer from the curse of dimensionality: exact matching and coarsened exact matching find exponentially fewer matches as the input dimension grows, and propensity score matching may match highly unrelated units together. To overcome this problem, we develop theoretical results which motivate the use of neural networks to obtain non-trivial, multivariate balancing scores of a chosen level of coarseness, in contrast to the classical, scalar propensity score. We leverage these balancing scores to perform matching for high-dimensional causal inference and call this procedure neural score matching. We show that our method is competitive against other matching approaches on semi-synthetic high-dimensional datasets, both in terms of treatment effect estimation and reducing imbalance. ",Neural Score Matching for High-Dimensional Causal Inference
153,1499019305475887107,336358884,Swati Gupta,"['Excited about our new paper with Yuri Faenza and Xuan Zhang on ""Discovering Opportunities in New York City\'s Discovery Program"". Check it out here: <LINK> [1/n]', 'The Discovery Program is an affirmative action policy for admissions to the top specialized high schools in New York City. It has been instrumental in increasing the number of disadvantaged students admitted to top high schools. [2/n]', 'However, our analysis on past data from NY DOE shows that this program has (i) created ~950 in-group blocking pairs, impacting ~650 students yearly, (ii) created an incentive for top disadvantaged students to underperform! (lower performers matched to better  schools). [3/n]', 'In this work, we analyze to mechanisms, which minimally change the discovery program - Joint Seat Allocation (JSA) and Minority Reserve (MR). Both are weakly group-strategy proof and result in no in-group blocking pairs. [4/n]', 'We show that for ""highly competitive"" markets such as NY High Schools, JSA dominates MR. This leads to our proposal to DOE: use JSA instead of Discovery, to account for students\' preference of summer school seats as higher than general admissions seats.', '@XuanZhang816 !!']",https://arxiv.org/abs/2203.00544,"Discovery program (DISC) is an affirmative action policy used by the New York City Department of Education (NYC DOE). It has been instrumental in increasing the number of admissions for disadvantaged students at specialized high schools. However, our empirical analysis of the student-school matches shows that about 950 in-group blocking pairs were created each year amongst the disadvantaged group of students, impacting about 650 disadvantaged students. Moreover, we find that this program usually benefits lower-performing disadvantaged students more than the top-performing ones, thus unintentionally creating an incentive to under-perform. In this work, we explore two affirmative action policies that can be used to minimally modify and improve the discovery program: minority reserve (MR) and joint-seat allocation (JSA). We show that (i) both MR and JSA result in no in-group blocking pairs, and (ii) JSA is weakly group strategy-proof, ensures that at least one disadvantaged is not worse off, and when reservation quotas are carefully chosen then no disadvantaged student is worse-off. In the general setting, we show that there is no clear winner in terms of the matchings provided by DISC, JSA and MR, from the perspective of disadvantaged students. We however characterize a condition for markets, that we term high competitiveness, where JSA dominates MR for disadvantaged students. This condition is verified in markets when there is a higher demand for seats than supply, and the performances of disadvantaged students are significantly lower than that of advantaged students. Data from NYC DOE satisfy the high competitiveness condition, and for this dataset our empirical results corroborate our theoretical predictions, showing the superiority of JSA. We believe that the discovery program, and more generally affirmative action mechanisms, can be changed for the better by implementing JSA. ","Discovering Opportunities in New York City's Discovery Program: an
  Analysis of Affirmative Action Mechanisms"
154,1511313491956846599,249039303,Salman Khan,['#CVPR2022 We develop an energy-based model (EBM) for incremental learning. The EBM aligns old and new task latent representations to minimize forgetting. üî•‚ö°Ô∏è\n\nPaperüìú<LINK>\nCodeüßë\u200düíª<LINK> <LINK>'],https://arxiv.org/abs/2203.14952,"Deep learning models tend to forget their earlier knowledge while incrementally learning new tasks. This behavior emerges because the parameter updates optimized for the new tasks may not align well with the updates suitable for older tasks. The resulting latent representation mismatch causes forgetting. In this work, we propose ELI: Energy-based Latent Aligner for Incremental Learning, which first learns an energy manifold for the latent representations such that previous task latents will have low energy and the current task latents have high energy values. This learned manifold is used to counter the representational shift that happens during incremental learning. The implicit regularization that is offered by our proposed methodology can be used as a plug-and-play module in existing incremental learning methodologies. We validate this through extensive evaluation on CIFAR-100, ImageNet subset, ImageNet 1k and Pascal VOC datasets. We observe consistent improvement when ELI is added to three prominent methodologies in class-incremental learning, across multiple incremental settings. Further, when added to the state-of-the-art incremental object detector, ELI provides over 5% improvement in detection accuracy, corroborating its effectiveness and complementary advantage to existing art. ",Energy-based Latent Aligner for Incremental Learning
155,1509367746966192132,1375406310930313220,Rak-Kyeong Seong,['New paper out today!\n<LINK> <LINK>'],https://arxiv.org/abs/2203.15816,"Reflexive polytopes in n dimensions have attracted much attention both in mathematics and theoretical physics due to their connection to Fano n-folds and mirror symmetry. This work focuses on the 18 regular reflexive polytopes corresponding to smooth Fano 3-folds. For the first time, we show that all 18 regular reflexive polytopes have corresponding 2d (0,2) gauge theories realized by brane brick models. These 2d gauge theories can be considered as the worldvolume theories of D1-branes probing the toric Calabi-Yau 4-singularities whose toric diagrams are given by the associated regular reflexive polytopes. The generators of the mesonic moduli space of the brane brick models are shown to form a lattice of generators due to the charges under the rank 3 mesonic flavor symmetry. It is shown that the lattice of generators is the exact polar dual reflexive polytope to the corresponding toric diagram of the brane brick model. This duality not only highlights the close relationship between the geometry and 2d gauge theory, but also opens up pathways towards new discoveries in relation to reflexive polytopes and brane brick models. ","Fano 3-Folds, Reflexive Polytopes and Brane Brick Models"
156,1508252725498294272,716963985245929472,Khai Nguyen,"['In our new paper <LINK>, we investigate the usage of amortized optimization in finding informative projecting directions in mini-batch sliced Wasserstein.', 'Seeking good projecting directions often requires iterative loops for optimization. This process is also repeated on all pairs of mini-batches. Therefore, it is computationally expensive.', 'Leveraging the inside of amortized optimization, we propose three types of amortized models including the linear model, the generalized linear model, and the non-linear model.', 'These models are trained to predict the best vector on the unit-hypersphere that can maximize the Wasserstein distance between the two projected one-dimensional probability measures of every two mini-batch probability measures.', 'We demonstrate the benefit of the new approach on training generative models in terms of generative quality, computational time, and computational memory. We refer to the paper for more details.']",https://arxiv.org/abs/2203.13417,"Seeking informative projecting directions has been an important task in utilizing sliced Wasserstein distance in applications. However, finding these directions usually requires an iterative optimization procedure over the space of projecting directions, which is computationally expensive. Moreover, the computational issue is even more severe in deep learning applications, where computing the distance between two mini-batch probability measures is repeated several times. This nested-loop has been one of the main challenges that prevent the usage of sliced Wasserstein distances based on good projections in practice. To address this challenge, we propose to utilize the learning-to-optimize technique or amortized optimization to predict the informative direction of any given two mini-batch probability measures. To the best of our knowledge, this is the first work that bridges amortized optimization and sliced Wasserstein generative models. In particular, we derive linear amortized models, generalized linear amortized models, and non-linear amortized models which are corresponding to three types of novel mini-batch losses, named amortized sliced Wasserstein. We demonstrate the favorable performance of the proposed sliced losses in deep generative modeling on standard benchmark datasets. ","Amortized Projection Optimization for Sliced Wasserstein Generative
  Models"
157,1506210581338112001,1506209211914596359,Pedro Parra-Rivas,['New paper just out! <LINK>'],https://arxiv.org/abs/2203.11193,"We demonstrate neuron-like spiking dynamics in the asymmetrically driven dissipative photonic Bose-Hubbard dimmer model which describes two coupled nonlinear passive Kerr cavities. Spiking dynamics appear due to the excitable nature of the system. In this context, excitable excursions in the phase space correspond to spikes in the temporal evolution of the field variables. In our case, excitability is mediated by the destruction of an oscillatory state in a global homoclinic bifurcation. In this type of excitability (known as type-I) the period of the oscillatory state diverges when approaching the bifurcation. Beyond this point, the system exhibits excitable dynamics under the application of suitable perturbations. We have also characterized the effect that additive Gaussian noise has on the spiking dynamics, showing that the system undergoes a coherence resonance for a given value of the noise strength. ","Neuron-like spiking dynamics in the asymmetrically-driven dissipative
  photonic Bose-Hubbard dimer"
158,1504876006921977858,134172617,Jonathan Davies,"['I have a new paper on the arXiv, with @apontzen and @rcrain_astro! We show that galaxy mergers can transform the baryon cycle by inducing AGN feedback. As a result, mergers can cause quenching billions of years after they occur! <LINK>', 'This study was made possible by the genetic modification technique - we are able to enhance or suppress the influence of an individual merger to examine its effects on the galaxy-CGM ecosystem. Find out more here: https://t.co/Y4rzv4QVOy']",https://arxiv.org/abs/2203.08157,"We use zoom simulations to show how merger-driven disruption of the gas disc in a galaxy provides its central active galactic nucleus (AGN) with fuel to drive outflows that entrain and expel a significant fraction of the circumgalactic medium (CGM). This in turn suppresses replenishment of the interstellar medium, causing the galaxy to quench up to several Gyr after the merger. We start by performing a zoom simulation of a present-day star-forming disc galaxy with the EAGLE galaxy formation model. Then, we re-simulate the galaxy with controlled changes to its initial conditions, using the genetic modification technique. These modifications either increase or decrease the stellar mass ratio of the galaxy's last significant merger, which occurs at $z\approx 0.74$. The halo reaches the same present-day mass in all cases, but changing the mass ratio of the merger yields markedly different galaxy and CGM properties. We find that a merger can unlock rapid growth of the central supermassive black hole if it disrupts the co-rotational motion of gas in the black hole's vicinity. Conversely, if a less disruptive merger occurs and gas close to the black hole is not disturbed, the AGN does not strongly affect the CGM, and consequently the galaxy continues to form stars. Our result illustrates how a unified view of AGN feedback, the baryon cycle and the interstellar medium is required to understand how mergers and quenching are connected over long timescales. ","Galaxy mergers can initiate quenching by unlocking an AGN-driven
  transformation of the baryon cycle"
159,1504023892691763201,1151278579193278465,Vivian Poulin,"['Thanks to the EFT of LSS, we were able to constrain decaying Dark Matter with BOSS-DR12! This has important consequences in the context of the S8 tension. Proud of 1st paper by new grad student Th√©o Simon w/ @GFAbellan, peizhi Du, Yuhsin Tsai!\n<LINK>', 'Concretely, we looked at two phenomenological decaying dark matter (DDM) models. One in which only a fraction of DM decays into massless invisible particles, and another in which all of DM decays, but the invisible decay products are both massive and massless.', 'If the products are massless (modeled as `dark radiation\') we constrain the lifetime of DM to be greater than 250 Gyrs! (thats almost 20 times the age of the universe). But if DM turns out to be partly made of ""short lived"" stuff, only 2.2% of it may have decayed away by now.', 'If the invisible products are massive (think of it as producing an exotic massive neutrinos), the model could explain a low-S8. However BOSS-DR12 strongly restricts the lifetime of DM! the model changes from a lifetime of 50 to 120 Gyrs with the EFTofLSS.', 'We could do even better with improved theoretical prediction and a better resolution of the galaxy power spectrum as a function of redshift (e.g. w/ DESI, Euclid, VRO)', ""This study was only made possible because clever people developed awesome tools (EFTofLSS and codes) for non-experts like me to use (see paper for all references). Thanks in particular to Pierre Zhang for his precious help with pybird! Can't wait to learn more about the EFTofLSS.""]",https://arxiv.org/abs/2203.07440,"We update cosmological constraints on two decaying dark matter models in light of BOSS-DR12 data analyzed under the Effective Field Theory of Large-Scale Structures (EFTofLSS) formalism, together with Planck, Pantheon and other BOSS measurements of the baryonic acoustic oscillation (BAO). In the first model, a fraction $f_{\rm dcdm}$ of cold dark matter (CDM) decays into dark radiation (DR) with a lifetime $\tau$. In the second model (recently suggested as a potential resolution to the $S_8$ tension), all the CDM decays with a lifetime $\tau$ into DR and a massive warm dark matter (WDM) particle, with a fraction $\varepsilon$ of the CDM rest mass energy transferred to the DR. Using numerical codes from the recent literature, we perform the first calculation of the mildly non-linear (matter and galaxy) power spectra with the EFTofLSS for these two models. In the case of DR products, we obtain the constraints $f_{\rm dcdm}\lesssim0.022$ (95\% C.L.) for lifetimes shorter than the age of the universe, and $\tau/f_{\rm dcdm} \gtrsim 250$ Gyr in the long-lived regime assuming $f_{\rm dcdm}\to1$. We show that Planck data contributes the most to these constraints, with EFTofBOSS providing a marginal improvement over conventional BAO and redshift space distortions ($f\sigma_8$) data. In the case of DR and WDM decay products, we find that EFTofBOSS data significantly improves the constraints at 68\% C.L. on the CDM lifetime with a $S_8$ prior from KiDS-1000. We show that, in order to fit EFTofBOSS data while lowering $S_8$ to match KiDS-1000, the best-fit model has a longer lifetime $\tau = 120$ Gyr, with a larger kick velocity $v_{\rm kick}/c \simeq \varepsilon \simeq 1.2\%$, than that without EFTofBOSS ($\tau = 43$ Gyr, $\varepsilon =0.6\%$). We anticipate that future surveys will provide exquisite constraints on such models. ","Constraining decaying dark matter with BOSS data and the effective field
  theory of large-scale structures"
160,1503370841957933056,762359343656361984,James Zou,"['Our new paper explains the intriguing #AI #ModalityGap: in multi-modal AI, there are large gaps in the representation space separating different data types.\n\nWe show changing the gap improves zero-shot learning and fairness <LINK>\nCode: <LINK> <LINK>', 'Interestingly, modality gaps are created at model initialization and are reinforced by contrastive learning.\n\nGreat work by @liang_weixin @Zhang_Yu_hui and Yongchan Kwon to explain this gap phenomenon mathematically and empirically!']",https://arxiv.org/abs/2203.02053,"We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (e.g. images and text) are embedded at arm's length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone. As a consequence, in a multi-modal model with two encoders, the representations of the two modalities are clearly apart when the model is initialized. During optimization, contrastive learning keeps the different modalities separate by a certain distance, which is influenced by the temperature parameter in the loss function. Our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model's downstream zero-shot classification performance and fairness. Our code and data are available at this https URL ","Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive
  Representation Learning"
161,1502200538124935168,1499062776312090632,Fawaz Sammani,['Are heatmaps enough for the expressiveness of XAI? Check out our new #CVPR2022 paper: NLX-GPT which is a model for Natural Language Explanations for vision and vision-language tasks  <LINK> <LINK>'],https://arxiv.org/abs/2203.05081,"Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15$\times$ faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a self-evaluation framework that requires no labels. Code is at: this https URL ","NLX-GPT: A Model for Natural Language Explanations in Vision and
  Vision-Language Tasks"
162,1501254481660448775,1005061118547714049,Melissa Newham,['Who pays for gifts to physicians? Marica Valente and I have a new working paper on industry payments to doctors using machine-learning methods to estimate effects and cost impacts for diabetes @maricaibox \nAvailable: <LINK> <LINK>'],https://arxiv.org/abs/2203.01778,"This paper estimates the impact of gifts - monetary or in-kind payments - from pharmaceutical firms on physicians' prescription decisions and drug costs in the US. Using exhaustive micro data on prescriptions for anti-diabetic drugs from Medicare Part D, we find that payments cause physicians to prescribe more brand drugs. On average, for every dollar spent, payments generate a $6 increase in drug costs. We then estimate heterogeneous causal effects via machine-learning methods. We find large heterogeneity in responses to payments across physicians. Differences are predominantly explained by the insurance coverage of patients: physicians prescribe more brand drugs in response to payments when patients benefit from subsidies that reduce out-of-pocket drug costs. Finally, we estimate that a gift ban would reduce drug costs to treat diabetes by 3%. ","Who pays for gifts to physicians? Heterogeneous effects of industry
  payments on drug costs"
163,1499212752472182793,1012125662117851136,Edward Kennedy,"['New paper!\n<LINK>\n\nHow do trt effects vary across people? Such heterogeneity is crucial for optimal allocation, generalizability, etc\n\nMany methods out there... but optimality\'s been unsolved. What is ""best""?\n\nWe derive minimax rates &amp; give new optimal estimator <LINK>', 'Minimax rates give a benchmark for the best possible performance of any estimator - ie when can you stop searching for better methods?\n\nAlso an important measure of fundamental limits - how difficult is this in statistical sense?\n\nhttps://t.co/jdhDkDH4bk\n\nhttps://t.co/kTnywEv6TQ https://t.co/raizPSn2wj', 'We show the CATE minimax rate has an unusual elbow phenomenon, &amp; interpolates bw regression &amp; functional rates\n\nie the CATE is a strange hybrid beast\n\nOur lower bd uses a ""mixed"" fuzzy hypotheses construction. And our estimator uses *localized* higher-order influence functions https://t.co/bsC48Asat4', 'The CATE minimax rate very clearly mixes regression &amp; functional est rates:\n\nMinimax rate for regression scales with d/2g (dim=d, smoothness=g)\n\nMinimax rate for functional estimation scales with d/4s (nuisance smoothness=s)\n\nAnd the CATE minimax rate scales with (d/2g + d/4s) ! https://t.co/yJIWDmlrwb', ""This paper has meant a lot to me\n\nIt's pretty different from my previous work - so I learned lots of fun new tools\n\nAlso:\n- resolves something I pondered for ~a decade\n- wrote it during a pandemic while raising 2 young kids\n- made for some really fun mtgs, thanks to Siva &amp; Larry https://t.co/GKe8YrDpJ7""]",https://arxiv.org/abs/2203.00837,"Estimation of heterogeneous causal effects - i.e., how effects of policies and treatments vary across subjects - is a fundamental task in causal inference, playing a crucial role in optimal treatment allocation, generalizability, subgroup effects, and more. Many flexible methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but questions surrounding optimality have remained largely unanswered. In particular, a minimax theory of optimality has yet to be developed, with the minimax rate of convergence and construction of rate-optimal estimators remaining open problems. In this paper we derive the minimax rate for CATE estimation, in a nonparametric model where distributional components are Holder-smooth, and present a new local polynomial estimator, giving high-level conditions under which it is minimax optimal. More specifically, our minimax lower bound is derived via a localized version of the method of fuzzy hypotheses, combining lower bound constructions for nonparametric regression and functional estimation. Our proposed estimator can be viewed as a local polynomial R-Learner, based on a localized modification of higher-order influence function methods; it is shown to be minimax optimal under a condition on how accurately the covariate distribution is estimated. The minimax rate we find exhibits several interesting features, including a non-standard elbow phenomenon and an unusual interpolation between nonparametric regression and functional estimation rates. The latter quantifies how the CATE, as an estimand, can be viewed as a regression/functional hybrid. We conclude with some discussion of a few remaining open problems. ",Minimax rates for heterogeneous causal effect estimation
164,1512496399786590210,966343078524186625,Mitchell Revalski,"['Interested in estimating the densities of AGN outflows? See Figure 1 in our new publication (<LINK>), where we find that the densities of narrow line region outflows can span more than 6 orders of magnitude depending on distance from the supermassive black hole!']",https://arxiv.org/abs/2203.07387,"Active galactic nuclei (AGN) can launch outflows of ionized gas that may influence galaxy evolution, and quantifying their full impact requires spatially resolved measurements of the gas masses, velocities, and radial extents. We previously reported these quantities for the ionized narrow-line region (NLR) outflows in six low-redshift AGN, where the gas velocities and extents were determined from Hubble Space Telescope long-slit spectroscopy. However, calculating the gas masses required multi-component photoionization models to account for radial variations in the gas densities, which span $\sim$6 orders of magnitude. In order to simplify this method for larger samples with less spectral coverage, we compare these gas masses with those calculated from techniques in the literature. First, we use a recombination equation with three different estimates for the radial density profiles. These include constant densities, those derived from [S II], and power-law profiles based on constant values of the ionization parameter ($U$). Second, we use single-component photoionization models with power-law density profiles based on constant $U$, and allow $U$ to vary with radius based on the [O III]/H$\beta$ ratios. We find that assuming a constant density of $n_\mathrm{H} =$ 10$^2$ cm$^{-3}$ overestimates the gas masses for all six outflows, particularly at small radii where the outflow rates peak. The use of [S II] marginally matches the total gas masses, but also overestimates at small radii. Overall, single-component photoionization models where $U$ varies with radius are able to best match the gas mass and outflow rate profiles when there are insufficient emission lines to construct detailed physical models. ","Quantifying Feedback from Narrow Line Region Outflows in Nearby Active
  Galaxies. IV. The Effects of Different Density Estimates on the Ionized Gas
  Masses and Outflow Rates"
165,1512362967748259851,132623500,Dr. Kelley Hess,"['ICYMI Tday is th 1 week arXiv annivrsry of our acceptd paper ""The Apertif Science Verification Campaign - Characteristics of Polarised Sources"" <LINK>. We find fractn of polarised sources is 10.6%; w median fractionl polarisatn of 4.7%, dominatd by radioloud AGN. <LINK>']",https://arxiv.org/abs/2203.16925,"We analyse five early science datasets from the APERture Tile in Focus (Apertif) phased array feed system to verify the polarisation capabilities of Apertif in view of future larger data releases. We aim to characterise the source population of the polarised sky in the L-Band using polarised source information in combination with IR and optical data. We use automatic routines to generate full field-of-view Q- and U-cubes and perform RM-Synthesis, source finding, and cross-matching with published radio, optical, and IR data to generate polarised source catalogues. SED-fitting routines were used to determine photometric redshifts, star-formation rates, and galaxy masses. IR colour information was used to classify sources as AGN or star-forming-dominated and early- or late-type. We surveyed an area of 56deg$^2$ and detected 1357 polarised source components in 1170 sources. The fraction of polarised sources is 10.57% with a median fractional polarisation of 4.70$\pm$0.14%. We confirmed the reliability of the Apertif measurements by comparing them with polarised cross-identified NVSS sources. Average RMs of the individual fields lie within the error of the best Milky Way foreground measurements. All of our polarised sources were found to be dominated by AGN activity in the radio regime with most of them being radio-loud (79%) and of the FRII class (87%). The host galaxies of our polarised source sample are dominated by intermediate disc and star-forming disc galaxies. The contribution of star formation to the radio emission is on the order of a few percent for $\approx$10% of the polarised sources while for $\approx$90% it is completely dominated by the AGN. We do not see any change in fractional polarisation for different star-formation rates of the AGN host galaxies. ","The Apertif science verification campaign - Characteristics of polarised
  radio sources"
166,1511761502159605764,3218435095,Tianqi Chen (Terence),"['We propose a mathematically rigorous definition for the tensor network stack approach, that compress a large amount of tensor networks into a single one without changing their structures and configurations. \n<LINK>']",https://arxiv.org/abs/2203.16338,"The tensor network, as a facterization of tensors, aims at performing the operations that are common for normal tensors, such as addition, contraction and stacking. However, due to its non-unique network structure, only the tensor network contraction is so far well defined. In this paper, we propose a mathematically rigorous definition for the tensor network stack approach, that compress a large amount of tensor networks into a single one without changing their structures and configurations. We illustrate the main ideas with the matrix product states based machine learning as an example. Our results are compared with the for loop and the efficient coding method on both CPU and GPU. ",Stack operation of tensor networks
167,1510918086509137921,1144657431382958082,Jannis Kurtz,"['If you are interested in #robustoptimization with discrete uncertainty sets maybe you want to check our new short paper: <LINK>\n\nWith Marc Goerigk we study the problem of selecting start scenarios for iterative scenario generation methods for RO problems    1/n', 'Our heuristic learns the relevance of a scenario by extracting information from training data. The main observation is that even choosing a single start scenario can lead to a significant benefit for the subsequent iterative process which was quite surprising for us.', 'Hence using available training data can be useful. Unfortunately there is still no data basis for robust optimization instances. In our case each scenario has to be labeled to be relevant or not, which implies that the instance need to be solved to optimality to be labeled.', 'We circumvent this problem by constructing instances where relevant scenarios are known by construction. However testing the idea for more realistic instances is desirable. Marc Goerigk recently started to collect instances https://t.co/3QzYWa5abW Feel free to submit instances!']",https://arxiv.org/abs/2203.16642,"In this work we study robust one- and two-stage problems with discrete uncertainty sets which are known to be hard to solve even if the underlying deterministic problem is easy. Popular solution methods iteratively generate scenario constraints and possibly second-stage variables. This way, by solving a sequence of smaller problems, it is often possible to avoid the complexity of considering all scenarios simultaneously. A key ingredient for the performance of the iterative methods is a good selection of start scenarios. In this paper we propose a data-driven heuristic to seed the iterative solution method with a set of starting scenarios that provide a strong lower bound early in the process, and result in considerably smaller overall solution times compared to other benchmark methods. Our heuristic learns the relevance of a scenario by extracting information from training data based on a combined similarity measure between robust problem instances and single scenarios. Our experiments show that predicting even a small number of good start scenarios by our method can considerably reduce the computation time of the iterative methods. ",Data-driven Prediction of Relevant Scenarios for Robust Optimization
168,1509894844537319431,1333825478730412032,Aditya Thapa,['Very excited to share this: We did a multi-wavelength study of a strange ‚Äúchanging look‚Äù active galactic nucleus. I worked on the Swift-XRT/UVOT (X-ray and UV) data of the source and I am grateful for Dr. @SibasishLaha‚Äôs guidance throughout the project. <LINK>'],https://arxiv.org/abs/2203.07446,"The nearby type-II AGN 1ES1927+654 went through a violent changing-look (CL) event beginning December 2017 during which the optical and UV fluxes increased by four magnitudes over a few months, and broad emission lines newly appeared in the optical/UV. By July 2018 the X-ray coronal emission had completely vanished, only to reappear a few months later. In this work we report the evolution of the radio, optical, UV and X-rays from the pre-flare state through mid-2021 with new and archival data from the Very Long Baseline Array (VLBA), the European VLBI Network, the Very Large Array (VLA), the Telescopio Nazionale Galileo (TNG), Gran Telescopio Canarias (GTC), The Neil Gehrels Swift observatory and XMM-Newton. The main results from our work are: (1) The source has returned to its pre-CL state in optical, UV, and X-ray; the disk-corona relation has been re-established as has been in the pre-CL state, with an $\alpha_{\rm OX}\sim 1.02$. The optical spectra are dominated by narrow emission lines. (2) The UV light curve follows a shallower slope of $\propto t^{-0.91\pm 0.04}$ compared to that predicted by a tidal disruption event. We conjecture that a magnetic flux inversion event is the possible cause for this enigmatic event. (3) The compact radio emission which we tracked in the pre-CL (2014), during CL (2018) and post-CL(2021) at spatial scales $<1$ pc was at its lowest level during the changing look event in 2018, nearly contemporaneous with a low $2-10$ keV emission. The radio to X-ray ratio of the compact source $L_{\rm Radio}/L_{\rm X-ray}\sim 10^{-5.5}$, follows the Gudel-Benz relation, typically found in coronally active stars, and several AGN. (4) We do not detect any presence of nascent jets at the spatial scales of $\sim 5-10$ pc. ","A radio, optical, UV and X-ray view of the enigmatic changing look
  Active Galactic Nucleus 1ES~1927+654 from its pre- to post-flare states"
169,1509849211533201412,584142796,Carl Rodriguez,"[""Paper day (not an April Fool's joke, pun notwithstanding)!  The second in our Great Balls of FIRE series, where we (me, @MikeGrudic, @ZachHafen, Astrid Lambert +) study the formation and evolution of globular clusters in a realistic MHD galaxy simulation <LINK>: <LINK>"", 'First paper in the series (https://t.co/nCis74DJqX)  identified all the clusters that formed over the history of the MHD m12i FIRE-2 galaxy, and using detailed simulations of collapsing molecular clouds, mapped them to a catalog of young cluster masses, radii, metallicities, etc:', 'This paper (https://t.co/pIF78XsZOH) takes the next step: integrating ~1000 of the most massive clusters (10^5 to 10^7 stars) in the catalog forward with a H√©non-style N-body code to the present day, with the initial conditions and tidal fields taken directly from the galaxy sim.', ""This is, to my knowledge, the first study of it's kind using a star-by-star N-body approach that can integrate clusters massive enough to become the old globular clusters we see in the MW and other galaxies.  The model predicts about 148 GCs in the m12i galaxy at z=0: https://t.co/4GugJzHoVB"", ""Because we're using realistic star cluster models, we can extract v-band surface brightness profiles, fit them to King 1966 models like observers do, and directly compare them to the properties of GCs in the MW and M31.  Here, we show the masses and radii of our clusters. https://t.co/PlUXAT0ecj"", 'Our clusters are typically smaller (in both core radius and mass) than GCs in the MW and M31.  This turns out to be related to the specific formation history of the host galaxy.  It turns out that clusters born later tend to lose mass more quickly.', 'At the same time, forming GCs later means they form at higher metallicities, and retain fewer stellar mass black holes due to natal kicks.  This means they undergo core collapse FASTER than older, metal pool GCs.', 'There is a LOT more information and analysis in the paper, mapping out how exactly the history of this galaxy shapes its z=0 GC population, so I encourage you to take a look!', 'Damnit, Lamberts, not Lambert!', '@evbauer_astro @MikeGrudic @ZachHafen Yeah I completely forgot about April fools.  Course one could argue that posting a paper with an April Fools-like title but real content is the greatest form of the April fools paper‚Ä¶']",https://arxiv.org/abs/2203.16547,"The current generation of galaxy simulations can resolve individual giant molecular clouds, the progenitors of dense star clusters. But the evolutionary fate of these young massive clusters (YMCs), and whether they can become the old globular clusters (GCs) observed in many galaxies, is determined by a complex interplay of internal dynamical processes and external galactic effects. We present the first star-by-star $N$-body models of massive ($N\sim10^5-10^7$) star clusters formed in a FIRE-2 MHD simulation of a Milky Way-mass galaxy, with all of the relevant initial conditions and galactic tidal effects extracted directly from the cosmological simulation. We randomly select 895 ($\sim 30\%$) of the YMCs with $ > 6\times10^4M_{\odot}$ from Grudi\'c et al. 2022 and integrate them to the present day using the Cluster Monte Carlo Code, CMC. This procedure predicts a MW-like system with 148 GCs, most of which were formed during the early, bursty mode of star formation in the galaxy. Our GCs are younger, less massive, and more core collapsed than clusters in the Milky Way or M31. This is a direct result of the assembly history and age-metallicity relationship of the GCs' host galaxy: younger clusters are preferentially born in stronger galactic tidal fields and initially retain fewer stellar-mass black holes, causing them to lose mass faster and reach core collapse sooner than their older counterparts. Our results suggest that the masses and core/half-light radii of GCs are shaped not only by internal dynamical processes, but by the specific evolutionary history of their host galaxies as well. These results emphasize that $N$-body studies with realistic stellar physics are crucial to understanding the evolution and present-day properties of galactic GC systems. ","Great Balls of FIRE II: The evolution and destruction of star clusters
  across cosmic time in a Milky Way-mass galaxy"
170,1509840163651870743,795318493675712512,Maura Pintor,"['ImageNet-Patch (<LINK>)\n\nWe added a cool Jupyter notebook tutorial ‚úÖ\nYou can open it in Google Colab or run it locally ‚úÖ\n\nThanks, @DAngioni97! \n\\w @biggiobattista @ambrademontis @zangobot @sotgiu_angelo Fabio Roli\n\nFind it here: <LINK>']",https://arxiv.org/abs/2203.04412,"Adversarial patches are optimized contiguous pixel blocks in an input image that cause a machine-learning model to misclassify it. However, their optimization is computationally demanding, and requires careful hyperparameter tuning, potentially leading to suboptimal robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machine-learning models against adversarial patches. It consists of a set of patches, optimized to generalize across different models, and readily applicable to ImageNet data after preprocessing them with affine transformations. This process enables an approximate yet faster robustness evaluation, leveraging the transferability of adversarial perturbations. We showcase the usefulness of this dataset by testing the effectiveness of the computed patches against 127 models. We conclude by discussing how our dataset could be used as a benchmark for robustness, and how our methodology can be generalized to other domains. We open source our dataset and evaluation code at this https URL ","ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness
  against Adversarial Patches"
171,1509832186354098185,1457653712067997700,Pierre Naz√©,"['At this time, it is not known an annealing protocol that will produce the fewest diabatic excitations in an adiabatic quantum computer. Check out below the manuscript where we study this question with non-linear protocols. \n\n<LINK>', 'Work produced with Artur Soriani Alves, Marcus V. S. Bonan√ßa, Bart≈Çomiej Gardas and Sebastian Deffner @quthermo_comp .']",https://arxiv.org/abs/2203.17009,"Current generation quantum annealers have already proven to be successful problem-solvers. Yet, quantum annealing is still very much in its infancy, with suboptimal applicability. For instance, to date it is still an open question which annealing protocol will universally cause the fewest diabatic excitations, and even whether there is a universally optimal strategy. Therefore, in the present work, we report analytical and numerical studies of the diabatic excitations arising from non-linear protocols applied to the transverse field Ising chain, the exactly solvable model that serves as quantum annealing playground. Our analysis focuses on several driving schemes that inhibit or facilitate the dynamic phases discussed in a previous work. Rather remarkably, we find that the paradigmatic Kibble-Zurek behavior can be suppressed with ""pauses"" in the evolution, both for crossing and stopping at the quantum critical point of the system. ",Assessing performance of quantum annealing with non-linear driving
172,1509767630730801152,1110110589202956289,Alessandro Strumia,"['If a multiverse is right, unconventional cheaper future colliders are relevant: we propose a new LEP or a muon collider at the top-quark threshold, motivating why they are strategic in reducing the Shannon entropy of the vacuum landscape.  \n<LINK> <LINK>', ""@AlastairHaines It's serious! No 1st April, no Snowmass.""]",https://arxiv.org/abs/2203.17197,"Capabilities of future colliders are usually discussed assuming specific hypothetical new physics. We consider the opposite possibility: that no new physics is accessible, and we want to learn if the unnatural Standard Model is part of a vast landscape. We quantify progress in this sense via the Shannon entropy of the multiverse, and argue that a main point would be establishing the possible instability scale of the Higgs potential. This primarily needs reducing the uncertainty on the strong coupling and on the top quark mass. We show that the top quark mass can be measured well enough via a $t \bar t$ threshold scan with low $10^{33}\,{\rm cm}^{-2}{\rm sec}^{-1}$ luminosity, that seems achievable at a `small' $e^+ e^-$ collider in the LEP tunnel, or at a muon collider demonstrator. ","The collider landscape: which collider for establishing the SM
  instability?"
173,1509745466212118530,1103008101618409472,Liming Jiang,"['We propose TransEditor in #CVPR2022, a Transformer-based dual-space GAN for highly controllable facial editing, as well as a dual-space editing strategy for additional flexibility.\n\nPaper: <LINK>\nCode: <LINK>\nProject page: <LINK> <LINK>']",https://arxiv.org/abs/2203.17266,"Recent advances like StyleGAN have promoted the growth of controllable facial editing. To address its core challenge of attribute decoupling in a single latent space, attempts have been made to adopt dual-space GAN for better disentanglement of style and content representations. Nonetheless, these methods are still incompetent to obtain plausible editing results with high controllability, especially for complicated attributes. In this study, we highlight the importance of interaction in a dual-space GAN for more controllable editing. We propose TransEditor, a novel Transformer-based framework to enhance such interaction. Besides, we develop a new dual-space editing and inversion strategy to provide additional editing flexibility. Extensive experiments demonstrate the superiority of the proposed framework in image quality and editing capability, suggesting the effectiveness of TransEditor for highly controllable facial editing. ","TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable
  Facial Editing"
174,1509697364591742987,581871304,Takami Sato,"['Our #CVPR2022 paper is on arXiv! Through a large-scale study with our own dataset, we show the limitations of existing metrics for LD in autonomous driving and propose new driving-oriented metrics.\n\nüåê <LINK>\nüìù <LINK>\nüìÑ <LINK>', 'Our dataset is published in @kaggle dataset. \nhttps://t.co/8pMd3mZHkY']",https://arxiv.org/abs/2203.16851,"After the 2017 TuSimple Lane Detection Challenge, its dataset and evaluation based on accuracy and F1 score have become the de facto standard to measure the performance of lane detection methods. While they have played a major role in improving the performance of lane detection methods, the validity of this evaluation method in downstream tasks has not been adequately researched. In this study, we design 2 new driving-oriented metrics for lane detection: End-to-End Lateral Deviation metric (E2E-LD) is directly formulated based on the requirements of autonomous driving, a core downstream task of lane detection; Per-frame Simulated Lateral Deviation metric (PSLD) is a lightweight surrogate metric of E2E-LD. To evaluate the validity of the metrics, we conduct a large-scale empirical study with 4 major types of lane detection approaches on the TuSimple dataset and our newly constructed dataset Comma2k19-LD. Our results show that the conventional metrics have strongly negative correlations ($\leq$-0.55) with E2E-LD, meaning that some recent improvements purely targeting the conventional metrics may not have led to meaningful improvements in autonomous driving, but rather may actually have made it worse by overfitting to the conventional metrics. As autonomous driving is a security/safety-critical system, the underestimation of robustness hinders the sound development of practical lane detection models. We hope that our study will help the community achieve more downstream task-aware evaluations for lane detection. ",Towards Driving-Oriented Metric for Lane Detection Models
175,1509493345147830273,1059553685691342848,Yunhao (Robin) Tang,"['Off-policy learning takes many forms: (1) operator-based learning, which is fundamental to DQN; (2) marginalized importance sampling, which is common in batch RL. To unify concepts in these two frameworks, we propose marginalized operators. #AISTATS2022\n\n<LINK> <LINK>', 'Joint work with Mark Rowland, Remi Munos, Michal Valko @misovalko']",https://arxiv.org/abs/2203.16177,"In this work, we propose marginalized operators, a new class of off-policy evaluation operators for reinforcement learning. Marginalized operators strictly generalize generic multi-step operators, such as Retrace, as special cases. Marginalized operators also suggest a form of sample-based estimates with potential variance reduction, compared to sample-based estimates of the original multi-step operators. We show that the estimates for marginalized operators can be computed in a scalable way, which also generalizes prior results on marginalized importance sampling as special cases. Finally, we empirically demonstrate that marginalized operators provide performance gains to off-policy evaluation and downstream policy optimization algorithms. ",Marginalized Operators for Off-policy Reinforcement Learning
176,1509383341417631744,422672164,Dr Michael Reidinger,"['A Universal Profile for Stacked Filaments from Cold Dark Matter Simulations\n\n""We study the stacked filaments connecting group-mass halo pairs, using dark-matter-only N-body simulations.""\n<LINK>']",https://arxiv.org/abs/2203.16170,"We study the stacked filaments connecting group-mass halo pairs, using dark-matter-only N-body simulations. We calculate the dark matter over-density profile of these stacked filaments at different redshifts as a function of the distance perpendicular to the filament axis. A four-parameter universal functional form, including three comoving scale radii and one amplitude parameter (core density), provides a good fit out to a radius of 20 cMpc/h for stacked filaments over a range of redshifts, lengths and masses. The scale radii are approximately independent of redshift but increase as power-laws with the comoving filament length. Lastly, we compare the scaling of the filament mass measured directly from the simulations to the predicted scaling from the halo-halo-matter three-point correlation function as a function of redshift and of the mass of the halo pairs. We find that both measured scalings are similar to, but somewhat shallower than the predictions, by 10% and 30%, respectively. These results provide a template to interpret present and upcoming observational results based on stacking, for example, weak lensing, thermal and kinetic Sunyaev-Zel'dovich, or X-ray observations. ","A Universal Profile for Stacked Filaments from Cold Dark Matter
  Simulations"
177,1509084870999486464,3236251346,Mikel Sanz,"['New paper ‚ÄúQuantum Genetic Algorithm with Individuals in Multiple Registers‚Äù <LINK> with @raist272 and @Gatgian we propose a subroutine-based fully quantum genetic algorithm distributable among quantum processors @OpenSuperQ @QUANTEK2122 @Ikerbasque @BCAMBilbao <LINK>', 'The use multiple registers allows us to introduce all the natural elements characterizing genetic algorithms: population-based search with selection of many individuals, crossover and mutation. Surprisingly, the mutation subroutine, has small impact on the average performance 2/3 https://t.co/l02rWINXWi', 'Finally, and I like this a lot üôÉ, we introduce a quantum channel analysis to prove the exponential convergence of our algorithm and even predict its convergence-ratio. @NquireC @ryc_upvehu @upvehu https://t.co/11SXwuDgIt']",http://arxiv.org/abs/2203.15039,"Genetic algorithms are heuristic optimization techniques inspired by Darwinian evolution, which are characterized by successfully finding robust solutions for optimization problems. Here, we propose a subroutine-based quantum genetic algorithm with individuals codified in independent registers. This distinctive codification allows our proposal to depict all the fundamental elements characterizing genetic algorithms, i.e. population-based search with selection of many individuals, crossover, and mutation. Our subroutine-based construction permits us to consider several variants of the algorithm. For instance, we firstly analyze the performance of two different quantum cloning machines, a key component of the crossover subroutine. Indeed, we study two paradigmatic examples, namely, the biomimetic cloning of quantum observables and the Bu\v zek-Hillery universal quantum cloning machine, observing a faster average convergence of the former, but better final populations of the latter. Additionally, we analyzed the effect of introducing a mutation subroutine, concluding a minor impact on the average performance. Furthermore, we introduce a quantum channel analysis to prove the exponential convergence of our algorithm and even predict its convergence-ratio. This tool could be extended to formally prove results on the convergence of general non-unitary iteration-based algorithms. ",Quantum Genetic Algorithm with Individuals in Multiple Registers
178,1509083448690065414,957689165902118912,Alexandre Dauphin,"['Fresh from the arXivs: <LINK>\nWe study the fate of the topological Mott Insulator in a  cold-atom setup with dressed-Rydberg interactions. We derive the phase diagram in the presence of long-range interactions and study its stability with respect to temperature. <LINK>', 'We also verify with DMRG calculations that the quantum anomalous Hall phase also appears on quasi 2D ladders. Finally,we discuss realistic experimental parameters where the phase could be observed. It has been a nice collaboration with @lorenzocarda , S. Juli√°-Farr√© and M. M√ºller']",https://arxiv.org/abs/2203.14818,"The interplay between many-body interactions and the kinetic energy gives rise to rich phase diagrams hosting, among others, interaction-induced topological phases. These phases are characterized by both a local order parameter and a global topological invariant, and can exhibit exotic ground states such as self-trapped polarons and interaction-induced edge states. In this work, we investigate a realistic scenario for the quantum simulation of such systems using cold Rydberg-dressed atoms in optical lattices. We consider spinless fermions on a checkerboard lattice, interacting via the tunable-range effective potential induced by the Rydberg dressing. We perform a detailed analysis of the phase diagram at half- and incommensurate fillings, in the mean-field approximation. We furthermore study the stability of the phases with respect to temperature within the mean-field approximation and with respect to quantum fluctuations using the density matrix renormalization group method. Finally, we propose an implementation protocol, and in particular identify attainable regimes of experimental parameters in which the topological properties of the model become accessible. Our work, thereby, opens a realistic pathway to the outstanding experimental observation of this predicted phase in state-of-the-art cold atom quantum simulators. ","Accessing the topological Mott insulator in cold atom quantum simulators
  with realistic Rydberg dressing"
179,1509069578269302785,248142831,Yuhuang Hu,['New paper accepted at #ICPR2022. Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks (<LINK>).\n\nWe propose a parameter-efficient kernel modulation (KM) method that adapts the parameters of a base network.'],https://arxiv.org/abs/2203.15297,"Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets), have achieved incredible success in many vision tasks, but they usually require millions of parameters for good accuracy performance. With increasing applications that use ConvNets, updating hundreds of networks for multiple tasks on an embedded device can be costly in terms of memory, bandwidth, and energy. Approaches to reduce this cost include model compression and parameter-efficient models that adapt a subset of network layers for each new task. This work proposes a novel parameter-efficient kernel modulation (KM) method that adapts all parameters of a base network instead of a subset of layers. KM uses lightweight task-specialized kernel modulators that require only an additional 1.4% of the base network parameters. With multiple tasks, only the task-specialized KM weights are communicated and stored on the end-user device. We applied this method in training ConvNets for Transfer Learning and Meta-Learning scenarios. Our results show that KM delivers up to 9% higher accuracy than other parameter-efficient methods on the Transfer Learning benchmark. ","Kernel Modulation: A Parameter-Efficient Method for Training
  Convolutional Neural Networks"
180,1508791099887923200,871719324867854336,Christos Charalambous,"['How can, a small group of people with prejudices, convince the rest of a community about their preference faster? We find that this requires the biased agents to form a more endogamous community, while the unbiased agents a more exogamous one. <LINK>', 'In the classical voter model, agents copy their neighbor‚Äôs opinion, if this is different. In the variation considered here, all agents have confidence, i.e. a non-zero probability to remain with their current opinion.', 'A subgroup of agents has an opinion-dependent confidence, modelling a prejudice towards one of the opinions.We study the problem analytically and numerically,at both the thermodynamic limit and when finite size effects are in place,on a complete graph and on an ER random network.', 'Finally, we consider the case where the topology of interactions depends on the type of agents residing at each node. We ask the question ‚Äúhow can the heterogeneous topology be exploited by the biased agents to convince the rest of the society about their prejudice faster?‚Äù', 'Given a constant global average number of interactions in the community, we find that increasing the number of confrontations among members of the two groups(inter-group biased and unbiased) at the expense of the interactions among their peers, does not benefit the biased agents.', 'On the contrary, if the inter-group (biased interacting with unbiased) interactions are kept constant, and the interactions of biased agents among themselves are increased at the expense of the interactions of unbiased agents among themselves, this reduces the time to consensus.', 'A closed biased group (biased agents interact more among themselves rather than with unbiased agents) and an open unbiased group (unbiased agents interact more with biased agents rather than among themselves) is a scenario that favors the biased agents to spread their preference.']",https://arxiv.org/abs/2203.05376,"We study the voter model dynamics in the presence of confidence and bias. We assume two types of voters. Unbiased voters whose confidence is indifferent to the state of the voter and biased voters whose confidence is biased towards a common fixed preferred state. We study the problem analytically on the complete graph using mean field theory and on an Erd\H{o}s-R\'enyi random network topology using the pair approximation, where we assume that the network of interactions topology is independent of the type of voters. We find that for the case of a random initial setup, and for sufficiently large number of voters $N$, the time to consensus increases proportionally to $\log(N)/\gamma v$, with $\gamma$ the fraction of biased voters and $v$ the parameter quantifying the bias of the voters ($v=0$ no bias). We verify our analytical results through numerical simulations. We study this model on a biased-dependent topology of the network of interactions and examine two distinct, global average-degree preserving strategies (model I and model II) to obtain such biased-dependent random topologies starting from the biased-independent random topology case as the initial setup. Keeping all other parameters constant, in model I, $\mu_{BU}$, the average number of links among biased (B) and unbiased (U) voters is varied at the expense of $\mu_{UU}$ and $\mu_{BB}$, i.e. the average number of links among only unbiased and biased voters respectively. In model II, $\mu_{BU}$ is kept constant, while $\mu_{BB}$ is varied at the expense of $\mu_{UU}$. We find that if the agents follow the strategy described by model II, they can achieve a significant reduction in the time to reach consensus as well as an increment in the probability to reach consensus to the preferred state. ",Biased-voter model: how persuasive a small group can be?
181,1508718414055059458,1271852576296906755,Ashley Chrimes,"['New paper on arxiv today! We find NIR emission at the location of 6 Galactic magnetars for the first time, and show that some known NIR counterparts are highly variable. We also discuss the nature of the NIR emission - more to come on that, so stay tuned! <LINK>']",https://arxiv.org/abs/2203.14947,"We report the discovery of six new magnetar counterpart candidates from deep near-infrared Hubble Space Telescope imaging. The new candidates are among a sample of nineteen magnetars for which we present HST data obtained between 2018-2020. We confirm the variability of previously established near-infrared counterparts, and newly identify candidates for PSRJ1622-4950, SwiftJ1822.3-1606, CXOUJ171405.7-381031, SwiftJ1833-0832, SwiftJ1834.9-0846 and AXJ1818.8-1559 based on their proximity to X-ray localisations. The new candidates are compared with the existing counterpart population in terms of their colours, magnitudes, and near-infrared to X-ray spectral indices. We find two candidates for AXJ1818.8-1559 which are both consistent with previously established counterparts. The other new candidates are likely to be chance alignments, or otherwise have a different origin for their near-infrared emission not previously seen in magnetar counterparts. Further observations and studies of these candidates are needed to firmly establish their nature. ","New candidates for magnetar counterparts from a deep search with the
  Hubble Space Telescope"
182,1508694462024749056,439364141,Ramon Ruiz-Dolz,"['We propose a new hybrid method for the automatic evaluation of natural language argumentative debates. This approach allows to estimate the outcome of professional debates considering both, the underlying logic of arguments and their natural language.\n\n<LINK>', '#NLP #Argumentation #DeepLearning #NeuralNetworks https://t.co/GFkl1KJiIc']",http://arxiv.org/abs/2203.14647,"The lack of annotated data on professional argumentation and complete argumentative debates has led to the oversimplification and the inability of approaching more complex natural language processing tasks. Such is the case of the automatic debate evaluation. In this paper, we propose an original hybrid method to automatically evaluate argumentative debates. For that purpose, we combine concepts from argumentation theory such as argumentation frameworks and semantics, with Transformer-based architectures and neural graph networks. Furthermore, we obtain promising results that lay the basis on an unexplored new instance of the automatic analysis of natural language arguments. ","Automatic Debate Evaluation with Argumentation Semantics and Natural
  Language Argument Graph Networks"
183,1508581494880673802,948010010,Antonis Anastasopoulos,"['Revisiting the Effects of Leakage on Dependency Parsing, ACL Findings\n<LINK>\nWe propose new measures on leakage (having similar trees in train and test) and find that it may explain zero-shot cross-lingual performance.\nHowever, 1/3 <LINK>', 'we think that more ""easy"" examples (e.g. short sentences) tend to leak; not that leakage allows  parsers to (seem to) perform better (as previous --cool-- work suggested).\nMore interestingly, 2/3', 'we also do a study on Faroese, transferring from various Germanic languages, and show that a smaller, diverse (in terms of syntax tree isomorphisms) training set is just as good or better than using the whole training set! 3/3 https://t.co/ZqnYpWM9Zp', 'Congratulations to @miriamwanner and Nate Krasner for their first published paper, two undergrads with a bright research future ahead of them! \nAnd thank you to @NSF for funding our REU site here at @GMUCompSci.', ""@complingy @miriamwanner @NSF @GMUCompSci Ah, good point, the original claim was about monolingual parsing, I should have been more clear!\nWe did experiments reproducing the original claim on more languages with more systems in a monolingual setting first -- it's only in zero-shot cross-lingual settings where it holds!"", '@miriamwanner @NSF @GMUCompSci Let me also highlight this earlier paper (which we had missed till now, yay for twitter #nlproc community): https://t.co/7h30sHk0Ie by Mark Anderson, Anders S√∏gaard, and @carlosgr_nlp, who also discuss the issues with the previous study (S√∏gaard, 2020).']",https://arxiv.org/abs/2203.12815,"Recent work by S{\o}gaard (2020) showed that, treebank size aside, overlap between training and test graphs (termed leakage) explains more of the observed variation in dependency parsing performance than other explanations. In this work we revisit this claim, testing it on more models and languages. We find that it only holds for zero-shot cross-lingual settings. We then propose a more fine-grained measure of such leakage which, unlike the original measure, not only explains but also correlates with observed performance variation. Code and data are available here: this https URL ",Revisiting the Effects of Leakage on Dependency Parsing
184,1508504916125528075,1159484728359043072,Fangyuan Xu ËÆ∏ÊñπÂõ≠,"['ü§î How do we answer complex questions, such as ‚ÄúHow much money is needed in order to not have to work for the rest of your life‚Äù? We (w/ @eunsolc and @jessyjli) study the discourse structure of long-form answers. #ACL2022 \nPaper: <LINK> <LINK>', 'We design 6 functional roles, annotate answers from ELI5, NQ and WebGPT, and discover their *different* answer structures. Yet, in all, 40%+ of the sentences don‚Äôt directly address the question but serve other roles ‚Äì e.g. provide background information, give an example, etc.', 'How about model generated answers? We annotate answers from a state-of-the-art LFQA system. Interestingly, the same set of human annotators agree with each other *less* on assigning roles and the role distribution is also different, with less sentences addressing the questions.', 'More analysis in the paper! LFQA is a challenging yet interesting problem, and we hope our work can inspire future research on discourse level modelling and evaluation of long-form QA systems. \nData and code: https://t.co/grqpYnK6l1\nWebsite: https://t.co/dmc7vkHQx2']",https://arxiv.org/abs/2203.11048,"Long-form answers, consisting of multiple sentences, can provide nuanced and comprehensive answers to a broader set of questions. To better understand this complex and understudied task, we study the functional structure of long-form answers collected from three datasets, ELI5, WebGPT and Natural Questions. Our main goal is to understand how humans organize information to craft complex answers. We develop an ontology of six sentence-level functional roles for long-form answers, and annotate 3.9k sentences in 640 answer paragraphs. Different answer collection methods manifest in different discourse structures. We further analyze model-generated answers -- finding that annotators agree less with each other when annotating model-generated answers compared to annotating human-written answers. Our annotated data enables training a strong classifier that can be used for automatic analysis. We hope our work can inspire future research on discourse-level modeling and evaluation of long-form QA systems. ","How Do We Answer Complex Questions: Discourse Structure of Long-form
  Answers"
185,1508462295231442948,1120914312,Arnav,['üö® New pre-print alert! üö®\n\nWe design probes to study whether Pre-Trained Language Models pick up on cross-cultural differences in values and whether those align with existing value surveys : <LINK>\n\nWork with my amazing colleagues @frimelle @IAugenstein\n#NLProc <LINK>'],https://arxiv.org/abs/2203.13722,"Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys. ","Probing Pre-Trained Language Models for Cross-Cultural Differences in
  Values"
186,1508407909155278848,13800042,Lukas Heinrich,"['Short new paper on lhood-free inference:We use the profile lhood ratio b/c of its asymptotically optimal properties but often need to approximate p(x|Œ∏) to compute it. But if we take its properties seriously, we can find it in a fully likelihood-free way: <LINK> <LINK>', ""As in the likelihood ratio trick, the idea is simple: if we use a test stat because it's optimal, that just means we can find it through optimization. The LRT is (asymptotically) optimal is various ways e.g. has best average power in a certain sense"", 'The idea is to optimizing a neural network-based statistic for best average power - this will generally give you a good test statistic. If the underlying model behaves asymptotically, this recovers the profile likelihood ratio without ever having to evaluate or fit p(x|Œ∏)']",http://arxiv.org/abs/2203.13079,"The design of optimal test statistics is a key task in frequentist statistics and for a number of scenarios optimal test statistics such as the profile-likelihood ratio are known. By turning this argument around we can find the profile likelihood ratio even in likelihood-free cases, where only samples from a simulator are available, by optimizing a test statistic within those scenarios. We propose a likelihood-free training algorithm that produces test statistics that are equivalent to the profile likelihood ratios in cases where the latter is known to be optimal. ",Learning Optimal Test Statistics in the Presence of Nuisance Parameters
187,1507408630269636609,976521596289671172,Thomas Fai,"['Announcing our latest preprint, joint with postdoc Ying Zhang, on the influence of the endothelial surface layer (ESL) on red blood cell motion. ESL structure is disrupted in diseases such as sepsis, and we here study its effect by simulating blood flow.  <LINK>', 'In particular, we study how ESL roughness influences the wall-induced migration of red cells. Interestingly, we find that ESL surface roughness causes increased lift and larger cell free layer at vessel walls, with potential implications for oxygen transport and immune response. https://t.co/kZ5km8U9eB']",https://arxiv.org/abs/2203.12113,"The endothelial lining of blood vessels presents a large surface area for exchanging materials between blood and tissues. The endothelial surface layer (ESL) plays a critical role in regulating vascular permeability, hindering leukocyte adhesion as well as inhibiting coagulation during inflammation. Changes in the ESL structure are believed to cause vascular hyperpermeability and induce thrombus formation during sepsis. In addition, ESL topography is relevant for the interactions between red blood cells (RBCs) and the vessel wall, including the wall-induced migration of RBCs and formation of a cell-free layer. To investigate the influence of the ESL on the motion of RBCs, we construct two models to represent the ESL using the immersed boundary method. In particular, we use simulations to study how lift force and drag force change over time when a RBC is placed close to the ESL as the thickness, roughness, and permeability of the ESL vary. We find that roughness has a significant effect on the wall-induced migration of the RBC when the ESL is highly permeable and that the wall-induced migration can be significantly inhibited by the presence of a thick ESL. ","Influence of the Endothelial Surface Layer on the Wall-induced Migration
  of Red Blood Cells"
188,1507400626979831808,972878356319473665,Sophia Economou,"['New paper on arxiv. We propose a new, easier to measure objective function combined with the ADAPT-VQE strategy to create Gibbs states with high fidelity. <LINK> With Ada Warren, @linghua_zhu, Ed Barnes and @nick_mayhall']",https://arxiv.org/abs/2203.12757,"The preparation of Gibbs thermal states is an important task in quantum computation with applications in quantum simulation, quantum optimization, and quantum machine learning. However, many algorithms for preparing Gibbs states rely on quantum subroutines which are difficult to implement on near-term hardware. Here, we address this by (i) introducing an objective function that, unlike the free energy, is easily measured, and (ii) using dynamically generated, problem-tailored ans\""atze. This allows for arbitrarily accurate Gibbs state preparation using low-depth circuits. To verify the effectiveness of our approach, we numerically demonstrate that our algorithm can prepare high-fidelity Gibbs states across a broad range of temperatures and for a variety of Hamiltonians. ",Adaptive variational algorithms for quantum Gibbs state preparation
189,1506959849934004225,3379982859,Hazel Doughty,"[""Excited to share our #CVPR2022 work 'How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs'\xa0<LINK>\xa0w/ @cgmsnoek\n\nWe propose a semi-supervised method to recognize adverbs so that subtle differences in videos can be described. <LINK>"", 'Our method uses multiple adverb pseudo-labelling and adaptive thresholding to successfully recognize adverbs despite the long-tail distribution. With this we can recognize adverbs in seen action-adverb compositions, unseen compositions and unseen domains.', 'We also find that adverbs can describe relationships between fine-grained actions.\n\nFor our new adverb datasets and more info check out the project page:\xa0https://t.co/iT8tBsKYA7 https://t.co/cpnBJmfhl7']",https://arxiv.org/abs/2203.12344,"We aim to understand how actions are performed and identify subtle differences, such as 'fold firmly' vs. 'fold gently'. To this end, we propose a method which recognizes adverbs across different actions. However, such fine-grained annotations are difficult to obtain and their long-tailed nature makes it challenging to recognize adverbs in rare action-adverb compositions. Our approach therefore uses semi-supervised learning with multiple adverb pseudo-labels to leverage videos with only action labels. Combined with adaptive thresholding of these pseudo-adverbs we are able to make efficient use of the available data while tackling the long-tailed distribution. Additionally, we gather adverb annotations for three existing video retrieval datasets, which allows us to introduce the new tasks of recognizing adverbs in unseen action-adverb compositions and unseen domains. Experiments demonstrate the effectiveness of our method, which outperforms prior work in recognizing adverbs and semi-supervised works adapted for adverb recognition. We also show how adverbs can relate fine-grained actions. ",How Do You Do It? Fine-Grained Action Understanding with Pseudo-Adverbs
190,1506942458248183812,1360175358,Michele Starnini,"['Last on @arxiv! The interplay between mobility &amp; spreading dynamics is a topic interesting for both epidemiology and active matter communities.\n\nWe study the impact of motility (run-and-tumble self-propelled particles) on epidemic spreading (SIS process). <LINK> <LINK>', 'In the long-time diffusive regime, the transition becomes of the mean-field type in 1D, 2D, and 3D, while dimensionality matters in the static case. Insights obtained from an analytical, continuum description are validated by numerical simulations of an agent-based model. https://t.co/YR0Vk0Aymm']",https://arxiv.org/abs/2203.12355,"Most spreading processes require spatial proximity between agents. The stationary state of spreading dynamics in a population of mobile agents thus depends on the interplay between the time and length scales involved in the epidemic process and their motion in space. We analyze the steady properties resulting from such interplay in a simple model describing epidemic spreading (modeled as a Susceptible-Infected-Susceptible process) on self-propelled particles (performing Run-and-Tumble motion). Focusing our attention on the diffusive long-time regime, we find that the agents' motion changes qualitatively the nature of the epidemic transition characterized by the emergence of a macroscopic fraction of infected agents. Indeed, the transition becomes of the mean-field type for agents diffusing in one, two and three dimensions, while, in the absence of motion, the epidemic outbreak depends on the dimension of the underlying static network determined by the agents' fixed locations. The insights obtained from a continuum description of the system are validated by numerical simulations of an agent-based model. Our work aims at bridging soft active matter physics and theoretical epidemiology, and may be of interest for researchers in both communities. ","Epidemic processes on self-propelled particles: continuum and
  agent-based modelling"
191,1506804903234060288,129826079,David Layden,['Sampling from the Boltzmann distribution of classical Ising models is a computational bottleneck in many fields. We propose a new quantum algorithm for this sampling problem and implement it on a superconducting quantum processor.\n<LINK>'],https://arxiv.org/abs/2203.12497,"Sampling from complicated probability distributions is a hard computational problem arising in many fields, including statistical physics, optimization, and machine learning. Quantum computers have recently been used to sample from complicated distributions that are hard to sample from classically, but which seldom arise in applications. Here we introduce a quantum algorithm to sample from distributions that pose a bottleneck in several applications, which we implement on a superconducting quantum processor. The algorithm performs Markov chain Monte Carlo (MCMC), a popular iterative sampling technique, to sample from the Boltzmann distribution of classical Ising models. In each step, the quantum processor explores the model in superposition to propose a random move, which is then accepted or rejected by a classical computer and returned to the quantum processor, ensuring convergence to the desired Boltzmann distribution. We find that this quantum algorithm converges in fewer iterations than common classical MCMC alternatives on relevant problem instances, both in simulations and experiments. It therefore opens a new path for quantum computers to solve useful--not merely difficult--problems in the near term. ",Quantum-enhanced Markov chain Monte Carlo
192,1506727412536537091,1313941507305472001,Ryotatsu Yanagimoto,"['Excited to announce our new preprint <LINK>!\n\nIn this work, we propose a novel approach to *deterministic* optical quantum computation (OQC): ""temporally trapped"" ultrashort pulses\n\nHow our prescription overcomes major challenges in OQC is explained inüßµ1/n <LINK>', 'Realization of a deterministic two-qubit entangling gate (e.g., CZ gate) is a central challenge in OQC.  To realize high-fidelity gates, photon-photon coupling rate g has to be greater than the loss rate Œ∫, which, to this date, has not been realized in a scalable platform. 2/n https://t.co/kEOgsy2oey', 'This is due the tradeoff between the mode volume V and the Q-factor of photonic systems; Stronger coupling requires small V, but it compromises the Q at the same time. As a result, various platforms, e.g., PhC and Œºring resonators, end up with similar figure of merit g/Œ∫. 3/n https://t.co/pMrdvZKo8E', 'In this work, we propose the use of temporally trapped ultrashort pulses to overcome this fundamental tradeoff: We introduce an auxiliary ‚Äútrap‚Äù pulse that forms temporal potential for the signal photons, which effectively creates a ‚Äúflying cavity‚Äù with tight 3D confinement. 4/n https://t.co/bDQiuvLctJ', 'By doing so, photons can simultaneously enjoy the low loss rate of a ring resonator and the wavelength-scale confinement in all 3-dimensions. This ‚Äúpulsed enhancement‚Äù could boost the optical nonlinearity by orders of magnitudes! 5/n https://t.co/twVXbvlLNd', 'Also, temporal trapping creates energy gaps between the computational modes (i.e., trapped modes) and the rest of the degrees of freedom, suppressing detrimental multimode interactions. This protection allows us to circumvent Shapiro‚Äôs no-go theorem for pulsed quantum gates! 6/n https://t.co/cgmYWw0yMe', 'The attached figure shows how two FH photons evolve under various depth of trap. Only in the presence of the temporal trap, we observe high-contrast Rabi oscillations, an indication of single-mode physics essential for high-fidelity quantum gates. 7/n https://t.co/p09rAr6vc2', 'Now, a controlled-Z (CZ) gate can be implemented by embedding our ""flying cavities"" into a Mach-Zehnder interferometer. This completes the universal gate set for OQC in all-deterministic manner! 8/n https://t.co/ZUjHTyF1CH', 'Our numerical simulations indicate that high-fidelity CZ gate operation is possible even with a moderate trap depth. 9/n https://t.co/j6P4re3ny3', 'Everything sounds good in theory, but how realistic are experiments? In fact, advanced dispersion engineering allows us to construct LN waveguide designs that can support our scheme. It is worth mentioning that loss as low as 3dB/m is demonstrated on the same platform. 10/n https://t.co/RxknblgBxE', 'Combining all these numbers, we find that the strong coupling regime g/Œ∫&gt;1 could be achievable with ultrashort pulses using technologies available to this date. Pushing the operation to shorter wavelength, even g/Œ∫=50 may not be a dream! 11/n https://t.co/mSEdcBCr3h', 'In summary, we have proposed temporal trapping as a novel tool to harness and enhance nonlinear quantum dynamics of ultrashort pulses. Our scheme is compatible with existing framework of NLOQC and CV quantum computations, suggesting a broad range of extensions. 12/n https://t.co/m3N0hGyqOj', ""Finally, I'd like to thank the collaborators at @Stanford, @MIT, and @NttResearch n/n https://t.co/Gpj2gCTzlS""]",https://arxiv.org/abs/2203.11909,"The realization of deterministic photon-photon gates is a central goal in optical quantum computation and engineering. While solid-state nonlinear optics offers a scalable, room temperature solution, weak material nonlinearities lead to extremely demanding requirements on optical loss and confinement, which to date have not been realized. In this work, we introduce a novel confinement method, dispersion-engineered temporal trapping, which enhances the nonlinear interaction strength by orders of magnitude by simultaneously confining photons in both time and space. Temporal confinement is imposed by a non-resonant ""trap"" pulse via cross-phase modulation, realizing a wavelength-scale ""flying cavity"". Numerical simulations confirm that temporal trapping confines the multimode nonlinear dynamics to a single-mode subspace, enabling high-fidelity deterministic quantum gate operations. With realistic dispersion engineering and loss figures, we show that temporally trapped ultrashort pulses could achieve strong coupling on near-term nonlinear nanophotonic platforms. Our scheme is compatible with traveling-wave implementations and intracavity time multiplexing, which can further enhance the operation bandwidth and scalability. We expect temporal trapping to open a new path to optical strong coupling, with applications in quantum computing, simulation, and light sources. ","Temporal trapping of ultrashort pulses enables deterministic optical
  quantum computation"
193,1506680751923867650,322460769,Yoav Artzi,"['Can we improve a QA system from user feedback *in deployment*? We study how effective this signal is (tldr: very effective) using simulation experiments with existing benchmarks. <LINK> @aclmeeting #ACL2022 work by Ge Gao collab with @eunsolc <LINK>', 'You can train a QA system with a tiny amount of supervised data, and then solicit user feedback to improve the system over time. For example, even with a 64-supervised-examples init that give ~20F1, systems can reach 80F1 (SQuAD), 67F1 (HotpotQA), and 62F1 (NQ)! https://t.co/RTiTEDUzFE', 'Improvement is rapid with much of the learning happening in the first 20% of simulated interactions, so most users would get to interact with a pretty good system. https://t.co/wKxgWB3mmN', 'Or, start from an existing domain/dataset, and with zero annotation in your new domain, simply let the system learn from user feedback. Meaning: you can easily jump to a completely new domain from data that you already have! https://t.co/ZHkPRqzAvt', 'Out-of-domain init can provide an effective starting point, even if the initial performance on the new domain is bad: a TriviaQA model performs terribly on HotpotQA, but with feedback performance jumps from 0 to 73F1! https://t.co/K1cCLBGBcm', 'More in the paper: robustness to feedback noise, online vs. offline learning comparison, regret analysis as indication of the user experience as the system learns, and a clipped bandit policy gradient objective that smoothly handles negative examples. https://t.co/1imd9YaCk6']",https://arxiv.org/abs/2203.10079,"We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on a small number of examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation, but instead improving the system on-the-fly via user feedback. ","Simulating Bandit Learning from User Feedback for Extractive Question
  Answering"
194,1506664048766685190,1248083315808260097,Manuel Weber,"['Today on the arXiv: We study the long-time dynamics of a CDW chain after excitation by a pump pulse. A combination of Monte Carlo sampling and Ehrenfest dynamics for the phonons reveals the ringing of the CDW mode, visible in the photoemission spectrum.\n<LINK>']",https://arxiv.org/abs/2203.11880,"We describe coupled electron-phonon systems semiclassically - Ehrenfest dynamics for the phonons and quantum mechanics for the electrons - using a classical Monte Carlo approach that determines the nonequilibrium response to a large pump field. The semiclassical approach is quite accurate, because the phonons are excited to average energies much higher than the phonon frequency, eliminating the need for a quantum description. The numerical efficiency of this method allows us to perform a self-consistent time evolution out to very long times (tens of picoseconds) enabling us to model pump-probe experiments of a charge density wave (CDW) material. Our system is a half-filled, one-dimensional (1D) Holstein chain that exhibits CDW ordering due to a Peierls transition. The chain is subjected to a time-dependent electromagnetic pump field that excites it out of equilibrium, and then a second probe pulse is applied after a time delay. By evolving the system to long times, we capture the complete process of lattice excitation and subsequent relaxation to a new equilibrium, due to an exchange of energy between the electrons and the lattice, leading to lattice relaxation at finite temperatures. We employ an indirect (impulsive) driving mechanism of the lattice by the pump pulse due to the driving of the electrons by the pump field. We identify two driving regimes, where the pump can either cause small perturbations or completely invert the initial CDW order. Our work successfully describes the ringing of the amplitude mode in CDW systems that has long been seen in experiment, but never successfully explained by microscopic theory. ","Theoretical description of time-resolved photoemission in
  charge-density-wave materials out to long times"
195,1506593717616058371,1231755108234547202,Ariel Werle,"['This is one of the post-starburst galaxies we have studied in our last paper (<LINK>) and I think it deserves a couple of tweets. <LINK>', 'The yellow-blue color map in the image above traces how much stars were recently formed in the galaxy disk, while the red region indicates a tail of ionized gas. Note that there is a stellar population gradient aligned with the direction of the tail!', 'This galaxy just had its gas removed due to the ram-pressure of the intracluster medium (around 60 million years ago if our math is right), and we were lucky enough to catch it in a stage when the removed gas is still visible.', 'We interpret this object as a ""missing link"" between post-starburst and jellyfish galaxies.']",https://arxiv.org/abs/2203.08862,"We present results from MUSE spatially-resolved spectroscopy of 21 post-starburst galaxies in the centers of 8 clusters from $z\sim0.3$ to $z\sim0.4$. We measure spatially resolved star-formation histories (SFHs), the time since quenching ($t_Q$) and the fraction of stellar mass assembled in the past 1.5 Gyr ($\mu_{1.5}$). The SFHs display a clear enhancement of star-formation prior to quenching for 16 out of 21 objects, with at least 10% (and up to $>50$%) of the stellar mass being assembled in the past 1.5 Gyr and $t_Q$ ranging from less than 100 Myrs to $\sim800$ Myrs. By mapping $t_Q$ and $\mu_{1.5}$, we analyze the quenching patterns of the galaxies. Most galaxies in our sample have quenched their star-formation from the outside-in or show a side-to-side/irregular pattern, both consistent with quenching by ram-pressure stripping. Only three objects show an inside-out quenching pattern, all of which are at the high-mass end of our sample. At least two of them currently host an active galactic nucleus. In two post-starbursts, we identify tails of ionized gas indicating that these objects had their gas stripped by ram pressure very recently. Post-starburst features are also found in the stripped regions of galaxies undergoing ram-pressure stripping in the same clusters, confirming the link between these classes of objects. Our results point to ram-pressure stripping as the main driver of fast quenching in these environments, with active galactic nuclei playing a role at high stellar masses. ",Post-starburst galaxies in the centers of intermediate redshift clusters
196,1506544527682715651,1190175298106675200,Jonas Latz,"['Minimising a cost function + {L1/LASSO regulariser, Ginzburg-Landau energy} is hard: we propose and analyse a randomised, continuous-time splitting method.\n\nNew preprint: Gradient flows and randomised thresholding: sparse inversion and classification. (<LINK>)']",https://arxiv.org/abs/2203.11555,"Sparse inversion and classification problems are ubiquitous in modern data science and imaging. They are often formulated as non-smooth minimisation problems. In sparse inversion, we minimise, e.g., the sum of a data fidelity term and an L1/LASSO regulariser. In classification, we consider, e.g., the sum of a data fidelity term and a non-smooth Ginzburg--Landau energy. Standard (sub)gradient descent methods have shown to be inefficient when approaching such problems. Splitting techniques are much more useful: here, the target function is partitioned into a sum of two subtarget functions -- each of which can be efficiently optimised. Splitting proceeds by performing optimisation steps alternately with respect to each of the two subtarget functions. In this work, we study splitting from a stochastic continuous-time perspective. Indeed, we define a differential inclusion that follows one of the two subtarget function's negative subgradient at each point in time. The choice of the subtarget function is controlled by a binary continuous-time Markov process. The resulting dynamical system is a stochastic approximation of the underlying subgradient flow. We investigate this stochastic approximation for an L1-regularised sparse inversion flow and for a discrete Allen-Cahn equation minimising a Ginzburg--Landau energy. In both cases, we study the longtime behaviour of the stochastic dynamical system and its ability to approximate the underlying subgradient flow at any accuracy. We illustrate our theoretical findings in a simple sparse estimation problem and also in a low-dimensional classification problem. ","Gradient flows and randomised thresholding: sparse inversion and
  classification"
197,1506365762058940417,1161312102486667264,Keith Burghardt,"['A new paper on understanding material states is out!\n<LINK>\nFor a range of simulations we find a simple way to determine if there are any number of changes in the material without having to know the ""order parameter"" (what the changes actually are). <LINK>', 'Research is in collaboration with @DingrevilleRemi, Marcin Abrams, @aram_galstyan, and @gesteller']",http://arxiv.org/abs/2203.10204,"The identification and classification of transitions in topological and microstructural regimes in pattern-forming processes is critical for understanding and fabricating microstructurally precise novel materials in many application domains. Unfortunately, relevant microstructure transitions may depend on process parameters in subtle and complex ways that are not captured by the classic theory of phase transition. While supervised machine learning methods may be useful for identifying transition regimes, they need labels which require prior knowledge of order parameters or relevant structures. Motivated by the universality principle for dynamical systems, we instead use a self-supervised approach to solve the inverse problem of predicting process parameters from observed microstructures using neural networks. This approach does not require labeled data about the target task of predicting microstructure transitions. We show that the difficulty of performing this prediction task is related to the goal of discovering microstructure regimes, because qualitative changes in microstructural patterns correspond to changes in uncertainty for our self-supervised prediction problem. We demonstrate the value of our approach by automatically discovering transitions in microstructural regimes in two distinct pattern-forming processes: the spinodal decomposition of a two-phase mixture and the formation of concentration modulations of binary alloys during physical vapor deposition of thin films. This approach opens a promising path forward for discovering and understanding unseen or hard-to-detect transition regimes, and ultimately for controlling complex pattern-forming processes. ","Inferring topological transitions in pattern-forming processes with
  self-supervised learning"
198,1506284963464990733,167708661,Peter Jedlicka,"['Which ion channel parameters from a huge theoretically possible parameter space are present in real neurons? In our preprint, we propose that Pareto optimality can serve as a guiding principle for addressing this issue (known as ion channel degeneracy). <LINK>', 'Pareto optimality could help find models with optimal ion channel configurations performing best for a trade-off between energy efficiency and functional effectiveness. This could reduce the high-dimensional parameter space to geometrically simple low-dimensional manifolds.', 'Pareto optimality might provide insights into neuronal ion channel correlations (e.g., in Patch-seq data). Multi-objective Pareto optimality has been applied to biology by Uri Alon https://t.co/5iyPRK41pr. See also a great review by @DiesPallas et al. https://t.co/2aWYVYofci']",https://arxiv.org/abs/2203.06391,"Nerve cells encounter unavoidable evolutionary trade-offs between multiple tasks. They must consume as little energy as possible (be energy-efficient or economical) but at the same time fulfil their functions (be functionally effective). Neurons displaying best performance for such multi-task trade-offs are said to be Pareto optimal. However, it is not understood how ion channel parameters contribute to the Pareto optimal performance of neurons. Ion channel degeneracy implies that multiple combinations of ion channel parameters can lead to functionally similar neuronal behavior. Therefore, to simulate functional behavior, instead of a single model, neuroscientists often use populations of valid models with distinct ion conductance configurations. This approach is called population (also database or ensemble) modeling. It remains unclear, which ion channel parameters in a vast population of functional models are more likely to be found in the brain. Here we propose that Pareto optimality can serve as a guiding principle for addressing this issue. The Pareto optimum concept can help identify the subpopulations of conductance-based models with ion channel configurations that perform best for the trade-off between economy and functional effectiveness. In this way, the high-dimensional parameter space of neuronal models might be reduced to geometrically simple low-dimensional manifolds. Therefore, Pareto optimality is a promising framework for improving population modeling of neurons and their circuits. We also discuss how Pareto inference might help deduce neuronal functions from high-dimensional Patch-seq data. Furthermore, we hypothesize that Pareto optimality might contribute to our understanding of observed ion channel correlations in neurons. ","Pareto optimality, economy-effectiveness trade-offs and ion channel
  degeneracy: Improving population models of neurons"
199,1506257526274375686,1120650694644596737,Paris Avgeriou,"[""Architectural Smells are the bane of developers' existence. Coming to their rescue, we worked with a  large tech industry to study how such smells evolve &amp; how they impact software practitioners.\nAccepted @emsejournal\nW. @darius_fenn &amp; U. Uyumaz \nPreprint: <LINK>""]",https://arxiv.org/abs/2203.08702,"Architectural smells (AS) are notorious for their long-term impact on the Maintainability and Evolvability of software systems. The majority of research work has investigated this topic by mining software repositories of open source Java systems, making it hard to generalise and apply them to an industrial context and other programming languages. To address this research gap, we conducted an embedded multiple-case case study, in collaboration with a large industry partner, to study how AS evolve in industrial embedded systems. We detect and track AS in 9 C/C++ projects with over 30 releases for each project that span over two years of development, with over 20 millions lines of code in the last release only. In addition to these quantitative results, we also interview 12 among the developers and architects working on these projects, collecting over six hours of qualitative data about the usefulness of AS analysis and the issues they experienced while maintaining and evolving artefacts affected by AS. Our quantitative findings show how individual smell instances evolve over time, how long they typically survive within the system, how they overlap with instances of other smell types, and finally what the introduction order of smell types is when they overlap. Our qualitative findings, instead, provide insights on the effects of AS on the long-term maintainability and evolvability of the system, supported by several excerpts from our interviews. Practitioners also mention what parts of the AS analysis actually provide actionable insights that they can use to plan refactoring activities. ","On the evolution and impact of Architectural Smells -- An industrial
  case study"
200,1506238318492721155,878678526865625088,Frank Noe,"['We propose a new+data-efficient way for #MachineLearning coarse-grained molecular force fields from simulation data (e.g. all-atom). With @jonkhler, @__kraemer__, Yaoyi Chen, @CecClementi. @bifoldberlin @MSFTResearch @ERC_Research @FU_Berlin \n\n<LINK>', 'The key idea is to do force matching without forces. Force matching is a standard approach where the derivative of a learned energy function is matched to the derivatives of the ground truth energy function saved before. https://t.co/Z3vKN6r7Dx', 'In the coarse-graining setting, while your coarse-grained forces are deterministic, instantaneous forces in the fine-grained model are noisy. The loss is now variational:, with a constant noise term. Pioneered by Noid,Izvekov,Voth\n\nhttps://t.co/vIFY3w67qk\nhttps://t.co/ejSYhmJUFe', 'This can be turned into a machine-learning approach to systematically learn coarse-grained force fields with neural nets - collaboration with @CecClementi and @gdefabritiis. Learning problem is easy, but this is data-inefficient because forces are noisy. \n\nhttps://t.co/Dmvg40vuhj', ""Alternative: don't match gradients, but instead the coarse-grained density. In #machinelearning known as  energy-based model with density matching. In #molecular sciences this is known as the relative entropy method by Scott Shell.\n\nhttps://t.co/FvaL2Gsf8u\nhttps://t.co/XJoiWanMXU"", 'Main limitations with that approach: training loss involves an average over the density generated by the coarse model and for that you need to keep simulating it while you train, which is very expensive and can cause convergence problems.', 'Or approach tries to combine the best of both worlds: do density matching without the re-simulation problem and do force matching without the noise in a model that has a desired network architecture to represent the molecule, and that also allows transferability across molecules.', 'Main ideas: \n(1) Density matching by training a normalizing flow on the coarse-grained coordinates given the all-atom simulation data. Similar as in Boltzmann Generators, but now only match the density in the coarse coordinates.\n\nhttps://t.co/TU5kKkQ5sd', '(2) The flow defines an output density p(x), but this also corresponds to a coarse-grained potential u(x) = -log p(x) with forces f(x) = -grad u(x) = grad log p(x). It\'s important to use smooth flows in order for these forces to be ""nice"".\n\nhttps://t.co/PHMbHp07sa', '(3) We now have flow which can sample CG coordinates and forces efficiently. We use augmented flows in order to have more expressivity, so there is still some noise on our forces, but very little.\n\nhttps://t.co/9e2ps9p7TH\nhttps://t.co/vkSkJN1kSw', '(4) Finally we match the forces of our learned coarse-grained energy function to the flow forces. In this paper we use a simple extension of CGnet, but we can use any function we like here.\n\nhttps://t.co/Dmvg40vuhj', 'In particular we could use graph convolution nets such as SchNet, PaiNN etc. in order to represent the molecular structure in a transferable way, so the final model can represent a transferable force field.\n\nhttps://t.co/XsMye2IjEV', ""The current limitation is that the flows don't scale to arbitrarily large systems because they work on internal coordinates, so the first part (training the flow) needs to be done on molecular fragments, e.g. proteins up to 20-30 amino acids."", ""However since the second part is transferable, this might be enough to learn a transferable force field for arbitrary proteins. Also this approach can be mixed with other training methods, so it's an important addition to our computational biology toolbox"", 'This indicates how much  more data-efficient the approach (blue) is to plain force matching (yellow), given the same amount of data. This is just alanine dipeptide, the differences are more extreme for larger systems. https://t.co/joSGY6sgBG', 'We can learn the structures and probability density of small fast-folding proteins up to Villian and BBA. They fold reliably in the right structure, the density is well matched in most parts, but some inaccuracies remain. https://t.co/awI83xf08i', 'Villin not villain ;-)', 'Also @hello_yaoyi welcome to Twitter ;-)']",https://arxiv.org/abs/2203.11167,"Coarse-grained (CG) molecular simulations have become a standard tool to study molecular processes on time-~and length-scales inaccessible to all-atom simulations. Learning CG force fields from all-atom data has mainly relied on force-matching and relative entropy minimization. Force-matching is straightforward to implement but requires the forces on the CG particles to be saved during all-atom simulation, and because these instantaneous forces depend on all degrees of freedom, they provide a very noisy signal that makes training the CG force field data inefficient. Relative entropy minimization does not require forces to be saved and is more data-efficient, but requires the CG model to be re-simulated during the iterative training procedure, which can make the training procedure extremely costly or lead to failure to converge. Here we present \emph{flow-matching}, a new training method for CG force fields that combines the advantages of force-matching and relative entropy minimization by leveraging normalizing flows, a generative deep learning method. Flow-matching first trains a normalizing flow to represent the CG probability density by using relative entropy minimization without suffering from the re-simulation problem because flows can directly sample from the equilibrium distribution they represent. Subsequently, the forces of the flow are used to train a CG force field by matching the coarse-grained forces directly, which is a much easier problem than traditional force-matching as it does not suffer from the noise problem. Besides not requiring forces, flow-matching also outperforms classical force-matching by an order of magnitude in terms of data efficiency and produces CG models that can capture the folding and unfolding of small proteins. ",Force-matching Coarse-Graining without Forces
201,1506222618243915779,60607937,F. G√ºney,"['as a follow-up to SLAMP, we propose to disentangle structure and motion for future prediction. we model the stochasticity in the underlying factors generating the pixels, i.e. predict future ego-motion, then conditioned on that, predict the object motion.\n<LINK> <LINK>', 'our method is significantly (40x) faster than VRNN and can handle interesting diverse examples like the one below (yes, turning is an interesting case where the majority of the data is going straight. https://t.co/VGF3K7HBU4']",https://arxiv.org/abs/2203.10528,"While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ",Stochastic Video Prediction with Structure and Motion
202,1506185787397836802,706100745255387137,Ali Safaya,"['New pre-print: <LINK> \n\nWe study event coreference for political events, and present a new challenging dataset for three languages.\n\nThis is a joint work by @hurrial, me, and collaborators.']",https://arxiv.org/abs/2203.10123,"We propose a dataset for event coreference resolution, which is based on random samples drawn from multiple sources, languages, and countries. Early scholarship on event information collection has not quantified the contribution of event coreference resolution. We prepared and analyzed a representative multilingual corpus and measured the performance and contribution of the state-of-the-art event coreference resolution approaches. We found that almost half of the event mentions in documents co-occur with other event mentions and this makes it inevitable to obtain erroneous or partial event information. We showed that event coreference resolution could help improving this situation. Our contribution sheds light on a challenge that has been overlooked or hard to study to date. Future event information collection studies can be designed based on the results we present in this report. The repository for this study is on this https URL ",Event Coreference Resolution for Contentious Politics Events
203,1505928221673545728,991687128387149824,Ajad Chhatkuli,"['Our ICLR 2022 work. We propose quite a different (old) perspective to boundary detection by using a vector field label representation and its vector divergence:\n<LINK>\nLabel representations can affect learning more than we think, as seen in implicit functions.']",https://arxiv.org/abs/2203.08795,"Boundaries are among the primary visual cues used by human and computer vision systems. One of the key problems in boundary detection is the label representation, which typically leads to class imbalance and, as a consequence, to thick boundaries that require non-differential post-processing steps to be thinned. In this paper, we re-interpret boundaries as 1-D surfaces and formulate a one-to-one vector transform function that allows for training of boundary prediction completely avoiding the class imbalance issue. Specifically, we define the boundary representation at any point as the unit vector pointing to the closest boundary surface. Our problem formulation leads to the estimation of direction as well as richer contextual information of the boundary, and, if desired, the availability of zero-pixel thin boundaries also at training time. Our method uses no hyper-parameter in the training loss and a fixed stable hyper-parameter at inference. We provide theoretical justification/discussions of the vector transform representation. We evaluate the proposed loss method using a standard architecture and show the excellent performance over other losses and representations on several datasets. ",Zero Pixel Directional Boundary by Vector Transform
204,1505829084210868224,16870304,Daniel Hershcovich,"['Typological diversity is necessary but not sufficient for inclusive #NLProc. In an ACL 2022 theme track paper titled ""Challenges and Strategies in Cross-Cultural NLP"", we propose a multidimensional framework for thinking about cultural bias and adaptation\n<LINK> <LINK>', 'Promising directions include inclusive data collection, robust and fair modeling, and even new tasks aiming to facilitate cross-cultural communication. https://t.co/aIo2l1vGQp', 'Particularly challenging is the balance between generalisation and representation - how to transfer ""universal"" knowledge effectively without promoting cultural homogenisation? https://t.co/7yp67x36rl', 'Human values may be in conflict depending on the stakeholders, and therefore AI alignment is complex. Common ""meta-objectives"" in our research community are in fact driven by cultural values. https://t.co/BJvlZdKx4b', 'Huge thanks to my co-authors @coastalcph: @stellalalallets @mdlhx @supernlpblog @StephanieBrandl @ebugliarello Laura Cabello Piqueras @KiddoThe2B @ruixiangcui @constanzafierro @katemargatina @rust_phillip and Anders S√∏gaard']",https://arxiv.org/abs/2203.10020,"Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies. ",Challenges and Strategies in Cross-Cultural NLP
205,1505202773046046723,1164592828640632832,Zorik Gekhman,"['New Preprint ‚Äì RED-ACE: Robust Error Detection for ASR using Confidence Embeddings.\n\nWe propose a novel model for ASR Errors Detection that leverages ASR confidence scores.\nJoint work with Dina Zverinski, Jonathan Mallinson and Genady Beryozkin\n<LINK>\n@GoogleAI <LINK>']",http://arxiv.org/abs/2203.07172,"ASR Error Detection (AED) models aim to post-process the output of Automatic Speech Recognition (ASR) systems, in order to detect transcription errors. Modern approaches usually use text-based input, comprised solely of the ASR transcription hypothesis, disregarding additional signals from the ASR model. Instead, we propose to utilize the ASR system's word-level confidence scores for improving AED performance. Specifically, we add an ASR Confidence Embedding (ACE) layer to the AED model's encoder, allowing us to jointly encode the confidence scores and the transcribed text into a contextualized representation. Our experiments show the benefits of ASR confidence scores for AED, their complementary effect over the textual signal, as well as the effectiveness and robustness of ACE for combining these signals. To foster further research, we publish a novel AED dataset consisting of ASR outputs on the LibriSpeech corpus with annotated transcription errors. ",RED-ACE: Robust Error Detection for ASR using Confidence Embeddings
206,1504639823184809984,22148802,Leo C. Stein ü¶Å,"[""üéâ New paper day! üéâ \n\nTidally-induced nonlinear resonances in EMRIs with an analogue model (<LINK>)\n\nThis is David's first paper! So, what did we study?\n1/6 <LINK>"", 'Orbits around spinning black holes have 3 frequencies, so there can be resonances. Adding a perturbation‚Äîe.g. a distant 3rd body‚Äîcan ""break"" resonant tori, creating nonlinear resonances. Here\'s what it looks like on a Poincar√© section. Our phase space is 6d ‚Üí 4d Poinc. sect.\n2/6 https://t.co/HQl3lOLS09', 'To visualize a 4-dimensional Poincar√© section, use 3 spatial dimensions, and color as a 4th dimension. Here is one for our system, a resonant torus that broke into a nonlinear resonance because of an external perturbation (the gravitational field of some distant stuff).\n3/6 https://t.co/AGnHAyU4oJ', 'Inside one of these resonances, we get libration of the ""resonance angle"" on a new time scale, seen below. So, what\'s the big idea in our paper? If we don\'t model this, will it screw up the ability of the LISA mission to detect systems that pass through resonance?\n4/6 https://t.co/26AnYhT0Hy', ""We computed the mismatch between signals where we do or don't attempt to model the nonlinear resonance, over a range of parameter space, to find out: how strong must the external perturbation be so that it *must be* modeled to get things right?\n\n5/6"", 'In the end, we found a simple approximate region of parameter space where the resonance must be modeled: Œµ ‚â≥ 300q¬≤, where q is the small mass ratio, and Œµ is a dimensionless measure of the strength of the perturbation.\n\nRead all about it here ‚û°Ô∏è https://t.co/GrWbb8uJ0F\n\n6/6ish', 'If you want to learn more about Poincar√© sections, check out this interactive web toy: https://t.co/AIGfyCcJoT', 'You can see our progress in this project by when I was tweeting about it long ago: https://t.co/CVzoT7n0zO']",https://arxiv.org/abs/2203.08841,"One of the important classes of targets for the future space-based gravitational wave observatory LISA is extreme mass ratio inspirals (EMRIs), where long and accurate waveform modeling is necessary for detection and characterization. When modeling the dynamics of an EMRI, several effects need to be included, such as the modifications caused by an external tidal field. The effects of such perturbations will generally break integrability at resonance, and can produce significant dephasing from an unperturbed system. In this paper, we use a Newtonian analogue of a Kerr black hole to study the effect of an external tidal field on the dynamics and the gravitational waveform. We have developed a numerical framework that takes advantage of the integrability of the background system to evolve it with a symplectic splitting integrator, and compute approximate gravitational waveforms to estimate the time scale over which the perturbation affects the dynamics. We find that different entry points into the resonance in phase-space can produce substantially different dynamics. Finally, by comparing this time scale with the inspiral time, we find tidal effects will need to be included when modeling EMRI gravitational waves when $\varepsilon \gtrsim 300\, q^2$, where $q$ is the small mass ratio, and $\varepsilon$ measures the strength of the external tidal field. ",Tidally-induced nonlinear resonances in EMRIs with an analogue model
207,1504024529009627139,892059194240532480,Mikel Artetxe,"['Check out our paper ‚ÄúDoes Corpus Quality Really Matter for Low-Resource Languages?‚Äù\n\nCommonCrawl-based multilingual corpora are noisy. But can we do better? What‚Äôs the impact in downstream tasks? We answer these taking Basque as a case study.\n\nThread üëá\n\n<LINK>', 'We collect a new corpus for Basque through tailored crawling (identifying and scraping 33 websites with high-quality content). The corpus is similar in size to the Basque portion of popular CommonCrawl corpora like mC4 and CC100. But is it better? https://t.co/R7Cdtjh2xj', 'We ask native annotators to assess the quality of documents from different corpora and annotate common problems. We find that CommonCrawl-based corpora have major quality issues as raised by prior work, while our corpus does much better. Does this carry over to downstream tasks? https://t.co/qdNJ4yDiYW', 'We train RoBERTa models in the different corpora, and evaluate them in 5 NLU tasks. Despite the big difference in corpus quality above, there is not a clear winner in this case‚Ä¶ https://t.co/MuXKwLATnL', 'All in all, our results suggest that investing on bigger and more diverse corpora might be more fruitful than improving data quality in low-resource languages.', 'Joint work with @jibalari, @ragerri, @olatz87 &amp; @Aitor57.', '@ZeerakTalat Yup, we also mention this hypothesis in the paper. But the main point still holds: data quality does not have a big impact, and other aspects (like domain) are more important.']",https://arxiv.org/abs/2203.08111,"The vast majority of non-English corpora are derived from automatically filtered versions of CommonCrawl. While prior work has identified major issues on the quality of these datasets (Kreutzer et al., 2021), it is not clear how this impacts downstream performance. Taking Basque as a case study, we explore tailored crawling (manually identifying and scraping websites with high-quality content) as an alternative to filtering CommonCrawl. Our new corpus, called EusCrawl, is similar in size to the Basque portion of popular multilingual corpora like CC100 and mC4, yet it has a much higher quality according to native annotators. For instance, 66% of documents are rated as high-quality for EusCrawl, in contrast with <33% for both mC4 and CC100. Nevertheless, we obtain similar results on downstream tasks regardless of the corpus used for pre-training. Our work suggests that NLU performance in low-resource languages is primarily constrained by the quantity rather than the quality of the data, prompting for methods to exploit more diverse data sources. ",Does Corpus Quality Really Matter for Low-Resource Languages?
208,1503700997566308352,952949678533849088,Kareem El-Badry,"['New paper! We study two ‚Äúmass gap‚Äù black hole candidates in binaries with red giant stars, ‚Äúthe Unicorn‚Äù and ‚Äúthe Giraffe‚Äù. 1/n <LINK> <LINK>', 'We used spectral disentangling to search for possible luminous companions (as opposed to BHs) to the giants. We found them! 2/n https://t.co/j5WPzU3ikL', 'What that means, roughly, is that two luminous stars do (much) a better job fitting the spectra than one. https://t.co/bogv9ie8nu', 'In both systems, the disentangled spectra of the companions look like subgiant stars (i.e., cooler and larger than main-sequence stars; warmer and smaller than giants). 4/n https://t.co/2cdMaFwDn2', 'Because these subgiants are cooler than main-sequence stars of similar mass, they are faint in the UV and consistent with the observed spectral energy distributions and UV limits. 5/n https://t.co/S20JlBiznj', 'We can measure the masses of the giants from the observed ellipsoidal variation. In both systems, they are 0.3-0.5 Msun. This is very low for a giant, and implies most of their initial mass was stripped off by a companion. 6/n https://t.co/jpx3Pk4kkh', ""We can also measure the masses of the subgiants dynamically. The dynamically-inferred values (1.8 and 2.8 Msun) are in reasonably good agreement with what we'd estimate from their temperature and luminosity. 7/n https://t.co/KSLzPBlDdm"", ""We used binary evolution models to investigate how these systems formed and how they'll evolve in the future. We think the giant are almost completely stripped of their envelopes and will soon contract to become low-mass helium white dwarfs. 8/n https://t.co/NKLLw40ob0"", ""This scenario (and the component masses) is almost identical to how we think Regulus, the ~20th brightest star in the sky, formed. It's a main-sequence star with a helium white dwarf companion in a wide orbit. https://t.co/nk0iFv0E9K 9/n"", 'The fact that the companions are subgiants (i.e, off the main sequence) implies that either the initial mass ratio was very close to 1 (like, q&gt;0.99), or that the companions are temporarily inflated due to rapid accretion. 9/', 'The second possibility is particularly exciting, but it will take more work (ideally a population model of interacting giant binaries) to test it.', 'The Unicorn and Giraffe join a growing population of mass-transfer binaries recently observed at various stages of the stripping process. Several of these other objects have also been previously interpreted as BHs. https://t.co/sxKQRKlkSb', 'Summary: stellar-mass BHs are small needles in a very large haystack. But they haystack contains a lot of other interesting stuff! n/n']",https://arxiv.org/abs/2203.06348,"We analyze two binary systems containing giant stars, V723 Mon (""the Unicorn"") and 2M04123153+6738486 (""the Giraffe""). Both giants orbit more massive but less luminous companions, previously proposed to be mass-gap black holes. Spectral disentangling reveals luminous companions with star-like spectra in both systems. Joint modeling of the spectra, light curves, and spectral energy distributions robustly constrains the masses, temperatures, and radii of both components: the primaries are luminous, cool giants ($T_{\rm eff,\,giant} = 3,800\,\rm K$ and $4,000\,\rm K$, $R_{\rm giant}= 22.5\,R_{\odot}$ and $25\,R_{\odot}$) with exceptionally low masses ($M_{\rm giant} \approx 0.4\,M_{\odot}$) that likely fill their Roche lobes. The secondaries are only slightly warmer subgiants ($T_{\rm eff,\,2} = 5,800\,\rm K$ and $5,150\,\rm K$, $R_2= 8.3\,R_{\odot}$ and $9\,R_{\odot}$) and thus are consistent with observed UV limits that would rule out main-sequence stars with similar masses ($M_2 \approx 2.8\,M_{\odot}$ and $\approx 1.8\,M_{\odot}$). In the Unicorn, rapid rotation blurs the spectral lines of the subgiant, making it challenging to detect even at wavelengths where it dominates the total light. Both giants have surface abundances indicative of CNO processing and subsequent envelope stripping. The properties of both systems can be reproduced by binary evolution models in which a $1-2\,M_{\odot}$ primary is stripped by a companion as it ascends the giant branch. The fact that the companions are also evolved implies either that the initial mass ratio was very near unity, or that the companions are temporarily inflated due to rapid accretion. The Unicorn and Giraffe offer a window into into a rarely-observed phase of binary evolution preceding the formation of wide-orbit helium white dwarfs, and eventually, compact binaries containing two helium white dwarfs. ","Unicorns and Giraffes in the binary zoo: stripped giants with subgiant
  companions"
209,1503641142855938049,3236251346,Mikel Sanz,"['New paper today <LINK> in which we study the limits of cv microwave entanglement distribution in open air. Great #qmics collaboration with WMI, @YasserOmar_ @mpmotton @SalariVahid congrats Tasio for this work! @NquireC @QuantumFlagship @QUANTEK2122 @ehuscientia <LINK>']",https://arxiv.org/abs/2203.07295,"Microwave technology plays a central role in current wireless communications, standing among them mobile communication and local area networks (LANs). The microwave range shows relevant advantages with respect to other frequencies in open-air transmission, such as low absorption losses and low energy consumption, and it is additionally the natural working frequency in superconducting quantum technologies. Entanglement distribution between separate parties is at the core of secure quantum communications. Therefore, understanding its limitations in realistic open-air settings, specially in the rather unexplored microwave regime, is crucial for transforming microwave quantum communications into a mainstream technology. Here, we investigate the feasibility of an open-air entanglement distribution scheme with microwave two-mode squeezed states. First, we study the reach of direct entanglement transmission in open-air, obtaining a maximum distance of approximately 500 meters in a realistic setting with state-of-the-art experimental parameters. Afterwards, we adapt entanglement distillation and entanglement swapping protocols to microwave technology in order to reduce environmental entanglement degradation. While entanglement distillation helps to increase quantum correlations in the short-distance low-squeezing regime by up to $46\%$, entanglement swapping increases the reach by $14\%$. Then, we compute the fidelity of a continuous-variable quantum teleportation protocol using open-air-distributed entanglement as a resource. Finally, we adapt the machinery to explore the limitations of quantum communication between satellites, where the thermal noise impact is substantially reduced and diffraction losses are dominant. ",Open-Air Microwave Entanglement Distribution for Quantum Teleportation
210,1503405126429650950,1242372799039246338,JF Rupprecht - OM team,"['Today on <LINK> Extending active nematics for curved surfaces with an up-down asymmetry to model epithelial tissues or biofilms grown on wavy substrates; we find thresholdless active flows, discontinuous transitions and hysteresis between flowing steady states', '... challenging established paradigms for activity-driven spontaneous flows ; * S. Bell, SZ Lin &amp; J. Prost @Lab_PCC']",https://arxiv.org/abs/2203.05644,"Cell monolayers are a central model system to tissue biophysics. In vivo, epithelial tissues are curved on the scale of microns, and curvature's role in the onset of spontaneous tissue flows is still not well-understood. Here, we present a hydrodynamic theory for an apical-basal asymmetric active nematic gel on a curved strip. We show that surface curvature qualitatively changes monolayer motion compared to flat space: the resulting flows can be thresholdless, and the transition to motion may change from continuous to discontinuous. Surface curvature, friction and active tractions are all shown to control the flow pattern selected: from simple shear to vortex chains. ",Active nematic flows on curved surfaces
211,1502225665763987456,719257180986335232,Daniel Matoz-Fernandez,['Our new preprint is out! We study how chemical active particles. We found that a chemical field induces competing attractive positional and repulsive orientational interactions which can drive the MIPS phase to very low densities. #ActiveMatter #Physics\n<LINK>'],https://arxiv.org/abs/2203.05213,"We study the dynamical steady-states of a monolayer of chemically active self-phoretic colloids as a function of packing fraction and self-propulsion speed by means of Brownian dynamics simulations. We focus on the case that a chemical field induces competing attractive positional and repulsive orientational interactions. Analyzing the distribution of cluster size and local density as well as the hexatic order parameter, we distinguish four distinct dynamical states which include collapsed, active gas, dynamical clustering, and motility-induced phase-separated states. The long-range chemical field-induced interactions shift the onset of motility-induced phase separation (MIPS) to very low packing fractions at intermediate self-propulsion speeds. We also find that the fraction of particles in the largest clusters is a suitable order parameter characterizing the dynamical phase transitions from an active gas or dynamical clustering steady-state to a phase-separated state upon increase of the packing fraction. The order parameter changes discontinuously when going from an active gas to a MIPS-like state at intermediate self-propulsion speeds, whereas it changes continuously at larger activities where the system undergoes a transition from a dynamical clustering state to MIPS-like state. ","Dynamical steady-states of active colloids interacting via chemical
  fields"
212,1502031553433681922,1268126508708777984,zahra parsaeian,"['Really excited to announce that my first publication got accepted in SoCG 2022 and is now available on arxiv. <LINK>\n\n""We initiate the study of diameter computation in geometric intersection graphs from the fine-grained complexity perspective.""']",https://arxiv.org/abs/2203.03663,"We initiate the study of diameter computation in geometric intersection graphs from the fine-grained complexity perspective. A geometric intersection graph is a graph whose vertices correspond to some shapes in $d$-dimensional Euclidean space, such as balls, segments, or hypercubes, and whose edges correspond to pairs of intersecting shapes. The diameter of a graph is the largest distance realized by a pair of vertices in the graph. Computing the diameter in near-quadratic time is possible in several classes of intersection graphs [Chan and Skrepetos 2019], but it is not at all clear if these algorithms are optimal, especially since in the related class of planar graphs the diameter can be computed in $\widetilde{\mathcal{O}}(n^{5/3})$ time [Cabello 2019, Gawrychowski et al. 2021]. In this work we (conditionally) rule out sub-quadratic algorithms in several classes of intersection graphs, i.e., algorithms of running time $\mathcal{O}(n^{2-\delta})$ for some $\delta>0$. In particular, there are no sub-quadratic algorithms already for fat objects in small dimensions: unit balls in $\mathbb{R}^3$ or congruent equilateral triangles in $\mathbb{R}^2$. For unit segments and congruent equilateral triangles, we can even rule out strong sub-quadratic approximations already in $\mathbb{R}^2$. It seems that the hardness of approximation may also depend on dimensionality: for axis-parallel unit hypercubes in~$\mathbb{R}^{12}$, distinguishing between diameter 2 and 3 needs quadratic time (ruling out $(3/2-\varepsilon)$- approximations), whereas for axis-parallel unit squares, we give an algorithm that distinguishes between diameter $2$ and $3$ in near-linear time. Note that many of our lower bounds match the best known algorithms up to sub-polynomial factors. ","Towards Sub-Quadratic Diameter Computation in Geometric Intersection
  Graphs"
213,1501978303078535174,1024247894474481665,Furkan G√ºrsoy,"['System Cards for AI... is out!\n\nWe propose:\n\n‚ú≥Ô∏èSystem Accountability Benchmark: 50 criteria organized in a framework to audit algorithmic accountability.\n\n‚ú≥Ô∏èSystem Cards: scorecards presenting the outcomes of such algorithm audits.\n\nFeedback is welcome!\n\n<LINK> <LINK>', 'üëâüèøThe use of AI systems expanded dramatically, including decisions in criminal justice, allocation of public resources, public education, etc.\n\nüëâüèø AI systems are not inherently free from issues such as bias, opaqueness, lack of explainability, maleficence, and the like.', 'üëâüèø Standards of accountability reflecting current legal obligations and societal concerns lag behind the extensive use of AI systems.\n\n‚úÖAuditing AI systems based on the proposed framework can ensure fairness, privacy, explainability, transparency, robustness, and more.']",https://arxiv.org/abs/2203.04754,"Decisions in public policy are increasingly being made or assisted by automated decision-making algorithms. Many of these algorithms process personal data for tasks such as predicting recidivism, assisting welfare decisions, identifying individuals using face recognition, and more. While potentially improving efficiency and effectiveness, such algorithms are not inherently free from issues such as bias, opaqueness, lack of explainability, maleficence, and the like. Given that the outcomes of these algorithms have significant impacts on individuals and society and are open to analysis and contestation after deployment, such issues must be accounted for before deployment. Formal audits are a way towards ensuring algorithms that are used in public policy meet the appropriate accountability standards. This work, based on an extensive analysis of the literature, proposes a unifying framework for system accountability benchmark for formal audits of artificial intelligence-based decision-aiding systems in public policy as well as system cards that serve as scorecards presenting the outcomes of such audits. The benchmark consists of 50 criteria organized within a four by four matrix consisting of the dimensions of (i) data, (ii) model, (iii) code, (iv) system and (a) development, (b) assessment, (c) mitigation, (d) assurance. Each criterion is described and discussed alongside a suggested measurement scale indicating whether the evaluations are to be performed by humans or computers and whether the evaluation outcomes are binary or on an ordinal scale. The proposed system accountability benchmark reflects the state-of-the-art developments for accountable systems, serves as a checklist for future algorithm audits, and paves the way for sequential work as future research. ",System Cards for AI-Based Decision-Making for Public Policy
214,1501816833963442181,1190175298106675200,Jonas Latz,"['An elephant üêò in the Gaussian random field: how to study classical regularity while working on a structured function space?\n\nThat is what we discuss in our new preprint:\nKorolev, Latz, Sch√∂nlieb: Gaussian random fields on non-separable Banach spaces (<LINK>)']",https://arxiv.org/abs/2203.04650,"We study Gaussian random fields on certain Banach spaces and investigate conditions for their existence. Our results apply inter alia to spaces of Radon measures and H\""older functions. In the former case, we are able to define Gaussian white noise on the space of measures directly, avoiding, e.g., an embedding into a negative-order Sobolev space. In the latter case, we demonstrate how H\""older regularity of the samples is controlled by that of the covariance kernel and, thus, show a connection to the Theorem of Kolmogorov-Chentsov. ",Gaussian random fields on non-separable Banach spaces
215,1501609588042444805,1701409680,Suhail Dhawan,"['Paper day! We present the pilot study of measuring H0 from a uniform DL with both rungs of SNe Ia from ZTF. The  luminosity is calibrated by the TRGB, an exciting route to percent level H0. Big thanks to Ariel, Joel, In Sung and the ZTF  / CCHP teams. \n<LINK>']",https://arxiv.org/abs/2203.04241,"The current Cepheid-calibrated distance ladder measurement of $H_0$ is reported to be in tension with the values inferred from the cosmic microwave background (CMB), assuming standard model cosmology. However, the tip of the red giant branch (TRGB) reports an estimate of $H_0$ in better agreement with the CMB. Hence, it is critical to reduce systematic uncertainties in local measurements to understand the origin of the Hubble tension. In this paper, we propose a uniform distance ladder, combining SNe~Ia observed by the Zwicky Transient Facility (ZTF) with a TRGB calibration of their absolute luminosity. A large, volume-limited, sample of both calibrator and Hubble flow SNe Ia from the \emph{same} survey minimizes two of the largest sources of systematics: host-galaxy bias and non-uniform photometric calibration. We present results from a pilot study using existing TRGB distance to the host galaxy of ZTF SN Ia SN 2021rhu (aka ZTF21abiuvdk). Combining the ZTF calibrator with a volume-limited sample from the first data release of ZTF Hubble flow SNe Ia, we infer $H_0 = 76.94 \pm 6.4\, {\rm km}\,{\rm s^{-1}}\,{\rm Mpc^{-1}}$, an $8.3 \%$ measurement. The error budget is dominated by the single object calibrating the SN Ia luminosity in this pilot study. However, the ZTF sample includes already five other SNe Ia within $\sim$ 20 Mpc for which TRGB distances can be obtained with HST. Finally, we present the prospects of building this distance ladder out to 80 Mpc with JWST observations of more than one hundred SNe Ia. ","A Uniform Type Ia Supernova Distance Ladder with the Zwicky Transient
  Facility: Absolute Calibration Based on the Tip of the Red Giant Branch
  (TRGB) Method"
216,1501578462980718598,1130880709638447104,Iulian Vlad Serban,"[""AI will transform online learning for millions!\n\nWe've just published our landmark study (n=199 participants) showing that personalized AI tutors double their skills completion rate and then achieve 2.5X higher learning gains on their skills assessments!\n<LINK>""]",https://arxiv.org/abs/2203.03724,"Despite artificial intelligence (AI) having transformed major aspects of our society, less than a fraction of its potential has been explored, let alone deployed, for education. AI-powered learning can provide millions of learners with a highly personalized, active and practical learning experience, which is key to successful learning. This is especially relevant in the context of online learning platforms. In this paper, we present the results of a comparative head-to-head study on learning outcomes for two popular online learning platforms (n=199 participants): A MOOC platform following a traditional model delivering content using lecture videos and multiple-choice quizzes, and the Korbit learning platform providing a highly personalized, active and practical learning experience. We observe a huge and statistically significant increase in the learning outcomes, with students on the Korbit platform providing full feedback resulting in higher course completion rates and achieving learning gains 2 to 2.5 times higher than both students on the MOOC platform and students in a control group who don't receive personalized feedback on the Korbit platform. The results demonstrate the tremendous impact that can be achieved with a personalized, active learning AI-powered system. Making this technology and learning experience available to millions of learners around the world will represent a significant leap forward towards the democratization of education. ","A New Era: Intelligent Tutoring Systems Will Transform Online Learning
  for Millions"
217,1501455243652915200,1306523919428521984,Simran Tinani,"['Conjugacy problems in groups have been used to build cryptographic systems since at least 1999. In this work we study the complexity of conjugacy search in some suggested nonabelian platform groups, often obtaining reductions back to the DLP.\n<LINK>']",https://arxiv.org/abs/2203.03525,"The most prominent algorithmic problem employed in the recently emerging field of nonabelian group-based cryptography is the Conjugacy Search Problem (CSP). While several methods of attacks on nonabelian protocols have been devised, many of these are heuristic, protocol-specific, and focus on retrieving the shared keys without solving the underlying CSP in the group. So far, the true complexity of the CSP in different platform groups has not been sufficiently investigated. In this paper, we study the complexity of various versions of the CSP in polycyclic groups and matrix groups over finite fields. In particular we show that in $\GL_n(\fq)$ and in polycyclic groups with two generators, a CSP where conjugators are restricted to a cyclic subgroup is reducible to a set of $\mathcal{O}(n^2)$ DLPs. As a consequence of our results we also demonstrate the cryptanalysis of a few independently proposed cryptosystems. ",Complexity of Conjugacy Search in some Polycyclic and Matrix Groups
218,1501256962855649287,1005482708066340864,Maxime Lucas,"['Do higher-order interactions promote synchronisation?\n\nWe find that they typically do in random hypergraphs, but have the opposite effect in random simplicial complexes. \n\nPreprint: <LINK>\nCode: <LINK>\nwith @YuanzhaoZhang &amp; @fede7j \n\nBut why? 1/ <LINK>', 'Higher-order interactions enable more nodes to exchange information simultaneously, thus allowing more efficient communication. It would seem physically plausible for this to promote sync in general. Several studies showed examples of improved sync, too.\n\nSo? 2/', 'In our model with 1- and 2-order interactions, we fixed the total coupling budget. This allows a fair comparison of the orders. \n\nAs parameter Œ± increases, we go from 1st-order-only (Œ±=0) to 2nd-order-only (Œ±=1), and stability changes. What makes it change differently? 3/', 'We identified degree heterogeneity as a key structural parameter. Using it, we provide analytical explanations for the promotion/inhibition of sync. \n\nIn simplicial complexes, e.g., filling empty triangles makes degree-rich nodes get richer ‚û°Ô∏è more heterogeneity ‚û°Ô∏è worse sync 4/4']",http://arxiv.org/abs/2203.03060,"Understanding how nonpairwise interactions alter dynamical processes in networks is of fundamental importance to the characterization and control of many coupled systems. Recent discoveries of hyperedge-enhanced synchronization under various settings raised speculations that such enhancements might be a general phenomenon. Here, we demonstrate that even for simple systems such as Kuramoto oscillators, the effects of higher-order interactions are highly representation-dependent. Specifically, we show numerically and analytically that hyperedges typically enhance synchronization in random hypergraphs, but have the opposite effect in simplicial complexes. As an explanation, we identify higher-order degree heterogeneity as the key structural determinant of synchronization stability in systems with a fixed coupling budget. Our findings highlight the importance of appropriate representations in describing higher-order interactions. In particular, the choice of simplicial complexes or hypergraphs has significant ramifications and should not be purely motivated by technical conveniences. ",Do higher-order interactions promote synchronization?
219,1501231837141618694,701189916387049472,Michael Zhang,"['Excited to share work improving ML robustness to spurious correlations! <LINK> \n\nWe propose Correct-N-Contrast, a contrastive learning method that trains models to ignore spurious correlations while learning class-separable representations.  \n\nDeets in üßµüëá (1/9)', 'Spurious correlations pose a fundamental problem for training ML models in the ‚Äúreal world‚Äù. \n\nLots of great work shows standard training (i.e., ERM) can result in learning these correlations‚Äîthink models relying on correlated backgrounds to classify objects of interest... (2/9) https://t.co/ubj074TAK8', 'These ‚ÄúERM models‚Äù are not robust. They can perform poorly on data subsets or ‚Äúgroups‚Äù where these correlations don‚Äôt hold. \n\nHowever, for many safety-critical or human-facing applications, we require deep learning models that perform well on *all* groups. (3/9)', 'Other issues: \nPrior work using training group labels can improve robustness, but we may not always have these labels (too expensive to collect).\n\nOther works avoid this by *inferring* training group labels, but they still underperform methods that require group labels. (4/9)', 'With Correct-N-Contrast (CNC), we substantially improve robustness without training group labels. This significantly closes the above performance gap. \n\nWe accomplish this with two stepsüëá (5/9)', 'Step 1: Infer groups as unique combos of true class labels and an ERM model‚Äôs predictions (similar to prior work).\n\nStep 2: Repurpose contrastive learning, and train a robust model to only ‚Äúpush together‚Äù samples in the same class, but different inferred groups (new stuff). (6/9) https://t.co/5pBAOEB1iM', 'CNC thus encourages ignoring differences among spurious features and focusing on class-relevant similarities. \n\nAmong methods not requiring training group labels, this leads to SoTA or near-SoTA robust performance on popular spurious correlation benchmarks. (7/9) https://t.co/FoDdpL21jU', 'We find CNC-learned representations exhibit several desirable properties for robustness (e.g., encoding less info about spurious artifacts). \n\nWe study this both empirically, and via new theoretical connections between a model‚Äôs representations and its robust performance. (8/9) https://t.co/0ef49oqknB', 'üôè Impossible without amazing co-authors @nimit_sohoni, @HongyangZhang, @chelseabfinn, @HazyResearch \n\nCode and blogpost to come soon! (9/9)', ""Addendum: \nI was told by @krandiash that my post didn't have my usual memes. \n\nHere's more motivation for why spurious correlations are a big problem for ML‚Äîwe can get models that act like thisüëá \n\n(iykyk; regardless see @sarameghanbeery's inspiring work https://t.co/Mv7xZ5FA5N) https://t.co/ueoQaxIXU2"", ""@AndrewLBeam Thanks! Not super formal, but at some level we're calling a correlation spurious if the relation b/t features and label isn't causal.\nI'm hesitant to stray from observed data land, but this is often a relation that holds for many but not all samples (as assumed by other work too)"", '@AndrewLBeam Great point that just the presence of spurious artifacts may not lead to poor worst-group error. The key issue is that in many settings, models actually tend to learn the spurious relations b/t these artifacts &amp; labels (e.g., bc they‚Äôre ""easier to learn"" than nonspurious ones).', ""@AndrewLBeam When they do, they misclassify the samples that don't exhibit these correlations, resulting in poor worst-group error. \n\nOne reference that comes to mind exploring why this can occur is @shiorisagawa, Aditi, @PangWeiKoh's great work https://t.co/UZcIfCgMbh""]",https://arxiv.org/abs/2203.01517,"Spurious correlations pose a major challenge for robust machine learning. Models trained with empirical risk minimization (ERM) may learn to rely on correlations between class labels and spurious attributes, leading to poor performance on data groups without these correlations. This is particularly challenging to address when spurious attribute labels are unavailable. To improve worst-group performance on spuriously correlated data without training attribute labels, we propose Correct-N-Contrast (CNC), a contrastive approach to directly learn representations robust to spurious correlations. As ERM models can be good spurious attribute predictors, CNC works by (1) using a trained ERM model's outputs to identify samples with the same class but dissimilar spurious features, and (2) training a robust model with contrastive learning to learn similar representations for same-class samples. To support CNC, we introduce new connections between worst-group error and a representation alignment loss that CNC aims to minimize. We empirically observe that worst-group error closely tracks with alignment loss, and prove that the alignment loss over a class helps upper-bound the class's worst-group vs. average error gap. On popular benchmarks, CNC reduces alignment loss drastically, and achieves state-of-the-art worst-group accuracy by 3.6% average absolute lift. CNC is also competitive with oracle methods that require group labels. ","Correct-N-Contrast: A Contrastive Approach for Improving Robustness to
  Spurious Correlations"
220,1501131899493883904,1248083315808260097,Manuel Weber,"['New preprint out today! We determine the phase diagram and criticality of an SU(2)-symmetric generalization of the celebrated spin-boson model. We track the annihilation of two intermediate-coupling fixed points and find a surprising duality between them. \n<LINK>', 'The SU(2)-symmetric spin-boson model (aka Bose Kondo model) is relevant to diverse problems such as magnetic moments in quantum critical magnets or Kondo-breakdown transitions in heavy fermion metals.', 'Moreover, it represents a numerically tractable model to study the fixed-point annihilation discussed in various contexts like the Abelian Higgs model, the chiral phase transition in QCD, the Q-state Potts model, or deconfined quantum criticality in quantum magnets.']",https://arxiv.org/abs/2203.02518,"Dissipative quantum impurity models represent conceptually simple yet non-trivial settings for quantum criticality. Here we present results from high-accuracy quantum Monte Carlo calculations for the SU(2)-symmetric spin-boson (or Bose-Kondo) model, relevant to diverse problems such as cavity quantum electrodynamics, magnetic moments in quantum critical magnets, and Kondo-breakdown transitions in heavy-fermion metals. We study the model with a power-law bath spectrum $\propto \omega^s$ where, in addition to a critical phase predicted by perturbative renormalization group (RG), a stable strong-coupling phase is present for all values of $0<s<1$. The critical phase ceases to exist for $s<s^\ast = 0.6540(2)$, rendering the perturbative prediction invalid. We provide direct numerical evidence for the collision and annihilation of two intermediate-coupling RG fixed points at $s^\ast$ responsible for that. We uncover a surprising duality between the two fixed points, corresponding to a reflection symmetry of the RG beta function. We then utilize this duality to make analytical predictions for critical properties at strong coupling which are in excellent agreement with the numerical results. We comment on the consequences for impurity moments in critical magnets. ","SU(2)-symmetric spin-boson model: Quantum criticality, fixed-point
  annihilation, and duality"
221,1500864861617541122,194377912,Brian Keating,['New @CMB3K primordial B-mode results! We increased the data volume by 80% and find the measured spectrum is consistent with ŒõCDM (including foregrounds). We place an upper limit on the tensor-to-scalar ratio of r&lt; 0.33 at 95% confidence. \n<LINK>\nüì∏: Debra Kellner <LINK>'],https://arxiv.org/abs/2203.02495,"We report an improved measurement of the degree-scale CMB $B$-mode angular power spectrum over 670 square-degree sky area with POLARBEAR. In the original analysis of the data, errors in the angle measurement of the continuously rotating half-wave plate, a polarization modulator, caused significant data loss. By introducing an angle-correction algorithm, the data volume is increased by a factor of 1.8. We report a new analysis using the larger data set. We find the measured $B$-mode spectrum is consistent with the $\Lambda$CDM model with Galactic foregrounds. We place an upper limit on the tensor-to-scalar ratio $r$ < 0.33 at 95% confidence level. ","Improved upper limit on degree-scale CMB B-mode polarization power from
  the 670 square-degree POLARBEAR survey"
222,1499812487193051137,18647972,Richard J. Chen,"['Excited to share work with @rahulgk @MSFTResearch, presented at #LMRL #NeurIPS 2021.\n\nWe pretrained ViTs on histopathology images - and find they learn meaningful visual concepts.\n\nPaper Link: <LINK>\nPretrained Weights: <LINK>\n\nKey Findings: 1/ <LINK>', 'In general CV, DINO by @mcaron31 and extensions can learn interpretable subparts of images, and has been used for object discovery.\n\nWe highlight interpreting images as subparts, e.g. - part-whole hierarchies, is very natural in histology in learning cell-tissue organization. 2/ https://t.co/HTL54DtTcC', 'Comparing w/ SimCLR and ImageNet features, DINO learns better &amp; more efficient representations, tested on patch- and slide-level tasks.\n\nSSL helps w/ domain shift. On raw &amp; stain-normalized CRC100K, global structure of morphological subtypes are better preserved than ImageNet. 3/ https://t.co/MxPJk1KaYq', 'Lastly, DINO localizes cell location quite well w/o supervision. Our findings demonstrate ViTs can easily localize visual concepts in histopathology via introspecting the attention heads. 4/ https://t.co/mZ3Xk4UXoJ', 'We plan to add more pretrained models + evaluation metrics, with a larger paper coming soon :^). Special thanks to also the BioML Group @MSRNE, @lorin_crawford, @apsoleimany, Kristen Severson, @KevinKaichuang, @nfusi, and @rahulgk again for supporting me over the summer! 5/', '@tae_hwang @rahulgk @MSFTResearch Thank you Tae!']",http://arxiv.org/abs/2203.00585,"Tissue phenotyping is a fundamental task in learning objective characterizations of histopathologic biomarkers within the tumor-immune microenvironment in cancer pathology. However, whole-slide imaging (WSI) is a complex computer vision in which: 1) WSIs have enormous image resolutions with precludes large-scale pixel-level efforts in data curation, and 2) diversity of morphological phenotypes results in inter- and intra-observer variability in tissue labeling. To address these limitations, current efforts have proposed using pretrained image encoders (transfer learning from ImageNet, self-supervised pretraining) in extracting morphological features from pathology, but have not been extensively validated. In this work, we conduct a search for good representations in pathology by training a variety of self-supervised models with validation on a variety of weakly-supervised and patch-level tasks. Our key finding is in discovering that Vision Transformers using DINO-based knowledge distillation are able to learn data-efficient and interpretable features in histology images wherein the different attention heads learn distinct morphological phenotypes. We make evaluation code and pretrained weights publicly-available at: this https URL ","Self-Supervised Vision Transformers Learn Visual Concepts in
  Histopathology"
223,1499739037015367680,1658897460,Tim Davis,"['Today is a paper day! We use very high-resolution @almaobs maps of the molecular gas in the centre of nearby galaxies (from WISDOM and PHANGS) to study the processes setting its morphology! A small üßµ below- see the paper here for more <LINK> (1/7)', ""Our data from WISDOM reveals the ISM in the centers of nearby galaxies at resolutions of ~10-30pc. As we collected the data we noticed something strange - the ISM in ETGs wasn't breaking apart into individual molecular clouds in the same way as seen in spirals... (2/7) https://t.co/vsqySeCUFd"", 'We borrowed the classic Asymmetry, Smoothness &amp; Gini parameters used by optical astronomers to quantify the morphology of the gas. This confirmed our suspicions- massive bulge-dominated galaxies have a smoother, more symmetric ISM morphology at the same spatial scale (3/7)', 'We also make use of state-of-the-art hydrodynamic simulations, extracting the same statistics. These systems behave in very similar ways- and there we know why: the deep potential well increases the shear, disrupting large molecular clouds. (4/7) https://t.co/5miE7vHVgw', ""So is that what's going on in the real universe? It looks like it could be! We correlate the ISM morphology indicators with a variety of galaxy properties and show that the best predictor is the stellar mass surface (or volume) density. You can even see the effect by eye! (5/7) https://t.co/QbTPboaXeN"", 'In bulge-dominated systems the stellar potential dominates the stability of the gas. This suppresses spiral arm formation, reduces inflows, and can shear the ISM into small pieces. This may also explain the reduction in star formation efficiency seen in these environments! (6/7)', 'Overall this work shows that the environment in which a gas disc finds itself is important in determining what happens to it: how it fragments and forms stars. Happy to answer any questions people have! (7/7)', ""@astro_francesca Yes- we suggest its the same underlying physics. The shear is high in objects with deep+concentrated potential wells, pulling gas clouds apart and making it harder for the gas to fragment and form stars. Jindra's nice paper shows that clearly for the sims: https://t.co/9RATCiTM1a"", 'Ooo and if you are interested in finding out more about WISDOM you can check out our new website! (8/7) https://t.co/yfCKHPUHde / https://t.co/sq4ntMnBVK']",https://arxiv.org/abs/2203.01358,"We use high-resolution maps of the molecular interstellar medium (ISM) in the centres of eighty-six nearby galaxies from the millimetre-Wave Interferometric Survey of Dark Object Masses (WISDOM) and Physics at High Angular Resolution in Nearby GalaxieS (PHANGS) surveys to investigate the physical mechanisms setting the morphology of the ISM at molecular cloud scales. We show that early-type galaxies tend to have smooth, regular molecular gas morphologies, while the ISM in spiral galaxy bulges is much more asymmetric and clumpy when observed at the same spatial scales. We quantify these differences using non-parametric morphology measures (Asymmetry, Smoothness and Gini), and compare these measurements with those extracted from idealised galaxy simulations. We show that the morphology of the molecular ISM changes systematically as a function of various large-scale galaxy parameters, including galaxy morphological type, stellar mass, stellar velocity dispersion, effective stellar mass surface density, molecular gas surface density, star formation efficiency and the presence of a bar. We perform a statistical analysis to determine which of these correlated parameters best predicts the morphology of the ISM. We find the effective stellar mass surface (or volume) density to be the strongest predictor of the morphology of the molecular gas, while star formation and bars maybe be important secondary drivers. We find that gas self-gravity is not the dominant process shaping the morphology of the molecular gas in galaxy centres. Instead effects caused by the depth of the potential well such as shear, suppression of stellar spiral density waves and/or inflow affect the ability of the gas to fragment. ","WISDOM Project -- X. The morphology of the molecular ISM in galaxy
  centres and its dependence on galaxy structure"
224,1499673181694709761,328430286,Jad C. Halimeh,"['New paper <LINK> (1st of 2 from today)! We study dynamical quantum phase transitions (DQPTs) in spin-S U(1) quantum link models, showing rich critical behavior in the lattice-QED limit with various distinct types of DQPTs\n@ERC_Research\n@MCQST_cluster\n@HaukeGroup <LINK>']",https://arxiv.org/abs/2203.01337,"Dynamical quantum phase transitions (DQPTs) are a powerful concept of probing far-from-equilibrium criticality in quantum many-body systems. With the strong ongoing experimental drive to quantum-simulate lattice gauge theories, it becomes important to investigate DQPTs in these models in order to better understand their far-from-equilibrium properties. In this work, we use infinite matrix product state techniques to study DQPTs in spin-$S$ $\mathrm{U}(1)$ quantum link models. Although we are able to reproduce literature results directly connecting DQPTs to a sign change in the dynamical order parameter in the case of $S=1/2$ for quenches starting in a vacuum initial state, we find that for different quench protocols or different values of the link spin length $S>1/2$ this direct connection is no longer present. In particular, we find that there is an abundance of different types of DQPTs not directly associated with any sign change of the order parameter. Our findings indicate that DQPTs are fundamentally different between the Wilson--Kogut--Susskind limit and its representation through the quantum link formalism. ","Dynamical quantum phase transitions in spin-$S$ $\mathrm{U}(1)$ quantum
  link models"
225,1499389400282583045,1486368673,Cagri Toraman,['Our study on large scale hate speech detection w/ @Furkansahinuc and @eyup_halit is now in arXiv. \n\nWe publish English and Turkish datasets with 100k tweets!\n\nAnd have interesting results on cross-topic transfer of hate speech. \n#NLProc #HateSpeech #LREC\n\n<LINK>'],https://arxiv.org/abs/2203.01111,"The performance of hate speech detection models relies on the datasets on which the models are trained. Existing datasets are mostly prepared with a limited number of instances or hate domains that define hate topics. This hinders large-scale analysis and transfer learning with respect to hate domains. In this study, we construct large-scale tweet datasets for hate speech detection in English and a low-resource language, Turkish, consisting of human-labeled 100k tweets per each. Our datasets are designed to have equal number of tweets distributed over five domains. The experimental results supported by statistical tests show that Transformer-based language models outperform conventional bag-of-words and neural models by at least 5% in English and 10% in Turkish for large-scale hate speech detection. The performance is also scalable to different training sizes, such that 98% of performance in English, and 97% in Turkish, are recovered when 20% of training instances are used. We further examine the generalization ability of cross-domain transfer among hate domains. We show that 96% of the performance of a target domain in average is recovered by other domains for English, and 92% for Turkish. Gender and religion are more successful to generalize to other domains, while sports fail most. ",Large-Scale Hate Speech Detection with Cross-Domain Transfer
226,1499372545702862848,911896919169011712,Alexander Lau,"['In our new #arXiv #preprint, we look into the superfluid weight of flat-band superconductors with disorder. We find a universal suppression independent of energy dispersion and #quantum geometry. More details in the thread below #MagTopCSL <LINK> 1/12', 'New research from MagTop at the Institute of Physics within @PAN_akademia. With funding from @MSCActions #MagTopCSL 2/12', 'Big thanks to my collaborators @SebastianoPeot1 and Timo Hyart from @AaltoUniversity @AaltoResearch, Dmitry I. Pikulin from @Microsoft Quantum @MSFTResearch, and Enrico Rossi from @williamandmary.   3/12', 'The superfluid weight is a transport coefficient closely connected to a superconductor‚Äôs hallmark properties: the presence of dissipationless electric currents and the expulsion of magnetic fields from its interior, also known as the Meissner effect. 4/12', 'Conventional superconductors arise from superconducting instabilities in metals with curved electronic energy bands. Here, the superfluid weight is connected to the band curvature: large curvature = large superfluid weight, small curvature = small superfluid weight. 5/12', 'This is, however, not the full story: also flat electronic bands can lead to a large superfluid weight provided the involved electrons have a nontrivial quantum geometry, such as in topological bands. This gives rise to a geometric contribution to the superfluid weight. 6/12', 'Topological bands are characterized by integer numbers, such as Chern numbers, and are inherently robust against small disorder. So we were wondering whether this robustness would carry over also to superconductors derived from flat topological bands. 7/12', 'For this purpose, we compared different theoretical models with flat topological, curved topological, and conventional energy bands with respect to how the superfluid weight of their superconducting phases behaves in the presence of disorder. 8/12', 'Surprisingly, we find that all considered superconductors show the same universal behaviour: the superfluid weight is suppressed in a way independent of the band curvature and, in particular, independent of the quantum geometry of the underlying electronic bands. 9/12', 'At the same time, this also means that flat-band superconductors are as resilient to disorder as conventional superconductors: the flatness of their bands doesn‚Äôt make them prone to losing their superconducting properties. 10/12', 'We hope that our theoretical predictions will soon be studied also experimentally, for instance in #graphene -based heterostructures, which have a high degree of tunability and are known to realize various superconducting phases. 11/12', 'All scripts and data of this study are available at https://t.co/5Km3KOrZEo #openscience #opendata 12/12']",http://arxiv.org/abs/2203.01058,"Motivated by the experimental progress in controlling the properties of the energy bands in superconductors, significant theoretical efforts have been devoted to study the effect of the quantum geometry and the flatness of the dispersion on the superfluid weight. In conventional superconductors, where the energy bands are wide and the Fermi energy is large, the contribution due to the quantum geometry is negligible, but in the opposite limit of flat-band superconductors the superfluid weight originates purely from the quantum geometry of Bloch wave functions. Here, we study how the energy band dispersion and the quantum geometry affect the disorder-induced suppression of the superfluid weight. Surprisingly, we find that the disorder-dependence of the superfluid weight is universal across a variety of models, and independent of the quantum geometry and the flatness of the dispersion. Our results suggest that a flat-band superconductor is as resilient to disorder as a conventional superconductor. ","Universal suppression of superfluid weight by disorder independent of
  quantum geometry and band dispersion"
227,1499049053035900930,1416408573303721988,Alexander Wietek,"[""Very happy to have new collaborators, Chunhan and Rajiv, on our new preprint together with Miles: <LINK>\n\nWe have been looking at 3D pyrochlore physics on a 1D tube geometry to study monopole confinement and establish the model's phase diagram.""]",https://arxiv.org/abs/2203.00032,"We study the ground state and thermodynamic properties of the spin-half XXZ model, with an Ising interaction $J_z$ and a transverse exchange interaction $J_{x}$, on a pyrochlore tube obtained by joining together elementary cubes in a one-dimensional array. Periodic boundary conditions in the transverse directions ensure that the bulk of the system consists of corner-sharing tetrahedra, with the same local geometry as the pyrochlore lattice. We use exact diagonalization, the density matrix renormalization group (DMRG), and minimally entangled typical thermal states (METTS) methods to study the system. When $J_z$ is antiferromagnetic ($J_{z}>0$) and $J_x$ is ferromagnetic ($J_{x}<0$), we find a transition from a spin liquid to an XY ferromagnet, which has power-law correlations at $T=0$. For $J_{z}<0$ and $J_{x}>0$, spin-two excitations are found to have lower energy than spin-one at the transition away from the fully polarized state, showing evidence for incipient spin-nematic order. When both interactions are antiferromagnetic, we find a non-degenerate ground state with no broken symmetries and a robust energy gap. The low energy spectra evolve smoothly from predominantly Ising to predominantly XY interactions. In the spin-liquid regime of small $|J_{x}|$, we study the confinement of monopole-anti-monopole pairs and find that the confinement length scale is larger for $J_x<0$ than for $J_{x}>0$, although both length scales are very short. These results are consistent with a local spin-liquid phase for the Heisenberg antiferromagnet with no broken symmetries. ","Order, Disorder and Monopole Confinement in the Spin-$1/2$ XXZ Model on
  a Pyrochlore Tube"
228,1513356855958859781,1025619269122285568,Saurav Jha,"['Our paper ""Towards Exemplar-free #Continual Learning in Vision Transformers"" gets accepted at #CLVision Workshop to appear in @CVPR proceedings.\n\nWe are among the first to study #Continual Learning in ViTs and the very first to do so w/o exemplars!\n\narXiv: <LINK>']",https://arxiv.org/abs/2203.13167,"In this paper, we investigate the continual learning of Vision Transformers (ViT) for the challenging exemplar-free scenario, with special focus on how to efficiently distill the knowledge of its crucial self-attention mechanism (SAM). Our work takes an initial step towards a surgical investigation of SAM for designing coherent continual learning methods in ViTs. We first carry out an evaluation of established continual learning regularization techniques. We then examine the effect of regularization when applied to two key enablers of SAM: (a) the contextualized embedding layers, for their ability to capture well-scaled representations with respect to the values, and (b) the prescaled attention maps, for carrying value-independent global contextual information. We depict the perks of each distilling strategy on two image recognition benchmarks (CIFAR100 and ImageNet-32) -- while (a) leads to a better overall accuracy, (b) helps enhance the rigidity by maintaining competitive performances. Furthermore, we identify the limitation imposed by the symmetric nature of regularization losses. To alleviate this, we propose an asymmetric variant and apply it to the pooled output distillation (POD) loss adapted for ViTs. Our experiments confirm that introducing asymmetry to POD boosts its plasticity while retaining stability across (a) and (b). Moreover, we acknowledge low forgetting measures for all the compared methods, indicating that ViTs might be naturally inclined continual learner ","Towards Exemplar-Free Continual Learning in Vision Transformers: an
  Account of Attention, Functional and Weight Regularization"
229,1509250726454693891,721001057908834304,Akash Kumar üë®üèª‚Äçüíªüèä,"['Happy to share our #CVPR2022 paper: End-to-End Semi-Supervised Learning for Video Action Detection. We propose a #consistency based approach exploiting Spatio-temporal regularization constraints @UCFCRCV \nPaper: <LINK> \n#phdlife #acceptance #ai #DeepLearning 1/5 <LINK>', 'We explore the #temporal continuity of videos in two ways:\n1) Temporal Coherence computes the #variance of pixels by measuring relative shift.\n2) Gradient Smoothness inspects the second-order gradient of the localization map. \nWe use it as #attention maps to calculate L2 loss.2/5 https://t.co/BfcWd4vjFN', 'Both help in refining detection boundaries. This approach beats the #supervised baseline with only 20% of #labeled data by a margin of 9% and 11% on f-mAP and v-mAP on the #UCF101 dataset. Our approach outperforms all #weakly supervised approaches. 3/5 https://t.co/dZqYEO3eI8', 'We also show improvement using #external bigger datasets such as #kinetics as an unlabeled dataset to improve upon the #supervised baseline. 4/5 https://t.co/CInHfXcxx0', 'Check out our code at: https://t.co/4enS91I48U \n@paperswithcode 5/5', 'Here are some visualizations comparing our work with the proposed #semisupervised image #object #dection and #classification approaches. https://t.co/7575IU6RvR']",https://arxiv.org/abs/2203.04251,"In this work, we focus on semi-supervised learning for video action detection which utilizes both labeled as well as unlabeled data. We propose a simple end-to-end consistency based approach which effectively utilizes the unlabeled data. Video action detection requires both, action class prediction as well as a spatio-temporal localization of actions. Therefore, we investigate two types of constraints, classification consistency, and spatio-temporal consistency. The presence of predominant background and static regions in a video makes it challenging to utilize spatio-temporal consistency for action detection. To address this, we propose two novel regularization constraints for spatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness. Both these aspects exploit the temporal continuity of action in videos and are found to be effective for utilizing unlabeled videos for action detection. We demonstrate the effectiveness of the proposed approach on two different action detection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show the effectiveness of the proposed approach for video object segmentation on the Youtube-VOS which demonstrates its generalization capability The proposed approach achieves competitive performance by using merely 20% of annotations on UCF101-24 when compared with recent fully supervised methods. On UCF101-24, it improves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively, compared to supervised approach. ",End-to-End Semi-Supervised Learning for Video Action Detection
230,1507358577467535387,91613176,Manuel Rivera,"[""We recently wrote this article on string topology for Sullivan's 80th. It summarizes recent advancements all spinning around one of the main questions studied by Dennis: What is the algebraic chain level meaning of a space being a manifold?\n<LINK>\n#Abelprize2022""]",https://arxiv.org/abs/2203.02429,"We describe two major string topology operations, the Chas-Sullivan product and the Goresky-Hingston coproduct, from geometric and algebraic perspectives. The geometric construction uses Thom-Pontrjagin intersection theory while the algebraic construction is phrased in terms of Hochschild homology. We give computations of products and coproducts on lens spaces via geometric intersection, and deduce that the coproduct distinguishes 3-dimensional lens spaces. Algebraically, we describe the structure these operations define together on the Tate-Hochschild complex. We use rational homotopy theory methods to sketch the equivalence between the geometric and algebraic definitions for simply connected manifolds and real coefficients, emphasizing the role of configuration spaces. Finally, we study invariance properties of the operations, both algebraically and geometrically. ",String topology in three flavours
231,1506675204931551234,1207922192916402176,Hongxu (Danny) Yin,"['Our recent work on vision transformers. We found ViTs are surprisingly vulnerable to gradient inversion named as GradViT.\n\npaper: <LINK>\nproject page: <LINK> <LINK>', 'Joint effort with @ahatamiz1 @holgerrroth, wenqi li, @jankautz, daguang xu and @PavloMolchanov at @NVIDIAAI']",https://arxiv.org/abs/2203.11894,"In this work we demonstrate the vulnerability of vision transformers (ViTs) to gradient-based inversion attacks. During this attack, the original data batch is reconstructed given model weights and the corresponding gradients. We introduce a method, named GradViT, that optimizes random noise into naturally looking images via an iterative process. The optimization objective consists of (i) a loss on matching the gradients, (ii) image prior in the form of distance to batch-normalization statistics of a pretrained CNN model, and (iii) a total variation regularization on patches to guide correct recovery locations. We propose a unique loss scheduling function to overcome local minima during optimization. We evaluate GadViT on ImageNet1K and MS-Celeb-1M datasets, and observe unprecedentedly high fidelity and closeness to the original (hidden) data. During the analysis we find that vision transformers are significantly more vulnerable than previously studied CNNs due to the presence of the attention mechanism. Our method demonstrates new state-of-the-art results for gradient inversion in both qualitative and quantitative metrics. Project page at this https URL ",GradViT: Gradient Inversion of Vision Transformers
232,1505049156351668224,4842523568,Rishabh Iyer,"['Can we make HPO and autoML orders of magnitude faster via subset selection?\nWe propose Automata, a gradient-based subset selection approach for compute-efficient HPO!\n\nWe achieve orders of magnitude speedups on SOTA HPO schedulers (hyperband, ASHA..) \n\n<LINK>', 'The figure below contrasts the running times w. and w/o subset selection. We see that just using 1% of the dataset is enough (with the right adaptive subset selection) to get equivalent performance to full training (with minimal acc loss) yet being orders of magnitude faster! https://t.co/fMKt7LbENm', 'The basic idea is to use a data subset selection-based training loop (like GradMatch) where the data subset adapts as the model is trained. This approach can be used with any search algo (TPE, random) or scheduler (hyperband, ASHA). https://t.co/2qsKVXoT75', 'Why only a 1% subset is enough? Unlike model training, in HPO, we care about the ordering of models in choosing hyper-parameters and not the accuracies themselves!', 'We believe there is a lot more room for improvement and we hope approaches like subset selection can be used more in AutoML and NAS!\n\nJoint work with @krishnatejakk (lead author), @ganramkr, @lucian_popa_us (IBM Research), and others!', '@debadeepta Yes, we are currently looking into this actually for NAS. This is exactly our hypothesis that this should also hold for networks but we are looking at principled approaches for ensuring this! \n\nOne question we encountered is if this can be the basis of the formulation...', '@debadeepta but so far, we have not been able to formulate it like that (i.e., select a subset such that the ordering between architectures/hyperparameters is maintained). \n\nAny thoughts here would be great, and would love to chat offline on this!']",https://arxiv.org/abs/2203.08212,"Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset for different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3$\times$-30$\times$ while achieving comparable performance to the hyper-parameters found using the entire dataset. ","AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient
  Hyper-parameter Tuning"
233,1503363872098553858,1246070462679040000,Randall Balestriero,"['Delighted to share our latest preprint with Bobak Kiani, @ylecun, and Seth Lloyd where we propose an **efficient and scalable** gradient based training of orthogonal/unitary matrices (e.g. used in each layer of a recurrent network/convolutional network).\n<LINK> <LINK>', 'When performing iterative updates, full-rank gradients are not needed, a low-rank projection of the update is sufficient. The same applies for standard training... small mini-batch produces low-rank updates, but doing enough of them can bring you anywhere in parameter space.', 'The advantage now is that when applying a low-rank update, projecting your (updated) parameter back onto the orthogonal/unitary manifold is fast. Especially since we found that rank-1 updates were enough regardless of the task and architecture at hand!']",https://arxiv.org/abs/2203.05483,"In learning with recurrent or very deep feed-forward networks, employing unitary matrices in each layer can be very effective at maintaining long-range stability. However, restricting network parameters to be unitary typically comes at the cost of expensive parameterizations or increased training runtime. We propose instead an efficient method based on rank-$k$ updates -- or their rank-$k$ approximation -- that maintains performance at a nearly optimal training runtime. We introduce two variants of this method, named Direct (projUNN-D) and Tangent (projUNN-T) projected Unitary Neural Networks, that can parameterize full $N$-dimensional unitary or orthogonal matrices with a training runtime scaling as $O(kN^2)$. Our method either projects low-rank gradients onto the closest unitary matrix (projUNN-T) or transports unitary matrices in the direction of the low-rank gradient (projUNN-D). Even in the fastest setting ($k=1$), projUNN is able to train a model's unitary parameters to reach comparable performances against baseline implementations. By integrating our projUNN algorithm into both recurrent and convolutional neural networks, our models can closely match or exceed benchmarked results from state-of-the-art algorithms. ","projUNN: efficient method for training deep networks with unitary
  matrices"
